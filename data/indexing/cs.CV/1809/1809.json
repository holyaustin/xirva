[{"id": "1809.00020", "submitter": "Stanley Chan", "authors": "Stanley H. Chan", "title": "Performance Analysis of Plug-and-Play ADMM: A Graph Signal Processing\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Plug-and-Play (PnP) ADMM algorithm is a powerful image restoration\nframework that allows advanced image denoising priors to be integrated into\nphysical forward models to generate high quality image restoration results.\nHowever, despite the enormous number of applications and several theoretical\nstudies trying to prove the convergence by leveraging tools in convex analysis,\nvery little is known about why the algorithm is doing so well. The goal of this\npaper is to fill the gap by discussing the performance of PnP ADMM. By\nrestricting the denoisers to the class of graph filters under a linearity\nassumption, or more specifically the symmetric smoothing filters, we offer\nthree contributions: (1) We show conditions under which an equivalent\nmaximum-a-posteriori (MAP) optimization exists, (2) we present a geometric\ninterpretation and show that the performance gain is due to an intrinsic\npre-denoising characteristic of the PnP prior, (3) we introduce a new analysis\ntechnique via the concept of consensus equilibrium, and provide interpretations\nto problems involving multiple priors.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 18:45:02 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 17:23:45 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 21:20:09 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Chan", "Stanley H.", ""]]}, {"id": "1809.00060", "submitter": "Ga Wu", "authors": "Yu Qing Zhou, Ga Wu, Scott Sanner, Putra Manggala", "title": "Aesthetic Features for Personalized Photo Recommendation", "comments": "In Proceedings of the Late-Breaking Results track part of the Twelfth\n  ACM Conference on Recommender Systems, Vancouver, BC, Canada, October 6,\n  2018, 2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many photography websites such as Flickr, 500px, Unsplash, and Adobe Behance\nare used by amateur and professional photography enthusiasts. Unlike\ncontent-based image search, such users of photography websites are not just\nlooking for photos with certain content, but more generally for photos with a\ncertain photographic \"aesthetic\". In this context, we explore personalized\nphoto recommendation and propose two aesthetic feature extraction methods based\non (i) color space and (ii) deep style transfer embeddings. Using a dataset\nfrom 500px, we evaluate how these features can be best leveraged by\ncollaborative filtering methods and show that (ii) provides a significant boost\nin photo recommendation performance.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 20:57:26 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Zhou", "Yu Qing", ""], ["Wu", "Ga", ""], ["Sanner", "Scott", ""], ["Manggala", "Putra", ""]]}, {"id": "1809.00072", "submitter": "Shubham Jain", "authors": "Shubham Jain, Abhronil Sengupta, Kaushik Roy, Anand Raghunathan", "title": "RxNN: A Framework for Evaluating Deep Neural Networks on Resistive\n  Crossbars", "comments": "13 pages, 16 figures, Accepted in IEEE Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems (TCAD) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resistive crossbars designed with non-volatile memory devices have emerged as\npromising building blocks for Deep Neural Network (DNN) hardware, due to their\nability to compactly and efficiently realize vector-matrix multiplication\n(VMM), the dominant computational kernel in DNNs. However, a key challenge with\nresistive crossbars is that they suffer from a range of device and circuit\nlevel non-idealities such as interconnect parasitics, peripheral circuits,\nsneak paths, and process variations. These non-idealities can lead to errors in\nVMMs, eventually degrading the DNN's accuracy. It is therefore critical to\nstudy the impact of crossbar non-idealities on the accuracy of large-scale\nDNNs. However, this is challenging because existing device and circuit models\nare too slow to use in application-level evaluations.\n  We present RxNN, a fast and accurate simulation framework to evaluate\nlarge-scale DNNs on resistive crossbar systems. RxNN splits and maps the\ncomputations involved in each DNN layer into crossbar operations, and evaluates\nthem using a Fast Crossbar Model (FCM) that accurately captures the errors\narising due to crossbar non-idealities while being four-to-five orders of\nmagnitude faster than circuit simulation. FCM models a crossbar-based VMM\noperation using three stages - non-linear models for the input and output\nperipheral circuits (DACs and ADCs), and an equivalent non-ideal conductance\nmatrix for the core crossbar array. We implement RxNN by extending the Caffe\nmachine learning framework and use it to evaluate a suite of six large-scale\nDNNs developed for the ImageNet Challenge. Our experiments reveal that\nresistive crossbar non-idealities can lead to significant accuracy degradations\n(9.6%-32%) for these large-scale DNNs. To the best of our knowledge, this work\nis the first quantitative evaluation of the accuracy of large-scale DNNs on\nresistive crossbar based hardware.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 22:22:53 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 15:20:13 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 03:33:11 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Jain", "Shubham", ""], ["Sengupta", "Abhronil", ""], ["Roy", "Kaushik", ""], ["Raghunathan", "Anand", ""]]}, {"id": "1809.00076", "submitter": "Ken C. L. Wong", "authors": "Ken C. L. Wong, Mehdi Moradi, Hui Tang, and Tanveer Syeda-Mahmood", "title": "3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced\n  Object Sizes", "comments": "Accepted by the International Conference on Medical Image Computing\n  and Computer-Assisted Intervention - MICCAI 2018 (oral presentation,\n  acceptance rate 4%)", "journal-ref": null, "doi": "10.1007/978-3-030-00931-1_70", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of fully convolutional neural networks, deep learning\nhas raised the benchmark for medical image segmentation on both speed and\naccuracy, and different networks have been proposed for 2D and 3D segmentation\nwith promising results. Nevertheless, most networks only handle relatively\nsmall numbers of labels (<10), and there are very limited works on handling\nhighly unbalanced object sizes especially in 3D segmentation. In this paper, we\npropose a network architecture and the corresponding loss function which\nimprove segmentation of very small structures. By combining skip connections\nand deep supervision with respect to the computational feasibility of 3D\nsegmentation, we propose a fast converging and computationally efficient\nnetwork architecture for accurate segmentation. Furthermore, inspired by the\nconcept of focal loss, we propose an exponential logarithmic loss which\nbalances the labels not only by their relative sizes but also by their\nsegmentation difficulties. We achieve an average Dice coefficient of 82% on\nbrain segmentation with 20 labels, with the ratio of the smallest to largest\nobject sizes as 0.14%. Less than 100 epochs are required to reach such\naccuracy, and segmenting a 128x128x128 volume only takes around 0.4 s.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 22:42:12 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 20:25:03 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Wong", "Ken C. L.", ""], ["Moradi", "Mehdi", ""], ["Tang", "Hui", ""], ["Syeda-Mahmood", "Tanveer", ""]]}, {"id": "1809.00084", "submitter": "Ishtar Nyawira", "authors": "Ishtar Nyawira, Kristi Bushman, Iris Qian, Annie Zhang", "title": "Understanding Neural Pathways in Zebrafish through Deep Learning and\n  High Resolution Electron Microscope Data", "comments": "8 pages, 5 figures (1a to 5c), PEARC '18: Practice and Experience in\n  Advanced Research Computing, July 22--26, 2018, Pittsburgh, PA, USA", "journal-ref": "PEARC '18 Proceedings of the Practice and Experience on Advanced\n  Research Computing, Article No. 65, 2018", "doi": "10.1145/3219104.3229285", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tracing of neural pathways through large volumes of image data is an\nincredibly tedious and time-consuming process that significantly encumbers\nprogress in neuroscience. We are exploring deep learning's potential to\nautomate segmentation of high-resolution scanning electron microscope (SEM)\nimage data to remove that barrier. We have started with neural pathway tracing\nthrough 5.1GB of whole-brain serial-section slices from larval zebrafish\ncollected by the Center for Brain Science at Harvard University. This kind of\nmanual image segmentation requires years of careful work to properly trace the\nneural pathways in an organism as small as a zebrafish larva (approximately 5mm\nin total body length). In automating this process, we would vastly improve\nproductivity, leading to faster data analysis and breakthroughs in\nunderstanding the complexity of the brain. We will build upon prior attempts to\nemploy deep learning for automatic image segmentation extending methods for\nunconventional deep learning data.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 23:42:11 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Nyawira", "Ishtar", ""], ["Bushman", "Kristi", ""], ["Qian", "Iris", ""], ["Zhang", "Annie", ""]]}, {"id": "1809.00085", "submitter": "Ishtar Nyawira", "authors": "Ishtar Nyawira, Kristi Bushman", "title": "A Simplified Approach to Deep Learning for Image Segmentation", "comments": "8 pages, 6 figures (1a to 6c, plus 5 in appendix), PEARC '18:\n  Practice and Experience in Advanced Research Computing, July 22--26, 2018,\n  Pittsburgh, PA, USA", "journal-ref": "PEARC '18 Proceedings of the Practice and Experience on Advanced\n  Research Computing, Article No. 56, 2018", "doi": "10.1145/3219104.3229286", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leaping into the rapidly developing world of deep learning is an exciting and\nsometimes confusing adventure. All of the advice and tutorials available can be\nhard to organize and work through, especially when training specific models on\nspecific datasets, different from those originally used to train the network.\nIn this short guide, we aim to walk the reader through the techniques that we\nhave used to successfully train two deep neural networks for pixel-wise\nclassification, including some data management and augmentation approaches for\nworking with image data that may be insufficiently annotated or relatively\nhomogenous.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 23:46:38 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Nyawira", "Ishtar", ""], ["Bushman", "Kristi", ""]]}, {"id": "1809.00095", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, Jungwon Lee", "title": "Learning Sparse Low-Precision Neural Networks With Learnable\n  Regularization", "comments": "IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2020.2996936", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning deep neural networks (DNNs) that consist of\nlow-precision weights and activations for efficient inference of fixed-point\noperations. In training low-precision networks, gradient descent in the\nbackward pass is performed with high-precision weights while quantized\nlow-precision weights and activations are used in the forward pass to calculate\nthe loss function for training. Thus, the gradient descent becomes suboptimal,\nand accuracy loss follows. In order to reduce the mismatch in the forward and\nbackward passes, we utilize mean squared quantization error (MSQE)\nregularization. In particular, we propose using a learnable regularization\ncoefficient with the MSQE regularizer to reinforce the convergence of\nhigh-precision weights to their quantized values. We also investigate how\npartial L2 regularization can be employed for weight pruning in a similar\nmanner. Finally, combining weight pruning, quantization, and entropy coding, we\nestablish a low-precision DNN compression pipeline. In our experiments, the\nproposed method yields low-precision MobileNet and ShuffleNet models on\nImageNet classification with the state-of-the-art compression ratios of 7.13\nand 6.79, respectively. Moreover, we examine our method for image super\nresolution networks to produce 8-bit low-precision models at negligible\nperformance loss.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 01:28:21 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 00:41:54 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1809.00101", "submitter": "Lingbo Liu", "authors": "Lingbo Liu, Ruimao Zhang, Jiefeng Peng, Guanbin Li, Bowen Du, and\n  Liang Lin", "title": "Attentive Crowd Flow Machines", "comments": "ACM MM, full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic flow prediction is crucial for urban traffic management and public\nsafety. Its key challenges lie in how to adaptively integrate the various\nfactors that affect the flow changes. In this paper, we propose a unified\nneural network module to address this problem, called Attentive Crowd Flow\nMachine~(ACFM), which is able to infer the evolution of the crowd flow by\nlearning dynamic representations of temporally-varying data with an attention\nmechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units\nconnected with a convolutional layer for spatial weight prediction. The first\nLSTM takes the sequential flow density representation as input and generates a\nhidden state at each time-step for attention map inference, while the second\nLSTM aims at learning the effective spatial-temporal feature expression from\nattentionally weighted crowd flow features. Based on the ACFM, we further build\na deep architecture with the application to citywide crowd flow prediction,\nwhich naturally incorporates the sequential and periodic data as well as other\nexternal influences. Extensive experiments on two standard benchmarks (i.e.,\ncrowd flow in Beijing and New York City) show that the proposed method achieves\nsignificant improvements over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 02:22:57 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Liu", "Lingbo", ""], ["Zhang", "Ruimao", ""], ["Peng", "Jiefeng", ""], ["Li", "Guanbin", ""], ["Du", "Bowen", ""], ["Lin", "Liang", ""]]}, {"id": "1809.00110", "submitter": "Xiaowei Xu", "authors": "Xiaowei Xu, Xinyi Zhang, Bei Yu, X. Sharon Hu, Christopher Rowen,\n  Jingtong Hu, Yiyu Shi", "title": "DAC-SDC Low Power Object Detection Challenge for UAV Applications", "comments": "12 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 55th Design Automation Conference (DAC) held its first System Design\nContest (SDC) in 2018. SDC'18 features a lower power object detection challenge\n(LPODC) on designing and implementing novel algorithms based object detection\nin images taken from unmanned aerial vehicles (UAV). The dataset includes 95\ncategories and 150k images, and the hardware platforms include Nvidia's TX2 and\nXilinx's PYNQ Z1. DAC-SDC'18 attracted more than 110 entries from 12 countries.\nThis paper presents in detail the dataset and evaluation procedure. It further\ndiscusses the methods developed by some of the entries as well as\nrepresentative results. The paper concludes with directions for future\nimprovements.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 03:37:37 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Xu", "Xiaowei", ""], ["Zhang", "Xinyi", ""], ["Yu", "Bei", ""], ["Hu", "X. Sharon", ""], ["Rowen", "Christopher", ""], ["Hu", "Jingtong", ""], ["Shi", "Yiyu", ""]]}, {"id": "1809.00168", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Implications of Ocular Pathologies for Iris Recognition Reliability", "comments": "Image and Vision Computing (2016)", "journal-ref": null, "doi": "10.1016/j.imavis.2016.08.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an analysis of how iris recognition is influenced by eye\ndisease and an appropriate dataset comprising 2996 images of irises taken from\n230 distinct eyes (including 184 affected by more than 20 different eye\nconditions). The images were collected in near infrared and visible light\nduring routine ophthalmological examination. The experimental study carried out\nutilizing four independent iris recognition algorithms (MIRLIN, VeriEye, OSIRIS\nand IriCore) renders four valuable results. First, the enrollment process is\nhighly sensitive to those eye conditions that obstruct the iris or cause\ngeometrical distortions. Second, even those conditions that do not produce\nvisible changes to the structure of the iris may increase the dissimilarity\nbetween samples of the same eyes. Third, eye conditions affecting the geometry\nor the tissue structure of the iris or otherwise producing obstructions\nsignificantly decrease same-eye similarity and have a lower, yet still\nstatistically significant, influence on impostor comparison scores. Fourth, for\nunhealthy eyes, the most prominent effect of disease on iris recognition is to\ncause segmentation errors. To our knowledge this paper describes the largest\ndatabase of iris images for disease-affected eyes made publicly available to\nresearchers and offers the most comprehensive study of what we can expect when\niris recognition is employed for diseased eyes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 13:03:48 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1809.00170", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz", "title": "Linear regression analysis of template aging in iris biometrics", "comments": "Accepted manuscript version of the IEEE IWBF2015 paper", "journal-ref": "3rd International Workshop on Biometrics and Forensics (IWBF\n  2015), Gjovik, 2015, pp. 1-6", "doi": "10.1109/IWBF.2015.7110233", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to determine how vulnerable different iris coding\nmethods are in relation to biometric template aging phenomenon. This is\nconsidered to be particularly important when the time lapse between gallery and\nprobe samples extends significantly, to more than a few years.\n  Our experiments employ iris aging analysis conducted using three different\niris recognition algorithms and a database of 583 samples from 58 irises\ncollected up to nine years apart. To determine the degradation rates of\nsimilarity scores with extending time lapse and also in relation to multiple\nimage quality and geometrical factors of sample images, a linear regression\nanalysis was performed. 29 regression models have been tested with both the\ntime parameter and geometrical factors being statistically significant in every\nmodel. Quality measures that showed statistically significant influence on the\npredicted variable were, depending on the method, image sharpness and local\ncontrast or their mutual relations.\n  To our best knowledge, this is the first paper describing aging analysis\nusing multiple regression models with data covering such a wide time period.\nResults presented suggest that template aging effect occurs in iris biometrics\nto a statistically significant extent. Image quality and geometrical factors\nmay contribute to the degradation of similarity score. However, the estimate of\ntime parameter showed statistical significance and similar value in each of the\ntested models. This reveals that the aging phenomenon may as well be unrelated\nto quality and geometrical measures of the image.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 13:12:32 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""]]}, {"id": "1809.00174", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Human Iris Recognition in Post-mortem Subjects: Study and Database", "comments": "Accepted manuscript version of the BTAS2016 paper", "journal-ref": "2016 IEEE 8th International Conference on Biometrics Theory,\n  Applications and Systems (BTAS), Niagara Falls, NY, 2016, pp. 1-6", "doi": "10.1109/BTAS.2016.7791175", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unique study of post-mortem human iris recognition and\nthe first known to us database of near-infrared and visible-light iris images\nof deceased humans collected up to almost 17 days after death. We used four\ndifferent iris recognition methods to analyze the dynamics of iris quality\ndecay in short-term comparisons (samples collected up to 60 hours after death)\nand long-term comparisons (for samples acquired up to 407 hours after demise).\nThis study shows that post-mortem iris recognition is possible and occasionally\nworks even 17 days after death. These conclusions contradict a promulgated\nrumor that iris is unusable shortly after decease. We make this dataset\npublicly available to let others verify our findings and to research new\naspects of this important and unfamiliar topic. We are not aware of any earlier\npapers offering post-mortem human iris images and such comprehensive analysis\nemploying four different matchers.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 13:26:18 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1809.00182", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Iris Recognition Under Biologically Troublesome Conditions - Effects of\n  Aging, Diseases and Post-mortem Changes", "comments": "Accepted manuscript version of the BIOSIGNALS 2017 paper", "journal-ref": "Proceedings of the 10th International Joint Conference on\n  Biomedical Engineering Systems and Technologies - Volume 4: BIOSIGNALS, pages\n  253-258, 2017, Porto, Portugal", "doi": "10.5220/0006251702530258", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the most comprehensive analysis of iris recognition\nreliability in the occurrence of various biological processes happening\nnaturally and pathologically in the human body, including aging, illnesses, and\npost-mortem changes to date. Insightful conclusions are offered in relation to\nall three of these aspects. Extensive regression analysis of the template aging\nphenomenon shows that differences in pupil dilation, combined with certain\nquality factors of the sample image and the progression of time itself can\nsignificantly degrade recognition accuracy. Impactful effects can also be\nobserved when iris recognition is employed with eyes affected by certain eye\npathologies or (even more) with eyes of the deceased subjects. Notably,\nappropriate databases are delivered to the biometric community to stimulate\nfurther research in these utterly important areas of iris biometrics studies.\nFinally, some open questions are stated to inspire further discussions and\nresearch on these important topics. To Authors' best knowledge, this is the\nonly scientific study of iris recognition reliability of such a broad scope and\nnovelty.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 13:51:07 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1809.00193", "submitter": "Tianyang Wang", "authors": "Tianyang Wang, Jun Huan, Bo Li", "title": "Data Dropout: Optimizing Training Data for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models learn to fit training data while they are highly\nexpected to generalize well to testing data. Most works aim at finding such\nmodels by creatively designing architectures and fine-tuning parameters. To\nadapt to particular tasks, hand-crafted information such as image prior has\nalso been incorporated into end-to-end learning. However, very little progress\nhas been made on investigating how an individual training sample will influence\nthe generalization ability of a model. In other words, to achieve high\ngeneralization accuracy, do we really need all the samples in a training\ndataset? In this paper, we demonstrate that deep learning models such as\nconvolutional neural networks may not favor all training samples, and\ngeneralization accuracy can be further improved by dropping those unfavorable\nsamples. Specifically, the influence of removing a training sample is\nquantifiable, and we propose a Two-Round Training approach, aiming to achieve\nhigher generalization accuracy. We locate unfavorable samples after the first\nround of training, and then retrain the model from scratch with the reduced\ntraining dataset in the second round. Since our approach is essentially\ndifferent from fine-tuning or further training, the computational cost should\nnot be a concern. Our extensive experimental results indicate that, with\nidentical settings, the proposed approach can boost performance of the\nwell-known networks on both high-level computer vision problems such as image\nclassification, and low-level vision problems such as image denoising.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 14:24:36 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 10:09:13 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Wang", "Tianyang", ""], ["Huan", "Jun", ""], ["Li", "Bo", ""]]}, {"id": "1809.00204", "submitter": "Stephan Baier", "authors": "Stephan Baier, Yunpu Ma, Volker Tresp", "title": "Improving Visual Relationship Detection using Semantic Modeling of Scene\n  Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured scene descriptions of images are useful for the automatic\nprocessing and querying of large image databases. We show how the combination\nof a semantic and a visual statistical model can improve on the task of mapping\nimages to their associated scene description. In this paper we consider scene\ndescriptions which are represented as a set of triples (subject, predicate,\nobject), where each triple consists of a pair of visual objects, which appear\nin the image, and the relationship between them (e.g. man-riding-elephant,\nman-wearing-hat). We combine a standard visual model for object detection,\nbased on convolutional neural networks, with a latent variable model for link\nprediction. We apply multiple state-of-the-art link prediction methods and\ncompare their capability for visual relationship detection. One of the main\nadvantages of link prediction methods is that they can also generalize to\ntriples, which have never been observed in the training data. Our experimental\nresults on the recently published Stanford Visual Relationship dataset, a\nchallenging real world dataset, show that the integration of a semantic model\nusing link prediction methods can significantly improve the results for visual\nrelationship detection. Our combined approach achieves superior performance\ncompared to the state-of-the-art method from the Stanford computer vision\ngroup.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 15:11:12 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Baier", "Stephan", ""], ["Ma", "Yunpu", ""], ["Tresp", "Volker", ""]]}, {"id": "1809.00206", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Assessment of iris recognition reliability for eyes affected by ocular\n  pathologies", "comments": "Manuscript accepted for publication at IEEE BTAS 2015. arXiv admin\n  note: text overlap with arXiv:1809.00168", "journal-ref": "2015 IEEE 7th International Conference on Biometrics Theory,\n  Applications and Systems (BTAS), Arlington, VA, 2015, pp. 1-6", "doi": "10.1109/BTAS.2015.7358747", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an analysis of how the iris recognition is impacted by\neye diseases and an appropriate dataset comprising 2996 iris images of 230\ndistinct eyes (including 184 illness-affected eyes representing more than 20\ndifferent eye conditions). The images were collected in near infrared and\nvisible light during a routine ophthalmological practice. The experimental\nstudy shows four valuable results. First, the enrollment process is highly\nsensitive to those eye conditions that make the iris obstructed or introduce\ngeometrical distortions. Second, even those conditions that do not produce\nvisible changes to the iris structure may increase the dissimilarity among\nsamples of the same eyes. Third, eye conditions affecting iris geometry, its\ntissue structure or producing obstructions significantly decrease the iris\nrecognition reliability. Fourth, for eyes afflicted by a disease, the most\nprominent effect of the disease on iris recognition is to cause segmentation\nerrors. To our knowledge this is the first database of iris images for\ndisease-affected eyes made publicly available to researchers, and the most\ncomprehensive study of what we can expect when the iris recognition is deployed\nfor non-healthy eyes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 15:17:43 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1809.00208", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Post-mortem Human Iris Recognition", "comments": "Accepted for publication version of the manuscript submitted for the\n  IEEE ICB 2016", "journal-ref": "2016 International Conference on Biometrics (ICB), Halmstad, 2016,\n  pp. 1-6", "doi": "10.1109/ICB.2016.7550073", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unique analysis of post-mortem human iris recognition.\nPost-mortem human iris images were collected at the university mortuary in\nthree sessions separated by approximately 11 hours, with the first session\norganized from 5 to 7 hours after demise. Analysis performed for four\nindependent iris recognition methods shows that the common claim of the iris\nbeing useless for biometric identification soon after death is not entirely\ntrue. Since the pupil has a constant and neutral dilation after death (the so\ncalled \"cadaveric position\"), this makes the iris pattern perfectly visible\nfrom the standpoint of dilation. We found that more than 90% of irises are\nstill correctly recognized when captured a few hours after death, and that\nserious iris deterioration begins approximately 22 hours later, since the\nrecognition rate drops to a range of 13.3-73.3% (depending on the method used)\nwhen the cornea starts to be cloudy. There were only two failures to enroll\n(out of 104 images) observed for only a single method (out of four employed in\nthis study). These findings show that the dynamics of post-mortem changes to\nthe iris that are important for biometric identification are much more moderate\nthan previously believed. To the best of our knowledge, this paper presents the\nfirst experimental study of how iris recognition works after death, and we hope\nthat these preliminary findings will stimulate further research in this area.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 15:24:57 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1809.00211", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Cataract influence on iris recognition performance", "comments": null, "journal-ref": "Photonics Applications in Astronomy, Communications, Industry, and\n  High-Energy Physics Experiments 2014, 929020 (2014)", "doi": "10.1117/12.2076040", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the experimental study revealing weaker performance of\nthe automatic iris recognition methods for cataract-affected eyes when compared\nto healthy eyes. There is little research on the topic, mostly incorporating\nscarce databases that are often deficient in images representing more than one\nillness. We built our own database, acquiring 1288 eye images of 37 patients of\nthe Medical University of Warsaw. Those images represent several common ocular\ndiseases, such as cataract, along with less ordinary conditions, such as iris\npattern alterations derived from illness or eye trauma. Images were captured in\nnear-infrared light (used in biometrics) and for selected cases also in visible\nlight (used in ophthalmological diagnosis). Since cataract is a disorder that\nis most populated by samples in the database, in this paper we focus solely on\nthis illness. To assess the extent of the performance deterioration we use\nthree iris recognition methodologies (commercial and academic solutions) to\ncalculate genuine match scores for healthy eyes and those influenced by\ncataract. Results show a significant degradation in iris recognition\nreliability manifesting by worsening the genuine scores in all three matchers\nused in this study (12% of genuine score increase for an academic matcher, up\nto 175% of genuine score increase obtained for an example commercial matcher).\nThis increase in genuine scores affected the final false non-match rate in two\nmatchers. To our best knowledge this is the only study of such kind that\nemploys more than one iris matcher, and analyzes the iris image segmentation as\na potential source of decreased reliability.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 15:40:47 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1809.00212", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Database of iris images acquired in the presence of ocular pathologies\n  and assessment of iris recognition reliability for disease-affected eyes", "comments": null, "journal-ref": "2015 IEEE 2nd International Conference on Cybernetics (CYBCONF),\n  Gdynia, 2015, pp. 495-500", "doi": "10.1109/CYBConf.2015.7175984", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a database of iris images collected from disease affected\neyes and an analysis related to the influence of ocular diseases on iris\nrecognition reliability. For that purpose we have collected a database of iris\nimages acquired for 91 different eyes during routine ophthalmology visits. This\ncollection gathers samples for healthy eyes as well as those with various eye\npathologies, including cataract, acute glaucoma, posterior and anterior\nsynechiae, retinal detachment, rubeosis iridis, corneal vascularization,\ncorneal grafting, iris damage and atrophy and corneal ulcers, haze or\nopacities. To our best knowledge this is the first database of such kind that\nwill be made publicly available. In the analysis the data were divided into\nfive groups of samples presenting similar anticipated impact on iris\nrecognition: 1) healthy (no impact), 2) unaffected, clear iris (although the\nillness was detected), 3) geometrically distorted irides, 4) distorted iris\ntissue and 5) obstructed iris tissue. Three different iris recognition methods\n(MIRLIN, VeriEye and OSIRIS) were then used to find differences in average\ngenuine and impostor comparison scores calculated for healthy eyes and those\nimpacted by a disease. Specifically, we obtained significantly worse genuine\ncomparison scores for all iris matchers and all disease-affected eyes when\ncompared to a group of healthy eyes, what have a high potential of impacting\nfalse non-match rate.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 15:47:23 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1809.00213", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Mateusz Szadkowski", "title": "Iris and periocular recognition in arabian race horses using deep\n  convolutional neural networks", "comments": null, "journal-ref": "2017 IEEE International Joint Conference on Biometrics (IJCB),\n  Denver, CO, 2017, pp. 510-516", "doi": "10.1109/BTAS.2017.8272736", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study devoted to recognizing horses by means of their\niris and periocular features using deep convolutional neural networks (DCNNs).\nIdentification of race horses is crucial for animal identity confirmation prior\nto racing. As this is usually done shortly before a race, fast and reliable\nmethods that are friendly and inflict no harm upon animals are important. Iris\nrecognition has been shown to work with horse irides, provided that algorithms\ndeployed for such task are fine-tuned for horse irides and input data is of\nvery high quality. In our work, we examine a possibility of utilizing deep\nconvolutional neural networks for a fusion of both iris and periocular region\nfeatures. With such methodology, ocular biometrics in horses could perform well\nwithout employing complicated algorithms that require a lot of fine-tuning and\nprior knowledge of the input image, while at the same time being rotation,\ntranslation, and to some extent also image quality invariant. We were able to\nachieve promising results, with EER=9.5% using two network architectures with\nscore-level fusion.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 15:54:29 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Szadkowski", "Mateusz", ""]]}, {"id": "1809.00214", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz", "title": "Iris Recognition with a Database of Iris Images Obtained in Visible\n  Light Using Smartphone Camera", "comments": "Accepted version of the IEEE ISBA 2016 conference", "journal-ref": "2016 IEEE International Conference on Identity, Security and\n  Behavior Analysis (ISBA), Sendai, 2016, pp. 1-6", "doi": "10.1109/ISBA.2016.7477233", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper delivers a new database of iris images collected in visible light\nusing a mobile phone's camera and presents results of experiments involving\nexisting commercial and open-source iris recognition methods, namely: IriCore,\nVeriEye, MIRLIN and OSIRIS. Several important observations are made.\n  First, we manage to show that after simple preprocessing, such images offer\ngood visibility of iris texture even in heavily-pigmented irides. Second, for\nall four methods, the enrollment stage is not much affected by the fact that\ndifferent type of data is used as input. This translates to zero or\nclose-to-zero Failure To Enroll, i.e., cases when templates could not be\nextracted from the samples. Third, we achieved good matching accuracy, with\ncorrect genuine match rate exceeding 94.5% for all four methods, while\nsimultaneously being able to maintain zero false match rate in every case.\nCorrect genuine match rate of over 99.5% was achieved using one of the\ncommercial methods, showing that such images can be used with the existing\nbiometric solutions with minimum additional effort required. Finally, the\nexperiments revealed that incorrect image segmentation is the most prevalent\ncause of recognition accuracy decrease.\n  To our best knowledge, this is the first database of iris images captured\nusing a mobile device, in which image quality exceeds this of a near-infrared\nilluminated iris images, as defined in ISO/IEC 19794-6 and 29794-6 documents.\nThis database will be publicly available to all researchers.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 15:55:40 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""]]}, {"id": "1809.00216", "submitter": "Lucas Schelkes", "authors": "Lucas Schelkes", "title": "Evaluation of Neural Networks for Image Recognition Applications:\n  Designing a 0-1 MILP Model of a CNN to create adversarials", "comments": "Thesis Bergische Universit\\\"at Wuppertal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Recognition is a central task in computer vision with applications\nranging across search, robotics, self-driving cars and many others. There are\nthree purposes of this document: 1. We follow up on (Fischetti & Jo, December,\n2017) and show how standard convolutional neural network can be optimized to a\nmore sophisticated capsule architecture. 2. We introduce a MILP model based on\nCNN to create adversarials. 3. We compare and evaluate each network for image\nrecognition tasks.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 16:10:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Schelkes", "Lucas", ""]]}, {"id": "1809.00219", "submitter": "Xintao Wang", "authors": "Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen\n  Change Loy, Yu Qiao, Xiaoou Tang", "title": "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks", "comments": "To appear in ECCV 2018 workshop. Won Region 3 in the PIRM2018-SR\n  Challenge. Code and models are at https://github.com/xinntao/ESRGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work\nthat is capable of generating realistic textures during single image\nsuper-resolution. However, the hallucinated details are often accompanied with\nunpleasant artifacts. To further enhance the visual quality, we thoroughly\nstudy three key components of SRGAN - network architecture, adversarial loss\nand perceptual loss, and improve each of them to derive an Enhanced SRGAN\n(ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block\n(RRDB) without batch normalization as the basic network building unit.\nMoreover, we borrow the idea from relativistic GAN to let the discriminator\npredict relative realness instead of the absolute value. Finally, we improve\nthe perceptual loss by using the features before activation, which could\nprovide stronger supervision for brightness consistency and texture recovery.\nBenefiting from these improvements, the proposed ESRGAN achieves consistently\nbetter visual quality with more realistic and natural textures than SRGAN and\nwon the first place in the PIRM2018-SR Challenge. The code is available at\nhttps://github.com/xinntao/ESRGAN .\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 16:21:03 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 07:00:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Wang", "Xintao", ""], ["Yu", "Ke", ""], ["Wu", "Shixiang", ""], ["Gu", "Jinjin", ""], ["Liu", "Yihao", ""], ["Dong", "Chao", ""], ["Loy", "Chen Change", ""], ["Qiao", "Yu", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1809.00226", "submitter": "Zongji Wang", "authors": "Zongji Wang, Feng Lu", "title": "VoxSegNet: Volumetric CNNs for Semantic Part Segmentation of 3D Shapes", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voxel is an important format to represent geometric data, which has been\nwidely used for 3D deep learning in shape analysis due to its generalization\nability and regular data format. However, fine-grained tasks like part\nsegmentation require detailed structural information, which increases voxel\nresolution and thus causes other issues such as the exhaustion of computational\nresources. In this paper, we propose a novel volumetric convolutional neural\nnetwork, which could extract discriminative features encoding detailed\ninformation from voxelized 3D data under a limited resolution. To this purpose,\na spatial dense extraction (SDE) module is designed to preserve the spatial\nresolution during the feature extraction procedure, alleviating the loss of\ndetail caused by sub-sampling operations such as max-pooling. An attention\nfeature aggregation (AFA) module is also introduced to adaptively select\ninformative features from different abstraction scales, leading to segmentation\nwith both semantic consistency and high accuracy of details. Experiment results\non the large-scale dataset demonstrate the effectiveness of our method in 3D\nshape part segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 17:34:08 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wang", "Zongji", ""], ["Lu", "Feng", ""]]}, {"id": "1809.00241", "submitter": "Ankit Parag Shah", "authors": "Ankit Shah, Harini Kesavamoorthy, Poorva Rane, Pramati Kalwad,\n  Alexander Hauptmann, Florian Metze", "title": "Activity Recognition on a Large Scale in Short Videos - Moments in Time\n  Dataset", "comments": "Action recognition submission for Moments in Time Dataset - Improved\n  results over challenge submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Moments capture a huge part of our lives. Accurate recognition of these\nmoments is challenging due to the diverse and complex interpretation of the\nmoments. Action recognition refers to the act of classifying the desired\naction/activity present in a given video. In this work, we perform experiments\non Moments in Time dataset to recognize accurately activities occurring in 3\nsecond clips. We use state of the art techniques for visual, auditory and\nspatio temporal localization and develop method to accurately classify the\nactivity in the Moments in Time dataset. Our novel approach of using Visual\nBased Textual features and fusion techniques performs well providing an overall\n89.23 % Top - 5 accuracy on the 20 classes - a significant improvement over the\nBaseline TRN model.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 19:39:06 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 15:37:34 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Shah", "Ankit", ""], ["Kesavamoorthy", "Harini", ""], ["Rane", "Poorva", ""], ["Kalwad", "Pramati", ""], ["Hauptmann", "Alexander", ""], ["Metze", "Florian", ""]]}, {"id": "1809.00251", "submitter": "Adrian Viera", "authors": "Leonardo Le\\'on, Felipe Moreno-Vera, Renato Castro, Jos\\'e Nav\\'io,\n  Marco Capcha", "title": "Car Monitoring System in Apartment Garages by Small Autonomous Car using\n  Deep Learning", "comments": "13 pages, 12 figures, Version 1 accepted in SimBig 2018. Improving to\n  get better results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Currently, there is an increase in the number of Peruvian families living in\napartments instead of houses for the lots of advantage; However, in some cases\nthere are troubles such as robberies of goods that are usually left at the\nparking lots or the entrance of strangers that use the tenants parking lots\n(this last trouble sometimes is related to kidnappings or robberies in building\napartments). Due to these problems, the use of a self-driving mini-car is\nproposed to implement a monitoring system of license plates in an underground\ngarage inside a building using a deep learning model with the aim of recording\nthe vehicles and identifying their owners if they were tenants or not. In\naddition, the small robot has its own location system using beacons that allow\nus to identify the position of the parking lot corresponding to each tenant of\nthe building while the mini-car is on its way. Finally, one of the objectives\nof this work is to build a low-cost mini-robot that would replace expensive\ncameras or work together in order to keep safe the goods of tenants.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 21:00:58 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 01:00:32 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 21:40:42 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Le\u00f3n", "Leonardo", ""], ["Moreno-Vera", "Felipe", ""], ["Castro", "Renato", ""], ["Nav\u00edo", "Jos\u00e9", ""], ["Capcha", "Marco", ""]]}, {"id": "1809.00263", "submitter": "Qiangeng Xu", "authors": "Qiangeng Xu, Hanwang Zhang, Weiyue Wang, Peter N. Belhumeur, Ulrich\n  Neumann", "title": "Stochastic Dynamics for Video Infilling", "comments": "Winter Conference on Applications of Computer Vision (WACV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a stochastic dynamics video infilling (SDVI)\nframework to generate frames between long intervals in a video. Our task\ndiffers from video interpolation which aims to produce transitional frames for\na short interval between every two frames and increase the temporal resolution.\nOur task, namely video infilling, however, aims to infill long intervals with\nplausible frame sequences. Our framework models the infilling as a constrained\nstochastic generation process and sequentially samples dynamics from the\ninferred distribution. SDVI consists of two parts: (1) a bi-directional\nconstraint propagation module to guarantee the spatial-temporal coherence among\nframes, (2) a stochastic sampling process to generate dynamics from the\ninferred distributions. Experimental results show that SDVI can generate clear\nframe sequences with varying contents. Moreover, motions in the generated\nsequence are realistic and able to transfer smoothly from the given start frame\nto the terminal frame. Our project site is\nhttps://xharlie.github.io/projects/project_sites/SDVI/video_results.html\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 22:58:49 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 03:25:49 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 04:56:46 GMT"}, {"version": "v4", "created": "Thu, 6 Jun 2019 02:24:44 GMT"}, {"version": "v5", "created": "Fri, 7 Jun 2019 09:13:07 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Xu", "Qiangeng", ""], ["Zhang", "Hanwang", ""], ["Wang", "Weiyue", ""], ["Belhumeur", "Peter N.", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1809.00287", "submitter": "Ze Yang", "authors": "Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, Jun Gao, Liwei Wang", "title": "Learning to Navigate for Fine-grained Classification", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained classification is challenging due to the difficulty of finding\ndiscriminative features. Finding those subtle traits that fully characterize\nthe object is not straightforward. To handle this circumstance, we propose a\nnovel self-supervision mechanism to effectively localize informative regions\nwithout the need of bounding-box/part annotations. Our model, termed NTS-Net\nfor Navigator-Teacher-Scrutinizer Network, consists of a Navigator agent, a\nTeacher agent and a Scrutinizer agent. In consideration of intrinsic\nconsistency between informativeness of the regions and their probability being\nground-truth class, we design a novel training paradigm, which enables\nNavigator to detect most informative regions under the guidance from Teacher.\nAfter that, the Scrutinizer scrutinizes the proposed regions from Navigator and\nmakes predictions. Our model can be viewed as a multi-agent cooperation,\nwherein agents benefit from each other, and make progress together. NTS-Net can\nbe trained end-to-end, while provides accurate fine-grained classification\npredictions as well as highly informative regions during inference. We achieve\nstate-of-the-art performance in extensive benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 03:40:08 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Yang", "Ze", ""], ["Luo", "Tiange", ""], ["Wang", "Dong", ""], ["Hu", "Zhiqiang", ""], ["Gao", "Jun", ""], ["Wang", "Liwei", ""]]}, {"id": "1809.00338", "submitter": "Jian Zhao", "authors": "Jian Zhao, Yu Cheng, Yi Cheng, Yang Yang, Haochong Lan, Fang Zhao, Lin\n  Xiong, Yan Xu, Jianshu Li, Sugiri Pranata, Shengmei Shen, Junliang Xing,\n  Hengzhu Liu, Shuicheng Yan, Jiashi Feng", "title": "Look Across Elapse: Disentangled Representation Learning and\n  Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable progress in face recognition related technologies,\nreliably recognizing faces across ages still remains a big challenge. The\nappearance of a human face changes substantially over time, resulting in\nsignificant intra-class variations. As opposed to current techniques for\nage-invariant face recognition, which either directly extract age-invariant\nfeatures for recognition, or first synthesize a face that matches target age\nbefore feature extraction, we argue that it is more desirable to perform both\ntasks jointly so that they can leverage each other. To this end, we propose a\ndeep Age-Invariant Model (AIM) for face recognition in the wild with three\ndistinct novelties. First, AIM presents a novel unified deep architecture\njointly performing cross-age face synthesis and recognition in a mutual\nboosting way. Second, AIM achieves continuous face rejuvenation/aging with\nremarkable photorealistic and identity-preserving properties, avoiding the\nrequirement of paired data and the true age of testing samples. Third, we\ndevelop effective and novel training strategies for end-to-end learning the\nwhole deep architecture, which generates powerful age-invariant face\nrepresentations explicitly disentangled from the age variation. Moreover, we\npropose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset\nto facilitate existing efforts and push the frontiers of age-invariant face\nrecognition research. Extensive experiments on both our CAFR and several other\ncross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the\nproposed AIM model over the state-of-the-arts. Benchmarking our model on one of\nthe most popular unconstrained face recognition datasets IJB-C additionally\nverifies the promising generalizability of AIM in recognizing faces in the\nwild.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 13:58:37 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 01:53:17 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Zhao", "Jian", ""], ["Cheng", "Yu", ""], ["Cheng", "Yi", ""], ["Yang", "Yang", ""], ["Lan", "Haochong", ""], ["Zhao", "Fang", ""], ["Xiong", "Lin", ""], ["Xu", "Yan", ""], ["Li", "Jianshu", ""], ["Pranata", "Sugiri", ""], ["Shen", "Shengmei", ""], ["Xing", "Junliang", ""], ["Liu", "Hengzhu", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1809.00339", "submitter": "Nabeel Mohammed", "authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen", "title": "Chittron: An Automatic Bangla Image Captioning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 14:03:30 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Rahman", "Motiur", ""], ["Mohammed", "Nabeel", ""], ["Mansoor", "Nafees", ""], ["Momen", "Sifat", ""]]}, {"id": "1809.00340", "submitter": "Soumyadeep Debnath Mr.", "authors": "Somnath Rakshit, Soumyadeep Debnath, Dhiman Mondal", "title": "Identifying Land Patterns from Satellite Imagery in Amazon Rainforest\n  using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Amazon rainforests have been suffering widespread damage, both via\nnatural and artificial means. Every minute, it is estimated that the world\nloses forest cover the size of 48 football fields. Deforestation in the Amazon\nrainforest has led to drastically reduced biodiversity, loss of habitat,\nclimate change, and other biological losses. In this respect, it has become\nessential to track how the nature of these forests change over time. Image\nclassification using deep learning can help speed up this process by removing\nthe manual task of classifying each image. Here, it is shown how convolutional\nneural networks can be used to track changes in land patterns in the Amazon\nrainforests. In this work, a testing accuracy of 96.71% was obtained. This can\nhelp governments and other agencies to track changes in land patterns more\neffectively and accurately.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 14:06:39 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Rakshit", "Somnath", ""], ["Debnath", "Soumyadeep", ""], ["Mondal", "Dhiman", ""]]}, {"id": "1809.00365", "submitter": "Ankit Parag Shah", "authors": "Ankit Shah, Tyler Vuong", "title": "Natural Language Person Search Using Deep Reinforcement Learning", "comments": "Equal Contribution - Work in Progress. Preprint results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent success in deep reinforcement learning is having an agent learn how to\nplay Go and beat the world champion without any prior knowledge of the game. In\nthat task, the agent has to make a decision on what action to take based on the\npositions of the pieces. Person Search is recently explored using natural\nlanguage based text description of images for video surveillance applications\n(S.Li et.al). We see (Fu.et al) provides an end to end approach for\nobject-based retrieval using deep reinforcement learning without constraints\nplaced on which objects are being detected. However, we believe for real-world\napplications such as person search defining specific constraints which identify\na person as opposed to starting with a general object detection will have\nbenefits in terms of performance and computational resources required. In our\ntask, Deep reinforcement learning would localize the person in an image by\nreshaping the sizes of the bounding boxes. Deep Reinforcement learning with\nappropriate constraints would look only for the relevant person in the image as\nopposed to an unconstrained approach where each individual objects in the image\nare ranked. For person search, the agent is trying to form a tight bounding box\naround the person in the image who matches the description. The bounding box is\ninitialized to the full image and at each time step, the agent makes a decision\non how to change the current bounding box so that it has a tighter bound around\nthe person based on the description of the person and the pixel values of the\ncurrent bounding box. After the agent takes an action, it will be given a\nreward based on the Intersection over Union (IoU) of the current bounding box\nand the ground truth box. Once the agent believes that the bounding box is\ncovering the person, it will indicate that the person is found.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 16:19:20 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Shah", "Ankit", ""], ["Vuong", "Tyler", ""]]}, {"id": "1809.00397", "submitter": "Sowmya Munukutla", "authors": "Akshita Mittel, Sowmya Munukutla, Himanshi Yadav", "title": "Visual Transfer between Atari Games using Competitive Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of deep reinforcement learning agents to transfer\nknowledge from one environment to another. More specifically, the method takes\nadvantage of asynchronous advantage actor critic (A3C) architecture to\ngeneralize a target game using an agent trained on a source game in Atari.\nInstead of fine-tuning a pre-trained model for the target game, we propose a\nlearning approach to update the model using multiple agents trained in parallel\nwith different representations of the target game. Visual mapping between video\nsequences of transfer pairs is used to derive new representations of the target\ngame; training on these visual representations of the target game improves\nmodel updates in terms of performance, data efficiency and stability. In order\nto demonstrate the functionality of the architecture, Atari games Pong-v0 and\nBreakout-v0 are being used from the OpenAI gym environment; as the source and\ntarget environment.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 21:34:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Mittel", "Akshita", ""], ["Munukutla", "Sowmya", ""], ["Yadav", "Himanshi", ""]]}, {"id": "1809.00402", "submitter": "Alejandro Cartas", "authors": "Alejandro Cartas and Estefania Talavera and Petia Radeva and Mariella\n  Dimiccoli", "title": "On the Role of Event Boundaries in Egocentric Activity Recognition from\n  Photostreams", "comments": "Presented as a short abstract in the EPIC workshop at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event boundaries play a crucial role as a pre-processing step for detection,\nlocalization, and recognition tasks of human activities in videos. Typically,\nalthough their intrinsic subjectiveness, temporal bounds are provided manually\nas input for training action recognition algorithms. However, their role for\nactivity recognition in the domain of egocentric photostreams has been so far\nneglected. In this paper, we provide insights of how automatically computed\nboundaries can impact activity recognition results in the emerging domain of\negocentric photostreams. Furthermore, we collected a new annotated dataset\nacquired by 15 people by a wearable photo-camera and we used it to show the\ngeneralization capabilities of several deep learning based architectures to\nunseen users.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 22:09:26 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 13:50:26 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Cartas", "Alejandro", ""], ["Talavera", "Estefania", ""], ["Radeva", "Petia", ""], ["Dimiccoli", "Mariella", ""]]}, {"id": "1809.00421", "submitter": "Yang Liu", "authors": "Yang Liu, Zhaoyang Lu, Jing Li, Tao Yang", "title": "Hierarchically Learned View-Invariant Representations for Cross-View\n  Action Recognition", "comments": "Published in IEEE Transactions on Circuits and Systems for Video\n  Technology, codes can be found at https://yangliu9208.github.io/JSRDA/", "journal-ref": null, "doi": "10.1109/TCSVT.2018.2868123", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human actions from varied views is challenging due to huge\nappearance variations in different views. The key to this problem is to learn\ndiscriminant view-invariant representations generalizing well across views. In\nthis paper, we address this problem by learning view-invariant representations\nhierarchically using a novel method, referred to as Joint Sparse Representation\nand Distribution Adaptation (JSRDA). To obtain robust and informative feature\nrepresentations, we first incorporate a sample-affinity matrix into the\nmarginalized stacked denoising Autoencoder (mSDA) to obtain shared features,\nwhich are then combined with the private features. In order to make the feature\nrepresentations of videos across views transferable, we then learn a\ntransferable dictionary pair simultaneously from pairs of videos taken at\ndifferent views to encourage each action video across views to have the same\nsparse representation. However, the distribution difference across views still\nexists because a unified subspace where the sparse representations of one\naction across views are the same may not exist when the view difference is\nlarge. Therefore, we propose a novel unsupervised distribution adaptation\nmethod that learns a set of projections that project the source and target\nviews data into respective low-dimensional subspaces where the marginal and\nconditional distribution differences are reduced simultaneously. Therefore, the\nfinally learned feature representation is view-invariant and robust for\nsubstantial distribution difference across views even the view difference is\nlarge. Experimental results on four multiview datasets show that our approach\noutperforms the state-ofthe-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 01:31:05 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 08:14:46 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Liu", "Yang", ""], ["Lu", "Zhaoyang", ""], ["Li", "Jing", ""], ["Yang", "Tao", ""]]}, {"id": "1809.00437", "submitter": "Yuan Yuan", "authors": "Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang, Chao Dong, and\n  Liang Lin", "title": "Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative\n  Adversarial Networks", "comments": "10 pages (reference included), 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the single image super-resolution problem in a more general case\nthat the low-/high-resolution pairs and the down-sampling process are\nunavailable. Different from traditional super-resolution formulation, the\nlow-resolution input is further degraded by noises and blurring. This\ncomplicated setting makes supervised learning and accurate kernel estimation\nimpossible. To solve this problem, we resort to unsupervised learning without\npaired data, inspired by the recent successful image-to-image translation\napplications. With generative adversarial networks (GAN) as the basic\ncomponent, we propose a Cycle-in-Cycle network structure to tackle the problem\nwithin three steps. First, the noisy and blurry input is mapped to a noise-free\nlow-resolution space. Then the intermediate image is up-sampled with a\npre-trained deep model. Finally, we fine-tune the two modules in an end-to-end\nmanner to get the high-resolution output. Experiments on NTIRE2018 datasets\ndemonstrate that the proposed unsupervised method achieves comparable results\nas the state-of-the-art supervised models.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 03:07:00 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Yuan", "Yuan", ""], ["Liu", "Siyuan", ""], ["Zhang", "Jiawei", ""], ["Zhang", "Yongbing", ""], ["Dong", "Chao", ""], ["Lin", "Liang", ""]]}, {"id": "1809.00461", "submitter": "Ning Xu", "authors": "Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen\n  Liang, Brian Price, Scott Cohen, and Thomas Huang", "title": "YouTube-VOS: Sequence-to-Sequence Video Object Segmentation", "comments": "ECCV 2018 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning long-term spatial-temporal features are critical for many video\nanalysis tasks. However, existing video segmentation methods predominantly rely\non static image segmentation techniques, and methods capturing temporal\ndependency for segmentation have to depend on pretrained optical flow models,\nleading to suboptimal solutions for the problem. End-to-end sequential learning\nto explore spatial-temporal features for video segmentation is largely limited\nby the scale of available video segmentation datasets, i.e., even the largest\nvideo segmentation dataset only contains 90 short video clips. To solve this\nproblem, we build a new large-scale video object segmentation dataset called\nYouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains\n3,252 YouTube video clips and 78 categories including common objects and human\nactivities. This is by far the largest video object segmentation dataset to our\nknowledge and we have released it at https://youtube-vos.org. Based on this\ndataset, we propose a novel sequence-to-sequence network to fully exploit\nlong-term spatial-temporal information in videos for segmentation. We\ndemonstrate that our method is able to achieve the best results on our\nYouTube-VOS test set and comparable results on DAVIS 2016 compared to the\ncurrent state-of-the-art methods. Experiments show that the large scale dataset\nis indeed a key factor to the success of our model.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 06:16:13 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Xu", "Ning", ""], ["Yang", "Linjie", ""], ["Fan", "Yuchen", ""], ["Yang", "Jianchao", ""], ["Yue", "Dingcheng", ""], ["Liang", "Yuchen", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Huang", "Thomas", ""]]}, {"id": "1809.00488", "submitter": "Ertunc Erdil", "authors": "Ertunc Erdil, Sinan Yildirim, Tolga Tasdizen, Mujdat Cetin", "title": "Image Segmentation with Pseudo-marginal MCMC Sampling and Nonparametric\n  Shape Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient pseudo-marginal Markov chain Monte\nCarlo (MCMC) sampling approach to draw samples from posterior shape\ndistributions for image segmentation. The computation time of the proposed\napproach is independent from the size of the training set used to learn the\nshape prior distribution nonparametrically. Therefore, it scales well for very\nlarge data sets. Our approach is able to characterize the posterior probability\ndensity in the space of shapes through its samples, and to return multiple\nsolutions, potentially from different modes of a multimodal probability\ndensity, which would be encountered, e.g., in segmenting objects from multiple\nshape classes. Experimental results demonstrate the potential of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 08:24:43 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Erdil", "Ertunc", ""], ["Yildirim", "Sinan", ""], ["Tasdizen", "Tolga", ""], ["Cetin", "Mujdat", ""]]}, {"id": "1809.00491", "submitter": "Boliang Lin", "authors": "Boliang Lin", "title": "Prediction of Electric Multiple Unit Fleet Size Based on Convolutional\n  Neural Network", "comments": "13 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the expansion of high-speed railway network and growth of passenger\ntransportation demands, the fleet size of electric multiple unit (EMU) in China\nneeds to be adjusted accordingly. Generally, an EMU train costs tens of\nmillions of dollars which constitutes a significant portion of capital\ninvestment. Thus, the prediction of EMU fleet size has attracted increasing\nattention from associated railway departments. First, this paper introduces a\ntypical architecture of convolutional neural network (CNN) and its basic\ntheory. Then, some data of nine indices, such as passenger traffic volume and\nlength of high-speed railways in operation, is collected and preprocessed.\nNext, a CNN and a backpropagation neural network (BPNN) are constructed and\ntrained aiming to predict EMU fleet size in the following years. The\ndifferences and performances of these two networks in computation experiments\nare analyzed in-depth. The results indicate that the CNN is superior to the\nBPNN both in generalization ability and fitting accuracy, and CNN can serve as\nan aid in EMU fleet size prediction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 08:30:17 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Lin", "Boliang", ""]]}, {"id": "1809.00496", "submitter": "Joon Son Chung", "authors": "Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman", "title": "LRS3-TED: a large-scale dataset for visual speech recognition", "comments": "The audio-visual dataset can be downloaded from\n  http://www.robots.ox.ac.uk/~vgg/data/lip_reading/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new multi-modal dataset for visual and audio-visual\nspeech recognition. It includes face tracks from over 400 hours of TED and TEDx\nvideos, along with the corresponding subtitles and word alignment boundaries.\nThe new dataset is substantially larger in scale compared to other public\ndatasets that are available for general research.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 08:38:34 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 14:29:46 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Afouras", "Triantafyllos", ""], ["Chung", "Joon Son", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1809.00543", "submitter": "Fabian Schilling", "authors": "Fabian Schilling, Julien Lecoeur, Fabrizio Schiano, Dario Floreano", "title": "Learning Vision-based Cohesive Flight in Drone Swarms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a data-driven approach to learning vision-based\ncollective behavior from a simple flocking algorithm. We simulate a swarm of\nquadrotor drones and formulate the controller as a regression problem in which\nwe generate 3D velocity commands directly from raw camera images. The dataset\nis created by simultaneously acquiring omnidirectional images and computing the\ncorresponding control command from the flocking algorithm. We show that a\nconvolutional neural network trained on the visual inputs of the drone can\nlearn not only robust collision avoidance but also coherence of the flock in a\nsample-efficient manner. The neural controller effectively learns to localize\nother agents in the visual input, which we show by visualizing the regions with\nthe most influence on the motion of an agent. This weakly supervised saliency\nmap can be computed efficiently and may be used as a prior for subsequent\ndetection and relative localization of other agents. We remove the dependence\non sharing positions among flock members by taking only local visual\ninformation into account for control. Our work can therefore be seen as the\nfirst step towards a fully decentralized, vision-based flock without the need\nfor communication or visual markers to aid detection of other agents.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 10:44:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Schilling", "Fabian", ""], ["Lecoeur", "Julien", ""], ["Schiano", "Fabrizio", ""], ["Floreano", "Dario", ""]]}, {"id": "1809.00553", "submitter": "Aditya Ganeshan Master", "authors": "Jogendra Nath Kundu, Rahul M. V., Aditya Ganeshan, and R. Venkatesh\n  Babu", "title": "Object Pose Estimation from Monocular Image using Multi-View Keypoint\n  Correspondence", "comments": "Accepted in ECCV-W; Code available at this http url:\n  https://github.com/val-iisc/pose_estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the geometry and pose of objects in 2D images is a fundamental\nnecessity for a wide range of real world applications. Driven by deep neural\nnetworks, recent methods have brought significant improvements to object pose\nestimation. However, they suffer due to scarcity of keypoint/pose-annotated\nreal images and hence can not exploit the object's 3D structural information\neffectively. In this work, we propose a data-efficient method which utilizes\nthe geometric regularity of intraclass objects for pose estimation. First, we\nlearn pose-invariant local descriptors of object parts from simple 2D RGB\nimages. These descriptors, along with keypoints obtained from renders of a\nfixed 3D template model are then used to generate keypoint correspondence maps\nfor a given monocular real image. Finally, a pose estimation network predicts\n3D pose of the object using these correspondence maps. This pipeline is further\nextended to a multi-view approach, which assimilates keypoint information from\ncorrespondence sets generated from multiple views of the 3D template model.\nFusion of multi-view information significantly improves geometric comprehension\nof the system which in turn enhances the pose estimation performance.\nFurthermore, use of correspondence framework responsible for the learning of\npose invariant keypoint descriptor also allows us to effectively alleviate the\ndata-scarcity problem. This enables our method to achieve state-of-the-art\nperformance on multiple real-image viewpoint estimation datasets, such as\nPascal3D+ and ObjectNet3D. To encourage reproducible research, we have released\nthe codes for our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 11:16:11 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["V.", "Rahul M.", ""], ["Ganeshan", "Aditya", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1809.00567", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Marc Assens, Xavier Giro-i-Nieto, Kevin McGuinness and Noel E.\n  O'Connor", "title": "PathGAN: Visual Scanpath Prediction with Generative Adversarial Networks", "comments": "ECCV 2018 Workshop on Egocentric Perception, Interaction and\n  Computing (EPIC). This work obtained the 2nd award in Prediction of Head-gaze\n  Scan-paths for Images, and the 2nd award in Prediction of Eye-gaze Scan-paths\n  for Images at the IEEE ICME 2018 Salient360! Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PathGAN, a deep neural network for visual scanpath prediction\ntrained on adversarial examples. A visual scanpath is defined as the sequence\nof fixation points over an image defined by a human observer with its gaze.\nPathGAN is composed of two parts, the generator and the discriminator. Both\nparts extract features from images using off-the-shelf networks, and train\nrecurrent layers to generate or discriminate scanpaths accordingly. In scanpath\nprediction, the stochastic nature of the data makes it very difficult to\ngenerate realistic predictions using supervised learning strategies, but we\nadopt adversarial training as a suitable alternative. Our experiments prove how\nPathGAN improves the state of the art of visual scanpath prediction on the iSUN\nand Salient360! datasets. Source code and models are available at\nhttps://imatge-upc.github.io/pathgan/\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 11:57:38 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Assens", "Marc", ""], ["Giro-i-Nieto", "Xavier", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1809.00588", "submitter": "Linping Zhang", "authors": "Liping Zhang, Zongqing Lu, Qingmin Liao", "title": "Optical Flow Super-Resolution Based on Image Guidence Using\n  Convolutional Neural Network", "comments": "20 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network model for optical flow estimation usually\noutputs a low-resolution(LR) optical flow field. To obtain the corresponding\nfull image resolution,interpolation and variational approach are the most\ncommon options, which do not effectively improve the results. With the\nmotivation of various convolutional neural network(CNN) structures succeeded in\nsingle image super-resolution(SISR) task, an end-to-end convolutional neural\nnetwork is proposed to reconstruct the high resolution(HR) optical flow field\nfrom initial LR optical flow with the guidence of the first frame used in\noptical flow estimation. Our optical flow super-resolution(OFSR) problem\ndiffers from the general SISR problem in two main aspects. Firstly, the optical\nflow includes less texture information than image so that the SISR CNN\nstructures can't be directly used in our OFSR problem. Secondly, the initial LR\noptical flow data contains estimation error, while the LR image data for SISR\nis generally a bicubic downsampled, blurred, and noisy version of HR ground\ntruth. We evaluate the proposed approach on two different optical flow\nestimation mehods and show that it can not only obtain the full image\nresolution, but generate more accurate optical flow field (Accuracy improve 15%\non FlyingChairs, 13% on MPI Sintel) with sharper edges than the estimation\nresult of original method.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 13:03:21 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Zhang", "Liping", ""], ["Lu", "Zongqing", ""], ["Liao", "Qingmin", ""]]}, {"id": "1809.00594", "submitter": "Sanli Tang", "authors": "Sanli Tang, Xiaolin Huang, Mingjian Chen, Chengjin Sun, and Jie Yang", "title": "Adversarial Attack Type I: Cheat Classifiers by Significant Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of deep neural networks, the adversarial attack can\ncheat some well-trained classifiers by small permutations. In this paper, we\npropose another type of adversarial attack that can cheat classifiers by\nsignificant changes. For example, we can significantly change a face but\nwell-trained neural networks still recognize the adversarial and the original\nexample as the same person. Statistically, the existing adversarial attack\nincreases Type II error and the proposed one aims at Type I error, which are\nhence named as Type II and Type I adversarial attack, respectively. The two\ntypes of attack are equally important but are essentially different, which are\nintuitively explained and numerically evaluated. To implement the proposed\nattack, a supervised variation autoencoder is designed and then the classifier\nis attacked by updating the latent variables using gradient information.\n{Besides, with pre-trained generative models, Type I attack on latent spaces is\ninvestigated as well.} Experimental results show that our method is practical\nand effective to generate Type I adversarial examples on large-scale image\ndatasets. Most of these generated examples can pass detectors designed for\ndefending Type II attack and the strengthening strategy is only efficient with\na specific type attack, both implying that the underlying reasons for Type I\nand Type II attack are different.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 13:25:06 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 13:00:58 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Tang", "Sanli", ""], ["Huang", "Xiaolin", ""], ["Chen", "Mingjian", ""], ["Sun", "Chengjin", ""], ["Yang", "Jie", ""]]}, {"id": "1809.00604", "submitter": "Antonios Perperidis Dr", "authors": "Antonios Perperidis, Kevin Dhaliwal, Stephen McLaughlin, Tom\n  Vercauteren", "title": "Image computing for fibre-bundle endomicroscopy: A review", "comments": "38 Pages, 2 Figures, 6 Tables", "journal-ref": null, "doi": "10.1016/j.media.2019.101620", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endomicroscopy is an emerging imaging modality, that facilitates the\nacquisition of in vivo, in situ optical biopsies, assisting diagnostic and\npotentially therapeutic interventions. While there is a diverse and constantly\nexpanding range of commercial and experimental optical biopsy platforms\navailable, fibre-bundle endomicroscopy is currently the most widely used\nplatform and is approved for clinical use in a range of clinical indications.\nMiniaturised, flexible fibre-bundles, guided through the working channel of\nendoscopes, needles and catheters, enable high-resolution imaging across a\nvariety of organ systems. Yet, the nature of image acquisition though a\nfibre-bundle gives rise to several inherent characteristics and limitations\nnecessitating novel and effective image pre- and post-processing algorithms,\nranging from image formation, enhancement and mosaicing to pathology detection\nand quantification. This paper introduces the underlying technology and most\nprevalent clinical applications of fibre-bundle endomicroscopy, and provides a\ncomprehensive, up-to-date, review of relevant image reconstruction, analysis\nand understanding/inference methodologies. Furthermore, current limitations as\nwell as future challenges and opportunities in fibre-bundle endomicroscopy\ncomputing are identified and discussed.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 13:57:47 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Perperidis", "Antonios", ""], ["Dhaliwal", "Kevin", ""], ["McLaughlin", "Stephen", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1809.00644", "submitter": "Shanghua Xiao", "authors": "Shanghua Xiao", "title": "Learning Saliency Prediction From Sparse Fixation Pixel Map", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground truth for saliency prediction datasets consists of two types of map\ndata: fixation pixel map which records the human eye movements on sample\nimages, and fixation blob map generated by performing gaussian blurring on the\ncorresponding fixation pixel map. Current saliency approaches perform\nprediction by directly pixel-wise regressing the input image into saliency map\nwith fixation blob as ground truth, yet learning saliency from fixation pixel\nmap is not explored. In this work, we propose a first-of-its-kind approach of\nlearning saliency prediction from sparse fixation pixel map, and a novel loss\nfunction for training from such sparse fixation. We utilize clustering to\nextract sparse fixation pixel from the raw fixation pixel map, and add a\nmax-pooling transformation on the output to avoid false penalty between sparse\noutputs and labels caused by nearby but non-overlapping saliency pixels when\ncalculating loss. This approach provides a novel perspective for achieving\nsaliency prediction. We evaluate our approach over multiple benchmark datasets,\nand achieve competitive performance in terms of multiple metrics comparing with\nstate-of-the-art saliency methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 16:31:11 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Xiao", "Shanghua", ""]]}, {"id": "1809.00646", "submitter": "Zhixiang Hao", "authors": "Zhixiang Hao, Yu Li, Shaodi You and Feng Lu", "title": "Detail Preserving Depth Estimation from a Single Image Using Attention\n  Guided Networks", "comments": "Published at IEEE International Conference on 3D Vision (3DV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have demonstrated superior performance on\nsingle image depth estimation in recent years. These works usually use stacked\nspatial pooling or strided convolution to get high-level information which are\ncommon practices in classification task. However, depth estimation is a dense\nprediction problem and low-resolution feature maps usually generate blurred\ndepth map which is undesirable in application. In order to produce high quality\ndepth map, say clean and accurate, we propose a network consists of a Dense\nFeature Extractor (DFE) and a Depth Map Generator (DMG). The DFE combines\nResNet and dilated convolutions. It extracts multi-scale information from input\nimage while keeping the feature maps dense. As for DMG, we use attention\nmechanism to fuse multi-scale features produced in DFE. Our Network is trained\nend-to-end and does not need any post-processing. Hence, it runs fast and can\npredict depth map in about 15 fps. Experiment results show that our method is\ncompetitive with the state-of-the-art in quantitative evaluation, but can\npreserve better structural details of the scene depth.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 16:33:30 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Hao", "Zhixiang", ""], ["Li", "Yu", ""], ["You", "Shaodi", ""], ["Lu", "Feng", ""]]}, {"id": "1809.00665", "submitter": "Junjun Jiang", "authors": "Junjun Jiang, Yi Yu, Suhua Tang, Jiayi Ma, Akiko Aizawa, Kiyoharu\n  Aizawa", "title": "Context-Patch Face Hallucination Based on Thresholding\n  Locality-constrained Representation and Reproducing Learning", "comments": "13 pages, 15 figures, Accepted by IEEE TCYB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face hallucination is a technique that reconstruct high-resolution (HR) faces\nfrom low-resolution (LR) faces, by using the prior knowledge learned from HR/LR\nface pairs. Most state-of-the-arts leverage position-patch prior knowledge of\nhuman face to estimate the optimal representation coefficients for each image\npatch. However, they focus only the position information and usually ignore the\ncontext information of image patch. In addition, when they are confronted with\nmisalignment or the Small Sample Size (SSS) problem, the hallucination\nperformance is very poor. To this end, this study incorporates the contextual\ninformation of image patch and proposes a powerful and efficient context-patch\nbased face hallucination approach, namely Thresholding Locality-constrained\nRepresentation and Reproducing learning (TLcR-RL). Under the context-patch\nbased framework, we advance a thresholding based representation method to\nenhance the reconstruction accuracy and reduce the computational complexity. To\nfurther improve the performance of the proposed algorithm, we propose a\npromotion strategy called reproducing learning. By adding the estimated HR face\nto the training set, which can simulates the case that the HR version of the\ninput LR face is present in the training set, thus iteratively enhancing the\nfinal hallucination result. Experiments demonstrate that the proposed TLcR-RL\nmethod achieves a substantial increase in the hallucinated results, both\nsubjectively and objectively. Additionally, the proposed framework is more\nrobust to face misalignment and the SSS problem, and its hallucinated HR face\nis still very good when the LR test face is from the real-world. The MATLAB\nsource code is available at https://github.com/junjun-jiang/TLcR-RL\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 17:23:38 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 03:15:30 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Jiang", "Junjun", ""], ["Yu", "Yi", ""], ["Tang", "Suhua", ""], ["Ma", "Jiayi", ""], ["Aizawa", "Akiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1809.00681", "submitter": "Moitreya Chatterjee", "authors": "Moitreya Chatterjee, Alexander G. Schwing", "title": "Diverse and Coherent Paragraph Generation from Images", "comments": "Camera Ready Version of ECCV 2018 paper; Coupled with supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paragraph generation from images, which has gained popularity recently, is an\nimportant task for video summarization, editing, and support of the disabled.\nTraditional image captioning methods fall short on this front, since they\naren't designed to generate long informative descriptions. Moreover, the\nvanilla approach of simply concatenating multiple short sentences, possibly\nsynthesized from a classical image captioning system, doesn't embrace the\nintricacies of paragraphs: coherent sentences, globally consistent structure,\nand diversity. To address those challenges, we propose to augment paragraph\ngeneration techniques with 'coherence vectors', 'global topic vectors', and\nmodeling of the inherent ambiguity of associating paragraphs with images, via a\nvariational auto-encoder formulation. We demonstrate the effectiveness of the\ndeveloped approach on two datasets, outperforming existing state-of-the-art\ntechniques on both.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 18:16:08 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Chatterjee", "Moitreya", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1809.00696", "submitter": "Nishant Nikhil", "authors": "Nishant Nikhil and Brendan Tran Morris", "title": "Convolutional Neural Network for Trajectory Prediction", "comments": "Accepted at ECCV 2018 workshop - Anticipating Human Behavior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting trajectories of pedestrians is quintessential for autonomous\nrobots which share the same environment with humans. In order to effectively\nand safely interact with humans, trajectory prediction needs to be both precise\nand computationally efficient. In this work, we propose a convolutional neural\nnetwork (CNN) based human trajectory prediction approach. Unlike more recent\nLSTM-based moles which attend sequentially to each frame, our model supports\nincreased parallelism and effective temporal representation. The proposed\ncompact CNN model is faster than the current approaches yet still yields\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 19:30:13 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 05:39:29 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Nikhil", "Nishant", ""], ["Morris", "Brendan Tran", ""]]}, {"id": "1809.00716", "submitter": "Sajad Saeedi", "authors": "Wenbin Li (1), Sajad Saeedi (1), John McCormac (1), Ronald Clark (1),\n  Dimos Tzoumanikas (1), Qing Ye (2), Yuzhong Huang (2), Rui Tang (2), Stefan\n  Leutenegger (1) ((1) Department of Computing, Imperial College London, London\n  UK, SW7 2AZ (2) KooLab, Kujiale.com, Hangzhou China)", "title": "InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes\n  Dataset", "comments": "British Machine Vision Conference (BMVC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets have gained an enormous amount of popularity in the computer vision\ncommunity, from training and evaluation of Deep Learning-based methods to\nbenchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt,\nsynthetic imagery bears a vast potential due to scalability in terms of amounts\nof data obtainable without tedious manual ground truth annotations or\nmeasurements. Here, we present a dataset with the aim of providing a higher\ndegree of photo-realism, larger scale, more variability as well as serving a\nwider range of purposes compared to existing datasets. Our dataset leverages\nthe availability of millions of professional interior designs and millions of\nproduction-level furniture and object assets -- all coming with fine geometric\ndetails and high-resolution texture. We render high-resolution and high\nframe-rate video sequences following realistic trajectories while supporting\nvarious camera types as well as providing inertial measurements. Together with\nthe release of the dataset, we will make executable program of our interactive\nsimulator software as well as our renderer available at\nhttps://interiornetdataset.github.io. To showcase the usability and uniqueness\nof our dataset, we show benchmarking results of both sparse and dense SLAM\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 20:42:27 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Li", "Wenbin", ""], ["Saeedi", "Sajad", ""], ["McCormac", "John", ""], ["Clark", "Ronald", ""], ["Tzoumanikas", "Dimos", ""], ["Ye", "Qing", ""], ["Huang", "Yuzhong", ""], ["Tang", "Rui", ""], ["Leutenegger", "Stefan", ""]]}, {"id": "1809.00720", "submitter": "Berkay Kicanaoglu", "authors": "Berkay Kicanaoglu and Ran Tao and Arnold W.M. Smeulders", "title": "Estimating Small Differences in Car-Pose from Orbits", "comments": "to appear in BMVC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinction among nearby poses and among symmetries of an object is\nchallenging. In this paper, we propose a unified, group-theoretic approach to\ntackle both. Different from existing works which directly predict absolute\npose, our method measures the pose of an object relative to another pose, i.e.,\nthe pose difference. The proposed method generates the complete orbit of an\nobject from a single view of the object with respect to the subgroup of SO(3)\nof rotations around the z-axis, and compares the orbit of the object with\nanother orbit using a novel orbit metric to estimate the pose difference. The\ngenerated orbit in the latent space records all the differences in pose in the\noriginal observational space, and as a result, the method is capable of finding\nsubtle differences in pose. We demonstrate the effectiveness of the proposed\nmethod on cars, where identifying the subtle pose differences is vital.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 21:17:59 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Kicanaoglu", "Berkay", ""], ["Tao", "Ran", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1809.00758", "submitter": "Myungsu Chae", "authors": "Myungsu Chae, Tae-Ho Kim, Young Hoon Shin, June-Woo Kim, and Soo-Young\n  Lee", "title": "End-to-end Multimodal Emotion and Gender Recognition with Dynamic Joint\n  Loss Weights", "comments": "IROS 2018 Workshop on Crossmodal Learning for Intelligent Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is a method for improving the generalizability of\nmultiple tasks. In order to perform multiple classification tasks with one\nneural network model, the losses of each task should be combined. Previous\nstudies have mostly focused on multiple prediction tasks using joint loss with\nstatic weights for training models, choosing the weights between tasks without\nmaking sufficient considerations by setting them uniformly or empirically. In\nthis study, we propose a method to calculate joint loss using dynamic weights\nto improve the total performance, instead of the individual performance, of\ntasks. We apply this method to design an end-to-end multimodal emotion and\ngender recognition model using audio and video data. This approach provides\nproper weights for the loss of each task when the training process ends. In our\nexperiments, emotion and gender recognition with the proposed method yielded a\nlower joint loss, which is computed as the negative log-likelihood, than using\nstatic weights for joint loss. Moreover, our proposed model has better\ngeneralizability than other models. To the best of our knowledge, this research\nis the first to demonstrate the strength of using dynamic weights for joint\nloss for maximizing overall performance in emotion and gender recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 00:52:25 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 06:55:13 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 04:16:54 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Chae", "Myungsu", ""], ["Kim", "Tae-Ho", ""], ["Shin", "Young Hoon", ""], ["Kim", "June-Woo", ""], ["Lee", "Soo-Young", ""]]}, {"id": "1809.00764", "submitter": "Menghui Jiang", "authors": "Huanfeng Shen, Menghui Jiang, Jie Li, Qiangqiang Yuan, Yanchong Wei,\n  and Liangpei Zhang", "title": "Spatial-Spectral Fusion by Combining Deep Learning and Variation Model", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2904659", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of spatial-spectral fusion, the model-based method and the deep\nlearning (DL)-based method are state-of-the-art. This paper presents a fusion\nmethod that incorporates the deep neural network into the model-based method\nfor the most common case in the spatial-spectral fusion: PAN/multispectral (MS)\nfusion. Specifically, we first map the gradient of the high spatial resolution\npanchromatic image (HR-PAN) and the low spatial resolution multispectral image\n(LR-MS) to the gradient of the high spatial resolution multispectral image\n(HR-MS) via a deep residual convolutional neural network (CNN). Then we\nconstruct a fusion framework by the LR-MS image, the gradient prior learned\nfrom the gradient network, and the ideal fused image. Finally, an iterative\noptimization algorithm is used to solve the fusion model. Both quantitative and\nvisual assessments on high-quality images from various sources demonstrate that\nthe proposed fusion method is superior to all the mainstream algorithms\nincluded in the comparison in terms of overall fusion accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 01:39:04 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Shen", "Huanfeng", ""], ["Jiang", "Menghui", ""], ["Li", "Jie", ""], ["Yuan", "Qiangqiang", ""], ["Wei", "Yanchong", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1809.00769", "submitter": "Rayson Laroca", "authors": "Cides S. Bezerra, Rayson Laroca, Diego R. Lucio, Evair Severo, Lucas\n  F. Oliveira, Alceu S. Britto Jr., David Menotti", "title": "Robust Iris Segmentation Based on Fully Convolutional Networks and\n  Generative Adversarial Networks", "comments": "Accepted for presentation at the Conference on Graphics, Patterns and\n  Images (SIBGRAPI) 2018", "journal-ref": null, "doi": "10.1109/SIBGRAPI.2018.00043", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iris can be considered as one of the most important biometric traits due\nto its high degree of uniqueness. Iris-based biometrics applications depend\nmainly on the iris segmentation whose suitability is not robust for different\nenvironments such as near-infrared (NIR) and visible (VIS) ones. In this paper,\ntwo approaches for robust iris segmentation based on Fully Convolutional\nNetworks (FCNs) and Generative Adversarial Networks (GANs) are described.\nSimilar to a common convolutional network, but without the fully connected\nlayers (i.e., the classification layers), an FCN employs at its end a\ncombination of pooling layers from different convolutional layers. Based on the\ngame theory, a GAN is designed as two networks competing with each other to\ngenerate the best segmentation. The proposed segmentation networks achieved\npromising results in all evaluated datasets (i.e., BioSec, CasiaI3, CasiaT4,\nIITD-1) of NIR images and (NICE.I, CrEye-Iris and MICHE-I) of VIS images in\nboth non-cooperative and cooperative domains, outperforming the baselines\ntechniques which are the best ones found so far in the literature, i.e., a new\nstate of the art for these datasets. Furthermore, we manually labeled 2,431\nimages from CasiaT4, CrEye-Iris and MICHE-I datasets, making the masks\navailable for research purposes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 02:10:41 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Bezerra", "Cides S.", ""], ["Laroca", "Rayson", ""], ["Lucio", "Diego R.", ""], ["Severo", "Evair", ""], ["Oliveira", "Lucas F.", ""], ["Britto", "Alceu S.", "Jr."], ["Menotti", "David", ""]]}, {"id": "1809.00774", "submitter": "Feiniu Yuan", "authors": "Feiniu Yuan, Lin Zhang, Xue Xia, Boyang Wan, Qinghua Huang, Xuelong Li", "title": "Deep Smoke Segmentation", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent success of fully convolutional networks (FCN) in\nsemantic segmentation, we propose a deep smoke segmentation network to infer\nhigh quality segmentation masks from blurry smoke images. To overcome large\nvariations in texture, color and shape of smoke appearance, we divide the\nproposed network into a coarse path and a fine path. The first path is an\nencoder-decoder FCN with skip structures, which extracts global context\ninformation of smoke and accordingly generates a coarse segmentation mask. To\nretain fine spatial details of smoke, the second path is also designed as an\nencoder-decoder FCN with skip structures, but it is shallower than the first\npath network. Finally, we propose a very small network containing only add,\nconvolution and activation layers to fuse the results of the two paths. Thus,\nwe can easily train the proposed network end to end for simultaneous\noptimization of network parameters. To avoid the difficulty in manually\nlabelling fuzzy smoke objects, we propose a method to generate synthetic smoke\nimages. According to results of our deep segmentation method, we can easily and\naccurately perform smoke detection from videos. Experiments on three synthetic\nsmoke datasets and a realistic smoke dataset show that our method achieves much\nbetter performance than state-of-the-art segmentation algorithms based on FCNs.\nTest results of our method on videos are also appealing.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 02:48:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Yuan", "Feiniu", ""], ["Zhang", "Lin", ""], ["Xia", "Xue", ""], ["Wan", "Boyang", ""], ["Huang", "Qinghua", ""], ["Li", "Xuelong", ""]]}, {"id": "1809.00778", "submitter": "Tommi Kerola", "authors": "Takuya Akiba, Tommi Kerola, Yusuke Niitani, Toru Ogawa, Shotaro Sano\n  and Shuji Suzuki", "title": "PFDet: 2nd Place Solution to Open Images Challenge 2018 Object Detection\n  Track", "comments": "Technical report for Open Images Challenge 2018 Object Detection\n  Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large-scale object detection system by team PFDet. Our system\nenables training with huge datasets using 512 GPUs, handles sparsely verified\nclasses, and massive class imbalance. Using our method, we achieved 2nd place\nin the Google AI Open Images Object Detection Track 2018 on Kaggle.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 02:59:07 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Akiba", "Takuya", ""], ["Kerola", "Tommi", ""], ["Niitani", "Yusuke", ""], ["Ogawa", "Toru", ""], ["Sano", "Shotaro", ""], ["Suzuki", "Shuji", ""]]}, {"id": "1809.00812", "submitter": "Aykut Erdem", "authors": "Semih Yagcioglu, Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis", "title": "RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking\n  Recipes", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and reasoning about cooking recipes is a fruitful research\ndirection towards enabling machines to interpret procedural text. In this work,\nwe introduce RecipeQA, a dataset for multimodal comprehension of cooking\nrecipes. It comprises of approximately 20K instructional recipes with multiple\nmodalities such as titles, descriptions and aligned set of images. With over\n36K automatically generated question-answer pairs, we design a set of\ncomprehension and reasoning tasks that require joint understanding of images\nand text, capturing the temporal flow of events and making sense of procedural\nknowledge. Our preliminary results indicate that RecipeQA will serve as a\nchallenging test bed and an ideal benchmark for evaluating machine\ncomprehension systems. The data and leaderboard are available at\nhttp://hucvl.github.io/recipeqa.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 07:04:55 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Yagcioglu", "Semih", ""], ["Erdem", "Aykut", ""], ["Erdem", "Erkut", ""], ["Ikizler-Cinbis", "Nazli", ""]]}, {"id": "1809.00837", "submitter": "Dan Dai", "authors": "Dan Dai, Zhiwen Yu, Yang Hu, Wenming Cao, Mingnan Luo", "title": "Metabolize Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The metabolism of cells is the most basic and important part of human\nfunction. Neural networks in deep learning stem from neuronal activity. It is\nself-evident that the significance of metabolize neuronal network(MetaNet) in\nmodel construction. In this study, we explore neuronal metabolism for shallow\nnetwork from proliferation and autophagy two aspects. First, we propose\ndifferent neuron proliferate methods that constructive the selfgrowing network\nin metabolism cycle. Proliferate neurons alleviate resources wasting and\ninsufficient model learning problem when network initializes more or less\nparameters. Then combined with autophagy mechanism in the process of model self\nconstruction to ablate under-expressed neurons. The MetaNet can automatically\ndetermine the number of neurons during training, further, save more resource\nconsumption. We verify the performance of the proposed methods on datasets:\nMNIST, Fashion-MNIST and CIFAR-10.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 08:42:52 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Dai", "Dan", ""], ["Yu", "Zhiwen", ""], ["Hu", "Yang", ""], ["Cao", "Wenming", ""], ["Luo", "Mingnan", ""]]}, {"id": "1809.00846", "submitter": "Ping Luo", "authors": "Ping Luo and Xinjiang Wang and Wenqi Shao and Zhanglin Peng", "title": "Towards Understanding Regularization in Batch Normalization", "comments": "International Conference on Learning Representations (ICLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) improves both convergence and generalization in\ntraining neural networks. This work understands these phenomena theoretically.\nWe analyze BN by using a basic block of neural networks, consisting of a kernel\nlayer, a BN layer, and a nonlinear activation function. This basic network\nhelps us understand the impacts of BN in three aspects. First, by viewing BN as\nan implicit regularizer, BN can be decomposed into population normalization\n(PN) and gamma decay as an explicit regularization. Second, learning dynamics\nof BN and the regularization show that training converged with large maximum\nand effective learning rate. Third, generalization of BN is explored by using\nstatistical mechanics. Experiments demonstrate that BN in convolutional neural\nnetworks share the same traits of regularization as the above analyses.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 09:01:10 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 17:56:50 GMT"}, {"version": "v3", "created": "Sun, 30 Sep 2018 06:11:53 GMT"}, {"version": "v4", "created": "Wed, 24 Apr 2019 05:23:45 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Luo", "Ping", ""], ["Wang", "Xinjiang", ""], ["Shao", "Wenqi", ""], ["Peng", "Zhanglin", ""]]}, {"id": "1809.00854", "submitter": "Dena Bazazian", "authors": "Dena Bazazian, Dimosthenis Karatzas, Andrew D. Bagdanov", "title": "Soft-PHOC Descriptor for End-to-End Word Spotting in Egocentric Scene\n  Images", "comments": "9 pages, 10 figures, The Third International Workshop on Egocentric\n  Perception, Interaction and Computing (EPIC) at ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word spotting in natural scene images has many applications in scene\nunderstanding and visual assistance. In this paper we propose a technique to\ncreate and exploit an intermediate representation of images based on text\nattributes which are character probability maps. Our representation extends the\nconcept of the Pyramidal Histogram Of Characters (PHOC) by exploiting Fully\nConvolutional Networks to derive a pixel-wise mapping of the character\ndistribution within candidate word regions. We call this representation the\nSoft-PHOC. Furthermore, we show how to use Soft-PHOC descriptors for word\nspotting tasks in egocentric camera streams through an efficient text line\nproposal algorithm. This is based on the Hough Transform over character\nattribute maps followed by scoring using Dynamic Time Warping (DTW). We\nevaluate our results on ICDAR 2015 Challenge 4 dataset of incidental scene text\ncaptured by an egocentric camera.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 09:21:32 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 17:01:28 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Bazazian", "Dena", ""], ["Karatzas", "Dimosthenis", ""], ["Bagdanov", "Andrew D.", ""]]}, {"id": "1809.00862", "submitter": "Omar Mohammed", "authors": "Omar Mohammed, Gerard Bailly, Damien Pellier", "title": "Handwriting styles: benchmarks and evaluation metrics", "comments": "Submitted to IEEE International Workshop on Deep and Transfer\n  Learning (DTL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the style of handwriting generation is a challenging problem,\nsince it is not well defined. It is a key component in order to develop in\ndeveloping systems with more personalized experiences with humans. In this\npaper, we propose baseline benchmarks, in order to set anchors to estimate the\nrelative quality of different handwriting style methods. This will be done\nusing deep learning techniques, which have shown remarkable results in\ndifferent machine learning tasks, learning classification, regression, and most\nrelevant to our work, generating temporal sequences. We discuss the challenges\nassociated with evaluating our methods, which is related to evaluation of\ngenerative models in general. We then propose evaluation metrics, which we find\nrelevant to this problem, and we discuss how we evaluate the evaluation\nmetrics. In this study, we use IRON-OFF dataset. To the best of our knowledge,\nthere is no work done before in generating handwriting (either in terms of\nmethodology or the performance metrics), our in exploring styles using this\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 09:54:25 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Mohammed", "Omar", ""], ["Bailly", "Gerard", ""], ["Pellier", "Damien", ""]]}, {"id": "1809.00888", "submitter": "Vincent Nozick", "authors": "Darius Afchar and Vincent Nozick and Junichi Yamagishi and Isao\n  Echizen", "title": "MesoNet: a Compact Facial Video Forgery Detection Network", "comments": "accepted to WIFS 2018", "journal-ref": null, "doi": "10.1109/WIFS.2018.8630761", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to automatically and efficiently detect face\ntampering in videos, and particularly focuses on two recent techniques used to\ngenerate hyper-realistic forged videos: Deepfake and Face2Face. Traditional\nimage forensics techniques are usually not well suited to videos due to the\ncompression that strongly degrades the data. Thus, this paper follows a deep\nlearning approach and presents two networks, both with a low number of layers\nto focus on the mesoscopic properties of images. We evaluate those fast\nnetworks on both an existing dataset and a dataset we have constituted from\nonline videos. The tests demonstrate a very successful detection rate with more\nthan 98% for Deepfake and 95% for Face2Face.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 10:59:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Afchar", "Darius", ""], ["Nozick", "Vincent", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1809.00898", "submitter": "Marie-Morgane Paumard", "authors": "M.-M. Paumard, D. Picard, H. Tabia", "title": "Image Reassembly Combining Deep Learning and Shortest Path Problem", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of reassembling images from disjointed\nfragments. More specifically, given an unordered set of fragments, we aim at\nreassembling one or several possibly incomplete images. The main contributions\nof this work are: 1) several deep neural architectures to predict the relative\nposition of image fragments that outperform the previous state of the art; 2)\ncasting the reassembly problem into the shortest path in a graph problem for\nwhich we provide several construction algorithms depending on available\ninformation; 3) a new dataset of images taken from the Metropolitan Museum of\nArt (MET) dedicated to image reassembly for which we provide a clear setup and\na strong baseline.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 11:39:03 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Paumard", "M. -M.", ""], ["Picard", "D.", ""], ["Tabia", "H.", ""]]}, {"id": "1809.00903", "submitter": "Xinge Zhu", "authors": "Xinge Zhu, Hui Zhou, Ceyuan Yang, Jianping Shi, Dahua Lin", "title": "Penalizing Top Performers: Conservative Loss for Semantic Segmentation\n  Adaptation", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the expensive and time-consuming annotations (e.g., segmentation) for\nreal-world images, recent works in computer vision resort to synthetic data.\nHowever, the performance on the real image often drops significantly because of\nthe domain shift between the synthetic data and the real images. In this\nsetting, domain adaptation brings an appealing option. The effective approaches\nof domain adaptation shape the representations that (1) are discriminative for\nthe main task and (2) have good generalization capability for domain shift. To\nthis end, we propose a novel loss function, i.e., Conservative Loss, which\npenalizes the extreme good and bad cases while encouraging the moderate\nexamples. More specifically, it enables the network to learn features that are\ndiscriminative by gradient descent and are invariant to the change of domains\nvia gradient ascend method. Extensive experiments on synthetic to real\nsegmentation adaptation show our proposed method achieves state of the art\nresults. Ablation studies give more insights into properties of the\nConservative Loss. Exploratory experiments and discussion demonstrate that our\nConservative Loss has good flexibility rather than restricting an exact form.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 11:50:09 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 03:31:48 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhu", "Xinge", ""], ["Zhou", "Hui", ""], ["Yang", "Ceyuan", ""], ["Shi", "Jianping", ""], ["Lin", "Dahua", ""]]}, {"id": "1809.00905", "submitter": "Mst Shamima Nasrin", "authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, and Md Zahangir\n  Alom", "title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)", "comments": "6 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 11:55:34 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Rahman", "M M Shaifur", ""], ["Nasrin", "Mst Shamima", ""], ["Mostakim", "Moin", ""], ["Alom", "Md Zahangir", ""]]}, {"id": "1809.00916", "submitter": "Yuhui Yuan", "authors": "Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, Jingdong\n  Wang", "title": "OCNet: Object Context Network for Scene Parsing", "comments": "Accepted at IJCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the semantic segmentation task with a new context\naggregation scheme named \\emph{object context}, which focuses on enhancing the\nrole of object information. Motivated by the fact that the category of each\npixel is inherited from the object it belongs to, we define the object context\nfor each pixel as the set of pixels that belong to the same category as the\ngiven pixel in the image. We use a binary relation matrix to represent the\nrelationship between all pixels, where the value one indicates the two selected\npixels belong to the same category and zero otherwise.\n  We propose to use a dense relation matrix to serve as a surrogate for the\nbinary relation matrix. The dense relation matrix is capable to emphasize the\ncontribution of object information as the relation scores tend to be larger on\nthe object pixels than the other pixels. Considering that the dense relation\nmatrix estimation requires quadratic computation overhead and memory\nconsumption w.r.t. the input size, we propose an efficient interlaced sparse\nself-attention scheme to model the dense relations between any two of all\npixels via the combination of two sparse relation matrices.\n  To capture richer context information, we further combine our interlaced\nsparse self-attention scheme with the conventional multi-scale context schemes\nincluding pyramid pooling~\\citep{zhao2017pyramid} and atrous spatial pyramid\npooling~\\citep{chen2018deeplab}. We empirically show the advantages of our\napproach with competitive performances on five challenging benchmarks\nincluding: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 12:22:10 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 02:08:02 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 07:54:53 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 03:27:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Yuan", "Yuhui", ""], ["Huang", "Lang", ""], ["Guo", "Jianyuan", ""], ["Zhang", "Chao", ""], ["Chen", "Xilin", ""], ["Wang", "Jingdong", ""]]}, {"id": "1809.00946", "submitter": "Jerry Li", "authors": "Jerry Li", "title": "Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing\n  GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for translating unlabeled images from one domain into\nanalog images in another domain. We employ a progressively growing\nskip-connected encoder-generator structure and train it with a GAN loss for\nrealistic output, a cycle consistency loss for maintaining same-domain\ntranslation identity, and a semantic consistency loss that encourages the\nnetwork to keep the input semantic features in the output. We apply our\nframework on the task of translating face images, and show that it is capable\nof learning semantic mappings for face images with no supervised one-to-one\nimage mapping.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 23:09:03 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Li", "Jerry", ""]]}, {"id": "1809.00948", "submitter": "Jonas Adler", "authors": "Jonas Adler, Sebastian Lunz, Olivier Verdier, Carola-Bibiane\n  Sch\\\"onlieb and Ozan \\\"Oktem", "title": "Task adapted reconstruction for inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the problem of performing a task defined on a model\nparameter that is only observed indirectly through noisy data in an ill-posed\ninverse problem. A key aspect is to formalize the steps of reconstruction and\ntask as appropriate estimators (non-randomized decision rules) in statistical\nestimation problems. The implementation makes use of (deep) neural networks to\nprovide a differentiable parametrization of the family of estimators for both\nsteps. These networks are combined and jointly trained against suitable\nsupervised training data in order to minimize a joint differentiable loss\nfunction, resulting in an end-to-end task adapted reconstruction method. The\nsuggested framework is generic, yet adaptable, with a plug-and-play structure\nfor adjusting both the inverse problem and the task at hand. More precisely,\nthe data model (forward operator and statistical model of the noise) associated\nwith the inverse problem is exchangeable, e.g., by using neural network\narchitecture given by a learned iterative method. Furthermore, any task that is\nencodable as a trainable neural network can be used. The approach is\ndemonstrated on joint tomographic image reconstruction, classification and\njoint tomographic image reconstruction segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 11:44:48 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Adler", "Jonas", ""], ["Lunz", "Sebastian", ""], ["Verdier", "Olivier", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["\u00d6ktem", "Ozan", ""]]}, {"id": "1809.00950", "submitter": "Amirafshar Moshtaghpour", "authors": "Amirafshar Moshtaghpour, Jos\\'e M. Bioucas-Dias, Laurent Jacques", "title": "Compressive Hyperspectral Imaging: Fourier Transform Interferometry\n  meets Single Pixel Camera", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a single-pixel HyperSpectral (HS) imaging framework\nbased on Fourier Transform Interferometry (FTI). By combining a space-time\ncoding of the light illumination with partial interferometric observations of a\ncollimated light beam (observed by a single pixel), our system benefits from\n(i) reduced measurement rate and light-exposure of the observed object compared\nto common (Nyquist) FTI imagers, and (ii) high spectral resolution as desirable\nin, e.g., Fluorescence Spectroscopy (FS). From the principles of compressive\nsensing with multilevel sampling, our method leverages the sparsity \"in level\"\nof FS data, both in the spectral and the spatial domains. This allows us to\noptimize the space-time light coding using time-modulated Hadamard patterns. We\nconfirm the effectiveness of our approach by a few numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 13:40:39 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Moshtaghpour", "Amirafshar", ""], ["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Jacques", "Laurent", ""]]}, {"id": "1809.00953", "submitter": "Burak Satar", "authors": "Burak Satar, Ahmet Emir Dirik", "title": "Deep Learning Based Vehicle Make-Model Classification", "comments": "10 pages, ICANN 2018: Artificial Neural Networks and Machine Learning", "journal-ref": "Lecture Notes in Computer Science book series 2018 (LNCS, volume\n  11141). Springer, Cham", "doi": "10.1007/978-3-030-01424-7_53", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper studies the problems of vehicle make & model classification. Some\nof the main challenges are reaching high classification accuracy and reducing\nthe annotation time of the images. To address these problems, we have created a\nfine-grained database using online vehicle marketplaces of Turkey. A pipeline\nis proposed to combine an SSD (Single Shot Multibox Detector) model with a CNN\n(Convolutional Neural Network) model to train on the database. In the pipeline,\nwe first detect the vehicles by following an algorithm which reduces the time\nfor annotation. Then, we feed them into the CNN model. It is reached\napproximately 4% better classification accuracy result than using a\nconventional CNN model. Next, we propose to use the detected vehicles as ground\ntruth bounding box (GTBB) of the images and feed them into an SSD model in\nanother pipeline. At this stage, it is reached reasonable classification\naccuracy result without using perfectly shaped GTBB. Lastly, an application is\nimplemented in a use case by using our proposed pipelines. It detects the\nunauthorized vehicles by comparing their license plate numbers and make &\nmodels. It is assumed that license plates are readable.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 14:05:31 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 20:46:17 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Satar", "Burak", ""], ["Dirik", "Ahmet Emir", ""]]}, {"id": "1809.00957", "submitter": "Pankaj Roy", "authors": "Pankaj Raj Roy and Guillaume-Alexandre Bilodeau", "title": "Road User Abnormal Trajectory Detection using a Deep Autoencoder", "comments": "This paper has been accepted for oral presentation at ISVC'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the development of a method that detects abnormal\ntrajectories of road users at traffic intersections. The main difficulty with\nthis is the fact that there are very few abnormal data and the normal ones are\ninsufficient for the training of any kinds of machine learning model. To tackle\nthese problems, we proposed the solution of using a deep autoencoder network\ntrained solely through augmented data considered as normal. By generating\nartificial abnormal trajectories, our method is tested on four different\noutdoor urban users scenes and performs better compared to some classical\noutlier detection methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 23:18:52 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Roy", "Pankaj Raj", ""], ["Bilodeau", "Guillaume-Alexandre", ""]]}, {"id": "1809.00958", "submitter": "Roberto Rey-De-Castro", "authors": "Roberto Rey-de-Castro and Herschel Rabitz", "title": "Targeted Nonlinear Adversarial Perturbations in Images and Videos", "comments": "Code and data available at:\n  https://github.com/roberto1648/adversarial-perturbations-on-images-and-videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for learning adversarial perturbations targeted to\nindividual images or videos. The learned perturbations are found to be sparse\nwhile at the same time containing a high level of feature detail. Thus, the\nextracted perturbations allow a form of object or action recognition and\nprovide insights into what features the studied deep neural network models\nconsider important when reaching their classification decisions. From an\nadversarial point of view, the sparse perturbations successfully confused the\nmodels into misclassifying, although the perturbed samples still belonged to\nthe same original class by visual examination. This is discussed in terms of a\nprospective data augmentation scheme. The sparse yet high-quality perturbations\nmay also be leveraged for image or video compression.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 14:09:18 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Rey-de-Castro", "Roberto", ""], ["Rabitz", "Herschel", ""]]}, {"id": "1809.00960", "submitter": "Yueyue Wang", "authors": "Yueyue Wang, Liang Zhao, Zhijian Song, Manning Wang", "title": "Organ at Risk Segmentation in Head and Neck CT Images by Using a\n  Two-Stage Segmentation Framework Based on 3D U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of organ at risk (OAR) play a critical role in the\ntreatment planning of image guided radiation treatment of head and neck cancer.\nThis segmentation task is challenging for both human and automatic algorithms\nbecause of the relatively large number of OARs to be segmented, the large\nvariability of the size and morphology across different OARs, and the low\ncontrast of between some OARs and the background. In this paper, we proposed a\ntwo-stage segmentation framework based on 3D U-Net. In this framework, the\nsegmentation of each OAR is decomposed into two sub-tasks: locating a bounding\nbox of the OAR and segment it from a small volume within the bounding box, and\neach sub-tasks is fulfilled by a dedicated 3D U-Net. The decomposition makes\neach of the two sub-tasks much easier, so that they can be better completed. We\nevaluated the proposed method and compared it to state-of-the-art methods by\nusing the MICCAI 2015 Challenge dataset. In terms of the boundary-based metric\n95HD, the proposed method ranked first in eight of all nine OARs and ranked\nsecond in the other OAR. In terms of the area-based metric DSC, the proposed\nmethod ranked first in six of the nine OARs and ranked second in the other\nthree OARs with small difference with the first one.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 14:09:28 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 08:40:03 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Wang", "Yueyue", ""], ["Zhao", "Liang", ""], ["Song", "Zhijian", ""], ["Wang", "Manning", ""]]}, {"id": "1809.00961", "submitter": "Ram Krishna Pandey", "authors": "Ram Krishna Pandey, Nabagata Saha, Samarjit Karmakar, A G Ramakrishnan", "title": "MSCE: An edge preserving robust loss function for improving\n  super-resolution algorithms", "comments": "Accepted in ICONIP-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advancement in the deep learning technologies such as CNNs\nand GANs, there is significant improvement in the quality of the images\nreconstructed by deep learning based super-resolution (SR) techniques. In this\nwork, we propose a robust loss function based on the preservation of edges\nobtained by the Canny operator. This loss function, when combined with the\nexisting loss function such as mean square error (MSE), gives better SR\nreconstruction measured in terms of PSNR and SSIM. Our proposed loss function\nguarantees improved performance on any existing algorithm using MSE loss\nfunction, without any increase in the computational complexity during testing.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 22:00:10 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Pandey", "Ram Krishna", ""], ["Saha", "Nabagata", ""], ["Karmakar", "Samarjit", ""], ["Ramakrishnan", "A G", ""]]}, {"id": "1809.00962", "submitter": "Albert Fannjiang", "authors": "A. Fannjiang and Z. Zhang", "title": "Blind Ptychography by Douglas-Rachford Splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind ptychography is the scanning version of coherent diffractive imaging\nwhich seeks to recover both the object and the probe simultaneously. Based on\nalternating minimization by Douglas-Rachford splitting, AMDRS is a blind\nptychographic algorithm informed by the uniqueness theory, the Poisson noise\nmodel and the stability analysis. Enhanced by the initialization method and the\nuse of a randomly phased mask, AMDRS converges globally and geometrically.\nThree boundary conditions are considered in the simulations: periodic,\ndark-field and bright-field boundary conditions. The dark-field boundary\ncondition is suited for isolated objects while the bright-field boundary\ncondition is for non-isolated objects. The periodic boundary condition is a\nmathematically convenient reference point. Depending on the avail- ability of\nthe boundary prior the dark-field and the bright-field boundary conditions may\nor may not be enforced in the reconstruction. Not surprisingly, enforcing the\nboundary condition improves the rate of convergence, sometimes in a significant\nway. Enforcing the bright-field condition in the reconstruction can also remove\nthe linear phase ambiguity.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 07:51:43 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 05:15:38 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 06:05:56 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Fannjiang", "A.", ""], ["Zhang", "Z.", ""]]}, {"id": "1809.00969", "submitter": "Anima Majumder", "authors": "Madhu Babu V, Anima Majumder, Kaushik Das and Swagat Kumar", "title": "A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth\n  and Ego-Motion Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised deep learning framework called UnDEMoN\nfor estimating dense depth map and 6-DoF camera pose information directly from\nmonocular images. The proposed network is trained using unlabeled monocular\nstereo image pairs and is shown to provide superior performance in depth and\nego-motion estimation compared to the existing state-of-the-art. These\nimprovements are achieved by introducing a new objective function that aims to\nminimize spatial as well as temporal reconstruction losses simultaneously.\nThese losses are defined using bi-linear sampling kernel and penalized using\nthe Charbonnier penalty function. The objective function, thus created,\nprovides robustness to image gradient noises thereby improving the overall\nestimation accuracy without resorting to any coarse to fine strategies which\nare currently prevalent in the literature. Another novelty lies in the fact\nthat we combine a disparity-based depth estimation network with a pose\nestimation network to obtain absolute scale-aware 6 DOF Camera pose and\nsuperior depth map. The effectiveness of the proposed approach is demonstrated\nthrough performance comparison with the existing supervised and unsupervised\nmethods on the KITTI driving dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 11:40:58 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 10:40:56 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 09:21:17 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Babu", "Madhu", "V"], ["Majumder", "Anima", ""], ["Das", "Kaushik", ""], ["Kumar", "Swagat", ""]]}, {"id": "1809.00970", "submitter": "Laurent Lejeune", "authors": "Laurent Lejeune, Jan Grossrieder, Raphael Sznitman", "title": "Iterative multi-path tracking for video and volume segmentation with\n  sparse point supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent machine learning strategies for segmentation tasks have shown great\nability when trained on large pixel-wise annotated image datasets. It remains a\nmajor challenge however to aggregate such datasets, as the time and monetary\ncost associated with collecting extensive annotations is extremely high. This\nis particularly the case for generating precise pixel-wise annotations in video\nand volumetric image data. To this end, this work presents a novel framework to\nproduce pixel-wise segmentations using minimal supervision. Our method relies\non 2D point supervision, whereby a single 2D location within an object of\ninterest is provided on each image of the data. Our method then estimates the\nobject appearance in a semi-supervised fashion by learning\nobject-image-specific features and by using these in a semi-supervised learning\nframework. Our object model is then used in a graph-based optimization problem\nthat takes into account all provided locations and the image data in order to\ninfer the complete pixel-wise segmentation. In practice, we solve this\noptimally as a tracking problem using a K-shortest path approach. Both the\nobject model and segmentation are then refined iteratively to further improve\nthe final segmentation. We show that by collecting 2D locations using a gaze\ntracker, our approach can provide state-of-the-art segmentations on a range of\nobjects and image modalities (video and 3D volumes), and that these can then be\nused to train supervised machine learning classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 13:38:50 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lejeune", "Laurent", ""], ["Grossrieder", "Jan", ""], ["Sznitman", "Raphael", ""]]}, {"id": "1809.00972", "submitter": "Yurui Qu", "authors": "Yurui Qu, Li Jing, Yichen Shen, Min Qiu, Marin Soljacic", "title": "Migrating Knowledge between Physical Scenarios based on Artificial\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is known to be data-hungry, which hinders its application in\nmany areas of science when datasets are small. Here, we propose to use transfer\nlearning methods to migrate knowledge between different physical scenarios and\nsignificantly improve the prediction accuracy of artificial neural networks\ntrained on a small dataset. This method can help reduce the demand for\nexpensive data by making use of additional inexpensive data. First, we\ndemonstrate that in predicting the transmission from multilayer photonic film,\nthe relative error rate is reduced by 46.8% (26.5%) when the source data comes\nfrom 10-layer (8-layer) films and the target data comes from 8-layer (10-layer)\nfilms. Second, we show that the relative error rate is decreased by 22% when\nknowledge is transferred between two very different physical scenarios:\ntransmission from multilayer films and scattering from multilayer\nnanoparticles. Finally, we propose a multi-task learning method to improve the\nperformance of different physical scenarios simultaneously in which each task\nonly has a small dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 20:46:50 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 01:17:20 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Qu", "Yurui", ""], ["Jing", "Li", ""], ["Shen", "Yichen", ""], ["Qiu", "Min", ""], ["Soljacic", "Marin", ""]]}, {"id": "1809.00977", "submitter": "Jacob Nogas", "authors": "Jacob Nogas, Shehroz S. Khan, Alex Mihailidis", "title": "DeepFall -- Non-invasive Fall Detection with Deep Spatio-Temporal\n  Convolutional Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human falls rarely occur; however, detecting falls is very important from the\nhealth and safety perspective. Due to the rarity of falls, it is difficult to\nemploy supervised classification techniques to detect them. Moreover, in these\nhighly skewed situations it is also difficult to extract domain specific\nfeatures to identify falls. In this paper, we present a novel framework,\n\\textit{DeepFall}, which formulates the fall detection problem as an anomaly\ndetection problem. The \\textit{DeepFall} framework presents the novel use of\ndeep spatio-temporal convolutional autoencoders to learn spatial and temporal\nfeatures from normal activities using non-invasive sensing modalities. We also\npresent a new anomaly scoring method that combines the reconstruction score of\nframes across a temporal window to detect unseen falls. We tested the\n\\textit{DeepFall} framework on three publicly available datasets collected\nthrough non-invasive sensing modalities, thermal camera and depth cameras and\nshow superior results in comparison to traditional autoencoder methods to\nidentify unseen falls.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 16:41:58 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 18:06:38 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 18:12:14 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Nogas", "Jacob", ""], ["Khan", "Shehroz S.", ""], ["Mihailidis", "Alex", ""]]}, {"id": "1809.00981", "submitter": "Xiaofeng Zhang", "authors": "Xiaofeng Zhang, Zhangyang Wang, Dong Liu, Qing Ling", "title": "DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime\n  Classification", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has revolutionized the performance of classification, but\nmeanwhile demands sufficient labeled data for training. Given insufficient\ndata, while many techniques have been developed to help combat overfitting, the\nchallenge remains if one tries to train deep networks, especially in the\nill-posed extremely low data regimes: only a small set of labeled data are\navailable, and nothing -- including unlabeled data -- else. Such regimes arise\nfrom practical situations where not only data labeling but also data collection\nitself is expensive. We propose a deep adversarial data augmentation (DADA)\ntechnique to address the problem, in which we elaborately formulate data\naugmentation as a problem of training a class-conditional and supervised\ngenerative adversarial network (GAN). Specifically, a new discriminator loss is\nproposed to fit the goal of data augmentation, through which both real and\naugmented samples are enforced to contribute to and be consistent in finding\nthe decision boundaries. Tailored training techniques are developed\naccordingly. To quantitatively validate its effectiveness, we first perform\nextensive simulations to show that DADA substantially outperforms both\ntraditional data augmentation and a few GAN-based options. We then extend\nexperiments to three real-world small labeled datasets where existing data\naugmentation and/or transfer learning strategies are either less effective or\ninfeasible. All results endorse the superior capability of DADA in enhancing\nthe generalization ability of deep networks trained in practical extremely low\ndata regimes. Source code is available at\nhttps://github.com/SchafferZhang/DADA.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 09:01:31 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Zhang", "Xiaofeng", ""], ["Wang", "Zhangyang", ""], ["Liu", "Dong", ""], ["Ling", "Qing", ""]]}, {"id": "1809.00982", "submitter": "Dedimuni De Silva", "authors": "D. D. N. De Silva, S. Fernando, I. T. S. Piyatilake, and A. V. S.\n  Karunarathne", "title": "Wavelet based edge feature enhancement for convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are able to perform a hierarchical learning\nprocess starting with local features. However, a limited attention is paid to\nenhancing such elementary level features like edges. We propose and evaluate\ntwo wavelet-based edge feature enhancement methods to preprocess the input\nimages to convolutional neural networks. The first method develops feature\nenhanced representations by decomposing the input images using wavelet\ntransform and limited reconstructing subsequently. The second method develops\nsuch feature enhanced inputs to the network using local modulus maxima of\nwavelet coefficients. For each method, we have developed a new preprocessing\nlayer by implementing each purposed method and have appended to the network\narchitecture. Our empirical evaluations demonstrate that the proposed methods\nare outperforming the baselines and previously published work with significant\naccuracy gains.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 06:13:01 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 17:56:20 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["De Silva", "D. D. N.", ""], ["Fernando", "S.", ""], ["Piyatilake", "I. T. S.", ""], ["Karunarathne", "A. V. S.", ""]]}, {"id": "1809.01000", "submitter": "Fei Jiang", "authors": "Fei Jiang, Guosheng Yin", "title": "Bayesian Outdoor Defect Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian defect detector to facilitate the defect detection on\nthe motion blurred images on rough texture surfaces. To enhance the accuracy of\nBayesian detection on removing non-defect pixels, we develop a class of\nreflected non-local prior distributions, which is constructed by using the mode\nof a distribution to subtract its density. The reflected non-local priors\nforces the Bayesian detector to approach 0 at the non-defect locations. We\nconduct experiments studies to demonstrate the superior performance of the\nBayesian detector in eliminating the non-defect points. We implement the\nBayesian detector in the motion blurred drone images, in which the detector\nsuccessfully identifies the hail damages on the rough surface and substantially\nenhances the accuracy of the entire defect detection pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 12:36:36 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jiang", "Fei", ""], ["Yin", "Guosheng", ""]]}, {"id": "1809.01011", "submitter": "Navaneethkrishnan Balamuralidhar", "authors": "Saumya Kumaar, Navaneethkrishnan B, Sumedh Mannar and S N Omkar", "title": "JuncNet: A Deep Neural Network for Road Junction Disambiguation for\n  Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a great amount of research going on in the field of autonomous vehicles\nor self-driving cars, there has been considerable progress in road detection\nand tracking algorithms. Most of these algorithms use GPS to handle road\njunctions and its subsequent decisions. However, there are places in the urban\nenvironment where it becomes difficult to get GPS fixes which render the\njunction decision handling erroneous or possibly risky. Vision-based junction\ndetection, however, does not have such problems. This paper proposes a novel\ndeep convolutional neural network architecture for disambiguation of junctions\nfrom roads with a high degree of accuracy. This network is benchmarked against\nother well known classifying network architectures like AlexNet and VGGnet.\nFurther, we discuss a potential road navigation methodology which uses the\nproposed network model. We conclude by performing an experimental validation of\nthe trained network and the navigational method on the roads of the Indian\nInstitute of Science (IISc).\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 11:18:18 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Kumaar", "Saumya", ""], ["B", "Navaneethkrishnan", ""], ["Mannar", "Sumedh", ""], ["Omkar", "S N", ""]]}, {"id": "1809.01015", "submitter": "Nicolo' Savioli", "authors": "Nicol\\'o Savioli, Miguel Silva Vieira, Pablo Lamata, Giovanni Montana", "title": "Automated segmentation on the entire cardiac cycle using a deep learning\n  work-flow", "comments": "6 pages, 2 figures, published on IEEE Xplore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The segmentation of the left ventricle (LV) from CINE MRI images is essential\nto infer important clinical parameters. Typically, machine learning algorithms\nfor automated LV segmentation use annotated contours from only two cardiac\nphases, diastole, and systole. In this work, we present an analysis work-flow\nfor fully-automated LV segmentation that learns from images acquired through\nthe cardiac cycle. The workflow consists of three components: first, for each\nimage in the sequence, we perform an automated localization and subsequent\ncropping of the bounding box containing the cardiac silhouette. Second, we\nidentify the LV contours using a Temporal Fully Convolutional Neural Network\n(T-FCNN), which extends Fully Convolutional Neural Networks (FCNN) through a\nrecurrent mechanism enforcing temporal coherence across consecutive frames.\nFinally, we further defined the boundaries using either one of two components:\nfully-connected Conditional Random Fields (CRFs) with Gaussian edge potentials\nand Semantic Flow. Our initial experiments suggest that significant improvement\nin performance can potentially be achieved by using a recurrent neural network\ncomponent that explicitly learns cardiac motion patterns whilst performing LV\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:07:31 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Savioli", "Nicol\u00f3", ""], ["Vieira", "Miguel Silva", ""], ["Lamata", "Pablo", ""], ["Montana", "Giovanni", ""]]}, {"id": "1809.01016", "submitter": "Yangling Ma", "authors": "Yangling Ma, Yixin Luo, Zhouwang Yang", "title": "Geometric Operator Convolutional Neural Network", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Convolutional Neural Network (CNN) has been successfully applied in many\nfields during recent decades; however it lacks the ability to utilize prior\ndomain knowledge when dealing with many realistic problems. We present a\nframework called Geometric Operator Convolutional Neural Network (GO-CNN) that\nuses domain knowledge, wherein the kernel of the first convolutional layer is\nreplaced with a kernel generated by a geometric operator function. This\nframework integrates many conventional geometric operators, which allows it to\nadapt to a diverse range of problems. Under certain conditions, we\ntheoretically analyze the convergence and the bound of the generalization\nerrors between GO-CNNs and common CNNs. Although the geometric operator\nconvolution kernels have fewer trainable parameters than common convolution\nkernels, the experimental results indicate that GO-CNN performs more accurately\nthan common CNN on CIFAR-10/100. Furthermore, GO-CNN reduces dependence on the\namount of training examples and enhances adversarial stability. In the\npractical task of medically diagnosing bone fractures, GO-CNN obtains 3%\nimprovement in terms of the recall.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 14:19:39 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ma", "Yangling", ""], ["Luo", "Yixin", ""], ["Yang", "Zhouwang", ""]]}, {"id": "1809.01019", "submitter": "Paul-Edouard Sarlin", "authors": "Paul-Edouard Sarlin, Fr\\'ed\\'eric Debraine, Marcin Dymczyk, Roland\n  Siegwart, Cesar Cadena", "title": "Leveraging Deep Visual Descriptors for Hierarchical Efficient\n  Localization", "comments": "CoRL 2018 Camera-ready (fix typos and update citations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many robotics applications require precise pose estimates despite operating\nin large and changing environments. This can be addressed by visual\nlocalization, using a pre-computed 3D model of the surroundings. The pose\nestimation then amounts to finding correspondences between 2D keypoints in a\nquery image and 3D points in the model using local descriptors. However,\ncomputational power is often limited on robotic platforms, making this task\nchallenging in large-scale environments. Binary feature descriptors\nsignificantly speed up this 2D-3D matching, and have become popular in the\nrobotics community, but also strongly impair the robustness to perceptual\naliasing and changes in viewpoint, illumination and scene structure. In this\nwork, we propose to leverage recent advances in deep learning to perform an\nefficient hierarchical localization. We first localize at the map level using\nlearned image-wide global descriptors, and subsequently estimate a precise pose\nfrom 2D-3D matches computed in the candidate places only. This restricts the\nlocal search and thus allows to efficiently exploit powerful non-binary\ndescriptors usually dismissed on resource-constrained devices. Our approach\nresults in state-of-the-art localization performance while running in real-time\non a popular mobile platform, enabling new prospects for robotics research.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 14:25:17 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 20:51:28 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Sarlin", "Paul-Edouard", ""], ["Debraine", "Fr\u00e9d\u00e9ric", ""], ["Dymczyk", "Marcin", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "1809.01040", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Iris recognition in cases of eye pathology", "comments": "Accepted for publication as a chapter in A. Nait-Ali (Ed.),\n  \"Biometrics under Biomedical Considerations\", Springer, 2019, ISBN\n  978-981-13-1143-7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter provides insight on how iris recognition, one of the leading\nbiometric identification technologies in the world, can be impacted by\npathologies and illnesses present in the eye, what are the possible\nrepercussions of this influence, and what are the possible means for taking\nsuch effects into account when matching iris samples.\n  To make this study possible, a special database of iris images has been used,\nrepresenting more than 20 different medical conditions of the ocular region\n(including cataract, glaucoma, rubeosis iridis, synechiae, iris defects,\ncorneal pathologies and other) and containing almost 3000 samples collected\nfrom 230 distinct irises. Then, with the use of four different iris recognition\nmethods, a series of experiments has been conducted, concluding in several\nimportant observations.\n  One of the most popular ocular disorders worldwide - the cataract - is shown\nto worsen genuine comparison scores when results obtained from\ncataract-affected eyes are compared to those coming from healthy irises. An\nanalysis devoted to different types of impact on eye structures caused by\ndiseases is also carried out with significant results. The enrollment process\nis highly sensitive to those eye conditions that make the iris obstructed or\nintroduce geometrical distortions. Disorders affecting iris geometry, or\nproducing obstructions are exceptionally capable of degrading the genuine\ncomparison scores, so that the performance of the entire biometric system can\nbe influenced. Experiments also reveal that imperfect execution of the image\nsegmentation stage is the most prominent contributor to recognition errors.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 15:22:42 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1809.01110", "submitter": "Fuwen Tan", "authors": "Fuwen Tan, Song Feng, Vicente Ordonez", "title": "Text2Scene: Generating Compositional Scenes from Textual Descriptions", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose Text2Scene, a model that generates various forms of\ncompositional scene representations from natural language descriptions. Unlike\nrecent works, our method does NOT use Generative Adversarial Networks (GANs).\nText2Scene instead learns to sequentially generate objects and their attributes\n(location, size, appearance, etc) at every time step by attending to different\nparts of the input text and the current status of the generated scene. We show\nthat under minor modifications, the proposed framework can handle the\ngeneration of different forms of scene representations, including cartoon-like\nscenes, object layouts corresponding to real images, and synthetic images. Our\nmethod is not only competitive when compared with state-of-the-art GAN-based\nmethods using automatic metrics and superior based on human judgments but also\nhas the advantage of producing interpretable results.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 17:31:13 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 06:20:36 GMT"}, {"version": "v3", "created": "Sun, 9 Jun 2019 16:22:47 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tan", "Fuwen", ""], ["Feng", "Song", ""], ["Ordonez", "Vicente", ""]]}, {"id": "1809.01123", "submitter": "Yuan-Ting Hu", "authors": "Yuan-Ting Hu, Jia-Bin Huang, Alexander G. Schwing", "title": "VideoMatch: Matching based Video Object Segmentation", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object segmentation is challenging yet important in a wide variety of\napplications for video analysis. Recent works formulate video object\nsegmentation as a prediction task using deep nets to achieve appealing\nstate-of-the-art performance. Due to the formulation as a prediction task, most\nof these methods require fine-tuning during test time, such that the deep nets\nmemorize the appearance of the objects of interest in the given video. However,\nfine-tuning is time-consuming and computationally expensive, hence the\nalgorithms are far from real time. To address this issue, we develop a novel\nmatching based algorithm for video object segmentation. In contrast to\nmemorization based classification techniques, the proposed approach learns to\nmatch extracted features to a provided template without memorizing the\nappearance of the objects. We validate the effectiveness and the robustness of\nthe proposed method on the challenging DAVIS-16, DAVIS-17, Youtube-Objects and\nJumpCut datasets. Extensive results show that our method achieves comparable\nperformance without fine-tuning and is much more favorable in terms of\ncomputational time.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 17:59:53 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Hu", "Yuan-Ting", ""], ["Huang", "Jia-Bin", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1809.01124", "submitter": "Medhini Narasimhan", "authors": "Medhini Narasimhan, Alexander G. Schwing", "title": "Straight to the Facts: Learning Knowledge Base Retrieval for Factual\n  Visual Question Answering", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering is an important task for autonomous agents and virtual\nassistants alike and was shown to support the disabled in efficiently\nnavigating an overwhelming environment. Many existing methods focus on\nobservation-based questions, ignoring our ability to seamlessly combine\nobserved content with general knowledge. To understand interactions with a\nknowledge base, a dataset has been introduced recently and keyword matching\ntechniques were shown to yield compelling results despite being vulnerable to\nmisconceptions due to synonyms and homographs. To address this issue, we\ndevelop a learning-based approach which goes straight to the facts via a\nlearned embedding space. We demonstrate state-of-the-art results on the\nchallenging recently introduced fact-based visual question answering dataset,\noutperforming competing methods by more than 5%.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 17:59:55 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Narasimhan", "Medhini", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1809.01125", "submitter": "Yuan-Ting Hu", "authors": "Yuan-Ting Hu, Jia-Bin Huang, Alexander G. Schwing", "title": "Unsupervised Video Object Segmentation using Motion Saliency-Guided\n  Spatio-Temporal Propagation", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised video segmentation plays an important role in a wide variety of\napplications from object identification to compression. However, to date, fast\nmotion, motion blur and occlusions pose significant challenges. To address\nthese challenges for unsupervised video segmentation, we develop a novel\nsaliency estimation technique as well as a novel neighborhood graph, based on\noptical flow and edge cues. Our approach leads to significantly better initial\nforeground-background estimates and their robust as well as accurate diffusion\nacross time. We evaluate our proposed algorithm on the challenging DAVIS,\nSegTrack v2 and FBMS-59 datasets. Despite the usage of only a standard edge\ndetector trained on 200 images, our method achieves state-of-the-art results\noutperforming deep learning based methods in the unsupervised setting. We even\ndemonstrate competitive results comparable to deep learning based methods in\nthe semi-supervised setting on the DAVIS dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 17:59:55 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Hu", "Yuan-Ting", ""], ["Huang", "Jia-Bin", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1809.01238", "submitter": "Zhangjie Cao", "authors": "Zhangjie Cao, Ziping Sun, Mingsheng Long, Jianmin Wang, and Philip S.\n  Yu", "title": "Deep Priority Hashing", "comments": "ACMMM 2018 Poster. arXiv admin note: text overlap with\n  arXiv:1702.00758", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing enables image retrieval by end-to-end learning of deep\nrepresentations and hash codes from training data with pairwise similarity\ninformation. Subject to the distribution skewness underlying the similarity\ninformation, most existing deep hashing methods may underperform for imbalanced\ndata due to misspecified loss functions. This paper presents Deep Priority\nHashing (DPH), an end-to-end architecture that generates compact and balanced\nhash codes in a Bayesian learning framework. The main idea is to reshape the\nstandard cross-entropy loss for similarity-preserving learning such that it\ndown-weighs the loss associated to highly-confident pairs. This idea leads to a\nnovel priority cross-entropy loss, which prioritizes the training on uncertain\npairs over confident pairs. Also, we propose another priority quantization\nloss, which prioritizes hard-to-quantize examples for generation of nearly\nlossless hash codes. Extensive experiments demonstrate that DPH can generate\nhigh-quality hash codes and yield state-of-the-art image retrieval results on\nthree datasets, ImageNet, NUS-WIDE, and MS-COCO.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 20:48:18 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Cao", "Zhangjie", ""], ["Sun", "Ziping", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1809.01263", "submitter": "Xi Mo", "authors": "Xi Mo, Ke Tao, Quan Wang, Guanghui Wang", "title": "An Efficient Approach for Polyps Detection in Endoscopic Videos Based on\n  Faster R-CNN", "comments": "6 pages, 10 figures,2018 International Conference on Pattern\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polyp has long been considered as one of the major etiologies to colorectal\ncancer which is a fatal disease around the world, thus early detection and\nrecognition of polyps plays a crucial role in clinical routines. Accurate\ndiagnoses of polyps through endoscopes operated by physicians becomes a\nchallenging task not only due to the varying expertise of physicians, but also\nthe inherent nature of endoscopic inspections. To facilitate this process,\ncomputer-aid techniques that emphasize fully-conventional image processing and\nnovel machine learning enhanced approaches have been dedicatedly designed for\npolyp detection in endoscopic videos or images. Among all proposed algorithms,\ndeep learning based methods take the lead in terms of multiple metrics in\nevolutions for algorithmic performance. In this work, a highly effective model,\nnamely the faster region-based convolutional neural network (Faster R-CNN) is\nimplemented for polyp detection. In comparison with the reported results of the\nstate-of-the-art approaches on polyps detection, extensive experiments\ndemonstrate that the Faster R-CNN achieves very competing results, and it is an\nefficient approach for clinical practice.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 22:43:13 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Mo", "Xi", ""], ["Tao", "Ke", ""], ["Wang", "Quan", ""], ["Wang", "Guanghui", ""]]}, {"id": "1809.01268", "submitter": "Julian Nubert", "authors": "Julian Nubert, Niklas Funk, Fabio Meier, Fabrice Oehler", "title": "Developing a Purely Visual Based Obstacle Detection using Inverse\n  Perspective Mapping", "comments": "Project report and analysis for the Duckietown Project\n  (https://www.duckietown.org/). 17 pages and 38 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our solution is implemented in and for the frame of Duckietown. The goal of\nDuckietown is to provide a relatively simple platform to explore, tackle and\nsolve many problems linked to autonomous driving. \"Duckietown\" is simple in the\nbasics, but an infinitely expandable environment. From controlling single\ndriving Duckiebots until complete fleet management, every scenario is possible\nand can be put into practice. So far, none of the existing modules was capable\nof reliably detecting obstacles and reacting to them in real time. We faced the\ngeneral problem of detecting obstacles given images from a monocular RGB camera\nmounted at the front of our Duckiebot and reacting to them properly without\ncrashing or erroneously stopping the Duckiebot. Both, the detection as well as\nthe reaction have to be implemented and have to run on a Raspberry Pi in real\ntime. Due to the strong hardware limitations, we decided to not use any\nlearning algorithms for the obstacle detection part. As it later transpired, a\nworking \"hard coded\" software needs thorough analysis and understanding of the\ngiven problem. In layman's terms, we simply seek to make Duckietown a safer\nplace.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 23:09:45 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Nubert", "Julian", ""], ["Funk", "Niklas", ""], ["Meier", "Fabio", ""], ["Oehler", "Fabrice", ""]]}, {"id": "1809.01281", "submitter": "Nadine Chang", "authors": "Nadine Chang, John A. Pyles, Abhinav Gupta, Michael J. Tarr, Elissa M.\n  Aminoff", "title": "BOLD5000: A public fMRI dataset of 5000 images", "comments": "Currently in submission to Scientific Data", "journal-ref": null, "doi": "10.1038/s41597-019-0052-3", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vision science, particularly machine vision, has been revolutionized by\nintroducing large-scale image datasets and statistical learning approaches.\nYet, human neuroimaging studies of visual perception still rely on small\nnumbers of images (around 100) due to time-constrained experimental procedures.\nTo apply statistical learning approaches that integrate neuroscience, the\nnumber of images used in neuroimaging must be significantly increased. We\npresent BOLD5000, a human functional MRI (fMRI) study that includes almost\n5,000 distinct images depicting real-world scenes. Beyond dramatically\nincreasing image dataset size relative to prior fMRI studies, BOLD5000 also\naccounts for image diversity, overlapping with standard computer vision\ndatasets by incorporating images from the Scene UNderstanding (SUN), Common\nObjects in Context (COCO), and ImageNet datasets. The scale and diversity of\nthese image datasets, combined with a slow event-related fMRI design, enable\nfine-grained exploration into the neural representation of a wide range of\nvisual features, categories, and semantics. Concurrently, BOLD5000 brings us\ncloser to realizing Marr's dream of a singular vision science - the intertwined\nstudy of biological and computer vision.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 00:50:34 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Chang", "Nadine", ""], ["Pyles", "John A.", ""], ["Gupta", "Abhinav", ""], ["Tarr", "Michael J.", ""], ["Aminoff", "Elissa M.", ""]]}, {"id": "1809.01318", "submitter": "Ke Wang", "authors": "Ke Wang, Han Song, Jiahui Zhang, Xinran Zhang, Hongen Liao", "title": "Reconstruction and Registration of Large-Scale Medical Scene Using Point\n  Clouds Data from Different Modalities", "comments": "2 pages, 4 figures, submitted to ACCAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensing the medical scenario can ensure the safety during the surgical\noperations. So, in this regard, a monitor platform which can obtain the\naccurate location information of the surgery room is desperately needed.\nCompared to 2D camera image, 3D data contains more information of distance and\ndirection. Therefore, 3D sensors are more suitable to be used in surgical scene\nmonitoring. However, each 3D sensor has its own limitations. For example, Lidar\n(Light Detection and Ranging) can detect large-scale environment with high\nprecision, but the point clouds or depth maps are very sparse. As for commodity\nRGBD sensors, such as Kinect, can accurately capture denser data, but limited\nto a small range from 0.5 to 4.5m. So, a proper method which can address these\nproblems for fusing different modalities data is important. In this paper, we\nproposed a method which can fuse different modalities 3D data to get a\nlarge-scale and dense point cloud. The key contributions of our work are as\nfollows. First, we proposed a 3D data collecting system to reconstruct the\nmedical scenes. By fusing the Lidar and Kinect data, a large-scale medical\nscene with more details can be reconstructed. Second, we proposed a\nlocation-based fast point clouds registration algorithm to deal with different\nmodality datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 04:32:42 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Wang", "Ke", ""], ["Song", "Han", ""], ["Zhang", "Jiahui", ""], ["Zhang", "Xinran", ""], ["Liao", "Hongen", ""]]}, {"id": "1809.01330", "submitter": "Hongyang Gao", "authors": "Hongyang Gao, Zhengyang Wang, Shuiwang Ji", "title": "ChannelNets: Compact and Efficient Convolutional Neural Networks via\n  Channel-Wise Convolutions", "comments": "10 pages, NIPS18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown great capability of solving\nvarious artificial intelligence tasks. However, the increasing model size has\nraised challenges in employing them in resource-limited applications. In this\nwork, we propose to compress deep models by using channel-wise convolutions,\nwhich re- place dense connections among feature maps with sparse ones in CNNs.\nBased on this novel operation, we build light-weight CNNs known as ChannelNets.\nChannel- Nets use three instances of channel-wise convolutions; namely group\nchannel-wise convolutions, depth-wise separable channel-wise convolutions, and\nthe convolu- tional classification layer. Compared to prior CNNs designed for\nmobile devices, ChannelNets achieve a significant reduction in terms of the\nnumber of parameters and computational cost without loss in accuracy. Notably,\nour work represents the first attempt to compress the fully-connected\nclassification layer, which usually accounts for about 25% of total parameters\nin compact CNNs. Experimental results on the ImageNet dataset demonstrate that\nChannelNets achieve consistently better performance compared to prior methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 05:15:14 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Gao", "Hongyang", ""], ["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1809.01337", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor\n  Darrell, Bryan Russell", "title": "Localizing Moments in Video with Temporal Language", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing moments in a longer video via natural language queries is a new,\nchallenging task at the intersection of language and video understanding.\nThough moment localization with natural language is similar to other language\nand vision tasks like natural language object retrieval in images, moment\nlocalization offers an interesting opportunity to model temporal dependencies\nand reasoning in text. We propose a new model that explicitly reasons about\ndifferent temporal segments in a video, and shows that temporal context is\nimportant for localizing phrases which include temporal language. To benchmark\nwhether our model, and other recent video localization models, can effectively\nreason about temporal language, we collect the novel TEMPOral reasoning in\nvideo and language (TEMPO) dataset. Our dataset consists of two parts: a\ndataset with real videos and template sentences (TEMPO - Template Language)\nwhich allows for controlled studies on temporal language, and a human language\ndataset which consists of temporal sentences annotated by humans (TEMPO - Human\nLanguage).\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 05:58:47 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Wang", "Oliver", ""], ["Shechtman", "Eli", ""], ["Sivic", "Josef", ""], ["Darrell", "Trevor", ""], ["Russell", "Bryan", ""]]}, {"id": "1809.01348", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Vineet Jain, Arnab Mondal, Prabir Kumar Biswas", "title": "Retinal Vessel Segmentation under Extreme Low Annotation: A Generative\n  Adversarial Network Approach", "comments": "* First 3 authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary deep learning based medical image segmentation algorithms\nrequire hours of annotation labor by domain experts. These data hungry deep\nmodels perform sub-optimally in the presence of limited amount of labeled data.\nIn this paper, we present a data efficient learning framework using the recent\nconcept of Generative Adversarial Networks; this allows a deep neural network\nto perform significantly better than its fully supervised counterpart in low\nannotation regime. The proposed method is an extension of our previous work\nwith the addition of a new unsupervised adversarial loss and a structured\nprediction based architecture. To the best of our knowledge, this work is the\nfirst demonstration of an adversarial framework based structured prediction\nmodel for medical image segmentation. Though generic, we apply our method for\nsegmentation of blood vessels in retinal fundus images. We experiment with\nextreme low annotation budget (0.8 - 1.6% of contemporary annotation size). On\nDRIVE and STARE datasets, the proposed method outperforms our previous method\nand other fully supervised benchmark models by significant margins especially\nwith very low number of annotated examples. In addition, our systematic\nablation studies suggest some key recipes for successfully training GAN based\nsemi-supervised algorithms with an encoder-decoder style network architecture.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 06:31:56 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Lahiri", "Avisek", ""], ["Jain", "Vineet", ""], ["Mondal", "Arnab", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1809.01354", "submitter": "Quan Chen", "authors": "Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, Kun Gai", "title": "Semantic Human Matting", "comments": "ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human matting, high quality extraction of humans from natural images, is\ncrucial for a wide variety of applications. Since the matting problem is\nseverely under-constrained, most previous methods require user interactions to\ntake user designated trimaps or scribbles as constraints. This user-in-the-loop\nnature makes them difficult to be applied to large scale data or time-sensitive\nscenarios. In this paper, instead of using explicit user input constraints, we\nemploy implicit semantic constraints learned from data and propose an automatic\nhuman matting algorithm (SHM). SHM is the first algorithm that learns to\njointly fit both semantic information and high quality details with deep\nnetworks. In practice, simultaneously learning both coarse semantics and fine\ndetails is challenging. We propose a novel fusion strategy which naturally\ngives a probabilistic estimation of the alpha matte. We also construct a very\nlarge dataset with high quality annotations consisting of 35,513 unique\nforegrounds to facilitate the learning and evaluation of human matting.\nExtensive experiments on this dataset and plenty of real images show that SHM\nachieves comparable results with state-of-the-art interactive matting methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 06:50:24 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 12:36:31 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Quan", ""], ["Ge", "Tiezheng", ""], ["Xu", "Yanyu", ""], ["Zhang", "Zhiqiang", ""], ["Yang", "Xinxin", ""], ["Gai", "Kun", ""]]}, {"id": "1809.01361", "submitter": "Alexander H. Liu", "authors": "Alexander H. Liu, Yen-Cheng Liu, Yu-Ying Yeh, Yu-Chiang Frank Wang", "title": "A Unified Feature Disentangler for Multi-Domain Image Translation and\n  Manipulation", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel and unified deep learning framework which is capable of\nlearning domain-invariant representation from data across multiple domains.\nRealized by adversarial training with additional ability to exploit\ndomain-specific information, the proposed network is able to perform continuous\ncross-domain image translation and manipulation, and produces desirable output\nimages accordingly. In addition, the resulting feature representation exhibits\nsuperior performance of unsupervised domain adaptation, which also verifies the\neffectiveness of the proposed model in learning disentangled features for\ndescribing cross-domain data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 07:39:59 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 07:14:56 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2018 05:08:09 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Liu", "Alexander H.", ""], ["Liu", "Yen-Cheng", ""], ["Yeh", "Yu-Ying", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1809.01368", "submitter": "Chong Luo", "authors": "Anfeng He, Chong Luo, Xinmei Tian and Wenjun Zeng", "title": "Towards a Better Match in Siamese Network Based Visual Object Tracker", "comments": "This paper is accepted by ECCV Visual Object Tracking Challenge\n  Workshop VOT2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Siamese network based trackers have received tremendous interest\nfor their fast tracking speed and high performance. Despite the great success,\nthis tracking framework still suffers from several limitations. First, it\ncannot properly handle large object rotation. Second, tracking gets easily\ndistracted when the background contains salient objects. In this paper, we\npropose two simple yet effective mechanisms, namely angle estimation and\nspatial masking, to address these issues. The objective is to extract more\nrepresentative features so that a better match can be obtained between the same\nobject from different frames. The resulting tracker, named Siam-BM, not only\nsignificantly improves the tracking performance, but more importantly maintains\nthe realtime capability. Evaluations on the VOT2017 dataset show that Siam-BM\nachieves an EAO of 0.335, which makes it the best-performing realtime tracker\nto date.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 07:53:37 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["He", "Anfeng", ""], ["Luo", "Chong", ""], ["Tian", "Xinmei", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1809.01372", "submitter": "Haozhi Huang", "authors": "Haozhi Huang, Senzhe Xu, Junxiong Cai, Wei Liu, Shimin Hu", "title": "Temporally Coherent Video Harmonization Using Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositing is one of the most important editing operations for images and\nvideos. The process of improving the realism of composite results is often\ncalled harmonization. Previous approaches for harmonization mainly focus on\nimages. In this work, we take one step further to attack the problem of video\nharmonization. Specifically, we train a convolutional neural network in an\nadversarial way, exploiting a pixel-wise disharmony discriminator to achieve\nmore realistic harmonized results and introducing a temporal loss to increase\ntemporal consistency between consecutive harmonized frames. Thanks to the\npixel-wise disharmony discriminator, we are also able to relieve the need of\ninput foreground masks. Since existing video datasets which have ground-truth\nforeground masks and optical flows are not sufficiently large, we propose a\nsimple yet efficient method to build up a synthetic dataset supporting\nsupervised training of the proposed adversarial network. Experiments show that\ntraining on our synthetic dataset generalizes well to the real-world composite\ndataset. Also, our method successfully incorporates temporal consistency during\ntraining and achieves more harmonious results than previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 08:01:15 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Huang", "Haozhi", ""], ["Xu", "Senzhe", ""], ["Cai", "Junxiong", ""], ["Liu", "Wei", ""], ["Hu", "Shimin", ""]]}, {"id": "1809.01396", "submitter": "Diana Sungatullina", "authors": "Diana Sungatullina, Egor Zakharov, Dmitry Ulyanov, and Victor\n  Lempitsky", "title": "Image Manipulation with Perceptual Discriminators", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems that perform image manipulation using deep convolutional networks\nhave achieved remarkable realism. Perceptual losses and losses based on\nadversarial discriminators are the two main classes of learning objectives\nbehind these advances. In this work, we show how these two ideas can be\ncombined in a principled and non-additive manner for unaligned image\ntranslation tasks. This is accomplished through a special architecture of the\ndiscriminator network inside generative adversarial learning framework. The new\narchitecture, that we call a perceptual discriminator, embeds the convolutional\nparts of a pre-trained deep classification network inside the discriminator\nnetwork. The resulting architecture can be trained on unaligned image datasets\nwhile benefiting from the robustness and efficiency of perceptual losses. We\ndemonstrate the merits of the new architecture in a series of qualitative and\nquantitative comparisons with baseline approaches and state-of-the-art\nframeworks for unaligned image translation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:17:05 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Sungatullina", "Diana", ""], ["Zakharov", "Egor", ""], ["Ulyanov", "Dmitry", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1809.01407", "submitter": "Xiaohang Zhan", "authors": "Xiaohang Zhan, Ziwei Liu, Junjie Yan, Dahua Lin, Chen Change Loy", "title": "Consensus-Driven Propagation in Massive Unlabeled Data for Face\n  Recognition", "comments": "In ECCV 2018. More details at the project page:\n  http://mmlab.ie.cuhk.edu.hk/projects/CDP/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has witnessed great progress in recent years, mainly\nattributed to the high-capacity model designed and the abundant labeled data\ncollected. However, it becomes more and more prohibitive to scale up the\ncurrent million-level identity annotations. In this work, we show that\nunlabeled face data can be as effective as the labeled ones. Here, we consider\na setting closely mimicking the real-world scenario, where the unlabeled data\nare collected from unconstrained environments and their identities are\nexclusive from the labeled ones. Our main insight is that although the class\ninformation is not available, we can still faithfully approximate these\nsemantic relationships by constructing a relational graph in a bottom-up\nmanner. We propose Consensus-Driven Propagation (CDP) to tackle this\nchallenging problem with two modules, the \"committee\" and the \"mediator\", which\nselect positive face pairs robustly by carefully aggregating multi-view\ninformation. Extensive experiments validate the effectiveness of both modules\nto discard outliers and mine hard positives. With CDP, we achieve a compelling\naccuracy of 78.18% on MegaFace identification challenge by using only 9% of the\nlabels, comparing to 61.78% when no unlabeled data are used and 78.52% when all\nlabels are employed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:41:16 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 08:43:56 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhan", "Xiaohang", ""], ["Liu", "Ziwei", ""], ["Yan", "Junjie", ""], ["Lin", "Dahua", ""], ["Loy", "Chen Change", ""]]}, {"id": "1809.01410", "submitter": "Christoph Baur", "authors": "Christoph Baur, Shadi Albarqouni, Nassir Navab", "title": "Generating Highly Realistic Images of Skin Lesions with GANs", "comments": "Accepted at the MICCAI 2018 ISIC Skin Lesion Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As many other machine learning driven medical image analysis tasks, skin\nimage analysis suffers from a chronic lack of labeled data and skewed class\ndistributions, which poses problems for the training of robust and\nwell-generalizing models. The ability to synthesize realistic looking images of\nskin lesions could act as a reliever for the aforementioned problems.\nGenerative Adversarial Networks (GANs) have been successfully used to\nsynthesize realistically looking medical images, however limited to low\nresolution, whereas machine learning models for challenging tasks such as skin\nlesion segmentation or classification benefit from much higher resolution data.\nIn this work, we successfully synthesize realistically looking images of skin\nlesions with GANs at such high resolution. Therefore, we utilize the concept of\nprogressive growing, which we both quantitatively and qualitatively compare to\nother GAN architectures such as the DCGAN and the LAPGAN. Our results show that\nwith the help of progressive growing, we can synthesize highly realistic\ndermoscopic images of skin lesions that even expert dermatologists find hard to\ndistinguish from real ones.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:47:13 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 09:39:44 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Baur", "Christoph", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""]]}, {"id": "1809.01436", "submitter": "Yan Ju", "authors": "Yan Ju, Lingling Li, Licheng Jiao, Zhongle Ren, Biao Hou, and Shuyuan\n  Yang", "title": "Modified Diversity of Class Probability Estimation Co-training for\n  Hyperspectral Image Classification", "comments": "13 pages, 10 figures and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the limited amount and imbalanced classes of labeled training data,\nthe conventional supervised learning can not ensure the discrimination of the\nlearned feature for hyperspectral image (HSI) classification. In this paper, we\npropose a modified diversity of class probability estimation (MDCPE) with two\ndeep neural networks to learn spectral-spatial feature for HSI classification.\nIn co-training phase, recurrent neural network (RNN) and convolutional neural\nnetwork (CNN) are utilized as two learners to extract features from labeled and\nunlabeled data. Based on the extracted features, MDCPE selects most credible\nsamples to update initial labeled data by combining k-means clustering with the\ntraditional diversity of class probability estimation (DCPE) co-training. In\nthis way, MDCPE can keep new labeled data class-balanced and extract\ndiscriminative features for both the minority and majority classes. During\ntesting process, classification results are acquired by co-decision of the two\nlearners. Experimental results demonstrate that the proposed semi-supervised\nco-training method can make full use of unlabeled information to enhance\ngenerality of the learners and achieve favorable accuracies on all three widely\nused data sets: Salinas, Pavia University and Pavia Center.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 11:12:48 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Ju", "Yan", ""], ["Li", "Lingling", ""], ["Jiao", "Licheng", ""], ["Ren", "Zhongle", ""], ["Hou", "Biao", ""], ["Yang", "Shuyuan", ""]]}, {"id": "1809.01438", "submitter": "Arash Akbarinia", "authors": "Arash Akbarinia and Karl R. Gegenfurtner", "title": "How is Contrast Encoded in Deep Neural Networks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrast is a crucial factor in visual information processing. It is desired\nfor a visual system - irrespective of being biological or artificial - to\n\"perceive\" the world robustly under large potential changes in illumination. In\nthis work, we studied the responses of deep neural networks (DNN) to identical\nimages at different levels of contrast. We analysed the activation of kernels\nin the convolutional layers of eight prominent networks with distinct\narchitectures (e.g. VGG and Inception). The results of our experiments indicate\nthat those networks with a higher tolerance to alteration of contrast have more\nthan one convolutional layer prior to the first max-pooling operator. It\nappears that the last convolutional layer before the first max-pooling acts as\na mitigator of contrast variation in input images. In our investigation,\ninterestingly, we observed many similarities between the mechanisms of these\nDNNs and biological visual systems. These comparisons allow us to understand\nmore profoundly the underlying mechanisms of a visual system that is grounded\non the basis of \"data-analysis\".\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 11:17:09 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Akbarinia", "Arash", ""], ["Gegenfurtner", "Karl R.", ""]]}, {"id": "1809.01442", "submitter": "F\\'abio Vin\\'icius Moreira Perez", "authors": "F\\'abio Perez, Cristina Vasconcelos, Sandra Avila, Eduardo Valle", "title": "Data Augmentation for Skin Lesion Analysis", "comments": "8 pages, 3 figures, to be presented on ISIC Skin Image Analysis\n  Workshop", "journal-ref": null, "doi": "10.1007/978-3-030-01201-4_33", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models show remarkable results in automated skin lesion\nanalysis. However, these models demand considerable amounts of data, while the\navailability of annotated skin lesion images is often limited. Data\naugmentation can expand the training dataset by transforming input images. In\nthis work, we investigate the impact of 13 data augmentation scenarios for\nmelanoma classification trained on three CNNs (Inception-v4, ResNet, and\nDenseNet). Scenarios include traditional color and geometric transforms, and\nmore unusual augmentations such as elastic transforms, random erasing and a\nnovel augmentation that mixes different lesions. We also explore the use of\ndata augmentation at test-time and the impact of data augmentation on various\ndataset sizes. Our results confirm the importance of data augmentation in both\ntraining and testing and show that it can lead to more performance gains than\nobtaining new images. The best scenario results in an AUC of 0.882 for melanoma\nclassification without using external data, outperforming the top-ranked\nsubmission (0.874) for the ISIC Challenge 2017, which was trained with\nadditional data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 11:35:41 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Perez", "F\u00e1bio", ""], ["Vasconcelos", "Cristina", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""]]}, {"id": "1809.01444", "submitter": "Clint Sebastian", "authors": "Clint Sebastian, Ries Uittenbogaard, Julien Vijverberg, Bas Boom, and\n  Peter H.N. de With", "title": "Conditional Transfer with Dense Residual Attention: Synthesizing traffic\n  signs from street-view imagery", "comments": "The first two authors have equal contribution. Accepted at\n  International Conference on Pattern Recognition 2018 (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection and classification of traffic signs in street-view imagery\nis an essential element for asset management, map making and autonomous\ndriving. However, some traffic signs occur rarely and consequently, they are\ndifficult to recognize automatically. To improve the detection and\nclassification rates, we propose to generate images of traffic signs, which are\nthen used to train a detector/classifier. In this research, we present an\nend-to-end framework that generates a realistic image of a traffic sign from a\ngiven image of a traffic sign and a pictogram of the target class. We propose a\nresidual attention mechanism with dense concatenation called Dense Residual\nAttention, that preserves the background information while transferring the\nobject information. We also propose to utilize multi-scale discriminators, so\nthat the smaller scales of the output guide the higher resolution output. We\nhave performed detection and classification tests across a large number of\ntraffic sign classes, by training the detector using the combination of real\nand generated data. The newly trained model reduces the number of false\npositives by 1.2 - 1.5% at 99% recall in the detection tests and an absolute\nimprovement of 4.65% (top-1 accuracy) in the classification tests.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 11:40:47 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Sebastian", "Clint", ""], ["Uittenbogaard", "Ries", ""], ["Vijverberg", "Julien", ""], ["Boom", "Bas", ""], ["de With", "Peter H. N.", ""]]}, {"id": "1809.01456", "submitter": "Chao Zhang", "authors": "Chao Zhang, Xuequan Lu, Takuya Akashi", "title": "Blur-Countering Keypoint Detection via Eigenvalue Asymmetry", "comments": "Contact us for high-resolution clear PDF file if you are interested\n  in this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well-known corner or local extrema feature based detectors such as FAST and\nDoG have achieved noticeable successes. However, detecting keypoints in the\npresence of blur has remained to be an unresolved issue. As a matter of fact,\nvarious kinds of blur (e.g., motion blur, out-of-focus, and space-variant)\nremarkably increase challenges for keypoint detection. As a result, those\nmethods have limited performance. To settle this issue, we propose a\nblur-countering method for detecting valid keypoints for various types and\ndegrees of blurred images. Specifically, we first present a distance metric for\nderivative distributions, which preserves the distinctiveness of patch pairs\nwell under blur. We then model the asymmetry by utilizing the difference of\nsquared eigenvalues based on the distance metric. To make it scale-robust, we\nalso extend it to scale space. The proposed detector is efficient as the main\ncomputational cost is the square of derivatives at each pixel. Extensive visual\nand quantitative results show that our method outperforms current approaches\nunder different types and degrees of blur. Without any parallelization, our\nimplementation\\footnote{We will make our code publicly available upon the\nacceptance.} achieves real-time performance for low-resolution images (e.g.,\n$320\\times240$ pixel).\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 12:24:18 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Zhang", "Chao", ""], ["Lu", "Xuequan", ""], ["Akashi", "Takuya", ""]]}, {"id": "1809.01465", "submitter": "Simon Jenni", "authors": "Simon Jenni, Paolo Favaro", "title": "Deep Bilevel Learning", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel regularization approach to train neural networks that\nenjoys better generalization and test error than standard stochastic gradient\ndescent. Our approach is based on the principles of cross-validation, where a\nvalidation set is used to limit the model overfitting. We formulate such\nprinciples as a bilevel optimization problem. This formulation allows us to\ndefine the optimization of a cost on the validation set subject to another\noptimization on the training set. The overfitting is controlled by introducing\nweights on each mini-batch in the training set and by choosing their values so\nthat they minimize the error on the validation set. In practice, these weights\ndefine mini-batch learning rates in a gradient descent update equation that\nfavor gradients with better generalization capabilities. Because of its\nsimplicity, this approach can be integrated with other regularization methods\nand training schemes. We evaluate extensively our proposed algorithm on several\nneural network architectures and datasets, and find that it consistently\nimproves the generalization of the model, especially when labels are noisy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 12:50:24 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Jenni", "Simon", ""], ["Favaro", "Paolo", ""]]}, {"id": "1809.01543", "submitter": "Weiping Zheng", "authors": "Weiping Zheng, Zhenyao Mo, Xiaotao Xing, and Gansen Zhao", "title": "CNNs-based Acoustic Scene Classification using Multi-Spectrogram Fusion\n  and Label Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectrograms have been widely used in Convolutional Neural Networks based\nschemes for acoustic scene classification, such as the STFT spectrogram and the\nMFCC spectrogram, etc. They have different time-frequency characteristics,\ncontributing to their own advantages and disadvantages in recognizing acoustic\nscenes. In this letter, a novel multi-spectrogram fusion framework is proposed,\nmaking the spectrograms complement each other. In the framework, a single CNN\narchitecture is applied onto multiple spectrograms for feature extraction. The\ndeep features extracted from multiple spectrograms are then fused to\ndiscriminate the acoustic scenes. Moreover, motivated by the inter-class\nsimilarities in acoustic scene datasets, a label expansion method is further\nproposed in which super-class labels are constructed upon the original classes.\nOn the help of the expanded labels, the CNN models are transformed into the\nmultitask learning form to improve the acoustic scene classification by\nappending the auxiliary task of super-class classification. To verify the\neffectiveness of the proposed methods, intensive experiments have been\nperformed on the DCASE2017 and the LITIS Rouen datasets. Experimental results\nshow that the proposed method can achieve promising accuracies on both\ndatasets. Specifically, accuracies of 0.9744, 0.8865 and 0.7778 are obtained\nfor the LITIS Rouen dataset, the DCASE Development set and Evaluation set\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 14:34:08 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Zheng", "Weiping", ""], ["Mo", "Zhenyao", ""], ["Xing", "Xiaotao", ""], ["Zhao", "Gansen", ""]]}, {"id": "1809.01564", "submitter": "Julian Nubert", "authors": "Julian Nubert, Nicholas Giai Truong, Abel Lim, Herbert Ilhan Tanujaya,\n  Leah Lim, Mai Anh Vu", "title": "Traffic Density Estimation using a Convolutional Neural Network", "comments": "Machine Learning Project National University of Singapore. 6 pages, 5\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to introduce and present a machine learning\napplication that aims to improve the quality of life of people in Singapore. In\nparticular, we investigate the use of machine learning solutions to tackle the\nproblem of traffic congestion in Singapore. In layman's terms, we seek to make\nSingapore (or any other city) a smoother place. To accomplish this aim, we\npresent an end-to-end system comprising of 1. A traffic density estimation\nalgorithm at traffic lights/junctions and 2. a suitable traffic signal control\nalgorithms that make use of the density information for better traffic control.\nTraffic density estimation can be obtained from traffic junction images using\nvarious machine learning techniques (combined with CV tools). After research\ninto various advanced machine learning methods, we decided on convolutional\nneural networks (CNNs). We conducted experiments on our algorithms, using the\npublicly available traffic camera dataset published by the Land Transport\nAuthority (LTA) to demonstrate the feasibility of this approach. With these\ntraffic density estimates, different traffic algorithms can be applied to\nminimize congestion at traffic junctions in general.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:03:23 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Nubert", "Julian", ""], ["Truong", "Nicholas Giai", ""], ["Lim", "Abel", ""], ["Tanujaya", "Herbert Ilhan", ""], ["Lim", "Leah", ""], ["Vu", "Mai Anh", ""]]}, {"id": "1809.01567", "submitter": "Marcela Carvalho", "authors": "Marcela Carvalho, Bertrand Le Saux, Pauline Trouv\\'e-Peloux, Andr\\'es\n  Almansa, Fr\\'ed\\'eric Champagnat", "title": "Deep Depth from Defocus: how can defocus blur improve 3D estimation\n  using dense neural networks?", "comments": "3DRW Workshop ECCV 2018. Code:\n  https://github.com/marcelampc/d3net_depth_estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation is of critical interest for scene understanding and accurate\n3D reconstruction. Most recent approaches in depth estimation with deep\nlearning exploit geometrical structures of standard sharp images to predict\ncorresponding depth maps. However, cameras can also produce images with defocus\nblur depending on the depth of the objects and camera settings. Hence, these\nfeatures may represent an important hint for learning to predict depth. In this\npaper, we propose a full system for single-image depth prediction in the wild\nusing depth-from-defocus and neural networks. We carry out thorough experiments\nto test deep convolutional networks on real and simulated defocused images\nusing a realistic model of blur variation with respect to depth. We also\ninvestigate the influence of blur on depth prediction observing model\nuncertainty with a Bayesian neural network approach. From these studies, we\nshow that out-of-focus blur greatly improves the depth-prediction network\nperformances. Furthermore, we transfer the ability learned on a synthetic,\nindoor dataset to real, indoor and outdoor images. For this purpose, we present\na new dataset containing real all-focus and defocused images from a Digital\nSingle-Lens Reflex (DSLR) camera, paired with ground truth depth maps obtained\nwith an active 3D sensor for indoor scenes. The proposed approach is\nsuccessfully validated on both this new dataset and standard ones as NYUv2 or\nDepth-in-the-Wild. Code and new datasets are available at\nhttps://github.com/marcelampc/d3net_depth_estimation\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:09:20 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 10:02:02 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Carvalho", "Marcela", ""], ["Saux", "Bertrand Le", ""], ["Trouv\u00e9-Peloux", "Pauline", ""], ["Almansa", "Andr\u00e9s", ""], ["Champagnat", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1809.01579", "submitter": "Gr\\'egory Paul", "authors": "Denis K. Samuylov, Prateek Purwar, G\\'abor Sz\\'ekely, and Gr\\'egory\n  Paul", "title": "Modelling Point Spread Function in Fluorescence Microscopy with a Sparse\n  Combination of Gaussian Mixture: Trade-off between Accuracy and Efficiency", "comments": "This paper has been accepted in the IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2898843", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deblurring is a fundamental inverse problem in bioimaging. It requires\nmodelling the point spread function (PSF), which captures the optical\ndistortions entailed by the image formation process. The PSF limits the spatial\nresolution attainable for a given microscope. However, recent applications\nrequire a higher resolution, and have prompted the development of\nsuper-resolution techniques to achieve sub-pixel accuracy. This requirement\nrestricts the class of suitable PSF models to analog ones. In addition,\ndeblurring is computationally intensive, hence further requiring\ncomputationally efficient models. A custom candidate fitting both requirements\nis the Gaussian model. However, this model cannot capture the rich tail\nstructures found in both theoretical and empirical PSFs. In this paper, we aim\nat improving the reconstruction accuracy beyond the Gaussian model, while\npreserving its computational efficiency. We introduce a new class of analog PSF\nmodels based on Gaussian mixtures. The number of Gaussian kernels controls both\nthe modelling accuracy and the computational efficiency of the model: the lower\nthe number of kernels, the lower accuracy and the higher efficiency. To explore\nthe accuracy--efficiency trade-off, we propose a variational formulation of the\nPSF calibration problem, where a convex sparsity-inducing penalty on the number\nof Gaussian kernels allows trading accuracy for efficiency. We derive an\nefficient algorithm based on a fully-split formulation of alternating split\nBregman. We assess our framework on synthetic and real data and demonstrate a\nbetter reconstruction accuracy in both geometry and photometry in point source\nlocalisation---a fundamental inverse problem in fluorescence microscopy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:32:37 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 22:34:53 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Samuylov", "Denis K.", ""], ["Purwar", "Prateek", ""], ["Sz\u00e9kely", "G\u00e1bor", ""], ["Paul", "Gr\u00e9gory", ""]]}, {"id": "1809.01590", "submitter": "Gr\\'egory Paul", "authors": "Denis K. Samuylov, G\\'abor Sz\\'ekely, and Gr\\'egory Paul", "title": "A Bayesian framework for the analog reconstruction of kymographs from\n  fluorescence microscopy data", "comments": "This paper has been accepted in the IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2867946", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kymographs are widely used to represent and anal- yse spatio-temporal\ndynamics of fluorescence markers along curvilinear biological compartments.\nThese objects have a sin- gular geometry, thus kymograph reconstruction is\ninherently an analog image processing task. However, the existing approaches\nare essentially digital: the kymograph photometry is sampled directly from the\ntime-lapse images. As a result, such kymographs rely on raw image data that\nsuffer from the degradations entailed by the image formation process and the\nspatio-temporal resolution of the imaging setup. In this work, we address these\nlimitations and introduce a well-grounded Bayesian framework for the analog\nreconstruction of kymographs. To handle the movement of the object, we\nintroduce an intrinsic description of kymographs using differential geometry: a\nkymograph is a photometry defined on a parameter space that is embedded in\nphysical space by a time-varying map that follows the object geometry. We model\nthe kymograph photometry as a L\\'evy innovation process, a flexible class of\nnon-parametric signal priors. We account for the image formation process using\nthe virtual microscope framework. We formulate a computationally tractable\nrepresentation of the associated maximum a posteriori problem and solve it\nusing a class of efficient and modular algorithms based on the alternating\nsplit Bregman. We assess the performance of our Bayesian framework on synthetic\ndata and apply it to reconstruct the fluorescence dynamics along microtubules\nin vivo in the budding yeast S. cerevisiae. We demonstrate that our framework\nallows revealing patterns from single time-lapse data that are invisible on\nstandard digital kymographs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:59:05 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Samuylov", "Denis K.", ""], ["Sz\u00e9kely", "G\u00e1bor", ""], ["Paul", "Gr\u00e9gory", ""]]}, {"id": "1809.01610", "submitter": "Mehdi Moradi", "authors": "Mehdi Moradi, Ali Madani, Yaniv Gur, Yufan Guo, Tanveer Syeda-Mahmood", "title": "Bimodal network architectures for automatic generation of image\n  annotation from text", "comments": "Accepted to MICCAI 2018, LNCS 11070", "journal-ref": "Lecture Notes in Computer Science (LNCS 11070), Proceedings of\n  Medical Image Computing & Computer Assisted Intervention (MICCAI 2018)", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image analysis practitioners have embraced big data methodologies.\nThis has created a need for large annotated datasets. The source of big data is\ntypically large image collections and clinical reports recorded for these\nimages. In many cases, however, building algorithms aimed at segmentation and\ndetection of disease requires a training dataset with markings of the areas of\ninterest on the image that match with the described anomalies. This process of\nannotation is expensive and needs the involvement of clinicians. In this work\nwe propose two separate deep neural network architectures for automatic marking\nof a region of interest (ROI) on the image best representing a finding\nlocation, given a textual report or a set of keywords. One architecture\nconsists of LSTM and CNN components and is trained end to end with images,\nmatching text, and markings of ROIs for those images. The output layer\nestimates the coordinates of the vertices of a polygonal region. The second\narchitecture uses a network pre-trained on a large dataset of the same image\ntypes for learning feature representations of the findings of interest. We show\nthat for a variety of findings from chest X-ray images, both proposed\narchitectures learn to estimate the ROI, as validated by clinical annotations.\nThere is a clear advantage obtained from the architecture with pre-trained\nimaging network. The centroids of the ROIs marked by this network were on\naverage at a distance equivalent to 5.1% of the image width from the centroids\nof the ground truth ROIs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 16:30:08 GMT"}], "update_date": "2018-09-09", "authors_parsed": [["Moradi", "Mehdi", ""], ["Madani", "Ali", ""], ["Gur", "Yaniv", ""], ["Guo", "Yufan", ""], ["Syeda-Mahmood", "Tanveer", ""]]}, {"id": "1809.01633", "submitter": "Jan Siebert", "authors": "Nina Hristozova, Piotr Ozimek, and Jan Paul Siebert", "title": "Efficient Egocentric Visual Perception Combining Eye-tracking, a\n  Software Retina and Deep Learning", "comments": "Accepted for: EPIC Workshop at the European Conference on Computer\n  Vision, ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ongoing work to harness biological approaches to achieving highly\nefficient egocentric perception by combining the space-variant imaging\narchitecture of the mammalian retina with Deep Learning methods. By\npre-processing images collected by means of eye-tracking glasses to control the\nfixation locations of a software retina model, we demonstrate that we can\nreduce the input to a DCNN by a factor of 3, reduce the required number of\ntraining epochs and obtain over 98% classification rates when training and\nvalidating the system on a database of over 26,000 images of 9 object classes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 17:19:07 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Hristozova", "Nina", ""], ["Ozimek", "Piotr", ""], ["Siebert", "Jan Paul", ""]]}, {"id": "1809.01649", "submitter": "Yuliang Zou", "authors": "Yuliang Zou, Zelun Luo, Jia-Bin Huang", "title": "DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task\n  Consistency", "comments": "ECCV 2018. Project website: http://yuliang.vision/DF-Net/ Code:\n  https://github.com/vt-vl-lab/DF-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised learning framework for simultaneously training\nsingle-view depth prediction and optical flow estimation models using unlabeled\nvideo sequences. Existing unsupervised methods often exploit brightness\nconstancy and spatial smoothness priors to train depth or flow models. In this\npaper, we propose to leverage geometric consistency as additional supervisory\nsignals. Our core idea is that for rigid regions we can use the predicted scene\ndepth and camera motion to synthesize 2D optical flow by backprojecting the\ninduced 3D scene flow. The discrepancy between the rigid flow (from depth\nprediction and camera motion) and the estimated flow (from optical flow model)\nallows us to impose a cross-task consistency loss. While all the networks are\njointly optimized during training, they can be applied independently at test\ntime. Extensive experiments demonstrate that our depth and flow models compare\nfavorably with state-of-the-art unsupervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 17:58:25 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Zou", "Yuliang", ""], ["Luo", "Zelun", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "1809.01687", "submitter": "Vivek Kumar Singh", "authors": "Vivek Kumar Singh, Hatem A. Rashwan, Santiago Romani, Farhan Akram,\n  Nidhi Pandey, Md. Mostafa Kamal Sarker, Adel Saleh, Meritexell Arenas, Miguel\n  Arquez, Domenec Puig, Jordina Torrents-Barrena", "title": "Breast Tumor Segmentation and Shape Classification in Mammograms using\n  Generative Adversarial and Convolutional Neural Network", "comments": "33 pages, Submitted to Expert Systems with Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammogram inspection in search of breast tumors is a tough assignment that\nradiologists must carry out frequently. Therefore, image analysis methods are\nneeded for the detection and delineation of breast masses, which portray\ncrucial morphological information that will support reliable diagnosis. In this\npaper, we proposed a conditional Generative Adversarial Network (cGAN) devised\nto segment a breast mass within a region of interest (ROI) in a mammogram. The\ngenerative network learns to recognize the breast mass area and to create the\nbinary mask that outlines the breast mass. In turn, the adversarial network\nlearns to distinguish between real (ground truth) and synthetic segmentations,\nthus enforcing the generative network to create binary masks as realistic as\npossible. The cGAN works well even when the number of training samples are\nlimited. Therefore, the proposed method outperforms several state-of-the-art\napproaches. This hypothesis is corroborated by diverse experiments performed on\ntwo datasets, the public INbreast and a private in-house dataset. The proposed\nsegmentation model provides a high Dice coefficient and Intersection over Union\n(IoU) of 94% and 87%, respectively. In addition, a shape descriptor based on a\nConvolutional Neural Network (CNN) is proposed to classify the generated masks\ninto four mass shapes: irregular, lobular, oval and round. The proposed shape\ndescriptor was trained on Digital Database for Screening Mammography (DDSM)\nyielding an overall accuracy of 80%, which outperforms the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 18:40:04 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 14:52:11 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 17:08:49 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Singh", "Vivek Kumar", ""], ["Rashwan", "Hatem A.", ""], ["Romani", "Santiago", ""], ["Akram", "Farhan", ""], ["Pandey", "Nidhi", ""], ["Sarker", "Md. Mostafa Kamal", ""], ["Saleh", "Adel", ""], ["Arenas", "Meritexell", ""], ["Arquez", "Miguel", ""], ["Puig", "Domenec", ""], ["Torrents-Barrena", "Jordina", ""]]}, {"id": "1809.01696", "submitter": "Jie Lei", "authors": "Jie Lei, Licheng Yu, Mohit Bansal, Tamara L. Berg", "title": "TVQA: Localized, Compositional Video Question Answering", "comments": "EMNLP 2018 (13 pages; Data and Leaderboard at:\n  http://tvqa.cs.unc.edu). Updated with test-public results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed an increasing interest in image-based\nquestion-answering (QA) tasks. However, due to data limitations, there has been\nmuch less work on video-based QA. In this paper, we present TVQA, a large-scale\nvideo QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs\nfrom 21,793 clips, spanning over 460 hours of video. Questions are designed to\nbe compositional in nature, requiring systems to jointly localize relevant\nmoments within a clip, comprehend subtitle-based dialogue, and recognize\nrelevant visual concepts. We provide analyses of this new dataset as well as\nseveral baselines and a multi-stream end-to-end trainable neural network\nframework for the TVQA task. The dataset is publicly available at\nhttp://tvqa.cs.unc.edu.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 19:14:11 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 21:34:05 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Lei", "Jie", ""], ["Yu", "Licheng", ""], ["Bansal", "Mohit", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1809.01701", "submitter": "Athindran Ramesh Kumar", "authors": "Athindran Ramesh Kumar, Balaraman Ravindran, Anand Raghunathan", "title": "Pack and Detect: Fast Object Detection in Videos Using\n  Region-of-Interest Packing", "comments": "Proceedings of the ACM India Joint International Conference on Data\n  Science and Management of Data. 2019", "journal-ref": null, "doi": "10.1145/3297001.3297020", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection in videos is an important task in computer vision for\nvarious applications such as object tracking, video summarization and video\nsearch. Although great progress has been made in improving the accuracy of\nobject detection in recent years due to the rise of deep neural networks, the\nstate-of-the-art algorithms are highly computationally intensive. In order to\naddress this challenge, we make two important observations in the context of\nvideos: (i) Objects often occupy only a small fraction of the area in each\nvideo frame, and (ii) There is a high likelihood of strong temporal correlation\nbetween consecutive frames. Based on these observations, we propose Pack and\nDetect (PaD), an approach to reduce the computational requirements of object\ndetection in videos. In PaD, only selected video frames called anchor frames\nare processed at full size. In the frames that lie between anchor frames\n(inter-anchor frames), regions of interest (ROIs) are identified based on the\ndetections in the previous frame. We propose an algorithm to pack the ROIs of\neach inter-anchor frame together into a reduced-size frame. The computational\nrequirements of the detector are reduced due to the lower size of the input. In\norder to maintain the accuracy of object detection, the proposed algorithm\nexpands the ROIs greedily to provide additional background around each object\nto the detector. PaD can use any underlying neural network architecture to\nprocess the full-size and reduced-size frames. Experiments using the ImageNet\nvideo object detection dataset indicate that PaD can potentially reduce the\nnumber of FLOPS required for a frame by $4\\times$. This leads to an overall\nincrease in throughput of $1.25\\times$ on a 2.1 GHz Intel Xeon server with a\nNVIDIA Titan X GPU at the cost of $1.1\\%$ drop in accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 19:29:34 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 21:31:47 GMT"}, {"version": "v3", "created": "Sat, 26 Jan 2019 02:46:43 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 04:44:14 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kumar", "Athindran Ramesh", ""], ["Ravindran", "Balaraman", ""], ["Raghunathan", "Anand", ""]]}, {"id": "1809.01726", "submitter": "Maciej P\\k{e}\\'sko", "authors": "Maciej P\\k{e}\\'sko and Tomasz Trzci\\'nski", "title": "Neural Comic Style Transfer: Case Study", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work by Gatys et al. [1] recently showed a neural style algorithm that\ncan produce an image in the style of another image. Some further works\nintroduced various improvements regarding generalization, quality and\nefficiency, but each of them was mostly focused on styles such as paintings,\nabstract images or photo-realistic style. In this paper, we present a\ncomparison of how state-of-the-art style transfer methods cope with\ntransferring various comic styles on different images. We select different\ncombinations of Adaptive Instance Normalization [11] and Universal Style\nTransfer [16] models and confront them to find their advantages and\ndisadvantages in terms of qualitative and quantitative analysis. Finally, we\npresent the results of a survey conducted on over 100 people that aims at\nvalidating the evaluation results in a real-life application of comic style\ntransfer.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 20:37:53 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 08:27:48 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["P\u0119\u015bko", "Maciej", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "1809.01791", "submitter": "Yuanwei Wu", "authors": "Wenchi Ma, Yuanwei Wu, Zongbo Wang and Guanghui Wang", "title": "MDCN: Multi-Scale, Deep Inception Convolutional Neural Networks for\n  Efficient Object Detection", "comments": null, "journal-ref": "IEEE ICPR2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection in challenging situations such as scale variation,\nocclusion, and truncation depends not only on feature details but also on\ncontextual information. Most previous networks emphasize too much on detailed\nfeature extraction through deeper and wider networks, which may enhance the\naccuracy of object detection to certain extent. However, the feature details\nare easily being changed or washed out after passing through complicated\nfiltering structures. To better handle these challenges, the paper proposes a\nnovel framework, multi-scale, deep inception convolutional neural network\n(MDCN), which focuses on wider and broader object regions by activating feature\nmaps produced in the deep part of the network. Instead of incepting inner\nlayers in the shallow part of the network, multi-scale inceptions are\nintroduced in the deep layers. The proposed framework integrates the contextual\ninformation into the learning process through a single-shot network structure.\nIt is computational efficient and avoids the hard training problem of previous\nmacro feature extraction network designed for shallow layers. Extensive\nexperiments demonstrate the effectiveness and superior performance of MDCN over\nthe state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 02:24:44 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Ma", "Wenchi", ""], ["Wu", "Yuanwei", ""], ["Wang", "Zongbo", ""], ["Wang", "Guanghui", ""]]}, {"id": "1809.01810", "submitter": "Qingxing Cao", "authors": "Qingxing Cao, Bailin Li, Xiaodan Liang and Liang Lin", "title": "Interpretable Visual Question Answering by Reasoning on Dependency Trees", "comments": "14 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1804.00105", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative reasoning for understanding image-question pairs is a very\ncritical but underexplored topic in interpretable visual question answering\nsystems. Although very recent studies have attempted to use explicit\ncompositional processes to assemble multiple subtasks embedded in questions,\ntheir models heavily rely on annotations or handcrafted rules to obtain valid\nreasoning processes, which leads to either heavy workloads or poor performance\non compositional reasoning. In this paper, to better align image and language\ndomains in diverse and unrestricted cases, we propose a novel neural network\nmodel that performs global reasoning on a dependency tree parsed from the\nquestion; thus, our model is called a parse-tree-guided reasoning network\n(PTGRN). This network consists of three collaborative modules: i) an attention\nmodule that exploits the local visual evidence of each word parsed from the\nquestion, ii) a gated residual composition module that composes the previously\nmined evidence, and iii) a parse-tree-guided propagation module that passes the\nmined evidence along the parse tree. Thus, PTGRN is capable of building an\ninterpretable visual question answering (VQA) system that gradually derives\nimage cues following question-driven parse-tree reasoning. Experiments on\nrelational datasets demonstrate the superiority of PTGRN over current\nstate-of-the-art VQA methods, and the visualization results highlight the\nexplainable capability of our reasoning system.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 04:09:28 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 04:32:51 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Cao", "Qingxing", ""], ["Li", "Bailin", ""], ["Liang", "Xiaodan", ""], ["Lin", "Liang", ""]]}, {"id": "1809.01816", "submitter": "Marcus Rohrbach", "authors": "Satwik Kottur, Jos\\'e M. F. Moura, Devi Parikh, Dhruv Batra, Marcus\n  Rohrbach", "title": "Visual Coreference Resolution in Visual Dialog using Neural Module\n  Networks", "comments": "ECCV 2018 + results on VisDial v1.0 dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dialog entails answering a series of questions grounded in an image,\nusing dialog history as context. In addition to the challenges found in visual\nquestion answering (VQA), which can be seen as one-round dialog, visual dialog\nencompasses several more. We focus on one such problem called visual\ncoreference resolution that involves determining which words, typically noun\nphrases and pronouns, co-refer to the same entity/object instance in an image.\nThis is crucial, especially for pronouns (e.g., `it'), as the dialog agent must\nfirst link it to a previous coreference (e.g., `boat'), and only then can rely\non the visual grounding of the coreference `boat' to reason about the pronoun\n`it'. Prior work (in visual dialog) models visual coreference resolution either\n(a) implicitly via a memory network over history, or (b) at a coarse level for\nthe entire question; and not explicitly at a phrase level of granularity. In\nthis work, we propose a neural module network architecture for visual dialog by\nintroducing two novel modules - Refer and Exclude - that perform explicit,\ngrounded, coreference resolution at a finer word level. We demonstrate the\neffectiveness of our model on MNIST Dialog, a visually simple yet\ncoreference-wise complex dataset, by achieving near perfect accuracy, and on\nVisDial, a large and challenging visual dialog dataset on real images, where\nour model outperforms other approaches, and is more interpretable, grounded,\nand consistent qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 04:36:22 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Kottur", "Satwik", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1809.01822", "submitter": "Dooseop Choi Dr", "authors": "Dooseop Choi and Taeg-Hyun An and Kyounghwan Ahn and Jeongdan Choi", "title": "Driving Experience Transfer Method for End-to-End Control of\n  Self-Driving Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a transfer learning method for the end-to-end\ncontrol of self-driving cars, which enables a convolutional neural network\n(CNN) trained on a source domain to be utilized for the same task in a\ndifferent target domain. A conventional CNN for the end-to-end control is\ndesigned to map a single front-facing camera image to a steering command. To\nenable the transfer learning, we let the CNN produce not only a steering\ncommand but also a lane departure level (LDL) by adding a new task module,\nwhich takes the output of the last convolutional layer as input. The CNN\ntrained on the source domain, called source network, is then utilized to train\nanother task module called target network, which also takes the output of the\nlast convolutional layer of the source network and is trained to produce a\nsteering command for the target domain. The steering commands from the source\nand target network are finally merged according to the LDL and the merged\ncommand is utilized for controlling a car in the target domain. To demonstrate\nthe effectiveness of the proposed method, we utilized two simulators, TORCS and\nGTAV, for the source and the target domains, respectively. Experimental results\nshow that the proposed method outperforms other baseline methods in terms of\nstable and safe control of cars.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 05:00:10 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 06:03:50 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Choi", "Dooseop", ""], ["An", "Taeg-Hyun", ""], ["Ahn", "Kyounghwan", ""], ["Choi", "Jeongdan", ""]]}, {"id": "1809.01826", "submitter": "Ding Liu", "authors": "Ding Liu, Bihan Wen, Jianbo Jiao, Xianming Liu, Zhangyang Wang, Thomas\n  S. Huang", "title": "Connecting Image Denoising and High-Level Vision Tasks via Deep Learning", "comments": "arXiv admin note: text overlap with arXiv:1706.04284", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising and high-level vision tasks are usually handled independently\nin the conventional practice of computer vision, and their connection is\nfragile. In this paper, we cope with the two jointly and explore the mutual\ninfluence between them with the focus on two questions, namely (1) how image\ndenoising can help improving high-level vision tasks, and (2) how the semantic\ninformation from high-level vision tasks can be used to guide image denoising.\nFirst for image denoising we propose a convolutional neural network in which\nconvolutions are conducted in various spatial resolutions via downsampling and\nupsampling operations in order to fuse and exploit contextual information on\ndifferent scales. Second we propose a deep neural network solution that\ncascades two modules for image denoising and various high-level tasks,\nrespectively, and use the joint loss for updating only the denoising network\nvia back-propagation. We experimentally show that on one hand, the proposed\ndenoiser has the generality to overcome the performance degradation of\ndifferent high-level vision tasks. On the other hand, with the guidance of\nhigh-level vision information, the denoising network produces more visually\nappealing results. Extensive experiments demonstrate the benefit of exploiting\nimage semantics simultaneously for image denoising and high-level vision tasks\nvia deep learning. The code is available online:\nhttps://github.com/Ding-Liu/DeepDenoising\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 05:13:22 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Liu", "Ding", ""], ["Wen", "Bihan", ""], ["Jiao", "Jianbo", ""], ["Liu", "Xianming", ""], ["Wang", "Zhangyang", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1809.01844", "submitter": "Junnan Li Mr", "authors": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli", "title": "Unsupervised Learning of View-invariant Action Representations", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success in human action recognition with deep learning methods\nmostly adopt the supervised learning paradigm, which requires significant\namount of manually labeled data to achieve good performance. However, label\ncollection is an expensive and time-consuming process. In this work, we propose\nan unsupervised learning framework, which exploits unlabeled data to learn\nvideo representations. Different from previous works in video representation\nlearning, our unsupervised learning task is to predict 3D motion in multiple\ntarget views using video representation from a source view. By learning to\nextrapolate cross-view motions, the representation can capture view-invariant\nmotion dynamics which is discriminative for the action. In addition, we propose\na view-adversarial training method to enhance learning of view-invariant\nfeatures. We demonstrate the effectiveness of the learned representations for\naction recognition on multiple datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 06:41:03 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Li", "Junnan", ""], ["Wong", "Yongkang", ""], ["Zhao", "Qi", ""], ["Kankanhalli", "Mohan S.", ""]]}, {"id": "1809.01890", "submitter": "Koichi Hamada", "authors": "Koichi Hamada, Kentaro Tachibana, Tianqi Li, Hiroto Honda, Yusuke\n  Uchida", "title": "Full-body High-resolution Anime Generation with Progressive\n  Structure-conditional Generative Adversarial Networks", "comments": "Accepted to ECCV 2018 Workshop: Computer Vision for Fashion, Art and\n  Design. Project page is at https://dena.com/intl/anime-generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Progressive Structure-conditional Generative Adversarial Networks\n(PSGAN), a new framework that can generate full-body and high-resolution\ncharacter images based on structural information. Recent progress in generative\nadversarial networks with progressive training has made it possible to generate\nhigh-resolution images. However, existing approaches have limitations in\nachieving both high image quality and structural consistency at the same time.\nOur method tackles the limitations by progressively increasing the resolution\nof both generated images and structural conditions during training. In this\npaper, we empirically demonstrate the effectiveness of this method by showing\nthe comparison with existing approaches and video generation results of diverse\nanime characters at 1024x1024 based on target pose sequences. We also create a\nnovel dataset containing full-body 1024x1024 high-resolution images and exact\n2D pose keypoints using Unity 3D Avatar models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 09:09:40 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Hamada", "Koichi", ""], ["Tachibana", "Kentaro", ""], ["Li", "Tianqi", ""], ["Honda", "Hiroto", ""], ["Uchida", "Yusuke", ""]]}, {"id": "1809.01924", "submitter": "Guillaume Zahnd", "authors": "Guillaume Zahnd, Kozue Saito, Kazuyuki Nagatsuka, Yoshito Otake, and\n  Yoshinobu Sato", "title": "Dynamic Block Matching to assess the longitudinal component of the dense\n  motion field of the carotid artery wall in B-mode ultrasound sequences --\n  Association with coronary artery disease", "comments": null, "journal-ref": null, "doi": "10.1002/mp.13186", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The motion of the common carotid artery tissue layers along the\nvessel axis during the cardiac cycle, observed in ultrasound imaging, is\nassociated with the presence of established cardiovascular risk factors.\nHowever, the vast majority of the methods are based on the tracking of a single\npoint, thus failing to capture the overall motion of the entire arterial wall.\nThe aim of this work is to introduce a motion tracking framework able to\nsimultaneously extract the trajectory of a large collection of points spanning\nthe entire exploitable width of the image.\n  Method: The longitudinal motion, which is the main focus of the present work,\nis determined in two steps. First, a series of independent block matching\noperations are carried out for all the tracked points. Then, an original\ndynamic-programming approach is exploited to regularize the collection of\nsimilarity maps and estimate the globally optimal motion over the entire vessel\nwall. Sixty-two atherosclerotic participants at high cardiovascular risk were\ninvolved in this study.\n  Results: A dense displacement field, describing the longitudinal motion of\nthe carotid far wall over time, was extracted. For each cine-loop, the method\nwas evaluated against manual reference tracings performed on three local\npoints, with an average absolute error of 150+/-163 um. A strong correlation\nwas found between motion inhomogeneity and the presence of coronary artery\ndisease (beta-coefficient=0.586, p=0.003).\n  Conclusions: To the best of our knowledge, this is the first time that a\nmethod is specifically proposed to assess the dense motion field of the carotid\nfar wall. This approach has potential to evaluate the (in)homogeneity of the\nwall dynamics. The proposed method has promising performances to improve the\nanalysis of arterial longitudinal motion and the understanding of the\nunderlying patho-physiological parameters.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 11:24:45 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 20:20:22 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 20:37:17 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zahnd", "Guillaume", ""], ["Saito", "Kozue", ""], ["Nagatsuka", "Kazuyuki", ""], ["Otake", "Yoshito", ""], ["Sato", "Yoshinobu", ""]]}, {"id": "1809.01936", "submitter": "Xiang Wu", "authors": "Xiang Wu, Huaibo Huang, Vishal M. Patel, Ran He, Zhenan Sun", "title": "Disentangled Variational Representation for Heterogeneous Face\n  Recognition", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible (VIS) to near infrared (NIR) face matching is a challenging problem\ndue to the significant domain discrepancy between the domains and a lack of\nsufficient data for training cross-modal matching algorithms. Existing\napproaches attempt to tackle this problem by either synthesizing visible faces\nfrom NIR faces, extracting domain-invariant features from these modalities, or\nprojecting heterogeneous data onto a common latent space for cross-modal\nmatching. In this paper, we take a different approach in which we make use of\nthe Disentangled Variational Representation (DVR) for cross-modal matching.\nFirst, we model a face representation with an intrinsic identity information\nand its within-person variations. By exploring the disentangled latent variable\nspace, a variational lower bound is employed to optimize the approximate\nposterior for NIR and VIS representations. Second, aiming at obtaining more\ncompact and discriminative disentangled latent space, we impose a minimization\nof the identity information for the same subject and a relaxed correlation\nalignment constraint between the NIR and VIS modality variations. An\nalternative optimization scheme is proposed for the disentangled variational\nrepresentation part and the heterogeneous face recognition network part. The\nmutual promotion between these two parts effectively reduces the NIR and VIS\ndomain discrepancy and alleviates over-fitting. Extensive experiments on three\nchallenging NIR-VIS heterogeneous face recognition databases demonstrate that\nthe proposed method achieves significant improvements over the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 12:15:59 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 00:58:18 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 07:25:42 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Wu", "Xiang", ""], ["Huang", "Huaibo", ""], ["Patel", "Vishal M.", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1809.01943", "submitter": "Jiaming Xu", "authors": "Yiqun Yao, Jiaming Xu, Feng Wang, Bo Xu", "title": "Cascaded Mutual Modulation for Visual Reasoning", "comments": "to appear in EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual reasoning is a special visual question answering problem that is\nmulti-step and compositional by nature, and also requires intensive text-vision\ninteractions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end\nvisual reasoning model. CMM includes a multi-step comprehension process for\nboth question and image. In each step, we use a Feature-wise Linear Modulation\n(FiLM) technique to enable textual/visual pipeline to mutually control each\nother. Experiments show that CMM significantly outperforms most related models,\nand reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR,\ncollected from both synthetic and natural languages. Ablation studies confirm\nthat both our multistep framework and our visual-guided language modulation are\ncritical to the task. Our code is available at\nhttps://github.com/FlamingHorizon/CMM-VR.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 12:26:24 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Yao", "Yiqun", ""], ["Xu", "Jiaming", ""], ["Wang", "Feng", ""], ["Xu", "Bo", ""]]}, {"id": "1809.01986", "submitter": "Vijay Anand", "authors": "Vijay Anand and Vivek kanhangad", "title": "Pore detection in high-resolution fingerprint images using Deep Residual\n  Network", "comments": "9 pages, 1 figure, 4 Tables", "journal-ref": null, "doi": "10.1117/1.JEI.28.2.020502", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents a residual learning-based convolutional neural network,\nreferred to as DeepResPore, for detection of pores in high-resolution\nfingerprint images. Specifically, the proposed DeepResPore model generates a\npore intensity map from the input fingerprint image. Subsequently, the local\nmaxima filter is operated on the pore intensity map to identify the pore\ncoordinates. The results of our experiments indicate that the proposed approach\nis effective in extracting pores with a true detection rate of 94:49% on Test\nset I and 93:78% on Test set II of the publicly available PolyU HRF dataset.\nMost importantly, the proposed approach achieves state-of-the-art performance\non both test sets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 13:38:16 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 09:41:17 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Anand", "Vijay", ""], ["kanhangad", "Vivek", ""]]}, {"id": "1809.01990", "submitter": "Jun Beom Kho", "authors": "Jun Beom Kho", "title": "Multi-Expert Gender Classification on Age Group by Integrating Deep\n  Neural Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally, facial age variations affect gender classification accuracy\nsignificantly, because facial shape and skin texture change as they grow old.\nThis requires re-examination on the gender classification system to consider\nfacial age information. In this paper, we propose Multi-expert Gender\nClassification on Age Group (MGA), an end-to-end multi-task learning schemes of\nage estimation and gender classification. First, two types of deep neural\nnetworks are utilized; Convolutional Appearance Network (CAN) for facial\nappearance feature and Deep Geometry Network (DGN) for facial geometric\nfeature. Then, CAN and DGN are integrated by the proposed model integration\nstrategy and fine-tuned in order to improve age and gender classification\naccuracy. The facial images are categorized into one of three age groups\n(young, adult and elder group) based on their estimated age, and the system\nmakes a gender prediction according to average fusion strategy of three gender\nclassification experts, which are trained to fit gender characteristics of each\nage group. Rigorous experimental results conducted on the challenging databases\nsuggest that the proposed MGA outperforms several state-of-art researches with\nsmaller computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 13:47:19 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 15:05:03 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Kho", "Jun Beom", ""]]}, {"id": "1809.01995", "submitter": "Natalia Neverova", "authors": "Natalia Neverova, Riza Alp Guler, Iasonas Kokkinos", "title": "Dense Pose Transfer", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we integrate ideas from surface-based modeling with neural\nsynthesis: we propose a combination of surface-based pose estimation and deep\ngenerative models that allows us to perform accurate pose transfer, i.e.\nsynthesize a new image of a person based on a single image of that person and\nthe image of a pose donor. We use a dense pose estimation system that maps\npixels from both images to a common surface-based coordinate system, allowing\nthe two images to be brought in correspondence with each other. We inpaint and\nrefine the source image intensities in the surface coordinate system, prior to\nwarping them onto the target pose. These predictions are fused with those of a\nconvolutional predictive module through a neural synthesis module allowing for\ntraining the whole pipeline jointly end-to-end, optimizing a combination of\nadversarial and perceptual losses. We show that dense pose estimation is a\nsubstantially more powerful conditioning input than landmark-, or mask-based\nalternatives, and report systematic improvements over state of the art\ngenerators on DeepFashion and MVC datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 13:53:00 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Neverova", "Natalia", ""], ["Guler", "Riza Alp", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1809.02002", "submitter": "Olivia Wiles", "authors": "Olivia Wiles and Andrew Zisserman", "title": "3D Surface Reconstruction by Pointillism", "comments": "ECCV workshop on Geometry meets Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to infer the 3D shape of an object from a\nsingle image. We use sculptures as our training and test bed, as these have\ngreat variety in shape and appearance.\n  To achieve this we build on the success of multiple view geometry (MVG) which\nis able to accurately provide correspondences between images of 3D objects\nunder varying viewpoint and illumination conditions, and make the following\ncontributions: first, we introduce a new loss function that can harness\nimage-to-image correspondences to provide a supervisory signal to train a deep\nnetwork to infer a depth map. The network is trained end-to-end by\ndifferentiating through the camera. Second, we develop a processing pipeline to\nautomatically generate a large scale multi-view set of correspondences for\ntraining the network. Finally, we demonstrate that we can indeed obtain a depth\nmap of a novel object from a single image for a variety of sculptures with\nvarying shape/texture, and that the network generalises at test time to new\ndomains (e.g. synthetic images).\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 14:11:50 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 06:56:42 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Wiles", "Olivia", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1809.02042", "submitter": "Jekan Thangavelautham", "authors": "Steve Morad and Ravi Teja Nallapu and Himangshu Kalita and Byon Kwon\n  and Vishnu Reddy and Roberto Furfaro and Erik Asphaug and Jekan\n  Thangavelautham", "title": "On-Orbit Smart Camera System to Observe Illuminated and Unilluminated\n  Space Objects", "comments": "12 pages, 11 figures, appears at Advanced Maui Optical and Space\n  Surveillance Technologies Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide availability of Commercial Off-The-Shelf (COTS) electronics that can\nwithstand Low Earth Orbit conditions has opened avenue for wide deployment of\nCubeSats and small-satellites. CubeSats thanks to their low developmental and\nlaunch costs offer new opportunities for rapidly demonstrating on-orbit\nsurveillance capabilities. In our earlier work, we proposed development of\nSWIMSat (Space based Wide-angle Imaging of Meteors) a 3U CubeSat demonstrator\nthat is designed to observe illuminated objects entering the Earth's\natmosphere. The spacecraft would operate autonomously using a smart camera with\nvision algorithms to detect, track and report of objects. Several CubeSats can\ntrack an object in a coordinated fashion to pinpoint an object's trajectory. An\nextension of this smart camera capability is to track unilluminated objects\nutilizing capabilities we have been developing to track and navigate to Near\nEarth Objects (NEOs). This extension enables detecting and tracking objects\nthat can't readily be detected by humans. The system maintains a dense star map\nof the night sky and performs round the clock observations. Standard optical\nflow algorithms are used to obtain trajectories of all moving objects in the\ncamera field of view. Through a process of elimination, certain stars maybe\noccluded by a transiting unilluminated object which is then used to first\ndetect and obtain a trajectory of the object. Using multiple cameras observing\nthe event from different points of view, it may be possible then to triangulate\nthe position of the object in space and obtain its orbital trajectory. In this\nwork, the performance of our space object detection algorithm coupled with a\nspacecraft guidance, navigation, and control system is demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 15:22:42 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Morad", "Steve", ""], ["Nallapu", "Ravi Teja", ""], ["Kalita", "Himangshu", ""], ["Kwon", "Byon", ""], ["Reddy", "Vishnu", ""], ["Furfaro", "Roberto", ""], ["Asphaug", "Erik", ""], ["Thangavelautham", "Jekan", ""]]}, {"id": "1809.02043", "submitter": "Xinxin Liu", "authors": "Xinxin Liu, Xiliang Lu, Huanfeng Shen, Qiangqiang Yuan, Liangpei Zhang", "title": "Oblique Stripe Removal in Remote Sensing Images via Oriented Variation", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Destriping is a classical problem in remote sensing image processing.\nAlthough considerable effort has been made to remove stripes, few of the\nexisting methods can eliminate stripe noise with arbitrary orientations. This\nsituation makes the removal of oblique stripes in the higher-level remote\nsensing products become an unfinished and urgent issue. To overcome the\nchallenging problem, we propose a novel destriping model which is self-adjusted\nto different orientations of stripe noise. First of all, the oriented variation\nmodel is designed to accomplish the stripe orientation approximation. In this\nmodel, the stripe direction is automatically estimated and then imbedded into\nthe constraint term to depict the along-stripe smoothness of the stripe\ncomponent. Mainly based on the oriented variation model, a whole destriping\nframework is proposed by jointly employing an L1-norm constraint and a TV\nregularization to separately capture the global distribution property of stripe\ncomponent and the piecewise smoothness of the clean image. The qualitative and\nquantitative experimental results of both orientation and destriping aspects\nconfirm the effectiveness and stability of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 15:24:52 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Liu", "Xinxin", ""], ["Lu", "Xiliang", ""], ["Shen", "Huanfeng", ""], ["Yuan", "Qiangqiang", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1809.02057", "submitter": "Jeong JOon Park", "authors": "Jeong Joon Park, Richard Newcombe, Steve Seitz", "title": "Surface Light Field Fusion", "comments": "Project Website: http://grail.cs.washington.edu/projects/slfusion/", "journal-ref": "3DV 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for interactively scanning highly reflective objects\nwith a commodity RGBD sensor. In addition to shape, our approach models the\nsurface light field, encoding scene appearance from all directions. By\nfactoring the surface light field into view-independent and\nwavelength-independent components, we arrive at a representation that can be\nrobustly estimated with IR-equipped commodity depth sensors, and achieves high\nquality results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 15:45:05 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Park", "Jeong Joon", ""], ["Newcombe", "Richard", ""], ["Seitz", "Steve", ""]]}, {"id": "1809.02058", "submitter": "Chenshen Wu", "authors": "Chenshen Wu, Luis Herranz, Xialei Liu, Yaxing Wang, Joost van de\n  Weijer, Bogdan Raducanu", "title": "Memory Replay GANs: learning to generate images from new categories\n  without forgetting", "comments": "Appear in NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on sequential learning address the problem of forgetting in\ndiscriminative models. In this paper we consider the case of generative models.\nIn particular, we investigate generative adversarial networks (GANs) in the\ntask of learning new categories in a sequential fashion. We first show that\nsequential fine tuning renders the network unable to properly generate images\nfrom previous categories (i.e. forgetting). Addressing this problem, we propose\nMemory Replay GANs (MeRGANs), a conditional GAN framework that integrates a\nmemory replay generator. We study two methods to prevent forgetting by\nleveraging these replays, namely joint training with replay and replay\nalignment. Qualitative and quantitative experimental results in MNIST, SVHN and\nLSUN datasets show that our memory replay approach can generate competitive\nimages while significantly mitigating the forgetting of previous categories.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 15:45:36 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 14:41:53 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 09:59:38 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Wu", "Chenshen", ""], ["Herranz", "Luis", ""], ["Liu", "Xialei", ""], ["Wang", "Yaxing", ""], ["van de Weijer", "Joost", ""], ["Raducanu", "Bogdan", ""]]}, {"id": "1809.02073", "submitter": "Hui-Lee Ooi", "authors": "Hui-Lee Ooi, Guillaume-Alexandre Bilodeau, Nicolas Saunier, and\n  David-Alexandre Beaupr\\'e", "title": "Multiple Object Tracking in Urban Traffic Scenes with a Multiclass\n  Object Detector", "comments": "13th International Symposium on Visual Computing (ISVC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple object tracking (MOT) in urban traffic aims to produce the\ntrajectories of the different road users that move across the field of view\nwith different directions and speeds and that can have varying appearances and\nsizes. Occlusions and interactions among the different objects are expected and\ncommon due to the nature of urban road traffic. In this work, a tracking\nframework employing classification label information from a deep learning\ndetection approach is used for associating the different objects, in addition\nto object position and appearances. We want to investigate the performance of a\nmodern multiclass object detector for the MOT task in traffic scenes. Results\nshow that the object labels improve tracking performance, but that the output\nof object detectors are not always reliable.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 16:17:10 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Ooi", "Hui-Lee", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""], ["Beaupr\u00e9", "David-Alexandre", ""]]}, {"id": "1809.02104", "submitter": "Tom Goldstein", "authors": "Ali Shafahi, W. Ronny Huang, Christoph Studer, Soheil Feizi, Tom\n  Goldstein", "title": "Are adversarial examples inevitable?", "comments": null, "journal-ref": "International Conference on Learning Representations, 2019.\n  https://openreview.net/forum?id=r1lWUoA9FQ", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of defenses have been proposed to harden neural networks against\nadversarial attacks. However, a pattern has emerged in which the majority of\nadversarial defenses are quickly broken by new attacks. Given the lack of\nsuccess at generating robust defenses, we are led to ask a fundamental\nquestion: Are adversarial attacks inevitable? This paper analyzes adversarial\nexamples from a theoretical perspective, and identifies fundamental bounds on\nthe susceptibility of a classifier to adversarial attacks. We show that, for\ncertain classes of problems, adversarial examples are inescapable. Using\nexperiments, we explore the implications of theoretical guarantees for\nreal-world problems and discuss how factors such as dimensionality and image\ncomplexity limit a classifier's robustness against adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 17:26:58 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 19:34:04 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 21:18:27 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Shafahi", "Ali", ""], ["Huang", "W. Ronny", ""], ["Studer", "Christoph", ""], ["Feizi", "Soheil", ""], ["Goldstein", "Tom", ""]]}, {"id": "1809.02108", "submitter": "Joon Son Chung", "authors": "Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals,\n  Andrew Zisserman", "title": "Deep Audio-Visual Speech Recognition", "comments": "Accepted for publication by IEEE Transactions on Pattern Analysis and\n  Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2889052", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to recognise phrases and sentences being spoken by a\ntalking face, with or without the audio. Unlike previous works that have\nfocussed on recognising a limited number of words or phrases, we tackle lip\nreading as an open-world problem - unconstrained natural language sentences,\nand in the wild videos. Our key contributions are: (1) we compare two models\nfor lip reading, one using a CTC loss, and the other using a\nsequence-to-sequence loss. Both models are built on top of the transformer\nself-attention architecture; (2) we investigate to what extent lip reading is\ncomplementary to audio speech recognition, especially when the audio signal is\nnoisy; (3) we introduce and publicly release a new dataset for audio-visual\nspeech recognition, LRS2-BBC, consisting of thousands of natural sentences from\nBritish television. The models that we train surpass the performance of all\nprevious work on a lip reading benchmark dataset by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 17:34:27 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 06:14:27 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Afouras", "Triantafyllos", ""], ["Chung", "Joon Son", ""], ["Senior", "Andrew", ""], ["Vinyals", "Oriol", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1809.02110", "submitter": "Daan de Geus", "authors": "Daan de Geus, Panagiotis Meletis, Gijs Dubbelman", "title": "Panoptic Segmentation with a Joint Semantic and Instance Segmentation\n  Network", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a single network method for panoptic segmentation. This method\ncombines the predictions from a jointly trained semantic and instance\nsegmentation network using heuristics. Joint training is the first step towards\nan end-to-end panoptic segmentation network and is faster and more memory\nefficient than training and predicting with two networks, as done in previous\nwork. The architecture consists of a ResNet-50 feature extractor shared by the\nsemantic segmentation and instance segmentation branch. For instance\nsegmentation, a Mask R-CNN type of architecture is used, while the semantic\nsegmentation branch is augmented with a Pyramid Pooling Module. Results for\nthis method are submitted to the COCO and Mapillary Joint Recognition Challenge\n2018. Our approach achieves a PQ score of 17.6 on the Mapillary Vistas\nvalidation set and 27.2 on the COCO test-dev set.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 17:35:39 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 16:10:41 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["de Geus", "Daan", ""], ["Meletis", "Panagiotis", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1809.02123", "submitter": "Carlos Esteves", "authors": "Carlos Esteves, Kostas Daniilidis, Ameesh Makadia", "title": "Labeling Panoramas with Spherical Hourglass Networks", "comments": "Accepted to the 360{\\deg} Perception and Interaction Workshop at ECCV\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent proliferation of consumer-grade 360{\\deg} cameras, it is\nworth revisiting visual perception challenges with spherical cameras given the\npotential benefit of their global field of view. To this end we introduce a\nspherical convolutional hourglass network (SCHN) for the dense labeling on the\nsphere. The SCHN is invariant to camera orientation (lifting the usual\nrequirement for `upright' panoramic images), and its design is scalable for\nlarger practical datasets. Initial experiments show promising results on a\nspherical semantic segmentation task.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 17:55:11 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Esteves", "Carlos", ""], ["Daniilidis", "Kostas", ""], ["Makadia", "Ameesh", ""]]}, {"id": "1809.02129", "submitter": "Safa Messaoud", "authors": "Safa Messaoud, David Forsyth, Alexander G. Schwing", "title": "Structural Consistency and Controllability for Diverse Colorization", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorizing a given gray-level image is an important task in the media and\nadvertising industry. Due to the ambiguity inherent to colorization (many\nshades are often plausible), recent approaches started to explicitly model\ndiversity. However, one of the most obvious artifacts, structural\ninconsistency, is rarely considered by existing methods which predict\nchrominance independently for every pixel. To address this issue, we develop a\nconditional random field based variational auto-encoder formulation which is\nable to achieve diversity while taking into account structural consistency.\nMoreover, we introduce a controllability mecha- nism that can incorporate\nexternal constraints from diverse sources in- cluding a user interface.\nCompared to existing baselines, we demonstrate that our method obtains more\ndiverse and globally consistent coloriza- tions on the LFW, LSUN-Church and\nILSVRC-2015 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 17:59:57 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Messaoud", "Safa", ""], ["Forsyth", "David", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "1809.02156", "submitter": "Lisa Anne Hendricks", "authors": "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, Kate\n  Saenko", "title": "Object Hallucination in Image Captioning", "comments": "Rohrbach and Hendricks contributed equally; accepted to EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite continuously improving performance, contemporary image captioning\nmodels are prone to \"hallucinating\" objects that are not actually in a scene.\nOne problem is that standard metrics only measure similarity to ground truth\ncaptions and may not fully capture image relevance. In this work, we propose a\nnew image relevance metric to evaluate current models with veridical visual\nlabels and assess their rate of object hallucination. We analyze how captioning\nmodel architectures and learning objectives contribute to object hallucination,\nexplore when hallucination is likely due to image misclassification or language\npriors, and assess how well current sentence metrics capture object\nhallucination. We investigate these questions on the standard image captioning\nbenchmark, MSCOCO, using a diverse set of models. Our analysis yields several\ninteresting findings, including that models which score best on standard\nsentence metrics do not always have lower hallucination and that models which\nhallucinate more tend to make errors driven by language priors.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 18:25:18 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 23:48:52 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Rohrbach", "Anna", ""], ["Hendricks", "Lisa Anne", ""], ["Burns", "Kaylee", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1809.02165", "submitter": "Li Liu", "authors": "Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang\n  Liu, Matti Pietik\\\"ainen", "title": "Deep Learning for Generic Object Detection: A Survey", "comments": "IJCV Minor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection, one of the most fundamental and challenging problems in\ncomputer vision, seeks to locate object instances from a large number of\npredefined categories in natural images. Deep learning techniques have emerged\nas a powerful strategy for learning feature representations directly from data\nand have led to remarkable breakthroughs in the field of generic object\ndetection. Given this period of rapid evolution, the goal of this paper is to\nprovide a comprehensive survey of the recent achievements in this field brought\nabout by deep learning techniques. More than 300 research contributions are\nincluded in this survey, covering many aspects of generic object detection:\ndetection frameworks, object feature representation, object proposal\ngeneration, context modeling, training strategies, and evaluation metrics. We\nfinish the survey by identifying promising directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 18:42:04 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 03:31:35 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 02:07:57 GMT"}, {"version": "v4", "created": "Thu, 22 Aug 2019 04:32:38 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Liu", "Li", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""], ["Fieguth", "Paul", ""], ["Chen", "Jie", ""], ["Liu", "Xinwang", ""], ["Pietik\u00e4inen", "Matti", ""]]}, {"id": "1809.02169", "submitter": "Mohsan Alvi", "authors": "Mohsan Alvi, Andrew Zisserman, Christoffer Nellaker", "title": "Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep\n  Neural Network Embeddings", "comments": "Will appear in Workshop on Bias Estimation in Face Analytics, ECCV\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks achieve the state-of-the-art in image classification tasks.\nHowever, they can encode spurious variations or biases that may be present in\nthe training data. For example, training an age predictor on a dataset that is\nnot balanced for gender can lead to gender biased predicitons (e.g. wrongly\npredicting that males are older if only elderly males are in the training set).\nWe present two distinct contributions: 1) An algorithm that can remove multiple\nsources of variation from the feature representation of a network. We\ndemonstrate that this algorithm can be used to remove biases from the feature\nrepresentation, and thereby improve classification accuracies, when training\nnetworks on extremely biased datasets. 2) An ancestral origin database of\n14,000 images of individuals from East Asia, the Indian subcontinent,\nsub-Saharan Africa, and Western Europe. We demonstrate on this dataset, for a\nnumber of facial attribute classification tasks, that we are able to remove\nracial biases from the network feature representation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 18:44:56 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 12:07:54 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Alvi", "Mohsan", ""], ["Zisserman", "Andrew", ""], ["Nellaker", "Christoffer", ""]]}, {"id": "1809.02176", "submitter": "Zhangjie Cao", "authors": "Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang", "title": "Multi-Adversarial Domain Adaptation", "comments": "AAAI 2018 Oral. arXiv admin note: substantial text overlap with\n  arXiv:1705.10667, arXiv:1707.07901", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep domain adaptation reveal that adversarial learning\ncan be embedded into deep networks to learn transferable features that reduce\ndistribution discrepancy between the source and target domains. Existing domain\nadversarial adaptation methods based on single domain discriminator only align\nthe source and target data distributions without exploiting the complex\nmultimode structures. In this paper, we present a multi-adversarial domain\nadaptation (MADA) approach, which captures multimode structures to enable\nfine-grained alignment of different data distributions based on multiple domain\ndiscriminators. The adaptation can be achieved by stochastic gradient descent\nwith the gradients computed by back-propagation in linear-time. Empirical\nevidence demonstrates that the proposed model outperforms state of the art\nmethods on standard domain adaptation datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 20:54:48 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Pei", "Zhongyi", ""], ["Cao", "Zhangjie", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""]]}, {"id": "1809.02220", "submitter": "Xin Chen", "authors": "Chuhan Min and Aosen Wang and Yiran Chen and Wenyao Xu and Xin Chen", "title": "2PFPCE: Two-Phase Filter Pruning Based on Conditional Entropy", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks~(CNNs) offer remarkable performance of\nclassifications and regressions in many high-dimensional problems and have been\nwidely utilized in real-word cognitive applications. However, high\ncomputational cost of CNNs greatly hinder their deployment in\nresource-constrained applications, real-time systems and edge computing\nplatforms. To overcome this challenge, we propose a novel filter-pruning\nframework, two-phase filter pruning based on conditional entropy, namely\n\\textit{2PFPCE}, to compress the CNN models and reduce the inference time with\nmarginal performance degradation. In our proposed method, we formulate filter\npruning process as an optimization problem and propose a novel filter selection\ncriteria measured by conditional entropy. Based on the assumption that the\nrepresentation of neurons shall be evenly distributed, we also develop a\nmaximum-entropy filter freeze technique that can reduce over fitting. Two\nfilter pruning strategies -- global and layer-wise strategies, are compared.\nOur experiment result shows that combining these two strategies can achieve a\nhigher neural network compression ratio than applying only one of them under\nthe same accuracy drop threshold. Two-phase pruning, that is, combining both\nglobal and layer-wise strategies, achieves 10 X FLOPs reduction and 46%\ninference time reduction on VGG-16, with 2% accuracy drop.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 21:13:00 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Min", "Chuhan", ""], ["Wang", "Aosen", ""], ["Chen", "Yiran", ""], ["Xu", "Wenyao", ""], ["Chen", "Xin", ""]]}, {"id": "1809.02226", "submitter": "Vedrana Andersen Dahl", "authors": "Vedrana Andersen Dahl, Monica Jane Emerson, Camilla Himmelstrup\n  Trinderup and Anders Bjorholm Dahl", "title": "Content-based Propagation of User Markings for Interactive Segmentation\n  of Patterned Images", "comments": "9 pages, 7 figures, PDFLaTeX", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, June 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and easy segmentation of images and volumes is of great practical\nimportance. Segmentation problems that motivate our approach originate from\nmicroscopy imaging commonly used in materials science, medicine, and biology.\nWe formulate image segmentation as a probabilistic pixel classification\nproblem, and we apply segmentation as a step towards characterising image\ncontent. Our method allows the user to define structures of interest by\ninteractively marking a subset of pixels. Thanks to the real-time feedback, the\nuser can place new markings strategically, depending on the current outcome.\nThe final pixel classification may be obtained from a very modest user input.\nAn important ingredient of our method is a graph that encodes image content.\nThis graph is built in an unsupervised manner during initialisation and is\nbased on clustering of image features. Since we combine a limited amount of\nuser-labelled data with the clustering information obtained from the unlabelled\nparts of the image, our method fits in the general framework of semi-supervised\nlearning. We demonstrate how this can be a very efficient approach to\nsegmentation through pixel classification.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 21:31:48 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 15:30:27 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 10:03:56 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Dahl", "Vedrana Andersen", ""], ["Emerson", "Monica Jane", ""], ["Trinderup", "Camilla Himmelstrup", ""], ["Dahl", "Anders Bjorholm", ""]]}, {"id": "1809.02228", "submitter": "Anna Smagina Mrs", "authors": "A.A. Smagina, D.A. Shepelev, E.I. Ershov, A.S. Grigoryev", "title": "Obstacle Detection Quality as a Problem-Oriented Approach to Stereo\n  Vision Algorithms Estimation in Road Situation Analysis", "comments": null, "journal-ref": "IOP Conf. Series: Journal of Physics: Conf. Series 1096 (2018)\n  012035", "doi": "10.1088/1742-6596/1096/1/012035", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a method for performance evaluation of stereo vision\nbased obstacle detection techniques that takes into account the specifics of\nroad situation analysis to minimize the effort required to prepare a test\ndataset. This approach has been designed to be implemented in systems such as\nself-driving cars or driver assistance and can also be used as problem-oriented\nquality criterion for evaluation of stereo vision algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 21:36:56 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Smagina", "A. A.", ""], ["Shepelev", "D. A.", ""], ["Ershov", "E. I.", ""], ["Grigoryev", "A. S.", ""]]}, {"id": "1809.02257", "submitter": "Michele Covell", "authors": "David Marwood and Pascal Massimino and Michele Covell and Shumeet\n  Baluja", "title": "Representing Images in 200 Bytes: Compression via Triangulation", "comments": "IEEE ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rapidly increasing portion of internet traffic is dominated by requests\nfrom mobile devices with limited and metered bandwidth constraints. To satisfy\nthese requests, it has become standard practice for websites to transmit small\nand extremely compressed image previews as part of the initial page load\nprocess to improve responsiveness. Increasing thumbnail compression beyond the\ncapabilities of existing codecs is therefore an active research direction. In\nthis work, we concentrate on extreme compression rates, where the size of the\nimage is typically 200 bytes or less. First, we propose a novel approach for\nimage compression that, unlike commonly used methods, does not rely on\nblock-based statistics. We use an approach based on an adaptive triangulation\nof the target image, devoting more triangles to high entropy regions of the\nimage. Second, we present a novel algorithm for encoding the triangles. The\nresults show favorable statistics, in terms of PSNR and SSIM, over both the\nJPEG and the WebP standards.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 00:05:57 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 23:57:51 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Marwood", "David", ""], ["Massimino", "Pascal", ""], ["Covell", "Michele", ""], ["Baluja", "Shumeet", ""]]}, {"id": "1809.02266", "submitter": "Yucheng Fu", "authors": "Yucheng Fu, Yang Liu", "title": "BubGAN: Bubble Generative Adversarial Networks for Synthesizing\n  Realistic Bubbly Flow Images", "comments": "20 pages, 15 figures", "journal-ref": null, "doi": "10.1016/j.ces.2019.04.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bubble segmentation and size detection algorithms have been developed in\nrecent years for their high efficiency and accuracy in measuring bubbly\ntwo-phase flows. In this work, we proposed an architecture called bubble\ngenerative adversarial networks (BubGAN) for the generation of realistic\nsynthetic images which could be further used as training or benchmarking data\nfor the development of advanced image processing algorithms. The BubGAN is\ntrained initially on a labeled bubble dataset consisting of ten thousand\nimages. By learning the distribution of these bubbles, the BubGAN can generate\nmore realistic bubbles compared to the conventional models used in the\nliterature. The trained BubGAN is conditioned on bubble feature parameters and\nhas full control of bubble properties in terms of aspect ratio, rotation angle,\ncircularity and edge ratio. A million bubble dataset is pre-generated using the\ntrained BubGAN. One can then assemble realistic bubbly flow images using this\ndataset and associated image processing tool. These images contain detailed\nbubble information, therefore do not require additional manual labeling. This\nis more useful compared with the conventional GAN which generates images\nwithout labeling information. The tool could be used to provide benchmarking\nand training data for existing image processing algorithms and to guide the\nfuture development of bubble detecting algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 01:19:59 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Fu", "Yucheng", ""], ["Liu", "Yang", ""]]}, {"id": "1809.02268", "submitter": "Deepak Keshwani", "authors": "Deepak Keshwani, Yoshiro Kitamura, Yuanzhong Li", "title": "Computation of Total Kidney Volume from CT images in Autosomal Dominant\n  Polycystic Kidney Disease using Multi-Task 3D Convolutional Neural Networks", "comments": "8 pages, 5 figures, To appear in the Proceedings of the 21st\n  International Conference On Medical Image Computing & Computer Assisted\n  Intervention, Machine Learning in Medical Imaging workshop, 16-20 September\n  2018, Granada, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Autosomal Dominant Polycystic Kidney Disease (ADPKD) characterized by\nprogressive growth of renal cysts is the most prevalent and potentially lethal\nmonogenic renal disease, affecting one in every 500-100 people. Total Kidney\nVolume (TKV) and its growth computed from Computed Tomography images has been\naccepted as an essential prognostic marker for renal function loss. Due to\nlarge variation in shape and size of kidney in ADPKD, existing methods to\ncompute TKV (i.e. to segment ADKP) including those based on 2D convolutional\nneural networks are not accurate enough to be directly useful in clinical\npractice. In this work, we propose multi-task 3D Convolutional Neural Networks\nto segment ADPK and achieve a mean DICE score of 0.95 and mean absolute\npercentage TKV error of 3.86. Additionally, to solve the challenge of class\nimbalance, we propose to simply bootstrap cross entropy loss and compare\nresults with recently prevalent dice loss in medical image segmentation\ncommunity.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 01:22:52 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Keshwani", "Deepak", ""], ["Kitamura", "Yoshiro", ""], ["Li", "Yuanzhong", ""]]}, {"id": "1809.02288", "submitter": "Longhao Yuan", "authors": "Longhao Yuan, Chao Li, Danilo Mandic, Jianting Cao, Qibin Zhao", "title": "Tensor Ring Decomposition with Rank Minimization on Latent Space: An\n  Efficient Approach for Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tensor completion tasks, the traditional low-rank tensor decomposition\nmodels suffer from the laborious model selection problem due to their high\nmodel sensitivity. In particular, for tensor ring (TR) decomposition, the\nnumber of model possibilities grows exponentially with the tensor order, which\nmakes it rather challenging to find the optimal TR decomposition. In this\npaper, by exploiting the low-rank structure of the TR latent space, we propose\na novel tensor completion method which is robust to model selection. In\ncontrast to imposing the low-rank constraint on the data space, we introduce\nnuclear norm regularization on the latent TR factors, resulting in the\noptimization step using singular value decomposition (SVD) being performed at a\nmuch smaller scale. By leveraging the alternating direction method of\nmultipliers (ADMM) scheme, the latent TR factors with optimal rank and the\nrecovered tensor can be obtained simultaneously. Our proposed algorithm is\nshown to effectively alleviate the burden of TR-rank selection, thereby greatly\nreducing the computational cost. The extensive experimental results on both\nsynthetic and real-world data demonstrate the superior performance and\nefficiency of the proposed approach against the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 03:05:08 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 08:03:46 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Yuan", "Longhao", ""], ["Li", "Chao", ""], ["Mandic", "Danilo", ""], ["Cao", "Jianting", ""], ["Zhao", "Qibin", ""]]}, {"id": "1809.02302", "submitter": "Chaoyou Fu", "authors": "Chaoyou Fu, Liangchen Song, Xiang Wu, Guoli Wang, Ran He", "title": "Neurons Merging Layer: Towards Progressive Redundancy Reduction for Deep\n  Supervised Hashing", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep supervised hashing has become an active topic in information retrieval.\nIt generates hashing bits by the output neurons of a deep hashing network.\nDuring binary discretization, there often exists much redundancy between\nhashing bits that degenerates retrieval performance in terms of both storage\nand accuracy. This paper proposes a simple yet effective Neurons Merging Layer\n(NMLayer) for deep supervised hashing. A graph is constructed to represent the\nredundancy relationship between hashing bits that is used to guide the learning\nof a hashing network. Specifically, it is dynamically learned by a novel\nmechanism defined in our active and frozen phases. According to the learned\nrelationship, the NMLayer merges the redundant neurons together to balance the\nimportance of each output neuron. Moreover, multiple NMLayers are progressively\ntrained for a deep hashing network to learn a more compact hashing code from a\nlong redundant code. Extensive experiments on four datasets demonstrate that\nour proposed method outperforms state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 04:10:47 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 01:35:24 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 10:03:12 GMT"}, {"version": "v4", "created": "Tue, 26 Nov 2019 08:40:00 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Fu", "Chaoyou", ""], ["Song", "Liangchen", ""], ["Wu", "Xiang", ""], ["Wang", "Guoli", ""], ["He", "Ran", ""]]}, {"id": "1809.02318", "submitter": "Samvit Jain", "authors": "Samvit Jain, Ganesh Ananthanarayanan, Junchen Jiang, Yuanchao Shu,\n  Joseph E. Gonzalez", "title": "Scaling Video Analytics Systems to Large Camera Deployments", "comments": "HotMobile 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by advances in computer vision and the falling costs of camera\nhardware, organizations are deploying video cameras en masse for the spatial\nmonitoring of their physical premises. Scaling video analytics to massive\ncamera deployments, however, presents a new and mounting challenge, as compute\ncost grows proportionally to the number of camera feeds. This paper is driven\nby a simple question: can we scale video analytics in such a way that cost\ngrows sublinearly, or even remains constant, as we deploy more cameras, while\ninference accuracy remains stable, or even improves. We believe the answer is\nyes. Our key observation is that video feeds from wide-area camera deployments\ndemonstrate significant content correlations (e.g. to other geographically\nproximate feeds), both in space and over time. These spatio-temporal\ncorrelations can be harnessed to dramatically reduce the size of the inference\nsearch space, decreasing both workload and false positive rates in multi-camera\nvideo analytics. By discussing use-cases and technical challenges, we propose a\nroadmap for scaling video analytics to large camera networks, and outline a\nplan for its realization.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 05:54:12 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 01:04:07 GMT"}, {"version": "v3", "created": "Sat, 3 Nov 2018 23:41:41 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 20:38:15 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Jain", "Samvit", ""], ["Ananthanarayanan", "Ganesh", ""], ["Jiang", "Junchen", ""], ["Shu", "Yuanchao", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1809.02333", "submitter": "Shulong Li", "authors": "Shulong Li, Panpan Xu, Bin Li, Liyuan Chen, Zhiguo Zhou, Hongxia Hao,\n  Yingying Duan, Michael Folkert, Jianhua Ma, Steve Jiang, and Jing Wang", "title": "Predicting Lung Nodule Malignancies by Combining Deep Convolutional\n  Neural Network and Handcrafted Features", "comments": "11 pages, 5 figures, 5 tables. This work has been submitted to the\n  IEEE for possible publication", "journal-ref": null, "doi": "10.1088/1361-6560/ab326a", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To predict lung nodule malignancy with a high sensitivity and specificity, we\npropose a fusion algorithm that combines handcrafted features (HF) into the\nfeatures learned at the output layer of a 3D deep convolutional neural network\n(CNN). First, we extracted twenty-nine handcrafted features, including nine\nintensity features, eight geometric features, and twelve texture features based\non grey-level co-occurrence matrix (GLCM) averaged from thirteen directions. We\nthen trained 3D CNNs modified from three state-of-the-art 2D CNN architectures\n(AlexNet, VGG-16 Net and Multi-crop Net) to extract the CNN features learned at\nthe output layer. For each 3D CNN, the CNN features combined with the 29\nhandcrafted features were used as the input for the support vector machine\n(SVM) coupled with the sequential forward feature selection (SFS) method to\nselect the optimal feature subset and construct the classifiers. The fusion\nalgorithm takes full advantage of the handcrafted features and the highest\nlevel CNN features learned at the output layer. It can overcome the\ndisadvantage of the handcrafted features that may not fully reflect the unique\ncharacteristics of a particular lesion by combining the intrinsic CNN features.\nMeanwhile, it also alleviates the requirement of a large scale annotated\ndataset for the CNNs based on the complementary of handcrafted features. The\npatient cohort includes 431 malignant nodules and 795 benign nodules extracted\nfrom the LIDC/IDRI database. For each investigated CNN architecture, the\nproposed fusion algorithm achieved the highest AUC, accuracy, sensitivity, and\nspecificity scores among all competitive classification models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 07:43:17 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 13:09:53 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Li", "Shulong", ""], ["Xu", "Panpan", ""], ["Li", "Bin", ""], ["Chen", "Liyuan", ""], ["Zhou", "Zhiguo", ""], ["Hao", "Hongxia", ""], ["Duan", "Yingying", ""], ["Folkert", "Michael", ""], ["Ma", "Jianhua", ""], ["Jiang", "Steve", ""], ["Wang", "Jing", ""]]}, {"id": "1809.02337", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Christoph K\\\"ading, Joachim Denzler", "title": "Information-Theoretic Active Learning for Content-Based Image Retrieval", "comments": "GCPR 2018 paper (14 pages text + 2 pages references + 6 pages\n  appendix)", "journal-ref": "Pattern Recognition. GCPR 2018. Lecture Notes in Computer Science,\n  vol 11269, pp. 650-666", "doi": "10.1007/978-3-030-12939-2_45", "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Information-Theoretic Active Learning (ITAL), a novel batch-mode\nactive learning method for binary classification, and apply it for acquiring\nmeaningful user feedback in the context of content-based image retrieval.\nInstead of combining different heuristics such as uncertainty, diversity, or\ndensity, our method is based on maximizing the mutual information between the\npredicted relevance of the images and the expected user feedback regarding the\nselected batch. We propose suitable approximations to this computationally\ndemanding problem and also integrate an explicit model of user behavior that\naccounts for possible incorrect labels and unnameable instances. Furthermore,\nour approach does not only take the structure of the data but also the expected\nmodel output change caused by the user feedback into account. In contrast to\nother methods, ITAL turns out to be highly flexible and provides\nstate-of-the-art performance across various datasets, such as MIRFLICKR and\nImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 07:57:26 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 15:19:35 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["K\u00e4ding", "Christoph", ""], ["Denzler", "Joachim", ""]]}, {"id": "1809.02371", "submitter": "Shiwan Zhao Mr", "authors": "Xiaolu Zhang, Shiwan Zhao, Lingxi Xie", "title": "Infinite Curriculum Learning for Efficiently Detecting Gastric Ulcers in\n  WCE Images", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wireless Capsule Endoscopy (WCE) is becoming a popular way of screening\ngastrointestinal system diseases and cancer. However, the time-consuming\nprocess in inspecting WCE data limits its applications and increases the cost\nof examinations. This paper considers WCE-based gastric ulcer detection, in\nwhich the major challenge is to detect the lesions in a local region. We\npropose an approach named infinite curriculum learning, which generalizes\ncurriculum learning to an infinite sampling space by approximately measuring\nthe difficulty of each patch by its scale. This allows us to adapt our model\nfrom local patches to global images gradually, leading to a consistent accuracy\ngain. Experiments are performed on a large dataset with more than 3 million WCE\nimages. Our approach achieves a binary classification accuracy of 87%, and is\nable to detect some lesions mis-annotated by the physicians. In a real-world\napplication, our approach can reduce the workload of a physician by 90%-98% in\ngastric ulcer screening.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 09:33:56 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Zhang", "Xiaolu", ""], ["Zhao", "Shiwan", ""], ["Xie", "Lingxi", ""]]}, {"id": "1809.02383", "submitter": "Haruo Hosoya", "authors": "Haruo Hosoya", "title": "Group-based Learning of Disentangled Representations with\n  Generalizability for Novel Contents", "comments": null, "journal-ref": "published in IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory data are often comprised of independent content and transformation\nfactors. For example, face images may have shapes as content and poses as\ntransformation. To infer separately these factors from given data, various\n``disentangling'' models have been proposed. However, many of these are\nsupervised or semi-supervised, either requiring attribute labels that are often\nunavailable or disallowing for generalization over new contents. In this study,\nwe introduce a novel deep generative model, called group-based variational\nautoencoders. In this, we assume no explicit labels, but a weaker form of\nstructure that groups together data instances having the same content but\ntransformed differently; we thereby separately estimate a group-common factor\nas content and an instance-specific factor as transformation. This approach\nallows for learning to represent a general continuous space of contents, which\ncan accommodate unseen contents. Despite the simplicity, our model succeeded in\nlearning, from five datasets, content representations that are highly separate\nfrom the transformation representation and generalizable to data with novel\ncontents. We further provide detailed analysis of the latent content code and\nshow insight into how our model obtains the notable transformation invariance\nand content generalizability.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 10:00:54 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 00:55:30 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hosoya", "Haruo", ""]]}, {"id": "1809.02440", "submitter": "Hugo Richard", "authors": "Hugo Richard (PARIETAL), Ana Pinho (NEUROSPIN), Bertrand Thirion\n  (PARIETAL), Guillaume Charpiat (TAU)", "title": "Optimizing deep video representation to match brain activity", "comments": null, "journal-ref": "2018 Conference on Cognitive Computational Neuroscience, Sep 2018,\n  Philadelphia, United States", "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The comparison of observed brain activity with the statistics generated by\nartificial intelligence systems is useful to probe brain functional\norganization under ecological conditions. Here we study fMRI activity in ten\nsubjects watching color natural movies and compute deep representations of\nthese movies with an architecture that relies on optical flow and image\ncontent. The association of activity in visual areas with the different layers\nof the deep architecture displays complexity-related contrasts across visual\nareas and reveals a striking foveal/peripheral dichotomy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 12:37:50 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Richard", "Hugo", "", "PARIETAL"], ["Pinho", "Ana", "", "NEUROSPIN"], ["Thirion", "Bertrand", "", "PARIETAL"], ["Charpiat", "Guillaume", "", "TAU"]]}, {"id": "1809.02492", "submitter": "Nikita Dvornik", "authors": "Nikita Dvornik, Julien Mairal, Cordelia Schmid", "title": "On the Importance of Visual Context for Data Augmentation in Scene\n  Understanding", "comments": "Updated the experimental section. arXiv admin note: substantial text\n  overlap with arXiv:1807.07428", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing data augmentation for learning deep neural networks is known to be\nimportant for training visual recognition systems. By artificially increasing\nthe number of training examples, it helps reducing overfitting and improves\ngeneralization. While simple image transformations can already improve\npredictive performance in most vision tasks, larger gains can be obtained by\nleveraging task-specific prior knowledge. In this work, we consider object\ndetection, semantic and instance segmentation and augment the training images\nby blending objects in existing scenes, using instance segmentation\nannotations. We observe that randomly pasting objects on images hurts the\nperformance, unless the object is placed in the right context. To resolve this\nissue, we propose an explicit context model by using a convolutional neural\nnetwork, which predicts whether an image region is suitable for placing a given\nobject or not. In our experiments, we show that our approach is able to improve\nobject detection, semantic and instance segmentation on the PASCAL VOC12 and\nCOCO datasets, with significant gains in a limited annotation scenario, i.e.\nwhen only one category is annotated. We also show that the method is not\nlimited to datasets that come with expensive pixel-wise instance annotations\nand can be used when only bounding boxes are available, by employing\nweakly-supervised learning for instance masks approximation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 08:37:15 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 14:25:59 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 09:41:11 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Dvornik", "Nikita", ""], ["Mairal", "Julien", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1809.02560", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su and Matheus Gadelha and Rui Wang and Subhransu Maji", "title": "A Deeper Look at 3D Shape Classifiers", "comments": "Accepted to Second Workshop on 3D Reconstruction Meets Semantics,\n  ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the role of representations and architectures for classifying\n3D shapes in terms of their computational efficiency, generalization, and\nrobustness to adversarial transformations. By varying the number of training\nexamples and employing cross-modal transfer learning we study the role of\ninitialization of existing deep architectures for 3D shape classification. Our\nanalysis shows that multiview methods continue to offer the best generalization\neven without pretraining on large labeled image datasets, and even when trained\non simplified inputs such as binary silhouettes. Furthermore, the performance\nof voxel-based 3D convolutional networks and point-based architectures can be\nimproved via cross-modal transfer from image representations. Finally, we\nanalyze the robustness of 3D shape classifiers to adversarial transformations\nand present a novel approach for generating adversarial perturbations of a 3D\nshape for multiview classifiers using a differentiable renderer. We find that\npoint-based networks are more robust to point position perturbations while\nvoxel-based and multiview networks are easily fooled with the addition of\nimperceptible noise to the input.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 16:10:23 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 18:14:18 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Gadelha", "Matheus", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "1809.02568", "submitter": "Shunsuke Kitada", "authors": "Shunsuke Kitada and Hitoshi Iyatomi", "title": "Skin lesion classification with ensemble of squeeze-and-excitation\n  networks and semi-supervised learning", "comments": "6 pages, 4 figures, ISIC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we introduce the outline of our system in Task 3: Disease\nClassification of ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection.\nWe fine-tuned multiple pre-trained neural network models based on\nSqueeze-and-Excitation Networks (SENet) which achieved state-of-the-art results\nin the field of image recognition. In addition, we used the mean teachers as a\nsemi-supervised learning framework and introduced some specially designed data\naugmentation strategies for skin lesion analysis. We confirmed our data\naugmentation strategy improved classification performance and demonstrated\n87.2% in balanced accuracy on the official ISIC2018 validation dataset.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 16:24:21 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Kitada", "Shunsuke", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "1809.02587", "submitter": "Pedro Morgado", "authors": "Pedro Morgado, Nuno Vasconcelos, Timothy Langlois and Oliver Wang", "title": "Self-Supervised Generation of Spatial Audio for 360 Video", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to convert mono audio recorded by a 360 video camera\ninto spatial audio, a representation of the distribution of sound over the full\nviewing sphere. Spatial audio is an important component of immersive 360 video\nviewing, but spatial audio microphones are still rare in current 360 video\nproduction. Our system consists of end-to-end trainable neural networks that\nseparate individual sound sources and localize them on the viewing sphere,\nconditioned on multi-modal analysis of audio and 360 video frames. We introduce\nseveral datasets, including one filmed ourselves, and one collected in-the-wild\nfrom YouTube, consisting of 360 videos uploaded with spatial audio. During\ntraining, ground-truth spatial audio serves as self-supervision and a mixed\ndown mono track forms the input to our network. Using our approach, we show\nthat it is possible to infer the spatial location of sound sources based only\non 360 video and a mono audio track.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 17:25:59 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Morgado", "Pedro", ""], ["Vasconcelos", "Nuno", ""], ["Langlois", "Timothy", ""], ["Wang", "Oliver", ""]]}, {"id": "1809.02601", "submitter": "Lingxi Xie", "authors": "Junran Peng, Lingxi Xie, Zhaoxiang Zhang, Tieniu Tan, Jingdong Wang", "title": "Accelerating Deep Neural Networks with Spatial Bottleneck Modules", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient module named spatial bottleneck for\naccelerating the convolutional layers in deep neural networks. The core idea is\nto decompose convolution into two stages, which first reduce the spatial\nresolution of the feature map, and then restore it to the desired size. This\noperation decreases the sampling density in the spatial domain, which is\nindependent yet complementary to network acceleration approaches in the channel\ndomain. Using different sampling rates, we can tradeoff between recognition\naccuracy and model complexity.\n  As a basic building block, spatial bottleneck can be used to replace any\nsingle convolutional layer, or the combination of two convolutional layers. We\nempirically verify the effectiveness of spatial bottleneck by applying it to\nthe deep residual networks. Spatial bottleneck achieves 2x and 1.4x speedup on\nthe regular and channel-bottlenecked residual blocks, respectively, with the\naccuracies retained in recognizing low-resolution images, and even improved in\nrecognizing high-resolution images.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 17:54:54 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Peng", "Junran", ""], ["Xie", "Lingxi", ""], ["Zhang", "Zhaoxiang", ""], ["Tan", "Tieniu", ""], ["Wang", "Jingdong", ""]]}, {"id": "1809.02651", "submitter": "Samiran Ganguly", "authors": "Samiran Ganguly, Yunfei Gu, Yunkun Xie, Mircea R. Stan, Avik W. Ghosh,\n  Nibir K. Dhar", "title": "Reservoir Computing based Neural Image Filters", "comments": "5 pages, 4 figures, To appear in Conference Proceedings of The 44th\n  Annual Conference of IEEE Industrial Electronics Society (2018): Special\n  Session on Machine Vision, Control and Navigation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clean images are an important requirement for machine vision systems to\nrecognize visual features correctly. However, the environment, optics,\nelectronics of the physical imaging systems can introduce extreme distortions\nand noise in the acquired images. In this work, we explore the use of reservoir\ncomputing, a dynamical neural network model inspired from biological systems,\nin creating dynamic image filtering systems that extracts signal from noise\nusing inverse modeling. We discuss the possibility of implementing these\nnetworks in hardware close to the sensors.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 19:58:53 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Ganguly", "Samiran", ""], ["Gu", "Yunfei", ""], ["Xie", "Yunkun", ""], ["Stan", "Mircea R.", ""], ["Ghosh", "Avik W.", ""], ["Dhar", "Nibir K.", ""]]}, {"id": "1809.02652", "submitter": "Harris Chan", "authors": "Harris Chan, Atef Chaudhury, Kevin Shen", "title": "Are You Sure You Want To Do That? Classification with Verification", "comments": "9 pages, 5 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification systems typically act in isolation, meaning they are required\nto implicitly memorize the characteristics of all candidate classes in order to\nclassify. The cost of this is increased memory usage and poor sample\nefficiency. We propose a model which instead verifies using reference images\nduring the classification process, reducing the burden of memorization. The\nmodel uses iterative nondifferentiable queries in order to classify an image.\nWe demonstrate that such a model is feasible to train and can match baseline\naccuracy while being more parameter efficient. However, we show that finding\nthe correct balance between image recognition and verification is essential to\npushing the model towards desired behavior, suggesting that a pipeline of\nrecognition followed by verification is a more promising approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 19:59:43 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 03:04:38 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Chan", "Harris", ""], ["Chaudhury", "Atef", ""], ["Shen", "Kevin", ""]]}, {"id": "1809.02681", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng, Liang Zheng, Yi Yang, Fei Wu", "title": "Query Attack via Opposite-Direction Feature:Towards Robust Image\n  Retrieval", "comments": "12 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing works of adversarial samples focus on attacking image\nrecognition models, while little attention is paid to the image retrieval task.\nIn this paper, we identify two inherent challenges in applying prevailing image\nrecognition attack methods to image retrieval. First, image retrieval demands\ndiscriminative visual features, which is significantly different from the\none-hot class prediction in image recognition. Second, due to the disjoint and\npotentially unrelated classes between the training and test set in image\nretrieval, predicting the query category from predefined training classes is\nnot accurate and leads to a sub-optimal adversarial gradient. To address these\nlimitations, we propose a new white-box attack approach, Opposite-Direction\nFeature Attack (ODFA), to generate adversarial queries. Opposite-Direction\nFeature Attack (ODFA) effectively exploits feature-level adversarial gradients\nand takes advantage of feature distance in the representation space. To our\nknowledge, we are among the early attempts to design an attack method\nspecifically for image retrieval. When we deploy an attacked image as the\nquery, the true matches are prone to receive low ranks. We demonstrate through\nextensive experiments that (1) only crafting adversarial queries is sufficient\nto fool the state-of-the-art retrieval systems; (2) the proposed attack method,\nODFA, leads to a higher attack success rate than classification attack methods,\nvalidating the necessity of leveraging characteristics of image retrieval; (3)\nthe adversarial queries generated by our method have good transferability to\nother retrieval models without accessing their parameters, i.e.,the black-box\nsetting.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 21:29:32 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 04:04:01 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zheng", "Zhedong", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "1809.02693", "submitter": "Shifeng Zhang", "authors": "Cheng Chi, Shifeng Zhang, Junliang Xing, Zhen Lei, Stan Z. Li, Xudong\n  Zou", "title": "Selective Refinement Network for High Performance Face Detection", "comments": "The first two authors have equal contributions. Corresponding author:\n  Shifeng Zhang (shifeng.zhang@nlpr.ia.ac.cn)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance face detection remains a very challenging problem,\nespecially when there exists many tiny faces. This paper presents a novel\nsingle-shot face detector, named Selective Refinement Network (SRN), which\nintroduces novel two-step classification and regression operations selectively\ninto an anchor-based face detector to reduce false positives and improve\nlocation accuracy simultaneously. In particular, the SRN consists of two\nmodules: the Selective Two-step Classification (STC) module and the Selective\nTwo-step Regression (STR) module. The STC aims to filter out most simple\nnegative anchors from low level detection layers to reduce the search space for\nthe subsequent classifier, while the STR is designed to coarsely adjust the\nlocations and sizes of anchors from high level detection layers to provide\nbetter initialization for the subsequent regressor. Moreover, we design a\nReceptive Field Enhancement (RFE) block to provide more diverse receptive\nfield, which helps to better capture faces in some extreme poses. As a\nconsequence, the proposed SRN detector achieves state-of-the-art performance on\nall the widely used face detection benchmarks, including AFW, PASCAL face,\nFDDB, and WIDER FACE datasets. Codes will be released to facilitate further\nstudies on the face detection problem.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 22:00:18 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Chi", "Cheng", ""], ["Zhang", "Shifeng", ""], ["Xing", "Junliang", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""], ["Zou", "Xudong", ""]]}, {"id": "1809.02714", "submitter": "Mohamed Abdelpakey", "authors": "Mohamed H. Abdelpakey, Mohamed S. Shehata, and Mostafa M. Mohamed", "title": "DensSiam: End-to-End Densely-Siamese Network with Self-Attention Model\n  for Object Tracking", "comments": "11 pages, 3 figures, Accepted by ISVC18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Siamese neural networks have been recently used to track\nobjects using deep features. Siamese architecture can achieve real time speed,\nhowever it is still difficult to find a Siamese architecture that maintains the\ngeneralization capability, high accuracy and speed while decreasing the number\nof shared parameters especially when it is very deep. Furthermore, a\nconventional Siamese architecture usually processes one local neighborhood at a\ntime, which makes the appearance model local and non-robust to appearance\nchanges.\n  To overcome these two problems, this paper proposes DensSiam, a novel\nconvolutional Siamese architecture, which uses the concept of dense layers and\nconnects each dense layer to all layers in a feed-forward fashion with a\nsimilarity-learning function. DensSiam also includes a Self-Attention mechanism\nto force the network to pay more attention to the non-local features during\noffline training. Extensive experiments are performed on four tracking\nbenchmarks: OTB2013 and OTB2015 for validation set; and VOT2015, VOT2016 and\nVOT2017 for testing set. The obtained results show that DensSiam achieves\nsuperior results on these benchmarks compared to other current state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 23:41:02 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Abdelpakey", "Mohamed H.", ""], ["Shehata", "Mohamed S.", ""], ["Mohamed", "Mostafa M.", ""]]}, {"id": "1809.02736", "submitter": "David Minnen", "authors": "David Minnen, Johannes Ball\\'e, and George Toderici", "title": "Joint Autoregressive and Hierarchical Priors for Learned Image\n  Compression", "comments": "Accepted at the 32nd Conference on Neural Information Processing\n  Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent models for learned image compression are based on autoencoders,\nlearning approximately invertible mappings from pixels to a quantized latent\nrepresentation. These are combined with an entropy model, a prior on the latent\nrepresentation that can be used with standard arithmetic coding algorithms to\nyield a compressed bitstream. Recently, hierarchical entropy models have been\nintroduced as a way to exploit more structure in the latents than simple fully\nfactorized priors, improving compression performance while maintaining\nend-to-end optimization. Inspired by the success of autoregressive priors in\nprobabilistic generative models, we examine autoregressive, hierarchical, as\nwell as combined priors as alternatives, weighing their costs and benefits in\nthe context of image compression. While it is well known that autoregressive\nmodels come with a significant computational penalty, we find that in terms of\ncompression performance, autoregressive and hierarchical priors are\ncomplementary and, together, exploit the probabilistic structure in the latents\nbetter than all previous learned models. The combined model yields\nstate-of-the-art rate--distortion performance, providing a 15.8% average\nreduction in file size over the previous state-of-the-art method based on deep\nlearning, which corresponds to a 59.8% size reduction over JPEG, more than 35%\nreduction compared to WebP and JPEG2000, and bitstreams 8.4% smaller than BPG,\nthe current state-of-the-art image codec. To the best of our knowledge, our\nmodel is the first learning-based method to outperform BPG on both PSNR and\nMS-SSIM distortion metrics.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 01:51:28 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Minnen", "David", ""], ["Ball\u00e9", "Johannes", ""], ["Toderici", "George", ""]]}, {"id": "1809.02743", "submitter": "Yan Xia", "authors": "Yan Xia, Yang Zhang, Dingfu Zhou, Xinyu Huang, Cheng Wang, Ruigang\n  Yang", "title": "RealPoint3D: Point Cloud Generation from a Single Image with Complex\n  Background", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud generation by the deep neural network from a single image has\nbeen attracting more and more researchers' attention. However,\nrecently-proposed methods require the objects be captured with relatively clean\nbackgrounds, fixed viewpoint, while this highly limits its application in the\nreal environment. To overcome these drawbacks, we proposed to integrate the\nprior 3D shape knowledge into the network to guide the 3D generation. By taking\nadditional 3D information, the proposed network can handle the 3D object\ngeneration from a single real image captured from any viewpoint and complex\nbackground. Specifically, giving a query image, we retrieve the nearest shape\nmodel from a pre-prepared 3D model database. Then, the image together with the\nretrieved shape model is fed into the proposed network to generate the\nfine-grained 3D point cloud. The effectiveness of our proposed framework has\nbeen verified on different kinds of datasets. Experimental results show that\nthe proposed framework achieves state-of-the-art accuracy compared to other\nvolumetric-based and point set generation methods. Furthermore, the proposed\nframework works well for real images in complex backgrounds with various view\nangles.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 02:24:19 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Xia", "Yan", ""], ["Zhang", "Yang", ""], ["Zhou", "Dingfu", ""], ["Huang", "Xinyu", ""], ["Wang", "Cheng", ""], ["Yang", "Ruigang", ""]]}, {"id": "1809.02766", "submitter": "Utkarsh Contractor", "authors": "Utkarsh Contractor, Chinmayi Dixit, Deepti Mahajan", "title": "CNNs for Surveillance Footage Scene Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we adapt high-performing CNN architectures to differentiate\nbetween scenes with and without abandoned luggage. Using frames from two video\ndatasets, we compare the results of training different architectures on each\ndataset as well as on combining the datasets. We additionally use network\nvisualization techniques to gain insight into what the neural network sees, and\nthe basis of the classification decision. We intend that our results benefit\nfurther work in applying CNNs in surveillance and security-related tasks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 06:45:22 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Contractor", "Utkarsh", ""], ["Dixit", "Chinmayi", ""], ["Mahajan", "Deepti", ""]]}, {"id": "1809.02776", "submitter": "Tianyang Wang", "authors": "Tianyang Wang, Jun Huan, Michelle Zhu", "title": "Instance-based Deep Transfer Learning", "comments": "Accepted to WACV 2019. This is a preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep transfer learning recently has acquired significant research interest.\nIt makes use of pre-trained models that are learned from a source domain, and\nutilizes these models for the tasks in a target domain. Model-based deep\ntransfer learning is probably the most frequently used method. However, very\nlittle research work has been devoted to enhancing deep transfer learning by\nfocusing on the influence of data. In this paper, we propose an instance-based\napproach to improve deep transfer learning in a target domain. Specifically, we\nchoose a pre-trained model from a source domain and apply this model to\nestimate the influence of training samples in a target domain. Then we optimize\nthe training data of the target domain by removing the training samples that\nwill lower the performance of the pre-trained model. We later either fine-tune\nthe pre-trained model with the optimized training data in the target domain, or\nbuild a new model which is initialized partially based on the pre-trained\nmodel, and fine-tune it with the optimized training data in the target domain.\nUsing this approach, transfer learning can help deep learning models to capture\nmore useful features. Extensive experiments demonstrate the effectiveness of\nour approach on boosting the quality of deep learning models for some common\ncomputer vision tasks, such as image classification.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 08:34:14 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 06:27:53 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Wang", "Tianyang", ""], ["Huan", "Jun", ""], ["Zhu", "Michelle", ""]]}, {"id": "1809.02786", "submitter": "Dan Peng", "authors": "Dan Peng, Zizhan Zheng, Xiaofeng Zhang", "title": "Structure-Preserving Transformation: Generating Diverse and Transferable\n  Adversarial Examples", "comments": "The AAAI-2019 Workshop on Artificial Intelligence for Cyber Security\n  (AICS)", "journal-ref": null, "doi": null, "report-no": "AICS/2019/09", "categories": "cs.LG cs.AI cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are perturbed inputs designed to fool machine learning\nmodels. Most recent works on adversarial examples for image classification\nfocus on directly modifying pixels with minor perturbations. A common\nrequirement in all these works is that the malicious perturbations should be\nsmall enough (measured by an L_p norm for some p) so that they are\nimperceptible to humans. However, small perturbations can be unnecessarily\nrestrictive and limit the diversity of adversarial examples generated. Further,\nan L_p norm based distance metric ignores important structure patterns hidden\nin images that are important to human perception. Consequently, even the minor\nperturbation introduced in recent works often makes the adversarial examples\nless natural to humans. More importantly, they often do not transfer well and\nare therefore less effective when attacking black-box models especially for\nthose protected by a defense mechanism. In this paper, we propose a\nstructure-preserving transformation (SPT) for generating natural and diverse\nadversarial examples with extremely high transferability. The key idea of our\napproach is to allow perceptible deviation in adversarial examples while\nkeeping structure patterns that are central to a human classifier. Empirical\nresults on the MNIST and the fashion-MNIST datasets show that adversarial\nexamples generated by our approach can easily bypass strong adversarial\ntraining. Further, they transfer well to other target models with no loss or\nlittle loss of successful attack rate.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 10:26:50 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 15:42:00 GMT"}, {"version": "v3", "created": "Sat, 22 Dec 2018 09:07:32 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Peng", "Dan", ""], ["Zheng", "Zizhan", ""], ["Zhang", "Xiaofeng", ""]]}, {"id": "1809.02791", "submitter": "Yaqi Liu", "authors": "Yaqi Liu, Xianfeng Zhao, Xiaobin Zhu, and Yun Cao", "title": "Adversarial Learning for Image Forensics Deep Matching with Atrous\n  Convolution", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained image splicing detection and localization (CISDL) is a newly\nproposed challenging task for image forensics, which investigates two input\nsuspected images and identifies whether one image has suspected regions pasted\nfrom the other. In this paper, we propose a novel adversarial learning\nframework to train the deep matching network for CISDL. Our framework mainly\nconsists of three building blocks: 1) the deep matching network based on atrous\nconvolution (DMAC) aims to generate two high-quality candidate masks which\nindicate the suspected regions of the two input images, 2) the detection\nnetwork is designed to rectify inconsistencies between the two corresponding\ncandidate masks, 3) the discriminative network drives the DMAC network to\nproduce masks that are hard to distinguish from ground-truth ones. In DMAC,\natrous convolution is adopted to extract features with rich spatial\ninformation, the correlation layer based on the skip architecture is proposed\nto capture hierarchical features, and atrous spatial pyramid pooling is\nconstructed to localize tampered regions at multiple scales. The detection\nnetwork and the discriminative network act as the losses with auxiliary\nparameters to supervise the training of DMAC in an adversarial way. Extensive\nexperiments, conducted on 21 generated testing sets and two public datasets,\ndemonstrate the effectiveness of the proposed framework and the superior\nperformance of DMAC.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 11:59:36 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Liu", "Yaqi", ""], ["Zhao", "Xianfeng", ""], ["Zhu", "Xiaobin", ""], ["Cao", "Yun", ""]]}, {"id": "1809.02802", "submitter": "Qixing Zhang", "authors": "Gao Xu, Yongming Zhang, Qixing Zhang, Gaohua Lin, Zhong Wang, Yang\n  Jia, Jinjun Wang", "title": "Video Smoke Detection Based on Deep Saliency Network", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video smoke detection is a promising fire detection method especially in open\nor large spaces and outdoor environments. Traditional video smoke detection\nmethods usually consist of candidate region extraction and classification, but\nlack powerful characterization for smoke. In this paper, we propose a novel\nvideo smoke detection method based on deep saliency network. Visual saliency\ndetection aims to highlight the most important object regions in an image. The\npixel-level and object-level salient convolutional neural networks are combined\nto extract the informative smoke saliency map. An end-to-end framework for\nsalient smoke detection and existence prediction of smoke is proposed for\napplication in video smoke detection. The deep feature map is combined with the\nsaliency map to predict the existence of smoke in an image. Initial and\naugmented dataset are built to measure the performance of frameworks with\ndifferent design strategies. Qualitative and quantitative analysis at\nframe-level and pixel-level demonstrate the excellent performance of the\nultimate framework.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 13:44:28 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 01:21:32 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Xu", "Gao", ""], ["Zhang", "Yongming", ""], ["Zhang", "Qixing", ""], ["Lin", "Gaohua", ""], ["Wang", "Zhong", ""], ["Jia", "Yang", ""], ["Wang", "Jinjun", ""]]}, {"id": "1809.02805", "submitter": "Jialin Wu", "authors": "Jialin Wu and Raymond J. Mooney", "title": "Faithful Multimodal Explanation for Visual Question Answering", "comments": "In ACL 2019 BlackboxNLP workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI systems' ability to explain their reasoning is critical to their utility\nand trustworthiness. Deep neural networks have enabled significant progress on\nmany challenging problems such as visual question answering (VQA). However,\nmost of them are opaque black boxes with limited explanatory capability. This\npaper presents a novel approach to developing a high-performing VQA system that\ncan elucidate its answers with integrated textual and visual explanations that\nfaithfully reflect important aspects of its underlying reasoning while\ncapturing the style of comprehensible human explanations. Extensive\nexperimental evaluation demonstrates the advantages of this approach compared\nto competing methods with both automatic evaluation metrics and human\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 14:14:03 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 19:54:15 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Wu", "Jialin", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1809.02850", "submitter": "Suhas Lohit", "authors": "Suhas Lohit, Rajhans Singh, Kuldeep Kulkarni and Pavan Turaga", "title": "Rate-Adaptive Neural Networks for Spatial Multiplexers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In resource-constrained environments, one can employ spatial multiplexing\ncameras to acquire a small number of measurements of a scene, and perform\neffective reconstruction or high-level inference using purely data-driven\nneural networks. However, once trained, the measurement matrix and the network\nare valid only for a single measurement rate (MR) chosen at training time. To\novercome this drawback, we answer the following question: How can we jointly\ndesign the measurement operator and the reconstruction/inference network so\nthat the system can operate over a \\textit{range} of MRs? To this end, we\npresent a novel training algorithm, for learning\n\\textbf{\\textit{rate-adaptive}} networks. Using standard datasets, we\ndemonstrate that, when tested over a range of MRs, a rate-adaptive network can\nprovide high quality reconstruction over a the entire range, resulting in up to\nabout 15 dB improvement over previous methods, where the network is valid for\nonly one MR. We demonstrate the effectiveness of our approach for\nsample-efficient object tracking where video frames are acquired at dynamically\nvarying MRs. We also extend this algorithm to learn the measurement operator in\nconjunction with image recognition networks. Experiments on MNIST and CIFAR-10\nconfirm the applicability of our algorithm to different tasks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 18:21:31 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Lohit", "Suhas", ""], ["Singh", "Rajhans", ""], ["Kulkarni", "Kuldeep", ""], ["Turaga", "Pavan", ""]]}, {"id": "1809.02851", "submitter": "Pierre-Luc St-Charles", "authors": "Pierre-Luc St-Charles, Guillaume-Alexandre Bilodeau, Robert Bergevin", "title": "Online Mutual Foreground Segmentation for Multispectral Stereo Videos", "comments": "Preprint accepted for publication in IJCV (December 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of video sequences into foreground and background regions is\na low-level process commonly used in video content analysis and smart\nsurveillance applications. Using a multispectral camera setup can improve this\nprocess by providing more diverse data to help identify objects despite adverse\nimaging conditions. The registration of several data sources is however not\ntrivial if the appearance of objects produced by each sensor differs\nsubstantially. This problem is further complicated when parallax effects cannot\nbe ignored when using close-range stereo pairs. In this work, we present a new\nmethod to simultaneously tackle multispectral segmentation and stereo\nregistration. Using an iterative procedure, we estimate the labeling result for\none problem using the provisional result of the other. Our approach is based on\nthe alternating minimization of two energy functions that are linked through\nthe use of dynamic priors. We rely on the integration of shape and appearance\ncues to find proper multispectral correspondences, and to properly segment\nobjects in low contrast regions. We also formulate our model as a frame\nprocessing pipeline using higher order terms to improve the temporal coherence\nof our results. Our method is evaluated under different configurations on\nmultiple multispectral datasets, and our implementation is available online.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 18:21:50 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 23:10:50 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["St-Charles", "Pierre-Luc", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Bergevin", "Robert", ""]]}, {"id": "1809.02854", "submitter": "Jianhui Chen Mr", "authors": "Jianhui Chen, Keyu Lu, Sijia Tian and James J. Little", "title": "Learning Sports Camera Selection from Internet Videos", "comments": "8 + 2 pages, WACV2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses camera selection, the task of predicting which camera\nshould be \"on air\" from multiple candidate cameras for soccer broadcast. The\ntask is challenging because of the scarcity of learning data with all candidate\nviews. Meanwhile, broadcast videos are freely available on the Internet (e.g.\nYoutube). However, these videos only record the selected camera views, omitting\nthe other candidate views. To overcome this problem, we first introduce a\nrandom survival forest (RSF) method to impute the incomplete data effectively.\nThen, we propose a spatial-appearance heatmap to describe foreground objects\n(e.g. players and balls) in an image. To evaluate the performance of our\nsystem, we collect the largest-ever dataset for soccer broadcasting camera\nselection. It has one main game which has all candidate views and twelve\nauxiliary games which only have the broadcast view. Our method significantly\noutperforms state-of-the-art methods on this challenging dataset. Further\nanalysis suggests that the improvement in performance is indeed from the extra\ninformation from auxiliary games.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 19:16:04 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Chen", "Jianhui", ""], ["Lu", "Keyu", ""], ["Tian", "Sijia", ""], ["Little", "James J.", ""]]}, {"id": "1809.02874", "submitter": "Xiatian Zhu", "authors": "Minxian Li, Xiatian Zhu, Shaogang Gong", "title": "Unsupervised Person Re-identification by Deep Learning Tracklet\n  Association", "comments": "ECCV 2018 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mostexistingpersonre-identification(re-id)methods relyon supervised model\nlearning on per-camera-pair manually labelled pairwise training data. This\nleads to poor scalability in practical re-id deployment due to the lack of\nexhaustive identity labelling of image positive and negative pairs for every\ncamera pair. In this work, we address this problem by proposing an unsupervised\nre-id deep learning approach capable of incrementally discovering and\nexploiting the underlying re-id discriminative information from automatically\ngenerated person tracklet data from videos in an end-to-end model optimisation.\nWe formulate a Tracklet Association Unsupervised Deep Learning (TAUDL)\nframework characterised by jointly learning per-camera (within-camera) tracklet\nassociation (labelling) and cross-camera tracklet correlation by maximising the\ndiscovery of most likely tracklet relationships across camera views. Extensive\nexperiments demonstrate the superiority of the proposed TAUDL model over the\nstate-of-the-art unsupervised and domain adaptation re- id methods using six\nperson re-id benchmarking datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 20:49:01 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Li", "Minxian", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1809.02875", "submitter": "Saumya Kumaar Saksena", "authors": "Saumya Kumaar, Abhinandan Dogra, Abrar Majeedi, Hanan Gani, Ravi M.\n  Vishwanath and S N Omkar", "title": "A Supervised Learning Methodology for Real-Time Disguised Face\n  Recognition in the Wild", "comments": "Accepted at 2018 International Conference on Robotics and Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial recognition has always been a challeng- ing task for computer vision\nscientists and experts. Despite complexities arising due to variations in\ncamera parameters, illumination and face orientations, significant progress has\nbeen made in the field with deep learning algorithms now competing with\nhuman-level accuracy. But in contrast to the recent advances in face\nrecognition techniques, Disguised Facial Identification continues to be a\ntougher challenge in the field of computer vision. The modern day scenario,\nwhere security is of prime concern, regular face identification techniques do\nnot perform as required when the faces are disguised, which calls for a\ndifferent approach to handle situations where intruders have their faces\nmasked. Along the same lines, we propose a deep learning architecture for\ndisguised facial recognition (DFR). The algorithm put forward in this paper\ndetects 20 facial key-points in the first stage, using a 14-layered\nconvolutional neural network (CNN). These facial key-points are later utilized\nby a support vector machine (SVM) for classifying the disguised faces based on\nthe euclidean distance ratios and angles between different facial key-points.\nThis overall architecture imparts a basic intelligence to our system. Our\nkey-point feature prediction accuracy is 65% while the classification rate is\n72.4%. Moreover, the architecture works at 19 FPS, thereby performing in almost\nreal-time. The efficiency of our approach is also compared with the\nstate-of-the-art Disguised Facial Identification methods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 20:53:19 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Kumaar", "Saumya", ""], ["Dogra", "Abhinandan", ""], ["Majeedi", "Abrar", ""], ["Gani", "Hanan", ""], ["Vishwanath", "Ravi M.", ""], ["Omkar", "S N", ""]]}, {"id": "1809.02882", "submitter": "Weicheng Kuo", "authors": "Weicheng Kuo, Christian H\\\"ane, Esther Yuh, Pratik Mukherjee, and\n  Jitendra Malik", "title": "Cost-Sensitive Active Learning for Intracranial Hemorrhage Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for clinical applications is subject to stringent performance\nrequirements, which raises a need for large labeled datasets. However, the\nenormous cost of labeling medical data makes this challenging. In this paper,\nwe build a cost-sensitive active learning system for the problem of\nintracranial hemorrhage detection and segmentation on head computed tomography\n(CT). We show that our ensemble method compares favorably with the\nstate-of-the-art, while running faster and using less memory. Moreover, our\nexperiments are done using a substantially larger dataset than earlier papers\non this topic. Since the labeling time could vary tremendously across examples,\nwe model the labeling time and optimize the return on investment. We validate\nthis idea by core-set selection on our large labeled dataset and by growing it\nwith data from the wild.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 21:43:18 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Kuo", "Weicheng", ""], ["H\u00e4ne", "Christian", ""], ["Yuh", "Esther", ""], ["Mukherjee", "Pratik", ""], ["Malik", "Jitendra", ""]]}, {"id": "1809.02918", "submitter": "Yali Du", "authors": "Yali Du, Meng Fang, Jinfeng Yi, Jun Cheng, Dacheng Tao", "title": "Towards Query Efficient Black-box Attacks: An Input-free Perspective", "comments": "Accepted by 11th ACM Workshop on Artificial Intelligence and Security\n  (AISec) with the 25th ACM Conference on Computer and Communications Security\n  (CCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have highlighted that deep neural networks (DNNs) are\nvulnerable to adversarial attacks, even in a black-box scenario. However, most\nof the existing black-box attack algorithms need to make a huge amount of\nqueries to perform attacks, which is not practical in the real world. We note\none of the main reasons for the massive queries is that the adversarial example\nis required to be visually similar to the original image, but in many cases,\nhow adversarial examples look like does not matter much. It inspires us to\nintroduce a new attack called \\emph{input-free} attack, under which an\nadversary can choose an arbitrary image to start with and is allowed to add\nperceptible perturbations on it. Following this approach, we propose two\ntechniques to significantly reduce the query complexity. First, we initialize\nan adversarial example with a gray color image on which every pixel has roughly\nthe same importance for the target model. Then we shrink the dimension of the\nattack space by perturbing a small region and tiling it to cover the input\nimage. To make our algorithm more effective, we stabilize a projected gradient\nascent algorithm with momentum, and also propose a heuristic approach for\nregion size selection. Through extensive experiments, we show that with only\n1,701 queries on average, we can perturb a gray image to any target class of\nImageNet with a 100\\% success rate on InceptionV3. Besides, our algorithm has\nsuccessfully defeated two real-world systems, the Clarifai food detection API\nand the Baidu Animal Identification API.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 03:49:06 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Du", "Yali", ""], ["Fang", "Meng", ""], ["Yi", "Jinfeng", ""], ["Cheng", "Jun", ""], ["Tao", "Dacheng", ""]]}, {"id": "1809.02940", "submitter": "Wenji Li", "authors": "Jiewei Lu, Zhun Fan, Ce Zheng, Jingan Feng, Longtao Huang, Wenji Li,\n  Erik D. Goodman", "title": "Automated Strabismus Detection for Telemedicine Applications", "comments": "8 page, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strabismus is one of the most influential ophthalmologic diseases in human's\nlife. Timely detection of strabismus contributes to its prognosis and\ntreatment. Telemedicine, which has great potential to alleviate the growing\ndemand of the diagnosis of ophthalmologic diseases, is an effective method to\nachieve timely strabismus detection. In this paper, a tele strabismus dataset\nis established by the ophthalmologists. Then an end-to-end framework named as\nRF-CNN is proposed to achieve automated strabismus detection on the established\ntele strabismus dataset. RF-CNN first performs eye region segmentation on each\nindividual image, and further classifies the segmented eye regions with deep\nneural networks. The experimental results on the established tele strabismus\ndataset demonstrates that the proposed RF-CNN can have a good performance on\nautomated strabismus detection for telemedicine application. Code is made\npublicly available at:\nhttps://github.com/jieWeiLu/Strabismus-Detection-for-Telemedicine-Application.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 08:46:59 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 02:13:43 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 02:06:46 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Lu", "Jiewei", ""], ["Fan", "Zhun", ""], ["Zheng", "Ce", ""], ["Feng", "Jingan", ""], ["Huang", "Longtao", ""], ["Li", "Wenji", ""], ["Goodman", "Erik D.", ""]]}, {"id": "1809.02945", "submitter": "HsuanKung Yang", "authors": "Hsuan-Kung Yang, An-Chieh Cheng, Kuan-Wei Ho, Tsu-Jui Fu, and Chun-Yi\n  Lee", "title": "Visual Relationship Prediction via Label Clustering and Incorporation of\n  Depth Information", "comments": "Won 2nd place in Person In Context Challenge (ECCV 2018 workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the use of an unsupervised label clustering\ntechnique and demonstrate that it enables substantial improvements in visual\nrelationship prediction accuracy on the Person in Context (PIC) dataset. We\npropose to group object labels with similar patterns of relationship\ndistribution in the dataset into fewer categories. Label clustering not only\nmitigates both the large classification space and class imbalance issues, but\nalso potentially increases data samples for each clustered category. We further\npropose to incorporate depth information as an additional feature into the\ninstance segmentation model. The additional depth prediction path supplements\nthe relationship prediction model in a way that bounding boxes or segmentation\nmasks are unable to deliver. We have rigorously evaluated the proposed\ntechniques and performed various ablation analysis to validate the benefits of\nthem.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 09:23:53 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Yang", "Hsuan-Kung", ""], ["Cheng", "An-Chieh", ""], ["Ho", "Kuan-Wei", ""], ["Fu", "Tsu-Jui", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "1809.02966", "submitter": "Ronald Clark", "authors": "Ronald Clark, Michael Bloesch, Jan Czarnowski, Stefan Leutenegger,\n  Andrew J. Davison", "title": "LS-Net: Learning to Solve Nonlinear Least Squares for Monocular Stereo", "comments": "ECCV 2018. Video: https://youtu.be/5bZbMm8UqbA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-of-squares objective functions are very popular in computer vision\nalgorithms. However, these objective functions are not always easy to optimize.\nThe underlying assumptions made by solvers are often not satisfied and many\nproblems are inherently ill-posed. In this paper, we propose LS-Net, a neural\nnonlinear least squares optimization algorithm which learns to effectively\noptimize these cost functions even in the presence of adversities. Unlike\ntraditional approaches, the proposed solver requires no hand-crafted\nregularizers or priors as these are implicitly learned from the data. We apply\nour method to the problem of motion stereo ie. jointly estimating the motion\nand scene geometry from pairs of images of a monocular sequence. We show that\nour learned optimizer is able to efficiently and effectively solve this\nchallenging optimization problem.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 13:20:00 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Clark", "Ronald", ""], ["Bloesch", "Michael", ""], ["Czarnowski", "Jan", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1809.02967", "submitter": "Linsen Song", "authors": "Linsen Song, Jie Cao, Linxiao Song, Yibo Hu, Ran He", "title": "Geometry-Aware Face Completion and Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face completion is a challenging generation task because it requires\ngenerating visually pleasing new pixels that are semantically consistent with\nthe unmasked face region. This paper proposes a geometry-aware Face Completion\nand Editing NETwork (FCENet) by systematically studying facial geometry from\nthe unmasked region. Firstly, a facial geometry estimator is learned to\nestimate facial landmark heatmaps and parsing maps from the unmasked face\nimage. Then, an encoder-decoder structure generator serves to complete a face\nimage and disentangle its mask areas conditioned on both the masked face image\nand the estimated facial geometry images. Besides, since low-rank property\nexists in manually labeled masks, a low-rank regularization term is imposed on\nthe disentangled masks, enforcing our completion network to manage occlusion\narea with various shape and size. Furthermore, our network can generate diverse\nresults from the same masked input by modifying estimated facial geometry,\nwhich provides a flexible mean to edit the completed face appearance. Extensive\nexperimental results qualitatively and quantitatively demonstrate that our\nnetwork is able to generate visually pleasing face completion results and edit\nface attributes as well.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 13:29:40 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 07:30:04 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Song", "Linsen", ""], ["Cao", "Jie", ""], ["Song", "Linxiao", ""], ["Hu", "Yibo", ""], ["He", "Ran", ""]]}, {"id": "1809.02983", "submitter": "Jun Fu", "authors": "Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang,\n  Hanqing Lu", "title": "Dual Attention Network for Scene Segmentation", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the scene segmentation task by capturing rich\ncontextual dependencies based on the selfattention mechanism. Unlike previous\nworks that capture contexts by multi-scale features fusion, we propose a Dual\nAttention Networks (DANet) to adaptively integrate local features with their\nglobal dependencies. Specifically, we append two types of attention modules on\ntop of traditional dilated FCN, which model the semantic interdependencies in\nspatial and channel dimensions respectively. The position attention module\nselectively aggregates the features at each position by a weighted sum of the\nfeatures at all positions. Similar features would be related to each other\nregardless of their distances. Meanwhile, the channel attention module\nselectively emphasizes interdependent channel maps by integrating associated\nfeatures among all channel maps. We sum the outputs of the two attention\nmodules to further improve feature representation which contributes to more\nprecise segmentation results. We achieve new state-of-the-art segmentation\nperformance on three challenging scene segmentation datasets, i.e., Cityscapes,\nPASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5%\non Cityscapes test set is achieved without using coarse data. We make the code\nand trained model publicly available at https://github.com/junfu1115/DANet\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 14:48:22 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 08:58:13 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 11:00:28 GMT"}, {"version": "v4", "created": "Sun, 21 Apr 2019 08:10:54 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Fu", "Jun", ""], ["Liu", "Jing", ""], ["Tian", "Haijie", ""], ["Li", "Yong", ""], ["Bao", "Yongjun", ""], ["Fang", "Zhiwei", ""], ["Lu", "Hanqing", ""]]}, {"id": "1809.03016", "submitter": "Sohom Mukherjee", "authors": "Sohom Mukherjee, Arif Ahmed, Debi Prosad Dogra, Samarjit Kar, Partha\n  Pratim Roy", "title": "Fingertip Detection and Tracking for Recognition of Air-Writing in\n  Videos", "comments": "32 pages, 10 figures, 2 tables. Submitted to Journal of Expert\n  Systems with Applications", "journal-ref": "Expert Systems with Applications Volume 136, 1 December 2019,\n  Pages 217-229", "doi": "10.1016/j.eswa.2019.06.034", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air-writing is the process of writing characters or words in free space using\nfinger or hand movements without the aid of any hand-held device. In this work,\nwe address the problem of mid-air finger writing using web-cam video as input.\nIn spite of recent advances in object detection and tracking, accurate and\nrobust detection and tracking of the fingertip remains a challenging task,\nprimarily due to small dimension of the fingertip. Moreover, the initialization\nand termination of mid-air finger writing is also challenging due to the\nabsence of any standard delimiting criterion. To solve these problems, we\npropose a new writing hand pose detection algorithm for initialization of\nair-writing using the Faster R-CNN framework for accurate hand detection\nfollowed by hand segmentation and finally counting the number of raised fingers\nbased on geometrical properties of the hand. Further, we propose a robust\nfingertip detection and tracking approach using a new signature function called\ndistance-weighted curvature entropy. Finally, a fingertip velocity-based\ntermination criterion is used as a delimiter to mark the completion of the\nair-writing gesture. Experiments show the superiority of the proposed fingertip\ndetection and tracking algorithm over state-of-the-art approaches giving a mean\nprecision of 73.1 % while achieving real-time performance at 18.5 fps, a\ncondition which is of vital importance to air-writing. Character recognition\nexperiments give a mean accuracy of 96.11 % using the proposed air-writing\nsystem, a result which is comparable to that of existing handwritten character\nrecognition systems.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 18:10:59 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Mukherjee", "Sohom", ""], ["Ahmed", "Arif", ""], ["Dogra", "Debi Prosad", ""], ["Kar", "Samarjit", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1809.03036", "submitter": "Anand Gopalakrishnan", "authors": "Anand Gopalakrishnan, Ankur Mali, Dan Kifer, C. Lee Giles and\n  Alexander G. Ororbia", "title": "A Neural Temporal Model for Human Motion Prediction", "comments": "accepted to cvpr 2019", "journal-ref": "In Proceedings of the IEEE Conference on Computer Vision and\n  Pattern Recognition, pp. 12116-12125. 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose novel neural temporal models for predicting and synthesizing human\nmotion, achieving state-of-the-art in modeling long-term motion trajectories\nwhile being competitive with prior work in short-term prediction and requiring\nsignificantly less computation. Key aspects of our proposed system include: 1)\na novel, two-level processing architecture that aids in generating planned\ntrajectories, 2) a simple set of easily computable features that integrate\nderivative information, and 3) a novel multi-objective loss function that helps\nthe model to slowly progress from simple next-step prediction to the harder\ntask of multi-step, closed-loop prediction. Our results demonstrate that these\ninnovations improve the modeling of long-term motion trajectories. Finally, we\npropose a novel metric, called Normalized Power Spectrum Similarity (NPSS), to\nevaluate the long-term predictive ability of motion synthesis models,\ncomplementing the popular mean-squared error (MSE) measure of Euler joint\nangles over time. We conduct a user study to determine if the proposed NPSS\ncorrelates with human evaluation of long-term motion more strongly than MSE and\nfind that it indeed does. We release code and additional results\n(visualizations) for this paper at:\nhttps://github.com/cr7anand/neural_temporal_models\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 20:12:32 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 20:30:59 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 05:02:36 GMT"}, {"version": "v4", "created": "Thu, 6 Dec 2018 08:20:04 GMT"}, {"version": "v5", "created": "Fri, 22 Nov 2019 14:08:15 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Gopalakrishnan", "Anand", ""], ["Mali", "Ankur", ""], ["Kifer", "Dan", ""], ["Giles", "C. Lee", ""], ["Ororbia", "Alexander G.", ""]]}, {"id": "1809.03044", "submitter": "Alexander Kuhnle", "authors": "Alexander Kuhnle, Huiyuan Xie, Ann Copestake", "title": "How clever is the FiLM model, and how clever can it be?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FiLM model achieves close-to-perfect performance on the diagnostic CLEVR\ndataset and is distinguished from other such models by having a comparatively\nsimple and easily transferable architecture. In this paper, we investigate in\nmore detail the ability of FiLM to learn various linguistic constructions. Our\nmain results show that (a) FiLM is not able to learn relational statements\nstraight away except for very simple instances, (b) training on a broader set\nof instances as well as pretraining on simpler instance types can help\nalleviate these learning difficulties, (c) mixing is less robust than\npretraining and very sensitive to the compositional structure of the dataset.\nOverall, our results suggest that the approach of big all-encompassing datasets\nand the paradigm of \"the effectiveness of data\" may have fundamental\nlimitations.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 21:08:57 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Kuhnle", "Alexander", ""], ["Xie", "Huiyuan", ""], ["Copestake", "Ann", ""]]}, {"id": "1809.03050", "submitter": "Dafang He", "authors": "Dafang He, Xiao Yang, Daniel Kifer, C.Lee Giles", "title": "TextContourNet: a Flexible and Effective Framework for Improving Scene\n  Text Detection Architecture with a Multi-task Cascade", "comments": "9 pages(including references); WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of extracting text instance contour information from\nimages and use it to assist scene text detection. We propose a novel and\neffective framework for this and experimentally demonstrate that: (1) A CNN\nthat can be effectively used to extract instance-level text contour from\nnatural images. (2) The extracted contour information can be used for better\nscene text detection. We propose two ways for learning the contour task\ntogether with the scene text detection: (1) as an auxiliary task and (2) as\nmulti-task cascade. Extensive experiments with different benchmark datasets\ndemonstrate that both designs improve the performance of a state-of-the-art\nscene text detector and that a multi-task cascade design achieves the best\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 22:31:37 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 04:42:39 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["He", "Dafang", ""], ["Yang", "Xiao", ""], ["Kifer", "Daniel", ""], ["Giles", "C. Lee", ""]]}, {"id": "1809.03055", "submitter": "T\\\"urker Tuncer", "authors": "Turker Tuncer", "title": "LDW-SCSA: Logistic Dynamic Weight based Sine Cosine Search Algorithm for\n  Numerical Functions Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle swarm optimization (PSO) and Sine Cosine algorithm (SCA) have been\nwidely used optimization methods but these methods have some disadvantages such\nas trapped local optimum point. In order to solve this problem and obtain more\nsuccessful results than others, a novel logistic dynamic weight based sine\ncosine search algorithm (LDW-SCSA) is presented in this paper. In the LDW-SCSA\nmethod, logistic map is used as dynamic weight generator. Logistic map is one\nof the famous and widely used chaotic map in the literature. Search process of\nSCA is modified in the LDW-SCSA. To evaluate performance of the LDW-SCSA, the\nwidely used numerical benchmark functions were utilized as test suite and other\nswarm optimization methods were used to obtain the comparison results. Superior\nperformances of the LDW-SCSA are proved success of this method.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 22:41:57 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Tuncer", "Turker", ""]]}, {"id": "1809.03119", "submitter": "Alex James Dr", "authors": "Kazybek Adam, Kamilya Smagulova, Alex Pappachen James", "title": "Memristive LSTM network hardware architecture for time-series predictive\n  modeling problem", "comments": "IEEE Asia Pacific Conference on Circuits and Systems, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of time-series data allows to identify long-term trends and make\npredictions that can help to improve our lives. With the rapid development of\nartificial neural networks, long short-term memory (LSTM) recurrent neural\nnetwork (RNN) configuration is found to be capable in dealing with time-series\nforecasting problems where data points are time-dependent and possess\nseasonality trends. Gated structure of LSTM cell and flexibility in network\ntopology (one-to-many, many-to-one, etc.) allows to model systems with multiple\ninput variables and control several parameters such as the size of the\nlook-back window to make a prediction and number of time steps to be predicted.\nThese make LSTM attractive tool over conventional methods such as\nautoregression models, the simple average, moving average, naive approach,\nARIMA, Holt's linear trend method, Holt's Winter seasonal method, and others.\nIn this paper, we propose a hardware implementation of LSTM network\narchitecture for time-series forecasting problem. All simulations were\nperformed using TSMC 0.18um CMOS technology and HP memristor model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 03:35:33 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Adam", "Kazybek", ""], ["Smagulova", "Kamilya", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1809.03137", "submitter": "Zhen He", "authors": "Zhen He, Jian Li, Daxue Liu, Hangen He, David Barber", "title": "Tracking by Animation: Unsupervised Learning of Multi-Object Attentive\n  Trackers", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Multi-Object Tracking (MOT) from videos is a challenging computer\nvision task which has been extensively studied for decades. Most of the\nexisting MOT algorithms are based on the Tracking-by-Detection (TBD) paradigm\ncombined with popular machine learning approaches which largely reduce the\nhuman effort to tune algorithm parameters. However, the commonly used\nsupervised learning approaches require the labeled data (e.g., bounding boxes),\nwhich is expensive for videos. Also, the TBD framework is usually suboptimal\nsince it is not end-to-end, i.e., it considers the task as detection and\ntracking, but not jointly. To achieve both label-free and end-to-end learning\nof MOT, we propose a Tracking-by-Animation framework, where a differentiable\nneural model first tracks objects from input frames and then animates these\nobjects into reconstructed frames. Learning is then driven by the\nreconstruction error through backpropagation. We further propose a\nReprioritized Attentive Tracking to improve the robustness of data association.\nExperiments conducted on both synthetic and real video datasets show the\npotential of the proposed model. Our project page is publicly available at:\nhttps://github.com/zhen-he/tracking-by-animation\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 04:59:25 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 05:39:56 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 02:02:16 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["He", "Zhen", ""], ["Li", "Jian", ""], ["Liu", "Daxue", ""], ["He", "Hangen", ""], ["Barber", "David", ""]]}, {"id": "1809.03140", "submitter": "Venkateswararao Cherukuri", "authors": "Venkateswararao Cherukuri, Tiantong Guo, Steven J. Schiff, Vishal\n  Monga", "title": "Deep MR Image Super-Resolution Using Structural Priors", "comments": "Accepted to IEEE ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution magnetic resonance (MR) images are desired for accurate\ndiagnostics. In practice, image resolution is restricted by factors like\nhardware, cost and processing constraints. Recently, deep learning methods have\nbeen shown to produce compelling state of the art results for image\nsuper-resolution. Paying particular attention to desired hi-resolution MR image\nstructure, we propose a new regularized network that exploits image priors,\nnamely a low-rank structure and a sharpness prior to enhance deep MR image\nsuperresolution. Our contributions are then incorporating these priors in an\nanalytically tractable fashion in the learning of a convolutional neural\nnetwork (CNN) that accomplishes the super-resolution task. This is particularly\nchallenging for the low rank prior, since the rank is not a differentiable\nfunction of the image matrix (and hence the network parameters), an issue we\naddress by pursuing differentiable approximations of the rank. Sharpness is\nemphasized by the variance of the Laplacian which we show can be implemented by\na fixed {\\em feedback} layer at the output of the network. Experiments\nperformed on two publicly available MR brain image databases exhibit promising\nresults particularly when training imagery is limited.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 05:20:26 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Cherukuri", "Venkateswararao", ""], ["Guo", "Tiantong", ""], ["Schiff", "Steven J.", ""], ["Monga", "Vishal", ""]]}, {"id": "1809.03171", "submitter": "Chris H. Bahnsen", "authors": "Chris H. Bahnsen, Andreas M{\\o}gelmose, Thomas B. Moeslund", "title": "The AAU Multimodal Annotation Toolboxes: Annotating Objects in Images\n  and Videos", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tech report gives an introduction to two annotation toolboxes that\nenable the creation of pixel and polygon-based masks as well as bounding boxes\naround objects of interest. Both toolboxes support the annotation of sequential\nimages in the RGB and thermal modalities. Each annotated object is assigned a\nclassification tag, a unique ID, and one or more optional meta data tags. The\ntoolboxes are written in C++ with the OpenCV and Qt libraries and are operated\nby using the visual interface and the extensive range of keyboard shortcuts.\nPre-built binaries are available for Windows and MacOS and the tools can be\nbuilt from source under Linux as well. So far, tens of thousands of frames have\nbeen annotated using the toolboxes.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 08:09:42 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Bahnsen", "Chris H.", ""], ["M\u00f8gelmose", "Andreas", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "1809.03175", "submitter": "Guangming Wu", "authors": "Guangming Wu and Zhiling Guo", "title": "Geoseg: A Computer Vision Package for Automatic Building Segmentation\n  and Outline Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning algorithms, especially fully convolutional network\nbased methods, are becoming very popular in the field of remote sensing.\nHowever, these methods are implemented and evaluated through various datasets\nand deep learning frameworks. There has not been a package that covers these\nmethods in a unifying manner. In this study, we introduce a computer vision\npackage termed Geoseg that focus on building segmentation and outline\nextraction. Geoseg implements over nine state-of-the-art models as well as\nutility scripts needed to conduct model training, logging, evaluating and\nvisualization. The implementation of Geoseg emphasizes unification, simplicity,\nand flexibility. The performance and computational efficiency of all\nimplemented methods are evaluated by comparison experiment through a unified,\nhigh-quality aerial image dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 08:29:42 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Wu", "Guangming", ""], ["Guo", "Zhiling", ""]]}, {"id": "1809.03185", "submitter": "Francesco La Rosa", "authors": "Francesco La Rosa, M\\'ario Jo\\~ao Fartaria, Tobias Kober, Jonas\n  Richiardi, Cristina Granziera, Jean-Philippe Thiran, Meritxell Bach Cuadra", "title": "Shallow vs deep learning architectures for white matter lesion\n  segmentation in the early stages of multiple sclerosis", "comments": "Accepted to the MICCAI 2018 Brain Lesion (BrainLes) workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a comparison of a shallow and a deep learning\narchitecture for the automated segmentation of white matter lesions in MR\nimages of multiple sclerosis patients. In particular, we train and test both\nmethods on early stage disease patients, to verify their performance in\nchallenging conditions, more similar to a clinical setting than what is\ntypically provided in multiple sclerosis segmentation challenges. Furthermore,\nwe evaluate a prototype naive combination of the two methods, which refines the\nfinal segmentation. All methods were trained on 32 patients, and the evaluation\nwas performed on a pure test set of 73 cases. Results show low lesion-wise\nfalse positives (30%) for the deep learning architecture, whereas the shallow\narchitecture yields the best Dice coefficient (63%) and volume difference\n(19%). Combining both shallow and deep architectures further improves the\nlesion-wise metrics (69% and 26% lesion-wise true and false positive rate,\nrespectively).\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 08:50:34 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["La Rosa", "Francesco", ""], ["Fartaria", "M\u00e1rio Jo\u00e3o", ""], ["Kober", "Tobias", ""], ["Richiardi", "Jonas", ""], ["Granziera", "Cristina", ""], ["Thiran", "Jean-Philippe", ""], ["Cuadra", "Meritxell Bach", ""]]}, {"id": "1809.03193", "submitter": "Jean Ogier Du Terrail", "authors": "Shivang Agarwal, Jean Ogier Du Terrail, Fr\\'ed\\'eric Jurie", "title": "Recent Advances in Object Detection in the Age of Deep Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection-the computer vision task dealing with detecting instances of\nobjects of a certain class (e.g., 'car', 'plane', etc.) in images-attracted a\nlot of attention from the community during the last 5 years. This strong\ninterest can be explained not only by the importance this task has for many\napplications but also by the phenomenal advances in this area since the arrival\nof deep convolutional neural networks (DCNN). This article reviews the recent\nliterature on object detection with deep CNN, in a comprehensive way, and\nprovides an in-depth view of these recent advances. The survey covers not only\nthe typical architectures (SSD, YOLO, Faster-RCNN) but also discusses the\nchallenges currently met by the community and goes on to show how the problem\nof object detection can be extended. This survey also reviews the public\ndatasets and associated state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 09:09:12 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 11:58:17 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Agarwal", "Shivang", ""], ["Terrail", "Jean Ogier Du", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1809.03218", "submitter": "Silvia-Laura Pintea", "authors": "Silvia L. Pintea, Jian Zheng, Xilin Li, Paulina J.M. Bank, Jacobus J.\n  van Hilten, Jan C. van Gemert", "title": "Hand-tremor frequency estimation in videos", "comments": "Best paper at ECCV-2018 Workshop on Observing and Understanding Hands\n  in Action", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of estimating human hand-tremor frequency from input\nRGB video data. Estimating tremors from video is important for non-invasive\nmonitoring, analyzing and diagnosing patients suffering from motor-disorders\nsuch as Parkinson's disease. We consider two approaches for hand-tremor\nfrequency estimation: (a) a Lagrangian approach where we detect the hand at\nevery frame in the video, and estimate the tremor frequency along the\ntrajectory; and (b) an Eulerian approach where we first localize the hand, we\nsubsequently remove the large motion along the movement trajectory of the hand,\nand we use the video information over time encoded as intensity values or phase\ninformation to estimate the tremor frequency.\n  We estimate hand tremors on a new human tremor dataset, TIM-Tremor,\ncontaining static tasks as well as a multitude of more dynamic tasks, involving\nlarger motion of the hands. The dataset has 55 tremor patient recordings\ntogether with: associated ground truth accelerometer data from the most\naffected hand, RGB video data, and aligned depth data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 09:43:19 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Pintea", "Silvia L.", ""], ["Zheng", "Jian", ""], ["Li", "Xilin", ""], ["Bank", "Paulina J. M.", ""], ["van Hilten", "Jacobus J.", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "1809.03239", "submitter": "Huazhu Fu", "authors": "Huazhu Fu, Yanwu Xu, Stephen Lin, Damon Wing Kee Wong, Baskaran Mani,\n  Meenakshi Mahesh, Tin Aung, Jiang Liu", "title": "Multi-Context Deep Network for Angle-Closure Glaucoma Screening in\n  Anterior Segment OCT", "comments": "Accepted by Medical Image Computing and Computer Assisted\n  Intervention (MICCAI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major cause of irreversible visual impairment is angle-closure glaucoma,\nwhich can be screened through imagery from Anterior Segment Optical Coherence\nTomography (AS-OCT). Previous computational diagnostic techniques address this\nscreening problem by extracting specific clinical measurements or handcrafted\nvisual features from the images for classification. In this paper, we instead\npropose to learn from training data a discriminative representation that may\ncapture subtle visual cues not modeled by predefined features. Based on\nclinical priors, we formulate this learning with a presented Multi-Context Deep\nNetwork (MCDN) architecture, in which parallel Convolutional Neural Networks\nare applied to particular image regions and at corresponding scales known to be\ninformative for clinically diagnosing angle-closure glaucoma. The output\nfeature maps of the parallel streams are merged into a classification layer to\nproduce the deep screening result. Moreover, we incorporate estimated clinical\nparameters to further enhance performance. On a clinical AS-OCT dataset, our\nsystem is validated through comparisons to previous screening methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 11:26:33 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Fu", "Huazhu", ""], ["Xu", "Yanwu", ""], ["Lin", "Stephen", ""], ["Wong", "Damon Wing Kee", ""], ["Mani", "Baskaran", ""], ["Mahesh", "Meenakshi", ""], ["Aung", "Tin", ""], ["Liu", "Jiang", ""]]}, {"id": "1809.03258", "submitter": "Silvia-Laura Pintea", "authors": "Omar Hommos, Silvia L. Pintea, Pascal S.M. Mettes, Jan C. van Gemert", "title": "Using phase instead of optical flow for action recognition", "comments": "ECCV-2018 Workshop on \"What is Optical Flow for?\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the most common motion representation for action recognition is\noptical flow. Optical flow is based on particle tracking which adheres to a\nLagrangian perspective on dynamics. In contrast to the Lagrangian perspective,\nthe Eulerian model of dynamics does not track, but describes local changes. For\nvideo, an Eulerian phase-based motion representation, using complex steerable\nfilters, has been successfully employed recently for motion magnification and\nvideo frame interpolation. Inspired by these previous works, here, we proposes\nlearning Eulerian motion representations in a deep architecture for action\nrecognition. We learn filters in the complex domain in an end-to-end manner. We\ndesign these complex filters to resemble complex Gabor filters, typically\nemployed for phase-information extraction. We propose a phase-information\nextraction module, based on these complex filters, that can be used in any\nnetwork architecture for extracting Eulerian representations. We experimentally\nanalyze the added value of Eulerian motion representations, as extracted by our\nproposed phase extraction module, and compare with existing motion\nrepresentations based on optical flow, on the UCF101 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 12:07:17 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 12:35:09 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Hommos", "Omar", ""], ["Pintea", "Silvia L.", ""], ["Mettes", "Pascal S. M.", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "1809.03259", "submitter": "Shideh Rezaeifar Mrs", "authors": "Shideh Rezaeifar, Olga Taran, Slava Voloshynovskiy", "title": "Classification by Re-generation: Towards Classification Based on\n  Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Deep Neural Networks (DNNs) are considered the state-of-the-art in many\nclassification tasks, the question of their semantic generalizations has been\nraised. To address semantic interpretability of learned features, we introduce\na novel idea of classification by re-generation based on variational\nautoencoder (VAE) in which a separate encoder-decoder pair of VAE is trained\nfor each class. Moreover, the proposed architecture overcomes the scalability\nissue in current DNN networks as there is no need to re-train the whole network\nwith the addition of new classes and it can be done for each class separately.\nWe also introduce a criterion based on Kullback-Leibler divergence to reject\ndoubtful examples. This rejection criterion should improve the trust in the\nobtained results and can be further exploited to reject adversarial examples.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 12:08:52 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Rezaeifar", "Shideh", ""], ["Taran", "Olga", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "1809.03298", "submitter": "Kong Zhaoming", "authors": "Zhaoming Kong, Xiaowei Yang", "title": "A Brief Review of Real-World Color Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering real-world color images is challenging due to the complexity of\nnoise that can not be formulated as a certain distribution. However, the rapid\ndevelopment of camera lens pos- es greater demands on image denoising in terms\nof both efficiency and effectiveness. Currently, the most widely accepted\nframework employs the combination of transform domain techniques and nonlocal\nsimilarity characteristics of natural images. Based on this framework, many\ncompetitive methods model the correlation of R, G, B channels with pre-defined\nor adaptively learned transforms. In this chapter, a brief review of related\nmethods and publicly available datasets is presented, moreover, a new dataset\nthat includes more natural outdoor scenes is introduced. Extensive experiments\nare performed and discussion on visual effect enhancement is included.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 13:30:15 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Kong", "Zhaoming", ""], ["Yang", "Xiaowei", ""]]}, {"id": "1809.03305", "submitter": "Yue Pan", "authors": "Yue Pan", "title": "Landslide Monitoring based on Terrestrial Laser Scanning: A Novel\n  Semi-automatic Workflow", "comments": "10 pages, 13 figures ,3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a workflow that uses Terrestrial Laser\nScanning(TLS) to semi-automatically monitor landslide and then test it in\npractice. Firstly, several groups of TLS stations are set on different time to\ncollect the raw point cloud of the object mountain. Next, Hierarchical Merging\nBased Multi-view (HMMR) registration algorithm is adapted to accomplish\nsingle-phase multi-view registration.In order to analyze deformation between\nmultiple periods, Iterative Global Similarity Point (IGSP) algorithm is applied\nto accomplish multiple-phase registration, which outperforms ICP in\nexperiments. Then the cloth simulation filtering (CSF) algorithm was used\ntogether with manual post-processing to remove vegetation on the slope. After\nthat, the mountain slope's digital terrain model (DTM) is generated for each\nperiod, and the distance between adjacent DTMs are calculated as the landslide\ndeformation mass. Furthermore, average deformation rate of the landslide\nsurface is calculated and analyzed.To validate the effectiveness of proposed\nworkflow, we uses the TLS data of five periods of the landslide in the Shanhou\nvillage of northern Changshan Island from 2013 to 2015. The results indicate\nthat the method can obtain centimeter-level deformation monitoring accuracy\nwhich can effectively monitor and analyze long-term landslide morphology and\ntrend as well as position the significant deformation area and determine the\ntype of landslide. In addition, the process can be automated to provide\nend-to-end TLS based long-term landslide monitoring applications, providing\nreference for monitoring and early warning of potential landslides.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 06:04:53 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Pan", "Yue", ""]]}, {"id": "1809.03306", "submitter": "Kuntoro Adi Nugroho", "authors": "Kuntoro Adi Nugroho", "title": "A Comparison of Handcrafted and Deep Neural Network Feature Extraction\n  for Classifying Optical Coherence Tomography (OCT) Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical Coherence Tomography allows ophthalmologist to obtain cross-section\nimaging of eye retina. Assisted with digital image analysis methods, effective\ndisease detection could be performed. Various methods exist to extract feature\nfrom OCT images. The proposed study aims to compare the effectiveness of\nhandcrafted and deep neural network features. The evaluated dataset consist of\n32339 instances distributed in four classes, namely CNV, DME, DRUSEN, and\nNORMAL. The methods are Histogram of Oriented Gradient (HOG), Local Binary\nPattern (LBP), DenseNet-169, and ResNet50. As a result, the deep neural network\nbased methods outperformed the handcrafted feature with 88% and 89% accuracy\nfor DenseNet and ResNet compared to 50 % and 42 % for HOG and LBP respectively.\nThe deep neural network based methods also demonstrated better result on the\nunder represented class.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 03:18:17 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Nugroho", "Kuntoro Adi", ""]]}, {"id": "1809.03308", "submitter": "Fang Liu", "authors": "Fang Liu, Li Feng, Richard Kijowski", "title": "MANTIS: Model-Augmented Neural neTwork with Incoherent k-space Sampling\n  for efficient MR T2 mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative mapping of magnetic resonance (MR) parameters have been shown as\nvaluable methods for improved assessment of a range of diseases. Due to the\nneed to image an anatomic structure multiple times, parameter mapping usually\nrequires long scan times compared to conventional static imaging. Therefore,\naccelerated parameter mapping is highly-desirable and remains a topic of great\ninterest in the MR research community. While many recent deep learning methods\nhave focused on highly efficient image reconstruction for conventional static\nMR imaging, applications of deep learning for dynamic imaging and in particular\naccelerated parameter mapping have been limited. The purpose of this work was\nto develop and evaluate a novel deep learning-based reconstruction framework\ncalled Model-Augmented Neural neTwork with Incoherent k-space Sampling (MANTIS)\nfor efficient MR parameter mapping. Our approach combines end-to-end CNN\nmapping with k-space consistency using the concept of cyclic loss to further\nenforce data and model fidelity. Incoherent k-space sampling is used to improve\nreconstruction performance. A physical model is incorporated into the proposed\nframework, so that the parameter maps can be efficiently estimated directly\nfrom undersampled images. The performance of MANTIS was demonstrated for the\nspin-spin relaxation time (T2) mapping of the knee joint. Compared to\nconventional reconstruction approaches that exploited image sparsity, MANTIS\nyielded lower errors and higher similarity with respect to the reference in the\nT2 estimation. Our study demonstrated that the proposed MANTIS framework, with\na combination of end-to-end CNN mapping, signal model-augmented data\nconsistency, and incoherent k-space sampling, represents a promising approach\nfor efficient MR parameter mapping. MANTIS can potentially be extended to other\ntypes of parameter mapping with appropriate models.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 21:43:49 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Liu", "Fang", ""], ["Feng", "Li", ""], ["Kijowski", "Richard", ""]]}, {"id": "1809.03313", "submitter": "Xiaohua Huang", "authors": "Xiaohua Huang and Abhinav Dhall and Roland Goecke and Matti\n  Pietikainen and Guoying Zhao", "title": "A Global Alignment Kernel based Approach for Group-level Happiness\n  Intensity Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the progress in automatic human behavior understanding, analysing the\nperceived affect of multiple people has been recieved interest in affective\ncomputing community. Unlike conventional facial expression analysis, this paper\nprimarily focuses on analysing the behaviour of multiple people in an image.\nThe proposed method is based on support vector regression with the combined\nglobal alignment kernels (GAKs) to estimate the happiness intensity of a group\nof people. We first exploit Riesz-based volume local binary pattern (RVLBP) and\ndeep convolutional neural network (CNN) based features for characterizing\nfacial images. Furthermore, we propose to use the GAK for RVLBP and deep CNN\nfeatures, respectively for explicitly measuring the similarity of two\ngroup-level images. Specifically, we exploit the global weight sort scheme to\nsort the face images from group-level image according to their spatial weights,\nmaking an efficient data structure to GAK. Lastly, we propose Multiple kernel\nlearning based on three combination strategies for combining two respective\nGAKs based on RVLBP and deep CNN features, such that enhancing the\ndiscriminative ability of each GAK. Intensive experiments are performed on the\nchallenging group-level happiness intensity database, namely HAPPEI. Our\nexperimental results demonstrate that the proposed approach achieves promising\nperformance for group happiness intensity analysis, when compared with the\nrecent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 20:23:30 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Huang", "Xiaohua", ""], ["Dhall", "Abhinav", ""], ["Goecke", "Roland", ""], ["Pietikainen", "Matti", ""], ["Zhao", "Guoying", ""]]}, {"id": "1809.03314", "submitter": "Xiaofan Yu", "authors": "Xiaofan Yu, Runze Yu, Jingsong Yang, Xiaohui Duan", "title": "A Robotic Auto-Focus System based on Deep Reinforcement Learning", "comments": "To Appear at ICARCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering its advantages in dealing with high-dimensional visual input and\nlearning control policies in discrete domain, Deep Q Network (DQN) could be an\nalternative method of traditional auto-focus means in the future. In this\npaper, based on Deep Reinforcement Learning, we propose an end-to-end approach\nthat can learn auto-focus policies from visual input and finish at a clear spot\nautomatically. We demonstrate that our method - discretizing the action space\nwith coarse to fine steps and applying DQN is not only a solution to auto-focus\nbut also a general approach towards vision-based control problems. Separate\nphases of training in virtual and real environments are applied to obtain an\neffective model. Virtual experiments, which are carried out after the virtual\ntraining phase, indicates that our method could achieve 100% accuracy on a\ncertain view with different focus range. Further training on real robots could\neliminate the deviation between the simulator and real scenario, leading to\nreliable performances in real applications.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 02:14:50 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Yu", "Xiaofan", ""], ["Yu", "Runze", ""], ["Yang", "Jingsong", ""], ["Duan", "Xiaohui", ""]]}, {"id": "1809.03316", "submitter": "Farzaneh Mahdisoltani", "authors": "Farzaneh Mahdisoltani, Roland Memisevic, David Fleet", "title": "Hierarchical Video Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a hierarchical architecture for video understanding that\nexploits the structure of real world actions by capturing targets at different\nlevels of granularity. We design the model such that it first learns simpler\ncoarse-grained tasks, and then moves on to learn more fine-grained targets. The\nmodel is trained with a joint loss on different granularity levels. We\ndemonstrate empirical results on the recent release of Something-Something\ndataset, which provides a hierarchy of targets, namely coarse-grained action\ngroups, fine-grained action categories, and captions. Experiments suggest that\nmodels that exploit targets at different levels of granularity achieve better\nperformance on all levels.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 02:29:06 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Mahdisoltani", "Farzaneh", ""], ["Memisevic", "Roland", ""], ["Fleet", "David", ""]]}, {"id": "1809.03318", "submitter": "Ruizhe Zhao", "authors": "Ruizhe Zhao, Ho-Cheung Ng, Wayne Luk and Xinyu Niu", "title": "Towards Efficient Convolutional Neural Network for Domain-Specific\n  Applications on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGA becomes a popular technology for implementing Convolutional Neural\nNetwork (CNN) in recent years. Most CNN applications on FPGA are\ndomain-specific, e.g., detecting objects from specific categories, in which\ncommonly-used CNN models pre-trained on general datasets may not be efficient\nenough. This paper presents TuRF, an end-to-end CNN acceleration framework to\nefficiently deploy domain-specific applications on FPGA by transfer learning\nthat adapts pre-trained models to specific domains, replacing standard\nconvolution layers with efficient convolution blocks, and applying layer fusion\nto enhance hardware design performance. We evaluate TuRF by deploying a\npre-trained VGG-16 model for a domain-specific image recognition task onto a\nStratix V FPGA. Results show that designs generated by TuRF achieve better\nperformance than prior methods for the original VGG-16 and ResNet-50 models,\nwhile for the optimised VGG-16 model TuRF designs are more accurate and easier\nto process.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 13:58:22 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Zhao", "Ruizhe", ""], ["Ng", "Ho-Cheung", ""], ["Luk", "Wayne", ""], ["Niu", "Xinyu", ""]]}, {"id": "1809.03322", "submitter": "Jonathan Heras", "authors": "\\'Angela Casado and J\\'onathan Heras", "title": "Guiding the Creation of Deep Learning-based Object Detectors", "comments": "To be published in I Workshop en Deep Learning of the CAEPIA\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a computer vision field that has applications in several\ncontexts ranging from biomedicine and agriculture to security. In the last\nyears, several deep learning techniques have greatly improved object detection\nmodels. Among those techniques, we can highlight the YOLO approach, that allows\nthe construction of accurate models that can be employed in real-time\napplications. However, as most deep learning techniques, YOLO has a steep\nlearning curve and creating models using this approach might be challenging for\nnon-expert users. In this work, we tackle this problem by constructing a suite\nof Jupyter notebooks that democratizes the construction of object detection\nmodels using YOLO. The suitability of our approach has been proven with a\ndataset of stomata images where we have achieved a mAP of 90.91%.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 07:07:12 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Casado", "\u00c1ngela", ""], ["Heras", "J\u00f3nathan", ""]]}, {"id": "1809.03326", "submitter": "Gwang-Il Ri", "authors": "Gwang-Il Ri, Mun-Chol Kim, Su-Rim Ji", "title": "A Stable Minutia Descriptor based on Gabor Wavelet and Linear\n  Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The minutia descriptor which describes characteristics of minutia, plays a\nmajor role in fingerprint recognition. Typically, fingerprint recognition\nsystems employ minutia descriptors to find potential correspondence between\nminutiae, and they use similarity between two minutia descriptors to calculate\noverall similarity between two fingerprint images. A good minutia descriptor\ncan improve recognition accuracy of fingerprint recognition system and largely\nreduce comparing time. A good minutia descriptor should have high ability to\ndistinguish between different minutiae and at the same time should be robust in\ndifficult conditions including poor quality image and small size image. It also\nshould be effective in computational cost of similarity among descriptors. In\nthis paper, a robust minutia descriptor is constructed using Gabor wavelet and\nlinear discriminant analysis. This minutia descriptor has high distinguishing\nability, stability and simple comparing method. Experimental results on FVC2004\nand FVC2006 databases show that the proposed minutia descriptor is very\neffective in fingerprint recognition.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 10:46:19 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Ri", "Gwang-Il", ""], ["Kim", "Mun-Chol", ""], ["Ji", "Su-Rim", ""]]}, {"id": "1809.03327", "submitter": "Ning Xu", "authors": "Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang,\n  Jianchao Yang, and Thomas Huang", "title": "YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark", "comments": "Dataset Report. arXiv admin note: substantial text overlap with\n  arXiv:1809.00461", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning long-term spatial-temporal features are critical for many video\nanalysis tasks. However, existing video segmentation methods predominantly rely\non static image segmentation techniques, and methods capturing temporal\ndependency for segmentation have to depend on pretrained optical flow models,\nleading to suboptimal solutions for the problem. End-to-end sequential learning\nto explore spatialtemporal features for video segmentation is largely limited\nby the scale of available video segmentation datasets, i.e., even the largest\nvideo segmentation dataset only contains 90 short video clips. To solve this\nproblem, we build a new large-scale video object segmentation dataset called\nYouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains\n4,453 YouTube video clips and 94 object categories. This is by far the largest\nvideo object segmentation dataset to our knowledge and has been released at\nhttp://youtube-vos.org. We further evaluate several existing state-of-the-art\nvideo object segmentation algorithms on this dataset which aims to establish\nbaselines for the development of new algorithms in the future.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 04:19:45 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Xu", "Ning", ""], ["Yang", "Linjie", ""], ["Fan", "Yuchen", ""], ["Yue", "Dingcheng", ""], ["Liang", "Yuchen", ""], ["Yang", "Jianchao", ""], ["Huang", "Thomas", ""]]}, {"id": "1809.03334", "submitter": "Jianfeng Zhang", "authors": "Jianfeng Zhang, Liezhuo Zhang, Yuankai Teng, Xiaoping Zhang, Song\n  Wang, Lili Ju", "title": "Interactive Binary Image Segmentation with Edge Preservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary image segmentation plays an important role in computer vision and has\nbeen widely used in many applications such as image and video editing, object\nextraction, and photo composition. In this paper, we propose a novel\ninteractive binary image segmentation method based on the Markov Random Field\n(MRF) framework and the fast bilateral solver (FBS) technique. Specifically, we\nemploy the geodesic distance component to build the unary term. To ensure both\ncomputation efficiency and effective responsiveness for interactive\nsegmentation, superpixels are used in computing geodesic distances instead of\npixels. Furthermore, we take a bilateral affinity approach for the pairwise\nterm in order to preserve edge information and denoise. Through the alternating\ndirection strategy, the MRF energy minimization problem is divided into two\nsubproblems, which then can be easily solved by steepest gradient descent (SGD)\nand FBS respectively. Experimental results on the VGG interactive image\nsegmentation dataset show that the proposed algorithm outperforms several\nstate-of-the-art ones, and in particular, it can achieve satisfactory\nedge-smooth segmentation results even when the foreground and background color\nappearances are quite indistinctive.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 14:14:21 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Zhang", "Jianfeng", ""], ["Zhang", "Liezhuo", ""], ["Teng", "Yuankai", ""], ["Zhang", "Xiaoping", ""], ["Wang", "Song", ""], ["Ju", "Lili", ""]]}, {"id": "1809.03336", "submitter": "Le Thanh Nguyen-Meidine", "authors": "Le Thanh Nguyen-Meidine, Eric Granger, Madhu Kiran, Louis-Antoine\n  Blais-Morin", "title": "A Comparison of CNN-based Face and Head Detectors for Real-Time Video\n  Surveillance Applications", "comments": null, "journal-ref": null, "doi": "10.1109/IPTA.2017.8310113", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting faces and heads appearing in video feeds are challenging tasks in\nreal-world video surveillance applications due to variations in appearance,\nocclusions and complex backgrounds. Recently, several CNN architectures have\nbeen proposed to increase the accuracy of detectors, although their\ncomputational complexity can be an issue, especially for real-time\napplications, where faces and heads must be detected live using high-resolution\ncameras. This paper compares the accuracy and complexity of state-of-the-art\nCNN architectures that are suitable for face and head detection. Single pass\nand region-based architectures are reviewed and compared empirically to\nbaseline techniques according to accuracy and to time and memory complexity on\nimages from several challenging datasets. The viability of these architectures\nis analyzed with real-time video surveillance applications in mind. Results\nsuggest that, although CNN architectures can achieve a very high level of\naccuracy compared to traditional detectors, their computational cost can\nrepresent a limitation for many practical real-time applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 14:14:34 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Nguyen-Meidine", "Le Thanh", ""], ["Granger", "Eric", ""], ["Kiran", "Madhu", ""], ["Blais-Morin", "Louis-Antoine", ""]]}, {"id": "1809.03355", "submitter": "Adri\\`a Recasens", "authors": "Adri\\`a Recasens, Petr Kellnhofer, Simon Stent, Wojciech Matusik and\n  Antonio Torralba", "title": "Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks", "comments": "European Conference on Computer Vision, 2018, 14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a saliency-based distortion layer for convolutional neural\nnetworks that helps to improve the spatial sampling of input data for a given\ntask. Our differentiable layer can be added as a preprocessing block to\nexisting task networks and trained altogether in an end-to-end fashion. The\neffect of the layer is to efficiently estimate how to sample from the original\ndata in order to boost task performance. For example, for an image\nclassification task in which the original data might range in size up to\nseveral megapixels, but where the desired input images to the task network are\nmuch smaller, our layer learns how best to sample from the underlying high\nresolution data in a manner which preserves task-relevant information better\nthan uniform downsampling. This has the effect of creating distorted,\ncaricature-like intermediate images, in which idiosyncratic elements of the\nimage that improve task performance are zoomed and exaggerated. Unlike\nalternative approaches such as spatial transformer networks, our proposed layer\nis inspired by image saliency, computed efficiently from uniformly downsampled\ndata, and degrades gracefully to a uniform sampling strategy under uncertainty.\nWe apply our layer to improve existing networks for the tasks of human gaze\nestimation and fine-grained object classification. Code for our method is\navailable in: http://github.com/recasens/Saliency-Sampler\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 14:36:15 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Recasens", "Adri\u00e0", ""], ["Kellnhofer", "Petr", ""], ["Stent", "Simon", ""], ["Matusik", "Wojciech", ""], ["Torralba", "Antonio", ""]]}, {"id": "1809.03363", "submitter": "Ethan Harris", "authors": "Ethan Harris, Matthew Painter and Jonathon Hare", "title": "Torchbearer: A Model Fitting Library for PyTorch", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce torchbearer, a model fitting library for pytorch aimed at\nresearchers working on deep learning or differentiable programming. The\ntorchbearer library provides a high level metric and callback API that can be\nused for a wide range of applications. We also include a series of built in\ncallbacks that can be used for: model persistence, learning rate decay,\nlogging, data visualization and more. The extensive documentation includes an\nexample library for deep learning and dynamic programming problems and can be\nfound at http://torchbearer.readthedocs.io. The code is licensed under the MIT\nLicense and available at https://github.com/ecs-vlc/torchbearer.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 14:46:35 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Harris", "Ethan", ""], ["Painter", "Matthew", ""], ["Hare", "Jonathon", ""]]}, {"id": "1809.03385", "submitter": "Dicong Qiu", "authors": "Dicong Qiu", "title": "SPASS: Scientific Prominence Active Search System with Deep Image\n  Captioning Network", "comments": "9 pages, 5 figures, 1 table. Preprint. Work in progress", "journal-ref": "Planetary and Space Science, 2020, 118: 104943", "doi": "10.1016/j.pss.2020.104943", "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Planetary exploration missions with Mars rovers are complicated, which\ngenerally require elaborated task planning by human experts, from the path to\ntake to the images to capture. NASA has been using this process to acquire over\n22 million images from the planet Mars. In order to improve the degree of\nautomation and thus efficiency in this process, we propose a system for\nplanetary rovers to actively search for prominence of prespecified scientific\nfeatures in captured images. Scientists can prespecify such search tasks in\nnatural language and upload them to a rover, on which the deployed system\nconstantly captions captured images with a deep image captioning network and\ncompare the auto-generated captions to the prespecified search tasks by certain\nmetrics so as to prioritize those images for transmission. As a beneficial side\neffect, the proposed system can also be deployed to ground-based planetary data\nsystems as a content-based search engine.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:18:37 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Qiu", "Dicong", ""]]}, {"id": "1809.03408", "submitter": "Ravi Shekhar", "authors": "Ravi Shekhar, Aashish Venkatesh, Tim Baumg\\\"artner, Elia Bruni,\n  Barbara Plank, Raffaella Bernardi and Raquel Fern\\'andez", "title": "Beyond task success: A closer look at jointly learning to see, ask, and\n  GuessWhat", "comments": "Accepted to NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a grounded dialogue state encoder which addresses a foundational\nissue on how to integrate visual grounding with dialogue system components. As\na test-bed, we focus on the GuessWhat?! game, a two-player game where the goal\nis to identify an object in a complex visual scene by asking a sequence of\nyes/no questions. Our visually-grounded encoder leverages synergies between\nguessing and asking questions, as it is trained jointly using multi-task\nlearning. We further enrich our model via a cooperative learning regime. We\nshow that the introduction of both the joint architecture and cooperative\nlearning lead to accuracy improvements over the baseline system. We compare our\napproach to an alternative system which extends the baseline with reinforcement\nlearning. Our in-depth analysis shows that the linguistic skills of the two\nmodels differ dramatically, despite approaching comparable performance levels.\nThis points at the importance of analyzing the linguistic output of competing\nsystems beyond numeric comparison solely based on task success.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:46:58 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 08:01:14 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Shekhar", "Ravi", ""], ["Venkatesh", "Aashish", ""], ["Baumg\u00e4rtner", "Tim", ""], ["Bruni", "Elia", ""], ["Plank", "Barbara", ""], ["Bernardi", "Raffaella", ""], ["Fern\u00e1ndez", "Raquel", ""]]}, {"id": "1809.03415", "submitter": "Shichao Yang", "authors": "Shichao Yang, Sebastian Scherer", "title": "Monocular Object and Plane SLAM in Structured Environments", "comments": "IEEE Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2019.2924848", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a monocular Simultaneous Localization and Mapping\n(SLAM) algorithm using high-level object and plane landmarks. The built map is\ndenser, more compact and semantic meaningful compared to feature point based\nSLAM. We first propose a high order graphical model to jointly infer the 3D\nobject and layout planes from single images considering occlusions and semantic\nconstraints. The extracted objects and planes are further optimized with camera\nposes in a unified SLAM framework. Objects and planes can provide more semantic\nconstraints such as Manhattan plane and object supporting relationships\ncompared to points. Experiments on various public and collected datasets\nincluding ICL NUIM and TUM Mono show that our algorithm can improve camera\nlocalization accuracy compared to state-of-the-art SLAM especially when there\nis no loop closure, and also generate dense maps robustly in many structured\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 15:51:13 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 04:52:08 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Yang", "Shichao", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1809.03443", "submitter": "Jun Zhang", "authors": "Jun Zhang", "title": "Inverse-Consistent Deep Networks for Unsupervised Deformable Image\n  Registration", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable image registration is a fundamental task in medical image\nanalysis, aiming to establish a dense and non-linear correspondence between a\npair of images. Previous deep-learning studies usually employ supervised neural\nnetworks to directly learn the spatial transformation from one image to\nanother, requiring task-specific ground-truth registration for model training.\nDue to the difficulty in collecting precise ground-truth registration,\nimplementation of these supervised methods is practically challenging. Although\nseveral unsupervised networks have been recently developed, these methods\nusually ignore the inherent inverse-consistent property (essential for\ndiffeomorphic mapping) of transformations between a pair of images. Also,\nexisting approaches usually encourage the to-be-estimated transformation to be\nlocally smooth via a smoothness constraint only, which could not completely\navoid folding in the resulting transformation. To this end, we propose an\nInverse-Consistent deep Network (ICNet) for unsupervised deformable image\nregistration. Specifically, we develop an inverse-consistent constraint to\nencourage that a pair of images are symmetrically deformed toward one another,\nuntil both warped images are matched. Besides using the conventional smoothness\nconstraint, we also propose an anti-folding constraint to further avoid folding\nin the transformation. The proposed method does not require any supervision\ninformation, while encouraging the diffeomoprhic property of the transformation\nvia the proposed inverse-consistent and anti-folding constraints. We evaluate\nour method on T1-weighted brain magnetic resonance imaging (MRI) scans for\ntissue segmentation and anatomical landmark detection, with results\ndemonstrating the superior performance of our ICNet over several\nstate-of-the-art approaches for deformable image registration. Our code will be\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 16:30:28 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Zhang", "Jun", ""]]}, {"id": "1809.03451", "submitter": "Hanqing Wang", "authors": "Hanqing Wang, Jiaolong Yang, Wei Liang, Xin Tong", "title": "Deep Single-View 3D Object Reconstruction with Visual Hull Embedding", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object reconstruction is a fundamental task of many robotics and AI\nproblems. With the aid of deep convolutional neural networks (CNNs), 3D object\nreconstruction has witnessed a significant progress in recent years. However,\npossibly due to the prohibitively high dimension of the 3D object space, the\nresults from deep CNNs are often prone to missing some shape details. In this\npaper, we present an approach which aims to preserve more shape details and\nimprove the reconstruction quality. The key idea of our method is to leverage\nobject mask and pose estimation from CNNs to assist the 3D shape learning by\nconstructing a probabilistic single-view visual hull inside of the network. Our\nmethod works by first predicting a coarse shape as well as the object pose and\nsilhouette using CNNs, followed by a novel 3D refinement CNN which refines the\ncoarse shapes using the constructed probabilistic visual hulls. Experiment on\nboth synthetic data and real images show that embedding a single-view visual\nhull for shape refinement can significantly improve the reconstruction quality\nby recovering more shapes details and improving shape consistency with the\ninput image.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 16:49:36 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Wang", "Hanqing", ""], ["Yang", "Jiaolong", ""], ["Liang", "Wei", ""], ["Tong", "Xin", ""]]}, {"id": "1809.03470", "submitter": "Marek Wydmuch", "authors": "Marek Wydmuch, Micha{\\l} Kempka, Wojciech Ja\\'skowski", "title": "ViZDoom Competitions: Playing Doom from Pixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first two editions of Visual Doom AI Competition,\nheld in 2016 and 2017. The challenge was to create bots that compete in a\nmulti-player deathmatch in a first-person shooter (FPS) game, Doom. The bots\nhad to make their decisions based solely on visual information, i.e., a raw\nscreen buffer. To play well, the bots needed to understand their surroundings,\nnavigate, explore, and handle the opponents at the same time. These aspects,\ntogether with the competitive multi-agent aspect of the game, make the\ncompetition a unique platform for evaluating the state of the art reinforcement\nlearning algorithms. The paper discusses the rules, solutions, results, and\nstatistics that give insight into the agents' behaviors. Best-performing agents\nare described in more detail. The results of the competition lead to the\nconclusion that, although reinforcement learning can produce capable Doom bots,\nthey still are not yet able to successfully compete against humans in this\ngame. The paper also revisits the ViZDoom environment, which is a flexible,\neasy to use, and efficient 3D platform for research for vision-based\nreinforcement learning, based on a well-recognized first-person perspective\ngame Doom.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 17:41:39 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Wydmuch", "Marek", ""], ["Kempka", "Micha\u0142", ""], ["Ja\u015bkowski", "Wojciech", ""]]}, {"id": "1809.03539", "submitter": "Maarten Wijntjes", "authors": "Maarten W.A. Wijntjes", "title": "Annotating shadows, highlights and faces: the contribution of a 'human\n  in the loop' for digital art history", "comments": "Presented at the \"1st KDD Workshop on Data Science for Digital Art\n  History: tackling big data Challenges, Algorithms, and Systems\", see\n  http://dsdah2018.blogs.dsv.su.se for more info. Manuscript should eventually\n  be published in Journal of Digital Art History (www.dah-journal.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While automatic computational techniques appear to reveal novel insights in\ndigital art history, a complementary approach seems to get less attention: that\nof human annotation. We argue and exemplify that a 'human in the loop' can\nreveal insights that may be difficult to detect automatically. Specifically, we\nfocussed on perceptual aspects within pictorial art. Using rather simple\nannotation tasks (e.g. delineate human lengths, indicate highlights and\nclassify gaze direction) we could both replicate earlier findings and reveal\nnovel insights into pictorial conventions. We found that Canaletto depicted\nhuman figures in rather accurate perspective, varied viewpoint elevation\nbetween approximately 3 and 9 meters and highly preferred light directions\nparallel to the projection plane. Furthermore, we found that taking the\naveraged images of leftward looking faces reveals a woman, and for rightward\nlooking faces showed a male, confirming earlier accounts on lateral gender bias\nin pictorial art. Lastly, we confirmed and refined the well-known\nlight-from-the-left bias. Together, the annotations, analyses and results\nexemplify how human annotation can contribute and complement to technical and\ndigital art history.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 18:34:22 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Wijntjes", "Maarten W. A.", ""]]}, {"id": "1809.03550", "submitter": "Jakub Mare\\v{c}ek", "authors": "Albert Akhriev and Jakub Marecek and Andrea Simonetto", "title": "Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and\n  Measurement Noise", "comments": "20 pages; camera-ready version + appendices", "journal-ref": "Proceedings of the Thirty-Fourth AAAI Conference on Artificial\n  Intelligence, 2020", "doi": null, "report-no": null, "categories": "math.OC cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tracking of time-varying low-rank models of time-varying matrices, we\npresent a method robust to both uniformly-distributed measurement noise and\narbitrarily-distributed ``sparse'' noise. In theory, we bound the tracking\nerror. In practice, our use of randomised coordinate descent is scalable and\nallows for encouraging results on changedetection net, a benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 19:00:34 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 13:16:17 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 16:22:33 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Akhriev", "Albert", ""], ["Marecek", "Jakub", ""], ["Simonetto", "Andrea", ""]]}, {"id": "1809.03576", "submitter": "Xia Zhu", "authors": "Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat\n  Kaul, Theodore L. Willke", "title": "Out-of-Distribution Detection Using an Ensemble of Self Supervised\n  Leave-out Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning methods form a critical part in commercially important\napplications such as autonomous driving and medical diagnostics, it is\nimportant to reliably detect out-of-distribution (OOD) inputs while employing\nthese algorithms. In this work, we propose an OOD detection algorithm which\ncomprises of an ensemble of classifiers. We train each classifier in a\nself-supervised manner by leaving out a random subset of training data as OOD\ndata and the rest as in-distribution (ID) data. We propose a novel margin-based\nloss over the softmax output which seeks to maintain at least a margin $m$\nbetween the average entropy of the OOD and in-distribution samples. In\nconjunction with the standard cross-entropy loss, we minimize the novel loss to\ntrain an ensemble of classifiers. We also propose a novel method to combine the\noutputs of the ensemble of classifiers to obtain OOD detection score and class\nprediction. Overall, our method convincingly outperforms Hendrycks et al.[7]\nand the current state-of-the-art ODIN[13] on several OOD detection benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 16:00:08 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Vyas", "Apoorv", ""], ["Jammalamadaka", "Nataraj", ""], ["Zhu", "Xia", ""], ["Das", "Dipankar", ""], ["Kaul", "Bharat", ""], ["Willke", "Theodore L.", ""]]}, {"id": "1809.03605", "submitter": "Wonhui Kim", "authors": "Wonhui Kim, Manikandasriram Srinivasan Ramanagopal, Charles Barto,\n  Ming-Yuan Yu, Karl Rosaen, Nick Goumas, Ram Vasudevan and Matthew\n  Johnson-Roberson", "title": "PedX: Benchmark Dataset for Metric 3D Pose Estimation of Pedestrians in\n  Complex Urban Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel dataset titled PedX, a large-scale multimodal\ncollection of pedestrians at complex urban intersections. PedX consists of more\nthan 5,000 pairs of high-resolution (12MP) stereo images and LiDAR data along\nwith providing 2D and 3D labels of pedestrians. We also present a novel 3D\nmodel fitting algorithm for automatic 3D labeling harnessing constraints across\ndifferent modalities and novel shape and temporal priors. All annotated 3D\npedestrians are localized into the real-world metric space, and the generated\n3D models are validated using a mocap system configured in a controlled outdoor\nenvironment to simulate pedestrians in urban intersections. We also show that\nthe manual 2D labels can be replaced by state-of-the-art automated labeling\napproaches, thereby facilitating automatic generation of large scale datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 21:31:42 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Kim", "Wonhui", ""], ["Ramanagopal", "Manikandasriram Srinivasan", ""], ["Barto", "Charles", ""], ["Yu", "Ming-Yuan", ""], ["Rosaen", "Karl", ""], ["Goumas", "Nick", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1809.03609", "submitter": "Mohamed Ibrahim", "authors": "Mohamed R. Ibrahim, James Haworth, Tao Cheng", "title": "URBAN-i: From urban scenes to mapping slums, transport modes, and\n  pedestrians in cities using deep learning and computer vision", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": "10.1177/2399808319846517", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Within the burgeoning expansion of deep learning and computer vision across\nthe different fields of science, when it comes to urban development, deep\nlearning and computer vision applications are still limited towards the notions\nof smart cities and autonomous vehicles. Indeed, a wide gap of knowledge\nappears when it comes to cities and urban regions in less developed countries\nwhere the chaos of informality is the dominant scheme. How can deep learning\nand Artificial Intelligence (AI) untangle the complexities of informality to\nadvance urban modelling and our understanding of cities? Various questions and\ndebates can be raised concerning the future of cities of the North and the\nSouth in the paradigm of AI and computer vision. In this paper, we introduce a\nnew method for multipurpose realistic-dynamic urban modelling relying on deep\nlearning and computer vision, using deep Convolutional Neural Networks (CNN),\nto sense and detect informality and slums in urban scenes from aerial and\nstreet view images in addition to detection of pedestrian and transport modes.\nThe model has been trained on images of urban scenes in cities across the\nglobe. The model shows a good validation of understanding a wide spectrum of\nnuances among the planned and the unplanned regions, including informal and\nslum areas. We attempt to advance urban modelling for better understanding the\ndynamics of city developments. We also aim to exemplify the significant impacts\nof AI in cities beyond how smart cities are discussed and perceived in the\nmainstream. The algorithms of the URBAN-i model are fully-coded in Python\nprogramming with the pre-trained deep learning models to be used as a tool for\nmapping and city modelling in the various corner of the globe, including\ninformal settlements and slum regions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 21:49:38 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ibrahim", "Mohamed R.", ""], ["Haworth", "James", ""], ["Cheng", "Tao", ""]]}, {"id": "1809.03625", "submitter": "Aaron Chadha", "authors": "Aaron Chadha, Yiannis Andreopoulos", "title": "Improved Techniques for Adversarial Discriminative Domain Adaptation", "comments": "To appear in IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial discriminative domain adaptation (ADDA) is an efficient framework\nfor unsupervised domain adaptation in image classification, where the source\nand target domains are assumed to have the same classes, but no labels are\navailable for the target domain. We investigate whether we can improve\nperformance of ADDA with a new framework and new loss formulations. Following\nthe framework of semi-supervised GANs, we first extend the discriminator output\nover the source classes, in order to model the joint distribution over domain\nand task. We thus leverage on the distribution over the source encoder\nposteriors (which is fixed during adversarial training) and propose maximum\nmean discrepancy (MMD) and reconstruction-based loss functions for aligning the\ntarget encoder distribution to the source domain. We compare and provide a\ncomprehensive analysis of how our framework and loss formulations extend over\nsimple multi-class extensions of ADDA and other discriminative variants of\nsemi-supervised GANs. In addition, we introduce various forms of regularization\nfor stabilizing training, including treating the discriminator as a denoising\nautoencoder and regularizing the target encoder with source examples to reduce\noverfitting under a contraction mapping (i.e., when the target per-class\ndistributions are contracting during alignment with the source). Finally, we\nvalidate our framework on standard domain adaptation datasets, such as SVHN and\nMNIST. We also examine how our framework benefits recognition problems based on\nmodalities that lack training data, by introducing and evaluating on a\nneuromorphic vision sensing (NVS) sign language recognition dataset, where the\nsource and target domains constitute emulated and real neuromorphic spike\nevents respectively. Our results on all datasets show that our proposal\ncompetes or outperforms the state-of-the-art in unsupervised domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 22:56:55 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 16:20:42 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 21:21:10 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Chadha", "Aaron", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1809.03658", "submitter": "Weipeng Xu", "authors": "Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Hyeongwoo Kim, Florian\n  Bernard, Marc Habermann, Wenping Wang, Christian Theobalt", "title": "Neural Rendering and Reenactment of Human Actor Videos", "comments": "ACM ToG paper. Project page:\n  http://gvv.mpi-inf.mpg.de/projects/wxu/HumanReenactment/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for generating video-realistic animations of real humans\nunder user control. In contrast to conventional human character rendering, we\ndo not require the availability of a production-quality photo-realistic 3D\nmodel of the human, but instead rely on a video sequence in conjunction with a\n(medium-quality) controllable 3D template model of the person. With that, our\napproach significantly reduces production cost compared to conventional\nrendering approaches based on production-quality 3D models, and can also be\nused to realistically edit existing videos. Technically, this is achieved by\ntraining a neural network that translates simple synthetic images of a human\ncharacter into realistic imagery. For training our networks, we first track the\n3D motion of the person in the video using the template model, and subsequently\ngenerate a synthetically rendered version of the video. These images are then\nused to train a conditional generative adversarial network that translates\nsynthetic images of the 3D model into realistic imagery of the human. We\nevaluate our method for the reenactment of another person that is tracked in\norder to obtain the motion data, and show video results generated from\nartist-designed skeleton motion. Our results outperform the state-of-the-art in\nlearning-based human image synthesis. Project page:\nhttp://gvv.mpi-inf.mpg.de/projects/wxu/HumanReenactment/\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 02:29:26 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 08:55:03 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 19:11:51 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Liu", "Lingjie", ""], ["Xu", "Weipeng", ""], ["Zollhoefer", "Michael", ""], ["Kim", "Hyeongwoo", ""], ["Bernard", "Florian", ""], ["Habermann", "Marc", ""], ["Wang", "Wenping", ""], ["Theobalt", "Christian", ""]]}, {"id": "1809.03668", "submitter": "Alex Biddulph", "authors": "Alexander Biddulph, Trent Houlistion, Alexandre Mendes, Stephan K.\n  Chalup", "title": "Comparing Computing Platforms for Deep Learning on a Humanoid Robot", "comments": "12 pages, 5 figures", "journal-ref": "Neural Information Processing, 11307 (2018), 120-131", "doi": "10.1007/978-3-030-04239-4_11", "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this study is to test two different computing platforms with\nrespect to their suitability for running deep networks as part of a humanoid\nrobot software system. One of the platforms is the CPU-centered Intel NUC7i7BNH\nand the other is a NVIDIA Jetson TX2 system that puts more emphasis on GPU\nprocessing. The experiments addressed a number of benchmarking tasks including\npedestrian detection using deep neural networks. Some of the results were\nunexpected but demonstrate that platforms exhibit both advantages and\ndisadvantages when taking computational performance and electrical power\nrequirements of such a system into account.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 03:25:36 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2019 23:38:47 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Biddulph", "Alexander", ""], ["Houlistion", "Trent", ""], ["Mendes", "Alexandre", ""], ["Chalup", "Stephan K.", ""]]}, {"id": "1809.03669", "submitter": "Xiaolin Song", "authors": "Xiaolin Song, Cuiling Lan, Wenjun Zeng, Junliang Xing, Jingyu Yang and\n  Xiaoyan Sun", "title": "Temporal-Spatial Mapping for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have enjoyed great success for image related computer\nvision tasks like image classification and object detection. For video related\ntasks like human action recognition, however, the advancements are not as\nsignificant yet. The main challenge is the lack of effective and efficient\nmodels in modeling the rich temporal spatial information in a video. We\nintroduce a simple yet effective operation, termed Temporal-Spatial Mapping\n(TSM), for capturing the temporal evolution of the frames by jointly analyzing\nall the frames of a video. We propose a video level 2D feature representation\nby transforming the convolutional features of all frames to a 2D feature map,\nreferred to as VideoMap. With each row being the vectorized feature\nrepresentation of a frame, the temporal-spatial features are compactly\nrepresented, while the temporal dynamic evolution is also well embedded. Based\non the VideoMap representation, we further propose a temporal attention model\nwithin a shallow convolutional neural network to efficiently exploit the\ntemporal-spatial dynamics. The experiment results show that the proposed scheme\nachieves the state-of-the-art performance, with 4.2% accuracy gain over\nTemporal Segment Network (TSN), a competing baseline method, on the challenging\nhuman action benchmark dataset HMDB51.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 03:29:28 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Song", "Xiaolin", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Xing", "Junliang", ""], ["Yang", "Jingyu", ""], ["Sun", "Xiaoyan", ""]]}, {"id": "1809.03676", "submitter": "Jonathan Balloch", "authors": "Jonathan C Balloch, Varun Agrawal, Irfan Essa, Sonia Chernova", "title": "Unbiasing Semantic Segmentation For Robot Perception using Synthetic\n  Data Feature Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot perception systems need to perform reliable image segmentation in\nreal-time on noisy, raw perception data. State-of-the-art segmentation\napproaches use large CNN models and carefully constructed datasets; however,\nthese models focus on accuracy at the cost of real-time inference. Furthermore,\nthe standard semantic segmentation datasets are not large enough for training\nCNNs without augmentation and are not representative of noisy, uncurated robot\nperception data. We propose improving the performance of real-time segmentation\nframeworks on robot perception data by transferring features learned from\nsynthetic segmentation data. We show that pretraining real-time segmentation\narchitectures with synthetic segmentation data instead of ImageNet improves\nfine-tuning performance by reducing the bias learned in pretraining and closing\nthe \\textit{transfer gap} as a result. Our experiments show that our real-time\nrobot perception models pretrained on synthetic data outperform those\npretrained on ImageNet for every scale of fine-tuning data examined. Moreover,\nthe degree to which synthetic pretraining outperforms ImageNet pretraining\nincreases as the availability of robot data decreases, making our approach\nattractive for robotics domains where dataset collection is hard and/or\nexpensive.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 04:27:40 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Balloch", "Jonathan C", ""], ["Agrawal", "Varun", ""], ["Essa", "Irfan", ""], ["Chernova", "Sonia", ""]]}, {"id": "1809.03705", "submitter": "Xiaoxiao Du", "authors": "Xiaoxiao Du and Ram Vasudevan and Matthew Johnson-Roberson", "title": "Bio-LSTM: A Biomechanically Inspired Recurrent Neural Network for 3D\n  Pedestrian Pose and Gait Prediction", "comments": "Typo corrected after Eq.(2)", "journal-ref": "IEEE Robotics and Automation Letters, vol. 4, no. 2, pp.\n  1501-1508, April 2019", "doi": "10.1109/LRA.2019.2895266", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as autonomous driving, it is important to understand,\ninfer, and anticipate the intention and future behavior of pedestrians. This\nability allows vehicles to avoid collisions and improve ride safety and\nquality. This paper proposes a biomechanically inspired recurrent neural\nnetwork (Bio-LSTM) that can predict the location and 3D articulated body pose\nof pedestrians in a global coordinate frame, given 3D poses and locations\nestimated in prior frames with inaccuracy. The proposed network is able to\npredict poses and global locations for multiple pedestrians simultaneously, for\npedestrians up to 45 meters from the cameras (urban intersection scale). The\noutputs of the proposed network are full-body 3D meshes represented in Skinned\nMulti-Person Linear (SMPL) model parameters. The proposed approach relies on a\nnovel objective function that incorporates the periodicity of human walking\n(gait), the mirror symmetry of the human body, and the change of ground\nreaction forces in a human gait cycle. This paper presents prediction results\non the PedX dataset, a large-scale, in-the-wild data set collected at real\nurban intersections with heavy pedestrian traffic. Results show that the\nproposed network can successfully learn the characteristics of pedestrian gait\nand produce accurate and consistent 3D pose predictions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 07:11:32 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 17:50:07 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 15:28:14 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Du", "Xiaoxiao", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1809.03707", "submitter": "Hector Basevi", "authors": "M. Wagner, H. Basevi, R. Shetty, W. Li, M. Malinowski, M. Fritz, A.\n  Leonardis", "title": "Answering Visual What-If Questions: From Actions to Predicted Scene\n  Descriptions", "comments": "Paper: 18 pages, 5 figures, 5 tables. Supplementary material: 3\n  pages, 1 figure, 1 table. To be published in VLEASE ECCV 2018 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-depth scene descriptions and question answering tasks have greatly\nincreased the scope of today's definition of scene understanding. While such\ntasks are in principle open ended, current formulations primarily focus on\ndescribing only the current state of the scenes under consideration. In\ncontrast, in this paper, we focus on the future states of the scenes which are\nalso conditioned on actions. We posit this as a question answering task, where\nan answer has to be given about a future scene state, given observations of the\ncurrent scene, and a question that includes a hypothetical action. Our solution\nis a hybrid model which integrates a physics engine into a question answering\narchitecture in order to anticipate future scene states resulting from\nobject-object interactions caused by an action. We demonstrate first results on\nthis challenging new problem and compare to baselines, where we outperform\nfully data-driven end-to-end learning approaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 07:22:28 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 16:39:39 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Wagner", "M.", ""], ["Basevi", "H.", ""], ["Shetty", "R.", ""], ["Li", "W.", ""], ["Malinowski", "M.", ""], ["Fritz", "M.", ""], ["Leonardis", "A.", ""]]}, {"id": "1809.03721", "submitter": "Seungjoon Yang", "authors": "Jinhyeok Jang, Hyunjoong Cho, Jaehong Kim, Jaeyeon Lee, and Seungjoon\n  Yang", "title": "Deep Asymmetric Networks with a Set of Node-wise Variant Activation\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents deep asymmetric networks with a set of node-wise variant\nactivation functions. The nodes' sensitivities are affected by activation\nfunction selections such that the nodes with smaller indices become\nincreasingly more sensitive. As a result, features learned by the nodes are\nsorted by the node indices in the order of their importance. Asymmetric\nnetworks not only learn input features but also the importance of those\nfeatures. Nodes of lesser importance in asymmetric networks can be pruned to\nreduce the complexity of the networks, and the pruned networks can be retrained\nwithout incurring performance losses. We validate the feature-sorting property\nusing both shallow and deep asymmetric networks as well as deep asymmetric\nnetworks transferred from famous networks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 08:09:25 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 07:24:15 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Jang", "Jinhyeok", ""], ["Cho", "Hyunjoong", ""], ["Kim", "Jaehong", ""], ["Lee", "Jaeyeon", ""], ["Yang", "Seungjoon", ""]]}, {"id": "1809.03757", "submitter": "Kazutaka Uchida", "authors": "Kazutaka Uchida, Masayuki Tanaka, and Masatoshi Okutomi", "title": "Non-blind Image Restoration Based on Convolutional Neural Network", "comments": "Accepted by IEEE 7th Global Conference on Consumer Electronics, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image restoration processors based on convolutional neural network\n(CNN) are intensively researched because of their high performance. However,\nthey are too sensitive to the perturbation of the degradation model. They\neasily fail to restore the image whose degradation model is slightly different\nfrom the trained degradation model. In this paper, we propose a non-blind\nCNN-based image restoration processor, aiming to be robust against a\nperturbation of the degradation model compared to the blind restoration\nprocessor. Experimental comparisons demonstrate that the proposed non-blind\nCNN-based image restoration processor can robustly restore images compared to\nexisting blind CNN-based image restoration processors.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 09:05:21 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Uchida", "Kazutaka", ""], ["Tanaka", "Masayuki", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "1809.03770", "submitter": "Aaron Jackson", "authors": "Aaron S. Jackson, Chris Manafas, Georgios Tzimiropoulos", "title": "3D Human Body Reconstruction from a Single Image via Volumetric\n  Regression", "comments": "Accepted to ECCV Workshops (PeopleCap) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the use of an end-to-end Convolutional Neural Network for\ndirect reconstruction of the 3D geometry of humans via volumetric regression.\nThe proposed method does not require the fitting of a shape model and can be\ntrained to work from a variety of input types, whether it be landmarks, images\nor segmentation masks. Additionally, non-visible parts, either self-occluded or\notherwise, are still reconstructed, which is not the case with depth map\nregression. We present results that show that our method can handle both pose\nvariation and detailed reconstruction given appropriate datasets for training.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 09:53:07 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Jackson", "Aaron S.", ""], ["Manafas", "Chris", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1809.03779", "submitter": "Zenith Purisha", "authors": "Zenith Purisha, Carl Jidling, Niklas Wahlstr\\\"om, Simo S\\\"arkk\\\"a,\n  Thomas B. Sch\\\"on", "title": "Probabilistic approach to limited-data computed tomography\n  reconstruction", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/ab2e2a", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the inverse problem of reconstructing the internal\nstructure of an object from limited x-ray projections. We use a Gaussian\nprocess prior to model the target function and estimate its (hyper)parameters\nfrom measured data. In contrast to other established methods, this comes with\nthe advantage of not requiring any manual parameter tuning, which usually\narises in classical regularization strategies. Our method uses a basis function\nexpansion technique for the Gaussian process which significantly reduces the\ncomputational complexity and avoids the need for numerical integration. The\napproach also allows for reformulation of come classical regularization methods\nas Laplacian and Tikhonov regularization as Gaussian process regression, and\nhence provides an efficient algorithm and principled means for their parameter\ntuning. Results from simulated and real data indicate that this approach is\nless sensitive to streak artifacts as compared to the commonly used method of\nfiltered backprojection.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 10:16:44 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 09:07:08 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 08:30:06 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Purisha", "Zenith", ""], ["Jidling", "Carl", ""], ["Wahlstr\u00f6m", "Niklas", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1809.03782", "submitter": "Marcel Schreiber", "authors": "Marcel Schreiber, Stefan Hoermann, Klaus Dietmayer", "title": "Long-Term Occupancy Grid Prediction Using Recurrent Neural Networks", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the long-term prediction of scene evolution in a complex downtown\nscenario for automated driving based on Lidar grid fusion and recurrent neural\nnetworks (RNNs). A bird's eye view of the scene, including occupancy and\nvelocity, is fed as a sequence to a RNN which is trained to predict future\noccupancy. The nature of prediction allows generation of multiple hours of\ntraining data without the need of manual labeling. Thus, the training strategy\nand loss function is designed for long sequences of real-world data\n(unbalanced, continuously changing situations, false labels, etc.). The deep\nCNN architecture comprises convolutional long short-term memories (ConvLSTMs)\nto separate static from dynamic regions and to predict dynamic objects in\nfuture frames. Novel recurrent skip connections show the ability to predict\nsmall occluded objects, i.e. pedestrians, and occluded static regions.\nSpatio-temporal correlations between grid cells are exploited to predict\nmultimodal future paths and interactions between objects. Experiments also\nquantify improvements to our previous network, a Monte Carlo approach, and\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 10:21:39 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 12:21:02 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Schreiber", "Marcel", ""], ["Hoermann", "Stefan", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1809.03783", "submitter": "Xiao-Yun Zhou", "authors": "Xiao-Yun Zhou, Guang-Zhong Yang", "title": "Normalization in Training U-Net for 2D Biomedical Semantic Segmentation", "comments": "5 figures, 5 tables, published on IEEE Robotics and Automation\n  Letters 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D biomedical semantic segmentation is important for robotic vision in\nsurgery. Segmentation methods based on Deep Convolutional Neural Network (DCNN)\ncan out-perform conventional methods in terms of both accuracy and levels of\nautomation. One common issue in training a DCNN for biomedical semantic\nsegmentation is the internal covariate shift where the training of\nconvolutional kernels is encumbered by the distribution change of input\nfeatures, hence both the training speed and performance are decreased. Batch\nNormalization (BN) is the first proposed method for addressing internal\ncovariate shift and is widely used. Instance Normalization (IN) and Layer\nNormalization (LN) have also been proposed. Group Normalization (GN) is\nproposed more recently and has not yet been applied to 2D biomedical semantic\nsegmentation, however, no specific validations on GN were given. Most DCNNs for\nbiomedical semantic segmentation adopt BN as the normalization method by\ndefault, without reviewing its performance. In this paper, four normalization\nmethods - BN, IN, LN and GN are compared in details, specifically for 2D\nbiomedical semantic segmentation. U-Net is adopted as the basic DCNN structure.\nThree datasets regarding the Right Ventricle (RV), aorta, and Left Ventricle\n(LV) are used for the validation. The results show that detailed subdivision of\nthe feature map, i.e. GN with a large group number or IN, achieves higher\naccuracy. This accuracy improvement mainly comes from better model\ngeneralization. Codes are uploaded and maintained at Xiao-Yun Zhou's Github.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 10:27:45 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 21:08:02 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 21:12:14 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Zhou", "Xiao-Yun", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1809.03788", "submitter": "Gabriele Valvano", "authors": "Gabriele Valvano, Gianmarco Santini, Nicola Martini, Andrea Ripoli,\n  Chiara Iacconi, Dante Chiappino and Daniele Della Latta", "title": "Convolutional Neural Networks for the segmentation of microcalcification\n  in Mammography Imaging", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster of microcalcifications can be an early sign of breast cancer. In this\npaper we propose a novel approach based on convolutional neural networks for\nthe detection and segmentation of microcalcification clusters. In this work we\nused 283 mammograms to train and validate our model, obtaining an accuracy of\n98.22% in the detection of preliminary suspect regions and of 97.47% in the\nsegmentation task. Our results show how deep learning could be an effective\ntool to effectively support radiologists during mammograms examination.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 10:51:19 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Valvano", "Gabriele", ""], ["Santini", "Gianmarco", ""], ["Martini", "Nicola", ""], ["Ripoli", "Andrea", ""], ["Iacconi", "Chiara", ""], ["Chiappino", "Dante", ""], ["Della Latta", "Daniele", ""]]}, {"id": "1809.03851", "submitter": "Pieter Van Molle", "authors": "Pieter Van Molle, Miguel De Strooper, Tim Verbelen, Bert\n  Vankeirsbilck, Pieter Simoens, Bart Dhoedt", "title": "Visualizing Convolutional Neural Networks to Improve Decision Support\n  for Skin Lesion Classification", "comments": "8 pages, 6 figures, Workshop on Interpretability of Machine\n  Intelligence in Medical Image Computing at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of their state-of-the-art performance in computer vision, CNNs are\nbecoming increasingly popular in a variety of fields, including medicine.\nHowever, as neural networks are black box function approximators, it is\ndifficult, if not impossible, for a medical expert to reason about their\noutput. This could potentially result in the expert distrusting the network\nwhen he or she does not agree with its output. In such a case, explaining why\nthe CNN makes a certain decision becomes valuable information. In this paper,\nwe try to open the black box of the CNN by inspecting and visualizing the\nlearned feature maps, in the field of dermatology. We show that, to some\nextent, CNNs focus on features similar to those used by dermatologists to make\na diagnosis. However, more research is required for fully explaining their\noutput.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 13:17:38 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Van Molle", "Pieter", ""], ["De Strooper", "Miguel", ""], ["Verbelen", "Tim", ""], ["Vankeirsbilck", "Bert", ""], ["Simoens", "Pieter", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1809.03917", "submitter": "Chengyao Qian", "authors": "Chengyao Qian, Ting Liu, Hao Jiang, Zhe Wang, Pengfei Wang, Mingxin\n  Guan and Biao Sun", "title": "A Detection and Segmentation Architecture for Skin Lesion Segmentation\n  on Dermoscopy Images", "comments": "5 pages, 9 figures, Ranked 1st place in ISIC 2018 task1, title\n  updated and results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report summarises our method and validation results for the ISIC\nChallenge 2018 - Skin Lesion Analysis Towards Melanoma Detection - Task 1:\nLesion Segmentation. We present a two-stage method for lesion segmentation with\noptimised training method and ensemble post-process. Our method achieves\nstate-of-the-art performance on lesion segmentation and we win the first place\nin ISIC 2018 task1.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 14:20:15 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 06:02:20 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Qian", "Chengyao", ""], ["Liu", "Ting", ""], ["Jiang", "Hao", ""], ["Wang", "Zhe", ""], ["Wang", "Pengfei", ""], ["Guan", "Mingxin", ""], ["Sun", "Biao", ""]]}, {"id": "1809.03951", "submitter": "Sebastien Valette Dr", "authors": "R\\'emi Agier, S\\'ebastien Valette, Razmig K\\'echichian, Laurent\n  Fanton, R\\'emy Prost", "title": "Hubless keypoint-based 3D deformable groupwise registration", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2019.101564", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for Fast Registration Of image Groups (FROG),\napplied to large 3D image groups. Our approach extracts 3D SURF keypoints from\nimages, computes matched pairs of keypoints and registers the group by\nminimizing pair distances in a hubless way i.e. without computing any central\nmean image. Using keypoints significantly reduces the problem complexity\ncompared to voxel-based approaches, and enables us to provide an in-core global\noptimization, similar to the Bundle Adjustment for 3D reconstruction. As we aim\nto register images of different patients, the matching step yields many\noutliers. Then we propose a new EM-weighting algorithm which efficiently\ndiscards outliers. Global optimization is carried out with a fast gradient\ndescent algorithm. This allows our approach to robustly register large\ndatasets. The result is a set of diffeomorphic half transforms which link the\nvolumes together and can be subsequently exploited for computational anatomy\nand landmark detection. We show experimental results on whole-body CT scans,\nwith groups of up to 103 volumes. On a benchmark based on anatomical landmarks,\nour algorithm compares favorably with the star-groupwise voxel-based ANTs and\nNiftyReg approaches while being much faster. We also discuss the limitations of\nour approach for lower resolution images such as brain MRI.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 14:59:03 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 09:57:37 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 12:31:51 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Agier", "R\u00e9mi", ""], ["Valette", "S\u00e9bastien", ""], ["K\u00e9chichian", "Razmig", ""], ["Fanton", "Laurent", ""], ["Prost", "R\u00e9my", ""]]}, {"id": "1809.03972", "submitter": "Alexander Khvostikov", "authors": "Alexander Khvostikov, Karim Aderghal, Andrey Krylov, Gwenaelle\n  Catheline, Jenny Benois-Pineau", "title": "3D Inception-based CNN with sMRI and MD-DTI data fusion for Alzheimer's\n  Disease diagnostics", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.05968", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, computer-aided early diagnostics of Alzheimer's Disease\n(AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the\nsubject of extensive research. Some recent studies have shown promising results\nin the AD and MCI determination using structural and functional Magnetic\nResonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and\nDiffusion Tensor Imaging (DTI) modalities. Furthermore, fusion of imaging\nmodalities in a supervised machine learning framework has shown promising\ndirection of research.\n  In this paper we first review major trends in automatic classification\nmethods such as feature extraction based methods as well as deep learning\napproaches in medical image analysis applied to the field of Alzheimer's\nDisease diagnostics. Then we propose our own design of a 3D Inception-based\nConvolutional Neural Network (CNN) for Alzheimer's Disease diagnostics. The\nnetwork is designed with an emphasis on the interior resource utilization and\nuses sMRI and DTI modalities fusion on hippocampal ROI. The comparison with the\nconventional AlexNet-based network using data from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) dataset (http://adni.loni.usc.edu) demonstrates\nsignificantly better performance of the proposed 3D Inception-based CNN.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 18:50:01 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Khvostikov", "Alexander", ""], ["Aderghal", "Karim", ""], ["Krylov", "Andrey", ""], ["Catheline", "Gwenaelle", ""], ["Benois-Pineau", "Jenny", ""]]}, {"id": "1809.03994", "submitter": "Shao-Yuan Lo", "authors": "Ping-Rong Chen, Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan,\n  Jing-Jhih Lin", "title": "Efficient Road Lane Marking Detection with Deep Learning", "comments": "Accepted at International Conference on Digital Signal Processing\n  (DSP) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane mark detection is an important element in the road scene analysis for\nAdvanced Driver Assistant System (ADAS). Limited by the onboard computing\npower, it is still a challenge to reduce system complexity and maintain high\naccuracy at the same time. In this paper, we propose a Lane Marking Detector\n(LMD) using a deep convolutional neural network to extract robust lane marking\nfeatures. To improve its performance with a target of lower complexity, the\ndilated convolution is adopted. A shallower and thinner structure is designed\nto decrease the computational cost. Moreover, we also design post-processing\nalgorithms to construct 3rd-order polynomial models to fit into the curved\nlanes. Our system shows promising results on the captured road scenes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 15:58:48 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Chen", "Ping-Rong", ""], ["Lo", "Shao-Yuan", ""], ["Hang", "Hsueh-Ming", ""], ["Chan", "Sheng-Wei", ""], ["Lin", "Jing-Jhih", ""]]}, {"id": "1809.04067", "submitter": "Minjia Zhang", "authors": "Minjia Zhang, Yuxiong He", "title": "Zoom: SSD-based Vector Search for Optimizing Accuracy, Latency and\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of machine learning and deep learning, vector search\nbecomes instrumental to many information retrieval systems, to search and find\nbest matches to user queries based on their semantic similarities.These online\nservices require the search architecture to be both effective with high\naccuracy and efficient with low latency and memory footprint, which existing\nwork fails to offer. We develop, Zoom, a new vector search solution that\ncollaboratively optimizes accuracy, latency and memory based on a multiview\napproach. (1) A \"preview\" step generates a small set of good candidates,\nleveraging compressed vectors in memory for reduced footprint and fast lookup.\n(2) A \"fullview\" step on SSDs reranks those candidates with their full-length\nvector, striking high accuracy. Our evaluation shows that, Zoom achieves an\norder of magnitude improvements on efficiency while attaining equal or higher\naccuracy, comparing with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 23:46:33 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Zhang", "Minjia", ""], ["He", "Yuxiong", ""]]}, {"id": "1809.04094", "submitter": "Giorgos Kordopatis-Zilos Mr.", "authors": "Giorgos Kordopatis-Zilos, Symeon Papadopoulos, Ioannis Patras, Ioannis\n  Kompatsiaris", "title": "FIVR: Fine-grained Incident Video Retrieval", "comments": null, "journal-ref": "IEEE Transactions on Multimedia 2019", "doi": "10.1109/TMM.2019.2905741", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the problem of Fine-grained Incident Video Retrieval\n(FIVR). Given a query video, the objective is to retrieve all associated\nvideos, considering several types of associations that range from duplicate\nvideos to videos from the same incident. FIVR offers a single framework that\ncontains several retrieval tasks as special cases. To address the benchmarking\nneeds of all such tasks, we construct and present a large-scale annotated video\ndataset, which we call FIVR-200K, and it comprises 225,960 videos. To create\nthe dataset, we devise a process for the collection of YouTube videos based on\nmajor news events from recent years crawled from Wikipedia and deploy a\nretrieval pipeline for the automatic selection of query videos based on their\nestimated suitability as benchmarks. We also devise a protocol for the\nannotation of the dataset with respect to the four types of video associations\ndefined by FIVR. Finally, we report the results of an experimental study on the\ndataset comparing five state-of-the-art methods developed based on a variety of\nvisual descriptors, highlighting the challenges of the current problem.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 18:09:44 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 08:59:05 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Kordopatis-Zilos", "Giorgos", ""], ["Papadopoulos", "Symeon", ""], ["Patras", "Ioannis", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1809.04096", "submitter": "Felix Gonda", "authors": "Felix Gonda and Donglai Wei and Toufiq Parag and Hanspeter Pfister", "title": "Parallel Separable 3D Convolution for Video and Volumetric Data\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For video and volumetric data understanding, 3D convolution layers are widely\nused in deep learning, however, at the cost of increasing computation and\ntraining time. Recent works seek to replace the 3D convolution layer with\nconvolution blocks, e.g. structured combinations of 2D and 1D convolution\nlayers. In this paper, we propose a novel convolution block, Parallel Separable\n3D Convolution (PmSCn), which applies m parallel streams of n 2D and one 1D\nconvolution layers along different dimensions. We first mathematically justify\nthe need of parallel streams (Pm) to replace a single 3D convolution layer\nthrough tensor decomposition. Then we jointly replace consecutive 3D\nconvolution layers, common in modern network architectures, with the multiple\n2D convolution layers (Cn). Lastly, we empirically show that PmSCn is\napplicable to different backbone architectures, such as ResNet, DenseNet, and\nUNet, for different applications, such as video action recognition, MRI brain\nsegmentation, and electron microscopy segmentation. In all three applications,\nwe replace the 3D convolution layers in state-of-the art models with PmSCn and\nachieve around 14% improvement in test performance and 40% reduction in model\nsize and on average.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 18:15:20 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Gonda", "Felix", ""], ["Wei", "Donglai", ""], ["Parag", "Toufiq", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1809.04098", "submitter": "Yusuke Tsuzuku", "authors": "Yusuke Tsuzuku and Issei Sato", "title": "On the Structural Sensitivity of Deep Convolutional Networks to the\n  Directions of Fourier Basis Functions", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-agnostic quasi-imperceptible perturbations on inputs are known to\ndegrade recognition accuracy of deep convolutional networks severely. This\nphenomenon is considered to be a potential security issue. Moreover, some\nresults on statistical generalization guarantees indicate that the phenomenon\ncan be a key to improve the networks' generalization. However, the\ncharacteristics of the shared directions of such harmful perturbations remain\nunknown. Our primal finding is that convolutional networks are sensitive to the\ndirections of Fourier basis functions. We derived the property by specializing\na hypothesis of the cause of the sensitivity, known as the linearity of neural\nnetworks, to convolutional networks and empirically validated it. As a\nby-product of the analysis, we propose an algorithm to create shift-invariant\nuniversal adversarial perturbations available in black-box settings.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 18:23:09 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 03:45:23 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Tsuzuku", "Yusuke", ""], ["Sato", "Issei", ""]]}, {"id": "1809.04120", "submitter": "Chaz Firestone", "authors": "Zhenglong Zhou, Chaz Firestone", "title": "Humans can decipher adversarial images", "comments": "14 pages, 4 figures", "journal-ref": "Nature Communications, 10, 1334 (2019)", "doi": "10.1038/s41467-019-08931-6", "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How similar is the human mind to the sophisticated machine-learning systems\nthat mirror its performance? Models of object categorization based on\nconvolutional neural networks (CNNs) have achieved human-level benchmarks in\nassigning known labels to novel images. These advances promise to support\ntransformative technologies such as autonomous vehicles and machine diagnosis;\nbeyond this, they also serve as candidate models for the visual system itself\n-- not only in their output but perhaps even in their underlying mechanisms and\nprinciples. However, unlike human vision, CNNs can be \"fooled\" by adversarial\nexamples -- carefully crafted images that appear as nonsense patterns to humans\nbut are recognized as familiar objects by machines, or that appear as one\nobject to humans and a different object to machines. This seemingly extreme\ndivergence between human and machine classification challenges the promise of\nthese new advances, both as applied image-recognition systems and also as\nmodels of the human mind. Surprisingly, however, little work has empirically\ninvestigated human classification of such adversarial stimuli: Does human and\nmachine performance fundamentally diverge? Or could humans decipher such images\nand predict the machine's preferred labels? Here, we show that human and\nmachine classification of adversarial stimuli are robustly related: In eight\nexperiments on five prominent and diverse adversarial imagesets, human subjects\nreliably identified the machine's chosen label over relevant foils. This\npattern persisted for images with strong antecedent identities, and even for\nimages described as \"totally unrecognizable to human eyes\". We suggest that\nhuman intuition may be a more reliable guide to machine (mis)classification\nthan has typically been imagined, and we explore the consequences of this\nresult for minds and machines alike.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 19:39:51 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 06:47:54 GMT"}, {"version": "v3", "created": "Sun, 3 Mar 2019 18:37:06 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zhou", "Zhenglong", ""], ["Firestone", "Chaz", ""]]}, {"id": "1809.04130", "submitter": "Thomas Kruezer", "authors": "Thomas Kruezer", "title": "Magnetically Guided Capsule Endoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following research undertakes a historical review of this technology with\nspecific highlighting of its advancement in medical diagnostics as well as the\ntherapeutic functionality of wireless capsule endoscopy. Without restriction to\nthe gastrointestinal tract alone, the review will additionally investigate the\ndevelopments in the technology of micro-robots guided through the magnetic\npower and are capable of navigating through multiple forms of air and fluid\nfilled lumina as well as cavities within the body. All these capabilities are\nof use in the utilization of minimally invasive medicine.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 19:55:52 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Kruezer", "Thomas", ""]]}, {"id": "1809.04135", "submitter": "Armon Shariati", "authors": "Armon Shariati, Bernd Pfrommer, Camillo J. Taylor", "title": "Simultaneous Localization and Layout Model Selection in Manhattan Worlds", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2019.2893417", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will demonstrate how Manhattan structure can be exploited\nto transform the Simultaneous Localization and Mapping (SLAM) problem, which is\ntypically solved by a nonlinear optimization over feature positions, into a\nmodel selection problem solved by a convex optimization over higher order\nlayout structures, namely walls, floors, and ceilings. Furthermore, we show how\nour novel formulation leads to an optimization procedure that automatically\nperforms data association and loop closure and which ultimately produces the\nsimplest model of the environment that is consistent with the available\nmeasurements. We verify our method on real world data sets collected with\nvarious sensing modalities.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:04:15 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 14:48:59 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 21:38:50 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Shariati", "Armon", ""], ["Pfrommer", "Bernd", ""], ["Taylor", "Camillo J.", ""]]}, {"id": "1809.04137", "submitter": "Xin Li", "authors": "Canyu Le and Xin Li", "title": "JigsawNet: Shredded Image Reassembly using Convolutional Neural Network\n  and Loop-based Composition", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2903298", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel algorithm to reassemble an arbitrarily shredded\nimage to its original status. Existing reassembly pipelines commonly consist of\na local matching stage and a global compositions stage. In the local stage, a\nkey challenge in fragment reassembly is to reliably compute and identify\ncorrect pairwise matching, for which most existing algorithms use handcrafted\nfeatures, and hence, cannot reliably handle complicated puzzles. We build a\ndeep convolutional neural network to detect the compatibility of a pairwise\nstitching, and use it to prune computed pairwise matches. To improve the\nnetwork efficiency and accuracy, we transfer the calculation of CNN to the\nstitching region and apply a boost training strategy. In the global composition\nstage, we modify the commonly adopted greedy edge selection strategies to two\nnew loop closure based searching algorithms. Extensive experiments show that\nour algorithm significantly outperforms existing methods on solving various\npuzzles, especially those challenging ones with many fragment pieces.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:07:27 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Le", "Canyu", ""], ["Li", "Xin", ""]]}, {"id": "1809.04144", "submitter": "Pranava Madhyastha", "authors": "Pranava Madhyastha, Josiah Wang, Lucia Specia", "title": "End-to-end Image Captioning Exploits Multimodal Distributional\n  Similarity", "comments": "Published in BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We hypothesize that end-to-end neural image captioning systems work seemingly\nwell because they exploit and learn `distributional similarity' in a multimodal\nfeature space by mapping a test image to similar training images in this space\nand generating a caption from the same space. To validate our hypothesis, we\nfocus on the `image' side of image captioning, and vary the input image\nrepresentation but keep the RNN text generation component of a CNN-RNN model\nconstant. Our analysis indicates that image captioning models (i) are capable\nof separating structure from noisy input representations; (ii) suffer virtually\nno significant performance loss when a high dimensional representation is\ncompressed to a lower dimensional space; (iii) cluster images with similar\nvisual and linguistic information together. Our findings indicate that our\ndistributional similarity hypothesis holds. We conclude that regardless of the\nimage representation used image captioning systems seem to match images and\ngenerate captions in a learned joint image-text semantic subspace.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:32:21 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Madhyastha", "Pranava", ""], ["Wang", "Josiah", ""], ["Specia", "Lucia", ""]]}, {"id": "1809.04154", "submitter": "Tejas Krishna Reddy", "authors": "Tejas K, Swathi C, Rajesh Kumar M", "title": "Intensity and Rescale Invariant Copy Move Forgery Detection Techniques", "comments": "Further research is active on this paper in VIT University. Hence,\n  the paper is yet not published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this contemporary world digital media such as videos and images behave as\nan active medium to carry valuable information across the globe on all fronts.\nHowever there are several techniques evolved to tamper the image which has made\ntheir authenticity untrustworthy. CopyMove Forgery CMF is one of the most\ncommon forgeries present in an image where a cluster of pixels are duplicated\nin the same image with potential postprocessing techniques. Various\nstate-of-art techniques are developed in the recent years which are effective\nin detecting passive image forgery. However most methods do fail when the\ncopied image is rescaled or added with certain intensity before being pasted\ndue to de-synchronization of pixels in the searching process. To tackle this\nproblem the paper proposes distinct novel algorithms which recognize a unique\napproach of using Hus invariant moments and Discreet Cosine Transformations DCT\nto attain the desired rescale invariant and intensity invariant CMF detection\ntechniques respectively. The experiments conducted quantitatively and\nqualitatively demonstrate the effectiveness of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:53:01 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["K", "Tejas", ""], ["C", "Swathi", ""], ["M", "Rajesh Kumar", ""]]}, {"id": "1809.04157", "submitter": "Xu Zhang", "authors": "Xu Zhang, Felix Xinnan Yu, Svebor Karaman, Wei Zhang, Shih-Fu Chang", "title": "Heated-Up Softmax Embedding", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning aims at learning a distance which is consistent with the\nsemantic meaning of the samples. The problem is generally solved by learning an\nembedding for each sample such that the embeddings of samples of the same\ncategory are compact while the embeddings of samples of different categories\nare spread-out in the feature space. We study the features extracted from the\nsecond last layer of a deep neural network based classifier trained with the\ncross entropy loss on top of the softmax layer. We show that training\nclassifiers with different temperature values of softmax function leads to\nfeatures with different levels of compactness. Leveraging these insights, we\npropose a \"heating-up\" strategy to train a classifier with increasing\ntemperatures, leading the corresponding embeddings to achieve state-of-the-art\nperformance on a variety of metric learning benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:56:02 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Zhang", "Xu", ""], ["Yu", "Felix Xinnan", ""], ["Karaman", "Svebor", ""], ["Zhang", "Wei", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1809.04182", "submitter": "Danielle Pace", "authors": "Danielle F. Pace, Adrian V. Dalca, Tom Brosch, Tal Geva, Andrew J.\n  Powell, J\\\"urgen Weese, Mehdi H. Moghari, Polina Golland", "title": "Iterative Segmentation from Limited Training Data: Applications to\n  Congenital Heart Disease", "comments": "Presented at the Deep Learning in Medical Image Analysis Workshop,\n  MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new iterative segmentation model which can be accurately learned\nfrom a small dataset. A common approach is to train a model to directly segment\nan image, requiring a large collection of manually annotated images to capture\nthe anatomical variability in a cohort. In contrast, we develop a segmentation\nmodel that recursively evolves a segmentation in several steps, and implement\nit as a recurrent neural network. We learn model parameters by optimizing the\ninterme- diate steps of the evolution in addition to the final segmentation. To\nthis end, we train our segmentation propagation model by presenting incom-\nplete and/or inaccurate input segmentations paired with a recommended next\nstep. Our work aims to alleviate challenges in segmenting heart structures from\ncardiac MRI for patients with congenital heart disease (CHD), which encompasses\na range of morphological deformations and topological changes. We demonstrate\nthe advantages of this approach on a dataset of 20 images from CHD patients,\nlearning a model that accurately segments individual heart chambers and great\nvessels. Com- pared to direct segmentation, the iterative method yields more\naccurate segmentation for patients with the most severe CHD malformations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 22:17:14 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Pace", "Danielle F.", ""], ["Dalca", "Adrian V.", ""], ["Brosch", "Tom", ""], ["Geva", "Tal", ""], ["Powell", "Andrew J.", ""], ["Weese", "J\u00fcrgen", ""], ["Moghari", "Mehdi H.", ""], ["Golland", "Polina", ""]]}, {"id": "1809.04184", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen, Maxwell D. Collins, Yukun Zhu, George Papandreou,\n  Barret Zoph, Florian Schroff, Hartwig Adam, Jonathon Shlens", "title": "Searching for Efficient Multi-Scale Architectures for Dense Image\n  Prediction", "comments": "Accepted by NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of neural network architectures is an important component for\nachieving state-of-the-art performance with machine learning systems across a\nbroad array of tasks. Much work has endeavored to design and build\narchitectures automatically through clever construction of a search space\npaired with simple learning algorithms. Recent progress has demonstrated that\nsuch meta-learning methods may exceed scalable human-invented architectures on\nimage classification tasks. An open question is the degree to which such\nmethods may generalize to new domains. In this work we explore the construction\nof meta-learning techniques for dense image prediction focused on the tasks of\nscene parsing, person-part segmentation, and semantic image segmentation.\nConstructing viable search spaces in this domain is challenging because of the\nmulti-scale representation of visual information and the necessity to operate\non high resolution imagery. Based on a survey of techniques in dense image\nprediction, we construct a recursive search space and demonstrate that even\nwith efficient random search, we can identify architectures that outperform\nhuman-invented architectures and achieve state-of-the-art performance on three\ndense prediction tasks including 82.7\\% on Cityscapes (street scene parsing),\n71.3\\% on PASCAL-Person-Part (person-part segmentation), and 87.9\\% on PASCAL\nVOC 2012 (semantic image segmentation). Additionally, the resulting\narchitecture is more computationally efficient, requiring half the parameters\nand half the computational cost as previous state of the art systems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 22:36:01 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Collins", "Maxwell D.", ""], ["Zhu", "Yukun", ""], ["Papandreou", "George", ""], ["Zoph", "Barret", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1809.04185", "submitter": "Hao Tang", "authors": "Hao Tang, Heng Wei, Wei Xiao, Wei Wang, Dan Xu, Yan Yan, Nicu Sebe", "title": "Deep Micro-Dictionary Learning and Coding Network", "comments": "10 page, 8 figures, accepted to WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Deep Micro-Dictionary Learning and Coding\nNetwork (DDLCN). DDLCN has most of the standard deep learning layers (pooling,\nfully, connected, input/output, etc.) but the main difference is that the\nfundamental convolutional layers are replaced by novel compound dictionary\nlearning and coding layers. The dictionary learning layer learns an\nover-complete dictionary for the input training data. At the deep coding layer,\na locality constraint is added to guarantee that the activated dictionary bases\nare close to each other. Next, the activated dictionary atoms are assembled\ntogether and passed to the next compound dictionary learning and coding layers.\nIn this way, the activated atoms in the first layer can be represented by the\ndeeper atoms in the second dictionary. Intuitively, the second dictionary is\ndesigned to learn the fine-grained components which are shared among the input\ndictionary atoms. In this way, a more informative and discriminative low-level\nrepresentation of the dictionary atoms can be obtained. We empirically compare\nthe proposed DDLCN with several dictionary learning methods and deep learning\narchitectures. The experimental results on four popular benchmark datasets\ndemonstrate that the proposed DDLCN achieves competitive results compared with\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 22:36:36 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 13:03:42 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Tang", "Hao", ""], ["Wei", "Heng", ""], ["Xiao", "Wei", ""], ["Wang", "Wei", ""], ["Xu", "Dan", ""], ["Yan", "Yan", ""], ["Sebe", "Nicu", ""]]}, {"id": "1809.04187", "submitter": "Majed El Helou", "authors": "Majed El Helou, Frederike D\\\"umbgen, Radhakrishna Achanta, Sabine\n  S\\\"usstrunk", "title": "Fourier-Domain Optimization for Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image optimization problems encompass many applications such as spectral\nfusion, deblurring, deconvolution, dehazing, matting, reflection removal and\nimage interpolation, among others. With current image sizes in the order of\nmegabytes, it is extremely expensive to run conventional algorithms such as\ngradient descent, making them unfavorable especially when closed-form solutions\ncan be derived and computed efficiently. This paper explains in detail the\nframework for solving convex image optimization and deconvolution in the\nFourier domain. We begin by explaining the mathematical background and\nmotivating why the presented setups can be transformed and solved very\nefficiently in the Fourier domain. We also show how to practically use these\nsolutions, by providing the corresponding implementations. The explanations are\naimed at a broad audience with minimal knowledge of convolution and image\noptimization. The eager reader can jump to Section 3 for a footprint of how to\nsolve and implement a sample optimization function, and Section 5 for the more\ncomplex cases.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 22:38:00 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Helou", "Majed El", ""], ["D\u00fcmbgen", "Frederike", ""], ["Achanta", "Radhakrishna", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1809.04191", "submitter": "Jeffrey Mckinstry", "authors": "Jeffrey L. McKinstry, Steven K. Esser, Rathinakumar Appuswamy, Deepika\n  Bablani, John V. Arthur, Izzet B. Yildiz, Dharmendra S. Modha", "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for\n  Efficient Embedded Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To realize the promise of ubiquitous embedded deep network inference, it is\nessential to seek limits of energy and area efficiency. To this end,\nlow-precision networks offer tremendous promise because both energy and area\nscale down quadratically with the reduction in precision. Here we demonstrate\nResNet-18, -34, -50, -152, Inception-v3, Densenet-161, and VGG-16bn networks on\nthe ImageNet classification benchmark that, at 8-bit precision exceed the\naccuracy of the full-precision baseline networks after one epoch of finetuning,\nthereby leveraging the availability of pretrained models. We also demonstrate\nResNet-18, -34, -50, -152, Densenet-161, and VGG-16bn 4-bit models that match\nthe accuracy of the full-precision baseline networks -- the highest scores to\ndate. Surprisingly, the weights of the low-precision networks are very close\n(in cosine similarity) to the weights of the corresponding baseline networks,\nmaking training from scratch unnecessary.\n  We find that gradient noise due to quantization during training increases\nwith reduced precision, and seek ways to overcome this noise. The number of\niterations required by SGD to achieve a given training error is related to the\nsquare of (a) the distance of the initial solution from the final plus (b) the\nmaximum variance of the gradient estimates. Therefore, we (a) reduce solution\ndistance by starting with pretrained fp32 precision baseline networks and\nfine-tuning, and (b) combat gradient noise introduced by quantization by\ntraining longer and reducing learning rates. Sensitivity analysis indicates\nthat these simple techniques, coupled with proper activation function range\ncalibration to take full advantage of the limited precision, are sufficient to\ndiscover low-precision networks, if they exist, close to fp32 precision\nbaseline networks. The results herein provide evidence that 4-bits suffice for\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 22:51:55 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 01:49:04 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["McKinstry", "Jeffrey L.", ""], ["Esser", "Steven K.", ""], ["Appuswamy", "Rathinakumar", ""], ["Bablani", "Deepika", ""], ["Arthur", "John V.", ""], ["Yildiz", "Izzet B.", ""], ["Modha", "Dharmendra S.", ""]]}, {"id": "1809.04195", "submitter": "Javier Turek", "authors": "Michael J. Anderson, Jonathan I. Tamir, Javier S. Turek, Marcus T.\n  Alley, Theodore L. Willke, Shreyas S. Vasanawala, Michael Lustig", "title": "Clinically Deployed Distributed Magnetic Resonance Imaging\n  Reconstruction: Application to Pediatric Knee Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging is capable of producing volumetric images without\nionizing radiation. Nonetheless, long acquisitions lead to prohibitively long\nexams. Compressed sensing (CS) can enable faster scanning via sub-sampling with\nreduced artifacts. However, CS requires significantly higher reconstruction\ncomputation, limiting current clinical applications to 2D/3D or\nlimited-resolution dynamic imaging. Here we analyze the practical limitations\nto T2 Shuffling, a four-dimensional CS-based acquisition, which provides sharp\n3D-isotropic-resolution and multi-contrast images in a single scan. Our\nimprovements to the pipeline on a single machine provide a 3x overall\nreconstruction speedup, which allowed us to add algorithmic changes improving\nimage quality. Using four machines, we achieved additional 2.1x improvement\nthrough distributed parallelization. Our solution reduced the reconstruction\ntime in the hospital to 90 seconds on a 4-node cluster, enabling its use\nclinically. To understand the implications of scaling this application, we\nsimulated running our reconstructions with a multiple scanner setup typical in\nhospitals.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 23:21:48 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Anderson", "Michael J.", ""], ["Tamir", "Jonathan I.", ""], ["Turek", "Javier S.", ""], ["Alley", "Marcus T.", ""], ["Willke", "Theodore L.", ""], ["Vasanawala", "Shreyas S.", ""], ["Lustig", "Michael", ""]]}, {"id": "1809.04212", "submitter": "Junjun Jiang", "authors": "Junjun Jiang, Jiayi Ma, Zheng Wang, Chen Chen, Xianming Liu", "title": "Hyperspectral Image Classification in the Presence of Noisy Labels", "comments": "Accepted by IEEE TGRS. In this version, the Table III is revised", "journal-ref": null, "doi": "10.1109/TGRS.2018.2861992", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label information plays an important role in supervised hyperspectral image\nclassification problem. However, current classification methods all ignore an\nimportant and inevitable problem---labels may be corrupted and collecting clean\nlabels for training samples is difficult, and often impractical. Therefore, how\nto learn from the database with noisy labels is a problem of great practical\nimportance. In this paper, we study the influence of label noise on\nhyperspectral image classification, and develop a random label propagation\nalgorithm (RLPA) to cleanse the label noise. The key idea of RLPA is to exploit\nknowledge (e.g., the superpixel based spectral-spatial constraints) from the\nobserved hyperspectral images and apply it to the process of label propagation.\nSpecifically, RLPA first constructs a spectral-spatial probability transfer\nmatrix (SSPTM) that simultaneously considers the spectral similarity and\nsuperpixel based spatial information. It then randomly chooses some training\nsamples as \"clean\" samples and sets the rest as unlabeled samples, and\npropagates the label information from the \"clean\" samples to the rest unlabeled\nsamples with the SSPTM. By repeating the random assignment (of \"clean\" labeled\nsamples and unlabeled samples) and propagation, we can obtain multiple labels\nfor each training sample. Therefore, the final propagated label can be\ncalculated by a majority vote algorithm. Experimental studies show that RLPA\ncan reduce the level of noisy label and demonstrates the advantages of our\nproposed method over four major classifiers with a significant margin---the\ngains in terms of the average OA, AA, Kappa are impressive, e.g., 9.18%, 9.58%,\nand 0.1043. The Matlab source code is available at\nhttps://github.com/junjun-jiang/RLPA\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 01:20:46 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 02:02:57 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Jiang", "Junjun", ""], ["Ma", "Jiayi", ""], ["Wang", "Zheng", ""], ["Chen", "Chen", ""], ["Liu", "Xianming", ""]]}, {"id": "1809.04228", "submitter": "Avinash Kori", "authors": "Avinash Kori, Sai Saketh Chennamsetty, Mohammed Safwan K.P., and\n  Varghese Alex", "title": "Ensemble of Convolutional Neural Networks for Automatic Grading of\n  Diabetic Retinopathy and Macular Edema", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this manuscript, we automate the procedure of grading of diabetic\nretinopathy and macular edema from fundus images using an ensemble of\nconvolutional neural networks. The availability of limited amount of labeled\ndata to perform supervised learning was circumvented by using transfer learning\napproach. The models in the ensemble were pre-trained on a large dataset\ncomprising natural images and were later fine-tuned with the limited data for\nthe task of choice. For an image, the ensemble of classifiers generate multiple\npredictions, and a max-voting based approach was utilized to attain the final\ngrade of the anomaly in the image. For the task of grading DR, on the test data\n(n=56), the ensemble achieved an accuracy of 83.9\\%, while for the task for\ngrading macular edema the network achieved an accuracy of 95.45% (n=44).\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 02:29:44 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Kori", "Avinash", ""], ["Chennamsetty", "Sai Saketh", ""], ["P.", "Mohammed Safwan K.", ""], ["Alex", "Varghese", ""]]}, {"id": "1809.04282", "submitter": "Suman Sedai", "authors": "Suman Sedai, Bhavna Antony, Dwarikanath Mahapatra and Rahil Garnavi", "title": "Joint Segmentation and Uncertainty Visualization of Retinal Layers in\n  Optical Coherence Tomography Images using Bayesian Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) is commonly used to analyze retinal layers\nfor assessment of ocular diseases. In this paper, we propose a method for\nretinal layer segmentation and quantification of uncertainty based on Bayesian\ndeep learning. Our method not only performs end-to-end segmentation of retinal\nlayers, but also gives the pixel wise uncertainty measure of the segmentation\noutput. The generated uncertainty map can be used to identify erroneously\nsegmented image regions which is useful in downstream analysis. We have\nvalidated our method on a dataset of 1487 images obtained from 15 subjects (OCT\nvolumes) and compared it against the state-of-the-art segmentation algorithms\nthat does not take uncertainty into account. The proposed uncertainty based\nsegmentation method results in comparable or improved performance, and most\nimportantly is more robust against noise.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 07:22:15 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Sedai", "Suman", ""], ["Antony", "Bhavna", ""], ["Mahapatra", "Dwarikanath", ""], ["Garnavi", "Rahil", ""]]}, {"id": "1809.04320", "submitter": "Zhang Yunhua", "authors": "Yunhua Zhang, Dong Wang, Lijun Wang, Jinqing Qi, Huchuan Lu", "title": "Learning regression and verification networks for long-term visual\n  tracking", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with short-term tracking, the long-term tracking task requires\ndetermining the tracked object is present or absent, and then estimating the\naccurate bounding box if present or conducting image-wide re-detection if\nabsent. Until now, few attempts have been done although this task is much\ncloser to designing practical tracking systems. In this work, we propose a\nnovel long-term tracking framework based on deep regression and verification\nnetworks. The offline-trained regression model is designed using the\nobject-aware feature fusion and region proposal networks to generate a series\nof candidates and estimate their similarity scores effectively. The\nverification network evaluates these candidates to output the optimal one as\nthe tracked object with its classification score, which is online updated to\nadapt to the appearance variations based on newly reliable observations. The\nsimilarity and classification scores are combined to obtain a final confidence\nvalue, based on which our tracker can determine the absence of the target\naccurately and conduct image-wide re-detection to capture the target\nsuccessfully when it reappears. Extensive experiments show that our tracker\nachieves the best performance on the VOT2018 long-term challenge and\nstate-of-the-art results on the OxUvA long-term dataset.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 09:13:48 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 04:15:33 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhang", "Yunhua", ""], ["Wang", "Dong", ""], ["Wang", "Lijun", ""], ["Qi", "Jinqing", ""], ["Lu", "Huchuan", ""]]}, {"id": "1809.04344", "submitter": "Sandro Pezzelle", "authors": "Shailza Jolly and Sandro Pezzelle and Tassilo Klein and Andreas Dengel\n  and Moin Nabi", "title": "The Wisdom of MaSSeS: Majority, Subjectivity, and Semantic Similarity in\n  the Evaluation of VQA", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MASSES, a simple evaluation metric for the task of Visual\nQuestion Answering (VQA). In its standard form, the VQA task is operationalized\nas follows: Given an image and an open-ended question in natural language,\nsystems are required to provide a suitable answer. Currently, model performance\nis evaluated by means of a somehow simplistic metric: If the predicted answer\nis chosen by at least 3 human annotators out of 10, then it is 100% correct.\nThough intuitively valuable, this metric has some important limitations. First,\nit ignores whether the predicted answer is the one selected by the Majority\n(MA) of annotators. Second, it does not account for the quantitative\nSubjectivity (S) of the answers in the sample (and dataset). Third, information\nabout the Semantic Similarity (SES) of the responses is completely neglected.\nBased on such limitations, we propose a multi-component metric that accounts\nfor all these issues. We show that our metric is effective in providing a more\nfine-grained evaluation both on the quantitative and qualitative level.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 10:11:39 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Jolly", "Shailza", ""], ["Pezzelle", "Sandro", ""], ["Klein", "Tassilo", ""], ["Dengel", "Andreas", ""], ["Nabi", "Moin", ""]]}, {"id": "1809.04364", "submitter": "Mateusz Trokielewicz", "authors": "Ewelina Bartuzi and Mateusz Trokielewicz", "title": "Thermal Features for Presentation Attack Detection in Hand Biometrics", "comments": "Accepted for the BTAS2018, Special Session on Image And Video\n  Forensics In Biometrics, 22-25 Oct, 2018, Los Angeles, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for utilizing thermal features of the hand for\nthe purpose of presentation attack detection (PAD) that can be employed in a\nhand biometrics system's pipeline. By envisaging two different operational\nmodes of our system, and by employing a DCNN-based classifiers fine-tuned with\na dataset of real and fake hand representations captured in both visible and\nther- mal spectrum, we were able to bring two important deliverables. First, a\nPAD method operating in an open-set mode, capable of correctly discerning 100%\nof fake thermal samples, achieving Attack Presentation Classification Error\nRate (APCER) and Bona-Fide Presentation Classification Error Rate (BPCER) equal\nto 0%, which can be easily implemented into any existing system as a separate\ncomponent. Second, a hand biometrics system operating in a closed-set mode,\nthat has PAD built right into the recognition pipeline, and operating\nsimultaneously with the user-wise classification, achieving rank-1 recognition\naccuracy of up to 99.75%. We also show that thermal images of the human hand,\nin addition to liveness features they carry, can also improve classification\naccuracy of a biometric system, when coupled with visible light images. To\nfollow the reproducibility guidelines and to stimulate further research in this\narea, we share the trained model weights, source codes, and a newly created\ndataset of fake hand representations with interested researchers.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 11:36:17 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Bartuzi", "Ewelina", ""], ["Trokielewicz", "Mateusz", ""]]}, {"id": "1809.04403", "submitter": "Elizaveta Logacheva", "authors": "Pavel Ostyakov, Elizaveta Logacheva, Roman Suvorov, Vladimir Aliev,\n  Gleb Sterkin, Oleg Khomenko, and Sergey I. Nikolenko", "title": "Label Denoising with Large Ensembles of Heterogeneous Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in computer vision based on various convolutional\narchitectures, video understanding remains an important challenge. In this\nwork, we present and discuss a top solution for the large-scale video\nclassification (labeling) problem introduced as a Kaggle competition based on\nthe YouTube-8M dataset. We show and compare different approaches to\npreprocessing, data augmentation, model architectures, and model combination.\nOur final model is based on a large ensemble of video- and frame-level models\nbut fits into rather limiting hardware constraints. We apply an approach based\non knowledge distillation to deal with noisy labels in the original dataset and\nthe recently developed mixup technique to improve the basic models.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 13:14:59 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 20:30:28 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Ostyakov", "Pavel", ""], ["Logacheva", "Elizaveta", ""], ["Suvorov", "Roman", ""], ["Aliev", "Vladimir", ""], ["Sterkin", "Gleb", ""], ["Khomenko", "Oleg", ""], ["Nikolenko", "Sergey I.", ""]]}, {"id": "1809.04427", "submitter": "Long Chen", "authors": "Long Chen, Haizhou Ai, Zijie Zhuang, Chong Shang", "title": "Real-time Multiple People Tracking with Deeply Learned Candidate\n  Selection and Person Re-Identification", "comments": "ICME 2018", "journal-ref": null, "doi": "10.1109/ICME.2018.8486597", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online multi-object tracking is a fundamental problem in time-critical video\nanalysis applications. A major challenge in the popular tracking-by-detection\nframework is how to associate unreliable detection results with existing\ntracks. In this paper, we propose to handle unreliable detection by collecting\ncandidates from outputs of both detection and tracking. The intuition behind\ngenerating redundant candidates is that detection and tracks can complement\neach other in different scenarios. Detection results of high confidence prevent\ntracking drifts in the long term, and predictions of tracks can handle noisy\ndetection caused by occlusion. In order to apply optimal selection from a\nconsiderable amount of candidates in real-time, we present a novel scoring\nfunction based on a fully convolutional neural network, that shares most\ncomputations on the entire image. Moreover, we adopt a deeply learned\nappearance representation, which is trained on large-scale person\nre-identification datasets, to improve the identification ability of our\ntracker. Extensive experiments show that our tracker achieves real-time and\nstate-of-the-art performance on a widely used people tracking benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 13:39:27 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Long", ""], ["Ai", "Haizhou", ""], ["Zhuang", "Zijie", ""], ["Shang", "Chong", ""]]}, {"id": "1809.04430", "submitter": "Stanislav Nikolov", "authors": "Stanislav Nikolov, Sam Blackwell, Alexei Zverovitch, Ruheena Mendes,\n  Michelle Livne, Jeffrey De Fauw, Yojan Patel, Clemens Meyer, Harry Askham,\n  Bernardino Romera-Paredes, Christopher Kelly, Alan Karthikesalingam, Carlton\n  Chu, Dawn Carnell, Cheng Boon, Derek D'Souza, Syed Ali Moinuddin, Bethany\n  Garie, Yasmin McQuinlan, Sarah Ireland, Kiarna Hampton, Krystle Fuller, Hugh\n  Montgomery, Geraint Rees, Mustafa Suleyman, Trevor Back, C\\'ian Hughes,\n  Joseph R. Ledsam, Olaf Ronneberger", "title": "Deep learning to achieve clinically applicable segmentation of head and\n  neck anatomy for radiotherapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over half a million individuals are diagnosed with head and neck cancer each\nyear worldwide. Radiotherapy is an important curative treatment for this\ndisease, but it requires manual time consuming delineation of radio-sensitive\norgans at risk (OARs). This planning process can delay treatment, while also\nintroducing inter-operator variability with resulting downstream radiation dose\ndifferences. While auto-segmentation algorithms offer a potentially time-saving\nsolution, the challenges in defining, quantifying and achieving expert\nperformance remain. Adopting a deep learning approach, we demonstrate a 3D\nU-Net architecture that achieves expert-level performance in delineating 21\ndistinct head and neck OARs commonly segmented in clinical practice. The model\nwas trained on a dataset of 663 deidentified computed tomography (CT) scans\nacquired in routine clinical practice and with both segmentations taken from\nclinical practice and segmentations created by experienced radiographers as\npart of this research, all in accordance with consensus OAR definitions. We\ndemonstrate the model's clinical applicability by assessing its performance on\na test set of 21 CT scans from clinical practice, each with the 21 OARs\nsegmented by two independent experts. We also introduce surface Dice similarity\ncoefficient (surface DSC), a new metric for the comparison of organ\ndelineation, to quantify deviation between OAR surface contours rather than\nvolumes, better reflecting the clinical task of correcting errors in the\nautomated organ segmentations. The model's generalisability is then\ndemonstrated on two distinct open source datasets, reflecting different centres\nand countries to model training. With appropriate validation studies and\nregulatory approvals, this system could improve the efficiency, consistency,\nand safety of radiotherapy pathways.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 13:42:38 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 11:09:59 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 17:43:14 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Nikolov", "Stanislav", ""], ["Blackwell", "Sam", ""], ["Zverovitch", "Alexei", ""], ["Mendes", "Ruheena", ""], ["Livne", "Michelle", ""], ["De Fauw", "Jeffrey", ""], ["Patel", "Yojan", ""], ["Meyer", "Clemens", ""], ["Askham", "Harry", ""], ["Romera-Paredes", "Bernardino", ""], ["Kelly", "Christopher", ""], ["Karthikesalingam", "Alan", ""], ["Chu", "Carlton", ""], ["Carnell", "Dawn", ""], ["Boon", "Cheng", ""], ["D'Souza", "Derek", ""], ["Moinuddin", "Syed Ali", ""], ["Garie", "Bethany", ""], ["McQuinlan", "Yasmin", ""], ["Ireland", "Sarah", ""], ["Hampton", "Kiarna", ""], ["Fuller", "Krystle", ""], ["Montgomery", "Hugh", ""], ["Rees", "Geraint", ""], ["Suleyman", "Mustafa", ""], ["Back", "Trevor", ""], ["Hughes", "C\u00edan", ""], ["Ledsam", "Joseph R.", ""], ["Ronneberger", "Olaf", ""]]}, {"id": "1809.04453", "submitter": "Cl\\'ement Pinard", "authors": "Cl\\'ement Pinard and Laure Chevalley and Antoine Manzanera and David\n  Filliat", "title": "End-to-end depth from motion with stabilized monocular videos", "comments": null, "journal-ref": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial\n  Information Sciences, Volume IV-2/W3, 2017 International Conference on\n  Unmanned Aerial Vehicles in Geomatics, 4-7 September 2017, Bonn, Germany", "doi": "10.5194/isprs-annals-IV-2-W3-67-2017", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a depth map inference system from monocular videos based on a\nnovel dataset for navigation that mimics aerial footage from gimbal stabilized\nmonocular camera in rigid scenes. Unlike most navigation datasets, the lack of\nrotation implies an easier structure from motion problem which can be leveraged\nfor different kinds of tasks such as depth inference and obstacle avoidance. We\nalso propose an architecture for end-to-end depth inference with a fully\nconvolutional network. Results show that although tied to camera inner\nparameters, the problem is locally solvable and leads to good quality depth\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 13:54:48 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Pinard", "Cl\u00e9ment", ""], ["Chevalley", "Laure", ""], ["Manzanera", "Antoine", ""], ["Filliat", "David", ""]]}, {"id": "1809.04467", "submitter": "Cl\\'ement Pinard", "authors": "Cl\\'ement Pinard and Laure Chevalley and Antoine Manzanera and David\n  Filliat", "title": "Multi range Real-time depth inference from a monocular stabilized\n  footage using a Fully Convolutional Neural Network", "comments": "arXiv admin note: text overlap with arXiv:1809.04453", "journal-ref": "European Conference on Mobile Robotics 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a neural network architecture for depth map inference from monocular\nstabilized videos with application to UAV videos in rigid scenes, we propose a\nmulti-range architecture for unconstrained UAV flight, leveraging flight data\nfrom sensors to make accurate depth maps for uncluttered outdoor environment.\nWe try our algorithm on both synthetic scenes and real UAV flight data.\nQuantitative results are given for synthetic scenes with a slightly noisy\norientation, and show that our multi-range architecture improves depth\ninference. Along with this article is a video that present our results more\nthoroughly.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 14:03:33 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Pinard", "Cl\u00e9ment", ""], ["Chevalley", "Laure", ""], ["Manzanera", "Antoine", ""], ["Filliat", "David", ""]]}, {"id": "1809.04471", "submitter": "Cl\\'ement Pinard", "authors": "Cl\\'ement Pinard and Laure Chevalley and Antoine Manzanera and David\n  Filliat", "title": "Learning structure-from-motion from motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is based on a questioning of the quality metrics used by deep\nneural networks performing depth prediction from a single image, and then of\nthe usability of recently published works on unsupervised learning of depth\nfrom videos. To overcome their limitations, we propose to learn in the same\nunsupervised manner a depth map inference system from monocular videos that\ntakes a pair of images as input. This algorithm actually learns\nstructure-from-motion from motion, and not only structure from context\nappearance. The scale factor issue is explicitly treated, and the absolute\ndepth map can be estimated from camera displacement magnitude, which can be\neasily measured from cheap external sensors. Our solution is also much more\nrobust with respect to domain variation and adaptation via fine tuning, because\nit does not rely entirely in depth from context. Two use cases are considered,\nunstabilized moving camera videos, and stabilized ones. This choice is\nmotivated by the UAV (for Unmanned Aerial Vehicle) use case that generally\nprovides reliable orientation measurement. We provide a set of experiments\nshowing that, used in real conditions where only speed can be known, our\nnetwork outperforms competitors for most depth quality measures. Results are\ngiven on the well known KITTI dataset, which provides robust stabilization for\nour second use case, but also contains moving scenes which are very typical of\nthe in-car road context. We then present results on a synthetic dataset that we\nbelieve to be more representative of typical UAV scenes. Lastly, we present two\ndomain adaptation use cases showing superior robustness of our method compared\nto single view depth algorithms, which indicates that it is better suited for\nhighly variable visual contexts.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 14:10:35 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 16:09:37 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Pinard", "Cl\u00e9ment", ""], ["Chevalley", "Laure", ""], ["Manzanera", "Antoine", ""], ["Filliat", "David", ""]]}, {"id": "1809.04482", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Carl Doersch", "title": "The Visual QA Devil in the Details: The Impact of Early Fusion and Batch\n  Norm on CLEVR", "comments": "Presented at ECCV'18 Workshop on Shortcomings in Vision and Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual QA is a pivotal challenge for higher-level reasoning, requiring\nunderstanding language, vision, and relationships between many objects in a\nscene. Although datasets like CLEVR are designed to be unsolvable without such\ncomplex relational reasoning, some surprisingly simple feed-forward, \"holistic\"\nmodels have recently shown strong performance on this dataset. These models\nlack any kind of explicit iterative, symbolic reasoning procedure, which are\nhypothesized to be necessary for counting objects, narrowing down the set of\nrelevant objects based on several attributes, etc. The reason for this strong\nperformance is poorly understood. Hence, our work analyzes such models, and\nfinds that minor architectural elements are crucial to performance. In\nparticular, we find that \\textit{early fusion} of language and vision provides\nlarge performance improvements. This contrasts with the late fusion approaches\npopular at the dawn of Visual QA. We propose a simple module we call Multimodal\nCore, which we hypothesize performs the fundamental operations for multimodal\ntasks. We believe that understanding why these elements are so important to\ncomplex question answering will aid the design of better-performing algorithms\nfor Visual QA while minimizing hand-engineering effort.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 07:14:30 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Doersch", "Carl", ""]]}, {"id": "1809.04508", "submitter": "Zhisheng Zhong", "authors": "Zhisheng Zhong, Tiancheng Shen, Yibo Yang, Zhouchen Lin and Chao Zhang", "title": "Joint Sub-bands Learning with Clique Structures for Wavelet Domain\n  Super-Resolution", "comments": "Accepted in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have recently achieved great success in\nsingle-image super-resolution (SISR). However, these methods tend to produce\nover-smoothed outputs and miss some textural details. To solve these problems,\nwe propose the Super-Resolution CliqueNet (SRCliqueNet) to reconstruct the high\nresolution (HR) image with better textural details in the wavelet domain. The\nproposed SRCliqueNet firstly extracts a set of feature maps from the low\nresolution (LR) image by the clique blocks group. Then we send the set of\nfeature maps to the clique up-sampling module to reconstruct the HR image. The\nclique up-sampling module consists of four sub-nets which predict the high\nresolution wavelet coefficients of four sub-bands. Since we consider the edge\nfeature properties of four sub-bands, the four sub-nets are connected to the\nothers so that they can learn the coefficients of four sub-bands jointly.\nFinally we apply inverse discrete wavelet transform (IDWT) to the output of\nfour sub-nets at the end of the clique up-sampling module to increase the\nresolution and reconstruct the HR image. Extensive quantitative and qualitative\nexperiments on benchmark datasets show that our method achieves superior\nperformance over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 15:19:37 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 02:45:15 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 12:29:12 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Zhong", "Zhisheng", ""], ["Shen", "Tiancheng", ""], ["Yang", "Yibo", ""], ["Lin", "Zhouchen", ""], ["Zhang", "Chao", ""]]}, {"id": "1809.04529", "submitter": "Sandeep Joshi", "authors": "Sandeep Joshi, Samrudh Kumar", "title": "Image contrast enhancement using fuzzy logic", "comments": "4 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image enhancement is a method of improving the quality of an image and\ncontrast is a major aspect. Traditional methods of contrast enhancement like\nhistogram equalization results in over/under enhancement of the image\nespecially a lower resolution one. This paper aims at developing a new Fuzzy\nInference System to enhance the contrast of the low resolution images\novercoming the shortcomings of the traditional methods. Results obtained using\nboth the approaches are compared.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 15:46:48 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Joshi", "Sandeep", ""], ["Kumar", "Samrudh", ""]]}, {"id": "1809.04536", "submitter": "Heran Yang", "authors": "Heran Yang, Jian Sun, Aaron Carass, Can Zhao, Junghoon Lee, Zongben\n  Xu, Jerry Prince", "title": "Unpaired Brain MR-to-CT Synthesis using a Structure-Constrained CycleGAN", "comments": "8 pages, 5 figures, accepted by MICCAI 2018 Workshop: Deep Learning\n  in Medical Image Analysis (DLMIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cycleGAN is becoming an influential method in medical image synthesis.\nHowever, due to a lack of direct constraints between input and synthetic\nimages, the cycleGAN cannot guarantee structural consistency between these two\nimages, and such consistency is of extreme importance in medical imaging. To\novercome this, we propose a structure-constrained cycleGAN for brain MR-to-CT\nsynthesis using unpaired data that defines an extra structure-consistency loss\nbased on the modality independent neighborhood descriptor to constrain\nstructural consistency. Additionally, we use a position-based selection\nstrategy for selecting training images instead of a completely random selection\nscheme. Experimental results on synthesizing CT images from brain MR images\ndemonstrate that our method is better than the conventional cycleGAN and\napproximates the cycleGAN trained with paired data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 15:56:42 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Yang", "Heran", ""], ["Sun", "Jian", ""], ["Carass", "Aaron", ""], ["Zhao", "Can", ""], ["Lee", "Junghoon", ""], ["Xu", "Zongben", ""], ["Prince", "Jerry", ""]]}, {"id": "1809.04553", "submitter": "Carlos Busso", "authors": "Fei Tao and Carlos Busso", "title": "End-to-end Audiovisual Speech Activity Detection with Bimodal Recurrent\n  Neural Models", "comments": "Submitted to Speech Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech activity detection (SAD) plays an important role in current speech\nprocessing systems, including automatic speech recognition (ASR). SAD is\nparticularly difficult in environments with acoustic noise. A practical\nsolution is to incorporate visual information, increasing the robustness of the\nSAD approach. An audiovisual system has the advantage of being robust to\ndifferent speech modes (e.g., whisper speech) or background noise. Recent\nadvances in audiovisual speech processing using deep learning have opened\nopportunities to capture in a principled way the temporal relationships between\nacoustic and visual features. This study explores this idea proposing a\n\\emph{bimodal recurrent neural network} (BRNN) framework for SAD. The approach\nmodels the temporal dynamic of the sequential audiovisual data, improving the\naccuracy and robustness of the proposed SAD system. Instead of estimating\nhand-crafted features, the study investigates an end-to-end training approach,\nwhere acoustic and visual features are directly learned from the raw data\nduring training. The experimental evaluation considers a large audiovisual\ncorpus with over 60.8 hours of recordings, collected from 105 speakers. The\nresults demonstrate that the proposed framework leads to absolute improvements\nup to 1.2% under practical scenarios over a VAD baseline using only audio\nimplemented with deep neural network (DNN). The proposed approach achieves\n92.7% F1-score when it is evaluated using the sensors from a portable tablet\nunder noisy acoustic environment, which is only 1.0% lower than the performance\nobtained under ideal conditions (e.g., clean speech obtained with a high\ndefinition camera and a close-talking microphone).\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:44:46 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Tao", "Fei", ""], ["Busso", "Carlos", ""]]}, {"id": "1809.04560", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, Mohit Bansal", "title": "Game-Based Video-Context Dialogue", "comments": "EMNLP 2018 (14 pages) (fixed Table5 typo in v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current dialogue systems focus more on textual and speech context knowledge\nand are usually based on two speakers. Some recent work has investigated static\nimage-based dialogue. However, several real-world human interactions also\ninvolve dynamic visual context (similar to videos) as well as dialogue\nexchanges among multiple speakers. To move closer towards such multimodal\nconversational skills and visually-situated applications, we introduce a new\nvideo-context, many-speaker dialogue dataset based on live-broadcast soccer\ngame videos and chats from Twitch.tv. This challenging testbed allows us to\ndevelop visually-grounded dialogue models that should generate relevant\ntemporal and spatial event language from the live video, while also being\nrelevant to the chat history. For strong baselines, we also present several\ndiscriminative and generative models, e.g., based on tridirectional attention\nflow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic\nphrase-matching metrics, as well as human evaluation studies. We also present\ndataset analyses, model ablations, and visualizations to understand the\ncontribution of different modalities and model components.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:53:13 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 15:26:48 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1809.04621", "submitter": "Bruna Frade", "authors": "Bruna Vieira Frade and Erickson R. Nascimento", "title": "A Two-Step Learning Method For Detecting Landmarks on Faces From\n  Different Domains", "comments": "https://ieeexplore.ieee.org/document/8451026/", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451026", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of fiducial points on faces has significantly been favored by\nthe rapid progress in the field of machine learning, in particular in the\nconvolution networks. However, the accuracy of most of the detectors strongly\ndepends on an enormous amount of annotated data. In this work, we present a\ndomain adaptation approach based on a two-step learning to detect fiducial\npoints on human and animal faces. We evaluate our method on three different\ndatasets composed of different animal faces (cats, dogs, and horses). The\nexperiments show that our method performs better than state of the art and can\nuse few annotated data to leverage the detection of landmarks reducing the\ndemand for large volume of annotated data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 18:14:12 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Frade", "Bruna Vieira", ""], ["Nascimento", "Erickson R.", ""]]}, {"id": "1809.04624", "submitter": "Walysson Vital Barbosa", "authors": "Walysson Vital Barbosa, Henrique Grandinetti Barbosa Amaral, Thiago\n  Lages Rocha, Erickson Rangel Nascimento", "title": "Visual-Quality-Driven Learning for Underwater Vision Enhancement", "comments": "Accepted for publication and presented in 2018 IEEE International\n  Conference on Image Processing (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451356", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image processing community has witnessed remarkable advances in enhancing\nand restoring images. Nevertheless, restoring the visual quality of underwater\nimages remains a great challenge. End-to-end frameworks might fail to enhance\nthe visual quality of underwater images since in several scenarios it is not\nfeasible to provide the ground truth of the scene radiance. In this work, we\npropose a CNN-based approach that does not require ground truth data since it\nuses a set of image quality metrics to guide the restoration learning process.\nThe experiments showed that our method improved the visual quality of\nunderwater images preserving their edges and also performed well considering\nthe UCIQE metric.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 18:22:38 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Barbosa", "Walysson Vital", ""], ["Amaral", "Henrique Grandinetti Barbosa", ""], ["Rocha", "Thiago Lages", ""], ["Nascimento", "Erickson Rangel", ""]]}, {"id": "1809.04642", "submitter": "Arun CS Kumar", "authors": "Arun CS Kumar and Shefali Srivastava and Anirban Mukhopadhyay and\n  Suchendra M. Bhandarkar", "title": "Deep Spectral Correspondence for Matching Disparate Image Pairs", "comments": "43 pages, under submission to Computer Vision and Image Understanding\n  Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel, non-learning-based, saliency-aware, shape-cognizant correspondence\ndetermination technique is proposed for matching image pairs that are\nsignificantly disparate in nature. Images in the real world often exhibit high\ndegrees of variation in scale, orientation, viewpoint, illumination and affine\nprojection parameters, and are often accompanied by the presence of textureless\nregions and complete or partial occlusion of scene objects. The above\nconditions confound most correspondence determination techniques by rendering\nimpractical the use of global contour-based descriptors or local pixel-level\nfeatures for establishing correspondence. The proposed deep spectral\ncorrespondence (DSC) determination scheme harnesses the representational power\nof local feature descriptors to derive a complex high-level global shape\nrepresentation for matching disparate images. The proposed scheme reasons about\ncorrespondence between disparate images using high-level global shape cues\nderived from low-level local feature descriptors. Consequently, the proposed\nscheme enjoys the best of both worlds, i.e., a high degree of invariance to\naffine parameters such as scale, orientation, viewpoint, illumination afforded\nby the global shape cues and robustness to occlusion provided by the low-level\nfeature descriptors. While the shape-based component within the proposed scheme\ninfers what to look for, an additional saliency-based component dictates where\nto look at thereby tackling the noisy correspondences arising from the presence\nof textureless regions and complex backgrounds. In the proposed scheme, a joint\nimage graph is constructed using distances computed between interest points in\nthe appearance (i.e., image) space. Eigenspectral decomposition of the joint\nimage graph allows for reasoning about shape similarity to be performed\njointly, in the appearance space and eigenspace.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 19:33:26 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Kumar", "Arun CS", ""], ["Srivastava", "Shefali", ""], ["Mukhopadhyay", "Anirban", ""], ["Bhandarkar", "Suchendra M.", ""]]}, {"id": "1809.04659", "submitter": "Dilip K. Prasad", "authors": "Dilip K. Prasad, Huixu Dong, Deepu Rajan, Chai Quek", "title": "Are object detection assessment criteria ready for maritime computer\n  vision?", "comments": null, "journal-ref": "IEEE Transactions on Intelligent Transportation Systems (2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maritime vessels equipped with visible and infrared cameras can complement\nother conventional sensors for object detection. However, application of\ncomputer vision techniques in maritime domain received attention only recently.\nThe maritime environment offers its own unique requirements and challenges.\nAssessment of the quality of detections is a fundamental need in computer\nvision. However, the conventional assessment metrics suitable for usual object\ndetection are deficient in the maritime setting. Thus, a large body of related\nwork in computer vision appears inapplicable to the maritime setting at the\nfirst sight. We discuss the problem of defining assessment metrics suitable for\nmaritime computer vision. We consider new bottom edge proximity metrics as\nassessment metrics for maritime computer vision. These metrics indicate that\nexisting computer vision approaches are indeed promising for maritime computer\nvision and can play a foundational role in the emerging field of maritime\ncomputer vision.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 20:18:04 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 16:29:39 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Prasad", "Dilip K.", ""], ["Dong", "Huixu", ""], ["Rajan", "Deepu", ""], ["Quek", "Chai", ""]]}, {"id": "1809.04693", "submitter": "Ulugbek Kamilov", "authors": "Yu Sun, Brendt Wohlberg, Ulugbek S. Kamilov", "title": "An Online Plug-and-Play Algorithm for Regularized Image Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TCI.2019.2893568", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plug-and-play priors (PnP) is a powerful framework for regularizing imaging\ninverse problems by using advanced denoisers within an iterative algorithm.\nRecent experimental evidence suggests that PnP algorithms achieve\nstate-of-the-art performance in a range of imaging applications. In this paper,\nwe introduce a new online PnP algorithm based on the iterative\nshrinkage/thresholding algorithm (ISTA). The proposed algorithm uses only a\nsubset of measurements at every iteration, which makes it scalable to very\nlarge datasets. We present a new theoretical convergence analysis, for both\nbatch and online variants of PnP-ISTA, for denoisers that do not necessarily\ncorrespond to proximal operators. We also present simulations illustrating the\napplicability of the algorithm to image reconstruction in diffraction\ntomography. The results in this paper have the potential to expand the\napplicability of the PnP framework to very large and redundant datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 22:26:26 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sun", "Yu", ""], ["Wohlberg", "Brendt", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1809.04696", "submitter": "Hassan Abu Alhaija", "authors": "Hassan Abu Alhaija, Siva Karthik Mustikovela, Andreas Geiger, Carsten\n  Rother", "title": "Geometric Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of generating natural images from 3D scenes has been a long standing\ngoal in computer graphics. On the other hand, recent developments in deep\nneural networks allow for trainable models that can produce natural-looking\nimages with little or no knowledge about the scene structure. While the\ngenerated images often consist of realistic looking local patterns, the overall\nstructure of the generated images is often inconsistent. In this work we\npropose a trainable, geometry-aware image generation method that leverages\nvarious types of scene information, including geometry and segmentation, to\ncreate realistic looking natural images that match the desired scene structure.\nOur geometrically-consistent image synthesis method is a deep neural network,\ncalled Geometry to Image Synthesis (GIS) framework, which retains the\nadvantages of a trainable method, e.g., differentiability and adaptiveness,\nbut, at the same time, makes a step towards the generalizability, control and\nquality output of modern graphics rendering engines. We utilize the GIS\nframework to insert vehicles in outdoor driving scenes, as well as to generate\nnovel views of objects from the Linemod dataset. We qualitatively show that our\nnetwork is able to generalize beyond the training set to novel scene\ngeometries, object shapes and segmentations. Furthermore, we quantitatively\nshow that the GIS framework can be used to synthesize large amounts of training\ndata which proves beneficial for training instance segmentation models.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 22:38:22 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 21:32:01 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Alhaija", "Hassan Abu", ""], ["Mustikovela", "Siva Karthik", ""], ["Geiger", "Andreas", ""], ["Rother", "Carsten", ""]]}, {"id": "1809.04704", "submitter": "Bernard Llanos", "authors": "Bernard Llanos (1) and Yee-Hong Yang (1) ((1) University of Alberta)", "title": "Do-It-Yourself Single Camera 3D Pointer Input Device", "comments": "8 pages, 6 figures, 2018 15th Conference on Computer and Robot Vision", "journal-ref": "B. Llanos, and Y.-H. Yang, \"Do-It-Yourself Single Camera 3D\n  Pointer Input Device,\" in 2018 15th Conference on Computer and Robot Vision\n  (CRV), 2018, pp. 214-221", "doi": "10.1109/CRV.2018.00038", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for single camera 3D reconstruction, or 3D input\nfor human-computer interfaces, based on precise tracking of an elongated\nobject, such as a pen, having a pattern of colored bands. To configure the\nsystem, the user provides no more than one labelled image of a handmade\npointer, measurements of its colored bands, and the camera's pinhole projection\nmatrix. Other systems are of much higher cost and complexity, requiring\ncombinations of multiple cameras, stereocameras, and pointers with sensors and\nlights. Instead of relying on information from multiple devices, we examine our\nsingle view more closely, integrating geometric and appearance constraints to\nrobustly track the pointer in the presence of occlusion and distractor objects.\nBy probing objects of known geometry with the pointer, we demonstrate\nacceptable accuracy of 3D localization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 23:07:43 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Llanos", "Bernard", "", "University of Alberta"], ["Yang", "Yee-Hong", "", "University of Alberta"]]}, {"id": "1809.04711", "submitter": "Galin Georgiev", "authors": "Galin Georgiev", "title": "Linear Algebra and Duality of Neural Networks", "comments": "48 pages with number of figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bases, mappings, projections and metrics, natural for Neural network\ntraining, are introduced. Graph-theoretical interpretation is offered.\nNon-Gaussianity naturally emerges, even in relatively simple datasets. Training\nstatistics, hierarchies and energies are analyzed, from physics point of view.\nDuality between observables (for example, pixels) and observations is\nestablished. Relationship between exact and numerical solutions is studied.\nPhysics and financial mathematics interpretations of a key problem are offered.\nExamples support all new concepts.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 23:39:18 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 22:23:26 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Georgiev", "Galin", ""]]}, {"id": "1809.04729", "submitter": "Alireza Shafaei", "authors": "Alireza Shafaei, Mark Schmidt, James J. Little", "title": "A Less Biased Evaluation of Out-of-distribution Sample Detectors", "comments": "to appear in BMVC 2019; v2 is more compact, with more results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, a learning system could receive an input that is unlike\nanything it has seen during training. Unfortunately, out-of-distribution\nsamples can lead to unpredictable behaviour. We need to know whether any given\ninput belongs to the population distribution of the training/evaluation data to\nprevent unpredictable behaviour in deployed systems. A recent surge of interest\nin this problem has led to the development of sophisticated techniques in the\ndeep learning literature. However, due to the absence of a standard problem\ndefinition or an exhaustive evaluation, it is not evident if we can rely on\nthese methods. What makes this problem different from a typical supervised\nlearning setting is that the distribution of outliers used in training may not\nbe the same as the distribution of outliers encountered in the application.\nClassical approaches that learn inliers vs. outliers with only two datasets can\nyield optimistic results. We introduce OD-test, a three-dataset evaluation\nscheme as a more reliable strategy to assess progress on this problem. We\npresent an exhaustive evaluation of a broad set of methods from related areas\non image classification tasks. Contrary to the existing results, we show that\nfor realistic applications of high-dimensional images the previous techniques\nhave low accuracy and are not reliable in practice.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 01:15:49 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 17:46:05 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Shafaei", "Alireza", ""], ["Schmidt", "Mark", ""], ["Little", "James J.", ""]]}, {"id": "1809.04730", "submitter": "Wei Zhou", "authors": "Wei Zhou, Alex Zyner, Stewart Worrall, and Eduardo Nebot", "title": "Adapting Semantic Segmentation Models for Changes in Illumination and\n  Camera Perspective", "comments": "Submitted to IEEE Robotics and Automation Letters (RA-L) and 2019\n  IEEE International Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": "10.1109/LRA.2019.2891027", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation using deep neural networks has been widely explored to\ngenerate high-level contextual information for autonomous vehicles. To acquire\na complete $180^\\circ$ semantic understanding of the forward surroundings, we\npropose to stitch semantic images from multiple cameras with varying\norientations. However, previously trained semantic segmentation models showed\nunacceptable performance after significant changes to the camera orientations\nand the lighting conditions. To avoid time-consuming hand labeling, we explore\nand evaluate the use of data augmentation techniques, specifically skew and\ngamma correction, from a practical real-world standpoint to extend the existing\nmodel and provide more robust performance. The presented experimental results\nhave shown significant improvements with varying illumination and camera\nperspective changes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 01:17:54 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zhou", "Wei", ""], ["Zyner", "Alex", ""], ["Worrall", "Stewart", ""], ["Nebot", "Eduardo", ""]]}, {"id": "1809.04734", "submitter": "Junming Zhang", "authors": "Junming Zhang, Katherine A. Skinner, Ram Vasudevan and Matthew\n  Johnson-Roberson", "title": "DispSegNet: Leveraging Semantics for End-to-End Learning of Disparity\n  Estimation from Stereo Imagery", "comments": "Add more description on the architecture of the model. Add more\n  discussion on section IV-C. Fix typo in formula 6", "journal-ref": "IEEE Robotics and Automation Letters, vol. 4, no. 2, pp.\n  1162-1169, April 2019", "doi": "10.1109/LRA.2019.2894913", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that convolutional neural networks (CNNs) can be\napplied successfully in disparity estimation, but these methods still suffer\nfrom errors in regions of low-texture, occlusions and reflections.\nConcurrently, deep learning for semantic segmentation has shown great progress\nin recent years. In this paper, we design a CNN architecture that combines\nthese two tasks to improve the quality and accuracy of disparity estimation\nwith the help of semantic segmentation. Specifically, we propose a network\nstructure in which these two tasks are highly coupled. One key novelty of this\napproach is the two-stage refinement process. Initial disparity estimates are\nrefined with an embedding learned from the semantic segmentation branch of the\nnetwork. The proposed model is trained using an unsupervised approach, in which\nimages from one half of the stereo pair are warped and compared against images\nfrom the other camera. Another key advantage of the proposed approach is that a\nsingle network is capable of outputting disparity estimates and semantic\nlabels. These outputs are of great use in autonomous vehicle operation; with\nreal-time constraints being key, such performance improvements increase the\nviability of driving applications. Experiments on KITTI and Cityscapes datasets\nshow that our model can achieve state-of-the-art results and that leveraging\nembedding learned from semantic segmentation improves the performance of\ndisparity estimation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 01:36:55 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 19:07:31 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Zhang", "Junming", ""], ["Skinner", "Katherine A.", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1809.04741", "submitter": "Yingjie Yin", "authors": "Yingjie Yin, Lei Zhang, De Xu and Xingang Wang", "title": "Adversarial Feature Sampling Learning for Efficient Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tracking-by-detection framework usually consist of two stages: drawing\nsamples around the target object in the first stage and classifying each sample\nas the target object or background in the second stage. Current popular\ntrackers based on tracking-by-detection framework typically draw samples in the\nraw image as the inputs of deep convolution networks in the first stage, which\nusually results in high computational burden and low running speed. In this\npaper, we propose a new visual tracking method using sampling deep\nconvolutional features to address this problem. Only one cropped image around\nthe target object is input into the designed deep convolution network and the\nsamples is sampled on the feature maps of the network by spatial bilinear\nresampling. In addition, a generative adversarial network is integrated into\nour network framework to augment positive samples and improve the tracking\nperformance. Extensive experiments on benchmark datasets demonstrate that the\nproposed method achieves a comparable performance to state-of-the-art trackers\nand accelerates tracking-by-detection trackers based on raw-image samples\neffectively.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 02:06:18 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 10:25:21 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Yin", "Yingjie", ""], ["Zhang", "Lei", ""], ["Xu", "De", ""], ["Wang", "Xingang", ""]]}, {"id": "1809.04763", "submitter": "Shu Liang", "authors": "Shu Liang, Linda G. Shapiro, Ira Kemelmacher-Shlizerman", "title": "Head Reconstruction from Internet Photos", "comments": "Published on ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face reconstruction from Internet photos has recently produced exciting\nresults. A person's face, e.g., Tom Hanks, can be modeled and animated in 3D\nfrom a completely uncalibrated photo collection. Most methods, however, focus\nsolely on face area and mask out the rest of the head. This paper proposes that\nhead modeling from the Internet is a problem we can solve. We target\nreconstruction of the rough shape of the head. Our method is to gradually\n\"grow\" the head mesh starting from the frontal face and extending to the rest\nof views using photometric stereo constraints. We call our method\nboundary-value growing algorithm. Results on photos of celebrities downloaded\nfrom the Internet are presented.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 04:13:26 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Liang", "Shu", ""], ["Shapiro", "Linda G.", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "1809.04764", "submitter": "Shu Liang", "authors": "Shu Liang, Ira Kemelmacher-Shlizerman, Linda G. Shapiro", "title": "3D Face Hallucination from a Single Depth Frame", "comments": "published on 3Dv 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that takes a single frame of a person's face from a\ndepth camera, e.g., Kinect, and produces a high-resolution 3D mesh of the input\nface. We leverage a dataset of 3D face meshes of 1204 distinct individuals\nranging from age 3 to 40, captured in a neutral expression. We divide the input\ndepth frame into semantically significant regions (eyes, nose, mouth, cheeks)\nand search the database for the best matching shape per region. We further\ncombine the input depth frame with the matched database shapes into a single\nmesh that results in a high-resolution shape of the input person. Our system is\nfully automatic and uses only depth data for matching, making it invariant to\nimaging conditions. We evaluate our results using ground truth shapes, as well\nas compare to state-of-the-art shape estimation methods. We demonstrate the\nrobustness of our local matching approach with high-quality reconstruction of\nfaces that fall outside of the dataset span, e.g., faces older than 40 years\nold, facial expressions, and different ethnicities.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 04:13:56 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Liang", "Shu", ""], ["Kemelmacher-Shlizerman", "Ira", ""], ["Shapiro", "Linda G.", ""]]}, {"id": "1809.04765", "submitter": "Shu Liang", "authors": "Shu Liang, Xiufeng Huang, Xianyu Meng, Kunyao Chen, Linda G. Shapiro,\n  Ira Kemelmacher-Shlizerman", "title": "Video to Fully Automatic 3D Hair Model", "comments": "supplementary video: https://www.youtube.com/watch?v=so_CMv7Xd40", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine taking a selfie video with your mobile phone and getting as output a\n3D model of your head (face and 3D hair strands) that can be later used in VR,\nAR, and any other domain. State of the art hair reconstruction methods allow\neither a single photo (thus compromising 3D quality) or multiple views, but\nthey require manual user interaction (manual hair segmentation and capture of\nfixed camera views that span full 360 degree). In this paper, we describe a\nsystem that can completely automatically create a reconstruction from any video\n(even a selfie video), and we don't require specific views, since taking your\n-90 degree, 90 degree, and full back views is not feasible in a selfie capture.\n  In the core of our system, in addition to the automatization components, hair\nstrands are estimated and deformed in 3D (rather than 2D as in state of the\nart) thus enabling superior results. We provide qualitative, quantitative, and\nMechanical Turk human studies that support the proposed system, and show\nresults on a diverse variety of videos (8 different celebrity videos, 9 selfie\nmobile videos, spanning age, gender, hair length, type, and styling).\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 04:14:53 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Liang", "Shu", ""], ["Huang", "Xiufeng", ""], ["Meng", "Xianyu", ""], ["Chen", "Kunyao", ""], ["Shapiro", "Linda G.", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "1809.04766", "submitter": "Vladimir Nekrasov", "authors": "Vladimir Nekrasov, Thanuja Dharmasiri, Andrew Spek, Tom Drummond,\n  Chunhua Shen and Ian Reid", "title": "Real-Time Joint Semantic Segmentation and Depth Estimation Using\n  Asymmetric Annotations", "comments": "The models are available here -\n  https://github.com/drsleep/multi-task-refinenet; supplementary video here -\n  https://youtu.be/qwShIBhaq8Y", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deployment of deep learning models in robotics as sensory information\nextractors can be a daunting task to handle, even using generic GPU cards.\nHere, we address three of its most prominent hurdles, namely, i) the adaptation\nof a single model to perform multiple tasks at once (in this work, we consider\ndepth estimation and semantic segmentation crucial for acquiring geometric and\nsemantic understanding of the scene), while ii) doing it in real-time, and iii)\nusing asymmetric datasets with uneven numbers of annotations per each modality.\nTo overcome the first two issues, we adapt a recently proposed real-time\nsemantic segmentation network, making changes to further reduce the number of\nfloating point operations. To approach the third issue, we embrace a simple\nsolution based on hard knowledge distillation under the assumption of having\naccess to a powerful `teacher' network. We showcase how our system can be\neasily extended to handle more tasks, and more datasets, all at once,\nperforming depth estimation and segmentation both indoors and outdoors with a\nsingle model. Quantitatively, we achieve results equivalent to (or better than)\ncurrent state-of-the-art approaches with one forward pass costing just 13ms and\n6.5 GFLOPs on 640x480 inputs. This efficiency allows us to directly incorporate\nthe raw predictions of our network into the SemanticFusion framework for dense\n3D semantic reconstruction of the scene.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 04:19:26 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 05:53:59 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Nekrasov", "Vladimir", ""], ["Dharmasiri", "Thanuja", ""], ["Spek", "Andrew", ""], ["Drummond", "Tom", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1809.04783", "submitter": "Manri Cheon", "authors": "Manri Cheon, Jun-Hyuk Kim, Jun-Ho Choi, Jong-Seok Lee", "title": "Generative adversarial network-based image super-resolution using\n  perceptual content losses", "comments": "To appear in ECCV 2018 workshop. Won the 2nd place for Region 1 in\n  the PIRM Challenge on Perceptual Super Resolution at ECCV 2018. Github at\n  https://github.com/manricheon/eusr-pcl-tf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep generative adversarial network for\nsuper-resolution considering the trade-off between perception and distortion.\nBased on good performance of a recently developed model for super-resolution,\ni.e., deep residual network using enhanced upscale modules (EUSR), the proposed\nmodel is trained to improve perceptual performance with only slight increase of\ndistortion. For this purpose, together with the conventional content loss,\ni.e., reconstruction loss such as L1 or L2, we consider additional losses in\nthe training phase, which are the discrete cosine transform coefficients loss\nand differential content loss. These consider perceptual part in the content\nloss, i.e., consideration of proper high frequency components is helpful for\nthe trade-off problem in super-resolution. The experimental results show that\nour proposed model has good performance for both perception and distortion, and\nis effective in perceptual super-resolution applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 05:23:54 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 07:30:56 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Cheon", "Manri", ""], ["Kim", "Jun-Hyuk", ""], ["Choi", "Jun-Ho", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.04789", "submitter": "Jun-Ho Choi", "authors": "Jun-Ho Choi, Jun-Hyuk Kim, Manri Cheon, Jong-Seok Lee", "title": "Deep Learning-based Image Super-Resolution Considering Quantitative and\n  Perceptual Quality", "comments": "Won the 2nd place for Region 2 in the PIRM Challenge on Perceptual\n  Super Resolution at ECCV 2018. GitHub at\n  https://github.com/idearibosome/tf-perceptual-eusr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been shown that in super-resolution, there exists a tradeoff\nrelationship between the quantitative and perceptual quality of super-resolved\nimages, which correspond to the similarity to the ground-truth images and the\nnaturalness, respectively. In this paper, we propose a novel super-resolution\nmethod that can improve the perceptual quality of the upscaled images while\npreserving the conventional quantitative performance. The proposed method\nemploys a deep network for multi-pass upscaling in company with a discriminator\nnetwork and two quantitative score predictor networks. Experimental results\ndemonstrate that the proposed method achieves a good balance of the\nquantitative and perceptual quality, showing more satisfactory results than\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 06:03:56 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 05:23:55 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Choi", "Jun-Ho", ""], ["Kim", "Jun-Hyuk", ""], ["Cheon", "Manri", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "1809.04820", "submitter": "Ikuro Sato", "authors": "Kent Fujiwara, Ikuro Sato, Mitsuru Ambai, Yuichi Yoshida, Yoshiaki\n  Sakakura", "title": "Canonical and Compact Point Cloud Representation for Shape\n  Classification", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel compact point cloud representation that is inherently\ninvariant to scale, coordinate change and point permutation. The key idea is to\nparametrize a distance field around an individual shape into a unique,\ncanonical, and compact vector in an unsupervised manner. We firstly project a\ndistance field to a $4$D canonical space using singular value decomposition. We\nthen train a neural network for each instance to non-linearly embed its\ndistance field into network parameters. We employ a bias-free Extreme Learning\nMachine (ELM) with ReLU activation units, which has scale-factor commutative\nproperty between layers. We demonstrate the descriptiveness of the\ninstance-wise, shape-embedded network parameters by using them to classify\nshapes in $3$D datasets. Our learning-based representation requires minimal\naugmentation and simple neural networks, where previous approaches demand\nnumerous representations to handle coordinate change and point permutation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 08:11:18 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Fujiwara", "Kent", ""], ["Sato", "Ikuro", ""], ["Ambai", "Mitsuru", ""], ["Yoshida", "Yuichi", ""], ["Sakakura", "Yoshiaki", ""]]}, {"id": "1809.04835", "submitter": "Haichao Shi", "authors": "Haichao Shi and Peng Li and Bo Wang and Zhenyu Wang", "title": "Image Captioning based on Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has shown that the policy-gradient methods for reinforcement\nlearning have been utilized to train deep end-to-end systems on natural\nlanguage processing tasks. What's more, with the complexity of understanding\nimage content and diverse ways of describing image content in natural language,\nimage captioning has been a challenging problem to deal with. To the best of\nour knowledge, most state-of-the-art methods follow a pattern of sequential\nmodel, such as recurrent neural networks (RNN). However, in this paper, we\npropose a novel architecture for image captioning with deep reinforcement\nlearning to optimize image captioning tasks. We utilize two networks called\n\"policy network\" and \"value network\" to collaboratively generate the captions\nof images. The experiments are conducted on Microsoft COCO dataset, and the\nexperimental results have verified the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 08:40:21 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Shi", "Haichao", ""], ["Li", "Peng", ""], ["Wang", "Bo", ""], ["Wang", "Zhenyu", ""]]}, {"id": "1809.04843", "submitter": "Felipe Codevilla", "authors": "Felipe Codevilla, Antonio M. L\\'opez, Vladlen Koltun and Alexey\n  Dosovitskiy", "title": "On Offline Evaluation of Vision-based Driving Models", "comments": "Published at the ECCV 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving models should ideally be evaluated by deploying them on a\nfleet of physical vehicles in the real world. Unfortunately, this approach is\nnot practical for the vast majority of researchers. An attractive alternative\nis to evaluate models offline, on a pre-collected validation dataset with\nground truth annotation. In this paper, we investigate the relation between\nvarious online and offline metrics for evaluation of autonomous driving models.\nWe find that offline prediction error is not necessarily correlated with\ndriving quality, and two models with identical prediction error can differ\ndramatically in their driving performance. We show that the correlation of\noffline evaluation with driving quality can be significantly improved by\nselecting an appropriate validation dataset and suitable offline metrics. The\nsupplementary video can be viewed at\nhttps://www.youtube.com/watch?v=P8K8Z-iF0cY\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 09:03:47 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Codevilla", "Felipe", ""], ["L\u00f3pez", "Antonio M.", ""], ["Koltun", "Vladlen", ""], ["Dosovitskiy", "Alexey", ""]]}, {"id": "1809.04931", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency", "title": "Multimodal Local-Global Ranking Fusion for Emotion Recognition", "comments": "ACM International Conference on Multimodal Interaction (ICMI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition is a core research area at the intersection of artificial\nintelligence and human communication analysis. It is a significant technical\nchallenge since humans display their emotions through complex idiosyncratic\ncombinations of the language, visual and acoustic modalities. In contrast to\ntraditional multimodal fusion techniques, we approach emotion recognition from\nboth direct person-independent and relative person-dependent perspectives. The\ndirect person-independent perspective follows the conventional emotion\nrecognition approach which directly infers absolute emotion labels from\nobserved multimodal features. The relative person-dependent perspective\napproaches emotion recognition in a relative manner by comparing partial video\nsegments to determine if there was an increase or decrease in emotional\nintensity. Our proposed model integrates these direct and relative prediction\nperspectives by dividing the emotion recognition task into three easier\nsubtasks. The first subtask involves a multimodal local ranking of relative\nemotion intensities between two short segments of a video. The second subtask\nuses local rankings to infer global relative emotion ranks with a Bayesian\nranking algorithm. The third subtask incorporates both direct predictions from\nobserved multimodal behaviors and relative emotion ranks from local-global\nrankings for final emotion prediction. Our approach displays excellent\nperformance on an audio-visual emotion recognition benchmark and improves over\nother algorithms for multimodal fusion.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 09:44:01 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Liang", "Paul Pu", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1809.04976", "submitter": "Jean Paul Ainam", "authors": "Jean-Paul Ainam, Ke Qin, Guisong Liu and Guangchun Luo", "title": "Sparse Label Smoothing Regularization for Person Re-Identification", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) is a cross-camera retrieval task which\nestablishes a correspondence between images of a person from multiple cameras.\nDeep Learning methods have been successfully applied to this problem and have\nachieved impressive results. However, these methods require a large amount of\nlabeled training data. Currently labeled datasets in person re-id are limited\nin their scale and manual acquisition of such large-scale datasets from\nsurveillance cameras is a tedious and labor-intensive task. In this paper, we\npropose a framework that performs intelligent data augmentation and assigns\npartial smoothing label to generated data. Our approach first exploits the\nclustering property of existing person re-id datasets to create groups of\nsimilar objects that model cross-view variations. Each group is then used to\ngenerate realistic images through adversarial training. Our aim is to emphasize\nfeature similarity between generated samples and the original samples. Finally,\nwe assign a non-uniform label distribution to the generated samples and define\na regularized loss function for training. The proposed approach tackles two\nproblems (1) how to efficiently use the generated data and (2) how to address\nthe over-smoothness problem found in current regularization methods. Extensive\nexperiments on four larges cale datasets show that our regularization method\nsignificantly improves the Re-ID accuracy compared to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:04:58 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 02:43:17 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 06:50:44 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Ainam", "Jean-Paul", ""], ["Qin", "Ke", ""], ["Liu", "Guisong", ""], ["Luo", "Guangchun", ""]]}, {"id": "1809.04983", "submitter": "Kalpit Thakkar", "authors": "Kalpit Thakkar, P J Narayanan", "title": "Part-based Graph Convolutional Network for Action Recognition", "comments": "Main: 13 pages, 3 figures, 2 tables. Supplementary: 5 pages, 3\n  figures, 1 table. Accepted at BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human actions comprise of joint motion of articulated body parts or\n`gestures'. Human skeleton is intuitively represented as a sparse graph with\njoints as nodes and natural connections between them as edges. Graph\nconvolutional networks have been used to recognize actions from skeletal\nvideos. We introduce a part-based graph convolutional network (PB-GCN) for this\ntask, inspired by Deformable Part-based Models (DPMs). We divide the skeleton\ngraph into four subgraphs with joints shared across them and learn a\nrecognition model using a part-based graph convolutional network. We show that\nsuch a model improves performance of recognition, compared to a model using\nentire skeleton graph. Instead of using 3D joint coordinates as node features,\nwe show that using relative coordinates and temporal displacements boosts\nperformance. Our model achieves state-of-the-art performance on two challenging\nbenchmark datasets NTURGB+D and HDM05, for skeletal action recognition.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:22:58 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Thakkar", "Kalpit", ""], ["Narayanan", "P J", ""]]}, {"id": "1809.04985", "submitter": "Dongao Ma", "authors": "Dongao Ma, Ping Tang, and Lijun Zhao", "title": "SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote\n  Sensing Image Scene Classification Baseline in vitro", "comments": "5 pages, 5 figures, 1 tables", "journal-ref": null, "doi": "10.1109/LGRS.2018.2890413", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lack of annotated samples greatly restrains the direct application of deep\nlearning in remote sensing image scene classification. Although researches have\nbeen done to tackle this issue by data augmentation with various image\ntransformation operations, they are still limited in quantity and diversity.\nRecently, the advent of the unsupervised learning based generative adversarial\nnetworks (GANs) bring us a new way to generate augmented samples. However, such\nGAN-generated samples are currently only served for training GANs model itself\nand for improving the performance of the discriminator in GANs internally (in\nvivo). It becomes a question of serious doubt whether the GAN-generated samples\ncan help better improve the scene classification performance of other deep\nlearning networks (in vitro), compared with the widely used transformed\nsamples. To answer this question, this paper proposes a SiftingGAN approach to\ngenerate more numerous, more diverse and more authentic labeled samples for\ndata augmentation. SiftingGAN extends traditional GAN framework with an\nOnline-Output method for sample generation, a Generative-Model-Sifting method\nfor model sifting, and a Labeled-Sample-Discriminating method for sample\nsifting. Experiments on the well-known AID dataset demonstrate that the\nproposed SiftingGAN method can not only effectively improve the performance of\nthe scene classification baseline that is achieved without data augmentation,\nbut also significantly excels the comparison methods based on traditional\ngeometric/radiometric transformation operations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:26:12 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 15:14:21 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 15:59:09 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 16:28:26 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Ma", "Dongao", ""], ["Tang", "Ping", ""], ["Zhao", "Lijun", ""]]}, {"id": "1809.04987", "submitter": "Istv\\'an S\\'ar\\'andi", "authors": "Istv\\'an S\\'ar\\'andi, Timm Linder, Kai O. Arras, Bastian Leibe", "title": "Synthetic Occlusion Augmentation with Volumetric Heatmaps for the 2018\n  ECCV PoseTrack Challenge on 3D Human Pose Estimation", "comments": "Extended abstract for the 2018 ECCV PoseTrack Workshop, updated with\n  full result tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our winning entry at the 2018 ECCV PoseTrack\nChallenge on 3D human pose estimation. Using a fully-convolutional backbone\narchitecture, we obtain volumetric heatmaps per body joint, which we convert to\ncoordinates using soft-argmax. Absolute person center depth is estimated by a\n1D heatmap prediction head. The coordinates are back-projected to 3D camera\nspace, where we minimize the L1 loss. Key to our good results is the training\ndata augmentation with randomly placed occluders from the Pascal VOC dataset.\nIn addition to reaching first place in the Challenge, our method also surpasses\nthe state-of-the-art on the full Human3.6M benchmark among methods that use no\nadditional pose datasets in training. Code for applying synthetic occlusions is\navailabe at https://github.com/isarandi/synthetic-occlusion.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:27:15 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 14:01:44 GMT"}, {"version": "v3", "created": "Tue, 6 Nov 2018 15:02:54 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["S\u00e1r\u00e1ndi", "Istv\u00e1n", ""], ["Linder", "Timm", ""], ["Arras", "Kai O.", ""], ["Leibe", "Bastian", ""]]}, {"id": "1809.04995", "submitter": "Olga Veksler", "authors": "Olga Veksler", "title": "Efficient Graph Cut Optimization for Full CRFs with Quantized Edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully connected pairwise Conditional Random Fields (Full-CRF) with Gaussian\nedge weights can achieve superior results compared to sparsely connected CRFs.\nHowever, traditional methods for Full-CRFs are too expensive. Previous work\ndevelops efficient approximate optimization based on mean field inference,\nwhich is a local optimization method and can be far from the optimum. We\npropose efficient and effective optimization based on graph cuts for Full-CRFs\nwith quantized edge weights. To quantize edge weights, we partition the image\ninto superpixels and assume that the weight of an edge between any two pixels\ndepends only on the superpixels these pixels belong to. Our quantized edge CRF\nis an approximation to the Gaussian edge CRF, and gets closer to it as\nsuperpixel size decreases. Being an approximation, our model offers an\nintuition about the regularization properties of the Guassian edge Full-CRF.\nFor efficient inference, we first consider the two-label case and develop an\napproximate method based on transforming the original problem into a smaller\ndomain. Then we handle multi-label CRF by showing how to implement expansion\nmoves. In both binary and multi-label cases, our solutions have significantly\nlower energy compared to that of mean field inference. We also show the\neffectiveness of our approach on semantic segmentation task.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:46:23 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Veksler", "Olga", ""]]}, {"id": "1809.05039", "submitter": "Yawei Hui", "authors": "Yawei Hui, Yaohua Liu, Byung-Hoon Park", "title": "Discovering Features in Sr$_{14}$Cu$_{24}$O$_{41}$ Neutron Single\n  Crystal Diffraction Data by Cluster Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the SMC'18 data challenge, \"Discovering Features in\nSr$_{14}$Cu$_{24}$O$_{41}$\", we have used the clustering algorithm \"DBSCAN\" to\nseparate the diffuse scattering features from the Bragg peaks, which takes into\naccount both spatial and photometric information in the dataset during in the\nclustering process. We find that, in additional to highly localized Bragg\npeaks, there exists broad diffuse scattering patterns consisting of\ndistinguishable geometries. Besides these two distinctive features, we also\nidentify a third distinguishable feature submerged in the low signal-to-noise\nregion in the reciprocal space, whose origin remains an open question.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 16:16:36 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Hui", "Yawei", ""], ["Liu", "Yaohua", ""], ["Park", "Byung-Hoon", ""]]}, {"id": "1809.05050", "submitter": "Kai Xu", "authors": "Xiaogang Wang, Bin Zhou, Haiyue Fang, Xiaowu Chen, Qinping Zhao, Kai\n  Xu", "title": "Learning to Group and Label Fine-Grained Shape Components", "comments": "Accepted to SIGGRAPH Asia 2018. Corresponding Author: Kai Xu\n  (kevin.kai.xu@gmail.com)", "journal-ref": "ACM Transactions on Graphics, 2018", "doi": "10.1145/3272127.3275009", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A majority of stock 3D models in modern shape repositories are assembled with\nmany fine-grained components. The main cause of such data form is the\ncomponent-wise modeling process widely practiced by human modelers. These\nmodeling components thus inherently reflect some function-based shape\ndecomposition the artist had in mind during modeling. On the other hand,\nmodeling components represent an over-segmentation since a functional part is\nusually modeled as a multi-component assembly. Based on these observations, we\nadvocate that labeled segmentation of stock 3D models should not overlook the\nmodeling components and propose a learning solution to grouping and labeling of\nthe fine-grained components. However, directly characterizing the shape of\nindividual components for the purpose of labeling is unreliable, since they can\nbe arbitrarily tiny and semantically meaningless. We propose to generate part\nhypotheses from the components based on a hierarchical grouping strategy, and\nperform labeling on those part groups instead of directly on the components.\nPart hypotheses are mid-level elements which are more probable to carry\nsemantic information. A multiscale 3D convolutional neural network is trained\nto extract context-aware features for the hypotheses. To accomplish a labeled\nsegmentation of the whole shape, we formulate higher-order conditional random\nfields (CRFs) to infer an optimal label assignment for all components.\nExtensive experiments demonstrate that our method achieves significantly robust\nlabeling results on raw 3D models from public shape repositories. Our work also\ncontributes the first benchmark for component-wise labeling.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 16:31:43 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wang", "Xiaogang", ""], ["Zhou", "Bin", ""], ["Fang", "Haiyue", ""], ["Chen", "Xiaowu", ""], ["Zhao", "Qinping", ""], ["Xu", "Kai", ""]]}, {"id": "1809.05067", "submitter": "Jiajun Wu", "authors": "Tianfan Xue, Jiajun Wu, Zhoutong Zhang, Chengkai Zhang, Joshua B.\n  Tenenbaum, William T. Freeman", "title": "Seeing Tree Structure from Vibration", "comments": "ECCV 2018. The first two authors contributed equally to this work.\n  Project page: http://tree.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans recognize object structure from both their appearance and motion;\noften, motion helps to resolve ambiguities in object structure that arise when\nwe observe object appearance only. There are particular scenarios, however,\nwhere neither appearance nor spatial-temporal motion signals are informative:\noccluding twigs may look connected and have almost identical movements, though\nthey belong to different, possibly disconnected branches. We propose to tackle\nthis problem through spectrum analysis of motion signals, because vibrations of\ndisconnected branches, though visually similar, often have distinctive natural\nfrequencies. We propose a novel formulation of tree structure based on a\nphysics-based link model, and validate its effectiveness by theoretical\nanalysis, numerical simulation, and empirical experiments. With this\nformulation, we use nonparametric Bayesian inference to reconstruct tree\nstructure from both spectral vibration signals and appearance cues. Our model\nperforms well in recognizing hierarchical tree structure from real-world videos\nof trees and vessels.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:23:08 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Xue", "Tianfan", ""], ["Wu", "Jiajun", ""], ["Zhang", "Zhoutong", ""], ["Zhang", "Chengkai", ""], ["Tenenbaum", "Joshua B.", ""], ["Freeman", "William T.", ""]]}, {"id": "1809.05068", "submitter": "Jiajun Wu", "authors": "Jiajun Wu, Chengkai Zhang, Xiuming Zhang, Zhoutong Zhang, William T.\n  Freeman, Joshua B. Tenenbaum", "title": "Learning Shape Priors for Single-View 3D Completion and Reconstruction", "comments": "ECCV 2018. The first two authors contributed equally to this work.\n  Project page: http://shapehd.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of single-view 3D shape completion or reconstruction is\nchallenging, because among the many possible shapes that explain an\nobservation, most are implausible and do not correspond to natural objects.\nRecent research in the field has tackled this problem by exploiting the\nexpressiveness of deep convolutional networks. In fact, there is another level\nof ambiguity that is often overlooked: among plausible shapes, there are still\nmultiple shapes that fit the 2D image equally well; i.e., the ground truth\nshape is non-deterministic given a single-view input. Existing fully supervised\napproaches fail to address this issue, and often produce blurry mean shapes\nwith smooth surfaces but no fine details.\n  In this paper, we propose ShapeHD, pushing the limit of single-view shape\ncompletion and reconstruction by integrating deep generative models with\nadversarially learned shape priors. The learned priors serve as a regularizer,\npenalizing the model only if its output is unrealistic, not if it deviates from\nthe ground truth. Our design thus overcomes both levels of ambiguity\naforementioned. Experiments demonstrate that ShapeHD outperforms state of the\nart by a large margin in both shape completion and shape reconstruction on\nmultiple real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:23:13 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wu", "Jiajun", ""], ["Zhang", "Chengkai", ""], ["Zhang", "Xiuming", ""], ["Zhang", "Zhoutong", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1809.05070", "submitter": "Jiajun Wu", "authors": "Zhijian Liu, William T. Freeman, Joshua B. Tenenbaum, Jiajun Wu", "title": "Physical Primitive Decomposition", "comments": "ECCV 2018. Project page: http://ppd.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects are made of parts, each with distinct geometry, physics,\nfunctionality, and affordances. Developing such a distributed, physical,\ninterpretable representation of objects will facilitate intelligent agents to\nbetter explore and interact with the world. In this paper, we study physical\nprimitive decomposition---understanding an object through its components, each\nwith physical and geometric attributes. As annotated data for object parts and\nphysics are rare, we propose a novel formulation that learns physical\nprimitives by explaining both an object's appearance and its behaviors in\nphysical events. Our model performs well on block towers and tools in both\nsynthetic and real scenarios; we also demonstrate that visual and physical\nobservations often provide complementary signals. We further present ablation\nand behavioral studies to better understand our model and contrast it with\nhuman performance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:23:20 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Liu", "Zhijian", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "1809.05076", "submitter": "Yawei Hui", "authors": "Yawei Hui, Yaohua Liu", "title": "Computer Vision-aided Atom Tracking in STEM Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the SMC'17 data challenge -- \"Data mining atomically resolved\nimages for material properties\", we first used the classic \"blob detection\"\nalgorithms developed in computer vision to identify all atom centers in each\nSTEM image frame. With the help of nearest neighbor analysis, we then found and\nlabeled every atom center common to all the STEM frames and tracked their\nmovements through the given time interval for both Molybdenum or Selenium\natoms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:33:18 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Hui", "Yawei", ""], ["Liu", "Yaohua", ""]]}, {"id": "1809.05165", "submitter": "Siyue Wang", "authors": "Siyue Wang, Xiao Wang, Pu Zhao, Wujie Wen, David Kaeli, Peter Chin,\n  Xue Lin", "title": "Defensive Dropout for Hardening Deep Neural Networks under Adversarial\n  Attacks", "comments": "Accepted as conference paper on ICCAD 2018", "journal-ref": null, "doi": "10.1145/3240765.3264699", "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That\nis, adversarial examples, obtained by adding delicately crafted distortions\nonto original legal inputs, can mislead a DNN to classify them as any target\nlabels. This work provides a solution to hardening DNNs under adversarial\nattacks through defensive dropout. Besides using dropout during training for\nthe best test accuracy, we propose to use dropout also at test time to achieve\nstrong defense effects. We consider the problem of building robust DNNs as an\nattacker-defender two-player game, where the attacker and the defender know\neach others' strategies and try to optimize their own strategies towards an\nequilibrium. Based on the observations of the effect of test dropout rate on\ntest accuracy and attack success rate, we propose a defensive dropout algorithm\nto determine an optimal test dropout rate given the neural network model and\nthe attacker's strategy for generating adversarial examples.We also investigate\nthe mechanism behind the outstanding defense effects achieved by the proposed\ndefensive dropout. Comparing with stochastic activation pruning (SAP), another\ndefense method through introducing randomness into the DNN model, we find that\nour defensive dropout achieves much larger variances of the gradients, which is\nthe key for the improved defense effects (much lower attack success rate). For\nexample, our defensive dropout can reduce the attack success rate from 100% to\n13.89% under the currently strongest attack i.e., C&W attack on MNIST dataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 20:26:32 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Wang", "Siyue", ""], ["Wang", "Xiao", ""], ["Zhao", "Pu", ""], ["Wen", "Wujie", ""], ["Kaeli", "David", ""], ["Chin", "Peter", ""], ["Lin", "Xue", ""]]}, {"id": "1809.05210", "submitter": "Laramie Paxton", "authors": "Laramie Paxton, Yufeng Cao, Kevin R. Vixie, Yuan Wang, Brian Hobbs,\n  Chaan Ng", "title": "A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors", "comments": "Image processing; image analysis; medical imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor detection in biomedical imaging is a time-consuming process for medical\nprofessionals and is not without errors. Thus in recent decades, researchers\nhave developed algorithmic techniques for image processing using a wide variety\nof mathematical methods, such as statistical modeling, variational techniques,\nand machine learning. In this paper, we propose a semi-automatic method for\nliver segmentation of 2D CT scans into three labels denoting healthy, vessel,\nor tumor tissue based on graph cuts. First, we create a feature vector for each\npixel in a novel way that consists of the 59 intensity values in the time\nseries data and propose a simplified perimeter cost term in the energy\nfunctional. We normalize the data and perimeter terms in the functional to\nexpedite the graph cut without having to optimize the scaling parameter\n$\\lambda$. In place of a training process, predetermined tissue means are\ncomputed based on sample regions identified by expert radiologists. The\nproposed method also has the advantage of being relatively simple to implement\ncomputationally. It was evaluated against the ground truth on a clinical CT\ndataset of 10 tumors and yielded segmentations with a mean Dice similarity\ncoefficient (DSC) of .77 and mean volume overlap error (VOE) of 36.7%. The\naverage processing time was 1.25 minutes per slice.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 23:56:24 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Paxton", "Laramie", ""], ["Cao", "Yufeng", ""], ["Vixie", "Kevin R.", ""], ["Wang", "Yuan", ""], ["Hobbs", "Brian", ""], ["Ng", "Chaan", ""]]}, {"id": "1809.05216", "submitter": "Avinash Kori", "authors": "Vismay Agrawal and Avinash Kori and Varghese Alex and Ganapathy\n  Krishnamurthi", "title": "Enhanced Optic Disk and Cup Segmentation with Glaucoma Screening from\n  Fundus Images using Position encoded CNNs", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this manuscript, we present a robust method for glaucoma screening from\nfundus images using an ensemble of convolutional neural networks (CNNs). The\npipeline comprises of first segmenting the optic disk and optic cup from the\nfundus image, then extracting a patch centered around the optic disk and\nsubsequently feeding to the classification network to differentiate the image\nas diseased or healthy. In the segmentation network, apart from the image, we\nmake use of spatial co-ordinate (X \\& Y) space so as to learn the structure of\ninterest better. The classification network is composed of a DenseNet201 and a\nResNet18 which were pre-trained on a large cohort of natural images. On the\nREFUGE validation data (n=400), the segmentation network achieved a dice score\nof 0.88 and 0.64 for optic disc and optic cup respectively. For the tasking\ndifferentiating images affected with glaucoma from healthy images, the area\nunder the ROC curve was observed to be 0.85.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 01:31:15 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Agrawal", "Vismay", ""], ["Kori", "Avinash", ""], ["Alex", "Varghese", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "1809.05225", "submitter": "Hyeonwoo Yu", "authors": "H. W. Yu and B. H. Le", "title": "A Variational Observation Model of 3D Object for Probabilistic Semantic\n  SLAM", "comments": "will be submitted to ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian object observation model for complete probabilistic\nsemantic SLAM. Recent studies on object detection and feature extraction have\nbecome important for scene understanding and 3D mapping. However, 3D shape of\nthe object is too complex to formulate the probabilistic observation model;\ntherefore, performing the Bayesian inference of the object-oriented features as\nwell as their pose is less considered. Besides, when the robot equipped with an\nRGB mono camera only observes the projected single view of an object, a\nsignificant amount of the 3D shape information is abandoned. Due to these\nlimitations, semantic SLAM and viewpoint-independent loop closure using\nvolumetric 3D object shape is challenging. In order to enable the complete\nformulation of probabilistic semantic SLAM, we approximate the observation\nmodel of a 3D object with a tractable distribution. We also estimate the\nvariational likelihood from the 2D image of the object to exploit its observed\nsingle view. In order to evaluate the proposed method, we perform pose and\nfeature estimation, and demonstrate that the automatic loop closure works\nseamlessly without additional loop detector in various environments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 02:27:58 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Yu", "H. W.", ""], ["Le", "B. H.", ""]]}, {"id": "1809.05231", "submitter": "Guha Balakrishnan", "authors": "Guha Balakrishnan, Amy Zhao, Mert R. Sabuncu, John Guttag, Adrian V.\n  Dalca", "title": "VoxelMorph: A Learning Framework for Deformable Medical Image\n  Registration", "comments": "Accepted to IEEE TMI ( (c) IEEE). This manuscript expands the CVPR\n  2018 paper (arXiv:1802.02604) by introducing an auxiliary model that uses\n  segmentation maps during training, an amortized optimization analysis, and\n  extensive model analysis. Code available at http://voxelmorph.csail.mit.edu", "journal-ref": null, "doi": "10.1109/TMI.2019.2897538", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VoxelMorph, a fast learning-based framework for deformable,\npairwise medical image registration. Traditional registration methods optimize\nan objective function for each pair of images, which can be time-consuming for\nlarge datasets or rich deformation models. In contrast to this approach, and\nbuilding on recent learning-based methods, we formulate registration as a\nfunction that maps an input image pair to a deformation field that aligns these\nimages. We parameterize the function via a convolutional neural network (CNN),\nand optimize the parameters of the neural network on a set of images. Given a\nnew pair of scans, VoxelMorph rapidly computes a deformation field by directly\nevaluating the function. In this work, we explore two different training\nstrategies. In the first (unsupervised) setting, we train the model to maximize\nstandard image matching objective functions that are based on the image\nintensities. In the second setting, we leverage auxiliary segmentations\navailable in the training data. We demonstrate that the unsupervised model's\naccuracy is comparable to state-of-the-art methods, while operating orders of\nmagnitude faster. We also show that VoxelMorph trained with auxiliary data\nimproves registration accuracy at test time, and evaluate the effect of\ntraining set size on registration. Our method promises to speed up medical\nimage analysis and processing pipelines, while facilitating novel directions in\nlearning-based registration and its applications. Our code is freely available\nat voxelmorph.csail.mit.edu.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 02:46:16 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 19:40:12 GMT"}, {"version": "v3", "created": "Sun, 1 Sep 2019 21:57:19 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Balakrishnan", "Guha", ""], ["Zhao", "Amy", ""], ["Sabuncu", "Mert R.", ""], ["Guttag", "John", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "1809.05262", "submitter": "Joonsang Yu", "authors": "Joonsang Yu, Sungbum Kang and Kiyoung Choi", "title": "Network Recasting: A Universal Method for Network Architecture\n  Transformation", "comments": "AAAI 2019 Oral presentation, source codes are available on github:\n  https://github.com/joonsang-yu/Network-Recasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes network recasting as a general method for network\narchitecture transformation. The primary goal of this method is to accelerate\nthe inference process through the transformation, but there can be many other\npractical applications. The method is based on block-wise recasting; it recasts\neach source block in a pre-trained teacher network to a target block in a\nstudent network. For the recasting, a target block is trained such that its\noutput activation approximates that of the source block. Such a block-by-block\nrecasting in a sequential manner transforms the network architecture while\npreserving the accuracy. This method can be used to transform an arbitrary\nteacher network type to an arbitrary student network type. It can even generate\na mixed-architecture network that consists of two or more types of block. The\nnetwork recasting can generate a network with fewer parameters and/or\nactivations, which reduce the inference time significantly. Naturally, it can\nbe used for network compression by recasting a trained network into a smaller\nnetwork of the same type. Our experiments show that it outperforms previous\ncompression approaches in terms of actual speedup on a GPU.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 05:39:15 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 09:38:15 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Yu", "Joonsang", ""], ["Kang", "Sungbum", ""], ["Choi", "Kiyoung", ""]]}, {"id": "1809.05267", "submitter": "Kanji Tanaka", "authors": "Tanaka Kanji", "title": "Detection-by-Localization: Maintenance-Free Change Object Detector", "comments": "7 pages, 3 figures, Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches demonstrate that self-localization performance is a very\nuseful measure of likelihood-of-change (LoC) for change detection. In this\npaper, this \"detection-by-localization\" scheme is studied in a novel\ngeneralized task of object-level change detection. In our framework, a given\nquery image is segmented into object-level subimages (termed \"scene parts\"),\nwhich are then converted to subimage-level pixel-wise LoC maps via the\ndetection-by-localization scheme. Our approach models a self-localization\nsystem as a ranking function, outputting a ranked list of reference images,\nwithout requiring relevance score. Thanks to this new setting, we can\ngeneralize our approach to a broad class of self-localization systems. Our\nranking based self-localization model allows to fuse self-localization results\nfrom different modalities via an unsupervised rank fusion derived from a field\nof multi-modal information retrieval (MMR).\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 06:25:55 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Kanji", "Tanaka", ""]]}, {"id": "1809.05285", "submitter": "Zhonghua Wu", "authors": "Zhonghua Wu, Guosheng Lin, Jianfei Cai", "title": "Keypoint Based Weakly Supervised Human Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional networks (FCN) have achieved great success in human\nparsing in recent years. In conventional human parsing tasks, pixel-level\nlabeling is required for guiding the training, which usually involves enormous\nhuman labeling efforts. To ease the labeling efforts, we propose a novel weakly\nsupervised human parsing method which only requires simple object keypoint\nannotations for learning. We develop an iterative learning method to generate\npseudo part segmentation masks from keypoint labels. With these pseudo masks,\nwe train an FCN network to output pixel-level human parsing predictions.\nFurthermore, we develop a correlation network to perform joint prediction of\npart and object segmentation masks and improve the segmentation performance.\nThe experiment results show that our weakly supervised method is able to\nachieve very competitive human parsing results. Despite our method only uses\nsimple keypoint annotations for learning, we are able to achieve comparable\nperformance with fully supervised methods which use the expensive pixel-level\nannotations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 07:32:43 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Wu", "Zhonghua", ""], ["Lin", "Guosheng", ""], ["Cai", "Jianfei", ""]]}, {"id": "1809.05286", "submitter": "Kian Ghodoussi", "authors": "Kian Ghodoussi, Nihar Sheth, Zane Durante, Markie Wagner", "title": "Deep CNN Frame Interpolation with Lessons Learned from Natural Language\n  Processing", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major area of growth within deep learning has been the study and\nimplementation of convolutional neural networks. The general explanation within\nthe deep learning community of the robustness of convolutional neural networks\n(CNNs) within image recognition rests upon the idea that CNNs are able to\nextract localized features. However, recent developments in fields such as\nNatural Language Processing are demonstrating that this paradigm may be\nincorrect. In this paper, we analyze the current state of the field concerning\nCNN's and present a hypothesis that provides a novel explanation for the\nrobustness of CNN models. From there, we demonstrate the effectiveness of our\napproach by presenting novel deep CNN frame interpolation architecture that is\ncomparable to the state of the art interpolation models with a fraction of the\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 07:44:46 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 00:43:58 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Ghodoussi", "Kian", ""], ["Sheth", "Nihar", ""], ["Durante", "Zane", ""], ["Wagner", "Markie", ""]]}, {"id": "1809.05292", "submitter": "Zaiyi Chen", "authors": "Zaiyi Chen", "title": "Efficient Rank Minimization via Solving Non-convexPenalties by Iterative\n  Shrinkage-Thresholding Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank minimization (RM) is a wildly investigated task of finding solutions by\nexploiting low-rank structure of parameter matrices. Recently, solving RM\nproblem by leveraging non-convex relaxations has received significant\nattention. It has been demonstrated by some theoretical and experimental work\nthat non-convex relaxation, e.g. Truncated Nuclear Norm Regularization (TNNR)\nand Reweighted Nuclear Norm Regularization (RNNR), can provide a better\napproximation of original problems than convex relaxations. However, designing\nan efficient algorithm with theoretical guarantee remains a challenging\nproblem. In this paper, we propose a simple but efficient proximal-type method,\nnamely Iterative Shrinkage-Thresholding Algorithm(ISTA), with concrete analysis\nto solve rank minimization problems with both non-convex weighted and\nreweighted nuclear norm as low-rank regularizers. Theoretically, the proposed\nmethod could converge to the critical point under very mild assumptions with\nthe rate in the order of $O(1/T)$. Moreover, the experimental results on both\nsynthetic data and real world data sets show that proposed algorithm\noutperforms state-of-arts in both efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 07:58:03 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Chen", "Zaiyi", ""]]}, {"id": "1809.05298", "submitter": "Rob Romijnders", "authors": "Rob Romijnders, Panagiotis Meletis, Gijs Dubbelman", "title": "A Domain Agnostic Normalization Layer for Unsupervised Adversarial\n  Domain Adaptation", "comments": null, "journal-ref": "IEEE WACV 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a normalization layer for unsupervised domain adaption in semantic\nscene segmentation. Normalization layers are known to improve convergence and\ngeneralization and are part of many state-of-the-art fully-convolutional neural\nnetworks. We show that conventional normalization layers worsen the performance\nof current Unsupervised Adversarial Domain Adaption (UADA), which is a method\nto improve network performance on unlabeled datasets and the focus of our\nresearch. Therefore, we propose a novel Domain Agnostic Normalization layer and\nthereby unlock the benefits of normalization layers for unsupervised\nadversarial domain adaptation. In our evaluation, we adapt from the synthetic\nGTA5 data set to the real Cityscapes data set, a common benchmark experiment,\nand surpass the state-of-the-art. As our normalization layer is domain agnostic\nat test time, we furthermore demonstrate that UADA using Domain Agnostic\nNormalization improves performance on unseen domains, specifically on\nApolloscape and Mapillary.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 08:15:52 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Romijnders", "Rob", ""], ["Meletis", "Panagiotis", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1809.05343", "submitter": "Wenbing Huang", "authors": "Wenbing Huang and Tong Zhang and Yu Rong and Junzhou Huang", "title": "Adaptive Sampling Towards Fast Graph Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have become a crucial tool on learning\nrepresentations of graph vertices. The main challenge of adapting GCNs on\nlarge-scale graphs is the scalability issue that it incurs heavy cost both in\ncomputation and memory due to the uncontrollable neighborhood expansion across\nlayers. In this paper, we accelerate the training of GCNs through developing an\nadaptive layer-wise sampling method. By constructing the network layer by layer\nin a top-down passway, we sample the lower layer conditioned on the top one,\nwhere the sampled neighborhoods are shared by different parent nodes and the\nover expansion is avoided owing to the fixed-size sampling. More importantly,\nthe proposed sampler is adaptive and applicable for explicit variance\nreduction, which in turn enhances the training of our method. Furthermore, we\npropose a novel and economical approach to promote the message passing over\ndistant nodes by applying skip connections. Intensive experiments on several\nbenchmarks verify the effectiveness of our method regarding the classification\naccuracy while enjoying faster convergence speed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 10:33:27 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 10:15:14 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 07:50:26 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Huang", "Wenbing", ""], ["Zhang", "Tong", ""], ["Rong", "Yu", ""], ["Huang", "Junzhou", ""]]}, {"id": "1809.05370", "submitter": "Mattias Heinrich", "authors": "Lasse Hansen and Jasper Diesel and Mattias P. Heinrich", "title": "Multi-Kernel Diffusion CNNs for Graph-Based Learning on Point Clouds", "comments": "accepted for ECCV 2018 Workshop Geometry Meets Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks are a new promising learning approach to deal\nwith data on irregular domains. They are predestined to overcome certain\nlimitations of conventional grid-based architectures and will enable efficient\nhandling of point clouds or related graphical data representations, e.g.\nsuperpixel graphs. Learning feature extractors and classifiers on 3D point\nclouds is still an underdeveloped area and has potential restrictions to equal\ngraph topologies. In this work, we derive a new architectural design that\ncombines rotationally and topologically invariant graph diffusion operators and\nnode-wise feature learning through 1x1 convolutions. By combining multiple\nisotropic diffusion operations based on the Laplace-Beltrami operator, we can\nlearn an optimal linear combination of diffusion kernels for effective feature\npropagation across nodes on an irregular graph. We validated our approach for\nlearning point descriptors as well as semantic classification on real 3D point\nclouds of human poses and demonstrate an improvement from 85% to 95% in Dice\noverlap with our multi-kernel approach.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 12:15:37 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Hansen", "Lasse", ""], ["Diesel", "Jasper", ""], ["Heinrich", "Mattias P.", ""]]}, {"id": "1809.05375", "submitter": "Philip Jackson", "authors": "Philip T. Jackson, Amir Atapour-Abarghouei, Stephen Bonner, Toby\n  Breckon, Boguslaw Obara", "title": "Style Augmentation: Data Augmentation via Style Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce style augmentation, a new form of data augmentation based on\nrandom style transfer, for improving the robustness of convolutional neural\nnetworks (CNN) over both classification and regression based tasks. During\ntraining, our style augmentation randomizes texture, contrast and color, while\npreserving shape and semantic content. This is accomplished by adapting an\narbitrary style transfer network to perform style randomization, by sampling\ninput style embeddings from a multivariate normal distribution instead of\ninferring them from a style image. In addition to standard classification\nexperiments, we investigate the effect of style augmentation (and data\naugmentation generally) on domain transfer tasks. We find that data\naugmentation significantly improves robustness to domain shift, and can be used\nas a simple, domain agnostic alternative to domain adaptation. Comparing style\naugmentation against a mix of seven traditional augmentation techniques, we\nfind that it can be readily combined with them to improve network performance.\nWe validate the efficacy of our technique with domain transfer experiments in\nclassification and monocular depth estimation, illustrating consistent\nimprovements in generalization.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 12:34:36 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 16:58:20 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Jackson", "Philip T.", ""], ["Atapour-Abarghouei", "Amir", ""], ["Bonner", "Stephen", ""], ["Breckon", "Toby", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1809.05398", "submitter": "Kai Xu", "authors": "Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Renjiao Yi, Hao Zhang", "title": "SCORES: Shape Composition with Recursive Substructure Priors", "comments": "Accepted to SIGGRAPH Asia 2018. Corresponding Author: Kai Xu\n  (kevin.kai.xu@gmail.com)", "journal-ref": "ACM Transactions on Graphics, 2018", "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SCORES, a recursive neural network for shape composition. Our\nnetwork takes as input sets of parts from two or more source 3D shapes and a\nrough initial placement of the parts. It outputs an optimized part structure\nfor the composed shape, leading to high-quality geometry construction. A unique\nfeature of our composition network is that it is not merely learning how to\nconnect parts. Our goal is to produce a coherent and plausible 3D shape,\ndespite large incompatibilities among the input parts. The network may\nsignificantly alter the geometry and structure of the input parts and\nsynthesize a novel shape structure based on the inputs, while adding or\nremoving parts to minimize a structure plausibility loss. We design SCORES as a\nrecursive autoencoder network. During encoding, the input parts are recursively\ngrouped to generate a root code. During synthesis, the root code is decoded,\nrecursively, to produce a new, coherent part assembly. Assembled shape\nstructures may be novel, with little global resemblance to training exemplars,\nyet have plausible substructures. SCORES therefore learns a hierarchical\nsubstructure shape prior based on per-node losses. It is trained on structured\nshapes from ShapeNet, and is applied iteratively to reduce the plausibility\nloss.We showresults of shape composition from multiple sources over different\ncategories of man-made shapes and compare with state-of-the-art alternatives,\ndemonstrating that our network can significantly expand the range of composable\nshapes for assembly-based modeling.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 13:20:06 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Zhu", "Chenyang", ""], ["Xu", "Kai", ""], ["Chaudhuri", "Siddhartha", ""], ["Yi", "Renjiao", ""], ["Zhang", "Hao", ""]]}, {"id": "1809.05408", "submitter": "Ce Ju", "authors": "Ce Ju, Zheng Wang, and Xiaoyu Zhang", "title": "Socially Aware Kalman Neural Networks for Trajectory Prediction", "comments": "Superceded by arXiv:1902.10928", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction is a critical technique in the navigation of robots and\nautonomous vehicles. However, the complex traffic and dynamic uncertainties\nyield challenges in the effectiveness and robustness in modeling. We purpose a\ndata-driven approach socially aware Kalman neural networks (SAKNN) where the\ninteraction layer and the Kalman layer are embedded in the architecture,\nresulting in a class of architectures with huge potential to directly learn\nfrom high variance sensor input and robustly generate low variance outcomes.\nThe evaluation of our approach on NGSIM dataset demonstrates that SAKNN\nperforms state-of-the-art on prediction effectiveness in a relatively long-term\nhorizon and significantly improves the signal-to-noise ratio of the predicted\nsignal.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 13:27:53 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 03:17:17 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 07:55:13 GMT"}, {"version": "v4", "created": "Wed, 26 Jun 2019 14:06:42 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Ju", "Ce", ""], ["Wang", "Zheng", ""], ["Zhang", "Xiaoyu", ""]]}, {"id": "1809.05474", "submitter": "Heikki Huttunen", "authors": "Janne Tommola, Pedram Ghazi, Bishwo Adhikari, Heikki Huttunen", "title": "Real Time System for Facial Analysis", "comments": "Submitted to EUVIP2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the anatomy of a real-time facial analysis system.\nThe system recognizes the age, gender and facial expression from users in\nappearing in front of the camera. All components are based on convolutional\nneural networks, whose accuracy we study on commonly used training and\nevaluation sets. A key contribution of the work is the description of the\ninterplay between processing threads for frame grabbing, face detection and the\nthree types of recognition. The python code for executing the system uses\ncommon libraries--keras/tensorflow, opencv and dlib--and is available for\ndownload.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 15:45:29 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Tommola", "Janne", ""], ["Ghazi", "Pedram", ""], ["Adhikari", "Bishwo", ""], ["Huttunen", "Heikki", ""]]}, {"id": "1809.05491", "submitter": "Xiuming Zhang", "authors": "Xiuming Zhang, Tali Dekel, Tianfan Xue, Andrew Owens, Qiurui He,\n  Jiajun Wu, Stefanie Mueller, William T. Freeman", "title": "MoSculp: Interactive Visualization of Shape and Time", "comments": "UIST 2018. Project page: http://mosculp.csail.mit.edu/", "journal-ref": null, "doi": "10.1145/3242587.3242592", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that allows users to visualize complex human motion via\n3D motion sculptures---a representation that conveys the 3D structure swept by\na human body as it moves through space. Given an input video, our system\ncomputes the motion sculptures and provides a user interface for rendering it\nin different styles, including the options to insert the sculpture back into\nthe original video, render it in a synthetic scene or physically print it.\n  To provide this end-to-end workflow, we introduce an algorithm that estimates\nthat human's 3D geometry over time from a set of 2D images and develop a\n3D-aware image-based rendering approach that embeds the sculpture back into the\nscene. By automating the process, our system takes motion sculpture creation\nout of the realm of professional artists, and makes it applicable to a wide\nrange of existing video material.\n  By providing viewers with 3D information, motion sculptures reveal space-time\nmotion information that is difficult to perceive with the naked eye, and allow\nviewers to interpret how different parts of the object interact over time. We\nvalidate the effectiveness of this approach with user studies, finding that our\nmotion sculpture visualizations are significantly more informative about motion\nthan existing stroboscopic and space-time visualization methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:27:08 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 17:56:47 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhang", "Xiuming", ""], ["Dekel", "Tali", ""], ["Xue", "Tianfan", ""], ["Owens", "Andrew", ""], ["He", "Qiurui", ""], ["Wu", "Jiajun", ""], ["Mueller", "Stefanie", ""], ["Freeman", "William T.", ""]]}, {"id": "1809.05499", "submitter": "Stefano Moriconi", "authors": "Stefano Moriconi, Maria A. Zuluaga, H. Rolf J\u007fager, Parashkev Nachev,\n  Sebastien Ourselin, and M. Jorge Cardoso", "title": "Elastic Registration of Geodesic Vascular Graphs", "comments": null, "journal-ref": "Medical Image Computing and Computer Assisted Intervention --\n  MICCAI 2018", "doi": "10.1007/978-3-030-00928-1_91", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vascular graphs can embed a number of high-level features, from morphological\nparameters, to functional biomarkers, and represent an invaluable tool for\nlongitudinal and cross-sectional clinical inference. This, however, is only\nfeasible when graphs are co-registered together, allowing coherent multiple\ncomparisons. The robust registration of vascular topologies stands therefore as\nkey enabling technology for group-wise analyses. In this work, we present an\nend-to-end vascular graph registration approach, that aligns networks with\nnon-linear geometries and topological deformations, by introducing a novel\noverconnected geodesic vascular graph formulation, and without enforcing any\nanatomical prior constraint. The 3D elastic graph registration is then\nperformed with state-of-the-art graph matching methods used in computer vision.\nPromising results of vascular matching are found using graphs from synthetic\nand real angiographies. Observations and future designs are discussed towards\npotential clinical applications.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:43:25 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Moriconi", "Stefano", ""], ["Zuluaga", "Maria A.", ""], ["J\u007fager", "H. Rolf", ""], ["Nachev", "Parashkev", ""], ["Ourselin", "Sebastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1809.05510", "submitter": "Bas Van Der Velden", "authors": "Bas H.M. van der Velden", "title": "BPE and computer-extracted parenchymal enhancement for breast cancer\n  risk, response monitoring, and prognosis", "comments": "This work has been distributed as part of the syllabus at the ISMRM\n  Workshop on Breast MRI: Advancing the State of the Art", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional behavior of breast cancer - representing underlying biology - can\nbe analyzed using MRI. The most widely used breast MR imaging protocol is\ndynamic contrast-enhanced T1-weighted imaging. The cancer enhances on dynamic\ncontrast-enhanced MR imaging because the contrast agent leaks from the leaky\nvessels into the interstitial space. The contrast agent subsequently leaks back\ninto the vascular space, creating a washout effect. The normal parenchymal\ntissue of the breast can also enhance after contrast injection. This\nenhancement generally increases over time. Typically, a radiologist assesses\nthis background parenchymal enhancement (BPE) using the Breast Imaging\nReporting and Data System (BI-RADS). According to the BI-RADS, BPE refers to\nthe volume of enhancement and the intensity of enhancement and is divided in\nfour incremental categories: minimal, mild, moderate, and marked.\n  Researchers have developed semi-automatic and automatic methods to extract\nproperties of BPE from MR images. For clarity, in this syllabus the BI-RADS\ndefinition will be referred to as BPE, whereas the computer-extracted\nproperties will not. Both BPE and computer-extracted parenchymal enhancement\nproperties have been linked to screening and diagnosis, hormone status and age,\nrisk of development of breast cancer, response monitoring, and prognosis.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 17:23:12 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["van der Velden", "Bas H. M.", ""]]}, {"id": "1809.05557", "submitter": "Hongming Li", "authors": "Hongming Li, Xiaofeng Zhu, Yong Fan", "title": "Identification of multi-scale hierarchical brain functional networks\n  using deep matrix factorization", "comments": "Accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep semi-nonnegative matrix factorization method for\nidentifying subject-specific functional networks (FNs) at multiple spatial\nscales with a hierarchical organization from resting state fMRI data. Our\nmethod is built upon a deep semi-nonnegative matrix factorization framework to\njointly detect the FNs at multiple scales with a hierarchical organization,\nenhanced by group sparsity regularization that helps identify subject-specific\nFNs without loss of inter-subject comparability. The proposed method has been\nvalidated for predicting subject-specific functional activations based on\nfunctional connectivity measures of the hierarchical multi-scale FNs of the\nsame subjects. Experimental results have demonstrated that our method could\nobtain subject-specific multi-scale hierarchical FNs and their functional\nconnectivity measures across different scales could better predict\nsubject-specific functional activations than those obtained by alternative\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 18:48:49 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Li", "Hongming", ""], ["Zhu", "Xiaofeng", ""], ["Fan", "Yong", ""]]}, {"id": "1809.05560", "submitter": "Hongming Li", "authors": "Hongming Li, Yong Fan", "title": "Identification of temporal transition of functional states using\n  recurrent neural networks from functional MRI", "comments": "Accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic functional connectivity analysis provides valuable information for\nunderstanding brain functional activity underlying different cognitive\nprocesses. Besides sliding window based approaches, a variety of methods have\nbeen developed to automatically split the entire functional MRI scan into\nsegments by detecting change points of functional signals to facilitate better\ncharacterization of temporally dynamic functional connectivity patterns.\nHowever, these methods are based on certain assumptions for the functional\nsignals, such as Gaussian distribution, which are not necessarily suitable for\nthe fMRI data. In this study, we develop a deep learning based framework for\nadaptively detecting temporally dynamic functional state transitions in a\ndata-driven way without any explicit modeling assumptions, by leveraging recent\nadvances in recurrent neural networks (RNNs) for sequence modeling.\nParticularly, we solve this problem in an anomaly detection framework with an\nassumption that the functional profile of one single time point could be\nreliably predicted based on its preceding profiles within stable functional\nstate, while large prediction errors would occur around change points of\nfunctional states. We evaluate the proposed method using both task and\nresting-state fMRI data obtained from the human connectome project and\nexperimental results have demonstrated that the proposed change point detection\nmethod could effectively identify change points between different task events\nand split the resting-state fMRI into segments with distinct functional\nconnectivity patterns.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 18:59:32 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Li", "Hongming", ""], ["Fan", "Yong", ""]]}, {"id": "1809.05561", "submitter": "Hongming Li", "authors": "Hongming Li, Yong Fan", "title": "Brain decoding from functional MRI using long short-term memory\n  recurrent neural networks", "comments": "Accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding brain functional states underlying different cognitive processes\nusing multivariate pattern recognition techniques has attracted increasing\ninterests in brain imaging studies. Promising performance has been achieved\nusing brain functional connectivity or brain activation signatures for a\nvariety of brain decoding tasks. However, most of existing studies have built\ndecoding models upon features extracted from imaging data at individual time\npoints or temporal windows with a fixed interval, which might not be optimal\nacross different cognitive processes due to varying temporal durations and\ndependency of different cognitive processes. In this study, we develop a deep\nlearning based framework for brain decoding by leveraging recent advances in\nsequence modeling using long short-term memory (LSTM) recurrent neural networks\n(RNNs). Particularly, functional profiles extracted from task functional\nimaging data based on their corresponding subject-specific intrinsic functional\nnetworks are used as features to build brain decoding models, and LSTM RNNs are\nadopted to learn decoding mappings between functional profiles and brain\nstates. We evaluate the proposed method using task fMRI data from the HCP\ndataset, and experimental results have demonstrated that the proposed method\ncould effectively distinguish brain states under different task events and\nobtain higher accuracy than conventional decoding models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 19:06:58 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Li", "Hongming", ""], ["Fan", "Yong", ""]]}, {"id": "1809.05571", "submitter": "Deqing Sun", "authors": "Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz", "title": "Models Matter, So Does Training: An Empirical Study of CNNs for Optical\n  Flow Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two crucial and closely related aspects of CNNs for optical\nflow estimation: models and training. First, we design a compact but effective\nCNN model, called PWC-Net, according to simple and well-established principles:\npyramidal processing, warping, and cost volume processing. PWC-Net is 17 times\nsmaller in size, 2 times faster in inference, and 11\\% more accurate on Sintel\nfinal than the recent FlowNet2 model. It is the winning entry in the optical\nflow competition of the robust vision challenge. Next, we experimentally\nanalyze the sources of our performance gains. In particular, we use the same\ntraining procedure of PWC-Net to retrain FlowNetC, a sub-network of FlowNet2.\nThe retrained FlowNetC is 56\\% more accurate on Sintel final than the\npreviously trained one and even 5\\% more accurate than the FlowNet2 model. We\nfurther improve the training procedure and increase the accuracy of PWC-Net on\nSintel by 10\\% and on KITTI 2012 and 2015 by 20\\%. Our newly trained model\nparameters and training protocols will be available on\nhttps://github.com/NVlabs/PWC-Net\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 20:27:49 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Sun", "Deqing", ""], ["Yang", "Xiaodong", ""], ["Liu", "Ming-Yu", ""], ["Kautz", "Jan", ""]]}, {"id": "1809.05611", "submitter": "Sebastin Santy", "authors": "Wazeer Zulfikar, Sebastin Santy, Sahith Dambekodi, Tirtharaj Dash", "title": "A study on the use of Boundary Equilibrium GAN for Approximate\n  Frontalization of Unconstrained Faces to aid in Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Face frontalization is the process of synthesizing frontal facing views of\nfaces given its angled poses. We implement a generative adversarial network\n(GAN) with spherical linear interpolation (Slerp) for frontalization of\nunconstrained facial images. Our special focus is intended towards the\ngeneration of approximate frontal faces of the side posed images captured from\nsurveillance cameras. Specifically, the present work is a comprehensive study\non the implementation of an auto-encoder based Boundary Equilibrium GAN (BEGAN)\nto generate frontal faces using an interpolation of a side view face and its\nmirrored view. To increase the quality of the interpolated output we implement\na BEGAN with Slerp. This approach could produce a promising output along with a\nfaster and more stable training for the model. The BEGAN model additionally has\na balanced generator-discriminator combination, which prevents mode collapse\nalong with a global convergence measure. It is expected that such an\napproximate face generation model would be able to replace face composites used\nin surveillance and crime detection.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 23:02:03 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Zulfikar", "Wazeer", ""], ["Santy", "Sebastin", ""], ["Dambekodi", "Sahith", ""], ["Dash", "Tirtharaj", ""]]}, {"id": "1809.05620", "submitter": "Yichun Shi", "authors": "Yichun Shi and Anil K. Jain", "title": "DocFace+: ID Document to Selfie Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous activities in our daily life require us to verify who we are by\nshowing our ID documents containing face images, such as passports and driver\nlicenses, to human operators. However, this process is slow, labor intensive\nand unreliable. As such, an automated system for matching ID document photos to\nlive face images (selfies) in real time and with high accuracy is required. In\nthis paper, we propose DocFace+ to meet this objective. We first show that\ngradient-based optimization methods converge slowly (due to the underfitting of\nclassifier weights) when many classes have very few samples, a characteristic\nof existing ID-selfie datasets. To overcome this shortcoming, we propose a\nmethod, called dynamic weight imprinting (DWI), to update the classifier\nweights, which allows faster convergence and more generalizable\nrepresentations. Next, a pair of sibling networks with partially shared\nparameters are trained to learn a unified face representation with\ndomain-specific parameters. Cross-validation on an ID-selfie dataset shows that\nwhile a publicly available general face matcher (SphereFace) only achieves a\nTrue Accept Rate (TAR) of 59.29+-1.55% at a False Accept Rate (FAR) of 0.1% on\nthe problem, DocFace+ improves the TAR to 97.51+-0.40%.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 00:31:44 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 05:15:51 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Shi", "Yichun", ""], ["Jain", "Anil K.", ""]]}, {"id": "1809.05645", "submitter": "Jake Sganga", "authors": "Jake Sganga, David Eng, Chauncey Graetzel, and David Camarillo", "title": "OffsetNet: Deep Learning for Localization in the Lung using Rendered\n  Images", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigating surgical tools in the dynamic and tortuous anatomy of the lung's\nairways requires accurate, real-time localization of the tools with respect to\nthe preoperative scan of the anatomy. Such localization can inform human\noperators or enable closed-loop control by autonomous agents, which would\nrequire accuracy not yet reported in the literature. In this paper, we\nintroduce a deep learning architecture, called OffsetNet, to accurately\nlocalize a bronchoscope in the lung in real-time. After training on only 30\nminutes of recorded camera images in conserved regions of a lung phantom,\nOffsetNet tracks the bronchoscope's motion on a held-out recording through\nthese same regions at an update rate of 47 Hz and an average position error of\n1.4 mm. Because this model performs poorly in less conserved regions, we\naugment the training dataset with simulated images from these regions. To\nbridge the gap between camera and simulated domains, we implement domain\nrandomization and a generative adversarial network (GAN). After training on\nsimulated images, OffsetNet tracks the bronchoscope's motion in less conserved\nregions at an average position error of 2.4 mm, which meets conservative\nthresholds required for successful tracking.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 04:15:16 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Sganga", "Jake", ""], ["Eng", "David", ""], ["Graetzel", "Chauncey", ""], ["Camarillo", "David", ""]]}, {"id": "1809.05680", "submitter": "Wenhao Ding", "authors": "Wenhao Ding and Wenshuo Wang and Ding Zhao", "title": "A New Multi-vehicle Trajectory Generator to Simulate Vehicle-to-Vehicle\n  Encounters", "comments": "6 pages, accepted by ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating multi-vehicle trajectories from existing limited data can provide\nrich resources for autonomous vehicle development and testing. This paper\nintroduces a multi-vehicle trajectory generator (MTG) that can encode\nmulti-vehicle interaction scenarios (called driving encounters) into an\ninterpretable representation from which new driving encounter scenarios are\ngenerated by sampling. The MTG consists of a bi-directional encoder and a\nmulti-branch decoder. A new disentanglement metric is then developed for model\nanalyses and comparisons in terms of model robustness and the independence of\nthe latent codes. Comparison of our proposed MTG with $\\beta$-VAE and InfoGAN\ndemonstrates that the MTG has stronger capability to purposely generate\nrational vehicle-to-vehicle encounters through operating the disentangled\nlatent codes. Thus the MTG could provide more data for engineers and\nresearchers to develop testing and evaluation scenarios for autonomous\nvehicles.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 09:16:34 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 14:24:55 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 05:42:39 GMT"}, {"version": "v4", "created": "Thu, 14 Feb 2019 04:16:25 GMT"}, {"version": "v5", "created": "Sun, 24 Feb 2019 14:53:07 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Ding", "Wenhao", ""], ["Wang", "Wenshuo", ""], ["Zhao", "Ding", ""]]}, {"id": "1809.05717", "submitter": "Thuong Nguyen Canh", "authors": "Thuong Nguyen Canh and Byeungwoo Jeon", "title": "Multi-Scale Deep Compressive Sensing Network", "comments": "4 pages, 4 figures, 2 tables, IEEE International Conference on Visual\n  Communication and Image Processing (VCIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With joint learning of sampling and recovery, the deep learning-based\ncompressive sensing (DCS) has shown significant improvement in performance and\nrunning time reduction. Its reconstructed image, however, losses high-frequency\ncontent especially at low subrates. This happens similarly in the multi-scale\nsampling scheme which also samples more low-frequency components. In this\npaper, we propose a multi-scale DCS convolutional neural network (MS-DCSNet) in\nwhich we convert image signal using multiple scale-based wavelet transform,\nthen capture it through convolution block by block across scales. The initial\nreconstructed image is directly recovered from multi-scale measurements.\nMulti-scale wavelet convolution is utilized to enhance the final reconstruction\nquality. The network is able to learn both multi-scale sampling and multi-scale\nreconstruction, thus results in better reconstruction quality.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 14:05:27 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 10:51:07 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Canh", "Thuong Nguyen", ""], ["Jeon", "Byeungwoo", ""]]}, {"id": "1809.05782", "submitter": "Jean-Baptiste Lamare", "authors": "Ankit Shah, Jean Baptiste Lamare, Tuan Nguyen Anh, Alexander Hauptmann", "title": "CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis", "comments": "Accepted at IEEE International Workshop on Traffic and Street\n  Surveillance for Safety and Security, First three authors contributed\n  equally, 7 pages + 1 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel dataset for traffic accidents analysis. Our goal\nis to resolve the lack of public data for research about automatic\nspatio-temporal annotations for traffic safety in the roads. Through the\nanalysis of the proposed dataset, we observed a significant degradation of\nobject detection in pedestrian category in our dataset, due to the object sizes\nand complexity of the scenes. To this end, we propose to integrate contextual\ninformation into conventional Faster R-CNN using Context Mining (CM) and\nAugmented Context Mining (ACM) to complement the accuracy for small pedestrian\ndetection. Our experiments indicate a considerable improvement in object\ndetection accuracy: +8.51% for CM and +6.20% for ACM. Finally, we demonstrate\nthe performance of accident forecasting in our dataset using Faster R-CNN and\nan Accident LSTM architecture. We achieved an average of 1.684 seconds in terms\nof Time-To-Accident measure with an Average Precision of 47.25%. Our Webpage\nfor the paper is https://goo.gl/cqK2wE\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 00:01:39 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 06:28:36 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Shah", "Ankit", ""], ["Lamare", "Jean Baptiste", ""], ["Anh", "Tuan Nguyen", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1809.05786", "submitter": "Yasin Almalioglu", "authors": "Yasin Almalioglu, Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao,\n  Andrew Markham, Niki Trigoni", "title": "GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation\n  with Generative Adversarial Networks", "comments": "ICRA 2019 - accepted", "journal-ref": "2019 International Conference on Robotics and Automation (ICRA)", "doi": "10.1109/ICRA.2019.8793512", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, supervised deep learning approaches have been extensively\nemployed in visual odometry (VO) applications, which is not feasible in\nenvironments where labelled data is not abundant. On the other hand,\nunsupervised deep learning approaches for localization and mapping in unknown\nenvironments from unlabelled data have received comparatively less attention in\nVO research. In this study, we propose a generative unsupervised learning\nframework that predicts 6-DoF pose camera motion and monocular depth map of the\nscene from unlabelled RGB image sequences, using deep convolutional Generative\nAdversarial Networks (GANs). We create a supervisory signal by warping view\nsequences and assigning the re-projection minimization to the objective loss\nfunction that is adopted in multi-view pose estimation and single-view depth\ngeneration network. Detailed quantitative and qualitative evaluations of the\nproposed framework on the KITTI and Cityscapes datasets show that the proposed\nmethod outperforms both existing traditional and unsupervised deep VO methods\nproviding better results for both pose estimation and depth recovery.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 00:27:09 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 14:01:53 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 12:45:15 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Almalioglu", "Yasin", ""], ["Saputra", "Muhamad Risqi U.", ""], ["de Gusmao", "Pedro P. B.", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1809.05825", "submitter": "Matthew Matl", "authors": "Michael Danielczuk, Matthew Matl, Saurabh Gupta, Andrew Li, Andrew\n  Lee, Jeffrey Mahler, Ken Goldberg", "title": "Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN\n  Trained on Synthetic Data", "comments": "In proceedings of ICRA 2019/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to segment unknown objects in depth images has potential to\nenhance robot skills in grasping and object tracking. Recent computer vision\nresearch has demonstrated that Mask R-CNN can be trained to segment specific\ncategories of objects in RGB images when massive hand-labeled datasets are\navailable. As generating these datasets is time consuming, we instead train\nwith synthetic depth images. Many robots now use depth sensors, and recent\nresults suggest training on synthetic depth data can transfer successfully to\nthe real world. We present a method for automated dataset generation and\nrapidly generate a synthetic training dataset of 50,000 depth images and\n320,000 object masks using simulated heaps of 3D CAD models. We train a variant\nof Mask R-CNN with domain randomization on the generated dataset to perform\ncategory-agnostic instance segmentation without any hand-labeled data and we\nevaluate the trained network, which we refer to as Synthetic Depth (SD) Mask\nR-CNN, on a set of real, high-resolution depth images of challenging,\ndensely-cluttered bins containing objects with highly-varied geometry. SD Mask\nR-CNN outperforms point cloud clustering baselines by an absolute 15% in\nAverage Precision and 20% in Average Recall on COCO benchmarks, and achieves\nperformance levels similar to a Mask R-CNN trained on a massive, hand-labeled\nRGB dataset and fine-tuned on real images from the experimental setup. We\ndeploy the model in an instance-specific grasping pipeline to demonstrate its\nusefulness in a robotics application. Code, the synthetic training dataset, and\nsupplementary material are available at https://bit.ly/2letCuE.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 07:08:58 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 01:29:13 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Danielczuk", "Michael", ""], ["Matl", "Matthew", ""], ["Gupta", "Saurabh", ""], ["Li", "Andrew", ""], ["Lee", "Andrew", ""], ["Mahler", "Jeffrey", ""], ["Goldberg", "Ken", ""]]}, {"id": "1809.05828", "submitter": "Se Young Chun", "authors": "Dongwon Park, Yonghyeok Seo, Se Young Chun", "title": "Real-Time, Highly Accurate Robotic Grasp Detection using Fully\n  Convolutional Neural Networks with High-Resolution Images", "comments": "This work was superceded by arXiv:1812.07762", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic grasp detection for novel objects is a challenging task, but for the\nlast few years, deep learning based approaches have achieved remarkable\nperformance improvements, up to 96.1% accuracy, with RGB-D data. In this paper,\nwe propose fully convolutional neural network (FCNN) based methods for robotic\ngrasp detection. Our methods also achieved state-of-the-art detection accuracy\n(up to 96.6%) with state-of- the-art real-time computation time for\nhigh-resolution images (6-20ms per 360x360 image) on Cornell dataset. Due to\nFCNN, our proposed method can be applied to images with any size for detecting\nmultigrasps on multiobjects. Proposed methods were evaluated using 4-axis robot\narm with small parallel gripper and RGB-D camera for grasping challenging\nsmall, novel objects. With accurate vision-robot coordinate calibration through\nour proposed learning-based, fully automatic approach, our proposed method\nyielded 90% success rate.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 07:36:02 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 08:05:22 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Park", "Dongwon", ""], ["Seo", "Yonghyeok", ""], ["Chun", "Se Young", ""]]}, {"id": "1809.05831", "submitter": "Mohsen Hajabdollahi", "authors": "Mohsen Hajabdollahi, Reza Esfandiarpoor, Elyas Sabeti, Nader Karimi,\n  Kayvan Najarian, S.M. Reza Soroushmehr, Shadrokh Samavi", "title": "Multiple Abnormality Detection for Automatic Medical Image Diagnosis\n  Using Bifurcated Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating classification and segmentation process of abnormal regions in\ndifferent body organs has a crucial role in most of medical imaging\napplications such as funduscopy, endoscopy, and dermoscopy. Detecting multiple\nabnormalities in each type of images is necessary for better and more accurate\ndiagnosis procedure and medical decisions. In recent years portable medical\nimaging devices such as capsule endoscopy and digital dermatoscope have been\nintroduced and made the diagnosis procedure easier and more efficient. However,\nthese portable devices have constrained power resources and limited\ncomputational capability. To address this problem, we propose a bifurcated\nstructure for convolutional neural networks performing both classification and\nsegmentation of multiple abnormalities simultaneously. The proposed network is\nfirst trained by each abnormality separately. Then the network is trained using\nall abnormalities. In order to reduce the computational complexity, the network\nis redesigned to share some features which are common among all abnormalities.\nLater, these shared features are used in different settings (directions) to\nsegment and classify the abnormal region of the image. Finally, results of the\nclassification and segmentation directions are fused to obtain the classified\nsegmentation map. Proposed framework is simulated using four frequent\ngastrointestinal abnormalities as well as three dermoscopic lesions and for\nevaluation of the proposed framework the results are compared with the\ncorresponding ground truth map. Properties of the bifurcated network like low\ncomplexity and resource sharing make it suitable to be implemented as a part of\nportable medical imaging devices.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 07:55:51 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 20:02:15 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Hajabdollahi", "Mohsen", ""], ["Esfandiarpoor", "Reza", ""], ["Sabeti", "Elyas", ""], ["Karimi", "Nader", ""], ["Najarian", "Kayvan", ""], ["Soroushmehr", "S. M. Reza", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1809.05848", "submitter": "Liu Jinlai", "authors": "Jinlai Liu, Zehuan Yuan, Changhu Wang", "title": "Towards Good Practices for Multi-modal Fusion in Large-scale Video\n  Classification", "comments": "ECCV YouTube-8M workshop general paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging both visual frames and audio has been experimentally proven\neffective to improve large-scale video classification. Previous research on\nvideo classification mainly focuses on the analysis of visual content among\nextracted video frames and their temporal feature aggregation. In contrast,\nmultimodal data fusion is achieved by simple operators like average and\nconcatenation. Inspired by the success of bilinear pooling in the visual and\nlanguage fusion, we introduce multi-modal factorized bilinear pooling (MFB) to\nfuse visual and audio representations. We combine MFB with different\nvideo-level features and explore its effectiveness in video classification.\nExperimental results on the challenging Youtube-8M v2 dataset demonstrate that\nMFB significantly outperforms simple fusion methods in large-scale video\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 10:17:37 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 02:28:12 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 03:52:16 GMT"}, {"version": "v4", "created": "Fri, 28 Sep 2018 02:12:57 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Liu", "Jinlai", ""], ["Yuan", "Zehuan", ""], ["Wang", "Changhu", ""]]}, {"id": "1809.05852", "submitter": "Huan Fu", "authors": "Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Kun Zhang,\n  Dacheng Tao", "title": "Geometry-Consistent Generative Adversarial Networks for One-Sided\n  Unsupervised Domain Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain mapping aims to learn a function to translate domain X to\nY by a function GXY in the absence of paired examples. Finding the optimal GXY\nwithout paired data is an ill-posed problem, so appropriate constraints are\nrequired to obtain reasonable solutions. One of the most prominent constraints\nis cycle consistency, which enforces the translated image by GXY to be\ntranslated back to the input image by an inverse mapping GYX. While cycle\nconsistency requires the simultaneous training of GXY and GY X, recent studies\nhave shown that one-sided domain mapping can be achieved by preserving pairwise\ndistances between images. Although cycle consistency and distance preservation\nsuccessfully constrain the solution space, they overlook the special properties\nthat simple geometric transformations do not change the semantic structure of\nimages. Based on this special property, we develop a geometry-consistent\ngenerative adversarial network (GcGAN), which enables one-sided unsupervised\ndomain mapping. GcGAN takes the original image and its counterpart image\ntransformed by a predefined geometric transformation as inputs and generates\ntwo images in the new domain coupled with the corresponding\ngeometry-consistency constraint. The geometry-consistency constraint reduces\nthe space of possible solutions while keep the correct solutions in the search\nspace. Quantitative and qualitative comparisons with the baseline (GAN alone)\nand the state-of-the-art methods including CycleGAN and DistanceGAN demonstrate\nthe effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 10:42:29 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 00:49:30 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Fu", "Huan", ""], ["Gong", "Mingming", ""], ["Wang", "Chaohui", ""], ["Batmanghelich", "Kayhan", ""], ["Zhang", "Kun", ""], ["Tao", "Dacheng", ""]]}, {"id": "1809.05861", "submitter": "Jianlin Su", "authors": "Jianlin Su, Guang Wu", "title": "f-VAEs: Improve VAEs with Conditional Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we integrate VAEs and flow-based generative models\nsuccessfully and get f-VAEs. Compared with VAEs, f-VAEs generate more vivid\nimages, solved the blurred-image problem of VAEs. Compared with flow-based\nmodels such as Glow, f-VAE is more lightweight and converges faster, achieving\nthe same performance under smaller-size architecture.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 12:23:09 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Su", "Jianlin", ""], ["Wu", "Guang", ""]]}, {"id": "1809.05864", "submitter": "Yao Zhai", "authors": "Yao Zhai, Xun Guo, Yan Lu and Houqiang Li", "title": "In Defense of the Classification Loss for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent research for person re-identification has been focused on two\ntrends. One is learning the part-based local features to form more informative\nfeature descriptors. The other is designing effective metric learning loss\nfunctions such as the triplet loss family. We argue that learning global\nfeatures with classification loss could achieve the same goal, even with some\nsimple and cost-effective architecture design. In this paper, we first explain\nwhy the person re-id framework with standard classification loss usually has\ninferior performance compared to metric learning. Based on that, we further\npropose a person re-id framework featured by channel grouping and multi-branch\nstrategy, which divides global features into multiple channel groups and learns\nthe discriminative channel group features by multi-branch classification\nlayers. The extensive experiments show that our framework outperforms prior\nstate-of-the-arts in terms of both accuracy and inference speed.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 12:35:53 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 02:54:49 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Zhai", "Yao", ""], ["Guo", "Xun", ""], ["Lu", "Yan", ""], ["Li", "Houqiang", ""]]}, {"id": "1809.05878", "submitter": "Adeyinka K. Akanbi MR", "authors": "Y. O. Agunbiade, J. O. Dehinbo, T. Zuva and A. K. Akanbi", "title": "Road Detection Technique Using Filters with Application to Autonomous\n  Driving System", "comments": "7 pages, 7 figures, International Journal of Computing,\n  Communications & Instrumentation Engg. (IJCCIE) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Autonomous driving systems are broadly used equipment in the industries and\nin our daily lives, they assist in production, but are majorly used for\nexploration in dangerous or unfamiliar locations. Thus, for a successful\nexploration, navigation plays a significant role. Road detection is an\nessential factor that assists autonomous robots achieved perfect navigation.\nVarious techniques using camera sensors have been proposed by numerous scholars\nwith inspiring results, but their techniques are still vulnerable to these\nenvironmental noises: rain, snow, light intensity and shadow. In addressing\nthese problems, this paper proposed to enhance the road detection system with\nfiltering algorithm to overcome these limitations. Normalized Differences Index\n(NDI) and morphological operation are the filtering algorithms used to address\nthe effect of shadow and guidance and re-guidance image filtering algorithms\nare used to address the effect of rain and/or snow, while dark channel image\nand specular-to-diffuse are the filters used to address light intensity\neffects. The experimental performance of the road detection system with\nfiltering algorithms was tested qualitatively and quantitatively using the\nfollowing evaluation schemes: False Negative Rate (FNR) and False Positive Rate\n(FPR). Comparison results of the road detection system with and without\nfiltering algorithm shows the filtering algorithm's capability to suppress the\neffect of environmental noises because better road/non-road classification is\nachieved by the road detection system. with filtering algorithm. This\nachievement has further improved path planning/region classification for\nautonomous driving system\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 14:14:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Agunbiade", "Y. O.", ""], ["Dehinbo", "J. O.", ""], ["Zuva", "T.", ""], ["Akanbi", "A. K.", ""]]}, {"id": "1809.05879", "submitter": "Abdallah Moussawi", "authors": "Abdallah Moussawi, Kamal Haddad and Anthony Chahine", "title": "An FPGA-Accelerated Design for Deep Learning Pedestrian Detection in\n  Self-Driving Vehicles", "comments": "7 pages. American University of Beirut, Faculty of Engineering and\n  Architecture Student and Alumni Conference 2017 FEASAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of self-driving vehicles comes the risk of accidents and the\nneed for higher safety, and protection for pedestrian detection in the\nfollowing scenarios: imminent crashes, thus the car should crash into an object\nand avoid the pedestrian, and in the case of road intersections, where it is\nimportant for the car to stop when pedestrians are crossing. Currently, a\nspecial topology of deep neural networks called Fused Deep Neural Network\n(F-DNN) is considered to be the state of the art in pedestrian detection, as it\nhas the lowest miss rate, yet it is very slow. Therefore, acceleration is\nneeded to speed up the performance. This project proposes two contributions to\naddress this problem, by using a deep neural network used for object detection,\ncalled Single Shot Multi-Box Detector (SSD). The first contribution is training\nand tuning the hyperparameters of SSD to improve pedestrian detection. The\nsecond contribution is a new FPGA design for accelerating the model on the\nAltera Arria 10 platform. The final system will be used in self-driving\nvehicles in real-time. Preliminary results of the improved SSD shows 3% higher\nmiss-rate than F-DNN on Caltech pedestrian detection benchmark, but 4x\nperformance improvement. The acceleration design is expected to achieve an\nadditional performance improvement significantly outweighing the minimal\ndifference in accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 14:16:33 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Moussawi", "Abdallah", ""], ["Haddad", "Kamal", ""], ["Chahine", "Anthony", ""]]}, {"id": "1809.05884", "submitter": "Yongcheng Liu", "authors": "Yongcheng Liu, Lu Sheng, Jing Shao, Junjie Yan, Shiming Xiang and\n  Chunhong Pan", "title": "Multi-Label Image Classification via Knowledge Distillation from\n  Weakly-Supervised Detection", "comments": "accepted by ACM Multimedia 2018, 9 pages, 4 figures, 5 tables", "journal-ref": null, "doi": "10.1145/3240508.3240567", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label image classification is a fundamental but challenging task\ntowards general visual understanding. Existing methods found the region-level\ncues (e.g., features from RoIs) can facilitate multi-label classification.\nNevertheless, such methods usually require laborious object-level annotations\n(i.e., object labels and bounding boxes) for effective learning of the\nobject-level visual features. In this paper, we propose a novel and efficient\ndeep framework to boost multi-label classification by distilling knowledge from\nweakly-supervised detection task without bounding box annotations.\nSpecifically, given the image-level annotations, (1) we first develop a\nweakly-supervised detection (WSD) model, and then (2) construct an end-to-end\nmulti-label image classification framework augmented by a knowledge\ndistillation module that guides the classification model by the WSD model\naccording to the class-level predictions for the whole image and the\nobject-level visual features for object RoIs. The WSD model is the teacher\nmodel and the classification model is the student model. After this cross-task\nknowledge distillation, the performance of the classification model is\nsignificantly improved and the efficiency is maintained since the WSD model can\nbe safely discarded in the test phase. Extensive experiments on two large-scale\ndatasets (MS-COCO and NUS-WIDE) show that our framework achieves superior\nperformances over the state-of-the-art methods on both performance and\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 14:35:03 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 12:28:58 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Liu", "Yongcheng", ""], ["Sheng", "Lu", ""], ["Shao", "Jing", ""], ["Yan", "Junjie", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1809.05910", "submitter": "Rana Hanocka", "authors": "Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman and\n  Daniel Cohen-Or", "title": "MeshCNN: A Network with an Edge", "comments": "For a two-minute explanation video see https://bit.ly/meshcnnvideo", "journal-ref": null, "doi": "10.1145/3306346.3322959", "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polygonal meshes provide an efficient representation for 3D shapes. They\nexplicitly capture both shape surface and topology, and leverage non-uniformity\nto represent large flat regions as well as sharp, intricate features. This\nnon-uniformity and irregularity, however, inhibits mesh analysis efforts using\nneural networks that combine convolution and pooling operations. In this paper,\nwe utilize the unique properties of the mesh for a direct analysis of 3D shapes\nusing MeshCNN, a convolutional neural network designed specifically for\ntriangular meshes. Analogous to classic CNNs, MeshCNN combines specialized\nconvolution and pooling layers that operate on the mesh edges, by leveraging\ntheir intrinsic geodesic connections. Convolutions are applied on edges and the\nfour edges of their incident triangles, and pooling is applied via an edge\ncollapse operation that retains surface topology, thereby, generating new mesh\nconnectivity for the subsequent convolutions. MeshCNN learns which edges to\ncollapse, thus forming a task-driven process where the network exposes and\nexpands the important features while discarding the redundant ones. We\ndemonstrate the effectiveness of our task-driven pooling on various learning\ntasks applied to 3D meshes.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 16:32:29 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 11:30:57 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Hanocka", "Rana", ""], ["Hertz", "Amir", ""], ["Fish", "Noa", ""], ["Giryes", "Raja", ""], ["Fleishman", "Shachar", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1809.05934", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar and Nikhil Naik", "title": "Maximum-Entropy Fine-Grained Classification", "comments": "Camera-ready, accepted to NIPS 2018, v2 has minor typo updates and\n  small changes in text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-Grained Visual Classification (FGVC) is an important computer vision\nproblem that involves small diversity within the different classes, and often\nrequires expert annotators to collect data. Utilizing this notion of small\nvisual diversity, we revisit Maximum-Entropy learning in the context of\nfine-grained classification, and provide a training routine that maximizes the\nentropy of the output probability distribution for training convolutional\nneural networks on FGVC tasks. We provide a theoretical as well as empirical\njustification of our approach, and achieve state-of-the-art performance across\na variety of classification tasks in FGVC, that can potentially be extended to\nany fine-tuning task. Our method is robust to different hyperparameter values,\namount of training data and amount of training label noise and can hence be a\nvaluable tool in many similar problems.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 18:58:22 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 21:11:37 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Gupta", "Otkrist", ""], ["Raskar", "Ramesh", ""], ["Naik", "Nikhil", ""]]}, {"id": "1809.05955", "submitter": "Jian-Qing Zheng", "authors": "Jian-Qing Zheng, Xiao-Yun Zhou, Celia Riga, Guang-Zhong Yang", "title": "3D Path Planning from a Single 2D Fluoroscopic Image for Robot Assisted\n  Fenestrated Endovascular Aortic Repair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current standard of intra-operative navigation during Fenestrated\nEndovascular Aortic Repair (FEVAR) calls for need of 3D alignments between\ninserted devices and aortic branches. The navigation commonly via 2D\nfluoroscopic images, lacks anatomical information, resulting in longer\noperation hours and radiation exposure. In this paper, a framework for\nreal-time 3D robotic path planning from a single 2D fluoroscopic image of\nAbdominal Aortic Aneurysm (AAA) is introduced. A graph matching method is\nproposed to establish the correspondence between the 3D preoperative and 2D\nintra-operative AAA skeletons, and then the two skeletons are registered by\nskeleton deformation and regularization in respect to skeleton length and\nsmoothness. Furthermore, deep learning was used to segment 3D pre-operative AAA\nfrom Computed Tomography (CT) scans to facilitate the framework automation.\nSimulation, phantom and patient AAA data sets have been used to validate the\nproposed framework. 3D distance error of 2mm was achieved in the phantom setup.\nPerformance advantages were also achieved in terms of accuracy, robustness and\ntime-efficiency. All the code will be open source.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 21:01:16 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Zheng", "Jian-Qing", ""], ["Zhou", "Xiao-Yun", ""], ["Riga", "Celia", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1809.05962", "submitter": "Yuezun Li", "authors": "Yuezun Li, Daniel Tian, Ming-Ching Chang, Xiao Bian and Siwei Lyu", "title": "Robust Adversarial Perturbation on Deep Proposal-based Models", "comments": "To appear in BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial noises are useful tools to probe the weakness of deep learning\nbased computer vision algorithms. In this paper, we describe a robust\nadversarial perturbation (R-AP) method to attack deep proposal-based object\ndetectors and instance segmentation algorithms. Our method focuses on attacking\nthe common component in these algorithms, namely Region Proposal Network (RPN),\nto universally degrade their performance in a black-box fashion. To do so, we\ndesign a loss function that combines a label loss and a novel shape loss, and\noptimize it with respect to image using a gradient based iterative algorithm.\nEvaluations are performed on the MS COCO 2014 dataset for the adversarial\nattacking of 6 state-of-the-art object detectors and 2 instance segmentation\nalgorithms. Experimental results demonstrate the efficacy of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 21:49:14 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 15:18:03 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Li", "Yuezun", ""], ["Tian", "Daniel", ""], ["Chang", "Ming-Ching", ""], ["Bian", "Xiao", ""], ["Lyu", "Siwei", ""]]}, {"id": "1809.05966", "submitter": "Yuezun Li", "authors": "Yuezun Li, Xiao Bian, Ming-ching Chang and Siwei Lyu", "title": "Exploring the Vulnerability of Single Shot Module in Object Detectors\n  via Imperceptible Background Patches", "comments": "To appear in BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works succeeded to generate adversarial perturbations on the entire\nimage or the object of interests to corrupt CNN based object detectors. In this\npaper, we focus on exploring the vulnerability of the Single Shot Module (SSM)\ncommonly used in recent object detectors, by adding small perturbations to\npatches in the background outside the object. The SSM is referred to the Region\nProposal Network used in a two-stage object detector or the single-stage object\ndetector itself. The SSM is typically a fully convolutional neural network\nwhich generates output in a single forward pass. Due to the excessive\nconvolutions used in SSM, the actual receptive field is larger than the object\nitself. As such, we propose a novel method to corrupt object detectors by\ngenerating imperceptible patches only in the background. Our method can find a\nfew background patches for perturbation, which can effectively decrease true\npositives and dramatically increase false positives. Efficacy is demonstrated\non 5 two-stage object detectors and 8 single-stage object detectors on the MS\nCOCO 2014 dataset. Results indicate that perturbations with small distortions\noutside the bounding box of object region can still severely damage the\ndetection performance.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 22:06:17 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 15:22:18 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 01:38:04 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Li", "Yuezun", ""], ["Bian", "Xiao", ""], ["Chang", "Ming-ching", ""], ["Lyu", "Siwei", ""]]}, {"id": "1809.05989", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mohammad Javad Shafiee, Brendan Chwyl, and Francis Li", "title": "FermiNets: Learning generative machines to generate efficient neural\n  networks via generative synthesis", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous potential exhibited by deep learning is often offset by\narchitectural and computational complexity, making widespread deployment a\nchallenge for edge scenarios such as mobile and other consumer devices. To\ntackle this challenge, we explore the following idea: Can we learn generative\nmachines to automatically generate deep neural networks with efficient network\narchitectures? In this study, we introduce the idea of generative synthesis,\nwhich is premised on the intricate interplay between a generator-inquisitor\npair that work in tandem to garner insights and learn to generate highly\nefficient deep neural networks that best satisfies operational requirements.\nWhat is most interesting is that, once a generator has been learned through\ngenerative synthesis, it can be used to generate not just one but a large\nvariety of different, unique highly efficient deep neural networks that satisfy\noperational requirements. Experimental results for image classification,\nsemantic segmentation, and object detection tasks illustrate the efficacy of\ngenerative synthesis in producing generators that automatically generate highly\nefficient deep neural networks (which we nickname FermiNets) with higher model\nefficiency and lower computational costs (reaching >10x more efficient and\nfewer multiply-accumulate operations than several tested state-of-the-art\nnetworks), as well as higher energy efficiency (reaching >4x improvements in\nimage inferences per joule consumed on a Nvidia Tegra X2 mobile processor). As\nsuch, generative synthesis can be a powerful, generalized approach for\naccelerating and improving the building of deep neural networks for on-device\nedge scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 01:26:57 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 19:14:50 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Wong", "Alexander", ""], ["Shafiee", "Mohammad Javad", ""], ["Chwyl", "Brendan", ""], ["Li", "Francis", ""]]}, {"id": "1809.05992", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Li Liu, Jie Qin, Fan Zhu, Fumin Shen, Yong Xu, Ling Shao,\n  Heng Tao Shen", "title": "Highly-Economized Multi-View Binary Compression for Scalable Image\n  Clustering", "comments": "European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to economically cluster large-scale multi-view images is a long-standing\nproblem in computer vision. To tackle this challenge, we introduce a novel\napproach named Highly-economized Scalable Image Clustering (HSIC) that\nradically surpasses conventional image clustering methods via binary\ncompression. We intuitively unify the binary representation learning and\nefficient binary cluster structure learning into a joint framework. In\nparticular, common binary representations are learned by exploiting both\nsharable and individual information across multiple views to capture their\nunderlying correlations. Meanwhile, cluster assignment with robust binary\ncentroids is also performed via effective discrete optimization under L21-norm\nconstraint. By this means, heavy continuous-valued Euclidean distance\ncomputations can be successfully reduced by efficient binary XOR operations\nduring the clustering procedure. To our best knowledge, HSIC is the first\nbinary clustering work specifically designed for scalable multi-view image\nclustering. Extensive experimental results on four large-scale image datasets\nshow that HSIC consistently outperforms the state-of-the-art approaches, whilst\nsignificantly reducing computational time and memory footprint.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 01:35:42 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Zhang", "Zheng", ""], ["Liu", "Li", ""], ["Qin", "Jie", ""], ["Zhu", "Fan", ""], ["Shen", "Fumin", ""], ["Xu", "Yong", ""], ["Shao", "Ling", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1809.05996", "submitter": "Ting Liu", "authors": "Tao Ruan, Ting Liu, Zilong Huang, Yunchao Wei, Shikui Wei, Yao Zhao,\n  Thomas Huang", "title": "Devil in the Details: Towards Accurate Single and Multiple Human Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human parsing has received considerable interest due to its wide application\npotentials. Nevertheless, it is still unclear how to develop an accurate human\nparsing system in an efficient and elegant way. In this paper, we identify\nseveral useful properties, including feature resolution, global context\ninformation and edge details, and perform rigorous analyses to reveal how to\nleverage them to benefit the human parsing task. The advantages of these useful\nproperties finally result in a simple yet effective Context Embedding with Edge\nPerceiving (CE2P) framework for single human parsing. Our CE2P is end-to-end\ntrainable and can be easily adopted for conducting multiple human parsing.\nBenefiting the superiority of CE2P, we achieved the 1st places on all three\nhuman parsing benchmarks. Without any bells and whistles, we achieved 56.50\\%\n(mIoU), 45.31\\% (mean $AP^r$) and 33.34\\% ($AP^p_{0.5}$) in LIP, CIHP and MHP\nv2.0, which outperform the state-of-the-arts more than 2.06\\%, 3.81\\% and\n1.87\\%, respectively. We hope our CE2P will serve as a solid baseline and help\nease future research in single/multiple human parsing. Code has been made\navailable at \\url{https://github.com/liutinglt/CE2P}.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 02:28:49 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 12:38:36 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 06:58:05 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Ruan", "Tao", ""], ["Liu", "Ting", ""], ["Huang", "Zilong", ""], ["Wei", "Yunchao", ""], ["Wei", "Shikui", ""], ["Zhao", "Yao", ""], ["Huang", "Thomas", ""]]}, {"id": "1809.05998", "submitter": "Zheng Zhang", "authors": "Jie Wen, Zheng Zhang, Yong Xu, Zuofeng Zhong", "title": "Incomplete Multi-view Clustering via Graph Regularized Matrix\n  Factorization", "comments": "ECCV 2018 International Workshop on Compact and Efficient Feature\n  Representation and Learning in Computer Vision (CEFRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering with incomplete views is a challenge in multi-view clustering. In\nthis paper, we provide a novel and simple method to address this issue.\nSpecifically, the proposed method simultaneously exploits the local information\nof each view and the complementary information among views to learn the common\nlatent representation for all samples, which can greatly improve the\ncompactness and discriminability of the obtained representation. Compared with\nthe conventional graph embedding methods, the proposed method does not\nintroduce any extra regularization term and corresponding penalty parameter to\npreserve the local structure of data, and thus does not increase the burden of\nextra parameter selection. By imposing the orthogonal constraint on the basis\nmatrix of each view, the proposed method is able to handle the out-of-sample.\nMoreover, the proposed method can be viewed as a unified framework for\nmulti-view learning since it can handle both incomplete and complete multi-view\nclustering and classification tasks. Extensive experiments conducted on several\nmulti-view datasets prove that the proposed method can significantly improve\nthe clustering performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 02:46:48 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Wen", "Jie", ""], ["Zhang", "Zheng", ""], ["Xu", "Yong", ""], ["Zhong", "Zuofeng", ""]]}, {"id": "1809.06006", "submitter": "Feras Dayoub", "authors": "Dimity Miller, Feras Dayoub, Michael Milford, Niko S\\\"underhauf", "title": "Evaluating Merging Strategies for Sampling-based Uncertainty Techniques\n  in Object Detection", "comments": "to appear in IEEE International Conference on Robotics and Automation\n  2019 (ICRA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent emergence of sampling-based techniques for estimating\nepistemic uncertainty in deep neural networks. While these methods can be\napplied to classification or semantic segmentation tasks by simply averaging\nsamples, this is not the case for object detection, where detection sample\nbounding boxes must be accurately associated and merged. A weak merging\nstrategy can significantly degrade the performance of the detector and yield an\nunreliable uncertainty measure. This paper provides the first in-depth\ninvestigation of the effect of different association and merging strategies. We\ncompare different combinations of three spatial and two semantic affinity\nmeasures with four clustering methods for MC Dropout with a Single Shot\nMulti-Box Detector. Our results show that the correct choice of\naffinity-clustering combination can greatly improve the effectiveness of the\nclassification and spatial uncertainty estimation and the resulting object\ndetection performance. We base our evaluation on a new mix of datasets that\nemulate near open-set conditions (semantically similar unknown classes),\ndistant open-set conditions (semantically dissimilar unknown classes) and the\ncommon closed-set conditions (only known classes).\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 03:16:03 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 04:04:54 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 00:02:33 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Miller", "Dimity", ""], ["Dayoub", "Feras", ""], ["Milford", "Michael", ""], ["S\u00fcnderhauf", "Niko", ""]]}, {"id": "1809.06013", "submitter": "Chuang Niu", "authors": "Chuang Niu, Shenghan Ren, Jimin Liang", "title": "DASNet: Reducing Pixel-level Annotations for Instance and Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-level annotation demands expensive human efforts and limits the\nperformance of deep networks that usually benefits from more such training\ndata. In this work we aim to achieve high quality instance and semantic\nsegmentation results over a small set of pixel-level mask annotations and a\nlarge set of box annotations. The basic idea is exploring detection models to\nsimplify the pixel-level supervised learning task and thus reduce the required\namount of mask annotations. Our architecture, named DASNet, consists of three\nmodules: detection, attention, and segmentation. The detection module detects\nall classes of objects, the attention module generates multi-scale\nclass-specific features, and the segmentation module recovers the binary masks.\nOur method demonstrates substantially improved performance compared to existing\nsemi-supervised approaches on PASCAL VOC 2012 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 04:23:20 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 18:07:53 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Niu", "Chuang", ""], ["Ren", "Shenghan", ""], ["Liang", "Jimin", ""]]}, {"id": "1809.06035", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL), Julien Mairal (Thoth), Bertrand Thirion\n  (PARIETAL), Ga\\\"el Varoquaux (PARIETAL)", "title": "Extracting representations of cognition across neuroimaging studies\n  improves brain decoding", "comments": null, "journal-ref": "PLoS Computational Biology, Public Library of Science, 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive brain imaging is accumulating datasets about the neural substrate\nof many different mental processes. Yet, most studies are based on few subjects\nand have low statistical power. Analyzing data across studies could bring more\nstatistical power; yet the current brain-imaging analytic framework cannot be\nused at scale as it requires casting all cognitive tasks in a unified\ntheoretical framework. We introduce a new methodology to analyze brain\nresponses across tasks without a joint model of the psychological processes.\nThe method boosts statistical power in small studies with specific cognitive\nfocus by analyzing them jointly with large studies that probe less focal mental\nprocesses. Our approach improves decoding performance for 80% of 35\nwidely-different functional-imaging studies. It finds commonalities across\ntasks in a data-driven way, via common brain representations that predict\nmental processes. These are brain networks tuned to psychological\nmanipulations. They outline interpretable and plausible brain structures. The\nextracted networks have been made available; they can be readily reused in new\nneuro-imaging studies. We provide a multi-study decoding tool to adapt to new\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 06:19:11 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 06:50:24 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 08:37:14 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL"], ["Mairal", "Julien", "", "Thoth"], ["Thirion", "Bertrand", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"]]}, {"id": "1809.06036", "submitter": "Zhuming Zhang", "authors": "Zhuming Zhang and Xinghong Hu and Xueting Liu and Tien-Tsin Wong", "title": "Binocular Tone Mapping with Improved Overall Contrast and Local Details", "comments": "Accepted by Pacific Graphics 2018", "journal-ref": "Computer Graphics Forum (Pacific Graphics issue) 37, 7 (2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tone mapping is a commonly used technique that maps the set of colors in\nhigh-dynamic-range (HDR) images to another set of colors in low-dynamic-range\n(LDR) images, to fit the need for print-outs, LCD monitors and projectors.\nUnfortunately, during the compression of dynamic range, the overall contrast\nand local details generally cannot be preserved simultaneously. Recently, with\nthe increased use of stereoscopic devices, the notion of binocular tone mapping\nhas been proposed in the existing research study. However, the existing\nresearch lacks the binocular perception study and is unable to generate the\noptimal binocular pair that presents the most visual content. In this paper, we\npropose a novel perception-based binocular tone mapping method, that can\ngenerate an optimal binocular image pair (generating left and right images\nsimultaneously) from an HDR image that presents the most visual content by\ndesigning a binocular perception metric. Our method outperforms the existing\nmethod in terms of both visual and time performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 06:19:54 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Zhang", "Zhuming", ""], ["Hu", "Xinghong", ""], ["Liu", "Xueting", ""], ["Wong", "Tien-Tsin", ""]]}, {"id": "1809.06045", "submitter": "Dominique Vaufreydaz", "authors": "Pavan Vasishta (UGA, CHROMA), Dominique Vaufreydaz (PERVASIVE), Anne\n  Spalanzani (CHROMA)", "title": "Building Prior Knowledge: A Markov Based Pedestrian Prediction Model\n  Using Urban Environmental Data", "comments": "15 th International Conference on Control, Automation, Robotics and\n  Vision (ICARCV 2018), Nov 2018, Singapore, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous Vehicles navigating in urban areas have a need to understand and\npredict future pedestrian behavior for safer navigation. This high level of\nsituational awareness requires observing pedestrian behavior and extrapolating\ntheir positions to know future positions. While some work has been done in this\nfield using Hidden Markov Models (HMMs), one of the few observed drawbacks of\nthe method is the need for informed priors for learning behavior. In this work,\nan extension to the Growing Hidden Markov Model (GHMM) method is proposed to\nsolve some of these drawbacks. This is achieved by building on existing work\nusing potential cost maps and the principle of Natural Vision. As a\nconsequence, the proposed model is able to predict pedestrian positions more\nprecisely over a longer horizon compared to the state of the art. The method is\ntested over \"legal\" and \"illegal\" behavior of pedestrians, having trained the\nmodel with sparse observations and partial trajectories. The method, with no\ntraining data, is compared against a trained state of the art model. It is\nobserved that the proposed method is robust even in new, previously unseen\nareas.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:06:44 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Vasishta", "Pavan", "", "UGA, CHROMA"], ["Vaufreydaz", "Dominique", "", "PERVASIVE"], ["Spalanzani", "Anne", "", "CHROMA"]]}, {"id": "1809.06064", "submitter": "Katia Sycara", "authors": "Yuezhang Li, Katia Sycara, Rahul Iyer", "title": "Object-sensitive Deep Reinforcement Learning", "comments": "15 pages, 6 figures, Accepted at 3rd Global Conference on Artificial\n  Intelligence (GCAI-17), Miami, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has become popular over recent years, showing\nsuperiority on different visual-input tasks such as playing Atari games and\nrobot navigation. Although objects are important image elements, few work\nconsiders enhancing deep reinforcement learning with object characteristics. In\nthis paper, we propose a novel method that can incorporate object recognition\nprocessing to deep reinforcement learning models. This approach can be adapted\nto any existing deep reinforcement learning frameworks. State-of-the-art\nresults are shown in experiments on Atari games. We also propose a new approach\ncalled \"object saliency maps\" to visually explain the actions made by deep\nreinforcement learning agents.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:59:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Li", "Yuezhang", ""], ["Sycara", "Katia", ""], ["Iyer", "Rahul", ""]]}, {"id": "1809.06065", "submitter": "Lei Tai", "authors": "Peng Yun, Lei Tai, Yuan Wang, Chengju Liu, Ming Liu", "title": "Focal Loss in 3D Object Detection", "comments": "IEEE RA-L 2019 to appear. Codes and trained weights are available on\n  the project page(https://goo.gl/2hFbmL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection is still an open problem in autonomous driving scenes.\nWhen recognizing and localizing key objects from sparse 3D inputs, autonomous\nvehicles suffer from a larger continuous searching space and higher\nfore-background imbalance compared to image-based object detection. In this\npaper, we aim to solve this fore-background imbalance in 3D object detection.\nInspired by the recent use of focal loss in image-based object detection, we\nextend this hard-mining improvement of binary cross entropy to\npoint-cloud-based object detection and conduct experiments to show its\nperformance based on two different 3D detectors: 3D-FCN and VoxelNet. The\nevaluation results show up to 11.2AP gains through the focal loss in a wide\nrange of hyperparameters for 3D object detection.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 08:02:00 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 09:11:06 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 09:59:12 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Yun", "Peng", ""], ["Tai", "Lei", ""], ["Wang", "Yuan", ""], ["Liu", "Chengju", ""], ["Liu", "Ming", ""]]}, {"id": "1809.06070", "submitter": "Mohamed Chafik Bakkay", "authors": "Mohamed Chafik Bakkay, Walid Barhoumi, Ezzeddine Zagrouba", "title": "Seuillage par hyst\\'er\\'esis pour le test de photo-consistance des\n  voxels dans le cadre de la reconstruction 3D", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voxel coloring is a popular method of reconstructing a three-dimensional\nsurface model from a set of calibrated 2D images. However, the reconstruction\nquality is largely dependent on a thresholding procedure allowing the authors\nto decide, for each voxel, whether it is photo-consistent or not. Even so, this\nmethod is widely used because of its simplicity and low computational cost. We\nhave returned to this method in order to propose an improvement in the\nthresholding step which will be fully automated. Indeed, the geometrical\ninformation is implicitly integrated using an hysteresis thresholding which\ntakes into account the spatial coherence of color voxels. Moreover, the\nambiguity of choosing the thresholds is extremely minimized by defining a fuzzy\ndegree of membership of each voxel into the class of consistent voxels. Also,\nthere is no need for preset thresholds since the hysteresis ones are defined\nautomatically and adaptively depending on the number of images that the voxel\nisprojected onto. Preliminary results are very promising and demonstrate that\nthe proposed method performs automatically precise and smooth volumetric scene\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 08:29:37 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Bakkay", "Mohamed Chafik", ""], ["Barhoumi", "Walid", ""], ["Zagrouba", "Ezzeddine", ""]]}, {"id": "1809.06079", "submitter": "Xiao Sun", "authors": "Xiao Sun, Chuankang Li, Stephen Lin", "title": "An Integral Pose Regression System for the ECCV2018 PoseTrack Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the ECCV 2018 PoseTrack Challenge, we present a 3D human pose estimation\nsystem based mainly on the integral human pose regression method. We show a\ncomprehensive ablation study to examine the key performance factors of the\nproposed system. Our system obtains 47mm MPJPE on the CHALL_H80K test dataset,\nplacing second in the ECCV2018 3D human pose estimation challenge. Code will be\nreleased to facilitate future work.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 08:59:22 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Sun", "Xiao", ""], ["Li", "Chuankang", ""], ["Lin", "Stephen", ""]]}, {"id": "1809.06130", "submitter": "Bob D. de Vos", "authors": "Bob D. de Vos, Floris F. Berendsen, Max A. Viergever, Hessam Sokooti,\n  Marius Staring, Ivana Isgum", "title": "A Deep Learning Framework for Unsupervised Affine and Deformable Image\n  Registration", "comments": "Accepted: Medical Image Analysis - Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration, the process of aligning two or more images, is the core\ntechnique of many (semi-)automatic medical image analysis tasks. Recent studies\nhave shown that deep learning methods, notably convolutional neural networks\n(ConvNets), can be used for image registration. Thus far training of ConvNets\nfor registration was supervised using predefined example registrations.\nHowever, obtaining example registrations is not trivial. To circumvent the need\nfor predefined examples, and thereby to increase convenience of training\nConvNets for image registration, we propose the Deep Learning Image\nRegistration (DLIR) framework for \\textit{unsupervised} affine and deformable\nimage registration. In the DLIR framework ConvNets are trained for image\nregistration by exploiting image similarity analogous to conventional\nintensity-based image registration. After a ConvNet has been trained with the\nDLIR framework, it can be used to register pairs of unseen images in one shot.\nWe propose flexible ConvNets designs for affine image registration and for\ndeformable image registration. By stacking multiple of these ConvNets into a\nlarger architecture, we are able to perform coarse-to-fine image registration.\nWe show for registration of cardiac cine MRI and registration of chest CT that\nperformance of the DLIR framework is comparable to conventional image\nregistration while being several orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 11:14:54 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 14:38:00 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["de Vos", "Bob D.", ""], ["Berendsen", "Floris F.", ""], ["Viergever", "Max A.", ""], ["Sokooti", "Hessam", ""], ["Staring", "Marius", ""], ["Isgum", "Ivana", ""]]}, {"id": "1809.06131", "submitter": "Bowen Cheng", "authors": "Bowen Cheng, Rong Xiao, Yandong Guo, Yuxiao Hu, Jianfeng Wang, Lei\n  Zhang", "title": "Revisit Multinomial Logistic Regression in Deep Learning: Data Dependent\n  Model Initialization for Image Recognition", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper how to initialize the parameters of multinomial\nlogistic regression (a fully connected layer followed with softmax and cross\nentropy loss), which is widely used in deep neural network (DNN) models for\nclassification problems. As logistic regression is widely known not having a\nclosed-form solution, it is usually randomly initialized, leading to several\ndeficiencies especially in transfer learning where all the layers except for\nthe last task-specific layer are initialized using a pre-trained model. The\ndeficiencies include slow convergence speed, possibility of stuck in local\nminimum, and the risk of over-fitting. To address those deficiencies, we first\nstudy the properties of logistic regression and propose a closed-form\napproximate solution named regularized Gaussian classifier (RGC). Then we adopt\nthis approximate solution to initialize the task-specific linear layer and\ndemonstrate superior performance over random initialization in terms of both\naccuracy and convergence speed on various tasks and datasets. For example, for\nimage classification, our approach can reduce the training time by 10 times and\nachieve 3.2% gain in accuracy for Flickr-style classification. For object\ndetection, our approach can also be 10 times faster in training for the same\naccuracy, or 5% better in terms of mAP for VOC 2007 with slightly longer\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 11:23:33 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Cheng", "Bowen", ""], ["Xiao", "Rong", ""], ["Guo", "Yandong", ""], ["Hu", "Yuxiao", ""], ["Wang", "Jianfeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1809.06139", "submitter": "Mathis Fleury", "authors": "Mathis Fleury (VisAGeS), Pierre Maurel (VisAGeS), Marsel Mano\n  (VisAGeS), Elise Bannier (VisAGeS), Christian Barillot (VisAGeS)", "title": "Automatic Electrodes Detection during simultaneous EEG/fMRI acquisition", "comments": "ISMRM, Jun 2018, Paris, France. 2018, https://www.ismrm.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous EEG/fMRI acquisition allows to measure brain activity at high\nspatial-temporal resolution. The localisation of EEG sources depends on several\nparameters including the position of the electrodes on the scalp. The position\nof the MR electrodes during its acquisitions is obtained with the use of the\nUTE sequence allowing their visualisation. The retrieval of the electrodes\nconsists in obtaining the volume where the electrodes are located by applying a\nsphere detection algorithm. We detect around 90% of electrodes for each\nsubject, and our UTE-based electrode detection showed an average position error\nof 3.7mm for all subjects.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 11:37:49 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Fleury", "Mathis", "", "VisAGeS"], ["Maurel", "Pierre", "", "VisAGeS"], ["Mano", "Marsel", "", "VisAGeS"], ["Bannier", "Elise", "", "VisAGeS"], ["Barillot", "Christian", "", "VisAGeS"]]}, {"id": "1809.06147", "submitter": "Yong Man Ro", "authors": "Jae-Hyeok Lee, Seong Tae Kim, Hakmin Lee, and Yong Man Ro", "title": "Feature2Mass: Visual Feature Processing in Latent Space for Realistic\n  Labeled Mass Generation", "comments": "This paper presented at ECCV 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a method for generating realistic labeled masses.\nRecently, there have been many attempts to apply deep learning to various\nbio-image computing fields including computer-aided detection and diagnosis. In\norder to learn deep network model to be well-behaved in bio-image computing\nfields, a lot of labeled data is required. However, in many bioimaging fields,\nthe large-size of labeled dataset is scarcely available. Although a few\nresearches have been dedicated to solving this problem through generative\nmodel, there are some problems as follows: 1) The generated bio-image does not\nseem realistic; 2) the variation of generated bio-image is limited; and 3)\nadditional label annotation task is needed. In this study, we propose a\nrealistic labeled bio-image generation method through visual feature processing\nin latent space. Experimental results have shown that mass images generated by\nthe proposed method were realistic and had wide expression range of targeted\nmass characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 12:01:47 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 06:09:00 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Lee", "Jae-Hyeok", ""], ["Kim", "Seong Tae", ""], ["Lee", "Hakmin", ""], ["Ro", "Yong Man", ""]]}, {"id": "1809.06157", "submitter": "Fernando Alonso-Fernandez", "authors": "Kevin Hernandez-Diaz, Fernando Alonso-Fernandez, Josef Bigun", "title": "Periocular Recognition Using CNN Features Off-the-Shelf", "comments": "Accepted to BIOSIG 2018: 17th International Conference of the\n  Biometrics Special Interest Group", "journal-ref": "Proc. 17th International Conference of the Biometrics Special\n  Interest Group, BIOSIG, Darmstadt, Germany, 26-28 September 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periocular refers to the region around the eye, including sclera, eyelids,\nlashes, brows and skin. With a surprisingly high discrimination ability, it is\nthe ocular modality requiring the least constrained acquisition. Here, we apply\nexisting pre-trained architectures, proposed in the context of the ImageNet\nLarge Scale Visual Recognition Challenge, to the task of periocular\nrecognition. These have proven to be very successful for many other computer\nvision tasks apart from the detection and classification tasks for which they\nwere designed. Experiments are done with a database of periocular images\ncaptured with a digital camera. We demonstrate that these off-the-shelf CNN\nfeatures can effectively recognize individuals based on periocular images,\ndespite being trained to classify generic objects. Compared against reference\nperiocular features, they show an EER reduction of up to ~40%, with the fusion\nof CNN and traditional features providing additional improvements.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 12:33:18 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hernandez-Diaz", "Kevin", ""], ["Alonso-Fernandez", "Fernando", ""], ["Bigun", "Josef", ""]]}, {"id": "1809.06181", "submitter": "Jianfeng Dong", "authors": "Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, Yuan He, Gang Yang,\n  Xun Wang", "title": "Dual Encoding for Zero-Example Video Retrieval", "comments": "Accepted by CVPR 2019. Code and data are available at\n  https://github.com/danieljf24/dual_encoding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attacks the challenging problem of zero-example video retrieval.\nIn such a retrieval paradigm, an end user searches for unlabeled videos by\nad-hoc queries described in natural language text with no visual example\nprovided. Given videos as sequences of frames and queries as sequences of\nwords, an effective sequence-to-sequence cross-modal matching is required. The\nmajority of existing methods are concept based, extracting relevant concepts\nfrom queries and videos and accordingly establishing associations between the\ntwo modalities. In contrast, this paper takes a concept-free approach,\nproposing a dual deep encoding network that encodes videos and queries into\npowerful dense representations of their own. Dual encoding is conceptually\nsimple, practically effective and end-to-end. As experiments on three\nbenchmarks, i.e. MSR-VTT, TRECVID 2016 and 2017 Ad-hoc Video Search show, the\nproposed solution establishes a new state-of-the-art for zero-example video\nretrieval.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:13:14 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 01:27:29 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2019 14:32:13 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Dong", "Jianfeng", ""], ["Li", "Xirong", ""], ["Xu", "Chaoxi", ""], ["Ji", "Shouling", ""], ["He", "Yuan", ""], ["Yang", "Gang", ""], ["Wang", "Xun", ""]]}, {"id": "1809.06186", "submitter": "Md. Abu Bakr Siddique", "authors": "Mohammad Mahmudur Rahman Khan, Rezoana Bente Arif, Md. Abu Bakr\n  Siddique, Mahjabin Rahman Oishe", "title": "Study and Observation of the Variation of Accuracies of KNN, SVM, LMNN,\n  ENN Algorithms on Eleven Different Datasets from UCI Machine Learning\n  Repository", "comments": "To be published in the 4th IEEE International Conference on\n  Electrical Engineering and Information & Communication Technology (iCEEiCT\n  2018)", "journal-ref": "2018 4th International Conference on Electrical Engineering and\n  Information & Communication Technology (iCEEiCT)", "doi": "10.1109/CEEICT.2018.8628041", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning qualifies computers to assimilate with data, without being\nsolely programmed [1, 2]. Machine learning can be classified as supervised and\nunsupervised learning. In supervised learning, computers learn an objective\nthat portrays an input to an output hinged on training input-output pairs [3].\nMost efficient and widely used supervised learning algorithms are K-Nearest\nNeighbors (KNN), Support Vector Machine (SVM), Large Margin Nearest Neighbor\n(LMNN), and Extended Nearest Neighbor (ENN). The main contribution of this\npaper is to implement these elegant learning algorithms on eleven different\ndatasets from the UCI machine learning repository to observe the variation of\naccuracies for each of the algorithms on all datasets. Analyzing the accuracy\nof the algorithms will give us a brief idea about the relationship of the\nmachine learning algorithms and the data dimensionality. All the algorithms are\ndeveloped in Matlab. Upon such accuracy observation, the comparison can be\nbuilt among KNN, SVM, LMNN, and ENN regarding their performances on each\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:27:43 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 12:11:41 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:57:47 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Khan", "Mohammad Mahmudur Rahman", ""], ["Arif", "Rezoana Bente", ""], ["Siddique", "Md. Abu Bakr", ""], ["Oishe", "Mahjabin Rahman", ""]]}, {"id": "1809.06187", "submitter": "Md. Abu Bakr Siddique", "authors": "Rezoana Bente Arif, Md. Abu Bakr Siddique, Mohammad Mahmudur Rahman\n  Khan, Mahjabin Rahman Oishe", "title": "Study and Observation of the Variations of Accuracies for Handwritten\n  Digits Recognition with Various Hidden Layers and Epochs using Convolutional\n  Neural Network", "comments": "To be published in the 4th IEEE International Conference on\n  Electrical Engineering and Information & Communication Technology (iCEEiCT\n  2018)", "journal-ref": "2018 4th International Conference on Electrical Engineering and\n  Information & Communication Technology (iCEEiCT)", "doi": "10.1109/CEEICT.2018.8628078", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nowadays, deep learning can be employed to a wide ranges of fields including\nmedicine, engineering, etc. In deep learning, Convolutional Neural Network\n(CNN) is extensively used in the pattern and sequence recognition, video\nanalysis, natural language processing, spam detection, topic categorization,\nregression analysis, speech recognition, image classification, object\ndetection, segmentation, face recognition, robotics, and control. The benefits\nassociated with its near human level accuracies in large applications lead to\nthe growing acceptance of CNN in recent years. The primary contribution of this\npaper is to analyze the impact of the pattern of the hidden layers of a CNN\nover the overall performance of the network. To demonstrate this influence, we\napplied neural network with different layers on the Modified National Institute\nof Standards and Technology (MNIST) dataset. Also, is to observe the variations\nof accuracies of the network for various numbers of hidden layers and epochs\nand to make comparison and contrast among them. The system is trained utilizing\nstochastic gradient and backpropagation algorithm and tested with feedforward\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:28:02 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 12:08:57 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:56:11 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Arif", "Rezoana Bente", ""], ["Siddique", "Md. Abu Bakr", ""], ["Khan", "Mohammad Mahmudur Rahman", ""], ["Oishe", "Mahjabin Rahman", ""]]}, {"id": "1809.06188", "submitter": "Md. Abu Bakr Siddique", "authors": "Md. Abu Bakr Siddique, Mohammad Mahmudur Rahman Khan, Rezoana Bente\n  Arif, Zahidun Ashrafi", "title": "Study and Observation of the Variations of Accuracies for Handwritten\n  Digits Recognition with Various Hidden Layers and Epochs using Neural Network\n  Algorithm", "comments": "To be published in the 4th IEEE International Conference on\n  Electrical Engineering and Information & Communication Technology (iCEEiCT\n  2018)", "journal-ref": "2018 4th International Conference on Electrical Engineering and\n  Information & Communication Technology (iCEEiCT)", "doi": "10.1109/CEEICT.2018.8628144", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent days, Artificial Neural Network (ANN) can be applied to a vast\nmajority of fields including business, medicine, engineering, etc. The most\npopular areas where ANN is employed nowadays are pattern and sequence\nrecognition, novelty detection, character recognition, regression analysis,\nspeech recognition, image compression, stock market prediction, Electronic\nnose, security, loan applications, data processing, robotics, and control. The\nbenefits associated with its broad applications leads to increasing popularity\nof ANN in the era of 21st Century. ANN confers many benefits such as organic\nlearning, nonlinear data processing, fault tolerance, and self-repairing\ncompared to other conventional approaches. The primary objective of this paper\nis to analyze the influence of the hidden layers of a neural network over the\noverall performance of the network. To demonstrate this influence, we applied\nneural network with different layers on the MNIST dataset. Also, another goal\nis to observe the variations of accuracies of ANN for different numbers of\nhidden layers and epochs and to compare and contrast among them.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:28:19 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 12:06:34 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:54:07 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Siddique", "Md. Abu Bakr", ""], ["Khan", "Mohammad Mahmudur Rahman", ""], ["Arif", "Rezoana Bente", ""], ["Ashrafi", "Zahidun", ""]]}, {"id": "1809.06189", "submitter": "Md. Abu Bakr Siddique", "authors": "Mohammad Mahmudur Rahman Khan, Md. Abu Bakr Siddique, Rezoana Bente\n  Arif, Mahjabin Rahman Oishe", "title": "ADBSCAN: Adaptive Density-Based Spatial Clustering of Applications with\n  Noise for Identifying Clusters with Varying Densities", "comments": "To be published in the 4th IEEE International Conference on\n  Electrical Engineering and Information & Communication Technology (iCEEiCT\n  2018)", "journal-ref": "2018 4th International Conference on Electrical Engineering and\n  Information & Communication Technology (iCEEiCT)", "doi": "10.1109/CEEICT.2018.8628138", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Density-based spatial clustering of applications with noise (DBSCAN) is a\ndata clustering algorithm which has the high-performance rate for dataset where\nclusters have the constant density of data points. One of the significant\nattributes of this algorithm is noise cancellation. However, DBSCAN\ndemonstrates reduced performances for clusters with different densities.\nTherefore, in this paper, an adaptive DBSCAN is proposed which can work\nsignificantly well for identifying clusters with varying densities.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:28:35 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 12:03:42 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:51:05 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Khan", "Mohammad Mahmudur Rahman", ""], ["Siddique", "Md. Abu Bakr", ""], ["Arif", "Rezoana Bente", ""], ["Oishe", "Mahjabin Rahman", ""]]}, {"id": "1809.06191", "submitter": "Mehmet Ayg\\\"un", "authors": "Mehmet Ayg\\\"un, Yusuf H\\\"useyin \\c{S}ahin, G\\\"ozde \\\"Unal", "title": "Multi Modal Convolutional Neural Networks for Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a multi-modal Convolutional Neural Network (CNN)\napproach for brain tumor segmentation. We investigate how to combine different\nmodalities efficiently in the CNN framework.We adapt various fusion methods,\nwhich are previously employed on video recognition problem, to the brain tumor\nsegmentation problem,and we investigate their efficiency in terms of memory and\nperformance.Our experiments, which are performed on BRATS dataset, lead us to\nthe conclusion that learning separate representations for each modality and\ncombining them for brain tumor segmentation could increase the performance of\nCNN systems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:33:21 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 07:27:38 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Ayg\u00fcn", "Mehmet", ""], ["\u015eahin", "Yusuf H\u00fcseyin", ""], ["\u00dcnal", "G\u00f6zde", ""]]}, {"id": "1809.06200", "submitter": "Mitchell Dawson", "authors": "Mitchell Dawson, Andrew Zisserman and Christoffer Nell{\\aa}ker", "title": "From Same Photo: Cheating on Visual Kinship Challenges", "comments": "Accepted to ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the propensity for deep learning models to learn unintended signals from\ndata sets there is always the possibility that the network can `cheat' in order\nto solve a task. In the instance of data sets for visual kinship verification,\none such unintended signal could be that the faces are cropped from the same\nphotograph, since faces from the same photograph are more likely to be from the\nsame family. In this paper we investigate the influence of this artefactual\ndata inference in published data sets for kinship verification.\n  To this end, we obtain a large dataset, and train a CNN classifier to\ndetermine if two faces are from the same photograph or not. Using this\nclassifier alone as a naive classifier of kinship, we demonstrate near state of\nthe art results on five public benchmark data sets for kinship verification -\nachieving over 90% accuracy on one of them. Thus, we conclude that faces\nderived from the same photograph are a strong inadvertent signal in all the\ndata sets we examined, and it is likely that the fraction of kinship explained\nby existing kinship models is small.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 13:49:44 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 12:09:39 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Dawson", "Mitchell", ""], ["Zisserman", "Andrew", ""], ["Nell\u00e5ker", "Christoffer", ""]]}, {"id": "1809.06201", "submitter": "Matthew Guzdial", "authors": "Zijin Luo, Matthew Guzdial, Nicholas Liao and Mark Riedl", "title": "Player Experience Extraction from Gameplay Video", "comments": "8 pages, 6 figures, AIIDE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to extract the sequence of game events for a given player's\nplay-through has traditionally required access to the game's engine or source\ncode. This serves as a barrier to researchers, developers, and hobbyists who\nmight otherwise benefit from these game logs. In this paper we present two\napproaches to derive game logs from game video via convolutional neural\nnetworks and transfer learning. We evaluate the approaches in a Super Mario\nBros. clone, Mega Man and Skyrim. Our results demonstrate our approach\noutperforms random forest and other transfer baselines.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 22:00:46 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Luo", "Zijin", ""], ["Guzdial", "Matthew", ""], ["Liao", "Nicholas", ""], ["Riedl", "Mark", ""]]}, {"id": "1809.06209", "submitter": "Ahsan Tufail Mr.", "authors": "Ahsan Bin Tufail, Qiu-Na Zhang, Yong-Kui Ma", "title": "Binary Classification of Alzheimer Disease using sMRI Imaging modality\n  and Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1007/s10278-019-00265-5", "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) is an irreversible devastative neurodegenerative\ndisorder associated with progressive impairment of memory and cognitive\nfunctions. Its early diagnosis is crucial for the development of possible\nfuture treatment option(s). Structural magnetic resonance images (sMRI) plays\nan important role to help in understanding the anatomical changes related to AD\nespecially in its early stages. Conventional methods require the expertise of\ndomain experts and extract hand-picked features such as gray matter\nsubstructures and train a classifier to distinguish AD subjects from healthy\nsubjects. Different from these methods, this paper proposes to construct\nmultiple deep 2D convolutional neural networks (2D-CNNs) to learn the various\nfeatures from local brain images which are combined to make the final\nclassification for AD diagnosis. The whole brain image was passed through two\ntransfer learning architectures; Inception version 3 and Xception; as well as\ncustom Convolutional Neural Network (CNN) built with the help of separable\nconvolutional layers which can automatically learn the generic features from\nimaging data for classification. Our study is conducted using cross-sectional\nT1-weighted structural MRI brain images from Open Access Series of Imaging\nStudies (OASIS) database to maintain the size and contrast over different MRI\nscans. Experimental results show that the transfer learning approaches exceed\nthe performance of non-transfer learning based approaches demonstrating the\neffectiveness of these approaches for the binary AD classification task.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 01:46:37 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 14:02:39 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 18:04:27 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Tufail", "Ahsan Bin", ""], ["Zhang", "Qiu-Na", ""], ["Ma", "Yong-Kui", ""]]}, {"id": "1809.06211", "submitter": "Rudrasis Chakraborty Dr.", "authors": "Rudrasis Chakraborty, Jose Bouza, Jonathan Manton and Baba C. Vemuri", "title": "ManifoldNet: A Deep Network Framework for Manifold-valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become the main work horse for many tasks involving\nlearning from data in a variety of applications in Science and Engineering.\nTraditionally, the input to these networks lie in a vector space and the\noperations employed within the network are well defined on vector-spaces. In\nthe recent past, due to technological advances in sensing, it has become\npossible to acquire manifold-valued data sets either directly or indirectly.\nExamples include but are not limited to data from omnidirectional cameras on\nautomobiles, drones etc., synthetic aperture radar imaging, diffusion magnetic\nresonance imaging, elastography and conductance imaging in the Medical Imaging\ndomain and others. Thus, there is need to generalize the deep neural networks\nto cope with input data that reside on curved manifolds where vector space\noperations are not naturally admissible. In this paper, we present a novel\ntheoretical framework to generalize the widely popular convolutional neural\nnetworks (CNNs) to high dimensional manifold-valued data inputs. We call these\nnetworks, ManifoldNets.\n  In ManifoldNets, convolution operation on data residing on Riemannian\nmanifolds is achieved via a provably convergent recursive computation of the\nweighted Fr\\'{e}chet Mean (wFM) of the given data, where the weights makeup the\nconvolution mask, to be learned. Further, we prove that the proposed wFM layer\nachieves a contraction mapping and hence ManifoldNet does not need the\nnon-linear ReLU unit used in standard CNNs. We present experiments, using the\nManifoldNet framework, to achieve dimensionality reduction by computing the\nprincipal linear subspaces that naturally reside on a Grassmannian. The\nexperimental results demonstrate the efficacy of ManifoldNets in the context of\nclassification and reconstruction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 00:27:48 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 04:17:34 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 18:58:35 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Bouza", "Jose", ""], ["Manton", "Jonathan", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "1809.06213", "submitter": "Zhen Cui", "authors": "Zhen Cui, Chunyan Xu, Wenming Zheng and Jian Yang", "title": "Context-Dependent Diffusion Network for Visual Relationship Detection", "comments": "8 pages, 3 figures, 2018 ACM Multimedia Conference (MM'18)", "journal-ref": null, "doi": "10.1145/3240508.3240668", "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship detection can bridge the gap between computer vision and\nnatural language for scene understanding of images. Different from pure object\nrecognition tasks, the relation triplets of subject-predicate-object lie on an\nextreme diversity space, such as \\textit{person-behind-person} and\n\\textit{car-behind-building}, while suffering from the problem of combinatorial\nexplosion. In this paper, we propose a context-dependent diffusion network\n(CDDN) framework to deal with visual relationship detection. To capture the\ninteractions of different object instances, two types of graphs, word semantic\ngraph and visual scene graph, are constructed to encode global context\ninterdependency. The semantic graph is built through language priors to model\nsemantic correlations across objects, whilst the visual scene graph defines the\nconnections of scene objects so as to utilize the surrounding scene\ninformation. For the graph-structured data, we design a diffusion network to\nadaptively aggregate information from contexts, which can effectively learn\nlatent representations of visual relationships and well cater to visual\nrelationship detection in view of its isomorphic invariance to graphs.\nExperiments on two widely-used datasets demonstrate that our proposed method is\nmore effective and achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 02:13:45 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Cui", "Zhen", ""], ["Xu", "Chunyan", ""], ["Zheng", "Wenming", ""], ["Yang", "Jian", ""]]}, {"id": "1809.06214", "submitter": "ChengKuan Chen", "authors": "Cheng Kuan Chen, Zhu Feng Pan, Min Sun, Ming-Yu Liu", "title": "Unsupervised Stylish Image Description Generation via Domain Layer Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing works on image description focus on generating\nexpressive descriptions. The only few works that are dedicated to generating\nstylish (e.g., romantic, lyric, etc.) descriptions suffer from limited style\nvariation and content digression. To address these limitations, we propose a\ncontrollable stylish image description generation model. It can learn to\ngenerate stylish image descriptions that are more related to image content and\ncan be trained with the arbitrary monolingual corpus without collecting new\npaired image and stylish descriptions. Moreover, it enables users to generate\nvarious stylish descriptions by plugging in style-specific parameters to\ninclude new styles into the existing model. We achieve this capability via a\nnovel layer normalization layer design, which we will refer to as the Domain\nLayer Norm (DLN). Extensive experimental validation and user study on various\nstylish image description generation tasks are conducted to show the\ncompetitive advantages of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 11:07:26 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Chen", "Cheng Kuan", ""], ["Pan", "Zhu Feng", ""], ["Sun", "Min", ""], ["Liu", "Ming-Yu", ""]]}, {"id": "1809.06215", "submitter": "Soumi Ray", "authors": "Soumi Ray, Vinod Kumar, Chirag Ahuja, Niranjan Khandelwal", "title": "An Automatic Method for Complete Brain Matter Segmentation from\n  Multislice CT scan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computed tomography imaging is well accepted for its imaging speed, image\ncontrast & resolution and cost. Thus it has wide use in detection and diagnosis\nof brain diseases. But unfortunately reported works on CT segmentation is not\nvery significant. In this paper, a robust automatic segmentation system is\npresented which is capable of segment complete brain matter from CT slices,\nwithout any lose in information. The proposed method is simple, fast, accurate\nand completely automatic. It can handle multislice CT scan in single run. From\na given multislice CT dataset, one slice is selected automatically to form\nmasks for segmentation. Two types of masks are created to handle nasal slices\nin a better way. Masks are created from selected reference slice using\nautomatic seed point selection and region growing technique. One mask is\ndesigned for brain matter and another includes the skull of the reference\nslice. This second mask is used as global reference mask for all slices whereas\nthe brain matter mask is implemented on only adjacent slices and continuously\nmodified for better segmentation. Slices in given dataset are divided into two\nbatches, before reference slice and after reference slice. Each batch segmented\nseparately. Successive propagation of brain matter mask has demonstrated very\nhigh potential in reported segmentation. Presented result shows highest\nsensitivity and more than 96% accuracy in all cases. Resulted segmented images\ncan be used for any brain disease diagnosis or further image analysis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 06:35:37 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 07:11:06 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Ray", "Soumi", ""], ["Kumar", "Vinod", ""], ["Ahuja", "Chirag", ""], ["Khandelwal", "Niranjan", ""]]}, {"id": "1809.06217", "submitter": "Hamid Tizhoosh", "authors": "Aravind Ravi, Harshwin Venugopal, Sruthy Paul, Hamid R. Tizhoosh", "title": "A Dataset and Preliminary Results for Umpire Pose Detection Using SVM\n  Classification of Deep Features", "comments": "To be published at the 2018 IEEE Symposium Series on Computational\n  Intelligence (IEEE SSCI 2018), 18-21 NOV, 2018, BENGALURU, INDIA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been increased interest in video summarization and\nautomatic sports highlights generation. In this work, we introduce a new\ndataset, called SNOW, for umpire pose detection in the game of cricket. The\nproposed dataset is evaluated as a preliminary aid for developing systems to\nautomatically generate cricket highlights. In cricket, the umpire has the\nauthority to make important decisions about events on the field. The umpire\nsignals important events using unique hand signals and gestures. We identify\nfour such events for classification namely SIX, NO BALL, OUT and WIDE based on\ndetecting the pose of the umpire from the frames of a cricket video.\nPre-trained convolutional neural networks such as Inception V3 and VGG19\nnet-works are selected as primary candidates for feature extraction. The\nresults are obtained using a linear SVM classifier. The highest classification\nperformance was achieved for the SVM trained on features extracted from the\nVGG19 network. The preliminary results suggest that the proposed system is an\neffective solution for the application of cricket highlights generation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 12:44:57 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Ravi", "Aravind", ""], ["Venugopal", "Harshwin", ""], ["Paul", "Sruthy", ""], ["Tizhoosh", "Hamid R.", ""]]}, {"id": "1809.06218", "submitter": "Hamid Tizhoosh", "authors": "Dhruv Sharma, Sarim Zafar, Morteza Babaie, H.R.Tizhoosh", "title": "Facial Recognition with Encoded Local Projections", "comments": "To be published at the 2018 IEEE Symp. Series on Comp. Intelligence\n  (IEEE SSCI 2018), 18-21 NOV, 2018, BENGALURU, INDIA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoded Local Projections (ELP) is a recently introduced dense sampling image\ndescriptor which uses projections in small neighbourhoods to construct a\nhistogram/descriptor for the entire image. ELP has shown to be as accurate as\nother state-of-the-art features in searching medical images while being time\nand resource efficient. This paper attempts for the first time to utilize ELP\ndescriptor as primary features for facial recognition and compare the results\nwith LBP histogram on the Labeled Faces in the Wild dataset. We have evaluated\ndescriptors by comparing the chi-squared distance of each image descriptor\nversus all others as well as training Support Vector Machines (SVM) with each\nfeature vector. In both cases, the results of ELP were better than LBP in the\nsame sub-image configuration.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 13:11:07 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Sharma", "Dhruv", ""], ["Zafar", "Sarim", ""], ["Babaie", "Morteza", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1809.06219", "submitter": "Meenakshi Khosla", "authors": "Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, Mert R. Sabuncu", "title": "Ensemble learning with 3D convolutional neural networks for\n  connectome-based prediction", "comments": "45 pages, 9 figures, 4 supplementary figures (To appear in\n  Neuroimage)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The specificty and sensitivity of resting state functional MRI (rs-fMRI)\nmeasurements depend on pre-processing choices, such as the parcellation scheme\nused to define regions of interest (ROIs). In this study, we critically\nevaluate the effect of brain parcellations on machine learning models applied\nto rs-fMRI data. Our experiments reveal a remarkable trend: On average, models\nwith stochastic parcellations consistently perform as well as models with\nwidely used atlases at the same spatial scale. We thus propose an ensemble\nlearning strategy to combine the predictions from models trained on\nconnectivity data extracted using different (e.g., stochastic) parcellations.\nWe further present an implementation of our ensemble learning strategy with a\nnovel 3D Convolutional Neural Network (CNN) approach. The proposed CNN approach\ntakes advantage of the full-resolution 3D spatial structure of rs-fMRI data and\nfits non-linear predictive models. Our ensemble CNN framework overcomes the\nlimitations of traditional machine learning models for connectomes that often\nrely on region-based summary statistics and/or linear models. We showcase our\napproach on a classification (autism patients versus healthy controls) and a\nregression problem (prediction of subject's age), and report promising results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 17:55:10 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 21:55:28 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Khosla", "Meenakshi", ""], ["Jamison", "Keith", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1809.06222", "submitter": "Christoph Baur", "authors": "Salome Kazeminia, Christoph Baur, Arjan Kuijper, Bram van Ginneken,\n  Nassir Navab, Shadi Albarqouni, Anirban Mukhopadhyay", "title": "GANs for Medical Image Analysis", "comments": "Salome Kazeminia and Christoph Baur contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) and their extensions have carved open\nmany exciting ways to tackle well known and challenging medical image analysis\nproblems such as medical image de-noising, reconstruction, segmentation, data\nsimulation, detection or classification. Furthermore, their ability to\nsynthesize images at unprecedented levels of realism also gives hope that the\nchronic scarcity of labeled data in the medical field can be resolved with the\nhelp of these generative models. In this review paper, a broad overview of\nrecent literature on GANs for medical applications is given, the shortcomings\nand opportunities of the proposed methods are thoroughly discussed and\npotential future work is elaborated. We review the most relevant papers\npublished until the submission date. For quick access, important details such\nas the underlying method, datasets and performance are tabulated. An\ninteractive visualization which categorizes all papers to keep the review\nalive, is available at\nhttp://livingreview.in.tum.de/GANs_for_Medical_Applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 21:38:29 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 23:34:44 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 06:31:07 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Kazeminia", "Salome", ""], ["Baur", "Christoph", ""], ["Kuijper", "Arjan", ""], ["van Ginneken", "Bram", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "1809.06225", "submitter": "Zheng Lian", "authors": "Zheng Lian, Ya Li, Jianhua Tao, Jian Huang", "title": "Investigation of Multimodal Features, Classifiers and Fusion Methods for\n  Emotion Recognition", "comments": "9 pages, 11 figures and 4 Tables. EmotiW2018 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic emotion recognition is a challenging task. In this paper, we\npresent our effort for the audio-video based sub-challenge of the Emotion\nRecognition in the Wild (EmotiW) 2018 challenge, which requires participants to\nassign a single emotion label to the video clip from the six universal emotions\n(Anger, Disgust, Fear, Happiness, Sad and Surprise) and Neutral. The proposed\nmultimodal emotion recognition system takes audio, video and text information\ninto account. Except for handcraft features, we also extract bottleneck\nfeatures from deep neutral networks (DNNs) via transfer learning. Both temporal\nclassifiers and non-temporal classifiers are evaluated to obtain the best\nunimodal emotion classification result. Then possibilities are extracted and\npassed into the Beam Search Fusion (BS-Fusion). We test our method in the\nEmotiW 2018 challenge and we gain promising results. Compared with the baseline\nsystem, there is a significant improvement. We achieve 60.34% accuracy on the\ntesting dataset, which is only 1.5% lower than the winner. It shows that our\nmethod is very competitive.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 08:56:25 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Lian", "Zheng", ""], ["Li", "Ya", ""], ["Tao", "Jianhua", ""], ["Huang", "Jian", ""]]}, {"id": "1809.06226", "submitter": "Stergios Christodoulidis Mr.", "authors": "Stergios Christodoulidis, Mihir Sahasrabudhe, Maria Vakalopoulou,\n  Guillaume Chassagnon, Marie-Pierre Revel, Stavroula Mougiakakou, Nikos\n  Paragios", "title": "Linear and Deformable Image Registration with 3D Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration and in particular deformable registration methods are\npillars of medical imaging. Inspired by the recent advances in deep learning,\nwe propose in this paper, a novel convolutional neural network architecture\nthat couples linear and deformable registration within a unified architecture\nendowed with near real-time performance. Our framework is modular with respect\nto the global transformation component, as well as with respect to the\nsimilarity function while it guarantees smooth displacement fields. We evaluate\nthe performance of our network on the challenging problem of MRI lung\nregistration, and demonstrate superior performance with respect to state of the\nart elastic registration methods. The proposed deformation (between inspiration\n& expiration) was considered within a clinically relevant task of interstitial\nlung disease (ILD) classification and showed promising results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 14:56:44 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Christodoulidis", "Stergios", ""], ["Sahasrabudhe", "Mihir", ""], ["Vakalopoulou", "Maria", ""], ["Chassagnon", "Guillaume", ""], ["Revel", "Marie-Pierre", ""], ["Mougiakakou", "Stavroula", ""], ["Paragios", "Nikos", ""]]}, {"id": "1809.06227", "submitter": "Tszhang Guo", "authors": "Tszhang Guo, Shiyu Chang, Mo Yu and Kun Bai", "title": "Improving Reinforcement Learning Based Image Captioning with Natural\n  Language Prior", "comments": "8 pages, 5 figures, EMNLP2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Reinforcement Learning (RL) approaches have demonstrated advanced\nperformance in image captioning by directly optimizing the metric used for\ntesting. However, this shaped reward introduces learning biases, which reduces\nthe readability of generated text. In addition, the large sample space makes\ntraining unstable and slow. To alleviate these issues, we propose a simple\ncoherent solution that constrains the action space using an n-gram language\nprior. Quantitative and qualitative evaluations on benchmarks show that RL with\nthe simple add-on module performs favorably against its counterpart in terms of\nboth readability and speed of convergence. Human evaluation results show that\nour model is more human readable and graceful. The implementation will become\npublicly available upon the acceptance of the paper.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:21:56 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Guo", "Tszhang", ""], ["Chang", "Shiyu", ""], ["Yu", "Mo", ""], ["Bai", "Kun", ""]]}, {"id": "1809.06247", "submitter": "Mai Nguyen", "authors": "Ehab Abdelmaguid, Jolene Huang, Sanjay Kenchareddy, Disha Singla,\n  Laura Wilke, Mai H. Nguyen, Ilkay Altintas", "title": "Left Ventricle Segmentation and Volume Estimation on Cardiac MRI using\n  Deep Learning", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the United States, heart disease is the leading cause of death for both\nmen and women, accounting for 610,000 deaths each year [1]. Physicians use\nMagnetic Resonance Imaging (MRI) scans to take images of the heart in order to\nnon-invasively estimate its structural and functional parameters for\ncardiovascular diagnosis and disease management. The end-systolic volume (ESV)\nand end-diastolic volume (EDV) of the left ventricle (LV), and the ejection\nfraction (EF) are indicators of heart disease. These measures can be derived\nfrom the segmented contours of the LV; thus, consistent and accurate\nsegmentation of the LV from MRI images are critical to the accuracy of the ESV,\nEDV, and EF, and to non-invasive cardiac disease detection.\n  In this work, various image preprocessing techniques, model configurations\nusing the U-Net deep learning architecture, postprocessing methods, and\napproaches for volume estimation are investigated. An end-to-end analytics\npipeline with multiple stages is provided for automated LV segmentation and\nvolume estimation. First, image data are reformatted and processed from DICOM\nand NIfTI formats to raw images in array format. Secondly, raw images are\nprocessed with multiple image preprocessing methods and cropped to include only\nthe Region of Interest (ROI). Thirdly, preprocessed images are segmented using\nU-Net models. Lastly, post processing of segmented images to remove extra\ncontours along with intelligent slice and frame selection are applied, followed\nby calculation of the ESV, EDV, and EF. This analytics pipeline is implemented\nand runs on a distributed computing environment with a GPU cluster at the San\nDiego Supercomputer Center at UCSD.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 06:40:07 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 16:58:27 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Abdelmaguid", "Ehab", ""], ["Huang", "Jolene", ""], ["Kenchareddy", "Sanjay", ""], ["Singla", "Disha", ""], ["Wilke", "Laura", ""], ["Nguyen", "Mai H.", ""], ["Altintas", "Ilkay", ""]]}, {"id": "1809.06256", "submitter": "Alexandra Carlson", "authors": "Alexandra Carlson, Katherine A. Skinner, Ram Vasudevan and Matthew\n  Johnson-Roberson", "title": "Sensor Transfer: Learning Optimal Sensor Effect Image Augmentation for\n  Sim-to-Real Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance on benchmark datasets has drastically improved with advances in\ndeep learning. Still, cross-dataset generalization performance remains\nrelatively low due to the domain shift that can occur between two different\ndatasets. This domain shift is especially exaggerated between synthetic and\nreal datasets. Significant research has been done to reduce this gap,\nspecifically via modeling variation in the spatial layout of a scene, such as\nocclusions, and scene environmental factors, such as time of day and weather\neffects. However, few works have addressed modeling the variation in the sensor\ndomain as a means of reducing the synthetic to real domain gap. The camera or\nsensor used to capture a dataset introduces artifacts into the image data that\nare unique to the sensor model, suggesting that sensor effects may also\ncontribute to domain shift. To address this, we propose a learned augmentation\nnetwork composed of physically-based augmentation functions. Our proposed\naugmentation pipeline transfers specific effects of the sensor model --\nchromatic aberration, blur, exposure, noise, and color temperature -- from a\nreal dataset to a synthetic dataset. We provide experiments that demonstrate\nthat augmenting synthetic training datasets with the proposed learned\naugmentation framework reduces the domain gap between synthetic and real\ndomains for object detection in urban driving scenes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 14:44:42 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 17:01:45 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Carlson", "Alexandra", ""], ["Skinner", "Katherine A.", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1809.06263", "submitter": "Yen-Chia Hsu", "authors": "Yen-Chia Hsu, Paul Dille, Randy Sargent, Illah Nourbakhsh", "title": "Industrial Smoke Detection and Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMU-RI-TR-16-55", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As sensing technology proliferates and becomes affordable to the general\npublic, there is a growing trend in citizen science where scientists and\nvolunteers form a strong partnership in conducting scientific research\nincluding problem finding, data collection, analysis, visualization, and\nstorytelling. Providing easy-to-use computational tools to support citizen\nscience has become an important issue. To raise the public awareness of\nenvironmental science and improve the air quality in local areas, we are\ncurrently collaborating with a local community in monitoring and documenting\nfugitive emissions from a coke refinery. We have helped the community members\nbuild a live camera system which captures and visualizes high resolution\ntimelapse imagery starting from November 2014. However, searching and\ndocumenting smoke emissions manually from all video frames requires manpower\nand takes an impractical investment of time. This paper describes a software\ntool which integrates four features: (1) an algorithm based on change detection\nand texture segmentation for identifying smoke emissions; (2) an interactive\ntimeline visualization providing indicators for seeking to interesting events;\n(3) an autonomous fast-forwarding mode for skipping uninteresting timelapse\nframes; and (4) a collection of animated smoke images generated automatically\naccording to the algorithm for documentation, presentation, storytelling, and\nsharing. With the help of this tool, citizen scientists can now focus on the\ncontent of the story instead of time-consuming and laborious works.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 14:51:56 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Hsu", "Yen-Chia", ""], ["Dille", "Paul", ""], ["Sargent", "Randy", ""], ["Nourbakhsh", "Illah", ""]]}, {"id": "1809.06269", "submitter": "Luis Herranz", "authors": "Xinhang Song, Shuqiang Jiang, Luis Herranz, Chengpeng Chen", "title": "Learning Effective RGB-D Representations for Scene Recognition", "comments": "Accepted at IEEE Transactions on Image Processing", "journal-ref": "IEEE Transactions on Image Processing, vol. 28, no. 2, pp.\n  980-993, Feb. 2019", "doi": "10.1109/TIP.2018.2872629", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks (CNN) can achieve impressive results on RGB scene\nrecognition thanks to large datasets such as Places. In contrast, RGB-D scene\nrecognition is still underdeveloped in comparison, due to two limitations of\nRGB-D data we address in this paper. The first limitation is the lack of depth\ndata for training deep learning models. Rather than fine tuning or transferring\nRGB-specific features, we address this limitation by proposing an architecture\nand a two-step training approach that directly learns effective depth-specific\nfeatures using weak supervision via patches. The resulting RGB-D model also\nbenefits from more complementary multimodal features. Another limitation is the\nshort range of depth sensors (typically 0.5m to 5.5m), resulting in depth\nimages not capturing distant objects in the scenes that RGB images can. We show\nthat this limitation can be addressed by using RGB-D videos, where more\ncomprehensive depth information is accumulated as the camera travels across the\nscene. Focusing on this scenario, we introduce the ISIA RGB-D video dataset to\nevaluate RGB-D scene recognition with videos. Our video recognition\narchitecture combines convolutional and recurrent neural networks (RNNs) that\nare trained in three steps with increasingly complex data to learn effective\nfeatures (i.e. patches, frames and sequences). Our approach obtains\nstate-of-the-art performances on RGB-D image (NYUD2 and SUN RGB-D) and video\n(ISIA RGB-D) scene recognition.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 15:07:47 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Song", "Xinhang", ""], ["Jiang", "Shuqiang", ""], ["Herranz", "Luis", ""], ["Chen", "Chengpeng", ""]]}, {"id": "1809.06276", "submitter": "Karim Armanious", "authors": "Karim Armanious, Sergios Gatidis, Konstantin Nikolaou, Bin Yang,\n  Thomas K\\\"ustner", "title": "Retrospective correction of Rigid and Non-Rigid MR motion artifacts\n  using GANs", "comments": "5 pages, 2 figures, under review for the IEEE International Symposium\n  for Biomedical Images", "journal-ref": null, "doi": "10.1109/ISBI.2019.8759509", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion artifacts are a primary source of magnetic resonance (MR) image\nquality deterioration with strong repercussions on diagnostic performance.\nCurrently, MR motion correction is carried out either prospectively, with the\nhelp of motion tracking systems, or retrospectively by mainly utilizing\ncomputationally expensive iterative algorithms. In this paper, we utilize a new\nadversarial framework, titled MedGAN, for the joint retrospective correction of\nrigid and non-rigid motion artifacts in different body regions and without the\nneed for a reference image. MedGAN utilizes a unique combination of\nnon-adversarial losses and a new generator architecture to capture the textures\nand fine-detailed structures of the desired artifact-free MR images.\nQuantitative and qualitative comparisons with other adversarial techniques have\nillustrated the proposed model performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 15:30:17 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 19:11:08 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Armanious", "Karim", ""], ["Gatidis", "Sergios", ""], ["Nikolaou", "Konstantin", ""], ["Yang", "Bin", ""], ["K\u00fcstner", "Thomas", ""]]}, {"id": "1809.06323", "submitter": "Shao-Yuan Lo", "authors": "Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, Jing-Jhih Lin", "title": "Efficient Dense Modules of Asymmetric Convolution for Real-Time Semantic\n  Segmentation", "comments": "Accepted in ACM International Conference on Multimedia in Asia\n  (MMAsia) 2019 [Best Paper Award]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic segmentation plays an important role in practical\napplications such as self-driving and robots. Most semantic segmentation\nresearch focuses on improving estimation accuracy with little consideration on\nefficiency. Several previous studies that emphasize high-speed inference often\nfail to produce high-accuracy segmentation results. In this paper, we propose a\nnovel convolutional network named Efficient Dense modules with Asymmetric\nconvolution (EDANet), which employs an asymmetric convolution structure and\nincorporates dilated convolution and dense connectivity to achieve high\nefficiency at low computational cost and model size. EDANet is 2.7 times faster\nthan the existing fast segmentation network, ICNet, while it achieves a similar\nmIoU score without any additional context module, post-processing scheme, and\npretrained model. We evaluate EDANet on Cityscapes and CamVid datasets, and\ncompare it with the other state-of-art systems. Our network can run with the\nhigh-resolution inputs at the speed of 108 FPS on one GTX 1080Ti.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 16:52:46 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 14:50:26 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 06:11:57 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lo", "Shao-Yuan", ""], ["Hang", "Hsueh-Ming", ""], ["Chan", "Sheng-Wei", ""], ["Lin", "Jing-Jhih", ""]]}, {"id": "1809.06357", "submitter": "Amy Tabb", "authors": "Philipe A. Dias, Amy Tabb, Henry Medeiros", "title": "Apple Flower Detection using Deep Convolutional Networks", "comments": "14 pages", "journal-ref": "Computers in Industry, vol. 99, pp. 17-28, Aug. 2018", "doi": "10.1016/j.compind.2018.03.010", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To optimize fruit production, a portion of the flowers and fruitlets of apple\ntrees must be removed early in the growing season. The proportion to be removed\nis determined by the bloom intensity, i.e., the number of flowers present in\nthe orchard. Several automated computer vision systems have been proposed to\nestimate bloom intensity, but their overall performance is still far from\nsatisfactory even in relatively controlled environments. With the goal of\ndevising a technique for flower identification which is robust to clutter and\nto changes in illumination, this paper presents a method in which a pre-trained\nconvolutional neural network is fine-tuned to become specially sensitive to\nflowers. Experimental results on a challenging dataset demonstrate that our\nmethod significantly outperforms three approaches that represent the state of\nthe art in flower detection, with recall and precision rates higher than\n$90\\%$. Moreover, a performance assessment on three additional datasets\npreviously unseen by the network, which consist of different flower species and\nwere acquired under different conditions, reveals that the proposed method\nhighly surpasses baseline approaches in terms of generalization capability.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 17:46:40 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Dias", "Philipe A.", ""], ["Tabb", "Amy", ""], ["Medeiros", "Henry", ""]]}, {"id": "1809.06367", "submitter": "Eugene Belilovsky", "authors": "Edouard Oyallon (CVN, GALEN), Sergey Zagoruyko (ENPC, LIGM), Gabriel\n  Huang (DIRO, MILA), Nikos Komodakis (ENPC, CSD-UOC, LIGM), Simon\n  Lacoste-Julien (DIRO, MILA), Matthew Blaschko (ESAT), Eugene Belilovsky\n  (DIRO, MILA)", "title": "Scattering Networks for Hybrid Representation Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1703.08961", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Institute of Electrical and Electronics Engineers, 2018, pp.11", "doi": "10.1109/TPAMI.2018.2855738", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scattering networks are a class of designed Convolutional Neural Networks\n(CNNs) with fixed weights. We argue they can serve as generic representations\nfor modelling images. In particular, by working in scattering space, we achieve\ncompetitive results both for supervised and unsupervised learning tasks, while\nmaking progress towards constructing more interpretable CNNs. For supervised\nlearning, we demonstrate that the early layers of CNNs do not necessarily need\nto be learned, and can be replaced with a scattering network instead. Indeed,\nusing hybrid architectures, we achieve the best results with predefined\nrepresentations to-date, while being competitive with end-to-end learned CNNs.\nSpecifically, even applying a shallow cascade of small-windowed scattering\ncoefficients followed by 1$\\times$1-convolutions results in AlexNet accuracy on\nthe ILSVRC2012 classification task. Moreover, by combining scattering networks\nwith deep residual networks, we achieve a single-crop top-5 error of 11.4% on\nILSVRC2012. Also, we show they can yield excellent performance in the small\nsample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end\ncounterparts, through their ability to incorporate geometrical priors. For\nunsupervised learning, scattering coefficients can be a competitive\nrepresentation that permits image recovery. We use this fact to train hybrid\nGANs to generate images. Finally, we empirically analyze several properties\nrelated to stability and reconstruction of images from scattering coefficients.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 06:27:40 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Oyallon", "Edouard", "", "CVN, GALEN"], ["Zagoruyko", "Sergey", "", "ENPC, LIGM"], ["Huang", "Gabriel", "", "DIRO, MILA"], ["Komodakis", "Nikos", "", "ENPC, CSD-UOC, LIGM"], ["Lacoste-Julien", "Simon", "", "DIRO, MILA"], ["Blaschko", "Matthew", "", "ESAT"], ["Belilovsky", "Eugene", "", "DIRO, MILA"]]}, {"id": "1809.06396", "submitter": "Alexandre Sablayrolles", "authors": "Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Herv\\'e\n  J\\'egou", "title": "D\\'ej\\`a Vu: an empirical evaluation of the memorization properties of\n  ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks memorize part of their training data, which is\nwhy strategies such as data augmentation and drop-out are employed to mitigate\noverfitting. This paper considers the related question of \"membership\ninference\", where the goal is to determine if an image was used during\ntraining. We consider it under three complementary angles. We show how to\ndetect which dataset was used to train a model, and in particular whether some\nvalidation images were used at train time. We then analyze explicit\nmemorization and extend classical random label experiments to the problem of\nlearning a model that predicts if an image belongs to an arbitrary set.\nFinally, we propose a new approach to infer membership when a few of the top\nlayers are not available or have been fine-tuned, and show that lower layers\nstill carry information about the training samples. To support our findings, we\nconduct large-scale experiments on Imagenet and subsets of YFCC-100M with\nmodern architectures such as VGG and Resnet.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 18:25:08 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Sablayrolles", "Alexandre", ""], ["Douze", "Matthijs", ""], ["Schmid", "Cordelia", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1809.06398", "submitter": "Amy Tabb", "authors": "Amy Tabb, Keith E. Duncan, Christopher N. Topp", "title": "Segmenting root systems in X-ray computed tomography images using level\n  sets", "comments": "11 pages", "journal-ref": "2018 IEEE Winter Conference on Applications of Computer Vision\n  (WACV), Lake Tahoe, NV/CA. pp. 586-595", "doi": "10.1109/WACV.2018.00070", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The segmentation of plant roots from soil and other growing media in X-ray\ncomputed tomography images is needed to effectively study the root system\narchitecture without excavation. However, segmentation is a challenging problem\nin this context because the root and non-root regions share similar features.\nIn this paper, we describe a method based on level sets and specifically\nadapted for this segmentation problem. In particular, we deal with the issues\nof using a level sets approach on large image volumes for root segmentation,\nand track active regions of the front using an occupancy grid. This method\nallows for straightforward modifications to a narrow-band algorithm such that\nexcessive forward and backward movements of the front can be avoided, distance\nmap computations in a narrow band context can be done in linear time through\nmodification of Meijster et al.'s distance transform algorithm, and regions of\nthe image volume are iteratively used to estimate distributions for root versus\nnon-root classes. Results are shown of three plant species of different\nmaturity levels, grown in three different media. Our method compares favorably\nto a state-of-the-art method for root segmentation in X-ray CT image volumes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 18:31:51 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Tabb", "Amy", ""], ["Duncan", "Keith E.", ""], ["Topp", "Christopher N.", ""]]}, {"id": "1809.06402", "submitter": "Saad Nadeem", "authors": "Saeed Boorboor, Saad Nadeem, Ji Hwan Park, Kevin Baker and Arie\n  Kaufman", "title": "Crowdsourcing Lung Nodules Detection and Annotation", "comments": "7 pages, SPIE Medical Imaging 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present crowdsourcing as an additional modality to aid radiologists in the\ndiagnosis of lung cancer from clinical chest computed tomography (CT) scans.\nMore specifically, a complete workflow is introduced which can help maximize\nthe sensitivity of lung nodule detection by utilizing the collective\nintelligence of the crowd. We combine the concept of overlapping thin-slab\nmaximum intensity projections (TS-MIPs) and cine viewing to render short videos\nthat can be outsourced as an annotation task to the crowd. These videos are\ngenerated by linearly interpolating overlapping TS-MIPs of CT slices through\nthe depth of each quadrant of a patient's lung. The resultant videos are\noutsourced to an online community of non-expert users who, after a brief\ntutorial, annotate suspected nodules in these video segments. Using our\ncrowdsourcing workflow, we achieved a lung nodule detection sensitivity of over\n90% for 20 patient CT datasets (containing 178 lung nodules with sizes between\n1-30mm), and only 47 false positives from a total of 1021 annotations on\nnodules of all sizes (96% sensitivity for nodules$>$4mm). These results show\nthat crowdsourcing can be a robust and scalable modality to aid radiologists in\nscreening for lung cancer, directly or in combination with computer-aided\ndetection (CAD) algorithms. For CAD algorithms, the presented workflow can\nprovide highly accurate training data to overcome the high false-positive rate\n(per scan) problem. We also provide, for the first time, analysis on nodule\nsize and position which can help improve CAD algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 18:41:58 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Boorboor", "Saeed", ""], ["Nadeem", "Saad", ""], ["Park", "Ji Hwan", ""], ["Baker", "Kevin", ""], ["Kaufman", "Arie", ""]]}, {"id": "1809.06408", "submitter": "Saad Nadeem", "authors": "Ji Hwan Park, Saad Nadeem, Joseph Marino, Kevin Baker, Matthew Barish\n  and Arie Kaufman", "title": "Crowd-Assisted Polyp Annotation of Virtual Colonoscopy Videos", "comments": "7 pages, SPIE Medical Imaging 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual colonoscopy (VC) allows a radiologist to navigate through a 3D colon\nmodel reconstructed from a computed tomography scan of the abdomen, looking for\npolyps, the precursors of colon cancer. Polyps are seen as protrusions on the\ncolon wall and haustral folds, visible in the VC fly-through videos. A complete\nreview of the colon surface requires full navigation from the rectum to the\ncecum in antegrade and retrograde directions, which is a tedious task that\ntakes an average of 30 minutes. Crowdsourcing is a technique for non-expert\nusers to perform certain tasks, such as image or video annotation. In this\nwork, we use crowdsourcing for the examination of complete VC fly-through\nvideos for polyp annotation by non-experts. The motivation for this is to\npotentially help the radiologist reach a diagnosis in a shorter period of time,\nand provide a stronger confirmation of the eventual diagnosis. The\ncrowdsourcing interface includes an interactive tool for the crowd to annotate\nsuspected polyps in the video with an enclosing box. Using our workflow, we\nachieve an overall polyps-per-patient sensitivity of 87.88% (95.65% for polyps\n$\\geq$5mm and 70% for polyps $<$5mm). We also demonstrate the efficacy and\neffectiveness of a non-expert user in detecting and annotating polyps and\ndiscuss their possibility in aiding radiologists in VC examinations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 18:58:44 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Park", "Ji Hwan", ""], ["Nadeem", "Saad", ""], ["Marino", "Joseph", ""], ["Baker", "Kevin", ""], ["Barish", "Matthew", ""], ["Kaufman", "Arie", ""]]}, {"id": "1809.06417", "submitter": "Saad Nadeem", "authors": "Liang Shen, Dengming Zhu, Saad Nadeem, Zhaoqi Wang and Arie Kaufman", "title": "Radiative Transport Based Flame Volume Reconstruction from Videos", "comments": "IEEE Transactions on Visualization and Computer Graphics, 24(7):\n  2209-2222, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for flame volume reconstruction from videos\nusing inexpensive charge-coupled device (CCD) consumer cameras. The approach\nincludes an economical data capture technique using inexpensive CCD cameras.\nLeveraging the smear feature of the CCD chip, we present a technique for\nsynchronizing CCD cameras while capturing flame videos from different views.\nOur reconstruction is based on the radiative transport equation which enables\ncomplex phenomena such as emission, extinction, and scattering to be used in\nthe rendering process. Both the color intensity and temperature reconstructions\nare implemented using the CUDA parallel computing framework, which provides\nreal-time performance and allows visualization of reconstruction results after\nevery iteration. We present the results of our approach using real captured\ndata and physically-based simulated data. Finally, we also compare our approach\nagainst the other state-of-the-art flame volume reconstruction methods and\ndemonstrate the efficacy and efficiency of our approach in four different\napplications: (1) rendering of reconstructed flames in virtual environments,\n(2) rendering of reconstructed flames in augmented reality, (3) flame\nstylization, and (4) reconstruction of other semitransparent phenomena.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 19:57:06 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Shen", "Liang", ""], ["Zhu", "Dengming", ""], ["Nadeem", "Saad", ""], ["Wang", "Zhaoqi", ""], ["Kaufman", "Arie", ""]]}, {"id": "1809.06420", "submitter": "Thomas K\\\"ohler", "authors": "Thomas K\\\"ohler, Michel B\\\"atz, Farzad Naderi, Andr\\'e Kaup, Andreas\n  Maier, Christian Riess", "title": "Toward Bridging the Simulated-to-Real Gap: Benchmarking Super-Resolution\n  on Real Data", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence; data and source code available at\n  https://superresolution.tf.fau.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing ground truth data to benchmark super-resolution (SR) is\nchallenging. Therefore, current quantitative studies are mainly evaluated on\nsimulated data artificially sampled from ground truth images. We argue that\nsuch evaluations overestimate the actual performance of SR methods compared to\ntheir behavior on real images. Toward bridging this simulated-to-real gap, we\nintroduce the Super-Resolution Erlangen (SupER) database, the first\ncomprehensive laboratory SR database of all-real acquisitions with pixel-wise\nground truth. It consists of more than 80k images of 14 scenes combining\ndifferent facets: CMOS sensor noise, real sampling at four resolution levels,\nnine scene motion types, two photometric conditions, and lossy video coding at\nfive levels. As such, the database exceeds existing benchmarks by an order of\nmagnitude in quality and quantity. This paper also benchmarks 19 popular\nsingle-image and multi-frame algorithms on our data. The benchmark comprises a\nquantitative study by exploiting ground truth data and qualitative evaluations\nin a large-scale observer study. We also rigorously investigate agreements\nbetween both evaluations from a statistical perspective. One interesting result\nis that top-performing methods on simulated data may be surpassed by others on\nreal data. Our insights can spur further algorithm development, and the publicy\navailable dataset can foster future evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 19:59:12 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 20:21:55 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["K\u00f6hler", "Thomas", ""], ["B\u00e4tz", "Michel", ""], ["Naderi", "Farzad", ""], ["Kaup", "Andr\u00e9", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""]]}, {"id": "1809.06442", "submitter": "Saad Nadeem", "authors": "Saad Nadeem, Xianfeng Gu and Arie Kaufman", "title": "LMap: Shape-Preserving Local Mappings for Biomedical Visualization", "comments": "IEEE Transactions on Visualization and Computer Graphics, 24(12):\n  3111-3122, 2018 (12 pages, 11 figures)", "journal-ref": "S. Nadeem, X. Gu, and A. Kaufman. LMap: Shape-Preserving Local\n  Mappings for Biomedical Visualization. IEEE Transactions on Visualization and\n  Computer Graphics, 24(12):3111-3122, 2018", "doi": "10.1109/TVCG.2017.2772237", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization of medical organs and biological structures is a challenging\ntask because of their complex geometry and the resultant occlusions. Global\nspherical and planar mapping techniques simplify the complex geometry and\nresolve the occlusions to aid in visualization. However, while resolving the\nocclusions these techniques do not preserve the geometric context, making them\nless suitable for mission-critical biomedical visualization tasks. In this\npaper, we present a shape-preserving local mapping technique for resolving\nocclusions locally while preserving the overall geometric context. More\nspecifically, we present a novel visualization algorithm, LMap, for conformally\nparameterizing and deforming a selected local region-of-interest (ROI) on an\narbitrary surface. The resultant shape-preserving local mappings help to\nvisualize complex surfaces while preserving the overall geometric context. The\nalgorithm is based on the robust and efficient extrinsic Ricci flow technique,\nand uses the dynamic Ricci flow algorithm to guarantee the existence of a local\nmap for a selected ROI on an arbitrary surface. We show the effectiveness and\nefficacy of our method in three challenging use cases: (1) multimodal brain\nvisualization, (2) optimal coverage of virtual colonoscopy centerline\nflythrough, and (3) molecular surface visualization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 20:56:17 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 20:24:11 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Nadeem", "Saad", ""], ["Gu", "Xianfeng", ""], ["Kaufman", "Arie", ""]]}, {"id": "1809.06461", "submitter": "Chuanhai Zhang", "authors": "Chuanhai Zhang, Kurt Loken, Zhiyu Chen, Zhiyong Xiao, Gary Kunkel", "title": "Mask Editor : an Image Annotation Tool for Image Segmentation Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network (DCNN) is the state-of-the-art method for\nimage segmentation, which is one of key challenging computer vision tasks.\nHowever, DCNN requires a lot of training images with corresponding image masks\nto get a good segmentation result. Image annotation software which is easy to\nuse and allows fast image mask generation is in great demand. To the best of\nour knowledge, all existing image annotation software support only drawing\nbounding polygons, bounding boxes, or bounding ellipses to mark target objects.\nThese existing software are inefficient when targeting objects that have\nirregular shapes (e.g., defects in fabric images or tire images). In this paper\nwe design an easy-to-use image annotation software called Mask Editor for image\nmask generation. Mask Editor allows drawing any bounding curve to mark objects\nand improves efficiency to mark objects with irregular shapes. Mask Editor also\nsupports drawing bounding polygons, drawing bounding boxes, drawing bounding\nellipses, painting, erasing, super-pixel-marking, image cropping, multi-class\nmasks, mask loading, and mask modifying.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 22:25:55 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Zhang", "Chuanhai", ""], ["Loken", "Kurt", ""], ["Chen", "Zhiyu", ""], ["Xiao", "Zhiyong", ""], ["Kunkel", "Gary", ""]]}, {"id": "1809.06508", "submitter": "Minghui Liao", "authors": "Minghui Liao, Jian Zhang, Zhaoyi Wan, Fengming Xie, Jiajun Liang,\n  Pengyuan Lyu, Cong Yao, Xiang Bai", "title": "Scene Text Recognition from Two-Dimensional Perspective", "comments": "To appear in AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by speech recognition, recent state-of-the-art algorithms mostly\nconsider scene text recognition as a sequence prediction problem. Though\nachieving excellent performance, these methods usually neglect an important\nfact that text in images are actually distributed in two-dimensional space. It\nis a nature quite different from that of speech, which is essentially a\none-dimensional signal. In principle, directly compressing features of text\ninto a one-dimensional form may lose useful information and introduce extra\nnoise. In this paper, we approach scene text recognition from a two-dimensional\nperspective. A simple yet effective model, called Character Attention Fully\nConvolutional Network (CA-FCN), is devised for recognizing the text of\narbitrary shapes. Scene text recognition is realized with a semantic\nsegmentation network, where an attention mechanism for characters is adopted.\nCombined with a word formation module, CA-FCN can simultaneously recognize the\nscript and predict the position of each character. Experiments demonstrate that\nthe proposed algorithm outperforms previous methods on both regular and\nirregular text datasets. Moreover, it is proven to be more robust to imprecise\nlocalizations in the text detection phase, which are very common in practice.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 02:34:04 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 06:26:25 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Liao", "Minghui", ""], ["Zhang", "Jian", ""], ["Wan", "Zhaoyi", ""], ["Xie", "Fengming", ""], ["Liang", "Jiajun", ""], ["Lyu", "Pengyuan", ""], ["Yao", "Cong", ""], ["Bai", "Xiang", ""]]}, {"id": "1809.06547", "submitter": "Sai Sagar Jinka", "authors": "Abbhinav Venkat, Sai Sagar Jinka, Avinash Sharma", "title": "Deep Textured 3D Reconstruction of Human Bodies", "comments": null, "journal-ref": "BRITISH MACHINE VISION CONFERENCE (BMVC), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering textured 3D models of non-rigid human body shapes is challenging\ndue to self-occlusions caused by complex body poses and shapes, clothing\nobstructions, lack of surface texture, background clutter, sparse set of\ncameras with non-overlapping fields of view, etc. Further, a calibration-free\nenvironment adds additional complexity to both - reconstruction and texture\nrecovery. In this paper, we propose a deep learning based solution for textured\n3D reconstruction of human body shapes from a single view RGB image. This is\nachieved by first recovering the volumetric grid of the non-rigid human body\ngiven a single view RGB image followed by orthographic texture view synthesis\nusing the respective depth projection of the reconstructed (volumetric) shape\nand input RGB image. We propose to co-learn the depth information readily\navailable with affordable RGBD sensors (e.g., Kinect) while showing multiple\nviews of the same object during the training phase. We show superior\nreconstruction performance in terms of quantitative and qualitative results, on\nboth, publicly available datasets (by simulating the depth channel with virtual\nKinect) as well as real RGBD data collected with our calibrated multi Kinect\nsetup.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 06:19:55 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Venkat", "Abbhinav", ""], ["Jinka", "Sai Sagar", ""], ["Sharma", "Avinash", ""]]}, {"id": "1809.06557", "submitter": "Bingchen Gong", "authors": "Weifeng Ge, Bingchen Gong, and Yizhou Yu", "title": "Image Super-Resolution via Deterministic-Stochastic Synthesis and Local\n  Statistical Rectification", "comments": "to appear in SIGGRAPH Asia 2018", "journal-ref": null, "doi": "10.1145/3272127.3275060", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image superresolution has been a popular research topic in the last\ntwo decades and has recently received a new wave of interest due to deep neural\nnetworks. In this paper, we approach this problem from a different perspective.\nWith respect to a downsampled low resolution image, we model a high resolution\nimage as a combination of two components, a deterministic component and a\nstochastic component. The deterministic component can be recovered from the\nlow-frequency signals in the downsampled image. The stochastic component, on\nthe other hand, contains the signals that have little correlation with the low\nresolution image. We adopt two complementary methods for generating these two\ncomponents. While generative adversarial networks are used for the stochastic\ncomponent, deterministic component reconstruction is formulated as a regression\nproblem solved using deep neural networks. Since the deterministic component\nexhibits clearer local orientations, we design novel loss functions tailored\nfor such properties for training the deep regression network. These two methods\nare first applied to the entire input image to produce two distinct\nhigh-resolution images. Afterwards, these two images are fused together using\nanother deep neural network that also performs local statistical rectification,\nwhich tries to make the local statistics of the fused image match the same\nlocal statistics of the groundtruth image. Quantitative results and a user\nstudy indicate that the proposed method outperforms existing state-of-the-art\nalgorithms with a clear margin.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 06:50:56 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Ge", "Weifeng", ""], ["Gong", "Bingchen", ""], ["Yu", "Yizhou", ""]]}, {"id": "1809.06576", "submitter": "Ty Nguyen", "authors": "Ty Nguyen, Tolga Ozaslan, Ian D. Miller, James Keller, Giuseppe\n  Loianno, Camillo J. Taylor, Daniel D. Lee, Vijay Kumar, Joseph H. Harwood,\n  Jennifer Wozencraft", "title": "U-Net for MAV-based Penstock Inspection: an Investigation of Focal Loss\n  in Multi-class Segmentation for Corrosion Identification", "comments": "8 Pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periodical inspection and maintenance of critical infrastructure such as\ndams, penstocks, and locks are of significant importance to prevent\ncatastrophic failures. Conventional manual inspection methods require\ninspectors to climb along a penstock to spot corrosion, rust and crack\nformation which is unsafe, labor-intensive, and requires intensive training.\nThis work presents an alternative approach using a Micro Aerial Vehicle (MAV)\nthat autonomously flies to collect imagery which is then fed into a pretrained\ndeep-learning model to identify corrosion. Our simplified U-Net trained with\nless than 40 image samples can do inference at 12 fps on a single GPU. We\nanalyze different loss functions to solve the class imbalance problem, followed\nby a discussion on choosing proper metrics and weights for object classes.\nResults obtained with the dataset collected from Center Hill Dam, TN show that\nfocal loss function, combined with a proper set of class weights yield better\nsegmentation results than the base loss, Softmax cross entropy. Our method can\nbe used in combination with planning algorithm to offer a complete, safe and\ncost-efficient solution to autonomous infrastructure inspection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 08:04:42 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Nguyen", "Ty", ""], ["Ozaslan", "Tolga", ""], ["Miller", "Ian D.", ""], ["Keller", "James", ""], ["Loianno", "Giuseppe", ""], ["Taylor", "Camillo J.", ""], ["Lee", "Daniel D.", ""], ["Kumar", "Vijay", ""], ["Harwood", "Joseph H.", ""], ["Wozencraft", "Jennifer", ""]]}, {"id": "1809.06582", "submitter": "Wladyslaw Skarbek", "authors": "Wladyslaw Skarbek", "title": "Symbolic Tensor Neural Networks for Digital Media - from Tensor\n  Processing via BNF Graph Rules to CREAMS Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial material on Convolutional Neural Networks (CNN) and its\napplications in digital media research is based on the concept of Symbolic\nTensor Neural Networks. The set of STNN expressions is specified in Backus-Naur\nForm (BNF) which is annotated by constraints typical for labeled acyclic\ndirected graphs (DAG). The BNF induction begins from a collection of neural\nunit symbols with extra (up to five) decoration fields (including tensor depth\nand sharing fields). The inductive rules provide not only the general graph\nstructure but also the specific shortcuts for residual blocks of units. A\nsyntactic mechanism for network fragments modularization is introduced via user\ndefined units and their instances. Moreover, the dual BNF rules are specified\nin order to generate the Dual Symbolic Tensor Neural Network (DSTNN). The\njoined interpretation of STNN and DSTNN provides the correct flow of gradient\ntensors, back propagated at the training stage. The proposed symbolic\nrepresentation of CNNs is illustrated for six generic digital media\napplications (CREAMS): Compression, Recognition, Embedding, Annotation, 3D\nModeling for human-computer interfacing, and data Security based on digital\nmedia objects. In order to make the CNN description and its gradient flow\ncomplete, for all presented applications, the symbolic representations of\nmathematically defined loss/gain functions and gradient flow equations for all\nused core units, are given. The tutorial is to convince the reader that STNN is\nnot only a convenient symbolic notation for public presentations of CNN based\nsolutions for CREAMS problems but also that it is a design blueprint with a\npotential for automatic generation of application source code.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 08:25:01 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 09:16:30 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Skarbek", "Wladyslaw", ""]]}, {"id": "1809.06591", "submitter": "Jiangjun Peng", "authors": "Jiangjun Peng, Qi Xie, Qian Zhao, Yao Wang, Deyu Meng, Yee Leung", "title": "Enhanced 3DTV Regularization and Its Applications on Hyper-spectral\n  Image Denoising and Compressed Sensing", "comments": "25 pages, 27 figures, 3 tables, machine learning, regularization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3-D total variation (3DTV) is a powerful regularization term, which\nencodes the local smoothness prior structure underlying a hyper-spectral image\n(HSI), for general HSI processing tasks. This term is calculated by assuming\nidentical and independent sparsity structures on all bands of gradient maps\ncalculated along spatial and spectral HSI modes. This, however, is always\nlargely deviated from the real cases, where the gradient maps are generally\nwith different while correlated sparsity structures across all their bands.\nSuch deviation tends to hamper the performance of the related method by\nadopting such prior term. To this is- sue, this paper proposes an enhanced 3DTV\n(E-3DTV) regularization term beyond conventional 3DTV. Instead of imposing\nsparsity on gradient maps themselves, the new term calculated sparsity on the\nsubspace bases on the gradient maps along their bands, which naturally encode\nthe correlation and difference across these bands, and more faithfully reflect\nthe insightful configurations of an HSI. The E-3DTV term can easily replace the\nprevious 3DTV term and be em- bedded into an HSI processing model to ameliorate\nits performance. The superiority of the proposed methods is substantiated by\nextensive experiments on two typical related tasks: HSI denoising and\ncompressed sensing, as compared with state-of-the-arts designed for both tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 08:35:18 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Peng", "Jiangjun", ""], ["Xie", "Qi", ""], ["Zhao", "Qian", ""], ["Wang", "Yao", ""], ["Meng", "Deyu", ""], ["Leung", "Yee", ""]]}, {"id": "1809.06647", "submitter": "Yunfan Liu", "authors": "Yunfan Liu, Qi Li, Zhenan Sun", "title": "Attribute-aware Face Aging with Wavelet-based Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since it is difficult to collect face images of the same subject over a long\nrange of age span, most existing face aging methods resort to unpaired datasets\nto learn age mappings. However, the matching ambiguity between young and aged\nface images inherent to unpaired training data may lead to unnatural changes of\nfacial attributes during the aging process, which could not be solved by only\nenforcing identity consistency like most existing studies do. In this paper, we\npropose a attribute-aware face aging model with wavelet-based Generative\nAdversarial Networks (GANs) to address the above issues. To be specific, we\nembed facial attribute vectors into both generator and discriminator of the\nmodel to encourage each synthesized elderly face image to be faithful to the\nattribute of its corresponding input. In addition, a wavelet packet transform\n(WPT) module is incorporated to improve the visual fidelity of generated images\nby capturing age-related texture details at multiple scales in the frequency\nspace. Qualitative results demonstrate the ability of our model to synthesize\nvisually plausible face images, and extensive quantitative evaluation results\nshow that the proposed method achieves state-of-the-art performance on existing\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 11:23:12 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 02:44:16 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 10:52:44 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Liu", "Yunfan", ""], ["Li", "Qi", ""], ["Sun", "Zhenan", ""]]}, {"id": "1809.06663", "submitter": "Mohamed Chafik Bakkay", "authors": "Mohamed Chafik Bakkay, Sylvie Chambon, Hatem A. Rashwan, Christian\n  Lubat, S\\'ebastien Barsotti", "title": "Support Vector Machine (SVM) Recognition Approach adapted to Individual\n  and Touching Moths Counting in Trap Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at developing an automatic algorithm for moth recognition\nfrom trap images in real-world conditions. This method uses our previous work\nfor detection [1] and introduces an adapted classification step. More\nprecisely, SVM classifier is trained with a multi-scale descriptor, Histogram\nOf Curviness Saliency (HCS). This descriptor is robust to illumination changes\nand is able to detect and to describe the external and the internal contours of\nthe target insect in multi-scale. The proposed classification method can be\ntrained with a small set of images. Quantitative evaluations show that the\nproposed method is able to classify insects with higher accuracy (rate of\n95.8%) than the state-of-the art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 12:19:01 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Bakkay", "Mohamed Chafik", ""], ["Chambon", "Sylvie", ""], ["Rashwan", "Hatem A.", ""], ["Lubat", "Christian", ""], ["Barsotti", "S\u00e9bastien", ""]]}, {"id": "1809.06664", "submitter": "Isaak Lim", "authors": "Isaak Lim, Alexander Dielen, Marcel Campen, Leif Kobbelt", "title": "A Simple Approach to Intrinsic Correspondence Learning on Unstructured\n  3D Meshes", "comments": "Presented at the ECCV workshop on Geometry meets Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of representation of 3D geometry is of vital importance when it\ncomes to leveraging the recent advances in the field of machine learning for\ngeometry processing tasks. For common unstructured surface meshes\nstate-of-the-art methods rely on patch-based or mapping-based techniques that\nintroduce resampling operations in order to encode neighborhood information in\na structured and regular manner. We investigate whether such resampling can be\navoided, and propose a simple and direct encoding approach. It does not only\nincrease processing efficiency due to its simplicity - its direct nature also\navoids any loss in data fidelity. To evaluate the proposed method, we perform a\nnumber of experiments in the challenging domain of intrinsic, non-rigid shape\ncorrespondence estimation. In comparisons to current methods we observe that\nour approach is able to achieve highly competitive results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 12:19:20 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 19:16:19 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Lim", "Isaak", ""], ["Dielen", "Alexander", ""], ["Campen", "Marcel", ""], ["Kobbelt", "Leif", ""]]}, {"id": "1809.06665", "submitter": "Raji Susan Mathew", "authors": "Raji Susan Mathew and Joseph Suresh Paul", "title": "Compressed Sensing Parallel MRI with Adaptive Shrinkage TV\n  Regularization", "comments": "27 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) methods in magnetic resonance imaging (MRI) offer\nrapid acquisition and improved image quality but require iterative\nreconstruction schemes with regularization to enforce sparsity. Regardless of\nthe difficulty in obtaining a fast numerical solution, the total variation (TV)\nregularization is a preferred choice due to its edge-preserving and structure\nrecovery capabilities. While many approaches have been proposed to overcome the\nnon-differentiability of the TV cost term, an iterative shrinkage based\nformulation allows recovering an image through recursive application of linear\nfiltering and soft thresholding. However, providing an optimal setting for the\nregularization parameter is critical due to its direct impact on the rate of\nconvergence as well as steady state error. In this paper, a regularizer\nadaptively varying in the derivative space is proposed, that follows the\ngeneralized discrepancy principle (GDP). The implementation proceeds by\nadaptively reducing the discrepancy level expressed as the absolute difference\nbetween TV norms of the consistency error and the sparse approximation error. A\ncriterion based on the absolute difference between TV norms of consistency and\nsparse approximation errors is used to update the threshold. Application of the\nadaptive shrinkage TV regularizer to CS recovery of parallel MRI (pMRI) and\ntemporal gradient adaptation in dynamic MRI are shown to result in improved\nimage quality with accelerated convergence. In addition, the adaptive TV-based\niterative shrinkage (ATVIS) provides a significant speed advantage over the\nfast iterative shrinkage-thresholding algorithm (FISTA).\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 12:23:55 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Mathew", "Raji Susan", ""], ["Paul", "Joseph Suresh", ""]]}, {"id": "1809.06690", "submitter": "Dominik Schlegel", "authors": "Dominik Schlegel and Giorgio Grisetti", "title": "Adding Cues to Binary Feature Descriptors for Visual Place Recognition", "comments": "8 pages, 8 figures, source: www.gitlab.com/srrg-software/srrg_bench,\n  submitted to ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an approach to embed continuous and selector cues in\nbinary feature descriptors used for visual place recognition. The embedding is\nachieved by extending each feature descriptor with a binary string that encodes\na cue and supports the Hamming distance metric. Augmenting the descriptors in\nsuch a way has the advantage of being transparent to the procedure used to\ncompare them. We present two concrete applications of our methodology,\ndemonstrating the two considered types of cues. In addition to that, we\nconducted on these applications a broad quantitative and comparative evaluation\ncovering five benchmark datasets and several state-of-the-art image retrieval\napproaches in combination with various binary descriptor types.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 13:19:33 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Schlegel", "Dominik", ""], ["Grisetti", "Giorgio", ""]]}, {"id": "1809.06691", "submitter": "Boyuan Feng", "authors": "Boyuan Feng, Kun Wan, Shu Yang, Yufei Ding", "title": "SECS: Efficient Deep Stream Processing via Class Skew Dichotomy", "comments": "arXiv admin note: text overlap with arXiv:1611.06453 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite that accelerating convolutional neural network (CNN) receives an\nincreasing research focus, the save on resource consumption always comes with a\ndecrease in accuracy. To both increase accuracy and decrease resource\nconsumption, we explore an environment information, called class skew, which is\neasily available and exists widely in daily life. Since the class skew may\nswitch as time goes, we bring up probability layer to utilize class skew\nwithout any overhead during the runtime. Further, we observe class skew\ndichotomy that some class skew may appear frequently in the future, called hot\nclass skew, and others will never appear again or appear seldom, called cold\nclass skew. Inspired by techniques from source code optimization, two modes,\ni.e., interpretation and compilation, are proposed. The interpretation mode\npursues efficient adaption during runtime for cold class skew and the\ncompilation mode aggressively optimize on hot ones for more efficient\ndeployment in the future. Aggressive optimization is processed by\nclass-specific pruning and provides extra benefit. Finally, we design a\nsystematic framework, SECS, to dynamically detect class skew, processing\ninterpretation and compilation, as well as select the most accurate\narchitectures under the runtime resource budget. Extensive evaluations show\nthat SECS can realize end-to-end classification speedups by a factor of 3x to\n11x relative to state-of-the-art convolutional neural networks, at a higher\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 18:03:47 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Feng", "Boyuan", ""], ["Wan", "Kun", ""], ["Yang", "Shu", ""], ["Ding", "Yufei", ""]]}, {"id": "1809.06693", "submitter": "Yuri G. Gordienko", "authors": "Nikita Gordienko, Yuriy Kochura, Vlad Taran, Gang Peng, Yuri Gordienko\n  and Sergii Stirenko", "title": "Capsule Deep Neural Network for Recognition of Historical Graffiti\n  Handwriting", "comments": "6 pages, 8 figures, accepted for 2018 IEEE Ukraine Student, Young\n  Professional and Women in Engineering Congress (UKRSYW), October 2-6, 2018\n  (Kyiv, Ukraine). arXiv admin note: text overlap with arXiv:1808.10862", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of the historical letters (XI-XVIII centuries) carved\non the stoned walls of St.Sophia cathedral in Kyiv (Ukraine) was demonstrated\nby means of capsule deep learning neural network. It was applied to the image\ndataset of the carved Glagolitic and Cyrillic letters (CGCL), which was\nassembled and pre-processed recently for recognition and prediction by machine\nlearning methods\n(https://www.kaggle.com/yoctoman/graffiti-st-sophia-cathedral-kyiv). CGCL\ndataset contains >4000 images for glyphs of 34 letters which are hardly\nrecognized by experts even in contrast to notMNIST dataset with the better\nimages of 10 letters taken from different fonts. Despite the much worse quality\nof CGCL dataset and extremely low number of samples (in comparison to notMNIST\ndataset) the capsule network model demonstrated much better results than the\npreviously used convolutional neural network (CNN). The validation accuracy\n(and validation loss) was higher (lower) for capsule network model than for CNN\nwithout data augmentation even. The area under curve (AUC) values for receiver\noperating characteristic (ROC) were also higher for the capsule network model\nthan for CNN model: 0.88-0.93 (capsule network) and 0.50 (CNN) without data\naugmentation, 0.91-0.95 (capsule network) and 0.51 (CNN) with lossless data\naugmentation, and similar results of 0.91-0.93 (capsule network) and 0.9 (CNN)\nin the regime of lossless data augmentation only. The confusion matrixes were\nmuch better for capsule network than for CNN model and gave the much lower type\nI (false positive) and type II (false negative) values in all three regimes of\ndata augmentation. These results supports the previous claims that capsule-like\nnetworks allow to reduce error rates not only on MNIST digit dataset, but on\nthe other notMNIST letter dataset and the more complex CGCL handwriting\ngraffiti letter dataset also.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 17:02:13 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Gordienko", "Nikita", ""], ["Kochura", "Yuriy", ""], ["Taran", "Vlad", ""], ["Peng", "Gang", ""], ["Gordienko", "Yuri", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1809.06706", "submitter": "Kai Chen", "authors": "Kai Chen, Jingmin Tu, Binbin Xiang, Li Li and Jian Yao", "title": "Multiple Combined Constraints for Image Stitching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several approaches to image stitching use different constraints to estimate\nthe motion model between image pairs. These constraints can be roughly divided\ninto two categories: geometric constraints and photometric constraints. In this\npaper, geometric and photometric constraints are combined to improve the\nalignment quality, which is based on the observation that these two kinds of\nconstraints are complementary. On the one hand, geometric constraints (e.g.,\npoint and line correspondences) are usually spatially biased and are\ninsufficient in some extreme scenes, while photometric constraints are always\nevenly and densely distributed. On the other hand, photometric constraints are\nsensitive to displacements and are not suitable for images with large\nparallaxes, while geometric constraints are usually imposed by feature matching\nand are more robust to handle parallaxes. The proposed method therefore\ncombines them together in an efficient mesh-based image warping framework. It\nachieves better alignment quality than methods only with geometric constraints,\nand can handle larger parallax than photometric-constraint-based method.\nExperimental results on various images illustrate that the proposed method\noutperforms representative state-of-the-art image stitching methods reported in\nthe literature.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 13:34:25 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Kai", ""], ["Tu", "Jingmin", ""], ["Xiang", "Binbin", ""], ["Li", "Li", ""], ["Yao", "Jian", ""]]}, {"id": "1809.06752", "submitter": "Bingjiang Qiu", "authors": "Bingjiang Qiu, Jiapan Guo, J. Kraeima, R.J.H. Borra, M.J.H. Witjes and\n  P.M.A. van Ooijen", "title": "3D segmentation of mandible from multisectional CT scans by\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of mandibles in CT scans during virtual surgical planning is\ncrucial for 3D surgical planning in order to obtain a detailed surface\nrepresentation of the patients bone. Automatic segmentation of mandibles in CT\nscans is a challenging task due to large variation in their shape and size\nbetween individuals. In order to address this challenge we propose a\nconvolutional neural network approach for mandible segmentation in CT scans by\nconsidering the continuum of anatomical structures through different planes.\nThe proposed convolutional neural network adopts the architecture of the U-Net\nand then combines the resulting 2D segmentations from three different planes\ninto a 3D segmentation. We implement such a segmentation approach on 11 neck CT\nscans and then evaluate the performance. We achieve an average dice coefficient\nof $ 0.89 $ on two testing mandible segmentation. Experimental results show\nthat our proposed approach for mandible segmentation in CT scans exhibits high\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 14:08:05 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Qiu", "Bingjiang", ""], ["Guo", "Jiapan", ""], ["Kraeima", "J.", ""], ["Borra", "R. J. H.", ""], ["Witjes", "M. J. H.", ""], ["van Ooijen", "P. M. A.", ""]]}, {"id": "1809.06781", "submitter": "Sam Green", "authors": "Jieliang Luo, Sam Green, Peter Feghali, George Legrady, and \\c{C}etin\n  Kaya Ko\\c{c}", "title": "Visual Diagnostics for Deep Reinforcement Learning Policy Development", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern vision-based reinforcement learning techniques often use convolutional\nneural networks (CNN) as universal function approximators to choose which\naction to take for a given visual input. Until recently, CNNs have been treated\nlike black-box functions, but this mindset is especially dangerous when used\nfor control in safety-critical settings. In this paper, we present our\nextensions of CNN visualization algorithms to the domain of vision-based\nreinforcement learning. We use a simulated drone environment as an example\nscenario. These visualization algorithms are an important tool for behavior\nintrospection and provide insight into the qualities and flaws of trained\npolicies when interacting with the physical world. A video may be seen at\nhttps://sites.google.com/view/drlvisual .\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 18:59:12 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 22:21:09 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Luo", "Jieliang", ""], ["Green", "Sam", ""], ["Feghali", "Peter", ""], ["Legrady", "George", ""], ["Ko\u00e7", "\u00c7etin Kaya", ""]]}, {"id": "1809.06783", "submitter": "Kai Chen", "authors": "Kai Chen, Jingmin Tu and Jian Yao", "title": "Generalized Content-Preserving Warps for Image Stitching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local misalignment caused by global homography is a common issue in image\nstitching task. Content-Preserving Warping (CPW) is a typical method to deal\nwith this issue, in which geometric and photometric constraints are imposed to\nguide the warping process. One of its essential condition however, is colour\nconsistency, and an elusive goal in real world applications. In this paper, we\npropose a Generalized Content-Preserving Warping (GCPW) method to alleviate\nthis problem. GCPW extends the original CPW by applying a colour model that\nexpresses the colour transformation between images locally, thus meeting the\nphotometric constraint requirements for effective image stitching. We combine\nthe photometric and geometric constraints and jointly estimate the colour\ntransformation and the warped mesh vertexes, simultaneously. We align images\nlocally with an optimal grid mesh generated by our GCPW method. Experiments on\nboth synthetic and real images demonstrate that our new method is robust to\ncolour variations, outperforming other state-of-the-art CPW-based image\nstitching methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 14:59:50 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Kai", ""], ["Tu", "Jingmin", ""], ["Yao", "Jian", ""]]}, {"id": "1809.06839", "submitter": "Alexandr A. Kalinin", "authors": "Alexander Buslaev, Alex Parinov, Eugene Khvedchenya, Vladimir I.\n  Iglovikov, Alexandr A. Kalinin", "title": "Albumentations: fast and flexible image augmentations", "comments": null, "journal-ref": null, "doi": "10.3390/info11020125", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a commonly used technique for increasing both the size\nand the diversity of labeled training sets by leveraging input transformations\nthat preserve output labels. In computer vision domain, image augmentations\nhave become a common implicit regularization technique to combat overfitting in\ndeep convolutional neural networks and are ubiquitously used to improve\nperformance. While most deep learning frameworks implement basic image\ntransformations, the list is typically limited to some variations and\ncombinations of flipping, rotating, scaling, and cropping. Moreover, the image\nprocessing speed varies in existing tools for image augmentation. We present\nAlbumentations, a fast and flexible library for image augmentations with many\nvarious image transform operations available, that is also an easy-to-use\nwrapper around other augmentation libraries. We provide examples of image\naugmentations for different computer vision tasks and show that Albumentations\nis faster than other commonly used image augmentation tools on the most of\ncommonly used image transformations. The source code for Albumentations is made\npublicly available online at https://github.com/albu/albumentations\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 17:28:08 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Buslaev", "Alexander", ""], ["Parinov", "Alex", ""], ["Khvedchenya", "Eugene", ""], ["Iglovikov", "Vladimir I.", ""], ["Kalinin", "Alexandr A.", ""]]}, {"id": "1809.06846", "submitter": "Divas Grover", "authors": "Divas Grover, Behrad Toghi", "title": "MNIST Dataset Classification Utilizing k-NN Classifier with Modified\n  Sliding-window Metric", "comments": "Accepted to the Computer Vision Conference (CVC2019), Las Vegas, NV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MNIST dataset of the handwritten digits is known as one of the commonly\nused datasets for machine learning and computer vision research. We aim to\nstudy a widely applicable classification problem and apply a simple yet\nefficient K-nearest neighbor classifier with an enhanced heuristic. We evaluate\nthe performance of the K-nearest neighbor classification algorithm on the MNIST\ndataset where the $L2$ Euclidean distance metric is compared to a modified\ndistance metric which utilizes the sliding window technique in order to avoid\nperformance degradation due to slight spatial misalignments. The accuracy\nmetric and confusion matrices are used as the performance indicators to compare\nthe performance of the baseline algorithm versus the enhanced sliding window\nmethod and results show significant improvement using this proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 17:55:53 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 15:30:09 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 21:49:42 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2019 14:09:28 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Grover", "Divas", ""], ["Toghi", "Behrad", ""]]}, {"id": "1809.06893", "submitter": "Gideon Billings", "authors": "Gideon Billings, Matthew Johnson-Roberson", "title": "SilhoNet: An RGB Method for 6D Object Pose Estimation", "comments": "8 pages, 3 figures", "journal-ref": "IEEE Robotics and Automation Letters 4.4 (2019): 3727-3734", "doi": "10.1109/LRA.2019.2928776", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robot manipulation involves estimating the translation and\norientation of the object to be manipulated as a 6-degree-of-freedom (6D) pose.\nMethods using RGB-D data have shown great success in solving this problem.\nHowever, there are situations where cost constraints or the working environment\nmay limit the use of RGB-D sensors. When limited to monocular camera data only,\nthe problem of object pose estimation is very challenging. In this work, we\nintroduce a novel method called SilhoNet that predicts 6D object pose from\nmonocular images. We use a Convolutional Neural Network (CNN) pipeline that\ntakes in Region of Interest (ROI) proposals to simultaneously predict an\nintermediate silhouette representation for objects with an associated occlusion\nmask and a 3D translation vector. The 3D orientation is then regressed from the\npredicted silhouettes. We show that our method achieves better overall\nperformance on the YCB-Video dataset than two state-of-the art networks for 6D\npose estimation from monocular image input.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 19:15:38 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 08:35:33 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 00:19:42 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 17:27:25 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Billings", "Gideon", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "1809.06965", "submitter": "Seung Bin Baik", "authors": "Seung Bin Baik, Keum Gang Cha", "title": "A Study on Deep Learning Based Sauvegrain Method for Measurement of\n  Puberty Bone Age", "comments": "5 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study applies a technique to expand the number of images to a level that\nallows deep learning. And the applicability of the Sauvegrain method through\ndeep learning with relatively few elbow X-rays is studied. The study was\ncomposed of processes similar to the physicians' bone age assessment\nprocedures. The selected reference images were learned without being included\nin the evaluation data, and at the same time, the data was extended to\naccommodate the number of cases. In addition, we adjusted the X-ray images to\nbetter images using U-Net and selected the ROI with RPN + so as to be able to\nperform bone age estimation through CNN. The mean absolute error of the\nSauvegrain method based on deep learning is 2.8 months and the Mean Absolute\nPercentage Error (MAPE) is 0.018. This result shows that X - ray analysis using\nthe Sauvegrain method shows higher accuracy than that of the age group of\npuberty even in the deep learning base. This means that deep learning of the\nSuvegrain method can be measured at a level similar to that of an expert, based\non the extended X-ray image with the image data extension technique. Finally,\nwe applied the Sauvegrain method to deep learning for accurate measurement of\nbone age at puberty. As a result, the present study is based on deep learning,\nand compared with the evaluation results of experts, it is possible to overcome\nlimitations of the method of measuring bone age based on machine learning which\nwas in TW3 or Greulich & Pyle due to lack of X- I confirmed the fact. And we\nalso presented the Sauvegrain method, which is applicable to adolescents as\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 23:47:08 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Baik", "Seung Bin", ""], ["Cha", "Keum Gang", ""]]}, {"id": "1809.06973", "submitter": "Behnaz Ghoraani", "authors": "Murtadha D. Hssayeni, Michelle A. Burack, M.D., Joohi Jimenez-Shahed,\n  M.D., Behnaz Ghoraani, Ph.D", "title": "Wearable-based Mediation State Detection in Individuals with Parkinson's\n  Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most prevalent complaints of individuals with mid-stage and\nadvanced Parkinson's disease (PD) is the fluctuating response to their\nmedication (i.e., ON state with maximum benefit from medication and OFF state\nwith no benefit from medication). In order to address these motor fluctuations,\nthe patients go through periodic clinical examination where the treating\nphysician reviews the patients' self-report about duration in different\nmedication states and optimize therapy accordingly. Unfortunately, the\npatients' self-report can be unreliable and suffer from recall bias. There is a\nneed to a technology-based system that can provide objective measures about the\nduration in different medication states that can be used by the treating\nphysician to successfully adjust the therapy. In this paper, we developed a\nmedication state detection algorithm to detect medication states using two\nwearable motion sensors. A series of significant features are extracted from\nthe motion data and used in a classifier that is based on a support vector\nmachine with fuzzy labeling. The developed algorithm is evaluated using a\ndataset with 19 PD subjects and a total duration of 1,052.24 minutes (17.54\nhours). The algorithm resulted in an average classification accuracy of 90.5%,\nsensitivity of 94.2%, and specificity of 85.4%.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 01:01:37 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Hssayeni", "Murtadha D.", ""], ["Burack", "Michelle A.", ""], ["D.", "M.", ""], ["Jimenez-Shahed", "Joohi", ""], ["D.", "M.", ""], ["Ghoraani", "Behnaz", ""], ["D", "Ph.", ""]]}, {"id": "1809.06993", "submitter": "Rima Arnaout", "authors": "Rima Arnaout, Lara Curran, Erin Chinn, Yili Zhao, Anita Moon-Grady", "title": "Deep-learning models improve on community-level diagnosis for common\n  congenital heart disease lesions", "comments": "rima.arnaout@ucsf.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prenatal diagnosis of tetralogy of Fallot (TOF) and hypoplastic left heart\nsyndrome (HLHS), two serious congenital heart defects, improves outcomes and\ncan in some cases facilitate in utero interventions. In practice, however, the\nfetal diagnosis rate for these lesions is only 30-50 percent in community\nsettings. Improving fetal diagnosis of congenital heart disease is therefore\ncritical. Deep learning is a cutting-edge machine learning technique for\nfinding patterns in images but has not yet been applied to prenatal diagnosis\nof congenital heart disease. Using 685 retrospectively collected\nechocardiograms from fetuses 18-24 weeks of gestational age from 2000-2018, we\ntrained convolutional and fully-convolutional deep learning models in a\nsupervised manner to (i) identify the five canonical screening views of the\nfetal heart and (ii) segment cardiac structures to calculate fetal cardiac\nbiometrics. We then trained models to distinguish by view between normal\nhearts, TOF, and HLHS. In a holdout test set of images, F-score for\nidentification of the five most important fetal cardiac views was 0.95. Binary\nclassification of unannotated cardiac views of normal heart vs. TOF reached an\noverall sensitivity of 75% and a specificity of 76%, while normal vs. HLHS\nreached a sensitivity of 100% and specificity of 90%, both well above average\ndiagnostic rates for these lesions. Furthermore, segmentation-based\nmeasurements for cardiothoracic ratio (CTR), cardiac axis (CA), and ventricular\nfractional area change (FAC) were compatible with clinically measured metrics\nfor normal, TOF, and HLHS hearts. Thus, using guideline-recommended imaging,\ndeep learning models can significantly improve detection of fetal congenital\nheart disease compared to the common standard of care.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 03:16:26 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Arnaout", "Rima", ""], ["Curran", "Lara", ""], ["Chinn", "Erin", ""], ["Zhao", "Yili", ""], ["Moon-Grady", "Anita", ""]]}, {"id": "1809.07016", "submitter": "Chong Xiang", "authors": "Chong Xiang, Charles R. Qi, Bo Li", "title": "Generating 3D Adversarial Point Clouds", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be vulnerable to adversarial examples which\nare carefully crafted instances to cause the models to make wrong predictions.\nWhile adversarial examples for 2D images and CNNs have been extensively\nstudied, less attention has been paid to 3D data such as point clouds. Given\nmany safety-critical 3D applications such as autonomous driving, it is\nimportant to study how adversarial point clouds could affect current deep 3D\nmodels. In this work, we propose several novel algorithms to craft adversarial\npoint clouds against PointNet, a widely used deep neural network for point\ncloud processing. Our algorithms work in two ways: adversarial point\nperturbation and adversarial point generation. For point perturbation, we shift\nexisting points negligibly. For point generation, we generate either a set of\nindependent and scattered points or a small number (1-3) of point clusters with\nmeaningful shapes such as balls and airplanes which could be hidden in the\nhuman psyche. In addition, we formulate six perturbation measurement metrics\ntailored to the attacks in point clouds and conduct extensive experiments to\nevaluate the proposed algorithms on the ModelNet40 3D shape classification\ndataset. Overall, our attack algorithms achieve a success rate higher than 99%\nfor all targeted attacks\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 05:01:06 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 13:48:49 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 09:36:18 GMT"}, {"version": "v4", "created": "Fri, 12 Jul 2019 12:35:10 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Xiang", "Chong", ""], ["Qi", "Charles R.", ""], ["Li", "Bo", ""]]}, {"id": "1809.07041", "submitter": "Ting Yao", "authors": "Ting Yao, Yingwei Pan, Yehao Li, Tao Mei", "title": "Exploring Visual Relationship for Image Captioning", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is always well believed that modeling relationships between objects would\nbe helpful for representing and eventually describing an image. Nevertheless,\nthere has not been evidence in support of the idea on image description\ngeneration. In this paper, we introduce a new design to explore the connections\nbetween objects for image captioning under the umbrella of attention-based\nencoder-decoder framework. Specifically, we present Graph Convolutional\nNetworks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that\nnovelly integrates both semantic and spatial object relationships into image\nencoder. Technically, we build graphs over the detected objects in an image\nbased on their spatial and semantic connections. The representations of each\nregion proposed on objects are then refined by leveraging graph structure\nthrough GCN. With the learnt region-level features, our GCN-LSTM capitalizes on\nLSTM-based captioning framework with attention mechanism for sentence\ngeneration. Extensive experiments are conducted on COCO image captioning\ndataset, and superior results are reported when comparing to state-of-the-art\napproaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1%\nto 128.7% on COCO testing set.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 07:50:17 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Li", "Yehao", ""], ["Mei", "Tao", ""]]}, {"id": "1809.07048", "submitter": "Jose Carballo", "authors": "Jose A. Carballo, Javier Bonilla, Manuel Berenguel, Jes\\'us\n  Fern\\'andez-Reche, Gin\\'es Garc\\'ia", "title": "New approach for solar tracking systems based on computer vision, low\n  cost hardware and deep learning", "comments": "12 pages, 10 figures,", "journal-ref": "Carballo, J. A., Bonilla, J., Berenguel, M., Fernandez-Reche, J.,\n  & Garcia, G. (2018). New approach for solar tracking systems based on\n  computer vision, low cost hardware and deep learning. Renewable Energy", "doi": "10.1016/j.renene.2018.08.101", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a new approach for Sun tracking systems is presented. Due to\nthe current system limitations regarding costs and operational problems, a new\napproach based on low cost, computer vision open hardware and deep learning has\nbeen developed. The preliminary tests carried out successfully in Plataforma\nsolar de Almeria (PSA), reveal the great potential and show the new approach as\na good alternative to traditional systems. The proposed approach can provide\nkey variables for the Sun tracking system control like cloud movements\nprediction, block and shadow detection, atmospheric attenuation or measures of\nconcentrated solar radiation, which can improve the control strategies of the\nsystem and therefore the system performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:09:04 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Carballo", "Jose A.", ""], ["Bonilla", "Javier", ""], ["Berenguel", "Manuel", ""], ["Fern\u00e1ndez-Reche", "Jes\u00fas", ""], ["Garc\u00eda", "Gin\u00e9s", ""]]}, {"id": "1809.07069", "submitter": "Roland S. Zimmermann", "authors": "Roland S. Zimmermann and Julien N. Siems", "title": "Faster Training of Mask R-CNN by Focusing on Instance Boundaries", "comments": "9 pages, 7 figures, 5 tables", "journal-ref": null, "doi": "10.1016/j.cviu.2019.102795", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an auxiliary task to Mask R-CNN, an instance segmentation network,\nwhich leads to faster training of the mask head. Our addition to Mask R-CNN is\na new prediction head, the Edge Agreement Head, which is inspired by the way\nhuman annotators perform instance segmentation. Human annotators copy the\ncontour of an object instance and only indirectly the occupied instance area.\nHence, the edges of instance masks are particularly useful as they characterize\nthe instance well. The Edge Agreement Head therefore encourages predicted masks\nto have similar image gradients to the ground-truth mask using edge detection\nfilters. We provide a detailed survey of loss combinations and show\nimprovements on the MS COCO Mask metrics compared to using no additional loss.\nOur approach marginally increases the model size and adds no additional\ntrainable model variables. While the computational costs are increased\nslightly, the increment is negligible considering the high computational cost\nof the Mask R-CNN architecture. As the additional network head is only relevant\nduring training, inference speed remains unchanged compared to Mask R-CNN. In a\ndefault Mask R-CNN setup, we achieve a training speed-up and a relative overall\nimprovement of 8.1% on the MS COCO metrics compared to the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:54:18 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 15:55:43 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 19:57:57 GMT"}, {"version": "v4", "created": "Sat, 10 Aug 2019 08:55:37 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zimmermann", "Roland S.", ""], ["Siems", "Julien N.", ""]]}, {"id": "1809.07075", "submitter": "Judith B\\\"utepage", "authors": "Judith B\\\"utepage, Danica Kragic", "title": "Detect, anticipate and generate: Semi-supervised recurrent latent\n  variable models for human activity modeling", "comments": "This paper has been accepted at the IROS 2018 workshop \"Human-Robot\n  Cooperation and Collaboration in Manipulation: Advancements and Challenges\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Successful Human-Robot collaboration requires a predictive model of human\nbehavior. The robot needs to be able to recognize current goals and actions and\nto predict future activities in a given context. However, the spatio-temporal\nsequence of human actions is difficult to model since latent factors such as\nintention, task, knowledge, intuition and preference determine the action\nchoices of each individual. In this work we introduce semi-supervised\nvariational recurrent neural networks which are able to a) model temporal\ndistributions over latent factors and the observable feature space, b)\nincorporate discrete labels such as activity type when available, and c)\ngenerate possible future action sequences on both feature and label level. We\nevaluate our model on the Cornell Activity Dataset CAD-120 dataset. Our model\noutperforms state-of-the-art approaches in both activity and affordance\ndetection and anticipation. Additionally, we show how samples of possible\nfuture action sequences are in line with past observations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 09:04:21 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["B\u00fctepage", "Judith", ""], ["Kragic", "Danica", ""]]}, {"id": "1809.07076", "submitter": "Maxime Ferrera", "authors": "Maxime Ferrera (DTIS, ONERA, Universit\\'e Paris Saclay, LIRMM), Julien\n  Moras (DTIS, ONERA, Universit\\'e Paris Saclay), Pauline Trouv\\'e-Peloux\n  (DTIS, ONERA, Universit\\'e Paris Saclay), Vincent Creuze (ICAR, LIRMM), Denis\n  D\\'egez (DRASSM)", "title": "The Aqualoc Dataset: Towards Real-Time Underwater Localization from a\n  Visual-Inertial-Pressure Acquisition System", "comments": null, "journal-ref": "IROS Workshop - New Horizons for Underwater Intervention Missions:\n  from Current Technologies to Future Applications, Oct 2018, Madrid, Spain", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new underwater dataset acquired from a\nvisual-inertial-pressure acquisition system and meant to be used to benchmark\nvisual odometry, visual SLAM and multi-sensors SLAM solutions. The dataset is\npublicly available and contains ground-truth trajectories for evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 09:04:22 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Ferrera", "Maxime", "", "DTIS, ONERA, Universit\u00e9 Paris Saclay, LIRMM"], ["Moras", "Julien", "", "DTIS, ONERA, Universit\u00e9 Paris Saclay"], ["Trouv\u00e9-Peloux", "Pauline", "", "DTIS, ONERA, Universit\u00e9 Paris Saclay"], ["Creuze", "Vincent", "", "ICAR, LIRMM"], ["D\u00e9gez", "Denis", "", "DRASSM"]]}, {"id": "1809.07082", "submitter": "Matthias Lenga", "authors": "Matthias Lenga, Tobias Klinder, Christian B\\\"urger, Jens von Berg,\n  Astrid Franz, Cristian Lorenz", "title": "Deep Learning Based Rib Centerline Extraction and Labeling", "comments": "This paper was accepted for presentation at the MICCAI MSKI 2018\n  Workshop", "journal-ref": null, "doi": "10.1007/978-3-030-11166-3_9", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated extraction and labeling of rib centerlines is a typically needed\nprerequisite for more advanced assisted reading tools that help the radiologist\nto efficiently inspect all 24 ribs in a CT volume. In this paper, we combine a\ndeep learning-based rib detection with a dedicated centerline extraction\nalgorithm applied to the detection result for the purpose of fast, robust and\naccurate rib centerline extraction and labeling from CT volumes. More\nspecifically, we first apply a fully convolutional neural network (FCNN) to\ngenerate a probability map for detecting the first rib pair, the twelfth rib\npair, and the collection of all intermediate ribs. In a second stage, a newly\ndesigned centerline extraction algorithm is applied to this multi-label\nprobability map. Finally, the distinct detection of first and twelfth rib\nseparately, allows to derive individual rib labels by simple sorting and\ncounting the detected centerlines. We applied our method to CT volumes from 116\npatients which included a variety of different challenges and achieved a\ncenterline accuracy of 0.787 mm with respect to manual centerline annotations.\n  This article is a preprint version of: Lenga M., Klinder T., B\\\"urger C., von\nBerg J., Franz A., Lorenz C. (2019) Deep Learning Based Rib Centerline\nExtraction and Labeling. In: Vrtovec T., Yao J., Zheng G., Pozo J. (eds)\nComputational Methods and Clinical Applications in Musculoskeletal Imaging.\nMSKI 2018. Lecture Notes in Computer Science, vol 11404. Springer, Cham\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 09:09:23 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 20:56:28 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Lenga", "Matthias", ""], ["Klinder", "Tobias", ""], ["B\u00fcrger", "Christian", ""], ["von Berg", "Jens", ""], ["Franz", "Astrid", ""], ["Lorenz", "Cristian", ""]]}, {"id": "1809.07091", "submitter": "Andres C Rodriguez", "authors": "Andres C. Rodriguez and Jan D. Wegner", "title": "Counting the uncountable: deep semantic density estimation from Space", "comments": "Accepted in GCPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to count objects of specific categories that are\nsignificantly smaller than the ground sampling distance of a satellite image.\nThis task is hard due to the cluttered nature of scenes where different object\ncategories occur. Target objects can be partially occluded, vary in appearance\nwithin the same class and look alike to different categories. Since traditional\nobject detection is infeasible due to the small size of objects with respect to\nthe pixel size, we cast object counting as a density estimation problem. To\ndistinguish objects of different classes, our approach combines density\nestimation with semantic segmentation in an end-to-end learnable convolutional\nneural network (CNN). Experiments show that deep semantic density estimation\ncan robustly count objects of various classes in cluttered scenes. Experiments\nalso suggest that we need specific CNN architectures in remote sensing instead\nof blindly applying existing ones from computer vision.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 09:29:35 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 07:15:02 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Rodriguez", "Andres C.", ""], ["Wegner", "Jan D.", ""]]}, {"id": "1809.07099", "submitter": "Mingkui Tan", "authors": "Yong Guo, Qi Chen, Jian Chen, Junzhou Huang, Yanwu Xu, Jiezhang Cao,\n  Peilin Zhao, Mingkui Tan", "title": "Dual Reconstruction Nets for Image Super-Resolution with Gradient\n  Sensitive Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have exhibited promising performance in image\nsuper-resolution (SR) due to the power in learning the non-linear mapping from\nlow-resolution (LR) images to high-resolution (HR) images. However, most deep\nlearning methods employ feed-forward architectures, and thus the dependencies\nbetween LR and HR images are not fully exploited, leading to limited learning\nperformance. Moreover, most deep learning based SR methods apply the pixel-wise\nreconstruction error as the loss, which, however, may fail to capture\nhigh-frequency information and produce perceptually unsatisfying results,\nwhilst the recent perceptual loss relies on some pre-trained deep model and\nthey may not generalize well. In this paper, we introduce a mask to separate\nthe image into low- and high-frequency parts based on image gradient magnitude,\nand then devise a gradient sensitive loss to well capture the structures in the\nimage without sacrificing the recovery of low-frequency content. Moreover, by\ninvestigating the duality in SR, we develop a dual reconstruction network (DRN)\nto improve the SR performance. We provide theoretical analysis on the\ngeneralization performance of our method and demonstrate its effectiveness and\nsuperiority with thorough experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 09:39:41 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Guo", "Yong", ""], ["Chen", "Qi", ""], ["Chen", "Jian", ""], ["Huang", "Junzhou", ""], ["Xu", "Yanwu", ""], ["Cao", "Jiezhang", ""], ["Zhao", "Peilin", ""], ["Tan", "Mingkui", ""]]}, {"id": "1809.07196", "submitter": "Elliot J. Crowley", "authors": "Jack Turner, Jos\\'e Cano, Valentin Radu, Elliot J. Crowley, Michael\n  O'Boyle, Amos Storkey", "title": "Characterising Across-Stack Optimisations for Deep Convolutional Neural\n  Networks", "comments": "IISWC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) are extremely computationally demanding,\npresenting a large barrier to their deployment on resource-constrained devices.\nSince such systems are where some of their most useful applications lie (e.g.\nobstacle detection for mobile robots, vision-based medical assistive\ntechnology), significant bodies of work from both machine learning and systems\ncommunities have attempted to provide optimisations that will make CNNs\navailable to edge devices. In this paper we unify the two viewpoints in a Deep\nLearning Inference Stack and take an across-stack approach by implementing and\nevaluating the most common neural network compression techniques (weight\npruning, channel pruning, and quantisation) and optimising their parallel\nexecution with a range of programming approaches (OpenMP, OpenCL) and hardware\narchitectures (CPU, GPU). We provide comprehensive Pareto curves to instruct\ntrade-offs under constraints of accuracy, execution time, and memory space.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 13:52:49 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Turner", "Jack", ""], ["Cano", "Jos\u00e9", ""], ["Radu", "Valentin", ""], ["Crowley", "Elliot J.", ""], ["O'Boyle", "Michael", ""], ["Storkey", "Amos", ""]]}, {"id": "1809.07217", "submitter": "M\\'arton V\\'eges", "authors": "M\\'arton V\\'eges, Viktor Varga, Andr\\'as L\\H{o}rincz", "title": "3D Human Pose Estimation with Siamese Equivariant Embedding", "comments": "Accepted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In monocular 3D human pose estimation a common setup is to first detect 2D\npositions and then lift the detection into 3D coordinates. Many algorithms\nsuffer from overfitting to camera positions in the training set. We propose a\nsiamese architecture that learns a rotation equivariant hidden representation\nto reduce the need for data augmentation. Our method is evaluated on multiple\ndatabases with different base networks and shows a consistent improvement of\nerror metrics. It achieves state-of-the-art cross-camera error rate among\nalgorithms that use estimated 2D joint coordinates only.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 14:26:14 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 16:33:41 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["V\u00e9ges", "M\u00e1rton", ""], ["Varga", "Viktor", ""], ["L\u0151rincz", "Andr\u00e1s", ""]]}, {"id": "1809.07238", "submitter": "Sumant Sharma", "authors": "Sumant Sharma, Connor Beierle, Simone D'Amico", "title": "Pose Estimation for Non-Cooperative Spacecraft Rendezvous Using\n  Convolutional Neural Networks", "comments": "Presented at the 2018 IEEE Aerospace Conference, Big Sky, MT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-board estimation of the pose of an uncooperative target spacecraft is an\nessential task for future on-orbit servicing and close-proximity formation\nflying missions. However, two issues hinder reliable on-board monocular vision\nbased pose estimation: robustness to illumination conditions due to a lack of\nreliable visual features and scarcity of image datasets required for training\nand benchmarking. To address these two issues, this work details the design and\nvalidation of a monocular vision based pose determination architecture for\nspaceborne applications. The primary contribution to the state-of-the-art of\nthis work is the introduction of a novel pose determination method based on\nConvolutional Neural Networks (CNN) to provide an initial guess of the pose in\nreal-time on-board. The method involves discretizing the pose space and\ntraining the CNN with images corresponding to the resulting pose labels. Since\nreliable training of the CNN requires massive image datasets and computational\nresources, the parameters of the CNN must be determined prior to the mission\nwith synthetic imagery. Moreover, reliable training of the CNN requires\ndatasets that appropriately account for noise, color, and illumination\ncharacteristics expected in orbit. Therefore, the secondary contribution of\nthis work is the introduction of an image synthesis pipeline, which is tailored\nto generate high fidelity images of any spacecraft 3D model. The proposed\ntechnique is scalable to spacecraft of different structural and physical\nproperties as well as robust to the dynamic illumination conditions of space.\nThrough metrics measuring classification and pose accuracy, it is shown that\nthe presented architecture has desirable robustness and scalable properties.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 15:19:45 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Sharma", "Sumant", ""], ["Beierle", "Connor", ""], ["D'Amico", "Simone", ""]]}, {"id": "1809.07257", "submitter": "Oliver Nina", "authors": "Oliver Nina and Washington Garcia and Scott Clouse and Alper Yilmaz", "title": "MTLE: A Multitask Learning Encoder of Visual Feature Representations for\n  Video and Movie Description", "comments": "This is a pre-print version of our soon to be released paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning visual feature representations for video analysis is a daunting task\nthat requires a large amount of training samples and a proper generalization\nframework. Many of the current state of the art methods for video captioning\nand movie description rely on simple encoding mechanisms through recurrent\nneural networks to encode temporal visual information extracted from video\ndata. In this paper, we introduce a novel multitask encoder-decoder framework\nfor automatic semantic description and captioning of video sequences. In\ncontrast to current approaches, our method relies on distinct decoders that\ntrain a visual encoder in a multitask fashion. Our system does not depend\nsolely on multiple labels and allows for a lack of training data working even\nwith datasets where only one single annotation is viable per video. Our method\nshows improved performance over current state of the art methods in several\nmetrics on multi-caption and single-caption datasets. To the best of our\nknowledge, our method is the first method to use a multitask approach for\nencoding video features. Our method demonstrates its robustness on the Large\nScale Movie Description Challenge (LSMDC) 2017 where our method won the movie\ndescription task and its results were ranked among other competitors as the\nmost helpful for the visually impaired.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 15:50:18 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Nina", "Oliver", ""], ["Garcia", "Washington", ""], ["Clouse", "Scott", ""], ["Yilmaz", "Alper", ""]]}, {"id": "1809.07294", "submitter": "Xin Yi", "authors": "Xin Yi, Ekta Walia, Paul Babyn", "title": "Generative Adversarial Network in Medical Imaging: A Review", "comments": "24 pages; v4; added missing references from before Jan 1st 2019;\n  accepted to MedIA", "journal-ref": null, "doi": "10.1016/j.media.2019.101552", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks have gained a lot of attention in the\ncomputer vision community due to their capability of data generation without\nexplicitly modelling the probability density function. The adversarial loss\nbrought by the discriminator provides a clever way of incorporating unlabeled\nsamples into training and imposing higher order consistency. This has proven to\nbe useful in many cases, such as domain adaptation, data augmentation, and\nimage-to-image translation. These properties have attracted researchers in the\nmedical imaging community, and we have seen rapid adoption in many traditional\nand novel applications, such as image reconstruction, segmentation, detection,\nclassification, and cross-modality synthesis. Based on our observations, this\ntrend will continue and we therefore conducted a review of recent advances in\nmedical imaging using the adversarial training scheme with the hope of\nbenefiting researchers interested in this technique.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 16:44:36 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 16:17:34 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 20:29:29 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 01:01:58 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Yi", "Xin", ""], ["Walia", "Ekta", ""], ["Babyn", "Paul", ""]]}, {"id": "1809.07316", "submitter": "Aljo\\v{s}a O\\v{s}ep", "authors": "Aljosa Osep, Paul Voigtlaender, Jonathon Luiten, Stefan Breuers,\n  Bastian Leibe", "title": "Towards Large-Scale Video Video Object Mining", "comments": "4 pages, 3 figures, 1 table. ECCV 2018 Workshop on Interactive and\n  Adaptive Learning in an Open World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to leverage a generic object tracker in order to perform object\nmining in large-scale unlabeled videos, captured in a realistic automotive\nsetting. We present a dataset of more than 360'000 automatically mined object\ntracks from 10+ hours of video data (560'000 frames) and propose a method for\nautomated novel category discovery and detector learning. In addition, we show\npreliminary results on using the mined tracks for object detector adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 17:49:35 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Osep", "Aljosa", ""], ["Voigtlaender", "Paul", ""], ["Luiten", "Jonathon", ""], ["Breuers", "Stefan", ""], ["Leibe", "Bastian", ""]]}, {"id": "1809.07357", "submitter": "Aljo\\v{s}a O\\v{s}ep", "authors": "Aljosa Osep, Wolfgang Mehner, Markus Mathias, Bastian Leibe", "title": "Combined Image- and World-Space Tracking in Traffic Scenes", "comments": "8 pages, 7 figures, 2 tables. ICRA 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking in urban street scenes plays a central role in autonomous systems\nsuch as self-driving cars. Most of the current vision-based tracking methods\nperform tracking in the image domain. Other approaches, eg based on LIDAR and\nradar, track purely in 3D. While some vision-based tracking methods invoke 3D\ninformation in parts of their pipeline, and some 3D-based methods utilize\nimage-based information in components of their approach, we propose to use\nimage- and world-space information jointly throughout our method. We present\nour tracking pipeline as a 3D extension of image-based tracking. From enhancing\nthe detections with 3D measurements to the reported positions of every tracked\nobject, we use world-space 3D information at every stage of processing. We\naccomplish this by our novel coupled 2D-3D Kalman filter, combined with a\nconceptually clean and extendable hypothesize-and-select framework. Our\napproach matches the current state-of-the-art on the official KITTI benchmark,\nwhich performs evaluation in the 2D image domain only. Further experiments show\nsignificant improvements in 3D localization precision by enabling our coupled\n2D-3D tracking.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 18:16:42 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Osep", "Aljosa", ""], ["Mehner", "Wolfgang", ""], ["Mathias", "Markus", ""], ["Leibe", "Bastian", ""]]}, {"id": "1809.07399", "submitter": "Rongjie Lai", "authors": "Stefan C. Schonsheck, Michael M. Bronstein, Rongjie Lai", "title": "Nonisometric Surface Registration via Conformal Laplace-Beltrami Basis\n  Pursuit", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface registration is one of the most fundamental problems in geometry\nprocessing. Many approaches have been developed to tackle this problem in cases\nwhere the surfaces are nearly isometric. However, it is much more challenging\nto compute correspondence between surfaces which are intrinsically less\nsimilar. In this paper, we propose a variational model to align the\nLaplace-Beltrami (LB) eigensytems of two non-isometric genus zero shapes via\nconformal deformations. This method enables us compute to geometric meaningful\npoint-to-point maps between non-isometric shapes. Our model is based on a novel\nbasis pursuit scheme whereby we simultaneously compute a conformal deformation\nof a 'target shape' and its deformed LB eigensytem. We solve the model using an\nproximal alternating minimization algorithm hybridized with the augmented\nLagrangian method which produces accurate correspondences given only a few\nlandmark points. We also propose a reinitialization scheme to overcome some of\nthe difficulties caused by the non-convexity of the variational problem.\nIntensive numerical experiments illustrate the effectiveness and robustness of\nthe proposed method to handle non-isometric surfaces with large deformation\nwith respect to both noise on the underlying manifolds and errors within the\ngiven landmarks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:30:24 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Schonsheck", "Stefan C.", ""], ["Bronstein", "Michael M.", ""], ["Lai", "Rongjie", ""]]}, {"id": "1809.07405", "submitter": "Tom Hanika", "authors": "Bastian Sch\\\"afermeier and Tom Hanika and Gerd Stumme", "title": "Distances for WiFi Based Topological Indoor Mapping", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": "10.1145/3360774.3360780", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For localization and mapping of indoor environments through WiFi signals,\nlocations are often represented as likelihoods of the received signal strength\nindicator. In this work we compare various measures of distance between such\nlikelihoods in combination with different methods for estimation and\nrepresentation. In particular, we show that among the considered distance\nmeasures the Earth Mover's Distance seems the most beneficial for the\nlocalization task. Combined with kernel density estimation we were able to\nretain the topological structure of rooms in a real-world office scenario.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:45:59 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Sch\u00e4fermeier", "Bastian", ""], ["Hanika", "Tom", ""], ["Stumme", "Gerd", ""]]}, {"id": "1809.07408", "submitter": "Yu Yao", "authors": "Yu Yao, Mingze Xu, Chiho Choi, David J. Crandall, Ella M. Atkins, and\n  Behzad Dariush", "title": "Egocentric Vision-based Future Vehicle Localization for Intelligent\n  Driving Assistance Systems", "comments": "To appear on ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future location of vehicles is essential for safety-critical\napplications such as advanced driver assistance systems (ADAS) and autonomous\ndriving. This paper introduces a novel approach to simultaneously predict both\nthe location and scale of target vehicles in the first-person (egocentric) view\nof an ego-vehicle. We present a multi-stream recurrent neural network (RNN)\nencoder-decoder model that separately captures both object location and scale\nand pixel-level observations for future vehicle localization. We show that\nincorporating dense optical flow improves prediction results significantly\nsince it captures information about motion as well as appearance change. We\nalso find that explicitly modeling future motion of the ego-vehicle improves\nthe prediction accuracy, which could be especially beneficial in intelligent\nand automated vehicles that have motion planning capability. To evaluate the\nperformance of our approach, we present a new dataset of first-person videos\ncollected from a variety of scenarios at road intersections, which are\nparticularly challenging moments for prediction because vehicle trajectories\nare diverse and dynamic.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:58:04 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 15:24:57 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Yao", "Yu", ""], ["Xu", "Mingze", ""], ["Choi", "Chiho", ""], ["Crandall", "David J.", ""], ["Atkins", "Ella M.", ""], ["Dariush", "Behzad", ""]]}, {"id": "1809.07417", "submitter": "Li Yi", "authors": "Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su,\n  Leonidas Guibas", "title": "Deep Part Induction from Articulated Object Pairs", "comments": null, "journal-ref": null, "doi": "10.1145/3272127.3275027", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object functionality is often expressed through part articulation -- as when\nthe two rigid parts of a scissor pivot against each other to perform the\ncutting function. Such articulations are often similar across objects within\nthe same functional category. In this paper, we explore how the observation of\ndifferent articulation states provides evidence for part structure and motion\nof 3D objects. Our method takes as input a pair of unsegmented shapes\nrepresenting two different articulation states of two functionally related\nobjects, and induces their common parts along with their underlying rigid\nmotion. This is a challenging setting, as we assume no prior shape structure,\nno prior shape category information, no consistent shape orientation, the\narticulation states may belong to objects of different geometry, plus we allow\ninputs to be noisy and partial scans, or point clouds lifted from RGB images.\nOur method learns a neural network architecture with three modules that\nrespectively propose correspondences, estimate 3D deformation flows, and\nperform segmentation. To achieve optimal performance, our architecture\nalternates between correspondence, deformation flow, and segmentation\nprediction iteratively in an ICP-like fashion. Our results demonstrate that our\nmethod significantly outperforms state-of-the-art techniques in the task of\ndiscovering articulated parts of objects. In addition, our part induction is\nobject-class agnostic and successfully generalizes to new and unseen objects.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 22:07:18 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Yi", "Li", ""], ["Huang", "Haibin", ""], ["Liu", "Difan", ""], ["Kalogerakis", "Evangelos", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1809.07436", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Yiheng Pan, Zexian Zeng, Liang Yao, Yuan Luo", "title": "Deep Generative Classifiers for Thoracic Disease Diagnosis with Chest\n  X-ray Images", "comments": "BIBM 2018 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thoracic diseases are very serious health problems that plague a large number\nof people. Chest X-ray is currently one of the most popular methods to diagnose\nthoracic diseases, playing an important role in the healthcare workflow.\nHowever, reading the chest X-ray images and giving an accurate diagnosis remain\nchallenging tasks for expert radiologists. With the success of deep learning in\ncomputer vision, a growing number of deep neural network architectures were\napplied to chest X-ray image classification. However, most of the previous deep\nneural network classifiers were based on deterministic architectures which are\nusually very noise-sensitive and are likely to aggravate the overfitting issue.\nIn this paper, to make a deep architecture more robust to noise and to reduce\noverfitting, we propose using deep generative classifiers to automatically\ndiagnose thorax diseases from the chest X-ray images. Unlike the traditional\ndeterministic classifier, a deep generative classifier has a distribution\nmiddle layer in the deep neural network. A sampling layer then draws a random\nsample from the distribution layer and input it to the following layer for\nclassification. The classifier is generative because the class label is\ngenerated from samples of a related distribution. Through training the model\nwith a certain amount of randomness, the deep generative classifiers are\nexpected to be robust to noise and can reduce overfitting and then achieve good\nperformances. We implemented our deep generative classifiers based on a number\nof well-known deterministic neural network architectures, and tested our models\non the chest X-ray14 dataset. The results demonstrated the superiority of deep\ngenerative classifiers compared with the corresponding deep deterministic\nclassifiers.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 00:13:50 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 18:41:48 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Mao", "Chengsheng", ""], ["Pan", "Yiheng", ""], ["Zeng", "Zexian", ""], ["Yao", "Liang", ""], ["Luo", "Yuan", ""]]}, {"id": "1809.07447", "submitter": "Peipei Li", "authors": "Peipei Li, Yibo Hu, Ran He and Zhenan Sun", "title": "A Coupled Evolutionary Network for Age Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age estimation of unknown persons is a challenging pattern analysis task due\nto the lacking of training data and various aging mechanisms for different\npeople. Label distribution learning-based methods usually make distribution\nassumptions to simplify age estimation. However, age label distributions are\noften complex and difficult to be modeled in a parameter way. Inspired by the\nbiological evolutionary mechanism, we propose a Coupled Evolutionary Network\n(CEN) with two concurrent evolutionary processes: evolutionary label\ndistribution learning and evolutionary slack regression. Evolutionary network\nlearns and refines age label distributions in an iteratively learning way.\nEvolutionary label distribution learning adaptively learns and constantly\nrefines the age label distributions without making strong assumptions on the\ndistribution patterns. To further utilize the ordered and continuous\ninformation of age labels, we accordingly propose an evolutionary slack\nregression to convert the discrete age label regression into the continuous age\ninterval regression. Experimental results on Morph, ChaLearn15 and\nMegaAge-Asian datasets show the superiority of our method.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 01:55:56 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Li", "Peipei", ""], ["Hu", "Yibo", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1809.07456", "submitter": "Bo Jiang", "authors": "Bo Jiang", "title": "Towards Discrete Solution: A Sparse Preserving Method for Correspondence\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems of interest in computer vision can be formulated as a problem\nof finding consistent correspondences between two feature sets. Feature\ncorrespondence (matching) problem with one-to-one mapping constraint is usually\nformulated as an Integral Quadratic Programming (IQP) problem with permutation\n(or orthogonal) constraint. Since it is NP-hard, relaxation models are\nrequired. One main challenge for optimizing IQP matching problem is how to\nincorporate the discrete one-to-one mapping (permutation) constraint in its\nquadratic objective optimization. In this paper, we present a new relaxation\nmodel, called Sparse Constraint Preserving Matching (SPM), for IQP matching\nproblem. SPM is motivated by our observation that the discrete permutation\nconstraint can be well encoded via a sparse constraint. Comparing with\ntraditional relaxation models, SPM can incorporate the discrete one-to-one\nmapping constraint straightly via a sparse constraint and thus provides a\ntighter relaxation for original IQP matching problem. A simple yet effective\nupdate algorithm has been derived to solve the proposed SPM model. Experimental\nresults on several feature matching tasks demonstrate the effectiveness and\nefficiency of SPM method.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 02:46:40 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Jiang", "Bo", ""]]}, {"id": "1809.07491", "submitter": "Changhao Chen", "authors": "Changhao Chen, Peijun Zhao, Chris Xiaoxuan Lu, Wei Wang, Andrew\n  Markham, Niki Trigoni", "title": "OxIOD: The Dataset for Deep Inertial Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in micro-electro-mechanical (MEMS) techniques enable inertial\nmeasurements units (IMUs) to be small, cheap, energy efficient, and widely used\nin smartphones, robots, and drones. Exploiting inertial data for accurate and\nreliable navigation and localization has attracted significant research and\nindustrial interest, as IMU measurements are completely ego-centric and\ngenerally environment agnostic. Recent studies have shown that the notorious\nissue of drift can be significantly alleviated by using deep neural networks\n(DNNs), e.g. IONet. However, the lack of sufficient labelled data for training\nand testing various architectures limits the proliferation of adopting DNNs in\nIMU-based tasks. In this paper, we propose and release the Oxford Inertial\nOdometry Dataset (OxIOD), a first-of-its-kind data collection for\ninertial-odometry research, with all sequences having ground-truth labels. Our\ndataset contains 158 sequences totalling more than 42 km in total distance,\nmuch larger than previous inertial datasets. Another notable feature of this\ndataset lies in its diversity, which can reflect the complex motions of\nphone-based IMUs in various everyday usage. The measurements were collected\nwith four different attachments (handheld, in the pocket, in the handbag and on\nthe trolley), four motion modes (halting, walking slowly, walking normally, and\nrunning), five different users, four types of off-the-shelf consumer phones,\nand large-scale localization from office buildings. Deep inertial tracking\nexperiments were conducted to show the effectiveness of our dataset in training\ndeep neural network models and evaluate learning-based and model-based\nalgorithms. The OxIOD Dataset is available at: http://deepio.cs.ox.ac.uk\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 06:48:37 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Chen", "Changhao", ""], ["Zhao", "Peijun", ""], ["Lu", "Chris Xiaoxuan", ""], ["Wang", "Wei", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1809.07494", "submitter": "Ayush Dewan", "authors": "Ayush Dewan, Tim Caselitz and Wolfram Burgard", "title": "Learning a Local Feature Descriptor for 3D LiDAR Scans", "comments": "Accepted for IROS-2018. Project details and code:\n  http://deep3d-descriptor.informatik.uni-freiburg.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust data association is necessary for virtually every SLAM system and\nfinding corresponding points is typically a preprocessing step for scan\nalignment algorithms. Traditionally, handcrafted feature descriptors were used\nfor these problems but recently learned descriptors have been shown to perform\nmore robustly. In this work, we propose a local feature descriptor for 3D LiDAR\nscans. The descriptor is learned using a Convolutional Neural Network (CNN).\nOur proposed architecture consists of a Siamese network for learning a feature\ndescriptor and a metric learning network for matching the descriptors. We also\npresent a method for estimating local surface patches and obtaining\nground-truth correspondences. In extensive experiments, we compare our learned\nfeature descriptor with existing 3D local descriptors and report highly\ncompetitive results for multiple experiments in terms of matching accuracy and\ncomputation time. \\end{abstract}\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 06:52:16 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Dewan", "Ayush", ""], ["Caselitz", "Tim", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1809.07499", "submitter": "Joseph K J", "authors": "K J Joseph and Vineeth N Balasubramanian", "title": "MASON: A Model AgnoStic ObjectNess Framework", "comments": "Accepted at AutoNUE Workshop, 15th European Conference on Computer\n  Vision (ECCV), September 2018, Munich, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a simple, yet very effective method to localize dominant\nforeground objects in an image, to pixel-level precision. The proposed method\n'MASON' (Model-AgnoStic ObjectNess) uses a deep convolutional network to\ngenerate category-independent and model-agnostic heat maps for any image. The\nnetwork is not explicitly trained for the task, and hence, can be used\noff-the-shelf in tandem with any other network or task. We show that this\nframework scales to a wide variety of images, and illustrate the effectiveness\nof MASON in three varied application contexts.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 07:08:38 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Joseph", "K J", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1809.07517", "submitter": "Roey Mechrez", "authors": "Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, Lihi\n  Zelnik-Manor", "title": "The 2018 PIRM Challenge on Perceptual Image Super-resolution", "comments": "Workshop and Challenge on Perceptual Image Restoration and\n  Manipulation in conjunction with ECCV 2018 webpage: https://www.pirm2018.org/", "journal-ref": "Proceedings of the European Conference on Computer Vision (ECCV)\n  Workshops, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on the 2018 PIRM challenge on perceptual super-resolution\n(SR), held in conjunction with the Perceptual Image Restoration and\nManipulation (PIRM) workshop at ECCV 2018. In contrast to previous SR\nchallenges, our evaluation methodology jointly quantifies accuracy and\nperceptual quality, therefore enabling perceptual-driven methods to compete\nalongside algorithms that target PSNR maximization. Twenty-one participating\nteams introduced algorithms which well-improved upon the existing\nstate-of-the-art methods in perceptual SR, as confirmed by a human opinion\nstudy. We also analyze popular image quality measures and draw conclusions\nregarding which of them correlates best with human opinion scores. We conclude\nwith an analysis of the current trends in perceptual SR, as reflected from the\nleading submissions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 08:00:42 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 08:05:51 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 07:44:49 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Blau", "Yochai", ""], ["Mechrez", "Roey", ""], ["Timofte", "Radu", ""], ["Michaeli", "Tomer", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1809.07539", "submitter": "Michael Gauding", "authors": "Michael Gauding, Lipo Wang, Jens Henrik Goebbert, Mathis Bode,\n  Luminita Danaila, Emilien Varea", "title": "On the self-similarity of line segments in decaying homogeneous\n  isotropic turbulence", "comments": null, "journal-ref": null, "doi": "10.1016/j.compfluid.2018.08.001", "report-no": null, "categories": "physics.flu-dyn cs.CV nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-similarity of a passive scalar in homogeneous isotropic decaying\nturbulence is investigated by the method of line segments (M. Gauding et al.,\nPhysics of Fluids 27.9 (2015): 095102). The analysis is based on a highly\nresolved direct numerical simulation of decaying turbulence. The method of line\nsegments is used to perform a decomposition of the scalar field into smaller\nsub-units based on the extremal points of the scalar along a straight line.\nThese sub-units (the so-called line segments) are parameterized by their length\n$\\ell$ and the difference $\\Delta\\phi$ of the scalar field between the ending\npoints. Line segments can be understood as thin local convective-diffusive\nstructures in which diffusive processes are enhanced by compressive strain.\nFrom DNS, it is shown that the marginal distribution function of the\nlength~$\\ell$ assumes complete self-similarity when re-scaled by the mean\nlength $\\ell_m$. The joint statistics of $\\Delta\\phi$ and $\\ell$, from which\nthe local gradient $g=\\Delta\\phi/\\ell$ can be defined, play an important role\nin understanding the turbulence mixing and flow structure. Large values of $g$\noccur at a small but finite length scale. Statistics of $g$ are characterized\nby rare but strong deviations that exceed the standard deviation by more than\none order of magnitude. It is shown that these events break complete\nself-similarity of line segments, which confirms the standard paradigm of\nturbulence that intense events (which are known as internal intermittency) are\nnot self-similar.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 09:07:29 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Gauding", "Michael", ""], ["Wang", "Lipo", ""], ["Goebbert", "Jens Henrik", ""], ["Bode", "Mathis", ""], ["Danaila", "Luminita", ""], ["Varea", "Emilien", ""]]}, {"id": "1809.07558", "submitter": "Irtiza Hasan", "authors": "Theodore Tsesmelis, Irtiza Hasan, Marco Cristani, Fabio Galasso,\n  Alessio Del Bue", "title": "RGBD2lux: Dense light intensity estimation with an RGBD sensor", "comments": "10 pages, 9 figures, this manuscript is accepted in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lighting design and modelling or industrial applications like luminaire\nplanning and commissioning rely heavily on time consuming manual measurements\nor on physically coherent computational simulations. Regarding the\nlatter,standard approaches are based on CAD modeling simulations and offline\nrendering, with long processing times and therefore inflexible workflows. Thus,\nin this paper we pro-pose a computer vision based system to measure lighting\nwith just a single RGBD camera. The proposed method uses both depth data and\nimages from the sensor to provide a dense measure of light intensity in the\nfield of view of the camera. We evaluate our system on novel ground truth data\nand compare it to state-of-the-art commercial light-planning software. Our\nsystem provides improved performance, while being completely automated, given\nthat the CAD model is extracted from the depth and the albedo estimated with\nthe support of RGB images. To the best of our knowledge, this is the first\nautomatic framework for the estimation of lighting in general indoor scenarios\nfrom RGBDinput.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 10:30:09 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 12:34:02 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 16:47:08 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Tsesmelis", "Theodore", ""], ["Hasan", "Irtiza", ""], ["Cristani", "Marco", ""], ["Galasso", "Fabio", ""], ["Del Bue", "Alessio", ""]]}, {"id": "1809.07586", "submitter": "Ankan Bansal", "authors": "Rajeev Ranjan, Ankan Bansal, Jingxiao Zheng, Hongyu Xu, Joshua\n  Gleason, Boyu Lu, Anirudh Nanduri, Jun-Cheng Chen, Carlos D. Castillo, Rama\n  Chellappa", "title": "A Fast and Accurate System for Face Detection, Identification, and\n  Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large annotated datasets and affordable computation power\nhave led to impressive improvements in the performance of CNNs on various\nobject detection and recognition benchmarks. These, along with a better\nunderstanding of deep learning methods, have also led to improved capabilities\nof machine understanding of faces. CNNs are able to detect faces, locate facial\nlandmarks, estimate pose, and recognize faces in unconstrained images and\nvideos. In this paper, we describe the details of a deep learning pipeline for\nunconstrained face identification and verification which achieves\nstate-of-the-art performance on several benchmark datasets. We propose a novel\nface detector, Deep Pyramid Single Shot Face Detector (DPSSD), which is fast\nand capable of detecting faces with large scale variations (especially tiny\nfaces). We give design details of the various modules involved in automatic\nface recognition: face detection, landmark localization and alignment, and face\nidentification/verification. We provide evaluation results of the proposed face\ndetector on challenging unconstrained face detection datasets. Then, we present\nexperimental results for IARPA Janus Benchmarks A, B and C (IJB-A, IJB-B,\nIJB-C), and the Janus Challenge Set 5 (CS5).\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 12:14:20 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Ranjan", "Rajeev", ""], ["Bansal", "Ankan", ""], ["Zheng", "Jingxiao", ""], ["Xu", "Hongyu", ""], ["Gleason", "Joshua", ""], ["Lu", "Boyu", ""], ["Nanduri", "Anirudh", ""], ["Chen", "Jun-Cheng", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1809.07589", "submitter": "Roberto Interdonato", "authors": "Roberto Interdonato, Dino Ienco, Raffaele Gaetano, Kenji Ose", "title": "DuPLO: A DUal view Point deep Learning architecture for time series\n  classificatiOn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, modern Earth Observation systems continuously generate huge amounts\nof data. A notable example is represented by the Sentinel-2 mission, which\nprovides images at high spatial resolution (up to 10m) with high temporal\nrevisit period (every 5 days), which can be organized in Satellite Image Time\nSeries (SITS). While the use of SITS has been proved to be beneficial in the\ncontext of Land Use/Land Cover (LULC) map generation, unfortunately, machine\nlearning approaches commonly leveraged in remote sensing field fail to take\nadvantage of spatio-temporal dependencies present in such data.\n  Recently, new generation deep learning methods allowed to significantly\nadvance research in this field. These approaches have generally focused on a\nsingle type of neural network, i.e., Convolutional Neural Networks (CNNs) or\nRecurrent Neural Networks (RNNs), which model different but complementary\ninformation: spatial autocorrelation (CNNs) and temporal dependencies (RNNs).\nIn this work, we propose the first deep learning architecture for the analysis\nof SITS data, namely \\method{} (DUal view Point deep Learning architecture for\ntime series classificatiOn), that combines Convolutional and Recurrent neural\nnetworks to exploit their complementarity. Our hypothesis is that, since CNNs\nand RNNs capture different aspects of the data, a combination of both models\nwould produce a more diverse and complete representation of the information for\nthe underlying land cover classification task. Experiments carried out on two\nstudy sites characterized by different land cover characteristics (i.e., the\n\\textit{Gard} site in France and the \\textit{Reunion Island} in the Indian\nOcean), demonstrate the significance of our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 12:19:35 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Interdonato", "Roberto", ""], ["Ienco", "Dino", ""], ["Gaetano", "Raffaele", ""], ["Ose", "Kenji", ""]]}, {"id": "1809.07628", "submitter": "Jean Ogier du Terrail", "authors": "Jean Ogier du Terrail and Fr\\'ed\\'eric Jurie", "title": "Faster RER-CNN: application to the detection of vehicles in aerial\n  images", "comments": "technical report v2 fixes formatting issues and add acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting small vehicles in aerial images is a difficult job that can be\nchallenging even for humans. Rotating objects, low resolution, small\ninter-class variability and very large images comprising complicated\nbackgrounds render the work of photo-interpreters tedious and wearisome.\nUnfortunately even the best classical detection pipelines like Faster R-CNN\ncannot be used off-the-shelf with good results because they were built to\nprocess object centric images from day-to-day life with multi-scale vertical\nobjects. In this work we build on the Faster R-CNN approach to turn it into a\ndetection framework that deals appropriately with the rotation equivariance\ninherent to any aerial image task. This new pipeline (Faster Rotation\nEquivariant Regions CNN) gives, without any bells and whistles,\nstate-of-the-art results on one of the most challenging aerial imagery\ndatasets: VeDAI and give good results w.r.t. the baseline Faster R-CNN on two\nothers: Munich and GoogleEarth .\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 13:56:20 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 10:08:37 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Terrail", "Jean Ogier du", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1809.07677", "submitter": "Shreyas Skandan Shivakumar", "authors": "Shreyas S. Shivakumar, Kartik Mohta, Bernd Pfrommer, Vijay Kumar and\n  Camillo J. Taylor", "title": "Real Time Dense Depth Estimation by Fusing Stereo with Sparse Depth\n  Measurements", "comments": "7 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to depth estimation that fuses information from a\nstereo pair with sparse range measurements derived from a LIDAR sensor or a\nrange camera. The goal of this work is to exploit the complementary strengths\nof the two sensor modalities, the accurate but sparse range measurements and\nthe ambiguous but dense stereo information. These two sources are effectively\nand efficiently fused by combining ideas from anisotropic diffusion and\nsemi-global matching.\n  We evaluate our approach on the KITTI 2015 and Middlebury 2014 datasets,\nusing randomly sampled ground truth range measurements as our sparse depth\ninput. We achieve significant performance improvements with a small fraction of\nrange measurements on both datasets. We also provide qualitative results from\nour platform using the PMDTec Monstar sensor. Our entire pipeline runs on an\nNVIDIA TX-2 platform at 5Hz on 1280x1024 stereo images with 128 disparity\nlevels.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 15:39:49 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Shivakumar", "Shreyas S.", ""], ["Mohta", "Kartik", ""], ["Pfrommer", "Bernd", ""], ["Kumar", "Vijay", ""], ["Taylor", "Camillo J.", ""]]}, {"id": "1809.07748", "submitter": "Shing Chan", "authors": "Shing Chan and Ahmed H. Elsheikh", "title": "Exemplar-based synthesis of geology using kernel discrepancies and\n  generative neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for synthesis of geological images based on an\nexemplar image. We synthesize new realizations such that the discrepancy in the\npatch distribution between the realizations and the exemplar image is\nminimized. Such discrepancy is quantified using a kernel method for two-sample\ntest called maximum mean discrepancy. To enable fast synthesis, we train a\ngenerative neural network in an offline phase to sample realizations\nefficiently during deployment, while also providing a parametrization of the\nsynthesis process. We assess the framework on a classical binary image\nrepresenting channelized subsurface reservoirs, finding that the method\nreproduces the visual patterns and spatial statistics (image histogram and\ntwo-point probability functions) of the exemplar image.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 17:33:20 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 09:31:45 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Chan", "Shing", ""], ["Elsheikh", "Ahmed H.", ""]]}, {"id": "1809.07759", "submitter": "Carlo Rapisarda", "authors": "Mart Karta\\v{s}ev, Carlo Rapisarda, Dominik Fay", "title": "Implementing Adaptive Separable Convolution for Video Frame\n  Interpolation", "comments": "All authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Deep Neural Networks are becoming more popular, much of the attention is\nbeing devoted to Computer Vision problems that used to be solved with more\ntraditional approaches. Video frame interpolation is one of such challenges\nthat has seen new research involving various techniques in deep learning. In\nthis paper, we replicate the work of Niklaus et al. on Adaptive Separable\nConvolution, which claims high quality results on the video frame interpolation\ntask. We apply the same network structure trained on a smaller dataset and\nexperiment with various different loss functions, in order to determine the\noptimal approach in data-scarce scenarios. The best resulting model is still\nable to provide visually pleasing videos, although achieving lower evaluation\nscores.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 17:48:27 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Karta\u0161ev", "Mart", ""], ["Rapisarda", "Carlo", ""], ["Fay", "Dominik", ""]]}, {"id": "1809.07764", "submitter": "Peipei Li", "authors": "Peipei Li, Yibo Hu, Ran He and Zhenan Sun", "title": "Global and Local Consistent Wavelet-domain Age Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age synthesis is a challenging task due to the complicated and non-linear\ntransformation in human aging process. Aging information is usually reflected\nin local facial parts, such as wrinkles at the eye corners. However, these\nlocal facial parts contribute less in previous GAN based methods for age\nsynthesis. To address this issue, we propose a Wavelet-domain Global and Local\nConsistent Age Generative Adversarial Network (WaveletGLCA-GAN), in which one\nglobal specific network and three local specific networks are integrated\ntogether to capture both global topology information and local texture details\nof human faces. Different from the most existing methods that modeling age\nsynthesis in image-domain, we adopt wavelet transform to depict the textual\ninformation in frequency-domain. %Moreover, to achieve accurate age generation\nunder the premise of preserving the identity information, age estimation\nnetwork and face verification network are employed. Moreover, five types of\nlosses are adopted: 1) adversarial loss aims to generate realistic wavelets; 2)\nidentity preserving loss aims to better preserve identity information; 3) age\npreserving loss aims to enhance the accuracy of age synthesis; 4) pixel-wise\nloss aims to preserve the background information of the input face; 5) the\ntotal variation regularization aims to remove ghosting artifacts. Our method is\nevaluated on three face aging datasets, including CACD2000, Morph and FG-NET.\nQualitative and quantitative experiments show the superiority of the proposed\nmethod over other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 02:44:58 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 01:10:24 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Li", "Peipei", ""], ["Hu", "Yibo", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1809.07786", "submitter": "Shadrokh Samavi", "authors": "Zahra Sobhaninia, Safiyeh Rezaei, Alireza Noroozi, Mehdi Ahmadi,\n  Hamidreza Zarrabi, Nader Karimi, Ali Emami, Shadrokh Samavi", "title": "Brain Tumor Segmentation Using Deep Learning by Type Specific Sorting of\n  Images", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep learning has been playing a major role in the field of computer\nvision. One of its applications is the reduction of human judgment in the\ndiagnosis of diseases. Especially, brain tumor diagnosis requires high\naccuracy, where minute errors in judgment may lead to disaster. For this\nreason, brain tumor segmentation is an important challenge for medical\npurposes. Currently several methods exist for tumor segmentation but they all\nlack high accuracy. Here we present a solution for brain tumor segmenting by\nusing deep learning. In this work, we studied different angles of brain MR\nimages and applied different networks for segmentation. The effect of using\nseparate networks for segmentation of MR images is evaluated by comparing the\nresults with a single network. Experimental evaluations of the networks show\nthat Dice score of 0.73 is achieved for a single network and 0.79 in obtained\nfor multiple networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 18:24:09 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Sobhaninia", "Zahra", ""], ["Rezaei", "Safiyeh", ""], ["Noroozi", "Alireza", ""], ["Ahmadi", "Mehdi", ""], ["Zarrabi", "Hamidreza", ""], ["Karimi", "Nader", ""], ["Emami", "Ali", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1809.07802", "submitter": "Mateusz Malinowski", "authors": "Julien Perolat and Mateusz Malinowski and Bilal Piot and Olivier\n  Pietquin", "title": "Playing the Game of Universal Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning classifiers robust to universal adversarial\nperturbations. While prior work approaches this problem via robust\noptimization, adversarial training, or input transformation, we instead phrase\nit as a two-player zero-sum game. In this new formulation, both players\nsimultaneously play the same game, where one player chooses a classifier that\nminimizes a classification loss whilst the other player creates an adversarial\nperturbation that increases the same loss when applied to every sample in the\ntraining set. By observing that performing a classification (respectively\ncreating adversarial samples) is the best response to the other player, we\npropose a novel extension of a game-theoretic algorithm, namely fictitious\nplay, to the domain of training robust classifiers. Finally, we empirically\nshow the robustness and versatility of our approach in two defence scenarios\nwhere universal attacks are performed on several image classification datasets\n-- CIFAR10, CIFAR100 and ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 18:48:36 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 20:16:45 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Perolat", "Julien", ""], ["Malinowski", "Mateusz", ""], ["Piot", "Bilal", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1809.07845", "submitter": "Heng Fan", "authors": "Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin\n  Bai, Yong Xu, Chunyuan Liao, Haibin Ling", "title": "LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking", "comments": "18 pages, including supplementary material, adding minor revisions\n  and correcting typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present LaSOT, a high-quality benchmark for Large-scale\nSingle Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M\nframes in total. Each frame in these sequences is carefully and manually\nannotated with a bounding box, making LaSOT the largest, to the best of our\nknowledge, densely annotated tracking benchmark. The average video length of\nLaSOT is more than 2,500 frames, and each sequence comprises various challenges\nderiving from the wild where target objects may disappear and re-appear again\nin the view. By releasing LaSOT, we expect to provide the community with a\nlarge-scale dedicated benchmark with high quality for both the training of deep\ntrackers and the veritable evaluation of tracking algorithms. Moreover,\nconsidering the close connections of visual appearance and natural language, we\nenrich LaSOT by providing additional language specification, aiming at\nencouraging the exploration of natural linguistic feature for tracking. A\nthorough experimental evaluation of 35 tracking algorithms on LaSOT is\npresented with detailed analysis, and the results demonstrate that there is\nstill a big room for improvements.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 20:37:24 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 02:17:15 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Fan", "Heng", ""], ["Lin", "Liting", ""], ["Yang", "Fan", ""], ["Chu", "Peng", ""], ["Deng", "Ge", ""], ["Yu", "Sijia", ""], ["Bai", "Hexin", ""], ["Xu", "Yong", ""], ["Liao", "Chunyuan", ""], ["Ling", "Haibin", ""]]}, {"id": "1809.07895", "submitter": "Choongyeun Cho", "authors": "Choongyeun Cho, Benjamin Antin, Sanchit Arora, Shwan Ashrafi, Peilin\n  Duan, Dang The Huynh, Lee James, Hang Tuan Nguyen, Mojtaba Solgi, Cuong Van\n  Than", "title": "Large-Scale Video Classification with Feature Space Augmentation coupled\n  with Learned Label Relations and Ensembling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Axon AI's solution to the 2nd YouTube-8M Video\nUnderstanding Challenge, achieving the final global average precision (GAP) of\n88.733% on the private test set (ranked 3rd among 394 teams, not considering\nthe model size constraint), and 87.287% using a model that meets size\nrequirement. Two sets of 7 individual models belonging to 3 different families\nwere trained separately. Then, the inference results on a training data were\naggregated from these multiple models and fed to train a compact model that\nmeets the model size requirement. In order to further improve performance we\nexplored and employed data over/sub-sampling in feature space, an additional\nregularization term during training exploiting label relationship, and learned\nweights for ensembling different individual models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 00:10:18 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Cho", "Choongyeun", ""], ["Antin", "Benjamin", ""], ["Arora", "Sanchit", ""], ["Ashrafi", "Shwan", ""], ["Duan", "Peilin", ""], ["Huynh", "Dang The", ""], ["James", "Lee", ""], ["Nguyen", "Hang Tuan", ""], ["Solgi", "Mojtaba", ""], ["Van Than", "Cuong", ""]]}, {"id": "1809.07917", "submitter": "Peng-Shuai Wang", "authors": "Peng-Shuai Wang and Chun-Yu Sun and Yang Liu and Xin Tong", "title": "Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes", "comments": null, "journal-ref": "ACM Transactions on Graphics, 2018", "doi": "10.1145/3272127.3275050", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an Adaptive Octree-based Convolutional Neural Network (Adaptive\nO-CNN) for efficient 3D shape encoding and decoding. Different from\nvolumetric-based or octree-based CNN methods that represent a 3D shape with\nvoxels in the same resolution, our method represents a 3D shape adaptively with\noctants at different levels and models the 3D shape within each octant with a\nplanar patch. Based on this adaptive patch-based representation, we propose an\nAdaptive O-CNN encoder and decoder for encoding and decoding 3D shapes. The\nAdaptive O-CNN encoder takes the planar patch normal and displacement as input\nand performs 3D convolutions only at the octants at each level, while the\nAdaptive O-CNN decoder infers the shape occupancy and subdivision status of\noctants at each level and estimates the best plane normal and displacement for\neach leaf octant. As a general framework for 3D shape analysis and generation,\nthe Adaptive O-CNN not only reduces the memory and computational cost, but also\noffers better shape generation capability than the existing 3D-CNN approaches.\nWe validate Adaptive O-CNN in terms of efficiency and effectiveness on\ndifferent shape analysis and generation tasks, including shape classification,\n3D autoencoding, shape prediction from a single image, and shape completion for\nnoisy and incomplete point clouds.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 02:24:48 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Wang", "Peng-Shuai", ""], ["Sun", "Chun-Yu", ""], ["Liu", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "1809.07921", "submitter": "Yao Li", "authors": "Kun Zhou, Jinmiao Cai, Yao Li, Yulong Shi, Xiaoguang Han, Nianjuan\n  Jiang, Kui Jia, Jiangbo Lu", "title": "Adversarial 3D Human Pose Estimation via Multimodal Depth Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel deep-learning based framework is proposed to infer 3D\nhuman poses from a single image. Specifically, a two-phase approach is\ndeveloped. We firstly utilize a generator with two branches for the extraction\nof explicit and implicit depth information respectively. During the training\nprocess, an adversarial scheme is also employed to further improve the\nperformance. The implicit and explicit depth information with the estimated 2D\njoints generated by a widely used estimator, in the second step, are together\nfed into a deep 3D pose regressor for the final pose generation. Our method\nachieves MPJPE of 58.68mm on the ECCV2018 3D Human Pose Estimation Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 02:48:34 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Zhou", "Kun", ""], ["Cai", "Jinmiao", ""], ["Li", "Yao", ""], ["Shi", "Yulong", ""], ["Han", "Xiaoguang", ""], ["Jiang", "Nianjuan", ""], ["Jia", "Kui", ""], ["Lu", "Jiangbo", ""]]}, {"id": "1809.07941", "submitter": "Luca Caltagirone", "authors": "Luca Caltagirone, Mauro Bellone, Lennart Svensson, Mattias Wahde", "title": "LIDAR-Camera Fusion for Road Detection Using Fully Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a deep learning approach has been developed to carry out road\ndetection by fusing LIDAR point clouds and camera images. An unstructured and\nsparse point cloud is first projected onto the camera image plane and then\nupsampled to obtain a set of dense 2D images encoding spatial information.\nSeveral fully convolutional neural networks (FCNs) are then trained to carry\nout road detection, either by using data from a single sensor, or by using\nthree fusion strategies: early, late, and the newly proposed cross fusion.\nWhereas in the former two fusion approaches, the integration of multimodal\ninformation is carried out at a predefined depth level, the cross fusion FCN is\ndesigned to directly learn from data where to integrate information; this is\naccomplished by using trainable cross connections between the LIDAR and the\ncamera processing branches.\n  To further highlight the benefits of using a multimodal system for road\ndetection, a data set consisting of visually challenging scenes was extracted\nfrom driving sequences of the KITTI raw data set. It was then demonstrated\nthat, as expected, a purely camera-based FCN severely underperforms on this\ndata set. A multimodal system, on the other hand, is still able to provide high\naccuracy. Finally, the proposed cross fusion FCN was evaluated on the KITTI\nroad benchmark where it achieved excellent performance, with a MaxF score of\n96.03%, ranking it among the top-performing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 04:31:40 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Caltagirone", "Luca", ""], ["Bellone", "Mauro", ""], ["Svensson", "Lennart", ""], ["Wahde", "Mattias", ""]]}, {"id": "1809.07977", "submitter": "Konstantin Schauwecker", "authors": "Konstantin Schauwecker", "title": "Real-Time Stereo Vision on FPGAs with SceneScan", "comments": "12 pages, 3 figures; accepted for publication at Forum\n  Bildverarbeitung 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a flexible FPGA stereo vision implementation that is capable of\nprocessing up to 100 frames per second or image resolutions up to 3.4\nmegapixels, while consuming only 8 W of power. The implementation uses a\nvariation of the Semi-Global Matching (SGM) algorithm, which provides superior\nresults compared to many simpler approaches. The stereo matching results are\nimproved significantly through a post-processing chain that operates on the\ncomputed cost cube and the disparity map. With this implementation we have\ncreated two stand-alone hardware systems for stereo vision, called SceneScan\nand SceneScan Pro. Both systems have been developed to market maturity and are\navailable from Nerian Vision GmbH.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 08:16:50 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Schauwecker", "Konstantin", ""]]}, {"id": "1809.07983", "submitter": "Fran\\c{c}ois Lauze", "authors": "Francois Lauze and Mads Nielsen", "title": "On Variational Methods for Motion Compensated Inpainting", "comments": "DIKU Technical report 2009 with some small corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop in this paper a generic Bayesian framework for the joint\nestimation of motion and recovery of missing data in a damaged video sequence.\nUsing standard maximum a posteriori to variational formulation rationale, we\nderive generic minimum energy formulations for the estimation of a\nreconstructed sequence as well as motion recovery. We instantiate these energy\nformulations and from their Euler-Lagrange Equations, we propose a full\nmultiresolution algorithms in order to compute good local minimizers for our\nenergies and discuss their numerical implementations, focusing on the missing\ndata recovery part, i.e. inpainting. Experimental results for synthetic as well\nas real sequences are presented. Image sequences and extra material is\navailable at http://image.diku.dk/francois/seqinp.php.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 08:26:05 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Lauze", "Francois", ""], ["Nielsen", "Mads", ""]]}, {"id": "1809.07987", "submitter": "Da Chen", "authors": "Da Chen, Jiong Zhang, Laurent D. Cohen", "title": "Minimal Paths for Tubular Structure Segmentation with Coherence Penalty\n  and Adaptive Anisotropy", "comments": "This manuscript has been accepted by IEEE Trans. Image Processing,\n  2018", "journal-ref": null, "doi": "10.1109/TIP.2018.2874282", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimal path method has proven to be particularly useful and efficient in\ntubular structure segmentation applications. In this paper, we propose a new\nminimal path model associated with a dynamic Riemannian metric embedded with an\nappearance feature coherence penalty and an adaptive anisotropy enhancement\nterm. The features that characterize the appearance and anisotropy properties\nof a tubular structure are extracted through the associated orientation score.\nThe proposed dynamic Riemannian metric is updated in the course of the geodesic\ndistance computation carried out by the efficient single-pass fast marching\nmethod. Compared to state-of-the-art minimal path models, the proposed minimal\npath model is able to extract the desired tubular structures from a complicated\nvessel tree structure. In addition, we propose an efficient prior path-based\nmethod to search for vessel radius value at each centerline position of the\ntarget. Finally, we perform the numerical experiments on both synthetic and\nreal images. The quantitive validation is carried out on retinal vessel images.\nThe results indicate that the proposed model indeed achieves a promising\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 08:35:59 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 10:03:13 GMT"}, {"version": "v3", "created": "Sun, 21 Oct 2018 07:23:35 GMT"}, {"version": "v4", "created": "Thu, 25 Oct 2018 15:13:20 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chen", "Da", ""], ["Zhang", "Jiong", ""], ["Cohen", "Laurent D.", ""]]}, {"id": "1809.07988", "submitter": "Ziqi Zhou", "authors": "Meijun Sun, Ziqi Zhou, QinGhua Hu, Zheng Wang, Jianmin Jiang", "title": "SG-FCN: A Motion and Memory-Based Deep Learning Model for Video Saliency\n  Detection", "comments": null, "journal-ref": "IEEE Transactions on Cybernetics ( Volume: PP, Issue: 99 ),2018", "doi": "10.1109/TCYB.2018.2832053", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven saliency detection has attracted strong interest as a result of\napplying convolutional neural networks to the detection of eye fixations.\nAlthough a number of imagebased salient object and fixation detection models\nhave been proposed, video fixation detection still requires more exploration.\nDifferent from image analysis, motion and temporal information is a crucial\nfactor affecting human attention when viewing video sequences. Although\nexisting models based on local contrast and low-level features have been\nextensively researched, they failed to simultaneously consider interframe\nmotion and temporal information across neighboring video frames, leading to\nunsatisfactory performance when handling complex scenes. To this end, we\npropose a novel and efficient video eye fixation detection model to improve the\nsaliency detection performance. By simulating the memory mechanism and visual\nattention mechanism of human beings when watching a video, we propose a\nstep-gained fully convolutional network by combining the memory information on\nthe time axis with the motion information on the space axis while storing the\nsaliency information of the current frame. The model is obtained through\nhierarchical training, which ensures the accuracy of the detection. Extensive\nexperiments in comparison with 11 state-of-the-art methods are carried out, and\nthe results show that our proposed model outperforms all 11 methods across a\nnumber of publicly available datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 08:36:15 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Sun", "Meijun", ""], ["Zhou", "Ziqi", ""], ["Hu", "QinGhua", ""], ["Wang", "Zheng", ""], ["Jiang", "Jianmin", ""]]}, {"id": "1809.07999", "submitter": "Kyungmin Kim", "authors": "Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, Byoung-Tak Zhang", "title": "Multimodal Dual Attention Memory for Video Story Question Answering", "comments": "Accepted for ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a video story question-answering (QA) architecture, Multimodal\nDual Attention Memory (MDAM). The key idea is to use a dual attention mechanism\nwith late fusion. MDAM uses self-attention to learn the latent concepts in\nscene frames and captions. Given a question, MDAM uses the second attention\nover these latent concepts. Multimodal fusion is performed after the dual\nattention processes (late fusion). Using this processing pipeline, MDAM learns\nto infer a high-level vision-language joint representation from an abstraction\nof the full video content. We evaluate MDAM on PororoQA and MovieQA datasets\nwhich have large-scale QA annotations on cartoon videos and movies,\nrespectively. For both datasets, MDAM achieves new state-of-the-art results\nwith significant margins compared to the runner-up models. We confirm the best\nperformance of the dual attention mechanism combined with late fusion by\nablation studies. We also perform qualitative analysis by visualizing the\ninference mechanisms of MDAM.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 09:19:12 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Kim", "Kyung-Min", ""], ["Choi", "Seong-Ho", ""], ["Kim", "Jin-Hwa", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1809.08001", "submitter": "Joon Son Chung", "authors": "Soo-Whan Chung, Joon Son Chung, Hong-Goo Kang", "title": "Perfect match: Improved cross-modal embeddings for audio-visual\n  synchronisation", "comments": "Preprint. Work in progress", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8682524", "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new strategy for learning powerful cross-modal\nembeddings for audio-to-video synchronization. Here, we set up the problem as\none of cross-modal retrieval, where the objective is to find the most relevant\naudio segment given a short video clip. The method builds on the recent\nadvances in learning representations from cross-modal self-supervision.\n  The main contributions of this paper are as follows: (1) we propose a new\nlearning strategy where the embeddings are learnt via a multi-way matching\nproblem, as opposed to a binary classification (matching or non-matching)\nproblem as proposed by recent papers; (2) we demonstrate that performance of\nthis method far exceeds the existing baselines on the synchronization task; (3)\nwe use the learnt embeddings for visual speech recognition in self-supervision,\nand show that the performance matches the representations learnt end-to-end in\na fully-supervised manner.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 09:24:37 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 07:41:21 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chung", "Soo-Whan", ""], ["Chung", "Joon Son", ""], ["Kang", "Hong-Goo", ""]]}, {"id": "1809.08016", "submitter": "William Johnson", "authors": "William R. Johnson, Ajmal Mian, David G. Lloyd, Jacqueline A. Alderson", "title": "On-field player workload exposure and knee injury risk monitoring via\n  deep learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.jbiomech.2019.07.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sports analytics, an understanding of accurate on-field 3D knee joint\nmoments (KJM) could provide an early warning system for athlete workload\nexposure and knee injury risk. Traditionally, this analysis has relied on\ncaptive laboratory force plates and associated downstream biomechanical\nmodeling, and many researchers have approached the problem of portability by\nextrapolating models built on linear statistics. An alternative approach would\nbe to capitalize on recent advances in deep learning. In this study, using the\npre-trained CaffeNet convolutional neural network (CNN) model, multivariate\nregression of marker-based motion capture to 3D KJM for three sports-related\nmovement types were compared. The strongest overall mean correlation to source\nmodeling of 0.8895 was achieved over the initial 33 % of stance phase for\nsidestepping. The accuracy of these mean predictions of the three critical KJM\nassociated with anterior cruciate ligament (ACL) injury demonstrate the\nfeasibility of on-field knee injury assessment using deep learning in lieu of\nlaboratory embedded force plates. This multidisciplinary research approach\nsignificantly advances machine representation of real-world physical models\nwith practical application for both community and professional level athletes.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 10:09:48 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 01:58:57 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 14:34:31 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Johnson", "William R.", ""], ["Mian", "Ajmal", ""], ["Lloyd", "David G.", ""], ["Alderson", "Jacqueline A.", ""]]}, {"id": "1809.08044", "submitter": "Julian Iseringhausen", "authors": "Julian Iseringhausen and Matthias B. Hullin", "title": "Non-Line-of-Sight Reconstruction using Efficient Transient Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to see beyond the direct line of sight is an intriguing\nprospective and could benefit a wide variety of important applications. Recent\nwork has demonstrated that time-resolved measurements of indirect diffuse light\ncontain valuable information for reconstructing shape and reflectance\nproperties of objects located around a corner. In this paper, we introduce a\nnovel reconstruction scheme that, by design, produces solutions that are\nconsistent with state-of-the-art physically-based rendering. Our method\ncombines an efficient forward model (a custom renderer for time-resolved\nthree-bounce indirect light transport) with an optimization framework to\nreconstruct object geometry in an analysis-by-synthesis sense. We evaluate our\nalgorithm on a variety of synthetic and experimental input data, and show that\nit gracefully handles uncooperative scenes with high levels of noise or\nnon-diffuse material reflectance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 11:34:44 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 11:41:21 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Iseringhausen", "Julian", ""], ["Hullin", "Matthias B.", ""]]}, {"id": "1809.08064", "submitter": "Meysam Madadi", "authors": "Meysam Madadi, Egils Avots, Sergio Escalera, Jordi Gonzalez, Xavier\n  Baro, Gholamreza Anbarjafari", "title": "From 2D to 3D Geodesic-based Garment Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for 2D to 3D garment retexturing is proposed based on Gaussian\nmixture models and thin plate splines (TPS). An automatically segmented garment\nof an individual is matched to a new source garment and rendered, resulting in\naugmented images in which the target garment has been retextured by using the\ntexture of the source garment. We divide the problem into garment boundary\nmatching based on Gaussian mixture models and then interpolate inner points\nusing surface topology extracted through geodesic paths, which leads to a more\nrealistic result than standard approaches. We evaluated and compared our system\nquantitatively by mean square error (MSE) and qualitatively using the mean\nopinion score (MOS), showing the benefits of the proposed methodology on our\ngathered dataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 12:37:56 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Madadi", "Meysam", ""], ["Avots", "Egils", ""], ["Escalera", "Sergio", ""], ["Gonzalez", "Jordi", ""], ["Baro", "Xavier", ""], ["Anbarjafari", "Gholamreza", ""]]}, {"id": "1809.08132", "submitter": "Csaba Nemes", "authors": "Csaba Nemes and Sandor Jordan", "title": "Analysing object detectors from the perspective of co-occurring object\n  categories", "comments": "accepted to 9th IEEE International Conference on Cognitive\n  InfoCommunications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of state-of-the-art Faster R-CNN and YOLO object detectors are\nevaluated and compared on a special masked MS COCO dataset to measure how much\ntheir predictions rely on contextual information encoded at object category\nlevel. Category level representation of context is motivated by the fact that\nit could be an adequate way to transfer knowledge between visual and non-visual\ndomains. According to our measurements, current detectors usually do not build\nstrong dependency on contextual information at category level, however, when\nthey does, they does it in a similar way, suggesting that contextual dependence\nof object categories is an independent property that is relevant to be\ntransferred.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 14:08:08 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Nemes", "Csaba", ""], ["Jordan", "Sandor", ""]]}, {"id": "1809.08134", "submitter": "Rafael Monroy", "authors": "Rafael Monroy, Matis Hudon, Aljosa Smolic", "title": "Dynamic Environment Mapping for Augmented Reality Applications on Mobile\n  Devices", "comments": "To be presented at VMV 2018 (Eurographics Digital Library)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality is a topic of foremost interest nowadays. Its main goal is\nto seamlessly blend virtual content in real-world scenes. Due to the lack of\ncomputational power in mobile devices, rendering a virtual object with\nhigh-quality, coherent appearance and in real-time, remains an area of active\nresearch. In this work, we present a novel pipeline that allows for coupled\nenvironment acquisition and virtual object rendering on a mobile device\nequipped with a depth sensor. While keeping human interaction to a minimum, our\nsystem can scan a real scene and project it onto a two-dimensional environment\nmap containing RGB+Depth data. Furthermore, we define a set of criteria that\nallows for an adaptive update of the environment map to account for dynamic\nchanges in the scene. Then, under the assumption of diffuse surfaces and\ndistant illumination, our method exploits an analytic expression for the\nirradiance in terms of spherical harmonic coefficients, which leads to a very\nefficient rendering algorithm. We show that all the processes in our pipeline\ncan be executed while maintaining an average frame rate of 31Hz on a mobile\ndevice.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 14:10:55 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Monroy", "Rafael", ""], ["Hudon", "Matis", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1809.08168", "submitter": "Raein Hashemi", "authors": "Seyed Raein Hashemi, Sanjay P. Prabhu, Simon K. Warfield, Ali\n  Gholipour", "title": "Exclusive Independent Probability Estimation using Deep 3D Fully\n  Convolutional DenseNets: Application to IsoIntense Infant Brain MRI\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most recent fast and accurate image segmentation methods are built upon\nfully convolutional deep neural networks. In this paper, we propose new deep\nlearning strategies for DenseNets to improve segmenting images with subtle\ndifferences in intensity values and features. We aim to segment brain tissue on\ninfant brain MRI at about 6 months of age where white matter and gray matter of\nthe developing brain show similar T1 and T2 relaxation times, thus appear to\nhave similar intensity values on both T1- and T2-weighted MRI scans. Brain\ntissue segmentation at this age is, therefore, very challenging. To this end,\nwe propose an exclusive multi-label training strategy to segment the mutually\nexclusive brain tissues with similarity loss functions that automatically\nbalance the training based on class prevalence. Using our proposed training\nstrategy based on similarity loss functions and patch prediction fusion we\ndecrease the number of parameters in the network, reduce the complexity of the\ntraining process focusing the attention on less number of tasks, while\nmitigating the effects of data imbalance between labels and inaccuracies near\npatch borders. By taking advantage of these strategies we were able to perform\nfast image segmentation (90 seconds per 3D volume), using a network with less\nparameters than many state-of-the-art networks, overcoming issues such as\n3Dvs2D training and large vs small patch size selection, while achieving the\ntop performance in segmenting brain tissue among all methods tested in first\nand second round submissions of the isointense infant brain MRI segmentation\n(iSeg) challenge according to the official challenge test results. Our proposed\nstrategy improves the training process through balanced training and by\nreducing its complexity while providing a trained model that works for any size\ninput image and is fast and more accurate than many state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 15:08:31 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 14:30:01 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 05:55:31 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Hashemi", "Seyed Raein", ""], ["Prabhu", "Sanjay P.", ""], ["Warfield", "Simon K.", ""], ["Gholipour", "Ali", ""]]}, {"id": "1809.08229", "submitter": "Rohit Pardasani Mr.", "authors": "Rohit Pardasani, Utkarsh Shreemali", "title": "Image Denoising and Super-Resolution using Residual Learning of Deep\n  Convolutional Network", "comments": "5 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution and denoising are two important tasks in image\nprocessing that can lead to improvement in image quality. Image\nsuper-resolution is the task of mapping a low resolution image to a high\nresolution image whereas denoising is the task of learning a clean image from a\nnoisy input. We propose and train a single deep learning network that we term\nas SuRDCNN (super-resolution and denoising convolutional neural network), to\nperform these two tasks simultaneously . Our model nearly replicates the\narchitecture of existing state-of-the-art deep learning models for\nsuper-resolution and denoising. We use the proven strategy of residual\nlearning, as supported by state-of-the-art networks in this domain. Our trained\nSuRDCNN is capable of super-resolving image in the presence of Gaussian noise,\nPoisson noise or any random combination of both of these noises.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 17:58:22 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Pardasani", "Rohit", ""], ["Shreemali", "Utkarsh", ""]]}, {"id": "1809.08264", "submitter": "Suo Qiu", "authors": "Suo Qiu", "title": "Global Weighted Average Pooling Bridges Pixel-level Localization and\n  Image-level Classification", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we first tackle the problem of simultaneous pixel-level\nlocalization and image-level classification with only image-level labels for\nfully convolutional network training. We investigate the global pooling method\nwhich plays a vital role in this task. Classical global max pooling and average\npooling methods are hard to indicate the precise regions of objects. Therefore,\nwe revisit the global weighted average pooling (GWAP) method for this task and\npropose the class-agnostic GWAP module and the class-specific GWAP module in\nthis paper. We evaluate the classification and pixel-level localization ability\non the ILSVRC benchmark dataset. Experimental results show that the proposed\nGWAP module can better capture the regions of the foreground objects. We\nfurther explore the knowledge transfer between the image classification task\nand the region-based object detection task. We propose a multi-task framework\nthat combines our class-specific GWAP module with R-FCN. The framework is\ntrained with few ground truth bounding boxes and large-scale image-level\nlabels. We evaluate this framework on PASCAL VOC dataset. Experimental results\nshow that this framework can use the data with only image-level labels to\nimprove the generalization of the object detection model.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 18:29:13 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Qiu", "Suo", ""]]}, {"id": "1809.08287", "submitter": "Xin Ye", "authors": "Xin Ye, Zhe Lin, Joon-Young Lee, Jianming Zhang, Shibin Zheng and\n  Yezhou Yang", "title": "GAPLE: Generalizable Approaching Policy LEarning for Robotic Object\n  Searching in Indoor Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a generalizable action policy for an\nintelligent agent to actively approach an object of interest in an indoor\nenvironment solely from its visual inputs. While scene-driven or\nrecognition-driven visual navigation has been widely studied, prior efforts\nsuffer severely from the limited generalization capability. In this paper, we\nfirst argue the object searching task is environment dependent while the\napproaching ability is general. To learn a generalizable approaching policy, we\npresent a novel solution dubbed as GAPLE which adopts two channels of visual\nfeatures: depth and semantic segmentation, as the inputs to the policy learning\nmodule. The empirical studies conducted on the House3D dataset as well as on a\nphysical platform in a real world scenario validate our hypothesis, and we\nfurther provide in-depth qualitative analysis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 19:54:27 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 02:31:04 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Ye", "Xin", ""], ["Lin", "Zhe", ""], ["Lee", "Joon-Young", ""], ["Zhang", "Jianming", ""], ["Zheng", "Shibin", ""], ["Yang", "Yezhou", ""]]}, {"id": "1809.08317", "submitter": "Jonas Wulff", "authors": "Jonas Wulff and Michael J. Black", "title": "Temporal Interpolation as an Unsupervised Pretraining Task for Optical\n  Flow Estimation", "comments": "16 pages, 7 figures; accepted for publication at the German\n  Conference for Pattern Recognition (GCPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of annotating training data is a major obstacle to using CNNs\nfor low-level tasks in video. Synthetic data often does not generalize to real\nvideos, while unsupervised methods require heuristic losses. Proxy tasks can\novercome these issues, and start by training a network for a task for which\nannotation is easier or which can be trained unsupervised. The trained network\nis then fine-tuned for the original task using small amounts of ground truth\ndata. Here, we investigate frame interpolation as a proxy task for optical\nflow. Using real movies, we train a CNN unsupervised for temporal\ninterpolation. Such a network implicitly estimates motion, but cannot handle\nuntextured regions. By fine-tuning on small amounts of ground truth flow, the\nnetwork can learn to fill in homogeneous regions and compute full optical flow\nfields. Using this unsupervised pre-training, our network outperforms similar\narchitectures that were trained supervised using synthetic optical flow.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 21:30:18 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Wulff", "Jonas", ""], ["Black", "Michael J.", ""]]}, {"id": "1809.08318", "submitter": "Adam Terwilliger", "authors": "Adam M. Terwilliger, Garrick Brazil, Xiaoming Liu", "title": "Recurrent Flow-Guided Semantic Forecasting", "comments": "10 pages, 5 figures, 8 tables, Accepted to WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the world around us and making decisions about the future is a\ncritical component to human intelligence. As autonomous systems continue to\ndevelop, their ability to reason about the future will be the key to their\nsuccess. Semantic anticipation is a relatively under-explored area for which\nautonomous vehicles could take advantage of (e.g., forecasting pedestrian\ntrajectories). Motivated by the need for real-time prediction in autonomous\nsystems, we propose to decompose the challenging semantic forecasting task into\ntwo subtasks: current frame segmentation and future optical flow prediction.\nThrough this decomposition, we built an efficient, effective, low overhead\nmodel with three main components: flow prediction network, feature-flow\naggregation LSTM, and end-to-end learnable warp layer. Our proposed method\nachieves state-of-the-art accuracy on short-term and moving objects semantic\nforecasting while simultaneously reducing model parameters by up to 95% and\nincreasing efficiency by greater than 40x.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 21:31:25 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 18:13:47 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Terwilliger", "Adam M.", ""], ["Brazil", "Garrick", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1809.08340", "submitter": "Kevin Frans", "authors": "Kevin Frans, Chin-Yi Cheng", "title": "Unsupervised Image to Sequence Translation with Canvas-Drawer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoding images as a series of high-level constructs, such as brush strokes\nor discrete shapes, can often be key to both human and machine understanding.\nIn many cases, however, data is only available in pixel form. We present a\nmethod for generating images directly in a high-level domain (e.g. brush\nstrokes), without the need for real pairwise data. Specifically, we train a\n\"canvas\" network to imitate the mapping of high-level constructs to pixels,\nfollowed by a high-level \"drawing\" network which is optimized through this\nmapping towards solving a desired image recreation or translation task. We\nsuccessfully discover sequential vector representations of symbols, large\nsketches, and 3D objects, utilizing only pixel data. We display applications of\nour method in image segmentation, and present several ablation studies\ncomparing various configurations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 23:16:44 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 17:58:24 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Frans", "Kevin", ""], ["Cheng", "Chin-Yi", ""]]}, {"id": "1809.08352", "submitter": "Tom B Brown", "authors": "Tom B. Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul\n  Christiano, Ian Goodfellow", "title": "Unrestricted Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a two-player contest for evaluating the safety and robustness of\nmachine learning systems, with a large prize pool. Unlike most prior work in ML\nrobustness, which studies norm-constrained adversaries, we shift our focus to\nunconstrained adversaries. Defenders submit machine learning models, and try to\nachieve high accuracy and coverage on non-adversarial data while making no\nconfident mistakes on adversarial inputs. Attackers try to subvert defenses by\nfinding arbitrary unambiguous inputs where the model assigns an incorrect label\nwith high confidence. We propose a simple unambiguous dataset (\"bird-or-\nbicycle\") to use as part of this contest. We hope this contest will help to\nmore comprehensively evaluate the worst-case adversarial risk of machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 00:16:18 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Brown", "Tom B.", ""], ["Carlini", "Nicholas", ""], ["Zhang", "Chiyuan", ""], ["Olsson", "Catherine", ""], ["Christiano", "Paul", ""], ["Goodfellow", "Ian", ""]]}, {"id": "1809.08371", "submitter": "Guanxiong Sun", "authors": "Guanxiong Sun, Chengqin Ye, Kuanquan Wang", "title": "Focus On What's Important: Self-Attention Model for Human Pose\n  Estimation", "comments": "some errors on it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation is an essential yet challenging task in computer\nvision. One of the reasons for this difficulty is that there are many redundant\nregions in the images. In this work, we proposed a convolutional network\narchitecture combined with the novel attention model. We named it attention\nconvolutional neural network (ACNN). ACNN learns to focus on specific regions\nof different input features. It's a multi-stage architecture. Early stages\nfiltrate the \"nothing-regions\", such as background and redundant body parts.\nAnd then, they submit the important regions which contain the joints of the\nhuman body to the following stages to get a more accurate result. What's more,\nit does not require extra manual annotations and self-learning is one of our\nintentions. We separately trained the network because the attention learning\ntask and the pose estimation task are not independent. State-of-the-art\nperformance is obtained on the MPII benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 02:45:10 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 13:51:51 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Sun", "Guanxiong", ""], ["Ye", "Chengqin", ""], ["Wang", "Kuanquan", ""]]}, {"id": "1809.08377", "submitter": "Reza Katebi", "authors": "Reza Katebi, Yadi Zhou, Ryan Chornock and Razvan Bunescu", "title": "Galaxy morphology prediction using capsule networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": "10.1093/mnras/stz915", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding morphological types of galaxies is a key parameter for studying\ntheir formation and evolution. Neural networks that have been used previously\nfor galaxy morphology classification have some disadvantages, such as not being\ninvariant under rotation. In this work, we studied the performance of Capsule\nNetwork, a recently introduced neural network architecture that is rotationally\ninvariant and spatially aware, on the task of galaxy morphology classification.\nWe designed two evaluation scenarios based on the answers from the question\ntree in the Galaxy Zoo project. In the first scenario, we used Capsule Network\nfor regression and predicted probabilities for all of the questions. In the\nsecond scenario, we chose the answer to the first morphology question that had\nthe highest user agreement as the class of the object and trained a Capsule\nNetwork classifier, where we also reconstructed galaxy images. We achieved\npromising results in both of these scenarios. Automated approaches such as the\none introduced here will greatly decrease the workload of astronomers and will\nplay a critical role in the upcoming large sky surveys.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 03:41:05 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Katebi", "Reza", ""], ["Zhou", "Yadi", ""], ["Chornock", "Ryan", ""], ["Bunescu", "Razvan", ""]]}, {"id": "1809.08381", "submitter": "Meera Hahn", "authors": "Meera Hahn, Nataniel Ruiz, Jean-Baptiste Alayrac, Ivan Laptev, and\n  James M. Rehg", "title": "Learning to Localize and Align Fine-Grained Actions to Sparse\n  Instructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of textual video descriptions that are time-aligned with\nvideo content is a long-standing goal in computer vision. The task is\nchallenging due to the difficulty of bridging the semantic gap between the\nvisual and natural language domains. This paper addresses the task of\nautomatically generating an alignment between a set of instructions and a first\nperson video demonstrating an activity. The sparse descriptions and ambiguity\nof written instructions create significant alignment challenges. The key to our\napproach is the use of egocentric cues to generate a concise set of action\nproposals, which are then matched to recipe steps using object recognition and\ncomputational linguistic techniques. We obtain promising results on both the\nExtended GTEA Gaze+ dataset and the Bristol Egocentric Object Interactions\nDataset.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 04:41:08 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Hahn", "Meera", ""], ["Ruiz", "Nataniel", ""], ["Alayrac", "Jean-Baptiste", ""], ["Laptev", "Ivan", ""], ["Rehg", "James M.", ""]]}, {"id": "1809.08391", "submitter": "Ryota Natsume", "authors": "Ryota Natsume, Kazuki Inoue, Yoshihiro Fukuhara, Shintaro Yamamoto,\n  Shigeo Morishima, Hirokatsu Kataoka", "title": "Understanding Fake Faces", "comments": "11 pages, 3 figures, ECCV 2018 Workshop on Brain-Driven Computer\n  Vision (BDCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition research is one of the most active topics in computer vision\n(CV), and deep neural networks (DNN) are now filling the gap between\nhuman-level and computer-driven performance levels in face verification\nalgorithms. However, although the performance gap appears to be narrowing in\nterms of accuracy-based expectations, a curious question has arisen;\nspecifically, \"Face understanding of AI is really close to that of human?\" In\nthe present study, in an effort to confirm the brain-driven concept, we conduct\nimage-based detection, classification, and generation using an in-house created\nfake face database. This database has two configurations: (i) false positive\nface detections produced using both the Viola Jones (VJ) method and\nconvolutional neural networks (CNN), and (ii) simulacra that have fundamental\ncharacteristics that resemble faces but are completely artificial. The results\nshow a level of suggestive knowledge that indicates the continuing existence of\na gap between the capabilities of recent vision-based face recognition\nalgorithms and human-level performance. On a positive note, however, we have\nobtained knowledge that will advance the progress of face-understanding models.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 06:42:34 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Natsume", "Ryota", ""], ["Inoue", "Kazuki", ""], ["Fukuhara", "Yoshihiro", ""], ["Yamamoto", "Shintaro", ""], ["Morishima", "Shigeo", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "1809.08397", "submitter": "Zongliang Zhang", "authors": "Zongliang Zhang, Hongbin Zeng, Jonathan Li, Yiping Chen, Chenhui Yang,\n  Cheng Wang", "title": "Geometric Multi-Model Fitting by Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the geometric multi-model fitting from noisy,\nunstructured point set data (e.g., laser scanned point clouds). We formulate\nmulti-model fitting problem as a sequential decision making process. We then\nuse a deep reinforcement learning algorithm to learn the optimal decisions\ntowards the best fitting result. In this paper, we have compared our method\nagainst the state-of-the-art on simulated data. The results demonstrated that\nour approach significantly reduced the number of fitting iterations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 07:13:05 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 12:47:47 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Zhang", "Zongliang", ""], ["Zeng", "Hongbin", ""], ["Li", "Jonathan", ""], ["Chen", "Yiping", ""], ["Yang", "Chenhui", ""], ["Wang", "Cheng", ""]]}, {"id": "1809.08402", "submitter": "Sovann En", "authors": "Sovann En, Alexis Lechervy, Fr\\'ed\\'eric Jurie", "title": "RPNet: an End-to-End Network for Relative Camera Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of relative camera pose estimation from raw\nimage pixels, by means of deep neural networks. The proposed RPNet network\ntakes pairs of images as input and directly infers the relative poses, without\nthe need of camera intrinsic/extrinsic. While state-of-the-art systems based on\nSIFT + RANSAC, are able to recover the translation vector only up to scale,\nRPNet is trained to produce the full translation vector, in an end-to-end way.\nExperimental results on the Cambridge Landmark dataset show very promising\nresults regarding the recovery of the full translation vector. They also show\nthat RPNet produces more accurate and more stable results than traditional\napproaches, especially for hard images (repetitive textures, textureless\nimages, etc). To the best of our knowledge, RPNet is the first attempt to\nrecover full translation vectors in relative pose estimation.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 08:03:38 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["En", "Sovann", ""], ["Lechervy", "Alexis", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1809.08406", "submitter": "Armen Allahverdyan", "authors": "Rongrong Xie, Shengfeng Deng, Weibing Deng and Armen E. Allahverdyan", "title": "Active image restoration", "comments": "17 pages, 6 figures", "journal-ref": "Phys. Rev. E 98, 052108 (2018)", "doi": "10.1103/PhysRevE.98.052108", "report-no": null, "categories": "cond-mat.stat-mech cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study active restoration of noise-corrupted images generated via the Gibbs\nprobability of an Ising ferromagnet in external magnetic field. Ferromagnetism\naccounts for the prior expectation of data smoothness, i.e. a positive\ncorrelation between neighbouring pixels (Ising spins), while the magnetic field\nrefers to the bias. The restoration is actively supervised by requesting the\ntrue values of certain pixels after a noisy observation. This additional\ninformation improves restoration of other pixels. The optimal strategy of\nactive inference is not known for realistic (two-dimensional) images. We\ndetermine this strategy for the mean-field version of the model and show that\nit amounts to supervising the values of spins (pixels) that do not agree with\nthe sign of the average magnetization. The strategy leads to a transparent\nanalytical expression for the minimal Bayesian risk, and shows that there is a\nmaximal number of pixels beyond of which the supervision is useless. We show\nnumerically that this strategy applies for two-dimensional images away from the\ncritical regime. Within this regime the strategy is outperformed by its local\n(adaptive) version, which supervises pixels that do not agree with their\nBayesian estimate. We show on transparent examples how active supervising can\nbe essential in recovering noise-corrupted images and advocate for a wider\nusage of active methods in image restoration.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 08:32:43 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Xie", "Rongrong", ""], ["Deng", "Shengfeng", ""], ["Deng", "Weibing", ""], ["Allahverdyan", "Armen E.", ""]]}, {"id": "1809.08417", "submitter": "Md. Abu Bakr Siddique", "authors": "Md. Abu Bakr Siddique, Rezoana Bente Arif, Mohammad Mahmudur Rahman\n  Khan, Zahidun Ashrafi", "title": "Implementation of Fuzzy C-Means and Possibilistic C-Means Clustering\n  Algorithms, Cluster Tendency Analysis and Cluster Validation", "comments": "8 pages, 13 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, several two-dimensional clustering scenarios are given. In\nthose scenarios, soft partitioning clustering algorithms (Fuzzy C-means (FCM)\nand Possibilistic c-means (PCM)) are applied. Afterward, VAT is used to\ninvestigate the clustering tendency visually, and then in order of checking\ncluster validation, three types of indices (e.g., PC, DI, and DBI) were used.\nAfter observing the clustering algorithms, it was evident that each of them has\nits limitations; however, PCM is more robust to noise than FCM as in case of\nFCM a noise point has to be considered as a member of any of the cluster.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 09:45:05 GMT"}, {"version": "v2", "created": "Sun, 11 Nov 2018 03:22:09 GMT"}, {"version": "v3", "created": "Sun, 12 May 2019 06:10:21 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Siddique", "Md. Abu Bakr", ""], ["Arif", "Rezoana Bente", ""], ["Khan", "Mohammad Mahmudur Rahman", ""], ["Ashrafi", "Zahidun", ""]]}, {"id": "1809.08440", "submitter": "Ya Jing", "authors": "Ya Jing, Chenyang Si, Junbo Wang, Wei Wang, Liang Wang, Tieniu Tan", "title": "Pose-Guided Multi-Granularity Attention Network for Text-Based Person\n  Search", "comments": "published in AAAI2020(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based person search aims to retrieve the corresponding person images in\nan image database by virtue of a describing sentence about the person, which\nposes great potential for various applications such as video surveillance.\nExtracting visual contents corresponding to the human description is the key to\nthis cross-modal matching problem. Moreover, correlated images and descriptions\ninvolve different granularities of semantic relevance, which is usually ignored\nin previous methods. To exploit the multilevel corresponding visual contents,\nwe propose a pose-guided multi-granularity attention network (PMA). Firstly, we\npropose a coarse alignment network (CA) to select the related image regions to\nthe global description by a similarity-based attention. To further capture the\nphrase-related visual body part, a fine-grained alignment network (FA) is\nproposed, which employs pose information to learn latent semantic alignment\nbetween visual body part and textual noun phrase. To verify the effectiveness\nof our model, we perform extensive experiments on the CUHK Person Description\nDataset (CUHK-PEDES) which is currently the only available dataset for\ntext-based person search. Experimental results show that our approach\noutperforms the state-of-the-art methods by 15 \\% in terms of the top-1 metric.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 14:18:41 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 08:48:05 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 03:03:21 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Jing", "Ya", ""], ["Si", "Chenyang", ""], ["Wang", "Junbo", ""], ["Wang", "Wei", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "1809.08448", "submitter": "Shadrokh Samavi", "authors": "Milad Tehrani, Mahnoosh Bagheri, Mahdi Ahmadi, Alireza Norouzi, Nader\n  Karimi, Shadrokh Samavi", "title": "Artistic Instance-Aware Image Filtering by Convolutional Neural Networks", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, public use of artistic effects for editing and\nbeautifying images has encouraged researchers to look for new approaches to\nthis task. Most of the existing methods apply artistic effects to the whole\nimage. Exploitation of neural network vision technologies like object detection\nand semantic segmentation could be a new viewpoint in this area. In this paper,\nwe utilize an instance segmentation neural network to obtain a class mask for\nseparately filtering the background and foreground of an image. We implement a\ntop prior-mask selection to let us select an object class for filtering\npurpose. Different artistic effects are used in the filtering process to meet\nthe requirements of a vast variety of users. Also, our method is flexible\nenough to allow the addition of new filters. We use pre-trained Mask R-CNN\ninstance segmentation on the COCO dataset as the segmentation network.\nExperimental results on the use of different filters are performed. System's\noutput results show that this novel approach can create satisfying artistic\nimages with fast operation and simple interface.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 15:40:45 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Tehrani", "Milad", ""], ["Bagheri", "Mahnoosh", ""], ["Ahmadi", "Mahdi", ""], ["Norouzi", "Alireza", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1809.08458", "submitter": "Yihui He", "authors": "Huasong Zhong, Xianggen Liu, Yihui He, Yuchun Ma", "title": "Shift-based Primitives for Efficient Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a collection of three shift-based primitives for building\nefficient compact CNN-based networks. These three primitives (channel shift,\naddress shift, shortcut shift) can reduce the inference time on GPU while\nmaintains the prediction accuracy. These shift-based primitives only moves the\npointer but avoids memory copy, thus very fast. For example, the channel shift\noperation is 12.7x faster compared to channel shuffle in ShuffleNet but\nachieves the same accuracy. The address shift and channel shift can be merged\ninto the point-wise group convolution and invokes only a single kernel call,\ntaking little time to perform spatial convolution and channel shift. Shortcut\nshift requires no time to realize residual connection through allocating space\nin advance. We blend these shift-based primitives with point-wise group\nconvolution and built two inference-efficient CNN architectures named\nAddressNet and Enhanced AddressNet. Experiments on CIFAR100 and ImageNet\ndatasets show that our models are faster and achieve comparable or better\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 17:43:28 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 02:59:32 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Zhong", "Huasong", ""], ["Liu", "Xianggen", ""], ["He", "Yihui", ""], ["Ma", "Yuchun", ""]]}, {"id": "1809.08488", "submitter": "Abhinav Kumar", "authors": "Mayank Gupta, Abhinav Kumar and Sriganesh Madhvanath", "title": "Parametric Synthesis of Text on Stylized Backgrounds using PGGANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel method of generating high-resolution real-world images of\ntext where the style and textual content of the images are described\nparametrically. Our method combines text to image retrieval techniques with\nprogressive growing of Generative Adversarial Networks (PGGANs) to achieve\nconditional generation of photo-realistic images that reflect specific styles,\nas well as artifacts seen in real-world images. We demonstrate our method in\nthe context of automotive license plates. We assess the impact of varying the\nnumber of training images of each style on the fidelity of the generated style,\nand demonstrate the quality of the generated images using license plate\nrecognition systems.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 21:10:06 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Gupta", "Mayank", ""], ["Kumar", "Abhinav", ""], ["Madhvanath", "Sriganesh", ""]]}, {"id": "1809.08493", "submitter": "Eran Dahan", "authors": "Eran Dahan and Yosi Keller", "title": "SelfKin: Self Adjusted Deep Model For Kinship Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the unsolved challenges in the field of biometrics and face\nrecognition is Kinship Verification. This problem aims to understand if two\npeople are family-related and how (sisters, brothers, etc.) Solving this\nproblem can give rise to varied tasks and applications. In the area of homeland\nsecurity (HLS) it is crucial to auto-detect if the person questioned is related\nto a wanted suspect, In the field of biometrics, kinship-verification can help\nto discriminate between families by photos and in the field of predicting or\nfashion it can help to predict an older or younger model of people faces.\nLately, and with the advanced deep learning technology, this problem has gained\nfocus from the research community in matters of data and research. In this\narticle, we propose using a Deep Learning approach for solving the\nKinship-Verification problem. Further, we offer a novel self-learning deep\nmodel, which learns the essential features from different faces. We show that\nour model wins the Recognize Families In the Wild(RFIW2018,FG2018) challenge\nand obtains state-of-the-art results. Moreover, we show that our proposed model\ncan reduce the size of the network by half without loss in performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 21:52:18 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Dahan", "Eran", ""], ["Keller", "Yosi", ""]]}, {"id": "1809.08495", "submitter": "Bichen Wu", "authors": "Bichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue and Kurt Keutzer", "title": "SqueezeSegV2: Improved Model Structure and Unsupervised Domain\n  Adaptation for Road-Object Segmentation from a LiDAR Point Cloud", "comments": "Bichen Wu, Xuanyu Zhou, and Sicheng Zhao contributed equally to this\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earlier work demonstrates the promise of deep-learning-based approaches for\npoint cloud segmentation; however, these approaches need to be improved to be\npractically useful. To this end, we introduce a new model SqueezeSegV2 that is\nmore robust to dropout noise in LiDAR point clouds. With improved model\nstructure, training loss, batch normalization and additional input channel,\nSqueezeSegV2 achieves significant accuracy improvement when trained on real\ndata. Training models for point cloud segmentation requires large amounts of\nlabeled point-cloud data, which is expensive to obtain. To sidestep the cost of\ncollection and annotation, simulators such as GTA-V can be used to create\nunlimited amounts of labeled, synthetic data. However, due to domain shift,\nmodels trained on synthetic data often do not generalize well to the real\nworld. We address this problem with a domain-adaptation training pipeline\nconsisting of three major components: 1) learned intensity rendering, 2)\ngeodesic correlation alignment, and 3) progressive domain calibration. When\ntrained on real data, our new model exhibits segmentation accuracy improvements\nof 6.0-8.6% over the original SqueezeSeg. When training our new model on\nsynthetic data using the proposed domain adaptation pipeline, we nearly double\ntest accuracy on real-world data, from 29.0% to 57.4%. Our source code and\nsynthetic dataset will be open-sourced.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 22:04:49 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Wu", "Bichen", ""], ["Zhou", "Xuanyu", ""], ["Zhao", "Sicheng", ""], ["Yue", "Xiangyu", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1809.08545", "submitter": "Yihui He", "authors": "Yihui He, Chenchen Zhu, Jianren Wang, Marios Savvides, Xiangyu Zhang", "title": "Bounding Box Regression with Uncertainty for Accurate Object Detection", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale object detection datasets (e.g., MS-COCO) try to define the\nground truth bounding boxes as clear as possible. However, we observe that\nambiguities are still introduced when labeling the bounding boxes. In this\npaper, we propose a novel bounding box regression loss for learning bounding\nbox transformation and localization variance together. Our loss greatly\nimproves the localization accuracies of various architectures with nearly no\nadditional computation. The learned localization variance allows us to merge\nneighboring bounding boxes during non-maximum suppression (NMS), which further\nimproves the localization performance. On MS-COCO, we boost the Average\nPrecision (AP) of VGG-16 Faster R-CNN from 23.6% to 29.1%. More importantly,\nfor ResNet-50-FPN Mask R-CNN, our method improves the AP and AP90 by 1.8% and\n6.2% respectively, which significantly outperforms previous state-of-the-art\nbounding box refinement methods. Our code and models are available at:\ngithub.com/yihui-he/KL-Loss\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 07:07:01 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 19:49:26 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 23:26:16 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["He", "Yihui", ""], ["Zhu", "Chenchen", ""], ["Wang", "Jianren", ""], ["Savvides", "Marios", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "1809.08556", "submitter": "Jean Paul Ainam", "authors": "Jean-Paul Ainam, Ke Qin and Guisong Liu", "title": "Self Attention Grid for Person Re-Identification", "comments": "10 pages, 4 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an attention mechanism scheme to improve person\nre-identification task. Inspired by biology, we propose Self Attention Grid\n(SAG) to discover the most informative parts from a high-resolution image using\nits internal representation. In particular, given an input image, the proposed\nmodel is fed with two copies of the same image and consists of two branches.\nThe upper branch processes the high-resolution image and learns high\ndimensional feature representation while the lower branch processes the\nlow-resolution image and learn a filtering attention grid. We apply a max\nfilter operation to non-overlapping sub-regions on the high feature\nrepresentation before element-wise multiplied with the output of the second\nbranch. The feature maps of the second branch are subsequently weighted to\nreflect the importance of each patch of the grid using a softmax operation. Our\nattention module helps the network learn the most discriminative visual\nfeatures of multiple image regions and is specifically optimized to attend\nfeature representation at different levels. Extensive experiments on three\nlarge-scale datasets show that our self-attention mechanism significantly\nimproves the baseline model and outperforms various state-of-art models by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 08:50:57 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Ainam", "Jean-Paul", ""], ["Qin", "Ke", ""], ["Liu", "Guisong", ""]]}, {"id": "1809.08573", "submitter": "Longguang Wang", "authors": "Longguang Wang, Yulan Guo, Zaiping Lin, Xinpu Deng and Wei An", "title": "Learning for Video Super-Resolution through HR Optical Flow Estimation", "comments": "To appear in ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (SR) aims to generate a sequence of high-resolution\n(HR) frames with plausible and temporally consistent details from their\nlow-resolution (LR) counterparts. The generation of accurate correspondence\nplays a significant role in video SR. It is demonstrated by traditional video\nSR methods that simultaneous SR of both images and optical flows can provide\naccurate correspondences and better SR results. However, LR optical flows are\nused in existing deep learning based methods for correspondence generation. In\nthis paper, we propose an end-to-end trainable video SR framework to\nsuper-resolve both images and optical flows. Specifically, we first propose an\noptical flow reconstruction network (OFRnet) to infer HR optical flows in a\ncoarse-to-fine manner. Then, motion compensation is performed according to the\nHR optical flows. Finally, compensated LR inputs are fed to a super-resolution\nnetwork (SRnet) to generate the SR results. Extensive experiments demonstrate\nthat HR optical flows provide more accurate correspondences than their LR\ncounterparts and improve both accuracy and consistency performance. Comparative\nresults on the Vid4 and DAVIS-10 datasets show that our framework achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 10:30:28 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 08:22:06 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Wang", "Longguang", ""], ["Guo", "Yulan", ""], ["Lin", "Zaiping", ""], ["Deng", "Xinpu", ""], ["An", "Wei", ""]]}, {"id": "1809.08617", "submitter": "Chenying Wang", "authors": "Chenying Wang, Li Yu, Shengwei Wang", "title": "Accelerate CU Partition in HEVC using Large-Scale Convolutional Neural\n  Network", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High efficiency video coding (HEVC) suffers high encoding computational\ncomplexity, partly attributed to the rate-distortion optimization quad-tree\nsearch in CU partition decision. Therefore, we propose a novel two-stage CU\npartition decision approach in HEVC intra-mode. In the proposed approach,\nCNN-based algorithm is designed to decide CU partition mode precisely in three\ndepths. In order to alleviate computational complexity further, an auxiliary\nearl-termination mechanism is also proposed to filter obvious homogeneous CUs\nout of the subsequent CNN-based algorithm. Experimental results show that the\nproposed approach achieves about 37% encoding time saving on average and\ninsignificant BD-Bitrate rise compared with the original HEVC encoder.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 15:39:19 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Wang", "Chenying", ""], ["Yu", "Li", ""], ["Wang", "Shengwei", ""]]}, {"id": "1809.08625", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Anton Mitrokhin, Cornelia Ferm\\\"uller, James A. Yorke,\n  Yiannis Aloimonos", "title": "Unsupervised Learning of Dense Optical Flow, Depth and Egomotion from\n  Sparse Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a lightweight, unsupervised learning pipeline for\n\\textit{dense} depth, optical flow and egomotion estimation from sparse event\noutput of the Dynamic Vision Sensor (DVS). To tackle this low level vision\ntask, we use a novel encoder-decoder neural network architecture - ECN.\n  Our work is the first monocular pipeline that generates dense depth and\noptical flow from sparse event data only. The network works in self-supervised\nmode and has just 150k parameters. We evaluate our pipeline on the MVSEC self\ndriving dataset and present results for depth, optical flow and and egomotion\nestimation. Due to the lightweight design, the inference part of the network\nruns at 250 FPS on a single GPU, making the pipeline ready for realtime\nrobotics applications. Our experiments demonstrate significant improvements\nupon previous works that used deep learning on event data, as well as the\nability of our pipeline to perform well during both day and night.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 16:27:58 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 18:17:25 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Ye", "Chengxi", ""], ["Mitrokhin", "Anton", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Yorke", "James A.", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1809.08626", "submitter": "Arash Mahyari", "authors": "Arash Golibagh Mahyari, Thomas Locker", "title": "Domain Adaptation for Robot Predictive Maintenance Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial robots play an increasingly important role in a growing number of\nfields. For example, robotics is used to increase productivity while reducing\ncosts in various aspects of manufacturing. Since robots are often set up in\nproduction lines, the breakdown of a single robot has a negative impact on the\nentire process, in the worst case bringing the whole line to a halt until the\nissue is resolved, leading to substantial financial losses due to the\nunforeseen downtime. Therefore, predictive maintenance systems based on the\ninternal signals of robots have gained attention as an essential component of\nrobotics service offerings. The main shortcoming of existing predictive\nmaintenance algorithms is that the extracted features typically differ\nsignificantly from the learnt model when the operation of the robot changes,\nincurring false alarms. In order to mitigate this problem, predictive\nmaintenance algorithms require the model to be retrained with normal data of\nthe new operation. In this paper, we propose a novel solution based on transfer\nlearning to pass the knowledge of the trained model from one operation to\nanother in order to prevent the need for retraining and to eliminate such false\nalarms. The deployment of the proposed unsupervised transfer learning algorithm\non real-world datasets demonstrates that the algorithm can not only distinguish\nbetween operation and mechanical condition change, it further yields a sharper\ndeviation from the trained model in case of a mechanical condition change and\nthus detects mechanical issues with higher confidence.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 16:29:29 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 15:44:18 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 19:37:38 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Mahyari", "Arash Golibagh", ""], ["Locker", "Thomas", ""]]}, {"id": "1809.08675", "submitter": "Ankush Gupta", "authors": "Ankush Gupta, Andrea Vedaldi, Andrew Zisserman", "title": "Learning to Read by Spelling: Towards Unsupervised Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a method for visual text recognition without using any\npaired supervisory data. We formulate the text recognition task as one of\naligning the conditional distribution of strings predicted from given text\nimages, with lexically valid strings sampled from target corpora. This enables\nfully automated, and unsupervised learning from just line-level text-images,\nand unpaired text-string samples, obviating the need for large aligned\ndatasets. We present detailed analysis for various aspects of the proposed\nmethod, namely - (1) impact of the length of training sequences on convergence,\n(2) relation between character frequencies and the order in which they are\nlearnt, (3) generalisation ability of our recognition network to inputs of\narbitrary lengths, and (4) impact of varying the text corpus on recognition\naccuracy. Finally, we demonstrate excellent text recognition accuracy on both\nsynthetically generated text images, and scanned images of real printed books,\nusing no labelled training examples.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 20:53:41 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 19:57:06 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Gupta", "Ankush", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1809.08678", "submitter": "Shuaa Alharbi", "authors": "Shuaa S. Alharbi, Cigdem Sazak, Carl J. Nelson and Boguslaw Obara", "title": "Curvilinear Structure Enhancement by Multiscale Top-Hat Tensor in 2D/3D\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of biomedical applications requires enhancement, detection,\nquantification and modelling of curvilinear structures in 2D and 3D images.\nCurvilinear structure enhancement is a crucial step for further analysis, but\nmany of the enhancement approaches still suffer from contrast variations and\nnoise. This can be addressed using a multiscale approach that produces a better\nquality enhancement for low contrast and noisy images compared with a\nsingle-scale approach in a wide range of biomedical images. Here, we propose\nthe Multiscale Top-Hat Tensor (MTHT) approach, which combines multiscale\nmorphological filtering with a local tensor representation of curvilinear\nstructures in 2D and 3D images. The proposed approach is validated on synthetic\nand real data and is also compared to the state-of-the-art approaches. Our\nresults show that the proposed approach achieves high-quality curvilinear\nstructure enhancement in synthetic examples and in a wide range of 2D and 3D\nimages.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 20:58:58 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 20:55:27 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Alharbi", "Shuaa S.", ""], ["Sazak", "Cigdem", ""], ["Nelson", "Carl J.", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1809.08696", "submitter": "Zeljko Kereta", "authors": "Ernesto de Vito and Zeljko Kereta and Valeria Naumova", "title": "Unsupervised parameter selection for denoising with the elastic net", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in regularisation theory, the issue of parameter\nselection still remains a challenge for most applications. In a recent work the\nframework of statistical learning was used to approximate the optimal Tikhonov\nregularisation parameter from noisy data. In this work, we improve their\nresults and extend the analysis to the elastic net regularisation, providing\nexplicit error bounds on the accuracy of the approximated parameter and the\ncorresponding regularisation solution in a simplified case. Furthermore, in the\ngeneral case we design a data-driven, automated algorithm for the computation\nof an approximate regularisation parameter. Our analysis combines statistical\nlearning theory with insights from regularisation theory. We compare our\napproach with state-of-the-art parameter selection criteria and illustrate its\nsuperiority in terms of accuracy and computational time on simulated and real\ndata sets.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 23:31:17 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 08:52:04 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 14:56:15 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["de Vito", "Ernesto", ""], ["Kereta", "Zeljko", ""], ["Naumova", "Valeria", ""]]}, {"id": "1809.08697", "submitter": "Khyathi Raghavi Chandu", "authors": "Khyathi Raghavi Chandu, Mary Arpita Pyreddy, Matthieu Felix, Narendra\n  Nath Joshi", "title": "Textually Enriched Neural Module Networks for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems at the intersection of language and vision, like visual question\nanswering, have recently been gaining a lot of attention in the field of\nmulti-modal machine learning as computer vision research moves beyond\ntraditional recognition tasks. There has been recent success in visual question\nanswering using deep neural network models which use the linguistic structure\nof the questions to dynamically instantiate network layouts. In the process of\nconverting the question to a network layout, the question is simplified, which\nresults in loss of information in the model. In this paper, we enrich the image\ninformation with textual data using image captions and external knowledge bases\nto generate more coherent answers. We achieve 57.1% overall accuracy on the\ntest-dev open-ended questions from the visual question answering (VQA 1.0) real\nimage dataset.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 23:45:54 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Chandu", "Khyathi Raghavi", ""], ["Pyreddy", "Mary Arpita", ""], ["Felix", "Matthieu", ""], ["Joshi", "Narendra Nath", ""]]}, {"id": "1809.08714", "submitter": "Bryan Plummer", "authors": "Bryan A. Plummer, M. Hadi Kiapour, Shuai Zheng, Robinson Piramuthu", "title": "Give me a hint! Navigating Image Databases using Human-in-the-loop\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an attribute-based interactive image search which\ncan leverage human-in-the-loop feedback to iteratively refine image search\nresults. We study active image search where human feedback is solicited\nexclusively in visual form, without using relative attribute annotations used\nby prior work which are not typically found in many datasets. In order to\noptimize the image selection strategy, a deep reinforcement model is trained to\nlearn what images are informative rather than rely on hand-crafted measures\ntypically leveraged in prior work. Additionally, we extend the recently\nintroduced Conditional Similarity Network to incorporate global similarity in\ntraining visual embeddings, which results in more natural transitions as the\nuser explores the learned similarity embeddings. Our experiments demonstrate\nthe effectiveness of our approach, producing compelling results on both active\nimage search and image attribute representation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 01:16:55 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Plummer", "Bryan A.", ""], ["Kiapour", "M. Hadi", ""], ["Zheng", "Shuai", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1809.08734", "submitter": "Jing Yuan", "authors": "Jing Yuan and Aaron Fenster", "title": "Modern Convex Optimization to Medical Image Analysis", "comments": "Book chapter, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, diagnosis, therapy and monitoring of human diseases involve a\nvariety of imaging modalities, such as magnetic resonance imaging(MRI),\ncomputed tomography(CT), Ultrasound(US) and Positron-emission tomography(PET)\nas well as a variety of modern optical techniques. Over the past two decade, it\nhas been recognized that advanced image processing techniques provide valuable\ninformation to physicians for diagnosis, image guided therapy and surgery, and\nmonitoring of the treated organ to the therapy. Many researchers and companies\nhave invested significant efforts in the developments of advanced medical image\nanalysis methods; especially in the two core studies of medical image\nsegmentation and registration, segmentations of organs and lesions are used to\nquantify volumes and shapes used in diagnosis and monitoring treatment;\nregistration of multimodality images of organs improves detection, diagnosis\nand staging of diseases as well as image-guided surgery and therapy,\nregistration of images obtained from the same modality are used to monitor\nprogression of therapy. These challenging clinical-motivated applications\nintroduce novel and sophisticated mathematical problems which stimulate\ndevelopments of advanced optimization and computing methods, especially convex\noptimization attaining optimum in a global sense, hence, bring an enormous\nspread of research topics for recent computational medical image analysis.\nParticularly, distinct from the usual image processing, most medical images\nhave a big volume of acquired data, often in 3D or 4D (3D + t) along with great\nnoises or incomplete image information, and form the challenging large-scale\noptimization problems; how to process such poor 'big data' of medical images\nefficiently and solve the corresponding optimization problems robustly are the\nkey factors of modern medical image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 03:03:38 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Yuan", "Jing", ""], ["Fenster", "Aaron", ""]]}, {"id": "1809.08754", "submitter": "Chih-Chung Hsu", "authors": "Chih-Chung Hsu, Chia-Yen Lee, Yi-Xiu Zhuang", "title": "Learning to Detect Fake Face Images in the Wild", "comments": "4 pages to appear in IEEE IS3C Conference (IEEE International\n  Symposium on Computer, Consumer and Control Conference), Dec. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Network (GAN) can be used to generate the\nrealistic image, improper use of these technologies brings hidden concerns. For\nexample, GAN can be used to generate a tampered video for specific people and\ninappropriate events, creating images that are detrimental to a particular\nperson, and may even affect that personal safety. In this paper, we will\ndevelop a deep forgery discriminator (DeepFD) to efficiently and effectively\ndetect the computer-generated images. Directly learning a binary classifier is\nrelatively tricky since it is hard to find the common discriminative features\nfor judging the fake images generated from different GANs. To address this\nshortcoming, we adopt contrastive loss in seeking the typical features of the\nsynthesized images generated by different GANs and follow by concatenating a\nclassifier to detect such computer-generated images. Experimental results\ndemonstrate that the proposed DeepFD successfully detected 94.7% fake images\ngenerated by several state-of-the-art GANs.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 04:45:24 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 03:07:56 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 14:55:42 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Hsu", "Chih-Chung", ""], ["Lee", "Chia-Yen", ""], ["Zhuang", "Yi-Xiu", ""]]}, {"id": "1809.08758", "submitter": "Chuan Guo", "authors": "Chuan Guo, Jared S. Frank, Kilian Q. Weinberger", "title": "Low Frequency Adversarial Perturbation", "comments": "9 pages, 9 figures. Accepted to UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial images aim to change a target model's decision by minimally\nperturbing a target image. In the black-box setting, the absence of gradient\ninformation often renders this search problem costly in terms of query\ncomplexity. In this paper we propose to restrict the search for adversarial\nimages to a low frequency domain. This approach is readily compatible with many\nexisting black-box attack frameworks and consistently reduces their query cost\nby 2 to 4 times. Further, we can circumvent image transformation defenses even\nwhen both the model and the defense strategy are unknown. Finally, we\ndemonstrate the efficacy of this technique by fooling the Google Cloud Vision\nplatform with an unprecedented low number of model queries.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 04:54:36 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 21:45:56 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Guo", "Chuan", ""], ["Frank", "Jared S.", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1809.08761", "submitter": "Mahmoud Azab", "authors": "Mahmoud Azab, Mingzhe Wang, Max Smith, Noriyuki Kojima, Jia Deng, Rada\n  Mihalcea", "title": "Speaker Naming in Movies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for speaker naming in movies that leverages visual,\ntextual, and acoustic modalities in an unified optimization framework. To\nevaluate the performance of our model, we introduce a new dataset consisting of\nsix episodes of the Big Bang Theory TV show and eighteen full movies covering\ndifferent genres. Our experiments show that our multimodal model significantly\noutperforms several competitive baselines on the average weighted F-score\nmetric. To demonstrate the effectiveness of our framework, we design an\nend-to-end memory network model that leverages our speaker naming model and\nachieves state-of-the-art results on the subtitles task of the MovieQA 2017\nChallenge.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 05:00:05 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Azab", "Mahmoud", ""], ["Wang", "Mingzhe", ""], ["Smith", "Max", ""], ["Kojima", "Noriyuki", ""], ["Deng", "Jia", ""], ["Mihalcea", "Rada", ""]]}, {"id": "1809.08766", "submitter": "Aditya Vora", "authors": "Aditya Vora, Vinay Chilaka", "title": "FCHD: Fast and accurate head detection in crowded scenes", "comments": "5 pages, 4 figures, accepted for publication at International\n  Conference on Image Processing, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose FCHD-Fully Convolutional Head Detector, an\nend-to-end trainable head detection model. Our proposed architecture is a\nsingle fully convolutional network which is responsible for both bounding box\nprediction and classification. This makes our model lightweight with low\ninference time and memory requirements. Along with run-time, our model has\nbetter overall average precision (AP) which is achieved by selection of anchor\nsizes based on the effective receptive field of the network. This can be\nconcluded from our experiments on several head detection datasets with varying\nhead counts. We achieve an AP of 0.70 on a challenging head detection dataset\nwhich is comparable to some standard benchmarks. Along with this our model runs\nat 5 FPS on Nvidia Quadro M1000M for VGA resolution images. Code is available\nat https://github.com/aditya-vora/FCHD-Fully-Convolutional-Head-Detector.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 05:31:16 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 06:43:00 GMT"}, {"version": "v3", "created": "Sun, 5 May 2019 05:18:29 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Vora", "Aditya", ""], ["Chilaka", "Vinay", ""]]}, {"id": "1809.08799", "submitter": "Christian Reisswig", "authors": "Anoop Raveendra Katti, Christian Reisswig, Cordula Guder, Sebastian\n  Brarda, Steffen Bickel, Johannes H\\\"ohne, Jean Baptiste Faddoul", "title": "Chargrid: Towards Understanding 2D Documents", "comments": "To be published at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel type of text representation that preserves the 2D layout\nof a document. This is achieved by encoding each document page as a\ntwo-dimensional grid of characters. Based on this representation, we present a\ngeneric document understanding pipeline for structured documents. This pipeline\nmakes use of a fully convolutional encoder-decoder network that predicts a\nsegmentation mask and bounding boxes. We demonstrate its capabilities on an\ninformation extraction task from invoices and show that it significantly\noutperforms approaches based on sequential text or document images.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 08:37:02 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Katti", "Anoop Raveendra", ""], ["Reisswig", "Christian", ""], ["Guder", "Cordula", ""], ["Brarda", "Sebastian", ""], ["Bickel", "Steffen", ""], ["H\u00f6hne", "Johannes", ""], ["Faddoul", "Jean Baptiste", ""]]}, {"id": "1809.08801", "submitter": "Safa Medin", "authors": "Safa C. Medin, John Murray-Bruce, David Casta\\~n\\'on, Vivek K Goyal", "title": "Beyond Binomial and Negative Binomial: Adaptation in Bernoulli Parameter\n  Estimation", "comments": "13 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the parameter of a Bernoulli process arises in many applications,\nincluding photon-efficient active imaging where each illumination period is\nregarded as a single Bernoulli trial. Motivated by acquisition efficiency when\nmultiple Bernoulli processes are of interest, we formulate the allocation of\ntrials under a constraint on the mean as an optimal resource allocation\nproblem. An oracle-aided trial allocation demonstrates that there can be a\nsignificant advantage from varying the allocation for different processes and\ninspires a simple trial allocation gain quantity. Motivated by realizing this\ngain without an oracle, we present a trellis-based framework for representing\nand optimizing stopping rules. Considering the convenient case of Beta priors,\nthree implementable stopping rules with similar performances are explored, and\nthe simplest of these is shown to asymptotically achieve the oracle-aided trial\nallocation. These approaches are further extended to estimating functions of a\nBernoulli parameter. In simulations inspired by realistic active imaging\nscenarios, we demonstrate significant mean-squared error improvements: up to\n4.36 dB for the estimation of p and up to 1.80 dB for the estimation of log p.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 08:38:34 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 19:08:11 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Medin", "Safa C.", ""], ["Murray-Bruce", "John", ""], ["Casta\u00f1\u00f3n", "David", ""], ["Goyal", "Vivek K", ""]]}, {"id": "1809.08809", "submitter": "Nikolai Chinaev", "authors": "Nikolai Chinaev, Alexander Chigorin and Ivan Laptev", "title": "MobileFace: 3D Face Reconstruction with Efficient CNN Regression", "comments": "ECCV Workshops (PeopleCap) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of facial shapes plays a central role for face transfer and\nanimation. Accurate 3D face reconstruction, however, often deploys iterative\nand costly methods preventing real-time applications. In this work we design a\ncompact and fast CNN model enabling real-time face reconstruction on mobile\ndevices. For this purpose, we first study more traditional but slow morphable\nface models and use them to automatically annotate a large set of images for\nCNN training. We then investigate a class of efficient MobileNet CNNs and adapt\nsuch models for the task of shape regression. Our evaluation on three datasets\ndemonstrates significant improvements in the speed and the size of our model\nwhile maintaining state-of-the-art reconstruction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 09:15:08 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Chinaev", "Nikolai", ""], ["Chigorin", "Alexander", ""], ["Laptev", "Ivan", ""]]}, {"id": "1809.08846", "submitter": "Pratik Dubal", "authors": "Rishabh Iyer, Pratik Dubal, Kunal Dargan, Suraj Kothawade, Rohan\n  Mahadev and Vishal Kaushal", "title": "Vis-DSS: An Open-Source toolkit for Visual Data Selection and\n  Summarization", "comments": "Vis-DSS is available at https://github.com/rishabhk108/vis-dss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing amounts of visual data being created in the form of videos\nand images, visual data selection and summarization are becoming ever\nincreasing problems. We present Vis-DSS, an open-source toolkit for Visual Data\nSelection and Summarization. Vis-DSS implements a framework of models for\nsummarization and data subset selection using submodular functions, which are\nbecoming increasingly popular today for these problems. We present several\nclasses of models, capturing notions of diversity, coverage, representation and\nimportance, along with optimization/inference and learning algorithms. Vis-DSS\nis the first open source toolkit for several Data selection and summarization\ntasks including Image Collection Summarization, Video Summarization, Training\nData selection for Classification and Diversified Active Learning. We\ndemonstrate state-of-the art performance on all these tasks, and also show how\nwe can scale to large problems. Vis-DSS allows easy integration for\napplications to be built on it, also can serve as a general skeleton that can\nbe extended to several use cases, including video and image sharing platforms\nfor creating GIFs, image montage creation, or as a component to surveillance\nsystems and we demonstrate this by providing a graphical user-interface (GUI)\ndesktop app built over Qt framework. Vis-DSS is available at\nhttps://github.com/rishabhk108/vis-dss\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 11:15:14 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Iyer", "Rishabh", ""], ["Dubal", "Pratik", ""], ["Dargan", "Kunal", ""], ["Kothawade", "Suraj", ""], ["Mahadev", "Rohan", ""], ["Kaushal", "Vishal", ""]]}, {"id": "1809.08854", "submitter": "Suraj Kothawade", "authors": "Vishal Kaushal, Sandeep Subramanian, Suraj Kothawade, Rishabh Iyer and\n  Ganesh Ramakrishnan", "title": "A Framework towards Domain Specific Video Summarization", "comments": "Accepted to WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the light of exponentially increasing video content, video summarization\nhas attracted a lot of attention recently due to its ability to optimize time\nand storage. Characteristics of a good summary of a video depend on the\nparticular domain under question. We propose a novel framework for domain\nspecific video summarization. Given a video of a particular domain, our system\ncan produce a summary based on what is important for that domain in addition to\npossessing other desired characteristics like representativeness, coverage,\ndiversity etc. as suitable to that domain. Past related work has focused either\non using supervised approaches for ranking the snippets to produce summary or\non using unsupervised approaches of generating the summary as a subset of\nsnippets with the above characteristics. We look at the joint problem of\nlearning domain specific importance of segments as well as the desired summary\ncharacteristic for that domain. Our studies show that the more efficient way of\nincorporating domain specific relevances into a summary is by obtaining ratings\nof shots as opposed to binary inclusion/exclusion information. We also argue\nthat ratings can be seen as unified representation of all possible ground truth\nsummaries of a video, taking us one step closer in dealing with challenges\nassociated with multiple ground truth summaries of a video. We also propose a\nnovel evaluation measure which is more naturally suited in assessing the\nquality of video summary for the task at hand than F1 like measures. It\nleverages the ratings information and is richer in appropriately modeling\ndesirable and undesirable characteristics of a summary. Lastly, we release a\ngold standard dataset for furthering research in domain specific video\nsummarization, which to our knowledge is the first dataset with long videos\nacross several domains with rating annotations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 11:36:53 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 08:54:33 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Kaushal", "Vishal", ""], ["Subramanian", "Sandeep", ""], ["Kothawade", "Suraj", ""], ["Iyer", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "1809.08875", "submitter": "Judith B\\\"utepage", "authors": "Judith B\\\"utepage, Hedvig Kjellstr\\\"om, Danica Kragic", "title": "A Probabilistic Semi-Supervised Approach to Multi-Task Human Activity\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human behavior is a continuous stochastic spatio-temporal process which is\ngoverned by semantic actions and affordances as well as latent factors.\nTherefore, video-based human activity modeling is concerned with a number of\ntasks such as inferring current and future semantic labels, predicting future\ncontinuous observations as well as imagining possible future label and feature\nsequences. In this paper we present a semi-supervised probabilistic deep latent\nvariable model that can represent both discrete labels and continuous\nobservations as well as latent dynamics over time. This allows the model to\nsolve several tasks at once without explicit fine-tuning. We focus here on the\ntasks of action classification, detection, prediction and anticipation as well\nas motion prediction and synthesis based on 3D human activity data recorded\nwith Kinect. We further extend the model to capture hierarchical label\nstructure and to model the dependencies between multiple entities, such as a\nhuman and objects. Our experiments demonstrate that our principled approach to\nhuman activity modeling can be used to detect current and anticipate future\nsemantic labels and to predict and synthesize future label and feature\nsequences. When comparing our model to state-of-the-art approaches, which are\nspecifically designed for e.g. action classification, we find that our\nprobabilistic formulation outperforms or is comparable to these task specific\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 12:39:21 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 10:59:58 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 12:11:48 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["B\u00fctepage", "Judith", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Kragic", "Danica", ""]]}, {"id": "1809.08881", "submitter": "Dario Mantegazza", "authors": "Dario Mantegazza, J\\'er\\^ome Guzzi, Luca M. Gambardella and Alessandro\n  Giusti", "title": "Vision-based Control of a Quadrotor in User Proximity: Mediated vs\n  End-to-End Learning Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of controlling a quadrotor to hover in front of a freely\nmoving user, using input data from an onboard camera. On this specific task we\ncompare two widespread learning paradigms: a mediated approach, which learns an\nhigh-level state from the input and then uses it for deriving control signals;\nand an end-to-end approach, which skips high-level state estimation altogether.\nWe show that despite their fundamental difference, both approaches yield\nequivalent performance on this task. We finally qualitatively analyze the\nbehavior of a quadrotor implementing such approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 12:49:38 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 15:02:06 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Mantegazza", "Dario", ""], ["Guzzi", "J\u00e9r\u00f4me", ""], ["Gambardella", "Luca M.", ""], ["Giusti", "Alessandro", ""]]}, {"id": "1809.08909", "submitter": "Zhanyu Ma", "authors": "Zhanyu Ma and Hong Yu", "title": "Language Identification with Deep Bottleneck Features", "comments": "Preliminary work report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we proposed an end-to-end short utterances speech language\nidentification(SLD) approach based on a Long Short Term Memory (LSTM) neural\nnetwork which is special suitable for SLD application in intelligent vehicles.\nFeatures used for LSTM learning are generated by a transfer learning method.\nBottle-neck features of a deep neural network (DNN) which are trained for\nmandarin acoustic-phonetic classification are used for LSTM training. In order\nto improve the SLD accuracy of short utterances a phase vocoder based\ntime-scale modification(TSM) method is used to reduce and increase speech rated\nof the test utterance. By splicing the normal, speech rate reduced and\nincreased utterances, we can extend length of test utterances so as to improved\nimproved the performance of the SLD system. The experimental results on\nAP17-OLR database shows that the proposed methods can improve the performance\nof SLD, especially on short utterance with 1s and 3s durations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 19:34:54 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 09:57:14 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Ma", "Zhanyu", ""], ["Yu", "Hong", ""]]}, {"id": "1809.08986", "submitter": "Pin-Yu Chen", "authors": "Chia-Yi Hsu, Pei-Hsuan Lu, Pin-Yu Chen, Chia-Mu Yu", "title": "On The Utility of Conditional Generation Based Mutual Information for\n  Characterizing Adversarial Subspaces", "comments": "Accepted to IEEE GlobalSIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have found that deep learning systems are vulnerable to\nadversarial examples; e.g., visually unrecognizable adversarial images can\neasily be crafted to result in misclassification. The robustness of neural\nnetworks has been studied extensively in the context of adversary detection,\nwhich compares a metric that exhibits strong discriminate power between natural\nand adversarial examples. In this paper, we propose to characterize the\nadversarial subspaces through the lens of mutual information (MI) approximated\nby conditional generation methods. We use MI as an information-theoretic metric\nto strengthen existing defenses and improve the performance of adversary\ndetection. Experimental results on MagNet defense demonstrate that our proposed\nMI detector can strengthen its robustness against powerful adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 15:05:01 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Hsu", "Chia-Yi", ""], ["Lu", "Pei-Hsuan", ""], ["Chen", "Pin-Yu", ""], ["Yu", "Chia-Mu", ""]]}, {"id": "1809.08993", "submitter": "Florian Piewak", "authors": "Florian Piewak, Peter Pinggera, Markus Enzweiler, David Pfeiffer,\n  Marius Z\\\"ollner", "title": "Improved Semantic Stixels via Multimodal Sensor Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a compact and accurate representation of 3D scenes that\nare observed by a LiDAR sensor and a monocular camera. The proposed method is\nbased on the well-established Stixel model originally developed for stereo\nvision applications. We extend this Stixel concept to incorporate data from\nmultiple sensor modalities. The resulting mid-level fusion scheme takes full\nadvantage of the geometric accuracy of LiDAR measurements as well as the high\nresolution and semantic detail of RGB images. The obtained environment model\nprovides a geometrically and semantically consistent representation of the 3D\nscene at a significantly reduced amount of data while minimizing information\nloss at the same time. Since the different sensor modalities are considered as\ninput to a joint optimization problem, the solution is obtained with only minor\ncomputational overhead. We demonstrate the effectiveness of the proposed\nmultimodal Stixel algorithm on a manually annotated ground truth dataset. Our\nresults indicate that the proposed mid-level fusion of LiDAR and camera data\nimproves both the geometric and semantic accuracy of the Stixel model\nsignificantly while reducing the computational overhead as well as the amount\nof generated data in comparison to using a single modality on its own.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 15:18:10 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 13:47:28 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Piewak", "Florian", ""], ["Pinggera", "Peter", ""], ["Enzweiler", "Markus", ""], ["Pfeiffer", "David", ""], ["Z\u00f6llner", "Marius", ""]]}, {"id": "1809.08999", "submitter": "Ali Dabouei", "authors": "Ali Dabouei, Sobhan Soleymani, Jeremy Dawson, Nasser M. Nasrabadi", "title": "Fast Geometrically-Perturbed Adversarial Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art performance of deep learning algorithms has led to a\nconsiderable increase in the utilization of machine learning in\nsecurity-sensitive and critical applications. However, it has recently been\nshown that a small and carefully crafted perturbation in the input space can\ncompletely fool a deep model. In this study, we explore the extent to which\nface recognition systems are vulnerable to geometrically-perturbed adversarial\nfaces. We propose a fast landmark manipulation method for generating\nadversarial faces, which is approximately 200 times faster than the previous\ngeometric attacks and obtains 99.86% success rate on the state-of-the-art face\nrecognition models. To further force the generated samples to be natural, we\nintroduce a second attack constrained on the semantic structure of the face\nwhich has the half speed of the first attack with the success rate of 99.96%.\nBoth attacks are extremely robust against the state-of-the-art defense methods\nwith the success rate of equal or greater than 53.59%. Code is available at\nhttps://github.com/alldbi/FLM\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 15:26:13 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 17:20:54 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Dabouei", "Ali", ""], ["Soleymani", "Sobhan", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1809.09004", "submitter": "Enzo Ferrante", "authors": "Enzo Ferrante and Puneet K. Dokania and Rafael Marini Silva and Nikos\n  Paragios", "title": "Weakly-Supervised Learning of Metric Aggregations for Deformable Image\n  Registration", "comments": "Accepted for publication in IEEE Journal of Biomedical and Health\n  Informatics, 2018", "journal-ref": null, "doi": "10.1109/JBHI.2018.2869700", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable registration has been one of the pillars of biomedical image\ncomputing. Conventional approaches refer to the definition of a similarity\ncriterion that, once endowed with a deformation model and a smoothness\nconstraint, determines the optimal transformation to align two given images.\nThe definition of this metric function is among the most critical aspects of\nthe registration process. We argue that incorporating semantic information (in\nthe form of anatomical segmentation maps) into the registration process will\nfurther improve the accuracy of the results. In this paper, we propose a novel\nweakly supervised approach to learn domain specific aggregations of\nconventional metrics using anatomical segmentations. This combination is\nlearned using latent structured support vector machines (LSSVM). The learned\nmatching criterion is integrated within a metric free optimization framework\nbased on graphical models, resulting in a multi-metric algorithm endowed with a\nspatially varying similarity metric function conditioned on the anatomical\nstructures. We provide extensive evaluation on three different datasets of CT\nand MRI images, showing that learned multi-metric registration outperforms\nsingle-metric approaches based on conventional similarity measures.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 15:32:48 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Ferrante", "Enzo", ""], ["Dokania", "Puneet K.", ""], ["Silva", "Rafael Marini", ""], ["Paragios", "Nikos", ""]]}, {"id": "1809.09061", "submitter": "N\\'icolas Dos Santos Rosa", "authors": "N\\'icolas Rosa, Vitor Guizilini, Valdir Grassi Jr", "title": "Sparse-to-Continuous: Enhancing Monocular Depth Estimation using\n  Occupancy Maps", "comments": "Accepted. (c) 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": "10.1109/ICAR46387.2019.8981652", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of single image depth estimation (SIDE),\nfocusing on improving the quality of deep neural network predictions. In a\nsupervised learning scenario, the quality of predictions is intrinsically\nrelated to the training labels, which guide the optimization process. For\nindoor scenes, structured-light-based depth sensors (e.g. Kinect) are able to\nprovide dense, albeit short-range, depth maps. On the other hand, for outdoor\nscenes, LiDARs are considered the standard sensor, which comparatively provides\nmuch sparser measurements, especially in areas further away. Rather than\nmodifying the neural network architecture to deal with sparse depth maps, this\narticle introduces a novel densification method for depth maps, using the\nHilbert Maps framework. A continuous occupancy map is produced based on 3D\npoints from LiDAR scans, and the resulting reconstructed surface is projected\ninto a 2D depth map with arbitrary resolution. Experiments conducted with\nvarious subsets of the KITTI dataset show a significant improvement produced by\nthe proposed Sparse-to-Continuous technique, without the introduction of extra\ninformation into the training stage.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 17:10:10 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 19:56:22 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 22:01:23 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Rosa", "N\u00edcolas", ""], ["Guizilini", "Vitor", ""], ["Grassi", "Valdir", "Jr"]]}, {"id": "1809.09077", "submitter": "Shao-Yuan Lo", "authors": "Shang-Wei Hung, Shao-Yuan Lo, Hsueh-Ming Hang", "title": "Incorporating Luminance, Depth and Color Information by a Fusion-based\n  Network for Semantic Segmentation", "comments": "Accepted in IEEE International Conference on Image Processing (ICIP)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has made encouraging progress due to the success of\ndeep convolutional networks in recent years. Meanwhile, depth sensors become\nprevalent nowadays, so depth maps can be acquired more easily. However, there\nare few studies that focus on the RGB-D semantic segmentation task. Exploiting\nthe depth information effectiveness to improve performance is a challenge. In\nthis paper, we propose a novel solution named LDFNet, which incorporates\nLuminance, Depth and Color information by a fusion-based network. It includes a\nsub-network to process depth maps and employs luminance images to assist the\ndepth information in processes. LDFNet outperforms the other state-of-art\nsystems on the Cityscapes dataset, and its inference speed is faster than most\nof the existing networks. The experimental results show the effectiveness of\nthe proposed multi-modal fusion network and its potential for practical\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 17:45:35 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 14:40:07 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 18:07:56 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Hung", "Shang-Wei", ""], ["Lo", "Shao-Yuan", ""], ["Hang", "Hsueh-Ming", ""]]}, {"id": "1809.09149", "submitter": "Mehdi Hosseinzadeh", "authors": "Mehdi Hosseinzadeh, Kejie Li, Yasir Latif, Ian Reid", "title": "Real-Time Monocular Object-Model Aware Sparse SLAM", "comments": "Accepted to ICRA 2019 (for video demo look at\n  https://youtu.be/UMWXd4sHONw and https://youtu.be/QPQqVrvP0dE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous Localization And Mapping (SLAM) is a fundamental problem in\nmobile robotics. While sparse point-based SLAM methods provide accurate camera\nlocalization, the generated maps lack semantic information. On the other hand,\nstate of the art object detection methods provide rich information about\nentities present in the scene from a single image. This work incorporates a\nreal-time deep-learned object detector to the monocular SLAM framework for\nrepresenting generic objects as quadrics that permit detections to be\nseamlessly integrated while allowing the real-time performance. Finer\nreconstruction of an object, learned by a CNN network, is also incorporated and\nprovides a shape prior for the quadric leading further refinement. To capture\nthe dominant structure of the scene, additional planar landmarks are detected\nby a CNN-based plane detector and modeled as independent landmarks in the map.\nExtensive experiments support our proposed inclusion of semantic objects and\nplanar structures directly in the bundle-adjustment of SLAM - Semantic SLAM -\nthat enriches the reconstructed map semantically, while significantly improving\nthe camera localization. The performance of our SLAM system is demonstrated in\nhttps://youtu.be/UMWXd4sHONw and https://youtu.be/QPQqVrvP0dE .\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 18:23:32 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 06:24:40 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Hosseinzadeh", "Mehdi", ""], ["Li", "Kejie", ""], ["Latif", "Yasir", ""], ["Reid", "Ian", ""]]}, {"id": "1809.09189", "submitter": "Sajjad Azami", "authors": "Sina Mokhtarzadeh Azar, Sajjad Azami, Mina Ghadimi Atigh, Mohammad\n  Javadi, Ahmad Nickabadi", "title": "Zoom-RNN: A Novel Method for Person Recognition Using Recurrent Neural\n  Networks", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overwhelming popularity of social media has resulted in bulk amounts of\npersonal photos being uploaded to the internet every day. Since these photos\nare taken in unconstrained settings, recognizing the identities of people among\nthe photos remains a challenge. Studies have indicated that utilizing evidence\nother than face appearance improves the performance of person recognition\nsystems. In this work, we aim to take advantage of additional cues obtained\nfrom different body regions in a zooming in fashion for person recognition.\nHence, we present Zoom-RNN, a novel method based on recurrent neural networks\nfor combining evidence extracted from the whole body, upper body, and head\nregions. Our model is evaluated on a challenging dataset, namely People In\nPhoto Albums (PIPA), and we demonstrate that employing our system improves the\nperformance of conventional fusion methods by a noticeable margin.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 19:45:13 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 01:01:50 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Azar", "Sina Mokhtarzadeh", ""], ["Azami", "Sajjad", ""], ["Atigh", "Mina Ghadimi", ""], ["Javadi", "Mohammad", ""], ["Nickabadi", "Ahmad", ""]]}, {"id": "1809.09195", "submitter": "Vedhus Hoskere", "authors": "Vedhus Hoskere, Yasutaka Narazaki, Tu A. Hoang, Billie F. Spencer Jr", "title": "Towards Automated Post-Earthquake Inspections with Deep Learning-based\n  Condition-Aware Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the aftermath of an earthquake, rapid structural inspections are required\nto get citizens back in to their homes and offices in a safe and timely manner.\nThese inspections gfare typically conducted by municipal authorities through\nstructural engineer volunteers. As manual inspec-tions can be time consuming,\nlaborious and dangerous, research has been underway to develop methods to help\nspeed up and increase the automation of the entire process. Researchers\ntypi-cally envisage the use of unmanned aerial vehicles (UAV) for data\nacquisition and computer vision for data processing to extract actionable\ninformation. In this work we propose a new framework to generate vision-based\ncondition-aware models that can serve as the basis for speeding up or\nautomating higher level inspection decisions. The condition-aware models are\ngenerated by projecting the inference of trained deep-learning models on a set\nof images of a structure onto a 3D mesh model generated through multi-view\nstereo from the same image set. Deep fully convolutional residual networks are\nused for semantic segmentation of images of buildings to provide (i) damage\ninformation such as cracks and spalling (ii) contextual infor-mation such as\nthe presence of a building and visually identifiable components like windows\nand doors. The proposed methodology was implemented on a damaged building that\nwas sur-veyed by the authors after the Central Mexico Earthquake in September\n2017 and qualitative-ly evaluated. Results demonstrate the promise of the\nproposed method towards the ultimate goal of rapid and automated\npost-earthquake inspections.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 19:59:07 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Hoskere", "Vedhus", ""], ["Narazaki", "Yasutaka", ""], ["Hoang", "Tu A.", ""], ["Spencer", "Billie F.", "Jr"]]}, {"id": "1809.09287", "submitter": "Devesh Walawalkar", "authors": "Asim Smailagic, Hae Young Noh, Pedro Costa, Devesh Walawalkar, Kartik\n  Khandelwal, Mostafa Mirshekari, Jonathon Fagert, Adri\\'an Galdr\\'an, Susu Xu", "title": "MedAL: Deep Active Learning Sampling Method for Medical Image Analysis", "comments": "Accepted as conference paper for ICMLA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models have been successfully used in medical image analysis\nproblems but they require a large amount of labeled images to obtain good\nperformance.Deep learning models have been successfully used in medical image\nanalysis problems but they require a large amount of labeled images to obtain\ngood performance. However, such large labeled datasets are costly to acquire.\nActive learning techniques can be used to minimize the number of required\ntraining labels while maximizing the model's performance.In this work, we\npropose a novel sampling method that queries the unlabeled examples that\nmaximize the average distance to all training set examples in a learned feature\nspace. We then extend our sampling method to define a better initial training\nset, without the need for a trained model, by using ORB feature descriptors. We\nvalidate MedAL on 3 medical image datasets and show that our method is robust\nto different dataset properties. MedAL is also efficient, achieving 80%\naccuracy on the task of Diabetic Retinopathy detection using only 425 labeled\nimages, corresponding to a 32% reduction in the number of required labeled\nexamples compared to the standard uncertainty sampling technique, and a 40%\nreduction compared to random sampling.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 02:30:24 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 19:59:58 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Smailagic", "Asim", ""], ["Noh", "Hae Young", ""], ["Costa", "Pedro", ""], ["Walawalkar", "Devesh", ""], ["Khandelwal", "Kartik", ""], ["Mirshekari", "Mostafa", ""], ["Fagert", "Jonathon", ""], ["Galdr\u00e1n", "Adri\u00e1n", ""], ["Xu", "Susu", ""]]}, {"id": "1809.09293", "submitter": "Vaneet Aggarwal", "authors": "Vaneet Aggarwal and Hamed Asadi and Mayank Gupta and Jae Joong Lee and\n  Denny Yu", "title": "Covfefe: A Computer Vision Approach For Estimating Force Exertion", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cumulative exposure to repetitive and forceful activities may lead to\nmusculoskeletal injuries which not only reduce workers' efficiency and\nproductivity, but also affect their quality of life. Thus, widely accessible\ntechniques for reliable detection of unsafe muscle force exertion levels for\nhuman activity is necessary for their well-being. However, measurement of force\nexertion levels is challenging and the existing techniques pose a great\nchallenge as they are either intrusive, interfere with human-machine interface,\nand/or subjective in the nature, thus are not scalable for all workers. In this\nwork, we use face videos and the photoplethysmography (PPG) signals to classify\nforce exertion levels of 0\\%, 50\\%, and 100\\% (representing rest, moderate\neffort, and high effort), thus providing a non-intrusive and scalable approach.\nEfficient feature extraction approaches have been investigated, including\nstandard deviation of the movement of different landmarks of the face,\ndistances between peaks and troughs in the PPG signals. We note that the PPG\nsignals can be obtained from the face videos, thus giving an efficient\nclassification algorithm for the force exertion levels using face videos. Based\non the data collected from 20 subjects, features extracted from the face videos\ngive 90\\% accuracy in classification among the 100\\% and the combination of 0\\%\nand 50\\% datasets. Further combining the PPG signals provide 81.7\\% accuracy.\nThe approach is also shown to be robust to the correctly identify force level\nwhen the person is talking, even though such datasets are not included in the\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 02:45:19 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Aggarwal", "Vaneet", ""], ["Asadi", "Hamed", ""], ["Gupta", "Mayank", ""], ["Lee", "Jae Joong", ""], ["Yu", "Denny", ""]]}, {"id": "1809.09294", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zhuang Liu and Jianguo Li and Yu-Gang Jiang and\n  Yurong Chen and Xiangyang Xue", "title": "Object Detection from Scratch with Deep Supervision", "comments": "More results and analysis in this version. This is an extension of\n  our previous conference paper: arXiv:1708.01241", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Deeply Supervised Object Detectors (DSOD), an object detection\nframework that can be trained from scratch. Recent advances in object detection\nheavily depend on the off-the-shelf models pre-trained on large-scale\nclassification datasets like ImageNet and OpenImage. However, one problem is\nthat adopting pre-trained models from classification to detection task may\nincur learning bias due to the different objective function and diverse\ndistributions of object categories. Techniques like fine-tuning on detection\ntask could alleviate this issue to some extent but are still not fundamental.\nFurthermore, transferring these pre-trained models across discrepant domains\nwill be more difficult (e.g., from RGB to depth images). Thus, a better\nsolution to handle these critical problems is to train object detectors from\nscratch, which motivates our proposed method. Previous efforts on this\ndirection mainly failed by reasons of the limited training data and naive\nbackbone network structures for object detection. In DSOD, we contribute a set\nof design principles for learning object detectors from scratch. One of the key\nprinciples is the deep supervision, enabled by layer-wise dense connections in\nboth backbone networks and prediction layers, plays a critical role in learning\ngood detectors from scratch. After involving several other principles, we build\nour DSOD based on the single-shot detection framework (SSD). We evaluate our\nmethod on PASCAL VOC 2007, 2012 and COCO datasets. DSOD achieves consistently\nbetter results than the state-of-the-art methods with much more compact models.\nSpecifically, DSOD outperforms baseline method SSD on all three benchmarks,\nwhile requiring only 1/2 parameters. We also observe that DSOD can achieve\ncomparable/slightly better results than Mask RCNN + FPN (under similar input\nsize) with only 1/3 parameters, using no extra data or pre-trained models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 02:56:44 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 01:34:46 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Liu", "Zhuang", ""], ["Li", "Jianguo", ""], ["Jiang", "Yu-Gang", ""], ["Chen", "Yurong", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1809.09297", "submitter": "Masayuki Tanaka", "authors": "Masayuki Tanaka, Takashi Shibata and Masatoshi Okutomi", "title": "Gradient-Based Low-Light Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-light image enhancement is a highly demanded image processing\ntechnique, especially for consumer digital cameras and cameras on mobile\nphones. In this paper, a gradient-based low-light image enhancement algorithm\nis proposed. The key is to enhance the gradients of dark region, because the\ngradients are more sensitive for human visual system than absolute values. In\naddition, we involve the intensity-range constraints for the image integration.\nBy using the intensity-range constraints, we can integrate the output image\nwith enhanced gradients preserving the given gradient information while\nenforcing the intensity range of the output image within a certain intensity\nrange. Experiments demonstrate that the proposed gradient-based low-light image\nenhancement can effectively enhance the low-light images.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 03:05:43 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Tanaka", "Masayuki", ""], ["Shibata", "Takashi", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "1809.09299", "submitter": "Yanwei Pang", "authors": "Jiale Cao, Yanwei Pang, Xuelong Li", "title": "Triply Supervised Decoder Networks for Joint Detection and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint object detection and semantic segmentation can be applied to many\nfields, such as self-driving cars and unmanned surface vessels. An initial and\nimportant progress towards this goal has been achieved by simply sharing the\ndeep convolutional features for the two tasks. However, this simple scheme is\nunable to make full use of the fact that detection and segmentation are\nmutually beneficial. To overcome this drawback, we propose a framework called\nTripleNet where triple supervisions including detection-oriented supervision,\nclass-aware segmentation supervision, and class-agnostic segmentation\nsupervision are imposed on each layer of the decoder network. Class-agnostic\nsegmentation supervision provides an objectness prior knowledge for both\nsemantic segmentation and object detection. Besides the three types of\nsupervisions, two light-weight modules (i.e., inner-connected module and\nattention skip-layer fusion) are also incorporated into each layer of the\ndecoder. In the proposed framework, detection and segmentation can sufficiently\nboost each other. Moreover, class-agnostic and class-aware segmentation on each\ndecoder layer are not performed at the test stage. Therefore, no extra\ncomputational costs are introduced at the test stage. Experimental results on\nthe VOC2007 and VOC2012 datasets demonstrate that the proposed TripleNet is\nable to improve both the detection and segmentation accuracies without adding\nextra computational costs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 03:11:55 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Cao", "Jiale", ""], ["Pang", "Yanwei", ""], ["Li", "Xuelong", ""]]}, {"id": "1809.09310", "submitter": "Daniel Fremont", "authors": "Daniel J. Fremont, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue,\n  Alberto L. Sangiovanni-Vincentelli, Sanjit A. Seshia", "title": "Scenic: A Language for Scenario Specification and Scene Generation", "comments": "41 pages, 36 figures. Full version of a PLDI 2019 paper (extending UC\n  Berkeley EECS Department Tech Report No. UCB/EECS-2018-8)", "journal-ref": null, "doi": "10.1145/3314221.3314633", "report-no": null, "categories": "cs.PL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new probabilistic programming language for the design and\nanalysis of perception systems, especially those based on machine learning.\nSpecifically, we consider the problems of training a perception system to\nhandle rare events, testing its performance under different conditions, and\ndebugging failures. We show how a probabilistic programming language can help\naddress these problems by specifying distributions encoding interesting types\nof inputs and sampling these to generate specialized training and test sets.\nMore generally, such languages can be used for cyber-physical systems and\nrobotics to write environment models, an essential prerequisite to any formal\nanalysis. In this paper, we focus on systems like autonomous cars and robots,\nwhose environment is a \"scene\", a configuration of physical objects and agents.\nWe design a domain-specific language, Scenic, for describing \"scenarios\" that\nare distributions over scenes. As a probabilistic programming language, Scenic\nallows assigning distributions to features of the scene, as well as\ndeclaratively imposing hard and soft constraints over the scene. We develop\nspecialized techniques for sampling from the resulting distribution, taking\nadvantage of the structure provided by Scenic's domain-specific syntax.\nFinally, we apply Scenic in a case study on a convolutional neural network\ndesigned to detect cars in road images, improving its performance beyond that\nachieved by state-of-the-art synthetic data generation methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 03:57:00 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 01:12:24 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Fremont", "Daniel J.", ""], ["Dreossi", "Tommaso", ""], ["Ghosh", "Shromona", ""], ["Yue", "Xiangyu", ""], ["Sangiovanni-Vincentelli", "Alberto L.", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1809.09326", "submitter": "Pablo Navarrete Michelini", "authors": "Pablo Navarrete Michelini, Hanwen Liu and Dan Zhu", "title": "Multigrid Backprojection Super-Resolution and Deep Filter Visualization", "comments": "Spotlight paper in the Thirty-Third AAAI Conference on Artificial\n  Intelligence (AAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel deep-learning architecture for image upscaling by large\nfactors (e.g. 4x, 8x) based on examples of pristine high-resolution images. Our\ntarget is to reconstruct high-resolution images from their downscale versions.\nThe proposed system performs a multi-level progressive upscaling, starting from\nsmall factors (2x) and updating for higher factors (4x and 8x). The system is\nrecursive as it repeats the same procedure at each level. It is also residual\nsince we use the network to update the outputs of a classic upscaler. The\nnetwork residuals are improved by Iterative Back-Projections (IBP) computed in\nthe features of a convolutional network. To work in multiple levels we extend\nthe standard back-projection algorithm using a recursion analogous to\nMulti-Grid algorithms commonly used as solvers of large systems of linear\nequations. We finally show how the network can be interpreted as a standard\nupsampling-and-filter upscaler with a space-variant filter that adapts to the\ngeometry. This approach allows us to visualize how the network learns to\nupscale. Finally, our system reaches state of the art quality for models with\nrelatively few number of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 05:36:51 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 15:37:28 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2019 02:04:15 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Michelini", "Pablo Navarrete", ""], ["Liu", "Hanwen", ""], ["Zhu", "Dan", ""]]}, {"id": "1809.09368", "submitter": "Ruben Gomez-Ojeda", "authors": "Ruben Gomez-Ojeda and Javier Gonzalez-Jimenez", "title": "Geometric-based Line Segment Tracking for HDR Stereo Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a purely geometrical approach for the robust\nmatching of line segments for challenging stereo streams with severe\nillumination changes or High Dynamic Range (HDR) environments. To that purpose,\nwe exploit the univocal nature of the matching problem, i.e. every observation\nmust be corresponded with a single feature or not corresponded at all. We state\nthe problem as a sparse, convex, L1-minimization of the matching vector\nregularized by the geometric constraints. This formulation allows for the\nrobust tracking of line segments along sequences where traditional\nappearance-based matching techniques tend to fail due to dynamic changes in\nillumination conditions. Moreover, the proposed matching algorithm also results\nin a considerable speed-up of previous state of the art techniques making it\nsuitable for real-time applications such as Visual Odometry (VO). This, of\ncourse, comes at expense of a slightly lower number of matches in comparison\nwith appearance based methods, and also limits its application to continuous\nvideo sequences, as it is rather constrained to small pose increments between\nconsecutive frames. We validate the claimed advantages by first evaluating the\nmatching performance in challenging video sequences, and then testing the\nmethod in a benchmarked point and line based VO algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 09:10:30 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Gomez-Ojeda", "Ruben", ""], ["Gonzalez-Jimenez", "Javier", ""]]}, {"id": "1809.09404", "submitter": "Gabriel Maicas", "authors": "Gabriel Maicas, Andrew P. Bradley, Jacinto C. Nascimento, Ian Reid,\n  Gustavo Carneiro", "title": "Pre and Post-hoc Diagnosis and Interpretation of Malignancy from Breast\n  DCE-MRI", "comments": "Submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for breast cancer screening from DCE-MRI based on a\npost-hoc approach that is trained using weakly annotated data (i.e., labels are\navailable only at the image level without any lesion delineation). Our proposed\npost-hoc method automatically diagnosis the whole volume and, for positive\ncases, it localizes the malignant lesions that led to such diagnosis.\nConversely, traditional approaches follow a pre-hoc approach that initially\nlocalises suspicious areas that are subsequently classified to establish the\nbreast malignancy -- this approach is trained using strongly annotated data\n(i.e., it needs a delineation and classification of all lesions in an image).\nAnother goal of this paper is to establish the advantages and disadvantages of\nboth approaches when applied to breast screening from DCE-MRI. Relying on\nexperiments on a breast DCE-MRI dataset that contains scans of 117 patients,\nour results show that the post-hoc method is more accurate for diagnosing the\nwhole volume per patient, achieving an AUC of 0.91, while the pre-hoc method\nachieves an AUC of 0.81. However, the performance for localising the malignant\nlesions remains challenging for the post-hoc method due to the weakly labelled\ndataset employed during training.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 10:48:10 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 04:59:37 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Maicas", "Gabriel", ""], ["Bradley", "Andrew P.", ""], ["Nascimento", "Jacinto C.", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1809.09409", "submitter": "Ayta\\c{c} Kanac{\\i}", "authors": "Ayta\\c{c} Kanac{\\i}, Xiatian Zhu, Shaogang Gong", "title": "Vehicle Re-Identification in Context", "comments": "Dataset available at: http://qmul-vric.github.io. To appear on German\n  Conference on Pattern Recognition (GCPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing vehicle re-identification (re-id) evaluation benchmarks consider\nstrongly artificial test scenarios by assuming the availability of high quality\nimages and fine-grained appearance at an almost constant image scale,\nreminiscent to images required for Automatic Number Plate Recognition, e.g.\nVeRi-776. Such assumptions are often invalid in realistic vehicle re-id\nscenarios where arbitrarily changing image resolutions (scales) are the norm.\nThis makes the existing vehicle re-id benchmarks limited for testing the true\nperformance of a re-id method. In this work, we introduce a more realistic and\nchallenging vehicle re-id benchmark, called Vehicle Re-Identification in\nContext (VRIC). In contrast to existing datasets, VRIC is uniquely\ncharacterised by vehicle images subject to more realistic and unconstrained\nvariations in resolution (scale), motion blur, illumination, occlusion, and\nviewpoint. It contains 60,430 images of 5,622 vehicle identities captured by 60\ndifferent cameras at heterogeneous road traffic scenes in both day-time and\nnight-time.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 11:21:13 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 09:49:16 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Kanac\u0131", "Ayta\u00e7", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1809.09468", "submitter": "S\\'ergio Pereira", "authors": "Sergio Pereira, Raphael Meier, Victor Alves, Mauricio Reyes and Carlos\n  A. Silva", "title": "Automatic brain tumor grading from MRI data using convolutional neural\n  networks and quality assessment", "comments": "Accepted and presented at iMIMIC - Workshop on Interpretability of\n  Machine Intelligence in Medical Image Computing", "journal-ref": null, "doi": "10.1007/978-3-030-02628-8", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glioblastoma Multiforme is a high grade, very aggressive, brain tumor, with\npatients having a poor prognosis. Lower grade gliomas are less aggressive, but\nthey can evolve into higher grade tumors over time. Patient management and\ntreatment can vary considerably with tumor grade, ranging from tumor resection\nfollowed by a combined radio- and chemotherapy to a \"wait and see\" approach.\nHence, tumor grading is important for adequate treatment planning and\nmonitoring. The gold standard for tumor grading relies on histopathological\ndiagnosis of biopsy specimens. However, this procedure is invasive, time\nconsuming, and prone to sampling error. Given these disadvantages, automatic\ntumor grading from widely used MRI protocols would be clinically important, as\na way to expedite treatment planning and assessment of tumor evolution. In this\npaper, we propose to use Convolutional Neural Networks for predicting tumor\ngrade directly from imaging data. In this way, we overcome the need for expert\nannotations of regions of interest. We evaluate two prediction approaches: from\nthe whole brain, and from an automatically defined tumor region. Finally, we\nemploy interpretability methodologies as a quality assurance stage to check if\nthe method is using image regions indicative of tumor grade for classification.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 13:32:41 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Pereira", "Sergio", ""], ["Meier", "Raphael", ""], ["Alves", "Victor", ""], ["Reyes", "Mauricio", ""], ["Silva", "Carlos A.", ""]]}, {"id": "1809.09478", "submitter": "Yawei Luo", "authors": "Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, Yi Yang", "title": "Taking A Closer Look at Domain Shift: Category-level Adversaries for\n  Semantics Consistent Domain Adaptation", "comments": "CVPR2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of unsupervised domain adaptation in semantic\nsegmentation. The key in this campaign consists in reducing the domain shift,\ni.e., enforcing the data distributions of the two domains to be similar. A\npopular strategy is to align the marginal distribution in the feature space\nthrough adversarial learning. However, this global alignment strategy does not\nconsider the local category-level feature distribution. A possible consequence\nof the global movement is that some categories which are originally well\naligned between the source and target may be incorrectly mapped. To address\nthis problem, this paper introduces a category-level adversarial network,\naiming to enforce local semantic consistency during the trend of global\nalignment. Our idea is to take a close look at the category-level data\ndistribution and align each class with an adaptive adversarial loss.\nSpecifically, we reduce the weight of the adversarial loss for category-level\naligned features while increasing the adversarial force for those poorly\naligned. In this process, we decide how well a feature is category-level\naligned between source and target by a co-training approach. In two domain\nadaptation tasks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, we\nvalidate that the proposed method matches the state of the art in segmentation\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 13:43:25 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 12:10:31 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 14:25:06 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Luo", "Yawei", ""], ["Zheng", "Liang", ""], ["Guan", "Tao", ""], ["Yu", "Junqing", ""], ["Yang", "Yi", ""]]}, {"id": "1809.09529", "submitter": "Kaoutar Ben Ahmed", "authors": "Kaoutar Ben Ahmed, Ahmad Babaeian Jelodar", "title": "Fine-Tuning VGG Neural Network For Fine-grained State Recognition of\n  Food Images", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State recognition of food images can be considered as one of the promising\napplications of object recognition and fine-grained image classification in\ncomputer vision. In this paper, evidence is provided for the power of\nconvolutional neural network (CNN) for food state recognition, even with a\nsmall data set. In this study, we fine-tuned a CNN initially trained on a large\nnatural image recognition dataset (Imagenet ILSVRC) and transferred the learned\nfeature representations to the food state recognition task. A small-scale\ndataset consisting of 5978 images of seven categories was constructed and\nannotated manually. Data augmentation was applied to increase the size of the\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 22:38:33 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Ahmed", "Kaoutar Ben", ""], ["Jelodar", "Ahmad Babaeian", ""]]}, {"id": "1809.09558", "submitter": "Evangelos Boukas Prof", "authors": "Jordi Spranger, Roxana Buzatoiu, Athanasios Polydoros, Lazaros\n  Nalpantidis and Evangelos Boukas", "title": "Human-Machine Interface for Remote Training of Robot Tasks", "comments": "Accepted in IEEE International Conference on Imaging Systems and\n  Techniques - IST2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regardless of their industrial or research application, the streamlining of\nrobot operations is limited by the proximity of experienced users to the actual\nhardware. Be it massive open online robotics courses, crowd-sourcing of robot\ntask training, or remote research on massive robot farms for machine learning,\nthe need to create an apt remote Human-Machine Interface is quite prevalent.\nThe paper at hand proposes a novel solution to the programming/training of\nremote robots employing an intuitive and accurate user-interface which offers\nall the benefits of working with real robots without imposing delays and\ninefficiency. The system includes: a vision-based 3D hand detection and gesture\nrecognition subsystem, a simulated digital twin of a robot as visual feedback,\nand the \"remote\" robot learning/executing trajectories using dynamic motion\nprimitives. Our results indicate that the system is a promising solution to the\nproblem of remote training of robot tasks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 15:44:35 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Spranger", "Jordi", ""], ["Buzatoiu", "Roxana", ""], ["Polydoros", "Athanasios", ""], ["Nalpantidis", "Lazaros", ""], ["Boukas", "Evangelos", ""]]}, {"id": "1809.09607", "submitter": "Melani Sanchez", "authors": "Melani Sanchez-Garcia, Ruben Martinez-Cantin, Jose J. Guerrero", "title": "Structural and object detection for phosphene images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prosthetic vision based on phosphenes is a promising way to provide visual\nperception to some blind people. However, phosphenic images are very limited in\nterms of spatial resolution (e.g.: 32 x 32 phosphene array) and luminance\nlevels (e.g.: 8 gray levels), which results in the subject receiving very\nlimited information about the scene. This requires using high-level processing\nto extract more information from the scene and present it to the subject with\nthe phosphenes limitations. In this work, we study the recognition of indoor\nenvironments under simulated prosthetic vision. Most research in simulated\nprosthetic vision is performed based on static images, while very few\nresearchers have addressed the problem of scene recognition through video\nsequences. We propose a new approach to build a schematic representation of\nindoor environments for phosphene images. Our schematic representation relies\non two parallel CNNs for the extraction of structural informative edges of the\nroom and the relevant object silhouettes based on mask segmentation. We have\nperformed a study with twelve normally sighted subjects to evaluate how our\nmethods were able to the room recognition by presenting phosphenic images and\nvideos. We show how our method is able to increase the recognition ability of\nthe user from 75% using alternative methods to 90% using our approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 17:38:16 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 17:09:14 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Sanchez-Garcia", "Melani", ""], ["Martinez-Cantin", "Ruben", ""], ["Guerrero", "Jose J.", ""]]}, {"id": "1809.09645", "submitter": "Kyongsik Yun", "authors": "Kyongsik Yun, Alexander Huyen, and Thomas Lu", "title": "Deep Neural Networks for Pattern Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of pattern recognition research, the method of using deep neural\nnetworks based on improved computing hardware recently attracted attention\nbecause of their superior accuracy compared to conventional methods. Deep\nneural networks simulate the human visual system and achieve human equivalent\naccuracy in image classification, object detection, and segmentation. This\nchapter introduces the basic structure of deep neural networks that simulate\nhuman neural networks. Then we identify the operational processes and\napplications of conditional generative adversarial networks, which are being\nactively researched based on the bottom-up and top-down mechanisms, the most\nimportant functions of the human visual perception process. Finally, recent\ndevelopments in training strategies for effective learning of complex deep\nneural networks are addressed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 18:23:49 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Yun", "Kyongsik", ""], ["Huyen", "Alexander", ""], ["Lu", "Thomas", ""]]}, {"id": "1809.09700", "submitter": "Audrey Wong-Kee-You MA", "authors": "Audrey M. B. Wong-Kee-You, John K. Tsotsos, and Scott A. Adler", "title": "Development of spatial suppression surrounding the focus of visual\n  attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity to filter out irrelevant information from our environment is\ncritical to efficient processing. Yet, during development, when building a\nknowledge base of the world is occurring, the ability to selectively allocate\nattentional resources is limited (e.g., Amso & Scerif, 2015). In adulthood,\nresearch has demonstrated that surrounding the spatial location of attentional\nfocus is a suppressive field, resulting from top-down attention promoting the\nprocessing of relevant stimuli and inhibiting surrounding distractors (e.g.,\nHopf et al., 2006). It is not fully known, however, whether this phenomenon\nmanifests in development. In the current study, we examined whether spatial\nsuppression surrounding the focus of visual attention is exhibited in\ndevelopmental age groups. Participants between 12 and 27 years of age exhibited\nspatial suppression surrounding their focus of visual attention. Their accuracy\nincreased as a function of the separation distance between a spatially cued\n(and attended) target and a second target, suggesting that a ring of\nsuppression surrounded the attended target. When a central cue was instead\npresented and therefore attention was no longer spatially cued, surround\nsuppression was not observed, indicating that our initial findings of\nsuppression were indeed related to the focus of attention. Attentional surround\nsuppression was not observed in 8- to 11-years-olds, even with a longer spatial\ncue presentation time, demonstrating that the lack of the effect at these ages\nis not due to slowed attentional feedback processes. Our findings demonstrate\nthat top-down attentional processes are still immature until approximately 12\nyears of age, and that they continue to be refined throughout adolescence,\nconverging well with previous research on attentional development.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 01:35:56 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Wong-Kee-You", "Audrey M. B.", ""], ["Tsotsos", "John K.", ""], ["Adler", "Scott A.", ""]]}, {"id": "1809.09745", "submitter": "Nitish Nag", "authors": "Nitish Nag, Vaibhav Pandey, Aishwarya Manjunath, Avinash Vaka, Ramesh\n  Jain", "title": "Surface Type Estimation from GPS Tracked Bicycle Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road conditions affect both machine and human powered modes of\ntransportation. In the case of human powered transportation, poor road\nconditions increase the work for the individual to travel. Previous estimates\nfor these parameters have used computationally expensive analysis of satellite\nimages. In this work, we use a computationally inexpensive and simple method by\nusing only GPS data from a human powered cyclist. By estimating if the road\ntaken by the user has high or low variations in their directional vector, we\nclassify if the user is on a paved road or on an unpaved trail. In order to do\nthis, three methods were adopted, changes in frequency of the direction of\nslope in a given path segment, fitting segments of the path, and finding the\nfirst derivative and the number of points of zero crossings of each segment.\nMachine learning models such as support vector machines, K-nearest neighbors,\nand decision trees were used for the classification of the path. We show in our\nmethods, the decision trees performed the best with an accuracy of 86\\%.\nEstimation of the type of surface can be used for many applications such as\nunderstanding rolling resistance for power estimation estimation or building\nexercise recommendation systems by user profiling as described in detail in the\npaper.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 22:24:12 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Nag", "Nitish", ""], ["Pandey", "Vaibhav", ""], ["Manjunath", "Aishwarya", ""], ["Vaka", "Avinash", ""], ["Jain", "Ramesh", ""]]}, {"id": "1809.09758", "submitter": "Ruichao Xiao", "authors": "Ruichao Xiao, Wenxiu Sun, Chengxi Yang", "title": "Confidence Inference for Focused Learning in Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present confidence inference approachin an unsupervised way\nin stereo matching. Deep Neu-ral Networks (DNNs) have recently been achieving\nstate-of-the-art performance. However, it is often hard to tellwhether the\ntrained model was making sensible predictionsor just guessing at random. To\naddress this problem, westart from a probabilistic interpretation of theL1loss\nusedin stereo matching, which inherently assumes an indepen-dent and identical\n(aka i.i.d.) Laplacian distribution. Weshow that with the newly introduced\ndense confidence map,the identical assumption is relaxed. Intuitively, the\nvari-ance in the Laplacian distribution is large for low confidentpixels while\nsmall for high-confidence pixels. In practice,the network learns\ntoattenuatelow-confidence pixels (e.g.,noisy input, occlusions, featureless\nregions) andfocusonhigh-confidence pixels. Moreover, it can be observed\nfromexperiments that the focused learning is very helpful in find-ing a better\nconvergence state of the trained model, reduc-ing over-fitting on a given\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 23:34:39 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Xiao", "Ruichao", ""], ["Sun", "Wenxiu", ""], ["Yang", "Chengxi", ""]]}, {"id": "1809.09761", "submitter": "Keunhong Park", "authors": "Keunhong Park, Konstantinos Rematas, Ali Farhadi, Steven M. Seitz", "title": "PhotoShape: Photorealistic Materials for Large-Scale Shape Collections", "comments": "To be presented at SIGGRAPH Asia 2018. Project page:\n  https://keunhong.com/publications/photoshape/", "journal-ref": null, "doi": "10.1145/3272127.3275066", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing online 3D shape repositories contain thousands of 3D models but lack\nphotorealistic appearance. We present an approach to automatically assign\nhigh-quality, realistic appearance models to large scale 3D shape collections.\nThe key idea is to jointly leverage three types of online data -- shape\ncollections, material collections, and photo collections, using the photos as\nreference to guide assignment of materials to shapes. By generating a large\nnumber of synthetic renderings, we train a convolutional neural network to\nclassify materials in real photos, and employ 3D-2D alignment techniques to\ntransfer materials to different parts of each shape model. Our system produces\nphotorealistic, relightable, 3D shapes (PhotoShapes).\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 00:01:03 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Park", "Keunhong", ""], ["Rematas", "Konstantinos", ""], ["Farhadi", "Ali", ""], ["Seitz", "Steven M.", ""]]}, {"id": "1809.09763", "submitter": "Ruichao Xiao", "authors": "Ruichao Xiao, Wenxiu Sun, Jiahao Pang, Qiong Yan, Jimmy Ren", "title": "DSR: Direct Self-rectification for Uncalibrated Dual-lens Cameras", "comments": "Accepted at 3DV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the developments of dual-lens camera modules,depth information\nrepresenting the third dimension of thecaptured scenes becomes available for\nsmartphones. It isestimated by stereo matching algorithms, taking as input\nthetwo views captured by dual-lens cameras at slightly differ-ent viewpoints.\nDepth-of-field rendering (also be referred toas synthetic defocus or bokeh) is\none of the trending depth-based applications. However, to achieve fast depth\nestima-tion on smartphones, the stereo pairs need to be rectified inthe first\nplace. In this paper, we propose a cost-effective so-lution to perform stereo\nrectification for dual-lens camerascalled direct self-rectification, short for\nDSR1. It removesthe need of individual offline calibration for every pair\nofdual-lens cameras. In addition, the proposed solution isrobust to the slight\nmovements, e.g., due to collisions, ofthe dual-lens cameras after fabrication.\nDifferent with ex-isting self-rectification approaches, our approach\ncomputesthe homography in a novel way with zero geometric distor-tions\nintroduced to the master image. It is achieved by di-rectly minimizing the\nvertical displacements of correspond-ing points between the original master\nimage and the trans-formed slave image. Our method is evaluated on both\nreal-istic and synthetic stereo image pairs, and produces supe-rior results\ncompared to the calibrated rectification or otherself-rectification approaches\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 00:10:42 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Xiao", "Ruichao", ""], ["Sun", "Wenxiu", ""], ["Pang", "Jiahao", ""], ["Yan", "Qiong", ""], ["Ren", "Jimmy", ""]]}, {"id": "1809.09767", "submitter": "Asha Anoosheh", "authors": "Asha Anoosheh, Torsten Sattler, Radu Timofte, Marc Pollefeys, Luc Van\n  Gool", "title": "Night-to-Day Image Translation for Retrieval-based Localization", "comments": "Published in ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization is a key step in many robotics pipelines, allowing the\nrobot to (approximately) determine its position and orientation in the world.\nAn efficient and scalable approach to visual localization is to use image\nretrieval techniques. These approaches identify the image most similar to a\nquery photo in a database of geo-tagged images and approximate the query's pose\nvia the pose of the retrieved database image. However, image retrieval across\ndrastically different illumination conditions, e.g. day and night, is still a\nproblem with unsatisfactory results, even in this age of powerful neural\nmodels. This is due to a lack of a suitably diverse dataset with true\ncorrespondences to perform end-to-end learning. A recent class of neural models\nallows for realistic translation of images among visual domains with relatively\nlittle training data and, most importantly, without ground-truth pairings. In\nthis paper, we explore the task of accurately localizing images captured from\ntwo traversals of the same area in both day and night. We propose ToDayGAN - a\nmodified image-translation model to alter nighttime driving images to a more\nuseful daytime representation. We then compare the daytime and translated night\nimages to obtain a pose estimate for the night image using the known 6-DOF\nposition of the closest day image. Our approach improves localization\nperformance by over 250% compared the current state-of-the-art, in the context\nof standard metrics in multiple categories.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 00:33:43 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 11:09:08 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Anoosheh", "Asha", ""], ["Sattler", "Torsten", ""], ["Timofte", "Radu", ""], ["Pollefeys", "Marc", ""], ["Van Gool", "Luc", ""]]}, {"id": "1809.09828", "submitter": "Toshiyuki Fukuzawa", "authors": "Toshiyuki Fukuzawa", "title": "A Problem Reduction Approach for Visual Relationships Detection", "comments": "ECCV 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying different objects (man and cup) is an important problem on its\nown, but identifying the relationship between them (holding) is critical for\nmany real world use cases. This paper describes an approach to reduce a visual\nrelationship detection problem to object detection problems. The method was\napplied to Google AI Open Images V4 Visual Relationship Track Challenge, which\nwas held in conjunction with 2018 European Conference on Computer Vision (ECCV\n2018) and it finished as a prize winner. The challenge was to build an\nalgorithm that detects pairs of objects in particular relations: things like\n\"woman playing guitar,\" \"beer on table,\" or \"dog inside car.\".\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 07:08:41 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Fukuzawa", "Toshiyuki", ""]]}, {"id": "1809.09839", "submitter": "Bo Jiang", "authors": "Bo Jiang and Doudou Lin", "title": "Graph Laplacian Regularized Graph Convolutional Networks for\n  Semi-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph convolutional network (GCN) has been widely used for\nsemi-supervised classification and deep feature representation on\ngraph-structured data. However, existing GCN generally fails to consider the\nlocal invariance constraint in learning and representation process. That is, if\ntwo data points Xi and Xj are close in the intrinsic geometry of the data\ndistribution, then their labels/representations should also be close to each\nother. This is known as local invariance assumption which plays an essential\nrole in the development of various kinds of traditional algorithms, such as\ndimensionality reduction and semi-supervised learning, in machine learning\narea. To overcome this limitation, we introduce a graph Laplacian GCN (gLGCN)\napproach for graph data representation and semi-supervised classification. The\nproposed gLGCN model is capable of encoding both graph structure and node\nfeatures together while maintains the local invariance constraint naturally for\nrobust data representation and semi-supervised classification. Experiments show\nthe benefit of the benefits the proposed gLGCN network.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 07:39:29 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Jiang", "Bo", ""], ["Lin", "Doudou", ""]]}, {"id": "1809.09875", "submitter": "Clemens-Alexander Brust", "authors": "Clemens-Alexander Brust, Christoph K\\\"ading, Joachim Denzler", "title": "Active Learning for Deep Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great success that deep models have achieved in the past is mainly owed\nto large amounts of labeled training data. However, the acquisition of labeled\ndata for new tasks aside from existing benchmarks is both challenging and\ncostly. Active learning can make the process of labeling new data more\nefficient by selecting unlabeled samples which, when labeled, are expected to\nimprove the model the most. In this paper, we combine a novel method of active\nlearning for object detection with an incremental learning scheme to enable\ncontinuous exploration of new unlabeled datasets. We propose a set of\nuncertainty-based active learning metrics suitable for most object detectors.\nFurthermore, we present an approach to leverage class imbalances during sample\nselection. All methods are evaluated systematically in a continuous exploration\ncontext on the PASCAL VOC 2012 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 09:47:42 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Brust", "Clemens-Alexander", ""], ["K\u00e4ding", "Christoph", ""], ["Denzler", "Joachim", ""]]}, {"id": "1809.09924", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Joachim Denzler", "title": "Hierarchy-based Image Embeddings for Semantic Image Retrieval", "comments": "Accepted at WACV 2019. Source code:\n  https://github.com/cvjena/semantic-embeddings", "journal-ref": "2019 IEEE Winter Conference on Applications of Computer Vision\n  (WACV), Waikoloa Village, HI, USA, 2019, pp. 638-647", "doi": "10.1109/WACV.2019.00073", "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained for classification have been found to learn\npowerful image representations, which are also often used for other tasks such\nas comparing images w.r.t. their visual similarity. However, visual similarity\ndoes not imply semantic similarity. In order to learn semantically\ndiscriminative features, we propose to map images onto class embeddings whose\npair-wise dot products correspond to a measure of semantic similarity between\nclasses. Such an embedding does not only improve image retrieval results, but\ncould also facilitate integrating semantics for other tasks, e.g., novelty\ndetection or few-shot learning. We introduce a deterministic algorithm for\ncomputing the class centroids directly based on prior world-knowledge encoded\nin a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds,\nand ImageNet show that our learned semantic image embeddings improve the\nsemantic consistency of image retrieval results by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 11:58:19 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 14:03:18 GMT"}, {"version": "v3", "created": "Wed, 21 Nov 2018 11:55:13 GMT"}, {"version": "v4", "created": "Tue, 19 Mar 2019 15:13:18 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "1809.09929", "submitter": "Junqiao Zhao", "authors": "Yewei Huang and Junqiao Zhao and Xudong He and Shaoming Zhang and\n  Tiantian Feng", "title": "Vision-based Semantic Mapping and Localization for Autonomous Indoor\n  Parking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a novel and practical solution for the real-time\nindoor localization of autonomous driving in parking lots. High-level\nlandmarks, the parking slots, are extracted and enriched with labels to avoid\nthe aliasing of low-level visual features. We then proposed a robust method for\ndetecting incorrect data associations between parking slots and further\nextended the optimization framework by dynamically eliminating suboptimal data\nassociations. Visual fiducial markers are introduced to improve the overall\nprecision. As a result, a semantic map of the parking lot can be established\nfully automatically and robustly. We experimented the performance of real-time\nlocalization based on the map using our autonomous driving platform TiEV, and\nthe average accuracy of 0.3m track tracing can be achieved at a speed of 10kph.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 12:03:45 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Huang", "Yewei", ""], ["Zhao", "Junqiao", ""], ["He", "Xudong", ""], ["Zhang", "Shaoming", ""], ["Feng", "Tiantian", ""]]}, {"id": "1809.09970", "submitter": "Di Wu", "authors": "Di Wu, Kun Zhang, Fei Cheng, Yang Zhao, Qi Liu, Chang-An Yuan and\n  De-Shuang Huang", "title": "Random Occlusion-recovery for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a basic task of multi-camera surveillance system, person re-identification\naims to re-identify a query pedestrian observed from non-overlapping multiple\ncameras or across different time with a single camera. Recently, deep\nlearning-based person re-identification models have achieved great success in\nmany benchmarks. However, these supervised models require a large amount of\nlabeled image data, and the process of manual labeling spends much manpower and\ntime. In this study, we introduce a method to automatically synthesize labeled\nperson images and adopt them to increase the sample number per identity for\nperson re-identification datasets. To be specific, we use block rectangles to\nrandomly occlude pedestrian images. Then, a generative adversarial network\n(GAN) model is proposed to use paired occluded and original images to\nsynthesize the de-occluded images that similar but not identical to the\noriginal image. Afterwards, we annotate the de-occluded images with the same\nlabels of their corresponding raw images and use them to augment the number of\nsamples per identity. Finally, we use the augmented datasets to train baseline\nmodel. The experiment results on CUHK03, Market-1501 and DukeMTMC-reID datasets\nshow that the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 13:29:26 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 06:11:34 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 11:34:15 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Wu", "Di", ""], ["Zhang", "Kun", ""], ["Cheng", "Fei", ""], ["Zhao", "Yang", ""], ["Liu", "Qi", ""], ["Yuan", "Chang-An", ""], ["Huang", "De-Shuang", ""]]}, {"id": "1809.09973", "submitter": "Michael Jacobs", "authors": "Vishwa S. Parekh and Michael A. Jacobs", "title": "MPRAD: A Multiparametric Radiomics Framework", "comments": "32 pages, 7 figures", "journal-ref": "Breast Cancer Res Treat (2020)", "doi": "10.1007/s10549-020-05533-5", "report-no": null, "categories": "cs.CV physics.bio-ph physics.med-ph q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiparametric radiological imaging is vital for detection, characterization\nand diagnosis of many different diseases. The use of radiomics for quantitative\nextraction of textural features from radiological imaging is increasing moving\ntowards clinical decision support. However, current methods in radiomics are\nlimited to using single images for the extraction of these textural features\nand may limit the applicable scope of radiomics in different clinical settings.\nThus, in the current form, they are not capable of capturing the true\nunderlying tissue characteristics in high dimensional multiparametric imaging\nspace. To overcome this challenge, we have developed a multiparametric imaging\nradiomic framework termed MPRAD for extraction of radiomic features from high\ndimensional datasets. MPRAD was tested on two different organs and diseases;\nbreast cancer and cerebrovascular accidents in brain, commonly referred to as\nstroke. The MPRAD framework classified malignant from benign breast lesions\nwith excellent sensitivity and specificity of 87% and 80.5% respectively with\nan AUC of 0.88 providing a 9%-28% increase in AUC over single radiomic\nparameters. More importantly, in breast, the glandular tissue MPRAD were\nsimilar between each group with no significance differences. Similarly, the\nMPRAD features in brain stroke demonstrated increased performance in\ndistinguishing the perfusion-diffusion mismatch compared to single parameter\nradiomics and there were no differences within the white and gray matter\ntissue. In conclusion, we have introduced the use of multiparametric radiomics\ninto a clinical setting\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 16:47:05 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Parekh", "Vishwa S.", ""], ["Jacobs", "Michael A.", ""]]}, {"id": "1809.09978", "submitter": "Adam Van Etten", "authors": "Adam Van Etten", "title": "Satellite Imagery Multiscale Rapid Detection with Windowed Networks", "comments": "8 pages, 7 figures, 2 tables, 1 appendix. arXiv admin note:\n  substantial text overlap with arXiv:1805.09512", "journal-ref": null, "doi": "10.1109/WACV.2019.00083", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting small objects over large areas remains a significant challenge in\nsatellite imagery analytics. Among the challenges is the sheer number of pixels\nand geographical extent per image: a single DigitalGlobe satellite image\nencompasses over 64 km2 and over 250 million pixels. Another challenge is that\nobjects of interest are often minuscule (~pixels in extent even for the highest\nresolution imagery), which complicates traditional computer vision techniques.\nTo address these issues, we propose a pipeline (SIMRDWN) that evaluates\nsatellite images of arbitrarily large size at native resolution at a rate of >\n0.2 km2/s. Building upon the tensorflow object detection API paper, this\npipeline offers a unified approach to multiple object detection frameworks that\ncan run inference on images of arbitrary size. The SIMRDWN pipeline includes a\nmodified version of YOLO (known as YOLT), along with the models of the\ntensorflow object detection API: SSD, Faster R-CNN, and R-FCN. The proposed\napproach allows comparison of the performance of these four frameworks, and can\nrapidly detect objects of vastly different scales with relatively little\ntraining data over multiple sensors. For objects of very different scales (e.g.\nairplanes versus airports) we find that using two different detectors at\ndifferent scales is very effective with negligible runtime cost.We evaluate\nlarge test images at native resolution and find mAP scores of 0.2 to 0.8 for\nvehicle localization, with the YOLT architecture achieving both the highest mAP\nand fastest inference speed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 03:00:05 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Van Etten", "Adam", ""]]}, {"id": "1809.10080", "submitter": "Amy Tabb", "authors": "Philipe A. Dias, Amy Tabb, Henry Medeiros", "title": "Multispecies fruit flower detection using a refined semantic\n  segmentation network", "comments": "8 pages", "journal-ref": "IEEE Robotics and Automation Letters, vol. 3, no. 4, pp.\n  3003-3010, Oct. 2018", "doi": "10.1109/LRA.2018.2849498", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In fruit production, critical crop management decisions are guided by bloom\nintensity, i.e., the number of flowers present in an orchard. Despite its\nimportance, bloom intensity is still typically estimated by means of human\nvisual inspection. Existing automated computer vision systems for flower\nidentification are based on hand-engineered techniques that work only under\nspecific conditions and with limited performance. This work proposes an\nautomated technique for flower identification that is robust to uncontrolled\nenvironments and applicable to different flower species. Our method relies on\nan end-to-end residual convolutional neural network (CNN) that represents the\nstate-of-the-art in semantic segmentation. To enhance its sensitivity to\nflowers, we fine-tune this network using a single dataset of apple flower\nimages. Since CNNs tend to produce coarse segmentations, we employ a refinement\nmethod to better distinguish between individual flower instances. Without any\npre-processing or dataset-specific training, experimental results on images of\napple, peach and pear flowers, acquired under different conditions demonstrate\nthe robustness and broad applicability of our method.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 00:08:42 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Dias", "Philipe A.", ""], ["Tabb", "Amy", ""], ["Medeiros", "Henry", ""]]}, {"id": "1809.10089", "submitter": "Christoph Markus Schikora", "authors": "Christoph Schikora, Markus Plack, and Andreas Kolb", "title": "Residuum-Condition Diagram and Reduction of Over-Complete Endmember-Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting reference spectra, or endmembers (EMs) from a given multi- or\nhyperspectral image, as well as estimating the size of the EM set, plays an\nimportant role in multispectral image processing. In this paper, we present\ncondition-residuum-diagrams. By plotting the residuum resulting from the\nunmixing and reconstruction and the condition number of various EM sets, the\nresulting diagram provides insight into the behavior of the spectral unmixing\nunder a varying amount of endmembers (EMs). Furthermore, we utilize\ncondition-residuum-diagrams to realize an EM reduction algorithm that starts\nwith an initially extracted, over-complete EM set. An over-complete EM set\ncommonly exhibits a good unmixing result, i.e. a lower reconstruction residuum,\nbut due to its partial redundancy, the unmixing gets numerically unstable, i.e.\nthe unmixed abundances values are less reliable. Our greedy reduction scheme\nimproves the EM set by reducing the condition number, i.e. enhancing the set's\nstability, while keeping the reconstruction error as low as possible. The\nresulting set sequence gives hint to the optimal EM set and its size. We\ndemonstrate the benefit of our condition-residuum-diagram and reduction scheme\non well-studied datasets with known reference EM set sizes for several\nwell-known EE algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 16:01:11 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Schikora", "Christoph", ""], ["Plack", "Markus", ""], ["Kolb", "Andreas", ""]]}, {"id": "1809.10097", "submitter": "Songyou Peng", "authors": "Bjoern Haefner, Songyou Peng, Alok Verma, Yvain Qu\\'eau, Daniel\n  Cremers", "title": "Photometric Depth Super-Resolution", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (T-PAMI), 2019. First three authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores the use of photometric techniques (shape-from-shading and\nuncalibrated photometric stereo) for upsampling the low-resolution depth map\nfrom an RGB-D sensor to the higher resolution of the companion RGB image. A\nsingle-shot variational approach is first put forward, which is effective as\nlong as the target's reflectance is piecewise-constant. It is then shown that\nthis dependency upon a specific reflectance model can be relaxed by focusing on\na specific class of objects (e.g., faces), and delegate reflectance estimation\nto a deep neural network. A multi-shot strategy based on randomly varying\nlighting conditions is eventually discussed. It requires no training or prior\non the reflectance, yet this comes at the price of a dedicated acquisition\nsetup. Both quantitative and qualitative evaluations illustrate the\neffectiveness of the proposed methods on synthetic and real-world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 16:09:26 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 07:09:00 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Haefner", "Bjoern", ""], ["Peng", "Songyou", ""], ["Verma", "Alok", ""], ["Qu\u00e9au", "Yvain", ""], ["Cremers", "Daniel", ""]]}, {"id": "1809.10099", "submitter": "Nikhil Bharadwaj Gosala", "authors": "Nikhil Bharadwaj Gosala, Andreas B\\\"uhler, Manish Prajapat, Claas\n  Ehmke, Mehak Gupta, Ramya Sivanesan, Abel Gawel, Mark Pfeiffer, Mathias\n  B\\\"urki, Inkyu Sa, Renaud Dub\\'e, Roland Siegwart", "title": "Redundant Perception and State Estimation for Reliable Autonomous Racing", "comments": "7 pages, 21 figures, submitted to the International Conference on\n  Robotics and Automation 2019, for accompanying video visit\n  https://www.youtube.com/watch?v=ir_uqEYuT84", "journal-ref": null, "doi": "10.1109/ICRA.2019.8794155", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous racing, vehicles operate close to the limits of handling and a\nsensor failure can have critical consequences. To limit the impact of such\nfailures, this paper presents the redundant perception and state estimation\napproaches developed for an autonomous race car. Redundancy in perception is\nachieved by estimating the color and position of the track delimiting objects\nusing two sensor modalities independently. Specifically, learning-based\napproaches are used to generate color and pose estimates, from LiDAR and camera\ndata respectively. The redundant perception inputs are fused by a particle\nfilter based SLAM algorithm that operates in real-time. Velocity is estimated\nusing slip dynamics, with reliability being ensured through a probabilistic\nfailure detection algorithm. The sub-modules are extensively evaluated in\nreal-world racing conditions using the autonomous race car \"gotthard\ndriverless\", achieving lateral accelerations up to 1.7G and a top speed of\n90km/h.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 16:14:25 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gosala", "Nikhil Bharadwaj", ""], ["B\u00fchler", "Andreas", ""], ["Prajapat", "Manish", ""], ["Ehmke", "Claas", ""], ["Gupta", "Mehak", ""], ["Sivanesan", "Ramya", ""], ["Gawel", "Abel", ""], ["Pfeiffer", "Mark", ""], ["B\u00fcrki", "Mathias", ""], ["Sa", "Inkyu", ""], ["Dub\u00e9", "Renaud", ""], ["Siegwart", "Roland", ""]]}, {"id": "1809.10117", "submitter": "Michalis Giannopoulos Mr", "authors": "Michalis Giannopoulos, Grigorios Tsagkatakis, Saverio Blasi, Farzad\n  Toutounchi, Athanasios Mouchtaris, Panagiotis Tsakalides, Marta Mrak, Ebroul\n  Izquierdo", "title": "Convolutional Neural Networks for Video Quality Assessment", "comments": "Number of Pages: 12, Number of Figures: 17, Submitted to: Signal\n  Processing: Image Communication (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Quality Assessment (VQA) is a very challenging task due to its highly\nsubjective nature. Moreover, many factors influence VQA. Compression of video\ncontent, while necessary for minimising transmission and storage requirements,\nintroduces distortions which can have detrimental effects on the perceived\nquality. Especially when dealing with modern video coding standards, it is\nextremely difficult to model the effects of compression due to the\nunpredictability of encoding on different content types. Moreover, transmission\nalso introduces delays and other distortion types which affect the perceived\nquality. Therefore, it would be highly beneficial to accurately predict the\nperceived quality of video to be distributed over modern content distribution\nplatforms, so that specific actions could be undertaken to maximise the Quality\nof Experience (QoE) of the users. Traditional VQA techniques based on feature\nextraction and modelling may not be sufficiently accurate. In this paper, a\nnovel Deep Learning (DL) framework is introduced for effectively predicting VQA\nof video content delivery mechanisms based on end-to-end feature learning. The\nproposed framework is based on Convolutional Neural Networks, taking into\naccount compression distortion as well as transmission delays. Training and\nevaluation of the proposed framework are performed on a user annotated VQA\ndataset specifically created to undertake this work. The experiments show that\nthe proposed methods can lead to high accuracy of the quality estimation,\nshowcasing the potential of using DL in complex VQA scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 16:55:54 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Giannopoulos", "Michalis", ""], ["Tsagkatakis", "Grigorios", ""], ["Blasi", "Saverio", ""], ["Toutounchi", "Farzad", ""], ["Mouchtaris", "Athanasios", ""], ["Tsakalides", "Panagiotis", ""], ["Mrak", "Marta", ""], ["Izquierdo", "Ebroul", ""]]}, {"id": "1809.10172", "submitter": "Joseph McGrath", "authors": "Joseph McGrath, Kevin W. Bowyer, Adam Czajka", "title": "Open Source Presentation Attack Detection Baseline for Iris Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the first, known to us, open source presentation attack\ndetection (PAD) solution to distinguish between authentic iris images (possibly\nwearing clear contact lenses) and irises with textured contact lenses. This\nsoftware can serve as a baseline in various PAD evaluations, and also as an\nopen-source platform with an up-to-date reference method for iris PAD. The\nsoftware is written in C++ and Python and uses only open source resources, such\nas OpenCV. This method does not incorporate iris image segmentation, which may\nbe problematic for unknown fake samples. Instead, it makes a best guess to\nlocalize the rough position of the iris. The PAD-related features are extracted\nwith the Binary Statistical Image Features (BSIF), which are classified by an\nensemble of classifiers incorporating support vector machine, random forest and\nmultilayer perceptron. The models attached to the current software have been\ntrained with the NDCLD'15 database and evaluated on the independent datasets\nincluded in the LivDet-Iris 2017 competition. The software implements the\nfunctionality of retraining the classifiers with any database of authentic and\nattack images. The accuracy of the current version offered with this paper\nexceeds 99% when tested on subject-disjoint subsets of NDCLD'15, and oscillates\naround 85% when tested on the LivDet-Iris 2017 benchmarks, which is on par with\nthe results obtained by the LivDet-Iris 2017 winner.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 18:11:01 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 03:46:52 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["McGrath", "Joseph", ""], ["Bowyer", "Kevin W.", ""], ["Czajka", "Adam", ""]]}, {"id": "1809.10190", "submitter": "Suraj Kothawade", "authors": "Suraj Kothawade, Kunjan Mhaske, Sahil Sharma, Furkhan Shaikh", "title": "Content Based Image Retrieval from AWiFS Images Repository of IRS\n  Resourcesat-2 Satellite Based on Water Bodies and Burnt Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite Remote Sensing Technology is becoming a major milestone in the\nprediction of weather anomalies, natural disasters as well as finding\nalternative resources in proximity using multiple multi-spectral sensors\nemitting electromagnetic waves at distinct wavelengths. Hence, it is imperative\nto extract water bodies and burnt areas from orthorectified tiles and\ncorrespondingly rank them using similarity measures. Different objects in all\nthe spheres of the earth have the inherent capability of absorbing\nelectromagnetic waves of distant wavelengths. This creates various unique masks\nin terms of reflectance on the receptor. We propose Dynamic Semantic\nSegmentation (DSS) algorithms that utilized the mentioned capability to extract\nand rank Advanced Wide Field Sensor (AWiFS) images according to various\nfeatures. This system stores data intelligently in the form of a sparse feature\nvector which drastically mitigates the computational and spatial costs incurred\nfor further analysis. The compressed source image is divided into chunks and\nstored in the database for quicker retrieval. This work is intended to utilize\nreadily available and cost effective resources like AWiFS dataset instead of\ndepending on advanced technologies like Moderate Resolution Imaging\nSpectroradiometer (MODIS) for data which is scarce.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 18:58:24 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Kothawade", "Suraj", ""], ["Mhaske", "Kunjan", ""], ["Sharma", "Sahil", ""], ["Shaikh", "Furkhan", ""]]}, {"id": "1809.10196", "submitter": "Yutao Ma", "authors": "Yutao Ma, Tao Xu, Xiaolei Huang, Xiaofang Wang, Canyu Li, Jason\n  Jerwick, Yuan Ning, Xianxu Zeng, Baojin Wang, Yihong Wang, Zhan Zhang, Xiaoan\n  Zhang, Chao Zhou", "title": "Computer-Aided Diagnosis of Label-Free 3-D Optical Coherence Microscopy\n  Images of Human Cervical Tissue", "comments": "9 pages, 5 figures, and 2 tables", "journal-ref": "IEEE Transactions on Biomedical Engineering, 2019, 66(9):\n  2447-2456", "doi": "10.1109/TBME.2018.2890167", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Ultrahigh-resolution optical coherence microscopy (OCM) has\nrecently demonstrated its potential for accurate diagnosis of human cervical\ndiseases. One major challenge for clinical adoption, however, is the steep\nlearning curve clinicians need to overcome to interpret OCM images. Developing\nan intelligent technique for computer-aided diagnosis (CADx) to accurately\ninterpret OCM images will facilitate clinical adoption of the technology and\nimprove patient care. Methods: 497 high-resolution 3-D OCM volumes (600\ncross-sectional images each) were collected from 159 ex vivo specimens of 92\nfemale patients. OCM image features were extracted using a convolutional neural\nnetwork (CNN) model, concatenated with patient information (e.g., age, HPV\nresults), and classified using a support vector machine classifier. Ten-fold\ncross-validations were utilized to test the performance of the CADx method in a\nfive-class classification task and a binary classification task. Results: An\n88.3 plus or minus 4.9% classification accuracy was achieved for five\nfine-grained classes of cervical tissue, namely normal, ectropion, low-grade\nand high-grade squamous intraepithelial lesions (LSIL and HSIL), and cancer. In\nthe binary classification task (low-risk [normal, ectropion and LSIL] vs.\nhigh-risk [HSIL and cancer]), the CADx method achieved an area-under-the-curve\n(AUC) value of 0.959 with an 86.7 plus or minus 11.4% sensitivity and 93.5 plus\nor minus 3.8% specificity. Conclusion: The proposed deep-learning based CADx\nmethod outperformed three human experts. It was also able to identify\nmorphological characteristics in OCM images that were consistent with\nhistopathological interpretations. Significance: Label-free OCM imaging,\ncombined with deep-learning based CADx methods, hold a great promise to be used\nin clinical settings for the effective screening and diagnosis of cervical\ndiseases.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 16:45:13 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Ma", "Yutao", ""], ["Xu", "Tao", ""], ["Huang", "Xiaolei", ""], ["Wang", "Xiaofang", ""], ["Li", "Canyu", ""], ["Jerwick", "Jason", ""], ["Ning", "Yuan", ""], ["Zeng", "Xianxu", ""], ["Wang", "Baojin", ""], ["Wang", "Yihong", ""], ["Zhang", "Zhan", ""], ["Zhang", "Xiaoan", ""], ["Zhou", "Chao", ""]]}, {"id": "1809.10198", "submitter": "Xiaolong Liu", "authors": "Xiaolong Liu, Zhidong Deng, Yuhan Yang", "title": "Recent progress in semantic image segmentation", "comments": "Pubulished at Artificial Intelligence review", "journal-ref": "Liu, Xiaolong, Zhidong Deng, and Yuhan Yang. \"Recent progress in\n  semantic image segmentation.\" Artificial Intelligence Review (2018): 1-18", "doi": "10.1007/s10462-018-9641-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation, which becomes one of the key applications in\nimage processing and computer vision domain, has been used in multiple domains\nsuch as medical area and intelligent transportation. Lots of benchmark datasets\nare released for researchers to verify their algorithms. Semantic segmentation\nhas been studied for many years. Since the emergence of Deep Neural Network\n(DNN), segmentation has made a tremendous progress. In this paper, we divide\nsemantic image segmentation methods into two categories: traditional and recent\nDNN method. Firstly, we briefly summarize the traditional method as well as\ndatasets released for segmentation, then we comprehensively investigate recent\nmethods based on DNN which are described in the eight aspects: fully\nconvolutional network, upsample ways, FCN joint with CRF methods, dilated\nconvolution approaches, progresses in backbone network, pyramid methods,\nMulti-level feature and multi-stage method, supervised, weakly-supervised and\nunsupervised methods. Finally, a conclusion in this area is drawn.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 03:20:42 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Liu", "Xiaolong", ""], ["Deng", "Zhidong", ""], ["Yang", "Yuhan", ""]]}, {"id": "1809.10199", "submitter": "Junqiao Zhao", "authors": "Lu Sun, Junqiao Zhao, Xudong He, Chen Ye", "title": "DLO: Direct LiDAR Odometry for 2.5D Outdoor Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For autonomous vehicles, high-precision real-time localization is the\nguarantee of stable driving. Compared with the visual odometry (VO), the LiDAR\nodometry (LO) has the advantages of higher accuracy and better stability.\nHowever, 2D LO is only suitable for the indoor environment, and 3D LO has less\nefficiency in general. Both are not suitable for the online localization of an\nautonomous vehicle in an outdoor driving environment. In this paper, a direct\nLO method based on the 2.5D grid map is proposed. The fast semi-dense direct\nmethod proposed for VO is employed to register two 2.5D maps. Experiments show\nthat this method is superior to both the 3D-NDT and LOAM in the outdoor\nenvironment.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 01:20:06 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Sun", "Lu", ""], ["Zhao", "Junqiao", ""], ["He", "Xudong", ""], ["Ye", "Chen", ""]]}, {"id": "1809.10200", "submitter": "Edouard Oyallon", "authors": "Edouard Oyallon and Eugene Belilovsky and Sergey Zagoruyko and Michal\n  Valko", "title": "Compressing the Input for CNNs with the First-Order Scattering Transform", "comments": null, "journal-ref": "ECCV 2018", "doi": "10.1007/978-3-030-01240-3_19", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the first-order scattering transform as a candidate for reducing the\nsignal processed by a convolutional neural network (CNN). We show theoretical\nand empirical evidence that in the case of natural images and sufficiently\nsmall translation invariance, this transform preserves most of the signal\ninformation needed for classification while substantially reducing the spatial\nresolution and total signal size. We demonstrate that cascading a CNN with this\nrepresentation performs on par with ImageNet classification models, commonly\nused in downstream tasks, such as the ResNet-50. We subsequently apply our\ntrained hybrid ImageNet model as a base model on a detection system, which has\ntypically larger image inputs. On Pascal VOC and COCO detection tasks we\ndemonstrate improvements in the inference speed and training memory consumption\ncompared to models trained directly on the input image.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 10:14:46 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Oyallon", "Edouard", ""], ["Belilovsky", "Eugene", ""], ["Zagoruyko", "Sergey", ""], ["Valko", "Michal", ""]]}, {"id": "1809.10201", "submitter": "Youya Xia", "authors": "Youya Xia and Junaed Sattar", "title": "Visual Diver Recognition for Underwater Human-Robot Collaboration", "comments": "submitted for ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for autonomous underwater robots to visually\ndetect and identify divers. The proposed approach enables an autonomous\nunderwater robot to detect multiple divers in a visual scene and distinguish\nbetween them. Such methods are useful for robots to identify a human leader,\nfor example, in multi-human/robot teams where only designated individuals are\nallowed to command or lean a team of robots. Initial diver identification is\nperformed using the Faster R-CNN algorithm with a region proposal network which\nproduces bounding boxes around the divers' locations. Subsequently, a suite of\nspatial and frequency domain descriptors are extracted from the bounding boxes\nto create a feature vector. A K-Means clustering algorithm, with k set to the\nnumber of detected bounding boxes, thereafter identifies the detected divers\nbased on these feature vectors. We evaluate the performance of the proposed\napproach on video footage of divers swimming in front of a mobile robot and\ndemonstrate its accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 20:21:46 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Xia", "Youya", ""], ["Sattar", "Junaed", ""]]}, {"id": "1809.10203", "submitter": "Defeng Chen", "authors": "Han Kang, Defeng Chen", "title": "Multi-Scale Fully Convolutional Network for Cardiac Left Ventricle\n  Segmentation", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The morphological structure of left ventricle segmented from cardiac magnetic\nresonance images can be used to calculate key clinical parameters, and it is of\ngreat significance to the accurate and efficient diagnosis of cardiovascular\ndiseases. Compared with traditional methods, the segmentation algorithms based\non fully convolutional neural network greatly improve the accuracy of semantic\nsegmentation. For the problem of left ventricular segmentation, a new fully\nconvolutional neural network structure named MS-FCN is proposed in this paper.\nThe MS-FCN network employs a multi-scale pooling module to ensure that the\nnetwork maximises the feature extraction ability and uses a dense connectivity\ndecoder to refine the boundaries of the object. Based on the Sunnybrook cine-MR\ndataset provided by the MICCAI 2009 challenge, numerical experiments\ndemonstrate that our proposed model has obtained state-of-the-art segmentation\nresults: the Dice score of our method reaches 0.93 on the endocardium, and 0.96\non the epicardium.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:13:05 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Kang", "Han", ""], ["Chen", "Defeng", ""]]}, {"id": "1809.10221", "submitter": "Shusil Dangi", "authors": "Shusil Dangi, Ziv Yaniv, and Cristian A. Linte", "title": "Left Ventricle Segmentation and Quantification from Cardiac Cine MR\n  Images via Multi-task Learning", "comments": "STACOM 2018 Workshop, MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the left ventricle and quantification of various cardiac\ncontractile functions is crucial for the timely diagnosis and treatment of\ncardiovascular diseases. Traditionally, the two tasks have been tackled\nindependently. Here we propose a convolutional neural network based multi-task\nlearning approach to perform both tasks simultaneously, such that, the network\nlearns better representation of the data with improved generalization\nperformance. Probabilistic formulation of the problem enables learning the task\nuncertainties during the training, which are used to automatically compute the\nweights for the tasks. We performed a five fold cross-validation of the\nmyocardium segmentation obtained from the proposed multi-task network on 97\npatient 4-dimensional cardiac cine-MRI datasets available through the STACOM LV\nsegmentation challenge against the provided gold-standard myocardium\nsegmentation, obtaining a Dice overlap of $0.849 \\pm 0.036$ and mean surface\ndistance of $0.274 \\pm 0.083$ mm, while simultaneously estimating the\nmyocardial area with mean absolute difference error of $205\\pm198$ mm$^2$.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 20:37:40 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Dangi", "Shusil", ""], ["Yaniv", "Ziv", ""], ["Linte", "Cristian A.", ""]]}, {"id": "1809.10229", "submitter": "Gabriel Dahia", "authors": "Gabriel Dahia, Maur\\'icio Pamplona Segundo", "title": "Automatic Dataset Annotation to Learn CNN Pore Description for\n  Fingerprint Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution fingerprint recognition often relies on sophisticated\nmatching algorithms based on hand-crafted keypoint descriptors, with pores\nbeing the most common keypoint choice. Our method is the opposite of the\nprevalent approach: we use instead a simple matching algorithm based on robust\nlocal pore descriptors that are learned from the data using a CNN. In order to\ntrain this CNN in a fully supervised manner, we describe how the automatic\nalignment of fingerprint images can be used to obtain the required training\nannotations, which are otherwise missing in all publicly available datasets.\nThis improves the state-of-the-art recognition results for both partial and\nfull fingerprints in a public benchmark. To confirm that the observed\nimprovement is due to the adoption of learned descriptors, we conduct an\nablation study using the most successful pore descriptors previously used in\nthe literature. All our code is available at\nhttps://github.com/gdahia/high-res-fingerprint-recognition\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 20:50:23 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 00:08:54 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 20:49:37 GMT"}, {"version": "v4", "created": "Thu, 22 Nov 2018 13:13:02 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Dahia", "Gabriel", ""], ["Segundo", "Maur\u00edcio Pamplona", ""]]}, {"id": "1809.10236", "submitter": "Burak Uzkent", "authors": "Evan Sheehan, Burak Uzkent, Chenlin Meng, Zhongyi Tang, Marshall\n  Burke, David Lobell, Stefano Ermon", "title": "Learning to Interpret Satellite Images Using Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress in computer vision, fine-grained interpretation of\nsatellite images remains challenging because of a lack of labeled training\ndata. To overcome this limitation, we propose using Wikipedia as a previously\nuntapped source of rich, georeferenced textual information with global\ncoverage. We construct a novel large-scale, multi-modal dataset by pairing\ngeo-referenced Wikipedia articles with satellite imagery of their corresponding\nlocations. To prove the efficacy of this dataset, we focus on the African\ncontinent and train a deep network to classify images based on labels extracted\nfrom articles. We then fine-tune the model on a human annotated dataset and\ndemonstrate that this weak form of supervision can drastically reduce the\nquantity of human annotated labels and time required for downstream tasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 21:58:14 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Sheehan", "Evan", ""], ["Uzkent", "Burak", ""], ["Meng", "Chenlin", ""], ["Tang", "Zhongyi", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "1809.10237", "submitter": "Jiachen Li", "authors": "Jiachen Li, Wei Zhan and Masayoshi Tomizuka", "title": "Generic Vehicle Tracking Framework Capable of Handling Occlusions Based\n  on Modified Mixture Particle Filter", "comments": "Presented in 2018 IEEE Intelligent Vehicles Symposium (IV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust tracking of surrounding road participants plays an\nimportant role in autonomous driving. However, there is usually no prior\nknowledge of the number of tracking targets due to object emergence, object\ndisappearance and false alarms. To overcome this challenge, we propose a\ngeneric vehicle tracking framework based on modified mixture particle filter,\nwhich can make the number of tracking targets adaptive to real-time\nobservations and track all the vehicles within sensor range simultaneously in a\nuniform architecture without explicit data association. Each object corresponds\nto a mixture component whose distribution is non-parametric and approximated by\nparticle hypotheses. Most tracking approaches employ vehicle kinematic models\nas the prediction model. However, it is hard for these models to make proper\npredictions when sensor measurements are lost or become low quality due to\npartial or complete occlusions. Moreover, these models are incapable of\nforecasting sudden maneuvers. To address these problems, we propose to\nincorporate learning-based behavioral models instead of pure vehicle kinematic\nmodels to realize prediction in the prior update of recursive Bayesian state\nestimation. Two typical driving scenarios including lane keeping and lane\nchange are demonstrated to verify the effectiveness and accuracy of the\nproposed framework as well as the advantages of employing learning-based\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 05:27:47 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Li", "Jiachen", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1809.10238", "submitter": "Joseph K J", "authors": "K J Joseph, Arghya Pal, Sailaja Rajanala, Vineeth N Balasubramanian", "title": "C4Synth: Cross-Caption Cycle-Consistent Text-to-Image Synthesis", "comments": "To appear in the proceedings of IEEE Winter Conference on\n  Applications of Computer Vision, WACV-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating an image from its description is a challenging task worth solving\nbecause of its numerous practical applications ranging from image editing to\nvirtual reality. All existing methods use one single caption to generate a\nplausible image. A single caption by itself, can be limited, and may not be\nable to capture the variety of concepts and behavior that may be present in the\nimage. We propose two deep generative models that generate an image by making\nuse of multiple captions describing it. This is achieved by ensuring\n'Cross-Caption Cycle Consistency' between the multiple captions and the\ngenerated image(s). We report quantitative and qualitative results on the\nstandard Caltech-UCSD Birds (CUB) and Oxford-102 Flowers datasets to validate\nthe efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 07:18:57 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Joseph", "K J", ""], ["Pal", "Arghya", ""], ["Rajanala", "Sailaja", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1809.10239", "submitter": "Berta Besc\\'os Torcal", "authors": "Berta Bescos, Jos\\'e Neira, Roland Siegwart, Cesar Cadena", "title": "Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space", "comments": "Accepted for Publication at IEEE International Conference on Robotics\n  and Automation (ICRA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we present an end-to-end deep learning framework to turn images\nthat show dynamic content, such as vehicles or pedestrians, into realistic\nstatic frames. This objective encounters two main challenges: detecting all the\ndynamic objects, and inpainting the static occluded background with plausible\nimagery. The second problem is approached with a conditional generative\nadversarial model that, taking as input the original dynamic image and its\ndynamic/static binary mask, is capable of generating the final static image.\nThe former challenge is addressed by the use of a convolutional network that\nlearns a multi-class semantic segmentation of the image.\n  These generated images can be used for applications such as augmented reality\nor vision-based robot localization purposes. To validate our approach, we show\nboth qualitative and quantitative comparisons against other state-of-the-art\ninpainting methods by removing the dynamic objects and hallucinating the static\nstructure behind them. Furthermore, to demonstrate the potential of our\nresults, we carry out pilot experiments that show the benefits of our proposal\nfor visual place recognition.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 08:13:52 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 09:36:18 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Bescos", "Berta", ""], ["Neira", "Jos\u00e9", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "1809.10240", "submitter": "Guanghua Xiao", "authors": "Shidan Wang, Tao Wang, Lin Yang, Faliu Yi, Xin Luo, Yikun Yang, Adi\n  Gazdar, Junya Fujimoto, Ignacio I. Wistuba, Bo Yao, ShinYi Lin, Yang Xie,\n  Yousheng Mao, Guanghua Xiao", "title": "ConvPath: A Software Tool for Lung Adenocarcinoma Digital Pathological\n  Image Analysis Aided by Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial distributions of different types of cells could reveal a cancer\ncell growth pattern, its relationships with the tumor microenvironment and the\nimmune response of the body, all of which represent key hallmarks of cancer.\nHowever, manually recognizing and localizing all the cells in pathology slides\nare almost impossible. In this study, we developed an automated cell type\nclassification pipeline, ConvPath, which includes nuclei segmentation,\nconvolutional neural network-based tumor, stromal and lymphocytes\nclassification, and extraction of tumor microenvironment related features for\nlung cancer pathology images. The overall classification accuracy is 92.9% and\n90.1% in training and independent testing datasets, respectively. By\nidentifying cells and classifying cell types, this pipeline can convert a\npathology image into a spatial map of tumor, stromal and lymphocyte cells. From\nthis spatial map, we can extracted features that characterize the tumor\nmicro-environment. Based on these features, we developed an image feature-based\nprognostic model and validated the model in two independent cohorts. The\npredicted risk group serves as an independent prognostic factor, after\nadjusting for clinical variables that include age, gender, smoking status, and\nstage.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 13:31:51 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Wang", "Shidan", ""], ["Wang", "Tao", ""], ["Yang", "Lin", ""], ["Yi", "Faliu", ""], ["Luo", "Xin", ""], ["Yang", "Yikun", ""], ["Gazdar", "Adi", ""], ["Fujimoto", "Junya", ""], ["Wistuba", "Ignacio I.", ""], ["Yao", "Bo", ""], ["Lin", "ShinYi", ""], ["Xie", "Yang", ""], ["Mao", "Yousheng", ""], ["Xiao", "Guanghua", ""]]}, {"id": "1809.10241", "submitter": "Shanshan Wang", "authors": "Jingxu Xu, Cheng Li, Yongjin Zhou, Lisha Mou, Hairong Zheng, and\n  Shanshan Wang", "title": "Classifying Mammographic Breast Density by Residual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammographic breast density, a parameter used to describe the proportion of\nbreast tissue fibrosis, is widely adopted as an evaluation characteristic of\nthe likelihood of breast cancer incidence. In this study, we present a\nradiomics approach based on residual learning for the classification of\nmammographic breast densities. Our method possesses several encouraging\nproperties such as being almost fully automatic, possessing big model capacity\nand flexibility. It can obtain outstanding classification results without the\nnecessity of result compensation using mammographs taken from different views.\nThe proposed method was instantiated with the INbreast dataset and\nclassification accuracies of 92.6% and 96.8% were obtained for the four BI-RADS\n(Breast Imaging and Reporting Data System) category task and the two BI-RADS\ncategory task,respectively. The superior performances achieved compared to the\nexisting state-of-the-art methods along with its encouraging properties\nindicate that our method has a great potential to be applied as a\ncomputer-aided diagnosis tool.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 06:29:50 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Xu", "Jingxu", ""], ["Li", "Cheng", ""], ["Zhou", "Yongjin", ""], ["Mou", "Lisha", ""], ["Zheng", "Hairong", ""], ["Wang", "Shanshan", ""]]}, {"id": "1809.10242", "submitter": "Zhujun Xiao", "authors": "Zhujun Xiao, Yanzi Zhu, Yuxin Chen, Ben Y. Zhao, Junchen Jiang, Haitao\n  Zheng", "title": "Addressing Training Bias via Automated Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Build accurate DNN models requires training on large labeled, context\nspecific datasets, especially those matching the target scenario. We believe\nadvances in wireless localization, working in unison with cameras, can produce\nautomated annotation of targets on images and videos captured in the wild.\nUsing pedestrian and vehicle detection as examples, we demonstrate the\nfeasibility, benefits, and challenges of an automatic image annotation system.\nOur work calls for new technical development on passive localization, mobile\ndata analytics, and error-resilient ML models, as well as design issues in user\nprivacy policies.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 19:47:01 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 12:09:25 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Xiao", "Zhujun", ""], ["Zhu", "Yanzi", ""], ["Chen", "Yuxin", ""], ["Zhao", "Ben Y.", ""], ["Jiang", "Junchen", ""], ["Zheng", "Haitao", ""]]}, {"id": "1809.10243", "submitter": "Navid Alemi Koohbanani", "authors": "Mostafa Jahanifar, Neda Zamani Tajeddin, Navid Alemi Koohbanani, Ali\n  Gooya, and Nasir Rajpoot", "title": "Segmentation of Skin Lesions and their Attributes Using Multi-Scale\n  Convolutional Neural Networks and Domain Specific Augmentations", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided diagnosis systems for classification of different type of skin\nlesions have been an active field of research in recent decades. It has been\nshown that introducing lesions and their attributes masks into lesion\nclassification pipeline can greatly improve the performance. In this paper, we\npropose a framework by incorporating transfer learning for segmenting lesions\nand their attributes based on the convolutional neural networks. The proposed\nframework is based on the encoder-decoder architecture which utilizes a variety\nof pre-trained networks in the encoding path and generates the prediction map\nby combining multi-scale information in decoding path using a pyramid pooling\nmanner. To address the lack of training data and increase the proposed model\ngeneralization, an extensive set of novel domain-specific augmentation routines\nhave been applied to simulate the real variations in dermoscopy images.\nFinally, by performing broad experiments on three different data sets obtained\nfrom International Skin Imaging Collaboration archive (ISIC2016, ISIC2017, and\nISIC2018 challenges data sets), we show that the proposed method outperforms\nother state-of-the-art approaches for ISIC2016 and ISIC2017 segmentation task\nand achieved the first rank on the leader-board of ISIC2018 attribute detection\ntask.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 18:01:14 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 02:16:00 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 09:12:34 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Jahanifar", "Mostafa", ""], ["Tajeddin", "Neda Zamani", ""], ["Koohbanani", "Navid Alemi", ""], ["Gooya", "Ali", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1809.10244", "submitter": "Yantao Lu", "authors": "Yantao Lu and Burak Kakillioglu and Senem Velipasalar", "title": "Autonomously and Simultaneously Refining Deep Neural Network Parameters\n  by a Bi-Generative Adversarial Network Aided Genetic Algorithm", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.09712", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The choice of parameters, and the design of the network architecture are\nimportant factors affecting the performance of deep neural networks. Genetic\nAlgorithms (GA) have been used before to determine parameters of a network.\nYet, GAs perform a finite search over a discrete set of pre-defined candidates,\nand cannot, in general, generate unseen configurations. In this paper, to move\nfrom exploration to exploitation, we propose a novel and systematic method that\nautonomously and simultaneously optimizes multiple parameters of any deep\nneural network by using a GA aided by a bi-generative adversarial network\n(Bi-GAN). The proposed Bi-GAN allows the autonomous exploitation and choice of\nthe number of neurons, for fully-connected layers, and number of filters, for\nconvolutional layers, from a large range of values. Our proposed Bi-GAN\ninvolves two generators, and two different models compete and improve each\nother progressively with a GAN-based strategy to optimize the networks during\nGA evolution. Our proposed approach can be used to autonomously refine the\nnumber of convolutional layers and dense layers, number and size of kernels,\nand the number of neurons for the dense layers; choose the type of the\nactivation function; and decide whether to use dropout and batch normalization\nor not, to improve the accuracy of different deep neural network architectures.\nWithout loss of generality, the proposed method has been tested with the\nModelNet database, and compared with the 3D Shapenets and two GA-only methods.\nThe results show that the presented approach can simultaneously and\nsuccessfully optimize multiple neural network parameters, and achieve higher\naccuracy even with shallower networks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 15:43:17 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Lu", "Yantao", ""], ["Kakillioglu", "Burak", ""], ["Velipasalar", "Senem", ""]]}, {"id": "1809.10245", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Sumeya Naqvi, Errol Colak, Joseph Barfett,\n  Shahrokh Valaee", "title": "Cylindrical Transform: 3D Semantic Segmentation of Kidneys With Limited\n  Annotated Images", "comments": "This paper is accepted for presentation at IEEE Global Conference on\n  Signal and Information Processing (IEEE GlobalSIP), California, USA, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel technique for sampling sequential images\nusing a cylindrical transform in a cylindrical coordinate system for kidney\nsemantic segmentation in abdominal computed tomography (CT). The images\ngenerated from a cylindrical transform augment a limited annotated set of\nimages in three dimensions. This approach enables us to train contemporary\nclassification deep convolutional neural networks (DCNNs) instead of fully\nconvolutional networks (FCNs) for semantic segmentation. Typical semantic\nsegmentation models segment a sequential set of images (e.g. CT or video) by\nsegmenting each image independently. However, the proposed method not only\nconsiders the spatial dependency in the x-y plane, but also the spatial\nsequential dependency along the z-axis. The results show that classification\nDCNNs, trained on cylindrical transformed images, can achieve a higher\nsegmentation performance value than FCNs using a limited number of annotated\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 17:58:54 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Naqvi", "Sumeya", ""], ["Colak", "Errol", ""], ["Barfett", "Joseph", ""], ["Valaee", "Shahrokh", ""]]}, {"id": "1809.10260", "submitter": "Chi Zhang", "authors": "Chi Zhang, Alexander Loui", "title": "A Coarse-To-Fine Framework For Video Object Segmentation", "comments": "6 pages, 11 figures, to appear in Electronic Imaging, Visual\n  Information Processing and Communication VIII, pp. 32-37(6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we develop an unsupervised coarse-to-fine video analysis\nframework and prototype system to extract a salient object in a video sequence.\nThis framework starts from tracking grid-sampled points along temporal frames,\ntypically using KLT tracking method. The tracking points could be divided into\nseveral groups due to their inconsistent movements. At the same time, the SLIC\nalgorithm is extended into 3D space to generate supervoxels. Coarse\nsegmentation is achieved by combining the categorized tracking points and\nsupervoxels of the corresponding frame in the video sequence. Finally, a\ngraph-based fine segmentation algorithm is used to extract the moving object in\nthe scene. Experimental results reveal that this method outperforms the\nprevious approaches in terms of accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 22:55:10 GMT"}], "update_date": "2018-09-30", "authors_parsed": [["Zhang", "Chi", ""], ["Loui", "Alexander", ""]]}, {"id": "1809.10274", "submitter": "Chi Zhang", "authors": "Shagan Sah, Dheeraj Peri, Ameya Shringi, Chi Zhang, Miguel Dominguez,\n  Andreas Savakis, Ray Ptucha", "title": "Semantically Invariant Text-to-Image Generation", "comments": "5 papers, 5 figures, Published in 2018 25th IEEE International\n  Conference on Image Processing (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2018.8451656", "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning has demonstrated models that are capable of generating\nplausible text given input images or videos. Further, recent work in image\ngeneration has shown significant improvements in image quality when text is\nused as a prior. Our work ties these concepts together by creating an\narchitecture that can enable bidirectional generation of images and text. We\ncall this network Multi-Modal Vector Representation (MMVR). Along with MMVR, we\npropose two improvements to the text conditioned image generation. Firstly, a\nn-gram metric based cost function is introduced that generalizes the caption\nwith respect to the image. Secondly, multiple semantically similar sentences\nare shown to help in generating better images. Qualitative and quantitative\nevaluations demonstrate that MMVR improves upon existing text conditioned image\ngeneration results by over 20%, while integrating visual and text modalities.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 00:11:25 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Sah", "Shagan", ""], ["Peri", "Dheeraj", ""], ["Shringi", "Ameya", ""], ["Zhang", "Chi", ""], ["Dominguez", "Miguel", ""], ["Savakis", "Andreas", ""], ["Ptucha", "Ray", ""]]}, {"id": "1809.10280", "submitter": "Albert Pumarola", "authors": "Albert Pumarola, Antonio Agudo, Alberto Sanfeliu and Francesc\n  Moreno-Noguer", "title": "Unsupervised Person Image Synthesis in Arbitrary Poses", "comments": "Accepted as Spotlight at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for synthesizing photo-realistic images of people\nin arbitrary poses using generative adversarial learning. Given an input image\nof a person and a desired pose represented by a 2D skeleton, our model renders\nthe image of the same person under the new pose, synthesizing novel views of\nthe parts visible in the input image and hallucinating those that are not seen.\nThis problem has recently been addressed in a supervised manner, i.e., during\ntraining the ground truth images under the new poses are given to the network.\nWe go beyond these approaches by proposing a fully unsupervised strategy. We\ntackle this challenging scenario by splitting the problem into two principal\nsubtasks. First, we consider a pose conditioned bidirectional generator that\nmaps back the initially rendered image to the original pose, hence being\ndirectly comparable to the input image without the need to resort to any\ntraining image. Second, we devise a novel loss function that incorporates\ncontent and style terms, and aims at producing images of high perceptual\nquality. Extensive experiments conducted on the DeepFashion dataset demonstrate\nthat the images rendered by our model are very close in appearance to those\nobtained by fully supervised approaches.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 00:32:22 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Pumarola", "Albert", ""], ["Agudo", "Antonio", ""], ["Sanfeliu", "Alberto", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1809.10305", "submitter": "Albert Pumarola", "authors": "Albert Pumarola, Antonio Agudo, Lorenzo Porzi, Alberto Sanfeliu,\n  Vincent Lepetit and Francesc Moreno-Noguer", "title": "Geometry-Aware Network for Non-Rigid Shape Prediction from a Single View", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for predicting the 3D shape of a deformable surface from\na single view. By contrast with previous approaches, we do not need a\npre-registered template of the surface, and our method is robust to the lack of\ntexture and partial occlusions. At the core of our approach is a {\\it\ngeometry-aware} deep architecture that tackles the problem as usually done in\nanalytic solutions: first perform 2D detection of the mesh and then estimate a\n3D shape that is geometrically consistent with the image. We train this\narchitecture in an end-to-end manner using a large dataset of synthetic\nrenderings of shapes under different levels of deformation, material\nproperties, textures and lighting conditions. We evaluate our approach on a\ntest split of this dataset and available real benchmarks, consistently\nimproving state-of-the-art solutions with a significantly lower computational\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 01:44:11 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Pumarola", "Albert", ""], ["Agudo", "Antonio", ""], ["Porzi", "Lorenzo", ""], ["Sanfeliu", "Alberto", ""], ["Lepetit", "Vincent", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1809.10328", "submitter": "Vladimir Nekrasov", "authors": "Vladimir Nekrasov, Chunhua Shen, Ian Reid", "title": "Diagnostics in Semantic Segmentation", "comments": "Supplementary Material: https://cv-conf.shinyapps.io/diag-sem-segm/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, computer vision community has contributed to enormous\nprogress in semantic image segmentation, a per-pixel classification task,\ncrucial for dense scene understanding and rapidly becoming vital in lots of\nreal-world applications, including driverless cars and medical imaging. Most\nrecent models are now reaching previously unthinkable numbers (e.g., 89% mean\niou on PASCAL VOC, 83% on CityScapes), and, while intersection-over-union and a\nrange of other metrics provide the general picture of model performance, in\nthis paper we aim to extend them into other meaningful and important for\napplications characteristics, answering such questions as 'how accurate the\nmodel segmentation is on small objects in the general scene?', or 'what are the\nsources of uncertainty that cause the model to make an erroneous prediction?'.\nBesides establishing a methodology that covers the performance of a single\nmodel from different perspectives, we also showcase several extensions that can\nbe worth pursuing in order to further improve current results in semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 03:24:49 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Nekrasov", "Vladimir", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1809.10352", "submitter": "Tahmida Mahmud", "authors": "Tahmida Mahmud, Mohammad Billah, Amit K. Roy-Chowdhury", "title": "Multi-View Frame Reconstruction with Conditional GAN", "comments": "5 pages, 4 figures, 3 tables, Accepted at IEEE Global Conference on\n  Signal and Information Processing, 2018", "journal-ref": null, "doi": "10.1109/GlobalSIP.2018.8646380", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view frame reconstruction is an important problem particularly when\nmultiple frames are missing and past and future frames within the camera are\nfar apart from the missing ones. Realistic coherent frames can still be\nreconstructed using corresponding frames from other overlapping cameras. We\npropose an adversarial approach to learn the spatio-temporal representation of\nthe missing frame using conditional Generative Adversarial Network (cGAN). The\nconditional input to each cGAN is the preceding or following frames within the\ncamera or the corresponding frames in other overlapping cameras, all of which\nare merged together using a weighted average. Representations learned from\nframes within the camera are given more weight compared to the ones learned\nfrom other cameras when they are close to the missing frames and vice versa.\nExperiments on two challenging datasets demonstrate that our framework produces\ncomparable results with the state-of-the-art reconstruction method in a single\ncamera and achieves promising performance in multi-camera scenario.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 05:49:52 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Mahmud", "Tahmida", ""], ["Billah", "Mohammad", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1809.10398", "submitter": "Wenxi Liu", "authors": "Wenxi Liu, Yuanlong Yu, Chun-Yang Zhang, Genggeng Liu, Naixue Xiong", "title": "An Intelligent Extraversion Analysis Scheme from Crowd Trajectories for\n  Surveillance", "comments": "require modification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, crowd analysis is important for applications such as smart\ncities, intelligent transportation system, customer behavior prediction, and\nvisual surveillance. Understanding the characteristics of the individual motion\nin a crowd can be beneficial for social event detection and abnormal detection,\nbut it has rarely been studied. In this paper, we focus on the extraversion\nmeasure of individual motions in crowds based on trajectory data. Extraversion\nis one of typical personalities that is often observed in human crowd behaviors\nand it can reflect not only the characteristics of the individual motion, but\nalso the that of the holistic crowd motions. To our best knowledge, this is the\nfirst attempt to analyze individual extraversion of crowd motions based on\ntrajectories. To accomplish this, we first present a effective composite motion\ndescriptor, which integrates the basic individual motion information and social\nmetrics, to describe the extraversion of each individual in a crowd. The social\nmetrics consider both the neighboring distribution and their interaction\npattern. Since our major goal is to learn a universal scoring function that can\nmeasure the degrees of extraversion across varied crowd scenes, we incorporate\nand adapt the active learning technique to the relative attribute approach.\nSpecifically, we assume the social groups in any crowds contain individuals\nwith the similar degree of extraversion. Based on such assumption, we\nsignificantly reduce the computation cost by clustering and ranking the\ntrajectories actively. Finally, we demonstrate the performance of our proposed\nmethod by measuring the degree of extraversion for real individual trajectories\nin crowds and analyzing crowd scenes from a real-world dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 08:28:58 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 02:29:23 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Liu", "Wenxi", ""], ["Yu", "Yuanlong", ""], ["Zhang", "Chun-Yang", ""], ["Liu", "Genggeng", ""], ["Xiong", "Naixue", ""]]}, {"id": "1809.10410", "submitter": "Po-Yu Liu", "authors": "Po-Yu Liu, Edmund Y. Lam", "title": "Image Reconstruction Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep learning architecture that attains statistically\nsignificant improvements over traditional algorithms in Poisson image denoising\nespically when the noise is strong. Poisson noise commonly occurs in low-light\nand photon- limited settings, where the noise can be most accurately modeled by\nthe Poission distribution. Poisson noise traditionally prevails only in\nspecific fields such as astronomical imaging. However, with the booming market\nof surveillance cameras, which commonly operate in low-light environments, or\nmobile phones, which produce noisy night scene pictures due to lower-grade\nsensors, the necessity for an advanced Poisson image denoising algorithm has\nincreased. Deep learning has achieved amazing breakthroughs in other imaging\nproblems, such image segmentation and recognition, and this paper proposes a\ndeep learning denoising network that outperforms traditional algorithms in\nPoisson denoising especially when the noise is strong. The architecture\nincorporates a hybrid of convolutional and deconvolutional layers along with\nsymmetric connections. The denoising network achieved statistically significant\n0.38dB, 0.68dB, and 1.04dB average PSNR gains over benchmark traditional\nalgorithms in experiments with image peak values 4, 2, and 1. The denoising\nnetwork can also operate with shorter computational time while still\noutperforming the benchmark algorithm by tuning the reconstruction stride\nsizes.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 08:58:38 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Liu", "Po-Yu", ""], ["Lam", "Edmund Y.", ""]]}, {"id": "1809.10417", "submitter": "Wenxi Liu", "authors": "Wenxi Liu, Yibing Song, Dengsheng Chen, Shengfeng He, Yuanlong Yu, Tao\n  Yan, Gerhard P. Hancke, Rynson W.H. Lau", "title": "Deformable Object Tracking with Gated Fusion", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2019", "doi": "10.1109/TIP.2019.2902784", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tracking-by-detection framework receives growing attentions through the\nintegration with the Convolutional Neural Networks (CNNs). Existing\ntracking-by-detection based methods, however, fail to track objects with severe\nappearance variations. This is because the traditional convolutional operation\nis performed on fixed grids, and thus may not be able to find the correct\nresponse while the object is changing pose or under varying environmental\nconditions. In this paper, we propose a deformable convolution layer to enrich\nthe target appearance representations in the tracking-by-detection framework.\nWe aim to capture the target appearance variations via deformable convolution,\nwhich adaptively enhances its original features. In addition, we also propose a\ngated fusion scheme to control how the variations captured by the deformable\nconvolution affect the original appearance. The enriched feature representation\nthrough deformable convolution facilitates the discrimination of the CNN\nclassifier on the target object and background. Extensive experiments on the\nstandard benchmarks show that the proposed tracker performs favorably against\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 09:15:27 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 07:34:35 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Liu", "Wenxi", ""], ["Song", "Yibing", ""], ["Chen", "Dengsheng", ""], ["He", "Shengfeng", ""], ["Yu", "Yuanlong", ""], ["Yan", "Tao", ""], ["Hancke", "Gerhard P.", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "1809.10430", "submitter": "J\\\"org Sander", "authors": "J\\\"org Sander, Bob D. de Vos, Jelmer M. Wolterink and Ivana I\\v{s}gum", "title": "Towards increased trustworthiness of deep learning segmentation methods\n  on cardiac MRI", "comments": "This work has been submitted to SPIE 2019 conference", "journal-ref": null, "doi": "10.1117/12.2511699", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art deep learning segmentation methods have not yet made\na broad entrance into the clinical setting in spite of high demand for such\nautomatic methods. One important reason is the lack of reliability caused by\nmodels that fail unnoticed and often locally produce anatomically implausible\nresults that medical experts would not make. This paper presents an automatic\nimage segmentation method based on (Bayesian) dilated convolutional networks\n(DCNN) that generate segmentation masks and spatial uncertainty maps for the\ninput image at hand. The method was trained and evaluated using segmentation of\nthe left ventricle (LV) cavity, right ventricle (RV) endocardium and myocardium\n(Myo) at end-diastole (ED) and end-systole (ES) in 100 cardiac 2D MR scans from\nthe MICCAI 2017 Challenge (ACDC). Combining segmentations and uncertainty maps\nand employing a human-in-the-loop setting, we provide evidence that image areas\nindicated as highly uncertain regarding the obtained segmentation almost\nentirely cover regions of incorrect segmentations. The fused information can be\nharnessed to increase segmentation performance. Our results reveal that we can\nobtain valuable spatial uncertainty maps with low computational effort using\nDCNNs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 09:49:46 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 07:31:22 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 09:30:59 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Sander", "J\u00f6rg", ""], ["de Vos", "Bob D.", ""], ["Wolterink", "Jelmer M.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1809.10432", "submitter": "Yustinus Soelistio Eko", "authors": "Richard Adiguna, Yustinus Eko Soelistio", "title": "CNN Based Posture-Free Hand Detection", "comments": "4 pages, 5 figures, in The 10th International Conference on\n  Information Technology and Electrical Engineering 2018, ISBN:\n  978-1-5386-4739-4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many studies suggest high performance hand detection methods, those\nmethods are likely to be overfitting. Fortunately, the Convolution Neural\nNetwork (CNN) based approach provides a better way that is less sensitive to\ntranslation and hand poses. However the CNN approach is complex and can\nincrease computational time, which at the end reduce its effectiveness on a\nsystem where the speed is essential.In this study we propose a shallow CNN\nnetwork which is fast, and insensitive to translation and hand poses. It is\ntested on two different domains of hand datasets, and performs in relatively\ncomparable performance and faster than the other state-of-the-art hand\nCNN-based hand detection method. Our evaluation shows that the proposed shallow\nCNN network performs at 93.9% accuracy and reaches much faster speed than its\ncompetitors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 10:00:31 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Adiguna", "Richard", ""], ["Soelistio", "Yustinus Eko", ""]]}, {"id": "1809.10449", "submitter": "Reuben Farrugia", "authors": "Reuben A. Farrugia, C. Guillemot", "title": "A Simple Framework to Leverage State-Of-The-Art Single-Image\n  Super-Resolution Methods to Restore Light Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenoptic cameras offer a cost effective solution to capture light fields by\nmultiplexing multiple views on a single image sensor. However, the high angular\nresolution is achieved at the expense of reducing the spatial resolution of\neach view by orders of magnitude compared to the raw sensor image. While light\nfield super-resolution is still at an early stage, the field of single image\nsuper-resolution (SISR) has recently known significant advances with the use of\ndeep learning techniques. This paper describes a simple framework allowing us\nto leverage state-of-the-art SISR techniques into light fields, while taking\ninto account specific light field geometrical constraints. The idea is to first\ncompute a representation compacting most of the light field energy into as few\ncomponents as possible. This is achieved by aligning the light field using\noptical flows and then by decomposing the aligned light field using singular\nvalue decomposition (SVD). The principal basis captures the information that is\ncoherent across all the views, while the other basis contain the high angular\nfrequencies. Super-resolving this principal basis using an SISR method allows\nus to super-resolve all the information that is coherent across the entire\nlight field. This framework allows the proposed light field super-resolution\nmethod to inherit the benefits of the SISR method used. Experimental results\nshow that the proposed method is competitive, and most of the time superior, to\nrecent light field super-resolution methods in terms of both PSNR and SSIM\nquality metrics, with a lower complexity.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 10:54:07 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Farrugia", "Reuben A.", ""], ["Guillemot", "C.", ""]]}, {"id": "1809.10463", "submitter": "Joseph Bethge", "authors": "Joseph Bethge, Haojin Yang, Christian Bartz, Christoph Meinel", "title": "Learning to Train a Binary Neural Network", "comments": "Code: https://github.com/Jopyth/BMXNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have achieved astonishing results in different\napplication areas. Various methods which allow us to use these models on mobile\nand embedded devices have been proposed. Especially binary neural networks seem\nto be a promising approach for these devices with low computational power.\nHowever, understanding binary neural networks and training accurate models for\npractical applications remains a challenge. In our work, we focus on increasing\nour understanding of the training process and making it accessible to everyone.\nWe publish our code and models based on BMXNet for everyone to use. Within this\nframework, we systematically evaluated different network architectures and\nhyperparameters to provide useful insights on how to train a binary neural\nnetwork. Further, we present how we improved accuracy by increasing the number\nof connections in the network.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 11:40:03 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Bethge", "Joseph", ""], ["Yang", "Haojin", ""], ["Bartz", "Christian", ""], ["Meinel", "Christoph", ""]]}, {"id": "1809.10468", "submitter": "Syeda Mariam Ahmed", "authors": "Syeda Mariam Ahmed, Yan Zhi Tan, Chee Meng Chew, Abdullah Al Mamun,\n  Fook Seng Wong", "title": "Edge and Corner Detection for Unorganized 3D Point Clouds with\n  Application to Robotic Welding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose novel edge and corner detection algorithms for\nunorganized point clouds. Our edge detection method evaluates symmetry in a\nlocal neighborhood and uses an adaptive density based threshold to\ndifferentiate 3D edge points. We extend this algorithm to propose a novel\ncorner detector that clusters curvature vectors and uses their geometrical\nstatistics to classify a point as corner. We perform rigorous evaluation of the\nalgorithms on RGB-D semantic segmentation and 3D washer models from the\nShapeNet dataset and report higher precision and recall scores. Finally, we\nalso demonstrate how our edge and corner detectors can be used as a novel\napproach towards automatic weld seam detection for robotic welding. We propose\nto generate weld seams directly from a point cloud as opposed to using 3D\nmodels for offline planning of welding paths. For this application, we show a\ncomparison between Harris 3D and our proposed approach on a panel workpiece.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 11:50:58 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Ahmed", "Syeda Mariam", ""], ["Tan", "Yan Zhi", ""], ["Chew", "Chee Meng", ""], ["Mamun", "Abdullah Al", ""], ["Wong", "Fook Seng", ""]]}, {"id": "1809.10483", "submitter": "Fabian Isensee", "authors": "Fabian Isensee and Philipp Kickingereder and Wolfgang Wick and Martin\n  Bendszus and Klaus H. Maier-Hein", "title": "No New-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate the effectiveness of a well trained U-Net in the\ncontext of the BraTS 2018 challenge. This endeavour is particularly interesting\ngiven that researchers are currently besting each other with architectural\nmodifications that are intended to improve the segmentation performance. We\ninstead focus on the training process arguing that a well trained U-Net is hard\nto beat. Our baseline U-Net, which has only minor modifications and is trained\nwith a large patch size and a Dice loss function indeed achieved competitive\nDice scores on the BraTS2018 validation data. By incorporating additional\nmeasures such as region based training, additional training data, a simple\npostprocessing technique and a combination of loss functions, we obtain Dice\nscores of 77.88, 87.81 and 80.62, and Hausdorff Distances (95th percentile) of\n2.90, 6.03 and 5.08 for the enhancing tumor, whole tumor and tumor core,\nrespectively on the test data. This setup achieved rank two in BraTS2018, with\nmore than 60 teams participating in the challenge.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 12:24:27 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 07:54:38 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Isensee", "Fabian", ""], ["Kickingereder", "Philipp", ""], ["Wick", "Wolfgang", ""], ["Bendszus", "Martin", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1809.10486", "submitter": "Fabian Isensee", "authors": "Fabian Isensee and Jens Petersen and Andre Klein and David Zimmerer\n  and Paul F. Jaeger and Simon Kohl and Jakob Wasserthal and Gregor Koehler and\n  Tobias Norajitra and Sebastian Wirkert and Klaus H. Maier-Hein", "title": "nnU-Net: Self-adapting Framework for U-Net-Based Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The U-Net was presented in 2015. With its straight-forward and successful\narchitecture it quickly evolved to a commonly used benchmark in medical image\nsegmentation. The adaptation of the U-Net to novel problems, however, comprises\nseveral degrees of freedom regarding the exact architecture, preprocessing,\ntraining and inference. These choices are not independent of each other and\nsubstantially impact the overall performance. The present paper introduces the\nnnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on\nthe basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away\nsuperfluous bells and whistles of many proposed network designs and instead\nfocus on the remaining aspects that make out the performance and\ngeneralizability of a method. We evaluate the nnU-Net in the context of the\nMedical Segmentation Decathlon challenge, which measures segmentation\nperformance in ten disciplines comprising distinct entities, image modalities,\nimage geometries and dataset sizes, with no manual adjustments between datasets\nallowed. At the time of manuscript submission, nnU-Net achieves the highest\nmean dice scores across all classes and seven phase 1 tasks (except class 1 in\nBrainTumour) in the online leaderboard of the challenge.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 12:25:52 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Isensee", "Fabian", ""], ["Petersen", "Jens", ""], ["Klein", "Andre", ""], ["Zimmerer", "David", ""], ["Jaeger", "Paul F.", ""], ["Kohl", "Simon", ""], ["Wasserthal", "Jakob", ""], ["Koehler", "Gregor", ""], ["Norajitra", "Tobias", ""], ["Wirkert", "Sebastian", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1809.10499", "submitter": "Claudio Jung", "authors": "Gustavo Fuhr, Claudio Rosito Jung", "title": "Collective behavior recognition using compact descriptors", "comments": "8 pages, 6 figures. Paper submitted do Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel hierarchical approach for collective behavior\nrecognition based solely on ground-plane trajectories. In the first layer of\nour classifier, we introduce a novel feature called Personal Interaction\nDescriptor (PID), which combines the spatial distribution of a pair of\npedestrians within a temporal window with a pyramidal representation of the\nrelative speed to detect pairwise interactions. These interactions are then\ncombined with higher level features related to the mean speed and shape formed\nby the pedestrians in the scene, generating a Collective Behavior Descriptor\n(CBD) that is used to identify collective behaviors in a second stage. In both\nlayers, Random Forests were used as classifiers, since they allow features of\ndifferent natures to be combined seamlessly. Our experimental results indicate\nthat the proposed method achieves results on par with state of the art\ntechniques with a better balance of class errors. Moreover, we show that our\nmethod can generalize well across different camera setups through cross-dataset\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 13:01:48 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Fuhr", "Gustavo", ""], ["Jung", "Claudio Rosito", ""]]}, {"id": "1809.10541", "submitter": "Chao Wang", "authors": "Chao Wang, Robert Plemmons, Sudhakar Prasad, Raymond Chan and Mila\n  Nikolova", "title": "Novel Sparse Recovery Algorithms for 3D Debris Localization using\n  Rotating Point Spread Function Imagery", "comments": "16 pages. arXiv admin note: substantial text overlap with\n  arXiv:1804.04000", "journal-ref": "Proc. 2018 AMOS Technical Conference", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An optical imager that exploits off-center image rotation to encode both the\nlateral and depth coordinates of point sources in a single snapshot can perform\n3D localization and tracking of space debris. When actively illuminated,\nunresolved space debris, which can be regarded as a swarm of point sources, can\nscatter a fraction of laser irradiance back into the imaging sensor.\nDetermining the source locations and fluxes is a large-scale sparse 3D inverse\nproblem, for which we have developed efficient and effective algorithms based\non sparse recovery using non-convex optimization. Numerical simulations\nillustrate the efficiency and stability of the algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 14:35:09 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Wang", "Chao", ""], ["Plemmons", "Robert", ""], ["Prasad", "Sudhakar", ""], ["Chan", "Raymond", ""], ["Nikolova", "Mila", ""]]}, {"id": "1809.10548", "submitter": "Ankit Dhall", "authors": "Ankit Dhall", "title": "Real-time 3D Pose Estimation with a Monocular Camera Using Deep Learning\n  and Object Priors On an Autonomous Racecar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a complete pipeline that allows object detection and\nsimultaneously estimate the pose of these multiple object instances using just\na single image. A novel \"keypoint regression\" scheme with a cross-ratio term is\nintroduced that exploits prior information about the object's shape and size to\nregress and find specific feature points. Further, a priori 3D information\nabout the object is used to match 2D-3D correspondences and accurately estimate\nobject positions up to a distance of 15m. A detailed discussion of the results\nand an in-depth analysis of the pipeline is presented. The pipeline runs\nefficiently on a low-powered Jetson TX2 and is deployed as part of the\nperception pipeline on a real-time autonomous vehicle cruising at a top speed\nof 54 km/hr.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 14:45:19 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Dhall", "Ankit", ""]]}, {"id": "1809.10562", "submitter": "Corina Gurau", "authors": "Corina Gurau, Alex Bewley, Ingmar Posner", "title": "Dropout Distillation for Efficiently Estimating Model Confidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient way to output better calibrated uncertainty scores\nfrom neural networks. The Distilled Dropout Network (DDN) makes standard\n(non-Bayesian) neural networks more introspective by adding a new training loss\nwhich prevents them from being overconfident. Our method is more efficient than\nBayesian neural networks or model ensembles which, despite providing more\nreliable uncertainty scores, are more cumbersome to train and slower to test.\nWe evaluate DDN on the the task of image classification on the CIFAR-10 dataset\nand show that our calibration results are competitive even when compared to 100\nMonte Carlo samples from a dropout network while they also increase the\nclassification accuracy. We also propose better calibration within the state of\nthe art Faster R-CNN object detection framework and show, using the COCO\ndataset, that DDN helps train better calibrated object detectors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 15:02:56 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Gurau", "Corina", ""], ["Bewley", "Alex", ""], ["Posner", "Ingmar", ""]]}, {"id": "1809.10572", "submitter": "Andrew Anderson", "authors": "Andrew Anderson, David Gregg", "title": "Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ARITH.2019.00018", "report-no": null, "categories": "cs.PF cs.CV cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization of weights and activations in Deep Neural Networks (DNNs) is a\npowerful technique for network compression, and has enjoyed significant\nattention and success. However, much of the inference-time benefit of\nquantization is accessible only through the use of customized hardware\naccelerators or by providing an FPGA implementation of quantized arithmetic.\n  Building on prior work, we show how to construct arbitrary bit-precise signed\nand unsigned integer operations using a software technique which logically\n\\emph{embeds} a vector architecture with custom bit-width lanes in universally\navailable fixed-width scalar arithmetic.\n  We evaluate our approach on a high-end Intel Haswell processor, and an\nembedded ARM processor. Our approach yields very fast implementations of\nbit-precise custom DNN operations, which often match or exceed the performance\nof operations quantized to the sizes supported in native arithmetic. At the\nstrongest level of quantization, our approach yields a maximum speedup of\n$\\thicksim6\\times$ on the Intel platform, and $\\thicksim10\\times$ on the ARM\nplatform versus quantization to native 8-bit integers.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 15:25:02 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 15:39:59 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Anderson", "Andrew", ""], ["Gregg", "David", ""]]}, {"id": "1809.10582", "submitter": "Jingang Shi", "authors": "Jiahe Shi and Chun Qi", "title": "Kernel based low-rank sparse model for single image super-resolution", "comments": "27 pages, Keywords: low-rank, sparse representation, kernel method,\n  self-similarity learning, super-resolution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-similarity learning has been recognized as a promising method for single\nimage super-resolution (SR) to produce high-resolution (HR) image in recent\nyears. The performance of learning based SR reconstruction, however, highly\ndepends on learned representation coeffcients. Due to the degradation of input\nimage, conventional sparse coding is prone to produce unfaithful representation\ncoeffcients. To this end, we propose a novel kernel based low-rank sparse model\nwith self-similarity learning for single image SR which incorporates\nnonlocalsimilarity prior to enforce similar patches having similar\nrepresentation weights. We perform a gradual magnification scheme, using\nself-examples extracted from the degraded input image and up-scaled versions.\nTo exploit nonlocal-similarity, we concatenate the vectorized input patch and\nits nonlocal neighbors at different locations into a data matrix which consists\nof similar components. Then we map the nonlocal data matrix into a\nhigh-dimensional feature space by kernel method to capture their nonlinear\nstructures. Under the assumption that the sparse coeffcients for the nonlocal\ndata in the kernel space should be low-rank, we impose low-rank constraint on\nsparse coding to share similarities among representation coeffcients and remove\noutliers in order that stable weights for SR reconstruction can be obtained.\nExperimental results demonstrate the advantage of our proposed method in both\nvisual quality and reconstruction error.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 15:41:41 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Shi", "Jiahe", ""], ["Qi", "Chun", ""]]}, {"id": "1809.10589", "submitter": "Alexandre Thiery", "authors": "Sripad Krishna Devalla, Giridhar Subramanian, Tan Hung Pham, Xiaofei\n  Wang, Shamira Perera, Tin A. Tun, Tin Aung, Leopold Schmetterer, Alexandre H.\n  Thiery, Michael J. A. Girard", "title": "A Deep Learning Approach to Denoise Optical Coherence Tomography Images\n  of the Optic Nerve Head", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop a deep learning approach to de-noise optical coherence\ntomography (OCT) B-scans of the optic nerve head (ONH).\n  Methods: Volume scans consisting of 97 horizontal B-scans were acquired\nthrough the center of the ONH using a commercial OCT device (Spectralis) for\nboth eyes of 20 subjects. For each eye, single-frame (without signal\naveraging), and multi-frame (75x signal averaging) volume scans were obtained.\nA custom deep learning network was then designed and trained with 2,328 \"clean\nB-scans\" (multi-frame B-scans), and their corresponding \"noisy B-scans\" (clean\nB-scans + gaussian noise) to de-noise the single-frame B-scans. The performance\nof the de-noising algorithm was assessed qualitatively, and quantitatively on\n1,552 B-scans using the signal to noise ratio (SNR), contrast to noise ratio\n(CNR), and mean structural similarity index metrics (MSSIM).\n  Results: The proposed algorithm successfully denoised unseen single-frame OCT\nB-scans. The denoised B-scans were qualitatively similar to their corresponding\nmulti-frame B-scans, with enhanced visibility of the ONH tissues. The mean SNR\nincreased from $4.02 \\pm 0.68$ dB (single-frame) to $8.14 \\pm 1.03$ dB\n(denoised). For all the ONH tissues, the mean CNR increased from $3.50 \\pm\n0.56$ (single-frame) to $7.63 \\pm 1.81$ (denoised). The MSSIM increased from\n$0.13 \\pm 0.02$ (single frame) to $0.65 \\pm 0.03$ (denoised) when compared with\nthe corresponding multi-frame B-scans.\n  Conclusions: Our deep learning algorithm can denoise a single-frame OCT\nB-scan of the ONH in under 20 ms, thus offering a framework to obtain superior\nquality OCT B-scans with reduced scanning times and minimal patient discomfort.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 15:53:36 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Devalla", "Sripad Krishna", ""], ["Subramanian", "Giridhar", ""], ["Pham", "Tan Hung", ""], ["Wang", "Xiaofei", ""], ["Perera", "Shamira", ""], ["Tun", "Tin A.", ""], ["Aung", "Tin", ""], ["Schmetterer", "Leopold", ""], ["Thiery", "Alexandre H.", ""], ["Girard", "Michael J. A.", ""]]}, {"id": "1809.10635", "submitter": "Gido van de Ven", "authors": "Gido M. van de Ven, Andreas S. Tolias", "title": "Generative replay with feedback connections as a general strategy for\n  continual learning", "comments": "17 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major obstacle to developing artificial intelligence applications capable\nof true lifelong learning is that artificial neural networks quickly or\ncatastrophically forget previously learned tasks when trained on a new one.\nNumerous methods for alleviating catastrophic forgetting are currently being\nproposed, but differences in evaluation protocols make it difficult to directly\ncompare their performance. To enable more meaningful comparisons, here we\nidentified three distinct scenarios for continual learning based on whether\ntask identity is known and, if it is not, whether it needs to be inferred.\nPerforming the split and permuted MNIST task protocols according to each of\nthese scenarios, we found that regularization-based approaches (e.g., elastic\nweight consolidation) failed when task identity needed to be inferred. In\ncontrast, generative replay combined with distillation (i.e., using class\nprobabilities as \"soft targets\") achieved superior performance in all three\nscenarios. Addressing the issue of efficiency, we reduced the computational\ncost of generative replay by integrating the generative model into the main\nmodel by equipping it with generative feedback or backward connections. This\nReplay-through-Feedback approach substantially shortened training time with no\nor negligible loss in performance. We believe this to be an important first\nstep towards making the powerful technique of generative replay scalable to\nreal-world continual learning applications.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:55:58 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 09:20:24 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["van de Ven", "Gido M.", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "1809.10636", "submitter": "Anoop Toffy", "authors": "Chae Young Lee, Anoop Toffy, Gue Jun Jung, Woo-Jin Han", "title": "Conditional WaveGAN", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative models are successfully used for image synthesis in the recent\nyears. But when it comes to other modalities like audio, text etc little\nprogress has been made. Recent works focus on generating audio from a\ngenerative model in an unsupervised setting. We explore the possibility of\nusing generative models conditioned on class labels. Concatenation based\nconditioning and conditional scaling were explored in this work with various\nhyper-parameter tuning methods. In this paper we introduce Conditional WaveGANs\n(cWaveGAN). Find our implementation at https://github.com/acheketa/cwavegan\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:56:23 GMT"}], "update_date": "2018-09-30", "authors_parsed": [["Lee", "Chae Young", ""], ["Toffy", "Anoop", ""], ["Jung", "Gue Jun", ""], ["Han", "Woo-Jin", ""]]}, {"id": "1809.10692", "submitter": "Amelia Jim\\'enez-S\\'anchez", "authors": "Amelia Jim\\'enez-S\\'anchez, Anees Kazi, Shadi Albarqouni, Sonja\n  Kirchhoff, Alexandra Str\\\"ater, Peter Biberthaler, Diana Mateus, Nassir Navab", "title": "Weakly-Supervised Localization and Classification of Proximal Femur\n  Fractures", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we target the problem of fracture classification from clinical\nX-Ray images towards an automated Computer Aided Diagnosis (CAD) system.\nAlthough primarily dealing with an image classification problem, we argue that\nlocalizing the fracture in the image is crucial to make good class predictions.\nTherefore, we propose and thoroughly analyze several schemes for simultaneous\nfracture localization and classification. We show that using an auxiliary\nlocalization task, in general, improves the classification performance.\nMoreover, it is possible to avoid the need for additional localization\nannotations thanks to recent advancements in weakly-supervised deep learning\napproaches. Among such approaches, we investigate and adapt Spatial\nTransformers (ST), Self-Transfer Learning (STL), and localization from global\npooling layers. We provide a detailed quantitative and qualitative validation\non a dataset of 1347 femur fractures images and report high accuracy with\nregard to inter-expert correlation values reported in the literature. Our\ninvestigations show that i) lesion localization improves the classification\noutcome, ii) weakly-supervised methods improve baseline classification without\nany additional cost, iii) STL guides feature activations and boost performance.\nWe plan to make both the dataset and code available.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 18:00:11 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Jim\u00e9nez-S\u00e1nchez", "Amelia", ""], ["Kazi", "Anees", ""], ["Albarqouni", "Shadi", ""], ["Kirchhoff", "Sonja", ""], ["Str\u00e4ter", "Alexandra", ""], ["Biberthaler", "Peter", ""], ["Mateus", "Diana", ""], ["Navab", "Nassir", ""]]}, {"id": "1809.10699", "submitter": "Yuval Litvak", "authors": "Yuval Litvak, Armin Biess, Aharon Bar-Hillel", "title": "Learning Pose Estimation for High-Precision Robotic Assembly Using\n  Simulated Depth Images", "comments": "8 pages, 5 figures. This work has been accepted to the International\n  Conference on Robotics and Automation (ICRA 2019). For associated video, see\n  https://youtu.be/uMvq2-Tg-9g", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of industrial robotic assembly tasks today require fixed initial\nconditions for successful assembly. These constraints induce high production\ncosts and low adaptability to new tasks. In this work we aim towards flexible\nand adaptable robotic assembly by using 3D CAD models for all parts to be\nassembled. We focus on a generic assembly task - the Siemens Innovation\nChallenge - in which a robot needs to assemble a gear-like mechanism with high\nprecision into an operating system. To obtain the millimeter-accuracy required\nfor this task and industrial settings alike, we use a depth camera mounted near\nthe robot end-effector. We present a high-accuracy two-stage pose estimation\nprocedure based on deep convolutional neural networks, which includes\ndetection, pose estimation, refinement, and handling of near- and full\nsymmetries of parts. The networks are trained on simulated depth images with\nmeans to ensure successful transfer to the real robot. We obtain an average\npose estimation error of 2.16 millimeters and 0.64 degree leading to 91%\nsuccess rate for robotic assembly of randomly distributed parts. To the best of\nour knowledge, this is the first time that the Siemens Innovation Challenge is\nfully addressed, with all the parts assembled with high success rates.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 18:02:24 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 12:57:02 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Litvak", "Yuval", ""], ["Biess", "Armin", ""], ["Bar-Hillel", "Aharon", ""]]}, {"id": "1809.10707", "submitter": "Jeffrey Liu", "authors": "Jeffrey Liu, Andrew Weinert, Saurabh Amin", "title": "Semantic Topic Analysis of Traffic Camera Images", "comments": "To be presented at IEEE-ITSC 2018, Nov 3-7 2018", "journal-ref": null, "doi": "10.1109/ITSC.2018.8569449", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic cameras are commonly deployed monitoring components in road\ninfrastructure networks, providing operators visual information about\nconditions at critical points in the network. However, human observers are\noften limited in their ability to process simultaneous information sources.\nRecent advancements in computer vision, driven by deep learning methods, have\nenabled general object recognition, unlocking opportunities for camera-based\nsensing beyond the existing human observer paradigm. In this paper, we present\na Natural Language Processing (NLP)-inspired approach, entitled\nBag-of-Label-Words (BoLW), for analyzing image data sets using exclusively\ntextual labels. The BoLW model represents the data in a conventional matrix\nform, enabling data compression and decomposition techniques, while preserving\nsemantic interpretability. We apply the Latent Dirichlet Allocation (LDA) topic\nmodel to decompose the label data into a small number of semantic topics. To\nillustrate our approach, we use freeway camera images collected from the Boston\narea between December 2017-January 2018. We analyze the cameras' sensitivity to\nweather events; identify temporal traffic patterns; and analyze the impact of\ninfrequent events, such as the winter holidays and the \"bomb cyclone\" winter\nstorm. This study demonstrates the flexibility of our approach, which allows us\nto analyze weather events and freeway traffic using only traffic camera image\nlabels.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 18:13:04 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Liu", "Jeffrey", ""], ["Weinert", "Andrew", ""], ["Amin", "Saurabh", ""]]}, {"id": "1809.10711", "submitter": "Pablo Navarrete Michelini", "authors": "Pablo Navarrete Michelini, Dan Zhu, and Hanwen Liu", "title": "Multi-Scale Recursive and Perception-Distortion Controllable Image\n  Super-Resolution", "comments": "In ECCV 2018 Workshops. Won 2nd place in Region 3 of PIRM-SR\n  Challenge 2018. Code and models are available at\n  https://github.com/pnavarre/pirm-sr-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe our solution for the PIRM Super-Resolution Challenge 2018 where\nwe achieved the 2nd best perceptual quality for average RMSE<=16, 5th best for\nRMSE<=12.5, and 7th best for RMSE<=11.5. We modify a recently proposed\nMulti-Grid Back-Projection (MGBP) architecture to work as a generative system\nwith an input parameter that can control the amount of artificial details in\nthe output. We propose a discriminator for adversarial training with the\nfollowing novel properties: it is multi-scale that resembles a progressive-GAN;\nit is recursive that balances the architecture of the generator; and it\nincludes a new layer to capture significant statistics of natural images.\nFinally, we propose a training strategy that avoids conflicts between\nreconstruction and perceptual losses. Our configuration uses only 281k\nparameters and upscales each image of the competition in 0.2s in average.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 18:26:09 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 03:07:47 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Michelini", "Pablo Navarrete", ""], ["Zhu", "Dan", ""], ["Liu", "Hanwen", ""]]}, {"id": "1809.10732", "submitter": "Nemanja Djuric", "authors": "Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin,\n  Thi Nguyen, Tzu-Kuo Huang, Jeff Schneider, Nemanja Djuric", "title": "Multimodal Trajectory Predictions for Autonomous Driving using Deep\n  Convolutional Networks", "comments": "Accepted for publication at IEEE International Conference on Robotics\n  and Automation (ICRA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving presents one of the largest problems that the robotics and\nartificial intelligence communities are facing at the moment, both in terms of\ndifficulty and potential societal impact. Self-driving vehicles (SDVs) are\nexpected to prevent road accidents and save millions of lives while improving\nthe livelihood and life quality of many more. However, despite large interest\nand a number of industry players working in the autonomous domain, there still\nremains more to be done in order to develop a system capable of operating at a\nlevel comparable to best human drivers. One reason for this is high uncertainty\nof traffic behavior and large number of situations that an SDV may encounter on\nthe roads, making it very difficult to create a fully generalizable system. To\nensure safe and efficient operations, an autonomous vehicle is required to\naccount for this uncertainty and to anticipate a multitude of possible\nbehaviors of traffic actors in its surrounding. We address this critical\nproblem and present a method to predict multiple possible trajectories of\nactors while also estimating their probabilities. The method encodes each\nactor's surrounding context into a raster image, used as input by deep\nconvolutional networks to automatically derive relevant features for the task.\nFollowing extensive offline evaluation and comparison to state-of-the-art\nbaselines, the method was successfully tested on SDVs in closed-course tests.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 04:07:13 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 14:07:02 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Cui", "Henggang", ""], ["Radosavljevic", "Vladan", ""], ["Chou", "Fang-Chieh", ""], ["Lin", "Tsung-Han", ""], ["Nguyen", "Thi", ""], ["Huang", "Tzu-Kuo", ""], ["Schneider", "Jeff", ""], ["Djuric", "Nemanja", ""]]}, {"id": "1809.10749", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen, Mahesh Chandra Mukkamala, Matthias Hein", "title": "On the loss landscape of a class of deep neural networks with no bad\n  local valleys", "comments": "Accepted at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify a class of over-parameterized deep neural networks with standard\nactivation functions and cross-entropy loss which provably have no bad local\nvalley, in the sense that from any point in parameter space there exists a\ncontinuous path on which the cross-entropy loss is non-increasing and gets\narbitrarily close to zero. This implies that these networks have no sub-optimal\nstrict local minima.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 20:09:59 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 00:58:29 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Nguyen", "Quynh", ""], ["Mukkamala", "Mahesh Chandra", ""], ["Hein", "Matthias", ""]]}, {"id": "1809.10792", "submitter": "Saad Bin Ahmed", "authors": "Saad Bin Ahmed, Saeeda Naz, Muhammad Imran Razzak, and Rubiyah Yusof", "title": "Cursive Scene Text Analysis by Deep Convolutional Linear Pyramids", "comments": "09 pages,6 figures, 25th International Conference on Neural\n  Information Processing (ICONIP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The camera captured images have various aspects to investigate. Generally,\nthe emphasis of research depends on the interesting regions. Sometimes the\nfocus could be on color segmentation, object detection or scene text analysis.\nThe image analysis, visibility and layout analysis are the tasks easier for\nhumans as suggested by behavioral trait of humans, but in contrast when these\nsame tasks are supposed to perform by machines then it seems to be challenging.\nThe learning machines always learn from the properties associated to provided\nsamples. The numerous approaches are designed in recent years for scene text\nextraction and recognition and the efforts are underway to improve the\naccuracy. The convolutional approach provided reasonable results on non-cursive\ntext analysis appeared in natural images. The work presented in this manuscript\nexploited the strength of linear pyramids by considering each pyramid as a\nfeature of the provided sample. Each pyramid image process through various\nempirically selected kernels. The performance was investigated by considering\nArabic text on each image pyramid of EASTR-42k dataset. The error rate of 0.17%\nwas reported on Arabic scene text recognition.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 22:51:56 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Ahmed", "Saad Bin", ""], ["Naz", "Saeeda", ""], ["Razzak", "Muhammad Imran", ""], ["Yusof", "Rubiyah", ""]]}, {"id": "1809.10801", "submitter": "Negin Hayatbini", "authors": "Negin Hayatbini, Kuo-lin Hsu, Soroosh Sorooshian, Yunji Zhang, and\n  Fuqing Zhang", "title": "Effective Cloud Detection and Segmentation using a Gradient-Based\n  Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to effectively identify clouds and monitor their evolution is one\nimportant step toward more accurate quantitative precipitation estimation and\nforecast. In this study, a new gradient-based cloud-image segmentation\ntechnique is developed using tools from image processing techniques. This\nmethod integrates morphological image gradient magnitudes to separable cloud\nsystems and patches boundaries. A varying scale-kernel is implemented to reduce\nthe sensitivity of image segmentation to noise and capture objects with various\nfinenesses of the edges in remote-sensing images. The proposed method is\nflexible and extendable from single- to multi-spectral imagery. Case studies\nwere carried out to validate the algorithm by applying the proposed\nsegmentation algorithm to synthetic radiances for channels of the Geostationary\nOperational Environmental Satellites (GOES-R) simulated by a high-resolution\nweather prediction model. The proposed method compares favorably with the\nexisting cloud-patch-based segmentation technique implemented in the\nPERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information using\nArtificial Neural Network - Cloud Classification System) rainfall retrieval\nalgorithm. Evaluation of event-based images indicates that the proposed\nalgorithm has potential to improve rain detection and estimation skills with an\naverage of more than 45% gain comparing to the segmentation technique used in\nPERSIANN-CCS and identifying cloud regions as objects with accuracy rates up to\n98%.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 23:59:24 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Hayatbini", "Negin", ""], ["Hsu", "Kuo-lin", ""], ["Sorooshian", "Soroosh", ""], ["Zhang", "Yunji", ""], ["Zhang", "Fuqing", ""]]}, {"id": "1809.10820", "submitter": "Chengqian Che", "authors": "Chengqian Che, Fujun Luan, Shuang Zhao, Kavita Bala, Ioannis\n  Gkioulekas", "title": "Inverse Transport Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce inverse transport networks as a learning architecture for\ninverse rendering problems where, given input image measurements, we seek to\ninfer physical scene parameters such as shape, material, and illumination.\nDuring training, these networks are evaluated not only in terms of how close\nthey can predict groundtruth parameters, but also in terms of whether the\nparameters they produce can be used, together with physically-accurate graphics\nrenderers, to reproduce the input image measurements. To en- able training of\ninverse transport networks using stochastic gradient descent, we additionally\ncreate a general-purpose, physically-accurate differentiable renderer, which\ncan be used to estimate derivatives of images with respect to arbitrary\nphysical scene parameters. Our experiments demonstrate that inverse transport\nnetworks can be trained efficiently using differentiable rendering, and that\nthey generalize to scenes with completely unseen geometry and illumination\nbetter than networks trained without appearance- matching regularization.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 01:50:51 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Che", "Chengqian", ""], ["Luan", "Fujun", ""], ["Zhao", "Shuang", ""], ["Bala", "Kavita", ""], ["Gkioulekas", "Ioannis", ""]]}, {"id": "1809.10821", "submitter": "Pingping Zhang Mr", "authors": "Yunzhi Zhuge, Pingping Zhang, Huchuan Lu", "title": "Boundary-guided Feature Aggregation Network for Salient Object Detection", "comments": "To appear in Signal Processing Letters (SPL), 5 pages, 5 figures and\n  3 tables", "journal-ref": null, "doi": "10.1109/LSP.2018.2875586", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional networks (FCN) has significantly improved the performance\nof many pixel-labeling tasks, such as semantic segmentation and depth\nestimation. However, it still remains non-trivial to thoroughly utilize the\nmulti-level convolutional feature maps and boundary information for salient\nobject detection. In this paper, we propose a novel FCN framework to integrate\nmulti-level convolutional features recurrently with the guidance of object\nboundary information. First, a deep convolutional network is used to extract\nmulti-level feature maps and separately aggregate them into multiple\nresolutions, which can be used to generate coarse saliency maps. Meanwhile,\nanother boundary information extraction branch is proposed to generate boundary\nfeatures. Finally, an attention-based feature fusion module is designed to fuse\nboundary information into salient regions to achieve accurate boundary\ninference and semantic enhancement. The final saliency maps are the combination\nof the predicted boundary maps and integrated saliency maps, which are more\ncloser to the ground truths. Experiments and analysis on four large-scale\nbenchmarks verify that our framework achieves new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 01:50:51 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Zhuge", "Yunzhi", ""], ["Zhang", "Pingping", ""], ["Lu", "Huchuan", ""]]}, {"id": "1809.10862", "submitter": "Zhiling Guo", "authors": "Zhiling Guo, Hiroaki Shengoku, Guangming Wu, Qi Chen, Wei Yuan,\n  Xiaodan Shi, Xiaowei Shao, Yongwei Xu, Ryosuke Shibasaki", "title": "Semantic Segmentation for Urban Planning Maps based on U-Net", "comments": "4 pages, 3 figures, conference, International Geoscience and Remote\n  Sensing Symposium (IGARSS 2018), Jul 2018, Valencia, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic digitizing of paper maps is a significant and challenging task\nfor both academia and industry. As an important procedure of map digitizing,\nthe semantic segmentation section mainly relies on manual visual interpretation\nwith low efficiency. In this study, we select urban planning maps as a\nrepresentative sample and investigate the feasibility of utilizing U-shape\nfully convolutional based architecture to perform end-to-end map semantic\nsegmentation. The experimental results obtained from the test area in Shibuya\ndistrict, Tokyo, demonstrate that our proposed method could achieve a very high\nJaccard similarity coefficient of 93.63% and an overall accuracy of 99.36%. For\nimplementation on GPGPU and cuDNN, the required processing time for the whole\nShibuya district can be less than three minutes. The results indicate the\nproposed method can serve as a viable tool for urban planning map semantic\nsegmentation task with high accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 05:32:45 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 03:00:11 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Guo", "Zhiling", ""], ["Shengoku", "Hiroaki", ""], ["Wu", "Guangming", ""], ["Chen", "Qi", ""], ["Yuan", "Wei", ""], ["Shi", "Xiaodan", ""], ["Shao", "Xiaowei", ""], ["Xu", "Yongwei", ""], ["Shibasaki", "Ryosuke", ""]]}, {"id": "1809.10917", "submitter": "Hyunjung Shim Dr.", "authors": "Seongjong Song and Hyunjung Shim", "title": "Depth Reconstruction of Translucent Objects from a Single Time-of-Flight\n  Camera using Deep Residual Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "Asian Conference on Computer Vision 2018 (Poster)", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to recovering the translucent objects from a\nsingle time-of-flight (ToF) depth camera using deep residual networks. When\nrecording the translucent objects using the ToF depth camera, their depth\nvalues are severely contaminated due to complex light interactions with the\nsurrounding environment. While existing methods suggested new capture systems\nor developed the depth distortion models, their solutions were less practical\nbecause of strict assumptions or heavy computational complexity. In this paper,\nwe adopt the deep residual networks for modeling the ToF depth distortion\ncaused by translucency. To fully utilize both the local and semantic\ninformation of objects, multi-scale patches are used to predict the depth\nvalue. Based on the quantitative and qualitative evaluation on our benchmark\ndatabase, we show the effectiveness and robustness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 08:42:35 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Song", "Seongjong", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1809.10954", "submitter": "Sheng He", "authors": "Sheng He and Lambert Schomaker", "title": "Deep Adaptive Learning for Writer Identification based on Single\n  Handwritten Word Images", "comments": "Under view of Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2018.11.003", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There are two types of information in each handwritten word image: explicit\ninformation which can be easily read or derived directly, such as lexical\ncontent or word length, and implicit attributes such as the author's identity.\nWhether features learned by a neural network for one task can be used for\nanother task remains an open question. In this paper, we present a deep\nadaptive learning method for writer identification based on single-word images\nusing multi-task learning. An auxiliary task is added to the training process\nto enforce the emergence of reusable features. Our proposed method transfers\nthe benefits of the learned features of a convolutional neural network from an\nauxiliary task such as explicit content recognition to the main task of writer\nidentification in a single procedure. Specifically, we propose a new adaptive\nconvolutional layer to exploit the learned deep features. A multi-task neural\nnetwork with one or several adaptive convolutional layers is trained\nend-to-end, to exploit robust generic features for a specific main task, i.e.,\nwriter identification. Three auxiliary tasks, corresponding to three explicit\nattributes of handwritten word images (lexical content, word length and\ncharacter attributes), are evaluated. Experimental results on two benchmark\ndatasets show that the proposed deep adaptive learning method can improve the\nperformance of writer identification based on single-word images, compared to\nnon-adaptive and simple linear-adaptive approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 10:40:23 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["He", "Sheng", ""], ["Schomaker", "Lambert", ""]]}, {"id": "1809.10961", "submitter": "Radu Horaud P", "authors": "Yutong Ban, Xavier Alameda-Pineda, Laurent Girin and Radu Horaud", "title": "Variational Bayesian Inference for Audio-Visual Tracking of Multiple\n  Speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of tracking multiple speakers via the\nfusion of visual and auditory information. We propose to exploit the\ncomplementary nature of these two modalities in order to accurately estimate\nsmooth trajectories of the tracked persons, to deal with the partial or total\nabsence of one of the modalities over short periods of time, and to estimate\nthe acoustic status -- either speaking or silent -- of each tracked person\nalong time. We propose to cast the problem at hand into a generative\naudio-visual fusion (or association) model formulated as a latent-variable\ntemporal graphical model. This may well be viewed as the problem of maximizing\nthe posterior joint distribution of a set of continuous and discrete latent\nvariables given the past and current observations, which is intractable. We\npropose a variational inference model which amounts to approximate the joint\ndistribution with a factorized distribution. The solution takes the form of a\nclosed-form expectation maximization procedure. We describe in detail the\ninference algorithm, we evaluate its performance and we compare it with several\nbaseline methods. These experiments show that the proposed audio-visual tracker\nperforms well in informal meetings involving a time-varying number of people.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 11:03:03 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 16:54:55 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ban", "Yutong", ""], ["Alameda-Pineda", "Xavier", ""], ["Girin", "Laurent", ""], ["Horaud", "Radu", ""]]}, {"id": "1809.10966", "submitter": "Antonio D'Innocente", "authors": "Antonio D'Innocente and Barbara Caputo", "title": "Domain Generalization with Domain-Specific Aggregation Modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition systems are meant to work in the real world. For this to\nhappen, they must work robustly in any visual domain, and not only on the data\nused during training. Within this context, a very realistic scenario deals with\ndomain generalization, i.e. the ability to build visual recognition algorithms\nable to work robustly in several visual domains, without having access to any\ninformation about target data statistic. This paper contributes to this\nresearch thread, proposing a deep architecture that maintains separated the\ninformation about the available source domains data while at the same time\nleveraging over generic perceptual information. We achieve this by introducing\ndomain-specific aggregation modules that through an aggregation layer strategy\nare able to merge generic and specific information in an effective manner.\nExperiments on two different benchmark databases show the power of our\napproach, reaching the new state of the art in domain generalization.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 11:24:02 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["D'Innocente", "Antonio", ""], ["Caputo", "Barbara", ""]]}, {"id": "1809.10976", "submitter": "Remi Delassus", "authors": "R\\'emi Delassus (LaBRI), Romain Giot (LaBRI)", "title": "CNNs Fusion for Building Detection in Aerial Images for the Building\n  Detection Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our contribution to the DeepGlobe Building Detection\nChallenge. We enhanced the SpaceNet Challenge winning solution by proposing a\nnew fusion strategy based on a deep combiner using segmentation both results of\ndifferent CNN and input data to segment. Segmentation results for all cities\nhave been significantly improved (between 1% improvement over the baseline for\nthe smallest one to more than 7% for the largest one). The separation of\nadjacent buildings should be the next enhancement made to the solution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 12:04:33 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Delassus", "R\u00e9mi", "", "LaBRI"], ["Giot", "Romain", "", "LaBRI"]]}, {"id": "1809.11036", "submitter": "Luis Guillermo Rold\\~ao Jimenez", "authors": "B Ravi Kiran, Luis Rold\\~ao, Benat Irastorza, Renzo Verastegui,\n  Sebastian Suss, Senthil Yogamani, Victor Talpaert, Alexandre Lepoutre, and\n  Guillaume Trehard", "title": "Real-time Dynamic Object Detection for Autonomous Driving using Prior\n  3D-Maps", "comments": "Preprint Submission to ECCVW AutoNUE 2018 - v2 author name accent\n  correction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar has become an essential sensor for autonomous driving as it provides\nreliable depth estimation. Lidar is also the primary sensor used in building 3D\nmaps which can be used even in the case of low-cost systems which do not use\nLidar. Computation on Lidar point clouds is intensive as it requires processing\nof millions of points per second. Additionally there are many subsequent tasks\nsuch as clustering, detection, tracking and classification which makes\nreal-time execution challenging. In this paper, we discuss real-time dynamic\nobject detection algorithms which leverages previously mapped Lidar point\nclouds to reduce processing. The prior 3D maps provide a static background\nmodel and we formulate dynamic object detection as a background subtraction\nproblem. Computation and modeling challenges in the mapping and online\nexecution pipeline are described. We propose a rejection cascade architecture\nto subtract road regions and other 3D regions separately. We implemented an\ninitial version of our proposed algorithm and evaluated the accuracy on CARLA\nsimulator.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:04:31 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 14:17:26 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Kiran", "B Ravi", ""], ["Rold\u00e3o", "Luis", ""], ["Irastorza", "Benat", ""], ["Verastegui", "Renzo", ""], ["Suss", "Sebastian", ""], ["Yogamani", "Senthil", ""], ["Talpaert", "Victor", ""], ["Lepoutre", "Alexandre", ""], ["Trehard", "Guillaume", ""]]}, {"id": "1809.11039", "submitter": "Jacek Komorowski", "authors": "Jacek Komorowski, Konrad Czarnota, Tomasz Trzcinski, Lukasz Dabala,\n  Simon Lynen", "title": "Interest point detectors stability evaluation on ApolloScape dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, a number of novel, deep-learning based, interest point\ndetectors, such as LIFT, DELF, Superpoint or LF-Net was proposed. However\nthere's a lack of a standard benchmark to evaluate suitability of these novel\nkeypoint detectors for real-live applications such as autonomous driving.\nTraditional benchmarks (e.g. Oxford VGG) are rather limited, as they consist of\nrelatively few images of mostly planar scenes taken in favourable conditions.\nIn this paper we verify if the recent, deep-learning based interest point\ndetectors have the advantage over the traditional, hand-crafted keypoint\ndetectors. To this end, we evaluate stability of a number of hand crafted and\nrecent, learning-based interest point detectors on the street-level view\nApolloScape dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:06:30 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Komorowski", "Jacek", ""], ["Czarnota", "Konrad", ""], ["Trzcinski", "Tomasz", ""], ["Dabala", "Lukasz", ""], ["Lynen", "Simon", ""]]}, {"id": "1809.11045", "submitter": "Zhe Jin", "authors": "Yen-Lung Lai, Jung-Yeon Hwang, Zhe Jin, Soohyong Kim, Sangrae Cho and\n  Andrew Beng Jin Teoh", "title": "A Symmetric Keyring Encryption Scheme for Biometric Cryptosystems", "comments": "15 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel biometric cryptosystem for vectorial\nbiometrics named symmetric keyring encryption (SKE) inspired by Rivest's\nkeyring model (2016). Unlike conventional biometric secret-binding primitives,\nsuch as fuzzy commitment and fuzzy vault, the proposed scheme reframes the\nbiometric secret-binding problem as a fuzzy symmetric encryption problem with a\nnotion called resilient vector pair. In this study, the pair resembles the\nencryption-decryption key pair in symmetric key cryptosystems. This notion is\nrealized using the index of maximum hashed vectors - a special instance of the\nranking-based locality-sensitive hashing function. With a simple filtering\nmechanism and [m,k] Shamir's secret-sharing scheme, we show that SKE, both in\ntheoretical and empirical evaluation, can retrieve the exact secret with\noverwhelming probability for a genuine input yet negligible probability for an\nimposter input. Though SKE can be applied to any vectorial biometrics, we adopt\nthe fingerprint vector as a case of study in this work. The experiments have\nbeen performed under several subsets of FVC 2002, 2004, and 2006 datasets. We\nformalize and analyze the threat model of SKE that encloses several major\nsecurity attacks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:10:51 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Lai", "Yen-Lung", ""], ["Hwang", "Jung-Yeon", ""], ["Jin", "Zhe", ""], ["Kim", "Soohyong", ""], ["Cho", "Sangrae", ""], ["Teoh", "Andrew Beng Jin", ""]]}, {"id": "1809.11054", "submitter": "Jacek Komorowski", "authors": "Tomasz Trzcinski, Jacek Komorowski, Lukasz Dabala, Konrad Czarnota,\n  Grzegorz Kurzejamski, Simon Lynen", "title": "SConE: Siamese Constellation Embedding Descriptor for Image Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous computer vision applications rely on local feature descriptors, such\nas SIFT, SURF or FREAK, for image matching. Although their local character\nmakes image matching processes more robust to occlusions, it often leads to\ngeometrically inconsistent keypoint matches that need to be filtered out, e.g.\nusing RANSAC. In this paper we propose a novel, more discriminative, descriptor\nthat includes not only local feature representation, but also information about\nthe geometric layout of neighbouring keypoints. To that end, we use a Siamese\narchitecture that learns a low-dimensional feature embedding of keypoint\nconstellation by maximizing the distances between non-corresponding pairs of\nmatched image patches, while minimizing it for correct matches. The\n48-dimensional oating point descriptor that we train is built on top of the\nstate-of-the-art FREAK descriptor achieves significant performance improvement\nover the competitors on a challenging TUM dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:24:30 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Trzcinski", "Tomasz", ""], ["Komorowski", "Jacek", ""], ["Dabala", "Lukasz", ""], ["Czarnota", "Konrad", ""], ["Kurzejamski", "Grzegorz", ""], ["Lynen", "Simon", ""]]}, {"id": "1809.11062", "submitter": "Jacek Komorowski", "authors": "Jacek Komorowski, Tomasz Trzcinski", "title": "Aggregation of binary feature descriptors for compact scene model\n  representation in large scale structure-from-motion applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an efficient method for aggregating binary feature\ndescriptors to allow compact representation of 3D scene model in incremental\nstructure-from-motion and SLAM applications. All feature descriptors linked\nwith one 3D scene point or landmark are represented by a single low-dimensional\nreal-valued vector called a \\emph{prototype}. The method allows significant\nreduction of memory required to store and process feature descriptors in\nlarge-scale structure-from-motion applications. An efficient approximate\nnearest neighbours search methods suited for real-valued descriptors, such as\nFLANN, can be used on the resulting prototypes to speed up matching processed\nframes.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:35:29 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Komorowski", "Jacek", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1809.11066", "submitter": "Jacek Komorowski", "authors": "Jacek Komorowski, Przemyslaw Rokita", "title": "Camera Pose Estimation from Sequence of Calibrated Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a method for camera pose estimation from a sequence of images\nis presented. The method assumes camera is calibrated (intrinsic parameters are\nknown) which allows to decrease a number of required pairs of corresponding\npoints compared to uncalibrated case. Our algorithm can be used as a first\nstage in a structure from motion stereo reconstruction system.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:42:29 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Komorowski", "Jacek", ""], ["Rokita", "Przemyslaw", ""]]}, {"id": "1809.11069", "submitter": "Jacek Komorowski", "authors": "Jacek Komorowski, Przemyslaw Rokita", "title": "Face Recognition Based on Sequence of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a face recognition method based on a sequence of images.\nFace shape is reconstructed from images using a combination of\nstructure-from-motion and multi-view stereo methods. The reconstructed 3D face\nmodel is compared against models held in a gallery. The novel element in the\npresented approach is the fact, that the reconstruction is based only on input\nimages and doesn't require a generic, deformable face model. Experimental\nverification of the proposed method is also included.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:54:12 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Komorowski", "Jacek", ""], ["Rokita", "Przemyslaw", ""]]}, {"id": "1809.11073", "submitter": "Jacek Komorowski", "authors": "Jacek Komorowski, Przemyslaw Rokita", "title": "Extrinsic camera calibration method and its performance evaluation", "comments": "arXiv admin note: text overlap with arXiv:1809.11066", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for extrinsic camera calibration (estimation of\ncamera rotation and translation matrices) from a sequence of images. It is\nassumed camera intrinsic matrix and distortion coefficients are known and fixed\nduring the entire sequence. %This allows to decrease a number of pairs of\ncorresponding keypoints between images needed to estimate epipolar geometry\ncompared to uncalibrated case. Performance of the presented method is evaluated\non a number of multi-view stereo test datasets. Presented algorithm can be used\nas a first stage in a dense stereo reconstruction system.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:00:46 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Komorowski", "Jacek", ""], ["Rokita", "Przemyslaw", ""]]}, {"id": "1809.11100", "submitter": "Zhihao Li", "authors": "Zhihao Li, Toshiyuki Motoyoshi, Kazuma Sasaki, Tetsuya Ogata, Shigeki\n  Sugano", "title": "Rethinking Self-driving: Multi-task Knowledge for Better Generalization\n  and Accident Explanation Ability", "comments": "11 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current end-to-end deep learning driving models have two problems: (1) Poor\ngeneralization ability of unobserved driving environment when diversity of\ntraining driving dataset is limited (2) Lack of accident explanation ability\nwhen driving models don't work as expected. To tackle these two problems,\nrooted on the believe that knowledge of associated easy task is benificial for\naddressing difficult task, we proposed a new driving model which is composed of\nperception module for \\textit{see and think} and driving module for\n\\textit{behave}, and trained it with multi-task perception-related basic\nknowledge and driving knowledge stepwisely.\n  Specifically segmentation map and depth map (pixel level understanding of\nimages) were considered as \\textit{what \\& where} and \\textit{how far}\nknowledge for tackling easier driving-related perception problems before\ngenerating final control commands for difficult driving task. The results of\nexperiments demonstrated the effectiveness of multi-task perception knowledge\nfor better generalization and accident explanation ability. With our method the\naverage sucess rate of finishing most difficult navigation tasks in untrained\ncity of CoRL test surpassed current benchmark method for 15 percent in trained\nweather and 20 percent in untrained weathers. Demonstration video link is:\nhttps://www.youtube.com/watch?v=N7ePnnZZwdE\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:45:32 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Li", "Zhihao", ""], ["Motoyoshi", "Toshiyuki", ""], ["Sasaki", "Kazuma", ""], ["Ogata", "Tetsuya", ""], ["Sugano", "Shigeki", ""]]}, {"id": "1809.11130", "submitter": "Yanting Hu", "authors": "Yanting Hu, Jie Li, Yuanfei Huang, Xinbo Gao", "title": "Channel-wise and Spatial Feature Modulation Network for Single Image\n  Super-Resolution", "comments": "14 pages,14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of single image super-resolution has achieved significant\nimprovement by utilizing deep convolutional neural networks (CNNs). The\nfeatures in deep CNN contain different types of information which make\ndifferent contributions to image reconstruction. However, most CNN-based models\nlack discriminative ability for different types of information and deal with\nthem equally, which results in the representational capacity of the models\nbeing limited. On the other hand, as the depth of neural networks grows, the\nlong-term information coming from preceding layers is easy to be weaken or lost\nin late layers, which is adverse to super-resolving image. To capture more\ninformative features and maintain long-term information for image\nsuper-resolution, we propose a channel-wise and spatial feature modulation\n(CSFM) network in which a sequence of feature-modulation memory (FMM) modules\nis cascaded with a densely connected structure to transform low-resolution\nfeatures to high informative features. In each FMM module, we construct a set\nof channel-wise and spatial attention residual (CSAR) blocks and stack them in\na chain structure to dynamically modulate multi-level features in a\nglobal-and-local manner. This feature modulation strategy enables the high\ncontribution information to be enhanced and the redundant information to be\nsuppressed. Meanwhile, for long-term information persistence, a gated fusion\n(GF) node is attached at the end of the FMM module to adaptively fuse\nhierarchical features and distill more effective information via the dense skip\nconnections and the gating mechanism. Extensive quantitative and qualitative\nevaluations on benchmark datasets illustrate the superiority of our proposed\nmethod over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 16:29:31 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Hu", "Yanting", ""], ["Li", "Jie", ""], ["Huang", "Yuanfei", ""], ["Gao", "Xinbo", ""]]}]