[{"id": "1403.0087", "submitter": "Francisco Estrada", "authors": "Francisco J. Estrada", "title": "Temporal Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces temporal image fusion. The proposed technique builds\nupon previous research in exposure fusion and expands it to deal with the\nlimited Temporal Dynamic Range of existing sensors and camera technologies. In\nparticular, temporal image fusion enables the rendering of long-exposure\neffects on full frame-rate video, as well as the generation of arbitrarily long\nexposures from a sequence of images of the same scene taken over time. We\nexplore the problem of temporal under-exposure, and show how it can be\naddressed by selectively enhancing dynamic structure. Finally, we show that the\nuse of temporal image fusion together with content-selective image filters can\nproduce a range of striking visual effects on a given input sequence.\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2014 14:08:22 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Estrada", "Francisco J.", ""]]}, {"id": "1403.0240", "submitter": "Ivo Sbalzarini", "authors": "Ivo F. Sbalzarini, Sophie Schneider, Janick Cardinale", "title": "Particle methods enable fast and simple approximation of Sobolev\n  gradients in image segmentation", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.NA q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bio-image analysis is challenging due to inhomogeneous intensity\ndistributions and high levels of noise in the images. Bayesian inference\nprovides a principled way for regularizing the problem using prior knowledge. A\nfundamental choice is how one measures \"distances\" between shapes in an image.\nIt has been shown that the straightforward geometric L2 distance is degenerate\nand leads to pathological situations. This is avoided when using Sobolev\ngradients, rendering the segmentation problem less ill-posed. The high\ncomputational cost and implementation overhead of Sobolev gradients, however,\nhave hampered practical applications. We show how particle methods as applied\nto image segmentation allow for a simple and computationally efficient\nimplementation of Sobolev gradients. We show that the evaluation of Sobolev\ngradients amounts to particle-particle interactions along the contour in an\nimage. We extend an existing particle-based segmentation algorithm to using\nSobolev gradients. Using synthetic and real-world images, we benchmark the\nresults for both 2D and 3D images using piecewise smooth and piecewise constant\nregion models. The present particle approximation of Sobolev gradients is 2.8\nto 10 times faster than the previous reference implementation, but retains the\nknown favorable properties of Sobolev gradients. This speedup is achieved by\nusing local particle-particle interactions instead of solving a global Poisson\nequation at each iteration. The computational time per iteration is higher for\nSobolev gradients than for L2 gradients. Since Sobolev gradients precondition\nthe optimization problem, however, a smaller number of overall iterations may\nbe necessary for the algorithm to converge, which can in some cases amortize\nthe higher per-iteration cost.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 16:58:29 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Sbalzarini", "Ivo F.", ""], ["Schneider", "Sophie", ""], ["Cardinale", "Janick", ""]]}, {"id": "1403.0284", "submitter": "Liang Zheng", "authors": "Liang Zheng and Shengjin Wang and Wengang Zhou and Qi Tian", "title": "Bayes Merging of Multiple Vocabularies for Scalable Image Retrieval", "comments": "8 pages, 7 figures, 6 tables, accepted to CVPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Bag-of-Words (BoW) representation is well applied to recent\nstate-of-the-art image retrieval works. Typically, multiple vocabularies are\ngenerated to correct quantization artifacts and improve recall. However, this\nroutine is corrupted by vocabulary correlation, i.e., overlapping among\ndifferent vocabularies. Vocabulary correlation leads to an over-counting of the\nindexed features in the overlapped area, or the intersection set, thus\ncompromising the retrieval accuracy. In order to address the correlation\nproblem while preserve the benefit of high recall, this paper proposes a Bayes\nmerging approach to down-weight the indexed features in the intersection set.\nThrough explicitly modeling the correlation problem in a probabilistic view, a\njoint similarity on both image- and feature-level is estimated for the indexed\nfeatures in the intersection set.\n  We evaluate our method through extensive experiments on three benchmark\ndatasets. Albeit simple, Bayes merging can be well applied in various merging\ntasks, and consistently improves the baselines on multi-vocabulary merging.\nMoreover, Bayes merging is efficient in terms of both time and memory cost, and\nyields competitive performance compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 00:51:29 GMT"}, {"version": "v2", "created": "Sun, 13 Apr 2014 10:14:54 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Zheng", "Liang", ""], ["Wang", "Shengjin", ""], ["Zhou", "Wengang", ""], ["Tian", "Qi", ""]]}, {"id": "1403.0309", "submitter": "Conrad Sanderson", "authors": "Sareh Shirazi, Mehrtash T. Harandi, Brian C. Lovell, Conrad Sanderson", "title": "Object Tracking via Non-Euclidean Geometry: A Grassmann Approach", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2014", "journal-ref": null, "doi": "10.1109/WACV.2014.6836008", "report-no": null, "categories": "cs.CV math.MG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust visual tracking system requires an object appearance model that is\nable to handle occlusion, pose, and illumination variations in the video\nstream. This can be difficult to accomplish when the model is trained using\nonly a single image. In this paper, we first propose a tracking approach based\non affine subspaces (constructed from several images) which are able to\naccommodate the abovementioned variations. We use affine subspaces not only to\nrepresent the object, but also the candidate areas that the object may occupy.\nWe furthermore propose a novel approach to measure affine subspace-to-subspace\ndistance via the use of non-Euclidean geometry of Grassmann manifolds. The\ntracking problem is then considered as an inference task in a Markov Chain\nMonte Carlo framework via particle filtering. Quantitative evaluation on\nchallenging video sequences indicates that the proposed approach obtains\nconsiderably better performance than several recent state-of-the-art methods\nsuch as Tracking-Learning-Detection and MILtrack.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 04:46:44 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Shirazi", "Sareh", ""], ["Harandi", "Mehrtash T.", ""], ["Lovell", "Brian C.", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1403.0315", "submitter": "Conrad Sanderson", "authors": "Johanna Carvajal, Chris McCool, Conrad Sanderson", "title": "Summarisation of Short-Term and Long-Term Videos using Texture and\n  Colour", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2014", "journal-ref": null, "doi": "10.1109/WACV.2014.6836025", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to video summarisation that makes use of a\nBag-of-visual-Textures (BoT) approach. Two systems are proposed, one based\nsolely on the BoT approach and another which exploits both colour information\nand BoT features. On 50 short-term videos from the Open Video Project we show\nthat our BoT and fusion systems both achieve state-of-the-art performance,\nobtaining an average F-measure of 0.83 and 0.86 respectively, a relative\nimprovement of 9% and 13% when compared to the previous state-of-the-art. When\napplied to a new underwater surveillance dataset containing 33 long-term\nvideos, the proposed system reduces the amount of footage by a factor of 27,\nwith only minor degradation in the information content. This order of magnitude\nreduction in video data represents significant savings in terms of time and\npotential labour cost when manually reviewing such footage.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 05:19:10 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Carvajal", "Johanna", ""], ["McCool", "Chris", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1403.0316", "submitter": "Kang Zhang", "authors": "Kang Zhang, Yuqiang Fang, Dongbo Min, Lifeng Sun, Shiqiang Yang.\n  Shuicheng Yan, Qi Tian", "title": "Cross-Scale Cost Aggregation for Stereo Matching", "comments": "To Appear in 2013 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR). 2014 (poster, 29.88%)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings process stereoscopic correspondence across multiple scales.\nHowever, this bio-inspiration is ignored by state-of-the-art cost aggregation\nmethods for dense stereo correspondence. In this paper, a generic cross-scale\ncost aggregation framework is proposed to allow multi-scale interaction in cost\naggregation. We firstly reformulate cost aggregation from a unified\noptimization perspective and show that different cost aggregation methods\nessentially differ in the choices of similarity kernels. Then, an inter-scale\nregularizer is introduced into optimization and solving this new optimization\nproblem leads to the proposed framework. Since the regularization term is\nindependent of the similarity kernel, various cost aggregation methods can be\nintegrated into the proposed general framework. We show that the cross-scale\nframework is important as it effectively and efficiently expands\nstate-of-the-art cost aggregation methods and leads to significant\nimprovements, when evaluated on Middlebury, KITTI and New Tsukuba datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 05:20:28 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Zhang", "Kang", ""], ["Fang", "Yuqiang", ""], ["Min", "Dongbo", ""], ["Sun", "Lifeng", ""], ["Yan", "Shiqiang Yang. Shuicheng", ""], ["Tian", "Qi", ""]]}, {"id": "1403.0320", "submitter": "Conrad Sanderson", "authors": "Shaokang Chen, Arnold Wiliem, Conrad Sanderson, Brian C. Lovell", "title": "Matching Image Sets via Adaptive Multi Convex Hull", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2014", "journal-ref": null, "doi": "10.1109/WACV.2014.6835985", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional nearest points methods use all the samples in an image set to\nconstruct a single convex or affine hull model for classification. However,\nstrong artificial features and noisy data may be generated from combinations of\ntraining samples when significant intra-class variations and/or noise occur in\nthe image set. Existing multi-model approaches extract local models by\nclustering each image set individually only once, with fixed clusters used for\nmatching with various image sets. This may not be optimal for discrimination,\nas undesirable environmental conditions (eg. illumination and pose variations)\nmay result in the two closest clusters representing different characteristics\nof an object (eg. frontal face being compared to non-frontal face). To address\nthe above problem, we propose a novel approach to enhance nearest points based\nmethods by integrating affine/convex hull classification with an adapted\nmulti-model approach. We first extract multiple local convex hulls from a query\nimage set via maximum margin clustering to diminish the artificial variations\nand constrain the noise in local convex hulls. We then propose adaptive\nreference clustering (ARC) to constrain the clustering of each gallery image\nset by forcing the clusters to have resemblance to the clusters in the query\nimage set. By applying ARC, noisy clusters in the query set can be discarded.\nExperiments on Honda, MoBo and ETH-80 datasets show that the proposed method\noutperforms single model approaches and other recent techniques, such as Sparse\nApproximated Nearest Points, Mutual Subspace Method and Manifold Discriminant\nAnalysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 06:19:45 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Chen", "Shaokang", ""], ["Wiliem", "Arnold", ""], ["Sanderson", "Conrad", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1403.0485", "submitter": "Brijesh Mehta", "authors": "Divyarajsinh N. Parmar and Brijesh B. Mehta", "title": "Face Recognition Methods & Applications", "comments": "3 pages, 1 figure", "journal-ref": "International Journal of Computer Technology & Applications, Vol 4\n  (1), pp. 84-86, Jan-Feb 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition presents a challenging problem in the field of image\nanalysis and computer vision. The security of information is becoming very\nsignificant and difficult. Security cameras are presently common in airports,\nOffices, University, ATM, Bank and in any locations with a security system.\nFace recognition is a biometric system used to identify or verify a person from\na digital image. Face Recognition system is used in security. Face recognition\nsystem should be able to automatically detect a face in an image. This involves\nextracts its features and then recognize it, regardless of lighting,\nexpression, illumination, ageing, transformations (translate, rotate and scale\nimage) and pose, which is a difficult task. This paper contains three sections.\nThe first section describes the common methods like holistic matching method,\nfeature extraction method and hybrid methods. The second section describes\napplications with examples and finally third section describes the future\nresearch directions of face recognition.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 16:56:53 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Parmar", "Divyarajsinh N.", ""], ["Mehta", "Brijesh B.", ""]]}, {"id": "1403.0699", "submitter": "Conrad Sanderson", "authors": "Azadeh Alavi, Yan Yang, Mehrtash Harandi, Conrad Sanderson", "title": "Multi-Shot Person Re-Identification via Relational Stein Divergence", "comments": "IEEE International Conference on Image Processing (ICIP), 2013", "journal-ref": null, "doi": "10.1109/ICIP.2013.6738731", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is particularly challenging due to significant\nappearance changes across separate camera views. In order to re-identify\npeople, a representative human signature should effectively handle differences\nin illumination, pose and camera parameters. While general appearance-based\nmethods are modelled in Euclidean spaces, it has been argued that some\napplications in image and video analysis are better modelled via non-Euclidean\nmanifold geometry. To this end, recent approaches represent images as\ncovariance matrices, and interpret such matrices as points on Riemannian\nmanifolds. As direct classification on such manifolds can be difficult, in this\npaper we propose to represent each manifold point as a vector of similarities\nto class representers, via a recently introduced form of Bregman matrix\ndivergence known as the Stein divergence. This is followed by using a\ndiscriminative mapping of similarity vectors for final classification. The use\nof similarity vectors is in contrast to the traditional approach of embedding\nmanifolds into tangent spaces, which can suffer from representing the manifold\nstructure inaccurately. Comparative evaluations on benchmark ETHZ and iLIDS\ndatasets for the person re-identification task show that the proposed approach\nobtains better performance than recent techniques such as Histogram Plus\nEpitome, Partial Least Squares, and Symmetry-Driven Accumulation of Local\nFeatures.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 06:44:17 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Alavi", "Azadeh", ""], ["Yang", "Yan", ""], ["Harandi", "Mehrtash", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1403.0700", "submitter": "Conrad Sanderson", "authors": "Azadeh Alavi, Arnold Wiliem, Kun Zhao, Brian C. Lovell, Conrad\n  Sanderson", "title": "Random Projections on Manifolds of Symmetric Positive Definite Matrices\n  for Image Classification", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2014", "journal-ref": null, "doi": "10.1109/WACV.2014.6836085", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances suggest that encoding images through Symmetric Positive\nDefinite (SPD) matrices and then interpreting such matrices as points on\nRiemannian manifolds can lead to increased classification performance. Taking\ninto account manifold geometry is typically done via (1) embedding the\nmanifolds in tangent spaces, or (2) embedding into Reproducing Kernel Hilbert\nSpaces (RKHS). While embedding into tangent spaces allows the use of existing\nEuclidean-based learning algorithms, manifold shape is only approximated which\ncan cause loss of discriminatory information. The RKHS approach retains more of\nthe manifold structure, but may require non-trivial effort to kernelise\nEuclidean-based learning algorithms. In contrast to the above approaches, in\nthis paper we offer a novel solution that allows SPD matrices to be used with\nunmodified Euclidean-based learning algorithms, with the true manifold shape\nwell-preserved. Specifically, we propose to project SPD matrices using a set of\nrandom projection hyperplanes over RKHS into a random projection space, which\nleads to representing each matrix as a vector of projection coefficients.\nExperiments on face recognition, person re-identification and texture\nclassification show that the proposed approach outperforms several recent\nmethods, such as Tensor Sparse Coding, Histogram Plus Epitome, Riemannian\nLocality Preserving Projection and Relational Divergence Classification.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 06:57:50 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Alavi", "Azadeh", ""], ["Wiliem", "Arnold", ""], ["Zhao", "Kun", ""], ["Lovell", "Brian C.", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1403.0728", "submitter": "Tolga Birdal", "authors": "Tolga Birdal, Emrah Bala", "title": "A Novel Method for Vectorization", "comments": "Prepared in Siggraph format, not published in a conference, 7 pages,\n  9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vectorization of images is a key concern uniting computer graphics and\ncomputer vision communities. In this paper we are presenting a novel idea for\nefficient, customizable vectorization of raster images, based on Catmull Rom\nspline fitting. The algorithm maintains a good balance between photo-realism\nand photo abstraction, and hence is applicable to applications with artistic\nconcerns or applications where less information loss is crucial. The resulting\nalgorithm is fast, parallelizable and can satisfy general soft realtime\nrequirements. Moreover, the smoothness of the vectorized images aesthetically\noutperforms outputs of many polygon-based methods\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 09:52:13 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Birdal", "Tolga", ""], ["Bala", "Emrah", ""]]}, {"id": "1403.0820", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Pavan Turaga", "title": "Geometry-based Adaptive Symbolic Approximation for Fast Sequence\n  Matching on Manifolds", "comments": "Under major revision at IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of fast and efficient indexing\ntechniques for sequences evolving in non-Euclidean spaces. This problem has\nseveral applications in the areas of human activity analysis, where there is a\nneed to perform fast search, and recognition in very high dimensional spaces.\nThe problem is made more challenging when representations such as landmarks,\ncontours, and human skeletons etc. are naturally studied in a non-Euclidean\nsetting where even simple operations are much more computationally intensive\nthan their Euclidean counterparts. We propose a geometry and data adaptive\nsymbolic framework that is shown to enable the deployment of fast and accurate\nalgorithms for activity recognition, dynamic texture recognition, motif\ndiscovery. Toward this end, we present generalizations of key concepts of\npiece-wise aggregation and symbolic approximation for the case of non-Euclidean\nmanifolds. We show that one can replace expensive geodesic computations with\nmuch faster symbolic computations with little loss of accuracy in activity\nrecognition and discovery applications. The framework is general enough to work\nacross both Euclidean and non-Euclidean spaces, depending on appropriate\nfeature representations without compromising on the ultra-low bandwidth, high\nspeed and high accuracy. The proposed methods are ideally suited for real-time\nsystems and low complexity scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 15:53:30 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 23:13:20 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Anirudh", "Rushil", ""], ["Turaga", "Pavan", ""]]}, {"id": "1403.0829", "submitter": "Weifeng Liu", "authors": "W. Liu, H. Liu, D. Tao, Y. Wang, Ke Lu", "title": "Multiview Hessian regularized logistic regression for action recognition", "comments": "13 pages,2 figures, submitted to signal processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of social media sharing, people often need to\nmanage the growing volume of multimedia data such as large scale video\nclassification and annotation, especially to organize those videos containing\nhuman activities. Recently, manifold regularized semi-supervised learning\n(SSL), which explores the intrinsic data probability distribution and then\nimproves the generalization ability with only a small number of labeled data,\nhas emerged as a promising paradigm for semiautomatic video classification. In\naddition, human action videos often have multi-modal content and different\nrepresentations. To tackle the above problems, in this paper we propose\nmultiview Hessian regularized logistic regression (mHLR) for human action\nrecognition. Compared with existing work, the advantages of mHLR lie in three\nfolds: (1) mHLR combines multiple Hessian regularization, each of which\nobtained from a particular representation of instance, to leverage the\nexploring of local geometry; (2) mHLR naturally handle multi-view instances\nwith multiple representations; (3) mHLR employs a smooth loss function and then\ncan be effectively optimized. We carefully conduct extensive experiments on the\nunstructured social activity attribute (USAA) dataset and the experimental\nresults demonstrate the effectiveness of the proposed multiview Hessian\nregularized logistic regression for human action recognition.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 01:11:40 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Liu", "W.", ""], ["Liu", "H.", ""], ["Tao", "D.", ""], ["Wang", "Y.", ""], ["Lu", "Ke", ""]]}, {"id": "1403.1024", "submitter": "Hyun Oh Song", "authors": "Hyun Oh Song, Ross Girshick, Stefanie Jegelka, Julien Mairal, Zaid\n  Harchaoui, Trevor Darrell", "title": "On learning to localize objects with minimal supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to localize objects with minimal supervision is an important problem\nin computer vision, since large fully annotated datasets are extremely costly\nto obtain. In this paper, we propose a new method that achieves this goal with\nonly image-level labels of whether the objects are present or not. Our approach\ncombines a discriminative submodular cover problem for automatically\ndiscovering a set of positive object windows with a smoothed latent SVM\nformulation. The latter allows us to leverage efficient quasi-Newton\noptimization techniques. Our experiments demonstrate that the proposed approach\nprovides a 50% relative improvement in mean average precision over the current\nstate-of-the-art on PASCAL VOC 2007 detection.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 07:21:20 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 00:50:26 GMT"}, {"version": "v3", "created": "Mon, 17 Mar 2014 21:04:49 GMT"}, {"version": "v4", "created": "Thu, 15 May 2014 22:08:59 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Song", "Hyun Oh", ""], ["Girshick", "Ross", ""], ["Jegelka", "Stefanie", ""], ["Mairal", "Julien", ""], ["Harchaoui", "Zaid", ""], ["Darrell", "Trevor", ""]]}, {"id": "1403.1056", "submitter": "Conrad Sanderson", "authors": "Andres Sanin, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell", "title": "K-Tangent Spaces on Riemannian Manifolds for Improved Pedestrian\n  Detection", "comments": "IEEE International Conference on Image Processing (ICIP), 2012", "journal-ref": null, "doi": "10.1109/ICIP.2012.6466899", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For covariance-based image descriptors, taking into account the curvature of\nthe corresponding feature space has been shown to improve discrimination\nperformance. This is often done through representing the descriptors as points\non Riemannian manifolds, with the discrimination accomplished on a tangent\nspace. However, such treatment is restrictive as distances between arbitrary\npoints on the tangent space do not represent true geodesic distances, and hence\ndo not represent the manifold structure accurately. In this paper we propose a\ngeneral discriminative model based on the combination of several tangent\nspaces, in order to preserve more details of the structure. The model can be\nused as a weak learner in a boosting-based pedestrian detection framework.\nExperiments on the challenging INRIA and DaimlerChrysler datasets show that the\nproposed model leads to considerably higher performance than methods based on\nhistograms of oriented gradients as well as previous Riemannian-based\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 09:44:41 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Sanin", "Andres", ""], ["Sanderson", "Conrad", ""], ["Harandi", "Mehrtash T.", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1403.1327", "submitter": "Weifeng Liu", "authors": "Hongli Liu, Weifeng Liu, Yanjiang Wang", "title": "Multi-view Face Analysis Based on Gabor Features", "comments": "8 pages, 3 figures, Journal of Information and Computational Science", "journal-ref": "Journal of Information and Computational Science,\n  2014,11(13):4637-4644", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial analysis has attracted much attention in the technology for\nhuman-machine interface. Different methods of classification based on sparse\nrepresentation and Gabor kernels have been widely applied in the fields of\nfacial analysis. However, most of these methods treat face from a whole view\nstandpoint. In terms of the importance of different facial views, in this\npaper, we present multi-view face analysis based on sparse representation and\nGabor wavelet coefficients. To evaluate the performance, we conduct face\nanalysis experiments including face recognition (FR) and face expression\nrecognition (FER) on JAFFE database. Experiments are conducted from two parts:\n(1) Face images are divided into three facial parts which are forehead, eye and\nmouth. (2) Face images are divided into 8 parts by the orientation of Gabor\nkernels. Experimental results demonstrate that the proposed methods can\nsignificantly boost the performance and perform better than the other methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 02:14:20 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Liu", "Hongli", ""], ["Liu", "Weifeng", ""], ["Wang", "Yanjiang", ""]]}, {"id": "1403.1343", "submitter": "Mark Simkin", "authors": "Mark Simkin and Dominique Schroeder and Andreas Bulling and Mario\n  Fritz", "title": "Ubic: Bridging the gap between digital cryptography and the physical\n  world", "comments": "In ESORICS 2014, volume 8712 of Lecture Notes in Computer Science,\n  pp. 56-75, Wroclaw, Poland, September 7-11, 2014. Springer, Berlin, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in computing technology increasingly blur the boundary between the\ndigital domain and the physical world. Although the research community has\ndeveloped a large number of cryptographic primitives and has demonstrated their\nusability in all-digital communication, many of them have not yet made their\nway into the real world due to usability aspects. We aim to make another step\ntowards a tighter integration of digital cryptography into real world\ninteractions. We describe Ubic, a framework that allows users to bridge the gap\nbetween digital cryptography and the physical world. Ubic relies on\nhead-mounted displays, like Google Glass, resource-friendly computer vision\ntechniques as well as mathematically sound cryptographic primitives to provide\nusers with better security and privacy guarantees. The framework covers key\ncryptographic primitives, such as secure identification, document verification\nusing a novel secure physical document format, as well as content hiding. To\nmake a contribution of practical value, we focused on making Ubic as simple,\neasily deployable, and user friendly as possible.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 04:35:45 GMT"}, {"version": "v2", "created": "Tue, 1 Apr 2014 11:58:25 GMT"}, {"version": "v3", "created": "Thu, 24 Jul 2014 13:37:03 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Simkin", "Mark", ""], ["Schroeder", "Dominique", ""], ["Bulling", "Andreas", ""], ["Fritz", "Mario", ""]]}, {"id": "1403.1353", "submitter": "Yang Wu", "authors": "Yang Wu, Vansteenberge Jarich, Masayuki Mukunoki, and Michihiko Minoh", "title": "Collaborative Representation for Classification, Sparse or Non-sparse?", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation based classification (SRC) has been proved to be a\nsimple, effective and robust solution to face recognition. As it gets popular,\ndoubts on the necessity of enforcing sparsity starts coming up, and primary\nexperimental results showed that simply changing the $l_1$-norm based\nregularization to the computationally much more efficient $l_2$-norm based\nnon-sparse version would lead to a similar or even better performance. However,\nthat's not always the case. Given a new classification task, it's still unclear\nwhich regularization strategy (i.e., making the coefficients sparse or\nnon-sparse) is a better choice without trying both for comparison. In this\npaper, we present as far as we know the first study on solving this issue,\nbased on plenty of diverse classification experiments. We propose a scoring\nfunction for pre-selecting the regularization strategy using only the dataset\nsize, the feature dimensionality and a discrimination score derived from a\ngiven feature representation. Moreover, we show that when dictionary learning\nis taking into account, non-sparse representation has a more significant\nsuperiority to sparse representation. This work is expected to enrich our\nunderstanding of sparse/non-sparse collaborative representation for\nclassification and motivate further research activities.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 05:44:32 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Wu", "Yang", ""], ["Jarich", "Vansteenberge", ""], ["Mukunoki", "Masayuki", ""], ["Minoh", "Michihiko", ""]]}, {"id": "1403.1362", "submitter": "Shireesha Chintalapati", "authors": "Shireesha Chintalapati and M. V. Raghunadh", "title": "Illumination,Expression and Occlusion Invariant Pose-Adaptive Face\n  Recognition System for Real-Time Applications", "comments": "7 pages,8 figures, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology(IJETT),\n  V8(6),292-298 February 2014. Published by seventh sense research group", "doi": "10.14445/22315381/IJETT-V8P254", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition in real-time scenarios is mainly affected by illumination,\nexpression and pose variations and also by occlusion. This paper presents the\nframework for pose adaptive component-based face recognition system. The\nframework proposed deals with all the above mentioned issues. The steps\ninvolved in the presented framework are (i) facial landmark localisation, (ii)\nfacial component extraction, (iii) pre-processing of facial image (iv) facial\npose estimation (v) feature extraction using Local Binary Pattern Histograms of\neach component followed by (vi) fusion of pose adaptive classification of\ncomponents. By employing pose adaptive classification, the recognition process\nis carried out on some part of database, based on estimated pose, instead of\napplying the recognition process on the whole database. Pre-processing\ntechniques employed to overcome the problems due to illumination variation are\nalso discussed in this paper. Component-based techniques provide better\nrecognition rates when face images are occluded compared to the holistic\nmethods. Our method is simple, feasible and provides better results when\ncompared to other holistic methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 07:19:24 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Chintalapati", "Shireesha", ""], ["Raghunadh", "M. V.", ""]]}, {"id": "1403.1430", "submitter": "Zhenfang Hu", "authors": "Zhenfang Hu, Gang Pan, Yueming Wang, and Zhaohui Wu", "title": "Sparse Principal Component Analysis via Rotation and Truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (sparse PCA) aims at finding a sparse\nbasis to improve the interpretability over the dense basis of PCA, meanwhile\nthe sparse basis should cover the data subspace as much as possible. In\ncontrast to most of existing work which deal with the problem by adding some\nsparsity penalties on various objectives of PCA, in this paper, we propose a\nnew method SPCArt, whose motivation is to find a rotation matrix and a sparse\nbasis such that the sparse basis approximates the basis of PCA after the\nrotation. The algorithm of SPCArt consists of three alternating steps: rotate\nPCA basis, truncate small entries, and update the rotation matrix. Its\nperformance bounds are also given. SPCArt is efficient, with each iteration\nscaling linearly with the data dimension. It is easy to choose parameters in\nSPCArt, due to its explicit physical explanations. Besides, we give a unified\nview to several existing sparse PCA methods and discuss the connection with\nSPCArt. Some ideas in SPCArt are extended to GPower, a popular sparse PCA\nalgorithm, to overcome its drawback. Experimental results demonstrate that\nSPCArt achieves the state-of-the-art performance. It also achieves a good\ntradeoff among various criteria, including sparsity, explained variance,\northogonality, balance of sparsity among loadings, and computational speed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 12:37:49 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 04:05:18 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Hu", "Zhenfang", ""], ["Pan", "Gang", ""], ["Wang", "Yueming", ""], ["Wu", "Zhaohui", ""]]}, {"id": "1403.1626", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu, Zhenyong Fu, Tao Xiang, Liwei Wang, and Ji-Rong Wen", "title": "Can Image-Level Labels Replace Pixel-Level Labels for Image Parsing", "comments": null, "journal-ref": "IEEE Trans. Pattern Anal. Mach. Intell. 39(3): 486-500 (2017)", "doi": "10.1109/TPAMI.2016.2552172", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a weakly supervised sparse learning approach to the\nproblem of noisily tagged image parsing, or segmenting all the objects within a\nnoisily tagged image and identifying their categories (i.e. tags). Different\nfrom the traditional image parsing that takes pixel-level labels as strong\nsupervisory information, our noisily tagged image parsing is provided with\nnoisy tags of all the images (i.e. image-level labels), which is a natural\nsetting for social image collections (e.g. Flickr). By oversegmenting all the\nimages into regions, we formulate noisily tagged image parsing as a weakly\nsupervised sparse learning problem over all the regions, where the initial\nlabels of each region are inferred from image-level labels. Furthermore, we\ndevelop an efficient algorithm to solve such weakly supervised sparse learning\nproblem. The experimental results on two benchmark datasets show the\neffectiveness of our approach. More notably, the reported surprising results\nshed some light on answering the question: can image-level labels replace\npixel-level labels (hard to access) as supervisory information for image\nparsing.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 00:39:49 GMT"}, {"version": "v2", "created": "Mon, 10 Mar 2014 01:21:00 GMT"}, {"version": "v3", "created": "Thu, 13 Nov 2014 11:42:54 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Lu", "Zhiwu", ""], ["Fu", "Zhenyong", ""], ["Xiang", "Tao", ""], ["Wang", "Liwei", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1403.1653", "submitter": "Marc Killpack", "authors": "Marc D. Killpack", "title": "Automated Tracking and Estimation for Control of Non-rigid Cloth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report is a summary of research conducted on cloth tracking for\nautomated textile manufacturing during a two semester long research course at\nGeorgia Tech. This work was completed in 2009. Advances in current sensing\ntechnology such as the Microsoft Kinect would now allow me to relax certain\nassumptions and generally improve the tracking performance. This is because a\nmajor part of my approach described in this paper was to track features in a 2D\nimage and use these to estimate the cloth deformation. Innovations such as the\nKinect would improve estimation due to the automatic depth information obtained\nwhen tracking 2D pixel locations. Additionally, higher resolution camera images\nwould probably give better quality feature tracking. However, although I would\nuse different technology now to implement this tracker, the algorithm described\nand implemented in this paper is still a viable approach which is why I am\npublishing this as a tech report for reference. In addition, although the\nrelated work is a bit exhaustive, it will be useful to a reader who is new to\nmethods for tracking and estimation as well as modeling of cloth.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 04:36:08 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Killpack", "Marc D.", ""]]}, {"id": "1403.1660", "submitter": "Neha Soorma", "authors": "Neha Soorma (M.TECH (DC) SSSIST, Sehore, M.P., India) Jaikaran Singh,\n  (Department of Electronics and Communication, SSSIST, Sehore, M.P. India)\n  Mukesh Tiwari (Department of Electronics and Communication, SSSIST, Sehore,\n  M.P. India)", "title": "Feature Extraction of ECG Signal Using HHT Algorithm", "comments": "7 pages,\"Published with International Journal of Engineering Trends\n  and Technology (IJETT)\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describe the features extraction algorithm for electrocardiogram\n(ECG) signal using Huang Hilbert Transform and Wavelet Transform. ECG signal\nfor an individual human being is different due to unique heart structure. The\npurpose of feature extraction of ECG signal would allow successful abnormality\ndetection and efficient prognosis due to heart disorder. Some major important\nfeatures will be extracted from ECG signals such as amplitude, duration,\npre-gradient, post-gradient and so on. Therefore, we need a strong mathematical\nmodel to extract such useful parameter. Here an adaptive mathematical analysis\nmodel is Hilbert-Huang transform (HHT). This new approach, the Hilbert-Huang\ntransform, is implemented to analyze the non-linear and nonstationary data. It\nis unique and different from the existing methods of data analysis and does not\nrequire an a priori functional basis. The effectiveness of the proposed scheme\nis verified through the simulation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 05:31:57 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Soorma", "Neha", "", "M.TECH"], ["Singh", "Jaikaran", "", "Department of Electronics and Communication, SSSIST, Sehore,\n  M.P. India"], ["Tiwari", "Mukesh", "", "Department of Electronics and Communication, SSSIST, Sehore,\n  M.P. India"]]}, {"id": "1403.1687", "submitter": "Laurent Sifre", "authors": "Laurent SIfre and St\\'ephane Mallat", "title": "Rigid-Motion Scattering for Texture Classification", "comments": "19 pages, submitted to International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rigid-motion scattering computes adaptive invariants along translations and\nrotations, with a deep convolutional network. Convolutions are calculated on\nthe rigid-motion group, with wavelets defined on the translation and rotation\nvariables. It preserves joint rotation and translation information, while\nproviding global invariants at any desired scale. Texture classification is\nstudied, through the characterization of stationary processes from a single\nrealization. State-of-the-art results are obtained on multiple texture data\nbases, with important rotation and scaling variabilities.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 08:57:12 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["SIfre", "Laurent", ""], ["Mallat", "St\u00e9phane", ""]]}, {"id": "1403.1697", "submitter": "Giulio Coluccia", "authors": "Simeon Kamdem Kuiteing, Giulio Coluccia, Alessandro Barducci, Mauro\n  Barni and Enrico Magli", "title": "Compressive Hyperspectral Imaging Using Progressive Total Variation", "comments": "To be published on ICASSP 2014 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed Sensing (CS) is suitable for remote acquisition of hyperspectral\nimages for earth observation, since it could exploit the strong spatial and\nspectral correlations, llowing to simplify the architecture of the onboard\nsensors. Solutions proposed so far tend to decouple spatial and spectral\ndimensions to reduce the complexity of the reconstruction, not taking into\naccount that onboard sensors progressively acquire spectral rows rather than\nacquiring spectral channels. For this reason, we propose a novel progressive CS\narchitecture based on separate sensing of spectral rows and joint\nreconstruction employing Total Variation. Experimental results run on raw\nAVIRIS and AIRS images confirm the validity of the proposed system.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 09:44:34 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Kuiteing", "Simeon Kamdem", ""], ["Coluccia", "Giulio", ""], ["Barducci", "Alessandro", ""], ["Barni", "Mauro", ""], ["Magli", "Enrico", ""]]}, {"id": "1403.1735", "submitter": "Ahmad Taher Azar Dr.", "authors": "Ahmed.H.Asad, Ahmad Taher Azar, Nashwa El-Bendary, Aboul Ella\n  Hassaanien", "title": "Ant Colony based Feature Selection Heuristics for Retinal Vessel\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Features selection is an essential step for successful data classification,\nsince it reduces the data dimensionality by removing redundant features.\nConsequently, that minimizes the classification complexity and time in addition\nto maximizing its accuracy. In this article, a comparative study considering\nsix features selection heuristics is conducted in order to select the best\nrelevant features subset. The tested features vector consists of fourteen\nfeatures that are computed for each pixel in the field of view of retinal\nimages in the DRIVE database. The comparison is assessed in terms of\nsensitivity, specificity, and accuracy measurements of the recommended features\nsubset resulted by each heuristic when applied with the ant colony system.\nExperimental results indicated that the features subset recommended by the\nrelief heuristic outperformed the subsets recommended by the other experienced\nheuristics.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 12:35:39 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Asad", "Ahmed. H.", ""], ["Azar", "Ahmad Taher", ""], ["El-Bendary", "Nashwa", ""], ["Hassaanien", "Aboul Ella", ""]]}, {"id": "1403.1840", "submitter": "Yunchao Gong", "authors": "Yunchao Gong and Liwei Wang and Ruiqi Guo and Svetlana Lazebnik", "title": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) have shown their promise as a\nuniversal representation for recognition. However, global CNN activations lack\ngeometric invariance, which limits their robustness for classification and\nmatching of highly variable scenes. To improve the invariance of CNN\nactivations without degrading their discriminative power, this paper presents a\nsimple but effective scheme called multi-scale orderless pooling (MOP-CNN).\nThis scheme extracts CNN activations for local patches at multiple scale\nlevels, performs orderless VLAD pooling of these activations at each level\nseparately, and concatenates the result. The resulting MOP-CNN representation\ncan be used as a generic feature for either supervised or unsupervised\nrecognition tasks, from image classification to instance-level retrieval; it\nconsistently outperforms global CNN activations without requiring any joint\ntraining of prediction layers for a particular target dataset. In absolute\nterms, it achieves state-of-the-art results on the challenging SUN397 and MIT\nIndoor Scenes classification datasets, and competitive results on\nILSVRC2012/2013 classification and INRIA Holidays retrieval datasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 19:03:15 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 17:38:52 GMT"}, {"version": "v3", "created": "Mon, 8 Sep 2014 22:03:21 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Gong", "Yunchao", ""], ["Wang", "Liwei", ""], ["Guo", "Ruiqi", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1403.1902", "submitter": "Soheil Bahrampour", "authors": "Soheil Bahrampour, Asok Ray, Nasser M. Nasrabadi, Kenneth W. Jenkins", "title": "Quality-based Multimodal Classification Using Tree-Structured Sparsity", "comments": "To Appear in 2014 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2014)", "journal-ref": "CVPR 2014, pp. 4114 - 4121", "doi": "10.1109/CVPR.2014.524", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated advantages of information fusion based on\nsparsity models for multimodal classification. Among several sparsity models,\ntree-structured sparsity provides a flexible framework for extraction of\ncross-correlated information from different sources and for enforcing group\nsparsity at multiple granularities. However, the existing algorithm only solves\nan approximated version of the cost functional and the resulting solution is\nnot necessarily sparse at group levels. This paper reformulates the\ntree-structured sparse model for multimodal classification task. An accelerated\nproximal algorithm is proposed to solve the optimization problem, which is an\nefficient tool for feature-level fusion among either homogeneous or\nheterogeneous sources of information. In addition, a (fuzzy-set-theoretic)\npossibilistic scheme is proposed to weight the available modalities, based on\ntheir respective reliability, in a joint optimization problem for finding the\nsparsity codes. This approach provides a general framework for quality-based\nfusion that offers added robustness to several sparsity-based multimodal\nclassification algorithms. To demonstrate their efficacy, the proposed methods\nare evaluated on three different applications - multiview face recognition,\nmultimodal face recognition, and target classification.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 00:00:15 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Bahrampour", "Soheil", ""], ["Ray", "Asok", ""], ["Nasrabadi", "Nasser M.", ""], ["Jenkins", "Kenneth W.", ""]]}, {"id": "1403.1937", "submitter": "Karthik Gurumoorthy", "authors": "Karthik S. Gurumoorthy, Adrian M. Peter, Birmingham Hang Guan and\n  Anand Rangarajan", "title": "A fast eikonal equation solver using the Schrodinger wave equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a Schr\\\"odinger wave equation formalism to solve the eikonal equation.\nIn our framework, a solution to the eikonal equation is obtained in the limit\nas Planck's constant $\\hbar$ (treated as a free parameter) tends to zero of the\nsolution to the corresponding linear Schr\\\"odinger equation. The Schr\\\"odinger\nequation corresponding to the eikonal turns out to be a \\emph{generalized,\nscreened Poisson equation}. Despite being linear, it does not have a\nclosed-form solution for arbitrary forcing functions. We present two different\ntechniques to solve the screened Poisson equation. In the first approach we use\na standard perturbation analysis approach to derive a new algorithm which is\nguaranteed to converge provided the forcing function is bounded and positive.\nThe perturbation technique requires a sequence of discrete convolutions which\ncan be performed in $O(N\\log N)$ using the Fast Fourier Transform (FFT) where\n$N$ is the number of grid points. In the second method we discretize the linear\nLaplacian operator by the finite difference method leading to a sparse linear\nsystem of equations which can be solved using the plethora of sparse solvers.\nThe eikonal solution is recovered from the exponent of the resultant scalar\nfield. Our approach eliminates the need to explicitly construct viscosity\nsolutions as customary with direct solutions to the eikonal. Since the linear\nequation is computed for a small but non-zero $\\hbar$, the obtained solution is\nan approximation. Though our solution framework is applicable to the general\nclass of eikonal problems, we detail specifics for the popular vision\napplications of shape-from-shading, vessel segmentation, and path planning.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 05:58:28 GMT"}, {"version": "v2", "created": "Sun, 8 Feb 2015 09:56:59 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Peter", "Adrian M.", ""], ["Guan", "Birmingham Hang", ""], ["Rangarajan", "Anand", ""]]}, {"id": "1403.1944", "submitter": "Ping Li PhD", "authors": "Ping Li and Hong Li and Min Wu", "title": "Multi-label ensemble based on variable pairwise constraint projection", "comments": "19 pages,5 tables, 2 figures; Published with Information Sciences\n  (INS)", "journal-ref": "Information Sciences, 222, 2013, pp.269-281.(Available online 7\n  August 2012)", "doi": "10.1016/j.ins.2012.07.066", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification has attracted an increasing amount of attention in\nrecent years. To this end, many algorithms have been developed to classify\nmulti-label data in an effective manner. However, they usually do not consider\nthe pairwise relations indicated by sample labels, which actually play\nimportant roles in multi-label classification. Inspired by this, we naturally\nextend the traditional pairwise constraints to the multi-label scenario via a\nflexible thresholding scheme. Moreover, to improve the generalization ability\nof the classifier, we adopt a boosting-like strategy to construct a multi-label\nensemble from a group of base classifiers. To achieve these goals, this paper\npresents a novel multi-label classification framework named Variable Pairwise\nConstraint projection for Multi-label Ensemble (VPCME). Specifically, we take\nadvantage of the variable pairwise constraint projection to learn a\nlower-dimensional data representation, which preserves the correlations between\nsamples and labels. Thereafter, the base classifiers are trained in the new\ndata space. For the boosting-like strategy, we employ both the variable\npairwise constraints and the bootstrap steps to diversify the base classifiers.\nEmpirical studies have shown the superiority of the proposed method in\ncomparison with other approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 07:20:05 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Li", "Ping", ""], ["Li", "Hong", ""], ["Wu", "Min", ""]]}, {"id": "1403.1974", "submitter": "Jaspinder Pal", "authors": "Jaspinder Pal Singh", "title": "Designing an FPGA Synthesizable Computer Vision Algorithm to Detect the\n  Greening of Potatoes", "comments": "5 pages, 8 figures, 2 tables, \"Published with International Journal\n  of Engineering Trends and Technology (IJETT)\" ISSN:2231-5381.\n  http://www.ijettjournal.org. published by seventh sense research group", "journal-ref": "International Journal of Engineering Trends and Technology(IJETT),\n  V8(8),438-442 February 2014", "doi": "10.14445/22315381/IJETT-V8P275", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potato quality control has improved in the last years thanks to automation\ntechniques like machine vision, mainly making the classification task between\ndifferent quality degrees faster, safer and less subjective. In our study we\nare going to design a computer vision algorithm for grading of potatoes\naccording to the greening of the surface color of potato. The ratio of green\npixels to the total number of pixels of the potato surface is found. The higher\nthe ratio the worse is the potato. First the image is converted into serial\ndata and then processing is done in RGB colour space. Green part of the potato\nis also shown by de-serializing the output. The same algorithm is then\nsynthesized on FPGA and the result shows thousand times speed improvement in\ncase of hardware synthesis.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 14:59:46 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Singh", "Jaspinder Pal", ""]]}, {"id": "1403.2031", "submitter": "Asha V", "authors": "V.Asha, N.U.Bhajantri, P.Nagabhushan", "title": "Texture Defect Detection in Gradient Space", "comments": "4 pages, ICFoCS-2011", "journal-ref": null, "doi": null, "report-no": "ICFoCS-2011", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a machine vision algorithm for automatically\ndetecting defects in patterned textures with the help of gradient space and its\nenergy. Experiments on real fabric images with defects show that the proposed\nmethod can be used for automatic detection of fabric defects in textile\nindustries.\n", "versions": [{"version": "v1", "created": "Sun, 9 Mar 2014 06:53:13 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Asha", "V.", ""], ["Bhajantri", "N. U.", ""], ["Nagabhushan", "P.", ""]]}, {"id": "1403.2295", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Sublinear Models for Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution extends linear models for feature vectors to sublinear\nmodels for graphs and analyzes their properties. The results are (i) a\ngeometric interpretation of sublinear classifiers, (ii) a generic learning rule\nbased on the principle of empirical risk minimization, (iii) a convergence\ntheorem for the margin perceptron in the sublinearly separable case, and (iv)\nthe VC-dimension of sublinear functions. Empirical results on graph data show\nthat sublinear models on graphs have similar properties as linear models for\nfeature vectors.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 16:36:23 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1403.2330", "submitter": "Chen Jie", "authors": "Jie Chen and Hua Mao and Yongsheng Sang and Zhang Yi", "title": "Subspace clustering using a symmetric low-rank representation", "comments": "12 pages", "journal-ref": null, "doi": "10.1016/j.knosys.2017.02.031", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a low-rank representation with symmetric constraint\n(LRRSC) method for robust subspace clustering. Given a collection of data\npoints approximately drawn from multiple subspaces, the proposed technique can\nsimultaneously recover the dimension and members of each subspace. LRRSC\nextends the original low-rank representation algorithm by integrating a\nsymmetric constraint into the low-rankness property of high-dimensional data\nrepresentation. The symmetric low-rank representation, which preserves the\nsubspace structures of high-dimensional data, guarantees weight consistency for\neach pair of data points so that highly correlated data points of subspaces are\nrepresented together. Moreover, it can be efficiently calculated by solving a\nconvex optimization problem. We provide a rigorous proof for minimizing the\nnuclear-norm regularized least square problem with a symmetric constraint. The\naffinity matrix for spectral clustering can be obtained by further exploiting\nthe angular information of the principal directions of the symmetric low-rank\nrepresentation. This is a critical step towards evaluating the memberships\nbetween data points. Experimental results on benchmark databases demonstrate\nthe effectiveness and robustness of LRRSC compared with several\nstate-of-the-art subspace clustering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 10:07:43 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 08:39:50 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 11:25:48 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Chen", "Jie", ""], ["Mao", "Hua", ""], ["Sang", "Yongsheng", ""], ["Yi", "Zhang", ""]]}, {"id": "1403.2395", "submitter": "Francisco Belch\\'i Guillam\\'on", "authors": "Francisco Belch\\'i Guillam\\'on and Aniceto Murillo Mas", "title": "A-infinity Persistence", "comments": "22 pages, no figures. In versions 2 and 3, we added our e-mail\n  addresses and made some minor corrections, thanks to Jim Stasheff", "journal-ref": "Appl. Algebra Engrg. Comm. Comput., 26(1-2):121-139, 2015", "doi": "10.1007/s00200-014-0241-4", "report-no": null, "categories": "math.AT cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study A-infinity persistence of a given homology filtration\nof topological spaces. This is a family, one for each n > 0, of homological\ninvariants which provide information not readily available by the (persistent)\nBetti numbers of the given filtration. This may help to detect noise, not just\nin the simplicial structure of the filtration but in further geometrical\nproperties in which the higher codiagonals of the A-infinity structure are\ntranslated. Based in the classification of zigzag modules, a characterization\nof the A-infinity persistence in terms of its associated barcode is given.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 20:06:24 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 15:30:24 GMT"}, {"version": "v3", "created": "Mon, 17 Mar 2014 10:43:28 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Guillam\u00f3n", "Francisco Belch\u00ed", ""], ["Mas", "Aniceto Murillo", ""]]}, {"id": "1403.2482", "submitter": "Haijuan Hu", "authors": "Haijuan Hu, Bing Li, Quansheng Liu", "title": "Removing Mixture of Gaussian and Impulse Noise by Patch-Based Weighted\n  Means", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first establish a law of large numbers and a convergence theorem in\ndistribution to show the rate of convergence of the non-local means filter for\nremoving Gaussian noise. We then introduce the notion of degree of similarity\nto measure the role of similarity for the non-local means filter. Based on the\nconvergence theorems, we propose a patch-based weighted means filter for\nremoving impulse noise and its mixture with Gaussian noise by combining the\nessential idea of the trilateral filter and that of the non-local means filter.\nOur experiments show that our filter is competitive compared to recently\nproposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 06:48:58 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Hu", "Haijuan", ""], ["Li", "Bing", ""], ["Liu", "Quansheng", ""]]}, {"id": "1403.2802", "submitter": "Zhimin Cao", "authors": "Haoqiang Fan, Zhimin Cao, Yuning Jiang, Qi Yin, Chinchilla Doudou", "title": "Learning Deep Face Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face representation is a crucial step of face recognition systems. An optimal\nface representation should be discriminative, robust, compact, and very\neasy-to-implement. While numerous hand-crafted and learning-based\nrepresentations have been proposed, considerable room for improvement is still\npresent. In this paper, we present a very easy-to-implement deep learning\nframework for face representation. Our method bases on a new structure of deep\nnetwork (called Pyramid CNN). The proposed Pyramid CNN adopts a\ngreedy-filter-and-down-sample operation, which enables the training procedure\nto be very fast and computation-efficient. In addition, the structure of\nPyramid CNN can naturally incorporate feature sharing across multi-scale face\nrepresentations, increasing the discriminative ability of resulting\nrepresentation. Our basic network is capable of achieving high recognition\naccuracy ($85.8\\%$ on LFW benchmark) with only 8 dimension representation. When\nextended to feature-sharing Pyramid CNN, our system achieves the\nstate-of-the-art performance ($97.3\\%$) on LFW benchmark. We also introduce a\nnew benchmark of realistic face images on social network and validate our\nproposed representation has a good ability of generalization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 03:47:18 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Fan", "Haoqiang", ""], ["Cao", "Zhimin", ""], ["Jiang", "Yuning", ""], ["Yin", "Qi", ""], ["Doudou", "Chinchilla", ""]]}, {"id": "1403.2871", "submitter": "Senosy Arrish", "authors": "Senosy Arrish, Fadhil Noer Afif, Ahmadu Maidorawa and Naomie Salim", "title": "Shape-Based Plagiarism Detection for Flowchart Figures in Texts", "comments": "12 pages", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 6, No 1, February 2014", "doi": "10.5121/ijcsit.2014.6108", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plagiarism detection is well known phenomenon in the academic arena. Copying\nother people is considered as serious offence that needs to be checked. There\nare many plagiarism detection systems such as turn-it-in that has been\ndeveloped to provide this checks. Most, if not all, discard the figures and\ncharts before checking for plagiarism. Discarding the figures and charts\nresults in look holes that people can take advantage. That means people can\nplagiarized figures and charts easily without the current plagiarism systems\ndetecting it. There are very few papers which talks about flowcharts plagiarism\ndetection. Therefore, there is a need to develop a system that will detect\nplagiarism in figures and charts. This paper presents a method for detecting\nflow chart figure plagiarism based on shape-based image processing and\nmultimedia retrieval. The method managed to retrieve flowcharts with ranked\nsimilarity according to different matching sets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 10:21:25 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Arrish", "Senosy", ""], ["Afif", "Fadhil Noer", ""], ["Maidorawa", "Ahmadu", ""], ["Salim", "Naomie", ""]]}, {"id": "1403.2895", "submitter": "Mario Mart\\'inez-Zarzuela", "authors": "M. Mart\\'inez-Zarzuela, M. Pedraza-Hueso, F.J. D\\'iaz-Pernas, D.\n  Gonz\\'alez-Ortega, M. Ant\\'on-Rodr\\'iguez", "title": "Indoor 3D Video Monitoring Using Multiple Kinect Depth-Cameras", "comments": null, "journal-ref": "International Journal of Multimedia & Its Applications 6(2014)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the design and development of a system for remote\nindoor 3D monitoring using an undetermined number of Microsoft(R) Kinect\nsensors. In the proposed client-server system, the Kinect cameras can be\nconnected to different computers, addressing this way the hardware limitation\nof one sensor per USB controller. The reason behind this limitation is the high\nbandwidth needed by the sensor, which becomes also an issue for the distributed\nsystem TCP/IP communications. Since traffic volume is too high, 3D data has to\nbe compressed before it can be sent over the network. The solution consists in\nselfcoding the Kinect data into RGB images and then using a standard multimedia\ncodec to compress color maps. Information from different sources is collected\ninto a central client computer, where point clouds are transformed to\nreconstruct the scene in 3D. An algorithm is proposed to merge the skeletons\ndetected locally by each Kinect conveniently, so that monitoring of people is\nrobust to self and inter-user occlusions. Final skeletons are labeled and\ntrajectories of every joint can be saved for event reconstruction or further\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 12:01:24 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Mart\u00ednez-Zarzuela", "M.", ""], ["Pedraza-Hueso", "M.", ""], ["D\u00edaz-Pernas", "F. J.", ""], ["Gonz\u00e1lez-Ortega", "D.", ""], ["Ant\u00f3n-Rodr\u00edguez", "M.", ""]]}, {"id": "1403.2980", "submitter": "Rocio Gonzalez-Diaz", "authors": "Rocio Gonzalez-Diaz, Maria-Jose Jimenez, Belen Medrano", "title": "3D Well-composed Polyhedral Complexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A binary three-dimensional (3D) image $I$ is well-composed if the boundary\nsurface of its continuous analog is a 2D manifold. Since 3D images are not\noften well-composed, there are several voxel-based methods (\"repairing\"\nalgorithms) for turning them into well-composed ones but these methods either\ndo not guarantee the topological equivalence between the original image and its\ncorresponding well-composed one or involve sub-sampling the whole image.\n  In this paper, we present a method to locally \"repair\" the cubical complex\n$Q(I)$ (embedded in $\\mathbb{R}^3$) associated to $I$ to obtain a polyhedral\ncomplex $P(I)$ homotopy equivalent to $Q(I)$ such that the boundary of every\nconnected component of $P(I)$ is a 2D manifold. The reparation is performed via\na new codification system for $P(I)$ under the form of a 3D grayscale image\nthat allows an efficient access to cells and their faces.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 15:44:17 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Gonzalez-Diaz", "Rocio", ""], ["Jimenez", "Maria-Jose", ""], ["Medrano", "Belen", ""]]}, {"id": "1403.3021", "submitter": "Guo-Niu Han", "authors": "Huazhong Shu (CRIBS, LIST), Jian Zhou (CRIBS, LTSI), Guo-Niu Han\n  (IRMA), Limin M. Luo (CRIBS, LIST), Jean-Louis Coatrieux (CRIBS, LTSI)", "title": "Image reconstruction from limited range projections using orthogonal\n  moments", "comments": null, "journal-ref": "Pattern Recognition 40, 2 (2007) 670-680", "doi": "10.1016/j.patcog.2006.05.035", "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of orthonormal polynomials is proposed for image reconstruction from\nprojection data. The relationship between the projection moments and image\nmoments is discussed in detail, and some interesting properties are\ndemonstrated. Simulation results are provided to validate the method and to\ncompare its performance with previous works.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 16:44:20 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Shu", "Huazhong", "", "CRIBS, LIST"], ["Zhou", "Jian", "", "CRIBS, LTSI"], ["Han", "Guo-Niu", "", "IRMA"], ["Luo", "Limin M.", "", "CRIBS, LIST"], ["Coatrieux", "Jean-Louis", "", "CRIBS, LTSI"]]}, {"id": "1403.3022", "submitter": "Guo-Niu Han", "authors": "Guanyu Yang (LTSI, CRIBS, LIST), Huazhong Shu (CRIBS, LIST), Christine\n  Toumoulin (LTSI, CRIBS), Guo-Niu Han (IRMA), Limin M. Luo (CRIBS, LIST)", "title": "Efficient Legendre moment computation for grey level images", "comments": null, "journal-ref": "Pattern Recognition 39, 1 (2006) 74-80", "doi": "10.1016/j.patcog.2005.08.008", "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legendre orthogonal moments have been widely used in the field of image\nanalysis. Because their computation by a direct method is very time expensive,\nrecent efforts have been devoted to the reduction of computational complexity.\nNevertheless, the existing algorithms are mainly focused on binary images. We\npropose here a new fast method for computing the Legendre moments, which is not\nonly suitable for binary images but also for grey levels. We first set up the\nrecurrence formula of one-dimensional (1D) Legendre moments by using the\nrecursive property of Legendre polynomials. As a result, the 1D Legendre\nmoments of order p, Lp = Lp(0), can be expressed as a linear combination of\nLp-1(1) and Lp-2(0). Based on this relationship, the 1D Legendre moments Lp(0)\nis thus obtained from the array of L1(a) and L0(a) where a is an integer number\nless than p. To further decrease the computation complexity, an algorithm, in\nwhich no multiplication is required, is used to compute these quantities. The\nmethod is then extended to the calculation of the two-dimensional Legendre\nmoments Lpq. We show that the proposed method is more efficient than the direct\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 16:44:47 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Yang", "Guanyu", "", "LTSI, CRIBS, LIST"], ["Shu", "Huazhong", "", "CRIBS, LIST"], ["Toumoulin", "Christine", "", "LTSI, CRIBS"], ["Han", "Guo-Niu", "", "IRMA"], ["Luo", "Limin M.", "", "CRIBS, LIST"]]}, {"id": "1403.3057", "submitter": "Maicon Sartin M.Sc.", "authors": "Maicon A. Sartin and Alexandre C. R. da Silva", "title": "Evaluation of Image Segmentation and Filtering With ANN in the Papaya\n  Leaf", "comments": "12 pages", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 6 No 1 (2014) 47-58", "doi": "10.5121/ijcsit.2014.6104", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision agriculture is area with lack of cheap technology. The refinement\nof the production system brings large advantages to the producer and the use of\nimages makes the monitoring a more cheap methodology. Macronutrients monitoring\ncan to determine the health and vulnerability of the plant in specific stages.\nIn this paper is analyzed the method based on computational intelligence to\nwork with image segmentation in the identification of symptoms of plant\nnutrient deficiency. Artificial neural networks are evaluated for image\nsegmentation and filtering, several variations of parameters and insertion\nimpulsive noise were evaluated too. Satisfactory results are achieved with\nartificial neural for segmentation same with high noise levels.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 18:32:16 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Sartin", "Maicon A.", ""], ["da Silva", "Alexandre C. R.", ""]]}, {"id": "1403.3083", "submitter": "Shuliang Wang", "authors": "Shuliang Wang, Yasen Chen", "title": "A Novel Method to Extract Rocks from Mars Images", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 2", "journal-ref": "Chinese Journal of Electronics,Vol.24, No.3, July 2015, pp.455-461", "doi": "10.1049/cje.2015.07.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel method is proposed to extract rocks from Martian\nsurface images by using 8 data field. It models the interaction between two\npixels of an image in the context of imagery 9 characteristics. First,\nforeground rocks are differed from background information by binarizing 10\nimage on roughly partitioned images. Second, foreground rocks are grouped into\nclusters by 11 locating the centers and edges of clusters in data field via\nhierarchical grids. Third, the target 12 rocks are discovered for the Mars\nExploration Rover (MER) to keep healthy paths. The 13 experiment with images\ntaken by MER shows the proposed method is practical and potential.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 06:23:09 GMT"}, {"version": "v2", "created": "Sun, 2 Nov 2014 01:08:06 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Wang", "Shuliang", ""], ["Chen", "Yasen", ""]]}, {"id": "1403.3118", "submitter": "Rodrigo da Silva Moreira", "authors": "Rodrigo da Silva Moreira and Nelson Francisco Favilla Ebecken", "title": "Parallel WiSARD object tracker: a ram-based tracking system", "comments": "15 pages, 7 figures", "journal-ref": "Computer Science & Engineering: An International Journal (CSEIJ),\n  Vol. 4, No. 1, February 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the Parallel WiSARD Object Tracker (PWOT), a new object\ntracker based on the WiSARD weightless neural network that is robust against\nquantization errors. Object tracking in video is an important and challenging\ntask in many applications. Difficulties can arise due to weather conditions,\ntarget trajectory and appearance, occlusions, lighting conditions and noise.\nTracking is a high-level application and requires the object location frame by\nframe in real time. This paper proposes a fast hybrid image segmentation\n(threshold and edge detection) in YcbCr color model and a parallel RAM based\ndiscriminator that improves efficiency when quantization errors occur. The\noriginal WiSARD training algorithm was changed to allow the tracking.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 21:23:52 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Moreira", "Rodrigo da Silva", ""], ["Ebecken", "Nelson Francisco Favilla", ""]]}, {"id": "1403.3155", "submitter": "Fei-Yun Zhu", "authors": "Feiyun Zhu, Ying Wang, Bin Fan, Gaofeng Meng, Shiming Xiang and\n  Chunhong Pan", "title": "Spectral Unmixing via Data-guided Sparsity", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2363423", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral unmixing, the process of estimating a common set of spectral\nbases and their corresponding composite percentages at each pixel, is an\nimportant task for hyperspectral analysis, visualization and understanding.\nFrom an unsupervised learning perspective, this problem is very\nchallenging---both the spectral bases and their composite percentages are\nunknown, making the solution space too large. To reduce the solution space,\nmany approaches have been proposed by exploiting various priors. In practice,\nthese priors would easily lead to some unsuitable solution. This is because\nthey are achieved by applying an identical strength of constraints to all the\nfactors, which does not hold in practice. To overcome this limitation, we\npropose a novel sparsity based method by learning a data-guided map to describe\nthe individual mixed level of each pixel. Through this data-guided map, the\n$\\ell_{p}(0<p<1)$ constraint is applied in an adaptive manner. Such\nimplementation not only meets the practical situation, but also guides the\nspectral bases toward the pixels under highly sparse constraint. What's more,\nan elegant optimization scheme as well as its convergence proof have been\nprovided in this paper. Extensive experiments on several datasets also\ndemonstrate that the data-guided map is feasible, and high quality unmixing\nresults could be obtained by our method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 03:29:22 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 13:49:14 GMT"}, {"version": "v3", "created": "Fri, 19 Sep 2014 02:59:51 GMT"}, {"version": "v4", "created": "Mon, 17 Nov 2014 15:18:15 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Zhu", "Feiyun", ""], ["Wang", "Ying", ""], ["Fan", "Bin", ""], ["Meng", "Gaofeng", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1403.3320", "submitter": "Jiong Zhang", "authors": "Jiong Zhang, Remco Duits, Gonzalo Sanguinetti and Bart M. ter Haar\n  Romeny", "title": "Numerical Approaches for Linear Left-invariant Diffusions on SE(2),\n  their Comparison to Exact Solutions, and their Applications in Retinal\n  Imaging", "comments": "A final and corrected version of the manuscript is Published in\n  Numerical Mathematics: Theory, Methods and Applications (NM-TMA), vol. (9),\n  p.1-50, 2016", "journal-ref": "Numerical Mathematics: Theory, Methods and Applications (NM-TMA),\n  vol. (9), p.1-50, 2016", "doi": "10.4208/nmtma.2015.m1411", "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Left-invariant PDE-evolutions on the roto-translation group $SE(2)$ (and\ntheir resolvent equations) have been widely studied in the fields of cortical\nmodeling and image analysis. They include hypo-elliptic diffusion (for contour\nenhancement) proposed by Citti & Sarti, and Petitot, and they include the\ndirection process (for contour completion) proposed by Mumford. This paper\npresents a thorough study and comparison of the many numerical approaches,\nwhich, remarkably, is missing in the literature. Existing numerical approaches\ncan be classified into 3 categories: Finite difference methods, Fourier based\nmethods (equivalent to $SE(2)$-Fourier methods), and stochastic methods (Monte\nCarlo simulations). There are also 3 types of exact solutions to the\nPDE-evolutions that were derived explicitly (in the spatial Fourier domain) in\nprevious works by Duits and van Almsick in 2005. Here we provide an overview of\nthese 3 types of exact solutions and explain how they relate to each of the 3\nnumerical approaches. We compute relative errors of all numerical approaches to\nthe exact solutions, and the Fourier based methods show us the best performance\nwith smallest relative errors. We also provide an improvement of Mathematica\nalgorithms for evaluating Mathieu-functions, crucial in implementations of the\nexact solutions. Furthermore, we include an asymptotical analysis of the\nsingularities within the kernels and we propose a probabilistic extension of\nunderlying stochastic processes that overcomes the singular behavior in the\norigin of time-integrated kernels. Finally, we show retinal imaging\napplications of combining left-invariant PDE-evolutions with invertible\norientation scores.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 16:35:24 GMT"}, {"version": "v2", "created": "Mon, 17 Mar 2014 21:22:54 GMT"}, {"version": "v3", "created": "Sat, 29 Mar 2014 23:38:31 GMT"}, {"version": "v4", "created": "Fri, 4 Apr 2014 15:03:59 GMT"}, {"version": "v5", "created": "Mon, 2 Mar 2015 23:38:32 GMT"}, {"version": "v6", "created": "Fri, 4 Dec 2015 18:11:32 GMT"}, {"version": "v7", "created": "Tue, 1 Mar 2016 17:29:36 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Zhang", "Jiong", ""], ["Duits", "Remco", ""], ["Sanguinetti", "Gonzalo", ""], ["Romeny", "Bart M. ter Haar", ""]]}, {"id": "1403.3522", "submitter": "Dirk Lorenz", "authors": "Dirk A. Lorenz, Thomas Pock", "title": "An inertial forward-backward algorithm for monotone inclusions", "comments": "The final publication is available at http://link.springer.com", "journal-ref": null, "doi": "10.1007/s10851-014-0523-2", "report-no": null, "categories": "cs.CV cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an inertial forward backward splitting algorithm to\ncompute a zero of the sum of two monotone operators, with one of the two\noperators being co-coercive. The algorithm is inspired by the accelerated\ngradient method of Nesterov, but can be applied to a much larger class of\nproblems including convex-concave saddle point problems and general monotone\ninclusions. We prove convergence of the algorithm in a Hilbert space setting\nand show that several recently proposed first-order methods can be obtained as\nspecial cases of the general algorithm. Numerical results show that the\nproposed algorithm converges faster than existing methods, while keeping the\ncomputational cost of each iteration basically unchanged.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 10:30:47 GMT"}, {"version": "v2", "created": "Fri, 12 Sep 2014 10:54:45 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Lorenz", "Dirk A.", ""], ["Pock", "Thomas", ""]]}, {"id": "1403.3602", "submitter": "Yogachandran Rahulamathavan", "authors": "Segun Aina, Yogachandran Rahulamathavan, Raphael C.-W. Phan, Jonathon\n  A. Chambers", "title": "Spontaneous expression classification in the encrypted domain", "comments": "4 pages. 9th IMA International Conference on Mathematics in Signal\n  Processing, Birmingham, UK, Dec. 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, most facial expression analysis have been based on posed image\ndatabases and is carried out without being able to protect the identity of the\nsubjects whose expressions are being recognised. In this paper, we propose and\nimplement a system for classifying facial expressions of images in the\nencrypted domain based on a Paillier cryptosystem implementation of Fisher\nLinear Discriminant Analysis and k-nearest neighbour (FLDA + kNN). We present\nresults of experiments carried out on a recently developed natural visible and\ninfrared facial expression (NVIE) database of spontaneous images. To the best\nof our knowledge, this is the first system that will allow the recog-nition of\nencrypted spontaneous facial expressions by a remote server on behalf of a\nclient.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 15:19:30 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Aina", "Segun", ""], ["Rahulamathavan", "Yogachandran", ""], ["Phan", "Raphael C. -W.", ""], ["Chambers", "Jonathon A.", ""]]}, {"id": "1403.3683", "submitter": "Rocio Gonzalez-Diaz", "authors": "Guillaume Damiand, Rocio Gonzalez-Diaz, Samuel Peltier", "title": "Removal and Contraction Operations in $n$D Generalized Maps for\n  Efficient Homology Computation", "comments": "Research report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that contraction operations preserve the homology of\n$n$D generalized maps, under some conditions. Removal and contraction\noperations are used to propose an efficient algorithm that compute homology\ngenerators of $n$D generalized maps. Its principle consists in simplifying a\ngeneralized map as much as possible by using removal and contraction\noperations. We obtain a generalized map having the same homology than the\ninitial one, while the number of cells decreased significantly.\n  Keywords: $n$D Generalized Maps; Cellular Homology; Homology Generators;\nContraction and Removal Operations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 19:45:56 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Damiand", "Guillaume", ""], ["Gonzalez-Diaz", "Rocio", ""], ["Peltier", "Samuel", ""]]}, {"id": "1403.3724", "submitter": "William Gray Roncal", "authors": "William Gray Roncal, Michael Pekala, Verena Kaynig-Fittkau, Dean M.\n  Kleissas, Joshua T. Vogelstein, Hanspeter Pfister, Randal Burns, R. Jacob\n  Vogelstein, Mark A. Chevillet, Gregory D. Hager", "title": "VESICLE: Volumetric Evaluation of Synaptic Interfaces using Computer\n  vision at Large Scale", "comments": "v4: added clarifying figures and updates for readability. v3: fixed\n  metadata. 11 pp v2: Added CNN classifier, significant changes to improve\n  performance and generalization", "journal-ref": "Proceedings of the British Machine Vision Conference (BMVC), pages\n  81.1-81.13. BMVA Press, September 2015", "doi": null, "report-no": null, "categories": "cs.CV cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open challenge problem at the forefront of modern neuroscience is to\nobtain a comprehensive mapping of the neural pathways that underlie human brain\nfunction; an enhanced understanding of the wiring diagram of the brain promises\nto lead to new breakthroughs in diagnosing and treating neurological disorders.\nInferring brain structure from image data, such as that obtained via electron\nmicroscopy (EM), entails solving the problem of identifying biological\nstructures in large data volumes. Synapses, which are a key communication\nstructure in the brain, are particularly difficult to detect due to their small\nsize and limited contrast. Prior work in automated synapse detection has relied\nupon time-intensive biological preparations (post-staining, isotropic slice\nthicknesses) in order to simplify the problem.\n  This paper presents VESICLE, the first known approach designed for mammalian\nsynapse detection in anisotropic, non-post-stained data. Our methods explicitly\nleverage biological context, and the results exceed existing synapse detection\nmethods in terms of accuracy and scalability. We provide two different\napproaches - one a deep learning classifier (VESICLE-CNN) and one a lightweight\nRandom Forest approach (VESICLE-RF) to offer alternatives in the\nperformance-scalability space. Addressing this synapse detection challenge\nenables the analysis of high-throughput imaging data soon expected to reach\npetabytes of data, and provide tools for more rapid estimation of brain-graphs.\nFinally, to facilitate community efforts, we developed tools for large-scale\nobject detection, and demonstrated this framework to find $\\approx$ 50,000\nsynapses in 60,000 $\\mu m ^3$ (220 GB on disk) of electron microscopy data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 23:16:36 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 16:53:05 GMT"}, {"version": "v3", "created": "Thu, 14 May 2015 01:01:16 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2015 21:41:20 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Roncal", "William Gray", ""], ["Pekala", "Michael", ""], ["Kaynig-Fittkau", "Verena", ""], ["Kleissas", "Dean M.", ""], ["Vogelstein", "Joshua T.", ""], ["Pfister", "Hanspeter", ""], ["Burns", "Randal", ""], ["Vogelstein", "R. Jacob", ""], ["Chevillet", "Mark A.", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1403.3780", "submitter": "Conrad Sanderson", "authors": "Arnold Wiliem, Conrad Sanderson, Yongkang Wong, Peter Hobson, Rodney\n  F. Minchin, Brian C. Lovell", "title": "Automatic Classification of Human Epithelial Type 2 Cell Indirect\n  Immunofluorescence Images using Cell Pyramid Matching", "comments": "arXiv admin note: substantial text overlap with arXiv:1304.1262", "journal-ref": "Pattern Recognition, Vol. 47, No. 7, pp. 2315-2324, 2014", "doi": "10.1016/j.patcog.2013.10.014", "report-no": null, "categories": "q-bio.CB cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel system for automatic classification of images\nobtained from Anti-Nuclear Antibody (ANA) pathology tests on Human Epithelial\ntype 2 (HEp-2) cells using the Indirect Immunofluorescence (IIF) protocol. The\nIIF protocol on HEp-2 cells has been the hallmark method to identify the\npresence of ANAs, due to its high sensitivity and the large range of antigens\nthat can be detected. However, it suffers from numerous shortcomings, such as\nbeing subjective as well as time and labour intensive. Computer Aided\nDiagnostic (CAD) systems have been developed to address these problems, which\nautomatically classify a HEp-2 cell image into one of its known patterns (eg.\nspeckled, homogeneous). Most of the existing CAD systems use handpicked\nfeatures to represent a HEp-2 cell image, which may only work in limited\nscenarios. We propose a novel automatic cell image classification method termed\nCell Pyramid Matching (CPM), which is comprised of regional histograms of\nvisual words coupled with the Multiple Kernel Learning framework. We present a\nstudy of several variations of generating histograms and show the efficacy of\nthe system on two publicly available datasets: the ICPR HEp-2 cell\nclassification contest dataset and the SNPHEp-2 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 15 Mar 2014 10:15:25 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Wiliem", "Arnold", ""], ["Sanderson", "Conrad", ""], ["Wong", "Yongkang", ""], ["Hobson", "Peter", ""], ["Minchin", "Rodney F.", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1403.3829", "submitter": "Wei  Di", "authors": "Zixuan Wang, Wei Di, Anurag Bhardwaj, Vignesh Jagadeesh, Robinson\n  Piramuthu", "title": "Geometric VLAD for Large Scale Image Search", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present a novel compact image descriptor for large scale image search. Our\nproposed descriptor - Geometric VLAD (gVLAD) is an extension of VLAD (Vector of\nLocally Aggregated Descriptors) that incorporates weak geometry information\ninto the VLAD framework. The proposed geometry cues are derived as a membership\nfunction over keypoint angles which contain evident and informative information\nbut yet often discarded. A principled technique for learning the membership\nfunction by clustering angles is also presented. Further, to address the\noverhead of iterative codebook training over real-time datasets, a novel\ncodebook adaptation strategy is outlined. Finally, we demonstrate the efficacy\nof proposed gVLAD based retrieval framework where we achieve more than 15%\nimprovement in mAP over existing benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 15 Mar 2014 17:35:26 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Wang", "Zixuan", ""], ["Di", "Wei", ""], ["Bhardwaj", "Anurag", ""], ["Jagadeesh", "Vignesh", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1403.3964", "submitter": "Hirotaka Niitsuma", "authors": "Hirotaka Niitsuma", "title": "Image processing using miniKanren", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An integral image is one of the most efficient optimization technique for\nimage processing. However an integral image is only a special case of delayed\nstream or memoization. This research discusses generalizing concept of integral\nimage optimization technique, and how to generate an integral image optimized\nprogram code automatically from abstracted image processing algorithm. In oder\nto abstruct algorithms, we forces to miniKanren.\n", "versions": [{"version": "v1", "created": "Sun, 16 Mar 2014 22:03:45 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Niitsuma", "Hirotaka", ""]]}, {"id": "1403.4232", "submitter": "Tanushri Chakravorty", "authors": "Tanushri Chakravorty, Guillaume-Alexandre Bilodeau, Eric Granger", "title": "Automatic Image Registration in Infrared-Visible Videos using Polygon\n  Vertices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an automatic method is proposed to perform image registration\nin visible and infrared pair of video sequences for multiple targets. In\nmultimodal image analysis like image fusion systems, color and IR sensors are\nplaced close to each other and capture a same scene simultaneously, but the\nvideos are not properly aligned by default because of different fields of view,\nimage capturing information, working principle and other camera specifications.\nBecause the scenes are usually not planar, alignment needs to be performed\ncontinuously by extracting relevant common information. In this paper, we\napproximate the shape of the targets by polygons and use affine transformation\nfor aligning the two video sequences. After background subtraction, keypoints\non the contour of the foreground blobs are detected using DCE (Discrete Curve\nEvolution)technique. These keypoints are then described by the local shape at\neach point of the obtained polygon. The keypoints are matched based on the\nconvexity of polygon's vertices and Euclidean distance between them. Only good\nmatches for each local shape polygon in a frame, are kept. To achieve a global\naffine transformation that maximises the overlapping of infrared and visible\nforeground pixels, the matched keypoints of each local shape polygon are stored\ntemporally in a buffer for a few number of frames. The matrix is evaluated at\neach frame using the temporal buffer and the best matrix is selected, based on\nan overlapping ratio criterion. Our experimental results demonstrate that this\nmethod can provide highly accurate registered images and that we outperform a\nprevious related method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 19:58:14 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Chakravorty", "Tanushri", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Granger", "Eric", ""]]}, {"id": "1403.4238", "submitter": "Guohui Wang", "authors": "Guohui Wang, Yingen Xiong, Jay Yun, and Joseph R. Cavallaro", "title": "Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU\n  Co-Processing", "comments": "15 pages, 15 figures. Submitted and accepted for publication in\n  Journal of Signal Processing Systems, 2014", "journal-ref": null, "doi": "10.1007/s11265-014-0878-z", "report-no": null, "categories": "cs.DC cs.CV cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an OpenCL-based heterogeneous implementation of a\ncomputer vision algorithm -- image inpainting-based object removal algorithm --\non mobile devices. To take advantage of the computation power of the mobile\nprocessor, the algorithm workflow is partitioned between the CPU and the GPU\nbased on the profiling results on mobile devices, so that the\ncomputationally-intensive kernels are accelerated by the mobile GPGPU\n(general-purpose computing using graphics processing units). By exploring the\nimplementation trade-offs and utilizing the proposed optimization strategies at\ndifferent levels including algorithm optimization, parallelism optimization,\nand memory access optimization, we significantly speed up the algorithm with\nthe CPU-GPU heterogeneous implementation, while preserving the quality of the\noutput images. Experimental results show that heterogeneous computing based on\nGPGPU co-processing can significantly speed up the computer vision algorithms\nand makes them practical on real-world mobile devices.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 18:26:41 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Wang", "Guohui", ""], ["Xiong", "Yingen", ""], ["Yun", "Jay", ""], ["Cavallaro", "Joseph R.", ""]]}, {"id": "1403.4334", "submitter": "Mehrtash Harandi", "authors": "Mehrtash Harandi and Mathieu Salzmann and Fatih Porikli", "title": "Bregman Divergences for Infinite Dimensional Covariance Matrices", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to computing and comparing Covariance Descriptors\n(CovDs) in infinite-dimensional spaces. CovDs have become increasingly popular\nto address classification problems in computer vision. While CovDs offer some\nrobustness to measurement variations, they also throw away part of the\ninformation contained in the original data by only retaining the second-order\nstatistics over the measurements. Here, we propose to overcome this limitation\nby first mapping the original data to a high-dimensional Hilbert space, and\nonly then compute the CovDs. We show that several Bregman divergences can be\ncomputed between the resulting CovDs in Hilbert space via the use of kernels.\nWe then exploit these divergences for classification purposes. Our experiments\ndemonstrate the benefits of our approach on several tasks, such as material and\ntexture recognition, person re-identification, and action recognition from\nmotion capture data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 03:55:33 GMT"}, {"version": "v2", "created": "Wed, 19 Mar 2014 00:52:09 GMT"}, {"version": "v3", "created": "Wed, 11 Jun 2014 08:15:19 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Harandi", "Mehrtash", ""], ["Salzmann", "Mathieu", ""], ["Porikli", "Fatih", ""]]}, {"id": "1403.4682", "submitter": "Fei-Yun Zhu", "authors": "Feiyun Zhu, Ying Wang, Shiming Xiang, Bin Fan, Chunhong Pan", "title": "Structured Sparse Method for Hyperspectral Unmixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral Unmixing (HU) has received increasing attention in the past\ndecades due to its ability of unveiling information latent in hyperspectral\ndata. Unfortunately, most existing methods fail to take advantage of the\nspatial information in data. To overcome this limitation, we propose a\nStructured Sparse regularized Nonnegative Matrix Factorization (SS-NMF) method\nfrom the following two aspects. First, we incorporate a graph Laplacian to\nencode the manifold structures embedded in the hyperspectral data space. In\nthis way, the highly similar neighboring pixels can be grouped together.\nSecond, the lasso penalty is employed in SS-NMF for the fact that pixels in the\nsame manifold structure are sparsely mixed by a common set of relevant bases.\nThese two factors act as a new structured sparse constraint. With this\nconstraint, our method can learn a compact space, where highly similar pixels\nare grouped to share correlated sparse representations. Experiments on real\nhyperspectral data sets with different noise levels demonstrate that our method\noutperforms the state-of-the-art methods significantly.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 03:23:30 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Zhu", "Feiyun", ""], ["Wang", "Ying", ""], ["Xiang", "Shiming", ""], ["Fan", "Bin", ""], ["Pan", "Chunhong", ""]]}, {"id": "1403.4777", "submitter": "Inma Mohino-Herranz", "authors": "Inma Mohino-Herranz, Roberto Gil-Pita, Sagrario Alonso-Diaz and Manuel\n  Rosa-Zurera", "title": "MFCC based Enlargement of the Training Set for Emotion Recognition in\n  Speech", "comments": "Signal & Image Processing : An International Journal (SIPIJ)", "journal-ref": "Signal & Image Processing : An International Journal (SIPIJ),\n  Vol.5, No,1. 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional state recognition through speech is being a very interesting\nresearch topic nowadays. Using subliminal information of speech, denominated as\nprosody, it is possible to recognize the emotional state of the person. One of\nthe main problems in the design of automatic emotion recognition systems is the\nsmall number of available patterns. This fact makes the learning process more\ndifficult, due to the generalization problems that arise under these\nconditions. In this work we propose a solution to this problem consisting in\nenlarging the training set through the creation the new virtual patterns. In\nthe case of emotional speech, most of the emotional information is included in\nspeed and pitch variations. So, a change in the average pitch that does not\nmodify neither the speed nor the pitch variations does not affect the expressed\nemotion. Thus, we use this prior information in order to create new patterns\napplying a gender dependent pitch shift modification in the feature extraction\nprocess of the classification system. For this purpose, we propose a frequency\nscaling modification of the Mel Frequency Cepstral Coefficients, used to\nclassify the emotion. For this purpose, we propose a gender dependent frequency\nscaling modification. This proposed process allows us to synthetically increase\nthe number of available patterns in the training set, thus increasing the\ngeneralization capability of the system and reducing the test error. Results\ncarried out with two different classifiers with different degree of\ngeneralization capability demonstrate the suitability of the proposal.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 12:03:09 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Mohino-Herranz", "Inma", ""], ["Gil-Pita", "Roberto", ""], ["Alonso-Diaz", "Sagrario", ""], ["Rosa-Zurera", "Manuel", ""]]}, {"id": "1403.5370", "submitter": "Mathieu Dubois", "authors": "Mathieu Dubois (LIMSI), Frenoux Emmanuelle (LIMSI), Philippe Tarroux\n  (LIMSI)", "title": "Using n-grams models for visual semantic place recognition", "comments": "VISAPP (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present a new method for visual place\nrecognition. Our system combines global image characterization and visual\nwords, which allows to use efficient Bayesian filtering methods to integrate\nseveral images. More precisely, we extend the classical HMM model with\ntechniques inspired by the field of Natural Language Processing. This paper\npresents our system and the Bayesian filtering algorithm. The performance of\nour system and the influence of the main parameters are evaluated on a standard\ndatabase. The discussion highlights the interest of using such models and\nproposes improvements.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 05:23:17 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Dubois", "Mathieu", "", "LIMSI"], ["Emmanuelle", "Frenoux", "", "LIMSI"], ["Tarroux", "Philippe", "", "LIMSI"]]}, {"id": "1403.5403", "submitter": "Giovanni Chierchia", "authors": "Giovanni Chierchia, Nelly Pustelnik, Beatrice Pesquet-Popescu,\n  Jean-Christophe Pesquet", "title": "A Non-Local Structure Tensor Based Approach for Multicomponent Image\n  Recovery Problems", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2364141", "report-no": null, "categories": "cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Local Total Variation (NLTV) has emerged as a useful tool in variational\nmethods for image recovery problems. In this paper, we extend the NLTV-based\nregularization to multicomponent images by taking advantage of the Structure\nTensor (ST) resulting from the gradient of a multicomponent image. The proposed\napproach allows us to penalize the non-local variations, jointly for the\ndifferent components, through various $\\ell_{1,p}$ matrix norms with $p \\ge 1$.\nTo facilitate the choice of the hyper-parameters, we adopt a constrained convex\noptimization approach in which we minimize the data fidelity term subject to a\nconstraint involving the ST-NLTV regularization. The resulting convex\noptimization problem is solved with a novel epigraphical projection method.\nThis formulation can be efficiently implemented thanks to the flexibility\noffered by recent primal-dual proximal algorithms. Experiments are carried out\nfor multispectral and hyperspectral images. The results demonstrate the\ninterest of introducing a non-local structure tensor regularization and show\nthat the proposed approach leads to significant improvements in terms of\nconvergence speed over current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 09:30:20 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 20:24:25 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Chierchia", "Giovanni", ""], ["Pustelnik", "Nelly", ""], ["Pesquet-Popescu", "Beatrice", ""], ["Pesquet", "Jean-Christophe", ""]]}, {"id": "1403.5473", "submitter": "Ahmad Taher Azar Dr.", "authors": "Reham Gharbia, Ahmad Taher Azar, Ali El Baz, Aboul Ella Hassanien", "title": "Image Fusion Techniques in Remote Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing image fusion is an effective way to use a large volume of data\nfrom multisensor images. Most earth satellites such as SPOT, Landsat 7, IKONOS\nand QuickBird provide both panchromatic (Pan) images at a higher spatial\nresolution and multispectral (MS) images at a lower spatial resolution and many\nremote sensing applications require both high spatial and high spectral\nresolutions, especially for GIS based applications. An effective image fusion\ntechnique can produce such remotely sensed images. Image fusion is the\ncombination of two or more different images to form a new image by using a\ncertain algorithm to obtain more and better information about an object or a\nstudy area than. The image fusion is performed at three different processing\nlevels which are pixel level, feature level and decision level according to the\nstage at which the fusion takes place. There are many image fusion methods that\ncan be used to produce high resolution multispectral images from a high\nresolution pan image and low resolution multispectral images. This paper\nexplores the major remote sensing data fusion techniques at pixel level and\nreviews the concept, principals, limitations and advantages for each technique.\nThis paper focused on traditional techniques like intensity hue-saturation-\n(HIS), Brovey, principal component analysis (PCA) and Wavelet.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 00:48:08 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Gharbia", "Reham", ""], ["Azar", "Ahmad Taher", ""], ["Baz", "Ali El", ""], ["Hassanien", "Aboul Ella", ""]]}, {"id": "1403.5475", "submitter": "V Karthikeyan VKK", "authors": "V. Karthikeyan, K. Vijayalakshmi, P. Jeyakumar", "title": "An Efficient Method for Face Recognition System In Various Assorted\n  Conditions", "comments": "9 figures and 5 pages. arXiv admin note: substantial text overlap\n  with arXiv:1401.6108", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the beginning stage, face verification is done using easy method of\ngeometric algorithm models, but the verification route has now developed into a\nscientific progress of complicated geometric representation and identical\nprocedure. In recent years the technologies have boosted face recognition\nsystem into the healthy focus. Researchers currently undergoing strong research\non finding face recognition system for wider area information taken under\nhysterical elucidation dissimilarity. The proposed face recognition system\nconsists of a narrative expositionindiscreet preprocessing method, a hybrid\nFourier-based facial feature extraction and a score fusion scheme. We have\nverified the face recognition in different lightening conditions (day or night)\nand at different locations (indoor or outdoor). Preprocessing, Image detection,\nFeature- extraction and Face recognition are the methods used for face\nverification system. This paper focuses mainly on the issue of toughness to\nlighting variations. The proposed system has obtained an average of 88.1%\nverification rate on Two-Dimensional images under different lightening\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 04:47:37 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Karthikeyan", "V.", ""], ["Vijayalakshmi", "K.", ""], ["Jeyakumar", "P.", ""]]}, {"id": "1403.5590", "submitter": "Petter Strandmark", "authors": "Petter Strandmark and Sameer Agarwal", "title": "Continuous Optimization for Fields of Experts Denoising Works", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent papers use image denoising with a Fields of Experts prior to\nbenchmark discrete optimization methods. We show that a non-linear least\nsquares solver significantly outperforms all known discrete methods on this\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 23:26:47 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Strandmark", "Petter", ""], ["Agarwal", "Sameer", ""]]}, {"id": "1403.5718", "submitter": "Hung-Kuo Chu", "authors": "Yu-Shiang Wong, Hung-Kuo Chu and Niloy J. Mitra", "title": "SmartAnnotator: An Interactive Tool for Annotating RGBD Indoor Images", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGBD images with high quality annotations in the form of geometric (i.e.,\nsegmentation) and structural (i.e., how do the segments are mutually related in\n3D) information provide valuable priors to a large number of scene and image\nmanipulation applications. While it is now simple to acquire RGBD images,\nannotating them, automatically or manually, remains challenging especially in\ncluttered noisy environments. We present SmartAnnotator, an interactive system\nto facilitate annotating RGBD images. The system performs the tedious tasks of\ngrouping pixels, creating potential abstracted cuboids, inferring object\ninteractions in 3D, and comes up with various hypotheses. The user simply has\nto flip through a list of suggestions for segment labels, finalize a selection,\nand the system updates the remaining hypotheses. As objects are finalized, the\nprocess speeds up with fewer ambiguities to resolve. Further, as more scenes\nare annotated, the system makes better suggestions based on structural and\ngeometric priors learns from the previous annotation sessions. We test our\nsystem on a large number of database scenes and report significant improvements\nover naive low-level annotation tools.\n", "versions": [{"version": "v1", "created": "Sun, 23 Mar 2014 03:45:26 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Wong", "Yu-Shiang", ""], ["Chu", "Hung-Kuo", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1403.5869", "submitter": "Sumaira Tasnim", "authors": "Akhlaqur Rahman, Sumaira Tasnim", "title": "Block Motion Based Dynamic Texture Analysis: A Review", "comments": "Published with International Journal of Computer Trends and\n  Technology (IJCTT)", "journal-ref": "Akhlaqur Rahman , Sumaira Tasnim . \"Block Motion Based Dynamic\n  Texture Analysis: A Review\". International Journal of Computer Trends and\n  Technology (IJCTT) V8(2):76-78, February 2014", "doi": "10.14445/22312803/IJCTT-V8P114", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic texture refers to image sequences of non-rigid objects that exhibit\nsome regularity in their movement. Videos of smoke, fire etc. fall under the\ncategory of dynamic texture. Researchers have investigated different ways to\nanalyze dynamic textures since early nineties. Both appearance based (image\nintensities) and motion based approaches are investigated. Motion based\napproaches turn out to be more effective. A group of researchers have\ninvestigated ways to utilize the motion vectors readily available with the\nblocks in video codes like MGEG/H26X. In this paper we provide a review of the\ndynamic texture analysis methods using block motion. Research into dynamic\ntexture analysis using block motion includes recognition, motion computation,\nsegmentation, and synthesis. We provide a comprehensive review of these\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 07:12:22 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Rahman", "Akhlaqur", ""], ["Tasnim", "Sumaira", ""]]}, {"id": "1403.5912", "submitter": "Lucas Paletta", "authors": "Bj\\\"orn Schuller, Erik Marchi, Simon Baron-Cohen, Helen O'Reilly,\n  Delia Pigat, Peter Robinson, Ian Daves", "title": "The state of play of ASC-Inclusion: An Integrated Internet-Based\n  Environment for Social Inclusion of Children with Autism Spectrum Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": "IDGEI/2014/05", "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals with Autism Spectrum Conditions (ASC) have marked difficulties\nusing verbal and non-verbal communication for social interaction. The running\nASC-Inclusion project aims to help children with ASC by allowing them to learn\nhow emotions can be expressed and recognised via playing games in a virtual\nworld. The platform includes analysis of users' gestures, facial, and vocal\nexpressions using standard microphone and web-cam or a depth sensor, training\nthrough games, text communication with peers, animation, video and audio clips.\nWe present the state of play in realising such a serious game platform and\nprovide results for the different modalities.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 11:09:27 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Schuller", "Bj\u00f6rn", ""], ["Marchi", "Erik", ""], ["Baron-Cohen", "Simon", ""], ["O'Reilly", "Helen", ""], ["Pigat", "Delia", ""], ["Robinson", "Peter", ""], ["Daves", "Ian", ""]]}, {"id": "1403.5919", "submitter": "Daniel Freedman", "authors": "Daniel Freedman, Eyal Krupka, Yoni Smolin, Ido Leichter, Mirko Schmidt", "title": "SRA: Fast Removal of General Multipath for ToF Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major issue with Time of Flight sensors is the presence of multipath\ninterference. We present Sparse Reflections Analysis (SRA), an algorithm for\nremoving this interference which has two main advantages. First, it allows for\nvery general forms of multipath, including interference with three or more\npaths, diffuse multipath resulting from Lambertian surfaces, and combinations\nthereof. SRA removes this general multipath with robust techniques based on\n$L_1$ optimization. Second, due to a novel dimension reduction, we are able to\nproduce a very fast version of SRA, which is able to run at frame rate.\nExperimental results on both synthetic data with ground truth, as well as real\nimages of challenging scenes, validate the approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 11:28:52 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Freedman", "Daniel", ""], ["Krupka", "Eyal", ""], ["Smolin", "Yoni", ""], ["Leichter", "Ido", ""], ["Schmidt", "Mirko", ""]]}, {"id": "1403.6002", "submitter": "Vaishali Khairnar mrs", "authors": "Narkhede Sachin G., Vaishali Khairnar, Sujata Kadu", "title": "Brain Tumor Detection Based On Mathematical Analysis and Symmetry\n  Information", "comments": "05 Pages,02 figures", "journal-ref": "Int.Journal of Engineering Research and Applications ISSN\n  2248-9622, Vol.4,Issue 2 Version 1,February 2014,pp.231-235", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation some of the challenging issues on brain magnetic resonance\nimage tumor segmentation caused by the weak correlation between magnetic\nresonance imaging intensity and anatomical meaning.With the objective of\nutilizing more meaningful information to improve brain tumor segmentation,an\napproach which employs bilateral symmetry information as an additional feature\nfor segmentation is proposed.This is motivated by potential performance\nimprovement in the general automatic brain tumor segmentation systems which are\nimportant for many medical and scientific applications.Brain Magnetic Resonance\nImaging segmentation is a complex problem in the field of medical imaging\ndespite various presented methods.MR image of human brain can be divided into\nseveral sub-regions especially soft tissues such as gray matter,white matter\nand cerebra spinal fluid.Although edge information is the main clue in image\nsegmentation,it cannot get a better result in analysis the content of images\nwithout combining other information.Our goal is to detect the position and\nboundary of tumors automatically.Experiments were conducted on real\npictures,and the results show that the algorithm is flexible and convenient.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 15:31:50 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["G.", "Narkhede Sachin", ""], ["Khairnar", "Vaishali", ""], ["Kadu", "Sujata", ""]]}, {"id": "1403.6173", "submitter": "Anna Senina", "authors": "Anna Senina and Marcus Rohrbach and Wei Qiu and Annemarie Friedrich\n  and Sikandar Amin and Mykhaylo Andriluka and Manfred Pinkal and Bernt Schiele", "title": "Coherent Multi-Sentence Video Description with Variable Level of Detail", "comments": "10 pages", "journal-ref": null, "doi": "10.1007/978-3-319-11752-2_15", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can easily describe what they see in a coherent way and at varying\nlevel of detail. However, existing approaches for automatic video description\nare mainly focused on single sentence generation and produce descriptions at a\nfixed level of detail. In this paper, we address both of these limitations: for\na variable level of detail we produce coherent multi-sentence descriptions of\ncomplex videos. We follow a two-step approach where we first learn to predict a\nsemantic representation (SR) from video and then generate natural language\ndescriptions from the SR. To produce consistent multi-sentence descriptions, we\nmodel across-sentence consistency at the level of the SR by enforcing a\nconsistent topic. We also contribute both to the visual recognition of objects\nproposing a hand-centric approach as well as to the robust generation of\nsentences using a word lattice. Human judges rate our multi-sentence\ndescriptions as more readable, correct, and relevant than related work. To\nunderstand the difference between more detailed and shorter descriptions, we\ncollect and analyze a video description corpus of three levels of detail.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 22:28:38 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Senina", "Anna", ""], ["Rohrbach", "Marcus", ""], ["Qiu", "Wei", ""], ["Friedrich", "Annemarie", ""], ["Amin", "Sikandar", ""], ["Andriluka", "Mykhaylo", ""], ["Pinkal", "Manfred", ""], ["Schiele", "Bernt", ""]]}, {"id": "1403.6183", "submitter": "Ali Avanaki", "authors": "Ali R. N. Avanaki, Kathryn S. Espig, Andrew D. A. Maidment, Cedric\n  Marchessoux, Predrag R. Bakic, Tom R. L. Kimpe", "title": "Development and evaluation of a 3D model observer with nonlinear\n  spatiotemporal contrast sensitivity", "comments": null, "journal-ref": null, "doi": "10.1117/12.2043793", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate improvements to our 3D model observer with the goal of better\nmatching human observer performance as a function of viewing distance,\neffective contrast, maximum luminance, and browsing speed. Two nonlinear\nmethods of applying the human contrast sensitivity function (CSF) to a 3D model\nobserver are proposed, namely the Probability Map (PM) and Monte Carlo (MC)\nmethods. In the PM method, the visibility probability for each frequency\ncomponent of the image stack, p, is calculated taking into account Barten's\nspatiotemporal CSF, the component modulation, and the human psychometric\nfunction. The probability p is considered to be equal to the perceived\namplitude of the frequency component and thus can be used by a traditional\nmodel observer (e.g., LG-msCHO) in the space-time domain. In the MC method,\neach component is randomly kept with probability p or discarded with 1-p. The\namplitude of the retained components is normalized to unity. The methods were\ntested using DBT stacks of an anthropomorphic breast phantom processed in a\ncomprehensive simulation pipeline. Our experiments indicate that both the PM\nand MC methods yield results that match human observer performance better than\nthe linear filtering method as a function of viewing distance, effective\ncontrast, maximum luminance, and browsing speed.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 23:06:25 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Avanaki", "Ali R. N.", ""], ["Espig", "Kathryn S.", ""], ["Maidment", "Andrew D. A.", ""], ["Marchessoux", "Cedric", ""], ["Bakic", "Predrag R.", ""], ["Kimpe", "Tom R. L.", ""]]}, {"id": "1403.6260", "submitter": "M.M.A. Hashem", "authors": "M. Ashrafuzzaman, M.M .Rahman and M.M.A. Hashem", "title": "Capturing and Recognizing Objects Appearance Employing Eigenspace", "comments": null, "journal-ref": "Procs. Of the 5th International Conference on Computer &\n  Information Technology (ICCIT 2002), pp. 434-436, Dhaka, Bangladesh, December\n  27-28, (2002)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method of capturing objects appearances from its\nenvironment and it also describes how to recognize unknown appearances creating\nan eigenspace. This representation and recognition can be done automatically\ntaking objects various appearances by using robotic vision from a defined\nenvironment. This technique also allows extracting objects from some sort of\ncomplicated scenes. In this case, some of object appearances are taken with\ndefined occlusions and eigenspaces are created by accepting both of\nnon-occluded and occluded appearances together. Eigenspace is constructed\nsuccessfully every times when a new object appears, and various appearances\naccumulated gradually. A sequence of appearances is generated from its\naccumulated shapes, which is used for recognition of the unknown objects\nappearances. Various objects environments are shown in the experiment to\ncapture objects appearances and experimental results show effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 08:56:25 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Ashrafuzzaman", "M.", ""], ["Rahman", "M. M .", ""], ["Hashem", "M. M. A.", ""]]}, {"id": "1403.6275", "submitter": "Vibhav Vineet Mr", "authors": "Vibhav Vineet, Jonathan Warrell and Philip H.S. Torr", "title": "A Tiered Move-making Algorithm for General Non-submodular Pairwise\n  Energies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of problems in computer vision can be modelled as energy\nminimization problems in a Markov Random Field (MRF) or Conditional Random\nField (CRF) framework. Graph-cuts based $\\alpha$-expansion is a standard\nmove-making method to minimize the energy functions with sub-modular pairwise\nterms. However, certain problems require more complex pairwise terms where the\n$\\alpha$-expansion method is generally not applicable.\n  In this paper, we propose an iterative {\\em tiered move making algorithm}\nwhich is able to handle general pairwise terms. Each move to the next\nconfiguration is based on the current labeling and an optimal tiered move,\nwhere each tiered move requires one application of the dynamic programming\nbased tiered labeling method introduced in Felzenszwalb et. al.\n\\cite{tiered_cvpr_felzenszwalbV10}. The algorithm converges to a local minimum\nfor any general pairwise potential, and we give a theoretical analysis of the\nproperties of the algorithm, characterizing the situations in which we can\nexpect good performance. We first evaluate our method on an object-class\nsegmentation problem using the Pascal VOC-11 segmentation dataset where we\nlearn general pairwise terms. Further we evaluate the algorithm on many other\nbenchmark labeling problems such as stereo, image segmentation, image stitching\nand image denoising. Our method consistently gets better accuracy and energy\nvalues than alpha-expansion, loopy belief propagation (LBP), quadratic\npseudo-boolean optimization (QPBO), and is competitive with TRWS.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 10:18:47 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Vineet", "Vibhav", ""], ["Warrell", "Jonathan", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1403.6290", "submitter": "Zhenfang Hu", "authors": "Zhenfang Hu, Gang Pan, Yueming Wang, Zhaohui Wu", "title": "Spectral Sparse Representation for Clustering: Evolved from PCA,\n  K-means, Laplacian Eigenmap, and Ratio Cut", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction, cluster analysis, and sparse representation are\nbasic components in machine learning. However, their relationships have not yet\nbeen fully investigated. In this paper, we find that the spectral graph theory\nunderlies a series of these elementary methods and can unify them into a\ncomplete framework. The methods include PCA, K-means, Laplacian eigenmap (LE),\nratio cut (Rcut), and a new sparse representation method developed by us,\ncalled spectral sparse representation (SSR). Further, extended relations to\nconventional over-complete sparse representations (e.g., method of optimal\ndirections, KSVD), manifold learning (e.g., kernel PCA, multidimensional\nscaling, Isomap, locally linear embedding), and subspace clustering (e.g.,\nsparse subspace clustering, low-rank representation) are incorporated. We show\nthat, under an ideal condition from the spectral graph theory, PCA, K-means,\nLE, and Rcut are unified together. And when the condition is relaxed, the\nunification evolves to SSR, which lies in the intermediate between PCA/LE and\nK-mean/Rcut. An efficient algorithm, NSCrt, is developed to solve the sparse\ncodes of SSR. SSR combines merits of both sides: its sparse codes reduce\ndimensionality of data meanwhile revealing cluster structure. For its inherent\nrelation to cluster analysis, the codes of SSR can be directly used for\nclustering. Scut, a clustering approach derived from SSR reaches the\nstate-of-the-art performance in the spectral clustering family. The one-shot\nsolution obtained by Scut is comparable to the optimal result of K-means that\nare run many times. Experiments on various data sets demonstrate the properties\nand strengths of SSR, NSCrt, and Scut.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 10:52:15 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 14:18:15 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 13:26:56 GMT"}, {"version": "v4", "created": "Fri, 19 May 2017 09:30:55 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Hu", "Zhenfang", ""], ["Pan", "Gang", ""], ["Wang", "Yueming", ""], ["Wu", "Zhaohui", ""]]}, {"id": "1403.6318", "submitter": "Brian Tracey", "authors": "Brian H. Tracey and Eric L. Miller", "title": "Stabilizing dual-energy X-ray computed tomography reconstructions using\n  patch-based regularization", "comments": null, "journal-ref": null, "doi": "10.1088/0266-5611/31/10/105004", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen growing interest in exploiting dual- and multi-energy\nmeasurements in computed tomography (CT) in order to characterize material\nproperties as well as object shape. Material characterization is performed by\ndecomposing the scene into constitutive basis functions, such as Compton\nscatter and photoelectric absorption functions. While well motivated\nphysically, the joint recovery of the spatial distribution of photoelectric and\nCompton properties is severely complicated by the fact that the data are\nseveral orders of magnitude more sensitive to Compton scatter coefficients than\nto photoelectric absorption, so small errors in Compton estimates can create\nlarge artifacts in the photoelectric estimate. To address these issues, we\npropose a model-based iterative approach which uses patch-based regularization\nterms to stabilize inversion of photoelectric coefficients, and solve the\nresulting problem though use of computationally attractive Alternating\nDirection Method of Multipliers (ADMM) solution techniques. Using simulations\nand experimental data acquired on a commercial scanner, we demonstrate that the\nproposed processing can lead to more stable material property estimates which\nshould aid materials characterization in future dual- and multi-energy CT\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 12:21:32 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Tracey", "Brian H.", ""], ["Miller", "Eric L.", ""]]}, {"id": "1403.6382", "submitter": "Hossein Azizpour", "authors": "Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan\n  Carlsson", "title": "CNN Features off-the-shelf: an Astounding Baseline for Recognition", "comments": "version 3 revisions: 1)Added results using feature processing and\n  data augmentation 2)Referring to most recent efforts of using CNN for\n  different visual recognition tasks 3) updated text/caption", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results indicate that the generic descriptors extracted from the\nconvolutional neural networks are very powerful. This paper adds to the\nmounting evidence that this is indeed the case. We report on a series of\nexperiments conducted for different recognition tasks using the publicly\navailable code and model of the \\overfeat network which was trained to perform\nobject classification on ILSVRC13. We use features extracted from the \\overfeat\nnetwork as a generic image representation to tackle the diverse range of\nrecognition tasks of object image classification, scene recognition, fine\ngrained recognition, attribute detection and image retrieval applied to a\ndiverse set of datasets. We selected these tasks and datasets as they gradually\nmove further away from the original task and data the \\overfeat network was\ntrained to solve. Astonishingly, we report consistent superior results compared\nto the highly tuned state-of-the-art systems in all the visual classification\ntasks on various datasets. For instance retrieval it consistently outperforms\nlow memory footprint methods except for sculptures dataset. The results are\nachieved using a linear SVM classifier (or $L2$ distance in case of retrieval)\napplied to a feature representation of size 4096 extracted from a layer in the\nnet. The representations are further modified using simple augmentation\ntechniques e.g. jittering. The results strongly suggest that features obtained\nfrom deep learning with convolutional nets should be the primary candidate in\nmost visual recognition tasks.\n", "versions": [{"version": "v1", "created": "Sun, 23 Mar 2014 13:42:03 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 12:43:13 GMT"}, {"version": "v3", "created": "Mon, 12 May 2014 08:53:31 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Razavian", "Ali Sharif", ""], ["Azizpour", "Hossein", ""], ["Sullivan", "Josephine", ""], ["Carlsson", "Stefan", ""]]}, {"id": "1403.6566", "submitter": "Weiming Dong", "authors": "Weiming Dong, Fuzhang Wu, Yan Kong, Xing Mei, Tong-Yee Lee, Xiaopeng\n  Zhang", "title": "Image Retargeting by Content-Aware Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world images usually contain vivid contents and rich textural details,\nwhich will complicate the manipulation on them. In this paper, we design a new\nframework based on content-aware synthesis to enhance content-aware image\nretargeting. By detecting the textural regions in an image, the textural image\ncontent can be synthesized rather than simply distorted or cropped. This method\nenables the manipulation of textural & non-textural regions with different\nstrategy since they have different natures. We propose to retarget the textural\nregions by content-aware synthesis and non-textural regions by fast\nmulti-operators. To achieve practical retargeting applications for general\nimages, we develop an automatic and fast texture detection method that can\ndetect multiple disjoint textural regions. We adjust the saliency of the image\naccording to the features of the textural regions. To validate the proposed\nmethod, comparisons with state-of-the-art image targeting techniques and a user\nstudy were conducted. Convincing visual results are shown to demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 03:29:25 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 06:16:01 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Dong", "Weiming", ""], ["Wu", "Fuzhang", ""], ["Kong", "Yan", ""], ["Mei", "Xing", ""], ["Lee", "Tong-Yee", ""], ["Zhang", "Xiaopeng", ""]]}, {"id": "1403.6614", "submitter": "Kin Tat Ho", "authors": "Kin Tat Ho and Lok Ming Lui", "title": "QCMC: Quasi-conformal Parameterizations for Multiply-connected domains", "comments": "26 pages, 23 figures, submitted. arXiv admin note: text overlap with\n  arXiv:1402.6908, arXiv:1307.2679 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV math.DG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a method to compute the {\\it quasi-conformal\nparameterization} (QCMC) for a multiply-connected 2D domain or surface. QCMC\ncomputes a quasi-conformal map from a multiply-connected domain $S$ onto a\npunctured disk $D_S$ associated with a given Beltrami differential. The\nBeltrami differential, which measures the conformality distortion, is a\ncomplex-valued function $\\mu:S\\to\\mathbb{C}$ with supremum norm strictly less\nthan 1. Every Beltrami differential gives a conformal structure of $S$. Hence,\nthe conformal module of $D_S$, which are the radii and centers of the inner\ncircles, can be fully determined by $\\mu$, up to a M\\\"obius transformation. In\nthis paper, we propose an iterative algorithm to simultaneously search for the\nconformal module and the optimal quasi-conformal parameterization. The key idea\nis to minimize the Beltrami energy subject to the boundary constraints. The\noptimal solution is our desired quasi-conformal parameterization onto a\npunctured disk. The parameterization of the multiply-connected domain\nsimplifies numerical computations and has important applications in various\nfields, such as in computer graphics and vision. Experiments have been carried\nout on synthetic data together with real multiply-connected Riemann surfaces.\nResults show that our proposed method can efficiently compute quasi-conformal\nparameterizations of multiply-connected domains and outperforms other\nstate-of-the-art algorithms. Applications of the proposed parameterization\ntechnique have also been explored.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 10:21:03 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Ho", "Kin Tat", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1403.6706", "submitter": "Karthikeyan Natesan Ramamurthy", "authors": "Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J.\n  Thiagarajan", "title": "Beyond L2-Loss Functions for Learning Sparse Models", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating sparsity priors in learning tasks can give rise to simple, and\ninterpretable models for complex high dimensional data. Sparse models have\nfound widespread use in structure discovery, recovering data from corruptions,\nand a variety of large scale unsupervised and supervised learning problems.\nAssuming the availability of sufficient data, these methods infer dictionaries\nfor sparse representations by optimizing for high-fidelity reconstruction. In\nmost scenarios, the reconstruction quality is measured using the squared\nEuclidean distance, and efficient algorithms have been developed for both batch\nand online learning cases. However, new application domains motivate looking\nbeyond conventional loss functions. For example, robust loss functions such as\n$\\ell_1$ and Huber are useful in learning outlier-resilient models, and the\nquantile loss is beneficial in discovering structures that are the\nrepresentative of a particular quantile. These new applications motivate our\nwork in generalizing sparse learning to a broad class of convex loss functions.\nIn particular, we consider the class of piecewise linear quadratic (PLQ) cost\nfunctions that includes Huber, as well as $\\ell_1$, quantile, Vapnik, hinge\nloss, and smoothed variants of these penalties. We propose an algorithm to\nlearn dictionaries and obtain sparse codes when the data reconstruction\nfidelity is measured using any smooth PLQ cost function. We provide convergence\nguarantees for the proposed algorithm, and demonstrate the convergence behavior\nusing empirical experiments. Furthermore, we present three case studies that\nrequire the use of PLQ cost functions: (i) robust image modeling, (ii) tag\nrefinement for image annotation and retrieval and (iii) computing empirical\nconfidence limits for subspace clustering.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 15:16:56 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Ramamurthy", "Karthikeyan Natesan", ""], ["Aravkin", "Aleksandr Y.", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "1403.6774", "submitter": "Benjamin Berkels", "authors": "Benjamin Berkels, Peter Binev, Douglas A. Blom, Wolfgang Dahmen,\n  Robert C. Sharpley, Thomas Vogt", "title": "Optimized imaging using non-rigid registration", "comments": null, "journal-ref": "Ultramicroscopy 13 (2014) 46-56", "doi": "10.1016/j.ultramic.2013.11.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraordinary improvements of modern imaging devices offer access to data\nwith unprecedented information content. However, widely used image processing\nmethodologies fall far short of exploiting the full breadth of information\noffered by numerous types of scanning probe, optical, and electron\nmicroscopies. In many applications, it is necessary to keep measurement\nintensities below a desired threshold. We propose a methodology for extracting\nan increased level of information by processing a series of data sets\nsuffering, in particular, from high degree of spatial uncertainty caused by\ncomplex multiscale motion during the acquisition process. An important role is\nplayed by a nonrigid pixel-wise registration method that can cope with low\nsignal-to-noise ratios. This is accompanied by formulating objective quality\nmeasures which replace human intervention and visual inspection in the\nprocessing chain. Scanning transmission electron microscopy of siliceous\nzeolite material exhibits the above-mentioned obstructions and therefore serves\nas orientation and a test of our procedures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 18:06:42 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Berkels", "Benjamin", ""], ["Binev", "Peter", ""], ["Blom", "Douglas A.", ""], ["Dahmen", "Wolfgang", ""], ["Sharpley", "Robert C.", ""], ["Vogt", "Thomas", ""]]}, {"id": "1403.6794", "submitter": "David Olivieri", "authors": "Iv\\'an G\\'omez-Conde and David N. Olivieri", "title": "KPCA Spatio-temporal trajectory point cloud classifier for recognizing\n  human actions in a CBVR system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a content based video retrieval (CBVR) software system for\nidentifying specific locations of a human action within a full length film, and\nretrieving similar video shots from a query. For this, we introduce the concept\nof a trajectory point cloud for classifying unique actions, encoded in a\nspatio-temporal covariant eigenspace, where each point is characterized by its\nspatial location, local Frenet-Serret vector basis, time averaged curvature and\ntorsion and the mean osculating hyperplane. Since each action can be\ndistinguished by their unique trajectories within this space, the trajectory\npoint cloud is used to define an adaptive distance metric for classifying\nqueries against stored actions. Depending upon the distance to other\ntrajectories, the distance metric uses either large scale structure of the\ntrajectory point cloud, such as the mean distance between cloud centroids or\nthe difference in hyperplane orientation, or small structure such as the time\naveraged curvature and torsion, to classify individual points in a fuzzy-KNN.\nOur system can function in real-time and has an accuracy greater than 93% for\nmultiple action recognition within video repositories. We demonstrate the use\nof our CBVR system in two situations: by locating specific frame positions of\ntrained actions in two full featured films, and video shot retrieval from a\ndatabase with a web search application.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 19:10:53 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["G\u00f3mez-Conde", "Iv\u00e1n", ""], ["Olivieri", "David N.", ""]]}, {"id": "1403.6888", "submitter": "Nenad Marku\\v{s}", "authors": "Nenad Marku\\v{s} and Miroslav Frljak and Igor S. Pand\\v{z}i\\'c and\n  J\\\"orgen Ahlberg and Robert Forchheimer", "title": "Fast Localization of Facial Landmark Points", "comments": null, "journal-ref": "Proceedings of the Croatian Compter Vision Workshop, 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization of salient facial landmark points, such as eye corners or the\ntip of the nose, is still considered a challenging computer vision problem\ndespite recent efforts. This is especially evident in unconstrained\nenvironments, i.e., in the presence of background clutter and large head pose\nvariations. Most methods that achieve state-of-the-art accuracy are slow, and,\nthus, have limited applications. We describe a method that can accurately\nestimate the positions of relevant facial landmarks in real-time even on\nhardware with limited processing power, such as mobile devices. This is\nachieved with a sequence of estimators based on ensembles of regression trees.\nThe trees use simple pixel intensity comparisons in their internal nodes and\nthis makes them able to process image regions very fast. We test the developed\nsystem on several publicly available datasets and analyse its processing speed\non various devices. Experimental results show that our method has practical\nvalue.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 23:12:08 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 12:19:05 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Marku\u0161", "Nenad", ""], ["Frljak", "Miroslav", ""], ["Pand\u017ei\u0107", "Igor S.", ""], ["Ahlberg", "J\u00f6rgen", ""], ["Forchheimer", "Robert", ""]]}, {"id": "1403.6950", "submitter": "Manuel Marin-Jimenez", "authors": "F.M. Castro and M.J. Marin-Jimenez and R. Medina-Carnicer", "title": "Pyramidal Fisher Motion for Multiview Gait Recognition", "comments": "Submitted to International Conference on Pattern Recognition, ICPR,\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to identify individuals by analyzing their gait.\nInstead of using binary silhouettes as input data (as done in many previous\nworks) we propose and evaluate the use of motion descriptors based on densely\nsampled short-term trajectories. We take advantage of state-of-the-art people\ndetectors to define custom spatial configurations of the descriptors around the\ntarget person. Thus, obtaining a pyramidal representation of the gait motion.\nThe local motion features (described by the Divergence-Curl-Shear descriptor)\nextracted on the different spatial areas of the person are combined into a\nsingle high-level gait descriptor by using the Fisher Vector encoding. The\nproposed approach, coined Pyramidal Fisher Motion, is experimentally validated\non the recent `AVA Multiview Gait' dataset. The results show that this new\napproach achieves promising results in the problem of gait recognition.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 08:39:31 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Castro", "F. M.", ""], ["Marin-Jimenez", "M. J.", ""], ["Medina-Carnicer", "R.", ""]]}, {"id": "1403.6958", "submitter": "Sylvain Rousseau", "authors": "S. Rousseau, D. Helbert, P. Carr\\'e and J. Blanc-Talon", "title": "Compressive Pattern Matching on Multispectral Data", "comments": "Published in IEEE Transactions on Geoscience and Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2014.2314483", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new constrained minimization problem that performs template\nand pattern detection on a multispectral image in a compressive sensing\ncontext. We use an original minimization problem from Guo and Osher that uses\n$L_1$ minimization techniques to perform template detection in a multispectral\nimage. We first adapt this minimization problem to work with compressive\nsensing data. Then we extend it to perform pattern detection using a formal\ntransform called the spectralization along a pattern. That extension brings out\nthe problem of measurement reconstruction. We introduce shifted measurements\nthat allow us to reconstruct all the measurement with a small overhead and we\ngive an optimality constraint for simple patterns. We present numerical results\nshowing the performances of the original minimization problem and the\ncompressed ones with different measurement rates and applied on remotely sensed\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 09:33:34 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Rousseau", "S.", ""], ["Helbert", "D.", ""], ["Carr\u00e9", "P.", ""], ["Blanc-Talon", "J.", ""]]}, {"id": "1403.7057", "submitter": "Alexander Kolesnikov", "authors": "Alexander Kolesnikov, Matthieu Guillaumin, Vittorio Ferrari and\n  Christoph H. Lampert", "title": "Closed-Form Training of Conditional Random Fields for Large Scale Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LS-CRF, a new method for very efficient large-scale training of\nConditional Random Fields (CRFs). It is inspired by existing closed-form\nexpressions for the maximum likelihood parameters of a generative graphical\nmodel with tree topology. LS-CRF training requires only solving a set of\nindependent regression problems, for which closed-form expression as well as\nefficient iterative solvers are available. This makes it orders of magnitude\nfaster than conventional maximum likelihood learning for CRFs that require\nrepeated runs of probabilistic inference. At the same time, the models learned\nby our method still allow for joint inference at test time. We apply LS-CRF to\nthe task of semantic image segmentation, showing that it is highly efficient,\neven for loopy models where probabilistic inference is problematic. It allows\nthe training of image segmentation models from significantly larger training\nsets than had been used previously. We demonstrate this on two new datasets\nthat form a second contribution of this paper. They consist of over 180,000\nimages with figure-ground segmentation annotations. Our large-scale experiments\nshow that the possibilities of CRF-based image segmentation are far from\nexhausted, indicating, for example, that semi-supervised learning and the use\nof non-linear predictors are promising directions for achieving higher\nsegmentation accuracy in the future.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 14:38:23 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Kolesnikov", "Alexander", ""], ["Guillaumin", "Matthieu", ""], ["Ferrari", "Vittorio", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1403.7311", "submitter": "Akbar Khan", "authors": "Akbar Khan, Pratap Reddy L", "title": "Performance Evaluation of Raster Based Shape Vectors in Object\n  Recognition", "comments": "11pages,12 figures", "journal-ref": "International Journal of Engineering Trends and Technology\n  (IJETT), V9(8),378-388 March 2014. ISSN:2231-5381. www.ijettjournal.org.\n  published by seventh sense research group", "doi": "10.14445/22315381/IJETT-V9P274", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition is still an impediment in the field of computer vision and\nmultimedia retrieval.Defining an object model is a critical task. Shape\ninformation of an object play a critical role in the process of object\nrecognition. Extraction of boundary information of an object from the\nmultimedia data and classifying this information with associated objects is the\nprimary step towards object recognition. Rasters play an important role while\ncomputing object boundary. The trade-off lies with the dimensionality of the\nobject versus computational cost while achieving maximum efficiency. In this\ntreatise an attempt is made to evaluate the performance of circular and spiral\nraster models in terms of average retrieval efficiency and computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 09:18:17 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Khan", "Akbar", ""], ["L", "Pratap Reddy", ""]]}, {"id": "1403.7321", "submitter": "Jack Valmadre", "authors": "Jack Valmadre, Sridha Sridharan, Simon Lucey", "title": "Learning detectors quickly using structured covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is increasingly becoming interested in the rapid estimation\nof object detectors. Canonical hard negative mining strategies are slow as they\nrequire multiple passes of the large negative training set. Recent work has\ndemonstrated that if the distribution of negative examples is assumed to be\nstationary, then Linear Discriminant Analysis (LDA) can learn comparable\ndetectors without ever revisiting the negative set. Even with this insight,\nhowever, the time to learn a single object detector can still be on the order\nof tens of seconds on a modern desktop computer. This paper proposes to\nleverage the resulting structured covariance matrix to obtain detectors with\nidentical performance in orders of magnitude less time and memory. We elucidate\nan important connection to the correlation filter literature, demonstrating\nthat these can also be trained without ever revisiting the negative set.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 10:02:02 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Valmadre", "Jack", ""], ["Sridharan", "Sridha", ""], ["Lucey", "Simon", ""]]}, {"id": "1403.7365", "submitter": "Vania Estrela Dr.", "authors": "Vania Vieira Estrela and Marcos Henrique da Silva Bassani", "title": "Expectation-Maximization Technique and Spatial-Adaptation Applied to\n  Pel-Recursive Motion Estimation", "comments": "6 pages, pp. 204-209, Proceedings of the 8th World Multi-Conference\n  on Systemics, Cybernetics and Informatics, Volume XVI, Organized by the\n  International Institute of Informatics and Systemics, International\n  Federation of Systems Research: IFSR, Edited by Nagib Callaos, Maria Sanchez,\n  and Juan M. Pineda, TIB/UB Hannover, ISSN 12615810X, July 18-21, 2004,\n  Orlando, Florida, USA", "journal-ref": "ISSN 12615810X, 2004", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Pel-recursive motion estimation isa well-established approach. However, in\nthe presence of noise, it becomes an ill-posed problem that requires\nregularization. In this paper, motion vectors are estimated in an iterative\nfashion by means of the Expectation-Maximization (EM) algorithm and a Gaussian\ndata model. Our proposed algorithm also utilizes the local image properties of\nthe scene to improve the motion vector estimates following a spatially adaptive\napproach. Numerical experiments are presented that demonstrate the merits of\nour method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 12:47:16 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Estrela", "Vania Vieira", ""], ["Bassani", "Marcos Henrique da Silva", ""]]}, {"id": "1403.7543", "submitter": "Dirk Lorenz", "authors": "Dirk A. Lorenz, Stephan Wenger, Frank Sch\\\"opfer, Marcus Magnor", "title": "A sparse Kaczmarz solver and a linearized Bregman method for online\n  compressed sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.IT math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithmic framework to compute sparse or minimal-TV solutions of linear\nsystems is proposed. The framework includes both the Kaczmarz method and the\nlinearized Bregman method as special cases and also several new methods such as\na sparse Kaczmarz solver. The algorithmic framework has a variety of\napplications and is especially useful for problems in which the linear\nmeasurements are slow and expensive to obtain. We present examples for online\ncompressed sensing, TV tomographic reconstruction and radio interferometry.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 21:14:05 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Lorenz", "Dirk A.", ""], ["Wenger", "Stephan", ""], ["Sch\u00f6pfer", "Frank", ""], ["Magnor", "Marcus", ""]]}, {"id": "1403.7588", "submitter": "Cun Mu", "authors": "Cun Mu, Yuqian Zhang, John Wright, Donald Goldfarb", "title": "Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods", "comments": null, "journal-ref": "SIAM Journal on Scientific Computing, 2016, Vol. 38, No. 5 : pp.\n  A3291-A3317", "doi": "10.1137/15M101628X", "report-no": null, "categories": "math.OC cs.CV cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering matrices from compressive and grossly corrupted observations is a\nfundamental problem in robust statistics, with rich applications in computer\nvision and machine learning. In theory, under certain conditions, this problem\ncan be solved in polynomial time via a natural convex relaxation, known as\nCompressive Principal Component Pursuit (CPCP). However, all existing provable\nalgorithms for CPCP suffer from superlinear per-iteration cost, which severely\nlimits their applicability to large scale problems. In this paper, we propose\nprovable, scalable and efficient methods to solve CPCP with (essentially)\nlinear per-iteration cost. Our method combines classical ideas from Frank-Wolfe\nand proximal methods. In each iteration, we mainly exploit Frank-Wolfe to\nupdate the low-rank component with rank-one SVD and exploit the proximal step\nfor the sparse term. Convergence results and implementation details are also\ndiscussed. We demonstrate the scalability of the proposed approach with\npromising numerical experiments on visual data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 04:04:43 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 21:16:42 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Mu", "Cun", ""], ["Zhang", "Yuqian", ""], ["Wright", "John", ""], ["Goldfarb", "Donald", ""]]}, {"id": "1403.7591", "submitter": "Yin Cui", "authors": "Yin Cui, Dong Liu, Jiawei Chen, Shih-Fu Chang", "title": "Building A Large Concept Bank for Representing Events in Video", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept-based video representation has proven to be effective in complex\nevent detection. However, existing methods either manually design concepts or\ndirectly adopt concept libraries not specifically designed for events. In this\npaper, we propose to build Concept Bank, the largest concept library consisting\nof 4,876 concepts specifically designed to cover 631 real-world events. To\nconstruct the Concept Bank, we first gather a comprehensive event collection\nfrom WikiHow, a collaborative writing project that aims to build the world's\nlargest manual for any possible How-To event. For each event, we then search\nFlickr and discover relevant concepts from the tags of the returned images. We\ntrain a Multiple Kernel Linear SVM for each discovered concept as a concept\ndetector in Concept Bank. We organize the concepts into a five-layer tree\nstructure, in which the higher-level nodes correspond to the event categories\nwhile the leaf nodes are the event-specific concepts discovered for each event.\nBased on such tree ontology, we develop a semantic matching method to select\nrelevant concepts for each textual event query, and then apply the\ncorresponding concept detectors to generate concept-based video\nrepresentations. We use TRECVID Multimedia Event Detection 2013 and Columbia\nConsumer Video open source event definitions and videos as our test sets and\nshow very promising results on two video event detection tasks: event modeling\nover concept space and zero-shot event retrieval. To the best of our knowledge,\nthis is the largest concept library covering the largest number of real-world\nevents.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 05:17:29 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Cui", "Yin", ""], ["Liu", "Dong", ""], ["Chen", "Jiawei", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1403.7783", "submitter": "Mohammed  Javed", "authors": "Mohammed Javed, P. Nagabhushan, B.B. Chaudhuri", "title": "Extraction of Line Word Character Segments Directly from Run Length\n  Compressed Printed Text Documents", "comments": "IEEE Proceedings in National Conference on Computer Vision, Pattern\n  Recognition, Image Processing and Graphics (NCVPRIPG 2013)", "journal-ref": null, "doi": "10.1109/NCVPRIPG.2013.6776195", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of a text-document into lines, words and characters, which is\nconsidered to be the crucial pre-processing stage in Optical Character\nRecognition (OCR) is traditionally carried out on uncompressed documents,\nalthough most of the documents in real life are available in compressed form,\nfor the reasons such as transmission and storage efficiency. However, this\nimplies that the compressed image should be decompressed, which indents\nadditional computing resources. This limitation has motivated us to take up\nresearch in document image analysis using compressed documents. In this paper,\nwe think in a new way to carry out segmentation at line, word and character\nlevel in run-length compressed printed-text-documents. We extract the\nhorizontal projection profile curve from the compressed file and using the\nlocal minima points perform line segmentation. However, tracing vertical\ninformation which leads to tracking words-characters in a run-length compressed\nfile is not very straight forward. Therefore, we propose a novel technique for\ncarrying out simultaneous word and character segmentation by popping out column\nruns from each row in an intelligent sequence. The proposed algorithms have\nbeen validated with 1101 text-lines, 1409 words and 7582 characters from a\ndata-set of 35 noise and skew free compressed documents of Bengali, Kannada and\nEnglish Scripts.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 16:48:31 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Javed", "Mohammed", ""], ["Nagabhushan", "P.", ""], ["Chaudhuri", "B. B.", ""]]}, {"id": "1403.7876", "submitter": "Hamed Kiani Galoogahi", "authors": "Hamed Kiani Galoogahi, Terence Sim, Simon Lucey", "title": "Correlation Filters with Limited Boundaries", "comments": "8 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filters take advantage of specific properties in the Fourier\ndomain allowing them to be estimated efficiently: O(NDlogD) in the frequency\ndomain, versus O(D^3 + ND^2) spatially where D is signal length, and N is the\nnumber of signals. Recent extensions to correlation filters, such as MOSSE,\nhave reignited interest of their use in the vision community due to their\nrobustness and attractive computational properties. In this paper we\ndemonstrate, however, that this computational efficiency comes at a cost.\nSpecifically, we demonstrate that only 1/D proportion of shifted examples are\nunaffected by boundary effects which has a dramatic effect on\ndetection/tracking performance. In this paper, we propose a novel approach to\ncorrelation filter estimation that: (i) takes advantage of inherent\ncomputational redundancies in the frequency domain, and (ii) dramatically\nreduces boundary effects. Impressive object tracking and detection results are\npresented in terms of both accuracy and computational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 05:54:54 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Galoogahi", "Hamed Kiani", ""], ["Sim", "Terence", ""], ["Lucey", "Simon", ""]]}, {"id": "1403.7877", "submitter": "Kui Jia", "authors": "Kui Jia, Tsung-Han Chan, Zinan Zeng, Shenghua Gao, Gang Wang, Tianzhu\n  Zhang, Yi Ma", "title": "ROML: A Robust Feature Correspondence Approach for Matching Objects in A\n  Set of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based object matching is a fundamental problem for many applications\nin computer vision, such as object recognition, 3D reconstruction, tracking,\nand motion segmentation. In this work, we consider simultaneously matching\nobject instances in a set of images, where both inlier and outlier features are\nextracted. The task is to identify the inlier features and establish their\nconsistent correspondences across the image set. This is a challenging\ncombinatorial problem, and the problem complexity grows exponentially with the\nimage number. To this end, we propose a novel framework, termed ROML, to\naddress this problem. ROML optimizes simultaneously a partial permutation\nmatrix (PPM) for each image, and feature correspondences are established by the\nobtained PPMs. Two of our key contributions are summarized as follows. (1) We\nformulate the problem as rank and sparsity minimization for PPM optimization,\nand treat simultaneous optimization of multiple PPMs as a regularized consensus\nproblem in the context of distributed optimization. (2) We use the ADMM method\nto solve the thus formulated ROML problem, in which a subproblem associated\nwith a single PPM optimization appears to be a difficult integer quadratic\nprogram (IQP). We prove that under wildly applicable conditions, this IQP is\nequivalent to a linear sum assignment problem (LSAP), which can be efficiently\nsolved to an exact solution. Extensive experiments on rigid/non-rigid object\nmatching, matching instances of a common object category, and common object\nlocalization show the efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 05:56:38 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 15:17:50 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Jia", "Kui", ""], ["Chan", "Tsung-Han", ""], ["Zeng", "Zinan", ""], ["Gao", "Shenghua", ""], ["Wang", "Gang", ""], ["Zhang", "Tianzhu", ""], ["Ma", "Yi", ""]]}, {"id": "1403.8003", "submitter": "Fabian Rathke", "authors": "Fabian Rathke, Stefan Schmidt, Christoph Schn\\\"orr", "title": "Probabilistic Intra-Retinal Layer Segmentation in 3-D OCT Images Using\n  Global Shape Regularization", "comments": "Accepted for publication in Medical Image Analysis (MIA), Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of spectral-domain optical coherence tomography (OCT),\nresulting in a significant increase in acquisition speed, the fast and accurate\nsegmentation of 3-D OCT scans has become evermore important. This paper\npresents a novel probabilistic approach, that models the appearance of retinal\nlayers as well as the global shape variations of layer boundaries. Given an OCT\nscan, the full posterior distribution over segmentations is approximately\ninferred using a variational method enabling efficient probabilistic inference\nin terms of computationally tractable model components: Segmenting a full 3-D\nvolume takes around a minute. Accurate segmentations demonstrate the benefit of\nusing global shape regularization: We segmented 35 fovea-centered 3-D volumes\nwith an average unsigned error of 2.46 $\\pm$ 0.22 {\\mu}m as well as 80 normal\nand 66 glaucomatous 2-D circular scans with errors of 2.92 $\\pm$ 0.53 {\\mu}m\nand 4.09 $\\pm$ 0.98 {\\mu}m respectively. Furthermore, we utilized the inferred\nposterior distribution to rate the quality of the segmentation, point out\npotentially erroneous regions and discriminate normal from pathological scans.\nNo pre- or postprocessing was required and we used the same set of parameters\nfor all data sets, underlining the robustness and out-of-the-box nature of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 14:10:03 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Rathke", "Fabian", ""], ["Schmidt", "Stefan", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1403.8067", "submitter": "Xiao Bian", "authors": "Xiao Bian, Hamid Krim", "title": "Robust Subspace Recovery via Bi-Sparsity Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful applications of sparse models in computer vision and machine\nlearning imply that in many real-world applications, high dimensional data is\ndistributed in a union of low dimensional subspaces. Nevertheless, the\nunderlying structure may be affected by sparse errors and/or outliers. In this\npaper, we propose a bi-sparse model as a framework to analyze this problem and\nprovide a novel algorithm to recover the union of subspaces in presence of\nsparse corruptions. We further show the effectiveness of our method by\nexperiments on both synthetic data and real-world vision data.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 16:09:27 GMT"}, {"version": "v2", "created": "Sun, 20 Apr 2014 16:31:55 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Bian", "Xiao", ""], ["Krim", "Hamid", ""]]}, {"id": "1403.8098", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Jos\\'e Bioucas-Dias, Luis B. Almeida, Jocelyn\n  Chanussot", "title": "Hyperspectral image superresolution: An edge-preserving convex\n  formulation", "comments": "International Conference on Image Processing (ICIP), 2014 - accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral remote sensing images (HSIs) are characterized by having a low\nspatial resolution and a high spectral resolution, whereas multispectral images\n(MSIs) are characterized by low spectral and high spatial resolutions. These\ncomplementary characteristics have stimulated active research in the inference\nof images with high spatial and spectral resolutions from HSI-MSI pairs.\n  In this paper, we formulate this data fusion problem as the minimization of a\nconvex objective function containing two data-fitting terms and an\nedge-preserving regularizer. The data-fitting terms are quadratic and account\nfor blur, different spatial resolutions, and additive noise; the regularizer, a\nform of vector Total Variation, promotes aligned discontinuities across the\nreconstructed hyperspectral bands.\n  The optimization described above is rather hard, owing to its\nnon-diagonalizable linear operators, to the non-quadratic and non-smooth nature\nof the regularizer, and to the very large size of the image to be inferred. We\ntackle these difficulties by tailoring the Split Augmented Lagrangian Shrinkage\nAlgorithm (SALSA)---an instance of the Alternating Direction Method of\nMultipliers (ADMM)---to this optimization problem. By using a convenient\nvariable splitting and by exploiting the fact that HSIs generally \"live\" in a\nlow-dimensional subspace, we obtain an effective algorithm that yields\nstate-of-the-art results, as illustrated by experiments.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 17:18:48 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 11:58:55 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Almeida", "Luis B.", ""], ["Chanussot", "Jocelyn", ""]]}]