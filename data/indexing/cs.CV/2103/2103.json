[{"id": "2103.00020", "submitter": "Jong Wook Kim", "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel\n  Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack\n  Clark, Gretchen Krueger, Ilya Sutskever", "title": "Learning Transferable Visual Models From Natural Language Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art computer vision systems are trained to predict a fixed set\nof predetermined object categories. This restricted form of supervision limits\ntheir generality and usability since additional labeled data is needed to\nspecify any other visual concept. Learning directly from raw text about images\nis a promising alternative which leverages a much broader source of\nsupervision. We demonstrate that the simple pre-training task of predicting\nwhich caption goes with which image is an efficient and scalable way to learn\nSOTA image representations from scratch on a dataset of 400 million (image,\ntext) pairs collected from the internet. After pre-training, natural language\nis used to reference learned visual concepts (or describe new ones) enabling\nzero-shot transfer of the model to downstream tasks. We study the performance\nof this approach by benchmarking on over 30 different existing computer vision\ndatasets, spanning tasks such as OCR, action recognition in videos,\ngeo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a\nfully supervised baseline without the need for any dataset specific training.\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28 million training examples it\nwas trained on. We release our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 19:04:58 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Radford", "Alec", ""], ["Kim", "Jong Wook", ""], ["Hallacy", "Chris", ""], ["Ramesh", "Aditya", ""], ["Goh", "Gabriel", ""], ["Agarwal", "Sandhini", ""], ["Sastry", "Girish", ""], ["Askell", "Amanda", ""], ["Mishkin", "Pamela", ""], ["Clark", "Jack", ""], ["Krueger", "Gretchen", ""], ["Sutskever", "Ilya", ""]]}, {"id": "2103.00053", "submitter": "Reyhan Kevser Keser", "authors": "Reyhan Kevser Keser, Aydin Ayanzadeh, Omid Abdollahi Aghdam, Caglar\n  Kilcioglu, Behcet Ugur Toreyin, Nazim Kemal Ure", "title": "PURSUhInT: In Search of Informative Hint Points Based on Layer\n  Clustering for Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel knowledge distillation methodology for compressing deep\nneural networks. One of the most efficient methods for knowledge distillation\nis hint distillation, where the student model is injected with information\n(hints) from several different layers of the teacher model. Although the\nselection of hint points can drastically alter the compression performance,\nthere is no systematic approach for selecting them, other than brute-force\nhyper-parameter search. We propose a clustering based hint selection\nmethodology, where the layers of teacher model are clustered with respect to\nseveral metrics and the cluster centers are used as the hint points. The\nproposed approach is validated in CIFAR-100 dataset, where ResNet-110 network\nwas used as the teacher model. Our results show that hint points selected by\nour algorithm results in superior compression performance with respect to\nstate-of-the-art knowledge distillation algorithms on the same student models\nand datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 21:18:34 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Keser", "Reyhan Kevser", ""], ["Ayanzadeh", "Aydin", ""], ["Aghdam", "Omid Abdollahi", ""], ["Kilcioglu", "Caglar", ""], ["Toreyin", "Behcet Ugur", ""], ["Ure", "Nazim Kemal", ""]]}, {"id": "2103.00086", "submitter": "Moshiur R Farazi", "authors": "Ce Wang, Moshiur Farazi, Nick Barnes", "title": "Recursive Training for Zero-Shot Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General purpose semantic segmentation relies on a backbone CNN network to\nextract discriminative features that help classify each image pixel into a\n'seen' object class (ie., the object classes available during training) or a\nbackground class. Zero-shot semantic segmentation is a challenging task that\nrequires a computer vision model to identify image pixels belonging to an\nobject class which it has never seen before. Equipping a general purpose\nsemantic segmentation model to separate image pixels of 'unseen' classes from\nthe background remains an open challenge. Some recent models have approached\nthis problem by fine-tuning the final pixel classification layer of a semantic\nsegmentation model for a Zero-Shot setting, but struggle to learn\ndiscriminative features due to the lack of supervision. We propose a recursive\ntraining scheme to supervise the retraining of a semantic segmentation model\nfor a zero-shot setting using a pseudo-feature representation. To this end, we\npropose a Zero-Shot Maximum Mean Discrepancy (ZS-MMD) loss that weighs high\nconfidence outputs of the pixel classification layer as a pseudo-feature\nrepresentation, and feeds it back to the generator. By closing-the-loop on the\ngenerator end, we provide supervision during retraining that in turn helps the\nmodel learn a more discriminative feature representation for 'unseen' classes.\nWe show that using our recursive training and ZS-MMD loss, our proposed model\nachieves state-of-the-art performance on the Pascal-VOC 2012 dataset and\nPascal-Context dataset.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 23:44:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Ce", ""], ["Farazi", "Moshiur", ""], ["Barnes", "Nick", ""]]}, {"id": "2103.00087", "submitter": "Domenico Gatti", "authors": "Haikal Abdulah, Benjamin Huber, Sinan Lal, Hassan Abdallah, Luigi L.\n  Palese, Hamid Soltanian-Zadeh, Domenico L. Gatti", "title": "CXR-Net: An Artificial Intelligence Pipeline for Quick Covid-19\n  Screening of Chest X-Rays", "comments": "16 pages, 14 figures. arXiv admin note: substantial text overlap with\n  arXiv:2011.08655", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  CXR-Net is a two-module Artificial Intelligence pipeline for the quick\ndetection of SARS-CoV-2 from chest X-rays (CXRs). Module 1 was trained on a\npublic dataset of 6395 CXRs with radiologist annotated lung contours to\ngenerate masks of the lungs that overlap the heart and large vasa. Module 2 is\na hybrid convnet in which the first convolutional layer with learned\ncoefficients is replaced by a layer with fixed coefficients provided by the\nWavelet Scattering Transform (WST). Module 2 takes as inputs the patients CXRs\nand corresponding lung masks calculated by Module 1, and produces as outputs a\nclass assignment (Covid vs. non-Covid) and high resolution heat maps that\nidentify the SARS associated lung regions. Module 2 was trained on a dataset of\nCXRs from non-Covid and RT-PCR confirmed Covid patients acquired at the Henry\nFord Health System (HFHS) Hospital in Detroit. All non-Covid CXRs were from\npre-Covid era (2018-2019), and included images from both normal lungs and lungs\naffected by non-Covid pathologies. Training and test sets consisted of 2265\nCXRs (1417 Covid negative, 848 Covid positive), and 1532 CXRs (945 Covid\nnegative, 587 Covid positive), respectively. Six distinct cross-validation\nmodels, each trained on 1887 images and validated against 378 images, were\ncombined into an ensemble model that was used to classify the CXR images of the\ntest set with resulting Accuracy = 0.789, Precision = 0.739, Recall = 0.693, F1\nscore = 0.715, ROC(AUC) = 0.852.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 23:45:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Abdulah", "Haikal", ""], ["Huber", "Benjamin", ""], ["Lal", "Sinan", ""], ["Abdallah", "Hassan", ""], ["Palese", "Luigi L.", ""], ["Soltanian-Zadeh", "Hamid", ""], ["Gatti", "Domenico L.", ""]]}, {"id": "2103.00112", "submitter": "Kai Han", "authors": "Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang", "title": "Transformer in Transformer", "comments": "PyTorch code is available at\n  https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer is a new kind of neural architecture which encodes the input data\nas powerful features via the attention mechanism. Basically, the visual\ntransformers first divide the input images into several local patches and then\ncalculate both representations and their relationship. Since natural images are\nof high complexity with abundant detail and color information, the granularity\nof the patch dividing is not fine enough for excavating features of objects in\ndifferent scales and locations. In this paper, we point out that the attention\ninside these local patches are also essential for building visual transformers\nwith high performance and we explore a new architecture, namely, Transformer iN\nTransformer (TNT). Specifically, we regard the local patches (e.g.,\n16$\\times$16) as \"visual sentences\" and present to further divide them into\nsmaller patches (e.g., 4$\\times$4) as \"visual words\". The attention of each\nword will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be\naggregated to enhance the representation ability. Experiments on several\nbenchmarks demonstrate the effectiveness of the proposed TNT architecture,\ne.g., we achieve an $81.5%$ top-1 accuracy on the ImageNet, which is about\n$1.7%$ higher than that of the state-of-the-art visual transformer with similar\ncomputational cost. The PyTorch code is available at\nhttps://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch, and the\nMindSpore code is at\nhttps://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 03:12:16 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 03:31:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Han", "Kai", ""], ["Xiao", "An", ""], ["Wu", "Enhua", ""], ["Guo", "Jianyuan", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "2103.00119", "submitter": "Ali Pourramezan Fard", "authors": "Ali Pourramezan Fard, Hojjat Abdollahi, Mohammad Mahoor", "title": "ASMNet: a Lightweight Deep Neural Network for Face Alignment and Pose\n  Estimation", "comments": "Accepted at CVPR 2021 Biometrics Workshop, jointly with the Workshop\n  on Analysis and Modeling of Faces and Gestures", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, 2021, pp. 1521-1530", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active Shape Model (ASM) is a statistical model of object shapes that\nrepresents a target structure. ASM can guide machine learning algorithms to fit\na set of points representing an object (e.g., face) onto an image. This paper\npresents a lightweight Convolutional Neural Network (CNN) architecture with a\nloss function being assisted by ASM for face alignment and estimating head pose\nin the wild. We use ASM to first guide the network towards learning a smoother\ndistribution of the facial landmark points. Inspired by transfer learning,\nduring the training process, we gradually harden the regression problem and\nguide the network towards learning the original landmark points distribution.\nWe define multi-tasks in our loss function that are responsible for detecting\nfacial landmark points as well as estimating the face pose. Learning multiple\ncorrelated tasks simultaneously builds synergy and improves the performance of\nindividual tasks. We compare the performance of our proposed model called\nASMNet with MobileNetV2 (which is about 2 times bigger than ASMNet) in both the\nface alignment and pose estimation tasks. Experimental results on challenging\ndatasets show that by using the proposed ASM assisted loss function, the ASMNet\nperformance is comparable with MobileNetV2 in the face alignment task. In\naddition, for face pose estimation, ASMNet performs much better than\nMobileNetV2. ASMNet achieves an acceptable performance for facial landmark\npoints detection and pose estimation while having a significantly smaller\nnumber of parameters and floating-point operations compared to many CNN-based\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 03:46:54 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:40:12 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 17:44:58 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Fard", "Ali Pourramezan", ""], ["Abdollahi", "Hojjat", ""], ["Mahoor", "Mohammad", ""]]}, {"id": "2103.00121", "submitter": "Mozhdeh Rouhsedaghat", "authors": "Mozhdeh Rouhsedaghat, Masoud Monajatipoor, Zohreh Azizi, C.-C. Jay Kuo", "title": "Successive Subspace Learning: An Overview", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successive Subspace Learning (SSL) offers a light-weight unsupervised feature\nlearning method based on inherent statistical properties of data units (e.g.\nimage pixels and points in point cloud sets). It has shown promising results,\nespecially on small datasets. In this paper, we intuitively explain this\nmethod, provide an overview of its development, and point out some open\nquestions and challenges for future research.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 04:03:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Rouhsedaghat", "Mozhdeh", ""], ["Monajatipoor", "Masoud", ""], ["Azizi", "Zohreh", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2103.00128", "submitter": "Suraj Kothawade", "authors": "Vishal Kaushal, Suraj Kothawade, Ganesh Ramakrishnan, Jeff Bilmes,\n  Rishabh Iyer", "title": "PRISM: A Unified Framework of Parameterized Submodular Information\n  Measures for Targeted Data Subset Selection and Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing data, techniques for finding smaller, yet effective subsets\nwith specific characteristics become important. Motivated by this, we present\nPRISM, a rich class of Parameterized Submodular Information Measures, that can\nbe used in applications where such targeted subsets are desired. We demonstrate\nthe utility of PRISM in two such applications. First, we apply PRISM to improve\na supervised model's performance at a given additional labeling cost by\ntargeted subset selection (PRISM-TSS) where a subset of unlabeled points\nmatching a target set are added to the training set. We show that PRISM-TSS\ngeneralizes and is connected to several existing approaches to targeted data\nsubset selection. Second, we apply PRISM to a more nuanced targeted\nsummarization (PRISM-TSUM) where data (e.g., image collections, text or videos)\nis summarized for quicker human consumption with additional user intent.\nPRISM-TSUM handles multiple flavors of targeted summarization such as\nquery-focused, topic-irrelevant, privacy-preserving and update summarization in\na unified way. We show that PRISM-TSUM also generalizes and unifies several\nexisting past work on targeted summarization. Through extensive experiments on\nimage classification and image-collection summarization we empirically verify\nthe superiority of PRISM-TSS and PRISM-TSUM over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 04:53:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kaushal", "Vishal", ""], ["Kothawade", "Suraj", ""], ["Ramakrishnan", "Ganesh", ""], ["Bilmes", "Jeff", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2103.00140", "submitter": "Fei Li", "authors": "Fei Li, Xiangxu Li, Jun Luo, Shiwei Fan and Hongbo Zhang", "title": "Open-set Intersection Intention Prediction for Autonomous Driving", "comments": "Accepted by ICRA, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intention prediction is a crucial task for Autonomous Driving (AD). Due to\nthe variety of size and layout of intersections, it is challenging to predict\nintention of human driver at different intersections, especially unseen and\nirregular intersections. In this paper, we formulate the prediction of\nintention at intersections as an open-set prediction problem that requires\ncontext specific matching of the target vehicle state and the diverse\nintersection configurations that are in principle unbounded. We capture\nmap-centric features that correspond to intersection structures under a\nspatial-temporal graph representation, and use two MAAMs (mutually auxiliary\nattention module) that cover respectively lane-level and exitlevel intentions\nto predict a target that best matches intersection elements in map-centric\nfeature space. Under our model, attention scores estimate the probability\ndistribution of the openset intentions that are contextually defined by the\nstructure of the current intersection. The proposed model is trained and\nevaluated on simulated dataset. Furthermore, the model, trained on simulated\ndataset and without any fine tuning, is directly validated on in-house\nreal-world dataset collected at 98 realworld intersections and exhibits\nsatisfactory performance,demonstrating the practical viability of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 06:38:26 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 01:42:48 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Li", "Fei", ""], ["Li", "Xiangxu", ""], ["Luo", "Jun", ""], ["Fan", "Shiwei", ""], ["Zhang", "Hongbo", ""]]}, {"id": "2103.00142", "submitter": "Tristan Aumentado-Armstrong", "authors": "Tristan Aumentado-Armstrong, Stavros Tsogkas, Sven Dickinson, and\n  Allan Jepson", "title": "Disentangling Geometric Deformation Spaces in Generative Latent Shape\n  Models", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A complete representation of 3D objects requires characterizing the space of\ndeformations in an interpretable manner, from articulations of a single\ninstance to changes in shape across categories. In this work, we improve on a\nprior generative model of geometric disentanglement for 3D shapes, wherein the\nspace of object geometry is factorized into rigid orientation, non-rigid pose,\nand intrinsic shape. The resulting model can be trained from raw 3D shapes,\nwithout correspondences, labels, or even rigid alignment, using a combination\nof classical spectral geometry and probabilistic disentanglement of a\nstructured latent representation space. Our improvements include more\nsophisticated handling of rotational invariance and the use of a diffeomorphic\nflow network to bridge latent and spectral space. The geometric structuring of\nthe latent space imparts an interpretable characterization of the deformation\nspace of an object. Furthermore, it enables tasks like pose transfer and\npose-aware retrieval without requiring supervision. We evaluate our model on\nits generative modelling, representation learning, and disentanglement\nperformance, showing improved rotation invariance and intrinsic-extrinsic\nfactorization quality over the prior model.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 06:54:31 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Aumentado-Armstrong", "Tristan", ""], ["Tsogkas", "Stavros", ""], ["Dickinson", "Sven", ""], ["Jepson", "Allan", ""]]}, {"id": "2103.00145", "submitter": "Fei Li", "authors": "Fei Li, Shiwei Fan, Pengzhen Chen, and Xiangxu Li", "title": "Pedestrian Motion State Estimation From 2D Pose", "comments": null, "journal-ref": "2020 IEEE Intelligent Vehicles Symposium (IV). IEEE, 1682-1687", "doi": "10.1109/IV47402.2020.9304784", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic violation and the flexible and changeable nature of pedestrians make\nit more difficult to predict pedestrian behavior or intention, which might be a\npotential safety hazard on the road. Pedestrian motion state (such as walking\nand standing) directly affects or reflects its intention. In combination with\npedestrian motion state and other influencing factors, pedestrian intention can\nbe predicted to avoid unnecessary accidents. In this paper, pedestrian is\ntreated as non-rigid object, which can be represented by a set of\ntwo-dimensional key points, and the movement of key point relative to the torso\nis introduced as micro motion. Static and dynamic micro motion features, such\nas position, angle and distance, and their differential calculations in time\ndomain, are used to describe its motion pattern. Gated recurrent neural network\nbased seq2seq model is used to learn the dependence of motion state transition\non previous information, finally the pedestrian motion state is estimated via a\nsoftmax classifier. The proposed method only needs the previous hidden state of\nGRU and current feature to evaluate the probability of current motion state,\nand it is computation efficient to deploy on vehicles. This paper verifies the\nproposed algorithm on the JAAD public dataset, and the accuracy is improved by\n11.6% compared with the existing method.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 07:00:06 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Li", "Fei", ""], ["Fan", "Shiwei", ""], ["Chen", "Pengzhen", ""], ["Li", "Xiangxu", ""]]}, {"id": "2103.00188", "submitter": "Mengxi Liu", "authors": "Mengxi Liu, Qian Shi, Andrea Marinoni, Da He, Xiaoping Liu, Liangpei\n  Zhang", "title": "Super-resolution-based Change Detection Network with Stacked Attention\n  Module for Images with Different Resolutions", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing. 2021", "doi": "10.1109/TGRS.2021.3091758", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection, which aims to distinguish surface changes based on\nbi-temporal images, plays a vital role in ecological protection and urban\nplanning. Since high resolution (HR) images cannot be typically acquired\ncontinuously over time, bi-temporal images with different resolutions are often\nadopted for change detection in practical applications. Traditional\nsubpixel-based methods for change detection using images with different\nresolutions may lead to substantial error accumulation when HR images are\nemployed; this is because of intraclass heterogeneity and interclass\nsimilarity. Therefore, it is necessary to develop a novel method for change\ndetection using images with different resolutions, that is more suitable for HR\nimages. To this end, we propose a super-resolution-based change detection\nnetwork (SRCDNet) with a stacked attention module. The SRCDNet employs a super\nresolution (SR) module containing a generator and a discriminator to directly\nlearn SR images through adversarial learning and overcome the resolution\ndifference between bi-temporal images. To enhance the useful information in\nmulti-scale features, a stacked attention module consisting of five\nconvolutional block attention modules (CBAMs) is integrated to the feature\nextractor. The final change map is obtained through a metric learning-based\nchange decision module, wherein a distance map between bi-temporal features is\ncalculated. The experimental results demonstrate the superiority of the\nproposed method, which not only outperforms all baselines -with the highest F1\nscores of 87.40% on the building change detection dataset and 92.94% on the\nchange detection dataset -but also obtains the best accuracies on experiments\nperformed with images having a 4x and 8x resolution difference. The source code\nof SRCDNet will be available at https://github.com/liumency/SRCDNet.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 11:17:40 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liu", "Mengxi", ""], ["Shi", "Qian", ""], ["Marinoni", "Andrea", ""], ["He", "Da", ""], ["Liu", "Xiaoping", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2103.00191", "submitter": "Senthil Yogamani", "authors": "Anna Konrad, Ciar\\'an Eising, Ganesh Sistu, John McDonald, Rudi\n  Villing, Senthil Yogamani", "title": "FisheyeSuperPoint: Keypoint Detection and Description Network for\n  Fisheye Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keypoint detection and description is a commonly used building block in\ncomputer vision systems particularly for robotics and autonomous driving.\nRecently CNN based approaches have surpassed classical methods in a number of\nperception tasks. However, the majority of techniques to date have focused on\nstandard cameras with little consideration given to fisheye cameras which are\ncommonly used in autonomous driving. In this paper, we propose a novel training\nand evaluation pipeline for fisheye images. We make use of SuperPoint as our\nbaseline which is a self-supervised keypoint detector and descriptor that has\nachieved state-of-the-art results on homography estimation. We introduce a\nfisheye adaptation pipeline to enable training on undistorted fisheye images.\nWe evaluate the performance on the HPatches benchmark, and, by introducing a\nfisheye based evaluation methods for detection repeatability and descriptor\nmatching correctness on the Oxford RobotCar datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 11:26:34 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Konrad", "Anna", ""], ["Eising", "Ciar\u00e1n", ""], ["Sistu", "Ganesh", ""], ["McDonald", "John", ""], ["Villing", "Rudi", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2103.00208", "submitter": "Hao Chen", "authors": "Hao Chen, Zipeng Qi and Zhenwei Shi", "title": "Remote Sensing Image Change Detection with Transformers", "comments": "14 pages, 8 figures. Accepted article by IEEE TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2021.3095166", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern change detection (CD) has achieved remarkable success by the powerful\ndiscriminative ability of deep convolutions. However, high-resolution remote\nsensing CD remains challenging due to the complexity of objects in the scene.\nObjects with the same semantic concept may show distinct spectral\ncharacteristics at different times and spatial locations. Most recent CD\npipelines using pure convolutions are still struggling to relate long-range\nconcepts in space-time. Non-local self-attention approaches show promising\nperformance via modeling dense relations among pixels, yet are computationally\ninefficient. Here, we propose a bitemporal image transformer (BIT) to\nefficiently and effectively model contexts within the spatial-temporal domain.\nOur intuition is that the high-level concepts of the change of interest can be\nrepresented by a few visual words, i.e., semantic tokens. To achieve this, we\nexpress the bitemporal image into a few tokens, and use a transformer encoder\nto model contexts in the compact token-based space-time. The learned\ncontext-rich tokens are then feedback to the pixel-space for refining the\noriginal features via a transformer decoder. We incorporate BIT in a deep\nfeature differencing-based CD framework. Extensive experiments on three CD\ndatasets demonstrate the effectiveness and efficiency of the proposed method.\nNotably, our BIT-based model significantly outperforms the purely convolutional\nbaseline using only 3 times lower computational costs and model parameters.\nBased on a naive backbone (ResNet18) without sophisticated structures (e.g.,\nFPN, UNet), our model surpasses several state-of-the-art CD methods, including\nbetter than four recent attention-based methods in terms of efficiency and\naccuracy. Our code is available at https://github.com/justchenhao/BIT\\_CD.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 13:08:46 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 14:57:03 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 08:47:27 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Hao", ""], ["Qi", "Zipeng", ""], ["Shi", "Zhenwei", ""]]}, {"id": "2103.00218", "submitter": "Qing Guo", "authors": "Felix Juefei-Xu and Run Wang and Yihao Huang and Qing Guo and Lei Ma\n  and Yang Liu", "title": "Countering Malicious DeepFakes: Survey, Battleground, and Horizon", "comments": "34 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The creation and the manipulation of facial appearance via deep generative\napproaches, known as DeepFake, have achieved significant progress and promoted\na wide range of benign and malicious applications. The evil side of this new\ntechnique poses another popular study, i.e., DeepFake detection aiming to\nidentify the fake faces from the real ones. With the rapid development of the\nDeepFake-related studies in the community, both sides (i.e., DeepFake\ngeneration and detection) have formed the relationship of the battleground,\npushing the improvements of each other and inspiring new directions, e.g., the\nevasion of DeepFake detection. Nevertheless, the overview of such battleground\nand the new direction is unclear and neglected by recent surveys due to the\nrapid increase of related publications, limiting the in-depth understanding of\nthe tendency and future works.\n  To fill this gap, in this paper, we provide a comprehensive overview and\ndetailed analysis of the research work on the topic of DeepFake generation,\nDeepFake detection as well as evasion of DeepFake detection, with more than 191\nresearch papers carefully surveyed. We present the taxonomy of various DeepFake\ngeneration methods and the categorization of various DeepFake detection\nmethods, and more importantly, we showcase the battleground between the two\nparties with detailed interactions between the adversaries (DeepFake\ngeneration) and the defenders (DeepFake detection). The battleground allows\nfresh perspective into the latest landscape of the DeepFake research and can\nprovide valuable analysis towards the research challenges and opportunities as\nwell as research trends and directions in the field of DeepFake generation and\ndetection. We also elaborately design interactive diagrams\n(http://www.xujuefei.com/dfsurvey) to allow researchers to explore their own\ninterests on popular DeepFake generators or detectors.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 13:48:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Juefei-Xu", "Felix", ""], ["Wang", "Run", ""], ["Huang", "Yihao", ""], ["Guo", "Qing", ""], ["Ma", "Lei", ""], ["Liu", "Yang", ""]]}, {"id": "2103.00236", "submitter": "Dayan Guan", "authors": "Dayan Guan, Jiaxing Huang, Aoran Xiao, Shijian Lu, Yanpeng Cao", "title": "Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection", "comments": "Accepted in the IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Unsupervised domain adaptive object detection aims to adapt detectors from a\nlabelled source domain to an unlabelled target domain. Most existing works take\na two-stage strategy that first generates region proposals and then detects\nobjects of interest, where adversarial learning is widely adopted to mitigate\nthe inter-domain discrepancy in both stages. However, adversarial learning may\nimpair the alignment of well-aligned samples as it merely aligns the global\ndistributions across domains. To address this issue, we design an\nuncertainty-aware domain adaptation network (UaDAN) that introduces conditional\nadversarial learning to align well-aligned and poorly-aligned samples\nseparately in different manners. Specifically, we design an uncertainty metric\nthat assesses the alignment of each sample and adjusts the strength of\nadversarial learning for well-aligned and poorly-aligned samples adaptively. In\naddition, we exploit the uncertainty metric to achieve curriculum learning that\nfirst performs easier image-level alignment and then more difficult\ninstance-level alignment progressively. Extensive experiments over four\nchallenging domain adaptive object detection datasets show that UaDAN achieves\nsuperior performance as compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 15:04:07 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 13:14:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Guan", "Dayan", ""], ["Huang", "Jiaxing", ""], ["Xiao", "Aoran", ""], ["Lu", "Shijian", ""], ["Cao", "Yanpeng", ""]]}, {"id": "2103.00238", "submitter": "Artyom Grigoryan", "authors": "Artyom M. Grigoryan and Sos S. Agaian", "title": "Color-Coded Symbology and New Computer Vision Tool to Predict the\n  Historical Color Pallets of the Renaissance Oil Artworks", "comments": "16 pages, 12 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we discuss possible color palletes, prediction and analysis of\noriginality of the colors that Artists used on the Renaissance oil paintings.\nThis framework goal is to help to use the color symbology and image enhancement\ntools, to predict the historical color palletes of the Renaissance oil\nartworks. This work is only the start of a development to explore the\npossibilities of prediction of color palletes of the Renaissance oil artworks.\nWe believe that framework might be very useful in the prediction of color\npalletes of the Renaissance oil artworks and other artworks. The images in\nnumber 105 have been taken from the paintings of three well-known artists,\nRafael, Leonardo Da Vinci, and Rembrandt that are available in the Olga's\nGallery. Images are processed in the frequency domain to enhance a quality of\nimages and ratios of primary colors are calculated and analyzed by using new\nmeasurements of color-ratios.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 15:16:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Grigoryan", "Artyom M.", ""], ["Agaian", "Sos S.", ""]]}, {"id": "2103.00241", "submitter": "Cat Le", "authors": "Cat P. Le, Mohammadreza Soltani, Robert Ravier, Vahid Tarokh", "title": "Neural Architecture Search From Task Similarity Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a neural architecture search framework based on a\nsimilarity measure between various tasks defined in terms of Fisher\ninformation. By utilizing the relation between a target and a set of existing\ntasks, the search space of architectures can be significantly reduced, making\nthe discovery of the best candidates in the set of possible architectures\ntractable. This method eliminates the requirement for training the networks\nfrom scratch for the target task. Simulation results illustrate the efficacy of\nour proposed approach and its competitiveness with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 15:26:14 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 14:53:53 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 22:05:22 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Le", "Cat P.", ""], ["Soltani", "Mohammadreza", ""], ["Ravier", "Robert", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2103.00259", "submitter": "Jiebin Yan", "authors": "Jiebin Yan, Yu Zhong, Yuming Fang, Zhangyang Wang, Kede Ma", "title": "Exposing Semantic Segmentation Failures via Maximum Discrepancy\n  Competition", "comments": "19 pages, 12 figures, 5 tables, accepted by IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is an extensively studied task in computer vision, with\nnumerous methods proposed every year. Thanks to the advent of deep learning in\nsemantic segmentation, the performance on existing benchmarks is close to\nsaturation. A natural question then arises: Does the superior performance on\nthe closed (and frequently re-used) test sets transfer to the open visual world\nwith unconstrained variations? In this paper, we take steps toward answering\nthe question by exposing failures of existing semantic segmentation methods in\nthe open visual world under the constraint of very limited human labeling\neffort. Inspired by previous research on model falsification, we start from an\narbitrarily large image set, and automatically sample a small image set by\nMAximizing the Discrepancy (MAD) between two segmentation methods. The selected\nimages have the greatest potential in falsifying either (or both) of the two\nmethods. We also explicitly enforce several conditions to diversify the exposed\nfailures, corresponding to different underlying root causes. A segmentation\nmethod, whose failures are more difficult to be exposed in the MAD competition,\nis considered better. We conduct a thorough MAD diagnosis of ten PASCAL VOC\nsemantic segmentation algorithms. With detailed analysis of experimental\nresults, we point out strengths and weaknesses of the competing algorithms, as\nwell as potential research directions for further advancement in semantic\nsegmentation. The codes are publicly available at\n\\url{https://github.com/QTJiebin/MAD_Segmentation}.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 16:06:25 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 14:22:13 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Yan", "Jiebin", ""], ["Zhong", "Yu", ""], ["Fang", "Yuming", ""], ["Wang", "Zhangyang", ""], ["Ma", "Kede", ""]]}, {"id": "2103.00262", "submitter": "Claudio Mura", "authors": "Claudio Mura, Renato Pajarola, Konrad Schindler, Niloy Mitra", "title": "Walk2Map: Extracting Floor Plans from Indoor Walk Trajectories", "comments": "To be published in Computer Graphics Forum (Proc. Eurographics 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a proliferation of new digital products for the\nefficient management of indoor spaces, with important applications like\nemergency management, virtual property showcasing and interior design. These\nproducts rely on accurate 3D models of the environments considered, including\ninformation on both architectural and non-permanent elements. These models must\nbe created from measured data such as RGB-D images or 3D point clouds, whose\ncapture and consolidation involves lengthy data workflows. This strongly limits\nthe rate at which 3D models can be produced, preventing the adoption of many\ndigital services for indoor space management. We provide an alternative to such\ndata-intensive procedures by presenting Walk2Map, a data-driven approach to\ngenerate floor plans only from trajectories of a person walking inside the\nrooms. Thanks to recent advances in data-driven inertial odometry, such\nminimalistic input data can be acquired from the IMU readings of consumer-level\nsmartphones, which allows for an effortless and scalable mapping of real-world\nindoor spaces. Our work is based on learning the latent relation between an\nindoor walk trajectory and the information represented in a floor plan:\ninterior space footprint, portals, and furniture. We distinguish between\nrecovering area-related (interior footprint, furniture) and wall-related\n(doors) information and use two different neural architectures for the two\ntasks: an image-based Encoder-Decoder and a Graph Convolutional Network,\nrespectively. We train our networks using scanned 3D indoor models and apply\nthem in a cascaded fashion on an indoor walk trajectory at inference time. We\nperform a qualitative and quantitative evaluation using both simulated and\nmeasured, real-world trajectories, and compare against a baseline method for\nimage-to-image translation. The experiments confirm the feasibility of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 16:29:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mura", "Claudio", ""], ["Pajarola", "Renato", ""], ["Schindler", "Konrad", ""], ["Mitra", "Niloy", ""]]}, {"id": "2103.00268", "submitter": "Naoki Wake", "authors": "Naoki Wake, Daichi Saito, Kazuhiro Sasabuchi, Hideki Koike, Katsushi\n  Ikeuchi", "title": "Object affordance as a guide for grasp-type recognition", "comments": "12 pages, 11 figures. Last updated February 27th, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human grasping strategies is an important factor in robot\nteaching as these strategies contain the implicit knowledge necessary to\nperform a series of manipulations smoothly. This study analyzed the effects of\nobject affordance-a prior distribution of grasp types for each object-on\nconvolutional neural network (CNN)-based grasp-type recognition. To this end,\nwe created datasets of first-person grasping-hand images labeled with grasp\ntypes and object names, and tested a recognition pipeline leveraging object\naffordance. We evaluated scenarios with real and illusory objects to be\ngrasped, to consider a teaching condition in mixed reality where the lack of\nvisual object information can make the CNN recognition challenging. The results\nshow that object affordance guided the CNN in both scenarios, increasing the\naccuracy by 1) excluding unlikely grasp types from the candidates and 2)\nenhancing likely grasp types. In addition, the \"enhancing effect\" was more\npronounced with high degrees of grasp-type heterogeneity. These results\nindicate the effectiveness of object affordance for guiding grasp-type\nrecognition in robot teaching applications.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 17:03:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wake", "Naoki", ""], ["Saito", "Daichi", ""], ["Sasabuchi", "Kazuhiro", ""], ["Koike", "Hideki", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "2103.00274", "submitter": "Yingying Xu", "authors": "Yingying Xu, Ming Cai, Lanfen Lin, Yue Zhang, Hongjie Hu, Zhiyi Peng,\n  Qiaowei Zhang, Qingqing Chen, Xiongwei Mao, Yutaro Iwamoto, Xian-Hua Han,\n  Yen-Wei Chen, Ruofeng Tong", "title": "PA-ResSeg: A Phase Attention Residual Network for Liver Tumor\n  Segmentation from Multi-phase CT Images", "comments": "A self-archive version to be published in Medical Physics, awaiting\n  minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a phase attention residual network (PA-ResSeg) to\nmodel multi-phase features for accurate liver tumor segmentation, in which a\nphase attention (PA) is newly proposed to additionally exploit the images of\narterial (ART) phase to facilitate the segmentation of portal venous (PV)\nphase. The PA block consists of an intra-phase attention (Intra-PA) module and\nan inter-phase attention (Inter-PA) module to capture channel-wise\nself-dependencies and cross-phase interdependencies, respectively. Thus it\nenables the network to learn more representative multi-phase features by\nrefining the PV features according to the channel dependencies and\nrecalibrating the ART features based on the learned interdependencies between\nphases. We propose a PA-based multi-scale fusion (MSF) architecture to embed\nthe PA blocks in the network at multiple levels along the encoding path to fuse\nmulti-scale features from multi-phase images. Moreover, a 3D boundary-enhanced\nloss (BE-loss) is proposed for training to make the network more sensitive to\nboundaries. To evaluate the performance of our proposed PA-ResSeg, we conducted\nexperiments on a multi-phase CT dataset of focal liver lesions (MPCT-FLLs).\nExperimental results show the effectiveness of the proposed method by achieving\na dice per case (DPC) of 0.77.87, a dice global (DG) of 0.8682, a volumetric\noverlap error (VOE) of 0.3328 and a relative volume difference (RVD) of 0.0443\non the MPCT-FLLs. Furthermore, to validate the effectiveness and robustness of\nPA-ResSeg, we conducted extra experiments on another multi-phase liver tumor\ndataset and obtained a DPC of 0.8290, a DG of 0.9132, a VOE of 0.2637 and a RVD\nof 0.0163. The proposed method shows its robustness and generalization\ncapability in different datasets and different backbones.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 17:30:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Yingying", ""], ["Cai", "Ming", ""], ["Lin", "Lanfen", ""], ["Zhang", "Yue", ""], ["Hu", "Hongjie", ""], ["Peng", "Zhiyi", ""], ["Zhang", "Qiaowei", ""], ["Chen", "Qingqing", ""], ["Mao", "Xiongwei", ""], ["Iwamoto", "Yutaro", ""], ["Han", "Xian-Hua", ""], ["Chen", "Yen-Wei", ""], ["Tong", "Ruofeng", ""]]}, {"id": "2103.00286", "submitter": "Raziyeh Dehbozorgi", "authors": "A. Ziaee, R. Dehbozorgi, M. D\\\"oller", "title": "A Novel Adaptive Deep Network for Building Footprint Segmentation", "comments": "Deep Learning Semantic Segmentation, Building Footprint Segmentation,\n  Conditional Generative Adversarial Networks(CGANs), Pix2Pix Network", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA eess.IV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building footprint segmentations for high resolution images are increasingly\ndemanded for many remote sensing applications. By the emerging deep learning\napproaches, segmentation networks have made significant advances in the\nsemantic segmentation of objects. However, these advances and the increased\naccess to satellite images require the generation of accurate object boundaries\nin satellite images. In the current paper, we propose a novel network-based on\nPix2Pix methodology to solve the problem of inaccurate boundaries obtained by\nconverting satellite images into maps using segmentation networks in order to\nsegment building footprints. To define the new network named G2G, our framework\nincludes two generators where the first generator extracts localization\nfeatures in order to merge them with the boundary features extracted from the\nsecond generator to segment all detailed building edges. Moreover, different\nstrategies are implemented to enhance the quality of the proposed networks'\nresults, implying that the proposed network outperforms state-of-the-art\nnetworks in segmentation accuracy with a large margin for all evaluation\nmetrics. The implementation is available at\nhttps://github.com/A2Amir/A-Novel-Adaptive-Deep-Network-for-Building-Footprint-Segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 18:13:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ziaee", "A.", ""], ["Dehbozorgi", "R.", ""], ["D\u00f6ller", "M.", ""]]}, {"id": "2103.00302", "submitter": "Denis Baru\\v{c}i\\'c", "authors": "Denis Baru\\v{c}i\\'c (1), Jan Kybic (1), Olga Tepl\\'a (2), Zinovij\n  Topurko (2), Irena Kratochv\\'ilov\\'a (3) ((1) Czech Technical University in\n  Prague, Czech Republic, (2) The First Faculty of Medicine and General\n  Teaching Hospital, Czech Republic, (3) Institute of Physics of the Czech\n  Academy of Sciences, Czech Republic)", "title": "Automatic evaluation of human oocyte developmental potential from\n  microscopy images", "comments": "submitted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infertility is becoming an issue for an increasing number of couples. The\nmost common solution, in vitro fertilization, requires embryologists to\ncarefully examine light microscopy images of human oocytes to determine their\ndevelopmental potential. We propose an automatic system to improve the speed,\nrepeatability, and accuracy of this process. We first localize individual\noocytes and identify their principal components using CNN (U-Net) segmentation.\nWe calculate several descriptors based on geometry and texture. The final step\nis an SVM classifier. Both the segmentation and classification training are\nbased on expert annotations. The presented approach leads to the classification\naccuracy of 70%.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 19:36:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Baru\u010di\u0107", "Denis", ""], ["Kybic", "Jan", ""], ["Tepl\u00e1", "Olga", ""], ["Topurko", "Zinovij", ""], ["Kratochv\u00edlov\u00e1", "Irena", ""]]}, {"id": "2103.00334", "submitter": "Ziyun Yang", "authors": "Ziyun Yang, Somayyeh Soltanian-Zadeh, Sina Farsiu", "title": "BiconNet: An Edge-preserved Connectivity-based Approach for Salient\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection (SOD) is viewed as a pixel-wise saliency modeling\ntask by traditional deep learning-based methods. A limitation of current SOD\nmodels is insufficient utilization of inter-pixel information, which usually\nresults in imperfect segmentation near edge regions and low spatial coherence.\nAs we demonstrate, using a saliency mask as the only label is suboptimal. To\naddress this limitation, we propose a connectivity-based approach called\nbilateral connectivity network (BiconNet), which uses connectivity masks\ntogether with saliency masks as labels for effective modeling of inter-pixel\nrelationships and object saliency. Moreover, we propose a bilateral voting\nmodule to enhance the output connectivity map, and a novel edge feature\nenhancement method that efficiently utilizes edge-specific features. Through\ncomprehensive experiments on five benchmark datasets, we demonstrate that our\nproposed method can be plugged into any existing state-of-the-art\nsaliency-based SOD framework to improve its performance with negligible\nparameter increase.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 21:39:04 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 01:38:56 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yang", "Ziyun", ""], ["Soltanian-Zadeh", "Somayyeh", ""], ["Farsiu", "Sina", ""]]}, {"id": "2103.00355", "submitter": "Weixiao Gao", "authors": "Weixiao Gao, Liangliang Nan, Bas Boom, Hugo Ledoux", "title": "SUM: A Benchmark Dataset of Semantic Urban Meshes", "comments": "27 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in data acquisition technology allow us to collect 3D\ntexture meshes quickly. Those can help us understand and analyse the urban\nenvironment, and as a consequence are useful for several applications like\nspatial analysis and urban planning. Semantic segmentation of texture meshes\nthrough deep learning methods can enhance this understanding, but it requires a\nlot of labelled data. The contributions of this work are threefold: (1) a new\nbenchmark dataset of semantic urban meshes, (2) a novel semi-automatic\nannotation framework, and (3) an annotation tool for 3D meshes. In particular,\nour dataset covers about 4 km2 in Helsinki (Finland), with six classes, and we\nestimate that we save about 600 hours of labelling work using our annotation\nframework, which includes initial segmentation and interactive refinement. We\nalso compare the performance of several state-of-theart 3D semantic\nsegmentation methods on the new benchmark dataset. Other researchers can use\nour results to train their networks: the dataset is publicly available, and the\nannotation tool is released as open-source.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 23:26:21 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 14:25:37 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gao", "Weixiao", ""], ["Nan", "Liangliang", ""], ["Boom", "Bas", ""], ["Ledoux", "Hugo", ""]]}, {"id": "2103.00356", "submitter": "Lei Gao", "authors": "Lei Gao, Lin Qi, Ling Guan", "title": "Online Behavioral Analysis with Application to Emotion State\n  Identification", "comments": null, "journal-ref": "IEEE Intelligent Systems, 2016", "doi": "10.1109/MIS.2016.26", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel discriminative model for online behavioral\nanalysis with application to emotion state identification. The proposed model\nis able to extract more discriminative characteristics from behavioral data\neffectively and find the direction of optimal projection efficiently to satisfy\nrequirements of online data analysis, leading to better utilization of the\nbehavioral information to produce more accurate recognition results.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 23:53:52 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gao", "Lei", ""], ["Qi", "Lin", ""], ["Guan", "Ling", ""]]}, {"id": "2103.00359", "submitter": "Lei Gao", "authors": "Lei Gao, Rui Zhang, Lin Qi, Enqing Chen, and Ling Guan", "title": "The Labeled Multiple Canonical Correlation Analysis for Information\n  Fusion", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, 2019", "doi": "10.1109/TMM.2018.2859590", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of multimodal information fusion is to mathematically analyze\ninformation carried in different sources and create a new representation which\nwill be more effectively utilized in pattern recognition and other multimedia\ninformation processing tasks. In this paper, we introduce a new method for\nmultimodal information fusion and representation based on the Labeled Multiple\nCanonical Correlation Analysis (LMCCA). By incorporating class label\ninformation of the training samples,the proposed LMCCA ensures that the fused\nfeatures carry discriminative characteristics of the multimodal information\nrepresentations, and are capable of providing superior recognition performance.\nWe implement a prototype of LMCCA to demonstrate its effectiveness on\nhandwritten digit recognition,face recognition and object recognition utilizing\nmultiple features,bimodal human emotion recognition involving information from\nboth audio and visual domains. The generic nature of LMCCA allows it to take as\ninput features extracted by any means,including those by deep learning (DL)\nmethods. Experimental results show that the proposed method enhanced the\nperformance of both statistical machine learning (SML) methods, and methods\nbased on DL.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 00:13:36 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gao", "Lei", ""], ["Zhang", "Rui", ""], ["Qi", "Lin", ""], ["Chen", "Enqing", ""], ["Guan", "Ling", ""]]}, {"id": "2103.00363", "submitter": "Guoyang Xie", "authors": "Guoyang Xie, Jinbao Wang, Guo Yu, Feng Zheng, Yaochu Jin", "title": "Tiny Adversarial Mulit-Objective Oneshot Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to limited computational cost and energy consumption, most neural network\nmodels deployed in mobile devices are tiny. However, tiny neural networks are\ncommonly very vulnerable to attacks. Current research has proved that larger\nmodel size can improve robustness, but little research focuses on how to\nenhance the robustness of tiny neural networks. Our work focuses on how to\nimprove the robustness of tiny neural networks without seriously deteriorating\nof clean accuracy under mobile-level resources. To this end, we propose a\nmulti-objective oneshot network architecture search (NAS) algorithm to obtain\nthe best trade-off networks in terms of the adversarial accuracy, the clean\naccuracy and the model size. Specifically, we design a novel search space based\non new tiny blocks and channels to balance model size and adversarial\nperformance. Moreover, since the supernet significantly affects the performance\nof subnets in our NAS algorithm, we reveal the insights into how the supernet\nhelps to obtain the best subnet under white-box adversarial attacks.\nConcretely, we explore a new adversarial training paradigm by analyzing the\nadversarial transferability, the width of the supernet and the difference\nbetween training the subnets from scratch and fine-tuning. Finally, we make a\nstatistical analysis for the layer-wise combination of certain blocks and\nchannels on the first non-dominated front, which can serve as a guideline to\ndesign tiny neural network architectures for the resilience of adversarial\nperturbations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 00:54:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xie", "Guoyang", ""], ["Wang", "Jinbao", ""], ["Yu", "Guo", ""], ["Zheng", "Feng", ""], ["Jin", "Yaochu", ""]]}, {"id": "2103.00364", "submitter": "Rohan Shad", "authors": "Rohan Shad, Nicolas Quach, Robyn Fong, Patpilai Kasinpila, Cayley\n  Bowles, Miguel Castro, Ashrith Guha, Eddie Suarez, Stefan Jovinge, Sangjin\n  Lee, Theodore Boeve, Myriam Amsallem, Xiu Tang, Francois Haddad, Yasuhiro\n  Shudo, Y. Joseph Woo, Jeffrey Teuteberg, John P. Cunningham, Curt P.\n  Langlotz, William Hiesinger", "title": "Predicting post-operative right ventricular failure using video-based\n  deep learning", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive and cost effective in nature, the echocardiogram allows for a\ncomprehensive assessment of the cardiac musculature and valves. Despite\nprogressive improvements over the decades, the rich temporally resolved data in\nechocardiography videos remain underutilized. Human reads of echocardiograms\nreduce the complex patterns of cardiac wall motion, to a small list of\nmeasurements of heart function. Furthermore, all modern echocardiography\nartificial intelligence (AI) systems are similarly limited by design -\nautomating measurements of the same reductionist metrics rather than utilizing\nthe wealth of data embedded within each echo study. This underutilization is\nmost evident in situations where clinical decision making is guided by\nsubjective assessments of disease acuity, and tools that predict disease onset\nwithin clinically actionable timeframes are unavailable. Predicting the\nlikelihood of developing post-operative right ventricular failure (RV failure)\nin the setting of mechanical circulatory support is one such clinical example.\nTo address this, we developed a novel video AI system trained to predict\npost-operative right ventricular failure (RV failure), using the full\nspatiotemporal density of information from pre-operative echocardiography\nscans. We achieve an AUC of 0.729, specificity of 52% at 80% sensitivity and\n46% sensitivity at 80% specificity. Furthermore, we show that our ML system\nsignificantly outperforms a team of human experts tasked with predicting RV\nfailure on independent clinical evaluation. Finally, the methods we describe\nare generalizable to any cardiac clinical decision support application where\ntreatment or patient selection is guided by qualitative echocardiography\nassessments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 00:58:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Shad", "Rohan", ""], ["Quach", "Nicolas", ""], ["Fong", "Robyn", ""], ["Kasinpila", "Patpilai", ""], ["Bowles", "Cayley", ""], ["Castro", "Miguel", ""], ["Guha", "Ashrith", ""], ["Suarez", "Eddie", ""], ["Jovinge", "Stefan", ""], ["Lee", "Sangjin", ""], ["Boeve", "Theodore", ""], ["Amsallem", "Myriam", ""], ["Tang", "Xiu", ""], ["Haddad", "Francois", ""], ["Shudo", "Yasuhiro", ""], ["Woo", "Y. Joseph", ""], ["Teuteberg", "Jeffrey", ""], ["Cunningham", "John P.", ""], ["Langlotz", "Curt P.", ""], ["Hiesinger", "William", ""]]}, {"id": "2103.00369", "submitter": "Muhammad Umar Karim Khan", "authors": "Muhammad Umar Karim Khan", "title": "Towards Continual, Online, Unsupervised Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although depth extraction with passive sensors has seen remarkable\nimprovement with deep learning, these approaches may fail to obtain correct\ndepth if they are exposed to environments not observed during training. Online\nadaptation, where the neural network trains while deployed, with unsupervised\nlearning provides a convenient solution. However, online adaptation causes a\nneural network to forget the past. Thus, past training is wasted and the\nnetwork is not able to provide good results if it observes past scenes. This\nwork deals with practical online-adaptation where the input is online and\ntemporally-correlated, and training is completely unsupervised. Regularization\nand replay-based methods without task boundaries are proposed to avoid\ncatastrophic forgetting while adapting to online data. Experiments are\nperformed on different datasets with both structure-from-motion and stereo.\nResults of forgetting as well as adaptation are provided, which are superior to\nrecent methods. The proposed approach is more inline with the artificial\ngeneral intelligence paradigm as the neural network learns the scene where it\nis deployed without any supervision (target labels and tasks) and without\nforgetting about the past. Code is available at github.com/umarKarim/cou_stereo\nand github.com/umarKarim/cou_sfm.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:18:49 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 02:36:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Khan", "Muhammad Umar Karim", ""]]}, {"id": "2103.00370", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Scott Lundberg, Lei Zhang, Stephanie Fu, William T.\n  Freeman", "title": "Model-Agnostic Explainability for Visual Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What makes two images similar? We propose new approaches to generate\nmodel-agnostic explanations for image similarity, search, and retrieval. In\nparticular, we extend Class Activation Maps (CAMs), Additive Shapley\nExplanations (SHAP), and Locally Interpretable Model-Agnostic Explanations\n(LIME) to the domain of image retrieval and search. These approaches enable\nblack and grey-box model introspection and can help diagnose errors and\nunderstand the rationale behind a model's similarity judgments. Furthermore, we\nextend these approaches to extract a full pairwise correspondence between the\nquery and retrieved image pixels, an approach we call \"joint interpretations\".\nFormally, we show joint search interpretations arise from projecting Harsanyi\ndividends, and that this approach generalizes Shapley Values and The\nShapley-Taylor indices. We introduce a fast kernel-based method for estimating\nShapley-Taylor indices and empirically show that these game-theoretic measures\nyield more consistent explanations for image similarity architectures.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:24:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hamilton", "Mark", ""], ["Lundberg", "Scott", ""], ["Zhang", "Lei", ""], ["Fu", "Stephanie", ""], ["Freeman", "William T.", ""]]}, {"id": "2103.00397", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, Zhangyang Wang", "title": "Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery\n  Ticket Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative adversarial networks (GANs) with limited real image data\ngenerally results in deteriorated performance and collapsed models. To conquer\nthis challenge, we are inspired by the latest observations, that one can\ndiscover independently trainable and highly sparse subnetworks (a.k.a., lottery\ntickets) from GANs. Treating this as an inductive prior, we suggest a brand-new\nangle towards data-efficient GAN training: by first identifying the lottery\nticket from the original GAN using the small training set of real images; and\nthen focusing on training that sparse subnetwork by re-using the same set. Both\nsteps have lower complexity and are more data-efficient to train. We find our\ncoordinated framework to offer orthogonal gains to existing real image data\naugmentation methods, and we additionally offer a new feature-level\naugmentation that can be applied together with them. Comprehensive experiments\nendorse the effectiveness of our proposed framework, across various GAN\narchitectures (SNGAN, BigGAN, and StyleGAN-V2) and diverse datasets (CIFAR-10,\nCIFAR-100, Tiny-ImageNet, and ImageNet). Our training framework also displays\npowerful few-shot generalization ability, i.e., generating high-fidelity images\nby training from scratch with just 100 real images, without any pre-training.\nCodes are available at:\nhttps://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 05:20:29 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 04:57:31 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Tianlong", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2103.00429", "submitter": "Jialin Peng", "authors": "Jialin Peng, Ye Wang", "title": "Medical Image Segmentation with Limited Supervision: A Review of Deep\n  Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the remarkable performance of deep learning methods on various tasks,\nmost cutting-edge models rely heavily on large-scale annotated training\nexamples, which are often unavailable for clinical and health care tasks. The\nlabeling costs for medical images are very high, especially in medical image\nsegmentation, which typically requires intensive pixel/voxel-wise labeling.\nTherefore, the strong capability of learning and generalizing from limited\nsupervision, including a limited amount of annotations, sparse annotations, and\ninaccurate annotations, is crucial for the successful application of deep\nlearning models in medical image segmentation. However, due to its intrinsic\ndifficulty, segmentation with limited supervision is challenging and specific\nmodel design and/or learning strategies are needed. In this paper, we provide a\nsystematic and up-to-date review of the solutions above, with summaries and\ncomments about the methodologies. We also highlight several problems in this\nfield, discussed future directions observing further investigations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 08:52:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Peng", "Jialin", ""], ["Wang", "Ye", ""]]}, {"id": "2103.00430", "submitter": "Chengchao Shen", "authors": "Chengchao Shen, Youtan Yin, Xinchao Wang, Xubin Li, Jie Song, Mingli\n  Song", "title": "Training Generative Adversarial Networks in One Stage", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have demonstrated unprecedented\nsuccess in various image generation tasks. The encouraging results, however,\ncome at the price of a cumbersome training process, during which the generator\nand discriminator are alternately updated in two stages. In this paper, we\ninvestigate a general training scheme that enables training GANs efficiently in\nonly one stage. Based on the adversarial losses of the generator and\ndiscriminator, we categorize GANs into two classes, Symmetric GANs and\nAsymmetric GANs, and introduce a novel gradient decomposition method to unify\nthe two, allowing us to train both classes in one stage and hence alleviate the\ntraining effort. We also computationally analyze the efficiency of the proposed\nmethod, and empirically demonstrate that, the proposed method yields a solid\n$1.5\\times$ acceleration across various datasets and network architectures.\nFurthermore, we show that the proposed method is readily applicable to other\nadversarial-training scenarios, such as data-free knowledge distillation. The\ncode is available at https://github.com/zju-vipa/OSGAN.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 09:03:39 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 02:56:12 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 13:05:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Shen", "Chengchao", ""], ["Yin", "Youtan", ""], ["Wang", "Xinchao", ""], ["Li", "Xubin", ""], ["Song", "Jie", ""], ["Song", "Mingli", ""]]}, {"id": "2103.00446", "submitter": "Mahdi Kazemi Moghaddam", "authors": "Mahdi Kazemi Moghaddam, Ehsan Abbasnejad, Qi Wu, Javen Shi and Anton\n  Van Den Hengel", "title": "Learning for Visual Navigation by Imagining the Success", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual navigation is often cast as a reinforcement learning (RL) problem.\nCurrent methods typically result in a suboptimal policy that learns general\nobstacle avoidance and search behaviours. For example, in the target-object\nnavigation setting, the policies learnt by traditional methods often fail to\ncomplete the task, even when the target is clearly within reach from a human\nperspective. In order to address this issue, we propose to learn to imagine a\nlatent representation of the successful (sub-)goal state. To do so, we have\ndeveloped a module which we call Foresight Imagination (ForeSIT). ForeSIT is\ntrained to imagine the recurrent latent representation of a future state that\nleads to success, e.g. either a sub-goal state that is important to reach\nbefore the target, or the goal state itself. By conditioning the policy on the\ngenerated imagination during training, our agent learns how to use this\nimagination to achieve its goal robustly. Our agent is able to imagine what the\n(sub-)goal state may look like (in the latent space) and can learn to navigate\ntowards that state. We develop an efficient learning algorithm to train ForeSIT\nin an on-policy manner and integrate it into our RL objective. The integration\nis not trivial due to the constantly evolving state representation shared\nbetween both the imagination and the policy. We, empirically, observe that our\nmethod outperforms the state-of-the-art methods by a large margin in the\ncommonly accepted benchmark AI2THOR environment. Our method can be readily\nintegrated or added to other model-free RL navigation frameworks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 10:25:46 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Moghaddam", "Mahdi Kazemi", ""], ["Abbasnejad", "Ehsan", ""], ["Wu", "Qi", ""], ["Shi", "Javen", ""], ["Hengel", "Anton Van Den", ""]]}, {"id": "2103.00466", "submitter": "Omar Sharif", "authors": "Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque", "title": "NLP-CUET@DravidianLangTech-EACL2021: Investigating Visual and Textual\n  Features to Identify Trolls from Multimodal Social Media Memes", "comments": "3rd rank DravidianLangTech workshop shared task, EACL-2021, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the past few years, the meme has become a new way of communication on the\nInternet. As memes are the images with embedded text, it can quickly spread\nhate, offence and violence. Classifying memes are very challenging because of\ntheir multimodal nature and region-specific interpretation. A shared task is\norganized to develop models that can identify trolls from multimodal social\nmedia memes. This work presents a computational model that we have developed as\npart of our participation in the task. Training data comes in two forms: an\nimage with embedded Tamil code-mixed text and an associated caption given in\nEnglish. We investigated the visual and textual features using CNN, VGG16,\nInception, Multilingual-BERT, XLM-Roberta, XLNet models. Multimodal features\nare extracted by combining image (CNN, ResNet50, Inception) and text (Long\nshort term memory network) features via early fusion approach. Results indicate\nthat the textual approach with XLNet achieved the highest weighted $f_1$-score\nof $0.58$, which enabled our model to secure $3^{rd}$ rank in this task.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 11:36:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hossain", "Eftekhar", ""], ["Sharif", "Omar", ""], ["Hoque", "Mohammed Moshiul", ""]]}, {"id": "2103.00497", "submitter": "Aryan Asadian", "authors": "Aryan Asadian, Amirali Salehi-Abari", "title": "Distilling Knowledge via Intermediate Classifiers", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crux of knowledge distillation is to effectively train a resource-limited\nstudent model with the guide of a pre-trained larger teacher model. However,\nwhen there is a large difference between the model complexities of teacher and\nstudent (i.e., capacity gap), knowledge distillation loses its strength in\ntransferring knowledge from the teacher to the student, thus training a weaker\nstudent. To mitigate the impact of the capacity gap, we introduce knowledge\ndistillation via intermediate heads. By extending the intermediate layers of\nthe teacher (at various depths) with classifier heads, we cheaply acquire a\ncohort of heterogeneous pre-trained teachers. The intermediate classifier heads\ncan all together be efficiently learned while freezing the backbone of the\npre-trained teacher. The cohort of teachers (including the original teacher)\nco-teach the student simultaneously. Our experiments on various teacher-student\npairs and datasets have demonstrated that the proposed approach outperforms the\ncanonical knowledge distillation approach and its extensions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 12:52:52 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 13:20:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Asadian", "Aryan", ""], ["Salehi-Abari", "Amirali", ""]]}, {"id": "2103.00528", "submitter": "Lie Ju", "authors": "Lie Ju, Xin Wang, Lin Wang, Dwarikanath Mahapatra, Xin Zhao, Mehrtash\n  Harandi, Tom Drummond, Tongliang Liu, Zongyuan Ge", "title": "Improving Medical Image Classification with Label Noise Using\n  Dual-uncertainty Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be data-driven and label noise can have a\nmarked impact on model performance. Recent studies have shown great robustness\nto classic image recognition even under a high noisy rate. In medical\napplications, learning from datasets with label noise is more challenging since\nmedical imaging datasets tend to have asymmetric (class-dependent) noise and\nsuffer from high observer variability.\n  In this paper, we systematically discuss and define the two common types of\nlabel noise in medical images - disagreement label noise from inconsistency\nexpert opinions and single-target label noise from wrong diagnosis record. We\nthen propose an uncertainty estimation-based framework to handle these two\nlabel noise amid the medical image classification task. We design a\ndual-uncertainty estimation approach to measure the disagreement label noise\nand single-target label noise via Direct Uncertainty Prediction and\nMonte-Carlo-Dropout.\n  A boosting-based curriculum training procedure is later introduced for robust\nlearning. We demonstrate the effectiveness of our method by conducting\nextensive experiments on three different diseases: skin lesions, prostate\ncancer, and retinal diseases. We also release a large re-engineered database\nthat consists of annotations from more than ten ophthalmologists with an\nunbiased golden standard dataset for evaluation and benchmarking.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 14:56:45 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 01:49:59 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ju", "Lie", ""], ["Wang", "Xin", ""], ["Wang", "Lin", ""], ["Mahapatra", "Dwarikanath", ""], ["Zhao", "Xin", ""], ["Harandi", "Mehrtash", ""], ["Drummond", "Tom", ""], ["Liu", "Tongliang", ""], ["Ge", "Zongyuan", ""]]}, {"id": "2103.00545", "submitter": "Takato Yasuno", "authors": "Takato Yasuno, Hiroaki Sugawara, Junichiro Fujii, Ryuto Yoshida", "title": "Snowy Night-to-Day Translator and Semantic Segmentation Label Similarity\n  for Snow Hazard Indicator", "comments": "9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2101.05616", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2021, Japan recorded more than three times as much snowfall as usual, so\nroad user maybe come across dangerous situation. The poor visibility caused by\nsnow triggers traffic accidents. For example, 2021 January 19, due to the dry\nsnow and the strong wind speed of 27 m / s, blizzards occurred and the outlook\nhas been ineffective. Because of the whiteout phenomenon, multiple accidents\nwith 17 casualties occurred, and 134 vehicles were stacked up for 10 hours over\n1 km. At the night time zone, the temperature drops and the road surface tends\nto freeze. CCTV images on the road surface have the advantage that we enable to\nmonitor the status of major points at the same time. Road managers are required\nto make decisions on road closures and snow removal work owing to the road\nsurface conditions even at night. In parallel, they would provide road users to\nalert for hazardous road surfaces. This paper propose a method to automate a\nsnow hazard indicator that the road surface region is generated from the night\nsnow image using the Conditional GAN, pix2pix. In addition, the road surface\nand the snow covered ROI are predicted using the semantic segmentation\nDeepLabv3+ with a backbone MobileNet, and the snow hazard indicator to\nautomatically compute how much the night road surface is covered with snow. We\ndemonstrate several results applied to the cold and snow region in the winter\nof Japan January 19 to 21 2021, and mention the usefulness of high similarity\nbetween snowy night-to-day fake output and real snowy day image for night snow\nvisibility.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 16:08:07 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yasuno", "Takato", ""], ["Sugawara", "Hiroaki", ""], ["Fujii", "Junichiro", ""], ["Yoshida", "Ryuto", ""]]}, {"id": "2103.00560", "submitter": "Alexander Mathis", "authors": "Maxime Vidal and Nathan Wolf and Beth Rosenberg and Bradley P. Harris\n  and Alexander Mathis", "title": "Perspectives on individual animal identification from biology and\n  computer vision", "comments": "12 pages, 1 figure, 2 boxes and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying individual animals is crucial for many biological investigations.\nIn response to some of the limitations of current identification methods, new\nautomated computer vision approaches have emerged with strong performance.\nHere, we review current advances of computer vision identification techniques\nto provide both computer scientists and biologists with an overview of the\navailable tools and discuss their applications. We conclude by offering\nrecommendations for starting an animal identification project, illustrate\ncurrent limitations and propose how they might be addressed in the future.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 16:50:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Vidal", "Maxime", ""], ["Wolf", "Nathan", ""], ["Rosenberg", "Beth", ""], ["Harris", "Bradley P.", ""], ["Mathis", "Alexander", ""]]}, {"id": "2103.00586", "submitter": "Luca Sestini", "authors": "Luca Sestini, Benoit Rosa, Elena De Momi, Giancarlo Ferrigno and\n  Nicolas Padoy", "title": "A Kinematic Bottleneck Approach For Pose Regression of Flexible Surgical\n  Instruments directly from Images", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2021.3062308", "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  3-D pose estimation of instruments is a crucial step towards automatic scene\nunderstanding in robotic minimally invasive surgery. Although robotic systems\ncan potentially directly provide joint values, this information is not commonly\nexploited inside the operating room, due to its possible unreliability, limited\naccess and the time-consuming calibration required, especially for continuum\nrobots. For this reason, standard approaches for 3-D pose estimation involve\nthe use of external tracking systems. Recently, image-based methods have\nemerged as promising, non-invasive alternatives. While many image-based\napproaches in the literature have shown accurate results, they generally\nrequire either a complex iterative optimization for each processed image,\nmaking them unsuitable for real-time applications, or a large number of\nmanually-annotated images for efficient learning. In this paper we propose a\nself-supervised image-based method, exploiting, at training time only, the\nimprecise kinematic information provided by the robot. In order to avoid\nintroducing time-consuming manual annotations, the problem is formulated as an\nauto-encoder, smartly bottlenecked by the presence of a physical model of the\nrobotic instruments and surgical camera, forcing a separation between image\nbackground and kinematic content. Validation of the method was performed on\nsemi-synthetic, phantom and in-vivo datasets, obtained using a flexible\nrobotized endoscope, showing promising results for real-time image-based 3-D\npose estimation of surgical instruments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 18:41:18 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sestini", "Luca", ""], ["Rosa", "Benoit", ""], ["De Momi", "Elena", ""], ["Ferrigno", "Giancarlo", ""], ["Padoy", "Nicolas", ""]]}, {"id": "2103.00652", "submitter": "Zhikang Zhang", "authors": "Jonathan Zhao, Matthew Westerham, Mark Lakatos-Toth, Zhikang Zhang,\n  Avi Moskoff, Fengbo Ren", "title": "OpenICS: Open Image Compressive Sensing Toolbox and Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present OpenICS, an image compressive sensing toolbox that includes\nmultiple image compressive sensing and reconstruction algorithms proposed in\nthe past decade. Due to the lack of standardization in the implementation and\nevaluation of the proposed algorithms, the application of image compressive\nsensing in the real-world is limited. We believe this toolbox is the first\nframework that provides a unified and standardized implementation of multiple\nimage compressive sensing algorithms. In addition, we also conduct a\nbenchmarking study on the methods included in this framework from two aspects:\nreconstruction accuracy and reconstruction efficiency. We wish this toolbox and\nbenchmark can serve the growing research community of compressive sensing and\nthe industry applying image compressive sensing to new problems as well as\ndeveloping new methods more efficiently. Code and models are available at\nhttps://github.com/PSCLab-ASU/OpenICS. The project is still under maintenance,\nand we will keep this document updated.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 22:53:40 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 23:37:25 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 01:43:08 GMT"}, {"version": "v4", "created": "Fri, 7 May 2021 00:12:58 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhao", "Jonathan", ""], ["Westerham", "Matthew", ""], ["Lakatos-Toth", "Mark", ""], ["Zhang", "Zhikang", ""], ["Moskoff", "Avi", ""], ["Ren", "Fengbo", ""]]}, {"id": "2103.00657", "submitter": "Eric Pryzant", "authors": "E. Pryzant, Q. Deng, B. Mei, E. Shrestha", "title": "Achieving Competitive Play Through Bottom-Up Approach in Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the renaissance of neural networks, object detection has slowly shifted\nfrom a bottom-up recognition problem to a top-down approach. Best in class\nalgorithms enumerate a near-complete list of objects and classify each into\nobject/not object. In this paper, we show that strong performance can still be\nachieved using a bottom-up approach for vision-based object recognition tasks\nand achieve competitive video game play. We propose PuckNet, which is used to\ndetect four extreme points (top, left, bottom, and right-most points) and one\ncenter point of objects using a fully convolutional neural network. Object\ndetection is then a purely keypoint-based appearance estimation problem,\nwithout implicit feature learning or region classification. The method proposed\nherein performs on-par with the best in class region-based detection methods,\nwith a bounding box AP of 36.4% on COCO test-dev. In addition, the extreme\npoints estimated directly resolve into a rectangular object mask, with a COCO\nMask AP of 17.6%, outperforming the Mask AP of vanilla bounding boxes. Guided\nsegmentation of extreme points further improves this to 32.1% Mask AP. We\napplied the PuckNet vision system to the SuperTuxKart video game to test it's\ncapacity to achieve competitive play in dynamic and co-operative multiplayer\nenvironments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 23:14:13 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Pryzant", "E.", ""], ["Deng", "Q.", ""], ["Mei", "B.", ""], ["Shrestha", "E.", ""]]}, {"id": "2103.00658", "submitter": "Aasma Aslam", "authors": "Aasma Aslam, Babar Hussian", "title": "Emotion recognition techniques with rule based and machine learning\n  approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion recognition using digital image processing is a multifarious task\nbecause facial emotions depend on warped facial features as well as on gender,\nage, and culture. Furthermore, there are several factors such as varied\nillumination and intricate settings that increase complexity in facial emotion\nrecognition. In this paper, we used four salient facial features, Eyebrows,\nMouth opening, Mouth corners, and Forehead wrinkles to identifying emotions\nfrom normal, occluded and partially-occluded images. We have employed\nrule-based approach and developed new methods to extract aforementioned facial\nfeatures similar to local bit patterns using novel techniques. We propose new\nmethods to detect eye location, eyebrow contraction, and mouth corners. For eye\ndetection, the proposed methods are Enhancement of Cr Red (ECrR) and\nSuppression of Cr Blue (SCrB) which results in 98% accuracy. Additionally, for\neyebrow contraction detection, we propose two techniques (1) Morphological\nGradient Image Intensity (MGII) and (2) Degree of Curvature Line (DCL).\nAdditionally, we present a new method for mouth corners detection. For\nclassification purpose, we use an individual classifier, majority voting (MV)\nand weighted majority voting (WMV) methods which mimic Human Emotions\nSensitivity (HES). These methods are straightforward to implement, improve the\naccuracy of results, and work best for emotion recognition using partially\noccluded images. It is ascertained from the results that our method outperforms\nprevious approaches. Overall accuracy rates are around 94%. The processing time\non one image using processor core i5 is ~0.12 sec.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 23:21:27 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Aslam", "Aasma", ""], ["Hussian", "Babar", ""]]}, {"id": "2103.00663", "submitter": "Henry Xu", "authors": "Henry Xu, An Ju, David Wagner", "title": "Model-Agnostic Defense for Lane Detection against Adversarial Attack", "comments": "6 pages, 6 figures, 3 tables. Part of AutoSec 2021 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Susceptibility of neural networks to adversarial attack prompts serious\nsafety concerns for lane detection efforts, a domain where such models have\nbeen widely applied. Recent work on adversarial road patches have successfully\ninduced perception of lane lines with arbitrary form, presenting an avenue for\nrogue control of vehicle behavior. In this paper, we propose a modular lane\nverification system that can catch such threats before the autonomous driving\nsystem is misled while remaining agnostic to the particular lane detection\nmodel. Our experiments show that implementing the system with a simple\nconvolutional neural network (CNN) can defend against a wide gamut of attacks\non lane detection models. With a 10% impact to inference time, we can detect\n96% of bounded non-adaptive attacks, 90% of bounded adaptive attacks, and 98%\nof patch attacks while preserving accurate identification at least 95% of true\nlanes, indicating that our proposed verification system is effective at\nmitigating lane detection security risks with minimal overhead.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 00:05:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Henry", ""], ["Ju", "An", ""], ["Wagner", "David", ""]]}, {"id": "2103.00673", "submitter": "Qing Qu", "authors": "Sheng Liu, Xiao Li, Yuexiang Zhai, Chong You, Zhihui Zhu, Carlos\n  Fernandez-Granda, and Qing Qu", "title": "Convolutional Normalization: Improving Deep Convolutional Network\n  Robustness and Training", "comments": "SL and XL contributed equally to this work; 23 pages, 6 figures, 6\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normalization techniques have become a basic component in modern\nconvolutional neural networks (ConvNets). In particular, many recent works\ndemonstrate that promoting the orthogonality of the weights helps train deep\nmodels and improve robustness. For ConvNets, most existing methods are based on\npenalizing or normalizing weight matrices derived from concatenating or\nflattening the convolutional kernels. These methods often destroy or ignore the\nbenign convolutional structure of the kernels; therefore, they are often\nexpensive or impractical for deep ConvNets. In contrast, we introduce a simple\nand efficient ``convolutional normalization'' method that can fully exploit the\nconvolutional structure in the Fourier domain and serve as a simple\nplug-and-play module to be conveniently incorporated into any ConvNets. Our\nmethod is inspired by recent work on preconditioning methods for convolutional\nsparse coding and can effectively promote each layer's channel-wise isometry.\nFurthermore, we show that convolutional normalization can reduce the layerwise\nspectral norm of the weight matrices and hence improve the Lipschitzness of the\nnetwork, leading to easier training and improved robustness for deep ConvNets.\nApplied to classification under noise corruptions and generative adversarial\nnetwork (GAN), we show that convolutional normalization improves the robustness\nof common ConvNets such as ResNet and the performance of GAN. We verify our\nfindings via extensive numerical experiments on CIFAR-10, CIFAR-100, and\nImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 00:33:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Sheng", ""], ["Li", "Xiao", ""], ["Zhai", "Yuexiang", ""], ["You", "Chong", ""], ["Zhu", "Zhihui", ""], ["Fernandez-Granda", "Carlos", ""], ["Qu", "Qing", ""]]}, {"id": "2103.00738", "submitter": "Aoran Xiao", "authors": "Aoran Xiao, Xiaofei Yang, Shijian Lu, Dayan Guan and Jiaxing Huang", "title": "FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point\n  Cloud Segmentation", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2021.04.011", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scene understanding based on LiDAR point cloud is an essential task for\nautonomous cars to drive safely, which often employs spherical projection to\nmap 3D point cloud into multi-channel 2D images for semantic segmentation. Most\nexisting methods simply stack different point attributes/modalities (e.g.\ncoordinates, intensity, depth, etc.) as image channels to increase information\ncapacity, but ignore distinct characteristics of point attributes in different\nimage channels. We design FPS-Net, a convolutional fusion network that exploits\nthe uniqueness and discrepancy among the projected image channels for optimal\npoint cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead\nof simply stacking multiple channel images as a single input, we group them\ninto different modalities to first learn modality-specific features separately\nand then map the learned features into a common high-dimensional feature space\nfor pixel-level fusion and learning. Specifically, we design a residual dense\nblock with multiple receptive fields as a building block in the encoder which\npreserves detailed information in each modality and learns hierarchical\nmodality-specific and fused features effectively. In the FPS-Net decoder, we\nuse a recurrent convolution block likewise to hierarchically decode fused\nfeatures into output space for pixel-level classification. Extensive\nexperiments conducted on two widely adopted point cloud datasets show that\nFPS-Net achieves superior semantic segmentation as compared with\nstate-of-the-art projection-based methods. In addition, the proposed modality\nfusion idea is compatible with typical projection-based methods and can be\nincorporated into them with consistent performance improvements.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 04:08:28 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Xiao", "Aoran", ""], ["Yang", "Xiaofei", ""], ["Lu", "Shijian", ""], ["Guan", "Dayan", ""], ["Huang", "Jiaxing", ""]]}, {"id": "2103.00760", "submitter": "Ukcheol Shin", "authors": "Ukcheol Shin, Kyunghyun Lee, Seokju Lee, In So Kweon", "title": "Unsupervised Depth and Ego-motion Estimation for Monocular Thermal Video\n  using Multi-spectral Consistency Loss", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": "10 pages", "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of the deep-learning based depth and ego-motion networks have been\ndesigned for visible cameras. However, visible cameras heavily rely on the\npresence of an external light source. Therefore, it is challenging to use them\nunder low-light conditions such as night scenes, tunnels, and other harsh\nconditions. A thermal camera is one solution to compensate for this problem\nbecause it detects Long Wave Infrared Radiation(LWIR) regardless of any\nexternal light sources. However, despite this advantage, both depth and\nego-motion estimation research for the thermal camera are not actively explored\nuntil so far. In this paper, we propose an unsupervised learning method for the\nall-day depth and ego-motion estimation. The proposed method exploits\nmulti-spectral consistency loss to gives complementary supervision for the\nnetworks by reconstructing visible and thermal images with the depth and pose\nestimated from thermal images. The networks trained with the proposed method\nrobustly estimate the depth and pose from monocular thermal video under\nlow-light and even zero-light conditions. To the best of our knowledge, this is\nthe first work to simultaneously estimate both depth and ego-motion from the\nmonocular thermal video in an unsupervised manner.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 05:29:04 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 02:05:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Shin", "Ukcheol", ""], ["Lee", "Kyunghyun", ""], ["Lee", "Seokju", ""], ["Kweon", "In So", ""]]}, {"id": "2103.00762", "submitter": "Fanbo Xiang", "authors": "Fanbo Xiang, Zexiang Xu, Milo\\v{s} Ha\\v{s}an, Yannick Hold-Geoffroy,\n  Kalyan Sunkavalli, Hao Su", "title": "NeuTex: Neural Texture Mapping for Volumetric Neural Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated that volumetric scene representations combined\nwith differentiable volume rendering can enable photo-realistic rendering for\nchallenging scenes that mesh reconstruction fails on. However, these methods\nentangle geometry and appearance in a \"black-box\" volume that cannot be edited.\nInstead, we present an approach that explicitly disentangles\ngeometry--represented as a continuous 3D volume--from appearance--represented\nas a continuous 2D texture map. We achieve this by introducing a 3D-to-2D\ntexture mapping (or surface parameterization) network into volumetric\nrepresentations. We constrain this texture mapping network using an additional\n2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D\nsurface points map to 2D texture points that map back to the original 3D\npoints. We demonstrate that this representation can be reconstructed using only\nmulti-view image supervision and generates high-quality rendering results. More\nimportantly, by separating geometry and texture, we allow users to edit\nappearance by simply editing 2D texture maps.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 05:34:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xiang", "Fanbo", ""], ["Xu", "Zexiang", ""], ["Ha\u0161an", "Milo\u0161", ""], ["Hold-Geoffroy", "Yannick", ""], ["Sunkavalli", "Kalyan", ""], ["Su", "Hao", ""]]}, {"id": "2103.00776", "submitter": "Tianyang Shi", "authors": "Yinglin Duan (1), Tianyang Shi (1), Zhengxia Zou (2), Yenan Lin (3),\n  Zhehui Qian (3), Bohan Zhang (3), Yi Yuan (1) ((1) NetEase Fuxi AI Lab, (2)\n  University of Michigan, (3) NetEase)", "title": "Single-Shot Motion Completion with Transformer", "comments": "10 pages, 6 figures. Project page: https://github.com/FuxiCV/SSMCT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion completion is a challenging and long-discussed problem, which is of\ngreat significance in film and game applications. For different motion\ncompletion scenarios (in-betweening, in-filling, and blending), most previous\nmethods deal with the completion problems with case-by-case designs. In this\nwork, we propose a simple but effective method to solve multiple motion\ncompletion problems under a unified framework and achieves a new state of the\nart accuracy under multiple evaluation settings. Inspired by the recent great\nsuccess of attention-based models, we consider the completion as a sequence to\nsequence prediction problem. Our method consists of two modules - a standard\ntransformer encoder with self-attention that learns long-range dependencies of\ninput motions, and a trainable mixture embedding module that models temporal\ninformation and discriminates key-frames. Our method can run in a\nnon-autoregressive manner and predict multiple missing frames within a single\nforward propagation in real time. We finally show the effectiveness of our\nmethod in music-dance applications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:00:17 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Duan", "Yinglin", ""], ["Shi", "Tianyang", ""], ["Zou", "Zhengxia", ""], ["Lin", "Yenan", ""], ["Qian", "Zhehui", ""], ["Zhang", "Bohan", ""], ["Yuan", "Yi", ""]]}, {"id": "2103.00780", "submitter": "Tong Zhang PhD", "authors": "Yang Yang, Jiancong Chen, Ruixuan Wang, Ting Ma, Lingwei Wang, Jie\n  Chen, Wei-Shi Zheng, Tong Zhang", "title": "Towards Unbiased COVID-19 Lesion Localisation and Segmentation via\n  Weakly Supervised Learning", "comments": "accepted by ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite tremendous efforts, it is very challenging to generate a robust model\nto assist in the accurate quantification assessment of COVID-19 on chest CT\nimages. Due to the nature of blurred boundaries, the supervised segmentation\nmethods usually suffer from annotation biases. To support unbiased lesion\nlocalisation and to minimise the labeling costs, we propose a data-driven\nframework supervised by only image-level labels. The framework can explicitly\nseparate potential lesions from original images, with the help of a generative\nadversarial network and a lesion-specific decoder. Experiments on two COVID-19\ndatasets demonstrate the effectiveness of the proposed framework and its\nsuperior performance to several existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:05:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yang", "Yang", ""], ["Chen", "Jiancong", ""], ["Wang", "Ruixuan", ""], ["Ma", "Ting", ""], ["Wang", "Lingwei", ""], ["Chen", "Jie", ""], ["Zheng", "Wei-Shi", ""], ["Zhang", "Tong", ""]]}, {"id": "2103.00783", "submitter": "Mu Hu", "authors": "Mu Hu, Shuling Wang, Bin Li, Shiyu Ning, Li Fan, and Xiaojin Gong", "title": "PENet: Towards Precise and Efficient Image Guided Depth Completion", "comments": "Accepted by ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image guided depth completion is the task of generating a dense depth map\nfrom a sparse depth map and a high quality image. In this task, how to fuse the\ncolor and depth modalities plays an important role in achieving good\nperformance. This paper proposes a two-branch backbone that consists of a\ncolor-dominant branch and a depth-dominant branch to exploit and fuse two\nmodalities thoroughly. More specifically, one branch inputs a color image and a\nsparse depth map to predict a dense depth map. The other branch takes as inputs\nthe sparse depth map and the previously predicted depth map, and outputs a\ndense depth map as well. The depth maps predicted from two branches are\ncomplimentary to each other and therefore they are adaptively fused. In\naddition, we also propose a simple geometric convolutional layer to encode 3D\ngeometric cues. The geometric encoded backbone conducts the fusion of different\nmodalities at multiple stages, leading to good depth completion results. We\nfurther implement a dilated and accelerated CSPN++ to refine the fused depth\nmap efficiently. The proposed full model ranks 1st in the KITTI depth\ncompletion online leaderboard at the time of submission. It also infers much\nfaster than most of the top ranked methods. The code of this work is available\nat https://github.com/JUGGHM/PENet_ICRA2021.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:09:23 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 02:27:07 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 14:16:27 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hu", "Mu", ""], ["Wang", "Shuling", ""], ["Li", "Bin", ""], ["Ning", "Shiyu", ""], ["Fan", "Li", ""], ["Gong", "Xiaojin", ""]]}, {"id": "2103.00785", "submitter": "Chuhui Xue", "authors": "Chuhui Xue, Shijian Lu, Steven Hoi", "title": "Detection and Rectification of Arbitrary Shaped Scene Texts by using\n  Text Keypoints and Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and recognition of scene texts of arbitrary shapes remain a grand\nchallenge due to the super-rich text shape variation in text line orientations,\nlengths, curvatures, etc. This paper presents a mask-guided multi-task network\nthat detects and rectifies scene texts of arbitrary shapes reliably. Three\ntypes of keypoints are detected which specify the centre line and so the shape\nof text instances accurately. In addition, four types of keypoint links are\ndetected of which the horizontal links associate the detected keypoints of each\ntext instance and the vertical links predict a pair of landmark points (for\neach keypoint) along the upper and lower text boundary, respectively. Scene\ntexts can be located and rectified by linking up the associated landmark points\n(giving localization polygon boxes) and transforming the polygon boxes via thin\nplate spline, respectively. Extensive experiments over several public datasets\nshow that the use of text keypoints is tolerant to the variation in text\norientations, lengths, and curvatures, and it achieves superior scene text\ndetection and rectification performance as compared with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:13:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xue", "Chuhui", ""], ["Lu", "Shijian", ""], ["Hoi", "Steven", ""]]}, {"id": "2103.00787", "submitter": "Xiang Gao", "authors": "Xiang Gao, Wei Hu, Guo-Jun Qi", "title": "Self-Supervised Multi-View Learning via Auto-Encoding 3D Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object representation learning is a fundamental challenge in computer\nvision to infer about the 3D world. Recent advances in deep learning have shown\ntheir efficiency in 3D object recognition, among which view-based methods have\nperformed best so far. However, feature learning of multiple views in existing\nmethods is mostly performed in a supervised fashion, which often requires a\nlarge amount of data labels with high costs. In contrast, self-supervised\nlearning aims to learn multi-view feature representations without involving\nlabeled data. To this end, we propose a novel self-supervised paradigm to learn\nMulti-View Transformation Equivariant Representations (MV-TER), exploring the\nequivariant transformations of a 3D object and its projected multiple views.\nSpecifically, we perform a 3D transformation on a 3D object, and obtain\nmultiple views before and after the transformation via projection. Then, we\nself-train a representation to capture the intrinsic 3D object representation\nby decoding 3D transformation parameters from the fused feature representations\nof multiple views before and after the transformation. Experimental results\ndemonstrate that the proposed MV-TER significantly outperforms the\nstate-of-the-art view-based approaches in 3D object classification and\nretrieval tasks, and show the generalization to real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:24:17 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gao", "Xiang", ""], ["Hu", "Wei", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "2103.00793", "submitter": "Shuchang Lyu", "authors": "Qi Zhao, Shuchang Lyu, Zhiwei Zhang, Ting-Bing Xu and Guangliang Cheng", "title": "Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real applications, different computation-resource devices need\ndifferent-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,\nexisting methods either design multiple networks and train them independently,\nor construct depth-level/width-level dynamic neural networks which is hard to\nprove the accuracy of each sub-net. In this article, we propose an elegant\nDepth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets\nof similar architectures. To improve the generalization of sub-nets, we design\nthe Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to\nimplement knowledge transfer from the teacher (full-net) to multiple students\n(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to\nconstrain the posterior class probability consistency between full-net and\nsub-nets, and self-attention distillation on the same resolution feature of\ndifferent depth is addressed to drive more abundant feature representations of\nsub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in\na DDNN via the online knowledge distillation in each training iteration without\nextra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet\ndatasets demonstrate that sub-nets in DDNN with EKD training achieve better\nperformance than individually training networks while preserving the original\nperformance of full-nets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:35:31 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 09:49:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhao", "Qi", ""], ["Lyu", "Shuchang", ""], ["Zhang", "Zhiwei", ""], ["Xu", "Ting-Bing", ""], ["Cheng", "Guangliang", ""]]}, {"id": "2103.00794", "submitter": "Haoran You", "authors": "Haoran You, Zhihan Lu, Zijian Zhou, Yingyan Lin", "title": "GEBT: Drawing Early-Bird Tickets in Graph Convolutional Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art deep\nlearning model for representation learning on graphs. However, it remains\nnotoriously challenging to train and inference GCNs over large graph datasets,\nlimiting their application to large real-world graphs and hindering the\nexploration of deeper and more sophisticated GCN graphs. This is because as the\ngraph size grows, the sheer number of node features and the large adjacency\nmatrix can easily explode the required memory and data movements. To tackle the\naforementioned challenge, we explore the possibility of drawing lottery tickets\nwhen sparsifying GCN graphs, i.e., subgraphs that largely shrink the adjacency\nmatrix yet are capable of achieving accuracy comparable to or even better than\ntheir corresponding full graphs. Specifically, we for the first time discover\nthe existence of graph early-bird (GEB) tickets that emerge at the very early\nstage when sparsifying GCN graphs, and propose a simple yet effective detector\nto automatically identify the emergence of such GEB tickets. Furthermore, we\ndevelop a generic efficient GCN training framework dubbed GEBT that can\nsignificantly boost the efficiency of GCN training by (1) drawing joint\nearly-bird tickets between the GCN graphs and models and (2) enabling\nsimultaneously sparsifying both GCN graphs and models, paving the way for\ntraining and inferencing large GCN graphs to handle real-world graph datasets.\nExperiments on various GCN models and datasets consistently validate our GEB\nfinding and the effectiveness of our GEBT, e.g., our GEBT achieves up to 80.2%\n~ 85.6% and 84.6% ~ 87.5% savings of GCN training and inference costs while\nleading to a comparable or even better accuracy as compared to state-of-the-art\nmethods. Code available at https://github.com/RICE-EIC/GEBT\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:36:24 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["You", "Haoran", ""], ["Lu", "Zhihan", ""], ["Zhou", "Zijian", ""], ["Lin", "Yingyan", ""]]}, {"id": "2103.00801", "submitter": "Tao Yang", "authors": "He Zhang, Zhixiong Nan, Tao Yang, Yifan Liu and Nanning Zheng", "title": "A Driving Behavior Recognition Model with Bi-LSTM and Multi-Scale CNN", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous driving, perceiving the driving behaviors of surrounding agents\nis important for the ego-vehicle to make a reasonable decision. In this paper,\nwe propose a neural network model based on trajectories information for driving\nbehavior recognition. Unlike existing trajectory-based methods that recognize\nthe driving behavior using the hand-crafted features or directly encoding the\ntrajectory, our model involves a Multi-Scale Convolutional Neural Network\n(MSCNN) module to automatically extract the high-level features which are\nsupposed to encode the rich spatial and temporal information. Given a\ntrajectory sequence of an agent as the input, firstly, the Bi-directional Long\nShort Term Memory (Bi-LSTM) module and the MSCNN module respectively process\nthe input, generating two features, and then the two features are fused to\nclassify the behavior of the agent. We evaluate the proposed model on the\npublic BLVD dataset, achieving a satisfying performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:47:29 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhang", "He", ""], ["Nan", "Zhixiong", ""], ["Yang", "Tao", ""], ["Liu", "Yifan", ""], ["Zheng", "Nanning", ""]]}, {"id": "2103.00803", "submitter": "Je Hyeong Hong", "authors": "Je Hyeong Hong, Hanjo Kim, Minsoo Kim, Gi Pyo Nam, Junghyun Cho,\n  Hyeong-Seok Ko, Ig-Jae Kim", "title": "A 3D model-based approach for fitting masks to faces in the wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition research now requires a large number of labelled masked face\nimages in the era of this unprecedented COVID-19 pandemic. Unfortunately, the\nrapid spread of the virus has left us little time to prepare for such dataset\nin the wild. To circumvent this issue, we present a 3D model-based approach\ncalled WearMask3D for augmenting face images of various poses to the masked\nface counterparts. Our method proceeds by first fitting a 3D morphable model on\nthe input image, second overlaying the mask surface onto the face model and\nwarping the respective mask texture, and last projecting the 3D mask back to\n2D. The mask texture is adapted based on the brightness and resolution of the\ninput image. By working in 3D, our method can produce more natural masked faces\nof diverse poses from a single mask texture. To compare precisely between\ndifferent augmentation approaches, we have constructed a dataset comprising\nmasked and unmasked faces with labels called MFW-mini. Experimental results\ndemonstrate WearMask3D, which will be made publicly available, produces more\nrealistic masked images, and utilizing these images for training leads to\nimproved recognition accuracy of masked faces compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:50:18 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hong", "Je Hyeong", ""], ["Kim", "Hanjo", ""], ["Kim", "Minsoo", ""], ["Nam", "Gi Pyo", ""], ["Cho", "Junghyun", ""], ["Ko", "Hyeong-Seok", ""], ["Kim", "Ig-Jae", ""]]}, {"id": "2103.00806", "submitter": "Sai Vemprala", "authors": "Sai Vemprala, Sami Mian, Ashish Kapoor", "title": "Representation Learning for Event-based Visuomotor Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras are dynamic vision sensors that can provide asynchronous\nmeasurements of changes in per-pixel brightness at a microsecond level. This\nmakes them significantly faster than conventional frame-based cameras, and an\nappealing choice for high-speed navigation. While an interesting sensor\nmodality, this asynchronous data poses a challenge for common machine learning\ntechniques. In this paper, we present an event variational autoencoder for\nunsupervised representation learning from asynchronous event camera data. We\nshow that it is feasible to learn compact representations from spatiotemporal\nevent data to encode the context. Furthermore, we show that such pretrained\nrepresentations can be beneficial for navigation, allowing for usage in\nreinforcement learning instead of end-to-end reward driven perception. We\nvalidate this framework of learning visuomotor policies by applying it to an\nobstacle avoidance scenario in simulation. We show that representations learnt\nfrom event data enable training fast control policies that can adapt to\ndifferent control capacities, and demonstrate a higher degree of robustness\nthan end-to-end learning from event images.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:04:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Vemprala", "Sai", ""], ["Mian", "Sami", ""], ["Kapoor", "Ashish", ""]]}, {"id": "2103.00809", "submitter": "Renshuai Tao", "authors": "Renshuai Tao, Yanlu Wei, Hainan Li, Aishan Liu, Yifu Ding, Haotong Qin\n  and Xianglong Liu", "title": "Over-sampling De-occlusion Attention Network for Prohibited Items\n  Detection in Noisy X-ray Images", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Security inspection is X-ray scanning for personal belongings in suitcases,\nwhich is significantly important for the public security but highly\ntime-consuming for human inspectors. Fortunately, deep learning has greatly\npromoted the development of computer vision, offering a possible way of\nautomatic security inspection. However, items within a luggage are randomly\noverlapped resulting in noisy X-ray images with heavy occlusions. Thus,\ntraditional CNN-based models trained through common image recognition datasets\nfail to achieve satisfactory performance in this scenario. To address these\nproblems, we contribute the first high-quality prohibited X-ray object\ndetection dataset named OPIXray, which contains 8885 X-ray images from 5\ncategories of the widely-occurred prohibited item ``cutters''. The images are\ngathered from an airport and these prohibited items are annotated manually by\nprofessional inspectors, which can be used as a benchmark for model training\nand further facilitate future research. To better improve occluded X-ray object\ndetection, we further propose an over-sampling de-occlusion attention network\n(DOAM-O), which consists of a novel de-occlusion attention module and a new\nover-sampling training strategy. Specifically, our de-occlusion module, namely\nDOAM, simultaneously leverages the different appearance information of the\nprohibited items; the over-sampling training strategy forces the model to put\nmore emphasis on these hard samples consisting these items of high occlusion\nlevels, which is more suitable for this scenario. We comprehensively evaluated\nDOAM-O on the OPIXray dataset, which proves that our model can stably improve\nthe performance of the famous detection models such as SSD, YOLOv3, and FCOS,\nand outperform many extensively-used attention mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:17:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tao", "Renshuai", ""], ["Wei", "Yanlu", ""], ["Li", "Hainan", ""], ["Liu", "Aishan", ""], ["Ding", "Yifu", ""], ["Qin", "Haotong", ""], ["Liu", "Xianglong", ""]]}, {"id": "2103.00810", "submitter": "Zhenxi Li", "authors": "Zhenxi Li, Guillaume-Alexandre Bilodeau, Wassim Bouachir", "title": "MFST: Multi-Features Siamese Tracker", "comments": "ICPR 2021, Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese trackers have recently achieved interesting results due to their\nbalance between accuracy and speed. This success is mainly due to the fact that\ndeep similarity networks were specifically designed to address the image\nsimilarity problem. Therefore, they are inherently more appropriate than\nclassical CNNs for the tracking task. However, Siamese trackers rely on the\nlast convolutional layers for similarity analysis and target search, which\nrestricts their performance. In this paper, we argue that using a single\nconvolutional layer as feature representation is not the optimal choice within\nthe deep similarity framework, as multiple convolutional layers provide several\nabstraction levels in characterizing an object. Starting from this motivation,\nwe present the Multi-Features Siamese Tracker (MFST), a novel tracking\nalgorithm exploiting several hierarchical feature maps for robust deep\nsimilarity tracking. MFST proceeds by fusing hierarchical features to ensure a\nricher and more efficient representation. Moreover, we handle appearance\nvariation by calibrating deep features extracted from two different CNN models.\nBased on this advanced feature representation, our algorithm achieves high\ntracking accuracy, while outperforming several state-of-the-art trackers,\nincluding standard Siamese trackers. The code and trained models are available\nat https://github.com/zhenxili96/MFST.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:18:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Zhenxi", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Bouachir", "Wassim", ""]]}, {"id": "2103.00813", "submitter": "Yi Wei", "authors": "Yi Wei, Xue Mei, Xin Liu, Pengxiang Xu", "title": "DST: Data Selection and joint Training for Learning with Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep neural network heavily relies on a large amount of training\ndata with accurate annotations. To alleviate this problem, various methods have\nbeen proposed to annotate the data automatically. However, automatically\ngenerating annotations will inevitably yields noisy labels. In this paper, we\npropose a Data Selection and joint Training (DST) method to automatically\nselect training samples with accurate annotations. Specifically, DST fits a\nmixture model according to the original annotation as well as the predicted\nlabel for each training sample, and the mixture model is utilized to\ndynamically divide the training dataset into a correctly labeled dataset, a\ncorrectly predicted set and a wrong dataset. Then, DST is trained with these\ndatasets in a supervised manner. Due to confirmation bias problem, we train the\ntwo networks alternately, and each network is tasked to establish the data\ndivision to teach another network. For each iteration, the correctly labeled\nand predicted labels are reweighted respectively by the probabilities from the\nmixture model, and a uniform distribution is used to generate the probabilities\nof the wrong samples. Experiments on CIFAR-10, CIFAR-100 and Clothing1M\ndemonstrate that DST is the comparable or superior to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:23:58 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wei", "Yi", ""], ["Mei", "Xue", ""], ["Liu", "Xin", ""], ["Xu", "Pengxiang", ""]]}, {"id": "2103.00820", "submitter": "Hung Le", "authors": "Hung Le, Nancy F. Chen, Steven C.H. Hoi", "title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded\n  Dialogues", "comments": "Accepted at ICLR (International Conference on Learning\n  Representations) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compared to traditional visual question answering, video-grounded dialogues\nrequire additional reasoning over dialogue context to answer questions in a\nmulti-turn setting. Previous approaches to video-grounded dialogues mostly use\ndialogue context as a simple text input without modelling the inherent\ninformation flows at the turn level. In this paper, we propose a novel\nframework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers\ninformation flows among dialogue turns through a semantic graph constructed\nbased on lexical components in each question and answer. PDC model then learns\nto predict reasoning paths over this semantic graph. Our path prediction model\npredicts a path from the current turn through past dialogue turns that contain\nadditional visual cues to answer the current question. Our reasoning model\nsequentially processes both visual and textual information through this\nreasoning path and the propagated features are used to generate the answer. Our\nexperimental results demonstrate the effectiveness of our method and provide\nadditional insights on how models use semantic dependencies in a dialogue\ncontext to retrieve visual cues.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:39:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Le", "Hung", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2103.00832", "submitter": "Yu Zhang", "authors": "Yu Zhang and Xiaoguang Di and Bin Zhang and Qingyan Li and Shiyu Yan\n  and Chunhui Wang", "title": "Self-supervised Low Light Image Enhancement and Denoising", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a self-supervised low light image enhancement method\nbased on deep learning, which can improve the image contrast and reduce noise\nat the same time to avoid the blur caused by pre-/post-denoising. The method\ncontains two deep sub-networks, an Image Contrast Enhancement Network (ICE-Net)\nand a Re-Enhancement and Denoising Network (RED-Net). The ICE-Net takes the low\nlight image as input and produces a contrast enhanced image. The RED-Net takes\nthe result of ICE-Net and the low light image as input, and can re-enhance the\nlow light image and denoise at the same time. Both of the networks can be\ntrained with low light images only, which is achieved by a Maximum Entropy\nbased Retinex (ME-Retinex) model and an assumption that noises are\nindependently distributed. In the ME-Retinex model, a new constraint on the\nreflectance image is introduced that the maximum channel of the reflectance\nimage conforms to the maximum channel of the low light image and its entropy\nshould be the largest, which converts the decomposition of reflectance and\nillumination in Retinex model to a non-ill-conditioned problem and allows the\nICE-Net to be trained with a self-supervised way. The loss functions of RED-Net\nare carefully formulated to separate the noises and details during training,\nand they are based on the idea that, if noises are independently distributed,\nafter the processing of smoothing filters (\\eg mean filter), the gradient of\nthe noise part should be smaller than the gradient of the detail part. It can\nbe proved qualitatively and quantitatively through experiments that the\nproposed method is efficient.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:05:02 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhang", "Yu", ""], ["Di", "Xiaoguang", ""], ["Zhang", "Bin", ""], ["Li", "Qingyan", ""], ["Yan", "Shiyu", ""], ["Wang", "Chunhui", ""]]}, {"id": "2103.00841", "submitter": "Yixing Xu", "authors": "Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, Yunhe Wang", "title": "Learning Frequency Domain Approximation for Binary Neural Networks", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks (BNNs) represent original full-precision weights and\nactivations into 1-bit with sign function. Since the gradient of the\nconventional sign function is almost zero everywhere which cannot be used for\nback-propagation, several attempts have been proposed to alleviate the\noptimization difficulty by using approximate gradient. However, those\napproximations corrupt the main direction of de facto gradient. To this end, we\npropose to estimate the gradient of sign function in the Fourier frequency\ndomain using the combination of sine functions for training BNNs, namely\nfrequency domain approximation (FDA). The proposed approach does not affect the\nlow-frequency information of the original sign function which occupies most of\nthe overall energy, and high-frequency coefficients will be ignored to avoid\nthe huge computational overhead. In addition, we embed a noise adaptation\nmodule into the training phase to compensate the approximation error. The\nexperiments on several benchmark datasets and neural architectures illustrate\nthat the binary network learned using our method achieves the state-of-the-art\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:25:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Yixing", ""], ["Han", "Kai", ""], ["Xu", "Chang", ""], ["Tang", "Yehui", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "2103.00844", "submitter": "Alessandra Micheletti", "authors": "Rongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka\n  Desnica", "title": "Emotion pattern detection on facial videos using functional statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing scientific interest in automatically analysing and\nunderstanding human behavior, with particular reference to the evolution of\nfacial expressions and the recognition of the corresponding emotions. In this\npaper we propose a technique based on Functional ANOVA to extract significant\npatterns of face muscles movements, in order to identify the emotions expressed\nby actors in recorded videos. We determine if there are time-related\ndifferences on expressions among emotional groups by using a functional F-test.\nSuch results are the first step towards the construction of a reliable\nautomatic emotion recognition system\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:31:08 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ji", "Rongjiao", ""], ["Micheletti", "Alessandra", ""], ["Jerinkic", "Natasa Krklec", ""], ["Desnica", "Zoranka", ""]]}, {"id": "2103.00847", "submitter": "Shahroz Tariq", "authors": "Shahroz Tariq, Sowon Jeon, Simon S. Woo", "title": "Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web\n  APIs under Deepfake Impersonation Attack", "comments": "27 pages, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, significant advancements have been made in face recognition\ntechnologies using Deep Neural Networks. As a result, companies such as\nMicrosoft, Amazon, and Naver offer highly accurate commercial face recognition\nweb services for diverse applications to meet the end-user needs. Naturally,\nhowever, such technologies are threatened persistently, as virtually any\nindividual can quickly implement impersonation attacks. In particular, these\nattacks can be a significant threat for authentication and identification\nservices, which heavily rely on their underlying face recognition technologies'\naccuracy and robustness. Despite its gravity, the issue regarding deepfake\nabuse using commercial web APIs and their robustness has not yet been\nthoroughly investigated. This work provides a measurement study on the\nrobustness of black-box commercial face recognition APIs against Deepfake\nImpersonation (DI) attacks using celebrity recognition APIs as an example case\nstudy. We use five deepfake datasets, two of which are created by us and\nplanned to be released. More specifically, we measure attack performance based\non two scenarios (targeted and non-targeted) and further analyze the differing\nsystem behaviors using fidelity, confidence, and similarity metrics.\nAccordingly, we demonstrate how vulnerable face recognition technologies from\npopular companies are to DI attack, achieving maximum success rates of 78.0%\nand 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with\nany celebrity) attacks, respectively. Moreover, we propose practical defense\nstrategies to mitigate DI attacks, reducing the attack success rates to as low\nas 0% and 0.02% for targeted and non-targeted attacks, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:40:10 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 07:56:46 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Tariq", "Shahroz", ""], ["Jeon", "Sowon", ""], ["Woo", "Simon S.", ""]]}, {"id": "2103.00852", "submitter": "Aly Magassouba", "authors": "Aly Magassouba, Komei Sugiura, and Hisashi Kawai", "title": "CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double\n  Back-Translation for Vision-and-Language Navigation", "comments": "8 pages, 5 figures, 5 tables. Submitted to IEEE Robotics and\n  Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation guided by natural language instructions is particularly suitable\nfor Domestic Service Robots that interacts naturally with users. This task\ninvolves the prediction of a sequence of actions that leads to a specified\ndestination given a natural language navigation instruction. The task thus\nrequires the understanding of instructions, such as ``Walk out of the bathroom\nand wait on the stairs that are on the right''. The Visual and Language\nNavigation remains challenging, notably because it requires the exploration of\nthe environment and at the accurate following of a path specified by the\ninstructions to model the relationship between language and vision. To address\nthis, we propose the CrossMap Transformer network, which encodes the linguistic\nand visual features to sequentially generate a path. The CrossMap transformer\nis tied to a Transformer-based speaker that generates navigation instructions.\nThe two networks share common latent features, for mutual enhancement through a\ndouble back translation model: Generated paths are translated into instructions\nwhile generated instructions are translated into path The experimental results\nshow the benefits of our approach in terms of instruction understanding and\ninstruction generation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:03:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Magassouba", "Aly", ""], ["Sugiura", "Komei", ""], ["Kawai", "Hisashi", ""]]}, {"id": "2103.00853", "submitter": "Vinay Kaushik", "authors": "Vinay Kaushik, Kartik Jindgar and Brejesh Lall", "title": "ADAADepth: Adapting Data Augmentation and Attention for Self-Supervised\n  Monocular Depth Estimation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Self-supervised learning of depth has been a highly studied topic of research\nas it alleviates the requirement of having ground truth annotations for\npredicting depth. Depth is learnt as an intermediate solution to the task of\nview synthesis, utilising warped photometric consistency. Although it gives\ngood results when trained using stereo data, the predicted depth is still\nsensitive to noise, illumination changes and specular reflections. Also,\nocclusion can be tackled better by learning depth from a single camera. We\npropose ADAA, utilising depth augmentation as depth supervision for learning\naccurate and robust depth. We propose a relational self-attention module that\nlearns rich contextual features and further enhances depth results. We also\noptimize the auto-masking strategy across all losses by enforcing L1\nregularisation over mask. Our novel progressive training strategy first learns\ndepth at a lower resolution and then progresses to the original resolution with\nslight training. We utilise a ResNet18 encoder, learning features for\nprediction of both depth and pose. We evaluate our predicted depth on the\nstandard KITTI driving dataset and achieve state-of-the-art results for\nmonocular depth estimation whilst having significantly lower number of\ntrainable parameters in our deep learning framework. We also evaluate our model\non Make3D dataset showing better generalization than other methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:06:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kaushik", "Vinay", ""], ["Jindgar", "Kartik", ""], ["Lall", "Brejesh", ""]]}, {"id": "2103.00860", "submitter": "Chongyi Li", "authors": "Chongyi Li and Chunle Guo and Chen Change Loy", "title": "Learning to Enhance Low-Light Image via Zero-Reference Deep Curve\n  Estimation", "comments": "This work is an extension of our earlier conference version\n  arXiv:2001.06826 (Zero-DCE) that has appeared in CVRP2020. This paper was\n  accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method, Zero-Reference Deep Curve Estimation\n(Zero-DCE), which formulates light enhancement as a task of image-specific\ncurve estimation with a deep network. Our method trains a lightweight deep\nnetwork, DCE-Net, to estimate pixel-wise and high-order curves for dynamic\nrange adjustment of a given image. The curve estimation is specially designed,\nconsidering pixel value range, monotonicity, and differentiability. Zero-DCE is\nappealing in its relaxed assumption on reference images, i.e., it does not\nrequire any paired or even unpaired data during training. This is achieved\nthrough a set of carefully formulated non-reference loss functions, which\nimplicitly measure the enhancement quality and drive the learning of the\nnetwork. Despite its simplicity, we show that it generalizes well to diverse\nlighting conditions. Our method is efficient as image enhancement can be\nachieved by an intuitive and simple nonlinear curve mapping. We further present\nan accelerated and light version of Zero-DCE, called Zero-DCE++, that takes\nadvantage of a tiny network with just 10K parameters. Zero-DCE++ has a fast\ninference speed (1000/11 FPS on a single GPU/CPU for an image of size\n1200*900*3) while keeping the enhancement performance of Zero-DCE. Extensive\nexperiments on various benchmarks demonstrate the advantages of our method over\nstate-of-the-art methods qualitatively and quantitatively. Furthermore, the\npotential benefits of our method to face detection in the dark are discussed.\nThe source code will be made publicly available at\nhttps://li-chongyi.github.io/Proj_Zero-DCE++.html.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:21:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Chongyi", ""], ["Guo", "Chunle", ""], ["Loy", "Chen Change", ""]]}, {"id": "2103.00868", "submitter": "Kailun Yang", "authors": "Alexander Jaus, Kailun Yang, Rainer Stiefelhagen", "title": "Panoramic Panoptic Segmentation: Towards Complete Surrounding\n  Understanding via Unsupervised Contrastive Learning", "comments": "7 pages, 4 figures, 2 tables. Accepted to 2021 IEEE Intelligent\n  Vehicles Symposium (IV2021). The project is at\n  https://github.com/alexanderjaus/PPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce panoramic panoptic segmentation as the most\nholistic scene understanding both in terms of field of view and image level\nunderstanding for standard camera based input. A complete surrounding\nunderstanding provides a maximum of information to the agent, which is\nessential for any intelligent vehicle in order to make informed decisions in a\nsafety-critical dynamic environment such as real-world traffic. In order to\novercome the lack of annotated panoramic images, we propose a framework which\nallows model training on standard pinhole images and transfers the learned\nfeatures to a different domain. Using our proposed method, we manage to achieve\nsignificant improvements of over 5% measured in PQ over non-adapted models on\nour Wild Panoramic Panoptic Segmentation (WildPPS) dataset. We show that our\nproposed Panoramic Robust Feature (PRF) framework is not only suitable to\nimprove performance on panoramic images but can be beneficial whenever model\ntraining and deployment are executed on data taken from different\ndistributions. As an additional contribution, we publish WildPPS: The first\npanoramic panoptic image dataset to foster progress in surrounding perception.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:37:27 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 10:37:56 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Jaus", "Alexander", ""], ["Yang", "Kailun", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2103.00869", "submitter": "David Williams", "authors": "David Williams, Matthew Gadd, Daniele De Martini and Paul Newman", "title": "Fool Me Once: Robust Selective Segmentation via Out-of-Distribution\n  Detection with Contrastive Learning", "comments": "Accepted for publication at the 2021 IEEE International Conference on\n  Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we train a network to simultaneously perform segmentation and\npixel-wise Out-of-Distribution (OoD) detection, such that the segmentation of\nunknown regions of scenes can be rejected. This is made possible by leveraging\nan OoD dataset with a novel contrastive objective and data augmentation scheme.\nBy combining data including unknown classes in the training data, a more robust\nfeature representation can be learned with known classes represented distinctly\nfrom those unknown. When presented with unknown classes or conditions, many\ncurrent approaches for segmentation frequently exhibit high confidence in their\ninaccurate segmentations and cannot be trusted in many operational\nenvironments. We validate our system on a real-world dataset of unusual driving\nscenes, and show that by selectively segmenting scenes based on what is\npredicted as OoD, we can increase the segmentation accuracy by an IoU of 0.2\nwith respect to alternative techniques.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:38:40 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Williams", "David", ""], ["Gadd", "Matthew", ""], ["De Martini", "Daniele", ""], ["Newman", "Paul", ""]]}, {"id": "2103.00871", "submitter": "Phong Tran", "authors": "Phong Tran, Anh Tran, Thao Nguyen, Minh Hoai", "title": "FineNet: Frame Interpolation and Enhancement for Face Video Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to deblur face videos. We propose a method that\ntackles this problem from two directions: (1) enhancing the blurry frames, and\n(2) treating the blurry frames as missing values and estimate them by\ninterpolation. These approaches are complementary to each other, and their\ncombination outperforms individual ones. We also introduce a novel module that\nleverages the structure of faces for finding positional offsets between video\nframes. This module can be integrated into the processing pipelines of both\napproaches, improving the quality of the final outcome. Experiments on three\nreal and synthetically generated blurry video datasets show that our method\noutperforms the previous state-of-the-art methods by a large margin in terms of\nboth quantitative and qualitative results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:47:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tran", "Phong", ""], ["Tran", "Anh", ""], ["Nguyen", "Thao", ""], ["Hoai", "Minh", ""]]}, {"id": "2103.00879", "submitter": "Kailun Yang", "authors": "Shuo Chen, Kailun Yang, Rainer Stiefelhagen", "title": "DR-TANet: Dynamic Receptive Temporal Attention Network for Street Scene\n  Change Detection", "comments": "8 pages, 9 figures, 6 tables. Accepted to IEEE Intelligent Vehicles\n  Symposium 2021 (IV2021). Code is available at\n  https://github.com/Herrccc/DR-TANet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Street scene change detection continues to capture researchers' interests in\nthe computer vision community. It aims to identify the changed regions of the\npaired street-view images captured at different times. The state-of-the-art\nnetwork based on the encoder-decoder architecture leverages the feature maps at\nthe corresponding level between two channels to gain sufficient information of\nchanges. Still, the efficiency of feature extraction, feature correlation\ncalculation, even the whole network requires further improvement. This paper\nproposes the temporal attention and explores the impact of the dependency-scope\nsize of temporal attention on the performance of change detection. In addition,\nbased on the Temporal Attention Module (TAM), we introduce a more efficient and\nlight-weight version - Dynamic Receptive Temporal Attention Module (DRTAM) and\npropose the Concurrent Horizontal and Vertical Attention (CHVA) to improve the\naccuracy of the network on specific challenging entities. On street scene\ndatasets `GSV', `TSUNAMI' and `VL-CMU-CD', our approach gains excellent\nperformance, establishing new state-of-the-art scores without bells and\nwhistles, while maintaining high efficiency applicable in autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 10:01:35 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 09:15:49 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Chen", "Shuo", ""], ["Yang", "Kailun", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2103.00885", "submitter": "Lasse Hansen", "authors": "Lasse Hansen and Mattias P. Heinrich", "title": "Deep learning based geometric registration for medical images: How\n  accurate can we get without visual features?", "comments": "accepted at IPMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As in other areas of medical image analysis, e.g. semantic segmentation, deep\nlearning is currently driving the development of new approaches for image\nregistration. Multi-scale encoder-decoder network architectures achieve\nstate-of-the-art accuracy on tasks such as intra-patient alignment of abdominal\nCT or brain MRI registration, especially when additional supervision, such as\nanatomical labels, is available. The success of these methods relies to a large\nextent on the outstanding ability of deep CNNs to extract descriptive visual\nfeatures from the input images. In contrast to conventional methods, the\nexplicit inclusion of geometric information plays only a minor role, if at all.\nIn this work we take a look at an exactly opposite approach by investigating a\ndeep learning framework for registration based solely on geometric features and\noptimisation. We combine graph convolutions with loopy belief message passing\nto enable highly accurate 3D point cloud registration. Our experimental\nvalidation is conducted on complex key-point graphs of inner lung structures,\nstrongly outperforming dense encoder-decoder networks and other point set\nregistration methods. Our code is publicly available at\nhttps://github.com/multimodallearning/deep-geo-reg.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 10:15:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hansen", "Lasse", ""], ["Heinrich", "Mattias P.", ""]]}, {"id": "2103.00887", "submitter": "Zhongqi Yue", "authors": "Zhongqi Yue, Tan Wang, Hanwang Zhang, Qianru Sun, Xian-Sheng Hua", "title": "Counterfactual Zero-Shot and Open-Set Visual Recognition", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel counterfactual framework for both Zero-Shot Learning (ZSL)\nand Open-Set Recognition (OSR), whose common challenge is generalizing to the\nunseen-classes by only training on the seen-classes. Our idea stems from the\nobservation that the generated samples for unseen-classes are often out of the\ntrue distribution, which causes severe recognition rate imbalance between the\nseen-class (high) and unseen-class (low). We show that the key reason is that\nthe generation is not Counterfactual Faithful, and thus we propose a faithful\none, whose generation is from the sample-specific counterfactual question: What\nwould the sample look like, if we set its class attribute to a certain class,\nwhile keeping its sample attribute unchanged? Thanks to the faithfulness, we\ncan apply the Consistency Rule to perform unseen/seen binary classification, by\nasking: Would its counterfactual still look like itself? If ``yes'', the sample\nis from a certain class, and ``no'' otherwise. Through extensive experiments on\nZSL and OSR, we demonstrate that our framework effectively mitigates the\nseen/unseen imbalance and hence significantly improves the overall performance.\nNote that this framework is orthogonal to existing methods, thus, it can serve\nas a new baseline to evaluate how ZSL/OSR models generalize. Codes are\navailable at https://github.com/yue-zhongqi/gcm-cf.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 10:20:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yue", "Zhongqi", ""], ["Wang", "Tan", ""], ["Zhang", "Hanwang", ""], ["Sun", "Qianru", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2103.00930", "submitter": "Niklas Gunnarsson", "authors": "Niklas Gunnarsson, Jens Sj\\\"olund and Thomas B. Sch\\\"on", "title": "Latent linear dynamics in spatiotemporal medical data", "comments": "Currently under review for a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal imaging is common in medical imaging, with applications in\ne.g. cardiac diagnostics, surgical guidance and radiotherapy monitoring. In\nthis paper, we present an unsupervised model that identifies the underlying\ndynamics of the system, only based on the sequential images. The model maps the\ninput to a low-dimensional latent space wherein a linear relationship holds\nbetween a hidden state process and the observed latent process. Knowledge of\nthe system dynamics enables denoising, imputation of missing values and\nextrapolation of future image frames. We use a Variational Auto-Encoder (VAE)\nfor the dimensionality reduction and a Linear Gaussian State Space Model\n(LGSSM) for the latent dynamics. The model, known as a Kalman Variational\nAuto-Encoder, is end-to-end trainable and the weights, both in the VAE and\nLGSSM, are simultaneously updated by maximizing the evidence lower bound of the\nmarginal log likelihood. Our experiment, on cardiac ultrasound time series,\nshows that the dynamical model provide better reconstructions than a similar\nmodel without dynamics. And also possibility to impute and extrapolate for\nmissing samples.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 11:42:21 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gunnarsson", "Niklas", ""], ["Sj\u00f6lund", "Jens", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "2103.00933", "submitter": "Huangying Zhan", "authors": "Huangying Zhan, Chamara Saroj Weerasekera, Jia-Wang Bian, Ravi Garg,\n  Ian Reid", "title": "DF-VO: What Should Be Learnt for Visual Odometry?", "comments": "extended version of ICRA-2020 paper (Visual Odometry Revisited: What\n  Should Be Learnt?)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-view geometry-based methods dominate the last few decades in monocular\nVisual Odometry for their superior performance, while they have been vulnerable\nto dynamic and low-texture scenes. More importantly, monocular methods suffer\nfrom scale-drift issue, i.e., errors accumulate over time. Recent studies show\nthat deep neural networks can learn scene depths and relative camera in a\nself-supervised manner without acquiring ground truth labels. More\nsurprisingly, they show that the well-trained networks enable scale-consistent\npredictions over long videos, while the accuracy is still inferior to\ntraditional methods because of ignoring geometric information. Building on top\nof recent progress in computer vision, we design a simple yet robust VO system\nby integrating multi-view geometry and deep learning on Depth and optical Flow,\nnamely DF-VO. In this work, a) we propose a method to carefully sample\nhigh-quality correspondences from deep flows and recover accurate camera poses\nwith a geometric module; b) we address the scale-drift issue by aligning\ngeometrically triangulated depths to the scale-consistent deep depths, where\nthe dynamic scenes are taken into account. Comprehensive ablation studies show\nthe effectiveness of the proposed method, and extensive evaluation results show\nthe state-of-the-art performance of our system, e.g., Ours (1.652%) v.s.\nORB-SLAM (3.247%}) in terms of translation error in KITTI Odometry benchmark.\nSource code is publicly available at:\n\\href{https://github.com/Huangying-Zhan/DF-VO}{DF-VO}.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 11:50:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhan", "Huangying", ""], ["Weerasekera", "Chamara Saroj", ""], ["Bian", "Jia-Wang", ""], ["Garg", "Ravi", ""], ["Reid", "Ian", ""]]}, {"id": "2103.00937", "submitter": "Hao Xu", "authors": "Hao Xu, Shuaicheng Liu, Guangfu Wang, Guanghui Liu, Bing Zeng", "title": "OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud registration is a key task in many computational fields. Previous\ncorrespondence matching based methods require the point clouds to have\ndistinctive geometric structures to fit a 3D rigid transformation according to\npoint-wise sparse feature matches. However, the accuracy of transformation\nheavily relies on the quality of extracted features, which are prone to errors\nwith respect partiality and noise of the inputs. In addition, they can not\nutilize the geometric knowledge of all regions. On the other hand, previous\nglobal feature based deep learning approaches can utilize the entire point\ncloud for the registration, however they ignore the negative effect of\nnon-overlapping points when aggregating global feature from point-wise\nfeatures. In this paper, we present OMNet, a global feature based iterative\nnetwork for partial-to-partial point cloud registration. We learn masks in a\ncoarse-to-fine manner to reject non-overlapping regions, which converting the\npartial-to-partial registration to the registration of the same shapes.\nMoreover, the data used in previous works are only sampled once from CAD models\nfor each object, resulting the same point cloud for the source and the\nreference. We propose a more practical manner for data generation, where a CAD\nmodel is sampled twice for the source and the reference point clouds, avoiding\nover-fitting issues that commonly exist previously. Experimental results show\nthat our approach achieves state-of-the-art performance compared to traditional\nand deep learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 11:59:59 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 06:00:29 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 14:44:56 GMT"}, {"version": "v4", "created": "Thu, 25 Mar 2021 09:51:06 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Xu", "Hao", ""], ["Liu", "Shuaicheng", ""], ["Wang", "Guangfu", ""], ["Liu", "Guanghui", ""], ["Zeng", "Bing", ""]]}, {"id": "2103.00940", "submitter": "Juan Marcos Ramirez Rond\\'on", "authors": "Juan Marcos Ram\\'irez, Jos\\'e Ignacio Mart\\'inez Torre, Henry Arguello\n  Fuentes", "title": "LADMM-Net: An Unrolled Deep Network For Spectral Image Fusion From\n  Compressive Data", "comments": "24 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Hyperspectral (HS) and multispectral (MS) image fusion aims at estimating a\nhigh-resolution spectral image from a low-spatial-resolution HS image and a\nlow-spectral-resolution MS image. Compressive spectral imaging (CSI) has\nemerged as an acquisition framework that captures the relevant information of\nspectral images using a reduced number of snapshots. Various spectral image\nfusion methods from multi-sensor CSI measurements have been proposed.\nNevertheless, these methods exhibit high running times and face the drawback of\nchoosing a representation transform. In this work, a deep learning architecture\nunder the algorithm unrolling approach is proposed for solving the fusion\nproblem from HS and MS compressive measurements. This architecture, dubbed\nLADMM-Net, casts each iteration of a linearized version of the alternating\ndirection method of multipliers into a processing layer whose concatenation\nforms a deep network. The linearized approach leads to estimate the target\nvariable without resorting to expensive matrix operations. This approach also\nestimates the image high-frequency component included in both the auxiliary\nvariable and the Lagrange multiplier. The performance of the proposed technique\nis evaluated on two spectral image databases and one dataset captured at the\nlaboratory. Extensive simulations show that the proposed method outperforms the\nstate-of-the-art approaches that fuse spectral images from compressive data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 12:04:42 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ram\u00edrez", "Juan Marcos", ""], ["Torre", "Jos\u00e9 Ignacio Mart\u00ednez", ""], ["Fuentes", "Henry Arguello", ""]]}, {"id": "2103.00944", "submitter": "Dengyu Wu", "authors": "Dengyu Wu, Xinping Yi, Xiaowei Huang", "title": "A Little Energy Goes a Long Way: Energy-Efficient, Accurate Conversion\n  from Convolutional Neural Networks to Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) offer an inherent ability to process\nspatial-temporal data, or in other words, realworld sensory data, but suffer\nfrom the difficulty of training high accuracy models. A major thread of\nresearch on SNNs is on converting a pre-trained convolutional neural network\n(CNN) to an SNN of the same structure. State-of-the-art conversion methods are\napproaching the accuracy limit, i.e., the near-zero accuracy loss of SNN\nagainst the original CNN. However, we note that this is made possible only when\nsignificantly more energy is consumed to process an input. In this paper, we\nargue that this trend of \"energy for accuracy\" is not necessary -- a little\nenergy can go a long way to achieve the near-zero accuracy loss. Specifically,\nwe propose a novel CNN-to-SNN conversion method that is able to use a\nreasonably short spike train (e.g., 256 timesteps for CIFAR10 images) to\nachieve the near-zero accuracy loss. The new conversion method, named as\nexplicit current control (ECC), contains three techniques (current\nnormalisation, thresholding for residual elimination, and consistency\nmaintenance for batch-normalisation), in order to explicitly control the\ncurrents flowing through the SNN when processing inputs. We implement ECC into\na tool nicknamed SpKeras, which can conveniently import Keras CNN models and\nconvert them into SNNs. We conduct an extensive set of experiments with the\ntool -- working with VGG16 and various datasets such as CIFAR10 and CIFAR100 --\nand compare with state-of-the-art conversion methods. Results show that ECC is\na promising method that can optimise over energy consumption and accuracy loss\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 12:15:29 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 12:24:59 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wu", "Dengyu", ""], ["Yi", "Xinping", ""], ["Huang", "Xiaowei", ""]]}, {"id": "2103.00945", "submitter": "Roger Mar\\'i", "authors": "Roger Mar\\'i, Carlo de Franchis, Enric Meinhardt-Llopis, Gabriele\n  Facciolo", "title": "Automatic Stockpile Volume Monitoring using Multi-view Stereo from\n  SkySat Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper proposes a system for automatic surface volume monitoring from\ntime series of SkySat pushframe imagery. A specific challenge of building and\ncomparing large 3D models from SkySat data is to correct inconsistencies\nbetween the camera models associated to the multiple views that are necessary\nto cover the area at a given time, where these camera models are represented as\nRational Polynomial Cameras (RPCs). We address the problem by proposing a\ndate-wise RPC refinement, able to handle dynamic areas covered by sets of\npartially overlapping views. The cameras are refined by means of a rotation\nthat compensates for errors due to inaccurate knowledge of the satellite\nattitude. The refined RPCs are then used to reconstruct multiple consistent\nDigital Surface Models (DSMs) from different stereo pairs at each date. RPC\nrefinement strengthens the consistency between the DSMs of each date, which is\nextremely beneficial to accurately measure volumes in the 3D surface models.\nThe system is tested in a real case scenario, to monitor large coal stockpiles.\nOur volume estimates are validated with measurements collected on site in the\nsame period of time.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 12:18:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mar\u00ed", "Roger", ""], ["de Franchis", "Carlo", ""], ["Meinhardt-Llopis", "Enric", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "2103.00947", "submitter": "Yunshuang Li", "authors": "Yunshuang Li, Zheyuan Huang, Zexi chen, Yue Wang and Rong Xiong", "title": "Collaborative Recognition of Feasible Region with Aerial and Ground\n  Robots through DPCN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground robots always get collision in that only if they get close to the\nobstacles, can they sense the danger and take actions, which is usually too\nlate to avoid the crash, causing severe damage to the robots. To address this\nissue, we present collaboration of aerial and ground robots in recognition of\nfeasible region. Taking the aerial robots' advantages of having large scale\nvariance of view points of the same route which the ground robots is on, the\ncollaboration work provides global information of road segmentation for the\nground robot, thus enabling it to obtain feasible region and adjust its pose\nahead of time. Under normal circumstance, the transformation between these two\ndevices can be obtained by GPS yet with much error, directly causing inferior\ninfluence on recognition of feasible region. Thereby, we utilize the\nstate-of-the-art research achievements in matching heterogeneous sensor\nmeasurements called deep phase correlation network(DPCN), which has excellent\nperformance on heterogeneous mapping, to refine the transformation. The network\nis light-weighted and promising for better generalization. We use Aero-Ground\ndataset which consists of heterogeneous sensor images and aerial road\nsegmentation images. The results show that our collaborative system has great\naccuracy, speed and stability.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 12:22:11 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 08:59:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Yunshuang", ""], ["Huang", "Zheyuan", ""], ["chen", "Zexi", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2103.00948", "submitter": "Anjith George", "authors": "Anjith George and Sebastien Marcel", "title": "Cross Modal Focal Loss for RGBD Face Anti-Spoofing", "comments": "10 pages, Accepted for publication in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic methods for detecting presentation attacks are essential to ensure\nthe reliable use of facial recognition technology. Most of the methods\navailable in the literature for presentation attack detection (PAD) fails in\ngeneralizing to unseen attacks. In recent years, multi-channel methods have\nbeen proposed to improve the robustness of PAD systems. Often, only a limited\namount of data is available for additional channels, which limits the\neffectiveness of these methods. In this work, we present a new framework for\nPAD that uses RGB and depth channels together with a novel loss function. The\nnew architecture uses complementary information from the two modalities while\nreducing the impact of overfitting. Essentially, a cross-modal focal loss\nfunction is proposed to modulate the loss contribution of each channel as a\nfunction of the confidence of individual channels. Extensive evaluations in two\npublicly available datasets demonstrate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 12:22:44 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["George", "Anjith", ""], ["Marcel", "Sebastien", ""]]}, {"id": "2103.00950", "submitter": "Patrik Joslin Kenfack", "authors": "Patrik Joslin Kenfack, Daniil Dmitrievich Arapov, Rasheed Hussain,\n  S.M. Ahsan Kazmi, Adil Mehmood Khan", "title": "On the Fairness of Generative Adversarial Networks (GANs)", "comments": "Corrected typos, added results on CelibA dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks (GANs) are one of the greatest advances in AI\nin recent years. With their ability to directly learn the probability\ndistribution of data, and then sample synthetic realistic data. Many\napplications have emerged, using GANs to solve classical problems in machine\nlearning, such as data augmentation, class unbalance problems, and fair\nrepresentation learning. In this paper, we analyze and highlight fairness\nconcerns of GANs model. In this regard, we show empirically that GANs models\nmay inherently prefer certain groups during the training process and therefore\nthey're not able to homogeneously generate data from different groups during\nthe testing phase. Furthermore, we propose solutions to solve this issue by\nconditioning the GAN model towards samples' group or using ensemble method\n(boosting) to allow the GAN model to leverage distributed structure of data\nduring the training phase and generate groups at equal rate during the testing\nphase.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 12:25:01 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 17:30:51 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kenfack", "Patrik Joslin", ""], ["Arapov", "Daniil Dmitrievich", ""], ["Hussain", "Rasheed", ""], ["Kazmi", "S. M. Ahsan", ""], ["Khan", "Adil Mehmood", ""]]}, {"id": "2103.00953", "submitter": "Guangyao Chen", "authors": "Guangyao Chen and Peixi Peng and Xiangqian Wang and Yonghong Tian", "title": "Adversarial Reciprocal Points Learning for Open Set Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open set recognition (OSR), aiming to simultaneously classify the seen\nclasses and identify the unseen classes as 'unknown', is essential for reliable\nmachine learning.The key challenge of OSR is how to reduce the empirical\nclassification risk on the labeled known data and the open space risk on the\npotential unknown data simultaneously. To handle the challenge, we formulate\nthe open space risk problem from the perspective of multi-class integration,\nand model the unexploited extra-class space with a novel concept Reciprocal\nPoint. Follow this, a novel learning framework, termed Adversarial Reciprocal\nPoint Learning (ARPL), is proposed to minimize the overlap of known\ndistribution and unknown distributions without loss of known classification\naccuracy. Specifically, each reciprocal point is learned by the extra-class\nspace with the corresponding known category, and the confrontation among\nmultiple known categories are employed to reduce the empirical classification\nrisk. Then, an adversarial margin constraint is proposed to reduce the open\nspace risk by limiting the latent open space constructed by reciprocal points.\nTo further estimate the unknown distribution from open space, an instantiated\nadversarial enhancement method is designed to generate diverse and confusing\ntraining samples, based on the adversarial mechanism between the reciprocal\npoints and known classes. This can effectively enhance the model\ndistinguishability to the unknown classes. Extensive experimental results on\nvarious benchmark datasets indicate that the proposed method is significantly\nsuperior to other existing approaches and achieves state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 12:25:45 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 02:04:04 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Chen", "Guangyao", ""], ["Peng", "Peixi", ""], ["Wang", "Xiangqian", ""], ["Tian", "Yonghong", ""]]}, {"id": "2103.00991", "submitter": "Pratik Mazumder", "authors": "Pratik Mazumder, Pravendra Singh, Piyush Rai", "title": "Few-Shot Lifelong Learning", "comments": "Accepted in AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world classification problems often have classes with very few\nlabeled training samples. Moreover, all possible classes may not be initially\navailable for training, and may be given incrementally. Deep learning models\nneed to deal with this two-fold problem in order to perform well in real-life\nsituations. In this paper, we propose a novel Few-Shot Lifelong Learning (FSLL)\nmethod that enables deep learning models to perform lifelong/continual learning\non few-shot data. Our method selects very few parameters from the model for\ntraining every new set of classes instead of training the full model. This\nhelps in preventing overfitting. We choose the few parameters from the model in\nsuch a way that only the currently unimportant parameters get selected. By\nkeeping the important parameters in the model intact, our approach minimizes\ncatastrophic forgetting. Furthermore, we minimize the cosine similarity between\nthe new and the old class prototypes in order to maximize their separation,\nthereby improving the classification performance. We also show that integrating\nour method with self-supervision improves the model performance significantly.\nWe experimentally show that our method significantly outperforms existing\nmethods on the miniImageNet, CIFAR-100, and CUB-200 datasets. Specifically, we\noutperform the state-of-the-art method by an absolute margin of 19.27% for the\nCUB dataset.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 13:26:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mazumder", "Pratik", ""], ["Singh", "Pravendra", ""], ["Rai", "Piyush", ""]]}, {"id": "2103.01039", "submitter": "Elmira Amirloo Abolfathi", "authors": "Elmira Amirloo, Mohsen Rohani, Ershad Banijamali, Jun Luo, Pascal\n  Poupart", "title": "Self-Supervised Simultaneous Multi-Step Prediction of Road Dynamics and\n  Cost Map", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While supervised learning is widely used for perception modules in\nconventional autonomous driving solutions, scalability is hindered by the huge\namount of data labeling needed. In contrast, while end-to-end architectures do\nnot require labeled data and are potentially more scalable, interpretability is\nsacrificed. We introduce a novel architecture that is trained in a fully\nself-supervised fashion for simultaneous multi-step prediction of space-time\ncost map and road dynamics. Our solution replaces the manually designed cost\nfunction for motion planning with a learned high dimensional cost map that is\nnaturally interpretable and allows diverse contextual information to be\nintegrated without manual data labeling. Experiments on real world driving data\nshow that our solution leads to lower number of collisions and road violations\nin long planning horizons in comparison to baselines, demonstrating the\nfeasibility of fully self-supervised prediction without sacrificing either\nscalability or interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:32:40 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 20:45:13 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Amirloo", "Elmira", ""], ["Rohani", "Mohsen", ""], ["Banijamali", "Ershad", ""], ["Luo", "Jun", ""], ["Poupart", "Pascal", ""]]}, {"id": "2103.01049", "submitter": "Haotong Qin", "authors": "Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan,\n  Renshuai Tao, Yuhang Li, Fengwei Yu, Xianglong Liu", "title": "Diversifying Sample Generation for Accurate Data-Free Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization has emerged as one of the most prevalent approaches to compress\nand accelerate neural networks. Recently, data-free quantization has been\nwidely studied as a practical and promising solution. It synthesizes data for\ncalibrating the quantized model according to the batch normalization (BN)\nstatistics of FP32 ones and significantly relieves the heavy dependency on real\ntraining data in traditional quantization methods. Unfortunately, we find that\nin practice, the synthetic data identically constrained by BN statistics\nsuffers serious homogenization at both distribution level and sample level and\nfurther causes a significant performance drop of the quantized model. We\npropose Diverse Sample Generation (DSG) scheme to mitigate the adverse effects\ncaused by homogenization. Specifically, we slack the alignment of feature\nstatistics in the BN layer to relax the constraint at the distribution level\nand design a layerwise enhancement to reinforce specific layers for different\ndata samples. Our DSG scheme is versatile and even able to be applied to the\nstate-of-the-art post-training quantization method like AdaRound. We evaluate\nthe DSG scheme on the large-scale image classification task and consistently\nobtain significant improvements over various network architectures and\nquantization methods, especially when quantized to lower bits (e.g., up to 22%\nimprovement on W4A4). Moreover, benefiting from the enhanced diversity, models\ncalibrated by synthetic data perform close to those calibrated by real data and\neven outperform them on W4A4.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:46:02 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 04:45:32 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhang", "Xiangguo", ""], ["Qin", "Haotong", ""], ["Ding", "Yifu", ""], ["Gong", "Ruihao", ""], ["Yan", "Qinghua", ""], ["Tao", "Renshuai", ""], ["Li", "Yuhang", ""], ["Yu", "Fengwei", ""], ["Liu", "Xianglong", ""]]}, {"id": "2103.01050", "submitter": "Jaikai Wang", "authors": "Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu Tang, and\n  Xianglong Liu", "title": "Dual Attention Suppression Attack: Generate Adversarial Camouflage in\n  Physical World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are vulnerable to adversarial examples. As a more\nthreatening type for practical deep learning systems, physical adversarial\nexamples have received extensive research attention in recent years. However,\nwithout exploiting the intrinsic characteristics such as model-agnostic and\nhuman-specific patterns, existing works generate weak adversarial perturbations\nin the physical world, which fall short of attacking across different models\nand show visually suspicious appearance. Motivated by the viewpoint that\nattention reflects the intrinsic characteristics of the recognition process,\nthis paper proposes the Dual Attention Suppression (DAS) attack to generate\nvisually-natural physical adversarial camouflages with strong transferability\nby suppressing both model and human attention. As for attacking, we generate\ntransferable adversarial camouflages by distracting the model-shared similar\nattention patterns from the target to non-target regions. Meanwhile, based on\nthe fact that human visual attention always focuses on salient items (e.g.,\nsuspicious distortions), we evade the human-specific bottom-up attention to\ngenerate visually-natural camouflages which are correlated to the scenario\ncontext. We conduct extensive experiments in both the digital and physical\nworld for classification and detection tasks on up-to-date models (e.g.,\nYolo-V5) and significantly demonstrate that our method outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:46:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Jiakai", ""], ["Liu", "Aishan", ""], ["Yin", "Zixin", ""], ["Liu", "Shunchang", ""], ["Tang", "Shiyu", ""], ["Liu", "Xianglong", ""]]}, {"id": "2103.01055", "submitter": "Bing Wang", "authors": "Bing Wang, Changhao Chen, Zhaopeng Cui, Jie Qin, Chris Xiaoxuan Lu,\n  Zhengdi Yu, Peijun Zhao, Zhen Dong, Fan Zhu, Niki Trigoni, Andrew Markham", "title": "P2-Net: Joint Description and Detection of Local Features for Pixel and\n  Point Matching", "comments": "ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately describing and detecting 2D and 3D keypoints is crucial to\nestablishing correspondences across images and point clouds. Despite a plethora\nof learning-based 2D or 3D local feature descriptors and detectors having been\nproposed, the derivation of a shared descriptor and joint keypoint detector\nthat directly matches pixels and points remains under-explored by the\ncommunity. This work takes the initiative to establish fine-grained\ncorrespondences between 2D images and 3D point clouds. In order to directly\nmatch pixels and points, a dual fully convolutional framework is presented that\nmaps 2D and 3D inputs into a shared latent representation space to\nsimultaneously describe and detect keypoints. Furthermore, an ultra-wide\nreception mechanism in combination with a novel loss function are designed to\nmitigate the intrinsic information variations between pixel and point local\nregions. Extensive experimental results demonstrate that our framework shows\ncompetitive performance in fine-grained matching between images and point\nclouds and achieves state-of-the-art results for the task of indoor visual\nlocalization. Our source code will be available at [no-name-for-blind-review].\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:59:40 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 08:26:37 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Wang", "Bing", ""], ["Chen", "Changhao", ""], ["Cui", "Zhaopeng", ""], ["Qin", "Jie", ""], ["Lu", "Chris Xiaoxuan", ""], ["Yu", "Zhengdi", ""], ["Zhao", "Peijun", ""], ["Dong", "Zhen", ""], ["Zhu", "Fan", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "2103.01075", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen\n  Qin, Dara Bahri, Da-Cheng Juan, Donald Metzler", "title": "OmniNet: Omnidirectional Representations from Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Omnidirectional Representations from Transformers\n(OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive\nfield, each token is allowed to attend to all tokens in the entire network.\nThis process can also be interpreted as a form of extreme or intensive\nattention mechanism that has the receptive field of the entire width and depth\nof the network. To this end, the omnidirectional attention is learned via a\nmeta-learner, which is essentially another self-attention based model. In order\nto mitigate the computationally expensive costs of full receptive field\nattention, we leverage efficient self-attention models such as kernel-based\n(Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer\net al.) as the meta-learner. Extensive experiments are conducted on\nautoregressive language modeling (LM1B, C4), Machine Translation, Long Range\nArena (LRA), and Image Recognition. The experiments show that OmniNet achieves\nconsiderable improvements across these tasks, including achieving\nstate-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena.\nMoreover, using omnidirectional representation in Vision Transformers leads to\nsignificant improvements on image recognition tasks on both few-shot learning\nand fine-tuning setups.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:31:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Aribandi", "Vamsi", ""], ["Gupta", "Jai", ""], ["Pham", "Philip", ""], ["Qin", "Zhen", ""], ["Bahri", "Dara", ""], ["Juan", "Da-Cheng", ""], ["Metzler", "Donald", ""]]}, {"id": "2103.01077", "submitter": "Aming Wu", "authors": "Aming Wu, Yahong Han, Linchao Zhu, Yi Yang, Cheng Deng", "title": "Universal-Prototype Augmentation for Few-Shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection (FSOD) aims to strengthen the performance of novel\nobject detection with few labeled samples. To alleviate the constraint of few\nsamples, enhancing the generalization ability of learned features for novel\nobjects plays a key role. Thus, the feature learning process of FSOD should\nfocus more on intrinsical object characteristics, which are invariant under\ndifferent visual changes and therefore are helpful for feature generalization.\nUnlike previous attempts of the meta-learning paradigm, in this paper, we\nexplore how to smooth object features with intrinsical characteristics that are\nuniversal across different object categories. We propose a new prototype,\nnamely universal prototype, that is learned from all object categories. Besides\nthe advantage of characterizing invariant characteristics, the universal\nprototypes alleviate the impact of unbalanced object categories. After\naugmenting object features with the universal prototypes, we impose a\nconsistency loss to maximize the agreement between the augmented features and\nthe original one, which is beneficial for learning invariant object\ncharacteristics. Thus, we develop a new framework of few-shot object detection\nwith universal prototypes (${FSOD}^{up}$) that owns the merit of feature\ngeneralization towards novel objects. Experimental results on PASCAL VOC and MS\nCOCO demonstrate the effectiveness of ${FSOD}^{up}$. Particularly, for the\n1-shot case of VOC Split2, ${FSOD}^{up}$ outperforms the baseline by 6.8\\% in\nterms of mAP. Moreover, we further verify ${FSOD}^{up}$ on a long-tail\ndetection dataset, i.e., LVIS. And employing ${FSOD}^{up}$ outperforms the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:35:36 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wu", "Aming", ""], ["Han", "Yahong", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""], ["Deng", "Cheng", ""]]}, {"id": "2103.01090", "submitter": "Xulei Yang", "authors": "Way Tan, Bihan Wen, Xulei Yang", "title": "Systematic Analysis and Removal of Circular Artifacts for StyleGAN", "comments": "8 pages, 15 figures, the original version firstly submitted to AAAI\n  2020 (in Aug. 2019) was titled \"Towards Winning the Last Mile: Analysis and\n  Removal of Circular Artifacts for StyleGAN\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  StyleGAN is one of the state-of-the-art image generators which is well-known\nfor synthesizing high-resolution and hyper-realistic face images. Though images\ngenerated by vanilla StyleGAN model are visually appealing, they sometimes\ncontain prominent circular artifacts which severely degrade the quality of\ngenerated images. In this work, we provide a systematic investigation on how\nthose circular artifacts are formed by studying the functionalities of\ndifferent stages of vanilla StyleGAN architecture, with both mechanism analysis\nand extensive experiments. The key modules of vanilla StyleGAN that promote\nsuch undesired artifacts are highlighted. Our investigation also explains why\nthe artifacts are usually circular, relatively small and rarely split into 2 or\nmore parts. Besides, we propose a simple yet effective solution to remove the\nprominent circular artifacts for vanilla StyleGAN, by applying a novel\npixel-instance normalization (PIN) layer.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:56:26 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 08:43:46 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Tan", "Way", ""], ["Wen", "Bihan", ""], ["Yang", "Xulei", ""]]}, {"id": "2103.01100", "submitter": "Cody Reading", "authors": "Cody Reading, Ali Harakeh, Julia Chae, and Steven L. Waslander", "title": "Categorical Depth Distribution Network for Monocular 3D Object Detection", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monocular 3D object detection is a key problem for autonomous vehicles, as it\nprovides a solution with simple configuration compared to typical multi-sensor\nsystems. The main challenge in monocular 3D detection lies in accurately\npredicting object depth, which must be inferred from object and scene cues due\nto the lack of direct range measurement. Many methods attempt to directly\nestimate depth to assist in 3D detection, but show limited performance as a\nresult of depth inaccuracy. Our proposed solution, Categorical Depth\nDistribution Network (CaDDN), uses a predicted categorical depth distribution\nfor each pixel to project rich contextual feature information to the\nappropriate depth interval in 3D space. We then use the computationally\nefficient bird's-eye-view projection and single-stage detector to produce the\nfinal output bounding boxes. We design CaDDN as a fully differentiable\nend-to-end approach for joint depth estimation and object detection. We\nvalidate our approach on the KITTI 3D object detection benchmark, where we rank\n1st among published monocular methods. We also provide the first monocular 3D\ndetection results on the newly released Waymo Open Dataset. We provide a code\nrelease for CaDDN which is made available.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:08:29 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 20:12:23 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Reading", "Cody", ""], ["Harakeh", "Ali", ""], ["Chae", "Julia", ""], ["Waslander", "Steven L.", ""]]}, {"id": "2103.01114", "submitter": "Juan Mier", "authors": "Juan Carlos Mier, Eddie Huang, Hossein Talebi, Feng Yang, Peyman\n  Milanfar", "title": "Deep Perceptual Image Quality Assessment for Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lossy Image compression is necessary for efficient storage and transfer of\ndata. Typically the trade-off between bit-rate and quality determines the\noptimal compression level. This makes the image quality metric an integral part\nof any imaging system. While the existing full-reference metrics such as PSNR\nand SSIM may be less sensitive to perceptual quality, the recently introduced\nlearning methods may fail to generalize to unseen data. In this paper we\npropose the largest image compression quality dataset to date with human\nperceptual preferences, enabling the use of deep learning, and we develop a\nfull reference perceptual quality assessment metric for lossy image compression\nthat outperforms the existing state-of-the-art methods. We show that the\nproposed model can effectively learn from thousands of examples available in\nthe new dataset, and consequently it generalizes better to other unseen\ndatasets of human perceptual preference.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:31:10 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 14:47:34 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Mier", "Juan Carlos", ""], ["Huang", "Eddie", ""], ["Talebi", "Hossein", ""], ["Yang", "Feng", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2103.01128", "submitter": "Xu Yan", "authors": "Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li,\n  Shuguang Cui", "title": "InstanceRefer: Cooperative Holistic Understanding for Visual Grounding\n  on Point Clouds through Instance Multi-level Contextual Referring", "comments": "To appear in ICCV 2021. Codes are released in\n  https://github.com/CurryYuan/InstanceRefer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with the visual grounding on 2D images, the natural-language-guided\n3D object localization on point clouds is more challenging. In this paper, we\npropose a new model, named InstanceRefer, to achieve a superior 3D visual\ngrounding through the grounding-by-matching strategy. In practice, our model\nfirst predicts the target category from the language descriptions using a\nsimple language classification model. Then, based on the category, our model\nsifts out a small number of instance candidates (usually less than 20) from the\npanoptic segmentation of point clouds. Thus, the non-trivial 3D visual\ngrounding task has been effectively re-formulated as a simplified\ninstance-matching problem, considering that instance-level candidates are more\nrational than the redundant 3D object proposals. Subsequently, for each\ncandidate, we perform the multi-level contextual inference, i.e., referring\nfrom instance attribute perception, instance-to-instance relation perception,\nand instance-to-background global localization perception, respectively.\nEventually, the most relevant candidate is selected and localized by ranking\nconfidence scores, which are obtained by the cooperative holistic\nvisual-language feature matching. Experiments confirm that our method\noutperforms previous state-of-the-arts on ScanRefer online benchmark and\nNr3D/Sr3D datasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:59:27 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 08:51:14 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Yuan", "Zhihao", ""], ["Yan", "Xu", ""], ["Liao", "Yinghong", ""], ["Zhang", "Ruimao", ""], ["Wang", "Sheng", ""], ["Li", "Zhen", ""], ["Cui", "Shuguang", ""]]}, {"id": "2103.01134", "submitter": "Prashant Pandey", "authors": "Prashant Pandey, Mrigank Raman, Sumanth Varambally, Prathosh AP", "title": "Domain Generalization via Inference-time Label-Preserving Target\n  Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalization of machine learning models trained on a set of source domains\non unseen target domains with different statistics, is a challenging problem.\nWhile many approaches have been proposed to solve this problem, they only\nutilize source data during training but do not take advantage of the fact that\na single target example is available at the time of inference. Motivated by\nthis, we propose a method that effectively uses the target sample during\ninference beyond mere classification. Our method has three components - (i) A\nlabel-preserving feature or metric transformation on source data such that the\nsource samples are clustered in accordance with their class irrespective of\ntheir domain (ii) A generative model trained on the these features (iii) A\nlabel-preserving projection of the target point on the source-feature manifold\nduring inference via solving an optimization problem on the input space of the\ngenerative model using the learned metric. Finally, the projected target is\nused in the classifier. Since the projected target feature comes from the\nsource manifold and has the same label as the real target by design, the\nclassifier is expected to perform better on it than the true target. We\ndemonstrate that our method outperforms the state-of-the-art Domain\nGeneralization methods on multiple datasets and tasks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 17:10:38 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 19:58:03 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 01:27:08 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Pandey", "Prashant", ""], ["Raman", "Mrigank", ""], ["Varambally", "Sumanth", ""], ["AP", "Prathosh", ""]]}, {"id": "2103.01141", "submitter": "Luca Clissa", "authors": "R. Morelli, L. Clissa, M. Dalla, M. Luppi, L. Rinaldi, A. Zoccoli", "title": "Automatic Cell Counting in Flourescent Microscopy Using Deep Learning", "comments": "11 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting cells in fluorescent microscopy is a tedious, time-consuming task\nthat researchers have to accomplish to assess the effects of different\nexperimental conditions on biological structures of interest. Although such\nobjects are generally easy to identify, the process of manually annotating\ncells is sometimes subject to arbitrariness due to the operator's\ninterpretation of the borderline cases.\n  We propose a Machine Learning approach that exploits a fully-convolutional\nnetwork in a binary segmentation fashion to localize the objects of interest.\nCounts are then retrieved as the number of detected items.\n  Specifically, we adopt a UNet-like architecture leveraging residual units and\nan extended bottleneck for enlarging the field-of-view. In addition, we make\nuse of weighted maps that penalize the errors on cells boundaries increasingly\nwith overcrowding. These changes provide more context and force the model to\nfocus on relevant features during pixel-wise classification. As a result, the\nmodel performance is enhanced, especially in presence of clumping cells,\nartifacts and confounding biological structures. Posterior assessment of the\nresults with domain experts confirms that the model detects cells of interest\ncorrectly. The model demonstrates a human-level ability inasmuch even erroneous\npredictions seem to fall within the limits of operator interpretation. This\nqualitative assessment is also corroborated by quantitative metrics as an\n${F_1}$ score of 0.87.\n  Despite some difficulties in interpretation, results are also satisfactory\nwith respect to the counting task, as testified by mean and median absolute\nerror of, respectively, 0.8 and 1.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 23:04:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Morelli", "R.", ""], ["Clissa", "L.", ""], ["Dalla", "M.", ""], ["Luppi", "M.", ""], ["Rinaldi", "L.", ""], ["Zoccoli", "A.", ""]]}, {"id": "2103.01146", "submitter": "Gilberto Ochoa-Ruiz", "authors": "Francisco Lopez, Andres Varela, Oscar Hinojosa, Mauricio Mendez,\n  Dinh-Hoan Trinh, Jonathan ElBeze, Jacques Hubert, Vincent Estrade, Miguel\n  Gonzalez, Gilberto Ochoa, Christian Daul", "title": "Assessing deep learning methods for the identification of kidney stones\n  in endoscopic images", "comments": "This paper is currently under review for the IEEE Engineering in\n  Medicine and Biology Conference (EMBC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowing the type (i.e., the biochemical composition) of kidney stones is\ncrucial to prevent relapses with an appropriate treatment. During\nureteroscopies, kidney stones are fragmented, extracted from the urinary tract,\nand their composition is determined using a morpho-constitutional analysis.\nThis procedure is time consuming (the morpho-constitutional analysis results\nare only available after some days) and tedious (the fragment extraction lasts\nup to an hour). Identifying the kidney stone type only with the in-vivo\nendoscopic images would allow for the dusting of the fragments, while the\nmorpho-constitutional analysis could be avoided. Only few contributions dealing\nwith the in vivo identification of kidney stones were published. This paper\ndiscusses and compares five classification methods including deep convolutional\nneural networks (DCNN)-based approaches and traditional (non DCNN-based) ones.\nEven if the best method is a DCCN approach with a precision and recall of 98%\nand 97% over four classes, this contribution shows that a XGBoost classifier\nexploiting well-chosen feature vectors can closely approach the performances of\nDCNN classifiers for a medical application with a limited number of annotated\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 17:31:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lopez", "Francisco", ""], ["Varela", "Andres", ""], ["Hinojosa", "Oscar", ""], ["Mendez", "Mauricio", ""], ["Trinh", "Dinh-Hoan", ""], ["ElBeze", "Jonathan", ""], ["Hubert", "Jacques", ""], ["Estrade", "Vincent", ""], ["Gonzalez", "Miguel", ""], ["Ochoa", "Gilberto", ""], ["Daul", "Christian", ""]]}, {"id": "2103.01205", "submitter": "Justin Terry", "authors": "J. K. Terry, Mario Jayakumar, Kusal De Alwis", "title": "Statistically Significant Stopping of Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:51:16 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 03:37:36 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 02:42:19 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Terry", "J. K.", ""], ["Jayakumar", "Mario", ""], ["De Alwis", "Kusal", ""]]}, {"id": "2103.01208", "submitter": "Francesco Croce", "authors": "Francesco Croce, Matthias Hein", "title": "Mind the box: $l_1$-APGD for sparse adversarial attacks on image\n  classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that when taking into account also the image domain $[0,1]^d$,\nestablished $l_1$-projected gradient descent (PGD) attacks are suboptimal as\nthey do not consider that the effective threat model is the intersection of the\n$l_1$-ball and $[0,1]^d$. We study the expected sparsity of the steepest\ndescent step for this effective threat model and show that the exact projection\nonto this set is computationally feasible and yields better performance.\nMoreover, we propose an adaptive form of PGD which is highly effective even\nwith a small budget of iterations. Our resulting $l_1$-APGD is a strong white\nbox attack showing that prior work overestimated their $l_1$-robustness. Using\n$l_1$-APGD for adversarial training we get a robust classifier with SOTA\n$l_1$-robustness. Finally, we combine $l_1$-APGD and an adaptation of the\nSquare Attack to $l_1$ into $l_1$-AutoAttack, an ensemble of attacks which\nreliably assesses adversarial robustness for the threat model of $l_1$-ball\nintersected with $[0,1]^d$.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:53:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Croce", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "2103.01209", "submitter": "Drew A. Hudson", "authors": "Drew A. Hudson and C. Lawrence Zitnick", "title": "Generative Adversarial Transformers", "comments": "Published as a conference paper at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:54:04 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 18:39:04 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 03:13:31 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hudson", "Drew A.", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "2103.01222", "submitter": "Zhenxi Li", "authors": "Zhenxi Li, Guillaume-Alexandre Bilodeau, Wassim Bouachir", "title": "Multiple Convolutional Features in Siamese Networks for Object Tracking", "comments": "Accepted for Machine Vision and Applications, 2021. arXiv admin note:\n  substantial text overlap with arXiv:2103.00810", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese trackers demonstrated high performance in object tracking due to\ntheir balance between accuracy and speed. Unlike classification-based CNNs,\ndeep similarity networks are specifically designed to address the image\nsimilarity problem, and thus are inherently more appropriate for the tracking\ntask. However, Siamese trackers mainly use the last convolutional layers for\nsimilarity analysis and target search, which restricts their performance. In\nthis paper, we argue that using a single convolutional layer as feature\nrepresentation is not an optimal choice in a deep similarity framework. We\npresent a Multiple Features-Siamese Tracker (MFST), a novel tracking algorithm\nexploiting several hierarchical feature maps for robust tracking. Since\nconvolutional layers provide several abstraction levels in characterizing an\nobject, fusing hierarchical features allows to obtain a richer and more\nefficient representation of the target. Moreover, we handle the target\nappearance variations by calibrating the deep features extracted from two\ndifferent CNN models. Based on this advanced feature representation, our method\nachieves high tracking accuracy, while outperforming the standard siamese\ntracker on object tracking benchmarks. The source code and trained models are\navailable at https://github.com/zhenxili96/MFST.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:02:27 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Li", "Zhenxi", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Bouachir", "Wassim", ""]]}, {"id": "2103.01255", "submitter": "Lan Fu", "authors": "Lan Fu, Changqing Zhou, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Wei\n  Feng, Yang Liu, Song Wang", "title": "Auto-Exposure Fusion for Single-Image Shadow Removal", "comments": "accepted to cvpr2021, code is available in\n  https://github.com/tsingqguo/exposure-fusion-shadow-removal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow removal is still a challenging task due to its inherent\nbackground-dependent and spatial-variant properties, leading to unknown and\ndiverse shadow patterns. Even powerful state-of-the-art deep neural networks\ncould hardly recover traceless shadow-removed background. This paper proposes a\nnew solution for this task by formulating it as an exposure fusion problem to\naddress the challenges. Intuitively, we can first estimate multiple\nover-exposure images w.r.t. the input image to let the shadow regions in these\nimages have the same color with shadow-free areas in the input image. Then, we\nfuse the original input with the over-exposure images to generate the final\nshadow-free counterpart. Nevertheless, the spatial-variant property of the\nshadow requires the fusion to be sufficiently `smart', that is, it should\nautomatically select proper over-exposure pixels from different images to make\nthe final output natural. To address this challenge, we propose the\nshadow-aware FusionNet that takes the shadow image as input to generate fusion\nweight maps across all the over-exposure images. Moreover, we propose the\nboundary-aware RefineNet to eliminate the remaining shadow trace further. We\nconduct extensive experiments on the ISTD, ISTD+, and SRD datasets to validate\nour method's effectiveness and show better performance in shadow regions and\ncomparable performance in non-shadow regions over the state-of-the-art methods.\nWe release the model and code in\nhttps://github.com/tsingqguo/exposure-fusion-shadow-removal.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:09:26 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 13:41:35 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Fu", "Lan", ""], ["Zhou", "Changqing", ""], ["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Yu", "Hongkai", ""], ["Feng", "Wei", ""], ["Liu", "Yang", ""], ["Wang", "Song", ""]]}, {"id": "2103.01261", "submitter": "Mianlun Zheng", "authors": "Mianlun Zheng, Yi Zhou, Duygu Ceylan, Jernej Barbi\\v{c}", "title": "A Deep Emulator for Secondary Motion of 3D Characters", "comments": "Accepted at CVPR 2021, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and light-weight methods for animating 3D characters are desirable in\nvarious applications such as computer games. We present a learning-based\napproach to enhance skinning-based animations of 3D characters with vivid\nsecondary motion effects. We design a neural network that encodes each local\npatch of a character simulation mesh where the edges implicitly encode the\ninternal forces between the neighboring vertices. The network emulates the\nordinary differential equations of the character dynamics, predicting new\nvertex positions from the current accelerations, velocities and positions.\nBeing a local method, our network is independent of the mesh topology and\ngeneralizes to arbitrarily shaped 3D character meshes at test time. We further\nrepresent per-vertex constraints and material properties such as stiffness,\nenabling us to easily adjust the dynamics in different parts of the mesh. We\nevaluate our method on various character meshes and complex motion sequences.\nOur method can be over 30 times more efficient than ground-truth physically\nbased simulation, and outperforms alternative solutions that provide fast\napproximations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:13:35 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:35:43 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 18:49:21 GMT"}, {"version": "v4", "created": "Sun, 11 Apr 2021 18:26:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zheng", "Mianlun", ""], ["Zhou", "Yi", ""], ["Ceylan", "Duygu", ""], ["Barbi\u010d", "Jernej", ""]]}, {"id": "2103.01272", "submitter": "Padmaja Kulkarni", "authors": "Taeke de Haan, Padmaja Kulkarni, and Robert Babuska", "title": "Geometry-Based Grasping of Vine Tomatoes", "comments": "8 pages, 12 figures. This work has been submitted to the IEEE for\n  possible publication (IROS + RAL). Copyright may be transferred without\n  notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a geometry-based grasping method for vine tomatoes. It relies on a\ncomputer-vision pipeline to identify the required geometric features of the\ntomatoes and of the truss stem. The grasping method then uses a geometric model\nof the robotic hand and the truss to determine a suitable grasping location on\nthe stem. This approach allows for grasping tomato trusses without requiring\ndelicate contact sensors or complex mechanistic models and under minimal risk\nof damaging the tomatoes. Lab experiments were conducted to validate the\nproposed methods, using an RGB-D camera and a low-cost robotic manipulator. The\nsuccess rate was 83% to 92%, depending on the type of truss.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:33:51 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["de Haan", "Taeke", ""], ["Kulkarni", "Padmaja", ""], ["Babuska", "Robert", ""]]}, {"id": "2103.01284", "submitter": "Matias Molina", "authors": "Mat\\'ias Molina (Universidad Nacional de C\\'ordoba) and Jorge\n  S\\'anchez (CONICET)", "title": "Performance Variability in Zero-Shot Classification", "comments": "LXAI Workshop @ NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero-shot classification (ZSC) is the task of learning predictors for classes\nnot seen during training. Although the different methods in the literature are\nevaluated using the same class splits, little is known about their stability\nunder different class partitions. In this work we show experimentally that ZSC\nperformance exhibits strong variability under changing training setups. We\npropose the use ensemble learning as an attempt to mitigate this phenomena.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:58:10 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Molina", "Mat\u00edas", "", "Universidad Nacional de C\u00f3rdoba"], ["S\u00e1nchez", "Jorge", "", "CONICET"]]}, {"id": "2103.01292", "submitter": "Weilin Li", "authors": "Wojciech Czaja, Weilin Li, Yiran Li, Mike Pekala", "title": "Maximal function pooling with applications", "comments": "18 pages, 1 figure, to appear in Excursions in Harmonic Analysis,\n  Volume 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the Hardy-Littlewood maximal function, we propose a novel pooling\nstrategy which is called maxfun pooling. It is presented both as a viable\nalternative to some of the most popular pooling functions, such as max pooling\nand average pooling, and as a way of interpolating between these two\nalgorithms. We demonstrate the features of maxfun pooling with two\napplications: first in the context of convolutional sparse coding, and then for\nimage classification.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:30:04 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Czaja", "Wojciech", ""], ["Li", "Weilin", ""], ["Li", "Yiran", ""], ["Pekala", "Mike", ""]]}, {"id": "2103.01299", "submitter": "Jonathan Frawley", "authors": "Jonathan Frawley, Chris G. Willcocks, Maged Habib, Caspar Geenen,\n  David H. Steel and Boguslaw Obara", "title": "Robust 3D U-Net Segmentation of Macular Holes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Macular holes are a common eye condition which result in visual impairment.\nWe look at the application of deep convolutional neural networks to the problem\nof macular hole segmentation. We use the 3D U-Net architecture as a basis and\nexperiment with a number of design variants. Manually annotating and measuring\nmacular holes is time consuming and error prone. Previous automated approaches\nto macular hole segmentation take minutes to segment a single 3D scan. Our\nproposed model generates significantly more accurate segmentations in less than\na second. We found that an approach of architectural simplification, by greatly\nsimplifying the network capacity and depth, exceeds both expert performance and\nstate-of-the-art models such as residual 3D U-Nets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:41:35 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 13:57:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Frawley", "Jonathan", ""], ["Willcocks", "Chris G.", ""], ["Habib", "Maged", ""], ["Geenen", "Caspar", ""], ["Steel", "David H.", ""], ["Obara", "Boguslaw", ""]]}, {"id": "2103.01302", "submitter": "Kumara Kahatapitiya", "authors": "Kumara Kahatapitiya and Michael S. Ryoo", "title": "Coarse-Fine Networks for Temporal Activity Detection in Videos", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Coarse-Fine Networks, a two-stream architecture\nwhich benefits from different abstractions of temporal resolution to learn\nbetter video representations for long-term motion. Traditional Video models\nprocess inputs at one (or few) fixed temporal resolution without any dynamic\nframe selection. However, we argue that, processing multiple temporal\nresolutions of the input and doing so dynamically by learning to estimate the\nimportance of each frame can largely improve video representations, specially\nin the domain of temporal activity localization. To this end, we propose (1)\nGrid Pool, a learned temporal downsampling layer to extract coarse features,\nand, (2) Multi-stage Fusion, a spatio-temporal attention mechanism to fuse a\nfine-grained context with the coarse features. We show that our method\noutperforms the state-of-the-arts for action detection in public datasets\nincluding Charades with a significantly reduced compute and memory footprint.\nThe code is available at https://github.com/kkahatapitiya/Coarse-Fine-Networks\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:48:01 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:57:04 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kahatapitiya", "Kumara", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "2103.01303", "submitter": "Weilin Li", "authors": "Wojciech Czaja, Ilya Kavalerov, Weilin Li", "title": "Exploring the high dimensional geometry of HSI features", "comments": "5 pages, 4 figures, to appear in WHISPERS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore feature space geometries induced by the 3-D Fourier scattering\ntransform and deep neural network with extended attribute profiles on four\nstandard hyperspectral images. We examine the distances and angles of class\nmeans, the variability of classes, and their low-dimensional structures. These\nstatistics are compared to that of raw features, and our results provide\ninsight into the vastly different properties of these two methods. We also\nexplore a connection with the newly observed deep learning phenomenon of neural\ncollapse.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:48:43 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Czaja", "Wojciech", ""], ["Kavalerov", "Ilya", ""], ["Li", "Weilin", ""]]}, {"id": "2103.01306", "submitter": "Jonathon Shlens", "authors": "Philipp Jund, Chris Sweeney, Nichola Abdo, Zhifeng Chen, Jonathon\n  Shlens", "title": "Scalable Scene Flow from Point Clouds in the Real World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles operate in highly dynamic environments necessitating an\naccurate assessment of which aspects of a scene are moving and where they are\nmoving to. A popular approach to 3D motion estimation -- termed scene flow --\nis to employ 3D point cloud data from consecutive LiDAR scans, although such\napproaches have been limited by the small size of real-world, annotated LiDAR\ndata. In this work, we introduce a new large scale benchmark for scene flow\nbased on the Waymo Open Dataset. The dataset is $\\sim$1,000$\\times$ larger than\nprevious real-world datasets in terms of the number of annotated frames and is\nderived from the corresponding tracked 3D objects. We demonstrate how previous\nworks were bounded based on the amount of real LiDAR data available, suggesting\nthat larger datasets are required to achieve state-of-the-art predictive\nperformance. Furthermore, we show how previous heuristics for operating on\npoint clouds such as artificial down-sampling heavily degrade performance,\nmotivating a new class of models that are tractable on the full point cloud. To\naddress this issue, we introduce the model architecture FastFlow3D that\nprovides real time inference on the full point cloud. Finally, we demonstrate\nthat this problem is amenable to techniques from semi-supervised learning by\nhighlighting open problems for generalizing methods for predicting motion on\nunlabeled objects. We hope that this dataset may provide new opportunities for\ndeveloping real world scene flow systems and motivate a new class of machine\nlearning problems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:56:05 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 20:06:15 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 16:23:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Jund", "Philipp", ""], ["Sweeney", "Chris", ""], ["Abdo", "Nichola", ""], ["Chen", "Zhifeng", ""], ["Shlens", "Jonathon", ""]]}, {"id": "2103.01315", "submitter": "Mamshad Nayeem Rizve", "authors": "Mamshad Nayeem Rizve, Salman Khan, Fahad Shahbaz Khan, Mubarak Shah", "title": "Exploring Complementary Strengths of Invariant and Equivariant\n  Representations for Few-Shot Learning", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many real-world problems, collecting a large number of labeled samples is\ninfeasible. Few-shot learning (FSL) is the dominant approach to address this\nissue, where the objective is to quickly adapt to novel categories in presence\nof a limited number of samples. FSL tasks have been predominantly solved by\nleveraging the ideas from gradient-based meta-learning and metric learning\napproaches. However, recent works have demonstrated the significance of\npowerful feature representations with a simple embedding network that can\noutperform existing sophisticated FSL algorithms. In this work, we build on\nthis insight and propose a novel training mechanism that simultaneously\nenforces equivariance and invariance to a general set of geometric\ntransformations. Equivariance or invariance has been employed standalone in the\nprevious works; however, to the best of our knowledge, they have not been used\njointly. Simultaneous optimization for both of these contrasting objectives\nallows the model to jointly learn features that are not only independent of the\ninput transformation but also the features that encode the structure of\ngeometric transformations. These complementary sets of features help generalize\nwell to novel classes with only a few data samples. We achieve additional\nimprovements by incorporating a novel self-supervised distillation objective.\nOur extensive experimentation shows that even without knowledge distillation\nour proposed method can outperform current state-of-the-art FSL methods on five\npopular benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 21:14:33 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 17:58:23 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rizve", "Mamshad Nayeem", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Shah", "Mubarak", ""]]}, {"id": "2103.01350", "submitter": "Xin Ye", "authors": "Xin Ye and Yezhou Yang", "title": "Hierarchical and Partially Observable Goal-driven Policy Learning with\n  Goals Relational Graph", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel two-layer hierarchical reinforcement learning approach\nequipped with a Goals Relational Graph (GRG) for tackling the partially\nobservable goal-driven task, such as goal-driven visual navigation. Our GRG\ncaptures the underlying relations of all goals in the goal space through a\nDirichlet-categorical process that facilitates: 1) the high-level network\nraising a sub-goal towards achieving a designated final goal; 2) the low-level\nnetwork towards an optimal policy; and 3) the overall system generalizing\nunseen environments and goals. We evaluate our approach with two settings of\npartially observable goal-driven tasks -- a grid-world domain and a robotic\nobject search task. Our experimental results show that our approach exhibits\nsuperior generalization performance on both unseen environments and new goals.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:21:46 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:27:39 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ye", "Xin", ""], ["Yang", "Yezhou", ""]]}, {"id": "2103.01353", "submitter": "Abhinav Valada", "authors": "Francisco Rivera Valverde, Juana Valeria Hurtado, Abhinav Valada", "title": "There is More than Meets the Eye: Self-Supervised Multi-Object Detection\n  and Tracking with Sound by Distilling Multimodal Knowledge", "comments": "Accepted at CVPR 2021. Dataset, code and models are available at\n  http://rl.uni-freiburg.de/research/multimodal-distill", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributes of sound inherent to objects can provide valuable cues to learn\nrich representations for object detection and tracking. Furthermore, the\nco-occurrence of audiovisual events in videos can be exploited to localize\nobjects over the image field by solely monitoring the sound in the environment.\nThus far, this has only been feasible in scenarios where the camera is static\nand for single object detection. Moreover, the robustness of these methods has\nbeen limited as they primarily rely on RGB images which are highly susceptible\nto illumination and weather changes. In this work, we present the novel\nself-supervised MM-DistillNet framework consisting of multiple teachers that\nleverage diverse modalities including RGB, depth and thermal images, to\nsimultaneously exploit complementary cues and distill knowledge into a single\naudio student network. We propose the new MTA loss function that facilitates\nthe distillation of information from multimodal teachers in a self-supervised\nmanner. Additionally, we propose a novel self-supervised pretext task for the\naudio student that enables us to not rely on labor-intensive manual\nannotations. We introduce a large-scale multimodal dataset with over 113,000\ntime-synchronized frames of RGB, depth, thermal, and audio modalities.\nExtensive experiments demonstrate that our approach outperforms\nstate-of-the-art methods while being able to detect multiple objects using only\nsound during inference and even while moving.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:42:18 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Valverde", "Francisco Rivera", ""], ["Hurtado", "Juana Valeria", ""], ["Valada", "Abhinav", ""]]}, {"id": "2103.01359", "submitter": "Gustavo Olague Dr.", "authors": "Gerardo Ibarra-Vazquez, Gustavo Olague, Mariana Chan-Ley, Cesar\n  Puente, Carlos Soubervielle-Montalvo", "title": "Brain Programming is Immune to Adversarial Attacks: Towards Accurate and\n  Robust Image Classification using Symbolic Learning", "comments": "58 pages, 9 figures, 13 tables, 81 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, the security concerns about the vulnerability of Deep\nConvolutional Neural Networks (DCNN) to Adversarial Attacks (AA) in the form of\nsmall modifications to the input image almost invisible to human vision make\ntheir predictions untrustworthy. Therefore, it is necessary to provide\nrobustness to adversarial examples in addition to an accurate score when\ndeveloping a new classifier. In this work, we perform a comparative study of\nthe effects of AA on the complex problem of art media categorization, which\ninvolves a sophisticated analysis of features to classify a fine collection of\nartworks. We tested a prevailing bag of visual words approach from computer\nvision, four state-of-the-art DCNN models (AlexNet, VGG, ResNet, ResNet101),\nand the Brain Programming (BP) algorithm. In this study, we analyze the\nalgorithms' performance using accuracy. Besides, we use the accuracy ratio\nbetween adversarial examples and clean images to measure robustness. Moreover,\nwe propose a statistical analysis of each classifier's predictions' confidence\nto corroborate the results. We confirm that BP predictions' change was below\n2\\% using adversarial examples computed with the fast gradient sign method.\nAlso, considering the multiple pixel attack, BP obtained four out of seven\nclasses without changes and the rest with a maximum error of 4\\% in the\npredictions. Finally, BP also gets four categories using adversarial patches\nwithout changes and for the remaining three classes with a variation of 1\\%.\nAdditionally, the statistical analysis showed that the predictions' confidence\nof BP were not significantly different for each pair of clean and perturbed\nimages in every experiment. These results prove BP's robustness against\nadversarial examples compared to DCNN and handcrafted features methods, whose\nperformance on the art media classification was compromised with the proposed\nperturbations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:49:26 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ibarra-Vazquez", "Gerardo", ""], ["Olague", "Gustavo", ""], ["Chan-Ley", "Mariana", ""], ["Puente", "Cesar", ""], ["Soubervielle-Montalvo", "Carlos", ""]]}, {"id": "2103.01361", "submitter": "Behrouz Rostami", "authors": "Behrouz Rostami, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu", "title": "Multiclass Burn Wound Image Classification Using Deep Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of people are affected by acute and chronic wounds yearly across the\nworld. Continuous wound monitoring is important for wound specialists to allow\nmore accurate diagnosis and optimization of management protocols. Machine\nLearning-based classification approaches provide optimal care strategies\nresulting in more reliable outcomes, cost savings, healing time reduction, and\nimproved patient satisfaction. In this study, we use a deep learning-based\nmethod to classify burn wound images into two or three different categories\nbased on the wound conditions. A pre-trained deep convolutional neural network,\nAlexNet, is fine-tuned using a burn wound image dataset and utilized as the\nclassifier. The classifier's performance is evaluated using classification\nmetrics such as accuracy, precision, and recall as well as confusion matrix. A\ncomparison with previous works that used the same dataset showed that our\ndesigned classifier improved the classification accuracy by more than 8%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:54:18 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Rostami", "Behrouz", ""], ["Niezgoda", "Jeffrey", ""], ["Gopalakrishnan", "Sandeep", ""], ["Yu", "Zeyun", ""]]}, {"id": "2103.01373", "submitter": "Aleksandra \\'Ciprijanovi\\'c", "authors": "A. \\'Ciprijanovi\\'c, D. Kafkes, K. Downey, S. Jenkins, G. N. Perdue,\n  S. Madireddy, T. Johnston, G. F. Snyder, B. Nord", "title": "DeepMerge II: Building Robust Deep Learning Algorithms for Merging\n  Galaxy Identification Across Domains", "comments": "Submitted to MNRAS; 21 pages, 9 figures, 9 tables", "journal-ref": "MNRAS, Volume 506, Issue 1, September 2021, Page 677", "doi": "10.1093/mnras/stab1677", "report-no": "FERMILAB-PUB-21-072-SCD", "categories": "astro-ph.IM astro-ph.GA cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In astronomy, neural networks are often trained on simulation data with the\nprospect of being used on telescope observations. Unfortunately, training a\nmodel on simulation data and then applying it to instrument data leads to a\nsubstantial and potentially even detrimental decrease in model accuracy on the\nnew target dataset. Simulated and instrument data represent different data\ndomains, and for an algorithm to work in both, domain-invariant learning is\nnecessary. Here we employ domain adaptation techniques$-$ Maximum Mean\nDiscrepancy (MMD) as an additional transfer loss and Domain Adversarial Neural\nNetworks (DANNs)$-$ and demonstrate their viability to extract domain-invariant\nfeatures within the astronomical context of classifying merging and non-merging\ngalaxies. Additionally, we explore the use of Fisher loss and entropy\nminimization to enforce better in-domain class discriminability. We show that\nthe addition of each domain adaptation technique improves the performance of a\nclassifier when compared to conventional deep learning algorithms. We\ndemonstrate this on two examples: between two Illustris-1 simulated datasets of\ndistant merging galaxies, and between Illustris-1 simulated data of nearby\nmerging galaxies and observed data from the Sloan Digital Sky Survey. The use\nof domain adaptation techniques in our experiments leads to an increase of\ntarget domain classification accuracy of up to ${\\sim}20\\%$. With further\ndevelopment, these techniques will allow astronomers to successfully implement\nneural network models trained on simulation data to efficiently detect and\nstudy astrophysical objects in current and future large-scale astronomical\nsurveys.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 00:24:10 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["\u0106iprijanovi\u0107", "A.", ""], ["Kafkes", "D.", ""], ["Downey", "K.", ""], ["Jenkins", "S.", ""], ["Perdue", "G. N.", ""], ["Madireddy", "S.", ""], ["Johnston", "T.", ""], ["Snyder", "G. F.", ""], ["Nord", "B.", ""]]}, {"id": "2103.01403", "submitter": "Qing Li", "authors": "Qing Li, Siyuan Huang, Yining Hong, Yixin Zhu, Ying Nian Wu, Song-Chun\n  Zhu", "title": "A HINT from Arithmetic: On Systematic Generalization of Perception,\n  Syntax, and Semantics", "comments": "Preliminary work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by humans' remarkable ability to master arithmetic and generalize to\nunseen problems, we present a new dataset, HINT, to study machines' capability\nof learning generalizable concepts at three different levels: perception,\nsyntax, and semantics. In particular, concepts in HINT, including both digits\nand operators, are required to learn in a weakly-supervised fashion: Only the\nfinal results of handwriting expressions are provided as supervision. Learning\nagents need to reckon how concepts are perceived from raw signals such as\nimages (i.e., perception), how multiple concepts are structurally combined to\nform a valid expression (i.e., syntax), and how concepts are realized to afford\nvarious reasoning tasks (i.e., semantics). With a focus on systematic\ngeneralization, we carefully design a five-fold test set to evaluate both the\ninterpolation and the extrapolation of learned concepts. To tackle this\nchallenging problem, we propose a neural-symbolic system by integrating neural\nnetworks with grammar parsing and program synthesis, learned by a novel\ndeduction--abduction strategy. In experiments, the proposed neural-symbolic\nsystem demonstrates strong generalization capability and significantly\noutperforms end-to-end neural methods like RNN and Transformer. The results\nalso indicate the significance of recursive priors for extrapolation on syntax\nand semantics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 01:32:54 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Li", "Qing", ""], ["Huang", "Siyuan", ""], ["Hong", "Yining", ""], ["Zhu", "Yixin", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2103.01415", "submitter": "A S M Mahmudul Hasan", "authors": "A S M Mahmudul Hasan, Ferdous Sohel, Dean Diepeveen, Hamid Laga and\n  Michael G.K. Jones", "title": "A Survey of Deep Learning Techniques for Weed Detection from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid advances in Deep Learning (DL) techniques have enabled rapid\ndetection, localisation, and recognition of objects from images or videos. DL\ntechniques are now being used in many applications related to agriculture and\nfarming. Automatic detection and classification of weeds can play an important\nrole in weed management and so contribute to higher yields. Weed detection in\ncrops from imagery is inherently a challenging problem because both weeds and\ncrops have similar colours ('green-on-green'), and their shapes and texture can\nbe very similar at the growth phase. Also, a crop in one setting can be\nconsidered a weed in another. In addition to their detection, the recognition\nof specific weed species is essential so that targeted controlling mechanisms\n(e.g. appropriate herbicides and correct doses) can be applied. In this paper,\nwe review existing deep learning-based weed detection and classification\ntechniques. We cover the detailed literature on four main procedures, i.e.,\ndata acquisition, dataset preparation, DL techniques employed for detection,\nlocation and classification of weeds in crops, and evaluation metrics\napproaches. We found that most studies applied supervised learning techniques,\nthey achieved high classification accuracy by fine-tuning pre-trained models on\nany plant dataset, and past experiments have already achieved high accuracy\nwhen a large amount of labelled data is available.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:02:24 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hasan", "A S M Mahmudul", ""], ["Sohel", "Ferdous", ""], ["Diepeveen", "Dean", ""], ["Laga", "Hamid", ""], ["Jones", "Michael G. K.", ""]]}, {"id": "2103.01435", "submitter": "Ximeng Sun", "authors": "Ximeng Sun, Rameswar Panda, Chun-Fu Chen, Naigang Wang, Bowen Pan\n  Kailash Gopalakrishnan, Aude Oliva, Rogerio Feris, Kate Saenko", "title": "All at Once Network Quantization via Collaborative Knowledge Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network quantization has rapidly become one of the most widely used methods\nto compress and accelerate deep neural networks on edge devices. While existing\napproaches offer impressive results on common benchmark datasets, they\ngenerally repeat the quantization process and retrain the low-precision network\nfrom scratch, leading to different networks tailored for different resource\nconstraints. This limits scalable deployment of deep networks in many\nreal-world applications, where in practice dynamic changes in bit-width are\noften desired. All at Once quantization addresses this problem, by flexibly\nadjusting the bit-width of a single deep network during inference, without\nrequiring re-training or additional memory to store separate models, for\ninstant adaptation in different scenarios. In this paper, we develop a novel\ncollaborative knowledge transfer approach for efficiently training the\nall-at-once quantization network. Specifically, we propose an adaptive\nselection strategy to choose a high-precision \\enquote{teacher} for\ntransferring knowledge to the low-precision student while jointly optimizing\nthe model with all bit-widths. Furthermore, to effectively transfer knowledge,\nwe develop a dynamic block swapping method by randomly replacing the blocks in\nthe lower-precision student network with the corresponding blocks in the\nhigher-precision teacher network. Extensive experiments on several challenging\nand diverse datasets for both image and video classification well demonstrate\nthe efficacy of our proposed approach over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:09:03 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Sun", "Ximeng", ""], ["Panda", "Rameswar", ""], ["Chen", "Chun-Fu", ""], ["Wang", "Naigang", ""], ["Gopalakrishnan", "Bowen Pan Kailash", ""], ["Oliva", "Aude", ""], ["Feris", "Rogerio", ""], ["Saenko", "Kate", ""]]}, {"id": "2103.01449", "submitter": "Danfeng Hong", "authors": "Danfeng Hong and Wei He and Naoto Yokoya and Jing Yao and Lianru Gao\n  and Liangpei Zhang and Jocelyn Chanussot and Xiao Xiang Zhu", "title": "Interpretable Hyperspectral AI: When Non-Convex Modeling meets\n  Hyperspectral Remote Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral imaging, also known as image spectrometry, is a landmark\ntechnique in geoscience and remote sensing (RS). In the past decade, enormous\nefforts have been made to process and analyze these hyperspectral (HS) products\nmainly by means of seasoned experts. However, with the ever-growing volume of\ndata, the bulk of costs in manpower and material resources poses new challenges\non reducing the burden of manual labor and improving efficiency. For this\nreason, it is, therefore, urgent to develop more intelligent and automatic\napproaches for various HS RS applications. Machine learning (ML) tools with\nconvex optimization have successfully undertaken the tasks of numerous\nartificial intelligence (AI)-related applications. However, their ability in\nhandling complex practical problems remains limited, particularly for HS data,\ndue to the effects of various spectral variabilities in the process of HS\nimaging and the complexity and redundancy of higher dimensional HS signals.\nCompared to the convex models, non-convex modeling, which is capable of\ncharacterizing more complex real scenes and providing the model\ninterpretability technically and theoretically, has been proven to be a\nfeasible solution to reduce the gap between challenging HS vision tasks and\ncurrently advanced intelligent data processing models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:32:10 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hong", "Danfeng", ""], ["He", "Wei", ""], ["Yokoya", "Naoto", ""], ["Yao", "Jing", ""], ["Gao", "Lianru", ""], ["Zhang", "Liangpei", ""], ["Chanussot", "Jocelyn", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2103.01451", "submitter": "Xinchen Liu", "authors": "Xiaodong Chen, Xinchen Liu, Wu Liu, Xiao-Ping Zhang, Yongdong Zhang,\n  and Tao Mei", "title": "AttriMeter: An Attribute-guided Metric Interpreter for Person\n  Re-Identification", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification (ReID) has achieved significant improvement due to\nthe adoption of Convolutional Neural Networks (CNNs). However, person ReID\nsystems only provide a distance or similarity when matching two persons, which\nmakes users hardly understand why they are similar or not. Therefore, we\npropose an Attribute-guided Metric Interpreter, named AttriMeter, to\nsemantically and quantitatively explain the results of CNN-based ReID models.\nThe AttriMeter has a pluggable structure that can be grafted on arbitrary\ntarget models, i.e., the ReID models that need to be interpreted. With an\nattribute decomposition head, it can learn to generate a group of\nattribute-guided attention maps (AAMs) from the target model. By applying AAMs\nto features of two persons from the target model, their distance will be\ndecomposed into a set of attribute-guided components that can measure the\ncontributions of individual attributes. Moreover, we design a distance\ndistillation loss to guarantee the consistency between the results from the\ntarget model and the decomposed components from AttriMeter, and an attribute\nprior loss to eliminate the biases caused by the unbalanced distribution of\nattributes. Finally, extensive experiments and analysis on a variety of ReID\nmodels and datasets show the effectiveness of AttriMeter.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:37:48 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Chen", "Xiaodong", ""], ["Liu", "Xinchen", ""], ["Liu", "Wu", ""], ["Zhang", "Xiao-Ping", ""], ["Zhang", "Yongdong", ""], ["Mei", "Tao", ""]]}, {"id": "2103.01456", "submitter": "Xinyang Li", "authors": "Xinyang Li, Shengchuan Zhang, Jie Hu, Liujuan Cao, Xiaopeng Hong,\n  Xudong Mao, Feiyue Huang, Yongjian Wu, Rongrong Ji", "title": "Image-to-image Translation via Hierarchical Style Disentanglement", "comments": "CVPR 2021. The code will be released at at\n  https://github.com/imlixinyang/HiSD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, image-to-image translation has made significant progress in\nachieving both multi-label (\\ie, translation conditioned on different labels)\nand multi-style (\\ie, generation with diverse styles) tasks. However, due to\nthe unexplored independence and exclusiveness in the labels, existing endeavors\nare defeated by involving uncontrolled manipulations to the translation\nresults. In this paper, we propose Hierarchical Style Disentanglement (HiSD) to\naddress this issue. Specifically, we organize the labels into a hierarchical\ntree structure, in which independent tags, exclusive attributes, and\ndisentangled styles are allocated from top to bottom. Correspondingly, a new\ntranslation process is designed to adapt the above structure, in which the\nstyles are identified for controllable translations. Both qualitative and\nquantitative results on the CelebA-HQ dataset verify the ability of the\nproposed HiSD. We hope our method will serve as a solid baseline and provide\nfresh insights with the hierarchically organized annotations for future\nresearch in image-to-image translation. The code has been released at\nhttps://github.com/imlixinyang/HiSD.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:43:18 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Li", "Xinyang", ""], ["Zhang", "Shengchuan", ""], ["Hu", "Jie", ""], ["Cao", "Liujuan", ""], ["Hong", "Xiaopeng", ""], ["Mao", "Xudong", ""], ["Huang", "Feiyue", ""], ["Wu", "Yongjian", ""], ["Ji", "Rongrong", ""]]}, {"id": "2103.01458", "submitter": "Shitong Luo", "authors": "Shitong Luo, Wei Hu", "title": "Diffusion Probabilistic Models for 3D Point Cloud Generation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model for point cloud generation, which is\nfundamental for various 3D vision tasks such as shape completion, upsampling,\nsynthesis and data augmentation. Inspired by the diffusion process in\nnon-equilibrium thermodynamics, we view points in point clouds as particles in\na thermodynamic system in contact with a heat bath, which diffuse from the\noriginal distribution to a noise distribution. Point cloud generation thus\namounts to learning the reverse diffusion process that transforms the noise\ndistribution to the distribution of a desired shape. Specifically, we propose\nto model the reverse diffusion process for point clouds as a Markov chain\nconditioned on certain shape latent. We derive the variational bound in closed\nform for training and provide implementations of the model. Experimental\nresults demonstrate that our model achieves competitive performance in point\ncloud generation and auto-encoding. The code is available at\n\\url{https://github.com/luost26/diffusion-point-cloud}.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:56:02 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 04:47:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Luo", "Shitong", ""], ["Hu", "Wei", ""]]}, {"id": "2103.01468", "submitter": "Brent Griffin Dr", "authors": "Brent A. Griffin and Jason J. Corso", "title": "Depth from Camera Motion and Object Detection", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of learning to estimate the depth of\ndetected objects given some measurement of camera motion (e.g., from robot\nkinematics or vehicle odometry). We achieve this by 1) designing a recurrent\nneural network (DBox) that estimates the depth of objects using a generalized\nrepresentation of bounding boxes and uncalibrated camera movement and 2)\nintroducing the Object Depth via Motion and Detection Dataset (ODMD). ODMD\ntraining data are extensible and configurable, and the ODMD benchmark includes\n21,600 examples across four validation and test sets. These sets include mobile\nrobot experiments using an end-effector camera to locate objects from the YCB\ndataset and examples with perturbations added to camera motion or bounding box\ndata. In addition to the ODMD benchmark, we evaluate DBox in other monocular\napplication domains, achieving state-of-the-art results on existing driving and\nrobotics benchmarks and estimating the depth of objects using a camera phone.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 04:43:17 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Griffin", "Brent A.", ""], ["Corso", "Jason J.", ""]]}, {"id": "2103.01486", "submitter": "Stephen Hausler", "authors": "Stephen Hausler, Sourav Garg, Ming Xu, Michael Milford, Tobias Fischer", "title": "Patch-NetVLAD: Multi-Scale Fusion of Locally-Global Descriptors for\n  Place Recognition", "comments": "Accepted to IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition is a challenging task for robotics and autonomous\nsystems, which must deal with the twin problems of appearance and viewpoint\nchange in an always changing world. This paper introduces Patch-NetVLAD, which\nprovides a novel formulation for combining the advantages of both local and\nglobal descriptor methods by deriving patch-level features from NetVLAD\nresiduals. Unlike the fixed spatial neighborhood regime of existing local\nkeypoint features, our method enables aggregation and matching of deep-learned\nlocal features defined over the feature-space grid. We further introduce a\nmulti-scale fusion of patch features that have complementary scales (i.e. patch\nsizes) via an integral feature space and show that the fused features are\nhighly invariant to both condition (season, structure, and illumination) and\nviewpoint (translation and rotation) changes. Patch-NetVLAD outperforms both\nglobal and local feature descriptor-based methods with comparable compute,\nachieving state-of-the-art visual place recognition results on a range of\nchallenging real-world datasets, including winning the Facebook Mapillary\nVisual Place Recognition Challenge at ECCV2020. It is also adaptable to user\nrequirements, with a speed-optimised version operating over an order of\nmagnitude faster than the state-of-the-art. By combining superior performance\nwith improved computational efficiency in a configurable framework,\nPatch-NetVLAD is well suited to enhance both stand-alone place recognition\ncapabilities and the overall performance of SLAM systems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 05:53:32 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hausler", "Stephen", ""], ["Garg", "Sourav", ""], ["Xu", "Ming", ""], ["Milford", "Michael", ""], ["Fischer", "Tobias", ""]]}, {"id": "2103.01498", "submitter": "Philipp Benz", "authors": "Chaoning Zhang, Philipp Benz, Chenguo Lin, Adil Karjauv, Jing Wu, In\n  So Kweon", "title": "A Survey On Universal Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have demonstrated remarkable performance for\nvarious applications, meanwhile, they are widely known to be vulnerable to the\nattack of adversarial perturbations. This intriguing phenomenon has attracted\nsignificant attention in machine learning and what might be more surprising to\nthe community is the existence of universal adversarial perturbations (UAPs),\ni.e. a single perturbation to fool the target DNN for most images. The\nadvantage of UAP is that it can be generated beforehand and then be applied\non-the-fly during the attack. With the focus on UAP against deep classifiers,\nthis survey summarizes the recent progress on universal adversarial attacks,\ndiscussing the challenges from both the attack and defense sides, as well as\nthe reason for the existence of UAP. Additionally, universal attacks in a wide\nrange of applications beyond deep classification are also covered.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:35:09 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhang", "Chaoning", ""], ["Benz", "Philipp", ""], ["Lin", "Chenguo", ""], ["Karjauv", "Adil", ""], ["Wu", "Jing", ""], ["Kweon", "In So", ""]]}, {"id": "2103.01501", "submitter": "Hyunjun Lim", "authors": "Hyunjun Lim, Yeeun Kim, Kwangik Jung, Sumin Hu, and Hyun Myung", "title": "Avoiding Degeneracy for Monocular Visual SLAM with Point and Line\n  Features", "comments": "8 pages to be published in ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a degeneracy avoidance method for a point and line based\nvisual SLAM algorithm is proposed. Visual SLAM predominantly uses point\nfeatures. However, point features lack robustness in low texture and\nilluminance variant environments. Therefore, line features are used to\ncompensate the weaknesses of point features. In addition, point features are\npoor in representing discernable features for the naked eye, meaning mapped\npoint features cannot be recognized. To overcome the limitations above, line\nfeatures were actively employed in previous studies. However, since degeneracy\narises in the process of using line features, this paper attempts to solve this\nproblem. First, a simple method to identify degenerate lines is presented. In\naddition, a novel structural constraint is proposed to avoid the degeneracy\nproblem. At last, a point and line based monocular SLAM system using a robust\noptical-flow based lien tracking method is implemented. The results are\nverified using experiments with the EuRoC dataset and compared with other\nstate-of-the-art algorithms. It is proven that our method yields more accurate\nlocalization as well as mapping results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:41:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Lim", "Hyunjun", ""], ["Kim", "Yeeun", ""], ["Jung", "Kwangik", ""], ["Hu", "Sumin", ""], ["Myung", "Hyun", ""]]}, {"id": "2103.01520", "submitter": "Zhizhong Huang", "authors": "Zhizhong Huang, Junping Zhang, Hongming Shan", "title": "When Age-Invariant Face Recognition Meets Face Age Synthesis: A\n  Multi-Task Learning Framework", "comments": "accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To minimize the effects of age variation in face recognition, previous work\neither extracts identity-related discriminative features by minimizing the\ncorrelation between identity- and age-related features, called age-invariant\nface recognition (AIFR), or removes age variation by transforming the faces of\ndifferent age groups into the same age group, called face age synthesis (FAS);\nhowever, the former lacks visual results for model interpretation while the\nlatter suffers from artifacts compromising downstream recognition. Therefore,\nthis paper proposes a unified, multi-task framework to jointly handle these two\ntasks, termed MTLFace, which can learn age-invariant identity-related\nrepresentation while achieving pleasing face synthesis. Specifically, we first\ndecompose the mixed face feature into two uncorrelated components -- identity-\nand age-related feature -- through an attention mechanism, and then decorrelate\nthese two components using multi-task training and continuous domain adaption.\nIn contrast to the conventional one-hot encoding that achieves group-level FAS,\nwe propose a novel identity conditional module to achieve identity-level FAS,\nwith a weight-sharing strategy to improve the age smoothness of synthesized\nfaces. In addition, we collect and release a large cross-age face dataset with\nage and gender annotations to advance the development of the AIFR and FAS.\nExtensive experiments on five benchmark cross-age datasets demonstrate the\nsuperior performance of our proposed MTLFace over existing state-of-the-art\nmethods for AIFR and FAS. We further validate MTLFace on two popular general\nface recognition datasets, showing competitive performance for face recognition\nin the wild. The source code and dataset are available\nat~\\url{https://github.com/Hzzone/MTLFace}.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:03:27 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 02:58:47 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Huang", "Zhizhong", ""], ["Zhang", "Junping", ""], ["Shan", "Hongming", ""]]}, {"id": "2103.01524", "submitter": "Lucas Young", "authors": "Lucas D. Young, Fitsum A. Reda, Rakesh Ranjan, Jon Morton, Jun Hu,\n  Yazhu Ling, Xiaoyu Xiang, David Liu, Vikas Chandra", "title": "Feature-Align Network with Knowledge Distillation for Efficient\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient neural network for RAW image denoising. Although\nneural network-based denoising has been extensively studied for image\nrestoration, little attention has been given to efficient denoising for compute\nlimited and power sensitive devices, such as smartphones and smartwatches. In\nthis paper, we present a novel architecture and a suite of training techniques\nfor high quality denoising in mobile devices. Our work is distinguished by\nthree main contributions. (1) Feature-Align layer that modulates the\nactivations of an encoder-decoder architecture with the input noisy images. The\nauto modulation layer enforces attention to spatially varying noise that tend\nto be \"washed away\" by successive application of convolutions and\nnon-linearity. (2) A novel Feature Matching Loss that allows knowledge\ndistillation from large denoising networks in the form of a perceptual content\nloss. (3) Empirical analysis of our efficient model trained to specialize on\ndifferent noise subranges. This opens additional avenue for model size\nreduction by sacrificing memory for compute. Extensive experimental validation\nshows that our efficient model produces high quality denoising results that\ncompete with state-of-the-art large networks, while using significantly fewer\nparameters and MACs. On the Darmstadt Noise Dataset benchmark, we achieve a\nPSNR of 48.28dB, while using 263 times fewer MACs, and 17.6 times fewer\nparameters than the state-of-the-art network, which achieves 49.12dB.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:09:32 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 03:13:00 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Young", "Lucas D.", ""], ["Reda", "Fitsum A.", ""], ["Ranjan", "Rakesh", ""], ["Morton", "Jon", ""], ["Hu", "Jun", ""], ["Ling", "Yazhu", ""], ["Xiang", "Xiaoyu", ""], ["Liu", "David", ""], ["Chandra", "Vikas", ""]]}, {"id": "2103.01530", "submitter": "Yuanxin Wu", "authors": "Qi Cai, Lilian Zhang, Yuanxin Wu, Wenxian Yu, Dewen Hu", "title": "A Pose-only Solution to Visual Reconstruction and Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual navigation and three-dimensional (3D) scene reconstruction are\nessential for robotics to interact with the surrounding environment.\nLarge-scale scenes and critical camera motions are great challenges facing the\nresearch community to achieve this goal. We raised a pose-only imaging geometry\nframework and algorithms that can help solve these challenges. The\nrepresentation is a linear function of camera global translations, which allows\nfor efficient and robust camera motion estimation. As a result, the spatial\nfeature coordinates can be analytically reconstructed and do not require\nnonlinear optimization. Experiments demonstrate that the computational\nefficiency of recovering the scene and associated camera poses is significantly\nimproved by 2-4 orders of magnitude. This solution might be promising to unlock\nreal-time 3D visual computing in many forefront applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:21:08 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 14:49:32 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Cai", "Qi", ""], ["Zhang", "Lilian", ""], ["Wu", "Yuanxin", ""], ["Yu", "Wenxian", ""], ["Hu", "Dewen", ""]]}, {"id": "2103.01537", "submitter": "Minki Jeong", "authors": "Minki Jeong, Seokeon Choi, Changick Kim", "title": "Few-shot Open-set Recognition by Transformation Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attack a few-shot open-set recognition (FSOSR) problem,\nwhich is a combination of few-shot learning (FSL) and open-set recognition\n(OSR). It aims to quickly adapt a model to a given small set of labeled samples\nwhile rejecting unseen class samples. Since OSR requires rich data and FSL\nconsiders closed-set classification, existing OSR and FSL methods show poor\nperformances in solving FSOSR problems. The previous FSOSR method follows the\npseudo-unseen class sample-based methods, which collect pseudo-unseen samples\nfrom the other dataset or synthesize samples to model unseen class\nrepresentations. However, this approach is heavily dependent on the composition\nof the pseudo samples. In this paper, we propose a novel unknown class sample\ndetector, named SnaTCHer, that does not require pseudo-unseen samples. Based on\nthe transformation consistency, our method measures the difference between the\ntransformed prototypes and a modified prototype set. The modified set is\ncomposed by replacing a query feature and its predicted class prototype.\nSnaTCHer rejects samples with large differences to the transformed prototypes.\nOur method alters the unseen class distribution estimation problem to a\nrelative feature transformation problem, independent of pseudo-unseen class\nsamples. We investigate our SnaTCHer with various prototype transformation\nmethods and observe that our method consistently improves unseen class sample\ndetection performance without closed-set classification reduction.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:37:17 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 10:53:40 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jeong", "Minki", ""], ["Choi", "Seokeon", ""], ["Kim", "Changick", ""]]}, {"id": "2103.01542", "submitter": "Bingyan Liu", "authors": "Bingyan Liu, Yifeng Cai, Yao Guo, Xiangqun Chen", "title": "TransTailor: Pruning the Pre-trained Model for Improved Transfer\n  Learning", "comments": "This paper has been accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing of pre-trained models has significantly facilitated the\nperformance on limited data tasks with transfer learning. However, progress on\ntransfer learning mainly focuses on optimizing the weights of pre-trained\nmodels, which ignores the structure mismatch between the model and the target\ntask. This paper aims to improve the transfer performance from another angle -\nin addition to tuning the weights, we tune the structure of pre-trained models,\nin order to better match the target task. To this end, we propose TransTailor,\ntargeting at pruning the pre-trained model for improved transfer learning.\nDifferent from traditional pruning pipelines, we prune and fine-tune the\npre-trained model according to the target-aware weight importance, generating\nan optimal sub-model tailored for a specific target task. In this way, we\ntransfer a more suitable sub-structure that can be applied during fine-tuning\nto benefit the final performance. Extensive experiments on multiple pre-trained\nmodels and datasets demonstrate that TransTailor outperforms the traditional\npruning methods and achieves competitive or even better performance than other\nstate-of-the-art transfer learning methods while using a smaller model.\nNotably, on the Stanford Dogs dataset, TransTailor can achieve 2.7% accuracy\nimprovement over other transfer methods with 20% fewer FLOPs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:58:35 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Liu", "Bingyan", ""], ["Cai", "Yifeng", ""], ["Guo", "Yao", ""], ["Chen", "Xiangqun", ""]]}, {"id": "2103.01546", "submitter": "Meiling Fang", "authors": "Meiling Fang, Naser Damer, Florian Kirchbuchner, Arjan Kuijper", "title": "Real Masks and Fake Faces: On the Masked Face Presentation Attack\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing COVID-19 pandemic has lead to massive public health issues. Face\nmasks have become one of the most efficient ways to reduce coronavirus\ntransmission. This makes face recognition (FR) a challenging task as several\ndiscriminative features are hidden. Moreover, face presentation attack\ndetection (PAD) is crucial to ensure the security of FR systems. In contrast to\ngrowing numbers of masked FR studies, the impact of masked attacks on PAD has\nnot been explored. Therefore, we present novel attacks with real masks placed\non presentations and attacks with subjects wearing masks to reflect the current\nreal-world situation. Furthermore, this study investigates the effect of masked\nattacks on PAD performance by using seven state-of-the-art PAD algorithms under\nintra- and cross-database scenarios. We also evaluate the vulnerability of FR\nsystems on masked attacks. The experiments show that real masked attacks pose a\nserious threat to the operation and security of FR systems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:05:50 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Fang", "Meiling", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2103.01559", "submitter": "Jiaheng Liu", "authors": "Jiaheng Liu, Yudong Wu, Yichao Wu, Zhenmao Li, Chen Ken, Ding Liang,\n  Junjie Yan", "title": "Inter-class Discrepancy Alignment for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of face recognition (FR) has witnessed great progress with the\nsurge of deep learning. Existing methods mainly focus on extracting\ndiscriminative features, and directly compute the cosine or L2 distance by the\npoint-to-point way without considering the context information. In this study,\nwe make a key observation that the local con-text represented by the\nsimilarities between the instance and its inter-class neighbors1plays an\nimportant role forFR. Specifically, we attempt to incorporate the local\nin-formation in the feature space into the metric, and pro-pose a unified\nframework calledInter-class DiscrepancyAlignment(IDA), with two dedicated\nmodules, Discrepancy Alignment Operator(IDA-DAO) andSupport Set\nEstimation(IDA-SSE). IDA-DAO is used to align the similarity scores considering\nthe discrepancy between the images and its neighbors, which is defined by\nadaptive support sets on the hypersphere. For practical inference, it is\ndifficult to acquire support set during online inference. IDA-SSE can provide\nconvincing inter-class neighbors by introducing virtual candidate images\ngenerated with GAN. Further-more, we propose the learnable IDA-SSE, which can\nimplicitly give estimation without the need of any other images in the\nevaluation process. The proposed IDA can be incorporated into existing FR\nsystems seamlessly and efficiently. Extensive experiments demonstrate that this\nframe-work can 1) significantly improve the accuracy, and 2) make the model\nrobust to the face images of various distributions.Without bells and whistles,\nour method achieves state-of-the-art performance on multiple standard FR\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:20:08 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Liu", "Jiaheng", ""], ["Wu", "Yudong", ""], ["Wu", "Yichao", ""], ["Li", "Zhenmao", ""], ["Ken", "Chen", ""], ["Liang", "Ding", ""], ["Yan", "Junjie", ""]]}, {"id": "2103.01566", "submitter": "Olcay Kursun", "authors": "Olcay Kursun, Semih Dinc, Oleg V. Favorov", "title": "Contextually Guided Convolutional Neural Networks for Learning Most\n  Transferable Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs), trained extensively on very large\nlabeled datasets, learn to recognize inferentially powerful features in their\ninput patterns and represent efficiently their objective content. Such\nobjectivity of their internal representations enables deep CNNs to readily\ntransfer and successfully apply these representations to new classification\ntasks. Deep CNNs develop their internal representations through a challenging\nprocess of error backpropagation-based supervised training. In contrast, deep\nneural networks of the cerebral cortex develop their even more powerful\ninternal representations in an unsupervised process, apparently guided at a\nlocal level by contextual information. Implementing such local contextual\nguidance principles in a single-layer CNN architecture, we propose an efficient\nalgorithm for developing broad-purpose representations (i.e., representations\ntransferable to new tasks without additional training) in shallow CNNs trained\non limited-size datasets. A contextually guided CNN (CG-CNN) is trained on\ngroups of neighboring image patches picked at random image locations in the\ndataset. Such neighboring patches are likely to have a common context and\ntherefore are treated for the purposes of training as belonging to the same\nclass. Across multiple iterations of such training on different context-sharing\ngroups of image patches, CNN features that are optimized in one iteration are\nthen transferred to the next iteration for further optimization, etc. In this\nprocess, CNN features acquire higher pluripotency, or inferential utility for\nany arbitrary classification task, which we quantify as a transfer utility. In\nour application to natural images, we find that CG-CNN features show the same,\nif not higher, transfer utility and classification accuracy as comparable\ntransferable features in the first CNN layer of the well-known deep networks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:41:12 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 21:19:00 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kursun", "Olcay", ""], ["Dinc", "Semih", ""], ["Favorov", "Oleg V.", ""]]}, {"id": "2103.01584", "submitter": "Shann-Ching Chen", "authors": "Feng-Yu Liu, Chih-Chi Chen, Shann-Ching Chen, Chien-Hung Liao", "title": "A Practical Framework for ROI Detection in Medical Images -- a case\n  study for hip detection in anteroposterior pelvic radiographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose Automated detection of region of interest (ROI) is a critical step\nfor many medical image applications such as heart ROIs detection in perfusion\nMRI images, lung boundary detection in chest X-rays, and femoral head detection\nin pelvic radiographs. Thus, we proposed a practical framework of ROIs\ndetection in medical images, with a case study for hip detection in\nanteroposterior (AP) pelvic radiographs.\n  Materials and Methods: We conducted a retrospective study which analyzed hip\njoints seen on 7,399 AP pelvic radiographs from three diverse sources,\nincluding 4,290 high resolution radiographs from Chang Gung Memorial Hospital\nOsteoarthritis, 3,008 low to medium resolution radiographs from Osteoarthritis\nInitiative, and 101 heterogeneous radiographs from Google image search engine.\nWe presented a deep learning-based ROI detection framework utilizing\nsingle-shot multi-box detector (SSD) with ResNet-101 backbone and customized\nhead structure based on the characteristics of the obtained datasets, whose\nground truths were labeled by non-medical annotators in a simple graphical\ninterface.\n  Results: Our method achieved average intersection over union (IoU)=0.8115,\naverage confidence=0.9812, and average precision with threshold IoU=0.5\n(AP50)=0.9901 in the independent test set, suggesting that the detected hip\nregions have appropriately covered main features of the hip joints.\n  Conclusion: The proposed approach featured on low-cost labeling, data-driven\nmodel design, and heterogeneous data testing. We have demonstrated the\nfeasibility of training a robust hip region detector for AP pelvic radiographs.\nThis practical framework has a promising potential for a wide range of medical\nimage applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:21:08 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Liu", "Feng-Yu", ""], ["Chen", "Chih-Chi", ""], ["Chen", "Shann-Ching", ""], ["Liao", "Chien-Hung", ""]]}, {"id": "2103.01592", "submitter": "Philipp Terh\\\"orst", "authors": "Philipp Terh\\\"orst, Jan Niklas Kolf, Marco Huber, Florian\n  Kirchbuchner, Naser Damer, Aythami Morales, Julian Fierrez, Arjan Kuijper", "title": "A Comprehensive Study on Face Recognition Biases Beyond Demographics", "comments": "Under review in IEEE Transactions on Technology and Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face recognition (FR) systems have a growing effect on critical\ndecision-making processes. Recent works have shown that FR solutions show\nstrong performance differences based on the user's demographics. However, to\nenable a trustworthy FR technology, it is essential to know the influence of an\nextended range of facial attributes on FR beyond demographics. Therefore, in\nthis work, we analyse FR bias over a wide range of attributes. We investigate\nthe influence of 47 attributes on the verification performance of two popular\nFR models. The experiments were performed on the publicly available MAADFace\nattribute database with over 120M high-quality attribute annotations. To\nprevent misleading statements about biased performances, we introduced control\ngroup based validity values to decide if unbalanced test data causes the\nperformance differences. The results demonstrate that also many non-demographic\nattributes strongly affect the recognition performance, such as accessories,\nhair-styles and colors, face shapes, or facial anomalies. The observations of\nthis work show the strong need for further advances in making FR system more\nrobust, explainable, and fair. Moreover, our findings might help to a better\nunderstanding of how FR networks work, to enhance the robustness of these\nnetworks, and to develop more generalized bias-mitigating face recognition\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:29:09 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Terh\u00f6rst", "Philipp", ""], ["Kolf", "Jan Niklas", ""], ["Huber", "Marco", ""], ["Kirchbuchner", "Florian", ""], ["Damer", "Naser", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2103.01593", "submitter": "Manish Sahu", "authors": "Manish Sahu, Anirban Mukhopadhyay, Stefan Zachow", "title": "Simulation-to-Real domain adaptation with teacher-student learning for\n  endoscopic instrument segmentation", "comments": "Accepted at IPCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Purpose: Segmentation of surgical instruments in endoscopic videos is\nessential for automated surgical scene understanding and process modeling.\nHowever, relying on fully supervised deep learning for this task is challenging\nbecause manual annotation occupies valuable time of the clinical experts.\n  Methods: We introduce a teacher-student learning approach that learns jointly\nfrom annotated simulation data and unlabeled real data to tackle the erroneous\nlearning problem of the current consistency-based unsupervised domain\nadaptation framework.\n  Results: Empirical results on three datasets highlight the effectiveness of\nthe proposed framework over current approaches for the endoscopic instrument\nsegmentation task. Additionally, we provide analysis of major factors affecting\nthe performance on all datasets to highlight the strengths and failure modes of\nour approach.\n  Conclusion: We show that our proposed approach can successfully exploit the\nunlabeled real endoscopic video frames and improve generalization performance\nover pure simulation-based training and the previous state-of-the-art. This\ntakes us one step closer to effective segmentation of surgical tools in the\nannotation scarce setting.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:30:28 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Sahu", "Manish", ""], ["Mukhopadhyay", "Anirban", ""], ["Zachow", "Stefan", ""]]}, {"id": "2103.01623", "submitter": "Yujiao Shi", "authors": "Yujiao Shi, Dylan Campbell, Xin Yu, Hongdong Li", "title": "Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery", "comments": "submitted to TPAMI in Aug, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for synthesizing a novel street-view\npanorama given an overhead satellite image. Taking a small satellite image\npatch as input, our method generates a Google's omnidirectional street-view\ntype panorama, as if it is captured from the same geographical location as the\ncenter of the satellite patch. Existing works tackle this task as an image\ngeneration problem which adopts generative adversarial networks to implicitly\nlearn the cross-view transformations, while ignoring the domain relevance. In\nthis paper, we propose to explicitly establish the geometric correspondences\nbetween the two-view images so as to facilitate the cross-view transformation\nlearning. Specifically, we observe that when a 3D point in the real world is\nvisible in both views, there is a deterministic mapping between the projected\npoints in the two-view images given the height information of this 3D point.\nMotivated by this, we develop a novel Satellite to Street-view image Projection\n(S2SP) module which explicitly establishes such geometric correspondences and\nprojects the satellite images to the street viewpoint. With these projected\nsatellite images as network input, we next employ a generator to synthesize\nrealistic street-view panoramas that are geometrically consistent with the\nsatellite images. Our S2SP module is differentiable and the whole framework is\ntrained in an end-to-end manner. Extensive experimental results on two\ncross-view benchmark datasets demonstrate that our method generates images that\nbetter respect the scene geometry than existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:27:05 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 10:36:23 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 06:33:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shi", "Yujiao", ""], ["Campbell", "Dylan", ""], ["Yu", "Xin", ""], ["Li", "Hongdong", ""]]}, {"id": "2103.01624", "submitter": "Lu Xu", "authors": "Lu Xu, Jiawei Zhang, Xuanye Cheng, Feng Zhang, Xing Wei, Jimmy Ren", "title": "Efficient Deep Image Denoising via Class Specific Convolution", "comments": "The Thirty-Fifth AAAI Conference on Artificial Intelligence(AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks have been widely used in image denoising during the past\nfew years. Even though they achieve great success on this problem, they are\ncomputationally inefficient which makes them inappropriate to be implemented in\nmobile devices. In this paper, we propose an efficient deep neural network for\nimage denoising based on pixel-wise classification. Despite using a\ncomputationally efficient network cannot effectively remove the noises from any\ncontent, it is still capable to denoise from a specific type of pattern or\ntexture. The proposed method follows such a divide and conquer scheme. We first\nuse an efficient U-net to pixel-wisely classify pixels in the noisy image based\non the local gradient statistics. Then we replace part of the convolution\nlayers in existing denoising networks by the proposed Class Specific\nConvolution layers (CSConv) which use different weights for different classes\nof pixels. Quantitative and qualitative evaluations on public datasets\ndemonstrate that the proposed method can reduce the computational costs without\nsacrificing the performance compared to state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:28:15 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Xu", "Lu", ""], ["Zhang", "Jiawei", ""], ["Cheng", "Xuanye", ""], ["Zhang", "Feng", ""], ["Wei", "Xing", ""], ["Ren", "Jimmy", ""]]}, {"id": "2103.01632", "submitter": "Babak Maser MSc", "authors": "Babak Maser, Andreas Uhl", "title": "Using CNNs to Identify the Origin of Finger Vein Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the finger vein (FV) sensor model identification task using a deep\nlearning approach. So far, for this biometric modality, only correlation-based\nPRNU and texture descriptor-based methods have been applied. We employ five\nprominent CNN architectures covering a wide range of CNN family models,\nincluding VGG16, ResNet, and the Xception model. In addition, a novel\narchitecture termed FV2021 is proposed in this work, which excels by its\ncompactness and a low number of parameters to be trained. Original samples, as\nwell as the region of interest data from eight publicly accessible FV datasets,\nare used in experimentation. An excellent sensor identification AUC-ROC score\nof 1.0 for patches of uncropped samples and 0.9997 for ROI samples have been\nachieved. The comparison with former methods shows that the CNN-based approach\nis superior and improved the results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:43:52 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Maser", "Babak", ""], ["Uhl", "Andreas", ""]]}, {"id": "2103.01634", "submitter": "Nicola Strisciuglio", "authors": "Nicola Strisciuglio", "title": "Brain-inspired algorithms for processing of visual data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of the visual system of the brain has attracted the attention and\ninterest of many neuro-scientists, that derived computational models of some\ntypes of neuron that compose it. These findings inspired researchers in image\nprocessing and computer vision to deploy such models to solve problems of\nvisual data processing. In this paper, we review approaches for image\nprocessing and computer vision, the design of which is based on\nneuro-scientific findings about the functions of some neurons in the visual\ncortex. Furthermore, we analyze the connection between the hierarchical\norganization of the visual system of the brain and the structure of\nConvolutional Networks (ConvNets). We pay particular attention to the\nmechanisms of inhibition of the responses of some neurons, which provide the\nvisual system with improved stability to changing input stimuli, and discuss\ntheir implementation in image processing operators and in ConvNets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:45:38 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Strisciuglio", "Nicola", ""]]}, {"id": "2103.01644", "submitter": "Albert Dulian", "authors": "Albert Dulian and John C. Murray", "title": "Exploiting latent representation of sparse semantic layers for improved\n  short-term motion prediction with Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As urban environments manifest high levels of complexity it is of vital\nimportance that safety systems embedded within autonomous vehicles (AVs) are\nable to accurately anticipate short-term future motion of nearby agents. This\nproblem can be further understood as generating a sequence of coordinates\ndescribing the future motion of the tracked agent. Various proposed approaches\ndemonstrate significant benefits of using a rasterised top-down image of the\nroad, with a combination of Convolutional Neural Networks (CNNs), for\nextraction of relevant features that define the road structure (eg. driveable\nareas, lanes, walkways). In contrast, this paper explores use of Capsule\nNetworks (CapsNets) in the context of learning a hierarchical representation of\nsparse semantic layers corresponding to small regions of the High-Definition\n(HD) map. Each region of the map is dismantled into separate geometrical layers\nthat are extracted with respect to the agent's current position. By using an\narchitecture based on CapsNets the model is able to retain hierarchical\nrelationships between detected features within images whilst also preventing\nloss of spatial data often caused by the pooling operation. We train and\nevaluate our model on publicly available dataset nuTonomy scenes and compare it\nto recently published methods. We show that our model achieves significant\nimprovement over recently published works on deterministic prediction, whilst\ndrastically reducing the overall size of the network.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:13:43 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 19:41:08 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 20:40:24 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Dulian", "Albert", ""], ["Murray", "John C.", ""]]}, {"id": "2103.01648", "submitter": "Mario Gonz\\'alez", "authors": "Mario Gonz\\'alez, Andr\\'es Almansa, Pauline Tan", "title": "Solving Inverse Problems by Joint Posterior Maximization with\n  Autoencoding Prior", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.06379", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG eess.IV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of solving ill-posed inverse problems in\nimaging where the prior is a variational autoencoder (VAE). Specifically we\nconsider the decoupled case where the prior is trained once and can be reused\nfor many different log-concave degradation models without retraining. Whereas\nprevious MAP-based approaches to this problem lead to highly non-convex\noptimization algorithms, our approach computes the joint (space-latent) MAP\nthat naturally leads to alternate optimization algorithms and to the use of a\nstochastic encoder to accelerate computations. The resulting technique (JPMAP)\nperforms Joint Posterior Maximization using an Autoencoding Prior. We show\ntheoretical and experimental evidence that the proposed objective function is\nquite close to bi-convex. Indeed it satisfies a weak bi-convexity property\nwhich is sufficient to guarantee that our optimization scheme converges to a\nstationary point. We also highlight the importance of correctly training the\nVAE using a denoising criterion, in order to ensure that the encoder\ngeneralizes well to out-of-distribution images, without affecting the quality\nof the generative model. This simple modification is key to providing\nrobustness to the whole procedure. Finally we show how our joint MAP\nmethodology relates to more common MAP approaches, and we propose a\ncontinuation scheme that makes use of our JPMAP algorithm to provide more\nrobust MAP estimates. Experimental results also show the higher quality of the\nsolutions obtained by our JPMAP approach with respect to other non-convex MAP\napproaches which more often get stuck in spurious local optima.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:18:34 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 13:45:36 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Gonz\u00e1lez", "Mario", ""], ["Almansa", "Andr\u00e9s", ""], ["Tan", "Pauline", ""]]}, {"id": "2103.01649", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Rongmei Lin, Zhen Liu, Li Xiong, Bernhard Sch\\\"olkopf,\n  Adrian Weller", "title": "Learning with Hyperspherical Uniformity", "comments": "AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the over-parameterization nature, neural networks are a powerful tool\nfor nonlinear function approximation. In order to achieve good generalization\non unseen data, a suitable inductive bias is of great importance for neural\nnetworks. One of the most straightforward ways is to regularize the neural\nnetwork with some additional objectives. L2 regularization serves as a standard\nregularization for neural networks. Despite its popularity, it essentially\nregularizes one dimension of the individual neuron, which is not strong enough\nto control the capacity of highly over-parameterized neural networks. Motivated\nby this, hyperspherical uniformity is proposed as a novel family of relational\nregularizations that impact the interaction among neurons. We consider several\ngeometrically distinct ways to achieve hyperspherical uniformity. The\neffectiveness of hyperspherical uniformity is justified by theoretical insights\nand empirical evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:20:30 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Liu", "Weiyang", ""], ["Lin", "Rongmei", ""], ["Liu", "Zhen", ""], ["Xiong", "Li", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Weller", "Adrian", ""]]}, {"id": "2103.01654", "submitter": "Guanyu Cai", "authors": "Guanyu Cai, Xinyang Jiang, Jun Zhang, Yifei Gong, Lianghua He, Pai\n  Peng, Xiaowei Guo, Xing Sun", "title": "Part2Whole: Iteratively Enrich Detail for Cross-Modal Retrieval with\n  Partial Query", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based image retrieval has seen considerable progress in recent years.\nHowever, the performance of existing methods suffers in real life since the\nuser is likely to provide an incomplete description of a complex scene, which\noften leads to results filled with false positives that fit the incomplete\ndescription. In this work, we introduce the partial-query problem and\nextensively analyze its influence on text-based image retrieval. We then\npropose an interactive retrieval framework called Part2Whole to tackle this\nproblem by iteratively enriching the missing details. Specifically, an\nInteractive Retrieval Agent is trained to build an optimal policy to refine the\ninitial query based on a user-friendly interaction and statistical\ncharacteristics of the gallery. Compared to other dialog-based methods that\nrely heavily on the user to feed back differentiating information, we let AI\ntake over the optimal feedback searching process and hint the user with\nconfirmation-based questions about details. Furthermore, since fully-supervised\ntraining is often infeasible due to the difficulty of obtaining human-machine\ndialog data, we present a weakly-supervised reinforcement learning method that\nneeds no human-annotated data other than the text-image dataset. Experiments\nshow that our framework significantly improves the performance of text-based\nimage retrieval under complex scenes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:27:05 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Cai", "Guanyu", ""], ["Jiang", "Xinyang", ""], ["Zhang", "Jun", ""], ["Gong", "Yifei", ""], ["He", "Lianghua", ""], ["Peng", "Pai", ""], ["Guo", "Xiaowei", ""], ["Sun", "Xing", ""]]}, {"id": "2103.01695", "submitter": "Wadii Boulila Prof.", "authors": "Wadii Boulila, Hamza Ghandorh, Mehshan Ahmed Khan, Fawad Ahmed, Jawad\n  Ahmad", "title": "A Novel CNN-LSTM-based Approach to Predict Urban Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-series remote sensing data offer a rich source of information that can\nbe used in a wide range of applications, from monitoring changes in land cover\nto surveilling crops, coastal changes, flood risk assessment, and urban sprawl.\nThis paper addresses the challenge of using time-series satellite images to\npredict urban expansion. Building upon previous work, we propose a novel\ntwo-step approach based on semantic image segmentation in order to predict\nurban expansion. The first step aims to extract information about urban regions\nat different time scales and prepare them for use in the training step. The\nsecond step combines Convolutional Neural Networks (CNN) with Long Short Term\nMemory (LSTM) methods in order to learn temporal features and thus predict\nurban expansion. In this paper, experimental results are conducted using\nseveral multi-date satellite images representing the three largest cities in\nSaudi Arabia, namely: Riyadh, Jeddah, and Dammam. We empirically evaluated our\nproposed technique, and examined its results by comparing them with\nstate-of-the-art approaches. Following this evaluation, we determined that our\nresults reveal improved performance for the new-coupled CNN-LSTM approach,\nparticularly in terms of assessments based on Mean Square Error, Root Mean\nSquare Error, Peak Signal to Noise Ratio, Structural Similarity Index, and\noverall classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:58:05 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Boulila", "Wadii", ""], ["Ghandorh", "Hamza", ""], ["Khan", "Mehshan Ahmed", ""], ["Ahmed", "Fawad", ""], ["Ahmad", "Jawad", ""]]}, {"id": "2103.01698", "submitter": "Hongming Luo", "authors": "Hongming Luo, Fei Zhou, Guangsen Liao, and Guoping Qiu", "title": "Super-resolving Compressed Images via Parallel and Series Integration of\n  Artifact Reduction and Resolution Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel compressed image super resolution (CISR)\nframework based on parallel and series integration of artifact removal and\nresolution enhancement. Based on maximum a posterior inference for estimating a\nclean low-resolution (LR) input image and a clean high resolution (HR) output\nimage from down-sampled and compressed observations, we have designed a CISR\narchitecture consisting of two deep neural network modules: the artifact\nreduction module (ARM) and resolution enhancement module (REM). ARM and REM\nwork in parallel with both taking the compressed LR image as their inputs,\nwhile they also work in series with REM taking the output of ARM as one of its\ninputs and ARM taking the output of REM as its other input. A unique property\nof our CSIR system is that a single trained model is able to super-resolve LR\nimages compressed by different methods to various qualities. This is achieved\nby exploiting deep neural net-works capacity for handling image degradations,\nand the parallel and series connections between ARM and REM to reduce the\ndependency on specific degradations. ARM and REM are trained simultaneously by\nthe deep unfolding technique. Experiments are conducted on a mixture of JPEG\nand WebP compressed images without a priori knowledge of the compression type\nand com-pression factor. Visual and quantitative comparisons demonstrate the\nsuperiority of our method over state-of-the-art super resolu-tion methods.Code\nlink: https://github.com/luohongming/CISR_PSI\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:04:28 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 01:00:09 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 09:47:20 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Luo", "Hongming", ""], ["Zhou", "Fei", ""], ["Liao", "Guangsen", ""], ["Qiu", "Guoping", ""]]}, {"id": "2103.01702", "submitter": "Alexandros Papadopoulos", "authors": "Alexandros Papadopoulos, Fotis Topouzis, Anastasios Delopoulos", "title": "An Interpretable Multiple-Instance Approach for the Detection of\n  referable Diabetic Retinopathy from Fundus Images", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetic Retinopathy (DR) is a leading cause of vision loss globally. Yet\ndespite its prevalence, the majority of affected people lack access to the\nspecialized ophthalmologists and equipment required for assessing their\ncondition. This can lead to delays in the start of treatment, thereby lowering\ntheir chances for a successful outcome. Machine learning systems that\nautomatically detect the disease in eye fundus images have been proposed as a\nmeans of facilitating access to DR severity estimates for patients in remote\nregions or even for complementing the human expert's diagnosis. In this paper,\nwe propose a machine learning system for the detection of referable DR in\nfundus images that is based on the paradigm of multiple-instance learning. By\nextracting local information from image patches and combining it efficiently\nthrough an attention mechanism, our system is able to achieve high\nclassification accuracy. Moreover, it can highlight potential image regions\nwhere DR manifests through its characteristic lesions. We evaluate our approach\non publicly available retinal image datasets, in which it exhibits near\nstate-of-the-art performance, while also producing interpretable visualizations\nof its predictions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:14:15 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Papadopoulos", "Alexandros", ""], ["Topouzis", "Fotis", ""], ["Delopoulos", "Anastasios", ""]]}, {"id": "2103.01705", "submitter": "Fangxin Liu", "authors": "Fangxin Liu, Wenbo Zhao, Yilong Zhao, Zongwu Wang, Tao Yang, Zhezhi\n  He, Naifeng Jing, Xiaoyao Liang, Li Jiang", "title": "SME: ReRAM-based Sparse-Multiplication-Engine to Squeeze-Out Bit\n  Sparsity of Neural Network", "comments": "7 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resistive Random-Access-Memory (ReRAM) crossbar is a promising technique for\ndeep neural network (DNN) accelerators, thanks to its in-memory and in-situ\nanalog computing abilities for Vector-Matrix Multiplication-and-Accumulations\n(VMMs). However, it is challenging for crossbar architecture to exploit the\nsparsity in the DNN. It inevitably causes complex and costly control to exploit\nfine-grained sparsity due to the limitation of tightly-coupled crossbar\nstructure. As the countermeasure, we developed a novel ReRAM-based DNN\naccelerator, named Sparse-Multiplication-Engine (SME), based on a hardware and\nsoftware co-design framework. First, we orchestrate the bit-sparse pattern to\nincrease the density of bit-sparsity based on existing quantization methods.\nSecond, we propose a novel weigh mapping mechanism to slice the bits of a\nweight across the crossbars and splice the activation results in peripheral\ncircuits. This mechanism can decouple the tightly-coupled crossbar structure\nand cumulate the sparsity in the crossbar. Finally, a superior squeeze-out\nscheme empties the crossbars mapped with highly-sparse non-zeros from the\nprevious two steps. We design the SME architecture and discuss its use for\nother quantization methods and different ReRAM cell technologies. Compared with\nprior state-of-the-art designs, the SME shrinks the use of crossbars up to 8.7x\nand 2.1x using Resent-50 and MobileNet-v2, respectively, with less than 0.3%\naccuracy drop on ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:27:15 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Liu", "Fangxin", ""], ["Zhao", "Wenbo", ""], ["Zhao", "Yilong", ""], ["Wang", "Zongwu", ""], ["Yang", "Tao", ""], ["He", "Zhezhi", ""], ["Jing", "Naifeng", ""], ["Liang", "Xiaoyao", ""], ["Jiang", "Li", ""]]}, {"id": "2103.01716", "submitter": "Fadi Boutros", "authors": "Fadi Boutros, Naser Damer, Florian Kirchbuchner and Arjan Kuijper", "title": "Unmasking Face Embeddings by Self-restrained Triplet Loss for Accurate\n  Masked Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the face as a biometric identity trait is motivated by the contactless\nnature of the capture process and the high accuracy of the recognition\nalgorithms. After the current COVID-19 pandemic, wearing a face mask has been\nimposed in public places to keep the pandemic under control. However, face\nocclusion due to wearing a mask presents an emerging challenge for face\nrecognition systems. In this paper, we presented a solution to improve the\nmasked face recognition performance. Specifically, we propose the Embedding\nUnmasking Model (EUM) operated on top of existing face recognition models. We\nalso propose a novel loss function, the Self-restrained Triplet (SRT), which\nenabled the EUM to produce embeddings similar to these of unmasked faces of the\nsame identities. The achieved evaluation results on two face recognition models\nand two real masked datasets proved that our proposed approach significantly\nimproves the performance in most experimental settings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:43:11 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Boutros", "Fadi", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2103.01717", "submitter": "Chen Wu", "authors": "Chen Wu, Sihan Zhu, Jiaqi Yang, Meiqi Hu, Bo Du, Liangpei Zhang, Lefei\n  Zhang, Chengxi Han, and Meng Lan", "title": "Transportation Density Reduction Caused by City Lockdowns Across the\n  World during the COVID-19 Epidemic: From the View of High-resolution Remote\n  Sensing Imagery", "comments": "14 pages, 7 figures, submitted to IEEE JSTARS", "journal-ref": null, "doi": "10.1109/JSTARS.2021.3078611", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the COVID-19 epidemic began to worsen in the first months of 2020,\nstringent lockdown policies were implemented in numerous cities throughout the\nworld to control human transmission and mitigate its spread. Although\ntransportation density reduction inside the city was felt subjectively, there\nhas thus far been no objective and quantitative study of its variation to\nreflect the intracity population flows and their corresponding relationship\nwith lockdown policy stringency from the view of remote sensing images with the\nhigh resolution under 1m. Accordingly, we here provide a quantitative\ninvestigation of the transportation density reduction before and after lockdown\nwas implemented in six epicenter cities (Wuhan, Milan, Madrid, Paris, New York,\nand London) around the world during the COVID-19 epidemic, which is\naccomplished by extracting vehicles from the multi-temporal high-resolution\nremote sensing images. A novel vehicle detection model combining unsupervised\nvehicle candidate extraction and deep learning identification was specifically\nproposed for the images with the resolution of 0.5m. Our results indicate that\ntransportation densities were reduced by an average of approximately 50% (and\nas much as 75.96%) in these six cities following lockdown. The influences on\ntransportation density reduction rates are also highly correlated with policy\nstringency, with an R^2 value exceeding 0.83. Even within a specific city, the\ntransportation density changes differed and tended to be distributed in\naccordance with the city's land-use patterns. Considering that public\ntransportation was mostly reduced or even forbidden, our results indicate that\ncity lockdown policies are effective at limiting human transmission within\ncities.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:45:16 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wu", "Chen", ""], ["Zhu", "Sihan", ""], ["Yang", "Jiaqi", ""], ["Hu", "Meiqi", ""], ["Du", "Bo", ""], ["Zhang", "Liangpei", ""], ["Zhang", "Lefei", ""], ["Han", "Chengxi", ""], ["Lan", "Meng", ""]]}, {"id": "2103.01721", "submitter": "Lazaro Janier Gonzalez-Soler Soler", "authors": "L\\'azaro J. Gonz\\'alez-Soler, Marta Gomez-Barrero, Christoph Busch", "title": "On the Generalisation Capabilities of Fisher Vector based Face\n  Presentation Attack Detection", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last decades, the broad development experienced by biometric systems\nhas unveiled several threats which may decrease their trustworthiness. Those\nare attack presentations which can be easily carried out by a non-authorised\nsubject to gain access to the biometric system. In order to mitigate those\nsecurity concerns, most face Presentation Attack Detection techniques have\nreported a good detection performance when they are evaluated on known\nPresentation Attack Instruments (PAI) and acquisition conditions, in contrast\nto more challenging scenarios where unknown attacks are included in the test\nset. For those more realistic scenarios, the existing algorithms face\ndifficulties to detect unknown PAI species in many cases. In this work, we use\na new feature space based on Fisher Vectors, computed from compact Binarised\nStatistical Image Features histograms, which allow discovering semantic feature\nsubsets from known samples in order to enhance the detection of unknown\nattacks. This new representation, evaluated for challenging unknown attacks\ntaken from freely available facial databases, shows promising results: a\nBPCER100 under 17% together with an AUC over 98% can be achieved in the\npresence of unknown attacks. In addition, by training a limited number of\nparameters, our method is able to achieve state-of-the-art deep learning-based\napproaches for cross-dataset scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:49:06 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Gonz\u00e1lez-Soler", "L\u00e1zaro J.", ""], ["Gomez-Barrero", "Marta", ""], ["Busch", "Christoph", ""]]}, {"id": "2103.01739", "submitter": "Bahram Mohammadi", "authors": "Bahram Mohammadi, Mahmood Fathy and Mohammad Sabokrou", "title": "Image/Video Deep Anomaly Detection: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The considerable significance of Anomaly Detection (AD) problem has recently\ndrawn the attention of many researchers. Consequently, the number of proposed\nmethods in this research field has been increased steadily. AD strongly\ncorrelates with the important computer vision and image processing tasks such\nas image/video anomaly, irregularity and sudden event detection. More recently,\nDeep Neural Networks (DNNs) offer a high performance set of solutions, but at\nthe expense of a heavy computational cost. However, there is a noticeable gap\nbetween the previously proposed methods and an applicable real-word approach.\nRegarding the raised concerns about AD as an ongoing challenging problem,\nnotably in images and videos, the time has come to argue over the pitfalls and\nprospects of methods have attempted to deal with visual AD tasks. Hereupon, in\nthis survey we intend to conduct an in-depth investigation into the\nimages/videos deep learning based AD methods. We also discuss current\nchallenges and future research directions thoroughly.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:15:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Mohammadi", "Bahram", ""], ["Fathy", "Mahmood", ""], ["Sabokrou", "Mohammad", ""]]}, {"id": "2103.01745", "submitter": "Yunqian Wen", "authors": "Yunqian Wen, Li Song, Bo Liu, Ming Ding, and Rong Xie", "title": "IdentityDP: Differential Private Identification Protection for Face\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the explosive growth of face photos as well as their widespread\ndissemination and easy accessibility in social media, the security and privacy\nof personal identity information becomes an unprecedented challenge. Meanwhile,\nthe convenience brought by advanced identity-agnostic computer vision\ntechnologies is attractive. Therefore, it is important to use face images while\ntaking careful consideration in protecting people's identities. Given a face\nimage, face de-identification, also known as face anonymization, refers to\ngenerating another image with similar appearance and the same background, while\nthe real identity is hidden. Although extensive efforts have been made,\nexisting face de-identification techniques are either insufficient in\nphoto-reality or incapable of well-balancing privacy and utility. In this\npaper, we focus on tackling these challenges to improve face de-identification.\nWe propose IdentityDP, a face anonymization framework that combines a\ndata-driven deep neural network with a differential privacy (DP) mechanism.\nThis framework encompasses three stages: facial representations\ndisentanglement, $\\epsilon$-IdentityDP perturbation and image reconstruction.\nOur model can effectively obfuscate the identity-related information of faces,\npreserve significant visual similarity, and generate high-quality images that\ncan be used for identity-agnostic computer vision tasks, such as detection,\ntracking, etc. Different from the previous methods, we can adjust the balance\nof privacy and utility through the privacy budget according to pratical demands\nand provide a diversity of results without pre-annotations. Extensive\nexperiments demonstrate the effectiveness and generalization ability of our\nproposed anonymization framework.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:26:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Wen", "Yunqian", ""], ["Song", "Li", ""], ["Liu", "Bo", ""], ["Ding", "Ming", ""], ["Xie", "Rong", ""]]}, {"id": "2103.01746", "submitter": "Florentin Bieder", "authors": "Florentin Bieder, Robin Sandk\\\"uhler, Philippe C. Cattin", "title": "Comparison of Methods Generalizing Max- and Average-Pooling", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max- and average-pooling are the most popular pooling methods for\ndownsampling in convolutional neural networks. In this paper, we compare\ndifferent pooling methods that generalize both max- and average-pooling.\nFurthermore, we propose another method based on a smooth approximation of the\nmaximum function and put it into context with related methods. For the\ncomparison, we use a VGG16 image classification network and train it on a large\ndataset of natural high-resolution images (Google Open Images v5). The results\nshow that none of the more sophisticated methods perform significantly better\nin this classification task than standard max- or average-pooling.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:26:51 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Bieder", "Florentin", ""], ["Sandk\u00fchler", "Robin", ""], ["Cattin", "Philippe C.", ""]]}, {"id": "2103.01760", "submitter": "Hilmi Enes Egilmez", "authors": "Hilmi E. Egilmez, Ankitesh K. Singh, Muhammed Coban, Marta Karczewicz,\n  Yinhao Zhu, Yang Yang, Amir Said, Taco S. Cohen", "title": "Transform Network Architectures for Deep Learning based End-to-End\n  Image/Video Coding in Subsampled Color Spaces", "comments": "10 pages, submitted to an IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing deep learning based end-to-end image/video coding (DLEC)\narchitectures are designed for non-subsampled RGB color format. However, in\norder to achieve a superior coding performance, many state-of-the-art\nblock-based compression standards such as High Efficiency Video Coding\n(HEVC/H.265) and Versatile Video Coding (VVC/H.266) are designed primarily for\nYUV 4:2:0 format, where U and V components are subsampled by considering the\nhuman visual system. This paper investigates various DLEC designs to support\nYUV 4:2:0 format by comparing their performance against the main profiles of\nHEVC and VVC standards under a common evaluation framework. Moreover, a new\ntransform network architecture is proposed to improve the efficiency of coding\nYUV 4:2:0 data. The experimental results on YUV 4:2:0 datasets show that the\nproposed architecture significantly outperforms naive extensions of existing\narchitectures designed for RGB format and achieves about 10% average BD-rate\nimprovement over the intra-frame coding in HEVC.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 06:47:27 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Egilmez", "Hilmi E.", ""], ["Singh", "Ankitesh K.", ""], ["Coban", "Muhammed", ""], ["Karczewicz", "Marta", ""], ["Zhu", "Yinhao", ""], ["Yang", "Yang", ""], ["Said", "Amir", ""], ["Cohen", "Taco S.", ""]]}, {"id": "2103.01780", "submitter": "Kai Lv", "authors": "Kai Lv, Zongqing Lu, Qingmin Liao", "title": "A region-based descriptor network for uniformly sampled keypoints", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matching keypoint pairs of different images is a basic task of computer\nvision. Most methods require customized extremum point schemes to obtain the\ncoordinates of feature points with high confidence, which often need complex\nalgorithmic design or a network with higher training difficulty and also ignore\nthe possibility that flat regions can be used as candidate regions of matching\npoints. In this paper, we design a region-based descriptor by combining the\ncontext features of a deep network. The new descriptor can give a robust\nrepresentation of a point even in flat regions. By the new descriptor, we can\nobtain more high confidence matching points without extremum operation. The\nexperimental results show that our proposed method achieves a performance\ncomparable to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 07:31:22 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Lv", "Kai", ""], ["Lu", "Zongqing", ""], ["Liao", "Qingmin", ""]]}, {"id": "2103.01786", "submitter": "Xin Yuan", "authors": "Zhengjue Wang and Hao Zhang and Ziheng Cheng and Bo Chen and Xin Yuan", "title": "MetaSCI: Scalable and Adaptive Reconstruction for Video Compressive\n  Sensing", "comments": "12 pages, 6 figures, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To capture high-speed videos using a two-dimensional detector, video snapshot\ncompressive imaging (SCI) is a promising system, where the video frames are\ncoded by different masks and then compressed to a snapshot measurement.\nFollowing this, efficient algorithms are desired to reconstruct the high-speed\nframes, where the state-of-the-art results are achieved by deep learning\nnetworks. However, these networks are usually trained for specific small-scale\nmasks and often have high demands of training time and GPU memory, which are\nhence {\\bf \\em not flexible} to $i$) a new mask with the same size and $ii$) a\nlarger-scale mask. We address these challenges by developing a Meta Modulated\nConvolutional Network for SCI reconstruction, dubbed MetaSCI. MetaSCI is\ncomposed of a shared backbone for different masks, and light-weight\nmeta-modulation parameters to evolve to different modulation parameters for\neach mask, thus having the properties of {\\bf \\em fast adaptation} to new masks\n(or systems) and ready to {\\bf \\em scale to large data}. Extensive simulation\nand real data results demonstrate the superior performance of our proposed\napproach. Our code is available at\n{\\small\\url{https://github.com/xyvirtualgroup/MetaSCI-CVPR2021}}.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:53:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Wang", "Zhengjue", ""], ["Zhang", "Hao", ""], ["Cheng", "Ziheng", ""], ["Chen", "Bo", ""], ["Yuan", "Xin", ""]]}, {"id": "2103.01795", "submitter": "Qingyao Wu", "authors": "Yukun Su, Ruizhou Sun, Guosheng Lin, Qingyao Wu", "title": "Context Decoupling Augmentation for Weakly Supervised Semantic\n  Segmentation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is vital for deep learning neural networks. By providing\nmassive training samples, it helps to improve the generalization ability of the\nmodel. Weakly supervised semantic segmentation (WSSS) is a challenging problem\nthat has been deeply studied in recent years, conventional data augmentation\napproaches for WSSS usually employ geometrical transformations, random cropping\nand color jittering. However, merely increasing the same contextual semantic\ndata does not bring much gain to the networks to distinguish the objects, e.g.,\nthe correct image-level classification of \"aeroplane\" may be not only due to\nthe recognition of the object itself, but also its co-occurrence context like\n\"sky\", which will cause the model to focus less on the object features. To this\nend, we present a Context Decoupling Augmentation (CDA) method, to change the\ninherent context in which the objects appear and thus drive the network to\nremove the dependence between object instances and contextual information. To\nvalidate the effectiveness of the proposed method, extensive experiments on\nPASCAL VOC 2012 dataset with several alternative network architectures\ndemonstrate that CDA can boost various popular WSSS methods to the new\nstate-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 15:05:09 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Su", "Yukun", ""], ["Sun", "Ruizhou", ""], ["Lin", "Guosheng", ""], ["Wu", "Qingyao", ""]]}, {"id": "2103.01823", "submitter": "Ioannis Psaromiligkos", "authors": "Pavel Sinha, Ioannis Psaromiligkos, Zeljko Zilic", "title": "A Structurally Regularized Convolutional Neural Network for Image\n  Classification using Wavelet-based SubBand Decomposition", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": "10.1109/ICIP.2019.8804202", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a convolutional neural network (CNN) architecture for image\nclassification based on subband decomposition of the image using wavelets. The\nproposed architecture decomposes the input image spectra into multiple\ncritically sampled subbands, extracts features using a single CNN per subband,\nand finally, performs classification by combining the extracted features using\na fully connected layer. Processing each of the subbands by an individual CNN,\nthereby limiting the learning scope of each CNN to a single subband, imposes a\nform of structural regularization. This provides better generalization\ncapability as seen by the presented results. The proposed architecture achieves\nbest-in-class performance in terms of total multiply-add-accumulator operations\nand nearly best-in-class performance in terms of total parameters required, yet\nit maintains competitive classification performance. We also show the proposed\narchitecture is more robust than the regular full-band CNN to noise caused by\nweight-and-bias quantization and input quantization.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:01:22 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Sinha", "Pavel", ""], ["Psaromiligkos", "Ioannis", ""], ["Zilic", "Zeljko", ""]]}, {"id": "2103.01837", "submitter": "Markus Borg", "authors": "Markus Borg, Ronald Jabangwe, Simon {\\AA}berg, Arvid Ekblom, Ludwig\n  Hedlund, August Lidfeldt", "title": "Test Automation with Grad-CAM Heatmaps -- A Future Pipe Segment in MLOps\n  for Vision AI?", "comments": "Accepted for publication in the Proc. of the 1st International\n  Workshop on DevOps Testing for Cyber-Physical Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) is a fundamental part of modern perception systems. In\nthe last decade, the performance of computer vision using trained deep neural\nnetworks has outperformed previous approaches based on careful feature\nengineering. However, the opaqueness of large ML models is a substantial\nimpediment for critical applications such as in the automotive context. As a\nremedy, Gradient-weighted Class Activation Mapping (Grad-CAM) has been proposed\nto provide visual explanations of model internals. In this paper, we\ndemonstrate how Grad-CAM heatmaps can be used to increase the explainability of\nan image recognition model trained for a pedestrian underpass. We argue how the\nheatmaps support compliance to the EU's seven key requirements for Trustworthy\nAI. Finally, we propose adding automated heatmap analysis as a pipe segment in\nan MLOps pipeline. We believe that such a building block can be used to\nautomatically detect if a trained ML-model is activated based on invalid pixels\nin test images, suggesting biased models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:23:49 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Borg", "Markus", ""], ["Jabangwe", "Ronald", ""], ["\u00c5berg", "Simon", ""], ["Ekblom", "Arvid", ""], ["Hedlund", "Ludwig", ""], ["Lidfeldt", "August", ""]]}, {"id": "2103.01843", "submitter": "Nikolaus Demmel", "authors": "Nikolaus Demmel, Christiane Sommer, Daniel Cremers, Vladyslav Usenko", "title": "Square Root Bundle Adjustment for Large-Scale Reconstruction", "comments": "Accepted to CVPR 2021. Updated version corresponding to CVPR\n  camera-ready. Formatting changes and minor tweaks to fit page requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new formulation for the bundle adjustment problem which relies\non nullspace marginalization of landmark variables by QR decomposition. Our\napproach, which we call square root bundle adjustment, is algebraically\nequivalent to the commonly used Schur complement trick, improves the numeric\nstability of computations, and allows for solving large-scale bundle adjustment\nproblems with single-precision floating-point numbers. We show in real-world\nexperiments with the BAL datasets that even in single precision the proposed\nsolver achieves on average equally accurate solutions compared to Schur\ncomplement solvers using double precision. It runs significantly faster, but\ncan require larger amounts of memory on dense problems. The proposed\nformulation relies on simple linear algebra operations and opens the way for\nefficient implementations of bundle adjustment on hardware platforms optimized\nfor single-precision linear algebra processing.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:26:20 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 23:50:04 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Demmel", "Nikolaus", ""], ["Sommer", "Christiane", ""], ["Cremers", "Daniel", ""], ["Usenko", "Vladyslav", ""]]}, {"id": "2103.01847", "submitter": "Yuenan Hou", "authors": "Yuenan Hou, Zheng Ma, Chunxiao Liu, Zhe Wang, and Chen Change Loy", "title": "Network Pruning via Resource Reallocation", "comments": "12 pages, 11 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Channel pruning is broadly recognized as an effective approach to obtain a\nsmall compact model through eliminating unimportant channels from a large\ncumbersome network. Contemporary methods typically perform iterative pruning\nprocedure from the original over-parameterized model, which is both tedious and\nexpensive especially when the pruning is aggressive. In this paper, we propose\na simple yet effective channel pruning technique, termed network Pruning via\nrEsource rEalLocation (PEEL), to quickly produce a desired slim model with\nnegligible cost. Specifically, PEEL first constructs a predefined backbone and\nthen conducts resource reallocation on it to shift parameters from less\ninformative layers to more important layers in one round, thus amplifying the\npositive effect of these informative layers. To demonstrate the effectiveness\nof PEEL , we perform extensive experiments on ImageNet with ResNet-18,\nResNet-50, MobileNetV2, MobileNetV3-small and EfficientNet-B0. Experimental\nresults show that structures uncovered by PEEL exhibit competitive performance\nwith state-of-the-art pruning algorithms under various pruning settings. Our\ncode is available at https://github.com/cardwing/Codes-for-PEEL.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:28:10 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hou", "Yuenan", ""], ["Ma", "Zheng", ""], ["Liu", "Chunxiao", ""], ["Wang", "Zhe", ""], ["Loy", "Chen Change", ""]]}, {"id": "2103.01849", "submitter": "Konrad Heidler", "authors": "Konrad Heidler, Lichao Mou, Celia Baumhoer, Andreas Dietz, Xiao Xiang\n  Zhu", "title": "HED-UNet: Combined Segmentation and Edge Detection for Monitoring the\n  Antarctic Coastline", "comments": "This work has been accepted by IEEE TGRS for publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": "10.1109/TGRS.2021.3064606", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based coastline detection algorithms have begun to outshine\ntraditional statistical methods in recent years. However, they are usually\ntrained only as single-purpose models to either segment land and water or\ndelineate the coastline. In contrast to this, a human annotator will usually\nkeep a mental map of both segmentation and delineation when performing manual\ncoastline detection. To take into account this task duality, we therefore\ndevise a new model to unite these two approaches in a deep learning model. By\ntaking inspiration from the main building blocks of a semantic segmentation\nframework (UNet) and an edge detection framework (HED), both tasks are combined\nin a natural way. Training is made efficient by employing deep supervision on\nside predictions at multiple resolutions. Finally, a hierarchical attention\nmechanism is introduced to adaptively merge these multiscale predictions into\nthe final model output. The advantages of this approach over other traditional\nand deep learning-based methods for coastline detection are demonstrated on a\ndataset of Sentinel-1 imagery covering parts of the Antarctic coast, where\ncoastline detection is notoriously difficult. An implementation of our method\nis available at \\url{https://github.com/khdlr/HED-UNet}.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:35:05 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Heidler", "Konrad", ""], ["Mou", "Lichao", ""], ["Baumhoer", "Celia", ""], ["Dietz", "Andreas", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2103.01856", "submitter": "Honggu Liu", "authors": "Honggu Liu, Xiaodan Li, Wenbo Zhou, Yuefeng Chen, Yuan He, Hui Xue,\n  Weiming Zhang and Nenghai Yu", "title": "Spatial-Phase Shallow Learning: Rethinking Face Forgery Detection in\n  Frequency Domain", "comments": "Accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable success in face forgery techniques has received considerable\nattention in computer vision due to security concerns. We observe that\nup-sampling is a necessary step of most face forgery techniques, and cumulative\nup-sampling will result in obvious changes in the frequency domain, especially\nin the phase spectrum. According to the property of natural images, the phase\nspectrum preserves abundant frequency components that provide extra information\nand complement the loss of the amplitude spectrum. To this end, we present a\nnovel Spatial-Phase Shallow Learning (SPSL) method, which combines spatial\nimage and phase spectrum to capture the up-sampling artifacts of face forgery\nto improve the transferability, for face forgery detection. And we also\ntheoretically analyze the validity of utilizing the phase spectrum. Moreover,\nwe notice that local texture information is more crucial than high-level\nsemantic information for the face forgery detection task. So we reduce the\nreceptive fields by shallowing the network to suppress high-level features and\nfocus on the local region. Extensive experiments show that SPSL can achieve the\nstate-of-the-art performance on cross-datasets evaluation as well as\nmulti-class classification and obtain comparable results on single dataset\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:45:08 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 02:40:00 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 12:36:20 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Liu", "Honggu", ""], ["Li", "Xiaodan", ""], ["Zhou", "Wenbo", ""], ["Chen", "Yuefeng", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "2103.01867", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, David Rosenberg, Gideon Mann, Mohit Bansal", "title": "Dual Reinforcement-Based Specification Generation for Image De-Rendering", "comments": "AAAI 2021 Scientific Document Understanding Workshop (9 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in deep learning have led to promising progress in inferring\ngraphics programs by de-rendering computer-generated images. However, current\nmethods do not explore which decoding methods lead to better inductive bias for\ninferring graphics programs. In our work, we first explore the effectiveness of\nLSTM-RNN versus Transformer networks as decoders for order-independent graphics\nprograms. Since these are sequence models, we must choose an ordering of the\nobjects in the graphics programs for likelihood training. We found that the\nLSTM performance was highly sensitive to the sequence ordering (random order\nvs. pattern-based order), while Transformer performance was roughly independent\nof the sequence ordering. Further, we present a policy gradient based\nreinforcement learning approach for better inductive bias in the decoder via\nmultiple diverse rewards based both on the graphics program specification and\nthe rendered image. We also explore the combination of these complementary\nrewards. We achieve state-of-the-art results on two graphics program generation\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:04:56 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Rosenberg", "David", ""], ["Mann", "Gideon", ""], ["Bansal", "Mohit", ""]]}, {"id": "2103.01890", "submitter": "Neil Jethani", "authors": "Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, Rajesh\n  Ranganath", "title": "Have We Learned to Explain?: How Interpretability Methods Can Learn to\n  Encode Predictions in their Interpretations", "comments": "15 pages, 3 figures, Proceedings of the 24th International Conference\n  on Artificial Intelligence and Statistics (AISTATS) 2021", "journal-ref": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the need for interpretable machine learning has been established, many\ncommon approaches are slow, lack fidelity, or hard to evaluate. Amortized\nexplanation methods reduce the cost of providing interpretations by learning a\nglobal selector model that returns feature importances for a single instance of\ndata. The selector model is trained to optimize the fidelity of the\ninterpretations, as evaluated by a predictor model for the target. Popular\nmethods learn the selector and predictor model in concert, which we show allows\npredictions to be encoded within interpretations. We introduce EVAL-X as a\nmethod to quantitatively evaluate interpretations and REAL-X as an amortized\nexplanation method, which learn a predictor model that approximates the true\ndata generating distribution given any subset of the input. We show EVAL-X can\ndetect when predictions are encoded in interpretations and show the advantages\nof REAL-X through quantitative and radiologist evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:42:33 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Jethani", "Neil", ""], ["Sudarshan", "Mukund", ""], ["Aphinyanaphongs", "Yindalon", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "2103.01895", "submitter": "Chiayi Hsu", "authors": "Chia-Yi Hsu, Pin-Yu Chen, Songtao Lu, Sijia Liu, Chia-Mu Yu", "title": "Adversarial Examples for Unsupervised Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples causing evasive predictions are widely used to evaluate\nand improve the robustness of machine learning models. However, current studies\non adversarial examples focus on supervised learning tasks, relying on the\nground-truth data label, a targeted objective, or supervision from a trained\nclassifier. In this paper, we propose a framework of generating adversarial\nexamples for unsupervised models and demonstrate novel applications to data\naugmentation. Our framework exploits a mutual information neural estimator as\nan information-theoretic similarity measure to generate adversarial examples\nwithout supervision. We propose a new MinMax algorithm with provable\nconvergence guarantees for efficient generation of unsupervised adversarial\nexamples. Our framework can also be extended to supervised adversarial\nexamples. When using unsupervised adversarial examples as a simple plug-in data\naugmentation tool for model retraining, significant improvements are\nconsistently observed across different unsupervised tasks and datasets,\nincluding data reconstruction, representation learning, and contrastive\nlearning. Our results show novel methods and advantages in studying and\nimproving robustness of unsupervised learning problems via adversarial\nexamples. Our codes are available at https://github.com/IBM/UAE.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:47:58 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 18:12:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hsu", "Chia-Yi", ""], ["Chen", "Pin-Yu", ""], ["Lu", "Songtao", ""], ["Liu", "Sijia", ""], ["Yu", "Chia-Mu", ""]]}, {"id": "2103.01903", "submitter": "Chenchen Zhu", "authors": "Chenchen Zhu, Fangyi Chen, Uzair Ahmed, Zhiqiang Shen, Marios Savvides", "title": "Semantic Relation Reasoning for Shot-Stable Few-Shot Object Detection", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection is an imperative and long-lasting problem due to\nthe inherent long-tail distribution of real-world data. Its performance is\nlargely affected by the data scarcity of novel classes. But the semantic\nrelation between the novel classes and the base classes is constant regardless\nof the data availability. In this work, we investigate utilizing this semantic\nrelation together with the visual information and introduce explicit relation\nreasoning into the learning of novel object detection. Specifically, we\nrepresent each class concept by a semantic embedding learned from a large\ncorpus of text. The detector is trained to project the image representations of\nobjects into this embedding space. We also identify the problems of trivially\nusing the raw embeddings with a heuristic knowledge graph and propose to\naugment the embeddings with a dynamic relation graph. As a result, our few-shot\ndetector, termed SRR-FSD, is robust and stable to the variation of shots of\nnovel objects. Experiments show that SRR-FSD can achieve competitive results at\nhigher shots, and more importantly, a significantly better performance given\nboth lower explicit and implicit shots. The benchmark protocol with implicit\nshots removed from the pretrained classification dataset can serve as a more\nrealistic setting for future research.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:04:38 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 21:48:36 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhu", "Chenchen", ""], ["Chen", "Fangyi", ""], ["Ahmed", "Uzair", ""], ["Shen", "Zhiqiang", ""], ["Savvides", "Marios", ""]]}, {"id": "2103.01913", "submitter": "Krishna Srinivasan", "authors": "Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky,\n  Marc Najork", "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The milestone improvements brought about by deep representation learning and\npre-training techniques have led to large performance gains across downstream\nNLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large\nhigh-quality visio-linguistic datasets for learning complementary information\n(across image and text modalities). In this paper, we introduce the\nWikipedia-based Image Text (WIT) Dataset\n(https://github.com/google-research-datasets/wit) to better facilitate\nmultimodal, multilingual learning. WIT is composed of a curated set of 37.6\nmillion entity rich image-text examples with 11.5 million unique images across\n108 Wikipedia languages. Its size enables WIT to be used as a pretraining\ndataset for multimodal models, as we show when applied to downstream tasks such\nas image-text retrieval. WIT has four main and unique advantages. First, WIT is\nthe largest multimodal dataset by the number of image-text examples by 3x (at\nthe time of writing). Second, WIT is massively multilingual (first of its kind)\nwith coverage over 100+ languages (each of which has at least 12K examples) and\nprovides cross-lingual texts for many images. Third, WIT represents a more\ndiverse set of concepts and real world entities relative to what previous\ndatasets cover. Lastly, WIT provides a very challenging real-world test set, as\nwe empirically illustrate using an image-text retrieval task as an example.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:13:54 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 16:41:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Srinivasan", "Krishna", ""], ["Raman", "Karthik", ""], ["Chen", "Jiecao", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "2103.01924", "submitter": "Naser Damer", "authors": "Naser Damer, Fadi Boutros, Marius S\\\"u{\\ss}milch, Meiling Fang,\n  Florian Kirchbuchner, Arjan Kuijper", "title": "Masked Face Recognition: Human vs. Machine", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent COVID-19 pandemic has increased the focus on hygienic and\ncontactless identity verification methods. However, the pandemic led to the\nwide use of face masks, essential to keep the pandemic under control. The\neffect of wearing a mask on face recognition in a collaborative environment is\ncurrently sensitive yet understudied issue. Recent reports have tackled this by\nevaluating the masked probe effect on the performance of automatic face\nrecognition solutions. However, such solutions can fail in certain processes,\nleading to performing the verification task by a human expert. This work\nprovides a joint evaluation and in-depth analyses of the face verification\nperformance of human experts in comparison to state-of-the-art automatic face\nrecognition solutions. This involves an extensive evaluation with 12 human\nexperts and 4 automatic recognition solutions. The study concludes with a set\nof take-home messages on different aspects of the correlation between the\nverification behavior of human and machine.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:36:01 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 16:41:01 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Damer", "Naser", ""], ["Boutros", "Fadi", ""], ["S\u00fc\u00dfmilch", "Marius", ""], ["Fang", "Meiling", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2103.01933", "submitter": "Tianmin Shu", "authors": "Aviv Netanyahu, Tianmin Shu, Boris Katz, Andrei Barbu, Joshua B.\n  Tenenbaum", "title": "PHASE: PHysically-grounded Abstract Social Events for Machine Social\n  Perception", "comments": "The first two authors contributed equally; AAAI 2021; 13 pages, 7\n  figures; Project page: https://www.tshu.io/PHASE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perceive and reason about social interactions in the context\nof physical environments is core to human social intelligence and human-machine\ncooperation. However, no prior dataset or benchmark has systematically\nevaluated physically grounded perception of complex social interactions that go\nbeyond short actions, such as high-fiving, or simple group activities, such as\ngathering. In this work, we create a dataset of physically-grounded abstract\nsocial events, PHASE, that resemble a wide range of real-life social\ninteractions by including social concepts such as helping another agent. PHASE\nconsists of 2D animations of pairs of agents moving in a continuous space\ngenerated procedurally using a physics engine and a hierarchical planner.\nAgents have a limited field of view, and can interact with multiple objects, in\nan environment that has multiple landmarks and obstacles. Using PHASE, we\ndesign a social recognition task and a social prediction task. PHASE is\nvalidated with human experiments demonstrating that humans perceive rich\ninteractions in the social events, and that the simulated agents behave\nsimilarly to humans. As a baseline model, we introduce a Bayesian inverse\nplanning approach, SIMPLE (SIMulation, Planning and Local Estimation), which\noutperforms state-of-the-art feed-forward neural networks. We hope that PHASE\ncan serve as a difficult new challenge for developing new models that can\nrecognize complex social interactions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:44:57 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 20:13:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Netanyahu", "Aviv", ""], ["Shu", "Tianmin", ""], ["Katz", "Boris", ""], ["Barbu", "Andrei", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2103.01938", "submitter": "Rohan Shad", "authors": "Rohan Shad, John P. Cunningham, Euan A. Ashley, Curtis P. Langlotz,\n  William Hiesinger", "title": "Medical Imaging and Machine Learning", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in computing power, deep learning architectures, and expert labelled\ndatasets have spurred the development of medical imaging artificial\nintelligence systems that rival clinical experts in a variety of scenarios. The\nNational Institutes of Health in 2018 identified key focus areas for the future\nof artificial intelligence in medical imaging, creating a foundational roadmap\nfor research in image acquisition, algorithms, data standardization, and\ntranslatable clinical decision support systems. Among the key issues raised in\nthe report: data availability, need for novel computing architectures and\nexplainable AI algorithms, are still relevant despite the tremendous progress\nmade over the past few years alone. Furthermore, translational goals of data\nsharing, validation of performance for regulatory approval, generalizability\nand mitigation of unintended bias must be accounted for early in the\ndevelopment process. In this perspective paper we explore challenges unique to\nhigh dimensional clinical imaging data, in addition to highlighting some of the\ntechnical and ethical considerations in developing high-dimensional,\nmulti-modality, machine learning systems for clinical decision support.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:53:39 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Shad", "Rohan", ""], ["Cunningham", "John P.", ""], ["Ashley", "Euan A.", ""], ["Langlotz", "Curtis P.", ""], ["Hiesinger", "William", ""]]}, {"id": "2103.01946", "submitter": "Sylvestre-Alvise Rebuffi", "authors": "Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg,\n  Olivia Wiles, Timothy Mann", "title": "Fixing Data Augmentation to Improve Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training suffers from robust overfitting, a phenomenon where the\nrobust test accuracy starts to decrease during training. In this paper, we\nfocus on both heuristics-driven and data-driven augmentations as a means to\nreduce robust overfitting. First, we demonstrate that, contrary to previous\nfindings, when combined with model weight averaging, data augmentation can\nsignificantly boost robust accuracy. Second, we explore how state-of-the-art\ngenerative models can be leveraged to artificially increase the size of the\ntraining set and further improve adversarial robustness. Finally, we evaluate\nour approach on CIFAR-10 against $\\ell_\\infty$ and $\\ell_2$ norm-bounded\nperturbations of size $\\epsilon = 8/255$ and $\\epsilon = 128/255$,\nrespectively. We show large absolute improvements of +7.06% and +5.88% in\nrobust accuracy compared to previous state-of-the-art methods. In particular,\nagainst $\\ell_\\infty$ norm-bounded perturbations of size $\\epsilon = 8/255$,\nour model reaches 64.20% robust accuracy without using any external data,\nbeating most prior works that use external data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:58:33 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Gowal", "Sven", ""], ["Calian", "Dan A.", ""], ["Stimberg", "Florian", ""], ["Wiles", "Olivia", ""], ["Mann", "Timothy", ""]]}, {"id": "2103.01950", "submitter": "Jacob Walker", "authors": "Jacob Walker, Ali Razavi, and A\\\"aron van den Oord", "title": "Predicting Video with VQVAE", "comments": "13 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the task of video prediction-forecasting future video given\npast video frames-has attracted attention in the research community. In this\npaper we propose a novel approach to this problem with Vector Quantized\nVariational AutoEncoders (VQ-VAE). With VQ-VAE we compress high-resolution\nvideos into a hierarchical set of multi-scale discrete latent variables.\nCompared to pixels, this compressed latent space has dramatically reduced\ndimensionality, allowing us to apply scalable autoregressive generative models\nto predict video. In contrast to previous work that has largely emphasized\nhighly constrained datasets, we focus on very diverse, large-scale datasets\nsuch as Kinetics-600. We predict video at a higher resolution on unconstrained\nvideos, 256x256, than any other previous method to our knowledge. We further\nvalidate our approach against prior work via a crowdsourced human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:59:10 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Walker", "Jacob", ""], ["Razavi", "Ali", ""], ["Oord", "A\u00e4ron van den", ""]]}, {"id": "2103.01954", "submitter": "Stephen Lombardi", "authors": "Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer,\n  Yaser Sheikh, Jason Saragih", "title": "Mixture of Volumetric Primitives for Efficient Neural Rendering", "comments": "13 pages; SIGGRAPH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time rendering and animation of humans is a core function in games,\nmovies, and telepresence applications. Existing methods have a number of\ndrawbacks we aim to address with our work. Triangle meshes have difficulty\nmodeling thin structures like hair, volumetric representations like Neural\nVolumes are too low-resolution given a reasonable memory budget, and\nhigh-resolution implicit representations like Neural Radiance Fields are too\nslow for use in real-time applications. We present Mixture of Volumetric\nPrimitives (MVP), a representation for rendering dynamic 3D content that\ncombines the completeness of volumetric representations with the efficiency of\nprimitive-based rendering, e.g., point-based or mesh-based methods. Our\napproach achieves this by leveraging spatially shared computation with a\ndeconvolutional architecture and by minimizing computation in empty regions of\nspace with volumetric primitives that can move to cover only occupied regions.\nOur parameterization supports the integration of correspondence and tracking\nconstraints, while being robust to areas where classical tracking fails, such\nas around thin or translucent structures and areas with large topological\nvariability. MVP is a hybrid that generalizes both volumetric and\nprimitive-based representations. Through a series of extensive experiments we\ndemonstrate that it inherits the strengths of each, while avoiding many of\ntheir limitations. We also compare our approach to several state-of-the-art\nmethods and demonstrate that MVP produces superior results in terms of quality\nand runtime performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:59:42 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 14:40:31 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lombardi", "Stephen", ""], ["Simon", "Tomas", ""], ["Schwartz", "Gabriel", ""], ["Zollhoefer", "Michael", ""], ["Sheikh", "Yaser", ""], ["Saragih", "Jason", ""]]}, {"id": "2103.01977", "submitter": "Ge Gao", "authors": "Ge Gao, Mikko Lauri, Xiaolin Hu, Jianwei Zhang and Simone Frintrop", "title": "CloudAAE: Learning 6D Object Pose Regression with On-line Data Synthesis\n  on Point Clouds", "comments": "Accepted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is often desired to train 6D pose estimation systems on synthetic data\nbecause manual annotation is expensive. However, due to the large domain gap\nbetween the synthetic and real images, synthesizing color images is expensive.\nIn contrast, this domain gap is considerably smaller and easier to fill for\ndepth information. In this work, we present a system that regresses 6D object\npose from depth information represented by point clouds, and a lightweight data\nsynthesis pipeline that creates synthetic point cloud segments for training. We\nuse an augmented autoencoder (AAE) for learning a latent code that encodes 6D\nobject pose information for pose regression. The data synthesis pipeline only\nrequires texture-less 3D object models and desired viewpoints, and it is cheap\nin terms of both time and hardware storage. Our data synthesis process is up to\nthree orders of magnitude faster than commonly applied approaches that render\nRGB image data. We show the effectiveness of our system on the LineMOD, LineMOD\nOcclusion, and YCB Video datasets. The implementation of our system is\navailable at: https://github.com/GeeeG/CloudAAE.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:00:21 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Gao", "Ge", ""], ["Lauri", "Mikko", ""], ["Hu", "Xiaolin", ""], ["Zhang", "Jianwei", ""], ["Frintrop", "Simone", ""]]}, {"id": "2103.01988", "submitter": "Priya Goyal", "authors": "Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao\n  Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand\n  Joulin, Piotr Bojanowski", "title": "Self-supervised Pretraining of Visual Features in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV\nhave reduced the gap with supervised methods. These results have been achieved\nin a control environment, that is the highly curated ImageNet dataset. However,\nthe premise of self-supervised learning is that it can learn from any random\nimage and from any unbounded dataset. In this work, we explore if\nself-supervision lives to its expectation by training large models on random,\nuncurated images with no supervision. Our final SElf-supERvised (SEER) model, a\nRegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves\n84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by\n1% and confirming that self-supervised learning works in a real world setting.\nInterestingly, we also observe that self-supervised models are good few-shot\nlearners achieving 77.9% top-1 with access to only 10% of ImageNet. Code:\nhttps://github.com/facebookresearch/vissl\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:12:29 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 13:53:36 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Goyal", "Priya", ""], ["Caron", "Mathilde", ""], ["Lefaudeux", "Benjamin", ""], ["Xu", "Min", ""], ["Wang", "Pengchao", ""], ["Pai", "Vivek", ""], ["Singh", "Mannat", ""], ["Liptchinsky", "Vitaliy", ""], ["Misra", "Ishan", ""], ["Joulin", "Armand", ""], ["Bojanowski", "Piotr", ""]]}, {"id": "2103.01994", "submitter": "Mihnea-Alexandru Tomita", "authors": "Mihnea-Alexandru Tomit\\u{a}, Mubariz Zaffar, Michael Milford, Klaus\n  McDonald-Maier, Shoaib Ehsan", "title": "Sequence-Based Filtering for Visual Route-Based Navigation: Analysing\n  the Benefits, Trade-offs and Design Choices", "comments": "7 pages, currently under-review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is the ability to correctly recall a\npreviously visited place using visual information under environmental,\nviewpoint and appearance changes. An emerging trend in VPR is the use of\nsequence-based filtering methods on top of single-frame-based place matching\ntechniques for route-based navigation. The combination leads to varying levels\nof potential place matching performance boosts at increased computational\ncosts. This raises a number of interesting research questions: How does\nperformance boost (due to sequential filtering) vary along the entire spectrum\nof single-frame-based matching methods? How does sequence matching length\naffect the performance curve? Which specific combinations provide a good\ntrade-off between performance and computation? However, there is lack of\nprevious work looking at these important questions and most of the\nsequence-based filtering work to date has been used without a systematic\napproach. To bridge this research gap, this paper conducts an in-depth\ninvestigation of the relationship between the performance of single-frame-based\nplace matching techniques and the use of sequence-based filtering on top of\nthose methods. It analyzes individual trade-offs, properties and limitations\nfor different combinations of single-frame-based and sequential techniques. A\nnumber of state-of-the-art VPR methods and widely used public datasets are\nutilized to present the findings that contain a number of meaningful insights\nfor the VPR community.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:24:58 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Tomit\u0103", "Mihnea-Alexandru", ""], ["Zaffar", "Mubariz", ""], ["Milford", "Michael", ""], ["McDonald-Maier", "Klaus", ""], ["Ehsan", "Shoaib", ""]]}, {"id": "2103.01997", "submitter": "Federico Zocco", "authors": "Federico Zocco and Se\\'an McLoone", "title": "Material Measurement Units: Foundations Through a Survey", "comments": "In preparation for submission to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term availability of minerals and industrial materials is a necessary\ncondition for sustainable development as they are the constituents of any\nmanufacturing product. In particular, technologies with increasing demand such\nas GPUs and photovoltaic panels are made of critical raw materials. To enhance\nthe efficiency of material management, in this paper we make three main\ncontributions: first, we identify in the literature an emerging\ncomputer-vision-enabled material monitoring technology which we call Material\nMeasurement Unit (MMU); second, we provide a survey of works relevant to the\ndevelopment of MMUs; third, we describe a material stock monitoring sensor\nnetwork deploying multiple MMUs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:36:12 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Zocco", "Federico", ""], ["McLoone", "Se\u00e1n", ""]]}, {"id": "2103.02015", "submitter": "Nati Daniel", "authors": "Nati Daniel, Ariel Larey, Eliel Aknin, Garrett A. Osswald, Julie M.\n  Caldwell, Mark Rochman, Margaret H. Collins, Guang-Yu Yang, Nicoleta C. Arva,\n  Kelley E. Capocelli, Marc E. Rothenberg, Yonatan Savir", "title": "PECNet: A Deep Multi-Label Segmentation Network for Eosinophilic\n  Esophagitis Biopsy Diagnostics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Eosinophilic esophagitis (EoE) is an allergic inflammatory\ncondition of the esophagus associated with elevated numbers of eosinophils.\nDisease diagnosis and monitoring requires determining the concentration of\neosinophils in esophageal biopsies, a time-consuming, tedious and somewhat\nsubjective task currently performed by pathologists. Methods. Herein, we aimed\nto use machine learning to identify, quantitate and diagnose EoE. We labeled\nmore than 100M pixels of 4345 images obtained by scanning whole slides of\nH&E-stained sections of esophageal biopsies derived from 23 EoE patients. We\nused this dataset to train a multi-label segmentation deep network. To validate\nthe network, we examined a replication cohort of 1089 whole slide images from\n419 patients derived from multiple institutions. Findings. PECNet segmented\nboth intact and not-intact eosinophils with a mean intersection over union\n(mIoU) of 0.93. This segmentation was able to quantitate intact eosinophils\nwith a mean absolute error of 0.611 eosinophils and classify EoE disease\nactivity with an accuracy of 98.5%. Using whole slide images from the\nvalidation cohort, PECNet achieved an accuracy of 94.8%, sensitivity of 94.3%,\nand specificity of 95.14% in reporting EoE disease activity. Interpretation. We\nhave developed a deep learning multi-label semantic segmentation network that\nsuccessfully addresses two of the main challenges in EoE diagnostics and\ndigital pathology, the need to detect several types of small features\nsimultaneously and the ability to analyze whole slides efficiently. Our results\npave the way for an automated diagnosis of EoE and can be utilized for other\nconditions with similar challenges.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 20:37:57 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Daniel", "Nati", ""], ["Larey", "Ariel", ""], ["Aknin", "Eliel", ""], ["Osswald", "Garrett A.", ""], ["Caldwell", "Julie M.", ""], ["Rochman", "Mark", ""], ["Collins", "Margaret H.", ""], ["Yang", "Guang-Yu", ""], ["Arva", "Nicoleta C.", ""], ["Capocelli", "Kelley E.", ""], ["Rothenberg", "Marc E.", ""], ["Savir", "Yonatan", ""]]}, {"id": "2103.02018", "submitter": "Siwei Lyu", "authors": "Yuezun Li, Cong Zhang, Pu Sun, Honggang Qi, and Siwei Lyu", "title": "DeepFake-o-meter: An Open Platform for DeepFake Detection", "comments": "submitted to SAPDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the advent of deep learning-based techniques and the\nsignificant reduction in the cost of computation resulted in the feasibility of\ncreating realistic videos of human faces, commonly known as DeepFakes. The\navailability of open-source tools to create DeepFakes poses as a threat to the\ntrustworthiness of the online media. In this work, we develop an open-source\nonline platform, known as DeepFake-o-meter, that integrates state-of-the-art\nDeepFake detection methods and provide a convenient interface for the users. We\ndescribe the design and function of DeepFake-o-meter in this work.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 20:45:33 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Li", "Yuezun", ""], ["Zhang", "Cong", ""], ["Sun", "Pu", ""], ["Qi", "Honggang", ""], ["Lyu", "Siwei", ""]]}, {"id": "2103.02023", "submitter": "Enzo Tartaglione", "authors": "Enzo Tartaglione, Carlo Alberto Barbano, Marco Grangetto", "title": "EnD: Entangling and Disentangling deep representations for bias\n  correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural networks perform state-of-the-art in an ever-growing number\nof tasks, and nowadays they are used to solve an incredibly large variety of\ntasks. There are problems, like the presence of biases in the training data,\nwhich question the generalization capability of these models. In this work we\npropose EnD, a regularization strategy whose aim is to prevent deep models from\nlearning unwanted biases. In particular, we insert an \"information bottleneck\"\nat a certain point of the deep neural network, where we disentangle the\ninformation about the bias, still letting the useful information for the\ntraining task forward-propagating in the rest of the model. One big advantage\nof EnD is that we do not require additional training complexity (like decoders\nor extra layers in the model), since it is a regularizer directly applied on\nthe trained model. Our experiments show that EnD effectively improves the\ngeneralization on unbiased test sets, and it can be effectively applied on\nreal-case scenarios, like removing hidden biases in the COVID-19 detection from\nradiographic images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 20:55:42 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Tartaglione", "Enzo", ""], ["Barbano", "Carlo Alberto", ""], ["Grangetto", "Marco", ""]]}, {"id": "2103.02074", "submitter": "Marvin Chanc\\'an", "authors": "Marvin Chanc\\'an, Michael Milford", "title": "Sequential Place Learning: Heuristic-Free High-Performance Long-Term\n  Place Recognition", "comments": "Submitted to RSS 2021. 14 pages, 5 tables, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequential matching using hand-crafted heuristics has been standard practice\nin route-based place recognition for enhancing pairwise similarity results for\nnearly a decade. However, precision-recall performance of these algorithms\ndramatically degrades when searching on short temporal window (TW) lengths,\nwhile demanding high compute and storage costs on large robotic datasets for\nautonomous navigation research. Here, influenced by biological systems that\nrobustly navigate spacetime scales even without vision, we develop a joint\nvisual and positional representation learning technique, via a sequential\nprocess, and design a learning-based CNN+LSTM architecture, trainable via\nbackpropagation through time, for viewpoint- and appearance-invariant place\nrecognition. Our approach, Sequential Place Learning (SPL), is based on a CNN\nfunction that visually encodes an environment from a single traversal, thus\nreducing storage capacity, while an LSTM temporally fuses each visual embedding\nwith corresponding positional data -- obtained from any source of motion\nestimation -- for direct sequential inference. Contrary to classical two-stage\npipelines, e.g., match-then-temporally-filter, our network directly eliminates\nfalse-positive rates while jointly learning sequence matching from a single\nmonocular image sequence, even using short TWs. Hence, we demonstrate that our\nmodel outperforms 15 classical methods while setting new state-of-the-art\nperformance standards on 4 challenging benchmark datasets, where one of them\ncan be considered solved with recall rates of 100% at 100% precision, correctly\nmatching all places under extreme sunlight-darkness changes. In addition, we\nshow that SPL can be up to 70x faster to deploy than classical methods on a 729\nkm route comprising 35,768 consecutive frames. Extensive experiments\ndemonstrate the... Baseline code available at\nhttps://github.com/mchancan/deepseqslam\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 22:57:43 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Chanc\u00e1n", "Marvin", ""], ["Milford", "Michael", ""]]}, {"id": "2103.02079", "submitter": "Tom Goldstein", "authors": "Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun\n  Gupta, Amin Ghiasi, Furong Huang, Micah Goldblum, Tom Goldstein", "title": "DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with\n  Differentially Private Data Augmentations", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data poisoning and backdoor attacks manipulate training data to induce\nsecurity breaches in a victim model. These attacks can be provably deflected\nusing differentially private (DP) training methods, although this comes with a\nsharp decrease in model performance. The InstaHide method has recently been\nproposed as an alternative to DP training that leverages supposed privacy\nproperties of the mixup augmentation, although without rigorous guarantees. In\nthis work, we show that strong data augmentations, such as mixup and random\nadditive noise, nullify poison attacks while enduring only a small accuracy\ntrade-off. To explain these finding, we propose a training method,\nDP-InstaHide, which combines the mixup regularizer with additive noise. A\nrigorous analysis of DP-InstaHide shows that mixup does indeed have privacy\nadvantages, and that training with k-way mixup provably yields at least k times\nstronger DP guarantees than a naive DP mechanism. Because mixup (as opposed to\nnoise) is beneficial to model performance, DP-InstaHide provides a mechanism\nfor achieving stronger empirical performance against poisoning attacks than\nother known DP methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 23:07:31 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 06:40:34 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Borgnia", "Eitan", ""], ["Geiping", "Jonas", ""], ["Cherepanova", "Valeriia", ""], ["Fowl", "Liam", ""], ["Gupta", "Arjun", ""], ["Ghiasi", "Amin", ""], ["Huang", "Furong", ""], ["Goldblum", "Micah", ""], ["Goldstein", "Tom", ""]]}, {"id": "2103.02083", "submitter": "Suman Sedai", "authors": "Suman Sedai, Bhavna Antony, Ravneet Rai, Katie Jones, Hiroshi\n  Ishikawa, Joel Schuman, Wollstein Gadi and Rahil Garnavi", "title": "Uncertainty guided semi-supervised segmentation of retinal layers in OCT\n  images", "comments": "MICCAI,19", "journal-ref": "MICCAI 2019 pp 282-290", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have shown outstanding performance in\nmedical image segmentation tasks. The usual problem when training supervised\ndeep learning methods is the lack of labeled data which is time-consuming and\ncostly to obtain. In this paper, we propose a novel uncertainty-guided\nsemi-supervised learning based on a student-teacher approach for training the\nsegmentation network using limited labeled samples and a large number of\nunlabeled images. First, a teacher segmentation model is trained from the\nlabeled samples using Bayesian deep learning. The trained model is used to\ngenerate soft segmentation labels and uncertainty maps for the unlabeled set.\nThe student model is then updated using the softly segmented samples and the\ncorresponding pixel-wise confidence of the segmentation quality estimated from\nthe uncertainty of the teacher model using a newly designed loss function.\nExperimental results on a retinal layer segmentation task show that the\nproposed method improves the segmentation performance in comparison to the\nfully supervised approach and is on par with the expert annotator. The proposed\nsemi-supervised segmentation framework is a key contribution and applicable for\nbiomedical image segmentation across various imaging modalities where access to\nannotated medical images is challenging\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 23:14:25 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Sedai", "Suman", ""], ["Antony", "Bhavna", ""], ["Rai", "Ravneet", ""], ["Jones", "Katie", ""], ["Ishikawa", "Hiroshi", ""], ["Schuman", "Joel", ""], ["Gadi", "Wollstein", ""], ["Garnavi", "Rahil", ""]]}, {"id": "2103.02093", "submitter": "Benjamin Caine", "authors": "Benjamin Caine, Rebecca Roelofs, Vijay Vasudevan, Jiquan Ngiam, Yuning\n  Chai, Zhifeng Chen, Jonathon Shlens", "title": "Pseudo-labeling for Scalable 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To safely deploy autonomous vehicles, onboard perception systems must work\nreliably at high accuracy across a diverse set of environments and geographies.\nOne of the most common techniques to improve the efficacy of such systems in\nnew domains involves collecting large labeled datasets, but such datasets can\nbe extremely costly to obtain, especially if each new deployment geography\nrequires additional data with expensive 3D bounding box annotations. We\ndemonstrate that pseudo-labeling for 3D object detection is an effective way to\nexploit less expensive and more widely available unlabeled data, and can lead\nto performance gains across various architectures, data augmentation\nstrategies, and sizes of the labeled dataset. Overall, we show that better\nteacher models lead to better student models, and that we can distill expensive\nteachers into efficient, simple students.\n  Specifically, we demonstrate that pseudo-label-trained student models can\noutperform supervised models trained on 3-10 times the amount of labeled\nexamples. Using PointPillars [24], a two-year-old architecture, as our student\nmodel, we are able to achieve state of the art accuracy simply by leveraging\nlarge quantities of pseudo-labeled data. Lastly, we show that these student\nmodels generalize better than supervised models to a new domain in which we\nonly have unlabeled data, making pseudo-label training an effective form of\nunsupervised domain adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 23:48:29 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Caine", "Benjamin", ""], ["Roelofs", "Rebecca", ""], ["Vasudevan", "Vijay", ""], ["Ngiam", "Jiquan", ""], ["Chai", "Yuning", ""], ["Chen", "Zhifeng", ""], ["Shlens", "Jonathon", ""]]}, {"id": "2103.02096", "submitter": "Ye Luo", "authors": "Shiqing Fan, Liu Liying, Ye Luo", "title": "An Alternative Practice of Tropical Convolution to Traditional\n  Convolutional Neural Networks", "comments": "10 pages, 4 figures, appear in International Conference on Compute\n  and Data Analysis (ICCDA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have been used in many machine learning\nfields. In practical applications, the computational cost of convolutional\nneural networks is often high with the deepening of the network and the growth\nof data volume, mostly due to a large amount of multiplication operations of\nfloating-point numbers in convolution operations. To reduce the amount of\nmultiplications, we propose a new type of CNNs called Tropical Convolutional\nNeural Networks (TCNNs) which are built on tropical convolutions in which the\nmultiplications and additions in conventional convolutional layers are replaced\nby additions and min/max operations respectively. In addition, since tropical\nconvolution operators are essentially nonlinear operators, we expect TCNNs to\nhave higher nonlinear fitting ability than conventional CNNs. In the\nexperiments, we test and analyze several different architectures of TCNNs for\nimage classification tasks in comparison with similar-sized conventional CNNs.\nThe results show that TCNN can achieve higher expressive power than ordinary\nconvolutional layers on the MNIST and CIFAR10 image data set. In different\nnoise environments, there are wins and losses in the robustness of TCNN and\nordinary CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 00:13:30 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 07:54:15 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Fan", "Shiqing", ""], ["Liying", "Liu", ""], ["Luo", "Ye", ""]]}, {"id": "2103.02111", "submitter": "Tixiao Shan", "authors": "Tixiao Shan, Brendan Englot, Fabio Duarte, Carlo Ratti, Daniela Rus", "title": "Robust Place Recognition using an Imaging Lidar", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a methodology for robust, real-time place recognition using an\nimaging lidar, which yields image-quality high-resolution 3D point clouds.\nUtilizing the intensity readings of an imaging lidar, we project the point\ncloud and obtain an intensity image. ORB feature descriptors are extracted from\nthe image and encoded into a bag-of-words vector. The vector, used to identify\nthe point cloud, is inserted into a database that is maintained by DBoW for\nfast place recognition queries. The returned candidate is further validated by\nmatching visual feature descriptors. To reject matching outliers, we apply PnP,\nwhich minimizes the reprojection error of visual features' positions in\nEuclidean space with their correspondences in 2D image space, using RANSAC.\nCombining the advantages from both camera and lidar-based place recognition\napproaches, our method is truly rotation-invariant, and can tackle reverse\nrevisiting and upside down revisiting. The proposed method is evaluated on\ndatasets gathered from a variety of platforms over different scales and\nenvironments. Our implementation and datasets are available at\nhttps://git.io/image-lidar\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 01:08:31 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 02:27:31 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Shan", "Tixiao", ""], ["Englot", "Brendan", ""], ["Duarte", "Fabio", ""], ["Ratti", "Carlo", ""], ["Rus", "Daniela", ""]]}, {"id": "2103.02121", "submitter": "Ye Luo", "authors": "Shiqing Fan, Ye Luo", "title": "Deblurring Processor for Motion-Blurred Faces Based on Generative\n  Adversarial Networks", "comments": "10 pages, 4 figures, appear in International Conference on Digital\n  Signal Processing (ICDSP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-quality face image restoration is a popular research direction in today's\ncomputer vision field. It can be used as a pre-work for tasks such as face\ndetection and face recognition. At present, there is a lot of work to solve the\nproblem of low-quality faces under various environmental conditions. This paper\nmainly focuses on the restoration of motion-blurred faces. In increasingly\nabundant mobile scenes, the fast recovery of motion-blurred faces can bring\nhighly effective speed improvements in tasks such as face matching. In order to\nachieve this goal, a deblurring method for motion-blurred facial image signals\nbased on generative adversarial networks(GANs) is proposed. It uses an\nend-to-end method to train a sharp image generator, i.e., a processor for\nmotion-blurred facial images. This paper introduce the processing progress of\nmotion-blurred images, the development and changes of GANs and some basic\nconcepts. After that, it give the details of network structure and training\noptimization design of the image processor. Then we conducted a motion blur\nimage generation experiment on some general facial data set, and used the pairs\nof blurred and sharp face image data to perform the training and testing\nexperiments of the processor GAN, and gave some visual displays. Finally, MTCNN\nis used to detect the faces of the image generated by the deblurring processor,\nand compare it with the result of the blurred image. From the results, the\nprocessing effect of the deblurring processor on the motion-blurred picture has\na significant improvement both in terms of intuition and evaluation indicators\nof face detection.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 01:35:02 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Fan", "Shiqing", ""], ["Luo", "Ye", ""]]}, {"id": "2103.02130", "submitter": "Yi Ding", "authors": "Kento Nishi, Yi Ding, Alex Rich, Tobias H\\\"ollerer", "title": "Augmentation Strategies for Learning with Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imperfect labels are ubiquitous in real-world datasets. Several recent\nsuccessful methods for training deep neural networks (DNNs) robust to label\nnoise have used two primary techniques: filtering samples based on loss during\na warm-up phase to curate an initial set of cleanly labeled samples, and using\nthe output of a network as a pseudo-label for subsequent loss calculations. In\nthis paper, we evaluate different augmentation strategies for algorithms\ntackling the \"learning with noisy labels\" problem. We propose and examine\nmultiple augmentation strategies and evaluate them using synthetic datasets\nbased on CIFAR-10 and CIFAR-100, as well as on the real-world dataset\nClothing1M. Due to several commonalities in these algorithms, we find that\nusing one set of augmentations for loss modeling tasks and another set for\nlearning is the most effective, improving results on the state-of-the-art and\nother previous methods. Furthermore, we find that applying augmentation during\nthe warm-up period can negatively impact the loss convergence behavior of\ncorrectly versus incorrectly labeled samples. We introduce this augmentation\nstrategy to the state-of-the-art technique and demonstrate that we can improve\nperformance across all evaluated noise levels. In particular, we improve\naccuracy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in\nabsolute accuracy, and we also improve performance on the Clothing1M dataset.\n  (K. Nishi and Y. Ding contributed equally to this work)\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:19:35 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 02:05:43 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 21:41:03 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Nishi", "Kento", ""], ["Ding", "Yi", ""], ["Rich", "Alex", ""], ["H\u00f6llerer", "Tobias", ""]]}, {"id": "2103.02140", "submitter": "Hao Liu", "authors": "Zongyong Deng, Hao Liu, Yaoxing Wang, Chenyang Wang, Zekuan Yu,\n  Xuehong Sun", "title": "PML: Progressive Margin Loss for Long-tailed Age Classification", "comments": "Accepted at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose a progressive margin loss (PML) approach for\nunconstrained facial age classification. Conventional methods make strong\nassumption on that each class owns adequate instances to outline its data\ndistribution, likely leading to bias prediction where the training samples are\nsparse across age classes. Instead, our PML aims to adaptively refine the age\nlabel pattern by enforcing a couple of margins, which fully takes in the\nin-between discrepancy of the intra-class variance, inter-class variance and\nclass center. Our PML typically incorporates with the ordinal margin and the\nvariational margin, simultaneously plugging in the globally-tuned deep neural\nnetwork paradigm. More specifically, the ordinal margin learns to exploit the\ncorrelated relationship of the real-world age labels. Accordingly, the\nvariational margin is leveraged to minimize the influence of head classes that\nmisleads the prediction of tailed samples. Moreover, our optimization carefully\nseeks a series of indicator curricula to achieve robust and efficient model\ntraining. Extensive experimental results on three face aging datasets\ndemonstrate that our PML achieves compelling performance compared to state of\nthe arts. Code will be made publicly.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:47:09 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Deng", "Zongyong", ""], ["Liu", "Hao", ""], ["Wang", "Yaoxing", ""], ["Wang", "Chenyang", ""], ["Yu", "Zekuan", ""], ["Sun", "Xuehong", ""]]}, {"id": "2103.02148", "submitter": "Pengfei Guo", "authors": "Pengfei Guo, Puyang Wang, Jinyuan Zhou, Shanshan Jiang, Vishal M.\n  Patel", "title": "Multi-institutional Collaborations for Improving Deep Learning-based\n  Magnetic Resonance Image Reconstruction Using Federated Learning", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fast and accurate reconstruction of magnetic resonance (MR) images from\nunder-sampled data is important in many clinical applications. In recent years,\ndeep learning-based methods have been shown to produce superior performance on\nMR image reconstruction. However, these methods require large amounts of data\nwhich is difficult to collect and share due to the high cost of acquisition and\nmedical data privacy regulations. In order to overcome this challenge, we\npropose a federated learning (FL) based solution in which we take advantage of\nthe MR data available at different institutions while preserving patients'\nprivacy. However, the generalizability of models trained with the FL setting\ncan still be suboptimal due to domain shift, which results from the data\ncollected at multiple institutions with different sensors, disease types, and\nacquisition protocols, etc. With the motivation of circumventing this\nchallenge, we propose a cross-site modeling for MR image reconstruction in\nwhich the learned intermediate latent features among different source sites are\naligned with the distribution of the latent features at the target site.\nExtensive experiments are conducted to provide various insights about FL for MR\nimage reconstruction. Experimental results demonstrate that the proposed\nframework is a promising direction to utilize multi-institutional data without\ncompromising patients' privacy for achieving improved MR image reconstruction.\nOur code will be available at https://github.com/guopengf/FLMRCM.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 03:04:40 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 03:02:26 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 17:00:54 GMT"}, {"version": "v4", "created": "Wed, 10 Mar 2021 22:10:18 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Guo", "Pengfei", ""], ["Wang", "Puyang", ""], ["Zhou", "Jinyuan", ""], ["Jiang", "Shanshan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2103.02152", "submitter": "Feng Liu", "authors": "Haozhe Liu, Haoqian Wu, Weicheng Xie, Feng Liu and Linlin Shen", "title": "Group-wise Inhibition based Feature Regularization for Robust\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network (CNN) is vulnerable to degraded images with\neven very small variations (e.g. corrupted and adversarial samples). One of the\npossible reasons is that CNN pays more attention to the most discriminative\nregions, but ignores the auxiliary features when learning, leading to the lack\nof feature diversity for final judgment. In our method, we propose to\ndynamically suppress significant activation values of CNN by group-wise\ninhibition, but not fixedly or randomly handle them when training. The feature\nmaps with different activation distribution are then processed separately to\ntake the feature independence into account. CNN is finally guided to learn\nricher discriminative features hierarchically for robust classification\naccording to the proposed regularization. Our method is comprehensively\nevaluated under multiple settings, including classification against\ncorruptions, adversarial attacks and low data regime. Extensive experimental\nresults show that the proposed method can achieve significant improvements in\nterms of both robustness and generalization performances, when compared with\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 03:19:32 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 07:21:11 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Liu", "Haozhe", ""], ["Wu", "Haoqian", ""], ["Xie", "Weicheng", ""], ["Liu", "Feng", ""], ["Shen", "Linlin", ""]]}, {"id": "2103.02155", "submitter": "Xiao Huang", "authors": "Xiao Huang, Di Zhu, Fan Zhang, Tao Liu, Xiao Li, Lei Zou", "title": "Sensing population distribution from satellite imagery via deep\n  learning: model selection, neighboring effect, and systematic biases", "comments": "15 pages. 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid development of remote sensing techniques provides rich,\nlarge-coverage, and high-temporal information of the ground, which can be\ncoupled with the emerging deep learning approaches that enable latent features\nand hidden geographical patterns to be extracted. This study marks the first\nattempt to cross-compare performances of popular state-of-the-art deep learning\nmodels in estimating population distribution from remote sensing images,\ninvestigate the contribution of neighboring effect, and explore the potential\nsystematic population estimation biases. We conduct an end-to-end training of\nfour popular deep learning architectures, i.e., VGG, ResNet, Xception, and\nDenseNet, by establishing a mapping between Sentinel-2 image patches and their\ncorresponding population count from the LandScan population grid. The results\nreveal that DenseNet outperforms the other three models, while VGG has the\nworst performances in all evaluating metrics under all selected neighboring\nscenarios. As for the neighboring effect, contradicting existing studies, our\nresults suggest that the increase of neighboring sizes leads to reduced\npopulation estimation performance, which is found universal for all four\nselected models in all evaluating metrics. In addition, there exists a notable,\nuniversal bias that all selected deep learning models tend to overestimate\nsparsely populated image patches and underestimate densely populated image\npatches, regardless of neighboring sizes. The methodological, experimental, and\ncontextual knowledge this study provides is expected to benefit a wide range of\nfuture studies that estimate population distribution via remote sensing\nimagery.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 03:40:24 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Huang", "Xiao", ""], ["Zhu", "Di", ""], ["Zhang", "Fan", ""], ["Liu", "Tao", ""], ["Li", "Xiao", ""], ["Zou", "Lei", ""]]}, {"id": "2103.02167", "submitter": "Zhaoqun Li", "authors": "Zhaoqun Li, Xu Liang, Dandan Fan, Jinxing Li, Wei Jia, David Zhang", "title": "Touchless Palmprint Recognition based on 3D Gabor Template and Block\n  Feature Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing demand for hand hygiene and convenience of use, palmprint\nrecognition with touchless manner made a great development recently, providing\nan effective solution for person identification. Despite many efforts that have\nbeen devoted to this area, it is still uncertain about the discriminative\nability of the contactless palmprint, especially for large-scale datasets. To\ntackle the problem, in this paper, we build a large-scale touchless palmprint\ndataset containing 2334 palms from 1167 individuals. To our best knowledge, it\nis the largest contactless palmprint image benchmark ever collected with regard\nto the number of individuals and palms. Besides, we propose a novel deep\nlearning framework for touchless palmprint recognition named 3DCPN (3D\nConvolution Palmprint recognition Network) which leverages 3D convolution to\ndynamically integrate multiple Gabor features. In 3DCPN, a novel variant of\nGabor filter is embedded into the first layer for enhancement of curve feature\nextraction. With a well-designed ensemble scheme,low-level 3D features are then\nconvolved to extract high-level features. Finally on the top, we set a\nregion-based loss function to strengthen the discriminative ability of both\nglobal and local descriptors. To demonstrate the superiority of our method,\nextensive experiments are conducted on our dataset and other popular databases\nTongJi and IITD, where the results show the proposed 3DCPN achieves\nstate-of-the-art or comparable performances.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 04:22:24 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Li", "Zhaoqun", ""], ["Liang", "Xu", ""], ["Fan", "Dandan", ""], ["Li", "Jinxing", ""], ["Jia", "Wei", ""], ["Zhang", "David", ""]]}, {"id": "2103.02184", "submitter": "Minghao Gou", "authors": "Minghao Gou, Hao-Shu Fang, Zhanda Zhu, Sheng Xu, Chenxi Wang, Cewu Lu", "title": "RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images", "comments": "Accepted by ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General object grasping is an important yet unsolved problem in the field of\nrobotics. Most of the current methods either generate grasp poses with few DoF\nthat fail to cover most of the success grasps, or only take the unstable depth\nimage or point cloud as input which may lead to poor results in some cases. In\nthis paper, we propose RGBD-Grasp, a pipeline that solves this problem by\ndecoupling 7-DoF grasp detection into two sub-tasks where RGB and depth\ninformation are processed separately. In the first stage, an encoder-decoder\nlike convolutional neural network Angle-View Net(AVN) is proposed to predict\nthe SO(3) orientation of the gripper at every location of the image.\nConsequently, a Fast Analytic Searching(FAS) module calculates the opening\nwidth and the distance of the gripper to the grasp point. By decoupling the\ngrasp detection problem and introducing the stable RGB modality, our pipeline\nalleviates the requirement for the high-quality depth image and is robust to\ndepth sensor noise. We achieve state-of-the-art results on GraspNet-1Billion\ndataset compared with several baselines. Real robot experiments on a UR5 robot\nwith an Intel Realsense camera and a Robotiq two-finger gripper show high\nsuccess rates for both single object scenes and cluttered scenes. Our code and\ntrained model will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:12:20 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Gou", "Minghao", ""], ["Fang", "Hao-Shu", ""], ["Zhu", "Zhanda", ""], ["Xu", "Sheng", ""], ["Wang", "Chenxi", ""], ["Lu", "Cewu", ""]]}, {"id": "2103.02185", "submitter": "Zhe Liu", "authors": "Zhe Liu, Yun Li, Lina Yao, Xianzhi Wang, Guodong Long", "title": "Task Aligned Generative Meta-learning for Zero-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) refers to the problem of learning to classify\ninstances from the novel classes (unseen) that are absent in the training set\n(seen). Most ZSL methods infer the correlation between visual features and\nattributes to train the classifier for unseen classes. However, such models may\nhave a strong bias towards seen classes during training. Meta-learning has been\nintroduced to mitigate the basis, but meta-ZSL methods are inapplicable when\ntasks used for training are sampled from diverse distributions. In this regard,\nwe propose a novel Task-aligned Generative Meta-learning model for Zero-shot\nlearning (TGMZ). TGMZ mitigates the potentially biased training and enables\nmeta-ZSL to accommodate real-world datasets containing diverse distributions.\nTGMZ incorporates an attribute-conditioned task-wise distribution alignment\nnetwork that projects tasks into a unified distribution to deliver an unbiased\nmodel. Our comparisons with state-of-the-art algorithms show the improvements\nof 2.1%, 3.0%, 2.5%, and 7.6% achieved by TGMZ on AWA1, AWA2, CUB, and aPY\ndatasets, respectively. TGMZ also outperforms competitors by 3.6% in\ngeneralized zero-shot learning (GZSL) setting and 7.9% in our proposed\nfusion-ZSL setting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:18:36 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Liu", "Zhe", ""], ["Li", "Yun", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Long", "Guodong", ""]]}, {"id": "2103.02186", "submitter": "Zhen Fu", "authors": "Zhen Fu, Bo Wang, Fei Chen, Xihong Wu, Jing Chen", "title": "Eye-gaze Estimation with HEOG and Neck EMG using Deep Neural Networks", "comments": "5 pages, 5 figures, submitted to EUSIPCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hearing-impaired listeners usually have troubles attending target talker in\nmulti-talker scenes, even with hearing aids (HAs). The problem can be solved\nwith eye-gaze steering HAs, which requires listeners eye-gazing on the target.\nIn a situation where head rotates, eye-gaze is subject to both behaviors of\nsaccade and head rotation. However, existing methods of eye-gaze estimation did\nnot work reliably, since the listener's strategy of eye-gaze varies and\nmeasurements of the two behaviors were not properly combined. Besides, existing\nmethods were based on hand-craft features, which could overlook some important\ninformation. In this paper, a head-fixed and a head-free experiments were\nconducted. We used horizontal electrooculography (HEOG) and neck\nelectromyography (NEMG), which separately measured saccade and head rotation to\ncommonly estimate eye-gaze. Besides traditional classifier and hand-craft\nfeatures, deep neural networks (DNN) were introduced to automatically extract\nfeatures from intact waveforms. Evaluation results showed that when the input\nwas HEOG with inertial measurement unit, the best performance of our proposed\nDNN classifiers achieved 93.3%; and when HEOG was with NEMG together, the\naccuracy reached 72.6%, higher than that with HEOG (about 71.0%) or NEMG (about\n35.7%) alone. These results indicated the feasibility to estimate eye-gaze with\nHEOG and NEMG.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:21:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Fu", "Zhen", ""], ["Wang", "Bo", ""], ["Chen", "Fei", ""], ["Wu", "Xihong", ""], ["Chen", "Jing", ""]]}, {"id": "2103.02193", "submitter": "Xingjian Li", "authors": "Abulikemu Abuduweili, Xingjian Li, Humphrey Shi, Cheng-Zhong Xu,\n  Dejing Dou", "title": "Adaptive Consistency Regularization for Semi-Supervised Transfer\n  Learning", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent studies on semi-supervised learning have shown remarkable\nprogress in leveraging both labeled and unlabeled data, most of them presume a\nbasic setting of the model is randomly initialized. In this work, we consider\nsemi-supervised learning and transfer learning jointly, leading to a more\npractical and competitive paradigm that can utilize both powerful pre-trained\nmodels from source domain as well as labeled/unlabeled data in the target\ndomain. To better exploit the value of both pre-trained weights and unlabeled\ntarget examples, we introduce adaptive consistency regularization that consists\nof two complementary components: Adaptive Knowledge Consistency (AKC) on the\nexamples between the source and target model, and Adaptive Representation\nConsistency (ARC) on the target model between labeled and unlabeled examples.\nExamples involved in the consistency regularization are adaptively selected\naccording to their potential contributions to the target task. We conduct\nextensive experiments on several popular benchmarks including CUB-200-2011, MIT\nIndoor-67, MURA, by fine-tuning the ImageNet pre-trained ResNet-50 model.\nResults show that our proposed adaptive consistency regularization outperforms\nstate-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean\nTeacher, and MixMatch. Moreover, our algorithm is orthogonal to existing\nmethods and thus able to gain additional improvements on top of MixMatch and\nFixMatch. Our code is available at\nhttps://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:46:39 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Abuduweili", "Abulikemu", ""], ["Li", "Xingjian", ""], ["Shi", "Humphrey", ""], ["Xu", "Cheng-Zhong", ""], ["Dou", "Dejing", ""]]}, {"id": "2103.02198", "submitter": "Kasumi Obi", "authors": "Kasumi Obi, Quan Huu Cap, Noriko Umegaki-Arao, Masaru Tanaka, Hitoshi\n  Iyatomi", "title": "Bulk Production Augmentation Towards Explainable Melanoma Diagnosis", "comments": "IEEE EMBS Conference on Biomedical Engineering and Sciences\n  (IECBES2020), Best Paper Award Student Category in Biomedical Imaging and\n  Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although highly accurate automated diagnostic techniques for melanoma have\nbeen reported, the realization of a system capable of providing diagnostic\nevidence based on medical indices remains an open issue because of difficulties\nin obtaining reliable training data. In this paper, we propose bulk production\naugmentation (BPA) to generate high-quality, diverse pseudo-skin tumor images\nwith the desired structural malignant features for additional training images\nfrom a limited number of labeled images. The proposed BPA acts as an effective\ndata augmentation in constructing the feature detector for the atypical pigment\nnetwork (APN), which is a key structure in melanoma diagnosis. Experiments show\nthat training with images generated by our BPA largely boosts the APN detection\nperformance by 20.0 percentage points in the area under the receiver operating\ncharacteristic curve, which is 11.5 to 13.7 points higher than that of\nconventional CycleGAN-based augmentations in AUC.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:06:31 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Obi", "Kasumi", ""], ["Cap", "Quan Huu", ""], ["Umegaki-Arao", "Noriko", ""], ["Tanaka", "Masaru", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "2103.02207", "submitter": "Shivang Agarwal", "authors": "Shivang Agarwal, C. Ravindranath Chowdary and Vivek Sourabh", "title": "EaZy Learning: An Adaptive Variant of Ensemble Learning for Fingerprint\n  Liveness Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of biometrics, fingerprint recognition systems are vulnerable to\npresentation attacks made by artificially generated spoof fingerprints.\nTherefore, it is essential to perform liveness detection of a fingerprint\nbefore authenticating it. Fingerprint liveness detection mechanisms perform\nwell under the within-dataset environment but fail miserably under cross-sensor\n(when tested on a fingerprint acquired by a new sensor) and cross-dataset (when\ntrained on one dataset and tested on another) settings. To enhance the\ngeneralization abilities, robustness and the interoperability of the\nfingerprint spoof detectors, the learning models need to be adaptive towards\nthe data. We propose a generic model, EaZy learning which can be considered as\nan adaptive midway between eager and lazy learning. We show the usefulness of\nthis adaptivity under cross-sensor and cross-dataset environments. EaZy\nlearning examines the properties intrinsic to the dataset while generating a\npool of hypotheses. EaZy learning is similar to ensemble learning as it\ngenerates an ensemble of base classifiers and integrates them to make a\nprediction. Still, it differs in the way it generates the base classifiers.\nEaZy learning develops an ensemble of entirely disjoint base classifiers which\nhas a beneficial influence on the diversity of the underlying ensemble. Also,\nit integrates the predictions made by these base classifiers based on their\nperformance on the validation data. Experiments conducted on the standard high\ndimensional datasets LivDet 2011, LivDet 2013 and LivDet 2015 prove the\nefficacy of the model under cross-dataset and cross-sensor environments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:40:19 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Agarwal", "Shivang", ""], ["Chowdary", "C. Ravindranath", ""], ["Sourabh", "Vivek", ""]]}, {"id": "2103.02211", "submitter": "Yeji Choi", "authors": "Yeji Choi, Hyunjung Park, Gi Pyo Nam, Haksub Kim, Heeseung Choi,\n  Junghyun Cho, Ig-Jae Kim", "title": "K-FACE: A Large-Scale KIST Face Database in Consideration with\n  Unconstrained Environments", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a new large-scale face database from KIST,\ndenoted as K-FACE, and describe a novel capturing device specifically designed\nto obtain the data. The K-FACE database contains more than 1 million\nhigh-quality images of 1,000 subjects selected by considering the ratio of\ngender and age groups. It includes a variety of attributes, including 27 poses,\n35 lighting conditions, three expressions, and occlusions by the combination of\nfive types of accessories. As the K-FACE database is systematically constructed\nthrough a hemispherical capturing system with elaborate lighting control and\nmultiple cameras, it is possible to accurately analyze the effects of factors\nthat cause performance degradation, such as poses, lighting changes, and\naccessories. We consider not only the balance of external environmental\nfactors, such as pose and lighting, but also the balance of personal\ncharacteristics such as gender and age group. The gender ratio is the same,\nwhile the age groups of subjects are uniformly distributed from the 20s to 50s\nfor both genders. The K-FACE database can be extensively utilized in various\nvision tasks, such as face recognition, face frontalization, illumination\nnormalization, face age estimation, and three-dimensional face model\ngeneration. We expect systematic diversity and uniformity of the K-FACE\ndatabase to promote these research fields.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:50:33 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Choi", "Yeji", ""], ["Park", "Hyunjung", ""], ["Nam", "Gi Pyo", ""], ["Kim", "Haksub", ""], ["Choi", "Heeseung", ""], ["Cho", "Junghyun", ""], ["Kim", "Ig-Jae", ""]]}, {"id": "2103.02220", "submitter": "Wenwen Yu", "authors": "Ping Gong, Wenwen Yu, Qiuwen Sun, Ruohan Zhao, Junfeng Hu", "title": "Unsupervised Domain Adaptation Network with Category-Centric Prototype\n  Aligner for Biomedical Image Segmentation", "comments": "Ping Gong and Wenwen Yu contributed equally to this work. 11 pages, 4\n  figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the widespread success of deep learning in biomedical image\nsegmentation, domain shift becomes a critical and challenging problem, as the\ngap between two domains can severely affect model performance when deployed to\nunseen data with heterogeneous features. To alleviate this problem, we present\na novel unsupervised domain adaptation network, for generalizing models learned\nfrom the labeled source domain to the unlabeled target domain for\ncross-modality biomedical image segmentation. Specifically, our approach\nconsists of two key modules, a conditional domain discriminator~(CDD) and a\ncategory-centric prototype aligner~(CCPA). The CDD, extended from conditional\ndomain adversarial networks in classifier tasks, is effective and robust in\nhandling complex cross-modality biomedical images. The CCPA, improved from the\ngraph-induced prototype alignment mechanism in cross-domain object detection,\ncan exploit precise instance-level features through an elaborate prototype\nrepresentation. In addition, it can address the negative effect of class\nimbalance via entropy-based loss. Extensive experiments on a public benchmark\nfor the cardiac substructure segmentation task demonstrate that our method\nsignificantly improves performance on the target domain.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 07:07:38 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Gong", "Ping", ""], ["Yu", "Wenwen", ""], ["Sun", "Qiuwen", ""], ["Zhao", "Ruohan", ""], ["Hu", "Junfeng", ""]]}, {"id": "2103.02221", "submitter": "Mohammed Suhail", "authors": "Mohammed Suhail, Abhay Mittal, Behjat Siddiquie, Chris Broaddus, Jayan\n  Eledath, Gerard Medioni, Leonid Sigal", "title": "Energy-Based Learning for Scene Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional scene graph generation methods are trained using cross-entropy\nlosses that treat objects and relationships as independent entities. Such a\nformulation, however, ignores the structure in the output space, in an\ninherently structured prediction problem. In this work, we introduce a novel\nenergy-based learning framework for generating scene graphs. The proposed\nformulation allows for efficiently incorporating the structure of scene graphs\nin the output space. This additional constraint in the learning framework acts\nas an inductive bias and allows models to learn efficiently from a small number\nof labels. We use the proposed energy-based framework to train existing\nstate-of-the-art models and obtain a significant performance improvement, of up\nto 21% and 27%, on the Visual Genome and GQA benchmark datasets, respectively.\nFurthermore, we showcase the learning efficiency of the proposed framework by\ndemonstrating superior performance in the zero- and few-shot settings where\ndata is scarce.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 07:11:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Suhail", "Mohammed", ""], ["Mittal", "Abhay", ""], ["Siddiquie", "Behjat", ""], ["Broaddus", "Chris", ""], ["Eledath", "Jayan", ""], ["Medioni", "Gerard", ""], ["Sigal", "Leonid", ""]]}, {"id": "2103.02242", "submitter": "Yisheng He", "authors": "Yisheng He and Haibin Huang and Haoqiang Fan and Qifeng Chen and Jian\n  Sun", "title": "FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present FFB6D, a Full Flow Bidirectional fusion network\ndesigned for 6D pose estimation from a single RGBD image. Our key insight is\nthat appearance information in the RGB image and geometry information from the\ndepth image are two complementary data sources, and it still remains unknown\nhow to fully leverage them. Towards this end, we propose FFB6D, which learns to\ncombine appearance and geometry information for representation learning as well\nas output representation selection. Specifically, at the representation\nlearning stage, we build bidirectional fusion modules in the full flow of the\ntwo networks, where fusion is applied to each encoding and decoding layer. In\nthis way, the two networks can leverage local and global complementary\ninformation from the other one to obtain better representations. Moreover, at\nthe output representation stage, we designed a simple but effective 3D\nkeypoints selection algorithm considering the texture and geometry information\nof objects, which simplifies keypoint localization for precise pose estimation.\nExperimental results show that our method outperforms the state-of-the-art by\nlarge margins on several benchmarks. Code and video are available at\n\\url{https://github.com/ethnhe/FFB6D.git}.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:07:29 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["He", "Yisheng", ""], ["Huang", "Haibin", ""], ["Fan", "Haoqiang", ""], ["Chen", "Qifeng", ""], ["Sun", "Jian", ""]]}, {"id": "2103.02243", "submitter": "Zhiyu Yao", "authors": "Haixu Wu, Zhiyu Yao, Jianmin Wang, Mingsheng Long", "title": "MotionRNN: A Flexible Model for Video Prediction with Spacetime-Varying\n  Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper tackles video prediction from a new dimension of predicting\nspacetime-varying motions that are incessantly changing across both space and\ntime. Prior methods mainly capture the temporal state transitions but overlook\nthe complex spatiotemporal variations of the motion itself, making them\ndifficult to adapt to ever-changing motions. We observe that physical world\nmotions can be decomposed into transient variation and motion trend, while the\nlatter can be regarded as the accumulation of previous motions. Thus,\nsimultaneously capturing the transient variation and the motion trend is the\nkey to make spacetime-varying motions more predictable. Based on these\nobservations, we propose the MotionRNN framework, which can capture the complex\nvariations within motions and adapt to spacetime-varying scenarios. MotionRNN\nhas two main contributions. The first is that we design the MotionGRU unit,\nwhich can model the transient variation and motion trend in a unified way. The\nsecond is that we apply the MotionGRU to RNN-based predictive models and\nindicate a new flexible video prediction architecture with a Motion Highway\nthat can significantly improve the ability to predict changeable motions and\navoid motion vanishing for stacked multiple-layer predictive models. With high\nflexibility, this framework can adapt to a series of models for deterministic\nspatiotemporal prediction. Our MotionRNN can yield significant improvements on\nthree challenging benchmarks for video prediction with spacetime-varying\nmotions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:11:50 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 08:55:59 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 01:42:51 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wu", "Haixu", ""], ["Yao", "Zhiyu", ""], ["Wang", "Jianmin", ""], ["Long", "Mingsheng", ""]]}, {"id": "2103.02250", "submitter": "Jongmin Yu", "authors": "Jongmin Yu, Hyeontaek Oh", "title": "Unsupervised Vehicle Re-Identification via Self-supervised Metric\n  Learning using Feature Dictionary", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The key challenge of unsupervised vehicle re-identification (Re-ID) is\nlearning discriminative features from unlabelled vehicle images. Numerous\nmethods using domain adaptation have achieved outstanding performance, but\nthose methods still need a labelled dataset as a source domain. This paper\naddresses an unsupervised vehicle Re-ID method, which no need any types of a\nlabelled dataset, through a Self-supervised Metric Learning (SSML) based on a\nfeature dictionary. Our method initially extracts features from vehicle images\nand stores them in a dictionary. Thereafter, based on the dictionary, the\nproposed method conducts dictionary-based positive label mining (DPLM) to\nsearch for positive labels. Pair-wise similarity, relative-rank consistency,\nand adjacent feature distribution similarity are jointly considered to find\nimages that may belong to the same vehicle of a given probe image. The results\nof DPLM are applied to dictionary-based triplet loss (DTL) to improve the\ndiscriminativeness of learnt features and to refine the quality of the results\nof DPLM progressively. The iterative process with DPLM and DTL boosts the\nperformance of unsupervised vehicle Re-ID. Experimental results demonstrate the\neffectiveness of the proposed method by producing promising vehicle Re-ID\nperformance without a pre-labelled dataset. The source code for this paper is\npublicly available on `https://github.com/andreYoo/VeRI_SSML_FD.git'.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:29:03 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Yu", "Jongmin", ""], ["Oh", "Hyeontaek", ""]]}, {"id": "2103.02263", "submitter": "Fabian Duerr", "authors": "Fabian Duerr, Mario Pfaller, Hendrik Weigel, Juergen Beyerer", "title": "LiDAR-based Recurrent 3D Semantic Segmentation with Temporal Memory\n  Alignment", "comments": null, "journal-ref": "International Conference on 3D Vision (3DV), pages 781-790, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and interpreting a 3d environment is a key challenge for\nautonomous vehicles. Semantic segmentation of 3d point clouds combines 3d\ninformation with semantics and thereby provides a valuable contribution to this\ntask. In many real-world applications, point clouds are generated by lidar\nsensors in a consecutive fashion. Working with a time series instead of single\nand independent frames enables the exploitation of temporal information. We\ntherefore propose a recurrent segmentation architecture (RNN), which takes a\nsingle range image frame as input and exploits recursively aggregated temporal\ninformation. An alignment strategy, which we call Temporal Memory Alignment,\nuses ego motion to temporally align the memory between consecutive frames in\nfeature space. A Residual Network and ConvGRU are investigated for the memory\nupdate. We demonstrate the benefits of the presented approach on two\nlarge-scale datasets and compare it to several stateof-the-art methods. Our\napproach ranks first on the SemanticKITTI multiple scan benchmark and achieves\nstate-of-the-art performance on the single scan benchmark. In addition, the\nevaluation shows that the exploitation of temporal information significantly\nimproves segmentation results compared to a single frame approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:01:45 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Duerr", "Fabian", ""], ["Pfaller", "Mario", ""], ["Weigel", "Hendrik", ""], ["Beyerer", "Juergen", ""]]}, {"id": "2103.02264", "submitter": "Mingyu Yin", "authors": "Mingyu Yin, Li Sun, Qingli Li", "title": "ID-Unet: Iterative Soft and Hard Deformation for View Synthesis", "comments": "CVPR2021(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  View synthesis is usually done by an autoencoder, in which the encoder maps a\nsource view image into a latent content code, and the decoder transforms it\ninto a target view image according to the condition. However, the source\ncontents are often not well kept in this setting, which leads to unnecessary\nchanges during the view translation. Although adding skipped connections, like\nUnet, alleviates the problem, but it often causes the failure on the view\nconformity. This paper proposes a new architecture by performing the\nsource-to-target deformation in an iterative way. Instead of simply\nincorporating the features from multiple layers of the encoder, we design soft\nand hard deformation modules, which warp the encoder features to the target\nview at different resolutions, and give results to the decoder to complement\nthe details. Particularly, the current warping flow is not only used to align\nthe feature of the same resolution, but also as an approximation to coarsely\ndeform the high resolution feature. Then the residual flow is estimated and\napplied in the high resolution, so that the deformation is built up in the\ncoarse-to-fine fashion. To better constrain the model, we synthesize a rough\ntarget view image based on the intermediate flows and their warped features.\nThe extensive ablation studies and the final results on two different data sets\nshow the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:02:00 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 06:14:32 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 06:43:16 GMT"}, {"version": "v4", "created": "Sun, 14 Mar 2021 03:03:57 GMT"}, {"version": "v5", "created": "Thu, 18 Mar 2021 06:13:30 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Yin", "Mingyu", ""], ["Sun", "Li", ""], ["Li", "Qingli", ""]]}, {"id": "2103.02278", "submitter": "Markus Horn", "authors": "Markus Horn, Ole Schumann, Markus Hahn, J\\\"urgen Dickmann, Klaus\n  Dietmayer", "title": "Motion Classification and Height Estimation of Pedestrians Using Sparse\n  Radar Data", "comments": "6 pages, 6 figures, 1 table", "journal-ref": "2018 Sensor Data Fusion: Trends, Solutions, Applications (SDF)", "doi": "10.1109/SDF.2018.8547092", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complete overview of the surrounding vehicle environment is important for\ndriver assistance systems and highly autonomous driving. Fusing results of\nmultiple sensor types like camera, radar and lidar is crucial for increasing\nthe robustness. The detection and classification of objects like cars, bicycles\nor pedestrians has been analyzed in the past for many sensor types. Beyond\nthat, it is also helpful to refine these classes and distinguish for example\nbetween different pedestrian types or activities. This task is usually\nperformed on camera data, though recent developments are based on radar\nspectrograms. However, for most automotive radar systems, it is only possible\nto obtain radar targets instead of the original spectrograms. This work\ndemonstrates that it is possible to estimate the body height of walking\npedestrians using 2D radar targets. Furthermore, different pedestrian motion\ntypes are classified.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:36:11 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Horn", "Markus", ""], ["Schumann", "Ole", ""], ["Hahn", "Markus", ""], ["Dickmann", "J\u00fcrgen", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2103.02288", "submitter": "Shoffan Saifullah", "authors": "Shoffan Saifullah", "title": "K-means Segmentation Based-on Lab Color Space for Embryo Egg Detection", "comments": "11 pages, 6 figures, ICoSiET Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The hatching process also influences the success of hatching eggs beside the\ninitial egg factor. So that the results have a large percentage of hatching, it\nis necessary to check the development of the embryo at the beginning of the\nhatching. This process aims to sort eggs that have embryos to remain hatched\nuntil the end. Maximum checking is done the first week in the hatching period.\nThis study aims to detect the presence of embryos in eggs. Detection of the\nexistence of embryos is processed using segmentation. Egg images are segmented\nusing the K-means algorithm based on Lab color images. The results of the\nimages acquisition are converted into Lab color space images. The results of\nLab color space images are processed using K-means for each color. The K-means\nprocess uses cluster k=3, where this cluster divided the image into three\nparts, namely background, eggs, and yolk eggs. Yolk eggs are part of eggs that\nhave embryonic characteristics. This study applies the concept of color in the\ninitial segmentation and grayscale in the final stages. The results of the\ninitial phase show that the image segmentation results using k-means clustering\nbased on Lab color space provide a grouping of three parts. At the grayscale\nimage processing stage, the results of color image segmentation are processed\nwith grayscaling, image enhancement, and morphology. Thus, it seems clear that\nthe yolk segmented shows the presence of egg embryos. Based on this process and\nresults, K-means segmentation based on Lab color space can be used for the\ninitial stages of the embryo detection process. The evaluation uses MSE and\nMSSIM, with values of 0.0486 and 0.9979; this can be used as a reference that\nthe results obtained can indicate the detection of embryos in egg yolk.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:03:36 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Saifullah", "Shoffan", ""]]}, {"id": "2103.02303", "submitter": "Alberto Sabater", "authors": "Alberto Sabater, I\\~nigo Alonso, Luis Montesano, Ana C. Murillo", "title": "Domain and View-point Agnostic Hand Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hand action recognition is a special case of action recognition with\napplications in human-robot interaction, virtual reality or life-logging\nsystems. Building action classifiers able to work for such heterogeneous action\ndomains is very challenging. There are very subtle changes across different\nactions from a given application but also large variations across domains (e.g.\nvirtual reality vs life-logging). This work introduces a novel skeleton-based\nhand motion representation model that tackles this problem. The framework we\npropose is agnostic to the application domain or camera recording view-point.\nWhen working on a single domain (intra-domain action classification) our\napproach performs better or similar to current state-of-the-art methods on\nwell-known hand action recognition benchmarks. And, more importantly, when\nperforming hand action recognition for action domains and camera perspectives\nwhich our approach has not been trained for (cross-domain action\nclassification), our proposed framework achieves comparable performance to\nintra-domain state-of-the-art methods. These experiments show the robustness\nand generalization capabilities of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:32:36 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 15:23:24 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Sabater", "Alberto", ""], ["Alonso", "I\u00f1igo", ""], ["Montesano", "Luis", ""], ["Murillo", "Ana C.", ""]]}, {"id": "2103.02305", "submitter": "Md Sadman Sakib", "authors": "Md Sadman Sakib", "title": "Cooking Object's State Identification Without Using Pretrained Model", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Robotic Cooking has been a very promising field. To execute a\nrecipe, a robot has to recognize different objects and their states. Contrary\nto object recognition, state identification has not been explored that much.\nBut it is very important because different recipe might require different state\nof an object. Moreover, robotic grasping depends on the state. Pretrained model\nusually perform very well in this type of tests. Our challenge was to handle\nthis problem without using any pretrained model. In this paper, we have\nproposed a CNN and trained it from scratch. The model is trained and tested on\nthe dataset from cooking state recognition challenge. We have also evaluated\nthe performance of our network from various perspective. Our model achieves\n65.8% accuracy on the unseen test dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:33:27 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Sakib", "Md Sadman", ""]]}, {"id": "2103.02325", "submitter": "Maksym Andriushchenko", "authors": "Klim Kireev, Maksym Andriushchenko, Nicolas Flammarion", "title": "On the effectiveness of adversarial training against common corruptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The literature on robustness towards common corruptions shows no consensus on\nwhether adversarial training can improve the performance in this setting.\nFirst, we show that, when used with an appropriately selected perturbation\nradius, $\\ell_p$ adversarial training can serve as a strong baseline against\ncommon corruptions. Then we explain why adversarial training performs better\nthan data augmentation with simple Gaussian noise which has been observed to be\na meaningful baseline on common corruptions. Related to this, we identify the\n$\\sigma$-overfitting phenomenon when Gaussian augmentation overfits to a\nparticular standard deviation used for training which has a significant\ndetrimental effect on common corruption accuracy. We discuss how to alleviate\nthis problem and then how to further enhance $\\ell_p$ adversarial training by\nintroducing an efficient relaxation of adversarial training with learned\nperceptual image patch similarity as the distance metric. Through experiments\non CIFAR-10 and ImageNet-100, we show that our approach does not only improve\nthe $\\ell_p$ adversarial training baseline but also has cumulative gains with\ndata augmentation methods such as AugMix, ANT, and SIN leading to\nstate-of-the-art performance on common corruptions. The code of our experiments\nis publicly available at https://github.com/tml-epfl/adv-training-corruptions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:04:09 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Kireev", "Klim", ""], ["Andriushchenko", "Maksym", ""], ["Flammarion", "Nicolas", ""]]}, {"id": "2103.02340", "submitter": "Xing Dai", "authors": "Xing Dai, Zeren Jiang, Zhao Wu, Yiping Bao, Zhicheng Wang, Si Liu,\n  Erjin Zhou", "title": "General Instance Distillation for Object Detection", "comments": "10 pages (including 2 pages of References), 5 figures, 7 tables.\n  Accepted by CVPR 2021. Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, knowledge distillation has been proved to be an effective\nsolution for model compression. This approach can make lightweight student\nmodels acquire the knowledge extracted from cumbersome teacher models. However,\nprevious distillation methods of detection have weak generalization for\ndifferent detection frameworks and rely heavily on ground truth (GT), ignoring\nthe valuable relation information between instances. Thus, we propose a novel\ndistillation method for detection tasks based on discriminative instances\nwithout considering the positive or negative distinguished by GT, which is\ncalled general instance distillation (GID). Our approach contains a general\ninstance selection module (GISM) to make full use of feature-based,\nrelation-based and response-based knowledge for distillation. Extensive results\ndemonstrate that the student model achieves significant AP improvement and even\noutperforms the teacher in various detection frameworks. Specifically,\nRetinaNet with ResNet-50 achieves 39.1% in mAP with GID on COCO dataset, which\nsurpasses the baseline 36.2% by 2.9%, and even better than the ResNet-101 based\nteacher model with 38.1% AP.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:41:26 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 02:58:38 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Dai", "Xing", ""], ["Jiang", "Zeren", ""], ["Wu", "Zhao", ""], ["Bao", "Yiping", ""], ["Wang", "Zhicheng", ""], ["Liu", "Si", ""], ["Zhou", "Erjin", ""]]}, {"id": "2103.02368", "submitter": "Honggang Chen", "authors": "Honggang Chen, Xiaohai He, Linbo Qing, Yuanyuan Wu, Chao Ren, Ce Zhu", "title": "Real-World Single Image Super-Resolution: A Brief Review", "comments": "18 pages, 12 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image super-resolution (SISR), which aims to reconstruct a\nhigh-resolution (HR) image from a low-resolution (LR) observation, has been an\nactive research topic in the area of image processing in recent decades.\nParticularly, deep learning-based super-resolution (SR) approaches have drawn\nmuch attention and have greatly improved the reconstruction performance on\nsynthetic data. Recent studies show that simulation results on synthetic data\nusually overestimate the capacity to super-resolve real-world images. In this\ncontext, more and more researchers devote themselves to develop SR approaches\nfor realistic images. This article aims to make a comprehensive review on\nreal-world single image super-resolution (RSISR). More specifically, this\nreview covers the critical publically available datasets and assessment metrics\nfor RSISR, and four major categories of RSISR methods, namely the degradation\nmodeling-based RSISR, image pairs-based RSISR, domain translation-based RSISR,\nand self-learning-based RSISR. Comparisons are also made among representative\nRSISR methods on benchmark datasets, in terms of both reconstruction quality\nand computational efficiency. Besides, we discuss challenges and promising\nresearch topics on RSISR.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:41:44 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Chen", "Honggang", ""], ["He", "Xiaohai", ""], ["Qing", "Linbo", ""], ["Wu", "Yuanyuan", ""], ["Ren", "Chao", ""], ["Zhu", "Ce", ""]]}, {"id": "2103.02370", "submitter": "Jiaxing Huang", "authors": "Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu", "title": "FSDR: Frequency Space Domain Randomization for Domain Generalization", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization aims to learn a generalizable model from a known source\ndomain for various unknown target domains. It has been studied widely by domain\nrandomization that transfers source images to different styles in spatial space\nfor learning domain-agnostic features. However, most existing randomization\nuses GANs that often lack of controls and even alter semantic structures of\nimages undesirably. Inspired by the idea of JPEG that converts spatial images\ninto multiple frequency components (FCs), we propose Frequency Space Domain\nRandomization (FSDR) that randomizes images in frequency space by keeping\ndomain-invariant FCs (DIFs) and randomizing domain-variant FCs (DVFs) only.\nFSDR has two unique features: 1) it decomposes images into DIFs and DVFs which\nallows explicit access and manipulation of them and more controllable\nrandomization; 2) it has minimal effects on semantic structures of images and\ndomain-invariant features. We examined domain variance and invariance property\nof FCs statistically and designed a network that can identify and fuse DIFs and\nDVFs dynamically through iterative learning. Extensive experiments over\nmultiple domain generalizable segmentation tasks show that FSDR achieves\nsuperior segmentation and its performance is even on par with domain adaptation\nmethods that access target data in training.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:42:28 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Huang", "Jiaxing", ""], ["Guan", "Dayan", ""], ["Xiao", "Aoran", ""], ["Lu", "Shijian", ""]]}, {"id": "2103.02376", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Wei Liao, Lei Yu, Wen Yang and Gui-Song Xia", "title": "Event-based Synthetic Aperture Imaging with a Hybrid Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic aperture imaging (SAI) is able to achieve the see through effect by\nblurring out the off-focus foreground occlusions and reconstructing the\nin-focus occluded targets from multi-view images. However, very dense\nocclusions and extreme lighting conditions may bring significant disturbances\nto the SAI based on conventional frame-based cameras, leading to performance\ndegeneration. To address these problems, we propose a novel SAI system based on\nthe event camera which can produce asynchronous events with extremely low\nlatency and high dynamic range. Thus, it can eliminate the interference of\ndense occlusions by measuring with almost continuous views, and simultaneously\ntackle the over/under exposure problems. To reconstruct the occluded targets,\nwe propose a hybrid encoder-decoder network composed of spiking neural networks\n(SNNs) and convolutional neural networks (CNNs). In the hybrid network, the\nspatio-temporal information of the collected events is first encoded by SNN\nlayers, and then transformed to the visual image of the occluded targets by a\nstyle-transfer CNN decoder. Through experiments, the proposed method shows\nremarkable performance in dealing with very dense occlusions and extreme\nlighting conditions, and high quality visual images can be reconstructed using\npure event data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:56:55 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 16:39:37 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 06:36:11 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Xiang", ""], ["Liao", "Wei", ""], ["Yu", "Lei", ""], ["Yang", "Wen", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2103.02380", "submitter": "Bin Chen", "authors": "Ruizhen Hu, Bin Chen, Juzhan Xu, Oliver van Kaick, Oliver Deussen, Hui\n  Huang", "title": "Shape-driven Coordinate Ordering for Star Glyph Sets via Reinforcement\n  Learning", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 2021", "doi": "10.1109/TVCG.2021.3052167", "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural optimization model trained with reinforcement learning to\nsolve the coordinate ordering problem for sets of star glyphs. Given a set of\nstar glyphs associated to multiple class labels, we propose to use shape\ncontext descriptors to measure the perceptual distance between pairs of glyphs,\nand use the derived silhouette coefficient to measure the perception of class\nseparability within the entire set. To find the optimal coordinate order for\nthe given set, we train a neural network using reinforcement learning to reward\norderings with high silhouette coefficients. The network consists of an encoder\nand a decoder with an attention mechanism. The encoder employs a recurrent\nneural network (RNN) to encode input shape and class information, while the\ndecoder together with the attention mechanism employs another RNN to output a\nsequence with the new coordinate order. In addition, we introduce a neural\nnetwork to efficiently estimate the similarity between shape context\ndescriptors, which allows to speed up the computation of silhouette\ncoefficients and thus the training of the axis ordering network. Two user\nstudies demonstrate that the orders provided by our method are preferred by\nusers for perceiving class separation. We tested our model on different\nsettings to show its robustness and generalization abilities and demonstrate\nthat it allows to order input sets with unseen data size, data dimension, or\nnumber of classes. We also demonstrate that our model can be adapted to\ncoordinate ordering of other types of plots such as RadViz by replacing the\nproposed shape-aware silhouette coefficient with the corresponding quality\nmetric to guide network training.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:05:10 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hu", "Ruizhen", ""], ["Chen", "Bin", ""], ["Xu", "Juzhan", ""], ["van Kaick", "Oliver", ""], ["Deussen", "Oliver", ""], ["Huang", "Hui", ""]]}, {"id": "2103.02394", "submitter": "Ping Xue", "authors": "Ping Xue, Yang Lu, Jingfei Chang, Xing Wei, Zhen Wei", "title": "Self-Distribution Binary Neural Networks", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the binary neural networks (BNNs) of which both the\nweights and activations are binary (i.e., 1-bit representation). Feature\nrepresentation is critical for deep neural networks, while in BNNs, the\nfeatures only differ in signs. Prior work introduces scaling factors into\nbinary weights and activations to reduce the quantization error and effectively\nimproves the classification accuracy of BNNs. However, the scaling factors not\nonly increase the computational complexity of networks, but also make no sense\nto the signs of binary features. To this end, Self-Distribution Binary Neural\nNetwork (SD-BNN) is proposed. Firstly, we utilize Activation Self Distribution\n(ASD) to adaptively adjust the sign distribution of activations, thereby\nimprove the sign differences of the outputs of the convolution. Secondly, we\nadjust the sign distribution of weights through Weight Self Distribution (WSD)\nand then fine-tune the sign distribution of the outputs of the convolution.\nExtensive experiments on CIFAR-10 and ImageNet datasets with various network\nstructures show that the proposed SD-BNN consistently outperforms the\nstate-of-the-art (SOTA) BNNs (e.g., achieves 92.5% on CIFAR-10 and 66.5% on\nImageNet with ResNet-18) with less computation cost. Code is available at\nhttps://github.com/ pingxue-hfut/SD-BNN.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:39:52 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 02:17:03 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Xue", "Ping", ""], ["Lu", "Yang", ""], ["Chang", "Jingfei", ""], ["Wei", "Xing", ""], ["Wei", "Zhen", ""]]}, {"id": "2103.02396", "submitter": "Yu-Kai Huang", "authors": "Yu-Kai Huang, Yueh-Cheng Liu, Tsung-Han Wu, Hung-Ting Su, Yu-Cheng\n  Chang, Tsung-Lin Tsou, Yu-An Wang, and Winston H. Hsu", "title": "$S^3$: Learnable Sparse Signal Superdensity for Guided Depth Estimation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dense depth estimation plays a key role in multiple applications such as\nrobotics, 3D reconstruction, and augmented reality. While sparse signal, e.g.,\nLiDAR and Radar, has been leveraged as guidance for enhancing dense depth\nestimation, the improvement is limited due to its low density and imbalanced\ndistribution. To maximize the utility from the sparse source, we propose $S^3$\ntechnique, which expands the depth value from sparse cues while estimating the\nconfidence of expanded region. The proposed $S^3$ can be applied to various\nguided depth estimation approaches and trained end-to-end at different stages,\nincluding input, cost volume and output. Extensive experiments demonstrate the\neffectiveness, robustness, and flexibility of the $S^3$ technique on LiDAR and\nRadar signal.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:44:21 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 14:06:45 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 13:16:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Huang", "Yu-Kai", ""], ["Liu", "Yueh-Cheng", ""], ["Wu", "Tsung-Han", ""], ["Su", "Hung-Ting", ""], ["Chang", "Yu-Cheng", ""], ["Tsou", "Tsung-Lin", ""], ["Wang", "Yu-An", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2103.02406", "submitter": "Hanqing Zhao", "authors": "Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Tianyi Wei, Weiming Zhang,\n  Nenghai Yu", "title": "Multi-attentional Deepfake Detection", "comments": "CVPR2021 preview", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face forgery by deepfake is widely spread over the internet and has raised\nsevere societal concerns. Recently, how to detect such forgery contents has\nbecome a hot research topic and many deepfake detection methods have been\nproposed. Most of them model deepfake detection as a vanilla binary\nclassification problem, i.e, first use a backbone network to extract a global\nfeature and then feed it into a binary classifier (real/fake). But since the\ndifference between the real and fake images in this task is often subtle and\nlocal, we argue this vanilla solution is not optimal. In this paper, we instead\nformulate deepfake detection as a fine-grained classification problem and\npropose a new multi-attentional deepfake detection network. Specifically, it\nconsists of three key components: 1) multiple spatial attention heads to make\nthe network attend to different local parts; 2) textural feature enhancement\nblock to zoom in the subtle artifacts in shallow features; 3) aggregate the\nlow-level textural feature and high-level semantic features guided by the\nattention maps. Moreover, to address the learning difficulty of this network,\nwe further introduce a new regional independence loss and an attention guided\ndata augmentation strategy. Through extensive experiments on different\ndatasets, we demonstrate the superiority of our method over the vanilla binary\nclassifier counterparts, and achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:56:14 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 02:09:39 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 13:10:36 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhao", "Hanqing", ""], ["Zhou", "Wenbo", ""], ["Chen", "Dongdong", ""], ["Wei", "Tianyi", ""], ["Zhang", "Weiming", ""], ["Yu", "Nenghai", ""]]}, {"id": "2103.02429", "submitter": "Rahul Ghosh Mr.", "authors": "Rahul Ghosh, Xiaowei Jia, Vipin Kumar", "title": "Land Cover Mapping in Limited Labels Scenario: A Survey", "comments": "8 pages, 1 figure. Submitted to IJCAI 2021: Survey Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Land cover mapping is essential for monitoring global environmental change\nand managing natural resources. Unfortunately, traditional classification\nmodels are plagued by limited training data available in existing land cover\nproducts and data heterogeneity over space and time. In this survey, we provide\na structured and comprehensive overview of challenges in land cover mapping and\nmachine learning methods used to address these problems. We also discuss the\ngaps and opportunities that exist for advancing research in this promising\ndirection.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:33:29 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ghosh", "Rahul", ""], ["Jia", "Xiaowei", ""], ["Kumar", "Vipin", ""]]}, {"id": "2103.02433", "submitter": "Hengli Wang", "authors": "Hengli Wang, Rui Fan, Yuxiang Sun, Ming Liu", "title": "Dynamic Fusion Module Evolves Drivable Area and Road Anomaly Detection:\n  A Benchmark and Algorithms", "comments": "11 pages, 12 figures and 5 tables. This paper is accepted by IEEE\n  T-Cyber", "journal-ref": null, "doi": "10.1109/TCYB.2021.3064089", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint detection of drivable areas and road anomalies is very important for\nmobile robots. Recently, many semantic segmentation approaches based on\nconvolutional neural networks (CNNs) have been proposed for pixel-wise drivable\narea and road anomaly detection. In addition, some benchmark datasets, such as\nKITTI and Cityscapes, have been widely used. However, the existing benchmarks\nare mostly designed for self-driving cars. There lacks a benchmark for ground\nmobile robots, such as robotic wheelchairs. Therefore, in this paper, we first\nbuild a drivable area and road anomaly detection benchmark for ground mobile\nrobots, evaluating the existing state-of-the-art single-modal and data-fusion\nsemantic segmentation CNNs using six modalities of visual features.\nFurthermore, we propose a novel module, referred to as the dynamic fusion\nmodule (DFM), which can be easily deployed in existing data-fusion networks to\nfuse different types of visual features effectively and efficiently. The\nexperimental results show that the transformed disparity image is the most\ninformative visual feature and the proposed DFM-RTFNet outperforms the\nstate-of-the-arts. Additionally, our DFM-RTFNet achieves competitive\nperformance on the KITTI road benchmark. Our benchmark is publicly available at\nhttps://sites.google.com/view/gmrb.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:38:27 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 06:01:08 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Hengli", ""], ["Fan", "Rui", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""]]}, {"id": "2103.02440", "submitter": "Sven Kreiss", "authors": "Sven Kreiss, Lorenzo Bertoni, Alexandre Alahi", "title": "OpenPifPaf: Composite Fields for Semantic Keypoint Detection and\n  Spatio-Temporal Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image-based perception tasks can be formulated as detecting, associating\nand tracking semantic keypoints, e.g., human body pose estimation and tracking.\nIn this work, we present a general framework that jointly detects and forms\nspatio-temporal keypoint associations in a single stage, making this the first\nreal-time pose detection and tracking algorithm. We present a generic neural\nnetwork architecture that uses Composite Fields to detect and construct a\nspatio-temporal pose which is a single, connected graph whose nodes are the\nsemantic keypoints (e.g., a person's body joints) in multiple frames. For the\ntemporal associations, we introduce the Temporal Composite Association Field\n(TCAF) which requires an extended network architecture and training method\nbeyond previous Composite Fields. Our experiments show competitive accuracy\nwhile being an order of magnitude faster on multiple publicly available\ndatasets such as COCO, CrowdPose and the PoseTrack 2017 and 2018 datasets. We\nalso show that our method generalizes to any class of semantic keypoints such\nas car and animal parts to provide a holistic perception framework that is well\nsuited for urban mobility such as self-driving cars and delivery robots.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:44:14 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Kreiss", "Sven", ""], ["Bertoni", "Lorenzo", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2103.02451", "submitter": "Hemang Chawla", "authors": "Hemang Chawla, Arnav Varma, Elahe Arani, Bahram Zonooz", "title": "Multimodal Scale Consistency and Awareness for Monocular Self-Supervised\n  Depth Estimation", "comments": "Accepted at 2021 IEEE International Conference on Robotics and\n  Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense depth estimation is essential to scene-understanding for autonomous\ndriving. However, recent self-supervised approaches on monocular videos suffer\nfrom scale-inconsistency across long sequences. Utilizing data from the\nubiquitously copresent global positioning systems (GPS), we tackle this\nchallenge by proposing a dynamically-weighted GPS-to-Scale (g2s) loss to\ncomplement the appearance-based losses. We emphasize that the GPS is needed\nonly during the multimodal training, and not at inference. The relative\ndistance between frames captured through the GPS provides a scale signal that\nis independent of the camera setup and scene distribution, resulting in richer\nlearned feature representations. Through extensive evaluation on multiple\ndatasets, we demonstrate scale-consistent and -aware depth estimation during\ninference, improving the performance even when training with low-frequency GPS\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:39:41 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Chawla", "Hemang", ""], ["Varma", "Arnav", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2103.02458", "submitter": "Idan Kligvasser", "authors": "Idan Kligvasser, Tomer Michaeli", "title": "Sparsity Aware Normalization for GANs", "comments": "AAAI Conference on Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are known to benefit from\nregularization or normalization of their critic (discriminator) network during\ntraining. In this paper, we analyze the popular spectral normalization scheme,\nfind a significant drawback and introduce sparsity aware normalization (SAN), a\nnew alternative approach for stabilizing GAN training. As opposed to other\nnormalization methods, our approach explicitly accounts for the sparse nature\nof the feature maps in convolutional networks with ReLU activations. We\nillustrate the effectiveness of our method through extensive experiments with a\nvariety of network architectures. As we show, sparsity is particularly dominant\nin critics used for image-to-image translation settings. In these cases our\napproach improves upon existing methods, in less training epochs and with\nsmaller capacity networks, while requiring practically no computational\noverhead.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:05:18 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 11:56:43 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kligvasser", "Idan", ""], ["Michaeli", "Tomer", ""]]}, {"id": "2103.02465", "submitter": "Shahnewaz Ali", "authors": "Shahnewaz Ali, Dr. Yaqub Jonmohamadi, Yu Takeda, Jonathan Roberts,\n  Ross Crawford, Cameron Brown, Dr. Ajay K. Pandey", "title": "Arthroscopic Multi-Spectral Scene Segmentation Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knee arthroscopy is a minimally invasive surgical (MIS) procedure which is\nperformed to treat knee-joint ailment. Lack of visual information of the\nsurgical site obtained from miniaturized cameras make this surgical procedure\nmore complex. Knee cavity is a very confined space; therefore, surgical scenes\nare captured at close proximity. Insignificant context of knee atlas often\nmakes them unrecognizable as a consequence unintentional tissue damage often\noccurred and shows a long learning curve to train new surgeons. Automatic\ncontext awareness through labeling of the surgical site can be an alternative\nto mitigate these drawbacks. However, from the previous studies, it is\nconfirmed that the surgical site exhibits several limitations, among others,\nlack of discriminative contextual information such as texture and features\nwhich drastically limits this vision task. Additionally, poor imaging\nconditions and lack of accurate ground-truth labels are also limiting the\naccuracy. To mitigate these limitations of knee arthroscopy, in this work we\nproposed a scene segmentation method that successfully segments multi\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:23:04 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ali", "Shahnewaz", ""], ["Jonmohamadi", "Dr. Yaqub", ""], ["Takeda", "Yu", ""], ["Roberts", "Jonathan", ""], ["Crawford", "Ross", ""], ["Brown", "Cameron", ""], ["Pandey", "Dr. Ajay K.", ""]]}, {"id": "2103.02484", "submitter": "Javier Hernandez", "authors": "Javier Hernandez, Daniel McDuff, Ognjen (Oggi) Rudovic, Alberto Fung,\n  Mary Czerwinski", "title": "DeepFN: Towards Generalizable Facial Action Unit Recognition with Deep\n  Face Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action unit recognition has many applications from market research to\npsychotherapy and from image captioning to entertainment. Despite its recent\nprogress, deployment of these models has been impeded due to their limited\ngeneralization to unseen people and demographics. This work conducts an\nin-depth analysis of performance across several dimensions: individuals(40\nsubjects), genders (male and female), skin types (darker and lighter), and\ndatabases (BP4D and DISFA). To help suppress the variance in data, we use the\nnotion of self-supervised denoising autoencoders to design a method for deep\nface normalization(DeepFN) that transfers facial expressions of different\npeople onto a common facial template which is then used to train and evaluate\nfacial action recognition models. We show that person-independent models yield\nsignificantly lower performance (55% average F1 and accuracy across 40\nsubjects) than person-dependent models (60.3%), leading to a generalization gap\nof 5.3%. However, normalizing the data with the newly introduced DeepFN\nsignificantly increased the performance of person-independent models (59.6%),\neffectively reducing the gap. Similarly, we observed generalization gaps when\nconsidering gender (2.4%), skin type (5.3%), and dataset (9.4%), which were\nsignificantly reduced with the use of DeepFN. These findings represent an\nimportant step towards the creation of more generalizable facial action unit\nrecognition systems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:50:51 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hernandez", "Javier", "", "Oggi"], ["McDuff", "Daniel", "", "Oggi"], ["Ognjen", "", "", "Oggi"], ["Rudovic", "", ""], ["Fung", "Alberto", ""], ["Czerwinski", "Mary", ""]]}, {"id": "2103.02488", "submitter": "Su Zhipeng", "authors": "Zhipeng Su, Yixiong Zhang, Xiao-Ping Zhang, Feng Qi", "title": "Non-local Channel Aggregation Network for Single Image Rain Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streaks showing in images or videos would severely degrade the\nperformance of computer vision applications. Thus, it is of vital importance to\nremove rain streaks and facilitate our vision systems. While recent\nconvolutinal neural network based methods have shown promising results in\nsingle image rain removal (SIRR), they fail to effectively capture long-range\nlocation dependencies or aggregate convolutional channel information\nsimultaneously. However, as SIRR is a highly illposed problem, these spatial\nand channel information are very important clues to solve SIRR. First, spatial\ninformation could help our model to understand the image context by gathering\nlong-range dependency location information hidden in the image. Second,\naggregating channels could help our model to concentrate on channels more\nrelated to image background instead of rain streaks. In this paper, we propose\na non-local channel aggregation network (NCANet) to address the SIRR problem.\nNCANet models 2D rainy images as sequences of vectors in three directions,\nnamely vertical direction, transverse direction and channel direction.\nRecurrently aggregating information from all three directions enables our model\nto capture the long-range dependencies in both channels and spaitials\nlocations. Extensive experiments on both heavy and light rain image data sets\ndemonstrate the effectiveness of the proposed NCANet model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:57:37 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Su", "Zhipeng", ""], ["Zhang", "Yixiong", ""], ["Zhang", "Xiao-Ping", ""], ["Qi", "Feng", ""]]}, {"id": "2103.02496", "submitter": "Saman Motamed", "authors": "Saman Motamed and Farzad Khalvati", "title": "Vanishing Twin GAN: How training a weak Generative Adversarial Network\n  can improve semi-supervised image classification", "comments": "arXiv admin note: substantial text overlap with arXiv:2102.06944", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks can learn the mapping of random noise to\nrealistic images in a semi-supervised framework. This mapping ability can be\nused for semi-supervised image classification to detect images of an unknown\nclass where there is no training data to be used for supervised classification.\nHowever, if the unknown class shares similar characteristics to the known\nclass(es), GANs can learn to generalize and generate images that look like both\nclasses. This generalization ability can hinder the classification performance.\nIn this work, we propose the Vanishing Twin GAN. By training a weak GAN and\nusing its generated output image parallel to the regular GAN, the Vanishing\nTwin training improves semi-supervised image classification where image\nsimilarity can hurt classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:08:27 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Motamed", "Saman", ""], ["Khalvati", "Farzad", ""]]}, {"id": "2103.02503", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, Chen Change Loy", "title": "Domain Generalization in Vision: A Survey", "comments": "v4: includes the word \"vision\" in the title; improves the\n  organization and clarity in Section 2-3; adds future directions; and more", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Since first introduced in 2011, research in DG has made great\nprogresses. In particular, intensive research in this topic has led to a broad\nspectrum of methodologies, e.g., those based on domain alignment,\nmeta-learning, data augmentation, or ensemble learning, just to name a few; and\nhas covered various vision applications such as object recognition,\nsegmentation, action recognition, and person re-identification. In this paper,\nfor the first time a comprehensive literature review is provided to summarize\nthe developments in DG for computer vision over the past decade. Specifically,\nwe first cover the background by formally defining DG and relating it to other\nresearch fields like domain adaptation and transfer learning. Second, we\nconduct a thorough review into existing methods and present a categorization\nbased on their methodologies and motivations. Finally, we conclude this survey\nwith insights and discussions on future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:12:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 11:23:32 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 04:48:14 GMT"}, {"version": "v4", "created": "Sun, 18 Jul 2021 04:28:15 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Liu", "Ziwei", ""], ["Qiao", "Yu", ""], ["Xiang", "Tao", ""], ["Loy", "Chen Change", ""]]}, {"id": "2103.02517", "submitter": "Yecheng Lyu", "authors": "Yecheng Lyu, Xinming Huang, Ziming Zhang", "title": "EllipsoidNet: Ellipsoid Representation for Point Cloud Classification\n  and Segmentation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud patterns are hard to learn because of the implicit local geometry\nfeatures among the orderless points. In recent years, point cloud\nrepresentation in 2D space has attracted increasing research interest since it\nexposes the local geometry features in a 2D space. By projecting those points\nto a 2D feature map, the relationship between points is inherited in the\ncontext between pixels, which are further extracted by a 2D convolutional\nneural network. However, existing 2D representing methods are either accuracy\nlimited or time-consuming. In this paper, we propose a novel 2D representation\nmethod that projects a point cloud onto an ellipsoid surface space, where local\npatterns are well exposed in ellipsoid-level and point-level. Additionally, a\nnovel convolutional neural network named EllipsoidNet is proposed to utilize\nthose features for point cloud classification and segmentation applications.\nThe proposed methods are evaluated in ModelNet40 and ShapeNet benchmarks, where\nthe advantages are clearly shown over existing 2D representation methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:43:08 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lyu", "Yecheng", ""], ["Huang", "Xinming", ""], ["Zhang", "Ziming", ""]]}, {"id": "2103.02521", "submitter": "Alec Diaz-Arias", "authors": "Alec Diaz-Arias, Mitchell Messmore, Dmitriy Shin, and Stephen Baek", "title": "On the role of depth predictions for 3D human pose estimation", "comments": "13 pages, 6 figures, and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the successful application of deep convolutional neural networks to\n2d human pose estimation, the next logical problem to solve is 3d human pose\nestimation from monocular images. While previous solutions have shown some\nsuccess, they do not fully utilize the depth information from the 2d inputs.\nWith the goal of addressing this depth ambiguity, we build a system that takes\n2d joint locations as input along with their estimated depth value and predicts\ntheir 3d positions in camera coordinates. Given the inherent noise and\ninaccuracy from estimating depth maps from monocular images, we perform an\nextensive statistical analysis showing that given this noise there is still a\nstatistically significant correlation between the predicted depth values and\nthe third coordinate of camera coordinates. We further explain how the\nstate-of-the-art results we achieve on the H3.6M validation set are due to the\nadditional input of depth. Notably, our results are produced on neural network\nthat accepts a low dimensional input and be integrated into a real-time system.\nFurthermore, our system can be combined with an off-the-shelf 2d pose detector\nand a depth map predictor to perform 3d pose estimation in the wild.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:51:38 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Diaz-Arias", "Alec", ""], ["Messmore", "Mitchell", ""], ["Shin", "Dmitriy", ""], ["Baek", "Stephen", ""]]}, {"id": "2103.02535", "submitter": "Chulin Xie", "authors": "Chulin Xie, Chuxin Wang, Bo Zhang, Hao Yang, Dong Chen, Fang Wen", "title": "Style-based Point Generator with Adversarial Rendering for Point Cloud\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a novel Style-based Point Generator with\nAdversarial Rendering (SpareNet) for point cloud completion. Firstly, we\npresent the channel-attentive EdgeConv to fully exploit the local structures as\nwell as the global shape in point features. Secondly, we observe that the\nconcatenation manner used by vanilla foldings limits its potential of\ngenerating a complex and faithful shape. Enlightened by the success of\nStyleGAN, we regard the shape feature as style code that modulates the\nnormalization layers during the folding, which considerably enhances its\ncapability. Thirdly, we realize that existing point supervisions, e.g., Chamfer\nDistance or Earth Mover's Distance, cannot faithfully reflect the perceptual\nquality of the reconstructed points. To address this, we propose to project the\ncompleted points to depth maps with a differentiable renderer and apply\nadversarial training to advocate the perceptual realism under different\nviewpoints. Comprehensive experiments on ShapeNet and KITTI prove the\neffectiveness of our method, which achieves state-of-the-art quantitative\nperformance while offering superior visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:21:29 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 08:40:17 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 06:25:44 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Xie", "Chulin", ""], ["Wang", "Chuxin", ""], ["Zhang", "Bo", ""], ["Yang", "Hao", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""]]}, {"id": "2103.02561", "submitter": "Cher Bass", "authors": "Cher Bass, Mariana da Silva, Carole Sudre, Logan Z. J. Williams,\n  Petru-Daniel Tudosiu, Fidel Alfaro-Almagro, Sean P. Fitzgibbon, Matthew F.\n  Glasser, Stephen M. Smith, Emma C. Robinson", "title": "ICAM-reg: Interpretable Classification and Regression with Feature\n  Attribution for Mapping Neurological Phenotypes in Individual Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important goal of medical imaging is to be able to precisely detect\npatterns of disease specific to individual scans; however, this is challenged\nin brain imaging by the degree of heterogeneity of shape and appearance.\nTraditional methods, based on image registration to a global template,\nhistorically fail to detect variable features of disease, as they utilise\npopulation-based analyses, suited primarily to studying group-average effects.\nIn this paper we therefore take advantage of recent developments in generative\ndeep learning to develop a method for simultaneous classification, or\nregression, and feature attribution (FA). Specifically, we explore the use of a\nVAE-GAN translation network called ICAM, to explicitly disentangle class\nrelevant features from background confounds for improved interpretability and\nregression of neurological phenotypes. We validate our method on the tasks of\nMini-Mental State Examination (MMSE) cognitive test score prediction for the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) cohort, as well as brain age\nprediction, for both neurodevelopment and neurodegeneration, using the\ndeveloping Human Connectome Project (dHCP) and UK Biobank datasets. We show\nthat the generated FA maps can be used to explain outlier predictions and\ndemonstrate that the inclusion of a regression module improves the\ndisentanglement of the latent space. Our code is freely available on Github\nhttps://github.com/CherBass/ICAM.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:55:14 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Bass", "Cher", ""], ["da Silva", "Mariana", ""], ["Sudre", "Carole", ""], ["Williams", "Logan Z. J.", ""], ["Tudosiu", "Petru-Daniel", ""], ["Alfaro-Almagro", "Fidel", ""], ["Fitzgibbon", "Sean P.", ""], ["Glasser", "Matthew F.", ""], ["Smith", "Stephen M.", ""], ["Robinson", "Emma C.", ""]]}, {"id": "2103.02574", "submitter": "Nelson Nauata Junior", "authors": "Nelson Nauata, Sepidehsadat Hosseini, Kai-Hung Chang, Hang Chu,\n  Chin-Yi Cheng, Yasutaka Furukawa", "title": "House-GAN++: Generative Adversarial Layout Refinement Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel generative adversarial layout refinement network\nfor automated floorplan generation. Our architecture is an integration of a\ngraph-constrained relational GAN and a conditional GAN, where a previously\ngenerated layout becomes the next input constraint, enabling iterative\nrefinement. A surprising discovery of our research is that a simple\nnon-iterative training process, dubbed component-wise GT-conditioning, is\neffective in learning such a generator. The iterative generator also creates a\nnew opportunity in further improving a metric of choice via meta-optimization\ntechniques by controlling when to pass which input constraints during iterative\nlayout refinement. Our qualitative and quantitative evaluation based on the\nthree standard metrics demonstrate that the proposed system makes significant\nimprovements over the current state-of-the-art, even competitive against the\nground-truth floorplans, designed by professional architects.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:15:52 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Nauata", "Nelson", ""], ["Hosseini", "Sepidehsadat", ""], ["Chang", "Kai-Hung", ""], ["Chu", "Hang", ""], ["Cheng", "Chin-Yi", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "2103.02583", "submitter": "Rohan Shad", "authors": "Rohan Shad, Nicolas Quach, Robyn Fong, Patpilai Kasinpila, Cayley\n  Bowles, Kate M. Callon, Michelle C. Li, Jeffrey Teuteberg, John P.\n  Cunningham, Curtis P. Langlotz, William Hiesinger", "title": "Simulating time to event prediction with spatiotemporal echocardiography\n  deep learning", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating methods for time-to-event prediction with diagnostic imaging\nmodalities is of considerable interest, as accurate estimates of survival\nrequires accounting for censoring of individuals within the observation period.\nNew methods for time-to-event prediction have been developed by extending the\ncox-proportional hazards model with neural networks. In this paper, to explore\nthe feasibility of these methods when applied to deep learning with\nechocardiography videos, we utilize the Stanford EchoNet-Dynamic dataset with\nover 10,000 echocardiograms, and generate simulated survival datasets based on\nthe expert annotated ejection fraction readings. By training on just the\nsimulated survival outcomes, we show that spatiotemporal convolutional neural\nnetworks yield accurate survival estimates.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:28:33 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Shad", "Rohan", ""], ["Quach", "Nicolas", ""], ["Fong", "Robyn", ""], ["Kasinpila", "Patpilai", ""], ["Bowles", "Cayley", ""], ["Callon", "Kate M.", ""], ["Li", "Michelle C.", ""], ["Teuteberg", "Jeffrey", ""], ["Cunningham", "John P.", ""], ["Langlotz", "Curtis P.", ""], ["Hiesinger", "William", ""]]}, {"id": "2103.02584", "submitter": "Jiaxing Huang", "authors": "Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu", "title": "Cross-View Regularization for Domain Adaptive Panoptic Segmentation", "comments": "Accepted to CVPR 2021 as an Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation unifies semantic segmentation and instance segmentation\nwhich has been attracting increasing attention in recent years. However, most\nexisting research was conducted under a supervised learning setup whereas\nunsupervised domain adaptive panoptic segmentation which is critical in\ndifferent tasks and applications is largely neglected. We design a domain\nadaptive panoptic segmentation network that exploits inter-style consistency\nand inter-task regularization for optimal domain adaptive panoptic\nsegmentation. The inter-style consistency leverages geometric invariance across\nthe same image of the different styles which fabricates certain\nself-supervisions to guide the network to learn domain-invariant features. The\ninter-task regularization exploits the complementary nature of instance\nsegmentation and semantic segmentation and uses it as a constraint for better\nfeature alignment across domains. Extensive experiments over multiple domain\nadaptive panoptic segmentation tasks (e.g., synthetic-to-real and real-to-real)\nshow that our proposed network achieves superior segmentation performance as\ncompared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:29:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Huang", "Jiaxing", ""], ["Guan", "Dayan", ""], ["Xiao", "Aoran", ""], ["Lu", "Shijian", ""]]}, {"id": "2103.02597", "submitter": "Zhaoyang Lv", "authors": "Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph\n  Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele,\n  Zhaoyang Lv", "title": "Neural 3D Video Synthesis", "comments": "Project website: https://neural-3d-video.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach for 3D video synthesis that is able to represent\nmulti-view video recordings of a dynamic real-world scene in a compact, yet\nexpressive representation that enables high-quality view synthesis and motion\ninterpolation. Our approach takes the high quality and compactness of static\nneural radiance fields in a new direction: to a model-free, dynamic setting. At\nthe core of our approach is a novel time-conditioned neural radiance fields\nthat represents scene dynamics using a set of compact latent codes. To exploit\nthe fact that changes between adjacent frames of a video are typically small\nand locally consistent, we propose two novel strategies for efficient training\nof our neural network: 1) An efficient hierarchical training scheme, and 2) an\nimportance sampling strategy that selects the next rays for training based on\nthe temporal variation of the input videos. In combination, these two\nstrategies significantly boost the training speed, lead to fast convergence of\nthe training process, and enable high quality results. Our learned\nrepresentation is highly compact and able to represent a 10 second 30 FPS\nmulti-view video recording by 18 cameras with a model size of just 28MB. We\ndemonstrate that our method can render high-fidelity wide-angle novel views at\nover 1K resolution, even for highly complex and dynamic scenes. We perform an\nextensive qualitative and quantitative evaluation that shows that our approach\noutperforms the current state of the art. We include additional video and\ninformation at: https://neural-3d-video.github.io/\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:47:40 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Li", "Tianye", ""], ["Slavcheva", "Mira", ""], ["Zollhoefer", "Michael", ""], ["Green", "Simon", ""], ["Lassner", "Christoph", ""], ["Kim", "Changil", ""], ["Schmidt", "Tanner", ""], ["Lovegrove", "Steven", ""], ["Goesele", "Michael", ""], ["Lv", "Zhaoyang", ""]]}, {"id": "2103.02603", "submitter": "Joseph K J", "authors": "K J Joseph, Salman Khan, Fahad Shahbaz Khan, Vineeth N Balasubramanian", "title": "Towards Open World Object Detection", "comments": "To appear in CVPR 2021 as an ORAL paper. Code is available in\n  https://github.com/JosephKJ/OWOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans have a natural instinct to identify unknown object instances in their\nenvironments. The intrinsic curiosity about these unknown instances aids in\nlearning about them, when the corresponding knowledge is eventually available.\nThis motivates us to propose a novel computer vision problem called: `Open\nWorld Object Detection', where a model is tasked to: 1) identify objects that\nhave not been introduced to it as `unknown', without explicit supervision to do\nso, and 2) incrementally learn these identified unknown categories without\nforgetting previously learned classes, when the corresponding labels are\nprogressively received. We formulate the problem, introduce a strong evaluation\nprotocol and provide a novel solution, which we call ORE: Open World Object\nDetector, based on contrastive clustering and energy based unknown\nidentification. Our experimental evaluation and ablation studies analyze the\nefficacy of ORE in achieving Open World objectives. As an interesting\nby-product, we find that identifying and characterizing unknown instances helps\nto reduce confusion in an incremental object detection setting, where we\nachieve state-of-the-art performance, with no extra methodological effort. We\nhope that our work will attract further research into this newly identified,\nyet crucial research direction.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:58:18 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 06:50:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Joseph", "K J", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2103.02662", "submitter": "Jiabo Huang", "authors": "Jiabo Huang and Shaogang Gong", "title": "Deep Clustering by Semantic Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst contrastive learning has achieved remarkable success in\nself-supervised representation learning, its potential for deep clustering\nremains unknown. This is due to its fundamental limitation that the instance\ndiscrimination strategy it takes is not class sensitive and hence unable to\nreason about the underlying decision boundaries between semantic concepts or\nclasses. In this work, we solve this problem by introducing a novel variant\ncalled Semantic Contrastive Learning (SCL). It explores the characteristics of\nboth conventional contrastive learning and deep clustering by imposing\ndistance-based cluster structures on unlabelled training data and also\nintroducing a discriminative contrastive loss formulation. For explicitly\nmodelling class boundaries on-the-fly, we further formulate a clustering\nconsistency condition on the two different predictions given by visual\nsimilarities and semantic decision boundaries. By advancing implicit\nrepresentation learning towards explicit understandings of visual semantics,\nSCL can amplify jointly the strengths of contrastive learning and deep\nclustering in a unified approach. Extensive experiments show that the proposed\nmodel outperforms the state-of-the-art deep clustering methods on six\nchallenging object recognition benchmarks, especially on finer-grained and\nlarger datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 20:20:48 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Huang", "Jiabo", ""], ["Gong", "Shaogang", ""]]}, {"id": "2103.02690", "submitter": "Xiaoshui Huang", "authors": "Xiaoshui Huang, Guofeng Mei, Jian Zhang, Rana Abbas", "title": "A comprehensive survey on point cloud registration", "comments": "review paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration is a transformation estimation problem between two point clouds,\nwhich has a unique and critical role in numerous computer vision applications.\nThe developments of optimization-based methods and deep learning methods have\nimproved registration robustness and efficiency. Recently, the combinations of\noptimization-based and deep learning methods have further improved performance.\nHowever, the connections between optimization-based and deep learning methods\nare still unclear. Moreover, with the recent development of 3D sensors and 3D\nreconstruction techniques, a new research direction emerges to align\ncross-source point clouds. This survey conducts a comprehensive survey,\nincluding both same-source and cross-source registration methods, and summarize\nthe connections between optimization-based and deep learning methods, to\nprovide further research insight. This survey also builds a new benchmark to\nevaluate the state-of-the-art registration algorithms in solving cross-source\nchallenges. Besides, this survey summarizes the benchmark data sets and\ndiscusses point cloud registration applications across various domains.\nFinally, this survey proposes potential research directions in this rapidly\ngrowing field.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 21:17:06 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 05:59:07 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Huang", "Xiaoshui", ""], ["Mei", "Guofeng", ""], ["Zhang", "Jian", ""], ["Abbas", "Rana", ""]]}, {"id": "2103.02696", "submitter": "Weilin Cong", "authors": "Weilin Cong, Morteza Ramezani, Mehrdad Mahdavi", "title": "On the Importance of Sampling in Learning Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have achieved impressive empirical\nadvancement across a wide variety of graph-related applications. Despite their\ngreat success, training GCNs on large graphs suffers from computational and\nmemory issues. A potential path to circumvent these obstacles is sampling-based\nmethods, where at each layer a subset of nodes is sampled. Although recent\nstudies have empirically demonstrated the effectiveness of sampling-based\nmethods, these works lack theoretical convergence guarantees under realistic\nsettings and cannot fully leverage the information of evolving parameters\nduring optimization. In this paper, we describe and analyze a general\n\\textbf{\\textit{doubly variance reduction}} schema that can accelerate any\nsampling method under the memory budget. The motivating impetus for the\nproposed schema is a careful analysis for the variance of sampling methods\nwhere it is shown that the induced variance can be decomposed into node\nembedding approximation variance (\\emph{zeroth-order variance}) during forward\npropagation and layerwise-gradient variance (\\emph{first-order variance})\nduring backward propagation. We theoretically analyze the convergence of the\nproposed schema and show that it enjoys an $\\mathcal{O}(1/T)$ convergence rate.\nWe complement our theoretical results by integrating the proposed schema in\ndifferent sampling methods and applying them to different large real-world\ngraphs. Code is public available\nat~\\url{https://github.com/CongWeilin/SGCN.git}.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 21:31:23 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Cong", "Weilin", ""], ["Ramezani", "Morteza", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "2103.02743", "submitter": "Armin Parchami", "authors": "Bruno Costa, Enrique Corona, Mostafa Parchami, Gint Puskorius, Dimitar\n  Filev", "title": "Efficient data-driven encoding of scene motion using Eccentricity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel approach of representing dynamic visual scenes\nwith static maps generated from video/image streams. Such representation allows\neasy visual assessment of motion in dynamic environments. These maps are 2D\nmatrices calculated recursively, in a pixel-wise manner, that is based on the\nrecently introduced concept of Eccentricity data analysis. Eccentricity works\nas a metric of a discrepancy between a particular pixel of an image and its\nnormality model, calculated in terms of mean and variance of past readings of\nthe same spatial region of the image. While Eccentricity maps carry temporal\ninformation about the scene, actual images do not need to be stored nor\nprocessed in batches. Rather, all the calculations are done recursively, based\non a small amount of statistical information stored in memory, thus resulting\nin a very computationally efficient (processor- and memory-wise) method. The\nlist of potential applications includes video-based activity recognition,\nintent recognition, object tracking, video description, and so on.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:11:21 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Costa", "Bruno", ""], ["Corona", "Enrique", ""], ["Parchami", "Mostafa", ""], ["Puskorius", "Gint", ""], ["Filev", "Dimitar", ""]]}, {"id": "2103.02758", "submitter": "Romero Morais", "authors": "Romero Morais, Vuong Le, Svetha Venkatesh, Truyen Tran", "title": "Learning Asynchronous and Sparse Human-Object Interaction in Videos", "comments": "Accepted for publication in CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activities can be learned from video. With effective modeling it is\npossible to discover not only the action labels but also the temporal\nstructures of the activities such as the progression of the sub-activities.\nAutomatically recognizing such structure from raw video signal is a new\ncapability that promises authentic modeling and successful recognition of\nhuman-object interactions. Toward this goal, we introduce Asynchronous-Sparse\nInteraction Graph Networks (ASSIGN), a recurrent graph network that is able to\nautomatically detect the structure of interaction events associated with\nentities in a video scene. ASSIGN pioneers learning of autonomous behavior of\nvideo entities including their dynamic structure and their interaction with the\ncoexisting neighbors. Entities' lives in our model are asynchronous to those of\nothers therefore more flexible in adaptation to complex scenarios. Their\ninteractions are sparse in time hence more faithful to the true underlying\nnature and more robust in inference and learning. ASSIGN is tested on\nhuman-object interaction recognition and shows superior performance in\nsegmenting and labeling of human sub-activities and object affordances from raw\nvideos. The native ability for discovering temporal structures of the model\nalso eliminates the dependence on external segmentation that was previously\nmandatory for this task.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:43:55 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Morais", "Romero", ""], ["Le", "Vuong", ""], ["Venkatesh", "Svetha", ""], ["Tran", "Truyen", ""]]}, {"id": "2103.02760", "submitter": "Andrew Bradley", "authors": "Ivan Fursa, Elias Fandi, Valentina Musat, Jacob Culley, Enric Gil,\n  Izzeddin Teeti, Louise Bilous, Isaac Vander Sluis, Alexander Rast and Andrew\n  Bradley", "title": "Worsening Perception: Real-time Degradation of Autonomous Vehicle\n  Perception Performance for Simulation of Adverse Weather Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous vehicles rely heavily upon their perception subsystems to see the\nenvironment in which they operate. Unfortunately, the effect of variable\nweather conditions presents a significant challenge to object detection\nalgorithms, and thus it is imperative to test the vehicle extensively in all\nconditions which it may experience. However, development of robust autonomous\nvehicle subsystems requires repeatable, controlled testing - while real weather\nis unpredictable and cannot be scheduled. Real-world testing in adverse\nconditions is an expensive and time-consuming task, often requiring access to\nspecialist facilities. Simulation is commonly relied upon as a substitute, with\nincreasingly visually realistic representations of the real-world being\ndeveloped. In the context of the complete autonomous vehicle control pipeline,\nsubsystems downstream of perception need to be tested with accurate recreations\nof the perception system output, rather than focusing on subjective visual\nrealism of the input - whether in simulation or the real world. This study\ndevelops the untapped potential of a lightweight weather augmentation method in\nan autonomous racing vehicle - focusing not on visual accuracy, but rather the\neffect upon perception subsystem performance in real time. With minimal\nadjustment, the prototype developed in this study can replicate the effects of\nwater droplets on the camera lens, and fading light conditions. This approach\nintroduces a latency of less than 8 ms using compute hardware well suited to\nbeing carried in the vehicle - rendering it ideal for real-time implementation\nthat can be run during experiments in simulation, and augmented reality testing\nin the real world.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:49:02 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 20:41:10 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 23:22:55 GMT"}, {"version": "v4", "created": "Wed, 7 Jul 2021 19:04:07 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Fursa", "Ivan", ""], ["Fandi", "Elias", ""], ["Musat", "Valentina", ""], ["Culley", "Jacob", ""], ["Gil", "Enric", ""], ["Teeti", "Izzeddin", ""], ["Bilous", "Louise", ""], ["Sluis", "Isaac Vander", ""], ["Rast", "Alexander", ""], ["Bradley", "Andrew", ""]]}, {"id": "2103.02766", "submitter": "Yujia Liu", "authors": "Yujia Liu, Stefano D'Aronco, Konrad Schindler, Jan Dirk Wegner", "title": "PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PC2WF, the first end-to-end trainable deep network architecture\nto convert a 3D point cloud into a wireframe model. The network takes as input\nan unordered set of 3D points sampled from the surface of some object, and\noutputs a wireframe of that object, i.e., a sparse set of corner points linked\nby line segments. Recovering the wireframe is a challenging task, where the\nnumbers of both vertices and edges are different for every instance, and\na-priori unknown. Our architecture gradually builds up the model: It starts by\nencoding the points into feature vectors. Based on those features, it\nidentifies a pool of candidate vertices, then prunes those candidates to a\nfinal set of corner vertices and refines their locations. Next, the corners are\nlinked with an exhaustive set of candidate edges, which is again pruned to\nobtain the final wireframe. All steps are trainable, and errors can be\nbackpropagated through the entire sequence. We validate the proposed model on a\npublicly available synthetic dataset, for which the ground truth wireframes are\naccessible, as well as on a new real-world dataset. Our model produces\nwireframe abstractions of good quality and outperforms several baselines.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 00:18:06 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liu", "Yujia", ""], ["D'Aronco", "Stefano", ""], ["Schindler", "Konrad", ""], ["Wegner", "Jan Dirk", ""]]}, {"id": "2103.02767", "submitter": "Dzung Pham", "authors": "Dzung L. Pham, Yi-Yu Chou, Blake E. Dewey, Daniel S. Reich, John A.\n  Butman, and Snehashis Roy", "title": "Contrast Adaptive Tissue Classification by Alternating Segmentation and\n  Synthesis", "comments": "10 pages. MICCAI SASHIMI Workshop 2021", "journal-ref": null, "doi": "10.1007/978-3-030-59520-3_1", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches to the segmentation of magnetic resonance images\nhave shown significant promise in automating the quantitative analysis of brain\nimages. However, a continuing challenge has been its sensitivity to the\nvariability of acquisition protocols. Attempting to segment images that have\ndifferent contrast properties from those within the training data generally\nleads to significantly reduced performance. Furthermore, heterogeneous data\nsets cannot be easily evaluated because the quantitative variation due to\nacquisition differences often dwarfs the variation due to the biological\ndifferences that one seeks to measure. In this work, we describe an approach\nusing alternating segmentation and synthesis steps that adapts the contrast\nproperties of the training data to the input image. This allows input images\nthat do not resemble the training data to be more consistently segmented. A\nnotable advantage of this approach is that only a single example of the\nacquisition protocol is required to adapt to its contrast properties. We\ndemonstrate the efficacy of our approaching using brain images from a set of\nhuman subjects scanned with two different T1-weighted volumetric protocols.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 00:25:24 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Pham", "Dzung L.", ""], ["Chou", "Yi-Yu", ""], ["Dewey", "Blake E.", ""], ["Reich", "Daniel S.", ""], ["Butman", "John A.", ""], ["Roy", "Snehashis", ""]]}, {"id": "2103.02770", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Alex Hanson, Abhinav Shrivastava, Larry Davis", "title": "SVMax: A Feature Embedding Regularizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neural network regularizer (e.g., weight decay) boosts performance by\nexplicitly penalizing the complexity of a network. In this paper, we penalize\ninferior network activations -- feature embeddings -- which in turn regularize\nthe network's weights implicitly. We propose singular value maximization\n(SVMax) to learn a more uniform feature embedding. The SVMax regularizer\nsupports both supervised and unsupervised learning. Our formulation mitigates\nmodel collapse and enables larger learning rates. We evaluate the SVMax\nregularizer using both retrieval and generative adversarial networks. We\nleverage a synthetic mixture of Gaussians dataset to evaluate SVMax in an\nunsupervised setting. For retrieval networks, SVMax achieves significant\nimprovement margins across various ranking losses. Code available at\nhttps://bit.ly/3jNkgDt\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 00:29:47 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Taha", "Ahmed", ""], ["Hanson", "Alex", ""], ["Shrivastava", "Abhinav", ""], ["Davis", "Larry", ""]]}, {"id": "2103.02772", "submitter": "Meng Ye", "authors": "Meng Ye, Mikael Kanski, Dong Yang, Qi Chang, Zhennan Yan, Qiaoying\n  Huang, Leon Axel, Dimitris Metaxas", "title": "DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on\n  Cardiac Tagging Magnetic Resonance Images", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardiac tagging magnetic resonance imaging (t-MRI) is the gold standard for\nregional myocardium deformation and cardiac strain estimation. However, this\ntechnique has not been widely used in clinical diagnosis, as a result of the\ndifficulty of motion tracking encountered with t-MRI images. In this paper, we\npropose a novel deep learning-based fully unsupervised method for in vivo\nmotion tracking on t-MRI images. We first estimate the motion field (INF)\nbetween any two consecutive t-MRI frames by a bi-directional generative\ndiffeomorphic registration neural network. Using this result, we then estimate\nthe Lagrangian motion field between the reference frame and any other frame\nthrough a differentiable composition layer. By utilizing temporal information\nto perform reasonable estimations on spatio-temporal motion fields, this novel\nmethod provides a useful solution for motion tracking and image registration in\ndynamic medical imaging. Our method has been validated on a representative\nclinical t-MRI dataset; the experimental results show that our method is\nsuperior to conventional motion tracking methods in terms of landmark tracking\naccuracy and inference efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 00:42:11 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 21:54:37 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 04:57:12 GMT"}, {"version": "v4", "created": "Sun, 18 Apr 2021 20:22:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ye", "Meng", ""], ["Kanski", "Mikael", ""], ["Yang", "Dong", ""], ["Chang", "Qi", ""], ["Yan", "Zhennan", ""], ["Huang", "Qiaoying", ""], ["Axel", "Leon", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2103.02781", "submitter": "Zhiqun Zhao", "authors": "Zhiqun Zhao, Hengyou Wang, Hao Sun and Zhihai He", "title": "Structure-Preserving Progressive Low-rank Image Completion for Defending\n  Adversarial Attacks", "comments": "10 pages, 12 figures, submitted to Journal of Visual Communication\n  and Image Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks recognize objects by analyzing local image details and\nsummarizing their information along the inference layers to derive the final\ndecision. Because of this, they are prone to adversarial attacks. Small\nsophisticated noise in the input images can accumulate along the network\ninference path and produce wrong decisions at the network output. On the other\nhand, human eyes recognize objects based on their global structure and semantic\ncues, instead of local image textures. Because of this, human eyes can still\nclearly recognize objects from images which have been heavily damaged by\nadversarial attacks. This leads to a very interesting approach for defending\ndeep neural networks against adversarial attacks. In this work, we propose to\ndevelop a structure-preserving progressive low-rank image completion (SPLIC)\nmethod to remove unneeded texture details from the input images and shift the\nbias of deep neural networks towards global object structures and semantic\ncues. We formulate the problem into a low-rank matrix completion problem with\nprogressively smoothed rank functions to avoid local minimums during the\noptimization process. Our experimental results demonstrate that the proposed\nmethod is able to successfully remove the insignificant local image details\nwhile preserving important global object structures. On black-box, gray-box,\nand white-box attacks, our method outperforms existing defense methods (by up\nto 12.6%) and significantly improves the adversarial robustness of the network.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 01:24:15 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhao", "Zhiqun", ""], ["Wang", "Hengyou", ""], ["Sun", "Hao", ""], ["He", "Zhihai", ""]]}, {"id": "2103.02782", "submitter": "Jianwei Song", "authors": "Jianwei Song, Ruoyu Yang", "title": "Feature Boosting, Suppression, and Diversification for Fine-Grained\n  Visual Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning feature representation from discriminative local regions plays a key\nrole in fine-grained visual classification. Employing attention mechanisms to\nextract part features has become a trend. However, there are two major\nlimitations in these methods: First, they often focus on the most salient part\nwhile neglecting other inconspicuous but distinguishable parts. Second, they\ntreat different part features in isolation while neglecting their\nrelationships. To handle these limitations, we propose to locate multiple\ndifferent distinguishable parts and explore their relationships in an explicit\nway. In this pursuit, we introduce two lightweight modules that can be easily\nplugged into existing convolutional neural networks. On one hand, we introduce\na feature boosting and suppression module that boosts the most salient part of\nfeature maps to obtain a part-specific representation and suppresses it to\nforce the following network to mine other potential parts. On the other hand,\nwe introduce a feature diversification module that learns semantically\ncomplementary information from the correlated part-specific representations.\nOur method does not need bounding boxes/part annotations and can be trained\nend-to-end. Extensive experimental results show that our method achieves\nstate-of-the-art performances on several benchmark fine-grained datasets.\nSource code is available at https://github.com/chaomaer/FBSD.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 01:49:53 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 14:58:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Song", "Jianwei", ""], ["Yang", "Ruoyu", ""]]}, {"id": "2103.02788", "submitter": "Jianwei Song", "authors": "Jianwei Song, Ruoyu Yang", "title": "Learning Granularity-Aware Convolutional Neural Network for Fine-Grained\n  Visual Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Locating discriminative parts plays a key role in fine-grained visual\nclassification due to the high similarities between different objects. Recent\nworks based on convolutional neural networks utilize the feature maps taken\nfrom the last convolutional layer to mine discriminative regions. However, the\nlast convolutional layer tends to focus on the whole object due to the large\nreceptive field, which leads to a reduced ability to spot the differences. To\naddress this issue, we propose a novel Granularity-Aware Convolutional Neural\nNetwork (GA-CNN) that progressively explores discriminative features.\nSpecifically, GA-CNN utilizes the differences of the receptive fields at\ndifferent layers to learn multi-granularity features, and it exploits larger\ngranularity information based on the smaller granularity information found at\nthe previous stages. To further boost the performance, we introduce an\nobject-attentive module that can effectively localize the object given a raw\nimage. GA-CNN does not need bounding boxes/part annotations and can be trained\nend-to-end. Extensive experimental results show that our approach achieves\nstate-of-the-art performances on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 02:18:07 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Song", "Jianwei", ""], ["Yang", "Ruoyu", ""]]}, {"id": "2103.02805", "submitter": "Baojin Huang", "authors": "Baojin Huang, Zhongyuan Wang, Guangcheng Wang, Kui Jiang, Kangli Zeng,\n  Zhen Han, Xin Tian, Yuhong Yang", "title": "When Face Recognition Meets Occlusion: A New Benchmark", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing face recognition datasets usually lack occlusion samples, which\nhinders the development of face recognition. Especially during the COVID-19\ncoronavirus epidemic, wearing a mask has become an effective means of\npreventing the virus spread. Traditional CNN-based face recognition models\ntrained on existing datasets are almost ineffective for heavy occlusion. To\nthis end, we pioneer a simulated occlusion face recognition dataset. In\nparticular, we first collect a variety of glasses and masks as occlusion, and\nrandomly combine the occlusion attributes (occlusion objects, textures,and\ncolors) to achieve a large number of more realistic occlusion types. We then\ncover them in the proper position of the face image with the normal occlusion\nhabit. Furthermore, we reasonably combine original normal face images and\noccluded face images to form our final dataset, termed as Webface-OCC. It\ncovers 804,704 face images of 10,575 subjects, with diverse occlusion types to\nensure its diversity and stability. Extensive experiments on public datasets\nshow that the ArcFace retrained by our dataset significantly outperforms the\nstate-of-the-arts. Webface-OCC is available at\nhttps://github.com/Baojin-Huang/Webface-OCC.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 03:07:42 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Huang", "Baojin", ""], ["Wang", "Zhongyuan", ""], ["Wang", "Guangcheng", ""], ["Jiang", "Kui", ""], ["Zeng", "Kangli", ""], ["Han", "Zhen", ""], ["Tian", "Xin", ""], ["Yang", "Yuhong", ""]]}, {"id": "2103.02808", "submitter": "Xi Li", "authors": "Hui Wang, Jian Tian, Songyuan Li, Hanbin Zhao, Qi Tian, Fei Wu, and Xi\n  Li", "title": "Unsupervised Domain Adaptation for Image Classification via\n  Structure-Conditioned Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) typically carries out knowledge transfer\nfrom a label-rich source domain to an unlabeled target domain by adversarial\nlearning. In principle, existing UDA approaches mainly focus on the global\ndistribution alignment between domains while ignoring the intrinsic local\ndistribution properties. Motivated by this observation, we propose an\nend-to-end structure-conditioned adversarial learning scheme (SCAL) that is\nable to preserve the intra-class compactness during domain distribution\nalignment. By using local structures as structure-aware conditions, the\nproposed scheme is implemented in a structure-conditioned adversarial learning\npipeline. The above learning procedure is iteratively performed by alternating\nbetween local structures establishment and structure-conditioned adversarial\nlearning. Experimental results demonstrate the effectiveness of the proposed\nscheme in UDA scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 03:12:54 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wang", "Hui", ""], ["Tian", "Jian", ""], ["Li", "Songyuan", ""], ["Zhao", "Hanbin", ""], ["Tian", "Qi", ""], ["Wu", "Fei", ""], ["Li", "Xi", ""]]}, {"id": "2103.02813", "submitter": "Yuxia Sheng", "authors": "Shiyao Guo, Yuxia Sheng, Shenpeng Li, Li Chai, Jingxin Zhang", "title": "PET Image Reconstruction with Multiple Kernels and Multiple Kernel Space\n  Regularizers", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelized maximum-likelihood (ML) expectation maximization (EM) methods have\nrecently gained prominence in PET image reconstruction, outperforming many\nprevious state-of-the-art methods. But they are not immune to the problems of\nnon-kernelized MLEM methods in potentially large reconstruction error and high\nsensitivity to iteration number. This paper demonstrates these problems by\ntheoretical reasoning and experiment results, and provides a novel solution to\nsolve these problems. The solution is a regularized kernelized MLEM with\nmultiple kernel matrices and multiple kernel space regularizers that can be\ntailored for different applications. To reduce the reconstruction error and the\nsensitivity to iteration number, we present a general class of multi-kernel\nmatrices and two regularizers consisting of kernel image dictionary and kernel\nimage Laplacian quatradic, and use them to derive the single-kernel regularized\nEM and multi-kernel regularized EM algorithms for PET image reconstruction.\nThese new algorithms are derived using the technical tools of multi-kernel\ncombination in machine learning, image dictionary learning in sparse coding,\nand graph Laplcian quadratic in graph signal processing. Extensive tests and\ncomparisons on the simulated and in vivo data are presented to validate and\nevaluate the new algorithms, and demonstrate their superior performance and\nadvantages over the kernelized MLEM and other conventional methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 03:28:17 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Guo", "Shiyao", ""], ["Sheng", "Yuxia", ""], ["Li", "Shenpeng", ""], ["Chai", "Li", ""], ["Zhang", "Jingxin", ""]]}, {"id": "2103.02835", "submitter": "Sifan Song", "authors": "Sifan Song, Daiyun Huang, Yalun Hu, Chunxiao Yang, Jia Meng, Fei Ma,\n  Jiaming Zhang, Jionglong Su", "title": "A Novel Application of Image-to-Image Translation: Chromosome\n  Straightening Framework by Learning from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, chromosome straightening plays a significant role in the\npathological study of chromosomes and in the development of cytogenetic maps.\nWhereas different approaches exist for the straightening task, they are mostly\ngeometric algorithms whose outputs are characterized by jagged edges or\nfragments with discontinued banding patterns. To address the flaws in the\ngeometric algorithms, we propose a novel framework based on image-to-image\ntranslation to learn a pertinent mapping dependence for synthesizing\nstraightened chromosomes with uninterrupted banding patterns and preserved\ndetails. In addition, to avoid the pitfall of deficient input chromosomes, we\nconstruct an augmented dataset using only one single curved chromosome image\nfor training models. Based on this framework, we apply two popular\nimage-to-image translation architectures, U-shape networks and conditional\ngenerative adversarial networks, to assess its efficacy. Experiments on a\ndataset comprising of 642 real-world chromosomes demonstrate the superiority of\nour framework as compared to the geometric method in straightening performance\nby rendering realistic and continued chromosome details. Furthermore, our\nstraightened results improve the chromosome classification, achieving\n0.98%-1.39% in mean accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 05:05:41 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Song", "Sifan", ""], ["Huang", "Daiyun", ""], ["Hu", "Yalun", ""], ["Yang", "Chunxiao", ""], ["Meng", "Jia", ""], ["Ma", "Fei", ""], ["Zhang", "Jiaming", ""], ["Su", "Jionglong", ""]]}, {"id": "2103.02844", "submitter": "Kibrom Berihu Girum", "authors": "Kibrom Berihu Girum, Gilles Cr\\'ehange, Alain Lalande", "title": "Learning With Context Feedback Loop for Robust Medical Image\n  Segmentation", "comments": "13 pages, accepted for publication in IEEE Transactions on Medical\n  Imaging (TMI); Applied to Heart (Cardiac cine-MRI and Echocardiography),\n  Prostate, and Inner ear image segmentation", "journal-ref": null, "doi": "10.1109/TMI.2021.3060497", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has successfully been leveraged for medical image segmentation.\nIt employs convolutional neural networks (CNN) to learn distinctive image\nfeatures from a defined pixel-wise objective function. However, this approach\ncan lead to less output pixel interdependence producing incomplete and\nunrealistic segmentation results. In this paper, we present a fully automatic\ndeep learning method for robust medical image segmentation by formulating the\nsegmentation problem as a recurrent framework using two systems. The first one\nis a forward system of an encoder-decoder CNN that predicts the segmentation\nresult from the input image. The predicted probabilistic output of the forward\nsystem is then encoded by a fully convolutional network (FCN)-based context\nfeedback system. The encoded feature space of the FCN is then integrated back\ninto the forward system's feed-forward learning process. Using the FCN-based\ncontext feedback loop allows the forward system to learn and extract more\nhigh-level image features and fix previous mistakes, thereby improving\nprediction accuracy over time. Experimental results, performed on four\ndifferent clinical datasets, demonstrate our method's potential application for\nsingle and multi-structure medical image segmentation by outperforming the\nstate of the art methods. With the feedback loop, deep learning methods can now\nproduce results that are both anatomically plausible and robust to low contrast\nimages. Therefore, formulating image segmentation as a recurrent framework of\ntwo interconnected networks via context feedback loop can be a potential method\nfor robust and efficient medical image analysis.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 05:44:59 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Girum", "Kibrom Berihu", ""], ["Cr\u00e9hange", "Gilles", ""], ["Lalande", "Alain", ""]]}, {"id": "2103.02845", "submitter": "Xingyu Chen", "authors": "Xingyu Chen, Yufeng Liu, Chongyang Ma, Jianlong Chang, Huayan Wang,\n  Tian Chen, Xiaoyan Guo, Pengfei Wan, Wen Zheng", "title": "Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive\n  2D-1D Registration", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent years have witnessed significant progress in 3D hand mesh recovery.\nNevertheless, because of the intrinsic 2D-to-3D ambiguity, recovering\ncamera-space 3D information from a single RGB image remains challenging. To\ntackle this problem, we divide camera-space mesh recovery into two sub-tasks,\ni.e., root-relative mesh recovery and root recovery. First, joint landmarks and\nsilhouette are extracted from a single input image to provide 2D cues for the\n3D tasks. In the root-relative mesh recovery task, we exploit semantic\nrelations among joints to generate a 3D mesh from the extracted 2D cues. Such\ngenerated 3D mesh coordinates are expressed relative to a root position, i.e.,\nwrist of the hand. In the root recovery task, the root position is registered\nto the camera space by aligning the generated 3D mesh back to 2D cues, thereby\ncompleting cameraspace 3D mesh recovery. Our pipeline is novel in that (1) it\nexplicitly makes use of known semantic relations among joints and (2) it\nexploits 1D projections of the silhouette and mesh to achieve robust\nregistration. Extensive experiments on popular datasets such as FreiHAND, RHD,\nand Human3.6M demonstrate that our approach achieves stateof-the-art\nperformance on both root-relative mesh recovery and root recovery. Our code is\npublicly available at https://github.com/SeanChenxy/HandMesh.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 05:46:04 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 08:22:07 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chen", "Xingyu", ""], ["Liu", "Yufeng", ""], ["Ma", "Chongyang", ""], ["Chang", "Jianlong", ""], ["Wang", "Huayan", ""], ["Chen", "Tian", ""], ["Guo", "Xiaoyan", ""], ["Wan", "Pengfei", ""], ["Zheng", "Wen", ""]]}, {"id": "2103.02850", "submitter": "Qi Yang", "authors": "Qi Yang, Siheng Chen, Yiling Xu, Jun Sun, M. Salman Asif, Zhan Ma", "title": "Point Cloud Distortion Quantification based on Potential Energy for\n  Human and Machine Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distortion quantification of point clouds plays a stealth, yet vital role in\na wide range of human and machine perception tasks. For human perception tasks,\na distortion quantification can substitute subjective experiments to guide 3D\nvisualization; while for machine perception tasks, a distortion quantification\ncan work as a loss function to guide the training of deep neural networks for\nunsupervised learning tasks. To handle a variety of demands in many\napplications, a distortion quantification needs to be distortion discriminable,\ndifferentiable, and have a low computational complexity. Currently, however,\nthere is a lack of a general distortion quantification that can satisfy all\nthree conditions. To fill this gap, this work proposes multiscale potential\nenergy discrepancy (MPED), a distortion quantification to measure point cloud\ngeometry and color difference. By evaluating at various neighborhood sizes, the\nproposed MPED achieves global-local tradeoffs, capturing distortion in a\nmultiscale fashion. Extensive experimental studies validate MPED's superiority\nfor both human and machine perception tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 06:24:04 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 08:26:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Yang", "Qi", ""], ["Chen", "Siheng", ""], ["Xu", "Yiling", ""], ["Sun", "Jun", ""], ["Asif", "M. Salman", ""], ["Ma", "Zhan", ""]]}, {"id": "2103.02852", "submitter": "Guanghan Ning", "authors": "Guanghan Ning, Guang Chen, Chaowei Tan, Si Luo, Liefeng Bo, Heng Huang", "title": "Data Augmentation for Object Detection via Differentiable Neural\n  Rendering", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to train a robust object detector under the supervised\nlearning setting when the annotated data are scarce. Thus, previous approaches\ntackling this problem are in two categories: semi-supervised learning models\nthat interpolate labeled data from unlabeled data, and self-supervised learning\napproaches that exploit signals within unlabeled data via pretext tasks. To\nseamlessly integrate and enhance existing supervised object detection methods,\nin this work, we focus on addressing the data scarcity problem from a\nfundamental viewpoint without changing the supervised learning paradigm. We\npropose a new offline data augmentation method for object detection, which\nsemantically interpolates the training data with novel views. Specifically, our\nnew system generates controllable views of training images based on\ndifferentiable neural rendering, together with corresponding bounding box\nannotations which involve no human intervention. Firstly, we extract and\nproject pixel-aligned image features into point clouds while estimating depth\nmaps. We then re-project them with a target camera pose and render a novel-view\n2d image. Objects in the form of keypoints are marked in point clouds to\nrecover annotations in new views. Our new method is fully compatible with\nonline data augmentation methods, such as affine transform, image mixup, etc.\nExtensive experiments show that our method, as a cost-free tool to enrich\nimages and labels, can significantly boost the performance of object detection\nsystems with scarce training data. Code is available at\n\\url{https://github.com/Guanghan/DANR}.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 06:31:06 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 04:58:40 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ning", "Guanghan", ""], ["Chen", "Guang", ""], ["Tan", "Chaowei", ""], ["Luo", "Si", ""], ["Bo", "Liefeng", ""], ["Huang", "Heng", ""]]}, {"id": "2103.02854", "submitter": "Dexter Neo", "authors": "Vassilios Vonikakis, Dexter Neo, Stefan Winkler", "title": "Morphset:Augmenting categorical emotion datasets with dimensional affect\n  labels using face morphing", "comments": "in Proc IEEE International Conference on Image Processing (ICIP),\n  Anchorage, Sep.2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Emotion recognition and understanding is a vital component in human-machine\ninteraction. Dimensional models of affect such as those using valence and\narousal have advantages over traditional categorical ones due to the complexity\nof emotional states in humans. However, dimensional emotion annotations are\ndifficult and expensive to collect, therefore they are not as prevalent in the\naffective computing community. To address these issues, we propose a method to\ngenerate synthetic images from existing categorical emotion datasets using face\nmorphing as well as dimensional labels in the circumplex space with full\ncontrol over the resulting sample distribution, while achieving augmentation\nfactors of at least 20x or more.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 06:33:06 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 03:36:06 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Vonikakis", "Vassilios", ""], ["Neo", "Dexter", ""], ["Winkler", "Stefan", ""]]}, {"id": "2103.02861", "submitter": "Avinash Paliwal", "authors": "Avinash Paliwal, Libing Zeng and Nima Khademi Kalantari", "title": "Multi-Stage Raw Video Denoising with Adversarial Loss and Gradient Mask", "comments": "Accepted to ICCP 2021. Project page containing code and video at\n  https://people.engr.tamu.edu/nimak/Papers/ICCP2021_denoising", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a learning-based approach for denoising raw videos\ncaptured under low lighting conditions. We propose to do this by first\nexplicitly aligning the neighboring frames to the current frame using a\nconvolutional neural network (CNN). We then fuse the registered frames using\nanother CNN to obtain the final denoised frame. To avoid directly aligning the\ntemporally distant frames, we perform the two processes of alignment and fusion\nin multiple stages. Specifically, at each stage, we perform the denoising\nprocess on three consecutive input frames to generate the intermediate denoised\nframes which are then passed as the input to the next stage. By performing the\nprocess in multiple stages, we can effectively utilize the information of\nneighboring frames without directly aligning the temporally distant frames. We\ntrain our multi-stage system using an adversarial loss with a conditional\ndiscriminator. Specifically, we condition the discriminator on a soft gradient\nmask to prevent introducing high-frequency artifacts in smooth regions. We show\nthat our system is able to produce temporally coherent videos with realistic\ndetails. Furthermore, we demonstrate through extensive experiments that our\napproach outperforms state-of-the-art image and video denoising methods both\nnumerically and visually.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 06:57:48 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 17:38:40 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 04:23:34 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Paliwal", "Avinash", ""], ["Zeng", "Libing", ""], ["Kalantari", "Nima Khademi", ""]]}, {"id": "2103.02870", "submitter": "Chun Yong Chong", "authors": "Hyejin Park, Taaha Waseem, Wen Qi Teo, Ying Hwei Low, Mei Kuan Lim and\n  Chun Yong Chong", "title": "Robustness Evaluation of Stacked Generative Adversarial Networks using\n  Metamorphic Testing", "comments": "8 pages, accepted at the 6th International Workshop on Metamorphic\n  Testing (MET'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Synthesising photo-realistic images from natural language is one of the\nchallenging problems in computer vision. Over the past decade, a number of\napproaches have been proposed, of which the improved Stacked Generative\nAdversarial Network (StackGAN-v2) has proven capable of generating high\nresolution images that reflect the details specified in the input text\ndescriptions. In this paper, we aim to assess the robustness and\nfault-tolerance capability of the StackGAN-v2 model by introducing variations\nin the training data. However, due to the working principle of Generative\nAdversarial Network (GAN), it is difficult to predict the output of the model\nwhen the training data are modified. Hence, in this work, we adopt Metamorphic\nTesting technique to evaluate the robustness of the model with a variety of\nunexpected training dataset. As such, we first implement StackGAN-v2 algorithm\nand test the pre-trained model provided by the original authors to establish a\nground truth for our experiments. We then identify a metamorphic relation, from\nwhich test cases are generated. Further, metamorphic relations were derived\nsuccessively based on the observations of prior test results. Finally, we\nsynthesise the results from our experiment of all the metamorphic relations and\nfound that StackGAN-v2 algorithm is susceptible to input images with obtrusive\nobjects, even if it overlaps with the main object minimally, which was not\nreported by the authors and users of StackGAN-v2 model. The proposed\nmetamorphic relations can be applied to other text-to-image synthesis models to\nnot only verify the robustness but also to help researchers understand and\ninterpret the results made by the machine learning models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 07:29:17 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Park", "Hyejin", ""], ["Waseem", "Taaha", ""], ["Teo", "Wen Qi", ""], ["Low", "Ying Hwei", ""], ["Lim", "Mei Kuan", ""], ["Chong", "Chun Yong", ""]]}, {"id": "2103.02884", "submitter": "Changyue Ma", "authors": "Changyue Ma, Zhao Wang, Ruling Liao, Yan Ye", "title": "A Cross Channel Context Model for Latents in Deep Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a cross channel context model for latents in deep image\ncompression. Generally, deep image compression is based on an autoencoder\nframework, which transforms the original image to latents at the encoder and\nrecovers the reconstructed image from the quantized latents at the decoder. The\ntransform is usually combined with an entropy model, which estimates the\nprobability distribution of the quantized latents for arithmetic coding.\nCurrently, joint autoregressive and hierarchical prior entropy models are\nwidely adopted to capture both the global contexts from the hyper latents and\nthe local contexts from the quantized latent elements. For the local contexts,\nthe widely adopted 2D mask convolution can only capture the spatial context.\nHowever, we observe that there are strong correlations between different\nchannels in the latents. To utilize the cross channel correlations, we propose\nto divide the latents into several groups according to channel index and code\nthe groups one by one, where previously coded groups are utilized to provide\ncross channel context for the current group. The proposed cross channel context\nmodel is combined with the joint autoregressive and hierarchical prior entropy\nmodel. Experimental results show that, using PSNR as the distortion metric, the\ncombined model achieves BD-rate reductions of 6.30% and 6.31% over the baseline\nentropy model, and 2.50% and 2.20% over the latest video coding standard\nVersatile Video Coding (VVC) for the Kodak and CVPR CLIC2020 professional\ndataset, respectively. In addition, when optimized for the MS-SSIM metric, our\napproach generates visually more pleasant reconstructed images.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:13:04 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ma", "Changyue", ""], ["Wang", "Zhao", ""], ["Liao", "Ruling", ""], ["Ye", "Yan", ""]]}, {"id": "2103.02886", "submitter": "Lili Chen", "authors": "Lili Chen, Kimin Lee, Aravind Srinivas, Pieter Abbeel", "title": "Improving Computational Efficiency in Visual Reinforcement Learning via\n  Stored Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in off-policy deep reinforcement learning (RL) have led to\nimpressive success in complex tasks from visual observations. Experience replay\nimproves sample-efficiency by reusing experiences from the past, and\nconvolutional neural networks (CNNs) process high-dimensional inputs\neffectively. However, such techniques demand high memory and computational\nbandwidth. In this paper, we present Stored Embeddings for Efficient\nReinforcement Learning (SEER), a simple modification of existing off-policy RL\nmethods, to address these computational and memory requirements. To reduce the\ncomputational overhead of gradient updates in CNNs, we freeze the lower layers\nof CNN encoders early in training due to early convergence of their parameters.\nAdditionally, we reduce memory requirements by storing the low-dimensional\nlatent vectors for experience replay instead of high-dimensional images,\nenabling an adaptive increase in the replay buffer capacity, a useful technique\nin constrained-memory settings. In our experiments, we show that SEER does not\ndegrade the performance of RL agents while significantly saving computation and\nmemory across a diverse set of DeepMind Control environments and Atari games.\nFinally, we show that SEER is useful for computation-efficient transfer\nlearning in RL because lower layers of CNNs extract generalizable features,\nwhich can be used for different tasks and domains.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:14:10 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Chen", "Lili", ""], ["Lee", "Kimin", ""], ["Srinivas", "Aravind", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2103.02904", "submitter": "Qigong Sun", "authors": "Qigong Sun, Licheng Jiao, Yan Ren, Xiufang Li, Fanhua Shang, Fang Liu", "title": "Effective and Fast: A Novel Sequential Single Path Search for\n  Mixed-Precision Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since model quantization helps to reduce the model size and computation\nlatency, it has been successfully applied in many applications of mobile\nphones, embedded devices and smart chips. The mixed-precision quantization\nmodel can match different quantization bit-precisions according to the\nsensitivity of different layers to achieve great performance. However, it is a\ndifficult problem to quickly determine the quantization bit-precision of each\nlayer in deep neural networks according to some constraints (e.g., hardware\nresources, energy consumption, model size and computation latency). To address\nthis issue, we propose a novel sequential single path search (SSPS) method for\nmixed-precision quantization,in which the given constraints are introduced into\nits loss function to guide searching process. A single path search cell is used\nto combine a fully differentiable supernet, which can be optimized by\ngradient-based algorithms. Moreover, we sequentially determine the candidate\nprecisions according to the selection certainties to exponentially reduce the\nsearch space and speed up the convergence of searching process. Experiments\nshow that our method can efficiently search the mixed-precision models for\ndifferent architectures (e.g., ResNet-20, 18, 34, 50 and MobileNet-V2) and\ndatasets (e.g., CIFAR-10, ImageNet and COCO) under given constraints, and our\nexperimental results verify that SSPS significantly outperforms their uniform\ncounterparts.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 09:15:08 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Sun", "Qigong", ""], ["Jiao", "Licheng", ""], ["Ren", "Yan", ""], ["Li", "Xiufang", ""], ["Shang", "Fanhua", ""], ["Liu", "Fang", ""]]}, {"id": "2103.02907", "submitter": "Qibin Hou", "authors": "Qibin Hou, Daquan Zhou, Jiashi Feng", "title": "Coordinate Attention for Efficient Mobile Network Design", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies on mobile network design have demonstrated the remarkable\neffectiveness of channel attention (e.g., the Squeeze-and-Excitation attention)\nfor lifting model performance, but they generally neglect the positional\ninformation, which is important for generating spatially selective attention\nmaps. In this paper, we propose a novel attention mechanism for mobile networks\nby embedding positional information into channel attention, which we call\n\"coordinate attention\". Unlike channel attention that transforms a feature\ntensor to a single feature vector via 2D global pooling, the coordinate\nattention factorizes channel attention into two 1D feature encoding processes\nthat aggregate features along the two spatial directions, respectively. In this\nway, long-range dependencies can be captured along one spatial direction and\nmeanwhile precise positional information can be preserved along the other\nspatial direction. The resulting feature maps are then encoded separately into\na pair of direction-aware and position-sensitive attention maps that can be\ncomplementarily applied to the input feature map to augment the representations\nof the objects of interest. Our coordinate attention is simple and can be\nflexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt,\nand EfficientNet with nearly no computational overhead. Extensive experiments\ndemonstrate that our coordinate attention is not only beneficial to ImageNet\nclassification but more interestingly, behaves better in down-stream tasks,\nsuch as object detection and semantic segmentation. Code is available at\nhttps://github.com/Andrew-Qibin/CoordAttention.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 09:18:02 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Hou", "Qibin", ""], ["Zhou", "Daquan", ""], ["Feng", "Jiashi", ""]]}, {"id": "2103.02911", "submitter": "Yicheng Wu", "authors": "Yicheng Wu, Minfeng Xu, Zongyuan Ge, Jianfei Cai and Lei Zhang", "title": "Semi-supervised Left Atrium Segmentation with Mutual Consistency\n  Training", "comments": "Accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Semi-supervised learning has attracted great attention in the field of\nmachine learning, especially for medical image segmentation tasks, since it\nalleviates the heavy burden of collecting abundant densely annotated data for\ntraining. However, most of existing methods underestimate the importance of\nchallenging regions (e.g. small branches or blurred edges) during training. We\nbelieve that these unlabeled regions may contain more crucial information to\nminimize the uncertainty prediction for the model and should be emphasized in\nthe training process. Therefore, in this paper, we propose a novel Mutual\nConsistency Network (MC-Net) for semi-supervised left atrium segmentation from\n3D MR images. Particularly, our MC-Net consists of one encoder and two slightly\ndifferent decoders, and the prediction discrepancies of two decoders are\ntransformed as an unsupervised loss by our designed cycled pseudo label scheme\nto encourage mutual consistency. Such mutual consistency encourages the two\ndecoders to have consistent and low-entropy predictions and enables the model\nto gradually capture generalized features from these unlabeled challenging\nregions. We evaluate our MC-Net on the public Left Atrium (LA) database and it\nobtains impressive performance gains by exploiting the unlabeled data\neffectively. Our MC-Net outperforms six recent semi-supervised methods for left\natrium segmentation, and sets the new state-of-the-art performance on the LA\ndatabase.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 09:34:32 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 07:15:17 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wu", "Yicheng", ""], ["Xu", "Minfeng", ""], ["Ge", "Zongyuan", ""], ["Cai", "Jianfei", ""], ["Zhang", "Lei", ""]]}, {"id": "2103.02927", "submitter": "Xiaodan Li", "authors": "Xiaodan Li, Jinfeng Li, Yuefeng Chen, Shaokai Ye, Yuan He, Shuhui\n  Wang, Hang Su, Hui Xue", "title": "QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the query-based attack against image retrieval to evaluate its\nrobustness against adversarial examples under the black-box setting, where the\nadversary only has query access to the top-k ranked unlabeled images from the\ndatabase. Compared with query attacks in image classification, which produce\nadversaries according to the returned labels or confidence score, the challenge\nbecomes even more prominent due to the difficulty in quantifying the attack\neffectiveness on the partial retrieved list. In this paper, we make the first\nattempt in Query-based Attack against Image Retrieval (QAIR), to completely\nsubvert the top-k retrieval results. Specifically, a new relevance-based loss\nis designed to quantify the attack effects by measuring the set similarity on\nthe top-k retrieval results before and after attacks and guide the gradient\noptimization. To further boost the attack efficiency, a recursive model\nstealing method is proposed to acquire transferable priors on the target model\nand generate the prior-guided gradients. Comprehensive experiments show that\nthe proposed attack achieves a high attack success rate with few queries\nagainst the image retrieval systems under the black-box setting. The attack\nevaluations on the real-world visual search engine show that it successfully\ndeceives a commercial system such as Bing Visual Search with 98% attack success\nrate by only 33 queries on average.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:18:43 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:13:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Li", "Xiaodan", ""], ["Li", "Jinfeng", ""], ["Chen", "Yuefeng", ""], ["Ye", "Shaokai", ""], ["He", "Yuan", ""], ["Wang", "Shuhui", ""], ["Su", "Hang", ""], ["Xue", "Hui", ""]]}, {"id": "2103.02937", "submitter": "Silvio Barra Dr", "authors": "Silvio Barra, Carmen Bisogni, Maria De Marsico, Stefano Ricciardi", "title": "Visual Question Answering: which investigated applications?", "comments": null, "journal-ref": "Pattern Recognition Letters 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is an extremely stimulating and challenging\nresearch area where Computer Vision (CV) and Natural Language Processig (NLP)\nhave recently met. In image captioning and video summarization, the semantic\ninformation is completely contained in still images or video dynamics, and it\nhas only to be mined and expressed in a human-consistent way. Differently from\nthis, in VQA semantic information in the same media must be compared with the\nsemantics implied by a question expressed in natural language, doubling the\nartificial intelligence-related effort. Some recent surveys about VQA\napproaches have focused on methods underlying either the image-related\nprocessing or the verbal-related one, or on the way to consistently fuse the\nconveyed information. Possible applications are only suggested, and, in fact,\nmost cited works rely on general-purpose datasets that are used to assess the\nbuilding blocks of a VQA system. This paper rather considers the proposals that\nfocus on real-world applications, possibly using as benchmarks suitable data\nbound to the application domain. The paper also reports about some recent\nchallenges in VQA research.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:38:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Barra", "Silvio", ""], ["Bisogni", "Carmen", ""], ["De Marsico", "Maria", ""], ["Ricciardi", "Stefano", ""]]}, {"id": "2103.02940", "submitter": "Dmitry V. Dylov", "authors": "Aleksandr Belov and Joel Stadelmann and Sergey Kastryulin and Dmitry\n  V. Dylov", "title": "Towards Ultrafast MRI via Extreme k-Space Undersampling and\n  Superresolution", "comments": "Main text: 10 pages and 8 figures. 18 pages and 14 figures total\n  (Supplementary material included)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We went below the MRI acceleration factors (a.k.a., k-space undersampling)\nreported by all published papers that reference the original fastMRI challenge,\nand then considered powerful deep learning based image enhancement methods to\ncompensate for the underresolved images. We thoroughly study the influence of\nthe sampling patterns, the undersampling and the downscaling factors, as well\nas the recovery models on the final image quality for both the brain and the\nknee fastMRI benchmarks. The quality of the reconstructed images surpasses that\nof the other methods, yielding an MSE of 0.00114, a PSNR of 29.6 dB, and an\nSSIM of 0.956 at x16 acceleration factor. More extreme undersampling factors of\nx32 and x64 are also investigated, holding promise for certain clinical\napplications such as computer-assisted surgery or radiation planning. We survey\n5 expert radiologists to assess 100 pairs of images and show that the recovered\nundersampled images statistically preserve their diagnostic value.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:45:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Belov", "Aleksandr", ""], ["Stadelmann", "Joel", ""], ["Kastryulin", "Sergey", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2103.02961", "submitter": "Juan E Arco", "authors": "Juan E. Arco, Andr\\'es Ortiz, Javier Ram\\'irez, Francisco J.\n  Mart\\'inez-Murcia, Yu-Dong Zhang, Jordi Broncano, M. \\'Alvaro Berb\\'is,\n  Javier Royuela-del-Val, Antonio Luna, Juan M. G\\'orriz", "title": "Probabilistic combination of eigenlungs-based classifiers for COVID-19\n  diagnosis in chest CT images", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The outbreak of the COVID-19 (Coronavirus disease 2019) pandemic has changed\nthe world. According to the World Health Organization (WHO), there have been\nmore than 100 million confirmed cases of COVID-19, including more than 2.4\nmillion deaths. It is extremely important the early detection of the disease,\nand the use of medical imaging such as chest X-ray (CXR) and chest Computed\nTomography (CCT) have proved to be an excellent solution. However, this process\nrequires clinicians to do it within a manual and time-consuming task, which is\nnot ideal when trying to speed up the diagnosis. In this work, we propose an\nensemble classifier based on probabilistic Support Vector Machine (SVM) in\norder to identify pneumonia patterns while providing information about the\nreliability of the classification. Specifically, each CCT scan is divided into\ncubic patches and features contained in each one of them are extracted by\napplying kernel PCA. The use of base classifiers within an ensemble allows our\nsystem to identify the pneumonia patterns regardless of their size or location.\nDecisions of each individual patch are then combined into a global one\naccording to the reliability of each individual classification: the lower the\nuncertainty, the higher the contribution. Performance is evaluated in a real\nscenario, yielding an accuracy of 97.86%. The large performance obtained and\nthe simplicity of the system (use of deep learning in CCT images would result\nin a huge computational cost) evidence the applicability of our proposal in a\nreal-world environment.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:30:38 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Arco", "Juan E.", ""], ["Ortiz", "Andr\u00e9s", ""], ["Ram\u00edrez", "Javier", ""], ["Mart\u00ednez-Murcia", "Francisco J.", ""], ["Zhang", "Yu-Dong", ""], ["Broncano", "Jordi", ""], ["Berb\u00eds", "M. \u00c1lvaro", ""], ["Royuela-del-Val", "Javier", ""], ["Luna", "Antonio", ""], ["G\u00f3rriz", "Juan M.", ""]]}, {"id": "2103.02969", "submitter": "Dinis Rodrigues", "authors": "Dinis L. Rodrigues, Miguel Nobre Menezes, Fausto J. Pinto, Arlindo L.\n  Oliveira", "title": "Automated Detection of Coronary Artery Stenosis in X-ray Angiography\n  using Deep Neural Networks", "comments": "10 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary artery disease leading up to stenosis, the partial or total blocking\nof coronary arteries, is a severe condition that affects millions of patients\neach year. Automated identification and classification of stenosis severity\nfrom minimally invasive procedures would be of great clinical value, but\nexisting methods do not match the accuracy of experienced cardiologists, due to\nthe complexity of the task. Although a number of computational approaches for\nquantitative assessment of stenosis have been proposed to date, the performance\nof these methods is still far from the required levels for clinical\napplications. In this paper, we propose a two-step deep-learning framework to\npartially automate the detection of stenosis from X-ray coronary angiography\nimages. In the two steps, we used two distinct convolutional neural network\narchitectures, one to automatically identify and classify the angle of view,\nand another to determine the bounding boxes of the regions of interest in\nframes where stenosis is visible. Transfer learning and data augmentation\ntechniques were used to boost the performance of the system in both tasks. We\nachieved a 0.97 accuracy on the task of classifying the Left/Right Coronary\nArtery (LCA/RCA) angle view and 0.68/0.73 recall on the determination of the\nregions of interest, for LCA and RCA, respectively. These results compare\nfavorably with previous results obtained using related approaches, and open the\nway to a fully automated method for the identification of stenosis severity\nfrom X-ray angiographies.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:45:54 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Rodrigues", "Dinis L.", ""], ["Menezes", "Miguel Nobre", ""], ["Pinto", "Fausto J.", ""], ["Oliveira", "Arlindo L.", ""]]}, {"id": "2103.02982", "submitter": "Emad Grais", "authors": "Emad M. Grais, Xiaoya Wang, Jie Wang, Fei Zhao, Wen Jiang, Yuexin Cai,\n  Lifang Zhang, Qingwen Lin, Haidi Yang", "title": "Analysing Wideband Absorbance Immittance in Normal and Ears with Otitis\n  Media with Effusion Using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wideband Absorbance Immittance (WAI) has been available for more than a\ndecade, however its clinical use still faces the challenges of limited\nunderstanding and poor interpretation of WAI results. This study aimed to\ndevelop Machine Learning (ML) tools to identify the WAI absorbance\ncharacteristics across different frequency-pressure regions in the normal\nmiddle ear and ears with otitis media with effusion (OME) to enable diagnosis\nof middle ear conditions automatically. Data analysis including pre-processing\nof the WAI data, statistical analysis and classification model development,\ntogether with key regions extraction from the 2D frequency-pressure WAI images\nare conducted in this study. Our experimental results show that ML tools appear\nto hold great potential for the automated diagnosis of middle ear diseases from\nWAI data. The identified key regions in the WAI provide guidance to\npractitioners to better understand and interpret WAI data and offer the\nprospect of quick and accurate diagnostic decisions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:07:36 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Grais", "Emad M.", ""], ["Wang", "Xiaoya", ""], ["Wang", "Jie", ""], ["Zhao", "Fei", ""], ["Jiang", "Wen", ""], ["Cai", "Yuexin", ""], ["Zhang", "Lifang", ""], ["Lin", "Qingwen", ""], ["Yang", "Haidi", ""]]}, {"id": "2103.02984", "submitter": "Dawit Mureja Argaw", "authors": "Dawit Mureja Argaw, Junsik Kim, Francois Rameau, In So Kweon", "title": "Motion-blurred Video Interpolation and Extrapolation", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abrupt motion of camera or objects in a scene result in a blurry video, and\ntherefore recovering high quality video requires two types of enhancements:\nvisual enhancement and temporal upsampling. A broad range of research attempted\nto recover clean frames from blurred image sequences or temporally upsample\nframes by interpolation, yet there are very limited studies handling both\nproblems jointly. In this work, we present a novel framework for deblurring,\ninterpolating and extrapolating sharp frames from a motion-blurred video in an\nend-to-end manner. We design our framework by first learning the pixel-level\nmotion that caused the blur from the given inputs via optical flow estimation\nand then predict multiple clean frames by warping the decoded features with the\nestimated flows. To ensure temporal coherence across predicted frames and\naddress potential temporal ambiguity, we propose a simple, yet effective\nflow-based rule. The effectiveness and favorability of our approach are\nhighlighted through extensive qualitative and quantitative evaluations on\nmotion-blurred datasets from high speed videos.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:18:25 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 08:23:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Argaw", "Dawit Mureja", ""], ["Kim", "Junsik", ""], ["Rameau", "Francois", ""], ["Kweon", "In So", ""]]}, {"id": "2103.02996", "submitter": "Dawit Mureja Argaw", "authors": "Dawit Mureja Argaw, Junsik Kim, Francois Rameau, Jae Won Cho, In So\n  Kweon", "title": "Optical Flow Estimation from a Single Motion-blurred Image", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In most of computer vision applications, motion blur is regarded as an\nundesirable artifact. However, it has been shown that motion blur in an image\nmay have practical interests in fundamental computer vision problems. In this\nwork, we propose a novel framework to estimate optical flow from a single\nmotion-blurred image in an end-to-end manner. We design our network with\ntransformer networks to learn globally and locally varying motions from encoded\nfeatures of a motion-blurred input, and decode left and right frame features\nwithout explicit frame supervision. A flow estimator network is then used to\nestimate optical flow from the decoded features in a coarse-to-fine manner. We\nqualitatively and quantitatively evaluate our model through a large set of\nexperiments on synthetic and real motion-blur datasets. We also provide\nin-depth analysis of our model in connection with related approaches to\nhighlight the effectiveness and favorability of our approach. Furthermore, we\nshowcase the applicability of the flow estimated by our method on deblurring\nand moving object segmentation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:45:18 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 08:17:56 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Argaw", "Dawit Mureja", ""], ["Kim", "Junsik", ""], ["Rameau", "Francois", ""], ["Cho", "Jae Won", ""], ["Kweon", "In So", ""]]}, {"id": "2103.02997", "submitter": "Jinshu Chen", "authors": "Jinshu Chen, Qihui Xu, Qi Kang and MengChu Zhou", "title": "MOGAN: Morphologic-structure-aware Generative Learning from a Single\n  Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most interactive image generation tasks, given regions of interest (ROI)\nby users, the generated results are expected to have adequate diversities in\nappearance while maintaining correct and reasonable structures in original\nimages. Such tasks become more challenging if only limited data is available.\nRecently proposed generative models complete training based on only one image.\nThey pay much attention to the monolithic feature of the sample while ignoring\nthe actual semantic information of different objects inside the sample. As a\nresult, for ROI-based generation tasks, they may produce inappropriate samples\nwith excessive randomicity and without maintaining the related objects' correct\nstructures. To address this issue, this work introduces a\nMOrphologic-structure-aware Generative Adversarial Network named MOGAN that\nproduces random samples with diverse appearances and reliable structures based\non only one image. For training for ROI, we propose to utilize the data coming\nfrom the original image being augmented and bring in a novel module to\ntransform such augmented data into knowledge containing both structures and\nappearances, thus enhancing the model's comprehension of the sample. To learn\nthe rest areas other than ROI, we employ binary masks to ensure the generation\nisolated from ROI. Finally, we set parallel and hierarchical branches of the\nmentioned learning process. Compared with other single image GAN schemes, our\napproach focuses on internal features including the maintenance of rational\nstructures and variation on appearance. Experiments confirm a better capacity\nof our model on ROI-based image generation tasks than its competitive peers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:45:23 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 06:54:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Jinshu", ""], ["Xu", "Qihui", ""], ["Kang", "Qi", ""], ["Zhou", "MengChu", ""]]}, {"id": "2103.03000", "submitter": "Paula Harder", "authors": "Paula Harder, Franz-Josef Pfreundt, Margret Keuper, Janis Keuper", "title": "SpectralDefense: Detecting Adversarial Attacks on CNNs in the Fourier\n  Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the success of convolutional neural networks (CNNs) in many computer\nvision and image analysis tasks, they remain vulnerable against so-called\nadversarial attacks: Small, crafted perturbations in the input images can lead\nto false predictions. A possible defense is to detect adversarial examples. In\nthis work, we show how analysis in the Fourier domain of input images and\nfeature maps can be used to distinguish benign test samples from adversarial\nimages. We propose two novel detection methods: Our first method employs the\nmagnitude spectrum of the input images to detect an adversarial attack. This\nsimple and robust classifier can successfully detect adversarial perturbations\nof three commonly used attack methods. The second method builds upon the first\nand additionally extracts the phase of Fourier coefficients of feature-maps at\ndifferent layers of the network. With this extension, we are able to improve\nadversarial detection rates compared to state-of-the-art detectors on five\ndifferent attack methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:48:28 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 09:35:41 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Harder", "Paula", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Margret", ""], ["Keuper", "Janis", ""]]}, {"id": "2103.03010", "submitter": "Chaoyi Han", "authors": "Chaoyi Han, Yiping Duan, Xiaoming Tao, Jianhua Lu", "title": "Perceptual Image Restoration with High-Quality Priori and Degradation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual image restoration seeks for high-fidelity images that most likely\ndegrade to given images. For better visual quality, previous work proposed to\nsearch for solutions within the natural image manifold, by exploiting the\nlatent space of a generative model. However, the quality of generated images\nare only guaranteed when latent embedding lies close to the prior distribution.\nIn this work, we propose to restrict the feasible region within the prior\nmanifold. This is accomplished with a non-parametric metric for two\ndistributions: the Maximum Mean Discrepancy (MMD). Moreover, we model the\ndegradation process directly as a conditional distribution. We show that our\nmodel performs well in measuring the similarity between restored and degraded\nimages. Instead of optimizing the long criticized pixel-wise distance over\ndegraded images, we rely on such model to find visual pleasing images with high\nprobability. Our simultaneous restoration and enhancement framework generalizes\nwell to real-world complicated degradation types. The experimental results on\nperceptual quality and no-reference image quality assessment (NR-IQA)\ndemonstrate the superior performance of our method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:19:50 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Han", "Chaoyi", ""], ["Duan", "Yiping", ""], ["Tao", "Xiaoming", ""], ["Lu", "Jianhua", ""]]}, {"id": "2103.03014", "submitter": "Lucas Liebenwein", "authors": "Lucas Liebenwein, Cenk Baykal, Brandon Carter, David Gifford, Daniela\n  Rus", "title": "Lost in Pruning: The Effects of Pruning Neural Networks beyond Test\n  Accuracy", "comments": "Published in MLSys 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network pruning is a popular technique used to reduce the inference\ncosts of modern, potentially overparameterized, networks. Starting from a\npre-trained network, the process is as follows: remove redundant parameters,\nretrain, and repeat while maintaining the same test accuracy. The result is a\nmodel that is a fraction of the size of the original with comparable predictive\nperformance (test accuracy). Here, we reassess and evaluate whether the use of\ntest accuracy alone in the terminating condition is sufficient to ensure that\nthe resulting model performs well across a wide spectrum of \"harder\" metrics\nsuch as generalization to out-of-distribution data and resilience to noise.\nAcross evaluations on varying architectures and data sets, we find that pruned\nnetworks effectively approximate the unpruned model, however, the prune ratio\nat which pruned networks achieve commensurate performance varies significantly\nacross tasks. These results call into question the extent of \\emph{genuine}\noverparameterization in deep learning and raise concerns about the\npracticability of deploying pruned networks, specifically in the context of\nsafety-critical systems, unless they are widely evaluated beyond test accuracy\nto reliably predict their performance. Our code is available at\nhttps://github.com/lucaslie/torchprune.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:22:16 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liebenwein", "Lucas", ""], ["Baykal", "Cenk", ""], ["Carter", "Brandon", ""], ["Gifford", "David", ""], ["Rus", "Daniela", ""]]}, {"id": "2103.03024", "submitter": "Yutong Xie", "authors": "Yutong Xie, Jianpeng Zhang, Chunhua Shen, Yong Xia", "title": "CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image\n  Segmentation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks (CNNs) have been the de facto standard for\nnowadays 3D medical image segmentation. The convolutional operations used in\nthese networks, however, inevitably have limitations in modeling the long-range\ndependency due to their inductive bias of locality and weight sharing. Although\nTransformer was born to address this issue, it suffers from extreme\ncomputational and spatial complexities in processing high-resolution 3D feature\nmaps. In this paper, we propose a novel framework that efficiently bridges a\n{\\bf Co}nvolutional neural network and a {\\bf Tr}ansformer {\\bf (CoTr)} for\naccurate 3D medical image segmentation. Under this framework, the CNN is\nconstructed to extract feature representations and an efficient deformable\nTransformer (DeTrans) is built to model the long-range dependency on the\nextracted feature maps. Different from the vanilla Transformer which treats all\nimage positions equally, our DeTrans pays attention only to a small set of key\npositions by introducing the deformable self-attention mechanism. Thus, the\ncomputational and spatial complexities of DeTrans have been greatly reduced,\nmaking it possible to process the multi-scale and high-resolution feature maps,\nwhich are usually of paramount importance for image segmentation. We conduct an\nextensive evaluation on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV)\ndataset that covers 11 major human organs. The results indicate that our CoTr\nleads to a substantial performance improvement over other CNN-based,\ntransformer-based, and hybrid methods on the 3D multi-organ segmentation task.\nCode is available at \\def\\UrlFont{\\rm\\small\\ttfamily}\n\\url{https://github.com/YtongXie/CoTr}\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:34:22 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Xie", "Yutong", ""], ["Zhang", "Jianpeng", ""], ["Shen", "Chunhua", ""], ["Xia", "Yong", ""]]}, {"id": "2103.03027", "submitter": "Praveen Tirupattur", "authors": "Praveen Tirupattur, Kevin Duarte, Yogesh Rawat, Mubarak Shah", "title": "Modeling Multi-Label Action Dependencies for Temporal Action\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world videos contain many complex actions with inherent relationships\nbetween action classes. In this work, we propose an attention-based\narchitecture that models these action relationships for the task of temporal\naction localization in untrimmed videos. As opposed to previous works that\nleverage video-level co-occurrence of actions, we distinguish the relationships\nbetween actions that occur at the same time-step and actions that occur at\ndifferent time-steps (i.e. those which precede or follow each other). We define\nthese distinct relationships as action dependencies. We propose to improve\naction localization performance by modeling these action dependencies in a\nnovel attention-based Multi-Label Action Dependency (MLAD)layer. The MLAD layer\nconsists of two branches: a Co-occurrence Dependency Branch and a Temporal\nDependency Branch to model co-occurrence action dependencies and temporal\naction dependencies, respectively. We observe that existing metrics used for\nmulti-label classification do not explicitly measure how well action\ndependencies are modeled, therefore, we propose novel metrics that consider\nboth co-occurrence and temporal dependencies between action classes. Through\nempirical evaluation and extensive analysis, we show improved performance over\nstate-of-the-art methods on multi-label action localization\nbenchmarks(MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:37:28 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 02:13:00 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 16:19:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tirupattur", "Praveen", ""], ["Duarte", "Kevin", ""], ["Rawat", "Yogesh", ""], ["Shah", "Mubarak", ""]]}, {"id": "2103.03038", "submitter": "Jannis Priesnitz", "authors": "Jannis Priesnitz, Rolf Huesmann, Christian Rathgeb, Nicolas Buchmann,\n  Christoph Busch", "title": "Mobile Touchless Fingerprint Recognition: Implementation, Performance\n  and Usability Aspects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work presents an automated touchless fingerprint recognition system for\nsmartphones. We provide a comprehensive description of the entire recognition\npipeline and discuss important requirements for a fully automated capturing\nsystem. Also, our implementation is made publicly available for research\npurposes. During a database acquisition, a total number of 1,360 touchless and\ntouch-based samples of 29 subjects are captured in two different environmental\nsituations. Experiments on the acquired database show a comparable performance\nof our touchless scheme and the touch-based baseline scheme under constrained\nenvironmental influences. A comparative usability study on both capturing\ndevice types indicates that the majority of subjects prefer the touchless\ncapturing method. Based on our experimental results we analyze the impact of\nthe current COVID-19 pandemic on fingerprint recognition systems. Finally,\nimplementation aspects of touchless fingerprint recognition are summarized.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:56:16 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Priesnitz", "Jannis", ""], ["Huesmann", "Rolf", ""], ["Rathgeb", "Christian", ""], ["Buchmann", "Nicolas", ""], ["Busch", "Christoph", ""]]}, {"id": "2103.03046", "submitter": "Hongbin Liu", "authors": "Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong", "title": "PointGuard: Provably Robust 3D Point Cloud Classification", "comments": "Published in IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud classification has many safety-critical applications such as\nautonomous driving and robotic grasping. However, several studies showed that\nit is vulnerable to adversarial attacks. In particular, an attacker can make a\nclassifier predict an incorrect label for a 3D point cloud via carefully\nmodifying, adding, and/or deleting a small number of its points. Randomized\nsmoothing is state-of-the-art technique to build certifiably robust 2D image\nclassifiers. However, when applied to 3D point cloud classification, randomized\nsmoothing can only certify robustness against adversarially modified points.\n  In this work, we propose PointGuard, the first defense that has provable\nrobustness guarantees against adversarially modified, added, and/or deleted\npoints. Specifically, given a 3D point cloud and an arbitrary point cloud\nclassifier, our PointGuard first creates multiple subsampled point clouds, each\nof which contains a random subset of the points in the original point cloud;\nthen our PointGuard predicts the label of the original point cloud as the\nmajority vote among the labels of the subsampled point clouds predicted by the\npoint cloud classifier. Our first major theoretical contribution is that we\nshow PointGuard provably predicts the same label for a 3D point cloud when the\nnumber of adversarially modified, added, and/or deleted points is bounded. Our\nsecond major theoretical contribution is that we prove the tightness of our\nderived bound when no assumptions on the point cloud classifier are made.\nMoreover, we design an efficient algorithm to compute our certified robustness\nguarantees. We also empirically evaluate PointGuard on ModelNet40 and ScanNet\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:09:37 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 09:02:55 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Liu", "Hongbin", ""], ["Jia", "Jinyuan", ""], ["Gong", "Neil Zhenqiang", ""]]}, {"id": "2103.03048", "submitter": "Christopher Kanan", "authors": "Usman Mahmood, Robik Shrestha, David D.B. Bates, Lorenzo Mannelli,\n  Giuseppe Corrias, Yusuf Erdi, Christopher Kanan", "title": "Detecting Spurious Correlations with Sanity Tests for Artificial\n  Intelligence Guided Radiology Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) has been successful at solving numerous problems\nin machine perception. In radiology, AI systems are rapidly evolving and show\nprogress in guiding treatment decisions, diagnosing, localizing disease on\nmedical images, and improving radiologists' efficiency. A critical component to\ndeploying AI in radiology is to gain confidence in a developed system's\nefficacy and safety. The current gold standard approach is to conduct an\nanalytical validation of performance on a generalization dataset from one or\nmore institutions, followed by a clinical validation study of the system's\nefficacy during deployment. Clinical validation studies are time-consuming, and\nbest practices dictate limited re-use of analytical validation data, so it is\nideal to know ahead of time if a system is likely to fail analytical or\nclinical validation. In this paper, we describe a series of sanity tests to\nidentify when a system performs well on development data for the wrong reasons.\nWe illustrate the sanity tests' value by designing a deep learning system to\nclassify pancreatic cancer seen in computed tomography scans.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:14:05 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Mahmood", "Usman", ""], ["Shrestha", "Robik", ""], ["Bates", "David D. B.", ""], ["Mannelli", "Lorenzo", ""], ["Corrias", "Giuseppe", ""], ["Erdi", "Yusuf", ""], ["Kanan", "Christopher", ""]]}, {"id": "2103.03055", "submitter": "Matej Gazda", "authors": "Matej Gazda, Jakub Gazda, Jan Plavka, Peter Drotar", "title": "Self-supervised deep convolutional neural network for chest X-ray\n  classification", "comments": "This work has been submitted to the IEEE transactions on medical\n  imaging for possible publication. Copyright may be transferred without\n  notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chest radiography is a relatively cheap, widely available medical procedure\nthat conveys key information for making diagnostic decisions. Chest X-rays are\nalmost always used in the diagnosis of respiratory diseases such as pneumonia\nor the recent COVID-19. In this paper, we propose a self-supervised deep neural\nnetwork that is pretrained on an unlabeled chest X-ray dataset. The learned\nrepresentations are transferred to downstream task - the classification of\nrespiratory diseases. The results obtained on four public datasets show that\nour approach yields competitive results without requiring large amounts of\nlabeled training data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:28:37 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 07:34:50 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Gazda", "Matej", ""], ["Gazda", "Jakub", ""], ["Plavka", "Jan", ""], ["Drotar", "Peter", ""]]}, {"id": "2103.03059", "submitter": "Samuel Earp", "authors": "Samuel W. F. Earp and Aubin Samacoits and Sanjana Jain and Pavit\n  Noinongyao and Siwa Boonpunmongkol", "title": "Sub-pixel face landmarks using heatmaps and a bag of tricks", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate face landmark localization is an essential part of face recognition,\nreconstruction and morphing. To accurately localize face landmarks, we present\nour heatmap regression approach. Each model consists of a MobileNetV2 backbone\nfollowed by several upscaling layers, with different tricks to optimize both\nperformance and inference cost. We use five na\\\"ive face landmarks from a\npublicly available face detector to position and align the face instead of\nusing the bounding box like traditional methods. Moreover, we show by adding\nrandom rotation, displacement and scaling -- after alignment -- that the model\nis more sensitive to the face position than orientation. We also show that it\nis possible to reduce the upscaling complexity by using a mixture of\ndeconvolution and pixel-shuffle layers without impeding localization\nperformance. We present our state-of-the-art face landmark localization model\n(ranking second on The 2nd Grand Challenge of 106-Point Facial Landmark\nLocalization validation set). Finally, we test the effect on face recognition\nusing these landmarks, using a publicly available model and benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:34:20 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 02:55:39 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Earp", "Samuel W. F.", ""], ["Samacoits", "Aubin", ""], ["Jain", "Sanjana", ""], ["Noinongyao", "Pavit", ""], ["Boonpunmongkol", "Siwa", ""]]}, {"id": "2103.03060", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Mehmet Yamac, Moncef Gabbouj", "title": "BM3D vs 2-Layer ONN", "comments": "Submitted for review in ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite their recent success on image denoising, the need for deep and\ncomplex architectures still hinders the practical usage of CNNs. Older but\ncomputationally more efficient methods such as BM3D remain a popular choice,\nespecially in resource-constrained scenarios. In this study, we aim to find out\nwhether compact neural networks can learn to produce competitive results as\ncompared to BM3D for AWGN image denoising. To this end, we configure networks\nwith only two hidden layers and employ different neuron models and layer widths\nfor comparing the performance with BM3D across different AWGN noise levels. Our\nresults conclusively show that the recently proposed self-organized variant of\noperational neural networks based on a generative neuron model (Self-ONNs) is\nnot only a better choice as compared to CNNs, but also provide competitive\nresults as compared to BM3D and even significantly surpass it for high noise\nlevels.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:37:23 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Yamac", "Mehmet", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2103.03062", "submitter": "Gintautas Palubinskas", "authors": "Gintautas Palubinskas", "title": "Model-based image adjustment for a successful pansharpening", "comments": "17 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new model-based image adjustment for the enhancement of multi-resolution\nimage fusion or pansharpening is proposed. Such image adjustment is needed for\nmost pansharpening methods using panchromatic band and/or intensity image\n(calculated as a weighted sum of multispectral bands) as an input. Due various\nreasons, e.g. calibration inaccuracies, usage of different sensors, input\nimages for pansharpening: low resolution multispectral image or more precisely\nthe calculated intensity image and high resolution panchromatic image may\ndiffer in values of their physical properties, e.g. radiances or reflectances\ndepending on the processing level. But the same objects/classes in both images\nshould exhibit similar values or more generally similar statistics. Similarity\ndefinition will depend on a particular application. For a successful fusion of\ndata from two sensors the energy balance between radiances/reflectances of both\nsensors should hold. A virtual band is introduced to compensate for total\nenergy disbalance in different sensors. Its estimation consists of several\nsteps: first, weights for individual spectral bands are estimated in a low\nresolution scale, where both multispectral and panchromatic images (low pass\nfiltered version) are available, then, the estimated virtual band is up-sampled\nto a high scale and, finally, high resolution panchromatic band is corrected by\nsubtracting virtual band. This corrected panchromatic band is used instead of\noriginal panchromatic image in the following pansharpening. It is shown, for\nexample, that the performance quality of component substitution based methods\ncan be increased significantly.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:38:22 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Palubinskas", "Gintautas", ""]]}, {"id": "2103.03067", "submitter": "Maosheng Ye", "authors": "Maosheng Ye, Tongyi Cao, Qifeng Chen", "title": "TPCN: Temporal Point Cloud Networks for Motion Forecasting", "comments": "accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Temporal Point Cloud Networks (TPCN), a novel and flexible\nframework with joint spatial and temporal learning for trajectory prediction.\nUnlike existing approaches that rasterize agents and map information as 2D\nimages or operate in a graph representation, our approach extends ideas from\npoint cloud learning with dynamic temporal learning to capture both spatial and\ntemporal information by splitting trajectory prediction into both spatial and\ntemporal dimensions. In the spatial dimension, agents can be viewed as an\nunordered point set, and thus it is straightforward to apply point cloud\nlearning techniques to model agents' locations. While the spatial dimension\ndoes not take kinematic and motion information into account, we further propose\ndynamic temporal learning to model agents' motion over time. Experiments on the\nArgoverse motion forecasting benchmark show that our approach achieves the\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:44:32 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ye", "Maosheng", ""], ["Cao", "Tongyi", ""], ["Chen", "Qifeng", ""]]}, {"id": "2103.03070", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Mehmet Yamac, Esin Guldogan, Moncef\n  Gabbouj", "title": "Convolutional versus Self-Organized Operational Neural Networks for\n  Real-World Blind Image Denoising", "comments": "Submitted for review in IEEE TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Real-world blind denoising poses a unique image restoration challenge due to\nthe non-deterministic nature of the underlying noise distribution. Prevalent\ndiscriminative networks trained on synthetic noise models have been shown to\ngeneralize poorly to real-world noisy images. While curating real-world noisy\nimages and improving ground truth estimation procedures remain key points of\ninterest, a potential research direction is to explore extensions to the widely\nused convolutional neuron model to enable better generalization with fewer data\nand lower network complexity, as opposed to simply using deeper Convolutional\nNeural Networks (CNNs). Operational Neural Networks (ONNs) and their recent\nvariant, Self-organized ONNs (Self-ONNs), propose to embed enhanced\nnon-linearity into the neuron model and have been shown to outperform CNNs\nacross a variety of regression tasks. However, all such comparisons have been\nmade for compact networks and the efficacy of deploying operational layers as a\ndrop-in replacement for convolutional layers in contemporary deep architectures\nremains to be seen. In this work, we tackle the real-world blind image\ndenoising problem by employing, for the first time, a deep Self-ONN. Extensive\nquantitative and qualitative evaluations spanning multiple metrics and four\nhigh-resolution real-world noisy image datasets against the state-of-the-art\ndeep CNN network, DnCNN, reveal that deep Self-ONNs consistently achieve\nsuperior results with performance gains of up to 1.76dB in PSNR. Furthermore,\nSelf-ONNs with half and even quarter the number of layers that require only a\nfraction of computational resources as that of DnCNN can still achieve similar\nor better results compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:49:17 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 20:21:56 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Yamac", "Mehmet", ""], ["Guldogan", "Esin", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2103.03078", "submitter": "Philippe Burlina", "authors": "William Paul, Yinzhi Cao, Miaomiao Zhang, and Phil Burlina", "title": "Defending Medical Image Diagnostics against Privacy Attacks using\n  Generative Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.CY cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models used in medical imaging diagnostics can be\nvulnerable to a variety of privacy attacks, including membership inference\nattacks, that lead to violations of regulations governing the use of medical\ndata and threaten to compromise their effective deployment in the clinic. In\ncontrast to most recent work in privacy-aware ML that has been focused on model\nalteration and post-processing steps, we propose here a novel and complementary\nscheme that enhances the security of medical data by controlling the data\nsharing process. We develop and evaluate a privacy defense protocol based on\nusing a generative adversarial network (GAN) that allows a medical data sourcer\n(e.g. a hospital) to provide an external agent (a modeler) a proxy dataset\nsynthesized from the original images, so that the resulting diagnostic systems\nmade available to model consumers is rendered resilient to privacy attackers.\nWe validate the proposed method on retinal diagnostics AI used for diabetic\nretinopathy that bears the risk of possibly leaking private information. To\nincorporate concerns of both privacy advocates and modelers, we introduce a\nmetric to evaluate privacy and utility performance in combination, and\ndemonstrate, using these novel and classical metrics, that our approach, by\nitself or in conjunction with other defenses, provides state of the art (SOTA)\nperformance for defending against privacy attacks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:02:57 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Paul", "William", ""], ["Cao", "Yinzhi", ""], ["Zhang", "Miaomiao", ""], ["Burlina", "Phil", ""]]}, {"id": "2103.03086", "submitter": "Rohan Bhowmik T", "authors": "Rohan Tan Bhowmik", "title": "A Multi-Modal Respiratory Disease Exacerbation Prediction Technique\n  Based on a Spatio-Temporal Machine Learning Architecture", "comments": "Updated Title, References, and Acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chronic respiratory diseases, such as chronic obstructive pulmonary disease\nand asthma, are a serious health crisis, affecting a large number of people\nglobally and inflicting major costs on the economy. Current methods for\nassessing the progression of respiratory symptoms are either subjective and\ninaccurate, or complex and cumbersome, and do not incorporate environmental\nfactors. Lacking predictive assessments and early intervention, unexpected\nexacerbations can lead to hospitalizations and high medical costs. This work\npresents a multi-modal solution for predicting the exacerbation risks of\nrespiratory diseases, such as COPD, based on a novel spatio-temporal machine\nlearning architecture for real-time and accurate respiratory events detection,\nand tracking of local environmental and meteorological data and trends. The\nproposed new machine learning architecture blends key attributes of both\nconvolutional and recurrent neural networks, allowing extraction of both\nspatial and temporal features encoded in respiratory sounds, thereby leading to\naccurate classification and tracking of symptoms. Combined with the data from\nenvironmental and meteorological sensors, and a predictive model based on\nretrospective medical studies, this solution can assess and provide early\nwarnings of respiratory disease exacerbations. This research will improve the\nquality of patients' lives through early medical intervention, thereby reducing\nhospitalization rates and medical costs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:24:53 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 17:34:53 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bhowmik", "Rohan Tan", ""]]}, {"id": "2103.03089", "submitter": "Ziheng Cheng", "authors": "Ziheng Cheng, Bo Chen, Guanliang Liu, Hao Zhang, Ruiying Lu, Zhengjue\n  Wang, Xin Yuan", "title": "Memory-Efficient Network for Large-scale Video Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video snapshot compressive imaging (SCI) captures a sequence of video frames\nin a single shot using a 2D detector. The underlying principle is that during\none exposure time, different masks are imposed on the high-speed scene to form\na compressed measurement. With the knowledge of masks, optimization algorithms\nor deep learning methods are employed to reconstruct the desired high-speed\nvideo frames from this snapshot measurement. Unfortunately, though these\nmethods can achieve decent results, the long running time of optimization\nalgorithms or huge training memory occupation of deep networks still preclude\nthem in practical applications. In this paper, we develop a memory-efficient\nnetwork for large-scale video SCI based on multi-group reversible 3D\nconvolutional neural networks. In addition to the basic model for the grayscale\nSCI system, we take one step further to combine demosaicing and SCI\nreconstruction to directly recover color video from Bayer measurements.\nExtensive results on both simulation and real data captured by SCI cameras\ndemonstrate that our proposed model outperforms previous state-of-the-art with\nless memory and thus can be used in large-scale problems. The code is at\nhttps://github.com/BoChenGroup/RevSCI-net.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:14:58 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 08:52:14 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Cheng", "Ziheng", ""], ["Chen", "Bo", ""], ["Liu", "Guanliang", ""], ["Zhang", "Hao", ""], ["Lu", "Ruiying", ""], ["Wang", "Zhengjue", ""], ["Yuan", "Xin", ""]]}, {"id": "2103.03096", "submitter": "Rudresh Dwivedi", "authors": "Devam Dave, Het Naik, Smiti Singhal, Rudresh Dwivedi, Pankesh Patel", "title": "Towards Designing Computer Vision-based Explainable-AI Solution: A Use\n  Case of Livestock Mart Industry", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of an online Mart is to match buyers and sellers, to weigh\nanimals and to oversee their sale. A reliable pricing method can be developed\nby ML models that can read through historical sales data. However, when AI\nmodels suggest or recommend a price, that in itself does not reveal too much\n(i.e., it acts like a black box) about the qualities and the abilities of an\nanimal. An interested buyer would like to know more about the salient features\nof an animal before making the right choice based on his requirements. A model\ncapable of explaining the different factors that impact the price point is\nessential for the needs of the market. It can also inspire confidence in buyers\nand sellers about the price point offered. To achieve these objectives, we have\nbeen working with the team at MartEye, a startup based in Portershed in Galway\nCity, Ireland. Through this paper, we report our work-in-progress research\ntowards building a smart video analytic platform, leveraging Explainable AI\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:11:19 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dave", "Devam", ""], ["Naik", "Het", ""], ["Singhal", "Smiti", ""], ["Dwivedi", "Rudresh", ""], ["Patel", "Pankesh", ""]]}, {"id": "2103.03097", "submitter": "Jindong Wang", "authors": "Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Wenjun Zeng, Tao\n  Qin", "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization", "comments": "15 pages; short version (6 pages) has been accepted by IJCAI-21\n  survey track; codebase:\n  https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning systems generally assume that the training and testing\ndistributions are the same. To this end, a key requirement is to develop models\nthat can generalize to unseen distributions. Domain generalization (DG), i.e.,\nout-of-distribution generalization, has attracted increasing interests in\nrecent years. Domain generalization deals with a challenging setting where one\nor several different but related domain(s) are given, and the goal is to learn\na model that can generalize to an unseen test domain. Great progress has been\nmade in the area of domain generalization for years. This paper presents the\nfirst review of recent advances in this area. First, we provide a formal\ndefinition of domain generalization and discuss several related fields. We then\nthoroughly review the theories related to domain generalization and carefully\nanalyze the theory behind generalization. We categorize recent algorithms into\nthree classes: data manipulation, representation learning, and learning\nstrategy, and present several popular algorithms in detail for each category.\nThird, we introduce the commonly used datasets and applications. Finally, we\nsummarize existing literature and present some potential research topics for\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:04:11 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 06:11:06 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 02:21:03 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 03:31:28 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Jindong", ""], ["Lan", "Cuiling", ""], ["Liu", "Chang", ""], ["Ouyang", "Yidong", ""], ["Zeng", "Wenjun", ""], ["Qin", "Tao", ""]]}, {"id": "2103.03102", "submitter": "Wei Dai", "authors": "Wei Dai, Daniel Berleant", "title": "Benchmarking Deep Learning Classifiers: Beyond Accuracy", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous research evaluating deep learning (DL) classifiers has often used\ntop-1/top-5 accuracy. However, the accuracy of DL classifiers is unstable in\nthat it often changes significantly when retested on imperfect or adversarial\nimages. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on imperfect images by proposing\na two-dimensional metric, consisting of mean accuracy and coefficient of\nvariation, to measure the robustness of DL classifiers. Spearman's rank\ncorrelation coefficient and Pearson's correlation coefficient are used and\ntheir independence evaluated. A statistical plot we call mCV is presented which\naims to help visualize the robustness of the performance of DL classifiers\nacross varying amounts of imperfection in tested images. Finally, we\ndemonstrate that defective images corrupted by two-factor corruption could be\nused to improve the robustness of DL classifiers. All source codes and related\nimage sets are shared on a website (http://www.animpala.com) to support future\nresearch projects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:10:54 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dai", "Wei", ""], ["Berleant", "Daniel", ""]]}, {"id": "2103.03114", "submitter": "Heng Yang", "authors": "Heng Yang, Wei Dong, Luca Carlone, Vladlen Koltun", "title": "Self-supervised Geometric Perception", "comments": "CVPR 2021, Oral presentation. 8 pages main results, 19 pages in\n  total, including references and supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present self-supervised geometric perception (SGP), the first general\nframework to learn a feature descriptor for correspondence matching without any\nground-truth geometric model labels (e.g., camera poses, rigid\ntransformations). Our first contribution is to formulate geometric perception\nas an optimization problem that jointly optimizes the feature descriptor and\nthe geometric models given a large corpus of visual measurements (e.g., images,\npoint clouds). Under this optimization formulation, we show that two important\nstreams of research in vision, namely robust model fitting and deep feature\nlearning, correspond to optimizing one block of the unknown variables while\nfixing the other block. This analysis naturally leads to our second\ncontribution -- the SGP algorithm that performs alternating minimization to\nsolve the joint optimization. SGP iteratively executes two meta-algorithms: a\nteacher that performs robust model fitting given learned features to generate\ngeometric pseudo-labels, and a student that performs deep feature learning\nunder noisy supervision of the pseudo-labels. As a third contribution, we apply\nSGP to two perception problems on large-scale real datasets, namely relative\ncamera pose estimation on MegaDepth and point cloud registration on 3DMatch. We\ndemonstrate that SGP achieves state-of-the-art performance that is on-par or\nsuperior to the supervised oracles trained using ground-truth labels.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:34:43 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yang", "Heng", ""], ["Dong", "Wei", ""], ["Carlone", "Luca", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2103.03123", "submitter": "Emilien Dupont", "authors": "Emilien Dupont, Adam Goli\\'nski, Milad Alizadeh, Yee Whye Teh, Arnaud\n  Doucet", "title": "COIN: COmpression with Implicit Neural representations", "comments": "Added qualitative comparisons and link to github repo\n  https://github.com/EmilienDupont/coin", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new simple approach for image compression: instead of storing\nthe RGB values for each pixel of an image, we store the weights of a neural\nnetwork overfitted to the image. Specifically, to encode an image, we fit it\nwith an MLP which maps pixel locations to RGB values. We then quantize and\nstore the weights of this MLP as a code for the image. To decode the image, we\nsimply evaluate the MLP at every pixel location. We found that this simple\napproach outperforms JPEG at low bit-rates, even without entropy coding or\nlearning a distribution over weights. While our framework is not yet\ncompetitive with state of the art compression methods, we show that it has\nvarious attractive properties which could make it a viable alternative to other\nneural data compression approaches.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:58:39 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 16:36:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dupont", "Emilien", ""], ["Goli\u0144ski", "Adam", ""], ["Alizadeh", "Milad", ""], ["Teh", "Yee Whye", ""], ["Doucet", "Arnaud", ""]]}, {"id": "2103.03129", "submitter": "Pushpak Pati", "authors": "Valentin Anklin, Pushpak Pati, Guillaume Jaume, Behzad Bozorgtabar,\n  Antonio Foncubierta-Rodr\\'iguez, Jean-Philippe Thiran, Mathilde Sibony, Maria\n  Gabrani, Orcun Goksel", "title": "Learning Whole-Slide Segmentation from Inexact and Incomplete Labels\n  using Tissue Graphs", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting histology images into diagnostically relevant regions is\nimperative to support timely and reliable decisions by pathologists. To this\nend, computer-aided techniques have been proposed to delineate relevant regions\nin scanned histology slides. However, the techniques necessitate task-specific\nlarge datasets of annotated pixels, which is tedious, time-consuming,\nexpensive, and infeasible to acquire for many histology tasks. Thus,\nweakly-supervised semantic segmentation techniques are proposed to utilize weak\nsupervision that is cheaper and quicker to acquire. In this paper, we propose\nSegGini, a weakly supervised segmentation method using graphs, that can utilize\nweak multiplex annotations, i.e. inexact and incomplete annotations, to segment\narbitrary and large images, scaling from tissue microarray (TMA) to whole slide\nimage (WSI). Formally, SegGini constructs a tissue-graph representation for an\ninput histology image, where the graph nodes depict tissue regions. Then, it\nperforms weakly-supervised segmentation via node classification by using\ninexact image-level labels, incomplete scribbles, or both. We evaluated SegGini\non two public prostate cancer datasets containing TMAs and WSIs. Our method\nachieved state-of-the-art segmentation performance on both datasets for various\nannotation settings while being comparable to a pathologist baseline.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 16:04:24 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Anklin", "Valentin", ""], ["Pati", "Pushpak", ""], ["Jaume", "Guillaume", ""], ["Bozorgtabar", "Behzad", ""], ["Foncubierta-Rodr\u00edguez", "Antonio", ""], ["Thiran", "Jean-Philippe", ""], ["Sibony", "Mathilde", ""], ["Gabrani", "Maria", ""], ["Goksel", "Orcun", ""]]}, {"id": "2103.03133", "submitter": "\\v{S}imon Bil\\'ik", "authors": "Simon Bilik, Lukas Kratochvila, Adam Ligocki, Ondrej Bostik, Tomas\n  Zemcik, Matous Hybl, Karel Horak, Ludek Zalud", "title": "Visual diagnosis of the Varroa destructor parasitic mite in honeybees\n  using object detector techniques", "comments": null, "journal-ref": null, "doi": "10.3390/s21082764", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Varroa destructor mite is one of the most dangerous Honey Bee (Apis\nmellifera) parasites worldwide and the bee colonies have to be regularly\nmonitored in order to control its spread. Here we present an object detector\nbased method for health state monitoring of bee colonies. This method has the\npotential for online measurement and processing. In our experiment, we compare\nthe YOLO and SSD object detectors along with the Deep SVDD anomaly detector.\nBased on the custom dataset with 600 ground-truth images of healthy and\ninfected bees in various scenes, the detectors reached a high F1 score up to\n0.874 in the infected bee detection and up to 0.727 in the detection of the\nVarroa Destructor mite itself. The results demonstrate the potential of this\napproach, which will be later used in the real-time computer vision based honey\nbee inspection system. To the best of our knowledge, this study is the first\none using object detectors for this purpose. We expect that performance of\nthose object detectors will enable us to inspect the health status of the honey\nbee colonies.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:01:31 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bilik", "Simon", ""], ["Kratochvila", "Lukas", ""], ["Ligocki", "Adam", ""], ["Bostik", "Ondrej", ""], ["Zemcik", "Tomas", ""], ["Hybl", "Matous", ""], ["Horak", "Karel", ""], ["Zalud", "Ludek", ""]]}, {"id": "2103.03150", "submitter": "Shoaib Azam", "authors": "Farzeen Munir, Shoaib Azam and Moongu Jeon", "title": "SSTN: Self-Supervised Domain Adaptation Thermal Object Detection for\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sensibility and sensitivity of the environment play a decisive role in\nthe safe and secure operation of autonomous vehicles. This perception of the\nsurrounding is way similar to human visual representation. The human's brain\nperceives the environment by utilizing different sensory channels and develop a\nview-invariant representation model. Keeping in this context, different\nexteroceptive sensors are deployed on the autonomous vehicle for perceiving the\nenvironment. The most common exteroceptive sensors are camera, Lidar and radar\nfor autonomous vehicle's perception. Despite being these sensors have\nillustrated their benefit in the visible spectrum domain yet in the adverse\nweather conditions, for instance, at night, they have limited operation\ncapability, which may lead to fatal accidents. In this work, we explore thermal\nobject detection to model a view-invariant model representation by employing\nthe self-supervised contrastive learning approach. For this purpose, we have\nproposed a deep neural network Self Supervised Thermal Network (SSTN) for\nlearning the feature embedding to maximize the information between visible and\ninfrared spectrum domain by contrastive learning, and later employing these\nlearned feature representation for the thermal object detection using\nmulti-scale encoder-decoder transformer network. The proposed method is\nextensively evaluated on the two publicly available datasets: the FLIR-ADAS\ndataset and the KAIST Multi-Spectral dataset. The experimental results\nillustrate the efficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 16:42:49 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Munir", "Farzeen", ""], ["Azam", "Shoaib", ""], ["Jeon", "Moongu", ""]]}, {"id": "2103.03158", "submitter": "Jacob Reinhold", "authors": "Jacob C. Reinhold, Aaron Carass, Jerry L. Prince", "title": "A Structural Causal Model for MR Images of Multiple Sclerosis", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Precision medicine involves answering counterfactual questions such as \"Would\nthis patient respond better to treatment A or treatment B?\" These types of\nquestions are causal in nature and require the tools of causal inference to be\nanswered, e.g., with a structural causal model (SCM). In this work, we develop\nan SCM that models the interaction between demographic information, disease\ncovariates, and magnetic resonance (MR) images of the brain for people with\nmultiple sclerosis. Inference in the SCM generates counterfactual images that\nshow what an MR image of the brain would look like if demographic or disease\ncovariates are changed. These images can be used for modeling disease\nprogression or used for image processing tasks where controlling for\nconfounders is necessary.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:04:26 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 16:31:14 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 20:55:53 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Reinhold", "Jacob C.", ""], ["Carass", "Aaron", ""], ["Prince", "Jerry L.", ""]]}, {"id": "2103.03166", "submitter": "Yuzhe Lu", "authors": "Yuzhe Lu, Aadarsh Jha, and Yuankai Huo", "title": "Contrastive Learning Meets Transfer Learning: A Case Study In Medical\n  Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotated medical images are typically rarer than labeled natural images\nsince they are limited by domain knowledge and privacy constraints. Recent\nadvances in transfer and contrastive learning have provided effective solutions\nto tackle such issues from different perspectives. The state-of-the-art\ntransfer learning (e.g., Big Transfer (BiT)) and contrastive learning (e.g.,\nSimple Siamese Contrastive Learning (SimSiam)) approaches have been\ninvestigated independently, without considering the complementary nature of\nsuch techniques. It would be appealing to accelerate contrastive learning with\ntransfer learning, given that slow convergence speed is a critical limitation\nof modern contrastive learning approaches. In this paper, we investigate the\nfeasibility of aligning BiT with SimSiam. From empirical analyses, different\nnormalization techniques (Group Norm in BiT vs. Batch Norm in SimSiam) are the\nkey hurdle of adapting BiT to SimSiam. When combining BiT with SimSiam, we\nevaluated the performance of using BiT, SimSiam, and BiT+SimSiam on CIFAR-10\nand HAM10000 datasets. The results suggest that the BiT models accelerate the\nconvergence speed of SimSiam. When used together, the model gives superior\nperformance over both of its counterparts. We hope this study will motivate\nresearchers to revisit the task of aggregating big pre-trained models with\ncontrastive learning models for image analysis.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:19:54 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Lu", "Yuzhe", ""], ["Jha", "Aadarsh", ""], ["Huo", "Yuankai", ""]]}, {"id": "2103.03170", "submitter": "Ruixu Liu", "authors": "Ruixu Liu, Ju Shen, He Wang, Chen Chen, Sen-ching Cheung, Vijayan K.\n  Asari", "title": "Enhanced 3D Human Pose Estimation from Videos by using Attention-Based\n  Neural Network with Dilated Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The attention mechanism provides a sequential prediction framework for\nlearning spatial models with enhanced implicit temporal consistency. In this\nwork, we show a systematic design (from 2D to 3D) for how conventional networks\nand other forms of constraints can be incorporated into the attention framework\nfor learning long-range dependencies for the task of pose estimation. The\ncontribution of this paper is to provide a systematic approach for designing\nand training of attention-based models for the end-to-end pose estimation, with\nthe flexibility and scalability of arbitrary video sequences as input. We\nachieve this by adapting temporal receptive field via a multi-scale structure\nof dilated convolutions. Besides, the proposed architecture can be easily\nadapted to a causal model enabling real-time performance. Any off-the-shelf 2D\npose estimation systems, e.g. Mocap libraries, can be easily integrated in an\nad-hoc fashion. Our method achieves the state-of-the-art performance and\noutperforms existing methods by reducing the mean per joint position error to\n33.4 mm on Human3.6M dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:26:51 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liu", "Ruixu", ""], ["Shen", "Ju", ""], ["Wang", "He", ""], ["Chen", "Chen", ""], ["Cheung", "Sen-ching", ""], ["Asari", "Vijayan K.", ""]]}, {"id": "2103.03188", "submitter": "Santiago Toledo-Cort\\'es", "authors": "Santiago Toledo-Cort\\'es, Diego H. Useche, and Fabio A. Gonz\\'alez", "title": "Prostate Tissue Grading with Deep Quantum Measurement Ordinal Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prostate cancer (PCa) is one of the most common and aggressive cancers\nworldwide. The Gleason score (GS) system is the standard way of classifying\nprostate cancer and the most reliable method to determine the severity and\ntreatment to follow. The pathologist looks at the arrangement of cancer cells\nin the prostate and assigns a score on a scale that ranges from 6 to 10.\nAutomatic analysis of prostate whole-slide images (WSIs) is usually addressed\nas a binary classification problem, which misses the finer distinction between\nstages given by the GS. This paper presents a probabilistic deep learning\nordinal classification method that can estimate the GS from a prostate WSI.\nApproaching the problem as an ordinal regression task using a differentiable\nprobabilistic model not only improves the interpretability of the results, but\nalso improves the accuracy of the model when compared to conventional deep\nclassification and regression architectures.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:52:00 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Toledo-Cort\u00e9s", "Santiago", ""], ["Useche", "Diego H.", ""], ["Gonz\u00e1lez", "Fabio A.", ""]]}, {"id": "2103.03206", "submitter": "Andrew Jaegle", "authors": "Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman\n  and Oriol Vinyals and Joao Carreira", "title": "Perceiver: General Perception with Iterative Attention", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological systems perceive the world by simultaneously processing\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\nproprioception, etc. The perception models used in deep learning on the other\nhand are designed for individual modalities, often relying on domain-specific\nassumptions such as the local grid structures exploited by virtually all\nexisting vision models. These priors introduce helpful inductive biases, but\nalso lock models to individual modalities. In this paper we introduce the\nPerceiver - a model that builds upon Transformers and hence makes few\narchitectural assumptions about the relationship between its inputs, but that\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\nleverages an asymmetric attention mechanism to iteratively distill inputs into\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\nshow that this architecture is competitive with or outperforms strong,\nspecialized models on classification tasks across various modalities: images,\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\nattending to 50,000 pixels. It is also competitive in all modalities in\nAudioSet.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:20:50 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 00:25:31 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Jaegle", "Andrew", ""], ["Gimeno", "Felix", ""], ["Brock", "Andrew", ""], ["Zisserman", "Andrew", ""], ["Vinyals", "Oriol", ""], ["Carreira", "Joao", ""]]}, {"id": "2103.03230", "submitter": "Jure Zbontar", "authors": "Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, St\\'ephane Deny", "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction", "comments": "13 pages, 6 figures, to appear at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning (SSL) is rapidly closing the gap with supervised\nmethods on large computer vision benchmarks. A successful approach to SSL is to\nlearn embeddings which are invariant to distortions of the input sample.\nHowever, a recurring issue with this approach is the existence of trivial\nconstant solutions. Most current methods avoid such solutions by careful\nimplementation details. We propose an objective function that naturally avoids\ncollapse by measuring the cross-correlation matrix between the outputs of two\nidentical networks fed with distorted versions of a sample, and making it as\nclose to the identity matrix as possible. This causes the embedding vectors of\ndistorted versions of a sample to be similar, while minimizing the redundancy\nbetween the components of these vectors. The method is called Barlow Twins,\nowing to neuroscientist H. Barlow's redundancy-reduction principle applied to a\npair of identical networks. Barlow Twins does not require large batches nor\nasymmetry between the network twins such as a predictor network, gradient\nstopping, or a moving average on the weight updates. Intriguingly it benefits\nfrom very high-dimensional output vectors. Barlow Twins outperforms previous\nmethods on ImageNet for semi-supervised classification in the low-data regime,\nand is on par with current state of the art for ImageNet classification with a\nlinear classifier head, and for transfer tasks of classification and object\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:55:09 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 09:36:29 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 14:09:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zbontar", "Jure", ""], ["Jing", "Li", ""], ["Misra", "Ishan", ""], ["LeCun", "Yann", ""], ["Deny", "St\u00e9phane", ""]]}, {"id": "2103.03231", "submitter": "Thomas Neff", "authors": "Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H.\n  Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger", "title": "DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields\n  using Depth Oracle Networks", "comments": "Accepted to EGSR 2021 in the CGF track; Project website:\n  https://depthoraclenerf.github.io/", "journal-ref": "Computer Graphics Forum Volume 40, Issue 4, 2021", "doi": "10.1111/cgf.14340", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent research explosion around implicit neural representations, such as\nNeRF, shows that there is immense potential for implicitly storing high-quality\nscene and lighting information in compact neural networks. However, one major\nlimitation preventing the use of NeRF in real-time rendering applications is\nthe prohibitive computational cost of excessive network evaluations along each\nview ray, requiring dozens of petaFLOPS. In this work, we bring compact neural\nrepresentations closer to practical rendering of synthetic content in real-time\napplications, such as games and virtual reality. We show that the number of\nsamples required for each view ray can be significantly reduced when samples\nare placed around surfaces in the scene without compromising image quality. To\nthis end, we propose a depth oracle network that predicts ray sample locations\nfor each view ray with a single network evaluation. We show that using a\nclassification network around logarithmically discretized and spherically\nwarped depth values is essential to encode surface locations rather than\ndirectly estimating depth. The combination of these techniques leads to DONeRF,\nour compact dual network design with a depth oracle network as its first step\nand a locally sampled shading network for ray accumulation. With DONeRF, we\nreduce the inference costs by up to 48x compared to NeRF when conditioning on\navailable ground truth depth information. Compared to concurrent acceleration\nmethods for raymarching-based neural representations, DONeRF does not require\nadditional memory for explicit caching or acceleration structures, and can\nrender interactively (20 frames per second) on a single GPU.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:55:09 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:57:56 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 09:56:38 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 09:05:10 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Neff", "Thomas", ""], ["Stadlbauer", "Pascal", ""], ["Parger", "Mathias", ""], ["Kurz", "Andreas", ""], ["Mueller", "Joerg H.", ""], ["Chaitanya", "Chakravarty R. Alla", ""], ["Kaplanyan", "Anton", ""], ["Steinberger", "Markus", ""]]}, {"id": "2103.03240", "submitter": "Kieran Murphy", "authors": "Kieran A. Murphy, Varun Jampani, Srikumar Ramalingam, Ameesh Makadia", "title": "Learn your ABCs: Approximate Bijective Correspondence for isolating\n  factors of variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Representational learning forms the backbone of most deep learning\napplications, and the value of a learned representation is intimately tied to\nits information content regarding different factors of variation. Finding good\nrepresentations depends on the nature of supervision and the learning\nalgorithm. We propose a novel algorithm that relies on a weak form of\nsupervision where the data is partitioned into sets according to certain\ninactive factors of variation. Our key insight is that by seeking approximate\ncorrespondence between elements of different sets, we learn strong\nrepresentations that exclude the inactive factors of variation and isolate the\nactive factors which vary within all sets. We demonstrate that the method can\nwork in a semi-supervised scenario, and that a portion of the unsupervised data\ncan belong to a different domain entirely. Further control over the content of\nthe learned representations is possible by folding in data augmentation to\nsuppress nuisance factors. We outperform competing baselines on the challenging\nproblem of synthetic-to-real object pose transfer.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:58:45 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 15:51:51 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Murphy", "Kieran A.", ""], ["Jampani", "Varun", ""], ["Ramalingam", "Srikumar", ""], ["Makadia", "Ameesh", ""]]}, {"id": "2103.03243", "submitter": "Ji Lin", "authors": "Ji Lin, Richard Zhang, Frieder Ganz, Song Han, Jun-Yan Zhu", "title": "Anycost GANs for Interactive Image Synthesis and Editing", "comments": "Accepted to CVPR 2021. The code and demo are available:\n  https://github.com/mit-han-lab/anycost-gan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have enabled photorealistic image\nsynthesis and editing. However, due to the high computational cost of\nlarge-scale generators (e.g., StyleGAN2), it usually takes seconds to see the\nresults of a single edit on edge devices, prohibiting interactive user\nexperience. In this paper, we take inspirations from modern rendering software\nand propose Anycost GAN for interactive natural image editing. We train the\nAnycost GAN to support elastic resolutions and channels for faster image\ngeneration at versatile speeds. Running subsets of the full generator produce\noutputs that are perceptually similar to the full generator, making them a good\nproxy for preview. By using sampling-based multi-resolution training,\nadaptive-channel training, and a generator-conditioned discriminator, the\nanycost generator can be evaluated at various configurations while achieving\nbetter image quality compared to separately trained models. Furthermore, we\ndevelop new encoder training and latent code optimization techniques to\nencourage consistency between the different sub-generators during image\nprojection. Anycost GAN can be executed at various cost budgets (up to 10x\ncomputation reduction) and adapt to a wide range of hardware and latency\nrequirements. When deployed on desktop CPUs and edge devices, our model can\nprovide perceptually similar previews at 6-12x speedup, enabling interactive\nimage editing. The code and demo are publicly available:\nhttps://github.com/mit-han-lab/anycost-gan.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:59:10 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Lin", "Ji", ""], ["Zhang", "Richard", ""], ["Ganz", "Frieder", ""], ["Han", "Song", ""], ["Zhu", "Jun-Yan", ""]]}, {"id": "2103.03278", "submitter": "Thomas Colligan", "authors": "Thomas Colligan, David Ketchum, Douglas Brinkerhoff, Marco Maneta", "title": "A Deep Learning Approach to Mapping Irrigation: IrrMapper-U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate maps of irrigation are essential for understanding and managing\nwater resources. We present a new method of mapping irrigation and demonstrate\nits accuracy for the state of Montana from years 2000-2019. The method is based\noff of an ensemble of convolutional neural networks that use reflectance\ninformation from Landsat imagery to classify irrigated pixels, that we call\nIrrMapper-U-Net. The methodology does not rely on extensive feature engineering\nand does not condition the classification with land use information from\nexisting geospatial datasets. The ensemble does not need exhaustive\nhyperparameter tuning and the analysis pipeline is lightweight enough to be\nimplemented on a personal computer. Furthermore, the proposed methodology\nprovides an estimate of the uncertainty associated with classification. We\nevaluated our methodology and the resulting irrigation maps using a highly\naccurate novel spatially-explicit ground truth data set, using county-scale\nUSDA surveys of irrigation extent, and using cadastral surveys. We found that\nthat our method outperforms other methods of mapping irrigation in Montana in\nterms of overall accuracy and precision. We found that our method agrees better\nstatewide with the USDA National Agricultural Statistics Survey estimates of\nirrigated area compared to other methods, and has far fewer errors of\ncommission in rainfed agriculture areas. The method learns to mask clouds and\nignore Landsat 7 scan-line failures without supervision, reducing the need for\npreprocessing data. This methodology has the potential to be applied across the\nentire United States and for the complete Landsat record.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 19:27:39 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Colligan", "Thomas", ""], ["Ketchum", "David", ""], ["Brinkerhoff", "Douglas", ""], ["Maneta", "Marco", ""]]}, {"id": "2103.03319", "submitter": "Yasamin Jafarian", "authors": "Yasamin Jafarian, Hyun Soo Park", "title": "Learning High Fidelity Depths of Dressed Humans by Watching Social Media\n  Dance Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge of learning the geometry of dressed humans lies in the\nlimited availability of the ground truth data (e.g., 3D scanned models), which\nresults in the performance degradation of 3D human reconstruction when applying\nto real-world imagery. We address this challenge by leveraging a new data\nresource: a number of social media dance videos that span diverse appearance,\nclothing styles, performances, and identities. Each video depicts dynamic\nmovements of the body and clothes of a single person while lacking the 3D\nground truth geometry. To utilize these videos, we present a new method to use\nthe local transformation that warps the predicted local geometry of the person\nfrom an image to that of another image at a different time instant. This allows\nself-supervision as enforcing a temporal coherence over the predictions. In\naddition, we jointly learn the depth along with the surface normals that are\nhighly responsive to local texture, wrinkle, and shade by maximizing their\ngeometric consistency. Our method is end-to-end trainable, resulting in high\nfidelity depth estimation that predicts fine geometry faithful to the input\nreal image. We demonstrate that our method outperforms the state-of-the-art\nhuman depth estimation and human shape recovery approaches on both real and\nrendered images.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:46:30 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Jafarian", "Yasamin", ""], ["Park", "Hyun Soo", ""]]}, {"id": "2103.03328", "submitter": "Aleksandar Vakanski", "authors": "Aleksandar Vakanski, Min Xian", "title": "Evaluation of Complexity Measures for Deep Learning Generalization in\n  Medical Image Analysis", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalization performance of deep learning models for medical image\nanalysis often decreases on images collected with different devices for data\nacquisition, device settings, or patient population. A better understanding of\nthe generalization capacity on new images is crucial for clinicians'\ntrustworthiness in deep learning. Although significant research efforts have\nbeen recently directed toward establishing generalization bounds and complexity\nmeasures, still, there is often a significant discrepancy between the predicted\nand actual generalization performance. As well, related large empirical studies\nhave been primarily based on validation with general-purpose image datasets.\nThis paper presents an empirical study that investigates the correlation\nbetween 25 complexity measures and the generalization abilities of supervised\ndeep learning classifiers for breast ultrasound images. The results indicate\nthat PAC-Bayes flatness-based and path norm-based measures produce the most\nconsistent explanation for the combination of models and data. We also\ninvestigate the use of multi-task classification and segmentation approach for\nbreast images, and report that such learning approach acts as an implicit\nregularizer and is conducive toward improved generalization.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:58:22 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 02:50:47 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Vakanski", "Aleksandar", ""], ["Xian", "Min", ""]]}, {"id": "2103.03375", "submitter": "Quin Thames", "authors": "Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia, Liviu Panait,\n  Tobias Weyand, Jack Sim", "title": "Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food", "comments": "8 pages, 3 of appendices. CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the nutritional content of food from visual data is a\nchallenging computer vision problem, with the potential to have a positive and\nwidespread impact on public health. Studies in this area are limited to\nexisting datasets in the field that lack sufficient diversity or labels\nrequired for training models with nutritional understanding capability. We\nintroduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes\nwith corresponding video streams, depth images, component weights, and high\naccuracy nutritional content annotation. We demonstrate the potential of this\ndataset by training a computer vision algorithm capable of predicting the\ncaloric and macronutrient values of a complex, real world dish at an accuracy\nthat outperforms professional nutritionists. Further we present a baseline for\nincorporating depth sensor data to improve nutrition predictions. We will\npublicly release Nutrition5k in the hope that it will accelerate innovation in\nthe space of nutritional understanding.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 22:59:22 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 05:05:14 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Thames", "Quin", ""], ["Karpur", "Arjun", ""], ["Norris", "Wade", ""], ["Xia", "Fangting", ""], ["Panait", "Liviu", ""], ["Weyand", "Tobias", ""], ["Sim", "Jack", ""]]}, {"id": "2103.03387", "submitter": "Farzan Erlik Nowruzi", "authors": "Farzan Erlik Nowruzi, Dhanvin Kolhatkar, Prince Kapoor, Elnaz Jahani\n  Heravi, Fahed Al Hassanat, Robert Laganiere, Julien Rebut, Waqas Malik", "title": "PolarNet: Accelerated Deep Open Space Segmentation Using Automotive\n  Radar in Polar Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Camera and Lidar processing have been revolutionized with the rapid\ndevelopment of deep learning model architectures. Automotive radar is one of\nthe crucial elements of automated driver assistance and autonomous driving\nsystems. Radar still relies on traditional signal processing techniques, unlike\ncamera and Lidar based methods. We believe this is the missing link to achieve\nthe most robust perception system. Identifying drivable space and occupied\nspace is the first step in any autonomous decision making task. Occupancy grid\nmap representation of the environment is often used for this purpose. In this\npaper, we propose PolarNet, a deep neural model to process radar information in\npolar domain for open space segmentation. We explore various input-output\nrepresentations. Our experiments show that PolarNet is a effective way to\nprocess radar data that achieves state-of-the-art performance and processing\nspeeds while maintaining a compact size.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 23:58:54 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Nowruzi", "Farzan Erlik", ""], ["Kolhatkar", "Dhanvin", ""], ["Kapoor", "Prince", ""], ["Heravi", "Elnaz Jahani", ""], ["Hassanat", "Fahed Al", ""], ["Laganiere", "Robert", ""], ["Rebut", "Julien", ""], ["Malik", "Waqas", ""]]}, {"id": "2103.03390", "submitter": "Nikola Zubi\\'c", "authors": "Nikola Zubi\\'c, Pietro Li\\`o", "title": "An Effective Loss Function for Generating 3D Models from Single 2D Image\n  without Rendering", "comments": "21 page, 13 figures, 6 tables, to appear as a full paper with oral\n  contribution in AIAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentiable rendering is a very successful technique that applies to a\nSingle-View 3D Reconstruction. Current renderers use losses based on pixels\nbetween a rendered image of some 3D reconstructed object and ground-truth\nimages from given matched viewpoints to optimise parameters of the 3D shape.\n  These models require a rendering step, along with visibility handling and\nevaluation of the shading model. The main goal of this paper is to demonstrate\nthat we can avoid these steps and still get reconstruction results as other\nstate-of-the-art models that are equal or even better than existing\ncategory-specific reconstruction methods. First, we use the same CNN\narchitecture for the prediction of a point cloud shape and pose prediction like\nthe one used by Insafutdinov & Dosovitskiy. Secondly, we propose the novel\neffective loss function that evaluates how well the projections of\nreconstructed 3D point clouds cover the ground truth object's silhouette. Then\nwe use Poisson Surface Reconstruction to transform the reconstructed point\ncloud into a 3D mesh. Finally, we perform a GAN-based texture mapping on a\nparticular 3D mesh and produce a textured 3D mesh from a single 2D image. We\nevaluate our method on different datasets (including ShapeNet, CUB-200-2011,\nand Pascal3D+) and achieve state-of-the-art results, outperforming all the\nother supervised and unsupervised methods and 3D representations, all in terms\nof performance, accuracy, and training time.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 00:02:18 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 09:47:39 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zubi\u0107", "Nikola", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "2103.03394", "submitter": "Farzan Erlik Nowruzi", "authors": "Farzan Erlik Nowruzi, Dhanvin Kolhatkar, Prince Kapoor, Robert\n  Laganiere", "title": "Point Cloud based Hierarchical Deep Odometry Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Processing point clouds using deep neural networks is still a challenging\ntask. Most existing models focus on object detection and registration with deep\nneural networks using point clouds. In this paper, we propose a deep model that\nlearns to estimate odometry in driving scenarios using point cloud data. The\nproposed model consumes raw point clouds in order to extract frame-to-frame\nodometry estimation through a hierarchical model architecture. Also, a local\nbundle adjustment variation of this model using LSTM layers is implemented.\nThese two approaches are comprehensively evaluated and are compared against the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 00:17:58 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Nowruzi", "Farzan Erlik", ""], ["Kolhatkar", "Dhanvin", ""], ["Kapoor", "Prince", ""], ["Laganiere", "Robert", ""]]}, {"id": "2103.03395", "submitter": "Tu-Hoa Pham", "authors": "Tu-Hoa Pham, William Seto, Shreyansh Daftry, Barry Ridge, Johanna\n  Hansen, Tristan Thrush, Mark Van der Merwe, Gerard Maggiolino, Alexander\n  Brinkman, John Mayo, Yang Cheng, Curtis Padgett, Eric Kulczycki, Renaud Detry", "title": "Rover Relocalization for Mars Sample Return by Virtual Template\n  Synthesis and Matching", "comments": "To appear in IEEE Robotics and Automation Letters (RA-L) and IEEE\n  International Conference on Robotics and Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of rover relocalization in the context of the\nnotional Mars Sample Return campaign. In this campaign, a rover (R1) needs to\nbe capable of autonomously navigating and localizing itself within an area of\napproximately 50 x 50 m using reference images collected years earlier by\nanother rover (R0). We propose a visual localizer that exhibits robustness to\nthe relatively barren terrain that we expect to find in relevant areas, and to\nlarge lighting and viewpoint differences between R0 and R1. The localizer\nsynthesizes partial renderings of a mesh built from reference R0 images and\nmatches those to R1 images. We evaluate our method on a dataset totaling 2160\nimages covering the range of expected environmental conditions (terrain,\nlighting, approach angle). Experimental results show the effectiveness of our\napproach. This work informs the Mars Sample Return campaign on the choice of a\nsite where Perseverance (R0) will place a set of sample tubes for future\nretrieval by another rover (R1).\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 00:18:33 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Pham", "Tu-Hoa", ""], ["Seto", "William", ""], ["Daftry", "Shreyansh", ""], ["Ridge", "Barry", ""], ["Hansen", "Johanna", ""], ["Thrush", "Tristan", ""], ["Van der Merwe", "Mark", ""], ["Maggiolino", "Gerard", ""], ["Brinkman", "Alexander", ""], ["Mayo", "John", ""], ["Cheng", "Yang", ""], ["Padgett", "Curtis", ""], ["Kulczycki", "Eric", ""], ["Detry", "Renaud", ""]]}, {"id": "2103.03417", "submitter": "Ken Burke", "authors": "Osman Aka, Ken Burke, Alex B\\\"auerle, Christina Greer, Margaret\n  Mitchell", "title": "Measuring Model Biases in the Absence of Ground Truth", "comments": null, "journal-ref": null, "doi": "10.1145/3461702.3462557", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The measurement of bias in machine learning often focuses on model\nperformance across identity subgroups (such as man and woman) with respect to\ngroundtruth labels. However, these methods do not directly measure the\nassociations that a model may have learned, for example between labels and\nidentity subgroups. Further, measuring a model's bias requires a fully\nannotated evaluation dataset which may not be easily available in practice. We\npresent an elegant mathematical solution that tackles both issues\nsimultaneously, using image classification as a working example. By treating a\nclassification model's predictions for a given image as a set of labels\nanalogous to a bag of words, we rank the biases that a model has learned with\nrespect to different identity labels. We use (man, woman) as a concrete example\nof an identity label set (although this set need not be binary), and present\nrankings for the labels that are most biased towards one identity or the other.\nWe demonstrate how the statistical properties of different association metrics\ncan lead to different rankings of the most \"gender biased\" labels, and conclude\nthat normalized pointwise mutual information (nPMI) is most useful in practice.\nFinally, we announce an open-sourced nPMI visualization tool using TensorBoard.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 01:23:22 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 15:55:18 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 17:09:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Aka", "Osman", ""], ["Burke", "Ken", ""], ["B\u00e4uerle", "Alex", ""], ["Greer", "Christina", ""], ["Mitchell", "Margaret", ""]]}, {"id": "2103.03423", "submitter": "Yu Tian", "authors": "Yu Tian and Guansong Pang and Fengbei Liu and Yuanhong chen and Seon\n  Ho Shin and Johan W. Verjans and Rajvinder Singh and Gustavo Carneiro", "title": "Constrained Contrastive Distribution Learning for Unsupervised Anomaly\n  Detection and Localisation in Medical Images", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised anomaly detection (UAD) learns one-class classifiers exclusively\nwith normal (i.e., healthy) images to detect any abnormal (i.e., unhealthy)\nsamples that do not conform to the expected normal patterns. UAD has two main\nadvantages over its fully supervised counterpart. Firstly, it is able to\ndirectly leverage large datasets available from health screening programs that\ncontain mostly normal image samples, avoiding the costly manual labelling of\nabnormal samples and the subsequent issues involved in training with extremely\nclass-imbalanced data. Further, UAD approaches can potentially detect and\nlocalise any type of lesions that deviate from the normal patterns. One\nsignificant challenge faced by UAD methods is how to learn effective\nlow-dimensional image representations to detect and localise subtle\nabnormalities, generally consisting of small lesions. To address this\nchallenge, we propose a novel self-supervised representation learning method,\ncalled Constrained Contrastive Distribution learning for anomaly detection\n(CCD), which learns fine-grained feature representations by simultaneously\npredicting the distribution of augmented data and image contexts using\ncontrastive learning with pretext constraints. The learned representations can\nbe leveraged to train more anomaly-sensitive detection models. Extensive\nexperiment results show that our method outperforms current state-of-the-art\nUAD approaches on three different colonoscopy and fundus screening datasets.\nOur code is available at https://github.com/tianyu0207/CCD.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 01:56:58 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 12:29:13 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Tian", "Yu", ""], ["Pang", "Guansong", ""], ["Liu", "Fengbei", ""], ["chen", "Yuanhong", ""], ["Shin", "Seon Ho", ""], ["Verjans", "Johan W.", ""], ["Singh", "Rajvinder", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2103.03433", "submitter": "Lei Zhou", "authors": "Yang Liu, Lei Zhou, Xiao Bai, Yifei Huang, Lin Gu, Jun Zhou, Tatsuya\n  Harada", "title": "Goal-Oriented Gaze Estimation for Zero-Shot Learning", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize novel classes by transferring\nsemantic knowledge from seen classes to unseen classes. Since semantic\nknowledge is built on attributes shared between different classes, which are\nhighly local, strong prior for localization of object attribute is beneficial\nfor visual-semantic embedding. Interestingly, when recognizing unseen images,\nhuman would also automatically gaze at regions with certain semantic clue.\nTherefore, we introduce a novel goal-oriented gaze estimation module (GEM) to\nimprove the discriminative attribute localization based on the class-level\nattributes for ZSL. We aim to predict the actual human gaze location to get the\nvisual attention regions for recognizing a novel object guided by attribute\ndescription. Specifically, the task-dependent attention is learned with the\ngoal-oriented GEM, and the global image features are simultaneously optimized\nwith the regression of local attribute features. Experiments on three ZSL\nbenchmarks, i.e., CUB, SUN and AWA2, show the superiority or competitiveness of\nour proposed method against the state-of-the-art ZSL methods. The ablation\nanalysis on real gaze data CUB-VWSW also validates the benefits and accuracy of\nour gaze estimation module. This work implies the promising benefits of\ncollecting human gaze dataset and automatic gaze estimation algorithms on\nhigh-level computer vision tasks. The code is available at\nhttps://github.com/osierboy/GEM-ZSL.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:14:57 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Liu", "Yang", ""], ["Zhou", "Lei", ""], ["Bai", "Xiao", ""], ["Huang", "Yifei", ""], ["Gu", "Lin", ""], ["Zhou", "Jun", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2103.03435", "submitter": "Teppei Suzuki", "authors": "Teppei Suzuki", "title": "Implicit Integration of Superpixel Segmentation into Fully Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Superpixels are a useful representation to reduce the complexity of image\ndata. However, to combine superpixels with convolutional neural networks (CNNs)\nin an end-to-end fashion, one requires extra models to generate superpixels and\nspecial operations such as graph convolution. In this paper, we propose a way\nto implicitly integrate a superpixel scheme into CNNs, which makes it easy to\nuse superpixels with CNNs in an end-to-end fashion. Our proposed method\nhierarchically groups pixels at downsampling layers and generates superpixels.\nOur method can be plugged into many existing architectures without a change in\ntheir feed-forward path because our method does not use superpixels in the\nfeed-forward path but use them to recover the lost resolution instead of\nbilinear upsampling. As a result, our method preserves detailed information\nsuch as object boundaries in the form of superpixels even when the model\ncontains downsampling layers. We evaluate our method on several tasks such as\nsemantic segmentation, superpixel segmentation, and monocular depth estimation,\nand confirm that it speeds up modern architectures and/or improves their\nprediction accuracy in these tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:20:26 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Suzuki", "Teppei", ""]]}, {"id": "2103.03438", "submitter": "Tao Zhang", "authors": "Mengting Xu, Tao Zhang, Zhongnian Li, Mingxia Liu, Daoqiang Zhang", "title": "Towards Evaluating the Robustness of Deep Diagnostic Models by\n  Adversarial Attack", "comments": "This version was accepted in the journal Medical Image Analysis\n  (MedIA)", "journal-ref": "Medical Image Analysis 69 (2021): 101977", "doi": "10.1016/j.media.2021.101977", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models (with neural networks) have been widely used in\nchallenging tasks such as computer-aided disease diagnosis based on medical\nimages. Recent studies have shown deep diagnostic models may not be robust in\nthe inference process and may pose severe security concerns in clinical\npractice. Among all the factors that make the model not robust, the most\nserious one is adversarial examples. The so-called \"adversarial example\" is a\nwell-designed perturbation that is not easily perceived by humans but results\nin a false output of deep diagnostic models with high confidence. In this\npaper, we evaluate the robustness of deep diagnostic models by adversarial\nattack. Specifically, we have performed two types of adversarial attacks to\nthree deep diagnostic models in both single-label and multi-label\nclassification tasks, and found that these models are not reliable when\nattacked by adversarial example. We have further explored how adversarial\nexamples attack the models, by analyzing their quantitative classification\nresults, intermediate features, discriminability of features and correlation of\nestimated labels for both original/clean images and those adversarial ones. We\nhave also designed two new defense methods to handle adversarial examples in\ndeep diagnostic models, i.e., Multi-Perturbations Adversarial Training (MPAdvT)\nand Misclassification-Aware Adversarial Training (MAAdvT). The experimental\nresults have shown that the use of defense methods can significantly improve\nthe robustness of deep diagnostic models against adversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:24:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Xu", "Mengting", ""], ["Zhang", "Tao", ""], ["Li", "Zhongnian", ""], ["Liu", "Mingxia", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "2103.03451", "submitter": "Yuqian Zhou", "authors": "Yuqian Zhou, Hanchao Yu, Humphrey Shi", "title": "Study Group Learning: Improving Retinal Vessel Segmentation Trained with\n  Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retinal vessel segmentation from retinal images is an essential task for\ndeveloping the computer-aided diagnosis system for retinal diseases. Efforts\nhave been made on high-performance deep learning-based approaches to segment\nthe retinal images in an end-to-end manner. However, the acquisition of retinal\nvessel images and segmentation labels requires onerous work from professional\nclinicians, which results in smaller training dataset with incomplete labels.\nAs known, data-driven methods suffer from data insufficiency, and the models\nwill easily over-fit the small-scale training data. Such a situation becomes\nmore severe when the training vessel labels are incomplete or incorrect. In\nthis paper, we propose a Study Group Learning (SGL) scheme to improve the\nrobustness of the model trained on noisy labels. Besides, a learned enhancement\nmap provides better visualization than conventional methods as an auxiliary\ntool for clinicians. Experiments demonstrate that the proposed method further\nimproves the vessel segmentation performance in DRIVE and CHASE$\\_$DB1\ndatasets, especially when the training labels are noisy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:09:51 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhou", "Yuqian", ""], ["Yu", "Hanchao", ""], ["Shi", "Humphrey", ""]]}, {"id": "2103.03454", "submitter": "Wenguan Wang", "authors": "Hanqing Wang, Wenguan Wang, Wei Liang, Caiming Xiong, Jianbing Shen", "title": "Structured Scene Memory for Vision-Language Navigation", "comments": "Accepted on CVPR2021; Implementation will be available at\n  https://github.com/HanqingWangAI/SSM-VLN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, numerous algorithms have been developed to tackle the problem of\nvision-language navigation (VLN), i.e., entailing an agent to navigate 3D\nenvironments through following linguistic instructions. However, current VLN\nagents simply store their past experiences/observations as latent states in\nrecurrent networks, failing to capture environment layouts and make long-term\nplanning. To address these limitations, we propose a crucial architecture,\ncalled Structured Scene Memory (SSM). It is compartmentalized enough to\naccurately memorize the percepts during navigation. It also serves as a\nstructured scene representation, which captures and disentangles visual and\ngeometric cues in the environment. SSM has a collect-read controller that\nadaptively collects information for supporting current decision making and\nmimics iterative algorithms for long-range reasoning. As SSM provides a\ncomplete action space, i.e., all the navigable places on the map, a\nfrontier-exploration based navigation decision making strategy is introduced to\nenable efficient and global planning. Experiment results on two VLN datasets\n(i.e., R2R and R4R) show that our method achieves state-of-the-art performance\non several metrics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:41:00 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wang", "Hanqing", ""], ["Wang", "Wenguan", ""], ["Liang", "Wei", ""], ["Xiong", "Caiming", ""], ["Shen", "Jianbing", ""]]}, {"id": "2103.03460", "submitter": "Hui Tang", "authors": "Hui Tang and Kui Jia", "title": "Vicinal and categorical domain adaptation", "comments": "Accepted by Pattern Recognition", "journal-ref": "Pattern Recognition, Volume 115, July 2021, 107907", "doi": "10.1016/j.patcog.2021.107907", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to learn a task classifier that performs\nwell on the unlabeled target domain, by utilizing the labeled source domain.\nInspiring results have been acquired by learning domain-invariant deep features\nvia domain-adversarial training. However, its parallel design of task and\ndomain classifiers limits the ability to achieve a finer category-level domain\nalignment. To promote categorical domain adaptation (CatDA), based on a joint\ncategory-domain classifier, we propose novel losses of adversarial training at\nboth domain and category levels. Since the joint classifier can be regarded as\na concatenation of individual task classifiers respectively for the two\ndomains, our design principle is to enforce consistency of category predictions\nbetween the two task classifiers. Moreover, we propose a concept of vicinal\ndomains whose instances are produced by a convex combination of pairs of\ninstances respectively from the two domains. Intuitively, alignment of the\npossibly infinite number of vicinal domains enhances that of original domains.\nWe propose novel adversarial losses for vicinal domain adaptation (VicDA) based\non CatDA, leading to Vicinal and Categorical Domain Adaptation (ViCatDA). We\nalso propose Target Discriminative Structure Recovery (TDSR) to recover the\nintrinsic target discrimination damaged by adversarial feature alignment. We\nalso analyze the principles underlying the ability of our key designs to align\nthe joint distributions. Extensive experiments on several benchmark datasets\ndemonstrate that we achieve the new state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:47:24 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Tang", "Hui", ""], ["Jia", "Kui", ""]]}, {"id": "2103.03465", "submitter": "Xiaohang Yang", "authors": "Xiaohang Yang, Lingtong Kong, Jie Yang", "title": "Unsupervised Motion Representation Enhanced Network for Action\n  Recognition", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning reliable motion representation between consecutive frames, such as\noptical flow, has proven to have great promotion to video understanding.\nHowever, the TV-L1 method, an effective optical flow solver, is time-consuming\nand expensive in storage for caching the extracted optical flow. To fill the\ngap, we propose UF-TSN, a novel end-to-end action recognition approach enhanced\nwith an embedded lightweight unsupervised optical flow estimator. UF-TSN\nestimates motion cues from adjacent frames in a coarse-to-fine manner and\nfocuses on small displacement for each level by extracting pyramid of feature\nand warping one to the other according to the estimated flow of the last level.\nDue to the lack of labeled motion for action datasets, we constrain the flow\nprediction with multi-scale photometric consistency and edge-aware smoothness.\nCompared with state-of-the-art unsupervised motion representation learning\nmethods, our model achieves better accuracy while maintaining efficiency, which\nis competitive with some supervised or more complicated approaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 04:14:32 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Yang", "Xiaohang", ""], ["Kong", "Lingtong", ""], ["Yang", "Jie", ""]]}, {"id": "2103.03467", "submitter": "Qing Jin", "authors": "Qing Jin, Jian Ren, Oliver J. Woodford, Jiazhuo Wang, Geng Yuan,\n  Yanzhi Wang, Sergey Tulyakov", "title": "Teachers Do More Than Teach: Compressing Image-to-Image Models", "comments": "18 pages, 10 figures, accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have achieved huge success in\ngenerating high-fidelity images, however, they suffer from low efficiency due\nto tremendous computational cost and bulky memory usage. Recent efforts on\ncompression GANs show noticeable progress in obtaining smaller generators by\nsacrificing image quality or involving a time-consuming searching process. In\nthis work, we aim to address these issues by introducing a teacher network that\nprovides a search space in which efficient network architectures can be found,\nin addition to performing knowledge distillation. First, we revisit the search\nspace of generative models, introducing an inception-based residual block into\ngenerators. Second, to achieve target computation cost, we propose a one-step\npruning algorithm that searches a student architecture from the teacher model\nand substantially reduces searching cost. It requires no l1 sparsity\nregularization and its associated hyper-parameters, simplifying the training\nprocedure. Finally, we propose to distill knowledge through maximizing feature\nsimilarity between teacher and student via an index named Global Kernel\nAlignment (GKA). Our compressed networks achieve similar or even better image\nfidelity (FID, mIoU) than the original models with much-reduced computational\ncost, e.g., MACs. Code will be released at\nhttps://github.com/snap-research/CAT.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 04:29:34 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Jin", "Qing", ""], ["Ren", "Jian", ""], ["Woodford", "Oliver J.", ""], ["Wang", "Jiazhuo", ""], ["Yuan", "Geng", ""], ["Wang", "Yanzhi", ""], ["Tulyakov", "Sergey", ""]]}, {"id": "2103.03480", "submitter": "Dingfu Zhou", "authors": "Dingfu Zhou, Xibin Song, Yuchao Dai, Junbo Yin, Feixiang Lu, Jin Fang,\n  Miao Liao and Liangjun Zhang", "title": "IAFA: Instance-aware Feature Aggregation for 3D Object Detection from a\n  Single Image", "comments": "Accepted by ACCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D object detection from a single image is an important task in Autonomous\nDriving (AD), where various approaches have been proposed. However, the task is\nintrinsically ambiguous and challenging as single image depth estimation is\nalready an ill-posed problem. In this paper, we propose an instance-aware\napproach to aggregate useful information for improving the accuracy of 3D\nobject detection with the following contributions. First, an instance-aware\nfeature aggregation (IAFA) module is proposed to collect local and global\nfeatures for 3D bounding boxes regression. Second, we empirically find that the\nspatial attention module can be well learned by taking coarse-level instance\nannotations as a supervision signal. The proposed module has significantly\nboosted the performance of the baseline method on both 3D detection and 2D\nbird-eye's view of vehicle detection among all three categories. Third, our\nproposed method outperforms all single image-based approaches (even these\nmethods trained with depth as auxiliary inputs) and achieves state-of-the-art\n3D detection performance on the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 05:47:52 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhou", "Dingfu", ""], ["Song", "Xibin", ""], ["Dai", "Yuchao", ""], ["Yin", "Junbo", ""], ["Lu", "Feixiang", ""], ["Fang", "Jin", ""], ["Liao", "Miao", ""], ["Zhang", "Liangjun", ""]]}, {"id": "2103.03483", "submitter": "Md Mohaimenuzzaman", "authors": "Md Mohaimenuzzaman, Christoph Bergmeir, Ian Thomas West and Bernd\n  Meyer", "title": "Environmental Sound Classification on the Edge: A Pipeline for Deep\n  Acoustic Networks on Extremely Resource-Constrained Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Significant efforts are being invested to bring state-of-the-art\nclassification and recognition to edge devices with extreme resource\nconstraints (memory, speed and lack of GPU support). Here, we demonstrate the\nfirst deep network for acoustic recognition that is small enough for an\noff-the-shelf microcrocontroller, yet achieves state-of-the-art performance on\nstandard benchmarks. Rather than handcrafting a once-off solution, we present a\nuniversal pipeline that converts a large deep convolutional network\nautomatically via compression and quantization into a network for\nresource-impoverished edge devices. After introducing ACDNet, which produces\nabove state-of-the-art accuracy on ESC-10 (96.65%) and ESC-50 (87.1%), we\ndescribe the compression pipeline and show that it allows us to achieve 97.22%\nsize reduction and 97.28% FLOP reduction while maintaining close to\nstate-of-the-art accuracy (83.65% on ESC-50). We describe a successful\nimplementation on a standard off-the-shelf microcontroller and, beyond\nlaboratory benchmarks, report successful tests on real-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 05:52:31 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 00:07:25 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 05:06:47 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mohaimenuzzaman", "Md", ""], ["Bergmeir", "Christoph", ""], ["West", "Ian Thomas", ""], ["Meyer", "Bernd", ""]]}, {"id": "2103.03493", "submitter": "Xu Yang", "authors": "Xu Yang, Hanwang Zhang, Guojun Qi, Jianfei Cai", "title": "Causal Attention for Vision-Language Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a novel attention mechanism: Causal Attention (CATT), to remove\nthe ever-elusive confounding effect in existing attention-based vision-language\nmodels. This effect causes harmful bias that misleads the attention module to\nfocus on the spurious correlations in training data, damaging the model\ngeneralization. As the confounder is unobserved in general, we use the\nfront-door adjustment to realize the causal intervention, which does not\nrequire any knowledge on the confounder. Specifically, CATT is implemented as a\ncombination of 1) In-Sample Attention (IS-ATT) and 2) Cross-Sample Attention\n(CS-ATT), where the latter forcibly brings other samples into every IS-ATT,\nmimicking the causal intervention. CATT abides by the Q-K-V convention and\nhence can replace any attention module such as top-down attention and\nself-attention in Transformers. CATT improves various popular attention-based\nvision-language models by considerable margins. In particular, we show that\nCATT has great potential in large-scale pre-training, e.g., it can promote the\nlighter LXMERT~\\cite{tan2019lxmert}, which uses fewer data and less\ncomputational power, comparable to the heavier UNITER~\\cite{chen2020uniter}.\nCode is published in \\url{https://github.com/yangxuntu/catt}.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 06:38:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Yang", "Xu", ""], ["Zhang", "Hanwang", ""], ["Qi", "Guojun", ""], ["Cai", "Jianfei", ""]]}, {"id": "2103.03501", "submitter": "Giang Truong", "authors": "Giang Truong, Huu Le, David Suter, Erchuan Zhang, Syed Zulqarnain\n  Gilani", "title": "Unsupervised Learning for Robust Fitting:A Reinforcement Learning\n  Approach", "comments": "The preprint of paper accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust model fitting is a core algorithm in a large number of computer vision\napplications. Solving this problem efficiently for datasets highly contaminated\nwith outliers is, however, still challenging due to the underlying\ncomputational complexity. Recent literature has focused on learning-based\nalgorithms. However, most approaches are supervised which require a large\namount of labelled training data. In this paper, we introduce a novel\nunsupervised learning framework that learns to directly solve robust model\nfitting. Unlike other methods, our work is agnostic to the underlying input\nfeatures, and can be easily generalized to a wide variety of LP-type problems\nwith quasi-convex residuals. We empirically show that our method outperforms\nexisting unsupervised learning approaches, and achieves competitive results\ncompared to traditional methods on several important computer vision problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 07:14:00 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Truong", "Giang", ""], ["Le", "Huu", ""], ["Suter", "David", ""], ["Zhang", "Erchuan", ""], ["Gilani", "Syed Zulqarnain", ""]]}, {"id": "2103.03503", "submitter": "Syed Safwan Khalid", "authors": "Syed Safwan Khalid, Muhammad Awais, Chi-Ho Chan, Zhenhua Feng, Ammarah\n  Farooq, Ali Akbari and Josef Kittler", "title": "NPT-Loss: A Metric Loss with Implicit Mining for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Face recognition (FR) using deep convolutional neural networks (DCNNs) has\nseen remarkable success in recent years. One key ingredient of DCNN-based FR is\nthe appropriate design of a loss function that ensures discrimination between\nvarious identities. The state-of-the-art (SOTA) solutions utilise normalised\nSoftmax loss with additive and/or multiplicative margins. Despite being\npopular, these Softmax+margin based losses are not theoretically motivated and\nthe effectiveness of a margin is justified only intuitively. In this work, we\nutilise an alternative framework that offers a more direct mechanism of\nachieving discrimination among the features of various identities. We propose a\nnovel loss that is equivalent to a triplet loss with proxies and an implicit\nmechanism of hard-negative mining. We give theoretical justification that\nminimising the proposed loss ensures a minimum separability between all\nidentities. The proposed loss is simple to implement and does not require heavy\nhyper-parameter tuning as in the SOTA solutions. We give empirical evidence\nthat despite its simplicity, the proposed loss consistently achieves SOTA\nperformance in various benchmarks for both high-resolution and low-resolution\nFR tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 07:26:40 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Khalid", "Syed Safwan", ""], ["Awais", "Muhammad", ""], ["Chan", "Chi-Ho", ""], ["Feng", "Zhenhua", ""], ["Farooq", "Ammarah", ""], ["Akbari", "Ali", ""], ["Kittler", "Josef", ""]]}, {"id": "2103.03510", "submitter": "Dan Xu", "authors": "Guanglei Yang, Paolo Rota, Xavier Alameda-Pineda, Dan Xu, Mingli Ding,\n  Elisa Ricci", "title": "Variational Structured Attention Networks for Deep Visual Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have enabled major progress in addressing\npixel-level prediction tasks such as semantic segmentation, depth estimation,\nsurface normal prediction, and so on, benefiting from their powerful\ncapabilities in visual representation learning. Typically, state-of-the-art\nmodels integrates attention mechanisms for improved deep feature\nrepresentations. Recently, some works have demonstrated the significance of\nlearning and combining both spatial- and channel-wise attentions for deep\nfeature refinement. In this paper, we aim at effectively boosting previous\napproaches and propose a unified deep framework to jointly learn both spatial\nattention maps and channel attention vectors in a principled manner so as to\nstructure the resulting attention tensors and model interactions between these\ntwo types of attentions. Specifically, we integrate the estimation and the\ninteraction of the attentions within a probabilistic representation learning\nframework, leading to Variational STructured Attention networks (VISTA-Net). We\nimplement the inference rules within the neural network, thus allowing for\nend-to-end learning of the probabilistic and the CNN front-end parameters. As\ndemonstrated by our extensive empirical evaluation on six large-scale datasets\nfor dense visual prediction, VISTA-Net outperforms the state-of-the-art in\nmultiple continuous and discrete prediction tasks, thus confirming the benefit\nof the proposed approach in joint structured spatial-channel attention\nestimation for deep representation learning. The code is available at\nhttps://github.com/ygjwd12345/VISTA-Net.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 07:37:24 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Yang", "Guanglei", ""], ["Rota", "Paolo", ""], ["Alameda-Pineda", "Xavier", ""], ["Xu", "Dan", ""], ["Ding", "Mingli", ""], ["Ricci", "Elisa", ""]]}, {"id": "2103.03516", "submitter": "Evangelos Kazakos", "authors": "Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen", "title": "Slow-Fast Auditory Streams For Audio Recognition", "comments": "Accepted for presentation at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a two-stream convolutional network for audio recognition, that\noperates on time-frequency spectrogram inputs. Following similar success in\nvisual recognition, we learn Slow-Fast auditory streams with separable\nconvolutions and multi-level lateral connections. The Slow pathway has high\nchannel capacity while the Fast pathway operates at a fine-grained temporal\nresolution. We showcase the importance of our two-stream proposal on two\ndiverse datasets: VGG-Sound and EPIC-KITCHENS-100, and achieve state-of-the-art\nresults on both.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 07:51:21 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kazakos", "Evangelos", ""], ["Nagrani", "Arsha", ""], ["Zisserman", "Andrew", ""], ["Damen", "Dima", ""]]}, {"id": "2103.03518", "submitter": "Julen Balzategui", "authors": "Julen Balzategui, Luka Eciolaza, and Daniel Maestro-Watson", "title": "Anomaly detection and automatic labeling for solar cell quality\n  inspection based on Generative Adversarial Network", "comments": "20 pages, 10 figures, 6 tables. This article is part of the special\n  issue \"Condition Monitoring, Field Inspection and Fault Diagnostic Methods\n  for Photovoltaic Systems\" Published in MDPI - Sensors: see\n  https://www.mdpi.com/journal/sensors/special_issues/Condition_Monitoring_Field_Inspection_and_Fault_Diagnostic_Methods_for_Photovoltaic_Systems", "journal-ref": "Sensors 2021, volume 21, issue 13, article-number 4361", "doi": "10.3390/s21134361", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quality inspection applications in industry are required to move towards a\nzero-defect manufacturing scenario, withnon-destructive inspection and\ntraceability of 100 % of produced parts. Developing robust fault detection and\nclassification modelsfrom the start-up of the lines is challenging due to the\ndifficulty in getting enough representative samples of the faulty patternsand\nthe need to manually label them. This work presents a methodology to develop a\nrobust inspection system, targeting thesepeculiarities, in the context of solar\ncell manufacturing. The methodology is divided into two phases: In the first\nphase, an anomalydetection model based on a Generative Adversarial Network\n(GAN) is employed. This model enables the detection and localizationof\nanomalous patterns within the solar cells from the beginning, using only\nnon-defective samples for training and without anymanual labeling involved. In\na second stage, as defective samples arise, the detected anomalies will be used\nas automaticallygenerated annotations for the supervised training of a Fully\nConvolutional Network that is capable of detecting multiple types offaults. The\nexperimental results using 1873 EL images of monocrystalline cells show that\n(a) the anomaly detection scheme can beused to start detecting features with\nvery little available data, (b) the anomaly detection may serve as automatic\nlabeling in order totrain a supervised model, and (c) segmentation and\nclassification results of supervised models trained with automatic labels\narecomparable to the ones obtained from the models trained with manual labels.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 07:53:59 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 08:08:20 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Balzategui", "Julen", ""], ["Eciolaza", "Luka", ""], ["Maestro-Watson", "Daniel", ""]]}, {"id": "2103.03571", "submitter": "Hong Liu", "authors": "Hong Liu and Jianmin Wang and Mingsheng Long", "title": "Cycle Self-Training for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mainstream approaches for unsupervised domain adaptation (UDA) learn\ndomain-invariant representations to bridge domain gap. More recently,\nself-training has been gaining momentum in UDA. Originated from semi-supervised\nlearning, self-training uses unlabeled data efficiently by training on\npseudo-labels. However, as corroborated in this work, under distributional\nshift in UDA, the pseudo-labels can be unreliable in terms of their large\ndiscrepancy from the ground truth labels. Thereby, we propose Cycle\nSelf-Training (CST), a principled self-training algorithm that enforces\npseudo-labels to generalize across domains. In the forward step, CST generates\ntarget pseudo-labels with a source-trained classifier. In the reverse step, CST\ntrains a target classifier using target pseudo-labels, and then updates the\nshared representations to make the target classifier perform well on the source\ndata. We introduce the Tsallis entropy, a novel regularization to improve the\nquality of target pseudo-labels. On quadratic neural networks, we prove that\nCST recovers target ground truth, while both invariant feature learning and\nvanilla self-training fail. Empirical results indicate that CST significantly\nimproves over prior state-of-the-arts in standard UDA benchmarks across visual\nrecognition and sentiment analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:04:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Liu", "Hong", ""], ["Wang", "Jianmin", ""], ["Long", "Mingsheng", ""]]}, {"id": "2103.03602", "submitter": "Ahmed Rasheed", "authors": "Ahmed Rasheed, Muhammad Shahzad Younis, Junaid Qadir and Muhammad\n  Bilal", "title": "Use of Transfer Learning and Wavelet Transform for Breast Cancer\n  Detection", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is one of the most common cause of deaths among women.\nMammography is a widely used imaging modality that can be used for cancer\ndetection in its early stages. Deep learning is widely used for the detection\nof cancerous masses in the images obtained via mammography. The need to improve\naccuracy remains constant due to the sensitive nature of the datasets so we\nintroduce segmentation and wavelet transform to enhance the important features\nin the image scans. Our proposed system aids the radiologist in the screening\nphase of cancer detection by using a combination of segmentation and wavelet\ntransforms as pre-processing augmentation that leads to transfer learning in\nneural networks. The proposed system with these pre-processing techniques\nsignificantly increases the accuracy of detection on Mini-MIAS.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:08:56 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Rasheed", "Ahmed", ""], ["Younis", "Muhammad Shahzad", ""], ["Qadir", "Junaid", ""], ["Bilal", "Muhammad", ""]]}, {"id": "2103.03604", "submitter": "Yan Wang", "authors": "Boxiang Yun, Yan Wang, Jieneng Chen, Huiyu Wang, Wei Shen, Qingli Li", "title": "SpecTr: Spectral Transformer for Hyperspectral Pathology Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging (HSI) unlocks the huge potential to a wide variety of\napplications relied on high-precision pathology image segmentation, such as\ncomputational pathology and precision medicine. Since hyperspectral pathology\nimages benefit from the rich and detailed spectral information even beyond the\nvisible spectrum, the key to achieve high-precision hyperspectral pathology\nimage segmentation is to felicitously model the context along high-dimensional\nspectral bands. Inspired by the strong context modeling ability of\ntransformers, we hereby, for the first time, formulate the contextual feature\nlearning across spectral bands for hyperspectral pathology image segmentation\nas a sequence-to-sequence prediction procedure by transformers. To assist\nspectral context learning procedure, we introduce two important strategies: (1)\na sparsity scheme enforces the learned contextual relationship to be sparse, so\nas to eliminates the distraction from the redundant bands; (2) a spectral\nnormalization, a separate group normalization for each spectral band, mitigates\nthe nuisance caused by heterogeneous underlying distributions of bands. We name\nour method Spectral Transformer (SpecTr), which enjoys two benefits: (1) it has\na strong ability to model long-range dependency among spectral bands, and (2)\nit jointly explores the spatial-spectral features of HSI. Experiments show that\nSpecTr outperforms other competing methods in a hyperspectral pathology image\nsegmentation benchmark without the need of pre-training. Code is available at\nhttps://github.com/hfut-xc-yun/SpecTr.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:12:22 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Yun", "Boxiang", ""], ["Wang", "Yan", ""], ["Chen", "Jieneng", ""], ["Wang", "Huiyu", ""], ["Shen", "Wei", ""], ["Li", "Qingli", ""]]}, {"id": "2103.03614", "submitter": "Christoph Sch\\\"oller", "authors": "Christoph Sch\\\"oller, Alois Knoll", "title": "FloMo: Tractable Motion Prediction with Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future motion of traffic participants is inherently uncertain. To plan\nsafely, therefore, an autonomous agent must take into account multiple possible\noutcomes and prioritize them. Recently, this problem has been addressed with\ngenerative neural networks. However, most generative models either do not learn\nthe true underlying trajectory distribution reliably, or do not allow\nlikelihoods to be associated with predictions. In our work, we model motion\nprediction directly as a density estimation problem with a normalizing flow\nbetween a noise sample and the future motion distribution. Our model, named\nFloMo, allows likelihoods to be computed in a single network pass and can be\ntrained directly with maximum likelihood estimation. Furthermore, we propose a\nmethod to stabilize training flows on trajectory datasets and a new data\naugmentation transformation that improves the performance and generalization of\nour model. Our method achieves state-of-the-art performance on three popular\nprediction datasets, with a significant gap to most competing models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:35:27 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Sch\u00f6ller", "Christoph", ""], ["Knoll", "Alois", ""]]}, {"id": "2103.03629", "submitter": "Fengbei Liu", "authors": "Fengbei Liu, Yu Tian, Filipe R. Cordeiro, Vasileios Belagiannis, Ian\n  Reid, Gustavo Carneiro", "title": "Self-supervised Mean Teacher for Semi-supervised Chest X-ray\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The training of deep learning models generally requires a large amount of\nannotated data for effective convergence and generalisation. However, obtaining\nhigh-quality annotations is a laboursome and expensive process due to the need\nof expert radiologists for the labelling task. The study of semi-supervised\nlearning in medical image analysis is then of crucial importance given that it\nis much less expensive to obtain unlabelled images than to acquire images\nlabelled by expert radiologists.Essentially, semi-supervised methods leverage\nlarge sets of unlabelled data to enable better training convergence and\ngeneralisation than if we use only the small set of labelled images.In this\npaper, we propose the Self-supervised Mean Teacher for Semi-supervised\n(S$^2$MTS$^2$) learning that combines self-supervised mean-teacher pre-training\nwith semi-supervised fine-tuning. The main innovation of S$^2$MTS$^2$ is the\nself-supervised mean-teacher pre-training based on the joint contrastive\nlearning, which uses an infinite number of pairs of positive query and key\nfeatures to improve the mean-teacher representation. The model is then\nfine-tuned using the exponential moving average teacher framework trained with\nsemi-supervised learning.We validate S$^2$MTS$^2$ on the thorax disease\nmulti-label classification problem from the dataset Chest X-ray14, where we\nshow that it outperforms the previous SOTA semi-supervised learning methods by\na large margin.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 12:25:36 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Liu", "Fengbei", ""], ["Tian", "Yu", ""], ["Cordeiro", "Filipe R.", ""], ["Belagiannis", "Vasileios", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2103.03636", "submitter": "Peijun Tang", "authors": "Lili Pan, Peijun Tang, Zhiyong Chen, Zenglin Xu", "title": "Contrastive Disentanglement in Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentanglement is defined as the problem of learninga representation that\ncan separate the distinct, informativefactors of variations of data. Learning\nsuch a representa-tion may be critical for developing explainable and\nhuman-controllable Deep Generative Models (DGMs) in artificialintelligence.\nHowever, disentanglement in GANs is not a triv-ial task, as the absence of\nsample likelihood and posteriorinference for latent variables seems to prohibit\nthe forwardstep. Inspired by contrastive learning (CL), this paper, froma new\nperspective, proposes contrastive disentanglement ingenerative adversarial\nnetworks (CD-GAN). It aims at dis-entangling the factors of inter-class\nvariation of visual datathrough contrasting image features, since the same\nfactorvalues produce images in the same class. More importantly,we probe a\nnovel way to make use of limited amount ofsupervision to the largest extent, to\npromote inter-class dis-entanglement performance. Extensive experimental\nresultson many well-known datasets demonstrate the efficacy ofCD-GAN for\ndisentangling inter-class variation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 12:44:22 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Pan", "Lili", ""], ["Tang", "Peijun", ""], ["Chen", "Zhiyong", ""], ["Xu", "Zenglin", ""]]}, {"id": "2103.03651", "submitter": "Biao Gao", "authors": "Biao Gao, Shaochi Hu, Xijun Zhao, Huijing Zhao", "title": "Fine-Grained Off-Road Semantic Segmentation and Mapping via Contrastive\n  Learning", "comments": "Video: https://youtu.be/YY1hp07XQ0g", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road detection or traversability analysis has been a key technique for a\nmobile robot to traverse complex off-road scenes. The problem has been mainly\nformulated in early works as a binary classification one, e.g. associating\npixels with road or non-road labels. Whereas understanding scenes with\nfine-grained labels are needed for off-road robots, as scenes are very diverse,\nand the various mechanical performance of off-road robots may lead to different\ndefinitions of safe regions to traverse. How to define and annotate\nfine-grained labels to achieve meaningful scene understanding for a robot to\ntraverse off-road is still an open question. This research proposes a\ncontrastive learning based method. With a set of human-annotated anchor\npatches, a feature representation is learned to discriminate regions with\ndifferent traversability, a method of fine-grained semantic segmentation and\nmapping is subsequently developed for off-road scene understanding. Experiments\nare conducted on a dataset of three driving segments that represent very\ndiverse off-road scenes. An anchor accuracy of 89.8% is achieved by evaluating\nthe matching with human-annotated image patches in cross-scene validation.\nExamined by associated 3D LiDAR data, the fine-grained segments of visual\nimages are demonstrated to have different levels of toughness and terrain\nelevation, which represents their semantical meaningfulness. The resultant maps\ncontain both fine-grained labels and confidence values, providing rich\ninformation to support a robot traversing complex off-road scenes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:23:24 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Gao", "Biao", ""], ["Hu", "Shaochi", ""], ["Zhao", "Xijun", ""], ["Zhao", "Huijing", ""]]}, {"id": "2103.03653", "submitter": "Maciej Besta", "authors": "Maciej Besta, Zur Vonarburg-Shmaria, Yannick Schaffner, Leonardo\n  Schwarz, Grzegorz Kwasniewski, Lukas Gianinazzi, Jakub Beranek, Kacper Janda,\n  Tobias Holenstein, Sebastian Leisinger, Peter Tatkowski, Esref Ozdemir,\n  Adrian Balla, Marcin Copik, Philipp Lindenberger, Pavel Kalvoda, Marek\n  Konieczny, Onur Mutlu, Torsten Hoefler", "title": "GraphMineSuite: Enabling High-Performance and Programmable Graph Mining\n  Algorithms with Set Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.DS cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose GraphMineSuite (GMS): the first benchmarking suite for graph\nmining that facilitates evaluating and constructing high-performance graph\nmining algorithms. First, GMS comes with a benchmark specification based on\nextensive literature review, prescribing representative problems, algorithms,\nand datasets. Second, GMS offers a carefully designed software platform for\nseamless testing of different fine-grained elements of graph mining algorithms,\nsuch as graph representations or algorithm subroutines. The platform includes\nparallel implementations of more than 40 considered baselines, and it\nfacilitates developing complex and fast mining algorithms. High modularity is\npossible by harnessing set algebra operations such as set intersection and\ndifference, which enables breaking complex graph mining algorithms into simple\nbuilding blocks that can be separately experimented with. GMS is supported with\na broad concurrency analysis for portability in performance insights, and a\nnovel performance metric to assess the throughput of graph mining algorithms,\nenabling more insightful evaluation. As use cases, we harness GMS to rapidly\nredesign and accelerate state-of-the-art baselines of core graph mining\nproblems: degeneracy reordering (by up to >2x), maximal clique listing (by up\nto >9x), k-clique listing (by 1.1x), and subgraph isomorphism (by up to 2.5x),\nalso obtaining better theoretical performance bounds.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:26:18 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Besta", "Maciej", ""], ["Vonarburg-Shmaria", "Zur", ""], ["Schaffner", "Yannick", ""], ["Schwarz", "Leonardo", ""], ["Kwasniewski", "Grzegorz", ""], ["Gianinazzi", "Lukas", ""], ["Beranek", "Jakub", ""], ["Janda", "Kacper", ""], ["Holenstein", "Tobias", ""], ["Leisinger", "Sebastian", ""], ["Tatkowski", "Peter", ""], ["Ozdemir", "Esref", ""], ["Balla", "Adrian", ""], ["Copik", "Marcin", ""], ["Lindenberger", "Philipp", ""], ["Kalvoda", "Pavel", ""], ["Konieczny", "Marek", ""], ["Mutlu", "Onur", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2103.03654", "submitter": "Christian Rathgeb", "authors": "Christian Rathgeb, Kevin Bernardo, Nathania E. Haryanto, Christoph\n  Busch", "title": "Effects of Image Compression on Face Image Manipulation Detection: A\n  Case Study on Facial Retouching", "comments": "to appear in IET Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past years, numerous methods have been introduced to reliably detect\ndigital face image manipulations. Lately, the generalizability of these schemes\nhas been questioned in particular with respect to image post-processing. Image\ncompression represents a post-processing which is frequently applied in diverse\nbiometric application scenarios. Severe compression might erase digital traces\nof face image manipulation and hence hamper a reliable detection thereof. In\nthis work, the effects of image compression on face image manipulation\ndetection are analyzed. In particular, a case study on facial retouching\ndetection under the influence of image compression is presented. To this end,\nICAO-compliant subsets of two public face databases are used to automatically\ncreate a database containing more than 9,000 retouched reference images\ntogether with unconstrained probe images. Subsequently, reference images are\ncompressed applying JPEG and JPEG 2000 at compression levels recommended for\nface image storage in electronic travel documents. Novel detection algorithms\nutilizing texture descriptors and deep face representations are proposed and\nevaluated in a single image and differential scenario. Results obtained from\nchallenging cross-database experiments in which the analyzed retouching\ntechnique is unknown during training yield interesting findings: (1) most\ncompetitive detection performance is achieved for differential scenarios\nemploying deep face representations; (2) image compression severely impacts the\nperformance of face image manipulation detection schemes based on texture\ndescriptors while methods utilizing deep face representations are found to be\nhighly robust; (3) in some cases, the application of image compression might as\nwell improve detection performance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:28:28 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Rathgeb", "Christian", ""], ["Bernardo", "Kevin", ""], ["Haryanto", "Nathania E.", ""], ["Busch", "Christoph", ""]]}, {"id": "2103.03663", "submitter": "Renat Bashirov", "authors": "Renat Bashirov, Anastasia Ianina, Karim Iskakov, Yevgeniy Kononenko,\n  Valeriya Strizhkova, Victor Lempitsky, Alexander Vakhitov", "title": "Real-time RGBD-based Extended Body Pose Estimation", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a system for real-time RGBD-based estimation of 3D human pose. We\nuse parametric 3D deformable human mesh model (SMPL-X) as a representation and\nfocus on the real-time estimation of parameters for the body pose, hands pose\nand facial expression from Kinect Azure RGB-D camera. We train estimators of\nbody pose and facial expression parameters. Both estimators use previously\npublished landmark extractors as input and custom annotated datasets for\nsupervision, while hand pose is estimated directly by a previously published\nmethod. We combine the predictions of those estimators into a temporally-smooth\nhuman pose. We train the facial expression extractor on a large talking face\ndataset, which we annotate with facial expression parameters. For the body pose\nwe collect and annotate a dataset of 56 people captured from a rig of 5 Kinect\nAzure RGB-D cameras and use it together with a large motion capture AMASS\ndataset. Our RGB-D body pose model outperforms the state-of-the-art RGB-only\nmethods and works on the same level of accuracy compared to a slower RGB-D\noptimization-based solution. The combined system runs at 30 FPS on a server\nwith a single GPU. The code will be available at\nhttps://saic-violet.github.io/rgbd-kinect-pose\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:37:50 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Bashirov", "Renat", ""], ["Ianina", "Anastasia", ""], ["Iskakov", "Karim", ""], ["Kononenko", "Yevgeniy", ""], ["Strizhkova", "Valeriya", ""], ["Lempitsky", "Victor", ""], ["Vakhitov", "Alexander", ""]]}, {"id": "2103.03664", "submitter": "Raunak Dey", "authors": "Raunak Dey and Yi Hong", "title": "ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly\n  Segmentation", "comments": "Accepted for MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural network framework, utilizing adversarial learning to\npartition an image into two cuts, with one cut falling into a reference\ndistribution provided by the user. This concept tackles the task of\nunsupervised anomaly segmentation, which has attracted increasing attention in\nrecent years due to their broad applications in tasks with unlabelled data.\nThis Adversarial-based Selective Cutting network (ASC-Net) bridges the two\ndomains of cluster-based deep learning methods and adversarial-based\nanomaly/novelty detection algorithms. We evaluate this unsupervised learning\nmodel on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and\nMS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN\nfamily, our model demonstrates tremendous performance gains in unsupervised\nanomaly segmentation tasks. Although there is still room to further improve\nperformance compared to supervised learning algorithms, the promising\nexperimental results shed light on building an unsupervised learning algorithm\nusing user-defined knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:38:24 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 16:01:37 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Dey", "Raunak", ""], ["Hong", "Yi", ""]]}, {"id": "2103.03678", "submitter": "Florian Heidecker", "authors": "Florian Heidecker, Jasmin Breitenstein, Kevin R\\\"osch, Jonas\n  L\\\"ohdefink, Maarten Bieshaar, Christoph Stiller, Tim Fingscheidt, Bernhard\n  Sick", "title": "An Application-Driven Conceptualization of Corner Cases for Perception\n  in Highly Automated Driving", "comments": "This paper is submitted to IEEE Intelligent Vehicles Symposium 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems and functions that rely on machine learning (ML) are the basis of\nhighly automated driving. An essential task of such ML models is to reliably\ndetect and interpret unusual, new, and potentially dangerous situations. The\ndetection of those situations, which we refer to as corner cases, is highly\nrelevant for successfully developing, applying, and validating automotive\nperception functions in future vehicles where multiple sensor modalities will\nbe used. A complication for the development of corner case detectors is the\nlack of consistent definitions, terms, and corner case descriptions, especially\nwhen taking into account various automotive sensors. In this work, we provide\nan application-driven view of corner cases in highly automated driving. To\nachieve this goal, we first consider existing definitions from the general\noutlier, novelty, anomaly, and out-of-distribution detection to show relations\nand differences to corner cases. Moreover, we extend an existing camera-focused\nsystematization of corner cases by adding RADAR (radio detection and ranging)\nand LiDAR (light detection and ranging) sensors. For this, we describe an\nexemplary toolchain for data acquisition and processing, highlighting the\ninterfaces of the corner case detection. We also define a novel level of corner\ncases, the method layer corner cases, which appear due to uncertainty inherent\nin the methodology or the data distribution.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:56:37 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Heidecker", "Florian", ""], ["Breitenstein", "Jasmin", ""], ["R\u00f6sch", "Kevin", ""], ["L\u00f6hdefink", "Jonas", ""], ["Bieshaar", "Maarten", ""], ["Stiller", "Christoph", ""], ["Fingscheidt", "Tim", ""], ["Sick", "Bernhard", ""]]}, {"id": "2103.03692", "submitter": "Pawel Drozdowski", "authors": "Pawel Drozdowski, Fabian Stockhardt, Christian Rathgeb, Christoph\n  Busch", "title": "Signal-level Fusion for Indexing and Retrieval of Facial Biometric Data", "comments": "12 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing scope, scale, and number of biometric deployments around the\nworld emphasise the need for research into technologies facilitating efficient\nand reliable biometric identification queries. This work presents a method of\nindexing biometric databases, which relies on signal-level fusion of facial\nimages (morphing) to create a multi-stage data-structure and retrieval\nprotocol. By successively pre-filtering the list of potential candidate\nidentities, the proposed method makes it possible to reduce the necessary\nnumber of biometric template comparisons to complete a biometric identification\ntransaction. The proposed method is extensively evaluated on publicly available\ndatabases using open-source and commercial off-the-shelf recognition systems.\nThe results show that using the proposed method, the computational workload can\nbe reduced down to around 30%, while the biometric performance of a baseline\nexhaustive search-based retrieval is fully maintained, both in closed-set and\nopen-set identification scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:06:54 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Drozdowski", "Pawel", ""], ["Stockhardt", "Fabian", ""], ["Rathgeb", "Christian", ""], ["Busch", "Christoph", ""]]}, {"id": "2103.03703", "submitter": "Tariq Bdair", "authors": "Tariq Bdair, Nassir Navab and Shadi Albarqouni", "title": "Peer Learning for Skin Lesion Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Skin cancer is one of the most deadly cancers worldwide. Yet, it can be\nreduced by early detection. Recent deep-learning methods have shown a\ndermatologist-level performance in skin cancer classification. Yet, this\nsuccess demands a large amount of centralized data, which is oftentimes not\navailable. Federated learning has been recently introduced to train machine\nlearning models in a privacy-preserved distributed fashion demanding annotated\ndata at the clients, which is usually expensive and not available, especially\nin the medical field. To this end, we propose FedPerl, a semi-supervised\nfederated learning method that utilizes peer learning from social sciences and\nensemble averaging from committee machines to build communities and encourage\nits members to learn from each other such that they produce more accurate\npseudo labels. We also propose the peer anonymization (PA) technique as a core\ncomponent of FedPerl. PA preserves privacy and reduces the communication cost\nwhile maintaining the performance without additional complexity. We validated\nour method on 38,000 skin lesion images collected from 4 publicly available\ndatasets. FedPerl achieves superior performance over the baselines and\nstate-of-the-art SSFL by 15.8%, and 1.8% respectively. Further, FedPerl shows\nless sensitivity to noisy clients.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:26:15 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 10:25:30 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bdair", "Tariq", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "2103.03705", "submitter": "Cosmin I. Bercea", "authors": "Cosmin I. Bercea, Benedikt Wiestler, Daniel Rueckert and Shadi\n  Albarqouni", "title": "FedDis: Disentangled Federated Learning for Unsupervised Brain Pathology\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, data-driven machine learning (ML) methods have\nrevolutionized the computer vision community by providing novel efficient\nsolutions to many unsolved (medical) image analysis problems. However, due to\nthe increasing privacy concerns and data fragmentation on many different sites,\nexisting medical data are not fully utilized, thus limiting the potential of\nML. Federated learning (FL) enables multiple parties to collaboratively train a\nML model without exchanging local data. However, data heterogeneity (non-IID)\namong the distributed clients is yet a challenge. To this end, we propose a\nnovel federated method, denoted Federated Disentanglement (FedDis), to\ndisentangle the parameter space into shape and appearance, and only share the\nshape parameter with the clients. FedDis is based on the assumption that the\nanatomical structure in brain MRI images is similar across multiple\ninstitutions, and sharing the shape knowledge would be beneficial in anomaly\ndetection. In this paper, we leverage healthy brain scans of 623 subjects from\nmultiple sites with real data (OASIS, ADNI) in a privacy-preserving fashion to\nlearn a model of normal anatomy, that allows to segment abnormal structures. We\ndemonstrate a superior performance of FedDis on real pathological databases\ncontaining 109 subjects; two publicly available MS Lesions (MSLUB, MSISBI), and\nan in-house database with MS and Glioblastoma (MSI and GBI). FedDis achieved an\naverage dice performance of 0.38, outperforming the state-of-the-art (SOTA)\nauto-encoder by 42% and the SOTA federated method by 11%. Further, we\nillustrate that FedDis learns a shape embedding that is orthogonal to the\nappearance and consistent under different intensity augmentations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:29:52 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Bercea", "Cosmin I.", ""], ["Wiestler", "Benedikt", ""], ["Rueckert", "Daniel", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "2103.03717", "submitter": "Flavio de Barros Vidal", "authors": "Andre da Silva Abade, Lucas Faria Porto, Paulo Afonso Ferreira, Flavio\n  de Barros Vidal", "title": "NemaNet: A convolutional neural network model for identification of\n  nematodes soybean crop in brazil", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phytoparasitic nematodes (or phytonematodes) are causing severe damage to\ncrops and generating large-scale economic losses worldwide. In soybean crops,\nannual losses are estimated at 10.6% of world production. Besides, identifying\nthese species through microscopic analysis by an expert with taxonomy knowledge\nis often laborious, time-consuming, and susceptible to failure. In this\nperspective, robust and automatic approaches are necessary for identifying\nphytonematodes capable of providing correct diagnoses for the classification of\nspecies and subsidizing the taking of all control and prevention measures. This\nwork presents a new public data set called NemaDataset containing 3,063\nmicroscopic images from five nematode species with the most significant damage\nrelevance for the soybean crop. Additionally, we propose a new Convolutional\nNeural Network (CNN) model defined as NemaNet and a comparative assessment with\nthirteen popular models of CNNs, all of them representing the state of the art\nclassification and recognition. The general average calculated for each model,\non a from-scratch training, the NemaNet model reached 96.99% accuracy, while\nthe best evaluation fold reached 98.03%. In training with transfer learning,\nthe average accuracy reached 98.88\\%. The best evaluation fold reached 99.34%\nand achieve an overall accuracy improvement over 6.83% and 4.1%, for\nfrom-scratch and transfer learning training, respectively, when compared to\nother popular models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:47:00 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Abade", "Andre da Silva", ""], ["Porto", "Lucas Faria", ""], ["Ferreira", "Paulo Afonso", ""], ["Vidal", "Flavio de Barros", ""]]}, {"id": "2103.03759", "submitter": "Jean Le'Clerc Arrastia", "authors": "Jean Le'Clerc Arrastia, Nick Heilenk\\\"otter, Daniel Otero Baguer, Lena\n  Hauberg-Lotte, Tobias Boskamp, Sonja Hetzer, Nicole Duschner, J\\\"org\n  Schaller, and Peter Maa{\\ss}", "title": "Deeply supervised UNet for semantic segmentation to assist\n  dermatopathological assessment of Basal Cell Carcinoma (BCC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate and fast assessment of resection margins is an essential part of a\ndermatopathologist's clinical routine. In this work, we successfully develop a\ndeep learning method to assist the pathologists by marking critical regions\nthat have a high probability of exhibiting pathological features in Whole Slide\nImages (WSI). We focus on detecting Basal Cell Carcinoma (BCC) through semantic\nsegmentation using several models based on the UNet architecture. The study\nincludes 650 WSI with 3443 tissue sections in total. Two clinical\ndermatopathologists annotated the data, marking tumor tissues' exact location\non 100 WSI. The rest of the data, with ground-truth section-wise labels, is\nused to further validate and test the models. We analyze two different encoders\nfor the first part of the UNet network and two additional training strategies:\na) deep supervision, b) linear combination of decoder outputs, and obtain some\ninterpretations about what the network's decoder does in each case. The best\nmodel achieves over 96%, accuracy, sensitivity, and specificity on the test\nset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:39:55 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 21:56:14 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Arrastia", "Jean Le'Clerc", ""], ["Heilenk\u00f6tter", "Nick", ""], ["Baguer", "Daniel Otero", ""], ["Hauberg-Lotte", "Lena", ""], ["Boskamp", "Tobias", ""], ["Hetzer", "Sonja", ""], ["Duschner", "Nicole", ""], ["Schaller", "J\u00f6rg", ""], ["Maa\u00df", "Peter", ""]]}, {"id": "2103.03761", "submitter": "Ananya Jana", "authors": "Ananya Jana, Hui Qu, Carlos D. Minacapelli, Carolyn Catalano, Vinod\n  Rustgi, Dimitris Metaxas", "title": "Liver Fibrosis and NAS scoring from CT images using self-supervised\n  learning and texture encoding", "comments": "5 pages, 2 figures, accepted at ISBI 2021, code at this URL:\n  https://github.com/ananyajana/fibrosis_code", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-alcoholic fatty liver disease (NAFLD) is one of the most common causes of\nchronic liver diseases (CLD) which can progress to liver cancer. The severity\nand treatment of NAFLD is determined by NAFLD Activity Scores (NAS)and liver\nfibrosis stage, which are usually obtained from liver biopsy. However, biopsy\nis invasive in nature and involves risk of procedural complications. Current\nmethods to predict the fibrosis and NAS scores from noninvasive CT images rely\nheavily on either a large annotated dataset or transfer learning using\npretrained networks. However, the availability of a large annotated dataset\ncannot be always ensured andthere can be domain shifts when using transfer\nlearning. In this work, we propose a self-supervised learning method to address\nboth problems. As the NAFLD causes changes in the liver texture, we also\npropose to use texture encoded inputs to improve the performance of the model.\nGiven a relatively small dataset with 30 patients, we employ a self-supervised\nnetwork which achieves better performance than a network trained via transfer\nlearning. The code is publicly available at\nhttps://github.com/ananyajana/fibrosis_code.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:40:55 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 14:56:02 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Jana", "Ananya", ""], ["Qu", "Hui", ""], ["Minacapelli", "Carlos D.", ""], ["Catalano", "Carolyn", ""], ["Rustgi", "Vinod", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2103.03764", "submitter": "Arniel Labrada", "authors": "Arniel Labrada, Benjamin Bustos, Ivan Sipiran", "title": "A Convolutional Architecture for 3D Model Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last years, many advances have been made in tasks like3D model\nretrieval, 3D model classification, and 3D model segmentation.The typical 3D\nrepresentations such as point clouds, voxels, and poly-gon meshes are mostly\nsuitable for rendering purposes, while their use forcognitive processes\n(retrieval, classification, segmentation) is limited dueto their high\nredundancy and complexity. We propose a deep learningarchitecture to handle 3D\nmodels as an input. We combine this architec-ture with other standard\narchitectures like Convolutional Neural Networksand autoencoders for computing\n3D model embeddings. Our goal is torepresent a 3D model as a vector with enough\ninformation to substitutethe 3D model for high-level tasks. Since this vector\nis a learned repre-sentation which tries to capture the relevant information of\na 3D model,we show that the embedding representation conveys semantic\ninformationthat helps to deal with the similarity assessment of 3D objects. Our\nex-periments show the benefit of computing the embeddings of a 3D modeldata set\nand use them for effective 3D Model Retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:46:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Labrada", "Arniel", ""], ["Bustos", "Benjamin", ""], ["Sipiran", "Ivan", ""]]}, {"id": "2103.03768", "submitter": "Attila Lengyel", "authors": "Robert-Jan Bruintjes, Attila Lengyel, Marcos Baptista Rios, Osman\n  Semih Kayhan, Jan van Gemert", "title": "VIPriors 1: Visual Inductive Priors for Data-Efficient Deep Learning\n  Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present the first edition of \"VIPriors: Visual Inductive Priors for\nData-Efficient Deep Learning\" challenges. We offer four data-impaired\nchallenges, where models are trained from scratch, and we reduce the number of\ntraining samples to a fraction of the full set. Furthermore, to encourage data\nefficient solutions, we prohibited the use of pre-trained models and other\ntransfer learning techniques. The majority of top ranking solutions make heavy\nuse of data augmentation, model ensembling, and novel and efficient network\narchitectures to achieve significant performance increases compared to the\nprovided baselines.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:58:17 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Bruintjes", "Robert-Jan", ""], ["Lengyel", "Attila", ""], ["Rios", "Marcos Baptista", ""], ["Kayhan", "Osman Semih", ""], ["van Gemert", "Jan", ""]]}, {"id": "2103.03781", "submitter": "Behzad Bozorgtabar", "authors": "Devavrat Tomar, Manana Lortkipanidze, Guillaume Vray, Behzad\n  Bozorgtabar, Jean-Philippe Thiran", "title": "Self-Attentive Spatial Adaptive Normalization for Cross-Modality Domain\n  Adaptation", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging\n  (IEEE TMI)", "journal-ref": null, "doi": "10.1109/TMI.2021.3059265", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite the successes of deep neural networks on many challenging vision\ntasks, they often fail to generalize to new test domains that are not\ndistributed identically to the training data. The domain adaptation becomes\nmore challenging for cross-modality medical data with a notable domain shift.\nGiven that specific annotated imaging modalities may not be accessible nor\ncomplete. Our proposed solution is based on the cross-modality synthesis of\nmedical images to reduce the costly annotation burden by radiologists and\nbridge the domain gap in radiological images. We present a novel approach for\nimage-to-image translation in medical images, capable of supervised or\nunsupervised (unpaired image data) setups. Built upon adversarial training, we\npropose a learnable self-attentive spatial normalization of the deep\nconvolutional generator network's intermediate activations. Unlike previous\nattention-based image-to-image translation approaches, which are either\ndomain-specific or require distortion of the source domain's structures, we\nunearth the importance of the auxiliary semantic information to handle the\ngeometric changes and preserve anatomical structures during image translation.\nWe achieve superior results for cross-modality segmentation between unpaired\nMRI and CT data for multi-modality whole heart and multi-modal brain tumor MRI\n(T1/T2) datasets compared to the state-of-the-art methods. We also observe\nencouraging results in cross-modality conversion for paired MRI and CT images\non a brain dataset. Furthermore, a detailed analysis of the cross-modality\nimage translation, thorough ablation studies confirm our proposed method's\nefficacy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 16:22:31 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Tomar", "Devavrat", ""], ["Lortkipanidze", "Manana", ""], ["Vray", "Guillaume", ""], ["Bozorgtabar", "Behzad", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "2103.03821", "submitter": "Viktor Varga", "authors": "Viktor Varga, Andr\\'as L\\H{o}rincz", "title": "Fast Interactive Video Object Segmentation with Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pixelwise annotation of image sequences can be very tedious for humans.\nInteractive video object segmentation aims to utilize automatic methods to\nspeed up the process and reduce the workload of the annotators. Most\ncontemporary approaches rely on deep convolutional networks to collect and\nprocess information from human annotations throughout the video. However, such\nnetworks contain millions of parameters and need huge amounts of labeled\ntraining data to avoid overfitting. Beyond that, label propagation is usually\nexecuted as a series of frame-by-frame inference steps, which is difficult to\nbe parallelized and is thus time consuming. In this paper we present a graph\nneural network based approach for tackling the problem of interactive video\nobject segmentation. Our network operates on superpixel-graphs which allow us\nto reduce the dimensionality of the problem by several magnitudes. We show,\nthat our network possessing only a few thousand parameters is able to achieve\nstate-of-the-art performance, while inference remains fast and can be trained\nquickly with very little data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:37:12 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 14:51:10 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Varga", "Viktor", ""], ["L\u0151rincz", "Andr\u00e1s", ""]]}, {"id": "2103.03827", "submitter": "Mathieu Labb\\'e", "authors": "Mathieu Labb\\'e and Fran\\c{c}ois Michaud", "title": "Multi-Session Visual SLAM for Illumination Invariant Localization in\n  Indoor Environments", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots navigating using only a camera, illumination changes in indoor\nenvironments can cause localization failures during autonomous navigation. In\nthis paper, we present a multi-session visual SLAM approach to create a map\nmade of multiple variations of the same locations in different illumination\nconditions. The multi-session map can then be used at any hour of the day for\nimproved localization capability. The approach presented is independent of the\nvisual features used, and this is demonstrated by comparing localization\nperformance between multi-session maps created using the RTAB-Map library with\nSURF, SIFT, BRIEF, FREAK, BRISK, KAZE, DAISY and SuperPoint visual features.\nThe approach is tested on six mapping and six localization sessions recorded at\n30 minutes intervals during sunset using a Google Tango phone in a real\napartment.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:41:27 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Labb\u00e9", "Mathieu", ""], ["Michaud", "Fran\u00e7ois", ""]]}, {"id": "2103.03840", "submitter": "Jiahong Ouyang", "authors": "Jiahong Ouyang and Qingyu Zhao and Ehsan Adeli and Edith V Sullivan\n  and Adolf Pfefferbaum and Greg Zaharchuk and Kilian M Pohl", "title": "Self-Supervised Longitudinal Neighbourhood Embedding", "comments": "Provisional Accepted by Medical Image Computing and Computer Assisted\n  Intervention (MICCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal MRIs are often used to capture the gradual deterioration of\nbrain structure and function caused by aging or neurological diseases.\nAnalyzing this data via machine learning generally requires a large number of\nground-truth labels, which are often missing or expensive to obtain. Reducing\nthe need for labels, we propose a self-supervised strategy for representation\nlearning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts\nin contrastive learning, LNE explicitly models the similarity between\ntrajectory vectors across different subjects. We do so by building a graph in\neach training iteration defining neighborhoods in the latent space so that the\nprogression direction of a subject follows the direction of its neighbors. This\nresults in a smooth trajectory field that captures the global morphological\nchange of the brain while maintaining the local continuity. We apply LNE to\nlongitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274\nhealthy subjects, and Alzheimer's Disease Neuroimaging Initiative (ADNI,\nN=632). The visualization of the smooth trajectory vector field and superior\nperformance on downstream tasks demonstrate the strength of the proposed method\nover existing self-supervised methods in extracting information associated with\nnormal aging and in revealing the impact of neurodegenerative disorders. The\ncode is available at\n\\url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding.git}.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:55:53 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 02:44:14 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 18:22:39 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ouyang", "Jiahong", ""], ["Zhao", "Qingyu", ""], ["Adeli", "Ehsan", ""], ["Sullivan", "Edith V", ""], ["Pfefferbaum", "Adolf", ""], ["Zaharchuk", "Greg", ""], ["Pohl", "Kilian M", ""]]}, {"id": "2103.03841", "submitter": "Charlie Nash", "authors": "Charlie Nash, Jacob Menick, Sander Dieleman, Peter W. Battaglia", "title": "Generating Images with Sparse Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high dimensionality of images presents architecture and\nsampling-efficiency challenges for likelihood-based generative models. Previous\napproaches such as VQ-VAE use deep autoencoders to obtain compact\nrepresentations, which are more practical as inputs for likelihood-based\nmodels. We present an alternative approach, inspired by common image\ncompression methods like JPEG, and convert images to quantized discrete cosine\ntransform (DCT) blocks, which are represented sparsely as a sequence of DCT\nchannel, spatial location, and DCT coefficient triples. We propose a\nTransformer-based autoregressive architecture, which is trained to sequentially\npredict the conditional distribution of the next element in such sequences, and\nwhich scales effectively to high resolution images. On a range of image\ndatasets, we demonstrate that our approach can generate high quality, diverse\nimages, with sample metric scores competitive with state of the art methods. We\nadditionally show that simple modifications to our method yield effective image\ncolorization and super-resolution models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:56:03 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Nash", "Charlie", ""], ["Menick", "Jacob", ""], ["Dieleman", "Sander", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "2103.03862", "submitter": "Jacob Whitehill", "authors": "Anand Ramakrishnan, Minh Pham, and Jacob Whitehill", "title": "Harnessing Geometric Constraints from Emotion Labels to improve Face\n  Verification", "comments": "8 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For the task of face verification, we explore the utility of harnessing\nauxiliary facial emotion labels to impose explicit geometric constraints on the\nembedding space when training deep embedding models. We introduce several novel\nloss functions that, in conjunction with a standard Triplet Loss [43], or\nArcFace loss [10], provide geometric constraints on the embedding space; the\nlabels for our loss functions can be provided using either manually annotated\nor automatically detected auxiliary emotion labels. Our method is implemented\npurely in terms of the loss function and does not require any changes to the\nneural network backbone of the embedding function.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:27:38 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 14:17:43 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 15:45:31 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ramakrishnan", "Anand", ""], ["Pham", "Minh", ""], ["Whitehill", "Jacob", ""]]}, {"id": "2103.03873", "submitter": "Tobias Czempiel", "authors": "Tobias Czempiel, Magdalini Paschali, Daniel Ostler, Seong Tae Kim,\n  Benjamin Busam, Nassir Navab", "title": "OperA: Attention-Regularized Transformers for Surgical Phase Recognition", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce OperA, a transformer-based model that accurately\npredicts surgical phases from long video sequences. A novel attention\nregularization loss encourages the model to focus on high-quality frames during\ntraining. Moreover, the attention weights are utilized to identify\ncharacteristic high attention frames for each surgical phase, which could\nfurther be used for surgery summarization. OperA is thoroughly evaluated on two\ndatasets of laparoscopic cholecystectomy videos, outperforming various\nstate-of-the-art temporal refinement approaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:59:14 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Czempiel", "Tobias", ""], ["Paschali", "Magdalini", ""], ["Ostler", "Daniel", ""], ["Kim", "Seong Tae", ""], ["Busam", "Benjamin", ""], ["Navab", "Nassir", ""]]}, {"id": "2103.03877", "submitter": "Aydogan Ozcan", "authors": "Yijie Zhang, Tairan Liu, Manmohan Singh, Yilin Luo, Yair Rivenson,\n  Kirill V. Larin, and Aydogan Ozcan", "title": "Neural network-based image reconstruction in swept-source optical\n  coherence tomography using undersampled spectral data", "comments": "20 Pages, 7 Figures, 1 Table", "journal-ref": "Light: Science & Applications (2021)", "doi": "10.1038/s41377-021-00594-7", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) is a widely used non-invasive biomedical\nimaging modality that can rapidly provide volumetric images of samples. Here,\nwe present a deep learning-based image reconstruction framework that can\ngenerate swept-source OCT (SS-OCT) images using undersampled spectral data,\nwithout any spatial aliasing artifacts. This neural network-based image\nreconstruction does not require any hardware changes to the optical set-up and\ncan be easily integrated with existing swept-source or spectral domain OCT\nsystems to reduce the amount of raw spectral data to be acquired. To show the\nefficacy of this framework, we trained and blindly tested a deep neural network\nusing mouse embryo samples imaged by an SS-OCT system. Using 2-fold\nundersampled spectral data (i.e., 640 spectral points per A-line), the trained\nneural network can blindly reconstruct 512 A-lines in ~6.73 ms using a desktop\ncomputer, removing spatial aliasing artifacts due to spectral undersampling,\nalso presenting a very good match to the images of the same samples,\nreconstructed using the full spectral OCT data (i.e., 1280 spectral points per\nA-line). We also successfully demonstrate that this framework can be further\nextended to process 3x undersampled spectral data per A-line, with some\nperformance degradation in the reconstructed image quality compared to 2x\nspectral undersampling. This deep learning-enabled image reconstruction\napproach can be broadly used in various forms of spectral domain OCT systems,\nhelping to increase their imaging speed without sacrificing image resolution\nand signal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 22:30:31 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhang", "Yijie", ""], ["Liu", "Tairan", ""], ["Singh", "Manmohan", ""], ["Luo", "Yilin", ""], ["Rivenson", "Yair", ""], ["Larin", "Kirill V.", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2103.03891", "submitter": "Brendan Duke", "authors": "Rohit Saha and Brendan Duke and Florian Shkurti and Graham W. Taylor\n  and Parham Aarabi", "title": "LOHO: Latent Optimization of Hairstyles via Orthogonalization", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hairstyle transfer is challenging due to hair structure differences in the\nsource and target hair. Therefore, we propose Latent Optimization of Hairstyles\nvia Orthogonalization (LOHO), an optimization-based approach using GAN\ninversion to infill missing hair structure details in latent space during\nhairstyle transfer. Our approach decomposes hair into three attributes:\nperceptual structure, appearance, and style, and includes tailored losses to\nmodel each of these attributes independently. Furthermore, we propose two-stage\noptimization and gradient orthogonalization to enable disentangled latent space\noptimization of our hair attributes. Using LOHO for latent space manipulation,\nusers can synthesize novel photorealistic images by manipulating hair\nattributes either individually or jointly, transferring the desired attributes\nfrom reference hairstyles. LOHO achieves a superior FID compared with the\ncurrent state-of-the-art (SOTA) for hairstyle transfer. Additionally, LOHO\npreserves the subject's identity comparably well according to PSNR and SSIM\nwhen compared to SOTA image embedding pipelines. Code is available at\nhttps://github.com/dukebw/LOHO.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 19:00:33 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 17:04:09 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Saha", "Rohit", ""], ["Duke", "Brendan", ""], ["Shkurti", "Florian", ""], ["Taylor", "Graham W.", ""], ["Aarabi", "Parham", ""]]}, {"id": "2103.03905", "submitter": "Jason Ramapuram", "authors": "Jason Ramapuram, Yan Wu, Alexandros Kalousis", "title": "Kanerva++: extending The Kanerva Machine with differentiable, locally\n  block allocated latent memory", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic and semantic memory are critical components of the human memory\nmodel. The theory of complementary learning systems (McClelland et al., 1995)\nsuggests that the compressed representation produced by a serial event\n(episodic memory) is later restructured to build a more generalized form of\nreusable knowledge (semantic memory). In this work we develop a new principled\nBayesian memory allocation scheme that bridges the gap between episodic and\nsemantic memory via a hierarchical latent variable model. We take inspiration\nfrom traditional heap allocation and extend the idea of locally contiguous\nmemory to the Kanerva Machine, enabling a novel differentiable block allocated\nlatent memory. In contrast to the Kanerva Machine, we simplify the process of\nmemory writing by treating it as a fully feed forward deterministic process,\nrelying on the stochasticity of the read key distribution to disperse\ninformation within the memory. We demonstrate that this allocation scheme\nimproves performance in memory conditional image generation, resulting in new\nstate-of-the-art conditional likelihood values on binarized MNIST (<=41.58\nnats/image) , binarized Omniglot (<=66.24 nats/image), as well as presenting\ncompetitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32x32.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:40:40 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 09:38:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ramapuram", "Jason", ""], ["Wu", "Yan", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "2103.03922", "submitter": "Zhengyu Huang", "authors": "Zhengyu Huang, Theodore B. Norris, Panqu Wang", "title": "ES-Net: An Efficient Stereo Matching Network", "comments": "Submitted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense stereo matching with deep neural networks is of great interest to the\nresearch community. Existing stereo matching networks typically use slow and\ncomputationally expensive 3D convolutions to improve the performance, which is\nnot friendly to real-world applications such as autonomous driving. In this\npaper, we propose the Efficient Stereo Network (ESNet), which achieves high\nperformance and efficient inference at the same time. ESNet relies only on 2D\nconvolution and computes multi-scale cost volume efficiently using a\nwarping-based method to improve the performance in regions with fine-details.\nIn addition, we address the matching ambiguity issue in the occluded region by\nproposing ESNet-M, a variant of ESNet that additionally estimates an occlusion\nmask without supervision. We further improve the network performance by\nproposing a new training scheme that includes dataset scheduling and\nunsupervised pre-training. Compared with other low-cost dense stereo depth\nestimation methods, our proposed approach achieves state-of-the-art performance\non the Scene Flow [1], DrivingStereo [2], and KITTI-2015 dataset [3]. Our code\nwill be made available.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 20:11:39 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Huang", "Zhengyu", ""], ["Norris", "Theodore B.", ""], ["Wang", "Panqu", ""]]}, {"id": "2103.03927", "submitter": "Efthymios Georgiou", "authors": "Efthymios Georgiou, Athanasios Katsamanis", "title": "AudioVisual Speech Synthesis: A brief literature review", "comments": "review is written in Greek", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This brief literature review studies the problem of audiovisual speech\nsynthesis, which is the problem of generating an animated talking head given a\ntext as input. Due to the high complexity of this problem, we approach it as\nthe composition of two problems. Specifically, that of Text-to-Speech (TTS)\nsynthesis as well as the voice-driven talking head animation. For TTS, we\npresent models that are used to map text to intermediate acoustic\nrepresentations, e.g. mel-spectrograms, as well as models that generate voice\nsignals conditioned on these intermediate representations, i.e vocoders. For\nthe talking-head animation problem, we categorize approaches based on whether\nthey produce human faces or anthropomorphic figures. An attempt is also made to\ndiscuss the importance of the choice of facial models in the second case.\nThroughout the review, we briefly describe the most important work in\naudiovisual speech synthesis, trying to highlight the advantages and\ndisadvantages of the various approaches.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 19:13:48 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Georgiou", "Efthymios", ""], ["Katsamanis", "Athanasios", ""]]}, {"id": "2103.03931", "submitter": "Xiaohang Fu", "authors": "Xiaohang Fu, Lei Bi, Ashnil Kumar, Michael Fulham, and Jinman Kim", "title": "Attention-Enhanced Cross-Task Network for Analysing Multiple Attributes\n  of Lung Nodules in CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate characterisation of visual attributes such as spiculation,\nlobulation, and calcification of lung nodules is critical in cancer management.\nThe characterisation of these attributes is often subjective, which may lead to\nhigh inter- and intra-observer variability. Furthermore, lung nodules are often\nheterogeneous in the cross-sectional image slices of a 3D volume. Current\nstate-of-the-art methods that score multiple attributes rely on deep\nlearning-based multi-task learning (MTL) schemes. These methods, however,\nextract shared visual features across attributes and then examine each\nattribute without explicitly leveraging their inherent intercorrelations.\nFurthermore, current methods either treat each slice with equal importance\nwithout considering their relevance or heterogeneity, which limits performance.\nIn this study, we address these challenges with a new convolutional neural\nnetwork (CNN)-based MTL model that incorporates multiple attention-based\nlearning modules to simultaneously score 9 visual attributes of lung nodules in\ncomputed tomography (CT) image volumes. Our model processes entire nodule\nvolumes of arbitrary depth and uses a slice attention module to filter out\nirrelevant slices. We also introduce cross-attribute and attribute\nspecialisation attention modules that learn an optimal amalgamation of\nmeaningful representations to leverage relationships between attributes. We\ndemonstrate that our model outperforms previous state-of-the-art methods at\nscoring attributes using the well-known public LIDC-IDRI dataset of pulmonary\nnodules from over 1,000 patients. Our model also performs competitively when\nrepurposed for benign-malignant classification. Our attention modules also\nprovide easy-to-interpret weights that offer insights into the predictions of\nthe model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 20:33:12 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 07:14:37 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Fu", "Xiaohang", ""], ["Bi", "Lei", ""], ["Kumar", "Ashnil", ""], ["Fulham", "Michael", ""], ["Kim", "Jinman", ""]]}, {"id": "2103.03934", "submitter": "Henrique Siqueira", "authors": "Henrique Siqueira, Pablo Barros, Sven Magg and Stefan Wermter", "title": "An Ensemble with Shared Representations Based on Convolutional Networks\n  for Continually Learning Facial Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social robots able to continually learn facial expressions could\nprogressively improve their emotion recognition capability towards people\ninteracting with them. Semi-supervised learning through ensemble predictions is\nan efficient strategy to leverage the high exposure of unlabelled facial\nexpressions during human-robot interactions. Traditional ensemble-based\nsystems, however, are composed of several independent classifiers leading to a\nhigh degree of redundancy, and unnecessary allocation of computational\nresources. In this paper, we proposed an ensemble based on convolutional\nnetworks where the early layers are strong low-level feature extractors, and\ntheir representations shared with an ensemble of convolutional branches. This\nresults in a significant drop in redundancy of low-level features processing.\nTraining in a semi-supervised setting, we show that our approach is able to\ncontinually learn facial expressions through ensemble predictions using\nunlabelled samples from different data distributions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 20:40:52 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Siqueira", "Henrique", ""], ["Barros", "Pablo", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "2103.03935", "submitter": "Marlon Marcon", "authors": "Andr\\'e Roberto Ortoncelli and Marlon Marcon and Franciele Beal", "title": "An automated approach to mitigate transcription errors in braille texts\n  for the Portuguese language", "comments": "Accepted on COTB'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The quota system in Brazil made it possible to include blind students in\nhigher education. Teachers' lack of knowledge about the braille system can\nrepresent a barrier between them and students who use it for writing and\nreading. Computer-vision-based transcription solutions represent mechanisms for\nreducing understanding restrictions on this system. However, such tools face\nnuisances inherent to image processing systems, e.g., illumination, noise, and\nscale, harming the result. This paper presents an automated approach to\nmitigate transcription errors in braille texts for the Portuguese language. We\npropose a selection function, combined with dictionaries, that provides the\nbest correspondence of words based on their braille representation. We\nvalidated our proposal on a dataset of synthetic images by submitting them to\ndifferent noise levels and testing the proposal's robustness. Experimental\nresults confirm the effectiveness of the solution compared to a standard\napproach. As a contribution of this paper, we expect to provide a method to\nsupport robust and adaptable solutions to real use conditions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 20:41:14 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ortoncelli", "Andr\u00e9 Roberto", ""], ["Marcon", "Marlon", ""], ["Beal", "Franciele", ""]]}, {"id": "2103.03940", "submitter": "Henrique Siqueira", "authors": "Henrique Siqueira, Alexander Sutherland, Pablo Barros, Mattias Kerzel,\n  Sven Magg, Stefan Wermter", "title": "Disambiguating Affective Stimulus Associations for Robot Perception and\n  Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effectively recognising and applying emotions to interactions is a highly\ndesirable trait for social robots. Implicitly understanding how subjects\nexperience different kinds of actions and objects in the world is crucial for\nnatural HRI interactions, with the possibility to perform positive actions and\navoid negative actions. In this paper, we utilize the NICO robot's appearance\nand capabilities to give the NICO the ability to model a coherent affective\nassociation between a perceived auditory stimulus and a temporally asynchronous\nemotion expression. This is done by combining evaluations of emotional valence\nfrom vision and language. NICO uses this information to make decisions about\nwhen to extend conversations in order to accrue more affective information if\nthe representation of the association is not coherent. Our primary contribution\nis providing a NICO robot with the ability to learn the affective associations\nbetween a perceived auditory stimulus and an emotional expression. NICO is able\nto do this for both individual subjects and specific stimuli, with the aid of\nan emotion-driven dialogue system that rectifies emotional expression\nincoherences. The robot is then able to use this information to determine a\nsubject's enjoyment of perceived auditory stimuli in a real HRI scenario.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 20:55:48 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Siqueira", "Henrique", ""], ["Sutherland", "Alexander", ""], ["Barros", "Pablo", ""], ["Kerzel", "Mattias", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "2103.03968", "submitter": "Davood Karimi", "authors": "Davood Karimi and Rabab K. Ward", "title": "Interpolation of CT Projections by Exploiting Their Self-Similarity and\n  Smoothness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the medical usage of computed tomography (CT) continues to grow, the\nradiation dose should remain at a low level to reduce the health risks.\nTherefore, there is an increasing need for algorithms that can reconstruct\nhigh-quality images from low-dose scans. In this regard, most of the recent\nstudies have focused on iterative reconstruction algorithms, and little\nattention has been paid to restoration of the projection measurements, i.e.,\nthe sinogram. In this paper, we propose a novel sinogram interpolation\nalgorithm. The proposed algorithm exploits the self-similarity and smoothness\nof the sinogram. Sinogram self-similarity is modeled in terms of the similarity\nof small blocks extracted from stacked projections. The smoothness is modeled\nvia second-order total variation. Experiments with simulated and real CT data\nshow that sinogram interpolation with the proposed algorithm leads to a\nsubstantial improvement in the quality of the reconstructed image, especially\non low-dose scans. The proposed method can result in a significant reduction in\nthe number of projection measurements. This will reduce the radiation dose and\nalso the amount of data that need to be stored or transmitted, if the\nreconstruction is to be performed in a remote site.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 22:41:25 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Karimi", "Davood", ""], ["Ward", "Rabab K.", ""]]}, {"id": "2103.03975", "submitter": "Nico Lang", "authors": "Nico Lang, Nikolai Kalischek, John Armston, Konrad Schindler, Ralph\n  Dubayah, Jan Dirk Wegner", "title": "Global canopy height estimation with GEDI LIDAR waveforms and Bayesian\n  deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NASA's Global Ecosystem Dynamics Investigation (GEDI) is a key climate\nmission whose goal is to advance our understanding of the role of forests in\nthe global carbon cycle. While GEDI is the first space-based LIDAR explicitly\noptimized to measure vertical forest structure predictive of aboveground\nbiomass, the accurate interpretation of this vast amount of waveform data\nacross the broad range of observational and environmental conditions is\nchallenging. Here, we present a novel supervised machine learning approach to\ninterpret GEDI waveforms and regress canopy top height globally. We propose a\nBayesian convolutional neural network (CNN) to avoid the explicit modelling of\nunknown effects, such as atmospheric noise. The model learns to extract robust\nfeatures that generalize to unseen geographical regions and, in addition,\nyields reliable estimates of predictive uncertainty. Ultimately, the global\ncanopy top height estimates produced by our model have an expected RMSE of 2.7\nm with low bias.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 23:08:27 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lang", "Nico", ""], ["Kalischek", "Nikolai", ""], ["Armston", "John", ""], ["Schindler", "Konrad", ""], ["Dubayah", "Ralph", ""], ["Wegner", "Jan Dirk", ""]]}, {"id": "2103.03977", "submitter": "Nguyen Anh Minh Mai", "authors": "Nguyen Anh Minh Mai, Pierre Duthon, Louahdi Khoudour, Alain Crouzil,\n  Sergio A. Velastin", "title": "Sparse LiDAR and Stereo Fusion (SLS-Fusion) for Depth Estimationand 3D\n  Object Detection", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to accurately detect and localize objects is recognized as being\nthe most important for the perception of self-driving cars. From 2D to 3D\nobject detection, the most difficult is to determine the distance from the\nego-vehicle to objects. Expensive technology like LiDAR can provide a precise\nand accurate depth information, so most studies have tended to focus on this\nsensor showing a performance gap between LiDAR-based methods and camera-based\nmethods. Although many authors have investigated how to fuse LiDAR with RGB\ncameras, as far as we know there are no studies to fuse LiDAR and stereo in a\ndeep neural network for the 3D object detection task. This paper presents\nSLS-Fusion, a new approach to fuse data from 4-beam LiDAR and a stereo camera\nvia a neural network for depth estimation to achieve better dense depth maps\nand thereby improves 3D object detection performance. Since 4-beam LiDAR is\ncheaper than the well-known 64-beam LiDAR, this approach is also classified as\na low-cost sensors-based method. Through evaluation on the KITTI benchmark, it\nis shown that the proposed method significantly improves depth estimation\nperformance compared to a baseline method. Also, when applying it to 3D object\ndetection, a new state of the art on low-cost sensor based method is achieved.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 23:10:09 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:44:41 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 10:51:11 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Mai", "Nguyen Anh Minh", ""], ["Duthon", "Pierre", ""], ["Khoudour", "Louahdi", ""], ["Crouzil", "Alain", ""], ["Velastin", "Sergio A.", ""]]}, {"id": "2103.03987", "submitter": "Tyler Hayes", "authors": "Tyler L. Hayes and Christopher Kanan", "title": "Selective Replay Enhances Learning in Online Continual Analogical\n  Reasoning", "comments": "To appear in the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision\n  (CLVision) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In continual learning, a system learns from non-stationary data streams or\nbatches without catastrophic forgetting. While this problem has been heavily\nstudied in supervised image classification and reinforcement learning,\ncontinual learning in neural networks designed for abstract reasoning has not\nyet been studied. Here, we study continual learning of analogical reasoning.\nAnalogical reasoning tests such as Raven's Progressive Matrices (RPMs) are\ncommonly used to measure non-verbal abstract reasoning in humans, and recently\noffline neural networks for the RPM problem have been proposed. In this paper,\nwe establish experimental baselines, protocols, and forward and backward\ntransfer metrics to evaluate continual learners on RPMs. We employ experience\nreplay to mitigate catastrophic forgetting. Prior work using replay for image\nclassification tasks has found that selectively choosing the samples to replay\noffers little, if any, benefit over random selection. In contrast, we find that\nselective replay can significantly outperform random selection for the RPM\ntask.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 00:04:10 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 15:38:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hayes", "Tyler L.", ""], ["Kanan", "Christopher", ""]]}, {"id": "2103.03995", "submitter": "Wei-Chang Yeh", "authors": "Wei-Chang Yeh, Yi-Ping Lin, Yun-Chia Liang, Chyh-Ming Lai", "title": "Convolution Neural Network Hyperparameter Optimization Using Simplified\n  Swarm Optimization", "comments": "There are 44 manuscript pages, 16 tables, and 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Among the machine learning approaches applied in computer vision,\nConvolutional Neural Network (CNN) is widely used in the field of image\nrecognition. However, although existing CNN models have been proven to be\nefficient, it is not easy to find a network architecture with better\nperformance. Some studies choose to optimize the network architecture, while\nothers chose to optimize the hyperparameters, such as the number and size of\nconvolutional kernels, convolutional strides, pooling size, etc. Most of them\nare designed manually, which requires relevant expertise and takes a lot of\ntime. Therefore, this study proposes the idea of applying Simplified Swarm\nOptimization (SSO) on the hyperparameter optimization of LeNet models while\nusing MNIST, Fashion MNIST, and Cifar10 as validation. The experimental results\nshow that the proposed algorithm has higher accuracy than the original LeNet\nmodel, and it only takes a very short time to find a better hyperparameter\nconfiguration after training. In addition, we also analyze the output shape of\nthe feature map after each layer, and surprisingly, the results were mostly\nrectangular. The contribution of the study is to provide users with a simpler\nway to get better results with the existing model., and this study can also be\napplied to other CNN architectures.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 00:23:27 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yeh", "Wei-Chang", ""], ["Lin", "Yi-Ping", ""], ["Liang", "Yun-Chia", ""], ["Lai", "Chyh-Ming", ""]]}, {"id": "2103.04003", "submitter": "Ke Wang", "authors": "Ke Wang, Michael Kellman, Christopher M. Sandino, Kevin Zhang, Shreyas\n  S. Vasanawala, Jonathan I. Tamir, Stella X. Yu, Michael Lustig", "title": "Memory-efficient Learning for High-Dimensional MRI Reconstruction", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) based unrolled reconstructions have shown state-of-the-art\nperformance for under-sampled magnetic resonance imaging (MRI). Similar to\ncompressed sensing, DL can leverage high-dimensional data (e.g. 3D, 2D+time,\n3D+time) to further improve performance. However, network size and depth are\ncurrently limited by the GPU memory required for backpropagation. Here we use a\nmemory-efficient learning (MEL) framework which favorably trades off storage\nwith a manageable increase in computation during training. Using MEL with\nmulti-dimensional data, we demonstrate improved image reconstruction\nperformance for in-vivo 3D MRI and 2D+time cardiac cine MRI. MEL uses far less\nGPU memory while marginally increasing the training time, which enables new\napplications of DL to high-dimensional MRI.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 01:36:25 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Ke", ""], ["Kellman", "Michael", ""], ["Sandino", "Christopher M.", ""], ["Zhang", "Kevin", ""], ["Vasanawala", "Shreyas S.", ""], ["Tamir", "Jonathan I.", ""], ["Yu", "Stella X.", ""], ["Lustig", "Michael", ""]]}, {"id": "2103.04008", "submitter": "Alexander Wong", "authors": "Alexander Wong, Jack Lu, Adam Dorfman, Paul McInnis, Mahmoud Famouri,\n  Daniel Manary, James Ren Hou Lee, and Michael Lynch", "title": "Fibrosis-Net: A Tailored Deep Convolutional Neural Network Design for\n  Prediction of Pulmonary Fibrosis Progression from Chest CT Images", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary fibrosis is a devastating chronic lung disease that causes\nirreparable lung tissue scarring and damage, resulting in progressive loss in\nlung capacity and has no known cure. A critical step in the treatment and\nmanagement of pulmonary fibrosis is the assessment of lung function decline,\nwith computed tomography (CT) imaging being a particularly effective method for\ndetermining the extent of lung damage caused by pulmonary fibrosis. Motivated\nby this, we introduce Fibrosis-Net, a deep convolutional neural network design\ntailored for the prediction of pulmonary fibrosis progression from chest CT\nimages. More specifically, machine-driven design exploration was leveraged to\ndetermine a strong architectural design for CT lung analysis, upon which we\nbuild a customized network design tailored for predicting forced vital capacity\n(FVC) based on a patient's CT scan, initial spirometry measurement, and\nclinical metadata. Finally, we leverage an explainability-driven performance\nvalidation strategy to study the decision-making behaviour of Fibrosis-Net as\nto verify that predictions are based on relevant visual indicators in CT\nimages. Experiments using a patient cohort from the OSIC Pulmonary Fibrosis\nProgression Challenge showed that the proposed Fibrosis-Net is able to achieve\na significantly higher modified Laplace Log Likelihood score than the winning\nsolutions on the challenge. Furthermore, explainability-driven performance\nvalidation demonstrated that the proposed Fibrosis-Net exhibits correct\ndecision-making behaviour by leveraging clinically-relevant visual indicators\nin CT images when making predictions on pulmonary fibrosis progress. While\nFibrosis-Net is not yet a production-ready clinical assessment solution, we\nhope that its release in open source manner will encourage researchers,\nclinicians, and citizen data scientists alike to leverage and build upon it.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 02:16:41 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:43:20 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wong", "Alexander", ""], ["Lu", "Jack", ""], ["Dorfman", "Adam", ""], ["McInnis", "Paul", ""], ["Famouri", "Mahmoud", ""], ["Manary", "Daniel", ""], ["Lee", "James Ren Hou", ""], ["Lynch", "Michael", ""]]}, {"id": "2103.04009", "submitter": "Chia-Yu Hsu", "authors": "Chia-Yu Hsu and Wenwen Li", "title": "Learning from Counting: Leveraging Temporal Classification for Weakly\n  Supervised Object Localization and Detection", "comments": "31st British Machine Vision Conference (BMVC), oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports a new solution of leveraging temporal classification to\nsupport weakly supervised object detection (WSOD). Specifically, we introduce\nraster scan-order techniques to serialize 2D images into 1D sequence data, and\nthen leverage a combined LSTM (Long, Short-Term Memory) and CTC (Connectionist\nTemporal Classification) network to achieve object localization based on a\ntotal count (of interested objects). We term our proposed network LSTM-CCTC\n(Count-based CTC). This \"learning from counting\" strategy differs from existing\nWSOD methods in that our approach automatically identifies critical points on\nor near a target object. This strategy significantly reduces the need of\ngenerating a large number of candidate proposals for object localization.\nExperiments show that our method yields state-of-the-art performance based on\nan evaluation on PASCAL VOC datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 02:18:03 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Hsu", "Chia-Yu", ""], ["Li", "Wenwen", ""]]}, {"id": "2103.04011", "submitter": "Yuchao Dai Dr.", "authors": "Yunqiu Lv and Jing Zhang and Yuchao Dai and Aixuan Li and Bowen Liu\n  and Nick Barnes and Deng-Ping Fan", "title": "Simultaneously Localize, Segment and Rank the Camouflaged Objects", "comments": "Accepted to IEEE/CVF CVPR 2021. Our code and dataset are publicly\n  available at https://github.com/JingZhang617/COD-Rank-Localize-and-Segment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camouflage is a key defence mechanism across species that is critical to\nsurvival. Common strategies for camouflage include background matching,\nimitating the color and pattern of the environment, and disruptive coloration,\ndisguising body outlines [35]. Camouflaged object detection (COD) aims to\nsegment camouflaged objects hiding in their surroundings. Existing COD models\nare built upon binary ground truth to segment the camouflaged objects without\nillustrating the level of camouflage. In this paper, we revisit this task and\nargue that explicitly modeling the conspicuousness of camouflaged objects\nagainst their particular backgrounds can not only lead to a better\nunderstanding about camouflage and evolution of animals, but also provide\nguidance to design more sophisticated camouflage techniques. Furthermore, we\nobserve that it is some specific parts of the camouflaged objects that make\nthem detectable by predators. With the above understanding about camouflaged\nobjects, we present the first ranking based COD network (Rank-Net) to\nsimultaneously localize, segment and rank camouflaged objects. The localization\nmodel is proposed to find the discriminative regions that make the camouflaged\nobject obvious. The segmentation model segments the full scope of the\ncamouflaged objects. And, the ranking model infers the detectability of\ndifferent camouflaged objects. Moreover, we contribute a large COD testing set\nto evaluate the generalization ability of COD models. Experimental results show\nthat our model achieves new state-of-the-art, leading to a more interpretable\nCOD network.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 02:53:36 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 15:54:52 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lv", "Yunqiu", ""], ["Zhang", "Jing", ""], ["Dai", "Yuchao", ""], ["Li", "Aixuan", ""], ["Liu", "Bowen", ""], ["Barnes", "Nick", ""], ["Fan", "Deng-Ping", ""]]}, {"id": "2103.04019", "submitter": "Jianing Qiu", "authors": "Jianing Qiu, Frank P.-W. Lo, Xiao Gu, Yingnan Sun, Shuo Jiang, and\n  Benny Lo", "title": "Indoor Future Person Localization from an Egocentric Wearable Camera", "comments": "accepted as conference paper in 2021 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Accurate prediction of future person location and movement trajectory from an\negocentric wearable camera can benefit a wide range of applications, such as\nassisting visually impaired people in navigation, and the development of\nmobility assistance for people with disability. In this work, a new egocentric\ndataset was constructed using a wearable camera, with 8,250 short clips of a\ntargeted person either walking 1) toward, 2) away, or 3) across the camera\nwearer in indoor environments, or 4) staying still in the scene, and 13,817\nperson bounding boxes were manually labelled. Apart from the bounding boxes,\nthe dataset also contains the estimated pose of the targeted person as well as\nthe IMU signal of the wearable camera at each time point. An LSTM-based\nencoder-decoder framework was designed to predict the future location and\nmovement trajectory of the targeted person in this egocentric setting.\nExtensive experiments have been conducted on the new dataset, and have shown\nthat the proposed method is able to reliably and better predict future person\nlocation and trajectory in egocentric videos captured by the wearable camera\ncompared to three baselines.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 03:32:42 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 09:47:11 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Qiu", "Jianing", ""], ["Lo", "Frank P. -W.", ""], ["Gu", "Xiao", ""], ["Sun", "Yingnan", ""], ["Jiang", "Shuo", ""], ["Lo", "Benny", ""]]}, {"id": "2103.04020", "submitter": "Hang Zhang", "authors": "Hang Zhang, Rongguang Wang, Jinwei Zhang, Chao Li, Gufeng Yang, Pascal\n  Spincemaille, Thanh Nguyen, and Yi Wang", "title": "NeRD: Neural Representation of Distribution for Medical Image\n  Segmentation", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Neural Representation of Distribution (NeRD) technique, a module\nfor convolutional neural networks (CNNs) that can estimate the feature\ndistribution by optimizing an underlying function mapping image coordinates to\nthe feature distribution. Using NeRD, we propose an end-to-end deep learning\nmodel for medical image segmentation that can compensate the negative impact of\nfeature distribution shifting issue caused by commonly used network operations\nsuch as padding and pooling. An implicit function is used to represent the\nparameter space of the feature distribution by querying the image coordinate.\nWith NeRD, the impact of issues such as over-segmenting and missing have been\nreduced, and experimental results on the challenging white matter lesion\nsegmentation and left atrial segmentation verify the effectiveness of the\nproposed method. The code is available via https://github.com/tinymilky/NeRD.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 03:45:47 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhang", "Hang", ""], ["Wang", "Rongguang", ""], ["Zhang", "Jinwei", ""], ["Li", "Chao", ""], ["Yang", "Gufeng", ""], ["Spincemaille", "Pascal", ""], ["Nguyen", "Thanh", ""], ["Wang", "Yi", ""]]}, {"id": "2103.04023", "submitter": "Jinsong Zhang", "authors": "Jinsong Zhang, Kun Li, Yu-Kun Lai, Jingyu Yang", "title": "PISE: Person Image Synthesis and Editing with Decoupled GAN", "comments": "10 pages, 9 figures. CVPR2021, https://github.com/Zhangjinso/PISE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person image synthesis, e.g., pose transfer, is a challenging problem due to\nlarge variation and occlusion. Existing methods have difficulties predicting\nreasonable invisible regions and fail to decouple the shape and style of\nclothing, which limits their applications on person image editing. In this\npaper, we propose PISE, a novel two-stage generative model for Person Image\nSynthesis and Editing, which is able to generate realistic person images with\ndesired poses, textures, or semantic layouts. For human pose transfer, we first\nsynthesize a human parsing map aligned with the target pose to represent the\nshape of clothing by a parsing generator, and then generate the final image by\nan image generator. To decouple the shape and style of clothing, we propose\njoint global and local per-region encoding and normalization to predict the\nreasonable style of clothing for invisible regions. We also propose\nspatial-aware normalization to retain the spatial context relationship in the\nsource image. The results of qualitative and quantitative experiments\ndemonstrate the superiority of our model on human pose transfer. Besides, the\nresults of texture transfer and region editing show that our model can be\napplied to person image editing.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 04:32:06 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 01:47:08 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 09:52:19 GMT"}, {"version": "v4", "created": "Wed, 31 Mar 2021 09:22:07 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhang", "Jinsong", ""], ["Li", "Kun", ""], ["Lai", "Yu-Kun", ""], ["Yang", "Jingyu", ""]]}, {"id": "2103.04026", "submitter": "Chentian Li", "authors": "Chentian Li, Chi Ma, William W. Lu", "title": "Morphological Operation Residual Blocks: Enhancing 3D Morphological\n  Feature Representation in Convolutional Neural Networks for Semantic\n  Segmentation of Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shapes and morphology of the organs and tissues are important prior\nknowledge in medical imaging recognition and segmentation. The morphological\noperation is a well-known method for morphological feature extraction. As the\nmorphological operation is performed well in hand-crafted image segmentation\ntechniques, it is also promising to design an approach to approximate\nmorphological operation in the convolutional networks. However, using the\ntraditional convolutional neural network as a black-box is usually hard to\nspecify the morphological operation action. Here, we introduced a 3D\nmorphological operation residual block to extract morphological features in\nend-to-end deep learning models for semantic segmentation. This study proposed\na novel network block architecture that embedded the morphological operation as\nan infinitely strong prior in the convolutional neural network. Several 3D deep\nlearning models with the proposed morphological operation block were built and\ncompared in different medical imaging segmentation tasks. Experimental results\nshowed the proposed network achieved a relatively higher performance in the\nsegmentation tasks comparing with the conventional approach. In conclusion, the\nnovel network block could be easily embedded in traditional networks and\nefficiently reinforce the deep learning models for medical imaging\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 04:41:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Chentian", ""], ["Ma", "Chi", ""], ["Lu", "William W.", ""]]}, {"id": "2103.04027", "submitter": "Haoran Song", "authors": "Haoran Song, Di Luan, Wenchao Ding, Michael Yu Wang, and Qifeng Chen", "title": "Learning to Predict Vehicle Trajectories with Model-based Planning", "comments": "12 pages, 7 figures. Project page at\n  http://haoran-song.github.io/prime", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future trajectories of on-road vehicles is critical for\nautonomous driving. In this paper, we introduce a novel prediction framework\ncalled PRIME, which stands for Prediction with Model-based Planning. Unlike\nrecent prediction works that utilize neural networks to model scene context and\nproduce unconstrained trajectories, PRIME is designed to generate accurate and\nfeasibility-guaranteed future trajectory predictions, which guarantees the\ntrajectory feasibility by exploiting a model-based generator to produce future\ntrajectories under explicit constraints and enables accurate multimodal\nprediction by using a learning-based evaluator to select future trajectories.\nWe conduct experiments on the large-scale Argoverse Motion Forecasting\nBenchmark. Our PRIME outperforms state-of-the-art methods in prediction\naccuracy, feasibility, and robustness under imperfect tracking. Furthermore, we\nachieve the 1st place on the Argoervese Leaderboard.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 04:49:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Song", "Haoran", ""], ["Luan", "Di", ""], ["Ding", "Wenchao", ""], ["Wang", "Michael Yu", ""], ["Chen", "Qifeng", ""]]}, {"id": "2103.04032", "submitter": "Vinay Verma Kumar", "authors": "Sakshi Varshney, Vinay Kumar Verma, Lawrence Carin, Piyush Rai", "title": "Efficient Continual Adaptation for Generative Adversarial Networks", "comments": "Under Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a continual learning approach for generative adversarial networks\n(GANs), by designing and leveraging parameter-efficient feature map\ntransformations. Our approach is based on learning a set of global and\ntask-specific parameters. The global parameters are fixed across tasks whereas\nthe task specific parameters act as local adapters for each task, and help in\nefficiently transforming the previous task's feature map to the new task's\nfeature map. Moreover, we propose an element-wise residual bias in the\ntransformed feature space which highly stabilizes GAN training. In contrast to\nthe recent approaches for continual GANs, we do not rely on memory replay,\nregularization towards previous tasks' parameters, or expensive weight\ntransformations. Through extensive experiments on challenging and diverse\ndatasets, we show that the feature-map transformation based approach\noutperforms state-of-the-art continual GANs methods, with substantially fewer\nparameters, and also generates high-quality samples that can be used in\ngenerative replay based continual learning of discriminative tasks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 05:09:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Varshney", "Sakshi", ""], ["Verma", "Vinay Kumar", ""], ["Carin", "Lawrence", ""], ["Rai", "Piyush", ""]]}, {"id": "2103.04037", "submitter": "Andrew Shin", "authors": "Andrew Shin, Masato Ishii, Takuya Narihira", "title": "Perspectives and Prospects on Transformer Architecture for Cross-Modal\n  Tasks with Language and Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer architectures have brought about fundamental changes to\ncomputational linguistic field, which had been dominated by recurrent neural\nnetworks for many years. Its success also implies drastic changes in\ncross-modal tasks with language and vision, and many researchers have already\ntackled the issue. In this paper, we review some of the most critical\nmilestones in the field, as well as overall trends on how transformer\narchitecture has been incorporated into visuolinguistic cross-modal tasks.\nFurthermore, we discuss its current limitations and speculate upon some of the\nprospects that we find imminent.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 05:44:27 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Shin", "Andrew", ""], ["Ishii", "Masato", ""], ["Narihira", "Takuya", ""]]}, {"id": "2103.04038", "submitter": "Yiming Li", "authors": "Yiming Li, Yanjie Li, Yalei Lv, Yong Jiang, Shu-Tao Xia", "title": "Hidden Backdoor Attack against Semantic Segmentation Models", "comments": "This is a 6-pages short version of our ongoing work. It is accepted\n  by the non-archival ICLR workshop on Security and Safety in Machine Learning\n  Systems, 2021. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to the \\emph{backdoor attack},\nwhich intends to embed hidden backdoors in DNNs by poisoning training data. The\nattacked model behaves normally on benign samples, whereas its prediction will\nbe changed to a particular target label if hidden backdoors are activated. So\nfar, backdoor research has mostly been conducted towards classification tasks.\nIn this paper, we reveal that this threat could also happen in semantic\nsegmentation, which may further endanger many mission-critical applications\n($e.g.$, autonomous driving). Except for extending the existing attack paradigm\nto maliciously manipulate the segmentation models from the image-level, we\npropose a novel attack paradigm, the \\emph{fine-grained attack}, where we treat\nthe target label ($i.e.$, annotation) from the object-level instead of the\nimage-level to achieve more sophisticated manipulation. In the annotation of\npoisoned samples generated by the fine-grained attack, only pixels of specific\nobjects will be labeled with the attacker-specified target class while others\nare still with their ground-truth ones. Experiments show that the proposed\nmethods can successfully attack semantic segmentation models by poisoning only\na small proportion of training data. Our method not only provides a new\nperspective for designing novel attacks but also serves as a strong baseline\nfor improving the robustness of semantic segmentation methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 05:50:29 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 10:47:02 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 05:07:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Yiming", ""], ["Li", "Yanjie", ""], ["Lv", "Yalei", ""], ["Jiang", "Yong", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "2103.04039", "submitter": "Xiangtao Kong", "authors": "Xiangtao Kong, Hengyuan Zhao, Yu Qiao, Chao Dong", "title": "ClassSR: A General Framework to Accelerate Super-Resolution Networks by\n  Data Characteristic", "comments": "CVPR2021 paper + supplementary file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim at accelerating super-resolution (SR) networks on large images\n(2K-8K). The large images are usually decomposed into small sub-images in\npractical usages. Based on this processing, we found that different image\nregions have different restoration difficulties and can be processed by\nnetworks with different capacities. Intuitively, smooth areas are easier to\nsuper-solve than complex textures. To utilize this property, we can adopt\nappropriate SR networks to process different sub-images after the\ndecomposition. On this basis, we propose a new solution pipeline -- ClassSR\nthat combines classification and SR in a unified framework. In particular, it\nfirst uses a Class-Module to classify the sub-images into different classes\naccording to restoration difficulties, then applies an SR-Module to perform SR\nfor different classes. The Class-Module is a conventional classification\nnetwork, while the SR-Module is a network container that consists of the\nto-be-accelerated SR network and its simplified versions. We further introduce\na new classification method with two losses -- Class-Loss and Average-Loss to\nproduce the classification results. After joint training, a majority of\nsub-images will pass through smaller networks, thus the computational cost can\nbe significantly reduced. Experiments show that our ClassSR can help most\nexisting methods (e.g., FSRCNN, CARN, SRResNet, RCAN) save up to 50% FLOPs on\nDIV8K datasets. This general framework can also be applied in other low-level\nvision tasks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 06:00:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kong", "Xiangtao", ""], ["Zhao", "Hengyuan", ""], ["Qiao", "Yu", ""], ["Dong", "Chao", ""]]}, {"id": "2103.04046", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Ghada Zamzmi, Xuanting Cai", "title": "Simplicial Complex Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplicial complexes form an important class of topological spaces that are\nfrequently used to in many applications areas such as computer-aided design,\ncomputer graphics, and simulation. The representation learning on graphs, which\nare just 1-d simplicial complexes, has witnessed a great attention and success\nin the past few years. Due to the additional complexity higher dimensional\nsimplicial hold, there has not been enough effort to extend representation\nlearning to these objects especially when it comes to learn entire-simplicial\ncomplex representation. In this work, we propose a method for simplicial\ncomplex-level representation learning that embeds a simplicial complex to a\nuniversal embedding space in a way that complex-to-complex proximity is\npreserved. Our method utilizes a simplex-level embedding induced by a\npre-trained simplicial autoencoder to learn an entire simplicial complex\nrepresentation. To the best of our knowledge, this work presents the first\nmethod for learning simplicial complex-level representation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 06:33:04 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 07:18:25 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 22:01:33 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Hajij", "Mustafa", ""], ["Zamzmi", "Ghada", ""], ["Cai", "Xuanting", ""]]}, {"id": "2103.04053", "submitter": "Fengbei Liu", "authors": "Fengbei Liu, Yu Tian, Filipe R. Cordeiro, Vasileios Belagiannis, Ian\n  Reid, Gustavo Carneiro", "title": "Noisy Label Learning for Large-scale Medical Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The classification accuracy of deep learning models depends not only on the\nsize of their training sets, but also on the quality of their labels. In\nmedical image classification, large-scale datasets are becoming abundant, but\ntheir labels will be noisy when they are automatically extracted from radiology\nreports using natural language processing tools. Given that deep learning\nmodels can easily overfit these noisy-label samples, it is important to study\ntraining approaches that can handle label noise. In this paper, we adapt a\nstate-of-the-art (SOTA) noisy-label multi-class training approach to learn a\nmulti-label classifier for the dataset Chest X-ray14, which is a large scale\ndataset known to contain label noise in the training set. Given that this\ndataset also has label noise in the testing set, we propose a new theoretically\nsound method to estimate the performance of the model on a hidden clean testing\ndata, given the result on the noisy testing data. Using our clean data\nperformance estimation, we notice that the majority of label noise on Chest\nX-ray14 is present in the class 'No Finding', which is intuitively correct\nbecause this is the most likely class to contain one or more of the 14 diseases\ndue to labelling mistakes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 07:42:36 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Fengbei", ""], ["Tian", "Yu", ""], ["Cordeiro", "Filipe R.", ""], ["Belagiannis", "Vasileios", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2103.04056", "submitter": "Di Feng", "authors": "Di Feng, Yiyang Zhou, Chenfeng Xu, Masayoshi Tomizuka, Wei Zhan", "title": "A Simple and Efficient Multi-task Network for 3D Object Detection and\n  Road Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting dynamic objects and predicting static road information such as\ndrivable areas and ground heights are crucial for safe autonomous driving.\nPrevious works studied each perception task separately, and lacked a collective\nquantitative analysis. In this work, we show that it is possible to perform all\nperception tasks via a simple and efficient multi-task network. Our proposed\nnetwork, LidarMTL, takes raw LiDAR point cloud as inputs, and predicts six\nperception outputs for 3D object detection and road understanding. The network\nis based on an encoder-decoder architecture with 3D sparse convolution and\ndeconvolution operations. Extensive experiments verify the proposed method with\ncompetitive accuracies compared to state-of-the-art object detectors and other\ntask-specific networks. LidarMTL is also leveraged for online localization.\nCode and pre-trained model have been made available at\nhttps://github.com/frankfengdi/LidarMTL.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 08:00:26 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Feng", "Di", ""], ["Zhou", "Yiyang", ""], ["Xu", "Chenfeng", ""], ["Tomizuka", "Masayoshi", ""], ["Zhan", "Wei", ""]]}, {"id": "2103.04059", "submitter": "Ali Cheraghian", "authors": "Ali Cheraghian, Shafin Rahman, Pengfei Fang, Soumava Kumar Roy, Lars\n  Petersson, Mehrtash Harandi", "title": "Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental\n  Learning", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot class incremental learning (FSCIL) portrays the problem of learning\nnew concepts gradually, where only a few examples per concept are available to\nthe learner. Due to the limited number of examples for training, the techniques\ndeveloped for standard incremental learning cannot be applied verbatim to\nFSCIL. In this work, we introduce a distillation algorithm to address the\nproblem of FSCIL and propose to make use of semantic information during\ntraining. To this end, we make use of word embeddings as semantic information\nwhich is cheap to obtain and which facilitate the distillation process.\nFurthermore, we propose a method based on an attention mechanism on multiple\nparallel embeddings of visual data to align visual and semantic vectors, which\nreduces issues related to catastrophic forgetting. Via experiments on\nMiniImageNet, CUB200, and CIFAR100 dataset, we establish new state-of-the-art\nresults by outperforming existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 08:07:26 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 02:27:53 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Cheraghian", "Ali", ""], ["Rahman", "Shafin", ""], ["Fang", "Pengfei", ""], ["Roy", "Soumava Kumar", ""], ["Petersson", "Lars", ""], ["Harandi", "Mehrtash", ""]]}, {"id": "2103.04062", "submitter": "Yuang Liu", "authors": "Yuang Liu, Wei Zhang, Jun Wang", "title": "Adaptive Multi-Teacher Multi-level Knowledge Distillation", "comments": "Neurocomputing 2020.07", "journal-ref": null, "doi": "10.1016/j.neucom.2020.07.048", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation~(KD) is an effective learning paradigm for improving\nthe performance of lightweight student networks by utilizing additional\nsupervision knowledge distilled from teacher networks. Most pioneering studies\neither learn from only a single teacher in their distillation learning methods,\nneglecting the potential that a student can learn from multiple teachers\nsimultaneously, or simply treat each teacher to be equally important, unable to\nreveal the different importance of teachers for specific examples. To bridge\nthis gap, we propose a novel adaptive multi-teacher multi-level knowledge\ndistillation learning framework~(AMTML-KD), which consists two novel insights:\n(i) associating each teacher with a latent representation to adaptively learn\ninstance-level teacher importance weights which are leveraged for acquiring\nintegrated soft-targets~(high-level knowledge) and (ii) enabling the\nintermediate-level hints~(intermediate-level knowledge) to be gathered from\nmultiple teachers by the proposed multi-group hint strategy. As such, a student\nmodel can learn multi-level knowledge from multiple teachers through AMTML-KD.\nExtensive results on publicly available datasets demonstrate the proposed\nlearning framework ensures student to achieve improved performance than strong\ncompetitors.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 08:18:16 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Yuang", ""], ["Zhang", "Wei", ""], ["Wang", "Jun", ""]]}, {"id": "2103.04068", "submitter": "Artjoms Gorpincenko", "authors": "Artjoms Gorpincenko, Geoffrey French, Peter Knight, Mike Challiss,\n  Michal Mackiewicz", "title": "Improving Automated Sonar Video Analysis to Notify About Jellyfish\n  Blooms", "comments": null, "journal-ref": "IEEE Sensors Journal, 21, 4981-4988 (2021)", "doi": "10.1109/JSEN.2020.3032031", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human enterprise often suffers from direct negative effects caused by\njellyfish blooms. The investigation of a prior jellyfish monitoring system\nshowed that it was unable to reliably perform in a cross validation setting,\ni.e. in new underwater environments. In this paper, a number of enhancements\nare proposed to the part of the system that is responsible for object\nclassification. First, the training set is augmented by adding synthetic data,\nmaking the deep learning classifier able to generalise better. Then, the\nframework is enhanced by employing a new second stage model, which analyzes the\noutputs of the first network to make the final prediction. Finally, weighted\nloss and confidence threshold are added to balance out true and false\npositives. With all the upgrades in place, the system can correctly classify\n30.16% (comparing to the initial 11.52%) of all spotted jellyfish, keep the\namount of false positives as low as 0.91% (comparing to the initial 2.26%) and\noperate in real-time within the computational constraints of an autonomous\nembedded platform.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 08:39:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gorpincenko", "Artjoms", ""], ["French", "Geoffrey", ""], ["Knight", "Peter", ""], ["Challiss", "Mike", ""], ["Mackiewicz", "Michal", ""]]}, {"id": "2103.04075", "submitter": "Xueying Shi", "authors": "Xueying Shi, Yueming Jin, Qi Dou, Jing Qin, and Pheng-Ann Heng", "title": "Domain Adaptive Robotic Gesture Recognition with Unsupervised\n  Kinematic-Visual Data Alignment", "comments": "Accepted as a conference paper in IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated surgical gesture recognition is of great importance in\nrobot-assisted minimally invasive surgery. However, existing methods assume\nthat training and testing data are from the same domain, which suffers from\nsevere performance degradation when a domain gap exists, such as the simulator\nand real robot. In this paper, we propose a novel unsupervised domain\nadaptation framework which can simultaneously transfer multi-modality\nknowledge, i.e., both kinematic and visual data, from simulator to real robot.\nIt remedies the domain gap with enhanced transferable features by using\ntemporal cues in videos, and inherent correlations in multi-modal towards\nrecognizing gesture. Specifically, we first propose an MDO-K to align\nkinematics, which exploits temporal continuity to transfer motion directions\nwith smaller gap rather than position values, relieving the adaptation burden.\nMoreover, we propose a KV-Relation-ATT to transfer the co-occurrence signals of\nkinematics and vision. Such features attended by correlation similarity are\nmore informative for enhancing domain-invariance of the model. Two feature\nalignment strategies benefit the model mutually during the end-to-end learning\nprocess. We extensively evaluate our method for gesture recognition using DESK\ndataset with peg transfer procedure. Results show that our approach recovers\nthe performance with great improvement gains, up to 12.91% in ACC and 20.16% in\nF1score without using any annotations in real robot.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 09:10:03 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 06:57:12 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Shi", "Xueying", ""], ["Jin", "Yueming", ""], ["Dou", "Qi", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2103.04098", "submitter": "Zheng Zhu", "authors": "Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze\n  Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Dalong Du, Jie Zhou", "title": "WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face\n  Recognition", "comments": "Accepted by CVPR2021. Benchmark website is\n  https://www.face-benchmark.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we contribute a new million-scale face benchmark containing\nnoisy 4M identities/260M faces (WebFace260M) and cleaned 2M identities/42M\nfaces (WebFace42M) training data, as well as an elaborately designed\ntime-constrained evaluation protocol. Firstly, we collect 4M name list and\ndownload 260M faces from the Internet. Then, a Cleaning Automatically utilizing\nSelf-Training (CAST) pipeline is devised to purify the tremendous WebFace260M,\nwhich is efficient and scalable. To the best of our knowledge, the cleaned\nWebFace42M is the largest public face recognition training set and we expect to\nclose the data gap between academia and industry. Referring to practical\nscenarios, Face Recognition Under Inference Time conStraint (FRUITS) protocol\nand a test set are constructed to comprehensively evaluate face matchers.\n  Equipped with this benchmark, we delve into million-scale face recognition\nproblems. A distributed framework is developed to train face recognition models\nefficiently without tampering with the performance. Empowered by WebFace42M, we\nreduce relative 40% failure rate on the challenging IJB-C set, and ranks the\n3rd among 430 entries on NIST-FRVT. Even 10% data (WebFace4M) shows superior\nperformance compared with public training set. Furthermore, comprehensive\nbaselines are established on our rich-attribute test set under\nFRUITS-100ms/500ms/1000ms protocol, including MobileNet, EfficientNet,\nAttentionNet, ResNet, SENet, ResNeXt and RegNet families. Benchmark website is\nhttps://www.face-benchmark.org.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 11:12:43 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhu", "Zheng", ""], ["Huang", "Guan", ""], ["Deng", "Jiankang", ""], ["Ye", "Yun", ""], ["Huang", "Junjie", ""], ["Chen", "Xinze", ""], ["Zhu", "Jiagang", ""], ["Yang", "Tian", ""], ["Lu", "Jiwen", ""], ["Du", "Dalong", ""], ["Zhou", "Jie", ""]]}, {"id": "2103.04128", "submitter": "Kailun Yang", "authors": "Wei Mao, Jiaming Zhang, Kailun Yang, Rainer Stiefelhagen", "title": "Panoptic Lintention Network: Towards Efficient Navigational Perception\n  for the Visually Impaired", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic computer vision algorithms, instance segmentation, and semantic\nsegmentation can not provide a holistic understanding of the surroundings for\nthe visually impaired. In this paper, we utilize panoptic segmentation to\nassist the navigation of visually impaired people by offering both things and\nstuff awareness in the proximity of the visually impaired efficiently. To this\nend, we propose an efficient Attention module -- Lintention which can model\nlong-range interactions in linear time using linear space. Based on Lintention,\nwe then devise a novel panoptic segmentation model which we term Panoptic\nLintention Net. Experiments on the COCO dataset indicate that the Panoptic\nLintention Net raises the Panoptic Quality (PQ) from 39.39 to 41.42 with 4.6\\%\nperformance gain while only requiring 10\\% fewer GFLOPs and 25\\% fewer\nparameters in the semantic branch. Furthermore, a real-world test via our\ndesigned compact wearable panoptic segmentation system, indicates that our\nsystem based on the Panoptic Lintention Net accomplishes a relatively stable\nand exceptionally remarkable panoptic segmentation in real-world scenes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 14:45:46 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Mao", "Wei", ""], ["Zhang", "Jiaming", ""], ["Yang", "Kailun", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2103.04130", "submitter": "Dongsu Zhang", "authors": "Dongsu Zhang, Changwoon Choi, Jeonghwan Kim, Young Min Kim", "title": "Learning to Generate 3D Shapes with Generative Cellular Automata", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a probabilistic 3D generative model, named Generative Cellular\nAutomata, which is able to produce diverse and high quality shapes. We\nformulate the shape generation process as sampling from the transition kernel\nof a Markov chain, where the sampling chain eventually evolves to the full\nshape of the learned distribution. The transition kernel employs the local\nupdate rules of cellular automata, effectively reducing the search space in a\nhigh-resolution 3D grid space by exploiting the connectivity and sparsity of 3D\nshapes. Our progressive generation only focuses on the sparse set of occupied\nvoxels and their neighborhood, thus enabling the utilization of an expressive\nsparse convolutional network. We propose an effective training scheme to obtain\nthe local homogeneous rule of generative cellular automata with sequences that\nare slightly different from the sampling chain but converge to the full shapes\nin the training data. Extensive experiments on probabilistic shape completion\nand shape generation demonstrate that our method achieves competitive\nperformance against recent methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 14:59:35 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Dongsu", ""], ["Choi", "Changwoon", ""], ["Kim", "Jeonghwan", ""], ["Kim", "Young Min", ""]]}, {"id": "2103.04132", "submitter": "Zhenwang Qin Mr.", "authors": "Zhenwang Qin, Wensheng Wang, Karl-Heinz Dammer, Leifeng Guo and Zhen\n  Cao", "title": "A Real-time Low-cost Artificial Intelligence System for Autonomous\n  Spraying in Palm Plantations", "comments": "19 pages,18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In precision crop protection, (target-orientated) object detection in image\nprocessing can help navigate Unmanned Aerial Vehicles (UAV, crop protection\ndrones) to the right place to apply the pesticide. Unnecessary application of\nnon-target areas could be avoided. Deep learning algorithms dominantly use in\nmodern computer vision tasks which require high computing time, memory\nfootprint, and power consumption. Based on the Edge Artificial Intelligence, we\ninvestigate the main three paths that lead to dealing with this problem,\nincluding hardware accelerators, efficient algorithms, and model compression.\nFinally, we integrate them and propose a solution based on a light deep neural\nnetwork (DNN), called Ag-YOLO, which can make the crop protection UAV have the\nability to target detection and autonomous operation. This solution is\nrestricted in size, cost, flexible, fast, and energy-effective. The hardware is\nonly 18 grams in weight and 1.5 watts in energy consumption, and the developed\nDNN model needs only 838 kilobytes of disc space. We tested the developed\nhardware and software in comparison to the tiny version of the state-of-art\nYOLOv3 framework, known as YOLOv3-Tiny to detect individual palm in a\nplantation. An average F1 score of 0.9205 at the speed of 36.5 frames per\nsecond (in comparison to similar accuracy at 18 frames per second and 8.66\nmegabytes of the YOLOv3-Tiny algorithm) was reached. This developed detection\nsystem is easily plugged into any machines already purchased as long as the\nmachines have USB ports and run Linux Operating System.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 15:05:14 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Qin", "Zhenwang", ""], ["Wang", "Wensheng", ""], ["Dammer", "Karl-Heinz", ""], ["Guo", "Leifeng", ""], ["Cao", "Zhen", ""]]}, {"id": "2103.04133", "submitter": "Lanyun Zhu", "authors": "Lanyun Zhu, Deyi Ji, Shiping Zhu, Weihao Gan, Wei Wu, Junjie Yan", "title": "Learning Statistical Texture for Semantic Segmentation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing semantic segmentation works mainly focus on learning the contextual\ninformation in high-level semantic features with CNNs. In order to maintain a\nprecise boundary, low-level texture features are directly skip-connected into\nthe deeper layers. Nevertheless, texture features are not only about local\nstructure, but also include global statistical knowledge of the input image. In\nthis paper, we fully take advantages of the low-level texture features and\npropose a novel Statistical Texture Learning Network (STLNet) for semantic\nsegmentation. For the first time, STLNet analyzes the distribution of low level\ninformation and efficiently utilizes them for the task. Specifically, a novel\nQuantization and Counting Operator (QCO) is designed to describe the texture\ninformation in a statistical manner. Based on QCO, two modules are introduced:\n(1) Texture Enhance Module (TEM), to capture texture-related information and\nenhance the texture details; (2) Pyramid Texture Feature Extraction Module\n(PTFEM), to effectively extract the statistical texture features from multiple\nscales. Through extensive experiments, we show that the proposed STLNet\nachieves state-of-the-art performance on three semantic segmentation\nbenchmarks: Cityscapes, PASCAL Context and ADE20K.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 15:05:35 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhu", "Lanyun", ""], ["Ji", "Deyi", ""], ["Zhu", "Shiping", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""]]}, {"id": "2103.04136", "submitter": "Kailun Yang", "authors": "Yingzhi Zhang, Haoye Chen, Kailun Yang, Jiaming Zhang, Rainer\n  Stiefelhagen", "title": "Perception Framework through Real-Time Semantic Segmentation and Scene\n  Recognition on a Wearable System for the Visually Impaired", "comments": "6 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the scene information, including objectness and scene type, are important\nfor people with visual impairment, in this work we present a multi-task\nefficient perception system for the scene parsing and recognition tasks.\nBuilding on the compact ResNet backbone, our designed network architecture has\ntwo paths with shared parameters. In the structure, the semantic segmentation\npath integrates fast attention, with the aim of harvesting long-range\ncontextual information in an efficient manner. Simultaneously, the scene\nrecognition path attains the scene type inference by passing the semantic\nfeatures into semantic-driven attention networks and combining the semantic\nextracted representations with the RGB extracted representations through a\ngated attention module. In the experiments, we have verified the systems'\naccuracy and efficiency on both public datasets and real-world scenes. This\nsystem runs on a wearable belt with an Intel RealSense LiDAR camera and an\nNvidia Jetson AGX Xavier processor, which can accompany visually impaired\npeople and provide assistive scene information in their navigation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 15:07:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Yingzhi", ""], ["Chen", "Haoye", ""], ["Yang", "Kailun", ""], ["Zhang", "Jiaming", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2103.04147", "submitter": "Hadi Moradi", "authors": "Mohammad Hossein Nasseri, Hadi Moradi, Reshad Hosseini, Mohammadreza\n  Babaee", "title": "Simple online and real-time tracking with occlusion handling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple object tracking is a challenging problem in computer vision due to\ndifficulty in dealing with motion prediction, occlusion handling, and object\nre-identification. Many recent algorithms use motion and appearance cues to\novercome these challenges. But using appearance cues increases the computation\ncost notably and therefore the speed of the algorithm decreases significantly\nwhich makes them inappropriate for online applications. In contrast, there are\nalgorithms that only use motion cues to increase speed, especially for online\napplications. But these algorithms cannot handle occlusions and re-identify\nlost objects. In this paper, a novel online multiple object tracking algorithm\nis presented that only uses geometric cues of objects to tackle the occlusion\nand reidentification challenges simultaneously. As a result, it decreases the\nidentity switch and fragmentation metrics. Experimental results show that the\nproposed algorithm could decrease identity switch by 40% and fragmentation by\n28% compared to the state of the art online tracking algorithms. The code is\nalso publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 16:04:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Nasseri", "Mohammad Hossein", ""], ["Moradi", "Hadi", ""], ["Hosseini", "Reshad", ""], ["Babaee", "Mohammadreza", ""]]}, {"id": "2103.04167", "submitter": "Hongwei Li", "authors": "Hongwei Li, Fei-Fei Xue, Krishna Chaitanya, Shengda Luo, Ivan Ezhov,\n  Benedikt Wiestler, Jianguo Zhang, Bjoern Menze", "title": "Imbalance-Aware Self-Supervised Learning for 3D Radiomic Representations", "comments": "camera-ready version in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiomic representations can quantify properties of regions of interest in\nmedical image data. Classically, they account for pre-defined statistics of\nshape, texture, and other low-level image features. Alternatively, deep\nlearning-based representations are derived from supervised learning but require\nexpensive annotations from experts and often suffer from overfitting and data\nimbalance issues. In this work, we address the challenge of learning\nrepresentations of 3D medical images for an effective quantification under data\nimbalance. We propose a \\emph{self-supervised} representation learning\nframework to learn high-level features of 3D volumes as a complement to\nexisting radiomics features. Specifically, we demonstrate how to learn image\nrepresentations in a self-supervised fashion using a 3D Siamese network. More\nimportantly, we deal with data imbalance by exploiting two unsupervised\nstrategies: a) sample re-weighting, and b) balancing the composition of\ntraining batches. When combining our learned self-supervised feature with\ntraditional radiomics, we show significant improvement in brain tumor\nclassification and lung cancer staging tasks covering MRI and CT imaging\nmodalities.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 18:17:03 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 11:21:19 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Hongwei", ""], ["Xue", "Fei-Fei", ""], ["Chaitanya", "Krishna", ""], ["Luo", "Shengda", ""], ["Ezhov", "Ivan", ""], ["Wiestler", "Benedikt", ""], ["Zhang", "Jianguo", ""], ["Menze", "Bjoern", ""]]}, {"id": "2103.04173", "submitter": "Filipe Cordeiro", "authors": "Filipe R. Cordeiro, Ragav Sachdeva, Vasileios Belagiannis, Ian Reid,\n  Gustavo Carneiro", "title": "LongReMix: Robust Learning with High Confidence Samples in a Noisy Label\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural network models are robust to a limited amount of label noise, but\ntheir ability to memorise noisy labels in high noise rate problems is still an\nopen issue. The most competitive noisy-label learning algorithms rely on a\n2-stage process comprising an unsupervised learning to classify training\nsamples as clean or noisy, followed by a semi-supervised learning that\nminimises the empirical vicinal risk (EVR) using a labelled set formed by\nsamples classified as clean, and an unlabelled set with samples classified as\nnoisy. In this paper, we hypothesise that the generalisation of such 2-stage\nnoisy-label learning methods depends on the precision of the unsupervised\nclassifier and the size of the training set to minimise the EVR. We empirically\nvalidate these two hypotheses and propose the new 2-stage noisy-label training\nalgorithm LongReMix. We test LongReMix on the noisy-label benchmarks CIFAR-10,\nCIFAR-100, WebVision, Clothing1M, and Food101-N. The results show that our\nLongReMix generalises better than competing approaches, particularly in high\nlabel noise problems. Furthermore, our approach achieves state-of-the-art\nperformance in most datasets. The code will be available upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 18:48:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cordeiro", "Filipe R.", ""], ["Sachdeva", "Ragav", ""], ["Belagiannis", "Vasileios", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2103.04174", "submitter": "Bohan Wu", "authors": "Bohan Wu, Suraj Nair, Roberto Martin-Martin, Li Fei-Fei, Chelsea Finn", "title": "Greedy Hierarchical Variational Autoencoders for Large-Scale Video\n  Prediction", "comments": "Equal advising and contribution for last two authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A video prediction model that generalizes to diverse scenes would enable\nintelligent agents such as robots to perform a variety of tasks via planning\nwith the model. However, while existing video prediction models have produced\npromising results on small datasets, they suffer from severe underfitting when\ntrained on large and diverse datasets. To address this underfitting challenge,\nwe first observe that the ability to train larger video prediction models is\noften bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep\nhierarchical latent variable models can produce higher quality predictions by\ncapturing the multi-level stochasticity of future observations, but end-to-end\noptimization of such models is notably difficult. Our key insight is that\ngreedy and modular optimization of hierarchical autoencoders can simultaneously\naddress both the memory constraints and the optimization challenges of\nlarge-scale video prediction. We introduce Greedy Hierarchical Variational\nAutoencoders (GHVAEs), a method that learns high-fidelity video predictions by\ngreedily training each level of a hierarchical autoencoder. In comparison to\nstate-of-the-art models, GHVAEs provide 17-55% gains in prediction performance\non four video datasets, a 35-40% higher success rate on real robot tasks, and\ncan improve performance monotonically by simply adding more modules.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 18:58:56 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 18:37:14 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 07:25:28 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wu", "Bohan", ""], ["Nair", "Suraj", ""], ["Martin-Martin", "Roberto", ""], ["Fei-Fei", "Li", ""], ["Finn", "Chelsea", ""]]}, {"id": "2103.04178", "submitter": "Lahiru D. Chamain Hewa Gamage", "authors": "Lahiru D. Chamain, Fabien Racap\\'e, Jean B\\'egaint, Akshay Pushparaja\n  and Simon Feltman", "title": "End-to-end optimized image compression for multiple machine tasks", "comments": "supplement is added to the same document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An increasing share of captured images and videos are transmitted for storage\nand remote analysis by computer vision algorithms, rather than to be viewed by\nhumans. Contrary to traditional standard codecs with engineered tools, neural\nnetwork based codecs can be trained end-to-end to optimally compress images\nwith respect to a target rate and any given differentiable performance metric.\nAlthough it is possible to train such compression tools to achieve better\nrate-accuracy performance for a particular computer vision task, it could be\npractical and relevant to re-use the compressed bit-stream for multiple machine\ntasks. For this purpose, we introduce 'Connectors' that are inserted between\nthe decoder and the task algorithms to enable a direct transformation of the\ncompressed content, which was previously optimized for a specific task, to\nmultiple other machine tasks. We demonstrate the effectiveness of the proposed\nmethod by achieving significant rate-accuracy performance improvement for both\nimage classification and object segmentation, using the same bit-stream,\noriginally optimized for object detection.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 19:09:05 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chamain", "Lahiru D.", ""], ["Racap\u00e9", "Fabien", ""], ["B\u00e9gaint", "Jean", ""], ["Pushparaja", "Akshay", ""], ["Feltman", "Simon", ""]]}, {"id": "2103.04192", "submitter": "Guy Ohayon", "authors": "Guy Ohayon, Theo Adrai, Gregory Vaksman, Michael Elad, Peyman Milanfar", "title": "High Perceptual Quality Image Denoising with a Posterior Sampling CGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast work in Deep Learning (DL) has led to a leap in image denoising\nresearch. Most DL solutions for this task have chosen to put their efforts on\nthe denoiser's architecture while maximizing distortion performance. However,\ndistortion driven solutions lead to blurry results with sub-optimal perceptual\nquality, especially in immoderate noise levels. In this paper we propose a\ndifferent perspective, aiming to produce sharp and visually pleasing denoised\nimages that are still faithful to their clean sources. Formally, our goal is to\nachieve high perceptual quality with acceptable distortion. This is attained by\na stochastic denoiser that samples from the posterior distribution, trained as\na generator in the framework of conditional generative adversarial networks\n(CGAN). Contrary to distortion-based regularization terms that conflict with\nperceptual quality, we introduce to the CGAN objective a theoretically founded\npenalty term that does not force a distortion requirement on individual\nsamples, but rather on their mean. We showcase our proposed method with a novel\ndenoiser architecture that achieves the reformed denoising goal and produces\nvivid and diverse outcomes in immoderate noise levels.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 20:18:45 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 18:13:05 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ohayon", "Guy", ""], ["Adrai", "Theo", ""], ["Vaksman", "Gregory", ""], ["Elad", "Michael", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2103.04200", "submitter": "Ruwan Tennakoon", "authors": "Ruwan Tennakoon, David Suter, Erchuan Zhang, Tat-Jun Chin, Alireza\n  Bab-Hadiashar", "title": "Consensus Maximisation Using Influences of Monotone Boolean Functions", "comments": "To appear in CVPR 2021 as an ORAL paper. arXiv admin note: text\n  overlap with arXiv:2005.05490", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consensus maximisation (MaxCon), which is widely used for robust fitting in\ncomputer vision, aims to find the largest subset of data that fits the model\nwithin some tolerance level. In this paper, we outline the connection between\nMaxCon problem and the abstract problem of finding the maximum upper zero of a\nMonotone Boolean Function (MBF) defined over the Boolean Cube. Then, we link\nthe concept of influences (in a MBF) to the concept of outlier (in MaxCon) and\nshow that influences of points belonging to the largest structure in data would\ngenerally be smaller under certain conditions. Based on this observation, we\npresent an iterative algorithm to perform consensus maximisation. Results for\nboth synthetic and real visual data experiments show that the MBF based\nalgorithm is capable of generating a near optimal solution relatively quickly.\nThis is particularly important where there are large number of outliers (gross\nor pseudo) in the observed data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 22:01:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tennakoon", "Ruwan", ""], ["Suter", "David", ""], ["Zhang", "Erchuan", ""], ["Chin", "Tat-Jun", ""], ["Bab-Hadiashar", "Alireza", ""]]}, {"id": "2103.04207", "submitter": "Sharmin Majumder", "authors": "Sharmin Majumder, Nasser Kehtarnavaz", "title": "Multitasking Deep Learning Model for Detection of Five Stages of\n  Diabetic Retinopathy", "comments": "10 pages, 4 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a multitask deep learning model to detect all the five\nstages of diabetic retinopathy (DR) consisting of no DR, mild DR, moderate DR,\nsevere DR, and proliferate DR. This multitask model consists of one\nclassification model and one regression model, each with its own loss function.\nNoting that a higher severity level normally occurs after a lower severity\nlevel, this dependency is taken into consideration by concatenating the\nclassification and regression models. The regression model learns the\ninter-dependency between the stages and outputs a score corresponding to the\nseverity level of DR generating a higher score for a higher severity level.\nAfter training the regression model and the classification model separately,\nthe features extracted by these two models are concatenated and inputted to a\nmultilayer perceptron network to classify the five stages of DR. A modified\nSqueeze Excitation Densely Connected deep neural network is developed to\nimplement this multitasking approach. The developed multitask model is then\nused to detect the five stages of DR by examining the two large Kaggle datasets\nof APTOS and EyePACS. A multitasking transfer learning model based on Xception\nnetwork is also developed to evaluate the proposed approach by classifying DR\ninto five stages. It is found that the developed model achieves a weighted\nKappa score of 0.90 and 0.88 for the APTOS and EyePACS datasets, respectively,\nhigher than any existing methods for detection of the five stages of DR\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 23:06:46 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Majumder", "Sharmin", ""], ["Kehtarnavaz", "Nasser", ""]]}, {"id": "2103.04216", "submitter": "Chunhua Shen", "authors": "Wei Yin and Yifan Liu and Chunhua Shen", "title": "Virtual Normal: Enforcing Geometric Constraints for Accurate and Robust\n  Depth Prediction", "comments": "Fxied typos. Extended version of arXiv:1907.12209 Int. Conf. Comp.\n  Vis. (ICCV) 2019. Code is available at: https://git.io/Depth. arXiv admin\n  note: substantial text overlap with arXiv:2002.00569", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Monocular depth prediction plays a crucial role in understanding 3D scene\ngeometry. Although recent methods have achieved impressive progress in terms of\nevaluation metrics such as the pixel-wise relative error, most methods neglect\nthe geometric constraints in the 3D space. In this work, we show the importance\nof the high-order 3D geometric constraints for depth prediction. By designing a\nloss term that enforces a simple geometric constraint, namely, virtual normal\ndirections determined by randomly sampled three points in the reconstructed 3D\nspace, we significantly improve the accuracy and robustness of monocular depth\nestimation. Significantly, the virtual normal loss can not only improve the\nperformance of learning metric depth, but also disentangle the scale\ninformation and enrich the model with better shape information. Therefore, when\nnot having access to absolute metric depth training data, we can use virtual\nnormal to learn a robust affine-invariant depth generated on diverse scenes. In\nexperiments, We show state-of-the-art results of learning metric depth on NYU\nDepth-V2 and KITTI. From the high-quality predicted depth, we are now able to\nrecover good 3D structures of the scene such as the point cloud and surface\nnormal directly, eliminating the necessity of relying on additional models as\nwas previously done. To demonstrate the excellent generalizability of learning\naffine-invariant depth on diverse data with the virtual normal loss, we\nconstruct a large-scale and diverse dataset for training affine-invariant\ndepth, termed Diverse Scene Depth dataset (DiverseDepth), and test on five\ndatasets with the zero-shot test setting. Code is available at:\nhttps://git.io/Depth\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 00:08:21 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 12:34:46 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 05:59:19 GMT"}, {"version": "v4", "created": "Sat, 17 Apr 2021 07:23:41 GMT"}, {"version": "v5", "created": "Sun, 27 Jun 2021 02:26:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yin", "Wei", ""], ["Liu", "Yifan", ""], ["Shen", "Chunhua", ""]]}, {"id": "2103.04217", "submitter": "Anton Obukhov", "authors": "Anton Obukhov, Maxim Rakhuba, Alexander Liniger, Zhiwu Huang,\n  Stamatios Georgoulis, Dengxin Dai, Luc Van Gool", "title": "Spectral Tensor Train Parameterization of Deep Learning Layers", "comments": "Accepted at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study low-rank parameterizations of weight matrices with embedded spectral\nproperties in the Deep Learning context. The low-rank property leads to\nparameter efficiency and permits taking computational shortcuts when computing\nmappings. Spectral properties are often subject to constraints in optimization\nproblems, leading to better models and stability of optimization. We start by\nlooking at the compact SVD parameterization of weight matrices and identifying\nredundancy sources in the parameterization. We further apply the Tensor Train\n(TT) decomposition to the compact SVD components, and propose a non-redundant\ndifferentiable parameterization of fixed TT-rank tensor manifolds, termed the\nSpectral Tensor Train Parameterization (STTP). We demonstrate the effects of\nneural network compression in the image classification setting and both\ncompression and improved training stability in the generative adversarial\ntraining setting.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 00:15:44 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 18:43:07 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Obukhov", "Anton", ""], ["Rakhuba", "Maxim", ""], ["Liniger", "Alexander", ""], ["Huang", "Zhiwu", ""], ["Georgoulis", "Stamatios", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2103.04224", "submitter": "Vibashan Vs", "authors": "Vibashan VS, Vikram Gupta, Poojan Oza, Vishwanath A. Sindagi, Vishal\n  M. Patel", "title": "MeGA-CDA: Memory Guided Attention for Category-Aware Unsupervised Domain\n  Adaptive Object Detection", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for unsupervised domain adaptive object detection perform\nfeature alignment via adversarial training. While these methods achieve\nreasonable improvements in performance, they typically perform\ncategory-agnostic domain alignment, thereby resulting in negative transfer of\nfeatures. To overcome this issue, in this work, we attempt to incorporate\ncategory information into the domain adaptation process by proposing Memory\nGuided Attention for Category-Aware Domain Adaptation (MeGA-CDA). The proposed\nmethod consists of employing category-wise discriminators to ensure\ncategory-aware feature alignment for learning domain-invariant discriminative\nfeatures. However, since the category information is not available for the\ntarget samples, we propose to generate memory-guided category-specific\nattention maps which are then used to route the features appropriately to the\ncorresponding category discriminator. The proposed method is evaluated on\nseveral benchmark datasets and is shown to outperform existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 01:08:21 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 15:07:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["VS", "Vibashan", ""], ["Gupta", "Vikram", ""], ["Oza", "Poojan", ""], ["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2103.04233", "submitter": "TIanrui Guan", "authors": "Tianrui Guan, Divya Kothandaraman, Rohan Chandra and Dinesh Manocha", "title": "GANav: Group-wise Attention Network for Classifying Navigable Regions in\n  Unstructured Outdoor Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new learning-based method for identifying safe and navigable\nregions in off-road terrains and unstructured environments from RGB images. Our\napproach consists of classifying groups of terrain classes based on their\nnavigability levels using coarse-grained semantic segmentation. We propose a\nbottleneck transformer-based deep neural network architecture that uses a novel\ngroup-wise attention mechanism to distinguish between navigability levels of\ndifferent terrains.Our group-wise attention heads enable the network to\nexplicitly focus on the different groups and improve the accuracy. In addition,\nwe propose a dynamic weighted cross entropy loss function to handle the\nlong-tailed nature of the dataset. We show through extensive evaluations on the\nRUGD and RELLIS-3D datasets that our learning algorithm improves the accuracy\nof visual perception in off-road terrains for navigation. We compare our\napproach with prior work on these datasets and achieve an improvement over the\nstate-of-the-art mIoU by 6.74-39.1% on RUGD and 3.82-10.64% on RELLIS-3D.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 02:16:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Guan", "Tianrui", ""], ["Kothandaraman", "Divya", ""], ["Chandra", "Rohan", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2103.04235", "submitter": "Huimin Huang", "authors": "Huimin Huang, Ming Cai, Lanfen Lin, Jing Zheng, Xiongwei Mao, Xiaohan\n  Qian, Zhiyi Peng, Jianying Zhou, Yutaro Iwamoto, Xian-Hua Han, Yen-Wei Chen,\n  Ruofeng Tong", "title": "Graph-based Pyramid Global Context Reasoning with a Saliency-aware\n  Projection for COVID-19 Lung Infections Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) has rapidly spread in 2020, emerging a\nmass of studies for lung infection segmentation from CT images. Though many\nmethods have been proposed for this issue, it is a challenging task because of\ninfections of various size appearing in different lobe zones. To tackle these\nissues, we propose a Graph-based Pyramid Global Context Reasoning (Graph-PGCR)\nmodule, which is capable of modeling long-range dependencies among disjoint\ninfections as well as adapt size variation. We first incorporate graph\nconvolution to exploit long-term contextual information from multiple lobe\nzones. Different from previous average pooling or maximum object probability,\nwe propose a saliency-aware projection mechanism to pick up infection-related\npixels as a set of graph nodes. After graph reasoning, the relation-aware\nfeatures are reversed back to the original coordinate space for the down-stream\ntasks. We further construct multiple graphs with different sampling rates to\nhandle the size variation problem. To this end, distinct multi-scale long-range\ncontextual patterns can be captured. Our Graph-PGCR module is plug-and-play,\nwhich can be integrated into any architecture to improve its performance.\nExperiments demonstrated that the proposed method consistently boost the\nperformance of state-of-the-art backbone architectures on both of public and\nour private COVID-19 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 02:28:10 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Huang", "Huimin", ""], ["Cai", "Ming", ""], ["Lin", "Lanfen", ""], ["Zheng", "Jing", ""], ["Mao", "Xiongwei", ""], ["Qian", "Xiaohan", ""], ["Peng", "Zhiyi", ""], ["Zhou", "Jianying", ""], ["Iwamoto", "Yutaro", ""], ["Han", "Xian-Hua", ""], ["Chen", "Yen-Wei", ""], ["Tong", "Ruofeng", ""]]}, {"id": "2103.04243", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Ziteng Cui, Yifan Wu, Lin Gu, Tatsuya Harada", "title": "Estimating and Improving Fairness with Adversarial Learning", "comments": "12 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fairness and accountability are two essential pillars for trustworthy\nArtificial Intelligence (AI) in healthcare. However, the existing AI model may\nbe biased in its decision marking. To tackle this issue, we propose an\nadversarial multi-task training strategy to simultaneously mitigate and detect\nbias in the deep learning-based medical image analysis system. Specifically, we\npropose to add a discrimination module against bias and a critical module that\npredicts unfairness within the base classification model. We further impose an\northogonality regularization to force the two modules to be independent during\ntraining. Hence, we can keep these deep learning tasks distinct from one\nanother, and avoid collapsing them into a singular point on the manifold.\nThrough this adversarial training method, the data from the underprivileged\ngroup, which is vulnerable to bias because of attributes such as sex and skin\ntone, are transferred into a domain that is neutral relative to these\nattributes. Furthermore, the critical module can predict fairness scores for\nthe data with unknown sensitive attributes. We evaluate our framework on a\nlarge-scale public-available skin lesion dataset under various fairness\nevaluation metrics. The experiments demonstrate the effectiveness of our\nproposed method for estimating and improving fairness in the deep\nlearning-based medical image analysis system.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 03:10:32 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:16:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Cui", "Ziteng", ""], ["Wu", "Yifan", ""], ["Gu", "Lin", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2103.04256", "submitter": "Kexue Fu", "authors": "Kexue Fu and Shaolei Liu and Xiaoyuan Luo and Manning Wang", "title": "Robust Point Cloud Registration Framework Based on Deep Graph Matching", "comments": "Accepted to CVPR 2021. The code will be made publicly available at\n  https://github.com/fukexue/RGM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud registration is a fundamental problem in computer vision and\nrobotics. There has been extensive research in this area, but existing methods\nmeet great challenges in situations with a large proportion of outliers and\ntime constraints, but without good transformation initialization. Recently, a\nseries of learning-based algorithms have been introduced and show advantages in\nspeed. Many of them are based on correspondences between the two point clouds,\nso they do not rely on transformation initialization. However, these\nlearning-based methods are sensitive to outliers, which lead to more incorrect\ncorrespondences. In this paper, we propose a novel deep graph matchingbased\nframework for point cloud registration. Specifically, we first transform point\nclouds into graphs and extract deep features for each point. Then, we develop a\nmodule based on deep graph matching to calculate a soft correspondence matrix.\nBy using graph matching, not only the local geometry of each point but also its\nstructure and topology in a larger range are considered in establishing\ncorrespondences, so that more correct correspondences are found. We train the\nnetwork with a loss directly defined on the correspondences, and in the test\nstage the soft correspondences are transformed into hard one-to-one\ncorrespondences so that registration can be performed by singular value\ndecomposition. Furthermore, we introduce a transformer-based method to generate\nedges for graph construction, which further improves the quality of the\ncorrespondences. Extensive experiments on registering clean, noisy,\npartial-to-partial and unseen category point clouds show that the proposed\nmethod achieves state-of-the-art performance. The code will be made publicly\navailable at https://github.com/fukexue/RGM.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 04:20:29 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Fu", "Kexue", ""], ["Liu", "Shaolei", ""], ["Luo", "Xiaoyuan", ""], ["Wang", "Manning", ""]]}, {"id": "2103.04257", "submitter": "Guodong Wang", "authors": "Guodong Wang, Shumin Han, Errui Ding, Di Huang", "title": "Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly\n  Detection", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection is a challenging task and usually formulated as an\nunsupervised learning problem for the unexpectedness of anomalies. This paper\nproposes a simple yet powerful approach to this issue, which is implemented in\nthe student-teacher framework for its advantages but substantially extends it\nin terms of both accuracy and efficiency. Given a strong model pre-trained on\nimage classification as the teacher, we distill the knowledge into a single\nstudent network with the identical architecture to learn the distribution of\nanomaly-free images and this one-step transfer preserves the crucial clues as\nmuch as possible. Moreover, we integrate the multi-scale feature matching\nstrategy into the framework, and this hierarchical feature alignment enables\nthe student network to receive a mixture of multi-level knowledge from the\nfeature pyramid under better supervision, thus allowing to detect anomalies of\nvarious sizes. The difference between feature pyramids generated by the two\nnetworks serves as a scoring function indicating the probability of anomaly\noccurring. Due to such operations, our approach achieves accurate and fast\npixel-level anomaly detection. Very competitive results are delivered on three\nmajor benchmarks, significantly superior to the state of the art ones. In\naddition, it makes inferences at a very high speed (with 100 FPS for images of\nthe size at 256x256), at least dozens of times faster than the latest\ncounterparts.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 04:25:04 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 08:27:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wang", "Guodong", ""], ["Han", "Shumin", ""], ["Ding", "Errui", ""], ["Huang", "Di", ""]]}, {"id": "2103.04258", "submitter": "Yunxiang Li", "authors": "Yunxiang Li, Yifan Zhang, Yaqi Wang, Shuai Wang, Ruizi Peng, Kai Tang,\n  Qianni Zhang, Jun Wang, Qun Jin, Lingling Sun", "title": "High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on\n  Polynomial Curve Fitting with Landmark Detection", "comments": "There are some mistakes in the description of the maximum number of\n  the shortest distances algorithm (MNSDA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the most economical and routine auxiliary examination in the diagnosis of\nroot canal treatment, oral X-ray has been widely used by stomatologists. It is\nstill challenging to segment the tooth root with a blurry boundary for the\ntraditional image segmentation method. To this end, we propose a model for\nhigh-resolution segmentation based on polynomial curve fitting with landmark\ndetection (HS-PCL). It is based on detecting multiple landmarks evenly\ndistributed on the edge of the tooth root to fit a smooth polynomial curve as\nthe segmentation of the tooth root, thereby solving the problem of fuzzy edge.\nIn our model, a maximum number of the shortest distances algorithm (MNSDA) is\nproposed to automatically reduce the negative influence of the wrong landmarks\nwhich are detected incorrectly and deviate from the tooth root on the fitting\nresult. Our numerical experiments demonstrate that the proposed approach not\nonly reduces Hausdorff95 (HD95) by 33.9% and Average Surface Distance (ASD) by\n42.1% compared with the state-of-the-art method, but it also achieves excellent\nresults on the minute quantity of datasets, which greatly improves the\nfeasibility of automatic root canal therapy evaluation by medical image\ncomputing.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 04:28:09 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 05:26:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Yunxiang", ""], ["Zhang", "Yifan", ""], ["Wang", "Yaqi", ""], ["Wang", "Shuai", ""], ["Peng", "Ruizi", ""], ["Tang", "Kai", ""], ["Zhang", "Qianni", ""], ["Wang", "Jun", ""], ["Jin", "Qun", ""], ["Sun", "Lingling", ""]]}, {"id": "2103.04260", "submitter": "Dongxu Li", "authors": "Dongxu Li, Chenchen Xu, Kaihao Zhang, Xin Yu, Yiran Zhong, Wenqi Ren,\n  Hanna Suominen, Hongdong Li", "title": "ARVo: Learning All-Range Volumetric Correspondence for Video Deblurring", "comments": "Preprint for CVPR 2021 Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video deblurring models exploit consecutive frames to remove blurs from\ncamera shakes and object motions. In order to utilize neighboring sharp\npatches, typical methods rely mainly on homography or optical flows to\nspatially align neighboring blurry frames. However, such explicit approaches\nare less effective in the presence of fast motions with large pixel\ndisplacements. In this work, we propose a novel implicit method to learn\nspatial correspondence among blurry frames in the feature space. To construct\ndistant pixel correspondences, our model builds a correlation volume pyramid\namong all the pixel-pairs between neighboring frames. To enhance the features\nof the reference frame, we design a correlative aggregation module that\nmaximizes the pixel-pair correlations with its neighbors based on the volume\npyramid. Finally, we feed the aggregated features into a reconstruction module\nto obtain the restored frame. We design a generative adversarial paradigm to\noptimize the model progressively. Our proposed method is evaluated on the\nwidely-adopted DVD dataset, along with a newly collected High-Frame-Rate (1000\nfps) Dataset for Video Deblurring (HFR-DVD). Quantitative and qualitative\nexperiments show that our model performs favorably on both datasets against\nprevious state-of-the-art methods, confirming the benefit of modeling all-range\nspatial correspondence for video deblurring.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 04:33:13 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Dongxu", ""], ["Xu", "Chenchen", ""], ["Zhang", "Kaihao", ""], ["Yu", "Xin", ""], ["Zhong", "Yiran", ""], ["Ren", "Wenqi", ""], ["Suominen", "Hanna", ""], ["Li", "Hongdong", ""]]}, {"id": "2103.04263", "submitter": "Jiameng Pu", "authors": "Jiameng Pu, Neal Mangaokar, Lauren Kelly, Parantapa Bhattacharya,\n  Kavya Sundaram, Mobin Javed, Bolun Wang, Bimal Viswanath", "title": "Deepfake Videos in the Wild: Analysis and Detection", "comments": "Accepted to The Web Conference 2021; First two authors contributed\n  equally to this work; 12 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI-manipulated videos, commonly known as deepfakes, are an emerging problem.\nRecently, researchers in academia and industry have contributed several\n(self-created) benchmark deepfake datasets, and deepfake detection algorithms.\nHowever, little effort has gone towards understanding deepfake videos in the\nwild, leading to a limited understanding of the real-world applicability of\nresearch contributions in this space. Even if detection schemes are shown to\nperform well on existing datasets, it is unclear how well the methods\ngeneralize to real-world deepfakes. To bridge this gap in knowledge, we make\nthe following contributions: First, we collect and present the largest dataset\nof deepfake videos in the wild, containing 1,869 videos from YouTube and\nBilibili, and extract over 4.8M frames of content. Second, we present a\ncomprehensive analysis of the growth patterns, popularity, creators,\nmanipulation strategies, and production methods of deepfake content in the\nreal-world. Third, we systematically evaluate existing defenses using our new\ndataset, and observe that they are not ready for deployment in the real-world.\nFourth, we explore the potential for transfer learning schemes and\ncompetition-winning techniques to improve defenses.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 04:40:15 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 01:08:38 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Pu", "Jiameng", ""], ["Mangaokar", "Neal", ""], ["Kelly", "Lauren", ""], ["Bhattacharya", "Parantapa", ""], ["Sundaram", "Kavya", ""], ["Javed", "Mobin", ""], ["Wang", "Bolun", ""], ["Viswanath", "Bimal", ""]]}, {"id": "2103.04273", "submitter": "Chenyang Lei", "authors": "Chenyang Lei and Qifeng Chen", "title": "Robust Reflection Removal with Reflection-free Flash-only Cues", "comments": "Accepted to CVPR2021, code:\n  https://github.com/ChenyangLEI/flash-reflection-removal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective reflection-free cue for robust reflection\nremoval from a pair of flash and ambient (no-flash) images. The reflection-free\ncue exploits a flash-only image obtained by subtracting the ambient image from\nthe corresponding flash image in raw data space. The flash-only image is\nequivalent to an image taken in a dark environment with only a flash on. We\nobserve that this flash-only image is visually reflection-free, and thus it can\nprovide robust cues to infer the reflection in the ambient image. Since the\nflash-only image usually has artifacts, we further propose a dedicated model\nthat not only utilizes the reflection-free cue but also avoids introducing\nartifacts, which helps accurately estimate reflection and transmission. Our\nexperiments on real-world images with various types of reflection demonstrate\nthe effectiveness of our model with reflection-free flash-only cues: our model\noutperforms state-of-the-art reflection removal approaches by more than 5.23dB\nin PSNR, 0.04 in SSIM, and 0.068 in LPIPS. Our source code and dataset are\npublicly available at {github.com/ChenyangLEI/flash-reflection-removal}.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 05:27:43 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 03:46:30 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lei", "Chenyang", ""], ["Chen", "Qifeng", ""]]}, {"id": "2103.04278", "submitter": "Haoyu Yang", "authors": "Haoyu Yang, Shuhe Li, Bei Yu", "title": "Routing Towards Discriminative Power of Class Capsules", "comments": "6 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks are recently proposed as an alternative to modern neural\nnetwork architectures. Neurons are replaced with capsule units that represent\nspecific features or entities with normalized vectors or matrices. The\nactivation of lower layer capsules affects the behavior of the following\ncapsules via routing links that are constructed during training via certain\nrouting algorithms. We discuss the routing-by-agreement scheme in dynamic\nrouting algorithm which, in certain cases, leads the networks away from\noptimality. To obtain better and faster convergence, we propose a routing\nalgorithm that incorporates a regularized quadratic programming problem which\ncan be solved efficiently. Particularly, the proposed routing algorithm targets\ndirectly on the discriminative power of class capsules making the correct\ndecision on input instances. We conduct experiments on MNIST, MNIST-Fashion,\nand CIFAR-10 and show competitive classification results compared to existing\ncapsule networks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 05:49:38 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yang", "Haoyu", ""], ["Li", "Shuhe", ""], ["Yu", "Bei", ""]]}, {"id": "2103.04279", "submitter": "Saif Mahmud", "authors": "M Tanjid Hasan Tonmoy, Saif Mahmud, A K M Mahbubur Rahman, M Ashraful\n  Amin, and Amin Ahsan Ali", "title": "Hierarchical Self Attention Based Autoencoder for Open-Set Human\n  Activity Recognition", "comments": "Accepted for publication in 25th Pacific-Asia Conference on Knowledge\n  Discovery and Data Mining (PAKDD-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wearable sensor based human activity recognition is a challenging problem due\nto difficulty in modeling spatial and temporal dependencies of sensor signals.\nRecognition models in closed-set assumption are forced to yield members of\nknown activity classes as prediction. However, activity recognition models can\nencounter an unseen activity due to body-worn sensor malfunction or disability\nof the subject performing the activities. This problem can be addressed through\nmodeling solution according to the assumption of open-set recognition. Hence,\nthe proposed self attention based approach combines data hierarchically from\ndifferent sensor placements across time to classify closed-set activities and\nit obtains notable performance improvement over state-of-the-art models on five\npublicly available datasets. The decoder in this autoencoder architecture\nincorporates self-attention based feature representations from encoder to\ndetect unseen activity classes in open-set recognition setting. Furthermore,\nattention maps generated by the hierarchical model demonstrate explainable\nselection of features in activity recognition. We conduct extensive leave one\nsubject out validation experiments that indicate significantly improved\nrobustness to noise and subject specific variability in body-worn sensor\nsignals. The source code is available at:\ngithub.com/saif-mahmud/hierarchical-attention-HAR\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 06:21:18 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tonmoy", "M Tanjid Hasan", ""], ["Mahmud", "Saif", ""], ["Rahman", "A K M Mahbubur", ""], ["Amin", "M Ashraful", ""], ["Ali", "Amin Ahsan", ""]]}, {"id": "2103.04285", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Zilong Zheng, Xiaolin Fang, Song-Chun Zhu, Ying Nian Wu", "title": "Learning Cycle-Consistent Cooperative Networks via Alternating MCMC\n  Teaching for Unsupervised Cross-Domain Translation", "comments": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the unsupervised cross-domain translation problem by\nproposing a generative framework, in which the probability distribution of each\ndomain is represented by a generative cooperative network that consists of an\nenergy-based model and a latent variable model. The use of generative\ncooperative network enables maximum likelihood learning of the domain model by\nMCMC teaching, where the energy-based model seeks to fit the data distribution\nof domain and distills its knowledge to the latent variable model via MCMC.\nSpecifically, in the MCMC teaching process, the latent variable model\nparameterized by an encoder-decoder maps examples from the source domain to the\ntarget domain, while the energy-based model further refines the mapped results\nby Langevin revision such that the revised results match to the examples in the\ntarget domain in terms of the statistical properties, which are defined by the\nlearned energy function. For the purpose of building up a correspondence\nbetween two unpaired domains, the proposed framework simultaneously learns a\npair of cooperative networks with cycle consistency, accounting for a two-way\ntranslation between two domains, by alternating MCMC teaching. Experiments show\nthat the proposed framework is useful for unsupervised image-to-image\ntranslation and unpaired image sequence translation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 07:09:38 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Xie", "Jianwen", ""], ["Zheng", "Zilong", ""], ["Fang", "Xiaolin", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "2103.04286", "submitter": "Hui Li", "authors": "Hui Li, Xiao-Jun Wu, Josef Kittler", "title": "RFN-Nest: An end-to-end residual fusion network for infrared and visible\n  images", "comments": "Accepted by Information Fusion. 17 pages, 18 figures, 8 tables", "journal-ref": null, "doi": "10.1016/j.inffus.2021.02.023", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the image fusion field, the design of deep learning-based fusion methods\nis far from routine. It is invariably fusion-task specific and requires a\ncareful consideration. The most difficult part of the design is to choose an\nappropriate strategy to generate the fused image for a specific task in hand.\nThus, devising learnable fusion strategy is a very challenging problem in the\ncommunity of image fusion. To address this problem, a novel end-to-end fusion\nnetwork architecture (RFN-Nest) is developed for infrared and visible image\nfusion. We propose a residual fusion network (RFN) which is based on a residual\narchitecture to replace the traditional fusion approach. A novel\ndetail-preserving loss function, and a feature enhancing loss function are\nproposed to train RFN. The fusion model learning is accomplished by a novel\ntwo-stage training strategy. In the first stage, we train an auto-encoder based\non an innovative nest connection (Nest) concept. Next, the RFN is trained using\nthe proposed loss functions. The experimental results on public domain data\nsets show that, compared with the existing methods, our end-to-end fusion\nnetwork delivers a better performance than the state-of-the-art methods in both\nsubjective and objective evaluation. The code of our fusion method is available\nat https://github.com/hli1221/imagefusion-rfn-nest\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 07:29:50 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 06:16:15 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Li", "Hui", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "2103.04297", "submitter": "Zexi Chen", "authors": "Zexi Chen, Zheyuan Huang, Yunkai Wang, Xuecheng Xu, Yue Wang, Rong\n  Xiong", "title": "Learn to Differ: Sim2Real Small Defection Segmentation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies on deep-learning-based small defection segmentation approaches\nare trained in specific settings and tend to be limited by fixed context.\nThroughout the training, the network inevitably learns the representation of\nthe background of the training data before figuring out the defection. They\nunderperform in the inference stage once the context changed and can only be\nsolved by training in every new setting. This eventually leads to the\nlimitation in practical robotic applications where contexts keep varying. To\ncope with this, instead of training a network context by context and hoping it\nto generalize, why not stop misleading it with any limited context and start\ntraining it with pure simulation? In this paper, we propose the network SSDS\nthat learns a way of distinguishing small defections between two images\nregardless of the context, so that the network can be trained once for all. A\nsmall defection detection layer utilizing the pose sensitivity of phase\ncorrelation between images is introduced and is followed by an outlier masking\nlayer. The network is trained on randomly generated simulated data with simple\nshapes and is generalized across the real world. Finally, SSDS is validated on\nreal-world collected data and demonstrates the ability that even when trained\nin cheap simulation, SSDS can still find small defections in the real world\nshowing the effectiveness and its potential for practical applications.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 08:25:56 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Zexi", ""], ["Huang", "Zheyuan", ""], ["Wang", "Yunkai", ""], ["Xu", "Xuecheng", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2103.04302", "submitter": "Jinyu Tian", "authors": "Jinyu Tian, Jiantao Zhou, Yuanman Li, Jia Duan", "title": "Detecting Adversarial Examples from Sensitivity Inconsistency of\n  Spatial-Transform Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep neural networks (DNNs) have been shown to be vulnerable against\nadversarial examples (AEs), which are maliciously designed to cause dramatic\nmodel output errors. In this work, we reveal that normal examples (NEs) are\ninsensitive to the fluctuations occurring at the highly-curved region of the\ndecision boundary, while AEs typically designed over one single domain (mostly\nspatial domain) exhibit exorbitant sensitivity on such fluctuations. This\nphenomenon motivates us to design another classifier (called dual classifier)\nwith transformed decision boundary, which can be collaboratively used with the\noriginal classifier (called primal classifier) to detect AEs, by virtue of the\nsensitivity inconsistency. When comparing with the state-of-the-art algorithms\nbased on Local Intrinsic Dimensionality (LID), Mahalanobis Distance (MD), and\nFeature Squeezing (FS), our proposed Sensitivity Inconsistency Detector (SID)\nachieves improved AE detection performance and superior generalization\ncapabilities, especially in the challenging cases where the adversarial\nperturbation levels are small. Intensive experimental results on ResNet and VGG\nvalidate the superiority of the proposed SID.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 08:43:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tian", "Jinyu", ""], ["Zhou", "Jiantao", ""], ["Li", "Yuanman", ""], ["Duan", "Jia", ""]]}, {"id": "2103.04316", "submitter": "Hyungtae Lim", "authors": "Hyungtae Lim, Sungwon Hwang, and Hyun Myung", "title": "ERASOR: Egocentric Ratio of Pseudo Occupancy-based Dynamic Object\n  Removal for Static 3D Point Cloud Map Building", "comments": "9 pages, 9 figures, RA-L with ICRA 2021 accepted", "journal-ref": null, "doi": "10.1109/LRA.2021.3061363", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scan data of urban environments often include representations of dynamic\nobjects, such as vehicles, pedestrians, and so forth. However, when it comes to\nconstructing a 3D point cloud map with sequential accumulations of the scan\ndata, the dynamic objects often leave unwanted traces in the map. These traces\nof dynamic objects act as obstacles and thus impede mobile vehicles from\nachieving good localization and navigation performances. To tackle the problem,\nthis paper presents a novel static map building method called ERASOR,\nEgocentric RAtio of pSeudo Occupancy-based dynamic object Removal, which is\nfast and robust to motion ambiguity. Our approach directs its attention to the\nnature of most dynamic objects in urban environments being inevitably in\ncontact with the ground. Accordingly, we propose the novel concept called\npseudo occupancy to express the occupancy of unit space and then discriminate\nspaces of varying occupancy. Finally, Region-wise Ground Plane Fitting (R-GPF)\nis adopted to distinguish static points from dynamic points within the\ncandidate bins that potentially contain dynamic points. As experimentally\nverified on SemanticKITTI, our proposed method yields promising performance\nagainst state-of-the-art methods overcoming the limitations of existing ray\ntracing-based and visibility-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 10:29:07 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lim", "Hyungtae", ""], ["Hwang", "Sungwon", ""], ["Myung", "Hyun", ""]]}, {"id": "2103.04329", "submitter": "Zaidao Wen", "authors": "Zaidao Wen, Jiaxiang Liu, Zhunga Liu, Quan Pan", "title": "Pose Discrepancy Spatial Transformer Based Feature Disentangling for\n  Partial Aspect Angles SAR Target Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This letter presents a novel framework termed DistSTN for the task of\nsynthetic aperture radar (SAR) automatic target recognition (ATR). In contrast\nto the conventional SAR ATR algorithms, DistSTN considers a more challenging\npractical scenario for non-cooperative targets whose aspect angles for training\nare incomplete and limited in a partial range while those of testing samples\nare unlimited. To address this issue, instead of learning the pose invariant\nfeatures, DistSTN newly involves an elaborated feature disentangling model to\nseparate the learned pose factors of a SAR target from the identity ones so\nthat they can independently control the representation process of the target\nimage. To disentangle the explainable pose factors, we develop a pose\ndiscrepancy spatial transformer module in DistSTN to characterize the intrinsic\ntransformation between the factors of two different targets with an explicit\ngeometric model. Furthermore, DistSTN develops an amortized inference scheme\nthat enables efficient feature extraction and recognition using an\nencoder-decoder mechanism. Experimental results with the moving and stationary\ntarget acquisition and recognition (MSTAR) benchmark demonstrate the\neffectiveness of our proposed approach. Compared with the other ATR algorithms,\nDistSTN can achieve higher recognition accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 11:47:34 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wen", "Zaidao", ""], ["Liu", "Jiaxiang", ""], ["Liu", "Zhunga", ""], ["Pan", "Quan", ""]]}, {"id": "2103.04331", "submitter": "David Peer", "authors": "David Peer, Sebastian Stabinger, Antonio Rodriguez-Sanchez", "title": "Auto-tuning of Deep Neural Networks by Conflicting Layer Removal", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.02956", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing neural network architectures is a challenging task and knowing\nwhich specific layers of a model must be adapted to improve the performance is\nalmost a mystery. In this paper, we introduce a novel methodology to identify\nlayers that decrease the test accuracy of trained models. Conflicting layers\nare detected as early as the beginning of training. In the worst-case scenario,\nwe prove that such a layer could lead to a network that cannot be trained at\nall. A theoretical analysis is provided on what is the origin of those layers\nthat result in a lower overall network performance, which is complemented by\nour extensive empirical evaluation. More precisely, we identified those layers\nthat worsen the performance because they would produce what we name conflicting\ntraining bundles. We will show that around 60% of the layers of trained\nresidual networks can be completely removed from the architecture with no\nsignificant increase in the test-error. We will further present a novel\nneural-architecture-search (NAS) algorithm that identifies conflicting layers\nat the beginning of the training. Architectures found by our auto-tuning\nalgorithm achieve competitive accuracy values when compared against more\ncomplex state-of-the-art architectures, while drastically reducing memory\nconsumption and inference time for different computer vision tasks. The source\ncode is available on https://github.com/peerdavid/conflicting-bundles\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 11:51:55 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Peer", "David", ""], ["Stabinger", "Sebastian", ""], ["Rodriguez-Sanchez", "Antonio", ""]]}, {"id": "2103.04337", "submitter": "Pingping Zhang Dr", "authors": "Xuehu Liu and Pingping Zhang and Chenyang Yu and Huchuan Lu and\n  Xiaoyun Yang", "title": "Watching You: Global-guided Reciprocal Learning for Video-based Person\n  Re-identification", "comments": "This is the camera-ready version of our Poster paper in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (Re-ID) aims to automatically retrieve\nvideo sequences of the same person under non-overlapping cameras. To achieve\nthis goal, it is the key to fully utilize abundant spatial and temporal cues in\nvideos. Existing methods usually focus on the most conspicuous image regions,\nthus they may easily miss out fine-grained clues due to the person varieties in\nimage sequences. To address above issues, in this paper, we propose a novel\nGlobal-guided Reciprocal Learning (GRL) framework for video-based person Re-ID.\nSpecifically, we first propose a Global-guided Correlation Estimation (GCE) to\ngenerate feature correlation maps of local features and global features, which\nhelp to localize the high- and low-correlation regions for identifying the same\nperson. After that, the discriminative features are disentangled into\nhigh-correlation features and low-correlation features under the guidance of\nthe global representations. Moreover, a novel Temporal Reciprocal Learning\n(TRL) mechanism is designed to sequentially enhance the high-correlation\nsemantic information and accumulate the low-correlation sub-critical clues.\nExtensive experiments are conducted on three public benchmarks. The\nexperimental results indicate that our approach can achieve better performance\nthan other state-of-the-art approaches. The code is released at\nhttps://github.com/flysnowtiger/GRL.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 12:27:42 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 05:17:58 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Liu", "Xuehu", ""], ["Zhang", "Pingping", ""], ["Yu", "Chenyang", ""], ["Lu", "Huchuan", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "2103.04351", "submitter": "David Hoeller", "authors": "David Hoeller, Lorenz Wellhausen, Farbod Farshidian, Marco Hutter", "title": "Learning a State Representation and Navigation in Cluttered and Dynamic\n  Environments", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": "IEEE Robotics and Automation Letters 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a learning-based pipeline to realise local\nnavigation with a quadrupedal robot in cluttered environments with static and\ndynamic obstacles. Given high-level navigation commands, the robot is able to\nsafely locomote to a target location based on frames from a depth camera\nwithout any explicit mapping of the environment. First, the sequence of images\nand the current trajectory of the camera are fused to form a model of the world\nusing state representation learning. The output of this lightweight module is\nthen directly fed into a target-reaching and obstacle-avoiding policy trained\nwith reinforcement learning. We show that decoupling the pipeline into these\ncomponents results in a sample efficient policy learning stage that can be\nfully trained in simulation in just a dozen minutes. The key part is the state\nrepresentation, which is trained to not only estimate the hidden state of the\nworld in an unsupervised fashion, but also helps bridging the reality gap,\nenabling successful sim-to-real transfer. In our experiments with the\nquadrupedal robot ANYmal in simulation and in reality, we show that our system\ncan handle noisy depth images, avoid dynamic obstacles unseen during training,\nand is endowed with local spatial awareness.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 13:19:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hoeller", "David", ""], ["Wellhausen", "Lorenz", ""], ["Farshidian", "Farbod", ""], ["Hutter", "Marco", ""]]}, {"id": "2103.04354", "submitter": "Zhenjie Tang", "authors": "Enhai Liu, Zhenjie Tang, Bin Pan, Zhenwei Shi", "title": "Spatial-Spectral Feedback Network for Super-Resolution of Hyperspectral\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, single gray/RGB image super-resolution (SR) methods based on deep\nlearning have achieved great success. However, there are two obstacles to limit\ntechnical development in the single hyperspectral image super-resolution. One\nis the high-dimensional and complex spectral patterns in hyperspectral image,\nwhich make it difficult to explore spatial information and spectral information\namong bands simultaneously. The other is that the number of available\nhyperspectral training samples is extremely small, which can easily lead to\noverfitting when training a deep neural network. To address these issues, in\nthis paper, we propose a novel Spatial-Spectral Feedback Network (SSFN) to\nrefine low-level representations among local spectral bands with high-level\ninformation from global spectral bands. It will not only alleviate the\ndifficulty in feature extraction due to high dimensional of hyperspectral data,\nbut also make the training process more stable. Specifically, we use hidden\nstates in an RNN with finite unfoldings to achieve such feedback manner. To\nexploit the spatial and spectral prior, a Spatial-Spectral Feedback Block\n(SSFB) is designed to handle the feedback connections and generate powerful\nhigh-level representations. The proposed SSFN comes with a early predictions\nand can reconstruct the final high-resolution hyperspectral image step by step.\nExtensive experimental results on three benchmark datasets demonstrate that the\nproposed SSFN achieves superior performance in comparison with the\nstate-of-the-art methods. The source code is available at\nhttps://github.com/tangzhenjie/SSFN.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 13:28:48 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Liu", "Enhai", ""], ["Tang", "Zhenjie", ""], ["Pan", "Bin", ""], ["Shi", "Zhenwei", ""]]}, {"id": "2103.04357", "submitter": "Lei Sun", "authors": "Lei Sun", "title": "IRON: Invariant-based Highly Robust Point Cloud Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we present IRON (Invariant-based global Robust estimation and\nOptimizatioN), a non-minimal and highly robust solution for point cloud\nregistration with a great number of outliers among the correspondences. To\nrealize this, we decouple the registration problem into the estimation of\nscale, rotation and translation, respectively. Our first contribution is to\npropose RANSIC (RANdom Samples with Invariant Compatibility), which employs the\ninvariant compatibility to seek inliers from random samples and robustly\nestimates the scale between two sets of point clouds in the meantime. Once the\nscale is estimated, our second contribution is to relax the non-convex global\nregistration problem into a convex Semi-Definite Program (SDP) in a certifiable\nway using Sum-of-Squares (SOS) Relaxation and show that the relaxation is\ntight. For robust estimation, we further propose RT-GNC (Rough Trimming and\nGraduated Non-Convexity), a global outlier rejection heuristic having better\nrobustness and time-efficiency than traditional GNC, as our third contribution.\nWith these contributions, we can render our registration algorithm, IRON.\nThrough experiments over real datasets, we show that IRON is efficient, highly\naccurate and robust against as many as 99% outliers whether the scale is known\nor unknown, outperforming the existing state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 13:46:56 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 07:46:14 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Sun", "Lei", ""]]}, {"id": "2103.04379", "submitter": "Nontawat Tritrong", "authors": "Nontawat Tritrong, Pitchaporn Rewatbowornwong, Supasorn Suwajanakorn", "title": "Repurposing GANs for One-shot Semantic Part Segmentation", "comments": "CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While GANs have shown success in realistic image generation, the idea of\nusing GANs for other tasks unrelated to synthesis is underexplored. Do GANs\nlearn meaningful structural parts of objects during their attempt to reproduce\nthose objects? In this work, we test this hypothesis and propose a simple and\neffective approach based on GANs for semantic part segmentation that requires\nas few as one label example along with an unlabeled dataset. Our key idea is to\nleverage a trained GAN to extract pixel-wise representation from the input\nimage and use it as feature vectors for a segmentation network. Our experiments\ndemonstrate that GANs representation is \"readily discriminative\" and produces\nsurprisingly good results that are comparable to those from supervised\nbaselines trained with significantly more labels. We believe this novel\nrepurposing of GANs underlies a new class of unsupervised representation\nlearning that is applicable to many other tasks. More results are available at\nhttps://repurposegans.github.io/.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 15:40:47 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 02:58:21 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 11:44:21 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 15:13:13 GMT"}, {"version": "v5", "created": "Mon, 5 Jul 2021 14:25:15 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tritrong", "Nontawat", ""], ["Rewatbowornwong", "Pitchaporn", ""], ["Suwajanakorn", "Supasorn", ""]]}, {"id": "2103.04384", "submitter": "Coloma Ballester", "authors": "Patricia Vitoria and Coloma Ballester", "title": "Automatic Flare Spot Artifact Detection and Removal in Photographs", "comments": "Journal of Mathematical Imaging and Vision, 2019", "journal-ref": "Journal of Mathematical Imaging and Vision, 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Flare spot is one type of flare artifact caused by a number of conditions,\nfrequently provoked by one or more high-luminance sources within or close to\nthe camera field of view. When light rays coming from a high-luminance source\nreach the front element of a camera, it can produce intra-reflections within\ncamera elements that emerge at the film plane forming non-image information or\nflare on the captured image. Even though preventive mechanisms are used,\nartifacts can appear. In this paper, we propose a robust computational method\nto automatically detect and remove flare spot artifacts. Our contribution is\nthreefold: firstly, we propose a characterization which is based on intrinsic\nproperties that a flare spot is likely to satisfy; secondly, we define a new\nconfidence measure able to select flare spots among the candidates; and,\nfinally, a method to accurately determine the flare region is given. Then, the\ndetected artifacts are removed by using exemplar-based inpainting. We show that\nour algorithm achieve top-tier quantitative and qualitative performance.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 15:51:49 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Vitoria", "Patricia", ""], ["Ballester", "Coloma", ""]]}, {"id": "2103.04400", "submitter": "JeongHun Baek", "authors": "Jeonghun Baek, Yusuke Matsui, Kiyoharu Aizawa", "title": "What If We Only Use Real Datasets for Scene Text Recognition? Toward\n  Scene Text Recognition With Fewer Labels", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Scene text recognition (STR) task has a common practice: All state-of-the-art\nSTR models are trained on large synthetic data. In contrast to this practice,\ntraining STR models only on fewer real labels (STR with fewer labels) is\nimportant when we have to train STR models without synthetic data: for\nhandwritten or artistic texts that are difficult to generate synthetically and\nfor languages other than English for which we do not always have synthetic\ndata. However, there has been implicit common knowledge that training STR\nmodels on real data is nearly impossible because real data is insufficient. We\nconsider that this common knowledge has obstructed the study of STR with fewer\nlabels. In this work, we would like to reactivate STR with fewer labels by\ndisproving the common knowledge. We consolidate recently accumulated public\nreal data and show that we can train STR models satisfactorily only with real\nlabeled data. Subsequently, we find simple data augmentation to fully exploit\nreal data. Furthermore, we improve the models by collecting unlabeled data and\nintroducing semi- and self-supervised methods. As a result, we obtain a\ncompetitive model to state-of-the-art methods. To the best of our knowledge,\nthis is the first study that 1) shows sufficient performance by only using real\nlabels and 2) introduces semi- and self-supervised methods into STR with fewer\nlabels. Our code and data are available:\nhttps://github.com/ku21fan/STR-Fewer-Labels\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 17:05:54 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 12:53:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Baek", "Jeonghun", ""], ["Matsui", "Yusuke", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "2103.04421", "submitter": "Xin Yuan", "authors": "Xin Yuan and David J. Brady and Aggelos K. Katsaggelos", "title": "Snapshot Compressive Imaging: Principle, Implementation, Theory,\n  Algorithms and Applications", "comments": "Extension of X. Yuan, D. J. Brady and A. K. Katsaggelos, \"Snapshot\n  Compressive Imaging: Theory, Algorithms, and Applications,\" in IEEE Signal\n  Processing Magazine, vol. 38, no. 2, pp. 65-88, March 2021, doi:\n  10.1109/MSP.2020.3023869", "journal-ref": "in IEEE Signal Processing Magazine, vol. 38, no. 2, pp. 65-88,\n  March 2021", "doi": "10.1109/MSP.2020.3023869.", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Capturing high-dimensional (HD) data is a long-term challenge in signal\nprocessing and related fields. Snapshot compressive imaging (SCI) uses a\ntwo-dimensional (2D) detector to capture HD ($\\ge3$D) data in a {\\em snapshot}\nmeasurement. Via novel optical designs, the 2D detector samples the HD data in\na {\\em compressive} manner; following this, algorithms are employed to\nreconstruct the desired HD data-cube. SCI has been used in hyperspectral\nimaging, video, holography, tomography, focal depth imaging, polarization\nimaging, microscopy, \\etc.~Though the hardware has been investigated for more\nthan a decade, the theoretical guarantees have only recently been derived.\nInspired by deep learning, various deep neural networks have also been\ndeveloped to reconstruct the HD data-cube in spectral SCI and video SCI. This\narticle reviews recent advances in SCI hardware, theory and algorithms,\nincluding both optimization-based and deep-learning-based algorithms. Diverse\napplications and the outlook of SCI are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 18:31:47 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yuan", "Xin", ""], ["Brady", "David J.", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "2103.04430", "submitter": "Chen Chen", "authors": "Wenxuan Wang, Chen Chen, Meng Ding, Jiangyun Li, Hong Yu, Sen Zha", "title": "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer, which can benefit from global (long-range) information modeling\nusing self-attention mechanisms, has been successful in natural language\nprocessing and 2D image classification recently. However, both local and global\nfeatures are crucial for dense prediction tasks, especially for 3D medical\nimage segmentation. In this paper, we for the first time exploit Transformer in\n3D CNN for MRI Brain Tumor Segmentation and propose a novel network named\nTransBTS based on the encoder-decoder structure. To capture the local 3D\ncontext information, the encoder first utilizes 3D CNN to extract the\nvolumetric spatial feature maps. Meanwhile, the feature maps are reformed\nelaborately for tokens that are fed into Transformer for global feature\nmodeling. The decoder leverages the features embedded by Transformer and\nperforms progressive upsampling to predict the detailed segmentation map.\nExtensive experimental results on both BraTS 2019 and 2020 datasets show that\nTransBTS achieves comparable or higher results than previous state-of-the-art\n3D methods for brain tumor segmentation on 3D MRI scans. The source code is\navailable at https://github.com/Wenxuan-1119/TransBTS\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 19:12:14 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 23:58:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Wenxuan", ""], ["Chen", "Chen", ""], ["Ding", "Meng", ""], ["Li", "Jiangyun", ""], ["Yu", "Hong", ""], ["Zha", "Sen", ""]]}, {"id": "2103.04450", "submitter": "Shengcao Cao", "authors": "Shengcao Cao, Xiaofang Wang, Kris Kitani", "title": "Efficient Model Performance Estimation via Feature Histories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important step in the task of neural network design, such as\nhyper-parameter optimization (HPO) or neural architecture search (NAS), is the\nevaluation of a candidate model's performance. Given fixed computational\nresources, one can either invest more time training each model to obtain more\naccurate estimates of final performance, or spend more time exploring a greater\nvariety of models in the configuration space. In this work, we aim to optimize\nthis exploration-exploitation trade-off in the context of HPO and NAS for image\nclassification by accurately approximating a model's maximal performance early\nin the training process. In contrast to recent accelerated NAS methods\ncustomized for certain search spaces, e.g., requiring the search space to be\ndifferentiable, our method is flexible and imposes almost no constraints on the\nsearch space. Our method uses the evolution history of features of a network\nduring the early stages of training to build a proxy classifier that matches\nthe peak performance of the network under consideration. We show that our\nmethod can be combined with multiple search algorithms to find better solutions\nto a wide range of tasks in HPO and NAS. Using a sampling-based search\nalgorithm and parallel computing, our method can find an architecture which is\nbetter than DARTS and with an 80% reduction in wall-clock search time.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 20:41:57 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cao", "Shengcao", ""], ["Wang", "Xiaofang", ""], ["Kitani", "Kris", ""]]}, {"id": "2103.04493", "submitter": "Qiaojun Feng", "authors": "Qiaojun Feng, Yue Meng, Mo Shan, Nikolay Atanasov", "title": "Localization and Mapping using Instance-specific Mesh Models", "comments": "8 pages, 9 figures", "journal-ref": "2019 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Macau, China, 2019, pp. 4985-4991", "doi": "10.1109/IROS40897.2019.8967662", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on building semantic maps, containing object poses and\nshapes, using a monocular camera. This is an important problem because robots\nneed rich understanding of geometry and context if they are to shape the future\nof transportation, construction, and agriculture. Our contribution is an\ninstance-specific mesh model of object shape that can be optimized online based\non semantic information extracted from camera images. Multi-view constraints on\nthe object shape are obtained by detecting objects and extracting\ncategory-specific keypoints and segmentation masks. We show that the errors\nbetween projections of the mesh model and the observed keypoints and masks can\nbe differentiated in order to obtain accurate instance-specific object shapes.\nWe evaluate the performance of the proposed approach in simulation and on the\nKITTI dataset by building maps of car poses and shapes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 00:24:23 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Feng", "Qiaojun", ""], ["Meng", "Yue", ""], ["Shan", "Mo", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2103.04494", "submitter": "Qiaojun Feng", "authors": "Qiaojun Feng, Nikolay Atanasov", "title": "Fully Convolutional Geometric Features for Category-level Object\n  Alignment", "comments": "7 pages, 9 figures", "journal-ref": "2020 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Las Vegas, NV, USA, 2020, pp. 8492-8498", "doi": "10.1109/IROS45743.2020.9341550", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on pose registration of different object instances from\nthe same category. This is required in online object mapping because object\ninstances detected at test time usually differ from the training instances. Our\napproach transforms instances of the same category to a normalized canonical\ncoordinate frame and uses metric learning to train fully convolutional\ngeometric features. The resulting model is able to generate pairs of matching\npoints between the instances, allowing category-level registration. Evaluation\non both synthetic and real-world data shows that our method provides robust\nfeatures, leading to accurate alignment of instances with different shapes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 00:31:56 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Feng", "Qiaojun", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2103.04503", "submitter": "Cheng Zou", "authors": "Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu Zhao, Boxun Li,\n  Chenguang Zhang, Chi Zhang, Yichen Wei, Jian Sun", "title": "End-to-End Human Object Interaction Detection with HOI Transformer", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose HOI Transformer to tackle human object interaction (HOI) detection\nin an end-to-end manner. Current approaches either decouple HOI task into\nseparated stages of object detection and interaction classification or\nintroduce surrogate interaction problem. In contrast, our method, named HOI\nTransformer, streamlines the HOI pipeline by eliminating the need for many\nhand-designed components. HOI Transformer reasons about the relations of\nobjects and humans from global image context and directly predicts HOI\ninstances in parallel. A quintuple matching loss is introduced to force HOI\npredictions in a unified way. Our method is conceptually much simpler and\ndemonstrates improved accuracy. Without bells and whistles, HOI Transformer\nachieves $26.61\\% $ $ AP $ on HICO-DET and $52.9\\%$ $AP_{role}$ on V-COCO,\nsurpassing previous methods with the advantage of being much simpler. We hope\nour approach will serve as a simple and effective alternative for HOI tasks.\nCode is available at https://github.com/bbepoch/HoiTransformer .\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 01:31:19 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zou", "Cheng", ""], ["Wang", "Bohan", ""], ["Hu", "Yue", ""], ["Liu", "Junqi", ""], ["Wu", "Qian", ""], ["Zhao", "Yu", ""], ["Li", "Boxun", ""], ["Zhang", "Chenguang", ""], ["Zhang", "Chi", ""], ["Wei", "Yichen", ""], ["Sun", "Jian", ""]]}, {"id": "2103.04507", "submitter": "Tingting Liang", "authors": "Tingting Liang, Yongtao Wang, Zhi Tang, Guosheng Hu, Haibin Ling", "title": "OPANAS: One-Shot Path Aggregation Network Architecture Search for Object\n  Detection", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recently, neural architecture search (NAS) has been exploited to design\nfeature pyramid networks (FPNs) and achieved promising results for visual\nobject detection. Encouraged by the success, we propose a novel One-Shot Path\nAggregation Network Architecture Search (OPANAS) algorithm, which significantly\nimproves both searching efficiency and detection accuracy. Specifically, we\nfirst introduce six heterogeneous information paths to build our search space,\nnamely top-down, bottom-up, fusing-splitting, scale-equalizing, skip-connect\nand none. Second, we propose a novel search space of FPNs, in which each FPN\ncandidate is represented by a densely-connected directed acyclic graph (each\nnode is a feature pyramid and each edge is one of the six heterogeneous\ninformation paths). Third, we propose an efficient one-shot search method to\nfind the optimal path aggregation architecture, that is, we first train a\nsuper-net and then find the optimal candidate with an evolutionary algorithm.\nExperimental results demonstrate the efficacy of the proposed OPANAS for object\ndetection: (1) OPANAS is more efficient than state-of-the-art methods (e.g.,\nNAS-FPN and Auto-FPN), at significantly smaller searching cost (e.g., only 4\nGPU days on MS-COCO); (2) the optimal architecture found by OPANAS\nsignificantly improves main-stream detectors including RetinaNet, Faster R-CNN\nand Cascade R-CNN, by 2.3-3.2 % mAP comparing to their FPN counterparts; and\n(3) a new state-of-the-art accuracy-speed trade-off (52.2 % mAP at 7.6 FPS) at\nsmaller training costs than comparable state-of-the-arts. Code will be released\nat https://github.com/VDIGPKU/OPANAS.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 01:48:53 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 05:24:03 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 05:17:08 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Liang", "Tingting", ""], ["Wang", "Yongtao", ""], ["Tang", "Zhi", ""], ["Hu", "Guosheng", ""], ["Ling", "Haibin", ""]]}, {"id": "2103.04508", "submitter": "Bowen Li", "authors": "Bowen Li, Yiming Li, Junjie Ye, Changhong Fu, and Hang Zhao", "title": "Predictive Visual Tracking: A New Benchmark and Baseline Approach", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a crucial robotic perception capability, visual tracking has been\nintensively studied recently. In the real-world scenarios, the onboard\nprocessing time of the image streams inevitably leads to a discrepancy between\nthe tracking results and the real-world states. However, existing visual\ntracking benchmarks commonly run the trackers offline and ignore such latency\nin the evaluation. In this work, we aim to deal with a more realistic problem\nof latency-aware tracking. The state-of-the-art trackers are evaluated in the\naerial scenarios with new metrics jointly assessing the tracking accuracy and\nefficiency. Moreover, a new predictive visual tracking baseline is developed to\ncompensate for the latency stemming from the onboard computation. Our\nlatency-aware benchmark can provide a more realistic evaluation of the trackers\nfor the robotic applications. Besides, exhaustive experiments have proven the\neffectiveness of the proposed predictive visual tracking baseline approach.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 01:50:05 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Bowen", ""], ["Li", "Yiming", ""], ["Ye", "Junjie", ""], ["Fu", "Changhong", ""], ["Zhao", "Hang", ""]]}, {"id": "2103.04513", "submitter": "Desheng Wang", "authors": "Desheng Wang (1), Weidong Jin (1), Yunpu Wu (1), Aamir Khan (1) ((1)\n  School of Electrical Engineering, Southwest Jiaotong University, Chengdu, P.\n  R. China)", "title": "Improving Global Adversarial Robustness Generalization With\n  Adversarially Trained GAN", "comments": "17 pages, 7 figures, uses simpleConference.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved beyond human-level\naccuracy in the image classification task and are widely deployed in real-world\nenvironments. However, CNNs show vulnerability to adversarial perturbations\nthat are well-designed noises aiming to mislead the classification models. In\norder to defend against the adversarial perturbations, adversarially trained\nGAN (ATGAN) is proposed to improve the adversarial robustness generalization of\nthe state-of-the-art CNNs trained by adversarial training. ATGAN incorporates\nadversarial training into standard GAN training procedure to remove obfuscated\ngradients which can lead to a false sense in defending against the adversarial\nperturbations and are commonly observed in existing GANs-based adversarial\ndefense methods. Moreover, ATGAN adopts the image-to-image generator as data\naugmentation to increase the sample complexity needed for adversarial\nrobustness generalization in adversarial training. Experimental results in\nMNIST SVHN and CIFAR-10 datasets show that the proposed method doesn't rely on\nobfuscated gradients and achieves better global adversarial robustness\ngeneralization performance than the adversarially trained state-of-the-art\nCNNs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 02:18:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Desheng", ""], ["Jin", "Weidong", ""], ["Wu", "Yunpu", ""], ["Khan", "Aamir", ""]]}, {"id": "2103.04523", "submitter": "XingJia Pan", "authors": "Xingjia Pan, Yingguo Gao, Zhiwen Lin, Fan Tang, Weiming Dong, Haolei\n  Yuan, Feiyue Huang, Changsheng Xu", "title": "Unveiling the Potential of Structure Preserving for Weakly Supervised\n  Object Localization", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weakly supervised object localization(WSOL) remains an open problem given the\ndeficiency of finding object extent information using a classification network.\nAlthough prior works struggled to localize objects through various spatial\nregularization strategies, we argue that how to extract object structural\ninformation from the trained classification network is neglected. In this\npaper, we propose a two-stage approach, termed structure-preserving activation\n(SPA), toward fully leveraging the structure information incorporated in\nconvolutional features for WSOL. First, a restricted activation module (RAM) is\ndesigned to alleviate the structure-missing issue caused by the classification\nnetwork on the basis of the observation that the unbounded classification map\nand global average pooling layer drive the network to focus only on object\nparts. Second, we designed a post-process approach, termed self-correlation map\ngenerating (SCG) module to obtain structure-preserving localization maps on the\nbasis of the activation maps acquired from the first stage. Specifically, we\nutilize the high-order self-correlation (HSC) to extract the inherent\nstructural information retained in the learned model and then aggregate HSC of\nmultiple points for precise object localization. Extensive experiments on two\npublicly available benchmarks including CUB-200-2011 and ILSVRC show that the\nproposed SPA achieves substantial and consistent performance gains compared\nwith baseline approaches.Code and models are available at\nhttps://github.com/Panxjia/SPA_CVPR2021\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 03:04:14 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 02:44:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Pan", "Xingjia", ""], ["Gao", "Yingguo", ""], ["Lin", "Zhiwen", ""], ["Tang", "Fan", ""], ["Dong", "Weiming", ""], ["Yuan", "Haolei", ""], ["Huang", "Feiyue", ""], ["Xu", "Changsheng", ""]]}, {"id": "2103.04524", "submitter": "Lingtong Kong", "authors": "Lingtong Kong, Chunhua Shen, Jie Yang", "title": "FastFlowNet: A Lightweight Network for Fast Optical Flow Estimation", "comments": "Accepted by ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense optical flow estimation plays a key role in many robotic vision tasks.\nIn the past few years, with the advent of deep learning, we have witnessed\ngreat progress in optical flow estimation. However, current networks often\nconsist of a large number of parameters and require heavy computation costs,\nlargely hindering its application on low power-consumption devices such as\nmobile phones. In this paper, we tackle this challenge and design a lightweight\nmodel for fast and accurate optical flow prediction. Our proposed FastFlowNet\nfollows the widely-used coarse-to-fine paradigm with following innovations.\nFirst, a new head enhanced pooling pyramid (HEPP) feature extractor is employed\nto intensify high-resolution pyramid features while reducing parameters.\nSecond, we introduce a new center dense dilated correlation (CDDC) layer for\nconstructing compact cost volume that can keep large search radius with reduced\ncomputation burden. Third, an efficient shuffle block decoder (SBD) is\nimplanted into each pyramid level to accelerate flow estimation with marginal\ndrops in accuracy. Experiments on both synthetic Sintel data and real-world\nKITTI datasets demonstrate the effectiveness of the proposed approach, which\nneeds only 1/10 computation of comparable networks to achieve on par accuracy.\nIn particular, FastFlowNet only contains 1.37M parameters; and can execute at\n90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a\npair of Sintel images of resolution 1024x436.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 03:09:37 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 14:13:41 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kong", "Lingtong", ""], ["Shen", "Chunhua", ""], ["Yang", "Jie", ""]]}, {"id": "2103.04526", "submitter": "Pengbo Liu", "authors": "Pengbo Liu, Li Xiao, S. Kevin Zhou", "title": "Incremental Learning for Multi-organ Segmentation with Partially Labeled\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a large number of datasets for organ segmentation, which are\npartially annotated, and sequentially constructed. A typical dataset is\nconstructed at a certain time by curating medical images and annotating the\norgans of interest. In other words, new datasets with annotations of new organ\ncategories are built over time. To unleash the potential behind these partially\nlabeled, sequentially-constructed datasets, we propose to learn a multi-organ\nsegmentation model through incremental learning (IL). In each IL stage, we lose\naccess to the previous annotations, whose knowledge is assumingly captured by\nthe current model, and gain the access to a new dataset with annotations of new\norgan categories, from which we learn to update the organ segmentation model to\ninclude the new organs. We give the first attempt to conjecture that the\ndifferent distribution is the key reason for 'catastrophic forgetting' that\ncommonly exists in IL methods, and verify that IL has the natural adaptability\nto medical image scenarios. Extensive experiments on five open-sourced datasets\nare conducted to prove the effectiveness of our method and the conjecture\nmentioned above.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 03:15:59 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Pengbo", ""], ["Xiao", "Li", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2103.04527", "submitter": "Qingsong Yao", "authors": "Qingsong Yao, Quan Quan, Li Xiao, S. Kevin Zhou", "title": "One-Shot Medical Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of deep learning methods relies on the availability of a large\nnumber of datasets with annotations; however, curating such datasets is\nburdensome, especially for medical images. To relieve such a burden for a\nlandmark detection task, we explore the feasibility of using only a single\nannotated image and propose a novel framework named Cascade Comparing to Detect\n(CC2D) for one-shot landmark detection. CC2D consists of two stages: 1)\nSelf-supervised learning (CC2D-SSL) and 2) Training with pseudo-labels\n(CC2D-TPL). CC2D-SSL captures the consistent anatomical information in a\ncoarse-to-fine fashion by comparing the cascade feature representations and\ngenerates predictions on the training set. CC2D-TPL further improves the\nperformance by training a new landmark detector with those predictions. The\neffectiveness of CC2D is evaluated on a widely-used public dataset of\ncephalometric landmark detection, which achieves a competitive detection\naccuracy of 81.01\\% within 4.0mm, comparable to the state-of-the-art\nfully-supervised methods using a lot more than one training image.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 03:16:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yao", "Qingsong", ""], ["Quan", "Quan", ""], ["Xiao", "Li", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2103.04537", "submitter": "Ruizhi Liao", "authors": "Ruizhi Liao, Daniel Moyer, Miriam Cha, Keegan Quigley, Seth Berkowitz,\n  Steven Horng, Polina Golland, William M. Wells", "title": "Multimodal Representation Learning via Maximization of Local Mutual\n  Information", "comments": "In Proceedings of International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose and demonstrate a representation learning approach by maximizing\nthe mutual information between local features of images and text. The goal of\nthis approach is to learn useful image representations by taking advantage of\nthe rich information contained in the free text that describes the findings in\nthe image. Our method trains image and text encoders by encouraging the\nresulting representations to exhibit high local mutual information. We make use\nof recent advances in mutual information estimation with neural network\ndiscriminators. We argue that the sum of local mutual information is typically\na lower bound on the global mutual information. Our experimental results in the\ndownstream image classification tasks demonstrate the advantages of using local\nfeatures for image-text representation learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 03:59:59 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 03:11:55 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Liao", "Ruizhi", ""], ["Moyer", "Daniel", ""], ["Cha", "Miriam", ""], ["Quigley", "Keegan", ""], ["Berkowitz", "Seth", ""], ["Horng", "Steven", ""], ["Golland", "Polina", ""], ["Wells", "William M.", ""]]}, {"id": "2103.04544", "submitter": "Toby Chong", "authors": "Toby Chong, Nolwenn Maudet, Katsuki Harima, Takeo Igarashi", "title": "Exploring a Makeup Support System for Transgender Passing based on\n  Automatic Gender Recognition", "comments": "Accepted to CHI2021. Project Page:\n  https://sites.google.com/view/flyingcolor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  How to handle gender with machine learning is a controversial topic. A\ngrowing critical body of research brought attention to the numerous issues\ntransgender communities face with the adoption of current automatic gender\nrecognition (AGR) systems. In contrast, we explore how such technologies could\npotentially be appropriated to support transgender practices and needs,\nespecially in non-Western contexts like Japan. We designed a virtual makeup\nprobe to assist transgender individuals with passing, that is to be perceived\nas the gender they identify as. To understand how such an application might\nsupport expressing transgender individuals gender identity or not, we\ninterviewed 15 individuals in Tokyo and found that in the right context and\nunder strict conditions, AGR based systems could assist transgender passing.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 04:43:10 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chong", "Toby", ""], ["Maudet", "Nolwenn", ""], ["Harima", "Katsuki", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2103.04552", "submitter": "Yuanyuan Lyu", "authors": "Yuanyuan Lyu, Jiajun Fu, Cheng Peng, S. Kevin Zhou", "title": "U-DuDoNet: Unpaired dual-domain network for CT metal artifact reduction", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, both supervised and unsupervised deep learning methods have been\nwidely applied on the CT metal artifact reduction (MAR) task. Supervised\nmethods such as Dual Domain Network (Du-DoNet) work well on simulation data;\nhowever, their performance on clinical data is limited due to domain gap.\nUnsupervised methods are more generalized, but do not eliminate artifacts\ncompletely through the sole processing on the image domain. To combine the\nadvantages of both MAR methods, we propose an unpaired dual-domain network\n(U-DuDoNet) trained using unpaired data. Unlike the artifact disentanglement\nnetwork (ADN) that utilizes multiple encoders and decoders for disentangling\ncontent from artifact, our U-DuDoNet directly models the artifact generation\nprocess through additions in both sinogram and image domains, which is\ntheoretically justified by an additive property associated with metal artifact.\nOur design includes a self-learned sinogram prior net, which provides guidance\nfor restoring the information in the sinogram domain, and cyclic constraints\nfor artifact reduction and addition on unpaired data. Extensive experiments on\nsimulation data and clinical images demonstrate that our novel framework\noutperforms the state-of-the-art unpaired approaches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 05:19:15 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Lyu", "Yuanyuan", ""], ["Fu", "Jiajun", ""], ["Peng", "Cheng", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2103.04558", "submitter": "Tao Ma", "authors": "Tao Ma, Zhizheng Liu, Guohang Yan, Yikang Li", "title": "CRLF: Automatic Calibration and Refinement based on Line Feature for\n  LiDAR and Camera in Road Scenes", "comments": "7 pages, 7 figures, submitted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For autonomous vehicles, an accurate calibration for LiDAR and camera is a\nprerequisite for multi-sensor perception systems. However, existing calibration\ntechniques require either a complicated setting with various calibration\ntargets, or an initial calibration provided beforehand, which greatly impedes\ntheir applicability in large-scale autonomous vehicle deployment. To tackle\nthese issues, we propose a novel method to calibrate the extrinsic parameter\nfor LiDAR and camera in road scenes. Our method introduces line features from\nstatic straight-line-shaped objects such as road lanes and poles in both image\nand point cloud and formulates the initial calibration of extrinsic parameters\nas a perspective-3-lines (P3L) problem. Subsequently, a cost function defined\nunder the semantic constraints of the line features is designed to perform\nrefinement on the solved coarse calibration. The whole procedure is fully\nautomatic and user-friendly without the need to adjust environment settings or\nprovide an initial calibration. We conduct extensive experiments on KITTI and\nour in-house dataset, quantitative and qualitative results demonstrate the\nrobustness and accuracy of our method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 06:02:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ma", "Tao", ""], ["Liu", "Zhizheng", ""], ["Yan", "Guohang", ""], ["Li", "Yikang", ""]]}, {"id": "2103.04559", "submitter": "Yuying Ge", "authors": "Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu and Ping\n  Luo", "title": "Parser-Free Virtual Try-on via Distilling Appearance Flows", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image virtual try-on aims to fit a garment image (target clothes) to a person\nimage. Prior methods are heavily based on human parsing. However,\nslightly-wrong segmentation results would lead to unrealistic try-on images\nwith large artifacts. Inaccurate parsing misleads parser-based methods to\nproduce visually unrealistic results where artifacts usually occur. A recent\npioneering work employed knowledge distillation to reduce the dependency of\nhuman parsing, where the try-on images produced by a parser-based method are\nused as supervisions to train a \"student\" network without relying on\nsegmentation, making the student mimic the try-on ability of the parser-based\nmodel. However, the image quality of the student is bounded by the parser-based\nmodel. To address this problem, we propose a novel approach,\n\"teacher-tutor-student\" knowledge distillation, which is able to produce highly\nphoto-realistic images without human parsing, possessing several appealing\nadvantages compared to prior arts. (1) Unlike existing work, our approach\ntreats the fake images produced by the parser-based method as \"tutor\nknowledge\", where the artifacts can be corrected by real \"teacher knowledge\",\nwhich is extracted from the real person images in a self-supervised way. (2)\nOther than using real images as supervisions, we formulate knowledge\ndistillation in the try-on problem as distilling the appearance flows between\nthe person image and the garment image, enabling us to find accurate dense\ncorrespondences between them to produce high-quality results. (3) Extensive\nevaluations show large superiority of our method (see Fig. 1).\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 06:05:38 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 05:37:48 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ge", "Yuying", ""], ["Song", "Yibing", ""], ["Zhang", "Ruimao", ""], ["Ge", "Chongjian", ""], ["Liu", "Wei", ""], ["Luo", "Ping", ""]]}, {"id": "2103.04565", "submitter": "Haimin Zhang", "authors": "Haimin Zhang, Min Xu", "title": "Improving Transformation-based Defenses against Adversarial Examples\n  with First-order Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been successfully applied in various machine\nlearning tasks. However, studies show that neural networks are susceptible to\nadversarial attacks. This exposes a potential threat to neural network-based\nintelligent systems. We observe that the probability of the correct result\noutputted by the neural network increases by applying small first-order\nperturbations generated for non-predicted class labels to adversarial examples.\nBased on this observation, we propose a method for counteracting adversarial\nperturbations to improve adversarial robustness. In the proposed method, we\nrandomly select a number of class labels and generate small first-order\nperturbations for these selected labels. The generated perturbations are added\ntogether and then clamped onto a specified space. The obtained perturbation is\nfinally added to the adversarial example to counteract the adversarial\nperturbation contained in the example. The proposed method is applied at\ninference time and does not require retraining or finetuning the model. We\nexperimentally validate the proposed method on CIFAR-10 and CIFAR-100. The\nresults demonstrate that our method effectively improves the defense\nperformance of several transformation-based defense methods, especially against\nstrong adversarial examples generated using more iterations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 06:27:24 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 06:46:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhang", "Haimin", ""], ["Xu", "Min", ""]]}, {"id": "2103.04570", "submitter": "Tianfei Zhou", "authors": "Tianfei Zhou, Wenguan Wang, Si Liu, Yi Yang, Luc Van Gool", "title": "Differentiable Multi-Granularity Human Representation Learning for\n  Instance-Aware Human Semantic Parsing", "comments": "CVPR 2021 (Oral). Code: https://github.com/tfzhou/MG-HumanParsing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To address the challenging task of instance-aware human part parsing, a new\nbottom-up regime is proposed to learn category-level human semantic\nsegmentation as well as multi-person pose estimation in a joint and end-to-end\nmanner. It is a compact, efficient and powerful framework that exploits\nstructural information over different human granularities and eases the\ndifficulty of person partitioning. Specifically, a dense-to-sparse projection\nfield, which allows explicitly associating dense human semantics with sparse\nkeypoints, is learnt and progressively improved over the network feature\npyramid for robustness. Then, the difficult pixel grouping problem is cast as\nan easier, multi-person joint assembling task. By formulating joint association\nas maximum-weight bipartite matching, a differentiable solution is developed to\nexploit projected gradient descent and Dykstra's cyclic projection algorithm.\nThis makes our method end-to-end trainable and allows back-propagating the\ngrouping error to directly supervise multi-granularity human representation\nlearning. This is distinguished from current bottom-up human parsers or pose\nestimators which require sophisticated post-processing or heuristic greedy\nalgorithms. Experiments on three instance-aware human parsing datasets show\nthat our model outperforms other bottom-up alternatives with much more\nefficient inference.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 06:55:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhou", "Tianfei", ""], ["Wang", "Wenguan", ""], ["Liu", "Si", ""], ["Yang", "Yi", ""], ["Van Gool", "Luc", ""]]}, {"id": "2103.04580", "submitter": "Qing Li", "authors": "Qing Li, Xiaojiang Peng, Yu Qiao, Qi Hao", "title": "Unsupervised Person Re-Identification with Multi-Label Learning Guided\n  Self-Paced Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although unsupervised person re-identification (Re-ID) has drawn increasing\nresearch attention recently, it remains challenging to learn discriminative\nfeatures without annotations across disjoint camera views. In this paper, we\naddress the unsupervised person Re-ID with a conceptually novel yet simple\nframework, termed as Multi-label Learning guided self-paced Clustering (MLC).\nMLC mainly learns discriminative features with three crucial modules, namely a\nmulti-scale network, a multi-label learning module, and a self-paced clustering\nmodule. Specifically, the multi-scale network generates multi-granularity\nperson features in both global and local views. The multi-label learning module\nleverages a memory feature bank and assigns each image with a multi-label\nvector based on the similarities between the image and feature bank. After\nmulti-label training for several epochs, the self-paced clustering joins in\ntraining and assigns a pseudo label for each image. The benefits of our MLC\ncome from three aspects: i) the multi-scale person features for better\nsimilarity measurement, ii) the multi-label assignment based on the whole\ndataset ensures that every image can be trained, and iii) the self-paced\nclustering removes some noisy samples for better feature learning. Extensive\nexperiments on three popular large-scale Re-ID benchmarks demonstrate that our\nMLC outperforms previous state-of-the-art methods and significantly improves\nthe performance of unsupervised person Re-ID.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 07:30:13 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Qing", ""], ["Peng", "Xiaojiang", ""], ["Qiao", "Yu", ""], ["Hao", "Qi", ""]]}, {"id": "2103.04584", "submitter": "Shuang Xu", "authors": "Shuang Xu and Jiangshe Zhang and Zixiang Zhao and Kai Sun and Junmin\n  Liu and Chunxia Zhang", "title": "Deep Gradient Projection Networks for Pan-sharpening", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pan-sharpening is an important technique for remote sensing imaging systems\nto obtain high resolution multispectral images. Recently, deep learning has\nbecome the most popular tool for pan-sharpening. This paper develops a\nmodel-based deep pan-sharpening approach. Specifically, two optimization\nproblems regularized by the deep prior are formulated, and they are separately\nresponsible for the generative models for panchromatic images and low\nresolution multispectral images. Then, the two problems are solved by a\ngradient projection algorithm, and the iterative steps are generalized into two\nnetwork blocks. By alternatively stacking the two blocks, a novel network,\ncalled gradient projection based pan-sharpening neural network, is constructed.\nThe experimental results on different kinds of satellite datasets demonstrate\nthat the new network outperforms state-of-the-art methods both visually and\nquantitatively. The codes are available at https://github.com/xsxjtu/GPPNN.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 07:51:58 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Xu", "Shuang", ""], ["Zhang", "Jiangshe", ""], ["Zhao", "Zixiang", ""], ["Sun", "Kai", ""], ["Liu", "Junmin", ""], ["Zhang", "Chunxia", ""]]}, {"id": "2103.04590", "submitter": "Pranav Rajpurkar", "authors": "Siyu Shi, Ishaan Malhi, Kevin Tran, Andrew Y. Ng, Pranav Rajpurkar", "title": "CheXseen: Unseen Disease Detection for Deep Learning Interpretation of\n  Chest X-rays", "comments": "Accepted at MIDL Conference 2021. Previous version accepted at ACM\n  Conference on Health, Inference, and Learning (ACM-CHIL) Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We systematically evaluate the performance of deep learning models in the\npresence of diseases not labeled for or present during training. First, we\nevaluate whether deep learning models trained on a subset of diseases (seen\ndiseases) can detect the presence of any one of a larger set of diseases. We\nfind that models tend to falsely classify diseases outside of the subset\n(unseen diseases) as \"no disease\". Second, we evaluate whether models trained\non seen diseases can detect seen diseases when co-occurring with diseases\noutside the subset (unseen diseases). We find that models are still able to\ndetect seen diseases even when co-occurring with unseen diseases. Third, we\nevaluate whether feature representations learned by models may be used to\ndetect the presence of unseen diseases given a small labeled set of unseen\ndiseases. We find that the penultimate layer of the deep neural network\nprovides useful features for unseen disease detection. Our results can inform\nthe safe clinical deployment of deep learning models trained on a\nnon-exhaustive set of disease classes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 08:13:21 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 05:15:55 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shi", "Siyu", ""], ["Malhi", "Ishaan", ""], ["Tran", "Kevin", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2103.04607", "submitter": "Wenkang Li", "authors": "Wenkang Li, Ke Qi, Wenbin Chen, Yicong Zhou", "title": "Unified Batch All Triplet Loss for Visible-Infrared Person\n  Re-identification", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible-Infrared cross-modality person re-identification (VI-ReID), whose aim\nis to match person images between visible and infrared modality, is a\nchallenging cross-modality image retrieval task. Batch Hard Triplet loss is\nwidely used in person re-identification tasks, but it does not perform well in\nthe Visible-Infrared person re-identification task. Because it only optimizes\nthe hardest triplet for each anchor image within the mini-batch, samples in the\nhardest triplet may all belong to the same modality, which will lead to the\nimbalance problem of modality optimization. To address this problem, we adopt\nthe batch all triplet selection strategy, which selects all the possible\ntriplets among samples to optimize instead of the hardest triplet. Furthermore,\nwe introduce Unified Batch All Triplet loss and Cosine Softmax loss to\ncollaboratively optimize the cosine distance between image vectors. Similarly,\nwe rewrite the Hetero Center Triplet loss, which is proposed for VI-ReID task,\ninto a batch all form to improve model performance. Extensive experiments\nindicate the effectiveness of the proposed methods, which outperform\nstate-of-the-art methods by a wide margin.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 08:58:52 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Wenkang", ""], ["Qi", "Ke", ""], ["Chen", "Wenbin", ""], ["Zhou", "Yicong", ""]]}, {"id": "2103.04612", "submitter": "Bohao Li", "authors": "Bohao Li, Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, Qixiang Ye", "title": "Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object\n  Detection", "comments": "This paper has been modified by the author due to errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection has made substantial progressby representing novel\nclass objects using the feature representation learned upon a set of base class\nobjects. However,an implicit contradiction between novel class classification\nand representation is unfortunately ignored. On the one hand, to achieve\naccurate novel class classification, the distributions of either two base\nclasses must be far away fromeach other (max-margin). On the other hand, to\nprecisely represent novel classes, the distributions of base classes should be\nclose to each other to reduce the intra-class distance of novel classes\n(min-margin). In this paper, we propose a class margin equilibrium (CME)\napproach, with the aim to optimize both feature space partition and novel class\nreconstruction in a systematic way. CME first converts the few-shot detection\nproblem to the few-shot classification problem by using a fully connected layer\nto decouple localization features. CME then reserves adequate margin space for\nnovel classes by introducing simple-yet-effective class margin loss during\nfeature learning. Finally, CME pursues margin equilibrium by disturbing the\nfeatures of novel class instances in an adversarial min-max fashion.\nExperiments on Pascal VOC and MS-COCO datasets show that CME significantly\nimproves upon two baseline detectors (up to $3\\sim 5\\%$ in average), achieving\nstate-of-the-art performance. Code is available at\nhttps://github.com/Bohao-Lee/CME .\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:04:03 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 05:45:52 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 04:55:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Bohao", ""], ["Yang", "Boyu", ""], ["Liu", "Chang", ""], ["Liu", "Feng", ""], ["Ji", "Rongrong", ""], ["Ye", "Qixiang", ""]]}, {"id": "2103.04617", "submitter": "Daniel Jim\\'enez-S\\'anchez", "authors": "Daniel Jim\\'enez-S\\'anchez, Mikel Ariz, Carlos Ortiz-de-Sol\\'orzano", "title": "Synplex: A synthetic simulator of highly multiplexed histological images", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM q-bio.TO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiplex tissue immunostaining is a technology of growing relevance as it\ncan capture in situ the complex interactions existing between the elements of\nthe tumor microenvironment. The existence and availability of large, annotated\nimage datasets is key for the objective development and benchmarking of\nbioimage analysis algorithms. Manual annotation of multiplex images, is\nhowever, laborious, often impracticable. In this paper, we present Synplex, a\nsimulation system able to generate multiplex immunostained in situ tissue\nimages based on user-defined parameters. This includes the specification of\nstructural attributes, such as the number of cell phenotypes, the number and\nlevel of expression of cellular markers, or the cell morphology. Synplex\nconsists of three sequential modules, each being responsible for a separate\ntask: modeling of cellular neighborhoods, modeling of cell phenotypes, and\nsynthesis of realistic cell/tissue textures. Synplex flexibility and accuracy\nare demonstrated qualitatively and quantitatively by generating synthetic\ntissues that simulate disease paradigms found in the real scenarios. Synplex is\npublicly available for scientific purposes, and we believe it will become a\nvaluable tool for the training and/or validation of multiplex image analysis\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:12:02 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Jim\u00e9nez-S\u00e1nchez", "Daniel", ""], ["Ariz", "Mikel", ""], ["Ortiz-de-Sol\u00f3rzano", "Carlos", ""]]}, {"id": "2103.04618", "submitter": "Fengxiang Yang", "authors": "Fengxiang Yang, Zhun Zhong, Zhiming Luo, Yuanzheng Cai, Yaojin Lin,\n  Shaozi Li, Nicu Sebe", "title": "Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for\n  Unsupervised Person Re-Identification", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of unsupervised person re-identification\n(re-ID), which aims to learn discriminative models with unlabeled data. One\npopular method is to obtain pseudo-label by clustering and use them to optimize\nthe model. Although this kind of approach has shown promising accuracy, it is\nhampered by 1) noisy labels produced by clustering and 2) feature variations\ncaused by camera shift. The former will lead to incorrect optimization and thus\nhinders the model accuracy. The latter will result in assigning the intra-class\nsamples of different cameras to different pseudo-label, making the model\nsensitive to camera variations. In this paper, we propose a unified framework\nto solve both problems. Concretely, we propose a Dynamic and Symmetric\nCross-Entropy loss (DSCE) to deal with noisy samples and a camera-aware\nmeta-learning algorithm (MetaCam) to adapt camera shift. DSCE can alleviate the\nnegative effects of noisy samples and accommodate the change of clusters after\neach clustering step. MetaCam simulates cross-camera constraint by splitting\nthe training data into meta-train and meta-test based on camera IDs. With the\ninteracted gradient from meta-train and meta-test, the model is enforced to\nlearn camera-invariant features. Extensive experiments on three re-ID\nbenchmarks show the effectiveness and the complementary of the proposed DSCE\nand MetaCam. Our method outperforms the state-of-the-art methods on both fully\nunsupervised re-ID and unsupervised domain adaptive re-ID.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:13:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yang", "Fengxiang", ""], ["Zhong", "Zhun", ""], ["Luo", "Zhiming", ""], ["Cai", "Yuanzheng", ""], ["Lin", "Yaojin", ""], ["Li", "Shaozi", ""], ["Sebe", "Nicu", ""]]}, {"id": "2103.04635", "submitter": "Yash Patel", "authors": "Yash Patel, Jiri Matas", "title": "FEDS -- Filtered Edit Distance Surrogate", "comments": "ICDAR 2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a procedure to train a scene text recognition model using\na robust learned surrogate of edit distance. The proposed method borrows from\nself-paced learning and filters out the training examples that are hard for the\nsurrogate. The filtering is performed by judging the quality of the\napproximation, using a ramp function, enabling end-to-end training. Following\nthe literature, the experiments are conducted in a post-tuning setup, where a\ntrained scene text recognition model is tuned using the learned surrogate of\nedit distance. The efficacy is demonstrated by improvements on various\nchallenging scene text datasets such as IIIT-5K, SVT, ICDAR, SVTP, and CUTE.\nThe proposed method provides an average improvement of $11.2 \\%$ on total edit\ndistance and an error reduction of $9.5\\%$ on accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:47:51 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 13:39:53 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Patel", "Yash", ""], ["Matas", "Jiri", ""]]}, {"id": "2103.04657", "submitter": "Heqin Zhu", "authors": "Heqin Zhu, QingsongYao, Li Xiao, S.kevin Zhou", "title": "You Only Learn Once: Universal Anatomical Landmark Detection", "comments": "Accepted for MICCAI 2021, 11 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Detecting anatomical landmarks in medical images plays an essential role in\nunderstanding the anatomy and planning automated processing. In recent years, a\nvariety of deep neural network methods have been developed to detect landmarks\nautomatically. However, all of those methods are unary in the sense that a\nhighly specialized network is trained for a single task say associated with a\nparticular anatomical region. In this work, for the first time, we investigate\nthe idea of ``You Only Learn Once (YOLO)'' and develop a universal anatomical\nlandmark detection model to realize multiple landmark detection tasks with\nend-to-end training based on mixed datasets. The model consists of a local\nnetwork and a global network: The local network is built upon the idea of\nuniversal U-Net to learn multi-domain local features and the global network is\na parallelly-duplicated sequential of dilated convolutions that extract global\nfeatures to further disambiguate the landmark locations. It is worth mentioning\nthat the new model design requires much fewer parameters than models with\nstandard convolutions to train. We evaluate our YOLO model on three X-ray\ndatasets of 1,588 images on the head, hand, and chest, collectively\ncontributing 62 landmarks. The experimental results show that our proposed\nuniversal model behaves largely better than any previous models trained on\nmultiple datasets. It even beats the performance of the model that is trained\nseparately for every single dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 10:38:52 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 16:13:53 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhu", "Heqin", ""], ["QingsongYao", "", ""], ["Xiao", "Li", ""], ["Zhou", "S. kevin", ""]]}, {"id": "2103.04677", "submitter": "Timo Milbich", "authors": "Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Bj\\\"orn Ommer", "title": "Behavior-Driven Synthesis of Human Dynamics", "comments": "Accepted to CVPR 2021 as Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating and representing human behavior are of major importance for\nvarious computer vision applications. Commonly, human video synthesis\nrepresents behavior as sequences of postures while directly predicting their\nlikely progressions or merely changing the appearance of the depicted persons,\nthus not being able to exercise control over their actual behavior during the\nsynthesis process. In contrast, controlled behavior synthesis and transfer\nacross individuals requires a deep understanding of body dynamics and calls for\na representation of behavior that is independent of appearance and also of\nspecific postures. In this work, we present a model for human behavior\nsynthesis which learns a dedicated representation of human dynamics independent\nof postures. Using this representation, we are able to change the behavior of a\nperson depicted in an arbitrary posture, or to even directly transfer behavior\nobserved in a given video sequence. To this end, we propose a conditional\nvariational framework which explicitly disentangles posture from behavior. We\ndemonstrate the effectiveness of our approach on this novel task, evaluating\ncapturing, transferring, and sampling fine-grained, diverse behavior, both\nquantitatively and qualitatively. Project page is available at\nhttps://cutt.ly/5l7rXEp\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 11:36:32 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 11:53:27 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Blattmann", "Andreas", ""], ["Milbich", "Timo", ""], ["Dorkenwald", "Michael", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2103.04680", "submitter": "Changhai Li", "authors": "Changhai Li, Huawei Chen, Jingqing Lu, Yang Huang and Yingying Liu", "title": "Time and Frequency Network for Human Action Detection in Videos", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, spatiotemporal features are embraced by most deep learning\napproaches for human action detection in videos, however, they neglect the\nimportant features in frequency domain. In this work, we propose an end-to-end\nnetwork that considers the time and frequency features simultaneously, named\nTFNet. TFNet holds two branches, one is time branch formed of three-dimensional\nconvolutional neural network(3D-CNN), which takes the image sequence as input\nto extract time features; and the other is frequency branch, extracting\nfrequency features through two-dimensional convolutional neural network(2D-CNN)\nfrom DCT coefficients. Finally, to obtain the action patterns, these two\nfeatures are deeply fused under the attention mechanism. Experimental results\non the JHMDB51-21 and UCF101-24 datasets demonstrate that our approach achieves\nremarkable performance for frame-mAP.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 11:42:05 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Changhai", ""], ["Chen", "Huawei", ""], ["Lu", "Jingqing", ""], ["Huang", "Yang", ""], ["Liu", "Yingying", ""]]}, {"id": "2103.04692", "submitter": "Tuomo Hiippala", "authors": "Tuomo Hiippala and John A. Bateman", "title": "Semiotically-grounded distant viewing of diagrams: insights from two\n  multimodal corpora", "comments": "22 pages, 11 figures. Under review at Digital Scholarship in the\n  Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we bring together theories of multimodal communication and\ncomputational methods to study how primary school science diagrams combine\nmultiple expressive resources. We position our work within the field of digital\nhumanities, and show how annotations informed by multimodality research, which\ntarget expressive resources and discourse structure, allow imposing structure\non the output of computational methods. We illustrate our approach by analysing\ntwo multimodal diagram corpora: the first corpus is intended to support\nresearch on automatic diagram processing, whereas the second is oriented\ntowards studying diagrams as a mode of communication. Our results show that\nmultimodally-informed annotations can bring out structural patterns in the\ndiagrams, which also extend across diagrams that deal with different topics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:04:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hiippala", "Tuomo", ""], ["Bateman", "John A.", ""]]}, {"id": "2103.04693", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Rishabh Kabra, Chris Burgess, Murray Shanahan", "title": "Unsupervised Object-Based Transition Models for 3D Partially Observable\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a slot-wise, object-based transition model that decomposes a scene\ninto objects, aligns them (with respect to a slot-wise object memory) to\nmaintain a consistent order across time, and predicts how those objects evolve\nover successive frames. The model is trained end-to-end without supervision\nusing losses at the level of the object-structured representation rather than\npixels. Thanks to its alignment module, the model deals properly with two\nissues that are not handled satisfactorily by other transition models, namely\nobject persistence and object identity. We show that the combination of an\nobject-level loss and correct object alignment over time enables the model to\noutperform a state-of-the-art baseline, and allows it to deal well with object\nocclusion and re-appearance in partially observable environments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:10:02 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Creswell", "Antonia", ""], ["Kabra", "Rishabh", ""], ["Burgess", "Chris", ""], ["Shanahan", "Murray", ""]]}, {"id": "2103.04701", "submitter": "Zhenhuan Huang", "authors": "Zhenhuan Huang, Xiaoyue Duan, Bo Zhao, Jinhu L\\\"u, Baochang Zhang", "title": "Interpretable Attention Guided Network for Fine-grained Visual\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual classification (FGVC) is challenging but more critical\nthan traditional classification tasks. It requires distinguishing different\nsubcategories with the inherently subtle intra-class object variations.\nPrevious works focus on enhancing the feature representation ability using\nmultiple granularities and discriminative regions based on the attention\nstrategy or bounding boxes. However, these methods highly rely on deep neural\nnetworks which lack interpretability. We propose an Interpretable Attention\nGuided Network (IAGN) for fine-grained visual classification. The contributions\nof our method include: i) an attention guided framework which can guide the\nnetwork to extract discriminitive regions in an interpretable way; ii) a\nprogressive training mechanism obtained to distill knowledge stage by stage to\nfuse features of various granularities; iii) the first interpretable FGVC\nmethod with a competitive performance on several standard FGVC benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:27:51 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 02:15:22 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Huang", "Zhenhuan", ""], ["Duan", "Xiaoyue", ""], ["Zhao", "Bo", ""], ["L\u00fc", "Jinhu", ""], ["Zhang", "Baochang", ""]]}, {"id": "2103.04704", "submitter": "Shiqi Yang", "authors": "Shiqi Yang, Kai Wang, Luis Herranz, Joost van de Weijer", "title": "On Implicit Attribute Localization for Generalized Zero-Shot Learning", "comments": "To appear in IEEE Signal Processing Letters. Overlapped with\n  arXiv:2006.05938", "journal-ref": null, "doi": "10.1109/LSP.2021.3073655", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to discriminate images from unseen classes by\nexploiting relations to seen classes via their attribute-based descriptions.\nSince attributes are often related to specific parts of objects, many recent\nworks focus on discovering discriminative regions. However, these methods\nusually require additional complex part detection modules or attention\nmechanisms. In this paper, 1) we show that common ZSL backbones (without\nexplicit attention nor part detection) can implicitly localize attributes, yet\nthis property is not exploited. 2) Exploiting it, we then propose SELAR, a\nsimple method that further encourages attribute localization, surprisingly\nachieving very competitive generalized ZSL (GZSL) performance when compared\nwith more complex state-of-the-art methods. Our findings provide useful insight\nfor designing future GZSL methods, and SELAR provides an easy to implement yet\nstrong baseline.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:31:37 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yang", "Shiqi", ""], ["Wang", "Kai", ""], ["Herranz", "Luis", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2103.04705", "submitter": "Shuaijun Chen", "authors": "Shuaijun Chen, Xu Jia, Jianzhong He, Yongjie Shi and Jianzhuang Liu", "title": "Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for\n  Semantic Segmentation", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven based approaches, in spite of great success in many tasks, have\npoor generalization when applied to unseen image domains, and require expensive\ncost of annotation especially for dense pixel prediction tasks such as semantic\nsegmentation. Recently, both unsupervised domain adaptation (UDA) from large\namounts of synthetic data and semi-supervised learning (SSL) with small set of\nlabeled data have been studied to alleviate this issue. However, there is still\na large gap on performance compared to their supervised counterparts. We focus\non a more practical setting of semi-supervised domain adaptation (SSDA) where\nboth a small set of labeled target data and large amounts of labeled source\ndata are available. To address the task of SSDA, a novel framework based on\ndual-level domain mixing is proposed. The proposed framework consists of three\nstages. First, two kinds of data mixing methods are proposed to reduce domain\ngap in both region-level and sample-level respectively. We can obtain two\ncomplementary domain-mixed teachers based on dual-level mixed data from\nholistic and partial views respectively. Then, a student model is learned by\ndistilling knowledge from these two teachers. Finally, pseudo labels of\nunlabeled data are generated in a self-training manner for another few rounds\nof teachers training. Extensive experimental results have demonstrated the\neffectiveness of our proposed framework on synthetic-to-real semantic\nsegmentation benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:33:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Shuaijun", ""], ["Jia", "Xu", ""], ["He", "Jianzhong", ""], ["Shi", "Yongjie", ""], ["Liu", "Jianzhuang", ""]]}, {"id": "2103.04708", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Jicong Zhang", "title": "Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation", "comments": "Accepted for PRCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning methods in medical image segmentation tasks\nusually requires a large amount of labeled data. However, obtaining reliable\nannotations is expensive and time-consuming. Semi-supervised learning has\nattracted much attention in medical image segmentation by taking the advantage\nof unlabeled data which is much easier to acquire. In this paper, we propose a\nnovel dual-task mutual learning framework for semi-supervised medical image\nsegmentation. Our framework can be formulated as an integration of two\nindividual segmentation networks based on two tasks: learning region-based\nshape constraint and learning boundary-based surface mismatch. Different from\nthe one-way transfer between teacher and student networks, an ensemble of\ndual-task students can learn collaboratively and implicitly explore useful\nknowledge from each other during the training process. By jointly learning the\nsegmentation probability maps and signed distance maps of targets, our\nframework can enforce the geometric shape constraint and learn more reliable\ninformation. Experimental results demonstrate that our method achieves\nperformance gains by leveraging unlabeled data and outperforms the\nstate-of-the-art semi-supervised segmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:38:23 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 11:43:42 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Yichi", ""], ["Zhang", "Jicong", ""]]}, {"id": "2103.04715", "submitter": "Remi Laumont", "authors": "R\\'emi Laumont, Valentin de Bortoli, Andr\\'es Almansa, Julie Delon,\n  Alain Durmus and Marcelo Pereyra", "title": "Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the seminal work of Venkatakrishnan et al. (2013), Plug & Play (PnP)\nmethods have become ubiquitous in Bayesian imaging. These methods derive\nMinimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for\ninverse problems in imaging by combining an explicit likelihood function with a\nprior that is implicitly defined by an image denoising algorithm. The PnP\nalgorithms proposed in the literature mainly differ in the iterative schemes\nthey use for optimisation or for sampling. In the case of optimisation schemes,\nsome recent works guarantee the convergence to a fixed point, albeit not\nnecessarily a MAP estimate. In the case of sampling schemes, to the best of our\nknowledge, there is no known proof of convergence. There also remain important\nopen questions regarding whether the underlying Bayesian models and estimators\nare well defined, well-posed, and have the basic regularity properties required\nto support these numerical schemes. To address these limitations, this paper\ndevelops theory, methods, and provably convergent algorithms for performing\nBayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA\n(Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference;\nand 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent\nresults on the quantitative convergence of Markov chains, we establish detailed\nconvergence guarantees for these two algorithms under realistic assumptions on\nthe denoising operators used, with special attention to denoisers based on deep\nneural networks. We also show that these algorithms approximately target a\ndecision-theoretically optimal Bayesian model that is well-posed. The proposed\nalgorithms are demonstrated on several canonical problems such as image\ndeblurring, inpainting, and denoising, where they are used for point estimation\nas well as for uncertainty visualisation and quantification.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:46:53 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 15:01:21 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 16:24:03 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 12:11:36 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Laumont", "R\u00e9mi", ""], ["de Bortoli", "Valentin", ""], ["Almansa", "Andr\u00e9s", ""], ["Delon", "Julie", ""], ["Durmus", "Alain", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "2103.04717", "submitter": "Jianzhong He", "authors": "Jianzhong He, Xu Jia, Shuaijun Chen, Jianzhuang Liu", "title": "Multi-Source Domain Adaptation with Collaborative Learning for Semantic\n  Segmentation", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-source unsupervised domain adaptation~(MSDA) aims at adapting models\ntrained on multiple labeled source domains to an unlabeled target domain. In\nthis paper, we propose a novel multi-source domain adaptation framework based\non collaborative learning for semantic segmentation. Firstly, a simple image\ntranslation method is introduced to align the pixel value distribution to\nreduce the gap between source domains and target domain to some extent. Then,\nto fully exploit the essential semantic information across source domains, we\npropose a collaborative learning method for domain adaptation without seeing\nany data from target domain. In addition, similar to the setting of\nunsupervised domain adaptation, unlabeled target domain data is leveraged to\nfurther improve the performance of domain adaptation. This is achieved by\nadditionally constraining the outputs of multiple adaptation models with pseudo\nlabels online generated by an ensembled model. Extensive experiments and\nablation studies are conducted on the widely-used domain adaptation benchmark\ndatasets in semantic segmentation. Our proposed method achieves 59.0\\% mIoU on\nthe validation set of Cityscapes by training on the labeled Synscapes and GTA5\ndatasets and unlabeled training set of Cityscapes. It significantly outperforms\nall previous state-of-the-arts single-source and multi-source unsupervised\ndomain adaptation methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:51:42 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 12:32:43 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 08:07:33 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["He", "Jianzhong", ""], ["Jia", "Xu", ""], ["Chen", "Shuaijun", ""], ["Liu", "Jianzhuang", ""]]}, {"id": "2103.04727", "submitter": "Patrick Wenzel", "authors": "Patrick Wenzel, Torsten Sch\\\"on, Laura Leal-Taix\\'e, Daniel Cremers", "title": "Vision-Based Mobile Robotics Obstacle Avoidance With Deep Reinforcement\n  Learning", "comments": "Accepted at 2021 IEEE International Conference on Robotics and\n  Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obstacle avoidance is a fundamental and challenging problem for autonomous\nnavigation of mobile robots. In this paper, we consider the problem of obstacle\navoidance in simple 3D environments where the robot has to solely rely on a\nsingle monocular camera. In particular, we are interested in solving this\nproblem without relying on localization, mapping, or planning techniques. Most\nof the existing work consider obstacle avoidance as two separate problems,\nnamely obstacle detection, and control. Inspired by the recent advantages of\ndeep reinforcement learning in Atari games and understanding highly complex\nsituations in Go, we tackle the obstacle avoidance problem as a data-driven\nend-to-end deep learning approach. Our approach takes raw images as input and\ngenerates control commands as output. We show that discrete action spaces are\noutperforming continuous control commands in terms of expected average reward\nin maze-like environments. Furthermore, we show how to accelerate the learning\nand increase the robustness of the policy by incorporating predicted depth maps\nby a generative adversarial network.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:05:46 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wenzel", "Patrick", ""], ["Sch\u00f6n", "Torsten", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Cremers", "Daniel", ""]]}, {"id": "2103.04731", "submitter": "Shinnosuke Matsuo", "authors": "Shinnosuke Matsuo, Seiichi Uchida, Brian Kenji Iwana", "title": "Self-Augmented Multi-Modal Feature Embedding", "comments": "Accepted at ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Oftentimes, patterns can be represented through different modalities. For\nexample, leaf data can be in the form of images or contours. Handwritten\ncharacters can also be either online or offline. To exploit this fact, we\npropose the use of self-augmentation and combine it with multi-modal feature\nembedding. In order to take advantage of the complementary information from the\ndifferent modalities, the self-augmented multi-modal feature embedding employs\na shared feature space. Through experimental results on classification with\nonline handwriting and leaf images, we demonstrate that the proposed method can\ncreate effective embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:10:52 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Matsuo", "Shinnosuke", ""], ["Uchida", "Seiichi", ""], ["Iwana", "Brian Kenji", ""]]}, {"id": "2103.04736", "submitter": "Rafael Padilha", "authors": "Rafael Padilha, Tawfiq Salem, Scott Workman, Fernanda A. Andal\\'o,\n  Anderson Rocha and Nathan Jacobs", "title": "Content-Based Detection of Temporal Metadata Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most pictures shared online are accompanied by a temporal context (i.e., the\nmoment they were taken) that aids their understanding and the history behind\nthem. Claiming that these images were captured in a different moment can be\nmisleading and help to convey a distorted version of reality. In this work, we\npresent the nascent problem of detecting timestamp manipulation. We propose an\nend-to-end approach to verify whether the purported time of capture of an image\nis consistent with its content and geographic location. The central idea is the\nuse of supervised consistency verification, in which we predict the probability\nthat the image content, capture time, and geographical location are consistent.\nWe also include a pair of auxiliary tasks, which can be used to explain the\nnetwork decision. Our approach improves upon previous work on a large benchmark\ndataset, increasing the classification accuracy from 59.03% to 81.07%. Finally,\nan ablation study highlights the importance of various components of the\nmethod, showing what types of tampering are detectable using our approach.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:16:19 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Padilha", "Rafael", ""], ["Salem", "Tawfiq", ""], ["Workman", "Scott", ""], ["Andal\u00f3", "Fernanda A.", ""], ["Rocha", "Anderson", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2103.04778", "submitter": "Wenkang Li", "authors": "Wenkang Li, Qi Ke, Wenbin Chen, Yicong Zhou", "title": "Bridging the Distribution Gap of Visible-Infrared Person\n  Re-identification with Modality Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible-infrared cross-modality person re-identification (VI-ReID), whose aim\nis to match person images between visible and infrared modality, is a\nchallenging cross-modality image retrieval task. Most existing works integrate\nbatch normalization layers into their neural network, but we found out that\nbatch normalization layers would lead to two types of distribution gap: 1)\ninter-mini-batch distribution gap -- the distribution gap of the same modality\nbetween each mini-batch; 2) intra-mini-batch modality distribution gap -- the\ndistribution gap of different modality within the same mini-batch. To address\nthese problems, we propose a new batch normalization layer called Modality\nBatch Normalization (MBN), which normalizes each modality sub-mini-batch\nrespectively instead of the whole mini-batch, and can reduce these distribution\ngap significantly. Extensive experiments show that our MBN is able to boost the\nperformance of VI-ReID models, even with different datasets, backbones and\nlosses.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 14:16:09 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Wenkang", ""], ["Ke", "Qi", ""], ["Chen", "Wenbin", ""], ["Zhou", "Yicong", ""]]}, {"id": "2103.04789", "submitter": "Qianyu Feng", "authors": "Qianyu Feng, Yawei Luo, Keyang Luo, Yi Yang", "title": "Look, Cast and Mold: Learning 3D Shape Manifold from Single-view\n  Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring the stereo structure of objects in the real world is a challenging\nyet practical task. To equip deep models with this ability usually requires\nabundant 3D supervision which is hard to acquire. It is promising that we can\nsimply benefit from synthetic data, where pairwise ground-truth is easy to\naccess. Nevertheless, the domain gaps are nontrivial considering the variant\ntexture, shape and context. To overcome these difficulties, we propose a\nVisio-Perceptual Adaptive Network for single-view 3D reconstruction, dubbed\nVPAN. To generalize the model towards a real scenario, we propose to fulfill\nseveral aspects: (1) Look: visually incorporate spatial structure from the\nsingle view to enhance the expressiveness of representation; (2) Cast:\nperceptually align the 2D image features to the 3D shape priors with\ncross-modal semantic contrastive mapping; (3) Mold: reconstruct stereo-shape of\ntarget by transforming embeddings into the desired manifold. Extensive\nexperiments on several benchmarks demonstrate the effectiveness and robustness\nof the proposed method in learning the 3D shape manifold from synthetic data\nvia a single-view. The proposed method outperforms state-of-the-arts on Pix3D\ndataset with IoU 0.292 and CD 0.108, and reaches IoU 0.329 and CD 0.104 on\nPascal 3D+.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 14:30:18 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 11:31:09 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Feng", "Qianyu", ""], ["Luo", "Yawei", ""], ["Luo", "Keyang", ""], ["Yang", "Yi", ""]]}, {"id": "2103.04813", "submitter": "Jizong Peng", "authors": "Jizong Peng and Marco Pedersoli and Christian Desrosiers", "title": "Boosting Semi-supervised Image Segmentation with Global and Local Mutual\n  Information Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of labeled data often impedes the application of deep learning\nto the segmentation of medical images. Semi-supervised learning seeks to\novercome this limitation by exploiting unlabeled examples in the learning\nprocess. In this paper, we present a novel semi-supervised segmentation method\nthat leverages mutual information (MI) on categorical distributions to achieve\nboth global representation invariance and local smoothness. In this method, we\nmaximize the MI for intermediate feature embeddings that are taken from both\nthe encoder and decoder of a segmentation network. We first propose a global MI\nloss constraining the encoder to learn an image representation that is\ninvariant to geometric transformations. Instead of resorting to\ncomputationally-expensive techniques for estimating the MI on continuous\nfeature embeddings, we use projection heads to map them to a discrete cluster\nassignment where MI can be computed efficiently. Our method also includes a\nlocal MI loss to promote spatial consistency in the feature maps of the decoder\nand provide a smoother segmentation. Since mutual information does not require\na strict ordering of clusters in two different assignments, we incorporate a\nfinal consistency regularization loss on the output which helps align the\ncluster labels throughout the network. We evaluate the method on four\nchallenging publicly-available datasets for medical image segmentation.\nExperimental results show our method to outperform recently-proposed approaches\nfor semi-supervised segmentation and provide an accuracy near to full\nsupervision while training with very few annotated images.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:13:25 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 06:25:06 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Peng", "Jizong", ""], ["Pedersoli", "Marco", ""], ["Desrosiers", "Christian", ""]]}, {"id": "2103.04814", "submitter": "Ding Jian", "authors": "Jian Ding, Enze Xie, Hang Xu, Chenhan Jiang, Zhenguo Li, Ping Luo,\n  Gui-Song Xia", "title": "Unsupervised Pretraining for Object Detection by Patch Reidentification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning achieves promising performances in\npre-training representations for object detectors. However, previous approaches\nare mainly designed for image-level classification, leading to suboptimal\ndetection performance. To bridge the performance gap, this work proposes a\nsimple yet effective representation learning method for object detection, named\npatch re-identification (Re-ID), which can be treated as a contrastive pretext\ntask to learn location-discriminative representation unsupervisedly, possessing\nappealing advantages compared to its counterparts. Firstly, unlike\nfully-supervised person Re-ID that matches a human identity in different camera\nviews, patch Re-ID treats an important patch as a pseudo identity and\ncontrastively learns its correspondence in two different image views, where the\npseudo identity has different translations and transformations, enabling to\nlearn discriminative features for object detection. Secondly, patch Re-ID is\nperformed in Deeply Unsupervised manner to learn multi-level representations,\nappealing to object detection. Thirdly, extensive experiments show that our\nmethod significantly outperforms its counterparts on COCO in all settings, such\nas different training iterations and data percentages. For example, Mask R-CNN\ninitialized with our representation surpasses MoCo v2 and even its\nfully-supervised counterparts in all setups of training iterations (e.g. 2.1\nand 1.1 mAP improvement compared to MoCo v2 in 12k and 90k iterations\nrespectively). Code will be released at https://github.com/dingjiansw101/DUPR.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:13:59 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ding", "Jian", ""], ["Xie", "Enze", ""], ["Xu", "Hang", ""], ["Jiang", "Chenhan", ""], ["Li", "Zhenguo", ""], ["Luo", "Ping", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2103.04838", "submitter": "Ramanpreet Pahwa Singh", "authors": "Ramanpreet S Pahwa, Soon Wee Ho, Ren Qin, Richard Chang, Oo Zaw Min,\n  Wang Jie, Vempati Srinivasa Rao, Tin Lay Nwe, Yanjing Yang, Jens Timo\n  Neumann, Ramani Pichumani, Thomas Gregorich", "title": "Machine-learning based methodologies for 3d x-ray measurement,\n  characterization and optimization for buried structures in advanced ic\n  packages", "comments": "7 pages, 9 figures", "journal-ref": "International Wafer-Level Packaging Conference (IWLPC) 2020", "doi": "10.23919/IWLPC52010.2020.9375903", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For over 40 years lithographic silicon scaling has driven circuit integration\nand performance improvement in the semiconductor industry. As silicon scaling\nslows down, the industry is increasingly dependent on IC package technologies\nto contribute to further circuit integration and performance improvements. This\nis a paradigm shift and requires the IC package industry to reduce the size and\nincrease the density of internal interconnects on a scale which has never been\ndone before. Traditional package characterization and process optimization\nrelies on destructive techniques such as physical cross-sections and delayering\nto extract data from internal package features. These destructive techniques\nare not practical with today's advanced packages. In this paper we will\ndemonstrate how data acquired non-destructively with a 3D X-ray microscope can\nbe enhanced and optimized using machine learning, and can then be used to\nmeasure, characterize and optimize the design and production of buried\ninterconnects in advanced IC packages. Test vehicles replicating 2.5D and HBM\nconstruction were designed and fabricated, and digital data was extracted from\nthese test vehicles using 3D X-ray and machine learning techniques. The\nextracted digital data was used to characterize and optimize the design and\nproduction of the interconnects and demonstrates a superior alternative to\ndestructive physical analysis. We report an mAP of 0.96 for 3D object\ndetection, a dice score of 0.92 for 3D segmentation, and an average of 2.1um\nerror for 3D metrology on the test dataset. This paper is the first part of a\nmulti-part report.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:44:18 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:13:02 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Pahwa", "Ramanpreet S", ""], ["Ho", "Soon Wee", ""], ["Qin", "Ren", ""], ["Chang", "Richard", ""], ["Min", "Oo Zaw", ""], ["Jie", "Wang", ""], ["Rao", "Vempati Srinivasa", ""], ["Nwe", "Tin Lay", ""], ["Yang", "Yanjing", ""], ["Neumann", "Jens Timo", ""], ["Pichumani", "Ramani", ""], ["Gregorich", "Thomas", ""]]}, {"id": "2103.04846", "submitter": "Fan Fu", "authors": "Fan Fu, Tingting Xie, Ioannis Patras, Sepehr Jalali", "title": "Relationship-based Neural Baby Talk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding interactions between objects in an image is an important\nelement for generating captions. In this paper, we propose a relationship-based\nneural baby talk (R-NBT) model to comprehensively investigate several types of\npairwise object interactions by encoding each image via three different\nrelationship-based graph attention networks (GATs). We study three main\nrelationships: \\textit{spatial relationships} to explore geometric\ninteractions, \\textit{semantic relationships} to extract semantic interactions,\nand \\textit{implicit relationships} to capture hidden information that could\nnot be modelled explicitly as above. We construct three relationship graphs\nwith the objects in an image as nodes, and the mutual relationships of pairwise\nobjects as edges. By exploring features of neighbouring regions individually\nvia GATs, we integrate different types of relationships into visual features of\neach node. Experiments on COCO dataset show that our proposed R-NBT model\noutperforms state-of-the-art models trained on COCO dataset in three image\ncaption generation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:51:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Fu", "Fan", ""], ["Xie", "Tingting", ""], ["Patras", "Ioannis", ""], ["Jalali", "Sepehr", ""]]}, {"id": "2103.04863", "submitter": "Mo Han", "authors": "Mo Han, Sezen Ya{\\u{g}}mur G\\\"unay, \\.Ilkay Y{\\i}ld{\\i}z, Paolo\n  Bonato, Cagdas D. Onal, Ta\\c{s}k{\\i}n Pad{\\i}r, Gunar Schirner, Deniz\n  Erdo{\\u{g}}mu\\c{s}", "title": "From Hand-Perspective Visual Information to Grasp Type Probabilities:\n  Deep Learning via Ranking Labels", "comments": null, "journal-ref": null, "doi": "10.1145/3316782.3316794", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limb deficiency severely affects the daily lives of amputees and drives\nefforts to provide functional robotic prosthetic hands to compensate this\ndeprivation. Convolutional neural network-based computer vision control of the\nprosthetic hand has received increased attention as a method to replace or\ncomplement physiological signals due to its reliability by training visual\ninformation to predict the hand gesture. Mounting a camera into the palm of a\nprosthetic hand is proved to be a promising approach to collect visual data.\nHowever, the grasp type labelled from the eye and hand perspective may differ\nas object shapes are not always symmetric. Thus, to represent this difference\nin a realistic way, we employed a dataset containing synchronous images from\neye- and hand- view, where the hand-perspective images are used for training\nwhile the eye-view images are only for manual labelling. Electromyogram (EMG)\nactivity and movement kinematics data from the upper arm are also collected for\nmulti-modal information fusion in future work. Moreover, in order to include\nhuman-in-the-loop control and combine the computer vision with physiological\nsignal inputs, instead of making absolute positive or negative predictions, we\nbuild a novel probabilistic classifier according to the Plackett-Luce model. To\npredict the probability distribution over grasps, we exploit the statistical\nmodel over label rankings to solve the permutation domain problems via a\nmaximum likelihood estimation, utilizing the manually ranked lists of grasps as\na new form of label. We indicate that the proposed model is applicable to the\nmost popular and productive convolutional neural network frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:12:38 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Han", "Mo", ""], ["G\u00fcnay", "Sezen Ya{\u011f}mur", ""], ["Y\u0131ld\u0131z", "\u0130lkay", ""], ["Bonato", "Paolo", ""], ["Onal", "Cagdas D.", ""], ["Pad\u0131r", "Ta\u015fk\u0131n", ""], ["Schirner", "Gunar", ""], ["Erdo{\u011f}mu\u015f", "Deniz", ""]]}, {"id": "2103.04870", "submitter": "Ankit Choudhary", "authors": "Ankit Choudhary and Deepak Mishra and Arnab Karmakar", "title": "Domain Adaptive Egocentric Person Re-identification", "comments": "12 pages, 4 figures, In Proceedings of the Fifth IAPR International\n  Conference on Computer Vision & Image Processing (CVIP), 2020", "journal-ref": null, "doi": "10.1007/978-981-16-1103-2_8", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Person re-identification (re-ID) in first-person (egocentric) vision is a\nfairly new and unexplored problem. With the increase of wearable video\nrecording devices, egocentric data becomes readily available, and person\nre-identification has the potential to benefit greatly from this. However,\nthere is a significant lack of large scale structured egocentric datasets for\nperson re-identification, due to the poor video quality and lack of individuals\nin most of the recorded content. Although a lot of research has been done in\nperson re-identification based on fixed surveillance cameras, these do not\ndirectly benefit egocentric re-ID. Machine learning models trained on the\npublicly available large scale re-ID datasets cannot be applied to egocentric\nre-ID due to the dataset bias problem. The proposed algorithm makes use of\nneural style transfer (NST) that incorporates a variant of Convolutional Neural\nNetwork (CNN) to utilize the benefits of both fixed camera vision and\nfirst-person vision. NST generates images having features from both egocentric\ndatasets and fixed camera datasets, that are fed through a VGG-16 network\ntrained on a fixed-camera dataset for feature extraction. These extracted\nfeatures are then used to re-identify individuals. The fixed camera dataset\nMarket-1501 and the first-person dataset EGO Re-ID are applied for this work\nand the results are on par with the present re-identification models in the\negocentric domain.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:19:32 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Choudhary", "Ankit", ""], ["Mishra", "Deepak", ""], ["Karmakar", "Arnab", ""]]}, {"id": "2103.04872", "submitter": "Dylan Stewart", "authors": "Dylan Stewart, Anna Hampton, Alina Zare, Jeff Dale, James Keller", "title": "The Weakly-Labeled Rand Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthetic Aperture Sonar (SAS) surveys produce imagery with large regions of\ntransition between seabed types. Due to these regions, it is difficult to label\nand segment the imagery and, furthermore, challenging to score the image\nsegmentations appropriately. While there are many approaches to quantify\nperformance in standard crisp segmentation schemes, drawing hard boundaries in\nremote sensing imagery where gradients and regions of uncertainty exist is\ninappropriate. These cases warrant weak labels and an associated appropriate\nscoring approach. In this paper, a labeling approach and associated modified\nversion of the Rand index for weakly-labeled data is introduced to address\nthese issues. Results are evaluated with the new index and compared to\ntraditional segmentation evaluation methods. Experimental results on a SAS data\nset containing must-link and cannot-link labels show that our Weakly-Labeled\nRand index scores segmentations appropriately in reference to qualitative\nperformance and is more suitable than traditional quantitative metrics for\nscoring weakly-labeled data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:21:15 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 02:58:37 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Stewart", "Dylan", ""], ["Hampton", "Anna", ""], ["Zare", "Alina", ""], ["Dale", "Jeff", ""], ["Keller", "James", ""]]}, {"id": "2103.04885", "submitter": "Takuya Kurihana", "authors": "Takuya Kurihana, Elisabeth Moyer, Rebecca Willett, Davis Gilton, and\n  Ian Foster", "title": "Data-driven Cloud Clustering via a Rotationally Invariant Autoencoder", "comments": "21 pages. 15 figures. Under review by IEEE Transactions on Geoscience\n  and Remote Sensing (TGRS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced satellite-born remote sensing instruments produce high-resolution\nmulti-spectral data for much of the globe at a daily cadence. These datasets\nopen up the possibility of improved understanding of cloud dynamics and\nfeedback, which remain the biggest source of uncertainty in global climate\nmodel projections. As a step towards answering these questions, we describe an\nautomated rotation-invariant cloud clustering (RICC) method that leverages deep\nlearning autoencoder technology to organize cloud imagery within large datasets\nin an unsupervised fashion, free from assumptions about predefined classes. We\ndescribe both the design and implementation of this method and its evaluation,\nwhich uses a sequence of testing protocols to determine whether the resulting\nclusters: (1) are physically reasonable, (i.e., embody scientifically relevant\ndistinctions); (2) capture information on spatial distributions, such as\ntextures; (3) are cohesive and separable in latent space; and (4) are\nrotationally invariant, (i.e., insensitive to the orientation of an image).\nResults obtained when these evaluation protocols are applied to RICC outputs\nsuggest that the resultant novel cloud clusters capture meaningful aspects of\ncloud physics, are appropriately spatially coherent, and are invariant to\norientations of input images. Our results support the possibility of using an\nunsupervised data-driven approach for automated clustering and pattern\ndiscovery in cloud imagery.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:45:14 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kurihana", "Takuya", ""], ["Moyer", "Elisabeth", ""], ["Willett", "Rebecca", ""], ["Gilton", "Davis", ""], ["Foster", "Ian", ""]]}, {"id": "2103.04893", "submitter": "Ildar Rakhmatulin", "authors": "Ildar Rakhmatulin", "title": "Deep learning, machine vision in agriculture in 2021", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decade, unprecedented progress in the development of neural\nnetworks influenced dozens of different industries, including weed recognition\nin the agro-industrial sector. The use of neural networks in agro-industrial\nactivity in the task of recognizing cultivated crops is a new direction. The\nabsence of any standards significantly complicates the understanding of the\nreal situation of the use of the neural network in the agricultural sector. The\nmanuscript presents the complete analysis of researches over the past 10 years\non the use of neural networks for the classification and tracking of weeds due\nto neural networks. In particular, the analysis of the results of using various\nneural network algorithms for the task of classification and tracking was\npresented. As a result, we presented the recommendation for the use of neural\nnetworks in the tasks of recognizing a cultivated object and weeds. Using this\nstandard can significantly improve the quality of research on this topic and\nsimplify the analysis and understanding of any paper.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 00:41:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Rakhmatulin", "Ildar", ""]]}, {"id": "2103.04912", "submitter": "Christopher Bendkowski", "authors": "Christopher Bendkowski, Laurent Mennillo, Tao Xu, Mohamed Elsayed,\n  Filip Stojic, Harrison Edwards, Shuailong Zhang, Cindi Morshead, Vijay Pawar,\n  Aaron R. Wheeler, Danail Stoyanov, Michael Shaw", "title": "Autonomous object harvesting using synchronized optoelectronic\n  microrobots", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Optoelectronic tweezer-driven microrobots (OETdMs) are a versatile\nmicromanipulation technology based on the use of light induced\ndielectrophoresis to move small dielectric structures (microrobots) across a\nphotoconductive substrate. The microrobots in turn can be used to exert forces\non secondary objects and carry out a wide range of micromanipulation\noperations, including collecting, transporting and depositing microscopic\ncargos. In contrast to alternative (direct) micromanipulation techniques,\nOETdMs are relatively gentle, making them particularly well suited to\ninteracting with sensitive objects such as biological cells. However, at\npresent such systems are used exclusively under manual control by a human\noperator. This limits the capacity for simultaneous control of multiple\nmicrorobots, reducing both experimental throughput and the possibility of\ncooperative multi-robot operations. In this article, we describe an approach to\nautomated targeting and path planning to enable open-loop control of multiple\nmicrorobots. We demonstrate the performance of the method in practice, using\nmicrorobots to simultaneously collect, transport and deposit silica\nmicrospheres. Using computational simulations based on real microscopic image\ndata, we investigate the capacity of microrobots to collect target cells from\nwithin a dissociated tissue culture. Our results indicate the feasibility of\nusing OETdMs to autonomously carry out micromanipulation tasks within complex,\nunstructured environments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:24:15 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bendkowski", "Christopher", ""], ["Mennillo", "Laurent", ""], ["Xu", "Tao", ""], ["Elsayed", "Mohamed", ""], ["Stojic", "Filip", ""], ["Edwards", "Harrison", ""], ["Zhang", "Shuailong", ""], ["Morshead", "Cindi", ""], ["Pawar", "Vijay", ""], ["Wheeler", "Aaron R.", ""], ["Stoyanov", "Danail", ""], ["Shaw", "Michael", ""]]}, {"id": "2103.04914", "submitter": "Sulabh Katiyar", "authors": "Sulabh Katiyar, Samir Kumar Borgohain", "title": "Analysis of Convolutional Decoder for Image Caption Generation", "comments": "18 pages, to be published in Book Series: Advances in Intelligent\n  Systems and Computing - ISSN 2194-5357", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently Convolutional Neural Networks have been proposed for Sequence\nModelling tasks such as Image Caption Generation. However, unlike Recurrent\nNeural Networks, the performance of Convolutional Neural Networks as Decoders\nfor Image Caption Generation has not been extensively studied. In this work, we\nanalyse various aspects of Convolutional Neural Network based Decoders such as\nNetwork complexity and depth, use of Data Augmentation, Attention mechanism,\nlength of sentences used during training, etc on performance of the model. We\nperform experiments using Flickr8k and Flickr30k image captioning datasets and\nobserve that unlike Recurrent Neural Network based Decoder, Convolutional\nDecoder for Image Captioning does not generally benefit from increase in\nnetwork depth, in the form of stacked Convolutional Layers, and also the use of\nData Augmentation techniques. In addition, use of Attention mechanism also\nprovides limited performance gains with Convolutional Decoder. Furthermore, we\nobserve that Convolutional Decoders show performance comparable with Recurrent\nDecoders only when trained using sentences of smaller length which contain up\nto 15 words but they have limitations when trained using higher sentence\nlengths which suggests that Convolutional Decoders may not be able to model\nlong-term dependencies efficiently. In addition, the Convolutional Decoder\nusually performs poorly on CIDEr evaluation metric as compared to Recurrent\nDecoder.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:25:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Katiyar", "Sulabh", ""], ["Borgohain", "Samir Kumar", ""]]}, {"id": "2103.04922", "submitter": "Sam Bond-Taylor", "authors": "Sam Bond-Taylor, Adam Leach, Yang Long, Chris G. Willcocks", "title": "Deep Generative Modelling: A Comparative Review of VAEs, GANs,\n  Normalizing Flows, Energy-Based and Autoregressive Models", "comments": "20 pages, 10 figures, updated version submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative modelling is a class of techniques that train deep neural\nnetworks to model the distribution of training samples. Research has fragmented\ninto various interconnected approaches, each of which making trade-offs\nincluding run-time, diversity, and architectural restrictions. In particular,\nthis compendium covers energy-based models, variational autoencoders,\ngenerative adversarial networks, autoregressive models, normalizing flows, in\naddition to numerous hybrid approaches. These techniques are drawn under a\nsingle cohesive framework, comparing and contrasting to explain the premises\nbehind each, while reviewing current state-of-the-art advances and\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:34:03 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 12:50:13 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bond-Taylor", "Sam", ""], ["Leach", "Adam", ""], ["Long", "Yang", ""], ["Willcocks", "Chris G.", ""]]}, {"id": "2103.04958", "submitter": "Xiaofan Zhang", "authors": "Xiaofan Zhang, Dawei Wang, Pierce Chuang, Shugao Ma, Deming Chen,\n  Yuecheng Li", "title": "F-CAD: A Framework to Explore Hardware Accelerators for Codec Avatar\n  Decoding", "comments": "Published as a conference paper at Design Automation Conference 2021\n  (DAC'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Creating virtual avatars with realistic rendering is one of the most\nessential and challenging tasks to provide highly immersive virtual reality\n(VR) experiences. It requires not only sophisticated deep neural network (DNN)\nbased codec avatar decoders to ensure high visual quality and precise motion\nexpression, but also efficient hardware accelerators to guarantee smooth\nreal-time rendering using lightweight edge devices, like untethered VR\nheadsets. Existing hardware accelerators, however, fail to deliver sufficient\nperformance and efficiency targeting such decoders which consist of\nmulti-branch DNNs and require demanding compute and memory resources. To\naddress these problems, we propose an automation framework, called F-CAD\n(Facebook Codec avatar Accelerator Design), to explore and deliver optimized\nhardware accelerators for codec avatar decoding. Novel technologies include 1)\na new accelerator architecture to efficiently handle multi-branch DNNs; 2) a\nmulti-branch dynamic design space to enable fine-grained architecture\nconfigurations; and 3) an efficient architecture search for picking the\noptimized hardware design based on both application-specific demands and\nhardware resource constraints. To the best of our knowledge, F-CAD is the first\nautomation tool that supports the whole design flow of hardware acceleration of\ncodec avatar decoders, allowing joint optimization on decoder designs in\npopular machine learning frameworks and corresponding customized accelerator\ndesign with cycle-accurate evaluation. Results show that the accelerators\ngenerated by F-CAD can deliver up to 122.1 frames per second (FPS) and 91.6%\nhardware efficiency when running the latest codec avatar decoder. Compared to\nthe state-of-the-art designs, F-CAD achieves 4.0X and 2.8X higher throughput,\n62.5% and 21.2% higher efficiency than DNNBuilder and HybridDNN by targeting\nthe same hardware device.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:28:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Xiaofan", ""], ["Wang", "Dawei", ""], ["Chuang", "Pierce", ""], ["Ma", "Shugao", ""], ["Chen", "Deming", ""], ["Li", "Yuecheng", ""]]}, {"id": "2103.04970", "submitter": "Niklas K\\\"uhl Dr", "authors": "Christoph Sager, Patrick Zschech, Niklas K\\\"uhl", "title": "labelCloud: A Lightweight Domain-Independent Labeling Tool for 3D Object\n  Detection in Point Clouds", "comments": "Preprint accepted for archival and presentation at CAD 21 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Within the past decade, the rise of applications based on artificial\nintelligence (AI) in general and machine learning (ML) in specific has led to\nmany significant contributions within different domains. The applications range\nfrom robotics over medical diagnoses up to autonomous driving. However, nearly\nall applications rely on trained data. In case this data consists of 3D images,\nit is of utmost importance that the labeling is as accurate as possible to\nensure high-quality outcomes of the ML models. Labeling in the 3D space is\nmostly manual work performed by expert workers, where they draw 3D bounding\nboxes around target objects the ML model should later automatically identify,\ne.g., pedestrians for autonomous driving or cancer cells within radiography.\n  While a small range of recent 3D labeling tools exist, they all share three\nmajor shortcomings: (i) they are specified for autonomous driving applications,\n(ii) they lack convenience and comfort functions, and (iii) they have high\ndependencies and little flexibility in data format. Therefore, we propose a\nnovel labeling tool for 3D object detection in point clouds to address these\nshortcomings.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 09:32:47 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Sager", "Christoph", ""], ["Zschech", "Patrick", ""], ["K\u00fchl", "Niklas", ""]]}, {"id": "2103.04980", "submitter": "Dongdong Chen", "authors": "Jie Zhang and Dongdong Chen and Jing Liao and Weiming Zhang and Huamin\n  Feng and Gang Hua and Nenghai Yu", "title": "Deep Model Intellectual Property Protection via Deep Watermarking", "comments": "To appear at TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the tremendous success, deep neural networks are exposed to serious\nIP infringement risks. Given a target deep model, if the attacker knows its\nfull information, it can be easily stolen by fine-tuning. Even if only its\noutput is accessible, a surrogate model can be trained through student-teacher\nlearning by generating many input-output training pairs. Therefore, deep model\nIP protection is important and necessary. However, it is still seriously\nunder-researched. In this work, we propose a new model watermarking framework\nfor protecting deep networks trained for low-level computer vision or image\nprocessing tasks. Specifically, a special task-agnostic barrier is added after\nthe target model, which embeds a unified and invisible watermark into its\noutputs. When the attacker trains one surrogate model by using the input-output\npairs of the barrier target model, the hidden watermark will be learned and\nextracted afterwards. To enable watermarks from binary bits to high-resolution\nimages, a deep invisible watermarking mechanism is designed. By jointly\ntraining the target model and watermark embedding, the extra barrier can even\nbe absorbed into the target model. Through extensive experiments, we\ndemonstrate the robustness of the proposed framework, which can resist attacks\nwith different network structures and objective functions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:58:21 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Jie", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Zhang", "Weiming", ""], ["Feng", "Huamin", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2103.04989", "submitter": "Varun Mannam", "authors": "Varun Mannam, Yide Zhang, Xiaotong Yuan, and Scott Howard", "title": "Deep learning-based super-resolution fluorescence microscopy on small\n  datasets", "comments": "SPIE Proceedings Volume 11650, Single Molecule Spectroscopy and\n  Superresolution Imaging XIV; 116500O (2021)", "journal-ref": null, "doi": "10.1117/12.2578519", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluorescence microscopy has enabled a dramatic development in modern biology\nby visualizing biological organisms with micrometer scale resolution. However,\ndue to the diffraction limit, sub-micron/nanometer features are difficult to\nresolve. While various super-resolution techniques are developed to achieve\nnanometer-scale resolution, they often either require expensive optical setup\nor specialized fluorophores. In recent years, deep learning has shown the\npotentials to reduce the technical barrier and obtain super-resolution from\ndiffraction-limited images. For accurate results, conventional deep learning\ntechniques require thousands of images as a training dataset. Obtaining large\ndatasets from biological samples is not often feasible due to the\nphotobleaching of fluorophores, phototoxicity, and dynamic processes occurring\nwithin the organism. Therefore, achieving deep learning-based super-resolution\nusing small datasets is challenging. We address this limitation with a new\nconvolutional neural network-based approach that is successfully trained with\nsmall datasets and achieves super-resolution images. We captured 750 images in\ntotal from 15 different field-of-views as the training dataset to demonstrate\nthe technique. In each FOV, a single target image is generated using the\nsuper-resolution radial fluctuation method. As expected, this small dataset\nfailed to produce a usable model using traditional super-resolution\narchitecture. However, using the new approach, a network can be trained to\nachieve super-resolution images from this small dataset. This deep learning\nmodel can be applied to other biomedical imaging modalities such as MRI and\nX-ray imaging, where obtaining large training datasets is challenging.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 03:17:47 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Mannam", "Varun", ""], ["Zhang", "Yide", ""], ["Yuan", "Xiaotong", ""], ["Howard", "Scott", ""]]}, {"id": "2103.04990", "submitter": "Lumeng Cao", "authors": "Lumeng Cao, Zhouwang Yang", "title": "Use square root affinity to regress labels in semantic segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation is a basic but non-trivial task in computer vision.\nMany previous work focus on utilizing affinity patterns to enhance segmentation\nnetworks. Most of these studies use the affinity matrix as a kind of feature\nfusion weights, which is part of modules embedded in the network, such as\nattention models and non-local models. In this paper, we associate affinity\nmatrix with labels, exploiting the affinity in a supervised way. Specifically,\nwe utilize the label to generate a multi-scale label affinity matrix as a\nstructural supervision, and we use a square root kernel to compute a non-local\naffinity matrix on output layers. With such two affinities, we define a novel\nloss called Affinity Regression loss (AR loss), which can be an auxiliary loss\nproviding pair-wise similarity penalty. Our model is easy to train and adds\nlittle computational burden without run-time inference. Extensive experiments\non NYUv2 dataset and Cityscapes dataset demonstrate that our proposed method is\nsufficient in promoting semantic segmentation networks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 12:49:27 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Cao", "Lumeng", ""], ["Yang", "Zhouwang", ""]]}, {"id": "2103.05041", "submitter": "David Rosen", "authors": "David M. Rosen, Kevin J. Doherty, Antonio Teran Espinoza, John J.\n  Leonard", "title": "Advances in Inference and Representation for Simultaneous Localization\n  and Mapping", "comments": "30 pages, 4 figures. To appear in Annual Review of Control, Robotics,\n  and Autonomous Systems 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous localization and mapping (SLAM) is the process of constructing a\nglobal model of an environment from local observations of it; this is a\nfoundational capability for mobile robots, supporting such core functions as\nplanning, navigation, and control. This article reviews recent progress in\nSLAM, focusing on advances in the expressive capacity of the environmental\nmodels used in SLAM systems (representation) and the performance of the\nalgorithms used to estimate these models from data (inference). A prominent\ntheme of recent SLAM research is the pursuit of environmental representations\n(including learned representations) that go beyond the classical attributes of\ngeometry and appearance to model properties such as hierarchical organization,\naffordance, dynamics, and semantics; these advances equip autonomous agents\nwith a more comprehensive understanding of the world, enabling more versatile\nand intelligent operation. A second major theme is a revitalized interest in\nthe mathematical properties of the SLAM estimation problem itself (including\nits computational and information-theoretic performance limits); this work has\nled to the development of novel classes of certifiable and robust inference\nmethods that dramatically improve the reliability of SLAM systems in real-world\noperation. We survey these advances with an emphasis on their ramifications for\nachieving robust, long-duration autonomy, and conclude with a discussion of\nopen challenges and a perspective on future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 19:53:29 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Rosen", "David M.", ""], ["Doherty", "Kevin J.", ""], ["Espinoza", "Antonio Teran", ""], ["Leonard", "John J.", ""]]}, {"id": "2103.05056", "submitter": "Daniele Cattaneo", "authors": "Daniele Cattaneo, Matteo Vaghi, Abhinav Valada", "title": "LCDNet: Deep Loop Closure Detection and Point Cloud Registration for\n  LiDAR SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop closure detection is an essential component of Simultaneous Localization\nand Mapping (SLAM) systems, which reduces the drift accumulated over time. Over\nthe years, several deep learning approaches have been proposed to address this\ntask, however their performance has been subpar compared to handcrafted\ntechniques, especially while dealing with reverse loops. In this paper, we\nintroduce the novel LCDNet that effectively detects loop closures in LiDAR\npoint clouds by simultaneously identifying previously visited places and\nestimating the 6-DoF relative transformation between the current scan and the\nmap. LCDNet is composed of a shared encoder, a place recognition head that\nextracts global descriptors, and a relative pose head that estimates the\ntransformation between two point clouds. We introduce a novel relative pose\nhead based on the unbalanced optimal transport theory that we implement in a\ndifferentiable manner to allow for end-to-end training. Extensive evaluations\nof LCDNet on multiple real-world autonomous driving datasets show that our\napproach outperforms state-of-the-art loop closure detection and point cloud\nregistration techniques by a large margin, especially while dealing with\nreverse loops. Moreover, we integrate our proposed loop closure detection\napproach into a LiDAR SLAM library to provide a complete mapping system and\ndemonstrate the generalization ability using different sensor setup in an\nunseen city.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 20:19:37 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 16:27:51 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cattaneo", "Daniele", ""], ["Vaghi", "Matteo", ""], ["Valada", "Abhinav", ""]]}, {"id": "2103.05073", "submitter": "Charles Ruizhongtai Qi", "authors": "Charles R. Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng,\n  Dragomir Anguelov", "title": "Offboard 3D Object Detection from Point Cloud Sequences", "comments": "18 pages, 7 figures, 19 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current 3D object recognition research mostly focuses on the real-time,\nonboard scenario, there are many offboard use cases of perception that are\nlargely under-explored, such as using machines to automatically generate\nhigh-quality 3D labels. Existing 3D object detectors fail to satisfy the\nhigh-quality requirement for offboard uses due to the limited input and speed\nconstraints. In this paper, we propose a novel offboard 3D object detection\npipeline using point cloud sequence data. Observing that different frames\ncapture complementary views of objects, we design the offboard detector to make\nuse of the temporal points through both multi-frame object detection and novel\nobject-centric refinement models. Evaluated on the Waymo Open Dataset, our\npipeline named 3D Auto Labeling shows significant gains compared to the\nstate-of-the-art onboard detectors and our offboard baselines. Its performance\nis even on par with human labels verified through a human label study. Further\nexperiments demonstrate the application of auto labels for semi-supervised\nlearning and provide extensive analysis to validate various design choices.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:02:37 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Qi", "Charles R.", ""], ["Zhou", "Yin", ""], ["Najibi", "Mahyar", ""], ["Sun", "Pei", ""], ["Vo", "Khoa", ""], ["Deng", "Boyang", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "2103.05086", "submitter": "Kunal Chelani", "authors": "Kunal Chelani and Fredrik Kahl and Torsten Sattler", "title": "How Privacy-Preserving are Line Clouds? Recovering Scene Details from 3D\n  Lines", "comments": "Computer Vision and Pattern Recognition (CVPR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual localization is the problem of estimating the camera pose of a given\nimage with respect to a known scene. Visual localization algorithms are a\nfundamental building block in advanced computer vision applications, including\nMixed and Virtual Reality systems. Many algorithms used in practice represent\nthe scene through a Structure-from-Motion (SfM) point cloud and use 2D-3D\nmatches between a query image and the 3D points for camera pose estimation. As\nrecently shown, image details can be accurately recovered from SfM point clouds\nby translating renderings of the sparse point clouds to images. To address the\nresulting potential privacy risks for user-generated content, it was recently\nproposed to lift point clouds to line clouds by replacing 3D points by randomly\noriented 3D lines passing through these points. The resulting representation is\nunintelligible to humans and effectively prevents point cloud-to-image\ntranslation. This paper shows that a significant amount of information about\nthe 3D scene geometry is preserved in these line clouds, allowing us to\n(approximately) recover the 3D point positions and thus to (approximately)\nrecover image content. Our approach is based on the observation that the\nclosest points between lines can yield a good approximation to the original 3D\npoints. Code is available at https://github.com/kunalchelani/Line2Point.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:32:43 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chelani", "Kunal", ""], ["Kahl", "Fredrik", ""], ["Sattler", "Torsten", ""]]}, {"id": "2103.05094", "submitter": "Abdul Waheed", "authors": "Abdul Waheed, Muskan Goyal, Deepak Gupta, Ashish Khanna, Fadi\n  Al-Turjman, Placido Rogerio Pinheiro", "title": "CovidGAN: Data Augmentation Using Auxiliary Classifier GAN for Improved\n  Covid-19 Detection", "comments": "Accepted at IEEE Access. Received April 30, 2020, accepted May 11,\n  2020, date of publication May 14, 2020, date of current version May 28, 2020", "journal-ref": "IEEE Access, vol. 8, pp. 91916-91923, 2020", "doi": "10.1109/ACCESS.2020.2994762", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronavirus (COVID-19) is a viral disease caused by severe acute respiratory\nsyndrome coronavirus 2 (SARS-CoV-2). The spread of COVID-19 seems to have a\ndetrimental effect on the global economy and health. A positive chest X-ray of\ninfected patients is a crucial step in the battle against COVID-19. Early\nresults suggest that abnormalities exist in chest X-rays of patients suggestive\nof COVID-19. This has led to the introduction of a variety of deep learning\nsystems and studies have shown that the accuracy of COVID-19 patient detection\nthrough the use of chest X-rays is strongly optimistic. Deep learning networks\nlike convolutional neural networks (CNNs) need a substantial amount of training\ndata. Because the outbreak is recent, it is difficult to gather a significant\nnumber of radiographic images in such a short time. Therefore, in this\nresearch, we present a method to generate synthetic chest X-ray (CXR) images by\ndeveloping an Auxiliary Classifier Generative Adversarial Network (ACGAN) based\nmodel called CovidGAN. In addition, we demonstrate that the synthetic images\nproduced from CovidGAN can be utilized to enhance the performance of CNN for\nCOVID-19 detection. Classification using CNN alone yielded 85% accuracy. By\nadding synthetic images produced by CovidGAN, the accuracy increased to 95%. We\nhope this method will speed up COVID-19 detection and lead to more robust\nsystems of radiology.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:53:29 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Waheed", "Abdul", ""], ["Goyal", "Muskan", ""], ["Gupta", "Deepak", ""], ["Khanna", "Ashish", ""], ["Al-Turjman", "Fadi", ""], ["Pinheiro", "Placido Rogerio", ""]]}, {"id": "2103.05099", "submitter": "Suiyi Ling", "authors": "Shaoguo Wen, Suiyi Ling, Junle Wang, Ximing Chen, Lizhi Fang, Yanqing\n  Jing, Patrick Le Callet", "title": "Subjective and Objective Quality Assessment of Mobile Gaming Video", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, with the vigorous expansion and development of gaming video\nstreaming techniques and services, the expectation of users, especially the\nmobile phone users, for higher quality of experience is also growing swiftly.\nAs most of the existing research focuses on traditional video streaming, there\nis a clear lack of both subjective study and objective quality models that are\ntailored for quality assessment of mobile gaming content. To this end, in this\nstudy, we first present a brand new Tencent Gaming Video dataset containing\n1293 mobile gaming sequences encoded with three different codecs. Second, we\npropose an objective quality framework, namely Efficient hard-RAnk Quality\nEstimator (ERAQUE), that is equipped with (1) a novel hard pairwise ranking\nloss, which forces the model to put more emphasis on differentiating similar\npairs; (2) an adapted model distillation strategy, which could be utilized to\ncompress the proposed model efficiently without causing significant performance\ndrop. Extensive experiments demonstrate the efficiency and robustness of our\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 19:48:15 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wen", "Shaoguo", ""], ["Ling", "Suiyi", ""], ["Wang", "Junle", ""], ["Chen", "Ximing", ""], ["Fang", "Lizhi", ""], ["Jing", "Yanqing", ""], ["Callet", "Patrick Le", ""]]}, {"id": "2103.05100", "submitter": "Jochen Triesch", "authors": "Zhetuo Zhao, Jochen Triesch, Bertram E. Shi", "title": "Learning Hierarchical Integration of Foveal and Peripheral Vision for\n  Vergence Control by Active Efficient Coding", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-97628-0_7", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The active efficient coding (AEC) framework parsimoniously explains the joint\ndevelopment of visual processing and eye movements, e.g., the emergence of\nbinocular disparity selective neurons and fusional vergence, the disjunctive\neye movements that align left and right eye images. Vergence can be driven by\ninformation in both the fovea and periphery, which play complementary roles.\nThe high resolution fovea can drive precise short range movements. The lower\nresolution periphery supports coarser long range movements. The fovea and\nperiphery may also contain conflicting information, e.g. due to objects at\ndifferent depths. While past AEC models did integrate peripheral and foveal\ninformation, they did not explicitly take into account these characteristics.\nWe propose here a two-level hierarchical approach that does. The bottom level\ngenerates different vergence actions from foveal and peripheral regions. The\ntop level selects one. We demonstrate that the hierarchical approach performs\nbetter than prior approaches in realistic environments, exhibiting better\nalignment and less oscillation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:53:42 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhao", "Zhetuo", ""], ["Triesch", "Jochen", ""], ["Shi", "Bertram E.", ""]]}, {"id": "2103.05101", "submitter": "Aytekin Nebisoy", "authors": "Aytekin Nebisoy and Saber Malekzadeh", "title": "Video Action Recognition Using spatio-temporal optical flow video frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing human actions based on videos has became one of the most popular\nareas of research in computer vision in recent years. This area has many\napplications such as surveillance, robotics, health care, video search and\nhuman-computer interaction. There are many problems associated with recognizing\nhuman actions in videos such as cluttered backgrounds, obstructions, viewpoints\nvariation, execution speed and camera movement. A large number of methods have\nbeen proposed to solve the problems. This paper focus on spatial and temporal\npattern recognition for the classification of videos using Deep Neural\nNetworks. This model takes RGB images and Optical Flow as input data and\noutputs an action class number. The final recognition accuracy was about 94%.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 19:46:49 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Nebisoy", "Aytekin", ""], ["Malekzadeh", "Saber", ""]]}, {"id": "2103.05102", "submitter": "Sudipan Saha", "authors": "Sudipan Saha, Patrick Ebel, Xiao Xiang Zhu", "title": "Self-supervised Multisensor Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most change detection methods assume that pre-change and post-change images\nare acquired by the same sensor. However, in many real-life scenarios, e.g.,\nnatural disaster, it is more practical to use the latest available images\nbefore and after the occurrence of incidence, which may be acquired using\ndifferent sensors. In particular, we are interested in the combination of the\nimages acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR\nimages appear vastly different from the optical images even when capturing the\nsame scene. Adding to this, change detection methods are often constrained to\nuse only target image-pair, no labeled data, and no additional unlabeled data.\nSuch constraints limit the scope of traditional supervised machine learning and\nunsupervised generative approaches for multi-sensor change detection. Recent\nrapid development of self-supervised learning methods has shown that some of\nthem can even work with only few images. Motivated by this, in this work we\npropose a method for multi-sensor change detection using only the unlabeled\ntarget bi-temporal images that are used for training a network in\nself-supervised fashion by using deep clustering and contrastive learning. The\nproposed method is evaluated on four multi-modal bi-temporal scenes showing\nchange and the benefits of our self-supervised approach are demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 12:31:10 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 11:06:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Saha", "Sudipan", ""], ["Ebel", "Patrick", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2103.05103", "submitter": "Shikha Dubey", "authors": "Farrukh Olimov, Shikha Dubey, Labina Shrestha, Tran Trung Tin, Moongu\n  Jeon", "title": "Image Captioning using Multiple Transformers for Self-Attention\n  Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time image captioning, along with adequate precision, is the main\nchallenge of this research field. The present work, Multiple Transformers for\nSelf-Attention Mechanism (MTSM), utilizes multiple transformers to address\nthese problems. The proposed algorithm, MTSM, acquires region proposals using a\ntransformer detector (DETR). Consequently, MTSM achieves the self-attention\nmechanism by transferring these region proposals and their visual and\ngeometrical features through another transformer and learns the objects' local\nand global interconnections. The qualitative and quantitative results of the\nproposed algorithm, MTSM, are shown on the MSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 05:35:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Olimov", "Farrukh", ""], ["Dubey", "Shikha", ""], ["Shrestha", "Labina", ""], ["Tin", "Tran Trung", ""], ["Jeon", "Moongu", ""]]}, {"id": "2103.05104", "submitter": "Ali Al-Sharadqah", "authors": "Ali A. Al-Sharadqah and Lorenzo Rull", "title": "New Methods for Detecting Concentric Objects With High Accuracy", "comments": "31 pages, 18 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fitting concentric geometric objects to digitized data is an important\nproblem in many areas such as iris detection, autonomous navigation, and\nindustrial robotics operations. There are two common approaches to fitting\ngeometric shapes to data: the geometric (iterative) approach and algebraic\n(non-iterative) approach. The geometric approach is a nonlinear iterative\nmethod that minimizes the sum of the squares of Euclidean distances of the\nobserved points to the ellipses and regarded as the most accurate method, but\nit needs a good initial guess to improve the convergence rate. The algebraic\napproach is based on minimizing the algebraic distances with some constraints\nimposed on parametric space. Each algebraic method depends on the imposed\nconstraint, and it can be solved with the aid of the generalized eigenvalue\nproblem. Only a few methods in literature were developed to solve the problem\nof concentric ellipses. Here we study the statistical properties of existing\nmethods by firstly establishing a general mathematical and statistical\nframework for this problem. Using rigorous perturbation analysis, we derive the\nvariances and biasedness of each method under the small-sigma model. We also\ndevelop new estimators, which can be used as reliable initial guesses for other\niterative methods. Then we compare the performance of each method according to\ntheir theoretical accuracy. Not only do our methods described here outperform\nother existing non-iterative methods, they are also quite robust against large\nnoise. These methods and their practical performances are assessed by a series\nof numerical experiments on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 08:19:18 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Al-Sharadqah", "Ali A.", ""], ["Rull", "Lorenzo", ""]]}, {"id": "2103.05105", "submitter": "Muhammad Kashif", "authors": "Muhammad Kashif", "title": "Urdu Handwritten Text Recognition Using ResNet18", "comments": "6 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handwritten text recognition is an active research area in the field of deep\nlearning and artificial intelligence to convert handwritten text into\nmachine-understandable. A lot of work has been done for other languages,\nespecially for English, but work for the Urdu language is very minimal due to\nthe cursive nature of Urdu characters. The need for Urdu HCR systems is\nincreasing because of the advancement of technology. In this paper, we propose\na ResNet18 model for handwritten text recognition using Urdu Nastaliq\nHandwritten Dataset (UNHD) which contains 3,12000 words written by 500\ncandidates.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 17:55:57 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Kashif", "Muhammad", ""]]}, {"id": "2103.05107", "submitter": "Wenshan Wang", "authors": "Wenshan Wang, Su Yang, and Weishan Zhang", "title": "Risk Prediction on Traffic Accidents using a Compact Neural Model for\n  Multimodal Information Fusion over Urban Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting risk map of traffic accidents is vital for accident prevention and\nearly planning of emergency response. Here, the challenge lies in the\nmultimodal nature of urban big data. We propose a compact neural ensemble model\nto alleviate overfitting in fusing multimodal features and develop some new\nfeatures such as fractal measure of road complexity in satellite images, taxi\nflows, POIs, and road width and connectivity in OpenStreetMap. The solution is\nmore promising in performance than the baseline methods and the single-modality\ndata based solutions. After visualization from a micro view, the visual\npatterns of the scenes related to high and low risk are revealed, providing\nlessons for future road design. From city point of view, the predicted risk map\nis close to the ground truth, and can act as the base in optimizing spatial\nconfiguration of resources for emergency response, and alarming signs. To the\nbest of our knowledge, it is the first work to fuse visual and spatio-temporal\nfeatures in traffic accident prediction while advances to bridge the gap\nbetween data mining based urban computing and computer vision based urban\nperception.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:21:19 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wang", "Wenshan", ""], ["Yang", "Su", ""], ["Zhang", "Weishan", ""]]}, {"id": "2103.05108", "submitter": "Jessica Cooper", "authors": "Jessica Cooper, Ognjen Arandjelovi\\'c, David J Harrison", "title": "Believe The HiPe: Hierarchical Perturbation for Fast, Robust and\n  Model-Agnostic Explanations", "comments": "github.com/jessicamarycooper/Hierarchical-Perturbation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the predictions made by Artificial Intelligence (AI) systems is\nbecoming more and more important as deep learning models are used for\nincreasingly complex and high-stakes tasks. Saliency mapping - an easily\ninterpretable visual attribution method - is one important tool for this, but\nexisting formulations are limited by either computational cost or architectural\nconstraints. We therefore propose Hierarchical Perturbation, a very fast and\ncompletely model-agnostic method for explaining model predictions with robust\nsaliency maps. Using standard benchmarks and datasets, we show that our\nsaliency maps are of competitive or superior quality to those generated by\nexisting model-agnostic methods - and are over 20X faster to compute.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:22:56 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 10:15:44 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Cooper", "Jessica", ""], ["Arandjelovi\u0107", "Ognjen", ""], ["Harrison", "David J", ""]]}, {"id": "2103.05109", "submitter": "Heng Hao", "authors": "Heng Hao, Sima Didari, Jae Oh Woo, Hankyu Moon, and Patrick Bangert", "title": "Highly Efficient Representation and Active Learning Framework for\n  Imbalanced Data and its Application to COVID-19 X-Ray Classification", "comments": "Submitted to ECML KDD ML journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a highly data-efficient classification and active learning\nframework for classifying chest X-rays. It is based on (1) unsupervised\nrepresentation learning of a Convolutional Neural Network and (2) the Gaussian\nProcess method. The unsupervised representation learning employs\nself-supervision that does not require class labels, and the learned features\nare proven to achieve label-efficient classification. GP is a kernel-based\nBayesian approach that also leads to data-efficient predictions with the added\nbenefit of estimating each decision's uncertainty. Our novel framework combines\nthese two elements in sequence to achieve highly data and label efficient\nclassifications. Moreover, both elements are less sensitive to the prevalent\nand challenging class imbalance issue, thanks to the (1) feature learned\nwithout labels and (2) the Bayesian nature of GP. The GP-provided uncertainty\nestimates enable active learning by ranking samples based on the uncertainty\nand selectively labeling samples showing higher uncertainty. We apply this\nnovel combination to the data-deficient and severely imbalanced case of\nCOVID-19 chest X-ray classification. We demonstrate that only $\\sim 10\\%$ of\nthe labeled data is needed to reach the accuracy from training all available\nlabels. Its application to the COVID-19 data in a fully supervised\nclassification scenario shows that our model, with a generic ResNet backbone,\noutperforms (COVID-19 case by 4\\%) the state-of-the-art model with a highly\ntuned architecture. Our model architecture and proposed framework are general\nand straightforward to apply to a broader class of datasets, with expected\nsuccess.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 02:48:59 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 21:26:35 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 16:33:13 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hao", "Heng", ""], ["Didari", "Sima", ""], ["Woo", "Jae Oh", ""], ["Moon", "Hankyu", ""], ["Bangert", "Patrick", ""]]}, {"id": "2103.05110", "submitter": "Heiko Paulheim", "authors": "Babette B\\\"uhler and Heiko Paulheim", "title": "Web Table Classification based on Visual Features", "comments": "Accepted at International Conference of Web Engineering (ICWE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables on the web constitute a valuable data source for many applications,\nlike factual search and knowledge base augmentation. However, as genuine tables\ncontaining relational knowledge only account for a small proportion of tables\non the web, reliable genuine web table classification is a crucial first step\nof table extraction. Previous works usually rely on explicit feature\nconstruction from the HTML code. In contrast, we propose an approach for web\ntable classification by exploiting the full visual appearance of a table, which\nworks purely by applying a convolutional neural network on the rendered image\nof the web table. Since these visual features can be extracted automatically,\nour approach circumvents the need for explicit feature construction. A new hand\nlabeled gold standard dataset containing HTML source code and images for 13,112\ntables was generated for this task. Transfer learning techniques are applied to\nwell known VGG16 and ResNet50 architectures. The evaluation of CNN image\nclassification with fine tuned ResNet50 (F1 93.29%) shows that this approach\nachieves results comparable to previous solutions using explicitly defined HTML\ncode based features. By combining visual and explicit features, an F-measure of\n93.70% can be achieved by Random Forest classification, which beats current\nstate of the art methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 07:39:19 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["B\u00fchler", "Babette", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2103.05111", "submitter": "Roman T\\\"ongi", "authors": "Roman T\\\"ongi", "title": "Application of Transfer Learning to Sign Language Recognition using an\n  Inflated 3D Deep Convolutional Neural Network", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language is the primary language for people with a hearing loss. Sign\nlanguage recognition (SLR) is the automatic recognition of sign language, which\nrepresents a challenging problem for computers, though some progress has been\nmade recently using deep learning. Huge amounts of data are generally required\nto train deep learning models. However, corresponding datasets are missing for\nthe majority of sign languages. Transfer learning is a technique to utilize a\nrelated task with an abundance of data available to help solve a target task\nlacking sufficient data. Transfer learning has been applied highly successfully\nin computer vision and natural language processing. However, much less research\nhas been conducted in the field of SLR. This paper investigates how effectively\ntransfer learning can be applied to isolated SLR using an inflated 3D\nconvolutional neural network as the deep learning architecture. Transfer\nlearning is implemented by pre-training a network on the American Sign Language\ndataset MS-ASL and subsequently fine-tuning it separately on three different\nsizes of the German Sign Language dataset SIGNUM. The results of the\nexperiments give clear empirical evidence that transfer learning can be\neffectively applied to isolated SLR. The accuracy performances of the networks\napplying transfer learning increased substantially by up to 21% as compared to\nthe baseline models that were not pre-trained on the MS-ASL dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 13:37:39 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["T\u00f6ngi", "Roman", ""]]}, {"id": "2103.05112", "submitter": "Adriano Lucieri", "authors": "Adriano Lucieri, Andreas Dengel and Sheraz Ahmed", "title": "Deep Learning Based Decision Support for Medicine -- A Case Study on\n  Skin Cancer Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Early detection of skin cancers like melanoma is crucial to ensure high\nchances of survival for patients. Clinical application of Deep Learning\n(DL)-based Decision Support Systems (DSS) for skin cancer screening has the\npotential to improve the quality of patient care. The majority of work in the\nmedical AI community focuses on a diagnosis setting that is mainly relevant for\nautonomous operation. Practical decision support should, however, go beyond\nplain diagnosis and provide explanations. This paper provides an overview of\nworks towards explainable, DL-based decision support in medical applications\nwith the example of skin cancer diagnosis from clinical, dermoscopic and\nhistopathologic images. Analysis reveals that comparably little attention is\npayed to the explanation of histopathologic skin images and that current work\nis dominated by visual relevance maps as well as dermoscopic feature\nidentification. We conclude that future work should focus on meeting the\nstakeholder's cognitive concepts, providing exhaustive explanations that\ncombine global and local approaches and leverage diverse modalities. Moreover,\nthe possibility to intervene and guide models in case of misbehaviour is\nidentified as a major step towards successful deployment of AI as DL-based DSS\nand beyond.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:07:49 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Lucieri", "Adriano", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2103.05113", "submitter": "Erim Yanik", "authors": "Erim Yanik, Xavier Intes, Uwe Kruger, Pingkun Yan, David Miller, Brian\n  Van Voorst, Basiel Makled, Jack Norfleet, Suvranu De", "title": "Deep Neural Networks for the Assessment of Surgical Skills: A Systematic\n  Review", "comments": "23 pages, 3 figures, 3 tables, Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical training in medical school residency programs has followed the\napprenticeship model. The learning and assessment process is inherently\nsubjective and time-consuming. Thus, there is a need for objective methods to\nassess surgical skills. Here, we use the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines to systematically\nsurvey the literature on the use of Deep Neural Networks for automated and\nobjective surgical skill assessment, with a focus on kinematic data as putative\nmarkers of surgical competency. There is considerable recent interest in deep\nneural networks (DNN) due to the availability of powerful algorithms, multiple\ndatasets, some of which are publicly available, as well as efficient\ncomputational hardware to train and host them. We have reviewed 530 papers, of\nwhich we selected 25 for this systematic review. Based on this review, we\nconcluded that DNNs are powerful tools for automated, objective surgical skill\nassessment using both kinematic and video data. The field would benefit from\nlarge, publicly available, annotated datasets that are representative of the\nsurgical trainee and expert demographics and multimodal data beyond kinematics\nand videos.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:08:37 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yanik", "Erim", ""], ["Intes", "Xavier", ""], ["Kruger", "Uwe", ""], ["Yan", "Pingkun", ""], ["Miller", "David", ""], ["Van Voorst", "Brian", ""], ["Makled", "Basiel", ""], ["Norfleet", "Jack", ""], ["De", "Suvranu", ""]]}, {"id": "2103.05114", "submitter": "Jindong Wang", "authors": "Jindong Wang, Wenjie Feng, Chang Liu, Chaohui Yu, Mingxuan Du, Renjun\n  Xu, Tao Qin, Tie-Yan Liu", "title": "Learning Invariant Representations across Domains and Tasks", "comments": "Technical report, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being expensive and time-consuming to collect massive COVID-19 image samples\nto train deep classification models, transfer learning is a promising approach\nby transferring knowledge from the abundant typical pneumonia datasets for\nCOVID-19 image classification. However, negative transfer may deteriorate the\nperformance due to the feature distribution divergence between two datasets and\ntask semantic difference in diagnosing pneumonia and COVID-19 that rely on\ndifferent characteristics. It is even more challenging when the target dataset\nhas no labels available, i.e., unsupervised task transfer learning. In this\npaper, we propose a novel Task Adaptation Network (TAN) to solve this\nunsupervised task transfer problem. In addition to learning transferable\nfeatures via domain-adversarial training, we propose a novel task semantic\nadaptor that uses the learning-to-learn strategy to adapt the task semantics.\nExperiments on three public COVID-19 datasets demonstrate that our proposed\nmethod achieves superior performance. Especially on COVID-DA dataset, TAN\nsignificantly increases the recall and F1 score by 5.0% and 7.8% compared to\nrecently strong baselines. Moreover, we show that TAN also achieves superior\nperformance on several public domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:18:43 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wang", "Jindong", ""], ["Feng", "Wenjie", ""], ["Liu", "Chang", ""], ["Yu", "Chaohui", ""], ["Du", "Mingxuan", ""], ["Xu", "Renjun", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2103.05115", "submitter": "S. Kevin Zhou", "authors": "S. Kevin Zhou, Hoang Ngan Le, Khoa Luu, Hien V. Nguyen, Nicholas\n  Ayache", "title": "Deep reinforcement learning in medical imaging: A literature review", "comments": "39 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) augments the reinforcement learning\nframework, which learns a sequence of actions that maximizes the expected\nreward, with the representative power of deep neural networks. Recent works\nhave demonstrated the great potential of DRL in medicine and healthcare. This\npaper presents a literature review of DRL in medical imaging. We start with a\ncomprehensive tutorial of DRL, including the latest model-free and model-based\nalgorithms. We then cover existing DRL applications for medical imaging, which\nare roughly divided into three main categories: (I) parametric medical image\nanalysis tasks including landmark detection, object/lesion detection,\nregistration, and view plane localization; (ii) solving optimization tasks\nincluding hyperparameter tuning, selecting augmentation strategies, and neural\narchitecture search; and (iii) miscellaneous applications including surgical\ngesture segmentation, personalized mobile health intervention, and\ncomputational model personalization. The paper concludes with discussions of\nfuture perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:12:49 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhou", "S. Kevin", ""], ["Le", "Hoang Ngan", ""], ["Luu", "Khoa", ""], ["Nguyen", "Hien V.", ""], ["Ayache", "Nicholas", ""]]}, {"id": "2103.05116", "submitter": "Sahar Yousefi", "authors": "Sahar Yousefi, Hessam Sokooti, Wouter M. Teeuwisse, Dennis F.R.\n  Heijtel, Aart J. Nederveen, Marius Staring, Matthias J.P. van Osch", "title": "ASL to PET Translation by a Semi-supervised Residual-based\n  Attention-guided Convolutional Neural Network", "comments": "11 pages, 4 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Positron Emission Tomography (PET) is an imaging method that can assess\nphysiological function rather than structural disturbances by measuring\ncerebral perfusion or glucose consumption. However, this imaging technique\nrelies on injection of radioactive tracers and is expensive. On the contrary,\nArterial Spin Labeling (ASL) MRI is a non-invasive, non-radioactive, and\nrelatively cheap imaging technique for brain hemodynamic measurements, which\nallows quantification to some extent. In this paper we propose a convolutional\nneural network (CNN) based model for translating ASL to PET images, which could\nbenefit patients as well as the healthcare system in terms of expenses and\nadverse side effects. However, acquiring a sufficient number of paired ASL-PET\nscans for training a CNN is prohibitive for many reasons. To tackle this\nproblem, we present a new semi-supervised multitask CNN which is trained on\nboth paired data, i.e. ASL and PET scans, and unpaired data, i.e. only ASL\nscans, which alleviates the problem of training a network on limited paired\ndata. Moreover, we present a new residual-based-attention guided mechanism to\nimprove the contextual features during the training process. Also, we show that\nincorporating T1-weighted scans as an input, due to its high resolution and\navailability of anatomical information, improves the results. We performed a\ntwo-stage evaluation based on quantitative image metrics by conducting a 7-fold\ncross validation followed by a double-blind observer study. The proposed\nnetwork achieved structural similarity index measure (SSIM), mean squared error\n(MSE) and peak signal-to-noise ratio (PSNR) values of $0.85\\pm0.08$,\n$0.01\\pm0.01$, and $21.8\\pm4.5$ respectively, for translating from 2D ASL and\nT1-weighted images to PET data. The proposed model is publicly available via\nhttps://github.com/yousefis/ASL2PET.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:06:02 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yousefi", "Sahar", ""], ["Sokooti", "Hessam", ""], ["Teeuwisse", "Wouter M.", ""], ["Heijtel", "Dennis F. R.", ""], ["Nederveen", "Aart J.", ""], ["Staring", "Marius", ""], ["van Osch", "Matthias J. P.", ""]]}, {"id": "2103.05121", "submitter": "Jevgenij Gamper", "authors": "Jevgenij Gamper, Nasir Rajpoot", "title": "Multiple Instance Captioning: Learning Representations from\n  Histopathology Textbooks and Articles", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present ARCH, a computational pathology (CP) multiple instance captioning\ndataset to facilitate dense supervision of CP tasks. Existing CP datasets focus\non narrow tasks; ARCH on the other hand contains dense diagnostic and\nmorphological descriptions for a range of stains, tissue types and pathologies.\nUsing intrinsic dimensionality estimation, we show that ARCH is the only CP\ndataset to (ARCH-)rival its computer vision analog MS-COCO Captions. We\nconjecture that an encoder pre-trained on dense image captions learns\ntransferable representations for most CP tasks. We support the conjecture with\nevidence that ARCH representation transfers to a variety of pathology sub-tasks\nbetter than ImageNet features or representations obtained via self-supervised\nor multi-task learning on pathology images alone. We release our best model and\ninvite other researchers to test it on their CP tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:18:36 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Gamper", "Jevgenij", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2103.05137", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Contemplating real-world object classification", "comments": "to appear in iclr 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep object recognition models have been very successful over benchmark\ndatasets such as ImageNet. How accurate and robust are they to distribution\nshifts arising from natural and synthetic variations in datasets? Prior\nresearch on this problem has primarily focused on ImageNet variations (e.g.,\nImageNetV2, ImageNet-A). To avoid potential inherited biases in these studies,\nwe take a different approach. Specifically, we reanalyze the ObjectNet dataset\nrecently proposed by Barbu et al. containing objects in daily life situations.\nThey showed a dramatic performance drop of the state of the art object\nrecognition models on this dataset. Due to the importance and implications of\ntheir results regarding the generalization ability of deep models, we take a\nsecond look at their analysis. We find that applying deep models to the\nisolated objects, rather than the entire scene as is done in the original\npaper, results in around 20-30% performance improvement. Relative to the\nnumbers reported in Barbu et al., around 10-15% of the performance loss is\nrecovered, without any test time data augmentation. Despite this gain, however,\nwe conclude that deep models still suffer drastically on the ObjectNet dataset.\nWe also investigate the robustness of models against synthetic image\nperturbations such as geometric transformations (e.g., scale, rotation,\ntranslation), natural image distortions (e.g., impulse noise, blur) as well as\nadversarial attacks (e.g., FGSM and PGD-5). Our results indicate that limiting\nthe object area as much as possible (i.e., from the entire image to the\nbounding box to the segmentation mask) leads to consistent improvement in\naccuracy and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 23:29:59 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 18:50:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2103.05152", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Abhinav Shrivastava, Larry Davis", "title": "Knowledge Evolution in Neural Networks", "comments": "CVPR Oral 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning relies on the availability of a large corpus of data (labeled\nor unlabeled). Thus, one challenging unsettled question is: how to train a deep\nnetwork on a relatively small dataset? To tackle this question, we propose an\nevolution-inspired training approach to boost performance on relatively small\ndatasets. The knowledge evolution (KE) approach splits a deep network into two\nhypotheses: the fit-hypothesis and the reset-hypothesis. We iteratively evolve\nthe knowledge inside the fit-hypothesis by perturbing the reset-hypothesis for\nmultiple generations. This approach not only boosts performance, but also\nlearns a slim network with a smaller inference cost. KE integrates seamlessly\nwith both vanilla and residual convolutional networks. KE reduces both\noverfitting and the burden for data collection.\n  We evaluate KE on various network architectures and loss functions. We\nevaluate KE using relatively small datasets (e.g., CUB-200) and randomly\ninitialized deep networks. KE achieves an absolute 21% improvement margin on a\nstate-of-the-art baseline. This performance improvement is accompanied by a\nrelative 73% reduction in inference cost. KE achieves state-of-the-art results\non classification and metric learning benchmarks. Code available at\nhttp://bit.ly/3uLgwYb\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 00:25:34 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Taha", "Ahmed", ""], ["Shrivastava", "Abhinav", ""], ["Davis", "Larry", ""]]}, {"id": "2103.05158", "submitter": "Hakdong Kim", "authors": "Hakdong Kim, Heonyeong Lim, Minkyu Jee, Yurim Lee, Jisoo Jeong, Kyudam\n  Choi, MinSung Yoon, and Cheongwon Kim", "title": "Deep Learning-based High-precision Depth Map Estimation from Missing\n  Viewpoints for 360 Degree Digital Holography", "comments": "12 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel, convolutional neural network model to\nextract highly precise depth maps from missing viewpoints, especially well\napplicable to generate holographic 3D contents. The depth map is an essential\nelement for phase extraction which is required for synthesis of\ncomputer-generated hologram (CGH). The proposed model called the HDD Net uses\nMSE for the better performance of depth map estimation as loss function, and\nutilizes the bilinear interpolation in up sampling layer with the Relu as\nactivation function. We design and prepare a total of 8,192 multi-view images,\neach resolution of 640 by 360 for the deep learning study. The proposed model\nestimates depth maps through extracting features, up sampling. For quantitative\nassessment, we compare the estimated depth maps with the ground truths by using\nthe PSNR, ACC, and RMSE. We also compare the CGH patterns made from estimated\ndepth maps with ones made from ground truths. Furthermore, we demonstrate the\nexperimental results to test the quality of estimated depth maps through\ndirectly reconstructing holographic 3D image scenes from the CGHs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 00:38:23 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Kim", "Hakdong", ""], ["Lim", "Heonyeong", ""], ["Jee", "Minkyu", ""], ["Lee", "Yurim", ""], ["Jeong", "Jisoo", ""], ["Choi", "Kyudam", ""], ["Yoon", "MinSung", ""], ["Kim", "Cheongwon", ""]]}, {"id": "2103.05164", "submitter": "Muhammad Uzair", "authors": "Hamza Riaz, Muhammad Uzair and Habib Ullah", "title": "Anomalous entities detection using a cascade of deep learning models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human actions that do not conform to usual behavior are considered as\nanomalous and such actors are called anomalous entities. Detection of anomalous\nentities using visual data is a challenging problem in computer vision. This\npaper presents a new approach to detect anomalous entities in complex\nsituations of examination halls. The proposed method uses a cascade of deep\nconvolutional neural network models. In the first stage, we apply a pretrained\nmodel of human pose estimation on frames of videos to extract key feature\npoints of body. Patches extracted from each key point are utilized in the\nsecond stage to build a densely connected deep convolutional neural network\nmodel for detecting anomalous entities. For experiments we collect a video\ndatabase of students undertaking examination in a hall. Our results show that\nthe proposed method can detect anomalous entities and warrant unusual behavior\nwith high accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 01:23:19 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Riaz", "Hamza", ""], ["Uzair", "Muhammad", ""], ["Ullah", "Habib", ""]]}, {"id": "2103.05170", "submitter": "Jieneng Chen", "authors": "Jieneng Chen, Ke Yan, Yu-Dong Zhang, Youbao Tang, Xun Xu, Shuwen Sun,\n  Qiuping Liu, Lingyun Huang, Jing Xiao, Alan L. Yuille, Ya Zhang, and Le Lu", "title": "Sequential Learning on Liver Tumor Boundary Semantics and Prognostic\n  Biomarker Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The boundary of tumors (hepatocellular carcinoma, or HCC) contains rich\nsemantics: capsular invasion, visibility, smoothness, folding and protuberance,\netc. Capsular invasion on tumor boundary has proven to be clinically correlated\nwith the prognostic indicator, microvascular invasion (MVI). Investigating\ntumor boundary semantics has tremendous clinical values. In this paper, we\npropose the first and novel computational framework that disentangles the task\ninto two components: spatial vertex localization and sequential semantic\nclassification. (1) A HCC tumor segmentor is built for tumor mask boundary\nextraction, followed by polar transform representing the boundary with radius\nand angle. Vertex generator is used to produce fixed-length boundary vertices\nwhere vertex features are sampled on the corresponding spatial locations. (2)\nThe sampled deep vertex features with positional embedding are mapped into a\nsequential space and decoded by a multilayer perceptron (MLP) for semantic\nclassification. Extensive experiments on tumor capsule semantics demonstrate\nthe effectiveness of our framework. Mining the correlation between the boundary\nsemantics and MVI status proves the feasibility to integrate this boundary\nsemantics as a valid HCC prognostic biomarker.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 01:43:05 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chen", "Jieneng", ""], ["Yan", "Ke", ""], ["Zhang", "Yu-Dong", ""], ["Tang", "Youbao", ""], ["Xu", "Xun", ""], ["Sun", "Shuwen", ""], ["Liu", "Qiuping", ""], ["Huang", "Lingyun", ""], ["Xiao", "Jing", ""], ["Yuille", "Alan L.", ""], ["Zhang", "Ya", ""], ["Lu", "Le", ""]]}, {"id": "2103.05187", "submitter": "Mingjie Sun", "authors": "Mingjie Sun, Jimin Xiao, Eng Gee Lim", "title": "Iterative Shrinking for Referring Expression Grounding Using Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we are tackling the proposal-free referring expression\ngrounding task, aiming at localizing the target object according to a query\nsentence, without relying on off-the-shelf object proposals. Existing\nproposal-free methods employ a query-image matching branch to select the\nhighest-score point in the image feature map as the target box center, with its\nwidth and height predicted by another branch. Such methods, however, fail to\nutilize the contextual relation between the target and reference objects, and\nlack interpretability on its reasoning procedure. To solve these problems, we\npropose an iterative shrinking mechanism to localize the target, where the\nshrinking direction is decided by a reinforcement learning agent, with all\ncontents within the current image patch comprehensively considered. Beside, the\nsequential shrinking process enables to demonstrate the reasoning about how to\niteratively find the target. Experiments show that the proposed method boosts\nthe accuracy by 4.32% against the previous state-of-the-art (SOTA) method on\nthe RefCOCOg dataset, where query sentences are long and complex, with many\ntargets referred by other reference objects.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:36:45 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Sun", "Mingjie", ""], ["Xiao", "Jimin", ""], ["Lim", "Eng Gee", ""]]}, {"id": "2103.05193", "submitter": "Yaxin Shi", "authors": "Yaxin Shi, Xiaowei Zhou, Ping Liu, Ivor Tsang", "title": "Generative Transition Mechanism to Image-to-Image Translation via\n  Encoded Transformation", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we revisit the Image-to-Image (I2I) translation problem with\ntransition consistency, namely the consistency defined on the conditional data\nmapping between each data pairs. Explicitly parameterizing each data mappings\nwith a transition variable $t$, i.e., $x \\overset{t(x,y)}{\\mapsto}y$, we\ndiscover that existing I2I translation models mainly focus on maintaining\nconsistency on results, e.g., image reconstruction or attribute prediction,\nnamed result consistency in our paper. This restricts their generalization\nability to generate satisfactory results with unseen transitions in the test\nphase. Consequently, we propose to enforce both result consistency and\ntransition consistency for I2I translation, to benefit the problem with a\ncloser consistency between the input and output. To benefit the generalization\nability of the translation model, we propose transition encoding to facilitate\nexplicit regularization of these two {kinds} of consistencies on unseen\ntransitions. We further generalize such explicitly regularized consistencies to\ndistribution-level, thus facilitating a generalized overall consistency for I2I\ntranslation problems. With the above design, our proposed model, named\nTransition Encoding GAN (TEGAN), can poss superb generalization ability to\ngenerate realistic and semantically consistent translation results with unseen\ntransitions in the test phase. It also provides a unified understanding of the\nexisting GAN-based I2I transition models with our explicitly modeling of the\ndata mapping, i.e., transition. Experiments on four different I2I translation\ntasks demonstrate the efficacy and generality of TEGAN.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:56:03 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Shi", "Yaxin", ""], ["Zhou", "Xiaowei", ""], ["Liu", "Ping", ""], ["Tsang", "Ivor", ""]]}, {"id": "2103.05213", "submitter": "Mingyuan Meng", "authors": "Mingyuan Meng, Lei Bi, Michael Fulham, David Dagan Feng, and Jinman\n  Kim", "title": "Enhancing Medical Image Registration via Appearance Adjustment Networks", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deformable image registration is fundamental for many medical image analyses.\nA key obstacle for accurate image registration is the variations in image\nappearance. Recently, deep learning-based registration methods (DLRs), using\ndeep neural networks, have computational efficiency that is several orders of\nmagnitude greater than traditional optimization-based registration methods\n(ORs). A major drawback, however, of DLRs is a disregard for the\ntarget-pair-specific optimization that is inherent in ORs and instead they rely\non a globally optimized network that is trained with a set of training samples\nto achieve faster registration. Thus, DLRs inherently have degraded ability to\nadapt to appearance variations and perform poorly, compared to ORs, when image\npairs (fixed/moving images) have large differences in appearance. Hence, we\npropose an Appearance Adjustment Network (AAN) where we leverage anatomy edges,\nthrough an anatomy-constrained loss function, to generate an anatomy-preserving\nappearance transformation. We designed the AAN so that it can be readily\ninserted into a wide range of DLRs, to reduce the appearance differences\nbetween the fixed and moving images. Our AAN and DLR's network can be trained\ncooperatively in an unsupervised and end-to-end manner. We evaluated our AAN\nwith two widely used DLRs - Voxelmorph (VM) and FAst IMage registration (FAIM)\n- on three public 3D brain magnetic resonance (MR) image datasets - IBSR18,\nMindboggle101, and LPBA40. The results show that DLRs, using the AAN, improved\nperformance and achieved higher results than state-of-the-art ORs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 04:24:48 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Meng", "Mingyuan", ""], ["Bi", "Lei", ""], ["Fulham", "Michael", ""], ["Feng", "David Dagan", ""], ["Kim", "Jinman", ""]]}, {"id": "2103.05214", "submitter": "Xinwen Liu", "authors": "Xinwen Liu, Jing Wang, Feng Liu, and S.Kevin Zhou", "title": "Universal Undersampled MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been extensively studied for undersampled MRI\nreconstruction. While achieving state-of-the-art performance, they are trained\nand deployed specifically for one anatomy with limited generalization ability\nto another anatomy. Rather than building multiple models, a universal model\nthat reconstructs images across different anatomies is highly desirable for\nefficient deployment and better generalization. Simply mixing images from\nmultiple anatomies for training a single network does not lead to an ideal\nuniversal model due to the statistical shift among datasets of various\nanatomies, the need to retrain from scratch on all datasets with the addition\nof a new dataset, and the difficulty in dealing with imbalanced sampling when\nthe new dataset is further of a smaller size. In this paper, for the first\ntime, we propose a framework to learn a universal deep neural network for\nundersampled MRI reconstruction. Specifically, anatomy-specific instance\nnormalization is proposed to compensate for statistical shift and allow easy\ngeneralization to new datasets. Moreover, the universal model is trained by\ndistilling knowledge from available independent models to further exploit\nrepresentations across anatomies. Experimental results show the proposed\nuniversal model can reconstruct both brain and knee images with high image\nquality. Also, it is easy to adapt the trained model to new datasets of smaller\nsize, i.e., abdomen, cardiac and prostate, with little effort and superior\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 04:25:22 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Liu", "Xinwen", ""], ["Wang", "Jing", ""], ["Liu", "Feng", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2103.05220", "submitter": "Mingyuan Meng", "authors": "Bingxin Gu, Mingyuan Meng, Lei Bi, Jinman Kim, David Dagan Feng, and\n  Shaoli Song", "title": "Prediction of 5-year Progression-Free Survival in Advanced\n  Nasopharyngeal Carcinoma with Pretreatment PET/CT using Multi-Modality Deep\n  Learning-based Radiomics", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Learning-based Radiomics (DLR) has achieved great success on medical\nimage analysis. In this study, we aim to explore the capability of DLR for\nsurvival prediction in NPC. We developed an end-to-end multi-modality DLR model\nusing pretreatment PET/CT images to predict 5-year Progression-Free Survival\n(PFS) in advanced NPC. A total of 170 patients with pathological confirmed\nadvanced NPC (TNM stage III or IVa) were enrolled in this study. A 3D\nConvolutional Neural Network (CNN), with two branches to process PET and CT\nseparately, was optimized to extract deep features from pretreatment\nmulti-modality PET/CT images and use the derived features to predict the\nprobability of 5-year PFS. Optionally, TNM stage, as a high-level clinical\nfeature, can be integrated into our DLR model to further improve prognostic\nperformance. For a comparison between CR and DLR, 1456 handcrafted features\nwere extracted, and three top CR methods were selected as benchmarks from 54\ncombinations of 6 feature selection methods and 9 classification methods.\nCompared to the three CR methods, our multi-modality DLR models using both PET\nand CT, with or without TNM stage (named PCT or PC model), resulted in the\nhighest prognostic performance. Furthermore, the multi-modality PCT model\noutperformed single-modality DLR models using only PET and TNM stage (PT model)\nor only CT and TNM stage (CT model). Our study identified potential\nradiomics-based prognostic model for survival prediction in advanced NPC, and\nsuggests that DLR could serve as a tool for aiding in cancer management.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 04:43:33 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Gu", "Bingxin", ""], ["Meng", "Mingyuan", ""], ["Bi", "Lei", ""], ["Kim", "Jinman", ""], ["Feng", "David Dagan", ""], ["Song", "Shaoli", ""]]}, {"id": "2103.05222", "submitter": "Wentao He", "authors": "Wentao He, Jialu Zhang, Chenglin Yao, Shihe Wang, Jianfeng Ren, Ruibin\n  Bai", "title": "A Data Augmentation Method by Mixing Up Negative Candidate Answers for\n  Solving Raven's Progressive Matrices", "comments": "Submitted to 2021 IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raven's Progressive Matrices (RPMs) are frequently-used in testing human's\nvisual reasoning ability. Recently developed RPM-like datasets and solution\nmodels transfer this kind of problems from cognitive science to computer\nscience. In view of the poor generalization performance due to insufficient\nsamples in RPM datasets, we propose a data augmentation strategy by image\nmix-up, which is generalizable to a variety of multiple-choice problems,\nespecially for image-based RPM-like problems. By focusing on potential\nfunctionalities of negative candidate answers, the visual reasoning capability\nof the model is enhanced. By applying the proposed data augmentation method, we\nachieve significant and consistent improvement on various RPM-like datasets\ncompared with the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 04:50:32 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["He", "Wentao", ""], ["Zhang", "Jialu", ""], ["Yao", "Chenglin", ""], ["Wang", "Shihe", ""], ["Ren", "Jianfeng", ""], ["Bai", "Ruibin", ""]]}, {"id": "2103.05226", "submitter": "Scarlett Raine Ms", "authors": "Scarlett Raine, Ross Marchant, Peyman Moghadam, Frederic Maire, Brett\n  Kettle and Brano Kusy", "title": "DeepSeagrass Dataset", "comments": "arXiv admin note: text overlap with arXiv:2009.09924", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dataset of seagrass images collected by a biologist\nsnorkelling in Moreton Bay, Queensland, Australia, as described in our\npublication: arXiv:2009.09924. The images are labelled at the image-level by\ncollecting images of the same morphotype in a folder hierarchy. We also release\npre-trained models and training codes for detection and classification of\nseagrass species at the patch level at\nhttps://github.com/csiro-robotics/deepseagrass.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 05:08:01 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Raine", "Scarlett", ""], ["Marchant", "Ross", ""], ["Moghadam", "Peyman", ""], ["Maire", "Frederic", ""], ["Kettle", "Brett", ""], ["Kusy", "Brano", ""]]}, {"id": "2103.05227", "submitter": "Yuhang Zhou", "authors": "Yuhang Zhou, Xiaoman Zhang, Shixiang Feng, Ya Zhang, and Yanfeng", "title": "Uncertainty-aware Incremental Learning for Multi-organ Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches to train a unified multi-organ segmentation model\nfrom several single-organ datasets require simultaneously access multiple\ndatasets during training. In the real scenarios, due to privacy and ethics\nconcerns, the training data of the organs of interest may not be publicly\navailable. To this end, we investigate a data-free incremental organ\nsegmentation scenario and propose a novel incremental training framework to\nsolve it. We use the pretrained model instead of its own training data for\nprivacy protection. Specifically, given a pretrained $K$ organ segmentation\nmodel and a new single-organ dataset, we train a unified $K+1$ organ\nsegmentation model without accessing any data belonging to the previous\ntraining stages. Our approach consists of two parts: the background label\nalignment strategy and the uncertainty-aware guidance strategy. The first part\nis used for knowledge transfer from the pretained model to the training model.\nThe second part is used to extract the uncertainty information from the\npretrained model to guide the whole knowledge transfer process. By combing\nthese two strategies, more reliable information is extracted from the\npretrained model without original training data. Experiments on multiple\npublicly available pretrained models and a multi-organ dataset MOBA have\ndemonstrated the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 05:12:39 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhou", "Yuhang", ""], ["Zhang", "Xiaoman", ""], ["Feng", "Shixiang", ""], ["Zhang", "Ya", ""], ["Yanfeng", "", ""]]}, {"id": "2103.05232", "submitter": "Lijun Gong", "authors": "Gege Qi, Lijun Gong, Yibing Song, Kai Ma, Yefeng Zheng", "title": "Stabilized Medical Image Attacks", "comments": "ICLR 2021 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have advanced existing medical systems\nfor automatic disease diagnosis. However, a threat to these systems arises that\nadversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a\nnegative influence on human healthcare. There is a need to investigate\npotential adversarial attacks to robustify deep medical diagnosis systems. On\nthe other side, there are several modalities of medical images (e.g., CT,\nfundus, and endoscopic image) of which each type is significantly different\nfrom others. It is more challenging to generate adversarial perturbations for\ndifferent types of medical images. In this paper, we propose an image-based\nmedical adversarial attack method to consistently produce adversarial\nperturbations on medical images. The objective function of our method consists\nof a loss deviation term and a loss stabilization term. The loss deviation term\nincreases the divergence between the CNN prediction of an adversarial example\nand its ground truth label. Meanwhile, the loss stabilization term ensures\nsimilar CNN predictions of this example and its smoothed input. From the\nperspective of the whole iterations for perturbation generation, the proposed\nloss stabilization term exhaustively searches the perturbation space to smooth\nthe single spot for local optimum escape. We further analyze the KL-divergence\nof the proposed loss function and find that the loss stabilization term makes\nthe perturbations updated towards a fixed objective spot while deviating from\nthe ground truth. This stabilization ensures the proposed medical attack\neffective for different types of medical images while producing perturbations\nin small variance. Experiments on several medical image analysis benchmarks\nincluding the recent COVID-19 dataset show the stability of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 05:40:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Qi", "Gege", ""], ["Gong", "Lijun", ""], ["Song", "Yibing", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2103.05248", "submitter": "Mo Zhou", "authors": "Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Yinghui Xu, Nanning\n  Zheng, Gang Hua", "title": "Practical Relative Order Attack in Deep Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies unveil the vulnerabilities of deep ranking models, where an\nimperceptible perturbation can trigger dramatic changes in the ranking result.\nWhile previous attempts focus on manipulating absolute ranks of certain\ncandidates, the possibility of adjusting their relative order remains\nunder-explored. In this paper, we formulate a new adversarial attack against\ndeep ranking systems, i.e., the Order Attack, which covertly alters the\nrelative order among a selected set of candidates according to an\nattacker-specified permutation, with limited interference to other unrelated\ncandidates. Specifically, it is formulated as a triplet-style loss imposing an\ninequality chain reflecting the specified permutation. However, direct\noptimization of such white-box objective is infeasible in a real-world attack\nscenario due to various black-box limitations. To cope with them, we propose a\nShort-range Ranking Correlation metric as a surrogate objective for black-box\nOrder Attack to approximate the white-box method. The Order Attack is evaluated\non the Fashion-MNIST and Stanford-Online-Products datasets under both white-box\nand black-box threat models. The black-box attack is also successfully\nimplemented on a major e-commerce platform. Comprehensive experimental\nevaluations demonstrate the effectiveness of the proposed methods, revealing a\nnew type of ranking model vulnerability.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:41:18 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 01:16:13 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 12:18:44 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Mo", ""], ["Wang", "Le", ""], ["Niu", "Zhenxing", ""], ["Zhang", "Qilin", ""], ["Xu", "Yinghui", ""], ["Zheng", "Nanning", ""], ["Hua", "Gang", ""]]}, {"id": "2103.05251", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Enhancing sensor resolution improves CNN accuracy given the same number\n  of parameters or FLOPS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High image resolution is critical to obtain a good performance in many\ncomputer vision applications. Computational complexity of CNNs, however, grows\nsignificantly with the increase in input image size. Here, we show that it is\nalmost always possible to modify a network such that it achieves higher\naccuracy at a higher input resolution while having the same number of\nparameters or/and FLOPS. The idea is similar to the EfficientNet paper but\ninstead of optimizing network width, depth and resolution simultaneously, here\nwe focus only on input resolution. This makes the search space much smaller\nwhich is more suitable for low computational budget regimes. More importantly,\nby controlling for the number of model parameters (and hence model capacity),\nwe show that the additional benefit in accuracy is indeed due to the higher\ninput resolution. Preliminary empirical investigation over MNIST, Fashion\nMNIST, and CIFAR10 datasets demonstrates the efficiency of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:47:01 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2103.05254", "submitter": "Xiaoqing Guo", "authors": "Xiaoqing Guo, Chen Yang, Baopu Li, Yixuan Yuan", "title": "MetaCorrection: Domain-aware Meta Loss Correction for Unsupervised\n  Domain Adaptation in Semantic Segmentation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to transfer the knowledge from the\nlabeled source domain to the unlabeled target domain. Existing self-training\nbased UDA approaches assign pseudo labels for target data and treat them as\nground truth labels to fully leverage unlabeled target data for model\nadaptation. However, the generated pseudo labels from the model optimized on\nthe source domain inevitably contain noise due to the domain gap. To tackle\nthis issue, we advance a MetaCorrection framework, where a Domain-aware\nMeta-learning strategy is devised to benefit Loss Correction (DMLC) for UDA\nsemantic segmentation. In particular, we model the noise distribution of pseudo\nlabels in target domain by introducing a noise transition matrix (NTM) and\nconstruct meta data set with domain-invariant source data to guide the\nestimation of NTM. Through the risk minimization on the meta data set, the\noptimized NTM thus can correct the noisy issues in pseudo labels and enhance\nthe generalization ability of the model on the target data. Considering the\ncapacity gap between shallow and deep features, we further employ the proposed\nDMLC strategy to provide matched and compatible supervision signals for\ndifferent level features, thereby ensuring deep adaptation. Extensive\nexperimental results highlight the effectiveness of our method against existing\nstate-of-the-art methods on three benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:57:03 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Guo", "Xiaoqing", ""], ["Yang", "Chen", ""], ["Li", "Baopu", ""], ["Yuan", "Yixuan", ""]]}, {"id": "2103.05255", "submitter": "Ce Wang", "authors": "Ce Wang, Haimiao Zhang, Qian Li, Kun Shang, Yuanyuan Lyu, Bin Dong, S.\n  Kevin Zhou", "title": "Improving Generalizability in Limited-Angle CT Reconstruction with\n  Sinogram Extrapolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) reconstruction from X-ray projections acquired\nwithin a limited angle range is challenging, especially when the angle range is\nextremely small. Both analytical and iterative models need more projections for\neffective modeling. Deep learning methods have gained prevalence due to their\nexcellent reconstruction performances, but such success is mainly limited\nwithin the same dataset and does not generalize across datasets with different\ndistributions. Hereby we propose ExtraPolationNetwork for limited-angle CT\nreconstruction via the introduction of a sinogram extrapolation module, which\nis theoretically justified. The module complements extra sinogram information\nand boots model generalizability. Extensive experimental results show that our\nreconstruction model achieves state-of-the-art performance on NIH-AAPM dataset,\nsimilar to existing approaches. More importantly, we show that using such a\nsinogram extrapolation module significantly improves the generalization\ncapability of the model on unseen datasets (e.g., COVID-19 and LIDC datasets)\nwhen compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:58:09 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 02:45:02 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 09:14:01 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wang", "Ce", ""], ["Zhang", "Haimiao", ""], ["Li", "Qian", ""], ["Shang", "Kun", ""], ["Lyu", "Yuanyuan", ""], ["Dong", "Bin", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2103.05259", "submitter": "Christian Schiffer", "authors": "Christian Schiffer, Stefan Harmeling, Katrin Amunts, Timo Dickscheid", "title": "2D histology meets 3D topology: Cytoarchitectonic brain mapping with\n  Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cytoarchitecture describes the spatial organization of neuronal cells in the\nbrain, including their arrangement into layers and columns with respect to cell\ndensity, orientation, or presence of certain cell types. It allows to segregate\nthe brain into cortical areas and subcortical nuclei, links structure with\nconnectivity and function, and provides a microstructural reference for human\nbrain atlases. Mapping boundaries between areas requires to scan histological\nsections at microscopic resolution. While recent high-throughput scanners allow\nto scan a complete human brain in the order of a year, it is practically\nimpossible to delineate regions at the same pace using the established gold\nstandard method. Researchers have recently addressed cytoarchitectonic mapping\nof cortical regions with deep neural networks, relying on image patches from\nindividual 2D sections for classification. However, the 3D context, which is\nneeded to disambiguate complex or obliquely cut brain regions, is not taken\ninto account. In this work, we combine 2D histology with 3D topology by\nreformulating the mapping task as a node classification problem on an\napproximate 3D midsurface mesh through the isocortex. We extract deep features\nfrom cortical patches in 2D histological sections which are descriptive of\ncytoarchitecture, and assign them to the corresponding nodes on the 3D mesh to\nconstruct a large attributed graph. By solving the brain mapping problem on\nthis graph using graph neural networks, we obtain significantly improved\nclassification results. The proposed framework lends itself nicely to\nintegration of additional neuroanatomical priors for mapping.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:09:42 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Schiffer", "Christian", ""], ["Harmeling", "Stefan", ""], ["Amunts", "Katrin", ""], ["Dickscheid", "Timo", ""]]}, {"id": "2103.05266", "submitter": "Yunfeng Diao", "authors": "Yunfeng Diao and Tianjia Shao and Yong-Liang Yang and Kun Zhou and He\n  Wang", "title": "BASAR:Black-box Attack on Skeletal Action Recognition", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skeletal motion plays a vital role in human activity recognition as either an\nindependent data source or a complement. The robustness of skeleton-based\nactivity recognizers has been questioned recently, which shows that they are\nvulnerable to adversarial attacks when the full-knowledge of the recognizer is\naccessible to the attacker. However, this white-box requirement is overly\nrestrictive in most scenarios and the attack is not truly threatening. In this\npaper, we show that such threats do exist under black-box settings too. To this\nend, we propose the first black-box adversarial attack method BASAR. Through\nBASAR, we show that adversarial attack is not only truly a threat but also can\nbe extremely deceitful, because on-manifold adversarial samples are rather\ncommon in skeletal motions, in contrast to the common belief that adversarial\nsamples only exist off-manifold. Through exhaustive evaluation and comparison,\nwe show that BASAR can deliver successful attacks across models, data, and\nattack modes. Through harsh perceptual studies, we show that it achieves\neffective yet imperceptible attacks. By analyzing the attack on different\nactivity recognizers, BASAR helps identify the potential causes of their\nvulnerability and provides insights on what classifiers are likely to be more\nrobust against attack. Code is available at\nhttps://github.com/realcrane/BASAR-Black-box-Attack-on-Skeletal-Action-Recognition.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:29:35 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:44:58 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 07:48:47 GMT"}, {"version": "v4", "created": "Thu, 29 Apr 2021 11:23:45 GMT"}, {"version": "v5", "created": "Thu, 13 May 2021 08:34:51 GMT"}, {"version": "v6", "created": "Mon, 26 Jul 2021 00:58:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Diao", "Yunfeng", ""], ["Shao", "Tianjia", ""], ["Yang", "Yong-Liang", ""], ["Zhou", "Kun", ""], ["Wang", "He", ""]]}, {"id": "2103.05270", "submitter": "Xi Li", "authors": "Xin Qin, Hanbin Zhao, Guangchen Lin, Hao Zeng, Songcen Xu, Xi Li", "title": "PcmNet: Position-Sensitive Context Modeling Network for Temporal Action\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization is an important and challenging task that aims\nto locate temporal regions in real-world untrimmed videos where actions occur\nand recognize their classes. It is widely acknowledged that video context is a\ncritical cue for video understanding, and exploiting the context has become an\nimportant strategy to boost localization performance. However, previous\nstate-of-the-art methods focus more on exploring semantic context which\ncaptures the feature similarity among frames or proposals, and neglect\npositional context which is vital for temporal localization. In this paper, we\npropose a temporal-position-sensitive context modeling approach to incorporate\nboth positional and semantic information for more precise action localization.\nSpecifically, we first augment feature representations with directed temporal\npositional encoding, and then conduct attention-based information propagation,\nin both frame-level and proposal-level. Consequently, the generated feature\nrepresentations are significantly empowered with the discriminative capability\nof encoding the position-aware context information, and thus benefit boundary\ndetection and proposal evaluation. We achieve state-of-the-art performance on\nboth two challenging datasets, THUMOS-14 and ActivityNet-1.3, demonstrating the\neffectiveness and generalization ability of our method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:34:01 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Qin", "Xin", ""], ["Zhao", "Hanbin", ""], ["Lin", "Guangchen", ""], ["Zeng", "Hao", ""], ["Xu", "Songcen", ""], ["Li", "Xi", ""]]}, {"id": "2103.05271", "submitter": "Gengcong Yang", "authors": "Gengcong Yang, Jingyi Zhang, Yong Zhang, Baoyuan Wu, Yujiu Yang", "title": "Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation", "comments": "CVPR 2021 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To generate \"accurate\" scene graphs, almost all existing methods predict\npairwise relationships in a deterministic manner. However, we argue that visual\nrelationships are often semantically ambiguous. Specifically, inspired by\nlinguistic knowledge, we classify the ambiguity into three types: Synonymy\nAmbiguity, Hyponymy Ambiguity, and Multi-view Ambiguity. The ambiguity\nnaturally leads to the issue of \\emph{implicit multi-label}, motivating the\nneed for diverse predictions. In this work, we propose a novel plug-and-play\nProbabilistic Uncertainty Modeling (PUM) module. It models each union region as\na Gaussian distribution, whose variance measures the uncertainty of the\ncorresponding visual content. Compared to the conventional deterministic\nmethods, such uncertainty modeling brings stochasticity of feature\nrepresentation, which naturally enables diverse predictions. As a byproduct,\nPUM also manages to cover more fine-grained relationships and thus alleviates\nthe issue of bias towards frequent relationships. Extensive experiments on the\nlarge-scale Visual Genome benchmark show that combining PUM with newly proposed\nResCAGCN can achieve state-of-the-art performances, especially under the mean\nrecall metric. Furthermore, we prove the universal effectiveness of PUM by\nplugging it into some existing models and provide insightful analysis of its\nability to generate diverse yet plausible visual relationships.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:36:09 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 05:20:48 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Gengcong", ""], ["Zhang", "Jingyi", ""], ["Zhang", "Yong", ""], ["Wu", "Baoyuan", ""], ["Yang", "Yujiu", ""]]}, {"id": "2103.05284", "submitter": "Ziqi Zhang", "authors": "Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing Li, Ying Deng,\n  Weiming Hu", "title": "Open-book Video Captioning with Retrieve-Copy-Generate Network", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the rapid emergence of short videos and the requirement for content\nunderstanding and creation, the video captioning task has received increasing\nattention in recent years. In this paper, we convert traditional video\ncaptioning task into a new paradigm, \\ie, Open-book Video Captioning, which\ngenerates natural language under the prompts of video-content-relevant\nsentences, not limited to the video itself. To address the open-book video\ncaptioning problem, we propose a novel Retrieve-Copy-Generate network, where a\npluggable video-to-text retriever is constructed to retrieve sentences as hints\nfrom the training corpus effectively, and a copy-mechanism generator is\nintroduced to extract expressions from multi-retrieved sentences dynamically.\nThe two modules can be trained end-to-end or separately, which is flexible and\nextensible. Our framework coordinates the conventional retrieval-based methods\nwith orthodox encoder-decoder methods, which can not only draw on the diverse\nexpressions in the retrieved sentences but also generate natural and accurate\ncontent of the video. Extensive experiments on several benchmark datasets show\nthat our proposed approach surpasses the state-of-the-art performance,\nindicating the effectiveness and promising of the proposed paradigm in the task\nof video captioning.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:17:17 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhang", "Ziqi", ""], ["Qi", "Zhongang", ""], ["Yuan", "Chunfeng", ""], ["Shan", "Ying", ""], ["Li", "Bing", ""], ["Deng", "Ying", ""], ["Hu", "Weiming", ""]]}, {"id": "2103.05285", "submitter": "Adnan Ahmad", "authors": "Adnan Ahmad, Drew Parker, Zahra Riahi Samani, Ragini Verma", "title": "3D-QCNet -- A Pipeline for Automated Artifact Detection in Diffusion MRI\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artifacts are a common occurrence in Diffusion MRI (dMRI) scans. Identifying\nand removing them is essential to ensure the accuracy and viability of any post\nprocessing carried out on these scans. This makes QC (quality control) a\ncrucial first step prior to any analysis of dMRI data. Several QC methods for\nartifact detection exist, however they suffer from problems like requiring\nmanual intervention and the inability to generalize across different artifacts\nand datasets. In this paper, we propose an automated deep learning (DL)\npipeline that utilizes a 3D-Densenet architecture to train a model on diffusion\nvolumes for automatic artifact detection. Our method is applied on a vast\ndataset consisting of 9000 volumes sourced from 7 large clinical datasets.\nThese datasets comprise scans from multiple scanners with different gradient\ndirections, high and low b values, single shell and multi shell acquisitions.\nAdditionally, they represent diverse subject demographics like the presence or\nabsence of pathologies. Our QC method is found to accurately generalize across\nthis heterogenous data by correctly detecting 92% artifacts on average across\nour test set. This consistent performance over diverse datasets underlines the\ngeneralizability of our method, which currently is a significant barrier\nhindering the widespread adoption of automated QC techniques. For these\nreasons, we believe that 3D-QCNet can be integrated in diffusion pipelines to\neffectively automate the arduous and time-intensive process of artifact\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:21:53 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ahmad", "Adnan", ""], ["Parker", "Drew", ""], ["Samani", "Zahra Riahi", ""], ["Verma", "Ragini", ""]]}, {"id": "2103.05310", "submitter": "Hailong Ning", "authors": "Yuan Yuan, Hailong Ning, and Xiaoqiang Lu", "title": "Bio-Inspired Representation Learning for Visual Attention Prediction", "comments": null, "journal-ref": null, "doi": "10.1109/TCYB.2019.2931735", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Attention Prediction (VAP) is a significant and imperative issue in\nthe field of computer vision. Most of existing VAP methods are based on deep\nlearning. However, they do not fully take advantage of the low-level contrast\nfeatures while generating the visual attention map. In this paper, a novel VAP\nmethod is proposed to generate visual attention map via bio-inspired\nrepresentation learning. The bio-inspired representation learning combines both\nlow-level contrast and high-level semantic features simultaneously, which are\ndeveloped by the fact that human eye is sensitive to the patches with high\ncontrast and objects with high semantics. The proposed method is composed of\nthree main steps: 1) feature extraction, 2) bio-inspired representation\nlearning and 3) visual attention map generation. Firstly, the high-level\nsemantic feature is extracted from the refined VGG16, while the low-level\ncontrast feature is extracted by the proposed contrast feature extraction block\nin a deep network. Secondly, during bio-inspired representation learning, both\nthe extracted low-level contrast and high-level semantic features are combined\nby the designed densely connected block, which is proposed to concatenate\nvarious features scale by scale. Finally, the weighted-fusion layer is\nexploited to generate the ultimate visual attention map based on the obtained\nrepresentations after bio-inspired representation learning. Extensive\nexperiments are performed to demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 09:15:36 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yuan", "Yuan", ""], ["Ning", "Hailong", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "2103.05337", "submitter": "Paul Smyth", "authors": "Tanguy Naets, Maarten Huijsmans, Paul Smyth, Laurent Sorber, Ga\\\"el de\n  Lannoy", "title": "A Mask R-CNN approach to counting bacterial colony forming units in\n  pharmaceutical development", "comments": "9 pages, 3 pdf figures. Extended version of poster presented at ESANN\n  2020 (European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an application of the well-known Mask R-CNN approach to the\ncounting of different types of bacterial colony forming units that were\ncultured in Petri dishes. Our model was made available to lab technicians in a\nmodern SPA (Single-Page Application). Users can upload images of dishes, after\nwhich the Mask R-CNN model that was trained and tuned specifically for this\ntask detects the number of BVG- and BVG+ colonies and displays these in an\ninteractive interface for the user to verify. Users can then check the model's\npredictions, correct them if deemed necessary, and finally validate them. Our\nadapted Mask R-CNN model achieves a mean average precision (mAP) of 94\\% at an\nintersection-over-union (IoU) threshold of 50\\%. With these encouraging\nresults, we see opportunities to bring the benefits of improved accuracy and\ntime saved to related problems, such as generalising to other bacteria types\nand viral foci counting.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:31:00 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Naets", "Tanguy", ""], ["Huijsmans", "Maarten", ""], ["Smyth", "Paul", ""], ["Sorber", "Laurent", ""], ["de Lannoy", "Ga\u00ebl", ""]]}, {"id": "2103.05342", "submitter": "Tianshu Xie", "authors": "Tianshu Xie, Xuan Cheng, Minghui Liu, Jiali Deng, Xiaomin Wang, Ming\n  Liu", "title": "Thumbnail: A Novel Data Augmentation for Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new data augmentation strategy named Thumbnail,\nwhich aims to strengthen the network's capture of global features. We get a\ngenerated image by reducing an image to a certain size, which is called as the\nthumbnail, and pasting it in the random position of the original image. The\ngenerated image not only retains most of the original image information but\nalso has the global information in the thumbnail. Furthermore, we find that the\nidea of thumbnail can be perfectly integrated with Mixed Sample Data\nAugmentation, so we paste the thumbnail in another image where the ground truth\nlabels are also mixed with a certain weight, which makes great achievements on\nvarious computer vision tasks. Extensive experiments show that Thumbnail works\nbetter than the state-of-the-art augmentation strategies across classification,\nfine-grained image classification, and object detection. On ImageNet\nclassification, ResNet50 architecture with our method achieves 79.21% accuracy,\nwhich is more than 2.89% improvement on the baseline.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:45:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Xie", "Tianshu", ""], ["Cheng", "Xuan", ""], ["Liu", "Minghui", ""], ["Deng", "Jiali", ""], ["Wang", "Xiaomin", ""], ["Liu", "Ming", ""]]}, {"id": "2103.05346", "submitter": "Jihan Yang", "authors": "Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, Xiaojuan Qi", "title": "ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object\n  Detection", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new domain adaptive self-training pipeline, named ST3D, for\nunsupervised domain adaptation on 3D object detection from point clouds. First,\nwe pre-train the 3D detector on the source domain with our proposed random\nobject scaling strategy for mitigating the negative effects of source domain\nbias. Then, the detector is iteratively improved on the target domain by\nalternatively conducting two steps, which are the pseudo label updating with\nthe developed quality-aware triplet memory bank and the model training with\ncurriculum data augmentation. These specific designs for 3D object detection\nenable the detector to be trained with consistent and high-quality pseudo\nlabels and to avoid overfitting to the large number of easy examples in pseudo\nlabeled data. Our ST3D achieves state-of-the-art performance on all evaluated\ndatasets and even surpasses fully supervised results on KITTI 3D object\ndetection benchmark. Code will be available at\nhttps://github.com/CVMI-Lab/ST3D.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:51:24 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 07:36:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yang", "Jihan", ""], ["Shi", "Shaoshuai", ""], ["Wang", "Zhe", ""], ["Li", "Hongsheng", ""], ["Qi", "Xiaojuan", ""]]}, {"id": "2103.05347", "submitter": "He Wang", "authors": "He Wang, Feixiang He, Zhexi Peng, Tianjia Shao, Yong-Liang Yang, Kun\n  Zhou, David Hogg", "title": "Understanding the Robustness of Skeleton-based Action Recognition under\n  Adversarial Attack", "comments": "Accepted in CVPR 2021. arXiv admin note: substantial text overlap\n  with arXiv:1911.07107", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition has been heavily employed in many applications such as\nautonomous vehicles, surveillance, etc, where its robustness is a primary\nconcern. In this paper, we examine the robustness of state-of-the-art action\nrecognizers against adversarial attack, which has been rarely investigated so\nfar. To this end, we propose a new method to attack action recognizers that\nrely on 3D skeletal motion. Our method involves an innovative perceptual loss\nthat ensures the imperceptibility of the attack. Empirical studies demonstrate\nthat our method is effective in both white-box and black-box scenarios. Its\ngeneralizability is evidenced on a variety of action recognizers and datasets.\nIts versatility is shown in different attacking strategies. Its deceitfulness\nis proven in extensive perceptual studies. Our method shows that adversarial\nattack on 3D skeletal motions, one type of time-series data, is significantly\ndifferent from traditional adversarial attack problems. Its success raises\nserious concern on the robustness of action recognizers and provides insights\non potential improvements.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:53:58 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 19:49:44 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Wang", "He", ""], ["He", "Feixiang", ""], ["Peng", "Zhexi", ""], ["Shao", "Tianjia", ""], ["Yang", "Yong-Liang", ""], ["Zhou", "Kun", ""], ["Hogg", "David", ""]]}, {"id": "2103.05354", "submitter": "Ahmed Aldahdooh", "authors": "Ahmed Aldahdooh, Wassim Hamidouche, and Olivier D\\'eforges", "title": "Revisiting Model's Uncertainty and Confidences for Adversarial Example\n  Detection", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Security-sensitive applications that rely on Deep Neural Networks (DNNs) are\nvulnerable to small perturbations that are crafted to generate Adversarial\nExamples(AEs). The AEs are imperceptible to humans and cause DNN to misclassify\nthem. Many defense and detection techniques have been proposed. Model's\nconfidences and Dropout, as a popular way to estimate the model's uncertainty,\nhave been used for AE detection but they showed limited success against black-\nand gray-box attacks. Moreover, the state-of-the-art detection techniques have\nbeen designed for specific attacks or broken by others, need knowledge about\nthe attacks, are not consistent, increase model parameters overhead, are\ntime-consuming, or have latency in inference time. To trade off these factors,\nwe revisit the model's uncertainty and confidences and propose a novel\nunsupervised ensemble AE detection mechanism that 1) uses the uncertainty\nmethod called SelectiveNet, 2) processes model layers outputs, i.e.feature\nmaps, to generate new confidence probabilities. The detection method is called\nSelective and Feature based Adversarial Detection (SFAD). Experimental results\nshow that the proposed approach achieves better performance against black- and\ngray-box attacks than the state-of-the-art methods and achieves comparable\nperformance against white-box attacks. Moreover, results show that SFAD is\nfully robust against High Confidence Attacks (HCAs) for MNIST and partially\nrobust for CIFAR10 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 11:06:15 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 15:21:49 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Aldahdooh", "Ahmed", ""], ["Hamidouche", "Wassim", ""], ["D\u00e9forges", "Olivier", ""]]}, {"id": "2103.05363", "submitter": "Qigong Sun", "authors": "Qigong Sun, Yan Ren, Licheng Jiao, Xiufang Li, Fanhua Shang, Fang Liu", "title": "MWQ: Multiscale Wavelet Quantized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model quantization can reduce the model size and computational latency, it\nhas become an essential technique for the deployment of deep neural networks on\nresourceconstrained hardware (e.g., mobile phones and embedded devices). The\nexisting quantization methods mainly consider the numerical elements of the\nweights and activation values, ignoring the relationship between elements. The\ndecline of representation ability and information loss usually lead to the\nperformance degradation. Inspired by the characteristics of images in the\nfrequency domain, we propose a novel multiscale wavelet quantization (MWQ)\nmethod. This method decomposes original data into multiscale frequency\ncomponents by wavelet transform, and then quantizes the components of different\nscales, respectively. It exploits the multiscale frequency and spatial\ninformation to alleviate the information loss caused by quantization in the\nspatial domain. Because of the flexibility of MWQ, we demonstrate three\napplications (e.g., model compression, quantized network optimization, and\ninformation enhancement) on the ImageNet and COCO datasets. Experimental\nresults show that our method has stronger representation ability and can play\nan effective role in quantized neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 11:21:59 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Sun", "Qigong", ""], ["Ren", "Yan", ""], ["Jiao", "Licheng", ""], ["Li", "Xiufang", ""], ["Shang", "Fanhua", ""], ["Liu", "Fang", ""]]}, {"id": "2103.05368", "submitter": "Jin-Man Park", "authors": "Jin-Man Park, Jae-Hyuk Jang, Sahng-Min Yoo, Sun-Kyung Lee, Ue-Hwan\n  Kim, and Jong-Hwan Kim", "title": "ChangeSim: Towards End-to-End Online Scene Change Detection in\n  Industrial Indoor Environments", "comments": "Accepted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a challenging dataset, ChangeSim, aimed at online scene change\ndetection (SCD) and more. The data is collected in photo-realistic simulation\nenvironments with the presence of environmental non-targeted variations, such\nas air turbidity and light condition changes, as well as targeted object\nchanges in industrial indoor environments. By collecting data in simulations,\nmulti-modal sensor data and precise ground truth labels are obtainable such as\nthe RGB image, depth image, semantic segmentation, change segmentation, camera\nposes, and 3D reconstructions. While the previous online SCD datasets evaluate\nmodels given well-aligned image pairs, ChangeSim also provides raw unpaired\nsequences that present an opportunity to develop an online SCD model in an\nend-to-end manner, considering both pairing and detection. Experiments show\nthat even the latest pair-based SCD models suffer from the bottleneck of the\npairing process, and it gets worse when the environment contains the\nnon-targeted variations. Our dataset is available at\nhttp://sammica.github.io/ChangeSim/.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 11:36:29 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 06:52:15 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Park", "Jin-Man", ""], ["Jang", "Jae-Hyuk", ""], ["Yoo", "Sahng-Min", ""], ["Lee", "Sun-Kyung", ""], ["Kim", "Ue-Hwan", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "2103.05376", "submitter": "Lu Yang", "authors": "Lu Yang, Hongbang Liu, Jinghao Zhou, Lingqiao Liu, Lei Zhang, Peng\n  Wang and Yanning Zhang", "title": "Pluggable Weakly-Supervised Cross-View Learning for Accurate Vehicle\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning cross-view consistent feature representation is the key for accurate\nvehicle Re-identification (ReID), since the visual appearance of vehicles\nchanges significantly under different viewpoints. To this end, most existing\napproaches resort to the supervised cross-view learning using extensive extra\nviewpoints annotations, which however, is difficult to deploy in real\napplications due to the expensive labelling cost and the continous viewpoint\nvariation that makes it hard to define discrete viewpoint labels. In this\nstudy, we present a pluggable Weakly-supervised Cross-View Learning (WCVL)\nmodule for vehicle ReID. Through hallucinating the cross-view samples as the\nhardest positive counterparts in feature domain, we can learn the consistent\nfeature representation via minimizing the cross-view feature distance based on\nvehicle IDs only without using any viewpoint annotation. More importantly, the\nproposed method can be seamlessly plugged into most existing vehicle ReID\nbaselines for cross-view learning without re-training the baselines. To\ndemonstrate its efficacy, we plug the proposed method into a bunch of\noff-the-shelf baselines and obtain significant performance improvement on four\npublic benchmark datasets, i.e., VeRi-776, VehicleID, VRIC and VRAI.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 11:51:09 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yang", "Lu", ""], ["Liu", "Hongbang", ""], ["Zhou", "Jinghao", ""], ["Liu", "Lingqiao", ""], ["Zhang", "Lei", ""], ["Wang", "Peng", ""], ["Zhang", "Yanning", ""]]}, {"id": "2103.05385", "submitter": "Daniel Jim\\'enez-S\\'anchez", "authors": "Daniel Jim\\'enez-S\\'anchez, Mikel Ariz, Hang Chang, Xavier\n  Matias-Guiu, Carlos E. de Andrea and Carlos Ortiz-de-Sol\\'orzano", "title": "NaroNet: Discovery of tumor microenvironment elements from highly\n  multiplexed images", "comments": "37 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many efforts have been made to discover tumor-specific microenvironment\nelements (TMEs) from immunostained tissue sections. However, the identification\nof yet unknown but relevant TMEs from multiplex immunostained tissues remains a\nchallenge, due to the number of markers involved (tens) and the complexity of\ntheir spatial interactions. We present NaroNet, which uses machine learning to\nidentify and annotate known as well as novel TMEs from self-supervised\nembeddings of cells, organized at different levels (local cell phenotypes and\ncellular neighborhoods). Then it uses the abundance of TMEs to classify\npatients based on biological or clinical features. We validate NaroNet using\nsynthetic patient cohorts with adjustable incidence of different TMEs and two\ncancer patient datasets. In both synthetic and real datasets, NaroNet\nunsupervisedly identifies novel TMEs, relevant for the user-defined\nclassification task. As NaroNet requires only patient-level information, it\nrenders state-of-the-art computational methods accessible to a broad audience,\naccelerating the discovery of biomarker signatures.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:08:13 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 14:43:15 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Jim\u00e9nez-S\u00e1nchez", "Daniel", ""], ["Ariz", "Mikel", ""], ["Chang", "Hang", ""], ["Matias-Guiu", "Xavier", ""], ["de Andrea", "Carlos E.", ""], ["Ortiz-de-Sol\u00f3rzano", "Carlos", ""]]}, {"id": "2103.05395", "submitter": "Jiao Bingliang", "authors": "Bingliang Jiao and Xin Tan and Jinghao Zhou and Lu Yang and Yunlong\n  Wang and Peng Wang", "title": "Instance and Pair-Aware Dynamic Networks for Re-Identification", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-identification (ReID) is to identify the same instance across different\ncameras. Existing ReID methods mostly utilize alignment-based or\nattention-based strategies to generate effective feature representations.\nHowever, most of these methods only extract general feature by employing single\ninput image itself, overlooking the exploration of relevance between comparing\nimages. To fill this gap, we propose a novel end-to-end trainable dynamic\nconvolution framework named Instance and Pair-Aware Dynamic Networks in this\npaper. The proposed model is composed of three main branches where a\nself-guided dynamic branch is constructed to strengthen instance-specific\nfeatures, focusing on every single image. Furthermore, we also design a\nmutual-guided dynamic branch to generate pair-aware features for each pair of\nimages to be compared. Extensive experiments are conducted in order to verify\nthe effectiveness of our proposed algorithm. We evaluate our algorithm in\nseveral mainstream person and vehicle ReID datasets including CUHK03,\nDukeMTMCreID, Market-1501, VeRi776 and VehicleID. In some datasets our\nalgorithm outperforms state-of-the-art methods and in others, our algorithm\nachieves a comparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:34:41 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 09:01:27 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jiao", "Bingliang", ""], ["Tan", "Xin", ""], ["Zhou", "Jinghao", ""], ["Yang", "Lu", ""], ["Wang", "Yunlong", ""], ["Wang", "Peng", ""]]}, {"id": "2103.05399", "submitter": "Masato Tamura", "authors": "Masato Tamura, Hiroki Ohashi, Tomoaki Yoshinaga", "title": "QPIC: Query-Based Pairwise Human-Object Interaction Detection with\n  Image-Wide Contextual Information", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, intuitive yet powerful method for human-object\ninteraction (HOI) detection. HOIs are so diverse in spatial distribution in an\nimage that existing CNN-based methods face the following three major drawbacks;\nthey cannot leverage image-wide features due to CNN's locality, they rely on a\nmanually defined location-of-interest for the feature aggregation, which\nsometimes does not cover contextually important regions, and they cannot help\nbut mix up the features for multiple HOI instances if they are located closely.\nTo overcome these drawbacks, we propose a transformer-based feature extractor,\nin which an attention mechanism and query-based detection play key roles. The\nattention mechanism is effective in aggregating contextually important\ninformation image-wide, while the queries, which we design in such a way that\neach query captures at most one human-object pair, can avoid mixing up the\nfeatures from multiple instances. This transformer-based feature extractor\nproduces so effective embeddings that the subsequent detection heads may be\nfairly simple and intuitive. The extensive analysis reveals that the proposed\nmethod successfully extracts contextually important features, and thus\noutperforms existing methods by large margins (5.37 mAP on HICO-DET, and 5.7\nmAP on V-COCO). The source codes are available at\n$\\href{https://github.com/hitachi-rd-cv/qpic}{\\text{this https URL}}$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:42:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Tamura", "Masato", ""], ["Ohashi", "Hiroki", ""], ["Yoshinaga", "Tomoaki", ""]]}, {"id": "2103.05422", "submitter": "Kai Kou", "authors": "Xuelong Li, Kai Kou, and Bin Zhao", "title": "Weather GAN: Multi-Domain Weather Translation Using Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new task is proposed, namely, weather translation, which\nrefers to transferring weather conditions of the image from one category to\nanother. It is important for photographic style transfer. Although lots of\napproaches have been proposed in traditional image translation tasks, few of\nthem can handle the multi-category weather translation task, since weather\nconditions have rich categories and highly complex semantic structures. To\naddress this problem, we develop a multi-domain weather translation approach\nbased on generative adversarial networks (GAN), denoted as Weather GAN, which\ncan achieve the transferring of weather conditions among sunny, cloudy, foggy,\nrainy and snowy. Specifically, the weather conditions in the image are\ndetermined by various weather-cues, such as cloud, blue sky, wet ground, etc.\nTherefore, it is essential for weather translation to focus the main attention\non weather-cues. To this end, the generator of Weather GAN is composed of an\ninitial translation module, an attention module and a weather-cue segmentation\nmodule. The initial translation module performs global translation during\ngeneration procedure. The weather-cue segmentation module identifies the\nstructure and exact distribution of weather-cues. The attention module learns\nto focus on the interesting areas of the image while keeping other areas\nunaltered. The final generated result is synthesized by these three parts. This\napproach suppresses the distortion and deformation caused by weather\ntranslation. our approach outperforms the state-of-the-arts has been shown by a\nlarge number of experiments and evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 13:51:58 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Li", "Xuelong", ""], ["Kou", "Kai", ""], ["Zhao", "Bin", ""]]}, {"id": "2103.05423", "submitter": "Yong He", "authors": "Yong He, Hongshan Yu, Xiaoyan Liu, Zhengeng Yang, Wei Sun, Yaonan\n  Wang, Qiang Fu, Yanmei Zou and Ajmal Mian", "title": "Deep Learning based 3D Segmentation: A Survey", "comments": "Under review of ACM Computing Surveys, 36 pages, 10 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object segmentation is a fundamental and challenging problem in computer\nvision with applications in autonomous driving, robotics, augmented reality and\nmedical image analysis. It has received significant attention from the computer\nvision, graphics and machine learning communities. Traditionally, 3D\nsegmentation was performed with hand-crafted features and engineered methods\nwhich failed to achieve acceptable accuracy and could not generalize to\nlarge-scale data. Driven by their great success in 2D computer vision, deep\nlearning techniques have recently become the tool of choice for 3D segmentation\ntasks as well. This has led to an influx of a large number of methods in the\nliterature that have been evaluated on different benchmark datasets. This paper\nprovides a comprehensive survey of recent progress in deep learning based 3D\nsegmentation covering over 150 papers. It summarizes the most commonly used\npipelines, discusses their highlights and shortcomings, and analyzes the\ncompetitive results of these segmentation methods. Based on the analysis, it\nalso provides promising research directions for the future.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 13:58:35 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 03:25:02 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["He", "Yong", ""], ["Yu", "Hongshan", ""], ["Liu", "Xiaoyan", ""], ["Yang", "Zhengeng", ""], ["Sun", "Wei", ""], ["Wang", "Yaonan", ""], ["Fu", "Qiang", ""], ["Zou", "Yanmei", ""], ["Mian", "Ajmal", ""]]}, {"id": "2103.05430", "submitter": "Chun Yui Wong", "authors": "Chun Yui Wong, Pranay Seshadri, Geoffrey T. Parks", "title": "Automatic Borescope Damage Assessments for Gas Turbine Blades via Deep\n  Learning", "comments": "AIAA SciTech Forum and Exposition 2021 with added material", "journal-ref": null, "doi": "10.2514/6.2021-1488", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To maximise fuel economy, bladed components in aero-engines operate close to\nmaterial limits. The severe operating environment leads to in-service damage on\ncompressor and turbine blades, having a profound and immediate impact on the\nperformance of the engine. Current methods of blade visual inspection are\nmainly based on borescope imaging. During these inspections, the sentencing of\ncomponents under inspection requires significant manual effort, with a lack of\nsystematic approaches to avoid human biases. To perform fast and accurate\nsentencing, we propose an automatic workflow based on deep learning for\ndetecting damage present on rotor blades using borescope videos. Building upon\nstate-of-the-art methods from computer vision, we show that damage statistics\ncan be presented for each blade in a blade row separately, and demonstrate the\nworkflow on two borescope videos.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 11:44:10 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wong", "Chun Yui", ""], ["Seshadri", "Pranay", ""], ["Parks", "Geoffrey T.", ""]]}, {"id": "2103.05437", "submitter": "Fabian Balsiger", "authors": "Fabian Balsiger, Alain Jungo, Naren Akash R J, Jianan Chen, Ivan\n  Ezhov, Shengnan Liu, Jun Ma, Johannes C. Paetzold, Vishva Saravanan R, Anjany\n  Sekuboyina, Suprosanna Shit, Yannick Suter, Moshood Yekini, Guodong Zeng,\n  Markus Rempfler", "title": "The MICCAI Hackathon on reproducibility, diversity, and selection of\n  papers at the MICCAI conference", "comments": "Revision of discussion; update e-mail address of one author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The MICCAI conference has encountered tremendous growth over the last years\nin terms of the size of the community, as well as the number of contributions\nand their technical success. With this growth, however, come new challenges for\nthe community. Methods are more difficult to reproduce and the ever-increasing\nnumber of paper submissions to the MICCAI conference poses new questions\nregarding the selection process and the diversity of topics. To exchange,\ndiscuss, and find novel and creative solutions to these challenges, a new\nformat of a hackathon was initiated as a satellite event at the MICCAI 2020\nconference: The MICCAI Hackathon. The first edition of the MICCAI Hackathon\ncovered the topics reproducibility, diversity, and selection of MICCAI papers.\nIn the manner of a small think-tank, participants collaborated to find\nsolutions to these challenges. In this report, we summarize the insights from\nthe MICCAI Hackathon into immediate and long-term measures to address these\nchallenges. The proposed measures can be seen as starting points and guidelines\nfor discussions and actions to possibly improve the MICCAI conference with\nregards to reproducibility, diversity, and selection of papers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:40:15 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 08:10:09 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Balsiger", "Fabian", ""], ["Jungo", "Alain", ""], ["J", "Naren Akash R", ""], ["Chen", "Jianan", ""], ["Ezhov", "Ivan", ""], ["Liu", "Shengnan", ""], ["Ma", "Jun", ""], ["Paetzold", "Johannes C.", ""], ["R", "Vishva Saravanan", ""], ["Sekuboyina", "Anjany", ""], ["Shit", "Suprosanna", ""], ["Suter", "Yannick", ""], ["Yekini", "Moshood", ""], ["Zeng", "Guodong", ""], ["Rempfler", "Markus", ""]]}, {"id": "2103.05445", "submitter": "Giancarlo Di Biase Troccoli", "authors": "Giancarlo Di Biase, Hermann Blum, Roland Siegwart, Cesar Cadena", "title": "Pixel-wise Anomaly Detection in Complex Driving Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inability of state-of-the-art semantic segmentation methods to detect\nanomaly instances hinders them from being deployed in safety-critical and\ncomplex applications, such as autonomous driving. Recent approaches have\nfocused on either leveraging segmentation uncertainty to identify anomalous\nareas or re-synthesizing the image from the semantic label map to find\ndissimilarities with the input image. In this work, we demonstrate that these\ntwo methodologies contain complementary information and can be combined to\nproduce robust predictions for anomaly segmentation. We present a pixel-wise\nanomaly detection framework that uses uncertainty maps to improve over existing\nre-synthesis methods in finding dissimilarities between the input and generated\nimages. Our approach works as a general framework around already trained\nsegmentation networks, which ensures anomaly detection without compromising\nsegmentation accuracy, while significantly outperforming all similar methods.\nTop-2 performance across a range of different anomaly datasets shows the\nrobustness of our approach to handling different anomaly instances.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 14:26:20 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Di Biase", "Giancarlo", ""], ["Blum", "Hermann", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "2103.05448", "submitter": "Varun Mannam", "authors": "Varun Mannam, Yide Zhang, Xiaotong Yuan, Takashi Hato, Pierre C.\n  Dagher, Evan L. Nichols, Cody J. Smith, Kenneth W. Dunn, and Scott Howard", "title": "Convolutional Neural Network Denoising in Fluorescence Lifetime Imaging\n  Microscopy (FLIM)", "comments": "SPIE Proceedings Volume 11648, Multiphoton Microscopy in the\n  Biomedical Sciences XXI; 116481C (2021)", "journal-ref": null, "doi": "10.1117/12.2578574", "report-no": "116481C", "categories": "eess.IV cs.CV cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluorescence lifetime imaging microscopy (FLIM) systems are limited by their\nslow processing speed, low signal-to-noise ratio (SNR), and expensive and\nchallenging hardware setups. In this work, we demonstrate applying a denoising\nconvolutional network to improve FLIM SNR. The network will be integrated with\nan instant FLIM system with fast data acquisition based on analog signal\nprocessing, high SNR using high-efficiency pulse-modulation, and cost-effective\nimplementation utilizing off-the-shelf radio-frequency components. Our instant\nFLIM system simultaneously provides the intensity, lifetime, and phasor plots\n\\textit{in vivo} and \\textit{ex vivo}. By integrating image denoising using the\ntrained deep learning model on the FLIM data, provide accurate FLIM phasor\nmeasurements are obtained. The enhanced phasor is then passed through the\nK-means clustering segmentation method, an unbiased and unsupervised machine\nlearning technique to separate different fluorophores accurately. Our\nexperimental \\textit{in vivo} mouse kidney results indicate that introducing\nthe deep learning image denoising model before the segmentation effectively\nremoves the noise in the phasor compared to existing methods and provides\nclearer segments. Hence, the proposed deep learning-based workflow provides\nfast and accurate automatic segmentation of fluorescence images using instant\nFLIM. The denoising operation is effective for the segmentation if the FLIM\nmeasurements are noisy. The clustering can effectively enhance the detection of\nbiological structures of interest in biomedical imaging applications.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 03:27:44 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Mannam", "Varun", ""], ["Zhang", "Yide", ""], ["Yuan", "Xiaotong", ""], ["Hato", "Takashi", ""], ["Dagher", "Pierre C.", ""], ["Nichols", "Evan L.", ""], ["Smith", "Cody J.", ""], ["Dunn", "Kenneth W.", ""], ["Howard", "Scott", ""]]}, {"id": "2103.05463", "submitter": "Chaohao Xie", "authors": "Chaohao Xie, Dongwei Ren, Lei Wang, Qinghua Hu, Liang Lin, Wangmeng\n  Zuo", "title": "Learning Class-Agnostic Pseudo Mask Generation for Box-Supervised\n  Semantic Segmentation", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several weakly supervised learning methods have been devoted to\nutilize bounding box supervision for training deep semantic segmentation\nmodels. Most existing methods usually leverage the generic proposal generators\n(\\eg, dense CRF and MCG) to produce enhanced segmentation masks for further\ntraining segmentation models. These proposal generators, however, are generic\nand not specifically designed for box-supervised semantic segmentation, thereby\nleaving some leeway for improving segmentation performance. In this paper, we\naim at seeking for a more accurate learning-based class-agnostic pseudo mask\ngenerator tailored to box-supervised semantic segmentation. To this end, we\nresort to a pixel-level annotated auxiliary dataset where the class labels are\nnon-overlapped with those of the box-annotated dataset. For learning pseudo\nmask generator from the auxiliary dataset, we present a bi-level optimization\nformulation. In particular, the lower subproblem is used to learn\nbox-supervised semantic segmentation, while the upper subproblem is used to\nlearn an optimal class-agnostic pseudo mask generator. The learned pseudo\nsegmentation mask generator can then be deployed to the box-annotated dataset\nfor improving weakly supervised semantic segmentation. Experiments on PASCAL\nVOC 2012 dataset show that the learned pseudo mask generator is effective in\nboosting segmentation performance, and our method can further close the\nperformance gap between box-supervised and fully-supervised models. Our code\nwill be made publicly available at\nhttps://github.com/Vious/LPG_BBox_Segmentation .\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 14:54:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Xie", "Chaohao", ""], ["Ren", "Dongwei", ""], ["Wang", "Lei", ""], ["Hu", "Qinghua", ""], ["Lin", "Liang", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2103.05465", "submitter": "Xuyang Bai Mr.", "authors": "Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li, Zeyu Hu, Hongbo\n  Fu, Chiew-Lan Tai", "title": "PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency", "comments": "Accepted to CVPR 2021, supplementary materials included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Removing outlier correspondences is one of the critical steps for successful\nfeature-based point cloud registration. Despite the increasing popularity of\nintroducing deep learning methods in this field, spatial consistency, which is\nessentially established by a Euclidean transformation between point clouds, has\nreceived almost no individual attention in existing learning frameworks. In\nthis paper, we present PointDSC, a novel deep neural network that explicitly\nincorporates spatial consistency for pruning outlier correspondences. First, we\npropose a nonlocal feature aggregation module, weighted by both feature and\nspatial coherence, for feature embedding of the input correspondences. Second,\nwe formulate a differentiable spectral matching module, supervised by pairwise\nspatial compatibility, to estimate the inlier confidence of each correspondence\nfrom the embedded features. With modest computation cost, our method\noutperforms the state-of-the-art hand-crafted and learning-based outlier\nrejection approaches on several real-world datasets by a significant margin. We\nalso show its wide applicability by combining PointDSC with different 3D local\ndescriptors.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 14:56:08 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Bai", "Xuyang", ""], ["Luo", "Zixin", ""], ["Zhou", "Lei", ""], ["Chen", "Hongkai", ""], ["Li", "Lei", ""], ["Hu", "Zeyu", ""], ["Fu", "Hongbo", ""], ["Tai", "Chiew-Lan", ""]]}, {"id": "2103.05467", "submitter": "Roni Saputra Permana", "authors": "Liana Ellen Taylor, Midriem Mirdanies, Roni Permana Saputra", "title": "Optimized Object Tracking Technique Using Kalman Filter", "comments": "10 pages, 14 figures, published in J. Mechatron. Electr. Power Veh.\n  Technol 07 (2016) 57-66", "journal-ref": "J. Mechatron. Electr. Power Veh. Technol 07 (2016) 57-66", "doi": "10.14203/j.mev.2016.v7.57-66", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focused on the design of an optimized object tracking technique\nwhich would minimize the processing time required in the object detection\nprocess while maintaining accuracy in detecting the desired moving object in a\ncluttered scene. A Kalman filter based cropped image is used for the image\ndetection process as the processing time is significantly less to detect the\nobject when a search window is used that is smaller than the entire video\nframe. This technique was tested with various sizes of the window in the\ncropping process. MATLAB was used to design and test the proposed method. This\npaper found that using a cropped image with 2.16 multiplied by the largest\ndimension of the object resulted in significantly faster processing time while\nstill providing a high success rate of detection and a detected center of the\nobject that was reasonably close to the actual center.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 13:32:31 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Taylor", "Liana Ellen", ""], ["Mirdanies", "Midriem", ""], ["Saputra", "Roni Permana", ""]]}, {"id": "2103.05469", "submitter": "Mark Stamp", "authors": "Andy Phung and Mark Stamp", "title": "Universal Adversarial Perturbations and Image Spam Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the name suggests, image spam is spam email that has been embedded in an\nimage. Image spam was developed in an effort to evade text-based filters.\nModern deep learning-based classifiers perform well in detecting typical image\nspam that is seen in the wild. In this chapter, we evaluate numerous\nadversarial techniques for the purpose of attacking deep learning-based image\nspam classifiers. Of the techniques tested, we find that universal perturbation\nperforms best. Using universal adversarial perturbations, we propose and\nanalyze a new transformation-based adversarial attack that enables us to create\ntailored \"natural perturbations\" in image spam. The resulting spam images\nbenefit from both the presence of concentrated natural features and a universal\nadversarial perturbation. We show that the proposed technique outperforms\nexisting adversarial attacks in terms of accuracy reduction, computation time\nper example, and perturbation distance. We apply our technique to create a\ndataset of adversarial spam images, which can serve as a challenge dataset for\nfuture research in image spam detection.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 14:36:02 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Phung", "Andy", ""], ["Stamp", "Mark", ""]]}, {"id": "2103.05471", "submitter": "Mingkui Tan", "authors": "Yaofo Chen, Yong Guo, Qi Chen, Minli Li, Wei Zeng, Yaowei Wang,\n  Mingkui Tan", "title": "Contrastive Neural Architecture Search with Neural Architecture\n  Comparators", "comments": "Accpeted by CVPR 2021. The code is available at\n  https://github.com/chenyaofo/CTNAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key steps in Neural Architecture Search (NAS) is to estimate the\nperformance of candidate architectures. Existing methods either directly use\nthe validation performance or learn a predictor to estimate the performance.\nHowever, these methods can be either computationally expensive or very\ninaccurate, which may severely affect the search efficiency and performance.\nMoreover, as it is very difficult to annotate architectures with accurate\nperformance on specific tasks, learning a promising performance predictor is\noften non-trivial due to the lack of labeled data. In this paper, we argue that\nit may not be necessary to estimate the absolute performance for NAS. On the\ncontrary, we may need only to understand whether an architecture is better than\na baseline one. However, how to exploit this comparison information as the\nreward and how to well use the limited labeled data remains two great\nchallenges. In this paper, we propose a novel Contrastive Neural Architecture\nSearch (CTNAS) method which performs architecture search by taking the\ncomparison results between architectures as the reward. Specifically, we design\nand learn a Neural Architecture Comparator (NAC) to compute the probability of\ncandidate architectures being better than a baseline one. Moreover, we present\na baseline updating scheme to improve the baseline iteratively in a curriculum\nlearning manner. More critically, we theoretically show that learning NAC is\nequivalent to optimizing the ranking over architectures. Extensive experiments\nin three search spaces demonstrate the superiority of our CTNAS over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 11:24:07 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 07:00:10 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chen", "Yaofo", ""], ["Guo", "Yong", ""], ["Chen", "Qi", ""], ["Li", "Minli", ""], ["Zeng", "Wei", ""], ["Wang", "Yaowei", ""], ["Tan", "Mingkui", ""]]}, {"id": "2103.05472", "submitter": "Tao Li", "authors": "Tao Li, Chris Clifton", "title": "Differentially Private Imaging via Latent Space Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing concern about image privacy due to the popularity of social\nmedia and photo devices, along with increasing use of face recognition systems.\nHowever, established image de-identification techniques are either too subject\nto re-identification, produce photos that are insufficiently realistic, or\nboth. To tackle this, we present a novel approach for image obfuscation by\nmanipulating latent spaces of an unconditionally trained generative model that\nis able to synthesize photo-realistic facial images of high resolution. This\nmanipulation is done in a way that satisfies the formal privacy standard of\nlocal differential privacy. To our knowledge, this is the first approach to\nimage privacy that satisfies $\\varepsilon$-differential privacy \\emph{for the\nperson.}\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:32:08 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 06:27:26 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Li", "Tao", ""], ["Clifton", "Chris", ""]]}, {"id": "2103.05484", "submitter": "Zhiyuan Dang", "authors": "Zhiyuan Dang, Cheng Deng, Xu Yang, Heng Huang", "title": "Doubly Contrastive Deep Clustering", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep clustering successfully provides more effective features than\nconventional ones and thus becomes an important technique in current\nunsupervised learning. However, most deep clustering methods ignore the vital\npositive and negative pairs introduced by data augmentation and further the\nsignificance of contrastive learning, which leads to suboptimal performance. In\nthis paper, we present a novel Doubly Contrastive Deep Clustering (DCDC)\nframework, which constructs contrastive loss over both sample and class views\nto obtain more discriminative features and competitive results. Specifically,\nfor the sample view, we set the class distribution of the original sample and\nits augmented version as positive sample pairs and set one of the other\naugmented samples as negative sample pairs. After that, we can adopt the\nsample-wise contrastive loss to pull positive sample pairs together and push\nnegative sample pairs apart. Similarly, for the class view, we build the\npositive and negative pairs from the sample distribution of the class. In this\nway, two contrastive losses successfully constrain the clustering results of\nmini-batch samples in both sample and class level. Extensive experimental\nresults on six benchmark datasets demonstrate the superiority of our proposed\nmodel against state-of-the-art methods. Particularly in the challenging dataset\nTiny-ImageNet, our method leads 5.6\\% against the latest comparison method. Our\ncode will be available at \\url{https://github.com/ZhiyuanDang/DCDC}.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 15:15:32 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Dang", "Zhiyuan", ""], ["Deng", "Cheng", ""], ["Yang", "Xu", ""], ["Huang", "Heng", ""]]}, {"id": "2103.05489", "submitter": "Jan Koh\\'ut", "authors": "Jan Koh\\'ut, Michal Hradi\\v{s}", "title": "TS-Net: OCR Trained to Switch Between Text Transcription Styles", "comments": "Submitted to ICDAR2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of OCR systems, from different institutions and scientific disciplines,\nprefer and produce different transcription styles. This presents a problem for\ntraining of consistent text recognition neural networks on real-world data. We\npropose to extend existing text recognition networks with a Transcription Style\nBlock (TSB) which can learn from data to switch between multiple transcription\nstyles without any explicit knowledge of transcription rules. TSB is an\nadaptive instance normalization conditioned by identifiers representing\nconsistently transcribed documents (e.g. single document, documents by a single\ntranscriber, or an institution). We show that TSB is able to learn completely\ndifferent transcription styles in controlled experiments on artificial data, it\nimproves text recognition accuracy on large-scale real-world data, and it\nlearns semantically meaningful transcription style embedding. We also show how\nTSB can efficiently adapt to transcription styles of new documents from\ntranscriptions of only a few text lines.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 15:21:40 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Koh\u00fat", "Jan", ""], ["Hradi\u0161", "Michal", ""]]}, {"id": "2103.05525", "submitter": "Seyoun Park", "authors": "Seyoun Park, Elliot K. Fishman, Alan L. Yuille", "title": "Multi-phase Deformable Registration for Time-dependent Abdominal Organ\n  Variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Human body is a complex dynamic system composed of various sub-dynamic parts.\nEspecially, thoracic and abdominal organs have complex internal shape\nvariations with different frequencies by various reasons such as respiration\nwith fast motion and peristalsis with slower motion. CT protocols for abdominal\nlesions are multi-phase scans for various tumor detection to use different\nvascular contrast, however, they are not aligned well enough to visually check\nthe same area. In this paper, we propose a time-efficient and accurate\ndeformable registration algorithm for multi-phase CT scans considering\nabdominal organ motions, which can be applied for differentiable or\nnon-differentiable motions of abdominal organs. Experimental results shows the\nregistration accuracy as 0.85 +/- 0.45mm (mean +/- STD) for pancreas within 1\nminute for the whole abdominal region.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:43:23 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Park", "Seyoun", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "2103.05529", "submitter": "K. Ruwani Fernando", "authors": "K. Ruwani M. Fernando and Chris P. Tsokos", "title": "Deep and Statistical Learning in Biomedical Imaging: State of the Art in\n  3D MRI Brain Tumor Segmentation", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Clinical diagnostic and treatment decisions rely upon the integration of\npatient-specific data with clinical reasoning. Cancer presents a unique context\nthat influence treatment decisions, given its diverse forms of disease\nevolution. Biomedical imaging allows noninvasive assessment of disease based on\nvisual evaluations leading to better clinical outcome prediction and\ntherapeutic planning. Early methods of brain cancer characterization\npredominantly relied upon statistical modeling of neuroimaging data. Driven by\nthe breakthroughs in computer vision, deep learning became the de facto\nstandard in the domain of medical imaging. Integrated statistical and deep\nlearning methods have recently emerged as a new direction in the automation of\nthe medical practice unifying multi-disciplinary knowledge in medicine,\nstatistics, and artificial intelligence. In this study, we critically review\nmajor statistical and deep learning models and their applications in brain\nimaging research with a focus on MRI-based brain tumor segmentation. The\nresults do highlight that model-driven classical statistics and data-driven\ndeep learning is a potent combination for developing automated systems in\nclinical oncology.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 16:15:47 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Fernando", "K. Ruwani M.", ""], ["Tsokos", "Chris P.", ""]]}, {"id": "2103.05546", "submitter": "Ziyang Wang", "authors": "Ziyang Wang", "title": "Quadruple Augmented Pyramid Network for Multi-class COVID-19\n  Segmentation via CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19, a new strain of coronavirus disease, has been one of the most\nserious and infectious disease in the world. Chest CT is essential in\nprognostication, diagnosing this disease, and assessing the complication. In\nthis paper, a multi-class COVID-19 CT segmentation is proposed aiming at\nhelping radiologists estimate the extent of effected lung volume. We utilized\nfour augmented pyramid networks on an encoder-decoder segmentation framework.\nQuadruple Augmented Pyramid Network (QAP-Net) not only enable CNN capture\nfeatures from variation size of CT images, but also act as spatial\ninterconnections and down-sampling to transfer sufficient feature information\nfor semantic segmentation. Experimental results achieve competitive performance\nin segmentation with the Dice of 0.8163, which outperforms other\nstate-of-the-art methods, demonstrating the proposed framework can segments of\nconsolidation as well as glass, ground area via COVID-19 chest CT efficiently\nand accurately.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 16:48:15 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wang", "Ziyang", ""]]}, {"id": "2103.05558", "submitter": "Chaoyi Zhang", "authors": "Chaoyi Zhang, Jianhui Yu, Yang Song, Weidong Cai", "title": "Exploiting Edge-Oriented Reasoning for 3D Point-based Scene Graph\n  Analysis", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding is a critical problem in computer vision. In this paper,\nwe propose a 3D point-based scene graph generation ($\\mathbf{SGG_{point}}$)\nframework to effectively bridge perception and reasoning to achieve scene\nunderstanding via three sequential stages, namely scene graph construction,\nreasoning, and inference. Within the reasoning stage, an EDGE-oriented Graph\nConvolutional Network ($\\texttt{EdgeGCN}$) is created to exploit\nmulti-dimensional edge features for explicit relationship modeling, together\nwith the exploration of two associated twinning interaction mechanisms between\nnodes and edges for the independent evolution of scene graph representations.\nOverall, our integrated $\\mathbf{SGG_{point}}$ framework is established to seek\nand infer scene structures of interest from both real-world and synthetic 3D\npoint-based scenes. Our experimental results show promising edge-oriented\nreasoning effects on scene graph generation studies. We also demonstrate our\nmethod advantage on several traditional graph representation learning benchmark\ndatasets, including the node-wise classification on citation networks and\nwhole-graph recognition problems for molecular analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:09:46 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 09:18:51 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhang", "Chaoyi", ""], ["Yu", "Jianhui", ""], ["Song", "Yang", ""], ["Cai", "Weidong", ""]]}, {"id": "2103.05568", "submitter": "Mayank Kothyari", "authors": "Aman Jain, Mayank Kothyari, Vishwajeet Kumar, Preethi Jyothi, Ganesh\n  Ramakrishnan, Soumen Chakrabarti", "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented\n  Visual Question Answering", "comments": "Added new references, some more implementation details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal IR, spanning text corpus, knowledge graph and images, called\noutside knowledge visual question answering (OKVQA), is of much recent\ninterest. However, the popular data set has serious limitations. A surprisingly\nlarge fraction of queries do not assess the ability to integrate cross-modal\ninformation. Instead, some are independent of the image, some depend on\nspeculation, some require OCR or are otherwise answerable from the image alone.\nTo add to the above limitations, frequency-based guessing is very effective\nbecause of (unintended) widespread answer overlaps between the train and test\nfolds. Overall, it is hard to determine when state-of-the-art systems exploit\nthese weaknesses rather than really infer the answers, because they are opaque\nand their 'reasoning' process is uninterpretable. An equally important\nlimitation is that the dataset is designed for the quantitative assessment only\nof the end-to-end answer retrieval task, with no provision for assessing the\ncorrect(semantic) interpretation of the input query. In response, we identify a\nkey structural idiom in OKVQA ,viz., S3 (select, substitute and search), and\nbuild a new data set and challenge around it. Specifically, the questioner\nidentifies an entity in the image and asks a question involving that entity\nwhich can be answered only by consulting a knowledge graph or corpus passage\nmentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA\nannotated based on the structural idiom and (ii)S3VQA, a new dataset built from\nscratch. We also present a neural but structurally transparent OKVQA system,\nS3, that explicitly addresses our challenge dataset, and outperforms recent\ncompetitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:19:50 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:31:16 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Jain", "Aman", ""], ["Kothyari", "Mayank", ""], ["Kumar", "Vishwajeet", ""], ["Jyothi", "Preethi", ""], ["Ramakrishnan", "Ganesh", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "2103.05569", "submitter": "Peijin Wang", "authors": "Xian Sun and Peijin Wang and Zhiyuan Yan and Feng Xu and Ruiping Wang\n  and Wenhui Diao and Jin Chen and Jihao Li and Yingchao Feng and Tao Xu and\n  Martin Weinmann and Stefan Hinz and Cheng Wang and Kun Fu", "title": "FAIR1M: A Benchmark Dataset for Fine-grained Object Recognition in\n  High-Resolution Remote Sensing Imagery", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of deep learning, many deep learning-based\napproaches have made great achievements in object detection task. It is\ngenerally known that deep learning is a data-driven method. Data directly\nimpact the performance of object detectors to some extent. Although existing\ndatasets have included common objects in remote sensing images, they still have\nsome limitations in terms of scale, categories, and images. Therefore, there is\na strong requirement for establishing a large-scale benchmark on object\ndetection in high-resolution remote sensing images. In this paper, we propose a\nnovel benchmark dataset with more than 1 million instances and more than 15,000\nimages for Fine-grAined object recognItion in high-Resolution remote sensing\nimagery which is named as FAIR1M. All objects in the FAIR1M dataset are\nannotated with respect to 5 categories and 37 sub-categories by oriented\nbounding boxes. Compared with existing detection datasets dedicated to object\ndetection, the FAIR1M dataset has 4 particular characteristics: (1) it is much\nlarger than other existing object detection datasets both in terms of the\nquantity of instances and the quantity of images, (2) it provides more rich\nfine-grained category information for objects in remote sensing images, (3) it\ncontains geographic information such as latitude, longitude and resolution, (4)\nit provides better image quality owing to a careful data cleaning procedure. To\nestablish a baseline for fine-grained object recognition, we propose a novel\nevaluation method and benchmark fine-grained object detection tasks and a\nvisual classification task using several State-Of-The-Art (SOTA) deep\nlearning-based models on our FAIR1M dataset. Experimental results strongly\nindicate that the FAIR1M dataset is closer to practical application and it is\nconsiderably more challenging than existing datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:20:15 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 18:05:34 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Sun", "Xian", ""], ["Wang", "Peijin", ""], ["Yan", "Zhiyuan", ""], ["Xu", "Feng", ""], ["Wang", "Ruiping", ""], ["Diao", "Wenhui", ""], ["Chen", "Jin", ""], ["Li", "Jihao", ""], ["Feng", "Yingchao", ""], ["Xu", "Tao", ""], ["Weinmann", "Martin", ""], ["Hinz", "Stefan", ""], ["Wang", "Cheng", ""], ["Fu", "Kun", ""]]}, {"id": "2103.05585", "submitter": "Quan Liu", "authors": "Quan Liu, Peter C. Louis, Yuzhe Lu, Aadarsh Jha, Mengyang Zhao,\n  Ruining Deng, Tianyuan Yao, Joseph T. Roland, Haichun Yang, Shilin Zhao, Lee\n  E. Wheless, Yuankai Huo", "title": "SimTriplet: Simple Triplet Representation Learning with a Single GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning is a key technique of modern self-supervised learning.\nThe broader accessibility of earlier approaches is hindered by the need of\nheavy computational resources (e.g., at least 8 GPUs or 32 TPU cores), which\naccommodate for large-scale negative samples or momentum. The more recent\nSimSiam approach addresses such key limitations via stop-gradient without\nmomentum encoders. In medical image analysis, multiple instances can be\nachieved from the same patient or tissue. Inspired by these advances, we\npropose a simple triplet representation learning (SimTriplet) approach on\npathological images. The contribution of the paper is three-fold: (1) The\nproposed SimTriplet method takes advantage of the multi-view nature of medical\nimages beyond self-augmentation; (2) The method maximizes both intra-sample and\ninter-sample similarities via triplets from positive pairs, without using\nnegative samples; and (3) The recent mix precision training is employed to\nadvance the training by only using a single GPU with 16GB memory. By learning\nfrom 79,000 unlabeled pathological patch images, SimTriplet achieved 10.58%\nbetter performance compared with supervised learning. It also achieved 2.13%\nbetter performance compared with SimSiam. Our proposed SimTriplet can achieve\ndecent performance using only 1% labeled data. The code and data are available\nat https://github.com/hrlblab/SimTriple.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:46:09 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Liu", "Quan", ""], ["Louis", "Peter C.", ""], ["Lu", "Yuzhe", ""], ["Jha", "Aadarsh", ""], ["Zhao", "Mengyang", ""], ["Deng", "Ruining", ""], ["Yao", "Tianyuan", ""], ["Roland", "Joseph T.", ""], ["Yang", "Haichun", ""], ["Zhao", "Shilin", ""], ["Wheless", "Lee E.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2103.05600", "submitter": "Stylianos Venieris", "authors": "Stylianos I. Venieris, Javier Fernandez-Marques, Nicholas D. Lane", "title": "unzipFPGA: Enhancing FPGA-based CNN Engines with On-the-Fly Weights\n  Generation", "comments": "Accepted at the 29th IEEE International Symposium on\n  Field-Programmable Custom Computing Machines (FCCM) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single computation engines have become a popular design choice for FPGA-based\nconvolutional neural networks (CNNs) enabling the deployment of diverse models\nwithout fabric reconfiguration. This flexibility, however, often comes with\nsignificantly reduced performance on memory-bound layers and resource\nunderutilisation due to suboptimal mapping of certain layers on the engine's\nfixed configuration. In this work, we investigate the implications in terms of\nCNN engine design for a class of models that introduce a pre-convolution stage\nto decompress the weights at run time. We refer to these approaches as\non-the-fly. To minimise the negative impact of limited bandwidth on\nmemory-bound layers, we present a novel hardware component that enables the\non-chip on-the-fly generation of weights. We further introduce an input\nselective processing element (PE) design that balances the load between PEs on\nsuboptimally mapped layers. Finally, we present unzipFPGA, a framework to train\non-the-fly models and traverse the design space to select the highest\nperforming CNN engine configuration. Quantitative evaluation shows that\nunzipFPGA yields an average speedup of 2.14x and 71% over optimised status-quo\nand pruned CNN engines under constrained bandwidth and up to 3.69x higher\nperformance density over the state-of-the-art FPGA-based CNN accelerators.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:19:41 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 14:15:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Fernandez-Marques", "Javier", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2103.05606", "submitter": "Pakkapon Phongthawee", "authors": "Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai,\n  Supasorn Suwajanakorn", "title": "NeX: Real-time View Synthesis with Neural Basis Expansion", "comments": "CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NeX, a new approach to novel view synthesis based on enhancements\nof multiplane image (MPI) that can reproduce next-level view-dependent effects\n-- in real time. Unlike traditional MPI that uses a set of simple RGB$\\alpha$\nplanes, our technique models view-dependent effects by instead parameterizing\neach pixel as a linear combination of basis functions learned from a neural\nnetwork. Moreover, we propose a hybrid implicit-explicit modeling strategy that\nimproves upon fine detail and produces state-of-the-art results. Our method is\nevaluated on benchmark forward-facing datasets as well as our newly-introduced\ndataset designed to test the limit of view-dependent modeling with\nsignificantly more challenging effects such as rainbow reflections on a CD. Our\nmethod achieves the best overall scores across all major metrics on these\ndatasets with more than 1000$\\times$ faster rendering time than the state of\nthe art. For real-time demos, visit https://nex-mpi.github.io/\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:27:27 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 09:40:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wizadwongsa", "Suttisak", ""], ["Phongthawee", "Pakkapon", ""], ["Yenphraphai", "Jiraphon", ""], ["Suwajanakorn", "Supasorn", ""]]}, {"id": "2103.05617", "submitter": "Shijie Li", "authors": "Shijie Li, Neel Dey, Katharina Bermond, Leon von der Emde, Christine\n  A. Curcio, Thomas Ach, Guido Gerig", "title": "Point-supervised Segmentation of Microscopy Images and Volumes via\n  Objectness Regularization", "comments": "Accepted to IEEE ISBI 2021. Code available at\n  https://github.com/CJLee94/Point-Supervised-Segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotation is a major hurdle in the semantic segmentation of microscopy\nimages and volumes due to its prerequisite expertise and effort. This work\nenables the training of semantic segmentation networks on images with only a\nsingle point for training per instance, an extreme case of weak supervision\nwhich drastically reduces the burden of annotation. Our approach has two key\naspects: (1) we construct a graph-theoretic soft-segmentation using individual\nseeds to be used within a regularizer during training and (2) we use an\nobjective function that enables learning from the constructed soft-labels. We\nachieve competitive results against the state-of-the-art in point-supervised\nsemantic segmentation on challenging datasets in digital pathology. Finally, we\nscale our methodology to point-supervised segmentation in 3D fluorescence\nmicroscopy volumes, obviating the need for arduous manual volumetric\ndelineation. Our code is freely available.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:40:00 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 01:39:18 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Shijie", ""], ["Dey", "Neel", ""], ["Bermond", "Katharina", ""], ["von der Emde", "Leon", ""], ["Curcio", "Christine A.", ""], ["Ach", "Thomas", ""], ["Gerig", "Guido", ""]]}, {"id": "2103.05630", "submitter": "Yinan He", "authors": "Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song,\n  Lu Sheng, Jing Shao and Ziwei Liu", "title": "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis", "comments": "17 pages, 11 figures, Accepted to CVPR 2021 (Oral), project webpage:\n  https://yinanhe.github.io/projects/forgerynet.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The rapid progress of photorealistic synthesis techniques has reached at a\ncritical point where the boundary between real and manipulated images starts to\nblur. Thus, benchmarking and advancing digital forgery analysis have become a\npressing issue. However, existing face forgery datasets either have limited\ndiversity or only support coarse-grained analysis. To counter this emerging\nthreat, we construct the ForgeryNet dataset, an extremely large face forgery\ndataset with unified annotations in image- and video-level data across four\ntasks: 1) Image Forgery Classification, including two-way (real / fake),\nthree-way (real / fake with identity-replaced forgery approaches / fake with\nidentity-remained forgery approaches), and n-way (real and 15 respective\nforgery approaches) classification. 2) Spatial Forgery Localization, which\nsegments the manipulated area of fake images compared to their corresponding\nsource real images. 3) Video Forgery Classification, which re-defines the\nvideo-level forgery classification with manipulated frames in random positions.\nThis task is important because attackers in real world are free to manipulate\nany target frame. and 4) Temporal Forgery Localization, to localize the\ntemporal segments which are manipulated. ForgeryNet is by far the largest\npublicly available deep face forgery dataset in terms of data-scale (2.9\nmillion images, 221,247 videos), manipulations (7 image-level approaches, 8\nvideo-level approaches), perturbations (36 independent and more mixed\nperturbations) and annotations (6.3 million classification labels, 2.9 million\nmanipulated area annotations and 221,247 temporal forgery segment labels). We\nperform extensive benchmarking and studies of existing face forensics methods\nand obtain several valuable observations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:58:32 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 06:26:05 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["He", "Yinan", ""], ["Gan", "Bei", ""], ["Chen", "Siyu", ""], ["Zhou", "Yichun", ""], ["Yin", "Guojun", ""], ["Song", "Luchuan", ""], ["Sheng", "Lu", ""], ["Shao", "Jing", ""], ["Liu", "Ziwei", ""]]}, {"id": "2103.05669", "submitter": "Beril Besbinar", "authors": "Beril Besbinar, Pascal Frossard", "title": "Self-Supervision by Prediction for Object Discovery in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their irresistible success, deep learning algorithms still heavily\nrely on annotated data. On the other hand, unsupervised settings pose many\nchallenges, especially about determining the right inductive bias in diverse\nscenarios. One scalable solution is to make the model generate the supervision\nfor itself by leveraging some part of the input data, which is known as\nself-supervised learning. In this paper, we use the prediction task as\nself-supervision and build a novel object-centric model for image sequence\nrepresentation. In addition to disentangling the notion of objects and the\nmotion dynamics, our compositional structure explicitly handles occlusion and\ninpaints inferred objects and background for the composition of the predicted\nframe. With the aid of auxiliary loss functions that promote spatially and\ntemporally consistent object representations, our self-supervised framework can\nbe trained without the help of any manual annotation or pretrained network.\nInitial experiments confirm that the proposed pipeline is a promising step\ntowards object-centric video prediction.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:14:33 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Besbinar", "Beril", ""], ["Frossard", "Pascal", ""]]}, {"id": "2103.05677", "submitter": "Mengmeng Ma", "authors": "Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, Xi Peng", "title": "SMIL: Multimodal Learning with Severely Missing Modality", "comments": "In AAAI 2021 (9 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A common assumption in multimodal learning is the completeness of training\ndata, i.e., full modalities are available in all training examples. Although\nthere exists research endeavor in developing novel methods to tackle the\nincompleteness of testing data, e.g., modalities are partially missing in\ntesting examples, few of them can handle incomplete training modalities. The\nproblem becomes even more challenging if considering the case of severely\nmissing, e.g., 90% training examples may have incomplete modalities. For the\nfirst time in the literature, this paper formally studies multimodal learning\nwith missing modality in terms of flexibility (missing modalities in training,\ntesting, or both) and efficiency (most training data have incomplete modality).\nTechnically, we propose a new method named SMIL that leverages Bayesian\nmeta-learning in uniformly achieving both objectives. To validate our idea, we\nconduct a series of experiments on three popular benchmarks: MM-IMDb, CMU-MOSI,\nand avMNIST. The results prove the state-of-the-art performance of SMIL over\nexisting methods and generative baselines including autoencoders and generative\nadversarial networks. Our code is available at\nhttps://github.com/mengmenm/SMIL.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:27:08 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ma", "Mengmeng", ""], ["Ren", "Jian", ""], ["Zhao", "Long", ""], ["Tulyakov", "Sergey", ""], ["Wu", "Cathy", ""], ["Peng", "Xi", ""]]}, {"id": "2103.05687", "submitter": "Kailun Yang", "authors": "Kailun Yang, Jiaming Zhang, Simon Rei{\\ss}, Xinxin Hu, Rainer\n  Stiefelhagen", "title": "Capturing Omni-Range Context for Omnidirectional Segmentation", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Networks (ConvNets) excel at semantic segmentation and have\nbecome a vital component for perception in autonomous driving. Enabling an\nall-encompassing view of street-scenes, omnidirectional cameras present\nthemselves as a perfect fit in such systems. Most segmentation models for\nparsing urban environments operate on common, narrow Field of View (FoV)\nimages. Transferring these models from the domain they were designed for to\n360-degree perception, their performance drops dramatically, e.g., by an\nabsolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of\nFoV and structural distribution between the imaging domains, we introduce\nEfficient Concurrent Attention Networks (ECANets), directly capturing the\ninherent long-range dependencies in omnidirectional imagery. In addition to the\nlearned attention-based contextual priors that can stretch across 360-degree\nimages, we upgrade model training by leveraging multi-source and\nomni-supervised learning, taking advantage of both: Densely labeled and\nunlabeled data originating from multiple datasets. To foster progress in\npanoramic image segmentation, we put forward and extensively evaluate models on\nWild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture\ndiverse scenes from all around the globe. Our novel model, training regimen and\nmulti-source prediction fusion elevate the performance (mIoU) to new\nstate-of-the-art results on the public PASS (60.2%) and the fresh WildPASS\n(69.0%) benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:46:09 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Kailun", ""], ["Zhang", "Jiaming", ""], ["Rei\u00df", "Simon", ""], ["Hu", "Xinxin", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2103.05690", "submitter": "Saad Nadeem", "authors": "Navdeep Dahiya, Sadegh R Alam, Pengpeng Zhang, Si-Yuan Zhang, Anthony\n  Yezzi, and Saad Nadeem", "title": "Multitask 3D CBCT-to-CT Translation and Organs-at-Risk Segmentation\n  Using Physics-Based Data Augmentation", "comments": "Medical Physics 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current clinical practice, noisy and artifact-ridden weekly cone-beam\ncomputed tomography (CBCT) images are only used for patient setup during\nradiotherapy. Treatment planning is done once at the beginning of the treatment\nusing high-quality planning CT (pCT) images and manual contours for\norgans-at-risk (OARs) structures. If the quality of the weekly CBCT images can\nbe improved while simultaneously segmenting OAR structures, this can provide\ncritical information for adapting radiotherapy mid-treatment as well as for\nderiving biomarkers for treatment response. Using a novel physics-based data\naugmentation strategy, we synthesize a large dataset of perfectly/inherently\nregistered planning CT and synthetic-CBCT pairs for locally advanced lung\ncancer patient cohort, which are then used in a multitask 3D deep learning\nframework to simultaneously segment and translate real weekly CBCT images to\nhigh-quality planning CT-like images. We compared the synthetic CT and OAR\nsegmentations generated by the model to real planning CT and manual OAR\nsegmentations and showed promising results. The real week 1 (baseline) CBCT\nimages which had an average MAE of 162.77 HU compared to pCT images are\ntranslated to synthetic CT images that exhibit a drastically improved average\nMAE of 29.31 HU and average structural similarity of 92% with the pCT images.\nThe average DICE scores of the 3D organs-at-risk segmentations are: lungs 0.96,\nheart 0.88, spinal cord 0.83 and esophagus 0.66. This approach could allow\nclinicians to adjust treatment plans using only the routine low-quality CBCT\nimages, potentially improving patient outcomes. Our code, data, and pre-trained\nmodels will be made available via our physics-based data augmentation library,\nPhysics-ArX, at https://github.com/nadeemlab/Physics-ArX.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:51:44 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 16:45:50 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 17:39:34 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Dahiya", "Navdeep", ""], ["Alam", "Sadegh R", ""], ["Zhang", "Pengpeng", ""], ["Zhang", "Si-Yuan", ""], ["Yezzi", "Anthony", ""], ["Nadeem", "Saad", ""]]}, {"id": "2103.05715", "submitter": "AmirAbbas Davari", "authors": "Amirabbas Davari, Christoph Baller, Thorsten Seehaus, Matthias Braun,\n  Andreas Maier, Vincent Christlein", "title": "Pixel-wise Distance Regression for Glacier Calving Front Detection and\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Glacier calving front position (CFP) is an important glaciological variable.\nTraditionally, delineating the CFPs has been carried out manually, which was\nsubjective, tedious and expensive. Automating this process is crucial for\ncontinuously monitoring the evolution and status of glaciers. Recently, deep\nlearning approaches have been investigated for this application. However, the\ncurrent methods get challenged by a severe class-imbalance problem. In this\nwork, we propose to mitigate the class-imbalance between the calving front\nclass and the non-calving front class by reformulating the segmentation problem\ninto a pixel-wise regression task. A Convolutional Neural Network gets\noptimized to predict the distance values to the glacier front for each pixel in\nthe image. The resulting distance map localizes the CFP and is further\npost-processed to extract the calving front line. We propose three\npost-processing methods, one method based on statistical thresholding, a second\nmethod based on conditional random fields (CRF), and finally the use of a\nsecond U-Net. The experimental results confirm that our approach significantly\noutperforms the state-of-the-art methods and produces accurate delineation. The\nSecond U-Net obtains the best performance results, resulting in an average\nimprovement of about 21% dice coefficient enhancement.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 20:58:33 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Davari", "Amirabbas", ""], ["Baller", "Christoph", ""], ["Seehaus", "Thorsten", ""], ["Braun", "Matthias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2103.05723", "submitter": "Fabio Valerio Massoli", "authors": "Fabio Valerio Massoli, Donato Cafarelli, Giuseppe Amato, Fabrizio\n  Falchi", "title": "A Multi-resolution Approach to Expression Recognition in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Facial expressions play a fundamental role in human communication. Indeed,\nthey typically reveal the real emotional status of people beyond the spoken\nlanguage. Moreover, the comprehension of human affect based on visual patterns\nis a key ingredient for any human-machine interaction system and, for such\nreasons, the task of Facial Expression Recognition (FER) draws both scientific\nand industrial interest. In the recent years, Deep Learning techniques reached\nvery high performance on FER by exploiting different architectures and learning\nparadigms. In such a context, we propose a multi-resolution approach to solve\nthe FER task. We ground our intuition on the observation that often faces\nimages are acquired at different resolutions. Thus, directly considering such\nproperty while training a model can help achieve higher performance on\nrecognizing facial expressions. To our aim, we use a ResNet-like architecture,\nequipped with Squeeze-and-Excitation blocks, trained on the Affect-in-the-Wild\n2 dataset. Not being available a test set, we conduct tests and models\nselection by employing the validation set only on which we achieve more than\n90\\% accuracy on classifying the seven expressions that the dataset comprises.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 21:21:02 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Massoli", "Fabio Valerio", ""], ["Cafarelli", "Donato", ""], ["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""]]}, {"id": "2103.05730", "submitter": "Kris Campbell", "authors": "Kristen M. Campbell (1), Haocheng Dai (1), Zhe Su (2), Martin Bauer\n  (3), P. Thomas Fletcher (4), Sarang C. Joshi (1 and 5) ((1) Scientific\n  Computing and Imaging Institute, University of Utah, (2) Department of\n  Neurology, University of California Los Angeles, (3) Department of\n  Mathematics, Florida State University, (4) Electrical & Computer Engineering,\n  University of Virginia, (5) Department of Bioengineering, University of Utah)", "title": "Structural Connectome Atlas Construction in the Space of Riemannian\n  Metrics", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structural connectome is often represented by fiber bundles generated\nfrom various types of tractography. We propose a method of analyzing\nconnectomes by representing them as a Riemannian metric, thereby viewing them\nas points in an infinite-dimensional manifold. After equipping this space with\na natural metric structure, the Ebin metric, we apply object-oriented\nstatistical analysis to define an atlas as the Fr\\'echet mean of a population\nof Riemannian metrics. We demonstrate connectome registration and atlas\nformation using connectomes derived from diffusion tensors estimated from a\nsubset of subjects from the Human Connectome Project.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 21:46:02 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Campbell", "Kristen M.", "", "1 and 5"], ["Dai", "Haocheng", "", "1 and 5"], ["Su", "Zhe", "", "1 and 5"], ["Bauer", "Martin", "", "1 and 5"], ["Fletcher", "P. Thomas", "", "1 and 5"], ["Joshi", "Sarang C.", "", "1 and 5"]]}, {"id": "2103.05745", "submitter": "Lin Zhang", "authors": "Devavrat Tomar, Lin Zhang, Tiziano Portenier, Orcun Goksel", "title": "Content-Preserving Unpaired Translation from Simulated to Realistic\n  Ultrasound Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive simulation of ultrasound imaging greatly facilitates sonography\ntraining. Although ray-tracing based methods have shown promising results,\nobtaining realistic images requires substantial modeling effort and manual\nparameter tuning. In addition, current techniques still result in a significant\nappearance gap between simulated images and real clinical scans. In this work\nwe introduce a novel image translation framework to bridge this appearance gap,\nwhile preserving the anatomical layout of the simulated scenes. We achieve this\ngoal by leveraging both simulated images with semantic segmentations and\nunpaired in-vivo ultrasound scans. Our framework is based on recent contrastive\nunpaired translation techniques and we propose a regularization approach by\nlearning an auxiliary segmentation-to-real image translation task, which\nencourages the disentanglement of content and style. In addition, we extend the\ngenerator to be class-conditional, which enables the incorporation of\nadditional losses, in particular a cyclic consistency loss, to further improve\nthe translation quality. Qualitative and quantitative comparisons against\nstate-of-the-art unpaired translation methods demonstrate the superiority of\nour proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:35:43 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Tomar", "Devavrat", ""], ["Zhang", "Lin", ""], ["Portenier", "Tiziano", ""], ["Goksel", "Orcun", ""]]}, {"id": "2103.05804", "submitter": "Calvin Murdock", "authors": "Calvin Murdock and Simon Lucey", "title": "Reframing Neural Networks: Deep Structure in Overcomplete\n  Representations", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.13866", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In comparison to classical shallow representation learning techniques, deep\nneural networks have achieved superior performance in nearly every application\nbenchmark. But despite their clear empirical advantages, it is still not well\nunderstood what makes them so effective. To approach this question, we\nintroduce deep frame approximation, a unifying framework for representation\nlearning with structured overcomplete frames. While exact inference requires\niterative optimization, it may be approximated by the operations of a\nfeed-forward deep neural network. We then indirectly analyze how model capacity\nrelates to the frame structure induced by architectural hyperparameters such as\ndepth, width, and skip connections. We quantify these structural differences\nwith the deep frame potential, a data-independent measure of coherence linked\nto representation uniqueness and stability. As a criterion for model selection,\nwe show correlation with generalization error on a variety of common deep\nnetwork architectures such as ResNets and DenseNets. We also demonstrate how\nrecurrent networks implementing iterative optimization algorithms achieve\nperformance comparable to their feed-forward approximations. This connection to\nthe established theory of overcomplete representations suggests promising new\ndirections for principled deep network architecture design with less reliance\non ad-hoc engineering.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 01:15:14 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Murdock", "Calvin", ""], ["Lucey", "Simon", ""]]}, {"id": "2103.05842", "submitter": "Jisheng Li", "authors": "Jisheng Li, Yuze He, Yubin Hu, Yuxing Han, Jiangtao Wen", "title": "Learning to compose 6-DoF omnidirectional videos using multi-sphere\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Omnidirectional video is an essential component of Virtual Reality. Although\nvarious methods have been proposed to generate content that can be viewed with\nsix degrees of freedom (6-DoF), existing systems usually involve complex depth\nestimation, image in-painting or stitching pre-processing. In this paper, we\npropose a system that uses a 3D ConvNet to generate a multi-sphere images (MSI)\nrepresentation that can be experienced in 6-DoF VR. The system utilizes\nconventional omnidirectional VR camera footage directly without the need for a\ndepth map or segmentation mask, thereby significantly simplifying the overall\ncomplexity of the 6-DoF omnidirectional video composition. By using a newly\ndesigned weighted sphere sweep volume (WSSV) fusing technique, our approach is\ncompatible with most panoramic VR camera setups. A ground truth generation\napproach for high-quality artifact-free 6-DoF contents is proposed and can be\nused by the research and development community for 6-DoF content generation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:09:55 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Jisheng", ""], ["He", "Yuze", ""], ["Hu", "Yubin", ""], ["Han", "Yuxing", ""], ["Wen", "Jiangtao", ""]]}, {"id": "2103.05843", "submitter": "Jisheng Li", "authors": "Jisheng Li, Qi Dai, Jiangtao Wen", "title": "Learning to Estimate Kernel Scale and Orientation of Defocus Blur with\n  Asymmetric Coded Aperture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consistent in-focus input imagery is an essential precondition for machine\nvision systems to perceive the dynamic environment. A defocus blur severely\ndegrades the performance of vision systems. To tackle this problem, we propose\na deep-learning-based framework estimating the kernel scale and orientation of\nthe defocus blur to adjust lens focus rapidly. Our pipeline utilizes 3D ConvNet\nfor a variable number of input hypotheses to select the optimal slice from the\ninput stack. We use random shuffle and Gumbel-softmax to improve network\nperformance. We also propose to generate synthetic defocused images with\nvarious asymmetric coded apertures to facilitate training. Experiments are\nconducted to demonstrate the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:12:15 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Jisheng", ""], ["Dai", "Qi", ""], ["Wen", "Jiangtao", ""]]}, {"id": "2103.05846", "submitter": "Zhenbo Song", "authors": "Peng Wan, Zhenbo Song, Jianfeng Lu", "title": "Incorporating Orientations into End-to-end Driving Model for Steering\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel end-to-end deep neural network model for\nautonomous driving that takes monocular image sequence as input, and directly\ngenerates the steering control angle. Firstly, we model the end-to-end driving\nproblem as a local path planning process. Inspired by the environmental\nrepresentation in the classical planning algorithms(i.e. the beam curvature\nmethod), pixel-wise orientations are fed into the network to learn\ndirection-aware features. Next, to handle the imbalanced distribution of\nsteering values in training datasets, we propose an improvement on a\ncost-sensitive loss function named SteeringLoss2. Besides, we also present a\nnew end-to-end driving dataset, which provides corresponding LiDAR and image\nsequences, as well as standard driving behaviors. Our dataset includes multiple\ndriving scenarios, such as urban, country, and off-road. Numerous experiments\nare conducted on both public available LiVi-Set and our own dataset, and the\nresults show that the model using our proposed methods can predict steering\nangle accurately.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:14:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Wan", "Peng", ""], ["Song", "Zhenbo", ""], ["Lu", "Jianfeng", ""]]}, {"id": "2103.05855", "submitter": "Songxiao Yang", "authors": "Songxiao Yang, Xiabi Liu, Zhongshu Zheng, Wei Wang, Xiaohong Ma", "title": "Fusing Medical Image Features and Clinical Features with Deep Learning\n  for Computer-Aided Diagnosis", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Computer-Aided Diagnosis (CAD) methods mainly depend on medical\nimages. The clinical information, which usually needs to be considered in\npractical clinical diagnosis, has not been fully employed in CAD. In this\npaper, we propose a novel deep learning-based method for fusing Magnetic\nResonance Imaging (MRI)/Computed Tomography (CT) images and clinical\ninformation for diagnostic tasks. Two paths of neural layers are performed to\nextract image features and clinical features, respectively, and at the same\ntime clinical features are employed as the attention to guide the extraction of\nimage features. Finally, these two modalities of features are concatenated to\nmake decisions. We evaluate the proposed method on its applications to\nAlzheimer's disease diagnosis, mild cognitive impairment converter prediction\nand hepatic microvascular invasion diagnosis. The encouraging experimental\nresults prove the values of the image feature extraction guided by clinical\nfeatures and the concatenation of two modalities of features for\nclassification, which improve the performance of diagnosis effectively and\nstably.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:37:21 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Songxiao", ""], ["Liu", "Xiabi", ""], ["Zheng", "Zhongshu", ""], ["Wang", "Wei", ""], ["Ma", "Xiaohong", ""]]}, {"id": "2103.05858", "submitter": "Jisheng Li", "authors": "Jisheng Li, Ziyu Wen, Sihan Li, Yikai Zhao, Bichuan Guo, Jiangtao Wen", "title": "Novel tile segmentation scheme for omnidirectional video", "comments": "Published in 2016 IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532381", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regular omnidirectional video encoding technics use map projection to flatten\na scene from a spherical shape into one or several 2D shapes. Common projection\nmethods including equirectangular and cubic projection have varying levels of\ninterpolation that create a large number of non-information-carrying pixels\nthat lead to wasted bitrate. In this paper, we propose a tile based\nomnidirectional video segmentation scheme which can save up to 28% of pixel\narea and 20% of BD-rate averagely compared to the traditional equirectangular\nprojection based approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:49:18 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Jisheng", ""], ["Wen", "Ziyu", ""], ["Li", "Sihan", ""], ["Zhao", "Yikai", ""], ["Guo", "Bichuan", ""], ["Wen", "Jiangtao", ""]]}, {"id": "2103.05861", "submitter": "Yehui Tang", "authors": "Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao,\n  Chang Xu", "title": "Manifold Regularized Dynamic Network Pruning", "comments": "This paper is accepted by CVPR 2021. Key words: Filter pruning,\n  Dynamic network, Network compression, Manifold regularization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network pruning is an essential approach for reducing the\ncomputational complexity of deep models so that they can be well deployed on\nresource-limited devices. Compared with conventional methods, the recently\ndeveloped dynamic pruning methods determine redundant filters variant to each\ninput instance which achieves higher acceleration. Most of the existing methods\ndiscover effective sub-networks for each instance independently and do not\nutilize the relationship between different inputs. To maximally excavate\nredundancy in the given network architecture, this paper proposes a new\nparadigm that dynamically removes redundant filters by embedding the manifold\ninformation of all instances into the space of pruned networks (dubbed as\nManiDP). We first investigate the recognition complexity and feature similarity\nbetween images in the training set. Then, the manifold relationship between\ninstances and the pruned sub-networks will be aligned in the training\nprocedure. The effectiveness of the proposed method is verified on several\nbenchmarks, which shows better performance in terms of both accuracy and\ncomputational cost compared to the state-of-the-art methods. For example, our\nmethod can reduce 55.3% FLOPs of ResNet-34 with only 0.57% top-1 accuracy\ndegradation on ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:59:03 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Tang", "Yehui", ""], ["Wang", "Yunhe", ""], ["Xu", "Yixing", ""], ["Deng", "Yiping", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""], ["Xu", "Chang", ""]]}, {"id": "2103.05863", "submitter": "Denis Gudovskiy", "authors": "Denis Gudovskiy, Luca Rigazio, Shun Ishizaka, Kazuki Kozuka, Sotaro\n  Tsukizawa", "title": "AutoDO: Robust AutoAugment for Biased Data with Label Noise via Scalable\n  Probabilistic Implicit Differentiation", "comments": "Accepted to CVPR 2021. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AutoAugment has sparked an interest in automated augmentation methods for\ndeep learning models. These methods estimate image transformation policies for\ntrain data that improve generalization to test data. While recent papers\nevolved in the direction of decreasing policy search complexity, we show that\nthose methods are not robust when applied to biased and noisy data. To overcome\nthese limitations, we reformulate AutoAugment as a generalized automated\ndataset optimization (AutoDO) task that minimizes the distribution shift\nbetween test data and distorted train dataset. In our AutoDO model, we\nexplicitly estimate a set of per-point hyperparameters to flexibly change\ndistribution of train data. In particular, we include hyperparameters for\naugmentation, loss weights, and soft-labels that are jointly estimated using\nimplicit differentiation. We develop a theoretical probabilistic interpretation\nof this framework using Fisher information and show that its complexity scales\nlinearly with the dataset size. Our experiments on SVHN, CIFAR-10/100, and\nImageNet classification show up to 9.3% improvement for biased datasets with\nlabel noise compared to prior methods and, importantly, up to 36.6% gain for\nunderrepresented SVHN classes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 04:05:33 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 22:15:41 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Gudovskiy", "Denis", ""], ["Rigazio", "Luca", ""], ["Ishizaka", "Shun", ""], ["Kozuka", "Kazuki", ""], ["Tsukizawa", "Sotaro", ""]]}, {"id": "2103.05886", "submitter": "Hilmil Pradana", "authors": "Hilmil Pradana and Keiichi Horio", "title": "Tuna Nutriment Tracking using Trajectory Mapping in Application to\n  Aquaculture Fish Tank", "comments": null, "journal-ref": "2020 Digital Image Computing: Techniques and Applications (DICTA)\n  (2020) 1-8", "doi": "10.1109/DICTA51227.2020.9363387", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of fish feeding is usually around 40 percent of total production\ncost. Estimating a state of fishes in a tank and adjusting an amount of\nnutriments play an important role to manage cost of fish feeding system. Our\napproach is based on tracking nutriments on videos collected from an active\naquaculture fish farm. Tracking approach is applied to acknowledge movement of\nnutriment to understand more about the fish behavior. Recently, there has been\nincreasing number of researchers focused on developing tracking algorithms to\ngenerate more accurate and faster determination of object. Unfortunately,\nrecent studies have shown that efficient and robust tracking of multiple\nobjects with complex relations remain unsolved. Hence, focusing to develop\ntracking algorithm in aquaculture is more challenging because tracked object\nhas a lot of aquatic variant creatures. By following aforementioned problem, we\ndevelop tuna nutriment tracking based on the classical minimum cost problem\nwhich consistently performs well in real environment datasets. In evaluation,\nthe proposed method achieved 21.32 pixels and 3.08 pixels for average error\ndistance and standard deviation, respectively. Quantitative evaluation based on\nthe data generated by human annotators shows that the proposed method is\nvaluable for aquaculture fish farm and can be widely applied to real\nenvironment datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 06:02:19 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Pradana", "Hilmil", ""], ["Horio", "Keiichi", ""]]}, {"id": "2103.05889", "submitter": "Pranjay Shyam", "authors": "Pranjay Shyam, Sandeep Singh Sengar, Kuk-Jin Yoon, Kyung-Soo Kim", "title": "Evaluating COPY-BLEND Augmentation for Low Level Vision Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Region modification-based data augmentation techniques have shown to improve\nperformance for high level vision tasks (object detection, semantic\nsegmentation, image classification, etc.) by encouraging underlying algorithms\nto focus on multiple discriminative features. However, as these techniques\ndestroy spatial relationship with neighboring regions, performance can be\ndeteriorated when using them to train algorithms designed for low level vision\ntasks (low light image enhancement, image dehazing, deblurring, etc.) where\ntextural consistency between recovered and its neighboring regions is important\nto ensure effective performance. In this paper, we examine the efficacy of a\nsimple copy-blend data augmentation technique that copies patches from noisy\nimages and blends onto a clean image and vice versa to ensure that an\nunderlying algorithm localizes and recovers affected regions resulting in\nincreased perceptual quality of a recovered image. To assess performance\nimprovement, we perform extensive experiments alongside different region\nmodification-based augmentation techniques and report observations such as\nimproved performance, reduced requirement for training dataset, and early\nconvergence across tasks such as low light image enhancement, image dehazing\nand image deblurring without any modification to baseline algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 06:17:52 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Shyam", "Pranjay", ""], ["Sengar", "Sandeep Singh", ""], ["Yoon", "Kuk-Jin", ""], ["Kim", "Kyung-Soo", ""]]}, {"id": "2103.05898", "submitter": "Collin Burns", "authors": "Collin Burns and Jacob Steinhardt", "title": "Limitations of Post-Hoc Feature Alignment for Robustness", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature alignment is an approach to improving robustness to distribution\nshift that matches the distribution of feature activations between the training\ndistribution and test distribution. A particularly simple but effective\napproach to feature alignment involves aligning the batch normalization\nstatistics between the two distributions in a trained neural network. This\ntechnique has received renewed interest lately because of its impressive\nperformance on robustness benchmarks. However, when and why this method works\nis not well understood. We investigate the approach in more detail and identify\nseveral limitations. We show that it only significantly helps with a narrow set\nof distribution shifts and we identify several settings in which it even\ndegrades performance. We also explain why these limitations arise by\npinpointing why this approach can be so effective in the first place. Our\nfindings call into question the utility of this approach and Unsupervised\nDomain Adaptation more broadly for improving robustness in practice.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 06:55:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Burns", "Collin", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2103.05900", "submitter": "Shaowei Wang", "authors": "Shaowei Wang, LingLing Zhang, Xuan Luo, Yi Yang, Xin Hu, and Jun Liu", "title": "RL-CSDia: Representation Learning of Computer Science Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on computer vision mainly focus on natural images that express\nreal-world scenes. They achieve outstanding performance on diverse tasks such\nas visual question answering. Diagram is a special form of visual expression\nthat frequently appears in the education field and is of great significance for\nlearners to understand multimodal knowledge. Current research on diagrams\npreliminarily focuses on natural disciplines such as Biology and Geography,\nwhose expressions are still similar to natural images. Another type of diagrams\nsuch as from Computer Science is composed of graphics containing complex\ntopologies and relations, and research on this type of diagrams is still blank.\nThe main challenges of graphic diagrams understanding are the rarity of data\nand the confusion of semantics, which are mainly reflected in the diversity of\nexpressions. In this paper, we construct a novel dataset of graphic diagrams\nnamed Computer Science Diagrams (CSDia). It contains more than 1,200 diagrams\nand exhaustive annotations of objects and relations. Considering the visual\nnoises caused by the various expressions in diagrams, we introduce the topology\nof diagrams to parse topological structure. After that, we propose Diagram\nParsing Net (DPN) to represent the diagram from three branches: topology,\nvisual feature, and text, and apply the model to the diagram classification\ntask to evaluate the ability of diagrams understanding. The results show the\neffectiveness of the proposed DPN on diagrams understanding.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:01:07 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Wang", "Shaowei", ""], ["Zhang", "LingLing", ""], ["Luo", "Xuan", ""], ["Yang", "Yi", ""], ["Hu", "Xin", ""], ["Liu", "Jun", ""]]}, {"id": "2103.05902", "submitter": "Dongseok Shim", "authors": "Dongseok Shim and H. Jin Kim", "title": "Learning a Domain-Agnostic Visual Representation for Autonomous Driving\n  via Contrastive Loss", "comments": "IEEE IROS 2021 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have been widely studied in autonomous driving\napplications such as semantic segmentation or depth estimation. However,\ntraining a neural network in a supervised manner requires a large amount of\nannotated labels which are expensive and time-consuming to collect. Recent\nstudies leverage synthetic data collected from a virtual environment which are\nmuch easier to acquire and more accurate compared to data from the real world,\nbut they usually suffer from poor generalization due to the inherent domain\nshift problem. In this paper, we propose a Domain-Agnostic Contrastive Learning\n(DACL) which is a two-stage unsupervised domain adaptation framework with\ncyclic adversarial training and contrastive loss. DACL leads the neural network\nto learn domain-agnostic representation to overcome performance degradation\nwhen there exists a difference between training and test data distribution. Our\nproposed approach achieves better performance in the monocular depth estimation\ntask compared to previous state-of-the-art methods and also shows effectiveness\nin the semantic segmentation task.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:06:03 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Shim", "Dongseok", ""], ["Kim", "H. Jin", ""]]}, {"id": "2103.05905", "submitter": "Yibing Song", "authors": "Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, and Wei Liu", "title": "VideoMoCo: Contrastive Video Representation Learning with Temporally\n  Adversarial Examples", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MoCo is effective for unsupervised image representation learning. In this\npaper, we propose VideoMoCo for unsupervised video representation learning.\nGiven a video sequence as an input sample, we improve the temporal feature\nrepresentations of MoCo from two perspectives. First, we introduce a generator\nto drop out several frames from this sample temporally. The discriminator is\nthen learned to encode similar feature representations regardless of frame\nremovals. By adaptively dropping out different frames during training\niterations of adversarial learning, we augment this input sample to train a\ntemporally robust encoder. Second, we use temporal decay to model key\nattenuation in the memory queue when computing the contrastive loss. As the\nmomentum encoder updates after keys enqueue, the representation ability of\nthese keys degrades when we use the current input sample for contrastive\nlearning. This degradation is reflected via temporal decay to attend the input\nsample to recent keys in the queue. As a result, we adapt MoCo to learn video\nrepresentations without empirically designing pretext tasks. By empowering the\ntemporal robustness of the encoder and modeling the temporal decay of the keys,\nour VideoMoCo improves MoCo temporally based on contrastive learning.\nExperiments on benchmark datasets including UCF101 and HMDB51 show that\nVideoMoCo stands as a state-of-the-art video representation learning method.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:22:21 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 02:45:50 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Pan", "Tian", ""], ["Song", "Yibing", ""], ["Yang", "Tianyu", ""], ["Jiang", "Wenhao", ""], ["Liu", "Wei", ""]]}, {"id": "2103.05918", "submitter": "Dong Shen", "authors": "Dong Shen, Shuai Zhao, Jinming Hu, Hao Feng, Deng Cai, Xiaofei He", "title": "ES-Net: Erasing Salient Parts to Learn More in Re-Identification", "comments": "11 pages, 6 figures. Accepted for publication in IEEE Transactions on\n  Image Processing 2021", "journal-ref": "IEEE Transactions on Image Processing, vol. 30, pp. 1676-1686,\n  2021", "doi": "10.1109/TIP.2020.3046904", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an instance-level recognition problem, re-identification (re-ID) requires\nmodels to capture diverse features. However, with continuous training, re-ID\nmodels pay more and more attention to the salient areas. As a result, the model\nmay only focus on few small regions with salient representations and ignore\nother important information. This phenomenon leads to inferior performance,\nespecially when models are evaluated on small inter-identity variation data. In\nthis paper, we propose a novel network, Erasing-Salient Net (ES-Net), to learn\ncomprehensive features by erasing the salient areas in an image. ES-Net\nproposes a novel method to locate the salient areas by the confidence of\nobjects and erases them efficiently in a training batch. Meanwhile, to mitigate\nthe over-erasing problem, this paper uses a trainable pooling layer P-pooling\nthat generalizes global max and global average pooling. Experiments are\nconducted on two specific re-identification tasks (i.e., Person re-ID, Vehicle\nre-ID). Our ES-Net outperforms state-of-the-art methods on three Person re-ID\nbenchmarks and two Vehicle re-ID benchmarks. Specifically, mAP / Rank-1 rate:\n88.6% / 95.7% on Market1501, 78.8% / 89.2% on DuckMTMC-reID, 57.3% / 80.9% on\nMSMT17, 81.9% / 97.0% on Veri-776, respectively. Rank-1 / Rank-5 rate: 83.6% /\n96.9% on VehicleID (Small), 79.9% / 93.5% on VehicleID (Medium), 76.9% / 90.7%\non VehicleID (Large), respectively. Moreover, the visualized salient areas show\nhuman-interpretable visual explanations for the ranking results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:19:46 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Shen", "Dong", ""], ["Zhao", "Shuai", ""], ["Hu", "Jinming", ""], ["Feng", "Hao", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "2103.05920", "submitter": "Shaochi Hu", "authors": "Shaochi Hu, Hanwei Fan, Biao Gao, XijunZhao and Huijing Zhao", "title": "An Image-based Approach of Task-driven Driving Scene Categorization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorizing driving scenes via visual perception is a key technology for\nsafe driving and the downstream tasks of autonomous vehicles.\n  Traditional methods infer scene category by detecting scene-related objects\nor using a classifier that is trained on large datasets of fine-labeled scene\nimages.\n  Whereas at cluttered dynamic scenes such as campus or park, human activities\nare not strongly confined by rules, and the functional attributes of places are\nnot strongly correlated with objects. So how to define, model and infer scene\ncategories is crucial to make the technique really helpful in assisting a robot\nto pass through the scene.\n  This paper proposes a method of task-driven driving scene categorization\nusing weakly supervised data.\n  Given a front-view video of a driving scene, a set of anchor points is marked\nby following the decision making of a human driver, where an anchor point is\nnot a semantic label but an indicator meaning the semantic attribute of the\nscene is different from that of the previous one.\n  A measure is learned to discriminate the scenes of different semantic\nattributes via contrastive learning, and a driving scene profiling and\ncategorization method is developed based on that measure.\n  Experiments are conducted on a front-view video that is recorded when a\nvehicle passed through the cluttered dynamic campus of Peking University. The\nscenes are categorized into straight road, turn road and alerting traffic. The\nresults of semantic scene similarity learning and driving scene categorization\nare extensively studied, and positive result of scene categorization is 97.17\n\\% on the learning video and 85.44\\% on the video of new scenes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:23:36 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Hu", "Shaochi", ""], ["Fan", "Hanwei", ""], ["Gao", "Biao", ""], ["XijunZhao", "", ""], ["Zhao", "Huijing", ""]]}, {"id": "2103.05927", "submitter": "James Lo", "authors": "Shi-Wei Lo, Jyh-Horng Wu, Jo-Yu Chang, Chien-Hao Tseng, Meng-Wei Lin,\n  Fang-Pang Lin", "title": "Deep Sensing of Urban Waterlogging", "comments": "19 pages, 14 figures, under submitting and patenting", "journal-ref": null, "doi": null, "report-no": "revise-2021-05-25", "categories": "cs.CV cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the monsoon season, sudden flood events occur frequently in urban areas,\nwhich hamper the social and economic activities and may threaten the\ninfrastructure and lives. The use of an efficient large-scale waterlogging\nsensing and information system can provide valuable real-time disaster\ninformation to facilitate disaster management and enhance awareness of the\ngeneral public to alleviate losses during and after flood disasters. Therefore,\nin this study, a visual sensing approach driven by deep neural networks and\ninformation and communication technology was developed to provide an end-to-end\nmechanism to realize waterlogging sensing and event-location mapping. The use\nof a deep sensing system in the monsoon season in Taiwan was demonstrated, and\nwaterlogging events were predicted on the island-wide scale. The system could\nsense approximately 2379 vision sources through an internet of video things\nframework and transmit the event-location information in 5 min. The proposed\napproach can sense waterlogging events at a national scale and provide an\nefficient and highly scalable alternative to conventional waterlogging sensing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:34:37 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 03:43:52 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Lo", "Shi-Wei", ""], ["Wu", "Jyh-Horng", ""], ["Chang", "Jo-Yu", ""], ["Tseng", "Chien-Hao", ""], ["Lin", "Meng-Wei", ""], ["Lin", "Fang-Pang", ""]]}, {"id": "2103.05929", "submitter": "JIn Fang", "authors": "Jin Fang, Dingfu Zhou, Xibin Song, Liangjun Zhang", "title": "MapFusion: A General Framework for 3D Object Detection with HDMaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  3D object detection is a key perception component in autonomous driving. Most\nrecent approaches are based on Lidar sensors only or fused with cameras. Maps\n(e.g., High Definition Maps), a basic infrastructure for intelligent vehicles,\nhowever, have not been well exploited for boosting object detection tasks. In\nthis paper, we propose a simple but effective framework - MapFusion to\nintegrate the map information into modern 3D object detector pipelines. In\nparticular, we design a FeatureAgg module for HD Map feature extraction and\nfusion, and a MapSeg module as an auxiliary segmentation head for the detection\nbackbone. Our proposed MapFusion is detector independent and can be easily\nintegrated into different detectors. The experimental results of three\ndifferent baselines on large public autonomous driving dataset demonstrate the\nsuperiority of the proposed framework. By fusing the map information, we can\nachieve 1.27 to 2.79 points improvements for mean Average Precision (mAP) on\nthree strong 3d object detection baselines.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:36:59 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Fang", "Jin", ""], ["Zhou", "Dingfu", ""], ["Song", "Xibin", ""], ["Zhang", "Liangjun", ""]]}, {"id": "2103.05930", "submitter": "Kangfu Mei", "authors": "Qi Song and Kangfu Mei and Rui Huang", "title": "AttaNet: Attention-Augmented Network for Fast and Accurate Scene Parsing", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two factors have proven to be very important to the performance of semantic\nsegmentation models: global context and multi-level semantics. However,\ngenerating features that capture both factors always leads to high\ncomputational complexity, which is problematic in real-time scenarios. In this\npaper, we propose a new model, called Attention-Augmented Network (AttaNet), to\ncapture both global context and multilevel semantics while keeping the\nefficiency high. AttaNet consists of two primary modules: Strip Attention\nModule (SAM) and Attention Fusion Module (AFM). Viewing that in challenging\nimages with low segmentation accuracy, there are a significantly larger amount\nof vertical strip areas than horizontal ones, SAM utilizes a striping operation\nto reduce the complexity of encoding global context in the vertical direction\ndrastically while keeping most of contextual information, compared to the\nnon-local approaches. Moreover, AFM follows a cross-level aggregation strategy\nto limit the computation, and adopts an attention strategy to weight the\nimportance of different levels of features at each pixel when fusing them,\nobtaining an efficient multi-level representation. We have conducted extensive\nexperiments on two semantic segmentation benchmarks, and our network achieves\ndifferent levels of speed/accuracy trade-offs on Cityscapes, e.g., 71 FPS/79.9%\nmIoU, 130 FPS/78.5% mIoU, and 180 FPS/70.1% mIoU, and leading performance on\nADE20K as well.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:38:29 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Song", "Qi", ""], ["Mei", "Kangfu", ""], ["Huang", "Rui", ""]]}, {"id": "2103.05940", "submitter": "Yin Dai", "authors": "Yin Dai and Yifan Gao", "title": "TransMed: Transformers Advance Multi-modal Medical Image Classification", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decade, convolutional neural networks (CNN) have shown very\ncompetitive performance in medical image analysis tasks, such as disease\nclassification, tumor segmentation, and lesion detection. CNN has great\nadvantages in extracting local features of images. However, due to the locality\nof convolution operation, it can not deal with long-range relationships well.\nRecently, transformers have been applied to computer vision and achieved\nremarkable success in large-scale datasets. Compared with natural images,\nmulti-modal medical images have explicit and important long-range dependencies,\nand effective multi-modal fusion strategies can greatly improve the performance\nof deep models. This prompts us to study transformer-based structures and apply\nthem to multi-modal medical images. Existing transformer-based network\narchitectures require large-scale datasets to achieve better performance.\nHowever, medical imaging datasets are relatively small, which makes it\ndifficult to apply pure transformers to medical image analysis. Therefore, we\npropose TransMed for multi-modal medical image classification. TransMed\ncombines the advantages of CNN and transformer to efficiently extract low-level\nfeatures of images and establish long-range dependencies between modalities. We\nevaluated our model for the challenging problem of preoperative diagnosis of\nparotid gland tumors, and the experimental results show the advantages of our\nproposed method. We argue that the combination of CNN and transformer has\ntremendous potential in a large number of medical image analysis tasks. To our\nbest knowledge, this is the first work to apply transformers to medical image\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:57:53 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Dai", "Yin", ""], ["Gao", "Yifan", ""]]}, {"id": "2103.05946", "submitter": "Shuang Xu", "authors": "Shuang Xu and Jiangshe Zhang and Kai Sun and Zixiang Zhao and Lu Huang\n  and Junmin Liu and Chunxia Zhang", "title": "Deep Convolutional Sparse Coding Network for Pansharpening with Guidance\n  of Side Information", "comments": "Accepted by ICME2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pansharpening is a fundamental issue in remote sensing field. This paper\nproposes a side information partially guided convolutional sparse coding (SCSC)\nmodel for pansharpening. The key idea is to split the low resolution\nmultispectral image into a panchromatic image related feature map and a\npanchromatic image irrelated feature map, where the former one is regularized\nby the side information from panchromatic images. With the principle of\nalgorithm unrolling techniques, the proposed model is generalized as a deep\nneural network, called as SCSC pansharpening neural network (SCSC-PNN).\nCompared with 13 classic and state-of-the-art methods on three satellites, the\nnumerical experiments show that SCSC-PNN is superior to others. The codes are\navailable at https://github.com/xsxjtu/SCSC-PNN.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:06:33 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Xu", "Shuang", ""], ["Zhang", "Jiangshe", ""], ["Sun", "Kai", ""], ["Zhao", "Zixiang", ""], ["Huang", "Lu", ""], ["Liu", "Junmin", ""], ["Zhang", "Chunxia", ""]]}, {"id": "2103.05950", "submitter": "Bo Sun", "authors": "Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, Chi Zhang", "title": "FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding", "comments": "CVPR 2021 Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emerging interests have been brought to recognize previously unseen objects\ngiven very few training examples, known as few-shot object detection (FSOD).\nRecent researches demonstrate that good feature embedding is the key to reach\nfavorable few-shot learning performance. We observe object proposals with\ndifferent Intersection-of-Union (IoU) scores are analogous to the intra-image\naugmentation used in contrastive approaches. And we exploit this analogy and\nincorporate supervised contrastive learning to achieve more robust objects\nrepresentations in FSOD. We present Few-Shot object detection via Contrastive\nproposals Encoding (FSCE), a simple yet effective approach to learning\ncontrastive-aware object proposal encodings that facilitate the classification\nof detected objects. We notice the degradation of average precision (AP) for\nrare objects mainly comes from misclassifying novel instances as confusable\nclasses. And we ease the misclassification issues by promoting instance level\nintra-class compactness and inter-class variance via our contrastive proposal\nencoding loss (CPE loss). Our design outperforms current state-of-the-art works\nin any shot and all data splits, with up to +8.8% on standard benchmark PASCAL\nVOC and +2.7% on challenging COCO benchmark. Code is available at: https:\n//github.com/MegviiDetection/FSCE\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:15:05 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 16:18:01 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sun", "Bo", ""], ["Li", "Banghuai", ""], ["Cai", "Shengcai", ""], ["Yuan", "Ye", ""], ["Zhang", "Chi", ""]]}, {"id": "2103.05955", "submitter": "Daqi Liu", "authors": "Daqi Liu, Alvaro Parra and Tat-Jun Chin", "title": "Spatiotemporal Registration for Event-based Visual Odometry", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A useful application of event sensing is visual odometry, especially in\nsettings that require high-temporal resolution. The state-of-the-art method of\ncontrast maximisation recovers the motion from a batch of events by maximising\nthe contrast of the image of warped events. However, the cost scales with image\nresolution and the temporal resolution can be limited by the need for large\nbatch sizes to yield sufficient structure in the contrast image. In this work,\nwe propose spatiotemporal registration as a compelling technique for\nevent-based rotational motion estimation. We theoretcally justify the approach\nand establish its fundamental and practical advantages over contrast\nmaximisation. In particular, spatiotemporal registration also produces feature\ntracks as a by-product, which directly supports an efficient visual odometry\npipeline with graph-based optimisation for motion averaging. The simplicity of\nour visual odometry pipeline allows it to process more than 1 M events/second.\nWe also contribute a new event dataset for visual odometry, where motion\nsequences with large velocity variations were acquired using a high-precision\nrobot arm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:23:24 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 00:50:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Daqi", ""], ["Parra", "Alvaro", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "2103.05959", "submitter": "Cheng Cui", "authors": "Cheng Cui and Ruoyu Guo and Yuning Du and Dongliang He and Fu Li and\n  Zewu Wu and Qiwen Liu and Shilei Wen and Jizhou Huang and Xiaoguang Hu and\n  Dianhai Yu and Errui Ding and Yanjun Ma", "title": "Beyond Self-Supervision: A Simple Yet Effective Network Distillation\n  Alternative to Improve Backbones", "comments": "10 pages, 3 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, research efforts have been concentrated on revealing how\npre-trained model makes a difference in neural network performance.\nSelf-supervision and semi-supervised learning technologies have been\nextensively explored by the community and are proven to be of great potential\nin obtaining a powerful pre-trained model. However, these models require huge\ntraining costs (i.e., hundreds of millions of images or training iterations).\nIn this paper, we propose to improve existing baseline networks via knowledge\ndistillation from off-the-shelf pre-trained big powerful models. Different from\nexisting knowledge distillation frameworks which require student model to be\nconsistent with both soft-label generated by teacher model and hard-label\nannotated by humans, our solution performs distillation by only driving\nprediction of the student model consistent with that of the teacher model.\nTherefore, our distillation setting can get rid of manually labeled data and\ncan be trained with extra unlabeled data to fully exploit capability of teacher\nmodel for better learning. We empirically find that such simple distillation\nsettings perform extremely effective, for example, the top-1 accuracy on\nImageNet-1k validation set of MobileNetV3-large and ResNet50-D can be\nsignificantly improved from 75.2% to 79% and 79.1% to 83%, respectively. We\nhave also thoroughly analyzed what are dominant factors that affect the\ndistillation performance and how they make a difference. Extensive downstream\ncomputer vision tasks, including transfer learning, object detection and\nsemantic segmentation, can significantly benefit from the distilled pretrained\nmodels. All our experiments are implemented based on PaddlePaddle, codes and a\nseries of improved pretrained models with ssld suffix are available in\nPaddleClas.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:32:44 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Cui", "Cheng", ""], ["Guo", "Ruoyu", ""], ["Du", "Yuning", ""], ["He", "Dongliang", ""], ["Li", "Fu", ""], ["Wu", "Zewu", ""], ["Liu", "Qiwen", ""], ["Wen", "Shilei", ""], ["Huang", "Jizhou", ""], ["Hu", "Xiaoguang", ""], ["Yu", "Dianhai", ""], ["Ding", "Errui", ""], ["Ma", "Yanjun", ""]]}, {"id": "2103.05961", "submitter": "Jian Zhang", "authors": "Chong Mou, Jian Zhang, Xiaopeng Fan, Hangfan Liu, Ronggang Wang", "title": "COLA-Net: Collaborative Attention Network for Image Restoration", "comments": "11 pages, 6 tables, 9 figures, to be published in IEEE Transactions\n  on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2021.3063916", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Local and non-local attention-based methods have been well studied in various\nimage restoration tasks while leading to promising performance. However, most\nof the existing methods solely focus on one type of attention mechanism (local\nor non-local). Furthermore, by exploiting the self-similarity of natural\nimages, existing pixel-wise non-local attention operations tend to give rise to\ndeviations in the process of characterizing long-range dependence due to image\ndegeneration. To overcome these problems, in this paper we propose a novel\ncollaborative attention network (COLA-Net) for image restoration, as the first\nattempt to combine local and non-local attention mechanisms to restore image\ncontent in the areas with complex textures and with highly repetitive details\nrespectively. In addition, an effective and robust patch-wise non-local\nattention model is developed to capture long-range feature correspondences\nthrough 3D patches. Extensive experiments on synthetic image denoising, real\nimage denoising and compression artifact reduction tasks demonstrate that our\nproposed COLA-Net is able to achieve state-of-the-art performance in both peak\nsignal-to-noise ratio and visual perception, while maintaining an attractive\ncomputational complexity. The source code is available on\nhttps://github.com/MC-E/COLA-Net.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:33:17 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Mou", "Chong", ""], ["Zhang", "Jian", ""], ["Fan", "Xiaopeng", ""], ["Liu", "Hangfan", ""], ["Wang", "Ronggang", ""]]}, {"id": "2103.05964", "submitter": "Davide Poggiali", "authors": "Davide Poggiali, Diego Cecchin, Cristina Campi, Stefano De Marchi", "title": "Oversampling errors in multimodal medical imaging are due to the Gibbs\n  effect", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.30924.13446", "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To analyse multimodal 3-dimensional medical images, interpolation is required\nfor resampling which - unavoidably - introduces an interpolation error. In this\nwork we consider three segmented 3-dimensional images resampled with three\ndifferent neuroimaging software tools for comparing undersampling and\noversampling strategies and to identify where the oversampling error lies. The\nresults indicate that undersampling to the lowest image size is advantageous in\nterms of mean value per segment errors and that the oversampling error is\nlarger where the gradient is steeper, showing a Gibbs effect.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:42:13 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 12:04:02 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Poggiali", "Davide", ""], ["Cecchin", "Diego", ""], ["Campi", "Cristina", ""], ["De Marchi", "Stefano", ""]]}, {"id": "2103.05977", "submitter": "Yuan-Gen Wang", "authors": "Fu-Zhao Ou, Xingyu Chen, Ruixin Zhang, Yuge Huang, Shaoxin Li, Jilin\n  Li, Yong Li, Liujuan Cao, and Yuan-Gen Wang", "title": "SDD-FIQA: Unsupervised Face Image Quality Assessment with Similarity\n  Distribution Distance", "comments": null, "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, Face Image Quality Assessment (FIQA) has become an\nindispensable part of the face recognition system to guarantee the stability\nand reliability of recognition performance in an unconstrained scenario. For\nthis purpose, the FIQA method should consider both the intrinsic property and\nthe recognizability of the face image. Most previous works aim to estimate the\nsample-wise embedding uncertainty or pair-wise similarity as the quality score,\nwhich only considers the information from partial intra-class. However, these\nmethods ignore the valuable information from the inter-class, which is for\nestimating to the recognizability of face image. In this work, we argue that a\nhigh-quality face image should be similar to its intra-class samples and\ndissimilar to its inter-class samples. Thus, we propose a novel unsupervised\nFIQA method that incorporates Similarity Distribution Distance for Face Image\nQuality Assessment (SDD-FIQA). Our method generates quality pseudo-labels by\ncalculating the Wasserstein Distance (WD) between the intra-class similarity\ndistributions and inter-class similarity distributions. With these quality\npseudo-labels, we are capable of training a regression network for quality\nprediction. Extensive experiments on benchmark datasets demonstrate that the\nproposed SDD-FIQA surpasses the state-of-the-arts by an impressive margin.\nMeanwhile, our method shows good generalization across different recognition\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 10:23:28 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ou", "Fu-Zhao", ""], ["Chen", "Xingyu", ""], ["Zhang", "Ruixin", ""], ["Huang", "Yuge", ""], ["Li", "Shaoxin", ""], ["Li", "Jilin", ""], ["Li", "Yong", ""], ["Cao", "Liujuan", ""], ["Wang", "Yuan-Gen", ""]]}, {"id": "2103.05983", "submitter": "Yue Liao", "authors": "Mingfei Chen, Yue Liao, Si Liu, Zhiyuan Chen, Fei Wang, Chen Qian", "title": "Reformulating HOI Detection as Adaptive Set Prediction", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining which image regions to concentrate on is critical for\nHuman-Object Interaction (HOI) detection. Conventional HOI detectors focus on\neither detected human and object pairs or pre-defined interaction locations,\nwhich limits learning of the effective features. In this paper, we reformulate\nHOI detection as an adaptive set prediction problem, with this novel\nformulation, we propose an Adaptive Set-based one-stage framework (AS-Net) with\nparallel instances and interaction branches. To attain this, we map a trainable\ninteraction query set to an interaction prediction set with a transformer. Each\nquery adaptively aggregates the interaction-relevant features from global\ncontexts through multi-head co-attention. Besides, the training process is\nsupervised adaptively by matching each ground truth with the interaction\nprediction. Furthermore, we design an effective instance-aware attention module\nto introduce instructive features from the instance branch into the interaction\nbranch. Our method outperforms previous state-of-the-art methods without any\nextra human pose and language features on three challenging HOI detection\ndatasets. Especially, we achieve over $31\\%$ relative improvement on a\nlarge-scale HICO-DET dataset. Code is available at\nhttps://github.com/yoyomimi/AS-Net.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 10:40:33 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 02:31:55 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Chen", "Mingfei", ""], ["Liao", "Yue", ""], ["Liu", "Si", ""], ["Chen", "Zhiyuan", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""]]}, {"id": "2103.05985", "submitter": "Renshuai Tao", "authors": "Hainan Li, Renshuai Tao, Jun Li, Haotong Qin, Yifu Ding, Shuo Wang and\n  Xianglong Liu", "title": "Multi-Pretext Attention Network for Few-shot Learning with\n  Self-supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot learning is an interesting and challenging study, which enables\nmachines to learn from few samples like humans. Existing studies rarely exploit\nauxiliary information from large amount of unlabeled data. Self-supervised\nlearning is emerged as an efficient method to utilize unlabeled data. Existing\nself-supervised learning methods always rely on the combination of geometric\ntransformations for the single sample by augmentation, while seriously neglect\nthe endogenous correlation information among different samples that is the same\nimportant for the task. In this work, we propose a Graph-driven Clustering\n(GC), a novel augmentation-free method for self-supervised learning, which does\nnot rely on any auxiliary sample and utilizes the endogenous correlation\ninformation among input samples. Besides, we propose Multi-pretext Attention\nNetwork (MAN), which exploits a specific attention mechanism to combine the\ntraditional augmentation-relied methods and our GC, adaptively learning their\noptimized weights to improve the performance and enabling the feature extractor\nto obtain more universal representations. We evaluate our MAN extensively on\nminiImageNet and tieredImageNet datasets and the results demonstrate that the\nproposed method outperforms the state-of-the-art (SOTA) relevant methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 10:48:37 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Hainan", ""], ["Tao", "Renshuai", ""], ["Li", "Jun", ""], ["Qin", "Haotong", ""], ["Ding", "Yifu", ""], ["Wang", "Shuo", ""], ["Liu", "Xianglong", ""]]}, {"id": "2103.05993", "submitter": "Shi Luo", "authors": "Shi Luo, Xiongfei Li, Xiaoli Zhang", "title": "Wide Aspect Ratio Matching for Robust Face Detection", "comments": "18 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, anchor-based methods have achieved great progress in face\ndetection. Once anchor design and anchor matching strategy determined, plenty\nof positive anchors will be sampled. However, faces with extreme aspect ratio\nalways fail to be sampled according to standard anchor matching strategy. In\nfact, the max IoUs between anchors and extreme aspect ratio faces are still\nlower than fixed sampling threshold. In this paper, we firstly explore the\nfactors that affect the max IoU of each face in theory. Then, anchor matching\nsimulation is performed to evaluate the sampling range of face aspect ratio.\nBesides, we propose a Wide Aspect Ratio Matching (WARM) strategy to collect\nmore representative positive anchors from ground-truth faces across a wide\nrange of aspect ratio. Finally, we present a novel feature enhancement module,\nnamed Receptive Field Diversity (RFD) module, to provide diverse receptive\nfield corresponding to different aspect ratios. Extensive experiments show that\nour method can help detectors better capture extreme aspect ratio faces and\nachieve promising detection performance on challenging face detection\nbenchmarks, including WIDER FACE and FDDB datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 11:05:38 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Luo", "Shi", ""], ["Li", "Xiongfei", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "2103.05997", "submitter": "Lu Yang", "authors": "Lu Yang and Qing Song and Zhihui Wang and Zhiwei Liu and Songcen Xu\n  and Zhihao Li", "title": "Quality-Aware Network for Human Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How to estimate the quality of the network output is an important issue, and\ncurrently there is no effective solution in the field of human parsing. In\norder to solve this problem, this work proposes a statistical method based on\nthe output probability map to calculate the pixel quality information, which is\ncalled pixel score. In addition, the Quality-Aware Module (QAM) is proposed to\nfuse the different quality information, the purpose of which is to estimate the\nquality of human parsing results. We combine QAM with a concise and effective\nnetwork design to propose Quality-Aware Network (QANet) for human parsing.\nBenefiting from the superiority of QAM and QANet, we achieve the best\nperformance on three multiple and one single human parsing benchmarks,\nincluding CIHP, MHP-v2, Pascal-Person-Part and LIP. Without increasing the\ntraining and inference time, QAM improves the AP$^\\text{r}$ criterion by more\nthan 10 points in the multiple human parsing task. QAM can be extended to other\ntasks with good quality estimation, e.g. instance segmentation. Specifically,\nQAM improves Mask R-CNN by ~1% mAP on COCO and LVISv1.0 datasets. Based on the\nproposed QAM and QANet, our overall system wins 1st place in CVPR2019 COCO\nDensePose Challenge, and 1st place in Track 1 & 2 of CVPR2020 LIP Challenge.\nCode and models are available at https://github.com/soeaver/QANet.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 11:17:40 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Lu", ""], ["Song", "Qing", ""], ["Wang", "Zhihui", ""], ["Liu", "Zhiwei", ""], ["Xu", "Songcen", ""], ["Li", "Zhihao", ""]]}, {"id": "2103.06011", "submitter": "Mathias Gehrig", "authors": "Mathias Gehrig, Willem Aarents, Daniel Gehrig, Davide Scaramuzza", "title": "DSEC: A Stereo Event Camera Dataset for Driving Scenarios", "comments": "IEEE Robotics and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Once an academic venture, autonomous driving has received unparalleled\ncorporate funding in the last decade. Still, the operating conditions of\ncurrent autonomous cars are mostly restricted to ideal scenarios. This means\nthat driving in challenging illumination conditions such as night, sunrise, and\nsunset remains an open problem. In these cases, standard cameras are being\npushed to their limits in terms of low light and high dynamic range\nperformance. To address these challenges, we propose, DSEC, a new dataset that\ncontains such demanding illumination conditions and provides a rich set of\nsensory data. DSEC offers data from a wide-baseline stereo setup of two color\nframe cameras and two high-resolution monochrome event cameras. In addition, we\ncollect lidar data and RTK GPS measurements, both hardware synchronized with\nall camera data. One of the distinctive features of this dataset is the\ninclusion of high-resolution event cameras. Event cameras have received\nincreasing attention for their high temporal resolution and high dynamic range\nperformance. However, due to their novelty, event camera datasets in driving\nscenarios are rare. This work presents the first high-resolution, large-scale\nstereo dataset with event cameras. The dataset contains 53 sequences collected\nby driving in a variety of illumination conditions and provides ground truth\ndisparity for the development and evaluation of event-based stereo algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 12:10:33 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Gehrig", "Mathias", ""], ["Aarents", "Willem", ""], ["Gehrig", "Daniel", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2103.06022", "submitter": "Stefan Schrunner", "authors": "Delmon Arous, Stefan Schrunner, Ingunn Hanson, Nina F.J. Edin, Eirik\n  Malinen", "title": "Principal component-based image segmentation: a new approach to outline\n  in vitro cell colonies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The in vitro clonogenic assay is a technique to study the ability of a cell\nto form a colony in a culture dish. By optical imaging, dishes with stained\ncolonies can be scanned and assessed digitally. Identification, segmentation\nand counting of stained colonies play a vital part in high-throughput screening\nand quantitative assessment of biological assays. Image processing of such\npictured/scanned assays can be affected by image/scan acquisition artifacts\nlike background noise and spatially varying illumination, and contaminants in\nthe suspension medium. Although existing approaches tackle these issues, the\nsegmentation quality requires further improvement, particularly on noisy and\nlow contrast images. In this work, we present an objective and versatile\nmachine learning procedure to amend these issues by characterizing, extracting\nand segmenting inquired colonies using principal component analysis, k-means\nclustering and a modified watershed segmentation algorithm. The intention is to\nautomatically identify visible colonies through spatial texture assessment and\naccordingly discriminate them from background in preparation for successive\nsegmentation. The proposed segmentation algorithm yielded a similar quality as\nmanual counting by human observers. High F1 scores (>0.9) and low\nroot-mean-square errors (around 14%) underlined good agreement with ground\ntruth data. Moreover, it outperformed a recent state-of-the-art method. The\nmethodology will be an important tool in future cancer research applications.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 12:37:51 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Arous", "Delmon", ""], ["Schrunner", "Stefan", ""], ["Hanson", "Ingunn", ""], ["Edin", "Nina F. J.", ""], ["Malinen", "Eirik", ""]]}, {"id": "2103.06028", "submitter": "Naiyan Wang", "authors": "Ziqi Pang, Zhichao Li, Naiyan Wang", "title": "Model-free Vehicle Tracking and State Estimation in Point Cloud\n  Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the states of surrounding traffic participants stays at the core\nof autonomous driving. In this paper, we study a novel setting of this problem:\nmodel-free single object tracking (SOT), which takes the object state in the\nfirst frame as input, and jointly solves state estimation and tracking in\nsubsequent frames. The main purpose for this new setting is to break the strong\nlimitation of the popular \"detection and tracking\" scheme in multi-object\ntracking. Moreover, we notice that shape completion by overlaying the point\nclouds, which is a by-product of our proposed task, not only improves the\nperformance of state estimation but also has numerous applications. As no\nbenchmark for this task is available so far, we construct a new dataset\nLiDAR-SOT and corresponding evaluation protocols based on the Waymo Open\ndataset. We then propose an optimization-based algorithm called SOTracker based\non point cloud registration, vehicle shapes, and motion priors. Our\nquantitative and qualitative results prove the effectiveness of our SOTracker\nand reveal the challenging cases for SOT in point clouds, including the\nsparsity of LiDAR data, abrupt motion variation, etc. Finally, we also explore\nhow the proposed task and algorithm may benefit other autonomous driving\napplications, including simulating LiDAR scans, generating motion data, and\nannotating optical flow. The code and protocols for our benchmark and algorithm\nare available at https://github.com/TuSimple/LiDAR_SOT/ . A video demonstration\nis at https://www.youtube.com/watch?v=BpHixKs91i8 .\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:01:26 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Pang", "Ziqi", ""], ["Li", "Zhichao", ""], ["Wang", "Naiyan", ""]]}, {"id": "2103.06030", "submitter": "Quande Liu", "authors": "Quande Liu, Cheng Chen, Jing Qin, Qi Dou, Pheng-Ann Heng", "title": "FedDG: Federated Domain Generalization on Medical Image Segmentation via\n  Episodic Learning in Continuous Frequency Space", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated learning allows distributed medical institutions to collaboratively\nlearn a shared prediction model with privacy protection. While at clinical\ndeployment, the models trained in federated learning can still suffer from\nperformance drop when applied to completely unseen hospitals outside the\nfederation. In this paper, we point out and solve a novel problem setting of\nfederated domain generalization (FedDG), which aims to learn a federated model\nfrom multiple distributed source domains such that it can directly generalize\nto unseen target domains. We present a novel approach, named as Episodic\nLearning in Continuous Frequency Space (ELCFS), for this problem by enabling\neach client to exploit multi-source data distributions under the challenging\nconstraint of data decentralization. Our approach transmits the distribution\ninformation across clients in a privacy-protecting way through an effective\ncontinuous frequency space interpolation mechanism. With the transferred\nmulti-source distributions, we further carefully design a boundary-oriented\nepisodic learning paradigm to expose the local learning to domain distribution\nshifts and particularly meet the challenges of model generalization in medical\nimage segmentation scenario. The effectiveness of our method is demonstrated\nwith superior performance over state-of-the-arts and in-depth ablation\nexperiments on two medical image segmentation tasks. The code is available at\n\"https://github.com/liuquande/FedDG-ELCFS\".\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:05:23 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Liu", "Quande", ""], ["Chen", "Cheng", ""], ["Qin", "Jing", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2103.06031", "submitter": "Qinghong Lin", "authors": "Qinghong Lin, Weichan Zhong, Jianglin Lu", "title": "Deep Superpixel Cut for Unsupervised Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation, one of the most critical vision tasks, has been studied\nfor many years. Most of the early algorithms are unsupervised methods, which\nuse hand-crafted features to divide the image into many regions. Recently,\nowing to the great success of deep learning technology, CNNs based methods show\nsuperior performance in image segmentation. However, these methods rely on a\nlarge number of human annotations, which are expensive to collect. In this\npaper, we propose a deep unsupervised method for image segmentation, which\ncontains the following two stages. First, a Superpixelwise Autoencoder\n(SuperAE) is designed to learn the deep embedding and reconstruct a smoothed\nimage, then the smoothed image is passed to generate superpixels. Second, we\npresent a novel clustering algorithm called Deep Superpixel Cut (DSC), which\nmeasures the deep similarity between superpixels and formulates image\nsegmentation as a soft partitioning problem. Via backpropagation, DSC\nadaptively partitions the superpixels into perceptual regions. Experimental\nresults on the BSDS500 dataset demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:07:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Lin", "Qinghong", ""], ["Zhong", "Weichan", ""], ["Lu", "Jianglin", ""]]}, {"id": "2103.06032", "submitter": "Chunbin Gu", "authors": "Chunbin Gu, Jiajun Bu, Xixi Zhou, Chengwei Yao, Dongfang Ma, Zhi Yu,\n  Xifeng Yan", "title": "Cross-modal Image Retrieval with Deep Mutual Information Maximization", "comments": "35 pages,7 figures, Submitted to Neuralcomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the cross-modal image retrieval, where the inputs\ncontain a source image plus some text that describes certain modifications to\nthis image and the desired image. Prior work usually uses a three-stage\nstrategy to tackle this task: 1) extract the features of the inputs; 2) fuse\nthe feature of the source image and its modified text to obtain fusion feature;\n3) learn a similarity metric between the desired image and the source image +\nmodified text by using deep metric learning. Since classical image/text\nencoders can learn the useful representation and common pair-based loss\nfunctions of distance metric learning are enough for cross-modal retrieval,\npeople usually improve retrieval accuracy by designing new fusion networks.\nHowever, these methods do not successfully handle the modality gap caused by\nthe inconsistent distribution and representation of the features of different\nmodalities, which greatly influences the feature fusion and similarity\nlearning. To alleviate this problem, we adopt the contrastive self-supervised\nlearning method Deep InforMax (DIM) to our approach to bridge this gap by\nenhancing the dependence between the text, the image, and their fusion.\nSpecifically, our method narrows the modality gap between the text modality and\nthe image modality by maximizing mutual information between their not exactly\nsemantically identical representation. Moreover, we seek an effective common\nsubspace for the semantically same fusion feature and desired image's feature\nby utilizing Deep InforMax between the low-level layer of the image encoder and\nthe high-level layer of the fusion network. Extensive experiments on three\nlarge-scale benchmark datasets show that we have bridged the modality gap\nbetween different modalities and achieve state-of-the-art retrieval\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:08:09 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Gu", "Chunbin", ""], ["Bu", "Jiajun", ""], ["Zhou", "Xixi", ""], ["Yao", "Chengwei", ""], ["Ma", "Dongfang", ""], ["Yu", "Zhi", ""], ["Yan", "Xifeng", ""]]}, {"id": "2103.06104", "submitter": "Olivier Petit", "authors": "Olivier Petit, Nicolas Thome, Cl\\'ement Rambour, Luc Soler", "title": "U-Net Transformer: Self and Cross Attention for Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation remains particularly challenging for complex and\nlow-contrast anatomical structures. In this paper, we introduce the\nU-Transformer network, which combines a U-shaped architecture for image\nsegmentation with self- and cross-attention from Transformers. U-Transformer\novercomes the inability of U-Nets to model long-range contextual interactions\nand spatial dependencies, which are arguably crucial for accurate segmentation\nin challenging contexts. To this end, attention mechanisms are incorporated at\ntwo main levels: a self-attention module leverages global interactions between\nencoder features, while cross-attention in the skip connections allows a fine\nspatial recovery in the U-Net decoder by filtering out non-semantic features.\nExperiments on two abdominal CT-image datasets show the large performance gain\nbrought out by U-Transformer compared to U-Net and local Attention U-Nets. We\nalso highlight the importance of using both self- and cross-attention, and the\nnice interpretability features brought out by U-Transformer.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:58:31 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 15:25:47 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Petit", "Olivier", ""], ["Thome", "Nicolas", ""], ["Rambour", "Cl\u00e9ment", ""], ["Soler", "Luc", ""]]}, {"id": "2103.06108", "submitter": "Raymond Baldwin", "authors": "R. Wes Baldwin, Ruixu Liu, Mohammed Almatrafi, Vijayan Asari, Keigo\n  Hirakawa", "title": "Time-Ordered Recent Event (TORE) Volumes for Event Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event cameras are an exciting, new sensor modality enabling high-speed\nimaging with extremely low-latency and wide dynamic range. Unfortunately, most\nmachine learning architectures are not designed to directly handle sparse data,\nlike that generated from event cameras. Many state-of-the-art algorithms for\nevent cameras rely on interpolated event representations - obscuring crucial\ntiming information, increasing the data volume, and limiting overall network\nperformance. This paper details an event representation called Time-Ordered\nRecent Event (TORE) volumes. TORE volumes are designed to compactly store raw\nspike timing information with minimal information loss. This bio-inspired\ndesign is memory efficient, computationally fast, avoids time-blocking (i.e.\nfixed and predefined frame rates), and contains \"local memory\" from past data.\nThe design is evaluated on a wide range of challenging tasks (e.g. event\ndenoising, image reconstruction, classification, and human pose estimation) and\nis shown to dramatically improve state-of-the-art performance. TORE volumes are\nan easy-to-implement replacement for any algorithm currently utilizing event\nrepresentations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:03:38 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Baldwin", "R. Wes", ""], ["Liu", "Ruixu", ""], ["Almatrafi", "Mohammed", ""], ["Asari", "Vijayan", ""], ["Hirakawa", "Keigo", ""]]}, {"id": "2103.06115", "submitter": "Veronica Sanz", "authors": "Gabriela Barenboim, Johannes Hirn and Veronica Sanz", "title": "Symmetry meets AI", "comments": "8 pages, 8 figures", "journal-ref": "SciPost Phys. 11, 014 (2021)", "doi": "10.21468/SciPostPhys.11.1.014", "report-no": null, "categories": "cs.LG cs.CV hep-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore whether Neural Networks (NNs) can {\\it discover} the presence of\nsymmetries as they learn to perform a task. For this, we train hundreds of NNs\non a {\\it decoy task} based on well-controlled Physics templates, where no\ninformation on symmetry is provided. We use the output from the last hidden\nlayer of all these NNs, projected to fewer dimensions, as the input for a\nsymmetry classification task, and show that information on symmetry had indeed\nbeen identified by the original NN without guidance. As an interdisciplinary\napplication of this procedure, we identify the presence and level of symmetry\nin artistic paintings from different styles such as those of Picasso, Pollock\nand Van Gogh.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:12:49 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 11:47:38 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Barenboim", "Gabriela", ""], ["Hirn", "Johannes", ""], ["Sanz", "Veronica", ""]]}, {"id": "2103.06116", "submitter": "Li Yang", "authors": "Li Yang, Mai Xu, Deng Xin and Bo Feng", "title": "Spatial Attention-based Non-reference Perceptual Quality Prediction\n  Network for Omnidirectional Images", "comments": "Accepted by IEEE ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the strong correlation between visual attention and perceptual\nquality, many methods attempt to use human saliency information for image\nquality assessment. Although this mechanism can get good performance, the\nnetworks require human saliency labels, which is not easily accessible for\nomnidirectional images (ODI). To alleviate this issue, we propose a spatial\nattention-based perceptual quality prediction network for non-reference quality\nassessment on ODIs (SAP-net). To drive our SAP-net, we establish a large-scale\nIQA dataset of ODIs (IQA-ODI), which is composed of subjective scores of 200\nsubjects on 1,080 ODIs. In IQA-ODI, there are 120 high quality ODIs as\nreference, and 960 ODIs with impairments in both JPEG compression and map\nprojection. Without any human saliency labels, our network can adaptively\nestimate human perceptual quality on impaired ODIs through a self-attention\nmanner, which significantly promotes the prediction performance of quality\nscores. Moreover, our method greatly reduces the computational complexity in\nquality assessment task on ODIs. Extensive experiments validate that our\nnetwork outperforms 9 state-of-the-art methods for quality assessment on ODIs.\nThe dataset and code have been available on \\url{\nhttps://github.com/yanglixiaoshen/SAP-Net}.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:14:37 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Li", ""], ["Xu", "Mai", ""], ["Xin", "Deng", ""], ["Feng", "Bo", ""]]}, {"id": "2103.06122", "submitter": "Byungseok Roh", "authors": "Byungseok Roh, Wuhyun Shin, Ildoo Kim, Sungwoong Kim", "title": "Spatially Consistent Representation Learning", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Self-supervised learning has been widely used to obtain transferrable\nrepresentations from unlabeled images. Especially, recent contrastive learning\nmethods have shown impressive performances on downstream image classification\ntasks. While these contrastive methods mainly focus on generating invariant\nglobal representations at the image-level under semantic-preserving\ntransformations, they are prone to overlook spatial consistency of local\nrepresentations and therefore have a limitation in pretraining for localization\ntasks such as object detection and instance segmentation. Moreover,\naggressively cropped views used in existing contrastive methods can minimize\nrepresentation distances between the semantically different regions of a single\nimage.\n  In this paper, we propose a spatially consistent representation learning\nalgorithm (SCRL) for multi-object and location-specific tasks. In particular,\nwe devise a novel self-supervised objective that tries to produce coherent\nspatial representations of a randomly cropped local region according to\ngeometric translations and zooming operations. On various downstream\nlocalization tasks with benchmark datasets, the proposed SCRL shows significant\nperformance improvements over the image-level supervised pretraining as well as\nthe state-of-the-art self-supervised learning methods.\n  Code is available at https://github.com/kakaobrain/scrl\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:23:45 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 09:50:07 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Roh", "Byungseok", ""], ["Shin", "Wuhyun", ""], ["Kim", "Ildoo", ""], ["Kim", "Sungwoong", ""]]}, {"id": "2103.06132", "submitter": "Alexandre Rame", "authors": "Alexandre Rame, Remy Sun, Matthieu Cord", "title": "MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks", "comments": "8 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent strategies achieved ensembling \"for free\" by fitting concurrently\ndiverse subnetworks inside a single base network. The main idea during training\nis that each subnetwork learns to classify only one of the multiple inputs\nsimultaneously provided. However, the question of how to best mix these\nmultiple inputs has not been studied so far. In this paper, we introduce MixMo,\na new generalized framework for learning multi-input multi-output deep\nsubnetworks. Our key motivation is to replace the suboptimal summing operation\nhidden in previous approaches by a more appropriate mixing mechanism. For that\npurpose, we draw inspiration from successful mixed sample data augmentations.\nWe show that binary mixing in features - particularly with rectangular patches\nfrom CutMix - enhances results by making subnetworks stronger and more diverse.\nWe improve state of the art for image classification on CIFAR-100 and Tiny\nImageNet datasets. Our easy to implement models notably outperform data\naugmented deep ensembles, without the inference and memory overheads. As we\noperate in features and simply better leverage the expressiveness of large\nnetworks, we open a new line of research complementary to previous works.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:31:02 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 11:49:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Rame", "Alexandre", ""], ["Sun", "Remy", ""], ["Cord", "Matthieu", ""]]}, {"id": "2103.06134", "submitter": "Jean-Baptiste Weibel", "authors": "Jean-Baptiste Weibel, Timothy Patten, Markus Vincze", "title": "Sim2Real 3D Object Classification using Spherical Kernel Point\n  Convolution and a Deep Center Voting Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While object semantic understanding is essential for most service robotic\ntasks, 3D object classification is still an open problem. Learning from\nartificial 3D models alleviates the cost of annotation necessary to approach\nthis problem, but most methods still struggle with the differences existing\nbetween artificial and real 3D data. We conjecture that the cause of those\nissue is the fact that many methods learn directly from point coordinates,\ninstead of the shape, as the former is hard to center and to scale under\nvariable occlusions reliably. We introduce spherical kernel point convolutions\nthat directly exploit the object surface, represented as a graph, and a voting\nscheme to limit the impact of poor segmentation on the classification results.\nOur proposed approach improves upon state-of-the-art methods by up to 36% when\ntransferring from artificial objects to real objects.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:32:04 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Weibel", "Jean-Baptiste", ""], ["Patten", "Timothy", ""], ["Vincze", "Markus", ""]]}, {"id": "2103.06140", "submitter": "Xishuang Dong", "authors": "Lucy Nwosu, Xiangfang Li, Lijun Qian, Seungchan Kim, Xishuang Dong", "title": "Semi-supervised Learning for COVID-19 Image Classification via ResNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic in over 200\ncountries and territories, which has resulted in a great public health concern\nacross the international community. Analysis of X-ray imaging data can play a\ncritical role in timely and accurate screening and fighting against COVID-19.\nSupervised deep learning has been successfully applied to recognize COVID-19\npathology from X-ray imaging datasets. However, it requires a substantial\namount of annotated X-ray images to train models, which is often not applicable\nto data analysis for emerging events such as COVID-19 outbreak, especially in\nthe early stage of the outbreak. To address this challenge, this paper proposes\na two-path semi-supervised deep learning model, ssResNet, based on Residual\nNeural Network (ResNet) for COVID-19 image classification, where two paths\nrefer to a supervised path and an unsupervised path, respectively. Moreover, we\ndesign a weighted supervised loss that assigns higher weight for the minority\nclasses in the training process to resolve the data imbalance. Experimental\nresults on a large-scale of X-ray image dataset COVIDx demonstrate that the\nproposed model can achieve promising performance even when trained on very few\nlabeled training images.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 04:25:39 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 01:53:59 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Nwosu", "Lucy", ""], ["Li", "Xiangfang", ""], ["Qian", "Lijun", ""], ["Kim", "Seungchan", ""], ["Dong", "Xishuang", ""]]}, {"id": "2103.06149", "submitter": "Youshan Zhang", "authors": "Youshan Zhang and Brian D. Davison", "title": "Adversarial Regression Learning for Bone Age Estimation", "comments": "27th Information Processing in Medical Imaging (IPMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Estimation of bone age from hand radiographs is essential to determine\nskeletal age in diagnosing endocrine disorders and depicting the growth status\nof children. However, existing automatic methods only apply their models to\ntest images without considering the discrepancy between training samples and\ntest samples, which will lead to a lower generalization ability. In this paper,\nwe propose an adversarial regression learning network (ARLNet) for bone age\nestimation. Specifically, we first extract bone features from a fine-tuned\nInception V3 neural network and propose regression percentage loss for\ntraining. To reduce the discrepancy between training and test data, we then\npropose adversarial regression loss and feature reconstruction loss to\nguarantee the transition from training data to test data and vice versa,\npreserving invariant features from both training and test data. Experimental\nresults show that the proposed model outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:58:26 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Zhang", "Youshan", ""], ["Davison", "Brian D.", ""]]}, {"id": "2103.06164", "submitter": "Pingfan Song", "authors": "Pingfan Song, Herman Verinaz Jadan, Carmel L. Howe, Peter Quicke,\n  Amanda J. Foust, Pier Luigi Dragotti", "title": "Model-inspired Deep Learning for Light-Field Microscopy with Application\n  to Neuron Localization", "comments": "5 pages, 6 figures, ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Light-field microscopes are able to capture spatial and angular information\nof incident light rays. This allows reconstructing 3D locations of neurons from\na single snap-shot.In this work, we propose a model-inspired deep learning\napproach to perform fast and robust 3D localization of sources using\nlight-field microscopy images. This is achieved by developing a deep network\nthat efficiently solves a convolutional sparse coding (CSC) problem to map\nEpipolar Plane Images (EPI) to corresponding sparse codes. The network\narchitecture is designed systematically by unrolling the convolutional\nIterative Shrinkage and Thresholding Algorithm (ISTA) while the network\nparameters are learned from a training dataset. Such principled design enables\nthe deep network to leverage both domain knowledge implied in the model, as\nwell as new parameters learned from the data, thereby combining advantages of\nmodel-based and learning-based methods. Practical experiments on localization\nof mammalian neurons from light-fields show that the proposed approach\nsimultaneously provides enhanced performance, interpretability and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:24:47 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Song", "Pingfan", ""], ["Jadan", "Herman Verinaz", ""], ["Howe", "Carmel L.", ""], ["Quicke", "Peter", ""], ["Foust", "Amanda J.", ""], ["Dragotti", "Pier Luigi", ""]]}, {"id": "2103.06168", "submitter": "Tommaso Di Noto", "authors": "Tommaso Di Noto, Guillaume Marie, Sebastien Tourbier, Yasser\n  Alem\\'an-G\\'omez, Oscar Esteban, Guillaume Saliou, Meritxell Bach Cuadra,\n  Patric Hagmann, Jonas Richiardi", "title": "Weak labels and anatomical knowledge: making deep learning practical for\n  intracranial aneurysm detection in TOF-MRA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Supervised segmentation algorithms yield state-of-the-art results for\nautomated anomaly detection. However, these models require voxel-wise labels\nwhich are time-consuming to draw for medical experts. An interesting\nalternative to voxel-wise annotations is the use of weak labels: these can be\ncoarse or oversized annotations that are less precise, but considerably faster\nto create. In this work, we address the task of brain aneurysm detection by\ndeveloping a fully automated, deep neural network that is trained utilizing\noversized weak labels. Furthermore, since aneurysms mainly occur in specific\nanatomical locations, we build our model leveraging the underlying anatomy of\nthe brain vasculature both during training and inference. We apply our model to\n250 subjects (120 patients, 130 controls) who underwent Time-Of-Flight Magnetic\nResonance Angiography (TOF-MRA) and presented a total of 154 aneurysms. To\nassess the robustness of the algorithm, we participated in a MICCAI challenge\nfor TOF-MRA data (93 patients, 20 controls, 125 aneurysms) which allowed us to\nobtain results also for subjects coming from a different institution. Our\nnetwork achieves an average sensitivity of 77% on our in-house data, with a\nmean False Positive (FP) rate of 0.72 per patient. Instead, on the challenge\ndata, we attain a sensitivity of 59% with a mean FP rate of 1.18, ranking in\n7th/14 position for detection and in 4th/11 for segmentation on the open\nleaderboard. When computing detection performances with respect to aneurysms'\nrisk of rupture, we found no statistical difference between two risk groups (p\n= 0.12), although the sensitivity for dangerous aneurysms was higher (78%). Our\napproach suggests that clinically useful sensitivity can be achieved using weak\nlabels and exploiting prior anatomical knowledge; this expands the feasibility\nof deep learning studies to hospitals that have limited time and data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:31:54 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 13:03:52 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Di Noto", "Tommaso", ""], ["Marie", "Guillaume", ""], ["Tourbier", "Sebastien", ""], ["Alem\u00e1n-G\u00f3mez", "Yasser", ""], ["Esteban", "Oscar", ""], ["Saliou", "Guillaume", ""], ["Cuadra", "Meritxell Bach", ""], ["Hagmann", "Patric", ""], ["Richiardi", "Jonas", ""]]}, {"id": "2103.06175", "submitter": "Junguang Jiang", "authors": "Junguang Jiang, Yifei Ji, Ximei Wang, Yufeng Liu, Jianmin Wang,\n  Mingsheng Long", "title": "Regressive Domain Adaptation for Unsupervised Keypoint Detection", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) aims at transferring knowledge from a labeled source\ndomain to an unlabeled target domain. Though many DA theories and algorithms\nhave been proposed, most of them are tailored into classification settings and\nmay fail in regression tasks, especially in the practical keypoint detection\ntask. To tackle this difficult but significant task, we present a method of\nregressive domain adaptation (RegDA) for unsupervised keypoint detection.\nInspired by the latest theoretical work, we first utilize an adversarial\nregressor to maximize the disparity on the target domain and train a feature\ngenerator to minimize this disparity. However, due to the high dimension of the\noutput space, this regressor fails to detect samples that deviate from the\nsupport of the source. To overcome this problem, we propose two important\nideas. First, based on our observation that the probability density of the\noutput space is sparse, we introduce a spatial probability distribution to\ndescribe this sparsity and then use it to guide the learning of the adversarial\nregressor. Second, to alleviate the optimization difficulty in the\nhigh-dimensional space, we innovatively convert the minimax game in the\nadversarial training to the minimization of two opposite goals. Extensive\nexperiments show that our method brings large improvement by 8% to 11% in terms\nof PCK on different datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:45:22 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 07:51:21 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Jiang", "Junguang", ""], ["Ji", "Yifei", ""], ["Wang", "Ximei", ""], ["Liu", "Yufeng", ""], ["Wang", "Jianmin", ""], ["Long", "Mingsheng", ""]]}, {"id": "2103.06179", "submitter": "Christian Reimers", "authors": "Christian Reimers and Paul Bodesheim and Jakob Runge and Joachim\n  Denzler", "title": "Towards Learning an Unbiased Classifier from Biased Data via Conditional\n  Adversarial Debiasing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bias in classifiers is a severe issue of modern deep learning methods,\nespecially for their application in safety- and security-critical areas. Often,\nthe bias of a classifier is a direct consequence of a bias in the training\ndataset, frequently caused by the co-occurrence of relevant features and\nirrelevant ones. To mitigate this issue, we require learning algorithms that\nprevent the propagation of bias from the dataset into the classifier. We\npresent a novel adversarial debiasing method, which addresses a feature that is\nspuriously connected to the labels of training images but statistically\nindependent of the labels for test images. Thus, the automatic identification\nof relevant features during training is perturbed by irrelevant features. This\nis the case in a wide range of bias-related problems for many computer vision\ntasks, such as automatic skin cancer detection or driver assistance. We argue\nby a mathematical proof that our approach is superior to existing techniques\nfor the abovementioned bias. Our experiments show that our approach performs\nbetter than state-of-the-art techniques on a well-known benchmark dataset with\nreal-world images of cats and dogs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:50:42 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Reimers", "Christian", ""], ["Bodesheim", "Paul", ""], ["Runge", "Jakob", ""], ["Denzler", "Joachim", ""]]}, {"id": "2103.06182", "submitter": "Heng Yang", "authors": "Heng Yang, Chris Doran, Jean-Jacques Slotine", "title": "Dynamical Pose Estimation", "comments": "Video: https://youtu.be/CDYXR1h98Q4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of aligning two sets of 3D geometric primitives given\nknown correspondences. Our first contribution is to show that this primitive\nalignment framework unifies five perception problems including point cloud\nregistration, primitive (mesh) registration, category-level 3D registration,\nabsolution pose estimation (APE), and category-level APE. Our second\ncontribution is to propose DynAMical Pose estimation (DAMP), the first general\nand practical algorithm to solve primitive alignment problem by simulating\nrigid body dynamics arising from virtual springs and damping, where the springs\nspan the shortest distances between corresponding primitives. Our third\ncontribution is to apply DAMP to the five perception problems in simulated and\nreal datasets and demonstrate (i) DAMP always converges to the globally optimal\nsolution in the first three problems with 3D-3D correspondences; (ii) although\nDAMP sometimes converges to suboptimal solutions in the last two problems with\n2D-3D correspondences, with a simple scheme for escaping local minima, DAMP\nalmost always succeeds. Our last contribution is to demystify the surprising\nempirical performance of DAMP and formally prove a global convergence result in\nthe case of point cloud registration by charactering local stability of the\nequilibrium points of the underlying dynamical system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:01:41 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 16:42:33 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Yang", "Heng", ""], ["Doran", "Chris", ""], ["Slotine", "Jean-Jacques", ""]]}, {"id": "2103.06191", "submitter": "Kaiyu Yang", "authors": "Kaiyu Yang, Jacqueline Yau, Li Fei-Fei, Jia Deng, Olga Russakovsky", "title": "A Study of Face Obfuscation in ImageNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face obfuscation (blurring, mosaicing, etc.) has been shown to be effective\nfor privacy protection; nevertheless, object recognition research typically\nassumes access to complete, unobfuscated images. In this paper, we explore the\neffects of face obfuscation on the popular ImageNet challenge visual\nrecognition benchmark. Most categories in the ImageNet challenge are not people\ncategories; however, many incidental people appear in the images, and their\nprivacy is a concern. We first annotate faces in the dataset. Then we\ndemonstrate that face blurring -- a typical obfuscation technique -- has\nminimal impact on the accuracy of recognition models. Concretely, we benchmark\nmultiple deep neural networks on face-blurred images and observe that the\noverall recognition accuracy drops only slightly (no more than 0.68%). Further,\nwe experiment with transfer learning to 4 downstream tasks (object recognition,\nscene recognition, face attribute classification, and object detection) and\nshow that features learned on face-blurred images are equally transferable. Our\nwork demonstrates the feasibility of privacy-aware visual recognition, improves\nthe highly-used ImageNet challenge benchmark, and suggests an important path\nfor future visual datasets. Data and code are available at\nhttps://github.com/princetonvisualai/imagenet-face-obfuscation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:11:34 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 15:23:55 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Yang", "Kaiyu", ""], ["Yau", "Jacqueline", ""], ["Fei-Fei", "Li", ""], ["Deng", "Jia", ""], ["Russakovsky", "Olga", ""]]}, {"id": "2103.06205", "submitter": "Ivan Ezhov", "authors": "Florian Kofler, Ivan Ezhov, Fabian Isensee, Fabian Balsiger, Christoph\n  Berger, Maximilian Koerner, Johannes Paetzold, Hongwei Li, Suprosanna Shit,\n  Richard McKinley, Spyridon Bakas, Claus Zimmer, Donna Ankerst, Jan Kirschke,\n  Benedikt Wiestler, Bjoern H. Menze", "title": "Are we using appropriate segmentation metrics? Identifying correlates of\n  human expert perception for CNN training beyond rolling the DICE coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we explore quantitative correlates of qualitative human expert\nperception. We discover that current quality metrics and loss functions,\nconsidered for biomedical image segmentation tasks, correlate moderately with\nsegmentation quality assessment by experts, especially for small yet clinically\nrelevant structures, such as the enhancing tumor in brain glioma. We propose a\nmethod employing classical statistics and experimental psychology to create\ncomplementary compound loss functions for modern deep learning methods, towards\nachieving a better fit with human quality assessment. When training a CNN for\ndelineating adult brain tumor in MR images, all four proposed loss candidates\noutperform the established baselines on the clinically important and hardest to\nsegment enhancing tumor label, while maintaining performance for other label\nchannels.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:29:11 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kofler", "Florian", ""], ["Ezhov", "Ivan", ""], ["Isensee", "Fabian", ""], ["Balsiger", "Fabian", ""], ["Berger", "Christoph", ""], ["Koerner", "Maximilian", ""], ["Paetzold", "Johannes", ""], ["Li", "Hongwei", ""], ["Shit", "Suprosanna", ""], ["McKinley", "Richard", ""], ["Bakas", "Spyridon", ""], ["Zimmer", "Claus", ""], ["Ankerst", "Donna", ""], ["Kirschke", "Jan", ""], ["Wiestler", "Benedikt", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "2103.06220", "submitter": "Daniel O\\~noro-Rubio", "authors": "Anjany Sekuboyina, Daniel O\\~noro-Rubio, Jens Kleesiek and Brandon\n  Malone", "title": "A Relational-learning Perspective to Multi-label Chest X-ray\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification of chest X-ray images is frequently performed\nusing discriminative approaches, i.e. learning to map an image directly to its\nbinary labels. Such approaches make it challenging to incorporate auxiliary\ninformation such as annotation uncertainty or a dependency among the labels.\nBuilding towards this, we propose a novel knowledge graph reformulation of\nmulti-label classification, which not only readily increases predictive\nperformance of an encoder but also serves as a general framework for\nintroducing new domain knowledge.\n  Specifically, we construct a multi-modal knowledge graph out of the chest\nX-ray images and its labels and pose multi-label classification as a link\nprediction problem. Incorporating auxiliary information can then simply be\nachieved by adding additional nodes and relations among them. When tested on a\npublicly-available radiograph dataset (CheXpert), our relational-reformulation\nusing a naive knowledge graph outperforms the state-of-art by achieving an\narea-under-ROC curve of 83.5%, an improvement of \"sim 1\" over a purely\ndiscriminative approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:44:59 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Sekuboyina", "Anjany", ""], ["O\u00f1oro-Rubio", "Daniel", ""], ["Kleesiek", "Jens", ""], ["Malone", "Brandon", ""]]}, {"id": "2103.06231", "submitter": "Sek Chai", "authors": "Sedigh Ghamari, Koray Ozcan, Thu Dinh, Andrey Melnikov, Juan Carvajal,\n  Jan Ernst, Sek Chai", "title": "Quantization-Guided Training for Compact TinyML Models", "comments": "TinyML Summit, March 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Quantization Guided Training (QGT) method to guide DNN training\ntowards optimized low-bit-precision targets and reach extreme compression\nlevels below 8-bit precision. Unlike standard quantization-aware training (QAT)\napproaches, QGT uses customized regularization to encourage weight values\ntowards a distribution that maximizes accuracy while reducing quantization\nerrors. One of the main benefits of this approach is the ability to identify\ncompression bottlenecks. We validate QGT using state-of-the-art model\narchitectures on vision datasets. We also demonstrate the effectiveness of QGT\nwith an 81KB tiny model for person detection down to 2-bit precision\n(representing 17.7x size reduction), while maintaining an accuracy drop of only\n3% compared to a floating-point baseline.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:06:05 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ghamari", "Sedigh", ""], ["Ozcan", "Koray", ""], ["Dinh", "Thu", ""], ["Melnikov", "Andrey", ""], ["Carvajal", "Juan", ""], ["Ernst", "Jan", ""], ["Chai", "Sek", ""]]}, {"id": "2103.06255", "submitter": "Duo Li", "authors": "Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong\n  Zhang, Qifeng Chen", "title": "Involution: Inverting the Inherence of Convolution for Visual\n  Recognition", "comments": "Accepted to CVPR 2021. Code and models are available at\n  https://github.com/d-li14/involution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution has been the core ingredient of modern neural networks,\ntriggering the surge of deep learning in vision. In this work, we rethink the\ninherent principles of standard convolution for vision tasks, specifically\nspatial-agnostic and channel-specific. Instead, we present a novel atomic\noperation for deep neural networks by inverting the aforementioned design\nprinciples of convolution, coined as involution. We additionally demystify the\nrecent popular self-attention operator and subsume it into our involution\nfamily as an over-complicated instantiation. The proposed involution operator\ncould be leveraged as fundamental bricks to build the new generation of neural\nnetworks for visual recognition, powering different deep learning models on\nseveral prevalent benchmarks, including ImageNet classification, COCO detection\nand segmentation, together with Cityscapes segmentation. Our involution-based\nmodels improve the performance of convolutional baselines using ResNet-50 by up\nto 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU\nabsolutely while compressing the computational cost to 66%, 65%, 72%, and 57%\non the above benchmarks, respectively. Code and pre-trained models for all the\ntasks are available at https://github.com/d-li14/involution.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:40:46 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 12:30:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Duo", ""], ["Hu", "Jie", ""], ["Wang", "Changhu", ""], ["Li", "Xiangtai", ""], ["She", "Qi", ""], ["Zhu", "Lei", ""], ["Zhang", "Tong", ""], ["Chen", "Qifeng", ""]]}, {"id": "2103.06256", "submitter": "Perrine Paul-Gilloteaux", "authors": "Guillaume Potier, Fr\\'ed\\'eric Lavancier, Stephan Kunne and Perrine\n  Paul-Gilloteaux", "title": "A registration error estimation framework for correlative imaging", "comments": "10 pages 2 figures (made of 10 panels in total)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Correlative imaging workflows are now widely used in bioimaging and aims to\nimage the same sample using at least two different and complementary imaging\nmodalities. Part of the workflow relies on finding the transformation linking a\nsource image to a target image. We are specifically interested in the\nestimation of registration error in point-based registration. We propose an\napplication of multivariate linear regression to solve the registration problem\nallowing us to propose a framework for the estimation of the associated error\nin the case of rigid and affine transformations and with anisotropic noise.\nThese developments can be used as a decision-support tool for the biologist to\nanalyze multimodal correlative images and are available under Ec-CLEM, an\nopen-source plugin under ICY.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:43:18 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Potier", "Guillaume", ""], ["Lavancier", "Fr\u00e9d\u00e9ric", ""], ["Kunne", "Stephan", ""], ["Paul-Gilloteaux", "Perrine", ""]]}, {"id": "2103.06270", "submitter": "Jack White", "authors": "Jack White, Alex Codoreanu, Ignacio Zuleta, Colm Lynch, Giovanni\n  Marchisio, Stephen Petrie, Alan R. Duffy", "title": "Super-Resolving Beyond Satellite Hardware Using Realistically Degraded\n  Images", "comments": "6 pages, 6 figures, for supplementary results, see\n  https://smpetrie.github.io/superres/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern deep Super-Resolution (SR) networks have established themselves as\nvaluable techniques in image reconstruction and enhancement. However, these\nnetworks are normally trained and tested on benchmark image data that lacks the\ntypical image degrading noise present in real images. In this paper, we test\nthe feasibility of using deep SR in real remote sensing payloads by assessing\nSR performance in reconstructing realistically degraded satellite images. We\ndemonstrate that a state-of-the-art SR technique called Enhanced Deep\nSuper-Resolution Network (EDSR), without domain specific pre-training, can\nrecover encoded pixel data on images with poor ground sampling distance,\nprovided the ground resolved distance is sufficient. However, this recovery\nvaries amongst selected geographical types. Our results indicate that custom\ntraining has potential to further improve reconstruction of overhead imagery,\nand that new satellite hardware should prioritise optical performance over\nminimising pixel size as deep SR can overcome a lack of the latter but not the\nformer.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 00:20:33 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["White", "Jack", ""], ["Codoreanu", "Alex", ""], ["Zuleta", "Ignacio", ""], ["Lynch", "Colm", ""], ["Marchisio", "Giovanni", ""], ["Petrie", "Stephen", ""], ["Duffy", "Alan R.", ""]]}, {"id": "2103.06304", "submitter": "Letitia Parcalabescu", "authors": "Letitia Parcalabescu, Nils Trost, Anette Frank", "title": "What is Multimodality?", "comments": "Paper accepted for publication at MMSR 2021; 10 pages, 5 figures", "journal-ref": "Proceedings of the 1st Workshop on Multimodal Semantic\n  Representations (MMSR), 2021, Groningen, Netherlands (Online), Association\n  for Computational Linguistics, p. 1--10", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.GL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last years have shown rapid developments in the field of multimodal\nmachine learning, combining e.g., vision, text or speech. In this position\npaper we explain how the field uses outdated definitions of multimodality that\nprove unfit for the machine learning era. We propose a new task-relative\ndefinition of (multi)modality in the context of multimodal machine learning\nthat focuses on representations and information that are relevant for a given\nmachine learning task. With our new definition of multimodality we aim to\nprovide a missing foundation for multimodal research, an important component of\nlanguage grounding and a crucial milestone towards NLU.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 19:14:07 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 09:17:44 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:32:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Parcalabescu", "Letitia", ""], ["Trost", "Nils", ""], ["Frank", "Anette", ""]]}, {"id": "2103.06331", "submitter": "Mahla Abdolahnejad", "authors": "Mahla Abdolahnejad and Peter Xiaoping Liu", "title": "Face Images as Jigsaw Puzzles: Compositional Perception of Human Faces\n  for Machines Using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important goal in human-robot-interaction (HRI) is for machines to achieve\na close to human level of face perception. One of the important differences\nbetween machine learning and human intelligence is the lack of\ncompositionality. This paper introduces a new scheme to enable generative\nadversarial networks to learn the distribution of face images composed of\nsmaller parts. This results in a more flexible machine face perception and\neasier generalization to outside training examples. We demonstrate that this\nmodel is able to produce realistic high-quality face images by generating and\npiecing together the parts. Additionally, we demonstrate that this model learns\nthe relations between the facial parts and their distributions. Therefore, the\nspecific facial parts are interchangeable between generated face images.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:25:38 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Abdolahnejad", "Mahla", ""], ["Liu", "Peter Xiaoping", ""]]}, {"id": "2103.06338", "submitter": "Fan Zhang Dr", "authors": "Fan Zhang and Angeliki Katsenou and Christos Bampis and Lukas Krasula\n  and Zhi Li and David Bull", "title": "Enhancing VMAF through New Feature Integration and Model Combination", "comments": "5 pages, 2 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  VMAF is a machine learning based video quality assessment method, originally\ndesigned for streaming applications, which combines multiple quality metrics\nand video features through SVM regression. It offers higher correlation with\nsubjective opinions compared to many conventional quality assessment methods.\nIn this paper we propose enhancements to VMAF through the integration of new\nvideo features and alternative quality metrics (selected from a diverse pool)\nalongside multiple model combination. The proposed combination approach enables\ntraining on multiple databases with varying content and distortion\ncharacteristics. Our enhanced VMAF method has been evaluated on eight HD video\ndatabases, and consistently outperforms the original VMAF model (0.6.1) and\nother benchmark quality metrics, exhibiting higher correlation with subjective\nground truth data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:43:19 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Fan", ""], ["Katsenou", "Angeliki", ""], ["Bampis", "Christos", ""], ["Krasula", "Lukas", ""], ["Li", "Zhi", ""], ["Bull", "David", ""]]}, {"id": "2103.06342", "submitter": "Umberto Michieli", "authors": "Umberto Michieli and Pietro Zanuttigh", "title": "Continual Semantic Segmentation via Repulsion-Attraction of Sparse and\n  Disentangled Latent Representations", "comments": "CVPR 2021. 22 pages, 10 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks suffer from the major limitation of catastrophic\nforgetting old tasks when learning new ones. In this paper we focus on class\nincremental continual learning in semantic segmentation, where new categories\nare made available over time while previous training data is not retained. The\nproposed continual learning scheme shapes the latent space to reduce forgetting\nwhilst improving the recognition of novel classes. Our framework is driven by\nthree novel components which we also combine on top of existing techniques\neffortlessly. First, prototypes matching enforces latent space consistency on\nold classes, constraining the encoder to produce similar latent representation\nfor previously seen classes in the subsequent steps. Second, features\nsparsification allows to make room in the latent space to accommodate novel\nclasses. Finally, contrastive learning is employed to cluster features\naccording to their semantics while tearing apart those of different classes.\nExtensive evaluation on the Pascal VOC2012 and ADE20K datasets demonstrates the\neffectiveness of our approach, significantly outperforming state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 21:02:05 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:58:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Michieli", "Umberto", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "2103.06360", "submitter": "Hans Liebl", "authors": "Hans Liebl (1), David Schinz (1), Anjany Sekuboyina (1 and 2), Luca\n  Malagutti (1), Maximilian T. L\\\"offler (3), Amirhossein Bayat (1 and 2),\n  Malek El Husseini (1 and 2), Giles Tetteh (1 and 2), Katharina Grau (1), Eva\n  Niederreiter (1), Thomas Baum (1), Benedikt Wiestler (1), Bjoern Menze (2),\n  Rickmer Braren (4), Claus Zimmer (1), Jan S. Kirschke (1) ((1) Department of\n  Diagnostic and Interventional Neuroradiology, School of Medicine, Klinikum\n  rechts der Isar, Technical University of Munich, Germany (2) Department of\n  Informatics, Technical University of Munich, Germany (3) Department of\n  Diagnostic and Interventional Radiology, University Medical Center Freiburg,\n  Freiburg im Breisgau, Germany (4) Department of Diagnostic and Interventional\n  Radiology, School of Medicine, Klinikum rechts der Isar, Technical University\n  of Munich, Germany)", "title": "A Computed Tomography Vertebral Segmentation Dataset with Anatomical\n  Variations and Multi-Vendor Scanner Data", "comments": "18 pages, 2 figures, 2 tables; Hans Liebl, David Schinz equally\n  contributed to this manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of deep learning algorithms, fully automated radiological\nimage analysis is within reach. In spine imaging, several atlas- and\nshape-based as well as deep learning segmentation algorithms have been\nproposed, allowing for subsequent automated analysis of morphology and\npathology. The first Large Scale Vertebrae Segmentation Challenge (VerSe 2019)\nshowed that these perform well on normal anatomy, but fail in variants not\nfrequently present in the training dataset. Building on that experience, we\nreport on the largely increased VerSe 2020 dataset and results from the second\niteration of the VerSe challenge (MICCAI 2020, Lima, Peru). VerSe 2020\ncomprises annotated spine computed tomography (CT) images from 300 subjects\nwith 4142 fully visualized and annotated vertebrae, collected across multiple\ncentres from four different scanner manufacturers, enriched with cases that\nexhibit anatomical variants such as enumeration abnormalities (n=77) and\ntransitional vertebrae (n=161). Metadata includes vertebral labelling\ninformation, voxel-level segmentation masks obtained with a human-machine\nhybrid algorithm and anatomical ratings, to enable the development and\nbenchmarking of robust and accurate segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:07:26 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Liebl", "Hans", "", "1 and 2"], ["Schinz", "David", "", "1 and 2"], ["Sekuboyina", "Anjany", "", "1 and 2"], ["Malagutti", "Luca", "", "1 and 2"], ["L\u00f6ffler", "Maximilian T.", "", "1 and 2"], ["Bayat", "Amirhossein", "", "1 and 2"], ["Husseini", "Malek El", "", "1 and 2"], ["Tetteh", "Giles", "", "1 and 2"], ["Grau", "Katharina", ""], ["Niederreiter", "Eva", ""], ["Baum", "Thomas", ""], ["Wiestler", "Benedikt", ""], ["Menze", "Bjoern", ""], ["Braren", "Rickmer", ""], ["Zimmer", "Claus", ""], ["Kirschke", "Jan S.", ""]]}, {"id": "2103.06366", "submitter": "Andrew Willis", "authors": "Akash Chandrashekar, John Papadakis, Andrew Willis, Jamie Gantert", "title": "Structure-From-Motion and RGBD Depth Fusion", "comments": null, "journal-ref": null, "doi": "10.1109/SECON.2018.8478927", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a technique to augment a typical RGBD sensor by\nintegrating depth estimates obtained via Structure-from-Motion (SfM) with\nsensor depth measurements. Limitations in the RGBD depth sensing technology\nprevent capturing depth measurements in four important contexts: (1) distant\nsurfaces (>5m), (2) dark surfaces, (3) brightly lit indoor scenes and (4)\nsunlit outdoor scenes. SfM technology computes depth via multi-view\nreconstruction from the RGB image sequence alone. As such, SfM depth estimates\ndo not suffer the same limitations and may be computed in all four of the\npreviously listed circumstances. This work describes a novel fusion of RGBD\ndepth data and SfM-estimated depths to generate an improved depth stream that\nmay be processed by one of many important downstream applications such as\nrobotic localization and mapping, as well as object recognition and tracking.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:14:11 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Chandrashekar", "Akash", ""], ["Papadakis", "John", ""], ["Willis", "Andrew", ""], ["Gantert", "Jamie", ""]]}, {"id": "2103.06384", "submitter": "Ayman Al-Kababji", "authors": "Ayman Al-Kababji, Faycal Bensaali, Sarada Prasad Dakua", "title": "Automated liver tissues delineation based on machine learning\n  techniques: A survey, current trends and future orientations", "comments": "41 pages, 4 figures, 13 equations, 1 table. A review paper on liver\n  tissues segmentation based on automated ML-based techniques", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no denying how machine learning and computer vision have grown in\nthe recent years. Their highest advantages lie within their automation,\nsuitability, and ability to generate astounding results in a matter of seconds\nin a reproducible manner. This is aided by the ubiquitous advancements reached\nin the computing capabilities of current graphical processing units and the\nhighly efficient implementation of such techniques. Hence, in this paper, we\nsurvey the key studies that are published between 2014 and 2020, showcasing the\ndifferent machine learning algorithms researchers have used to segment the\nliver, hepatic-tumors, and hepatic-vasculature structures. We divide the\nsurveyed studies based on the tissue of interest (hepatic-parenchyma,\nhepatic-tumors, or hepatic-vessels), highlighting the studies that tackle more\nthan one task simultaneously. Additionally, the machine learning algorithms are\nclassified as either supervised or unsupervised, and further partitioned if the\namount of works that fall under a certain scheme is significant. Moreover,\ndifferent datasets and challenges found in literature and websites, containing\nmasks of the aforementioned tissues, are thoroughly discussed, highlighting the\norganizers original contributions, and those of other researchers. Also, the\nmetrics that are used excessively in literature are mentioned in our review\nstressing their relevancy to the task at hand. Finally, critical challenges and\nfuture directions are emphasized for innovative researchers to tackle, exposing\ngaps that need addressing such as the scarcity of many studies on the vessels\nsegmentation challenge, and why their absence needs to be dealt with in an\naccelerated manner.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:11:16 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Al-Kababji", "Ayman", ""], ["Bensaali", "Faycal", ""], ["Dakua", "Sarada Prasad", ""]]}, {"id": "2103.06403", "submitter": "Amir Ehsan Niaraki Asli", "authors": "Jeremy Roghair, Kyungtae Ko, Amir Ehsan Niaraki Asli and Ali Jannesari", "title": "A Vision Based Deep Reinforcement Learning Algorithm for UAV Obstacle\n  Avoidance", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration of reinforcement learning with unmanned aerial vehicles (UAVs) to\nachieve autonomous flight has been an active research area in recent years. An\nimportant part focuses on obstacle detection and avoidance for UAVs navigating\nthrough an environment. Exploration in an unseen environment can be tackled\nwith Deep Q-Network (DQN). However, value exploration with uniform sampling of\nactions may lead to redundant states, where often the environments inherently\nbear sparse rewards. To resolve this, we present two techniques for improving\nexploration for UAV obstacle avoidance. The first is a convergence-based\napproach that uses convergence error to iterate through unexplored actions and\ntemporal threshold to balance exploration and exploitation. The second is a\nguidance-based approach using a Domain Network which uses a Gaussian mixture\ndistribution to compare previously seen states to a predicted next state in\norder to select the next action. Performance and evaluation of these approaches\nwere implemented in multiple 3-D simulation environments, with variation in\ncomplexity. The proposed approach demonstrates a two-fold improvement in\naverage rewards compared to state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 01:15:26 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Roghair", "Jeremy", ""], ["Ko", "Kyungtae", ""], ["Asli", "Amir Ehsan Niaraki", ""], ["Jannesari", "Ali", ""]]}, {"id": "2103.06417", "submitter": "Yasunori Ozaki", "authors": "Yuki Tamaru, Yasunori Ozaki, Yuki Okafuji, Jun Baba, Junya Nakanishi,\n  Yuichiro Yoshikawa", "title": "3D Head-Position Prediction in First-Person View by Considering Head\n  Pose for Human-Robot Eye Contact", "comments": "Submit to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For a humanoid robot to make eye contact to initiate communication with a\nhuman, it is necessary to estimate the human's head position.However, eye\ncontact becomes difficult due to the mechanical delay of the robot while the\nsubject with whom the robot is interacting with is moving. Owing to these\nissues, it is important to perform head-position prediction to mitigate the\neffect of the delay in the robot's motion. Based on the fact that humans turn\ntheir heads before changing direction while walking, we hypothesized that the\naccuracy of three-dimensional(3D) head-position prediction from the\nfirst-person view can be improved by considering the head pose into account.We\ncompared our method with the conventional Kalman filter-based method, and found\nour method to be more accurate. The experimental results show that considering\nthe head pose helps improve the accuracy of 3D head-position prediction.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 02:16:53 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Tamaru", "Yuki", ""], ["Ozaki", "Yasunori", ""], ["Okafuji", "Yuki", ""], ["Baba", "Jun", ""], ["Nakanishi", "Junya", ""], ["Yoshikawa", "Yuichiro", ""]]}, {"id": "2103.06419", "submitter": "Changfa Shi", "authors": "Jinke Wang, Peiqing Lv, Haiying Wang, Changfa Shi", "title": "SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid\n  pooling based residual U-Net for automatic liver segmentation in Computed\n  Tomography", "comments": "25 pages, 17 figures, accepted by Computer Methods and Programs in\n  Biomedicine, DOI:10.1016/j.cmpb.2021.106268", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and objective: In this paper, a modified U-Net based framework is\npresented, which leverages techniques from Squeeze-and-Excitation (SE) block,\nAtrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and\nrobust liver CT segmentation, and the effectiveness of the proposed method was\ntested on two public datasets LiTS17 and SLiver07.\n  Methods: A new network architecture called SAR-U-Net was designed. Firstly,\nthe SE block is introduced to adaptively extract image features after each\nconvolution in the U-Net encoder, while suppressing irrelevant regions, and\nhighlighting features of specific segmentation task; Secondly, ASPP was\nemployed to replace the transition layer and the output layer, and acquire\nmulti-scale image information via different receptive fields. Thirdly, to\nalleviate the degradation problem, the traditional convolution block was\nreplaced with the residual block and thus prompt the network to gain accuracy\nfrom considerably increased depth.\n  Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and\nMSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other\nclosely related 2D-based models, the proposed method achieved the highest\naccuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,\nASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared\nwith other closely related models, the proposed method achieved the highest\nsegmentation accuracy except for the RVD.\n  Conclusion: The proposed model enables a great improvement on the accuracy\ncompared to 2D-based models, and its robustness in circumvent challenging\nproblems, such as small liver regions, discontinuous liver regions, and fuzzy\nliver boundaries, is also well demonstrated and validated.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 02:32:59 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 05:01:26 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 03:40:40 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Jinke", ""], ["Lv", "Peiqing", ""], ["Wang", "Haiying", ""], ["Shi", "Changfa", ""]]}, {"id": "2103.06422", "submitter": "Zhaopeng Cui", "authors": "Cheng Zhang, Zhaopeng Cui, Yinda Zhang, Bing Zeng, Marc Pollefeys,\n  Shuaicheng Liu", "title": "Holistic 3D Scene Understanding from a Single Image with Implicit\n  Representation", "comments": "Published in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new pipeline for holistic 3D scene understanding from a single\nimage, which could predict object shapes, object poses, and scene layout. As it\nis a highly ill-posed problem, existing methods usually suffer from inaccurate\nestimation of both shapes and layout especially for the cluttered scene due to\nthe heavy occlusion between objects. We propose to utilize the latest deep\nimplicit representation to solve this challenge. We not only propose an\nimage-based local structured implicit network to improve the object shape\nestimation, but also refine the 3D object pose and scene layout via a novel\nimplicit scene graph neural network that exploits the implicit local object\nfeatures. A novel physical violation loss is also proposed to avoid incorrect\ncontext between objects. Extensive experiments demonstrate that our method\noutperforms the state-of-the-art methods in terms of object shape, scene layout\nestimation, and 3D object detection.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 02:52:46 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 10:09:28 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhang", "Cheng", ""], ["Cui", "Zhaopeng", ""], ["Zhang", "Yinda", ""], ["Zeng", "Bing", ""], ["Pollefeys", "Marc", ""], ["Liu", "Shuaicheng", ""]]}, {"id": "2103.06432", "submitter": "Feixiang Lu", "authors": "Hui Miao, Feixiang Lu, Zongdai Liu, Liangjun Zhang, Dinesh Manocha,\n  Bin Zhou", "title": "Robust 2D/3D Vehicle Parsing in CVIS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel approach to robustly detect and perceive vehicles in\ndifferent camera views as part of a cooperative vehicle-infrastructure system\n(CVIS). Our formulation is designed for arbitrary camera views and makes no\nassumptions about intrinsic or extrinsic parameters. First, to deal with\nmulti-view data scarcity, we propose a part-assisted novel view synthesis\nalgorithm for data augmentation. We train a part-based texture inpainting\nnetwork in a self-supervised manner. Then we render the textured model into the\nbackground image with the target 6-DoF pose. Second, to handle various camera\nparameters, we present a new method that produces dense mappings between image\npixels and 3D points to perform robust 2D/3D vehicle parsing. Third, we build\nthe first CVIS dataset for benchmarking, which annotates more than 1540 images\n(14017 instances) from real-world traffic scenarios. We combine these novel\nalgorithms and datasets to develop a robust approach for 2D/3D vehicle parsing\nfor CVIS. In practice, our approach outperforms SOTA methods on 2D detection,\ninstance segmentation, and 6-DoF pose estimation, by 4.5%, 4.3%, and 2.9%,\nrespectively. More details and results are included in the supplement. To\nfacilitate future research, we will release the source code and the dataset on\nGitHub.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:35:05 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Miao", "Hui", ""], ["Lu", "Feixiang", ""], ["Liu", "Zongdai", ""], ["Zhang", "Liangjun", ""], ["Manocha", "Dinesh", ""], ["Zhou", "Bin", ""]]}, {"id": "2103.06443", "submitter": "Sourav Garg", "authors": "Sourav Garg, Tobias Fischer and Michael Milford", "title": "Where is your place, Visual Place Recognition?", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is often characterized as being able to\nrecognize the same place despite significant changes in appearance and\nviewpoint. VPR is a key component of Spatial Artificial Intelligence, enabling\nrobotic platforms and intelligent augmentation platforms such as augmented\nreality devices to perceive and understand the physical world. In this paper,\nwe observe that there are three \"drivers\" that impose requirements on spatially\nintelligent agents and thus VPR systems: 1) the particular agent including its\nsensors and computational resources, 2) the operating environment of this\nagent, and 3) the specific task that the artificial agent carries out. In this\npaper, we characterize and survey key works in the VPR area considering those\ndrivers, including their place representation and place matching choices. We\nalso provide a new definition of VPR based on the visual overlap -- akin to\nspatial view cells in the brain -- that enables us to find similarities and\ndifferences to other research areas in the robotics and computer vision fields.\nWe identify numerous open challenges and suggest areas that require more\nin-depth attention in future works.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:11:04 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Garg", "Sourav", ""], ["Fischer", "Tobias", ""], ["Milford", "Michael", ""]]}, {"id": "2103.06450", "submitter": "Sumeet Sohan Singh", "authors": "Sumeet S. Singh, Sergey Karayev", "title": "Full Page Handwriting Recognition via Image to Sequence Extraction", "comments": "To appear in ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:37:29 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 18:52:44 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Singh", "Sumeet S.", ""], ["Karayev", "Sergey", ""]]}, {"id": "2103.06460", "submitter": "Huan Wang", "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu", "title": "Emerging Paradigms of Neural Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-parameterization of neural networks benefits the optimization and\ngeneralization yet brings cost in practice. Pruning is adopted as a\npost-processing solution to this problem, which aims to remove unnecessary\nparameters in a neural network with little performance compromised. It has been\nbroadly believed the resulted sparse neural network cannot be trained from\nscratch to comparable accuracy. However, several recent works (e.g., [Frankle\nand Carbin, 2019a]) challenge this belief by discovering random sparse networks\nwhich can be trained to match the performance with their dense counterpart.\nThis new pruning paradigm later inspires more new methods of pruning at\ninitialization. In spite of the encouraging progress, how to coordinate these\nnew pruning fashions with the traditional pruning has not been explored yet.\nThis survey seeks to bridge the gap by proposing a general pruning framework so\nthat the emerging pruning paradigms can be accommodated well with the\ntraditional one. With it, we systematically reflect the major differences and\nnew insights brought by these new pruning fashions, with representative works\ndiscussed at length. Finally, we summarize the open questions as worthy future\ndirections.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:01:52 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Wang", "Huan", ""], ["Qin", "Can", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""]]}, {"id": "2103.06467", "submitter": "James-Andrew Sarmiento", "authors": "James-Andrew Sarmiento", "title": "Pavement Distress Detection and Segmentation using YOLOv4 and DeepLabv3\n  on Pavements in the Philippines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Road transport infrastructure is critical for safe, fast, economical, and\nreliable mobility within the whole country that is conducive to a productive\nsociety. However, roads tend to deteriorate over time due to natural causes in\nthe environment and repeated traffic loads. Pavement Distress (PD) detection is\nessential in monitoring the current conditions of the public roads to enable\ntargeted rehabilitation and preventive maintenance. Nonetheless, distress\ndetection surveys are still done via manual inspection for developing countries\nsuch as the Philippines. This study proposed the use of deep learning for two\nways of recording pavement distresses from 2D RGB images - detection and\nsegmentation. YOLOv4 is used for pavement distress detection while DeepLabv3 is\nemployed for pavement distress segmentation on a small dataset of pavement\nimages in the Philippines. This study aims to provide a basis to potentially\nspark solutions in building a cheap, scalable, and automated end-to-end\nsolution for PD detection in the country.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:25:29 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Sarmiento", "James-Andrew", ""]]}, {"id": "2103.06487", "submitter": "Haowen Liu", "authors": "Haowen Liu, Ping Yi, Hsiao-Ying Lin, Jie Shi, Weidong Qiu", "title": "DAFAR: Defending against Adversaries by Feedback-Autoencoder\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown impressive performance on challenging perceptual\ntasks and has been widely used in software to provide intelligent services.\nHowever, researchers found deep neural networks vulnerable to adversarial\nexamples. Since then, many methods are proposed to defend against adversaries\nin inputs, but they are either attack-dependent or shown to be ineffective with\nnew attacks. And most of existing techniques have complicated structures or\nmechanisms that cause prohibitively high overhead or latency, impractical to\napply on real software.\n  We propose DAFAR, a feedback framework that allows deep learning models to\ndetect/purify adversarial examples in high effectiveness and universality, with\nlow area and time overhead. DAFAR has a simple structure, containing a victim\nmodel, a plug-in feedback network, and a detector. The key idea is to import\nthe high-level features from the victim model's feature extraction layers into\nthe feedback network to reconstruct the input. This data stream forms a\nfeedback autoencoder. For strong attacks, it transforms the imperceptible\nattack on the victim model into the obvious reconstruction-error attack on the\nfeedback autoencoder directly, which is much easier to detect; for weak\nattacks, the reformation process destroys the structure of adversarial\nexamples. Experiments are conducted on MNIST and CIFAR-10 data-sets, showing\nthat DAFAR is effective against popular and arguably most advanced attacks\nwithout losing performance on legitimate samples, with high effectiveness and\nuniversality across attack methods and parameters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:18:50 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:49:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Haowen", ""], ["Yi", "Ping", ""], ["Lin", "Hsiao-Ying", ""], ["Shi", "Jie", ""], ["Qiu", "Weidong", ""]]}, {"id": "2103.06495", "submitter": "Shancheng Fang", "authors": "Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, Yongdong Zhang", "title": "Read Like Humans: Autonomous, Bidirectional and Iterative Language\n  Modeling for Scene Text Recognition", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Linguistic knowledge is of great benefit to scene text recognition. However,\nhow to effectively model linguistic rules in end-to-end deep networks remains a\nresearch challenge. In this paper, we argue that the limited capacity of\nlanguage models comes from: 1) implicitly language modeling; 2) unidirectional\nfeature representation; and 3) language model with noise input.\nCorrespondingly, we propose an autonomous, bidirectional and iterative ABINet\nfor scene text recognition. Firstly, the autonomous suggests to block gradient\nflow between vision and language models to enforce explicitly language\nmodeling. Secondly, a novel bidirectional cloze network (BCN) as the language\nmodel is proposed based on bidirectional feature representation. Thirdly, we\npropose an execution manner of iterative correction for language model which\ncan effectively alleviate the impact of noise input. Additionally, based on the\nensemble of iterative predictions, we propose a self-training method which can\nlearn from unlabeled images effectively. Extensive experiments indicate that\nABINet has superiority on low-quality images and achieves state-of-the-art\nresults on several mainstream benchmarks. Besides, the ABINet trained with\nensemble self-training shows promising improvement in realizing human-level\nrecognition. Code is available at https://github.com/FangShancheng/ABINet.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:47:45 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Fang", "Shancheng", ""], ["Xie", "Hongtao", ""], ["Wang", "Yuxin", ""], ["Mao", "Zhendong", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2103.06498", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A. Jeni, Fernando\n  De la Torre", "title": "3D Human Pose, Shape and Texture from Low-Resolution Images and Videos", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.13666", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose and shape estimation from monocular images has been an active\nresearch area in computer vision. Existing deep learning methods for this task\nrely on high-resolution input, which however, is not always available in many\nscenarios such as video surveillance and sports broadcasting. Two common\napproaches to deal with low-resolution images are applying super-resolution\ntechniques to the input, which may result in unpleasant artifacts, or simply\ntraining one model for each resolution, which is impractical in many realistic\napplications.\n  To address the above issues, this paper proposes a novel algorithm called\nRSC-Net, which consists of a Resolution-aware network, a Self-supervision loss,\nand a Contrastive learning scheme. The proposed method is able to learn 3D body\npose and shape across different resolutions with one single model. The\nself-supervision loss enforces scale-consistency of the output, and the\ncontrastive learning scheme enforces scale-consistency of the deep features. We\nshow that both these new losses provide robustness when learning in a\nweakly-supervised manner. Moreover, we extend the RSC-Net to handle\nlow-resolution videos and apply it to reconstruct textured 3D pedestrians from\nlow-resolution input. Extensive experiments demonstrate that the RSC-Net can\nachieve consistently better results than the state-of-the-art methods for\nchallenging low-resolution images.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:52:12 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Xu", "Xiangyu", ""], ["Chen", "Hao", ""], ["Moreno-Noguer", "Francesc", ""], ["Jeni", "Laszlo A.", ""], ["De la Torre", "Fernando", ""]]}, {"id": "2103.06501", "submitter": "Chi Zhang", "authors": "Chi Zhang, Zihang Lin, Liheng Xu, Zongliang Li, Le Wang, Yuehu Liu,\n  Gaofeng Meng, Li Li, and Nanning Zheng", "title": "Level-aware Haze Image Synthesis by Self-Supervised Content-Style\n  Disentanglement", "comments": "14 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key procedure of haze image translation through adversarial training lies\nin the disentanglement between the feature only involved in haze synthesis,\ni.e.style feature, and the feature representing the invariant semantic content,\ni.e. content feature. Previous methods separate content feature apart by\nutilizing it to classify haze image during the training process. However, in\nthis paper we recognize the incompleteness of the content-style disentanglement\nin such technical routine. The flawed style feature entangled with content\ninformation inevitably leads the ill-rendering of the haze images. To address,\nwe propose a self-supervised style regression via stochastic linear\ninterpolation to reduce the content information in style feature. The ablative\nexperiments demonstrate the disentangling completeness and its superiority in\nlevel-aware haze image synthesis. Moreover, the generated haze data are applied\nin the testing generalization of vehicle detectors. Further study between\nhaze-level and detection performance shows that haze has obvious impact on the\ngeneralization of the vehicle detectors and such performance degrading level is\nlinearly correlated to the haze-level, which, in turn, validates the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:53:18 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Chi", ""], ["Lin", "Zihang", ""], ["Xu", "Liheng", ""], ["Li", "Zongliang", ""], ["Wang", "Le", ""], ["Liu", "Yuehu", ""], ["Meng", "Gaofeng", ""], ["Li", "Li", ""], ["Zheng", "Nanning", ""]]}, {"id": "2103.06509", "submitter": "Savannah Thais", "authors": "Savannah Thais, Gage DeZoort", "title": "Instance Segmentation GNNs for One-Shot Conformal Tracking at the LHC", "comments": "Presented at NeurIPS Machine Learning and the Physical Sciences\n  Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D instance segmentation remains a challenging problem in computer vision.\nParticle tracking at colliders like the LHC can be conceptualized as an\ninstance segmentation task: beginning from a point cloud of hits in a particle\ndetector, an algorithm must identify which hits belong to individual particle\ntrajectories and extract track properties. Graph Neural Networks (GNNs) have\nshown promising performance on standard instance segmentation tasks. In this\nwork we demonstrate the applicability of instance segmentation GNN\narchitectures to particle tracking; moreover, we re-imagine the traditional\nCartesian space approach to track-finding and instead work in a conformal\ngeometry that allows the GNN to identify tracks and extract parameters in a\nsingle shot.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 07:15:55 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Thais", "Savannah", ""], ["DeZoort", "Gage", ""]]}, {"id": "2103.06510", "submitter": "Zhaolin Xiao", "authors": "Zhaolin Xiao, Jinglei Shi, Xiaoran Jiang, Christine Guillemot", "title": "A learning-based view extrapolation method for axial super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Axial light field resolution refers to the ability to distinguish features at\ndifferent depths by refocusing. The axial refocusing precision corresponds to\nthe minimum distance in the axial direction between two distinguishable\nrefocusing planes. High refocusing precision can be essential for some light\nfield applications like microscopy. In this paper, we propose a learning-based\nmethod to extrapolate novel views from axial volumes of sheared epipolar plane\nimages (EPIs). As extended numerical aperture (NA) in classical imaging, the\nextrapolated light field gives re-focused images with a shallower depth of\nfield (DOF), leading to more accurate refocusing results. Most importantly, the\nproposed approach does not need accurate depth estimation. Experimental results\nwith both synthetic and real light fields show that the method not only works\nwell for light fields with small baselines as those captured by plenoptic\ncameras (especially for the plenoptic 1.0 cameras), but also applies to light\nfields with larger baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 07:22:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Xiao", "Zhaolin", ""], ["Shi", "Jinglei", ""], ["Jiang", "Xiaoran", ""], ["Guillemot", "Christine", ""]]}, {"id": "2103.06526", "submitter": "Jiehong Lin", "authors": "Jiehong Lin, Zewei Wei, Zhihao Li, Songcen Xu, Kui Jia, Yuanqing Li", "title": "DualPoseNet: Category-level 6D Object Pose and Size Estimation using\n  Dual Pose Network with Refined Learning of Pose Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Category-level 6D object pose and size estimation is to predict 9\ndegrees-of-freedom (9DoF) pose configurations of rotation, translation, and\nsize for object instances observed in single, arbitrary views of cluttered\nscenes. It extends previous related tasks with learning of the two additional\nrotation angles. This seemingly small difference poses technical challenges due\nto the learning and prediction in the full rotation space of SO(3). In this\npaper, we propose a new method of Dual Pose Network with refined learning of\npose consistency for this task, shortened as DualPoseNet. DualPoseNet stacks\ntwo parallel pose decoders on top of a shared pose encoder, where the implicit\ndecoder predicts object poses with a working mechanism different from that of\nthe explicit one; they thus impose complementary supervision on the training of\npose encoder. We construct the encoder based on spherical convolutions, and\ndesign a module of Spherical Fusion wherein for a better embedding of\npose-sensitive features from the appearance and shape observations. Given no\nthe testing CAD models, it is the novel introduction of the implicit decoder\nthat enables the refined pose prediction during testing, by enforcing the\npredicted pose consistency between the two decoders using a self-adaptive loss\nterm. Thorough experiments on the benchmark 9DoF object pose datasets of\nCAMERA25 and REAL275 confirm efficacy of our designs. DualPoseNet outperforms\nexisting methods with a large margin in the regime of high precision.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 08:33:47 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 12:03:17 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Lin", "Jiehong", ""], ["Wei", "Zewei", ""], ["Li", "Zhihao", ""], ["Xu", "Songcen", ""], ["Jia", "Kui", ""], ["Li", "Yuanqing", ""]]}, {"id": "2103.06533", "submitter": "Zhihao Chen", "authors": "Zhihao Chen, Liang Wan, Lei Zhu, Jia Shen, Huazhu Fu, Wennan Liu, Jing\n  Qin", "title": "Triple-cooperative Video Shadow Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shadow detection in a single image has received significant research interest\nin recent years. However, much fewer works have been explored in shadow\ndetection over dynamic scenes. The bottleneck is the lack of a well-established\ndataset with high-quality annotations for video shadow detection. In this work,\nwe collect a new video shadow detection dataset, which contains 120 videos with\n11, 685 frames, covering 60 object categories, varying lengths, and different\nmotion/lighting conditions. All the frames are annotated with a high-quality\npixel-level shadow mask. To the best of our knowledge, this is the first\nlearning-oriented dataset for video shadow detection. Furthermore, we develop a\nnew baseline model, named triple-cooperative video shadow detection network\n(TVSD-Net). It utilizes triple parallel networks in a cooperative manner to\nlearn discriminative representations at intra-video and inter-video levels.\nWithin the network, a dual gated co-attention module is proposed to constrain\nfeatures from neighboring frames in the same video, while an auxiliary\nsimilarity loss is introduced to mine semantic information between different\nvideos. Finally, we conduct a comprehensive study on ViSha, evaluating 12\nstate-of-the-art models (including single image shadow detectors, video object\nsegmentation, and saliency detection methods). Experiments demonstrate that our\nmodel outperforms SOTA competitors.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 08:54:19 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Chen", "Zhihao", ""], ["Wan", "Liang", ""], ["Zhu", "Lei", ""], ["Shen", "Jia", ""], ["Fu", "Huazhu", ""], ["Liu", "Wennan", ""], ["Qin", "Jing", ""]]}, {"id": "2103.06535", "submitter": "Snehal Bhayani", "authors": "Snehal Bhayani, Torsten Sattler, Daniel Barath, Patrik Beliansky,\n  Janne Heikkila and Zuzana Kukelova", "title": "Calibrated and Partially Calibrated Semi-Generalized Homographies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the first minimal solutions for estimating the\nsemi-generalized homography given a perspective and a generalized camera. The\nproposed solvers use five 2D-2D image point correspondences induced by a scene\nplane. One of them assumes the perspective camera to be fully calibrated, while\nthe other solver estimates the unknown focal length together with the absolute\npose parameters. This setup is particularly important in structure-from-motion\nand image-based localization pipelines, where a new camera is localized in each\nstep with respect to a set of known cameras and 2D-3D correspondences might not\nbe available. As a consequence of a clever parametrization and the elimination\nideal method, our approach only needs to solve a univariate polynomial of\ndegree five or three. The proposed solvers are stable and efficient as\ndemonstrated by a number of synthetic and real-world experiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 08:56:24 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 09:23:25 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bhayani", "Snehal", ""], ["Sattler", "Torsten", ""], ["Barath", "Daniel", ""], ["Beliansky", "Patrik", ""], ["Heikkila", "Janne", ""], ["Kukelova", "Zuzana", ""]]}, {"id": "2103.06541", "submitter": "Trisha Mittal", "authors": "Trisha Mittal, Puneet Mathur, Aniket Bera, Dinesh Manocha", "title": "Affect2MM: Affective Analysis of Multimedia Content Using Emotion\n  Causality", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Affect2MM, a learning method for time-series emotion prediction\nfor multimedia content. Our goal is to automatically capture the varying\nemotions depicted by characters in real-life human-centric situations and\nbehaviors. We use the ideas from emotion causation theories to computationally\nmodel and determine the emotional state evoked in clips of movies. Affect2MM\nexplicitly models the temporal causality using attention-based methods and\nGranger causality. We use a variety of components like facial features of\nactors involved, scene understanding, visual aesthetics, action/situation\ndescription, and movie script to obtain an affective-rich representation to\nunderstand and perceive the scene. We use an LSTM-based learning model for\nemotion perception. To evaluate our method, we analyze and compare our\nperformance on three datasets, SENDv1, MovieGraphs, and the LIRIS-ACCEDE\ndataset, and observe an average of 10-15% increase in the performance over SOTA\nmethods for all three datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:07:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Mittal", "Trisha", ""], ["Mathur", "Puneet", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2103.06546", "submitter": "Li Fan", "authors": "Fan Li, Yongming Li, Pin Wang, Jie Xiao, Fang Yan, Xinke Li", "title": "Integrated Age Estimation Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning-based age estimation has received lots of attention.\nTraditional age estimation mechanism focuses estimation age error, but ignores\nthat there is a deviation between the estimated age and real age due to\ndisease. Pathological age estimation mechanism the author proposed before\nintroduces age deviation to solve the above problem and improves classification\ncapability of the estimated age significantly. However,it does not consider the\nage estimation error of the normal control (NC) group and results in a larger\nerror between the estimated age and real age of NC group. Therefore, an\nintegrated age estimation mechanism based on Decision-Level fusion of error and\ndeviation orientation model is proposed to solve the problem.Firstly, the\ntraditional age estimation and pathological age estimation mechanisms are\nweighted together.Secondly, their optimal weights are obtained by minimizing\nmean absolute error (MAE) between the estimated age and real age of normal\npeople. In the experimental section, several representative age-related\ndatasets are used for verification of the proposed method. The results show\nthat the proposed age estimation mechanism achieves a good tradeoff effect of\nage estimation. It not only improves the classification ability of the\nestimated age, but also reduces the age estimation error of the NC group. In\ngeneral, the proposed age estimation mechanism is effective. Additionally, the\nmechanism is a framework mechanism that can be used to construct different\nspecific age estimation algorithms, contributing to relevant research.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:14:10 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Li", "Fan", ""], ["Li", "Yongming", ""], ["Wang", "Pin", ""], ["Xiao", "Jie", ""], ["Yan", "Fang", ""], ["Li", "Xinke", ""]]}, {"id": "2103.06552", "submitter": "Theodoros Georgiou", "authors": "Theodoros Georgiou, Sebastian Schmitt, Thomas B\\\"ack, Nan Pu, Wei\n  Chen, Michael Lew", "title": "PREPRINT: Comparison of deep learning and hand crafted features for\n  mining simulation data", "comments": null, "journal-ref": "Proceedings of the International Conference on Pattern Recognition\n  (ICPR) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computational Fluid Dynamics (CFD) simulations are a very important tool for\nmany industrial applications, such as aerodynamic optimization of engineering\ndesigns like cars shapes, airplanes parts etc. The output of such simulations,\nin particular the calculated flow fields, are usually very complex and hard to\ninterpret for realistic three-dimensional real-world applications, especially\nif time-dependent simulations are investigated. Automated data analysis methods\nare warranted but a non-trivial obstacle is given by the very large\ndimensionality of the data. A flow field typically consists of six measurement\nvalues for each point of the computational grid in 3D space and time (velocity\nvector values, turbulent kinetic energy, pressure and viscosity). In this paper\nwe address the task of extracting meaningful results in an automated manner\nfrom such high dimensional data sets. We propose deep learning methods which\nare capable of processing such data and which can be trained to solve relevant\ntasks on simulation data, i.e. predicting drag and lift forces applied on an\nairfoil. We also propose an adaptation of the classical hand crafted features\nknown from computer vision to address the same problem and compare a large\nvariety of descriptors and detectors. Finally, we compile a large dataset of 2D\nsimulations of the flow field around airfoils which contains 16000 flow fields\nwith which we tested and compared approaches. Our results show that the deep\nlearning-based methods, as well as hand crafted feature based approaches, are\nwell-capable to accurately describe the content of the CFD simulation output on\nthe proposed dataset.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:28:00 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Georgiou", "Theodoros", ""], ["Schmitt", "Sebastian", ""], ["B\u00e4ck", "Thomas", ""], ["Pu", "Nan", ""], ["Chen", "Wei", ""], ["Lew", "Michael", ""]]}, {"id": "2103.06561", "submitter": "Zhiwu Lu", "authors": "Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing\n  Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi,\n  Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang,\n  Yuqing Song, Xin Hong, Wanqing Cui, Danyang Hou, Yingyan Li, Junyi Li, Peiyu\n  Liu, Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng\n  Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, and Ji-Rong Wen", "title": "WenLan: Bridging Vision and Language by Large-Scale Multi-Modal\n  Pre-Training", "comments": "This paper is the outcome of the Chinese multi-modal pre-training\n  project called 'WenLan'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-modal pre-training models have been intensively explored to bridge\nvision and language in recent years. However, most of them explicitly model the\ncross-modal interaction between image-text pairs, by assuming that there exists\nstrong semantic correlation between the text and image modalities. Since this\nstrong assumption is often invalid in real-world scenarios, we choose to\nimplicitly model the cross-modal correlation for large-scale multi-modal\npre-training, which is the focus of the Chinese project `WenLan' led by our\nteam. Specifically, with the weak correlation assumption over image-text pairs,\nwe propose a two-tower pre-training model called BriVL within the cross-modal\ncontrastive learning framework. Unlike OpenAI CLIP that adopts a simple\ncontrastive learning method, we devise a more advanced algorithm by adapting\nthe latest method MoCo into the cross-modal scenario. By building a large\nqueue-based dictionary, our BriVL can incorporate more negative samples in\nlimited GPU resources. We further construct a large Chinese multi-source\nimage-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.\nExtensive experiments demonstrate that the pre-trained BriVL model outperforms\nboth UNITER and OpenAI CLIP on various downstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:39:49 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 07:52:50 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 14:01:03 GMT"}, {"version": "v4", "created": "Wed, 17 Mar 2021 12:17:02 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 23:30:38 GMT"}, {"version": "v6", "created": "Thu, 8 Jul 2021 13:56:05 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Huo", "Yuqi", ""], ["Zhang", "Manli", ""], ["Liu", "Guangzhen", ""], ["Lu", "Haoyu", ""], ["Gao", "Yizhao", ""], ["Yang", "Guoxing", ""], ["Wen", "Jingyuan", ""], ["Zhang", "Heng", ""], ["Xu", "Baogui", ""], ["Zheng", "Weihao", ""], ["Xi", "Zongzheng", ""], ["Yang", "Yueqian", ""], ["Hu", "Anwen", ""], ["Zhao", "Jinming", ""], ["Li", "Ruichen", ""], ["Zhao", "Yida", ""], ["Zhang", "Liang", ""], ["Song", "Yuqing", ""], ["Hong", "Xin", ""], ["Cui", "Wanqing", ""], ["Hou", "Danyang", ""], ["Li", "Yingyan", ""], ["Li", "Junyi", ""], ["Liu", "Peiyu", ""], ["Gong", "Zheng", ""], ["Jin", "Chuhao", ""], ["Sun", "Yuchong", ""], ["Chen", "Shizhe", ""], ["Lu", "Zhiwu", ""], ["Dou", "Zhicheng", ""], ["Jin", "Qin", ""], ["Lan", "Yanyan", ""], ["Zhao", "Wayne Xin", ""], ["Song", "Ruihua", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2103.06564", "submitter": "Xiangtai Li", "authors": "Xiangtai Li, Hao He, Xia Li, Duo Li, Guangliang Cheng, Jianping Shi,\n  Lubin Weng, Yunhai Tong, Zhouchen Lin", "title": "PointFlow: Flowing Semantics Through Points for Aerial Image\n  Segmentation", "comments": "accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aerial Image Segmentation is a particular semantic segmentation problem and\nhas several challenging characteristics that general semantic segmentation does\nnot have. There are two critical issues: The one is an extremely\nforeground-background imbalanced distribution, and the other is multiple small\nobjects along with the complex background. Such problems make the recent dense\naffinity context modeling perform poorly even compared with baselines due to\nover-introduced background context. To handle these problems, we propose a\npoint-wise affinity propagation module based on the Feature Pyramid Network\n(FPN) framework, named PointFlow. Rather than dense affinity learning, a sparse\naffinity map is generated upon selected points between the adjacent features,\nwhich reduces the noise introduced by the background while keeping efficiency.\nIn particular, we design a dual point matcher to select points from the salient\narea and object boundaries, respectively. Experimental results on three\ndifferent aerial segmentation datasets suggest that the proposed method is more\neffective and efficient than state-of-the-art general semantic segmentation\nmethods. Especially, our methods achieve the best speed and accuracy trade-off\non three aerial benchmarks. Further experiments on three general semantic\nsegmentation datasets prove the generality of our method. Code will be provided\nin (https: //github.com/lxtGH/PFSegNets).\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:42:32 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Li", "Xiangtai", ""], ["He", "Hao", ""], ["Li", "Xia", ""], ["Li", "Duo", ""], ["Cheng", "Guangliang", ""], ["Shi", "Jianping", ""], ["Weng", "Lubin", ""], ["Tong", "Yunhai", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2103.06575", "submitter": "Swati Rai", "authors": "Swati Rai, Jignesh S. Bhatt, and S. K. Patra", "title": "An unsupervised deep learning framework for medical image denoising", "comments": "22 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image acquisition is often intervented by unwanted noise that\ncorrupts the information content. This paper introduces an unsupervised medical\nimage denoising technique that learns noise characteristics from the available\nimages and constructs denoised images. It comprises of two blocks of data\nprocessing, viz., patch-based dictionaries that indirectly learn the noise and\nresidual learning (RL) that directly learns the noise. The model is generalized\nto account for both 2D and 3D images considering different medical imaging\ninstruments. The images are considered one-by-one from the stack of MRI/CT\nimages as well as the entire stack is considered, and decomposed into\noverlapping image/volume patches. These patches are given to the patch-based\ndictionary learning to learn noise characteristics via sparse representation\nwhile given to the RL part to directly learn the noise properties. K-singular\nvalue decomposition (K-SVD) algorithm for sparse representation is used for\ntraining patch-based dictionaries. On the other hand, residue in the patches is\ntrained using the proposed deep residue network. Iterating on these two parts,\nan optimum noise characterization for each image/volume patch is captured and\nin turn it is subtracted from the available respective image/volume patch. The\nobtained denoised image/volume patches are finally assembled to a denoised\nimage or 3D stack. We provide an analysis of the proposed approach with other\napproaches. Experiments on MRI/CT datasets are run on a GPU-based supercomputer\nand the comparative results show that the proposed algorithm preserves the\ncritical information in the images as well as improves the visual quality of\nthe images.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:03:02 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Rai", "Swati", ""], ["Bhatt", "Jignesh S.", ""], ["Patra", "S. K.", ""]]}, {"id": "2103.06583", "submitter": "Theodoros Georgiou", "authors": "Theodoros Georgiou, Sebastian Schmitt, Thomas B\\\"ack, Wei Chen,\n  Michael Lew", "title": "Preprint: Norm Loss: An efficient yet effective regularization method\n  for deep neural networks", "comments": null, "journal-ref": "Proceedings of the International Conference on Pattern Recognition\n  (ICPR) 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural network training can suffer from diverse issues like\nexploding or vanishing gradients, scaling-based weight space symmetry and\ncovariant-shift. In order to address these issues, researchers develop weight\nregularization methods and activation normalization methods. In this work we\npropose a weight soft-regularization method based on the Oblique manifold. The\nproposed method uses a loss function which pushes each weight vector to have a\nnorm close to one, i.e. the weight matrix is smoothly steered toward the\nso-called Oblique manifold. We evaluate our method on the very popular\nCIFAR-10, CIFAR-100 and ImageNet 2012 datasets using two state-of-the-art\narchitectures, namely the ResNet and wide-ResNet. Our method introduces\nnegligible computational overhead and the results show that it is competitive\nto the state-of-the-art and in some cases superior to it. Additionally, the\nresults are less sensitive to hyperparameter settings such as batch size and\nregularization factor.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:24:49 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Georgiou", "Theodoros", ""], ["Schmitt", "Sebastian", ""], ["B\u00e4ck", "Thomas", ""], ["Chen", "Wei", ""], ["Lew", "Michael", ""]]}, {"id": "2103.06587", "submitter": "Yuki Asano", "authors": "Peiyang He, Charlie Griffin, Krzysztof Kacprzyk, Artjom Joosen,\n  Michael Collyer, Aleksandar Shtedritski, Yuki M. Asano", "title": "Privacy-preserving Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Privacy considerations and bias in datasets are quickly becoming\nhigh-priority issues that the computer vision community needs to face. So far,\nlittle attention has been given to practical solutions that do not involve\ncollection of new datasets. In this work, we show that for object detection on\nCOCO, both anonymizing the dataset by blurring faces, as well as swapping faces\nin a balanced manner along the gender and skin tone dimension, can retain\nobject detection performances while preserving privacy and partially balancing\nbias.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:34:54 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["He", "Peiyang", ""], ["Griffin", "Charlie", ""], ["Kacprzyk", "Krzysztof", ""], ["Joosen", "Artjom", ""], ["Collyer", "Michael", ""], ["Shtedritski", "Aleksandar", ""], ["Asano", "Yuki M.", ""]]}, {"id": "2103.06627", "submitter": "Qiang Meng", "authors": "Qiang Meng, Shichao Zhao, Zhida Huang, Feng Zhou", "title": "MagFace: A Universal Representation for Face Recognition and Quality\n  Assessment", "comments": "accepted at CVPR 2021, Oral", "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of face recognition system degrades when the variability of\nthe acquired faces increases. Prior work alleviates this issue by either\nmonitoring the face quality in pre-processing or predicting the data\nuncertainty along with the face feature. This paper proposes MagFace, a\ncategory of losses that learn a universal feature embedding whose magnitude can\nmeasure the quality of the given face. Under the new loss, it can be proven\nthat the magnitude of the feature embedding monotonically increases if the\nsubject is more likely to be recognized. In addition, MagFace introduces an\nadaptive mechanism to learn a wellstructured within-class feature distributions\nby pulling easy samples to class centers while pushing hard samples away. This\nprevents models from overfitting on noisy low-quality samples and improves face\nrecognition in the wild. Extensive experiments conducted on face recognition,\nquality assessments as well as clustering demonstrate its superiority over\nstate-of-the-arts. The code is available at\nhttps://github.com/IrvingMeng/MagFace.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:58:21 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 06:50:26 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 04:47:05 GMT"}, {"version": "v4", "created": "Mon, 26 Jul 2021 12:54:25 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Meng", "Qiang", ""], ["Zhao", "Shichao", ""], ["Huang", "Zhida", ""], ["Zhou", "Feng", ""]]}, {"id": "2103.06638", "submitter": "Maria Leyva-Vallina", "authors": "Mar\\'ia Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov", "title": "Generalized Contrastive Optimization of Siamese Networks for Place\n  Recognition", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is a challenging task in computer vision and a key\ncomponent of camera-based localization and navigation systems. Recently,\nConvolutional Neural Networks (CNNs) achieved high results and good\ngeneralization capabilities. They are usually trained using pairs or triplets\nof images labeled as either similar or dissimilar, in a binary fashion. In\npractice, the similarity between two images is not binary, but rather\ncontinuous. Furthermore, training these CNNs is computationally complex and\ninvolves costly pair and triplet mining strategies.\n  We propose a Generalized Contrastive loss (GCL) function that relies on image\nsimilarity as a continuous measure, and use it to train a siamese CNN.\nFurthermore, we propose three techniques for automatic annotation of image\npairs with labels indicating their degree of similarity, and deploy them to\nre-annotate the MSLS, TB-Places, and 7Scenes datasets.\n  We demonstrate that siamese CNNs trained using the GCL function and the\nimproved annotations consistently outperform their binary counterparts. Our\nmodels trained on MSLS outperform the state-of-the-art methods, including\nNetVLAD, and generalize well on the Pittsburgh, TokyoTM and Tokyo 24/7\ndatasets. Furthermore, training a siamese network using the GCL function does\nnot require complex pair mining. We release the source code at\nhttps://github.com/marialeyvallina/generalized_contrastive_loss.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 12:32:05 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Leyva-Vallina", "Mar\u00eda", ""], ["Strisciuglio", "Nicola", ""], ["Petkov", "Nicolai", ""]]}, {"id": "2103.06643", "submitter": "Quankai Gao", "authors": "Quankai Gao, Fudong Wang, Nan Xue, Jin-Gang Yu, Gui-Song Xia", "title": "Deep Graph Matching under Quadratic Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, deep learning based methods have demonstrated promising results on\nthe graph matching problem, by relying on the descriptive capability of deep\nfeatures extracted on graph nodes. However, one main limitation with existing\ndeep graph matching (DGM) methods lies in their ignorance of explicit\nconstraint of graph structures, which may lead the model to be trapped into\nlocal minimum in training. In this paper, we propose to explicitly formulate\npairwise graph structures as a \\textbf{quadratic constraint} incorporated into\nthe DGM framework. The quadratic constraint minimizes the pairwise structural\ndiscrepancy between graphs, which can reduce the ambiguities brought by only\nusing the extracted CNN features.\n  Moreover, we present a differentiable implementation to the quadratic\nconstrained-optimization such that it is compatible with the unconstrained deep\nlearning optimizer. To give more precise and proper supervision, a\nwell-designed false matching loss against class imbalance is proposed, which\ncan better penalize the false negatives and false positives with less\noverfitting. Exhaustive experiments demonstrate that our method competitive\nperformance on real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 12:51:12 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 06:47:22 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Gao", "Quankai", ""], ["Wang", "Fudong", ""], ["Xue", "Nan", ""], ["Yu", "Jin-Gang", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2103.06644", "submitter": "Andrew Willis", "authors": "John Papadakis, Andrew R. Willis", "title": "Real-Time Surface Fitting to RGBD Sensor Data", "comments": null, "journal-ref": null, "doi": "10.1109/SECON.2017.7925286", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes novel approaches to quickly estimate planar surfaces\nfrom RGBD sensor data. The approach manipulates the standard algebraic fitting\nequations into a form that allows many of the needed regression variables to be\ncomputed directly from the camera calibration information. As such, much of the\ncomputational burden required by a standard algebraic surface fit can be\npre-computed. This provides a significant time and resource savings, especially\nwhen many surface fits are being performed which is often the case when RGBD\npoint-cloud data is being analyzed for normal estimation, curvature estimation,\npolygonization or 3D segmentation applications. Using an integral image\nimplementation, the proposed approaches show a significant increase in\nperformance compared to the standard algebraic fitting approaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 12:52:31 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Papadakis", "John", ""], ["Willis", "Andrew R.", ""]]}, {"id": "2103.06669", "submitter": "Yazan Abu Farha", "authors": "Zhe Li, Yazan Abu Farha, Juergen Gall", "title": "Temporal Action Segmentation from Timestamp Supervision", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action segmentation approaches have been very successful recently.\nHowever, annotating videos with frame-wise labels to train such models is very\nexpensive and time consuming. While weakly supervised methods trained using\nonly ordered action lists require less annotation effort, the performance is\nstill worse than fully supervised approaches. In this paper, we propose to use\ntimestamp supervision for the temporal action segmentation task. Timestamps\nrequire a comparable annotation effort to weakly supervised approaches, and yet\nprovide a more supervisory signal. To demonstrate the effectiveness of\ntimestamp supervision, we propose an approach to train a segmentation model\nusing only timestamps annotations. Our approach uses the model output and the\nannotated timestamps to generate frame-wise labels by detecting the action\nchanges. We further introduce a confidence loss that forces the predicted\nprobabilities to monotonically decrease as the distance to the timestamps\nincreases. This ensures that all and not only the most distinctive frames of an\naction are learned during training. The evaluation on four datasets shows that\nmodels trained with timestamps annotations achieve comparable performance to\nthe fully supervised approaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 13:52:41 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 09:50:40 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 15:44:26 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Li", "Zhe", ""], ["Farha", "Yazan Abu", ""], ["Gall", "Juergen", ""]]}, {"id": "2103.06725", "submitter": "Zijin Yin", "authors": "Zijin Yin, Kongming Liang, Zhanyu Ma, Jun Guo", "title": "Duplex Contextual Relation Network for Polyp Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polyp segmentation is of great importance in the early diagnosis and\ntreatment of colorectal cancer. Since polyps vary in their shape, size, color,\nand texture, accurate polyp segmentation is very challenging. One promising way\nto mitigate the diversity of polyps is to model the contextual relation for\neach pixel such as using attention mechanism. However, previous methods only\nfocus on learning the dependencies between the position within an individual\nimage and ignore the contextual relation across different images. In this\npaper, we propose Duplex Contextual Relation Network (DCRNet) to capture both\nwithin-image and cross-image contextual relations. Specifically, we first\ndesign Interior Contextual-Relation Module to estimate the similarity between\neach position and all the positions within the same image. Then Exterior\nContextual-Relation Module is incorporated to estimate the similarity between\neach position and the positions across different images. Based on the above two\ntypes of similarity, the feature at one position can be further enhanced by the\ncontextual region embedding within and across images. To store the\ncharacteristic region embedding from all the images, a memory bank is designed\nand operates as a queue. Therefore, the proposed method can relate similar\nfeatures even though they come from different images. We evaluate the proposed\nmethod on the EndoScene, Kvasir-SEG and the recently released large-scale\nPICCOLO dataset. Experimental results show that the proposed DCRNet outperforms\nthe state-of-the-art methods in terms of the widely-used evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 15:19:54 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 03:58:13 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Yin", "Zijin", ""], ["Liang", "Kongming", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""]]}, {"id": "2103.06733", "submitter": "Simon Carbonnelle", "authors": "Carbonnelle Simon and Christophe De Vleeschouwer", "title": "Intraclass clustering: an implicit learning ability that regularizes\n  DNNs", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several works have shown that the regularization mechanisms underlying deep\nneural networks' generalization performances are still poorly understood. In\nthis paper, we hypothesize that deep neural networks are regularized through\ntheir ability to extract meaningful clusters among the samples of a class. This\nconstitutes an implicit form of regularization, as no explicit training\nmechanisms or supervision target such behaviour. To support our hypothesis, we\ndesign four different measures of intraclass clustering, based on the neuron-\nand layer-level representations of the training data. We then show that these\nmeasures constitute accurate predictors of generalization performance across\nvariations of a large set of hyperparameters (learning rate, batch size,\noptimizer, weight decay, dropout rate, data augmentation, network depth and\nwidth).\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 15:26:27 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Simon", "Carbonnelle", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "2103.06747", "submitter": "Yannan He", "authors": "Yannan He, Anqi Pang, Xin Chen, Han Liang, Minye Wu, Yuexin Ma, Lan Xu", "title": "ChallenCap: Monocular 3D Capture of Challenging Human Performances using\n  Multi-Modal References", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing challenging human motions is critical for numerous applications,\nbut it suffers from complex motion patterns and severe self-occlusion under the\nmonocular setting. In this paper, we propose ChallenCap -- a template-based\napproach to capture challenging 3D human motions using a single RGB camera in a\nnovel learning-and-optimization framework, with the aid of multi-modal\nreferences. We propose a hybrid motion inference stage with a generation\nnetwork, which utilizes a temporal encoder-decoder to extract the motion\ndetails from the pair-wise sparse-view reference, as well as a motion\ndiscriminator to utilize the unpaired marker-based references to extract\nspecific challenging motion characteristics in a data-driven manner. We further\nadopt a robust motion optimization stage to increase the tracking accuracy, by\njointly utilizing the learned motion details from the supervised multi-modal\nreferences as well as the reliable motion hints from the input image reference.\nExtensive experiments on our new challenging motion dataset demonstrate the\neffectiveness and robustness of our approach to capture challenging human\nmotions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 15:49:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 13:08:54 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["He", "Yannan", ""], ["Pang", "Anqi", ""], ["Chen", "Xin", ""], ["Liang", "Han", ""], ["Wu", "Minye", ""], ["Ma", "Yuexin", ""], ["Xu", "Lan", ""]]}, {"id": "2103.06759", "submitter": "Mert Seker", "authors": "Mert Seker, Anssi M\\\"annist\\\"o, Alexandros Iosifidis, Jenni Raitoharju", "title": "Automatic Social Distance Estimation From Images: Performance\n  Evaluation, Test Benchmark, and Algorithm", "comments": "14 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 virus has caused a global pandemic since March 2020. The World\nHealth Organization (WHO) has provided guidelines on how to reduce the spread\nof the virus and one of the most important measures is social distancing.\nMaintaining a minimum of one meter distance from other people is strongly\nsuggested to reduce the risk of infection. This has created a strong interest\nin monitoring the social distances either as a safety measure or to study how\nthe measures have affected human behavior and country-wise differences in this.\nThe need for automatic social distance estimation algorithms is evident, but\nthere is no suitable test benchmark for such algorithms. Collecting images with\nmeasured ground-truth pair-wise distances between all the people using\ndifferent camera settings is cumbersome. Furthermore, performance evaluation\nfor social distance estimation algorithms is not straightforward and there is\nno widely accepted evaluation protocol. In this paper, we provide a dataset of\nvarying images with measured pair-wise social distances under different camera\npositionings and focal length values. We suggest a performance evaluation\nprotocol and provide a benchmark to easily evaluate social distance estimation\nalgorithms. We also propose a method for automatic social distance estimation.\nOur method takes advantage of object detection and human pose estimation. It\ncan be applied on any single image as long as focal length and sensor size\ninformation are known. The results on our benchmark are encouraging with 92%\nhuman detection rate and only 28.9% average error in distance estimation among\nthe detected people.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:15:20 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 17:27:42 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 05:51:55 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Seker", "Mert", ""], ["M\u00e4nnist\u00f6", "Anssi", ""], ["Iosifidis", "Alexandros", ""], ["Raitoharju", "Jenni", ""]]}, {"id": "2103.06796", "submitter": "Maximilian Durner", "authors": "Maximilian Durner, Wout Boerdijk, Martin Sundermeyer, Werner Friedl,\n  Zoltan-Csaba Marton, Rudolph Triebel", "title": "Unknown Object Segmentation from Stereo Images", "comments": "8 pages, 5 figures, 6 tables, code will be made available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although instance-aware perception is a key prerequisite for many autonomous\nrobotic applications, most of the methods only partially solve the problem by\nfocusing solely on known object categories. However, for robots interacting in\ndynamic and cluttered environments, this is not realistic and severely limits\nthe range of potential applications. Therefore, we propose a novel object\ninstance segmentation approach that does not require any semantic or geometric\ninformation of the objects beforehand. In contrast to existing works, we do not\nexplicitly use depth data as input, but rely on the insight that slight\nviewpoint changes, which for example are provided by stereo image pairs, are\noften sufficient to determine object boundaries and thus to segment objects.\nFocusing on the versatility of stereo sensors, we employ a transformer-based\narchitecture that maps directly from the pair of input images to the object\ninstances. This has the major advantage that instead of a noisy, and\npotentially incomplete depth map as an input, on which the segmentation is\ncomputed, we use the original image pair to infer the object instances and a\ndense depth map. In experiments in several different application domains, we\nshow that our Instance Stereo Transformer (INSTR) algorithm outperforms current\nstate-of-the-art methods that are based on depth maps. Training code and\npretrained models will be made available.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:03:44 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Durner", "Maximilian", ""], ["Boerdijk", "Wout", ""], ["Sundermeyer", "Martin", ""], ["Friedl", "Werner", ""], ["Marton", "Zoltan-Csaba", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2103.06818", "submitter": "Aysim Toker", "authors": "Aysim Toker, Qunjie Zhou, Maxim Maximov and Laura Leal-Taix\\'e", "title": "Coming Down to Earth: Satellite-to-Street View Synthesis for\n  Geo-Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of cross-view image based geo-localization is to determine the\nlocation of a given street view image by matching it against a collection of\ngeo-tagged satellite images. This task is notoriously challenging due to the\ndrastic viewpoint and appearance differences between the two domains. We show\nthat we can address this discrepancy explicitly by learning to synthesize\nrealistic street views from satellite inputs. Following this observation, we\npropose a novel multi-task architecture in which image synthesis and retrieval\nare considered jointly. The rationale behind this is that we can bias our\nnetwork to learn latent feature representations that are useful for retrieval\nif we utilize them to generate images across the two input domains. To the best\nof our knowledge, ours is the first approach that creates realistic street\nviews from satellite images and localizes the corresponding query street-view\nsimultaneously in an end-to-end manner. In our experiments, we obtain\nstate-of-the-art performance on the CVUSA and CVACT benchmarks. Finally, we\nshow compelling qualitative results for satellite-to-street view synthesis.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:40:59 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Toker", "Aysim", ""], ["Zhou", "Qunjie", ""], ["Maximov", "Maxim", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2103.06842", "submitter": "Lina Zhuang", "authors": "Lina Zhuang and Jose M. Bioucas-Dias", "title": "Fast Hyperspectral Image Denoising and Inpainting Based on Low-Rank and\n  Sparse Representations", "comments": null, "journal-ref": null, "doi": "10.1109/JSTARS.2018.2796570", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces two very fast and competitive hyperspectral image (HSI)\nrestoration algorithms: fast hyperspectral denoising (FastHyDe), a denoising\nalgorithm able to cope with Gaussian and Poissonian noise, and fast\nhyperspectral inpainting (FastHyIn), an inpainting algorithm to restore HSIs\nwhere some observations from known pixels in some known bands are missing.\nFastHyDe and FastHyIn fully exploit extremely compact and sparse HSI\nrepresentations linked with their low-rank and self-similarity characteristics.\nIn a series of experiments with simulated and real data, the newly introduced\nFastHyDe and FastHyIn compete with the state-of-the-art methods, with much\nlower computational complexity.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:12:29 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhuang", "Lina", ""], ["Bioucas-Dias", "Jose M.", ""]]}, {"id": "2103.06871", "submitter": "Enric Corona", "authors": "Enric Corona, Albert Pumarola, Guillem Aleny\\`a, Gerard Pons-Moll,\n  Francesc Moreno-Noguer", "title": "SMPLicit: Topology-aware Generative Model for Clothed People", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce SMPLicit, a novel generative model to jointly\nrepresent body pose, shape and clothing geometry. In contrast to existing\nlearning-based approaches that require training specific models for each type\nof garment, SMPLicit can represent in a unified manner different garment\ntopologies (e.g. from sleeveless tops to hoodies and to open jackets), while\ncontrolling other properties like the garment size or tightness/looseness. We\nshow our model to be applicable to a large variety of garments including\nT-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The\nrepresentation flexibility of SMPLicit builds upon an implicit model\nconditioned with the SMPL human body parameters and a learnable latent space\nwhich is semantically interpretable and aligned with the clothing attributes.\nThe proposed model is fully differentiable, allowing for its use into larger\nend-to-end trainable systems. In the experimental section, we demonstrate\nSMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in\nimages of dressed people. In both cases we are able to go beyond state of the\nart, by retrieving complex garment geometries, handling situations with\nmultiple clothing layers and providing a tool for easy outfit editing. To\nstimulate further research in this direction, we will make our code and model\npublicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:57:03 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 21:54:29 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Corona", "Enric", ""], ["Pumarola", "Albert", ""], ["Aleny\u00e0", "Guillem", ""], ["Pons-Moll", "Gerard", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "2103.06877", "submitter": "Piotr Doll\\'ar", "authors": "Piotr Doll\\'ar and Mannat Singh and Ross Girshick", "title": "Fast and Accurate Model Scaling", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we analyze strategies for convolutional neural network scaling;\nthat is, the process of scaling a base convolutional network to endow it with\ngreater computational complexity and consequently representational power.\nExample scaling strategies may include increasing model width, depth,\nresolution, etc. While various scaling strategies exist, their tradeoffs are\nnot fully understood. Existing analysis typically focuses on the interplay of\naccuracy and flops (floating point operations). Yet, as we demonstrate, various\nscaling strategies affect model parameters, activations, and consequently\nactual runtime quite differently. In our experiments we show the surprising\nresult that numerous scaling strategies yield networks with similar accuracy\nbut with widely varying properties. This leads us to propose a simple fast\ncompound scaling strategy that encourages primarily scaling model width, while\nscaling depth and resolution to a lesser extent. Unlike currently popular\nscaling strategies, which result in about $O(s)$ increase in model activation\nw.r.t. scaling flops by a factor of $s$, the proposed fast compound scaling\nresults in close to $O(\\sqrt{s})$ increase in activations, while achieving\nexcellent accuracy. This leads to comparable speedups on modern memory-limited\nhardware (e.g., GPU, TPU). More generally, we hope this work provides a\nframework for analyzing and selecting scaling strategies under various\ncomputational constraints.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:59:14 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Doll\u00e1r", "Piotr", ""], ["Singh", "Mannat", ""], ["Girshick", "Ross", ""]]}, {"id": "2103.06878", "submitter": "Dongdong Chen", "authors": "Zhentao Tan and Menglei Chai and Dongdong Chen and Jing Liao and Qi\n  Chu and Bin Liu and Gang Hua and Nenghai Yu", "title": "Diverse Semantic Image Synthesis via Probability Distribution Modeling", "comments": "Accepted By CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image synthesis, translating semantic layouts to photo-realistic\nimages, is a one-to-many mapping problem. Though impressive progress has been\nrecently made, diverse semantic synthesis that can efficiently produce\nsemantic-level multimodal results, still remains a challenge. In this paper, we\npropose a novel diverse semantic image synthesis framework from the perspective\nof semantic class distributions, which naturally supports diverse generation at\nsemantic or even instance level. We achieve this by modeling class-level\nconditional modulation parameters as continuous probability distributions\ninstead of discrete values, and sampling per-instance modulation parameters\nthrough instance-adaptive stochastic sampling that is consistent across the\nnetwork. Moreover, we propose prior noise remapping, through linear\nperturbation parameters encoded from paired references, to facilitate\nsupervised training and exemplar-based instance style control at test time.\nExtensive experiments on multiple datasets show that our method can achieve\nsuperior diversity and comparable quality compared to state-of-the-art methods.\nCode will be available at \\url{https://github.com/tzt101/INADE.git}\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:59:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Tan", "Zhentao", ""], ["Chai", "Menglei", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Chu", "Qi", ""], ["Liu", "Bin", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2103.06879", "submitter": "Fabio Pizzati", "authors": "Fabio Pizzati, Pietro Cerri, Raoul de Charette", "title": "CoMoGAN: continuous model-guided image-to-image translation", "comments": "CVPR 2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CoMoGAN is a continuous GAN relying on the unsupervised reorganization of the\ntarget data on a functional manifold. To that matter, we introduce a new\nFunctional Instance Normalization layer and residual mechanism, which together\ndisentangle image content from position on target manifold. We rely on naive\nphysics-inspired models to guide the training while allowing private\nmodel/translations features. CoMoGAN can be used with any GAN backbone and\nallows new types of image translation, such as cyclic image translation like\ntimelapse generation, or detached linear translation. On all datasets, it\noutperforms the literature. Our code is available at\nhttp://github.com/cv-rits/CoMoGAN .\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:59:50 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 17:59:57 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Pizzati", "Fabio", ""], ["Cerri", "Pietro", ""], ["de Charette", "Raoul", ""]]}, {"id": "2103.06902", "submitter": "Kripasindhu Sarkar", "authors": "Kripasindhu Sarkar and Lingjie Liu and Vladislav Golyanik and\n  Christian Theobalt", "title": "HumanGAN: A Generative Model of Humans Images", "comments": "Accepted at CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks achieve great performance in photorealistic\nimage synthesis in various domains, including human images. However, they\nusually employ latent vectors that encode the sampled outputs globally. This\ndoes not allow convenient control of semantically-relevant individual parts of\nthe image, and is not able to draw samples that only differ in partial aspects,\nsuch as clothing style. We address these limitations and present a generative\nmodel for images of dressed humans offering control over pose, local body part\nappearance and garment style. This is the first method to solve various aspects\nof human image generation such as global appearance sampling, pose transfer,\nparts and garment transfer, and parts sampling jointly in a unified framework.\nAs our model encodes part-based latent appearance vectors in a normalized\npose-independent space and warps them to different poses, it preserves body and\nclothing appearance under varying posture. Experiments show that our flexible\nand general generative method outperforms task-specific baselines for\npose-conditioned image generation, pose transfer and part sampling in terms of\nrealism and output resolution.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:00:38 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Sarkar", "Kripasindhu", ""], ["Liu", "Lingjie", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""]]}, {"id": "2103.06911", "submitter": "Qiaojun Feng", "authors": "Tianyu Zhao, Qiaojun Feng, Sai Jadhav, Nikolay Atanasov", "title": "CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers online object-level mapping using partial point-cloud\nobservations obtained online in an unknown environment. We develop and approach\nfor fully Convolutional Object Retrieval and Symmetry-AIded Registration\n(CORSAIR). Our model extends the Fully Convolutional Geometric Features model\nto learn a global object-shape embedding in addition to local point-wise\nfeatures from the point-cloud observations. The global feature is used to\nretrieve a similar object from a category database, and the local features are\nused for robust pose registration between the observed and the retrieved\nobject. Our formulation also leverages symmetries, present in the object\nshapes, to obtain promising local-feature pairs from different symmetry classes\nfor matching. We present results from synthetic and real-world datasets with\ndifferent object categories to verify the robustness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:12:48 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhao", "Tianyu", ""], ["Feng", "Qiaojun", ""], ["Jadhav", "Sai", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2103.06929", "submitter": "Hong-Shuo Chen", "authors": "Hong-Shuo Chen, Mozhdeh Rouhsedaghat, Hamza Ghani, Shuowen Hu, Suya\n  You, C.-C. Jay Kuo", "title": "DefakeHop: A Light-Weight High-Performance Deepfake Detector", "comments": "Accepted at ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A light-weight high-performance Deepfake detection method, called DefakeHop,\nis proposed in this work. State-of-the-art Deepfake detection methods are built\nupon deep neural networks. DefakeHop extracts features automatically using the\nsuccessive subspace learning (SSL) principle from various parts of face images.\nThe features are extracted by c/w Saab transform and further processed by our\nfeature distillation module using spatial dimension reduction and soft\nclassification for each channel to get a more concise description of the face.\nExtensive experiments are conducted to demonstrate the effectiveness of the\nproposed DefakeHop method. With a small model size of 42,845 parameters,\nDefakeHop achieves state-of-the-art performance with the area under the ROC\ncurve (AUC) of 100%, 94.95%, and 90.56% on UADFV, Celeb-DF v1 and Celeb-DF v2\ndatasets, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:01:30 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Chen", "Hong-Shuo", ""], ["Rouhsedaghat", "Mozhdeh", ""], ["Ghani", "Hamza", ""], ["Hu", "Shuowen", ""], ["You", "Suya", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2103.06937", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su and Subhransu Maji", "title": "The Semi-Supervised iNaturalist-Aves Challenge at FGVC7 Workshop", "comments": "Tech report for Semi-iNat 2020 challenge, please see\n  http://github.com/cvl-umass/semi-inat-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document describes the details and the motivation behind a new dataset\nwe collected for the semi-supervised recognition challenge~\\cite{semi-aves} at\nthe FGVC7 workshop at CVPR 2020. The dataset contains 1000 species of birds\nsampled from the iNat-2018 dataset for a total of nearly 150k images. From this\ncollection, we sample a subset of classes and their labels, while adding the\nimages from the remaining classes to the unlabeled set of images. The presence\nof out-of-domain data (novel classes), high class-imbalance, and fine-grained\nsimilarity between classes poses significant challenges for existing\nsemi-supervised recognition techniques in the literature. The dataset is\navailable here: \\url{https://github.com/cvl-umass/semi-inat-2020}\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:21:16 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Maji", "Subhransu", ""]]}, {"id": "2103.06966", "submitter": "Laurent Chauvin", "authors": "Laurent Chauvin, Kuldeep Kumar, Christian Desrosiers, William Wells\n  III and Matthew Toews", "title": "Efficient Pairwise Neuroimage Analysis using the Soft Jaccard Index and\n  3D Keypoint Sets", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a novel pairwise distance measure between variable-sized sets of\nimage keypoints for the purpose of large-scale medical image indexing. Our\nmeasure generalizes the Jaccard index to account for soft set equivalence (SSE)\nbetween set elements, via an adaptive kernel framework accounting for\nuncertainty in keypoint appearance and geometry. Novel kernels are proposed to\nquantify the variability of keypoint geometry in location and scale. Our\ndistance measure may be estimated between $N^2$ image pairs in $O(N~\\log~N)$\noperations via keypoint indexing. Experiments validate our method in predicting\n509,545 pairwise relationships from T1-weighted MRI brain volumes of\nmonozygotic and dizygotic twins, siblings and half-siblings sharing 100%-25% of\ntheir polymorphic genes. Soft set equivalence and keypoint geometry kernels\noutperform standard hard set equivalence (HSE) in predicting family\nrelationships. High accuracy is achieved, with monozygotic twin identification\nnear 100% and several cases of unknown family labels, due to errors in the\ngenotyping process, are correctly paired with family members. Software is\nprovided for efficient fine-grained curation of large, generic image datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 21:41:59 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 16:08:31 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chauvin", "Laurent", ""], ["Kumar", "Kuldeep", ""], ["Desrosiers", "Christian", ""], ["Wells", "William", "III"], ["Toews", "Matthew", ""]]}, {"id": "2103.06982", "submitter": "Ben Saunders", "authors": "Ben Saunders, Necati Cihan Camgoz, Richard Bowden", "title": "Continuous 3D Multi-Channel Sign Language Production via Progressive\n  Transformers and Mixture Density Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign languages are multi-channel visual languages, where signers use a\ncontinuous 3D space to communicate.Sign Language Production (SLP), the\nautomatic translation from spoken to sign languages, must embody both the\ncontinuous articulation and full morphology of sign to be truly understandable\nby the Deaf community. Previous deep learning-based SLP works have produced\nonly a concatenation of isolated signs focusing primarily on the manual\nfeatures, leading to a robotic and non-expressive production.\n  In this work, we propose a novel Progressive Transformer architecture, the\nfirst SLP model to translate from spoken language sentences to continuous 3D\nmulti-channel sign pose sequences in an end-to-end manner. Our transformer\nnetwork architecture introduces a counter decoding that enables variable length\ncontinuous sequence generation by tracking the production progress over time\nand predicting the end of sequence. We present extensive data augmentation\ntechniques to reduce prediction drift, alongside an adversarial training regime\nand a Mixture Density Network (MDN) formulation to produce realistic and\nexpressive sign pose sequences.\n  We propose a back translation evaluation mechanism for SLP, presenting\nbenchmark quantitative results on the challenging PHOENIX14T dataset and\nsetting baselines for future research. We further provide a user evaluation of\nour SLP model, to understand the Deaf reception of our sign pose productions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 22:11:17 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Saunders", "Ben", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2103.06997", "submitter": "Scott Burns", "authors": "Scott A. Burns", "title": "The Location of Optimal Object Colors with More Than Two Transitions\n  (Preprint)", "comments": "5/14/21 version adds notice of acceptance for publication and changes\n  made in final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The chromaticity diagram associated with the CIE 1931 color matching\nfunctions is shown to be slightly non-convex. While having no impact on\npractical colorimetric computations, the non-convexity does have a significant\nimpact on the shape of some optimal object color reflectance distributions\nassociated with the outer surface of the object color solid. Instead of the\nusual two-transition Schrodinger form, many optimal colors exhibit higher\ntransition counts. A linear programming formulation is developed and is used to\nlocate where these higher-transition optimal object colors reside on the object\ncolor solid surface. The regions of higher transition count appear to have a\npoint-symmetric complementary structure. The final peer-reviewed version (to\nappear) contains additional material concerning convexification of the\ncolor-matching functions and and additional analysis of modern\n\"physiologically-relevant\" CMFs transformed from cone fundamentals.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 23:14:01 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 09:01:58 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 13:42:41 GMT"}, {"version": "v4", "created": "Sat, 20 Mar 2021 18:20:05 GMT"}, {"version": "v5", "created": "Tue, 23 Mar 2021 13:05:53 GMT"}, {"version": "v6", "created": "Fri, 26 Mar 2021 11:04:01 GMT"}, {"version": "v7", "created": "Fri, 14 May 2021 15:57:48 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Burns", "Scott A.", ""]]}, {"id": "2103.06999", "submitter": "Qinwen Deng", "authors": "Qinwen Deng, Songyang Zhang and Zhi Ding", "title": "An Efficient Hypergraph Approach to Robust Point Cloud Resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient processing and feature extraction of largescale point clouds are\nimportant in related computer vision and cyber-physical systems. This work\ninvestigates point cloud resampling based on hypergraph signal processing\n(HGSP) to better explore the underlying relationship among different cloud\npoints and to extract contour-enhanced features. Specifically, we design\nhypergraph spectral filters to capture multi-lateral interactions among the\nsignal nodes of point clouds and to better preserve their surface outlines.\nWithout the need and the computation to first construct the underlying\nhypergraph, our low complexity approach directly estimates hypergraph spectrum\nof point clouds by leveraging hypergraph stationary processes from the observed\n3D coordinates. Evaluating the proposed resampling methods with several\nmetrics, our test results validate the high efficacy of hypergraph\ncharacterization of point clouds and demonstrate the robustness of\nhypergraph-based resampling under noisy observations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 23:19:54 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Deng", "Qinwen", ""], ["Zhang", "Songyang", ""], ["Ding", "Zhi", ""]]}, {"id": "2103.07013", "submitter": "Brennan Shacklett", "authors": "Brennan Shacklett, Erik Wijmans, Aleksei Petrenko, Manolis Savva,\n  Dhruv Batra, Vladlen Koltun, Kayvon Fatahalian", "title": "Large Batch Simulation for Deep Reinforcement Learning", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We accelerate deep reinforcement learning-based training in visually complex\n3D environments by two orders of magnitude over prior work, realizing\nend-to-end training speeds of over 19,000 frames of experience per second on a\nsingle GPU and up to 72,000 frames per second on a single eight-GPU machine.\nThe key idea of our approach is to design a 3D renderer and embodied navigation\nsimulator around the principle of \"batch simulation\": accepting and executing\nlarge batches of requests simultaneously. Beyond exposing large amounts of work\nat once, batch simulation allows implementations to amortize in-memory storage\nof scene assets, rendering work, data loading, and synchronization costs across\nmany simulation requests, dramatically improving the number of simulated agents\nper GPU and overall simulation throughput. To balance DNN inference and\ntraining costs with faster simulation, we also build a computationally\nefficient policy DNN that maintains high task performance, and modify training\nalgorithms to maintain sample efficiency when training with large mini-batches.\nBy combining batch simulation and DNN performance optimizations, we demonstrate\nthat PointGoal navigation agents can be trained in complex 3D environments on a\nsingle GPU in 1.5 days to 97% of the accuracy of agents trained on a prior\nstate-of-the-art system using a 64-GPU cluster over three days. We provide\nopen-source reference implementations of our batch 3D renderer and simulator to\nfacilitate incorporation of these ideas into RL systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:22:50 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Shacklett", "Brennan", ""], ["Wijmans", "Erik", ""], ["Petrenko", "Aleksei", ""], ["Savva", "Manolis", ""], ["Batra", "Dhruv", ""], ["Koltun", "Vladlen", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "2103.07017", "submitter": "Noranart Vesdapunt", "authors": "Noranart Vesdapunt, Baoyuan Wang", "title": "CRFace: Confidence Ranker for Model-Agnostic Face Detection Refinement", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is a fundamental problem for many downstream face\napplications, and there is a rising demand for faster, more accurate yet\nsupport for higher resolution face detectors. Recent smartphones can record a\nvideo in 8K resolution, but many of the existing face detectors still fail due\nto the anchor size and training data. We analyze the failure cases and observe\na large number of correct predicted boxes with incorrect confidences. To\ncalibrate these confidences, we propose a confidence ranking network with a\npairwise ranking loss to re-rank the predicted confidences locally within the\nsame image. Our confidence ranker is model-agnostic, so we can augment the data\nby choosing the pairs from multiple face detectors during the training, and\ngeneralize to a wide range of face detectors during the testing. On WiderFace,\nwe achieve the highest AP on the single-scale, and our AP is competitive with\nthe previous multi-scale methods while being significantly faster. On 8K\nresolution, our method solves the GPU memory issue and allows us to indirectly\ntrain on 8K. We collect 8K resolution test set to show the improvement, and we\nwill release our test set as a new benchmark for future research.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:50:26 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Vesdapunt", "Noranart", ""], ["Wang", "Baoyuan", ""]]}, {"id": "2103.07018", "submitter": "Pengtao Xie", "authors": "Hao Ban, Pengtao Xie", "title": "Interleaving Learning, with Application to Neural Architecture Search", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.04863", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interleaving learning is a human learning technique where a learner\ninterleaves the studies of multiple topics, which increases long-term retention\nand improves ability to transfer learned knowledge. Inspired by the\ninterleaving learning technique of humans, in this paper we explore whether\nthis learning methodology is beneficial for improving the performance of\nmachine learning models as well. We propose a novel machine learning framework\nreferred to as interleaving learning (IL). In our framework, a set of models\ncollaboratively learn a data encoder in an interleaving fashion: the encoder is\ntrained by model 1 for a while, then passed to model 2 for further training,\nthen model 3, and so on; after trained by all models, the encoder returns back\nto model 1 and is trained again, then moving to model 2, 3, etc. This process\nrepeats for multiple rounds. Our framework is based on multi-level optimization\nconsisting of multiple inter-connected learning stages. An efficient\ngradient-based algorithm is developed to solve the multi-level optimization\nproblem. We apply interleaving learning to search neural architectures for\nimage classification on CIFAR-10, CIFAR-100, and ImageNet. The effectiveness of\nour method is strongly demonstrated by the experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:54:22 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ban", "Hao", ""], ["Xie", "Pengtao", ""]]}, {"id": "2103.07051", "submitter": "Kaihao Zhang", "authors": "Kaihao Zhang, Dongxu Li, Wenhan Luo, Wenqi Ren, Lin Ma, Hongdong Li", "title": "Dual Attention-in-Attention Model for Joint Rain Streak and Raindrop\n  Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streaks and rain drops are two natural phenomena, which degrade image\ncapture in different ways. Currently, most existing deep deraining networks\ntake them as two distinct problems and individually address one, and thus\ncannot deal adequately with both simultaneously. To address this, we propose a\nDual Attention-in-Attention Model (DAiAM) which includes two DAMs for removing\nboth rain streaks and raindrops. Inside the DAM, there are two attentive maps -\neach of which attends to the heavy and light rainy regions, respectively, to\nguide the deraining process differently for applicable regions. In addition, to\nfurther refine the result, a Differential-driven Dual Attention-in-Attention\nModel (D-DAiAM) is proposed with a \"heavy-to-light\" scheme to remove rain via\naddressing the unsatisfying deraining regions. Extensive experiments on one\npublic raindrop dataset, one public rain streak and our synthesized joint rain\nstreak and raindrop (JRSRD) dataset have demonstrated that the proposed method\nnot only is capable of removing rain streaks and raindrops simultaneously, but\nalso achieves the state-of-the-art performance on both tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:00:33 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhang", "Kaihao", ""], ["Li", "Dongxu", ""], ["Luo", "Wenhan", ""], ["Ren", "Wenqi", ""], ["Ma", "Lin", ""], ["Li", "Hongdong", ""]]}, {"id": "2103.07054", "submitter": "Wei Chen", "authors": "Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, Linlin Shen, Ales\n  Leonardis", "title": "FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose\n  Estimation with Decoupled Rotation Mechanism", "comments": "accepted by CVPR2021, oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on category-level 6D pose and size estimation from\nmonocular RGB-D image. Previous methods suffer from inefficient category-level\npose feature extraction which leads to low accuracy and inference speed. To\ntackle this problem, we propose a fast shape-based network (FS-Net) with\nefficient category-level feature extraction for 6D pose estimation. First, we\ndesign an orientation aware autoencoder with 3D graph convolution for latent\nfeature extraction. The learned latent feature is insensitive to point shift\nand object size thanks to the shift and scale-invariance properties of the 3D\ngraph convolution. Then, to efficiently decode category-level rotation\ninformation from the latent feature, we propose a novel decoupled rotation\nmechanism that employs two decoders to complementarily access the rotation\ninformation. Meanwhile, we estimate translation and size by two residuals,\nwhich are the difference between the mean of object points and ground truth\ntranslation, and the difference between the mean size of the category and\nground truth size, respectively. Finally, to increase the generalization\nability of FS-Net, we propose an online box-cage based 3D deformation mechanism\nto augment the training data. Extensive experiments on two benchmark datasets\nshow that the proposed method achieves state-of-the-art performance in both\ncategory- and instance-level 6D object pose estimation. Especially in\ncategory-level pose estimation, without extra synthetic data, our method\noutperforms existing methods by 6.3% on the NOCS-REAL dataset.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:07:24 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 09:50:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Wei", ""], ["Jia", "Xi", ""], ["Chang", "Hyung Jin", ""], ["Duan", "Jinming", ""], ["Shen", "Linlin", ""], ["Leonardis", "Ales", ""]]}, {"id": "2103.07055", "submitter": "Jong Chul Ye", "authors": "Sangjoon Park, Gwanghyun Kim, Yujin Oh, Joon Beom Seo, Sang Min Lee,\n  Jin Hwan Kim, Sungjun Moon, Jae-Kwang Lim, Jong Chul Ye", "title": "Vision Transformer for COVID-19 CXR Diagnosis using Chest X-ray Feature\n  Corpus", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the global COVID-19 crisis, developing robust diagnosis algorithm for\nCOVID-19 using CXR is hampered by the lack of the well-curated COVID-19 data\nset, although CXR data with other disease are abundant. This situation is\nsuitable for vision transformer architecture that can exploit the abundant\nunlabeled data using pre-training. However, the direct use of existing vision\ntransformer that uses the corpus generated by the ResNet is not optimal for\ncorrect feature embedding. To mitigate this problem, we propose a novel vision\nTransformer by using the low-level CXR feature corpus that are obtained to\nextract the abnormal CXR features. Specifically, the backbone network is\ntrained using large public datasets to obtain the abnormal features in routine\ndiagnosis such as consolidation, glass-grass opacity (GGO), etc. Then, the\nembedded features from the backbone network are used as corpus for vision\ntransformer training. We examine our model on various external test datasets\nacquired from totally different institutions to assess the generalization\nability. Our experiments demonstrate that our method achieved the state-of-art\nperformance and has better generalization capability, which are crucial for a\nwidespread deployment.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:07:27 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Park", "Sangjoon", ""], ["Kim", "Gwanghyun", ""], ["Oh", "Yujin", ""], ["Seo", "Joon Beom", ""], ["Lee", "Sang Min", ""], ["Kim", "Jin Hwan", ""], ["Moon", "Sungjun", ""], ["Lim", "Jae-Kwang", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2103.07062", "submitter": "Jong Chul Ye", "authors": "Gwanghyun Kim, Sangjoon Park, Yujin Oh, Joon Beom Seo, Sang Min Lee,\n  Jin Hwan Kim, Sungjun Moon, Jae-Kwang Lim, Jong Chul Ye", "title": "Severity Quantification and Lesion Localization of COVID-19 on CXR using\n  Vision Transformer", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the global pandemic of COVID-19, building an automated framework that\nquantifies the severity of COVID-19 and localizes the relevant lesion on chest\nX-ray images has become increasingly important. Although pixel-level lesion\nseverity labels, e.g. lesion segmentation, can be the most excellent target to\nbuild a robust model, collecting enough data with such labels is difficult due\nto time and labor-intensive annotation tasks. Instead, array-based severity\nlabeling that assigns integer scores on six subdivisions of lungs can be an\nalternative choice enabling the quick labeling. Several groups proposed deep\nlearning algorithms that quantify the severity of COVID-19 using the\narray-based COVID-19 labels and localize the lesions with explainability maps.\nTo further improve the accuracy and interpretability, here we propose a novel\nVision Transformer tailored for both quantification of the severity and\nclinically applicable localization of the COVID-19 related lesions. Our model\nis trained in a weakly-supervised manner to generate the full probability maps\nfrom weak array-based labels. Furthermore, a novel progressive self-training\nmethod enables us to build a model with a small labeled dataset. The\nquantitative and qualitative analysis on the external testset demonstrates that\nour method shows comparable performance with radiologists for both tasks with\nstability in a real-world application.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:17:19 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kim", "Gwanghyun", ""], ["Park", "Sangjoon", ""], ["Oh", "Yujin", ""], ["Seo", "Joon Beom", ""], ["Lee", "Sang Min", ""], ["Kim", "Jin Hwan", ""], ["Moon", "Sungjun", ""], ["Lim", "Jae-Kwang", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2103.07065", "submitter": "Binghan Li", "authors": "Binghan Li, Yindong Hua, Mi Lu", "title": "Advanced Multiple Linear Regression Based Dark Channel Prior Applied on\n  Dehazing Image and Generating Synthetic Haze", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Haze removal is an extremely challenging task, and object detection in the\nhazy environment has recently gained much attention due to the popularity of\nautonomous driving and traffic surveillance. In this work, the authors propose\na multiple linear regression haze removal model based on a widely adopted\ndehazing algorithm named Dark Channel Prior. Training this model with a\nsynthetic hazy dataset, the proposed model can reduce the unanticipated\ndeviations generated from the rough estimations of transmission map and\natmospheric light in Dark Channel Prior. To increase object detection accuracy\nin the hazy environment, the authors further present an algorithm to build a\nsynthetic hazy COCO training dataset by generating the artificial haze to the\nMS COCO training dataset. The experimental results demonstrate that the\nproposed model obtains higher image quality and shares more similarity with\nground truth images than most conventional pixel-based dehazing algorithms and\nneural network based haze-removal models. The authors also evaluate the mean\naverage precision of Mask R-CNN when training the network with synthetic hazy\nCOCO training dataset and preprocessing test hazy dataset by removing the haze\nwith the proposed dehazing model. It turns out that both approaches can\nincrease the object detection accuracy significantly and outperform most\nexisting object detection models over hazy images.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:32:08 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Li", "Binghan", ""], ["Hua", "Yindong", ""], ["Lu", "Mi", ""]]}, {"id": "2103.07073", "submitter": "Bo Liu Dr", "authors": "Bo Liu, Ming Ding, Hanyu Xue, Tianqing Zhu, Dayong Ye, Li Song, Wanlei\n  Zhou", "title": "DP-Image: Differential Privacy for Image Data in Feature Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The excessive use of images in social networks, government databases, and\nindustrial applications has posed great privacy risks and raised serious\nconcerns from the public. Even though differential privacy (DP) is a widely\naccepted criterion that can provide a provable privacy guarantee, the\napplication of DP on unstructured data such as images is not trivial due to the\nlack of a clear qualification on the meaningful difference between any two\nimages. In this paper, for the first time, we introduce a novel notion of\nimage-aware differential privacy, referred to as DP-image, that can protect\nuser's personal information in images, from both human and AI adversaries. The\nDP-Image definition is formulated as an extended version of traditional\ndifferential privacy, considering the distance measurements between feature\nspace vectors of images. Then we propose a mechanism to achieve DP-Image by\nadding noise to an image feature vector. Finally, we conduct experiments with a\ncase study on face image privacy. Our results show that the proposed DP-Image\nmethod provides excellent DP protection on images, with a controllable\ndistortion to faces.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 04:02:23 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Liu", "Bo", ""], ["Ding", "Ming", ""], ["Xue", "Hanyu", ""], ["Zhu", "Tianqing", ""], ["Ye", "Dayong", ""], ["Song", "Li", ""], ["Zhou", "Wanlei", ""]]}, {"id": "2103.07074", "submitter": "Shi Qiu", "authors": "Shi Qiu, Saeed Anwar and Nick Barnes", "title": "Semantic Segmentation for Real Point Cloud Scenes via Bilateral\n  Augmentation and Adaptive Fusion", "comments": "Accepted in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the prominence of current 3D sensors, a fine-grained analysis on the\nbasic point cloud data is worthy of further investigation. Particularly, real\npoint cloud scenes can intuitively capture complex surroundings in the real\nworld, but due to 3D data's raw nature, it is very challenging for machine\nperception. In this work, we concentrate on the essential visual task, semantic\nsegmentation, for large-scale point cloud data collected in reality. On the one\nhand, to reduce the ambiguity in nearby points, we augment their local context\nby fully utilizing both geometric and semantic features in a bilateral\nstructure. On the other hand, we comprehensively interpret the distinctness of\nthe points from multiple resolutions and represent the feature map following an\nadaptive fusion method at point-level for accurate semantic segmentation.\nFurther, we provide specific ablation studies and intuitive visualizations to\nvalidate our key modules. By comparing with state-of-the-art networks on three\ndifferent benchmarks, we demonstrate the effectiveness of our network.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 04:13:20 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 04:38:15 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Qiu", "Shi", ""], ["Anwar", "Saeed", ""], ["Barnes", "Nick", ""]]}, {"id": "2103.07087", "submitter": "Felipe Gutierrez-Barragan", "authors": "Felipe Gutierrez-Barragan, Huaijin Chen, Mohit Gupta, Andreas Velten,\n  Jinwei Gu", "title": "iToF2dToF: A Robust and Flexible Representation for Data-Driven\n  Time-of-Flight Imaging", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect Time-of-Flight (iToF) cameras are a promising depth sensing\ntechnology. However, they are prone to errors caused by multi-path interference\n(MPI) and low signal-to-noise ratio (SNR). Traditional methods, after\ndenoising, mitigate MPI by estimating a transient image that encodes depths.\nRecently, data-driven methods that jointly denoise and mitigate MPI have become\nstate-of-the-art without using the intermediate transient representation. In\nthis paper, we propose to revisit the transient representation. Using\ndata-driven priors, we interpolate/extrapolate iToF frequencies and use them to\nestimate the transient image. Given direct ToF (dToF) sensors capture transient\nimages, we name our method iToF2dToF. The transient representation is flexible.\nIt can be integrated with different rule-based depth sensing algorithms that\nare robust to low SNR and can deal with ambiguous scenarios that arise in\npractice (e.g., specular MPI, optical cross-talk). We demonstrate the benefits\nof iToF2dToF over previous methods in real depth sensing scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 04:57:52 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 04:48:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gutierrez-Barragan", "Felipe", ""], ["Chen", "Huaijin", ""], ["Gupta", "Mohit", ""], ["Velten", "Andreas", ""], ["Gu", "Jinwei", ""]]}, {"id": "2103.07094", "submitter": "Hengli Wang", "authors": "Hengli Wang, Rui Fan, Peide Cai, Ming Liu", "title": "PVStereo: Pyramid Voting Module for End-to-End Self-Supervised Stereo\n  Matching", "comments": "8 pages, 8 figures and 2 tables. This paper is accepted by IEEE RA-L\n  with ICRA 2021", "journal-ref": null, "doi": "10.1109/LRA.2021.3068108", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning with deep convolutional neural networks (DCNNs) has seen\nhuge adoption in stereo matching. However, the acquisition of large-scale\ndatasets with well-labeled ground truth is cumbersome and labor-intensive,\nmaking supervised learning-based approaches often hard to implement in\npractice. To overcome this drawback, we propose a robust and effective\nself-supervised stereo matching approach, consisting of a pyramid voting module\n(PVM) and a novel DCNN architecture, referred to as OptStereo. Specifically,\nour OptStereo first builds multi-scale cost volumes, and then adopts a\nrecurrent unit to iteratively update disparity estimations at high resolution;\nwhile our PVM can generate reliable semi-dense disparity images, which can be\nemployed to supervise OptStereo training. Furthermore, we publish the\nHKUST-Drive dataset, a large-scale synthetic stereo dataset, collected under\ndifferent illumination and weather conditions for research purposes. Extensive\nexperimental results demonstrate the effectiveness and efficiency of our\nself-supervised stereo matching approach on the KITTI Stereo benchmarks and our\nHKUST-Drive dataset. PVStereo, our best-performing implementation, greatly\noutperforms all other state-of-the-art self-supervised stereo matching\napproaches. Our project page is available at sites.google.com/view/pvstereo.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 05:27:14 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Hengli", ""], ["Fan", "Rui", ""], ["Cai", "Peide", ""], ["Liu", "Ming", ""]]}, {"id": "2103.07131", "submitter": "Jianhui Chang", "authors": "Jianhui Chang, Zhenghui Zhao, Lingbo Yang, Chuanmin Jia, Jian Zhang,\n  Siwei Ma", "title": "Thousand to One: Semantic Prior Modeling for Conceptual Coding", "comments": "ICME 2021 ORAL accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conceptual coding has been an emerging research topic recently, which encodes\nnatural images into disentangled conceptual representations for compression.\nHowever, the compression performance of the existing methods is still\nsub-optimal due to the lack of comprehensive consideration of rate constraint\nand reconstruction quality. To this end, we propose a novel end-to-end semantic\nprior modeling-based conceptual coding scheme towards extremely low bitrate\nimage compression, which leverages semantic-wise deep representations as a\nunified prior for entropy estimation and texture synthesis. Specifically, we\nemploy semantic segmentation maps as structural guidance for extracting deep\nsemantic prior, which provides fine-grained texture distribution modeling for\nbetter detail construction and higher flexibility in subsequent high-level\nvision tasks. Moreover, a cross-channel entropy model is proposed to further\nexploit the inter-channel correlation of the spatially independent semantic\nprior, leading to more accurate entropy estimation for rate-constrained\ntraining. The proposed scheme achieves an ultra-high 1000x compression ratio,\nwhile still enjoying high visual reconstruction quality and versatility towards\nvisual processing and analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 08:02:07 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 02:57:07 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chang", "Jianhui", ""], ["Zhao", "Zhenghui", ""], ["Yang", "Lingbo", ""], ["Jia", "Chuanmin", ""], ["Zhang", "Jian", ""], ["Ma", "Siwei", ""]]}, {"id": "2103.07138", "submitter": "Yudong Wang", "authors": "Yudong Wang, Jichang Guo, Huan Gao, Huihui Yue", "title": "UIEC^2-Net: CNN-based Underwater Image Enhancement Using Two Color Space", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Underwater image enhancement has attracted much attention due to the rise of\nmarine resource development in recent years. Benefit from the powerful\nrepresentation capabilities of Convolution Neural Networks(CNNs), multiple\nunderwater image enhancement algorithms based on CNNs have been proposed in the\nlast few years. However, almost all of these algorithms employ RGB color space\nsetting, which is insensitive to image properties such as luminance and\nsaturation. To address this problem, we proposed Underwater Image Enhancement\nConvolution Neural Network using 2 Color Space (UICE^2-Net) that efficiently\nand effectively integrate both RGB Color Space and HSV Color Space in one\nsingle CNN. To our best knowledge, this method is the first to use HSV color\nspace for underwater image enhancement based on deep learning. UIEC^2-Net is an\nend-to-end trainable network, consisting of three blocks as follow: a RGB\npixel-level block implements fundamental operations such as denoising and\nremoving color cast, a HSV global-adjust block for globally adjusting\nunderwater image luminance, color and saturation by adopting a novel neural\ncurve layer, and an attention map block for combining the advantages of RGB and\nHSV block output images by distributing weight to each pixel. Experimental\nresults on synthetic and real-world underwater images show the good performance\nof our proposed method in both subjective comparisons and objective metrics.\nThe code are available at https://github.com/BIGWangYuDong/UWEnhancement.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 08:23:21 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 04:21:44 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wang", "Yudong", ""], ["Guo", "Jichang", ""], ["Gao", "Huan", ""], ["Yue", "Huihui", ""]]}, {"id": "2103.07152", "submitter": "Tao Huang", "authors": "Tao Huang, Weisheng Dong, Xin Yuan, Jinjian Wu, Guangming Shi", "title": "Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging", "comments": "10 pages, 8 figures, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In coded aperture snapshot spectral imaging (CASSI) system, the real-world\nhyperspectral image (HSI) can be reconstructed from the captured compressive\nimage in a snapshot. Model-based HSI reconstruction methods employed\nhand-crafted priors to solve the reconstruction problem, but most of which\nachieved limited success due to the poor representation capability of these\nhand-crafted priors. Deep learning based methods learning the mappings between\nthe compressive images and the HSIs directly achieved much better results. Yet,\nit is nontrivial to design a powerful deep network heuristically for achieving\nsatisfied results. In this paper, we propose a novel HSI reconstruction method\nbased on the Maximum a Posterior (MAP) estimation framework using learned\nGaussian Scale Mixture (GSM) prior. Different from existing GSM models using\nhand-crafted scale priors (e.g., the Jeffrey's prior), we propose to learn the\nscale prior through a deep convolutional neural network (DCNN). Furthermore, we\nalso propose to estimate the local means of the GSM models by the DCNN. All the\nparameters of the MAP estimation algorithm and the DCNN parameters are jointly\noptimized through end-to-end training. Extensive experimental results on both\nsynthetic and real datasets demonstrate that the proposed method outperforms\nexisting state-of-the-art methods. The code is available at\nhttps://see.xidian.edu.cn/faculty/wsdong/Projects/DGSM-SCI.htm.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 08:57:06 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 08:24:38 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Huang", "Tao", ""], ["Dong", "Weisheng", ""], ["Yuan", "Xin", ""], ["Wu", "Jinjian", ""], ["Shi", "Guangming", ""]]}, {"id": "2103.07153", "submitter": "Hugo Germain", "authors": "Hugo Germain and Vincent Lepetit and Guillaume Bourmaud", "title": "Neural Reprojection Error: Merging Feature Learning and Camera Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Absolute camera pose estimation is usually addressed by sequentially solving\ntwo distinct subproblems: First a feature matching problem that seeks to\nestablish putative 2D-3D correspondences, and then a Perspective-n-Point\nproblem that minimizes, with respect to the camera pose, the sum of so-called\nReprojection Errors (RE). We argue that generating putative 2D-3D\ncorrespondences 1) leads to an important loss of information that needs to be\ncompensated as far as possible, within RE, through the choice of a robust loss\nand the tuning of its hyperparameters and 2) may lead to an RE that conveys\nerroneous data to the pose estimator. In this paper, we introduce the Neural\nReprojection Error (NRE) as a substitute for RE. NRE allows to rethink the\ncamera pose estimation problem by merging it with the feature learning problem,\nhence leveraging richer information than 2D-3D correspondences and eliminating\nthe need for choosing a robust loss and its hyperparameters. Thus NRE can be\nused as training loss to learn image descriptors tailored for pose estimation.\nWe also propose a coarse-to-fine optimization method able to very efficiently\nminimize a sum of NRE terms with respect to the camera pose. We experimentally\ndemonstrate that NRE is a good substitute for RE as it significantly improves\nboth the robustness and the accuracy of the camera pose estimate while being\ncomputationally and memory highly efficient. From a broader point of view, we\nbelieve this new way of merging deep learning and 3D geometry may be useful in\nother computer vision applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 09:00:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Germain", "Hugo", ""], ["Lepetit", "Vincent", ""], ["Bourmaud", "Guillaume", ""]]}, {"id": "2103.07156", "submitter": "Kohei Yamamoto", "authors": "Kohei Yamamoto", "title": "Learnable Companding Quantization for Accurate Low-bit Neural Networks", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantizing deep neural networks is an effective method for reducing memory\nconsumption and improving inference speed, and is thus useful for\nimplementation in resource-constrained devices. However, it is still hard for\nextremely low-bit models to achieve accuracy comparable with that of\nfull-precision models. To address this issue, we propose learnable companding\nquantization (LCQ) as a novel non-uniform quantization method for 2-, 3-, and\n4-bit models. LCQ jointly optimizes model weights and learnable companding\nfunctions that can flexibly and non-uniformly control the quantization levels\nof weights and activations. We also present a new weight normalization\ntechnique that allows more stable training for quantization. Experimental\nresults show that LCQ outperforms conventional state-of-the-art methods and\nnarrows the gap between quantized and full-precision models for image\nclassification and object detection tasks. Notably, the 2-bit ResNet-50 model\non ImageNet achieves top-1 accuracy of 75.1% and reduces the gap to 1.7%,\nallowing LCQ to further exploit the potential of non-uniform quantization.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 09:06:52 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Yamamoto", "Kohei", ""]]}, {"id": "2103.07202", "submitter": "Cl\\'ement Rambour", "authors": "Cl\\'ement Rambour, Lo\\\"ic Denis, Florence Tupin, H\\'el\\`ene Oriot, Yue\n  Huang, Laurent Ferro-Famil", "title": "Urban Surface Reconstruction in SAR Tomography by Graph-Cuts", "comments": null, "journal-ref": "Computer Vision and Image Understanding 188 (2019) 102791", "doi": "10.1016/j.cviu.2019.07.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAR (Synthetic Aperture Radar) tomography reconstructs 3-D volumes from\nstacks of SAR images. High-resolution satellites such as TerraSAR-X provide\nimages that can be combined to produce 3-D models. In urban areas, sparsity\npriors are generally enforced during the tomographic inversion process in order\nto retrieve the location of scatterers seen within a given radar resolution\ncell. However, such priors often miss parts of the urban surfaces. Those\nmissing parts are typically regions of flat areas such as ground or rooftops.\nThis paper introduces a surface segmentation algorithm based on the computation\nof the optimal cut in a flow network. This segmentation process can be included\nwithin the 3-D reconstruction framework in order to improve the recovery of\nurban surfaces. Illustrations on a TerraSAR-X tomographic dataset demonstrate\nthe potential of the approach to produce a 3-D model of urban surfaces such as\nground, fa\\c{c}ades and rooftops.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:53:18 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Rambour", "Cl\u00e9ment", ""], ["Denis", "Lo\u00efc", ""], ["Tupin", "Florence", ""], ["Oriot", "H\u00e9l\u00e8ne", ""], ["Huang", "Yue", ""], ["Ferro-Famil", "Laurent", ""]]}, {"id": "2103.07208", "submitter": "Nikolai Kalischek", "authors": "Nikolai Kalischek, Jan Dirk Wegner, Konrad Schindler", "title": "In the light of feature distributions: moment matching for Neural Style\n  Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Style transfer aims to render the content of a given image in the\ngraphical/artistic style of another image. The fundamental concept underlying\nNeuralStyle Transfer (NST) is to interpret style as a distribution in the\nfeature space of a Convolutional Neural Network, such that a desired style can\nbe achieved by matching its feature distribution. We show that most current\nimplementations of that concept have important theoretical and practical\nlimitations, as they only partially align the feature distributions. We propose\na novel approach that matches the distributions more precisely, thus\nreproducing the desired style more faithfully, while still being\ncomputationally efficient. Specifically, we adapt the dual form of Central\nMoment Discrepancy (CMD), as recently proposed for domain adaptation, to\nminimize the difference between the target style and the feature distribution\nof the output image. The dual interpretation of this metric explicitly matches\nall higher-order centralized moments and is therefore a natural extension of\nexisting NST methods that only take into account the first and second moments.\nOur experiments confirm that the strong theoretical properties also translate\nto visually better style transfer, and better disentangle style from semantic\nimage content.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 11:00:44 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kalischek", "Nikolai", ""], ["Wegner", "Jan Dirk", ""], ["Schindler", "Konrad", ""]]}, {"id": "2103.07230", "submitter": "Chaorong Li", "authors": "Chaorong Li, Malu Zhang, Wei Huang, Fengqing Qin, Anping Zeng,\n  Yuanyuan Huang", "title": "Sequential Random Network for Fine-grained Image Classification", "comments": "The performance of the model is very severely affected by the order\n  of the test samples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Convolutional Neural Network (DCNN) and Transformer have achieved\nremarkable successes in image recognition. However, their performance in\nfine-grained image recognition is still difficult to meet the requirements of\nactual needs. This paper proposes a Sequence Random Network (SRN) to enhance\nthe performance of DCNN. The output of DCNN is one-dimensional features. This\none-dimensional feature abstractly represents image information, but it does\nnot express well the detailed information of image. To address this issue, we\nuse the proposed SRN which composed of BiLSTM and several Tanh-Dropout blocks\n(called BiLSTM-TDN), to further process DCNN one-dimensional features for\nhighlighting the detail information of image. After the feature transform by\nBiLSTM-TDN, the recognition performance has been greatly improved. We conducted\nthe experiments on six fine-grained image datasets. Except for FGVC-Aircraft,\nthe accuracies of the proposed methods on the other datasets exceeded 99%.\nExperimental results show that BiLSTM-TDN is far superior to the existing\nstate-of-the-art methods. In addition to DCNN, BiLSTM-TDN can also be extended\nto other models, such as Transformer.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:16:03 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 09:59:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Chaorong", ""], ["Zhang", "Malu", ""], ["Huang", "Wei", ""], ["Qin", "Fengqing", ""], ["Zeng", "Anping", ""], ["Huang", "Yuanyuan", ""]]}, {"id": "2103.07240", "submitter": "Seong Tae Kim", "authors": "Seong Tae Kim, Leili Goli, Magdalini Paschali, Ashkan Khakzar,\n  Matthias Keicher, Tobias Czempiel, Egon Burian, Rickmer Braren, Nassir Navab,\n  Thomas Wendler", "title": "Longitudinal Quantitative Assessment of COVID-19 Infection Progression\n  from Chest CTs", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest computed tomography (CT) has played an essential diagnostic role in\nassessing patients with COVID-19 by showing disease-specific image features\nsuch as ground-glass opacity and consolidation. Image segmentation methods have\nproven to help quantify the disease burden and even help predict the outcome.\nThe availability of longitudinal CT series may also result in an efficient and\neffective method to reliably assess the progression of COVID-19, monitor the\nhealing process and the response to different therapeutic strategies. In this\npaper, we propose a new framework to identify infection at a voxel level\n(identification of healthy lung, consolidation, and ground-glass opacity) and\nvisualize the progression of COVID-19 using sequential low-dose non-contrast CT\nscans. In particular, we devise a longitudinal segmentation network that\nutilizes the reference scan information to improve the performance of disease\nidentification. Experimental results on a clinical longitudinal dataset\ncollected in our institution show the effectiveness of the proposed method\ncompared to the static deep neural networks for disease quantification.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:35:11 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 09:13:28 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Kim", "Seong Tae", ""], ["Goli", "Leili", ""], ["Paschali", "Magdalini", ""], ["Khakzar", "Ashkan", ""], ["Keicher", "Matthias", ""], ["Czempiel", "Tobias", ""], ["Burian", "Egon", ""], ["Braren", "Rickmer", ""], ["Navab", "Nassir", ""], ["Wendler", "Thomas", ""]]}, {"id": "2103.07246", "submitter": "Beomyoung Kim", "authors": "Beomyoung Kim, Sangeun Han, Junmo Kim", "title": "Discriminative Region Suppression for Weakly-Supervised Semantic\n  Segmentation", "comments": "AAAI 2021, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised semantic segmentation (WSSS) using image-level labels has\nrecently attracted much attention for reducing annotation costs. Existing WSSS\nmethods utilize localization maps from the classification network to generate\npseudo segmentation labels. However, since localization maps obtained from the\nclassifier focus only on sparse discriminative object regions, it is difficult\nto generate high-quality segmentation labels. To address this issue, we\nintroduce discriminative region suppression (DRS) module that is a simple yet\neffective method to expand object activation regions. DRS suppresses the\nattention on discriminative regions and spreads it to adjacent\nnon-discriminative regions, generating dense localization maps. DRS requires\nfew or no additional parameters and can be plugged into any network.\nFurthermore, we introduce an additional learning strategy to give a\nself-enhancement of localization maps, named localization map refinement\nlearning. Benefiting from this refinement learning, localization maps are\nrefined and enhanced by recovering some missing parts or removing noise itself.\nDue to its simplicity and effectiveness, our approach achieves mIoU 71.4% on\nthe PASCAL VOC 2012 segmentation benchmark using only image-level labels.\nExtensive experiments demonstrate the effectiveness of our approach. The code\nis available at https://github.com/qjadud1994/DRS.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:56:06 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 09:09:41 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kim", "Beomyoung", ""], ["Han", "Sangeun", ""], ["Kim", "Junmo", ""]]}, {"id": "2103.07254", "submitter": "Runyang Feng", "authors": "Zhenguang Liu, Haoming Chen, Runyang Feng, Shuang Wu, Shouling Ji,\n  Bailin Yang, Xun Wang", "title": "Deep Dual Consecutive Network for Human Pose Estimation", "comments": "This paper is accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-frame human pose estimation in complicated situations is challenging.\nAlthough state-of-the-art human joints detectors have demonstrated remarkable\nresults for static images, their performances come short when we apply these\nmodels to video sequences. Prevalent shortcomings include the failure to handle\nmotion blur, video defocus, or pose occlusions, arising from the inability in\ncapturing the temporal dependency among video frames. On the other hand,\ndirectly employing conventional recurrent neural networks incurs empirical\ndifficulties in modeling spatial contexts, especially for dealing with pose\nocclusions. In this paper, we propose a novel multi-frame human pose estimation\nframework, leveraging abundant temporal cues between video frames to facilitate\nkeypoint detection. Three modular components are designed in our framework. A\nPose Temporal Merger encodes keypoint spatiotemporal context to generate\neffective searching scopes while a Pose Residual Fusion module computes\nweighted pose residuals in dual directions. These are then processed via our\nPose Correction Network for efficient refining of pose estimations. Our method\nranks No.1 in the Multi-frame Person Pose Estimation Challenge on the\nlarge-scale benchmark datasets PoseTrack2017 and PoseTrack2018. We have\nreleased our code, hoping to inspire future research.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:11:27 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 02:24:12 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 11:49:02 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Zhenguang", ""], ["Chen", "Haoming", ""], ["Feng", "Runyang", ""], ["Wu", "Shuang", ""], ["Ji", "Shouling", ""], ["Yang", "Bailin", ""], ["Wang", "Xun", ""]]}, {"id": "2103.07262", "submitter": "Mikkel Fly Kragh", "authors": "J{\\o}rgen Berntsen, Jens Rimestad, Jacob Theilgaard Lassen, Dang Tran,\n  Mikkel Fly Kragh", "title": "Robust and generalizable embryo selection based on artificial\n  intelligence and time-lapse image sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Assessing and selecting the most viable embryos for transfer is an essential\npart of in vitro fertilization (IVF). In recent years, several approaches have\nbeen made to improve and automate the procedure using artificial intelligence\n(AI) and deep learning. Based on images of embryos with known implantation data\n(KID), AI models have been trained to automatically score embryos related to\ntheir chance of achieving a successful implantation. However, as of now, only\nlimited research has been conducted to evaluate how embryo selection models\ngeneralize to new clinics and how they perform in subgroup analyses across\nvarious conditions. In this paper, we investigate how a deep learning-based\nembryo selection model using only time-lapse image sequences performs across\ndifferent patient ages and clinical conditions, and how it correlates with\ntraditional morphokinetic parameters. The model was trained and evaluated based\non a large dataset from 18 IVF centers consisting of 115,832 embryos, of which\n14,644 embryos were transferred KID embryos. In an independent test set, the AI\nmodel sorted KID embryos with an area under the curve (AUC) of a receiver\noperating characteristic curve of 0.67 and all embryos with an AUC of 0.95. A\nclinic hold-out test showed that the model generalized to new clinics with an\nAUC range of 0.60-0.75 for KID embryos. Across different subgroups of age,\ninsemination method, incubation time, and transfer protocol, the AUC ranged\nbetween 0.63 and 0.69. Furthermore, model predictions correlated positively\nwith blastocyst grading and negatively with direct cleavages. The fully\nautomated iDAScore v1.0 model was shown to perform at least as good as a\nstate-of-the-art manual embryo selection model. Moreover, full automatization\nof embryo scoring implies fewer manual evaluations and eliminates biases due to\ninter- and intraobserver variation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:36:30 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Berntsen", "J\u00f8rgen", ""], ["Rimestad", "Jens", ""], ["Lassen", "Jacob Theilgaard", ""], ["Tran", "Dang", ""], ["Kragh", "Mikkel Fly", ""]]}, {"id": "2103.07278", "submitter": "Julien Despois", "authors": "Hugo Thimonier, Julien Despois, Robin Kips, Matthieu Perrot", "title": "Learning Long-Term Style-Preserving Blind Video Temporal Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When trying to independently apply image-trained algorithms to successive\nframes in videos, noxious flickering tends to appear. State-of-the-art\npost-processing techniques that aim at fostering temporal consistency, generate\nother temporal artifacts and visually alter the style of videos. We propose a\npostprocessing model, agnostic to the transformation applied to videos (e.g.\nstyle transfer, image manipulation using GANs, etc.), in the form of a\nrecurrent neural network. Our model is trained using a Ping Pong procedure and\nits corresponding loss, recently introduced for GAN video generation, as well\nas a novel style preserving perceptual loss. The former improves long-term\ntemporal consistency learning, while the latter fosters style preservation. We\nevaluate our model on the DAVIS and videvo.net datasets and show that our\napproach offers state-of-the-art results concerning flicker removal, and better\nkeeps the overall style of the videos than previous approaches.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:54:34 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Thimonier", "Hugo", ""], ["Despois", "Julien", ""], ["Kips", "Robin", ""], ["Perrot", "Matthieu", ""]]}, {"id": "2103.07279", "submitter": "Thomas Wendler", "authors": "Christina Bukas, Bailiang Jian, Luis F. Rodriguez Venegas, Francesca\n  De Benetti, Sebastian Ruehling, Anjany Sekuboyina, Jens Gempt, Jan S.\n  Kirschke, Marie Piraud, Johannes Oberreuter, Nassir Navab and Thomas Wendler", "title": "Patient-specific virtual spine straightening and vertebra inpainting: An\n  automatic framework for osteoplasty planning", "comments": "7 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symptomatic spinal vertebral compression fractures (VCFs) often require\nosteoplasty treatment. A cement-like material is injected into the bone to\nstabilize the fracture, restore the vertebral body height and alleviate pain.\nLeakage is a common complication and may occur due to too much cement being\ninjected. In this work, we propose an automated patient-specific framework that\ncan allow physicians to calculate an upper bound of cement for the injection\nand estimate the optimal outcome of osteoplasty. The framework uses the patient\nCT scan and the fractured vertebra label to build a virtual healthy spine using\na high-level approach. Firstly, the fractured spine is segmented with a\nthree-step Convolution Neural Network (CNN) architecture. Next, a per-vertebra\nrigid registration to a healthy spine atlas restores its curvature. Finally, a\nGAN-based inpainting approach replaces the fractured vertebra with an\nestimation of its original shape. Based on this outcome, we then estimate the\nmaximum amount of bone cement for injection. We evaluate our framework by\ncomparing the virtual vertebrae volumes of ten patients to their healthy\nequivalent and report an average error of 3.88$\\pm$7.63\\%. The presented\npipeline offers a first approach to a personalized automatic high-level\nframework for planning osteoplasty procedures.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:55:08 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 17:42:23 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Bukas", "Christina", ""], ["Jian", "Bailiang", ""], ["Venegas", "Luis F. Rodriguez", ""], ["De Benetti", "Francesca", ""], ["Ruehling", "Sebastian", ""], ["Sekuboyina", "Anjany", ""], ["Gempt", "Jens", ""], ["Kirschke", "Jan S.", ""], ["Piraud", "Marie", ""], ["Oberreuter", "Johannes", ""], ["Navab", "Nassir", ""], ["Wendler", "Thomas", ""]]}, {"id": "2103.07289", "submitter": "Wei-Ta Chu", "authors": "Sian-Yao Huang and Wei-Ta Chu", "title": "Searching by Generating: Flexible and Efficient One-Shot NAS with\n  Architecture Generator", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In one-shot NAS, sub-networks need to be searched from the supernet to meet\ndifferent hardware constraints. However, the search cost is high and $N$ times\nof searches are needed for $N$ different constraints. In this work, we propose\na novel search strategy called architecture generator to search sub-networks by\ngenerating them, so that the search process can be much more efficient and\nflexible. With the trained architecture generator, given target hardware\nconstraints as the input, $N$ good architectures can be generated for $N$\nconstraints by just one forward pass without re-searching and supernet\nretraining. Moreover, we propose a novel single-path supernet, called unified\nsupernet, to further improve search efficiency and reduce GPU memory\nconsumption of the architecture generator. With the architecture generator and\nthe unified supernet, we propose a flexible and efficient one-shot NAS\nframework, called Searching by Generating NAS (SGNAS). With the pre-trained\nsupernt, the search time of SGNAS for $N$ different hardware constraints is\nonly 5 GPU hours, which is $4N$ times faster than previous SOTA single-path\nmethods. After training from scratch, the top1-accuracy of SGNAS on ImageNet is\n77.1%, which is comparable with the SOTAs. The code is available at:\nhttps://github.com/eric8607242/SGNAS.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 14:04:50 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Huang", "Sian-Yao", ""], ["Chu", "Wei-Ta", ""]]}, {"id": "2103.07292", "submitter": "Matthew Vowels", "authors": "Matthew J. Vowels, Necati Cihan Camgoz and Richard Bowden", "title": "VDSM: Unsupervised Video Disentanglement with State-Space Modeling and\n  Deep Mixtures of Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangled representations support a range of downstream tasks including\ncausal reasoning, generative modeling, and fair machine learning.\nUnfortunately, disentanglement has been shown to be impossible without the\nincorporation of supervision or inductive bias. Given that supervision is often\nexpensive or infeasible to acquire, we choose to incorporate structural\ninductive bias and present an unsupervised, deep State-Space-Model for Video\nDisentanglement (VDSM). The model disentangles latent time-varying and dynamic\nfactors via the incorporation of hierarchical structure with a dynamic prior\nand a Mixture of Experts decoder. VDSM learns separate disentangled\nrepresentations for the identity of the object or person in the video, and for\nthe action being performed. We evaluate VDSM across a range of qualitative and\nquantitative tasks including identity and dynamics transfer, sequence\ngeneration, Fr\\'echet Inception Distance, and factor classification. VDSM\nprovides state-of-the-art performance and exceeds adversarial methods, even\nwhen the methods use additional supervision.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 14:05:35 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 18:55:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Vowels", "Matthew J.", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2103.07293", "submitter": "Peisong Wen", "authors": "Peisong Wen, Qianqian Xu, Yangbangyan Jiang, Zhiyong Yang, Yuan He and\n  Qingming Huang", "title": "Seeking the Shape of Sound: An Adaptive Framework for Learning\n  Voice-Face Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, we have witnessed the early progress on learning the association\nbetween voice and face automatically, which brings a new wave of studies to the\ncomputer vision community. However, most of the prior arts along this line (a)\nmerely adopt local information to perform modality alignment and (b) ignore the\ndiversity of learning difficulty across different subjects. In this paper, we\npropose a novel framework to jointly address the above-mentioned issues.\nTargeting at (a), we propose a two-level modality alignment loss where both\nglobal and local information are considered. Compared with the existing\nmethods, we introduce a global loss into the modality alignment process. The\nglobal component of the loss is driven by the identity classification.\nTheoretically, we show that minimizing the loss could maximize the distance\nbetween embeddings across different identities while minimizing the distance\nbetween embeddings belonging to the same identity, in a global sense (instead\nof a mini-batch). Targeting at (b), we propose a dynamic reweighting scheme to\nbetter explore the hard but valuable identities while filtering out the\nunlearnable identities. Experiments show that the proposed method outperforms\nthe previous methods in multiple settings, including voice-face matching,\nverification and retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 14:10:48 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wen", "Peisong", ""], ["Xu", "Qianqian", ""], ["Jiang", "Yangbangyan", ""], ["Yang", "Zhiyong", ""], ["He", "Yuan", ""], ["Huang", "Qingming", ""]]}, {"id": "2103.07302", "submitter": "Jingchao Peng", "authors": "Peng Jingchao, Zhao Haitao, Hu Zhengwei, Zhuang Yi, Wang Bofan", "title": "Siamese Infrared and Visible Light Fusion Network for RGB-T Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the different photosensitive properties of infrared and visible light,\nthe registered RGB-T image pairs shot in the same scene exhibit quite different\ncharacteristics. This paper proposes a siamese infrared and visible light\nfusion Network (SiamIVFN) for RBG-T image-based tracking. SiamIVFN contains two\nmain subnetworks: a complementary-feature-fusion network (CFFN) and a\ncontribution-aggregation network (CAN). CFFN utilizes a two-stream multilayer\nconvolutional structure whose filters for each layer are partially coupled to\nfuse the features extracted from infrared images and visible light images. CFFN\nis a feature-level fusion network, which can cope with the misalignment of the\nRGB-T image pairs. Through adaptively calculating the contributions of infrared\nand visible light features obtained from CFFN, CAN makes the tracker robust\nunder various light conditions. Experiments on two RGB-T tracking benchmark\ndatasets demonstrate that the proposed SiamIVFN has achieved state-of-the-art\nperformance. The tracking speed of SiamIVFN is 147.6FPS, the current fastest\nRGB-T fusion tracker.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 14:20:41 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Jingchao", "Peng", ""], ["Haitao", "Zhao", ""], ["Zhengwei", "Hu", ""], ["Yi", "Zhuang", ""], ["Bofan", "Wang", ""]]}, {"id": "2103.07348", "submitter": "Dominik Laupheimer", "authors": "Dominik Laupheimer and Norbert Haala", "title": "Juggling With Representations: On the Information Transfer Between\n  Imagery, Point Clouds, and Meshes for Multi-Modal Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The automatic semantic segmentation of the huge amount of acquired remote\nsensing data has become an important task in the last decade. Images and Point\nClouds (PCs) are fundamental data representations, particularly in urban\nmapping applications. Textured 3D meshes integrate both data representations\ngeometrically by wiring the PC and texturing the surface elements with\navailable imagery. We present a mesh-centered holistic geometry-driven\nmethodology that explicitly integrates entities of imagery, PC and mesh. Due to\nits integrative character, we choose the mesh as the core representation that\nalso helps to solve the visibility problem for points in imagery. Utilizing the\nproposed multi-modal fusion as the backbone and considering the established\nentity relationships, we enable the sharing of information across the\nmodalities imagery, PC and mesh in a two-fold manner: (i) feature transfer and\n(ii) label transfer. By these means, we achieve to enrich feature vectors to\nmulti-modal feature vectors for each representation. Concurrently, we achieve\nto label all representations consistently while reducing the manual label\neffort to a single representation. Consequently, we facilitate to train machine\nlearning algorithms and to semantically segment any of these data\nrepresentations - both in a multi-modal and single-modal sense. The paper\npresents the association mechanism and the subsequent information transfer,\nwhich we believe are cornerstones for multi-modal scene analysis. Furthermore,\nwe discuss the preconditions and limitations of the presented approach in\ndetail. We demonstrate the effectiveness of our methodology on the ISPRS 3D\nsemantic labeling contest (Vaihingen 3D) and a proprietary data set (Hessigheim\n3D).\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:26:30 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Laupheimer", "Dominik", ""], ["Haala", "Norbert", ""]]}, {"id": "2103.07350", "submitter": "Vicen Fan", "authors": "Zhenyan Hou, Wenxuan Fan", "title": "A New Training Framework for Deep Neural Network", "comments": "Withdraw this paper for internal review. Because we were not familiar\n  with the use of arXiv, our initial manuscript was uploaded by mistake and we\n  found many inappropriate and unmodified parts of it. I am sorry to say that\n  this work still needs to be further completed and we do not intend to use it\n  for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is the process of transferring the knowledge from a\nlarge model to a small model. In this process, the small model learns the\ngeneralization ability of the large model and retains the performance close to\nthat of the large model. Knowledge distillation provides a training means to\nmigrate the knowledge of models, facilitating model deployment and speeding up\ninference. However, previous distillation methods require pre-trained teacher\nmodels, which still bring computational and storage overheads. In this paper, a\nnovel general training framework called Self Distillation (SD) is proposed. We\ndemonstrate the effectiveness of our method by enumerating its performance\nimprovements in diverse tasks and benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:29:00 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 17:10:13 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 14:59:05 GMT"}, {"version": "v4", "created": "Wed, 24 Mar 2021 07:01:15 GMT"}, {"version": "v5", "created": "Thu, 25 Mar 2021 01:51:00 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Hou", "Zhenyan", ""], ["Fan", "Wenxuan", ""]]}, {"id": "2103.07351", "submitter": "Hou-Ning Hu", "authors": "Hou-Ning Hu, Yung-Hsu Yang, Tobias Fischer, Trevor Darrell, Fisher Yu,\n  Min Sun", "title": "Monocular Quasi-Dense 3D Object Tracking", "comments": "24 pages, 14 figures, including appendix (6 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reliable and accurate 3D tracking framework is essential for predicting\nfuture locations of surrounding objects and planning the observer's actions in\nnumerous applications such as autonomous driving. We propose a framework that\ncan effectively associate moving objects over time and estimate their full 3D\nbounding box information from a sequence of 2D images captured on a moving\nplatform. The object association leverages quasi-dense similarity learning to\nidentify objects in various poses and viewpoints with appearance cues only.\nAfter initial 2D association, we further utilize 3D bounding boxes\ndepth-ordering heuristics for robust instance association and motion-based 3D\ntrajectory prediction for re-identification of occluded vehicles. In the end,\nan LSTM-based object velocity learning module aggregates the long-term\ntrajectory information for more accurate motion extrapolation. Experiments on\nour proposed simulation data and real-world benchmarks, including KITTI,\nnuScenes, and Waymo datasets, show that our tracking framework offers robust\nobject association and tracking on urban-driving scenarios. On the Waymo Open\nbenchmark, we establish the first camera-only baseline in the 3D tracking and\n3D detection challenges. Our quasi-dense 3D tracking pipeline achieves\nimpressive improvements on the nuScenes 3D tracking benchmark with near five\ntimes tracking accuracy of the best vision-only submission among all published\nmethods. Our code, data and trained models are available at\nhttps://github.com/SysCV/qd-3dt.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:30:02 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Hu", "Hou-Ning", ""], ["Yang", "Yung-Hsu", ""], ["Fischer", "Tobias", ""], ["Darrell", "Trevor", ""], ["Yu", "Fisher", ""], ["Sun", "Min", ""]]}, {"id": "2103.07362", "submitter": "Juan Luis Gonzalez Bello", "authors": "Juan Luis Gonzalez Bello, Munchurl Kim", "title": "PLADE-Net: Towards Pixel-Level Accuracy for Self-Supervised Single-View\n  Depth Estimation with Neural Positional Encoding and Distilled Matting Loss", "comments": "Accepted paper (poster) at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose a self-supervised single-view pixel-level accurate\ndepth estimation network, called PLADE-Net. The PLADE-Net is the first work\nthat shows unprecedented accuracy levels, exceeding 95\\% in terms of the\n$\\delta^1$ metric on the challenging KITTI dataset. Our PLADE-Net is based on a\nnew network architecture with neural positional encoding and a novel loss\nfunction that borrows from the closed-form solution of the matting Laplacian to\nlearn pixel-level accurate depth estimation from stereo images. Neural\npositional encoding allows our PLADE-Net to obtain more consistent depth\nestimates by letting the network reason about location-specific image\nproperties such as lens and projection distortions. Our novel distilled matting\nLaplacian loss allows our network to predict sharp depths at object boundaries\nand more consistent depths in highly homogeneous regions. Our proposed method\noutperforms all previous self-supervised single-view depth estimation methods\nby a large margin on the challenging KITTI dataset, with unprecedented levels\nof accuracy. Furthermore, our PLADE-Net, naively extended for stereo inputs,\noutperforms the most recent self-supervised stereo methods, even without any\nadvanced blocks like 1D correlations, 3D convolutions, or spatial pyramid\npooling. We present extensive ablation studies and experiments that support our\nmethod's effectiveness on the KITTI, CityScapes, and Make3D datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:54:46 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Bello", "Juan Luis Gonzalez", ""], ["Kim", "Munchurl", ""]]}, {"id": "2103.07364", "submitter": "Quanshi Zhang", "authors": "Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Xu Cheng, Xin\n  Wang, Yiting Chen, Jie Shi, Quanshi Zhang", "title": "Game-theoretic Understanding of Adversarially Learned Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to understand adversarial attacks and defense from a new\nperspecitve, i.e., the signal-processing behavior of DNNs. We novelly define\nthe multi-order interaction in game theory, which satisfies six properties.\nWith the multi-order interaction, we discover that adversarial attacks mainly\naffect high-order interactions to fool the DNN. Furthermore, we find that the\nrobustness of adversarially trained DNNs comes from category-specific low-order\ninteractions. Our findings provide more insights into and make a revision of\nprevious understanding for the shape bias of adversarially learned features.\nBesides, the multi-order interaction can also explain the recoverability of\nadversarial examples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:56:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ren", "Jie", ""], ["Zhang", "Die", ""], ["Wang", "Yisen", ""], ["Chen", "Lu", ""], ["Zhou", "Zhanpeng", ""], ["Cheng", "Xu", ""], ["Wang", "Xin", ""], ["Chen", "Yiting", ""], ["Shi", "Jie", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2103.07368", "submitter": "Foivos Ntelemis", "authors": "Foivos Ntelemis, Yaochu Jin, Spencer A. Thomas", "title": "Information Maximization Clustering via Multi-View Self-Labelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image clustering is a particularly challenging computer vision task, which\naims to generate annotations without human supervision. Recent advances focus\non the use of self-supervised learning strategies in image clustering, by first\nlearning valuable semantics and then clustering the image representations.\nThese multiple-phase algorithms, however, increase the computational time and\ntheir final performance is reliant on the first stage. By extending the\nself-supervised approach, we propose a novel single-phase clustering method\nthat simultaneously learns meaningful representations and assigns the\ncorresponding annotations. This is achieved by integrating a discrete\nrepresentation into the self-supervised paradigm through a classifier net.\nSpecifically, the proposed clustering objective employs mutual information, and\nmaximizes the dependency between the integrated discrete representation and a\ndiscrete probability distribution. The discrete probability distribution is\nderived though the self-supervised process by comparing the learnt latent\nrepresentation with a set of trainable prototypes. To enhance the learning\nperformance of the classifier, we jointly apply the mutual information across\nmulti-crop views. Our empirical results show that the proposed framework\noutperforms state-of-the-art techniques with the average accuracy of 89.1% and\n49.0%, respectively, on CIFAR-10 and CIFAR-100/20 datasets. Finally, the\nproposed method also demonstrates attractive robustness to parameter settings,\nmaking it ready to be applicable to other datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 16:04:41 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ntelemis", "Foivos", ""], ["Jin", "Yaochu", ""], ["Thomas", "Spencer A.", ""]]}, {"id": "2103.07371", "submitter": "Huizi Mao", "authors": "Huizi Mao, Sibo Zhu, Song Han, William J. Dally", "title": "PatchNet -- Short-range Template Matching for Efficient Video Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Object recognition is a fundamental problem in many video processing tasks,\naccurately locating seen objects at low computation cost paves the way for\non-device video recognition. We propose PatchNet, an efficient convolutional\nneural network to match objects in adjacent video frames. It learns the\npatchwise correlation features instead of pixel features. PatchNet is very\ncompact, running at just 58MFLOPs, $5\\times$ simpler than MobileNetV2. We\ndemonstrate its application on two tasks, video object detection and visual\nobject tracking. On ImageNet VID, PatchNet reduces the flops of R-FCN\nResNet-101 by 5x and EfficientDet-D0 by 3.4x with less than 1% mAP loss. On\nOTB2015, PatchNet reduces SiamFC and SiamRPN by 2.5x with no accuracy loss.\nExperiments on Jetson Nano further demonstrate 2.8x to 4.3x speed-ups\nassociated with flops reduction. Code is open sourced at\nhttps://github.com/RalphMao/PatchNet.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:56:07 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Mao", "Huizi", ""], ["Zhu", "Sibo", ""], ["Han", "Song", ""], ["Dally", "William J.", ""]]}, {"id": "2103.07372", "submitter": "Zhengwei Wang", "authors": "Zhengwei Wang, Qi She, Aljosa Smolic", "title": "ACTION-Net: Multipath Excitation for Action Recognition", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial-temporal, channel-wise, and motion patterns are three complementary\nand crucial types of information for video action recognition. Conventional 2D\nCNNs are computationally cheap but cannot catch temporal relationships; 3D CNNs\ncan achieve good performance but are computationally intensive. In this work,\nwe tackle this dilemma by designing a generic and effective module that can be\nembedded into 2D CNNs. To this end, we propose a spAtio-temporal, Channel and\nmoTion excitatION (ACTION) module consisting of three paths: Spatio-Temporal\nExcitation (STE) path, Channel Excitation (CE) path, and Motion Excitation (ME)\npath. The STE path employs one channel 3D convolution to characterize\nspatio-temporal representation. The CE path adaptively recalibrates\nchannel-wise feature responses by explicitly modeling interdependencies between\nchannels in terms of the temporal aspect. The ME path calculates feature-level\ntemporal differences, which is then utilized to excite motion-sensitive\nchannels. We equip 2D CNNs with the proposed ACTION module to form a simple yet\neffective ACTION-Net with very limited extra computational cost. ACTION-Net is\ndemonstrated by consistently outperforming 2D CNN counterparts on three\nbackbones (i.e., ResNet-50, MobileNet V2 and BNInception) employing three\ndatasets (i.e., Something-Something V2, Jester, and EgoGesture). Codes are\navailable at \\url{https://github.com/V-Sense/ACTION-Net}.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:23:40 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wang", "Zhengwei", ""], ["She", "Qi", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2103.07414", "submitter": "Haoyin Zhou", "authors": "Haoyin Zhou, Jagadeesan Jayender", "title": "Real-time Nonrigid Mosaicking of Laparoscopy Images", "comments": "published on IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2021.3065030", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The ability to extend the field of view of laparoscopy images can help the\nsurgeons to obtain a better understanding of the anatomical context. However,\ndue to tissue deformation, complex camera motion and significant\nthree-dimensional (3D) anatomical surface, image pixels may have non-rigid\ndeformation and traditional mosaicking methods cannot work robustly for\nlaparoscopy images in real-time. To solve this problem, a novel two-dimensional\n(2D) non-rigid simultaneous localization and mapping (SLAM) system is proposed\nin this paper, which is able to compensate for the deformation of pixels and\nperform image mosaicking in real-time. The key algorithm of this 2D non-rigid\nSLAM system is the expectation maximization and dual quaternion (EMDQ)\nalgorithm, which can generate smooth and dense deformation field from sparse\nand noisy image feature matches in real-time. An uncertainty-based loop closing\nmethod has been proposed to reduce the accumulative errors. To achieve\nreal-time performance, both CPU and GPU parallel computation technologies are\nused for dense mosaicking of all pixels. Experimental results on \\textit{in\nvivo} and synthetic data demonstrate the feasibility and accuracy of our\nnon-rigid mosaicking method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:18:27 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhou", "Haoyin", ""], ["Jayender", "Jagadeesan", ""]]}, {"id": "2103.07423", "submitter": "Marwa Ismail", "authors": "Marwa Ismail, Prateek Prasanna, Kaustav Bera, Volodymyr Statsevych,\n  Virginia Hill, Gagandeep Singh, Sasan Partovi, Niha Beig, Sean McGarry, Peter\n  Laviolette, Manmeet Ahluwalia, Anant Madabhushi, and Pallavi Tiwari", "title": "Radiomic Deformation and Textural Heterogeneity (R-DepTH) Descriptor to\n  characterize Tumor Field Effect: Application to Survival Prediction in\n  Glioblastoma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concept of tumor field effect implies that cancer is a systemic disease\nwith its impact way beyond the visible tumor confines. For instance, in\nGlioblastoma (GBM), an aggressive brain tumor, the increase in intracranial\npressure due to tumor burden often leads to brain herniation and poor outcomes.\nOur work is based on the rationale that highly aggressive tumors tend to grow\nuncontrollably, leading to pronounced biomechanical tissue deformations in the\nnormal parenchyma, which when combined with local morphological differences in\nthe tumor confines on MRI scans, will comprehensively capture tumor field\neffect. Specifically, we present an integrated MRI-based descriptor,\nradiomic-Deformation and Textural Heterogeneity (r-DepTH). This descriptor\ncomprises measurements of the subtle perturbations in tissue deformations\nthroughout the surrounding normal parenchyma due to mass effect. This involves\nnon-rigidly aligning the patients MRI scans to a healthy atlas via\ndiffeomorphic registration. The resulting inverse mapping is used to obtain the\ndeformation field magnitudes in the normal parenchyma. These measurements are\nthen combined with a 3D texture descriptor, Co-occurrence of Local Anisotropic\nGradient Orientations (COLLAGE), which captures the morphological heterogeneity\nwithin the tumor confines, on MRI scans. R-DepTH, on N = 207 GBM cases\n(training set (St) = 128, testing set (Sv) = 79), demonstrated improved\nprognosis of overall survival by categorizing patients into low- (prolonged\nsurvival) and high-risk (poor survival) groups (on St, p-value = 0.0000035, and\non Sv, p-value = 0.0024). R-DepTH descriptor may serve as a comprehensive\nMRI-based prognostic marker of disease aggressiveness and survival in solid\ntumors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:38:54 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ismail", "Marwa", ""], ["Prasanna", "Prateek", ""], ["Bera", "Kaustav", ""], ["Statsevych", "Volodymyr", ""], ["Hill", "Virginia", ""], ["Singh", "Gagandeep", ""], ["Partovi", "Sasan", ""], ["Beig", "Niha", ""], ["McGarry", "Sean", ""], ["Laviolette", "Peter", ""], ["Ahluwalia", "Manmeet", ""], ["Madabhushi", "Anant", ""], ["Tiwari", "Pallavi", ""]]}, {"id": "2103.07437", "submitter": "Lina Zhuang", "authors": "Lina Zhuang, Lianru Gao, Bing Zhang, Xiyou Fu, Jose M. Bioucas-Dias", "title": "Hyperspectral Image Denoising and Anomaly Detection Based on Low-rank\n  and Sparse Representations", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.3040221", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging measures the amount of electromagnetic energy across\nthe instantaneous field of view at a very high resolution in hundreds or\nthousands of spectral channels. This enables objects to be detected and the\nidentification of materials that have subtle differences between them. However,\nthe increase in spectral resolution often means that there is a decrease in the\nnumber of photons received in each channel, which means that the noise linked\nto the image formation process is greater. This degradation limits the quality\nof the extracted information and its potential applications. Thus, denoising is\na fundamental problem in hyperspectral image (HSI) processing. As images of\nnatural scenes with highly correlated spectral channels, HSIs are characterized\nby a high level of self-similarity and can be well approximated by low-rank\nrepresentations. These characteristics underlie the state-of-the-art methods\nused in HSI denoising. However, where there are rarely occurring pixel types,\nthe denoising performance of these methods is not optimal, and the subsequent\ndetection of these pixels may be compromised. To address these hurdles, in this\narticle, we introduce RhyDe (Robust hyperspectral Denoising), a powerful HSI\ndenoiser, which implements explicit low-rank representation, promotes\nself-similarity, and, by using a form of collaborative sparsity, preserves rare\npixels. The denoising and detection effectiveness of the proposed robust HSI\ndenoiser is illustrated using semireal and real data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:07:27 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhuang", "Lina", ""], ["Gao", "Lianru", ""], ["Zhang", "Bing", ""], ["Fu", "Xiyou", ""], ["Bioucas-Dias", "Jose M.", ""]]}, {"id": "2103.07458", "submitter": "Petros Boufounos", "authors": "Yanting Ma, Petros T. Boufounos, Hassan Mansour, Shuchin Aeron", "title": "Multiview Sensing With Unknown Permutations: An Optimal Transport\n  Approach", "comments": "5 pages, 3 figures, ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG eess.IV eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several applications, including imaging of deformable objects while in\nmotion, simultaneous localization and mapping, and unlabeled sensing, we\nencounter the problem of recovering a signal that is measured subject to\nunknown permutations. In this paper we take a fresh look at this problem\nthrough the lens of optimal transport (OT). In particular, we recognize that in\nmost practical applications the unknown permutations are not arbitrary but some\nare more likely to occur than others. We exploit this by introducing a\nregularization function that promotes the more likely permutations in the\nsolution. We show that, even though the general problem is not convex, an\nappropriate relaxation of the resulting regularized problem allows us to\nexploit the well-developed machinery of OT and develop a tractable algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:48:18 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ma", "Yanting", ""], ["Boufounos", "Petros T.", ""], ["Mansour", "Hassan", ""], ["Aeron", "Shuchin", ""]]}, {"id": "2103.07461", "submitter": "Xingyi Zhou", "authors": "Xingyi Zhou, Vladlen Koltun, Philipp Kr\\\"ahenb\\\"uhl", "title": "Probabilistic two-stage detection", "comments": "Code is available at https://github.com/xingyizhou/CenterNet2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a probabilistic interpretation of two-stage object detection. We\nshow that this probabilistic interpretation motivates a number of common\nempirical training practices. It also suggests changes to two-stage detection\npipelines. Specifically, the first stage should infer proper\nobject-vs-background likelihoods, which should then inform the overall score of\nthe detector. A standard region proposal network (RPN) cannot infer this\nlikelihood sufficiently well, but many one-stage detectors can. We show how to\nbuild a probabilistic two-stage detector from any state-of-the-art one-stage\ndetector. The resulting detectors are faster and more accurate than both their\none- and two-stage precursors. Our detector achieves 56.4 mAP on COCO test-dev\nwith single-scale testing, outperforming all published results. Using a\nlightweight backbone, our detector achieves 49.2 mAP on COCO at 33 fps on a\nTitan Xp, outperforming the popular YOLOv4 model.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:56:17 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhou", "Xingyi", ""], ["Koltun", "Vladlen", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2103.07466", "submitter": "Luis Guillermo Rold\\~ao Jimenez", "authors": "Luis Roldao, Raoul de Charette, Anne Verroust-Blondet", "title": "3D Semantic Scene Completion: a Survey", "comments": "Accepted in IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Scene Completion (SSC) aims to jointly estimate the complete\ngeometry and semantics of a scene, assuming partial sparse input. In the last\nyears following the multiplication of large-scale 3D datasets, SSC has gained\nsignificant momentum in the research community because it holds unresolved\nchallenges. Specifically, SSC lies in the ambiguous completion of large\nunobserved areas and the weak supervision signal of the ground truth. This led\nto a substantially increasing number of papers on the matter. This survey aims\nto identify, compare and analyze the techniques providing a critical analysis\nof the SSC literature on both methods and datasets. Throughout the paper, we\nprovide an in-depth analysis of the existing works covering all choices made by\nthe authors while highlighting the remaining avenues of research. SSC\nperformance of the SoA on the most popular datasets is also evaluated and\nanalyzed.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:59:51 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 15:17:43 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 16:06:10 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Roldao", "Luis", ""], ["de Charette", "Raoul", ""], ["Verroust-Blondet", "Anne", ""]]}, {"id": "2103.07503", "submitter": "Masoud Faraki", "authors": "Masoud Faraki, Xiang Yu, Yi-Hsuan Tsai, Yumin Suh, Manmohan Chandraker", "title": "Cross-Domain Similarity Learning for Face Recognition in Unseen Domains", "comments": "Accepted to CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition models trained under the assumption of identical training\nand test distributions often suffer from poor generalization when faced with\nunknown variations, such as a novel ethnicity or unpredictable individual\nmake-ups during test time. In this paper, we introduce a novel cross-domain\nmetric learning loss, which we dub Cross-Domain Triplet (CDT) loss, to improve\nface recognition in unseen domains. The CDT loss encourages learning\nsemantically meaningful features by enforcing compact feature clusters of\nidentities from one domain, where the compactness is measured by underlying\nsimilarity metrics that belong to another training domain with different\nstatistics. Intuitively, it discriminatively correlates explicit metrics\nderived from one domain, with triplet samples from another domain in a unified\nloss function to be minimized within a network, which leads to better alignment\nof the training domains. The network parameters are further enforced to learn\ngeneralized features under domain shift, in a model-agnostic learning pipeline.\nUnlike the recent work of Meta Face Recognition, our method does not require\ncareful hard-pair sample mining and filtering strategy during training.\nExtensive experiments on various face recognition benchmarks show the\nsuperiority of our method in handling variations, compared to baseline and the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 19:48:01 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Faraki", "Masoud", ""], ["Yu", "Xiang", ""], ["Tsai", "Yi-Hsuan", ""], ["Suh", "Yumin", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2103.07531", "submitter": "Fengchun Qiao", "authors": "Fengchun Qiao, Xi Peng", "title": "Uncertainty-guided Model Generalization to Unseen Domains", "comments": "In CVPR 2021 (13 pages including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a worst-case scenario in generalization: Out-of-domain\ngeneralization from a single source. The goal is to learn a robust model from a\nsingle source and expect it to generalize over many unknown distributions. This\nchallenging problem has been seldom investigated while existing solutions\nsuffer from various limitations. In this paper, we propose a new solution. The\nkey idea is to augment the source capacity in both input and label spaces,\nwhile the augmentation is guided by uncertainty assessment. To the best of our\nknowledge, this is the first work to (1) access the generalization uncertainty\nfrom a single source and (2) leverage it to guide both input and label\naugmentation for robust generalization. The model training and deployment are\neffectively organized in a Bayesian meta-learning framework. We conduct\nextensive comparisons and ablation study to validate our approach. The results\nprove our superior performance in a wide scope of tasks including image\nclassification, semantic segmentation, text classification, and speech\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 21:13:21 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Qiao", "Fengchun", ""], ["Peng", "Xi", ""]]}, {"id": "2103.07562", "submitter": "Zeman Shao", "authors": "Zeman Shao, Shaobo Fang, Runyu Mao, Jiangpeng He, Janine Wright,\n  Deborah Kerr, Carol Jo Boushey, Fengqing Zhu", "title": "Towards Learning Food Portion From Monocular Images With Cross-Domain\n  Feature Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to estimate food portion size, a property that is strongly related to\nthe presence of food object in 3D space, from single monocular images under\nreal life setting. Specifically, we are interested in end-to-end estimation of\nfood portion size, which has great potential in the field of personal health\nmanagement. Unlike image segmentation or object recognition where annotation\ncan be obtained through large scale crowd sourcing, it is much more challenging\nto collect datasets for portion size estimation since human cannot accurately\nestimate the size of an object in an arbitrary 2D image without expert\nknowledge. To address such challenge, we introduce a real life food image\ndataset collected from a nutrition study where the groundtruth food energy\n(calorie) is provided by registered dietitians, and will be made available to\nthe research community. We propose a deep regression process for portion size\nestimation by combining features estimated from both RGB and learned energy\ndistribution domains. Our estimates of food energy achieved state-of-the-art\nwith a MAPE of 11.47%, significantly outperforms non-expert human estimates by\n27.56%.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 22:58:37 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shao", "Zeman", ""], ["Fang", "Shaobo", ""], ["Mao", "Runyu", ""], ["He", "Jiangpeng", ""], ["Wright", "Janine", ""], ["Kerr", "Deborah", ""], ["Boushey", "Carol Jo", ""], ["Zhu", "Fengqing", ""]]}, {"id": "2103.07570", "submitter": "Binghan Li", "authors": "Binghan Li, Yindong Hua, Yifeng Liu, Mi Lu", "title": "Dilated Fully Convolutional Neural Network for Depth Estimation from a\n  Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth prediction plays a key role in understanding a 3D scene. Several\ntechniques have been developed throughout the years, among which Convolutional\nNeural Network has recently achieved state-of-the-art performance on estimating\ndepth from a single image. However, traditional CNNs suffer from the lower\nresolution and information loss caused by the pooling layers. And oversized\nparameters generated from fully connected layers often lead to a exploded\nmemory usage problem. In this paper, we present an advanced Dilated Fully\nConvolutional Neural Network to address the deficiencies. Taking advantages of\nthe exponential expansion of the receptive field in dilated convolutions, our\nmodel can minimize the loss of resolution. It also reduces the amount of\nparameters significantly by replacing the fully connected layers with the fully\nconvolutional layers. We show experimentally on NYU Depth V2 datasets that the\ndepth prediction obtained from our model is considerably closer to ground truth\nthan that from traditional CNNs techniques.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 23:19:32 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Li", "Binghan", ""], ["Hua", "Yindong", ""], ["Liu", "Yifeng", ""], ["Lu", "Mi", ""]]}, {"id": "2103.07573", "submitter": "Thaicia Stona De Almeida", "authors": "Thaicia Stona de Almeida", "title": "Mining Artifacts in Mycelium SEM Micrographs", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mycelium is a promising biomaterial based on fungal mycelium, a highly\nporous, nanofibrous structure. Scanning electron micrographs are used to\ncharacterize its network, but the currently available tools for nanofibrous\nmicrostructures do not contemplate the particularities of biomaterials. The\nadoption of a software for artificial nanofibrous in mycelium characterization\nadds the uncertainty of imaging artifact formation to the analysis. The\nreported work combines supervised and unsupervised machine learning methods to\nautomate the identification of artifacts in the mapped pores of mycelium\nmicrostructure.\n  Keywords: Machine learning; unsupervised learning; image processing;\nmycelium; microstructure informatics\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 23:32:06 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["de Almeida", "Thaicia Stona", ""]]}, {"id": "2103.07579", "submitter": "Irwan Bello", "authors": "Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind\n  Srinivas, Tsung-Yi Lin, Jonathon Shlens, Barret Zoph", "title": "Revisiting ResNets: Improved Training and Scaling Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel computer vision architectures monopolize the spotlight, but the impact\nof the model architecture is often conflated with simultaneous changes to\ntraining methodology and scaling strategies. Our work revisits the canonical\nResNet (He et al., 2015) and studies these three aspects in an effort to\ndisentangle them. Perhaps surprisingly, we find that training and scaling\nstrategies may matter more than architectural changes, and further, that the\nresulting ResNets match recent state-of-the-art models. We show that the best\nperforming scaling strategy depends on the training regime and offer two new\nscaling strategies: (1) scale model depth in regimes where overfitting can\noccur (width scaling is preferable otherwise); (2) increase image resolution\nmore slowly than previously recommended (Tan & Le, 2019). Using improved\ntraining and scaling strategies, we design a family of ResNet architectures,\nResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while\nachieving similar accuracies on ImageNet. In a large-scale semi-supervised\nlearning setup, ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being\n4.7x faster than EfficientNet NoisyStudent. The training techniques improve\ntransfer performance on a suite of downstream tasks (rivaling state-of-the-art\nself-supervised algorithms) and extend to video classification on Kinetics-400.\nWe recommend practitioners use these simple revised ResNets as baselines for\nfuture research.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 00:18:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Bello", "Irwan", ""], ["Fedus", "William", ""], ["Du", "Xianzhi", ""], ["Cubuk", "Ekin D.", ""], ["Srinivas", "Aravind", ""], ["Lin", "Tsung-Yi", ""], ["Shlens", "Jonathon", ""], ["Zoph", "Barret", ""]]}, {"id": "2103.07600", "submitter": "Guanzhe Hong", "authors": "Guanzhe Hong, Zhiyuan Mao, Xiaojun Lin, Stanley H. Chan", "title": "Student-Teacher Learning from Clean Inputs to Noisy Inputs", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based student-teacher learning, a training method that encourages the\nstudent's hidden features to mimic those of the teacher network, is empirically\nsuccessful in transferring the knowledge from a pre-trained teacher network to\nthe student network. Furthermore, recent empirical results demonstrate that,\nthe teacher's features can boost the student network's generalization even when\nthe student's input sample is corrupted by noise. However, there is a lack of\ntheoretical insights into why and when this method of transferring knowledge\ncan be successful between such heterogeneous tasks. We analyze this method\ntheoretically using deep linear networks, and experimentally using nonlinear\nnetworks. We identify three vital factors to the success of the method: (1)\nwhether the student is trained to zero training loss; (2) how knowledgeable the\nteacher is on the clean-input problem; (3) how the teacher decomposes its\nknowledge in its hidden features. Lack of proper control in any of the three\nfactors leads to failure of the student-teacher learning method.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 02:29:35 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hong", "Guanzhe", ""], ["Mao", "Zhiyuan", ""], ["Lin", "Xiaojun", ""], ["Chan", "Stanley H.", ""]]}, {"id": "2103.07609", "submitter": "Kristina Monakhova", "authors": "Kristina Monakhova, Vi Tran, Grace Kuo, Laura Waller", "title": "Untrained networks for compressive lensless photography", "comments": "17 pages, 8 figures", "journal-ref": "Optics Express Vol. 29, Issue 13, pp. 20913-20929 (2021)", "doi": "10.1364/OE.424075", "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compressive lensless imagers enable novel applications in an extremely\ncompact device, requiring only a phase or amplitude mask placed close to the\nsensor. They have been demonstrated for 2D and 3D microscopy, single-shot\nvideo, and single-shot hyperspectral imaging; in each of these cases, a\ncompressive-sensing-based inverse problem is solved in order to recover a 3D\ndata-cube from a 2D measurement. Typically, this is accomplished using convex\noptimization and hand-picked priors. Alternatively, deep learning-based\nreconstruction methods offer the promise of better priors, but require many\nthousands of ground truth training pairs, which can be difficult or impossible\nto acquire. In this work, we propose the use of untrained networks for\ncompressive image recovery. Our approach does not require any labeled training\ndata, but instead uses the measurement itself to update the network weights. We\ndemonstrate our untrained approach on lensless compressive 2D imaging as well\nas single-shot high-speed video recovery using the camera's rolling shutter,\nand single-shot hyperspectral imaging. We provide simulation and experimental\nverification, showing that our method results in improved image quality over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 03:47:06 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 01:01:25 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Monakhova", "Kristina", ""], ["Tran", "Vi", ""], ["Kuo", "Grace", ""], ["Waller", "Laura", ""]]}, {"id": "2103.07615", "submitter": "Jiahao Xia", "authors": "Jiahao Xia, Haimin Zhang, Shiping Wen, Shuo Yang and Min Xu", "title": "An Efficient Multitask Neural Network for Face Alignment, Head Pose\n  Estimation and Face Tracking", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While convolutional neural networks (CNNs) have significantly boosted the\nperformance of face related algorithms, maintaining accuracy and efficiency\nsimultaneously in practical use remains challenging. Recent study shows that\nusing a cascade of hourglass modules which consist of a number of bottom-up and\ntop-down convolutional layers can extract facial structural information for\nface alignment to improve accuracy. However, previous studies have shown that\nfeatures produced by shallow convolutional layers are highly correspond to\nedges. These features could be directly used to provide the structural\ninformation without addition cost. Motivated by this intuition, we propose an\nefficient multitask face alignment, face tracking and head pose estimation\nnetwork (ATPN). Specifically, we introduce a shortcut connection between\nshallow-layer features and deep-layer features to provide the structural\ninformation for face alignment and apply the CoordConv to the last few layers\nto provide coordinate information. The predicted facial landmarks enable us to\ngenerate a cheap heatmap which contains both geometric and appearance\ninformation for head pose estimation and it also provides attention clues for\nface tracking. Moreover, the face tracking task saves us the face detection\nprocedure for each frame, which is significant to boost performance for\nvideo-based tasks. The proposed framework is evaluated on four benchmark\ndatasets, WFLW, 300VW, WIDER Face and 300W-LP. The experimental results show\nthat the ATPN achieves improved performance compared to previous\nstate-of-the-art methods while having less number of parameters and FLOPS.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 04:41:15 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Xia", "Jiahao", ""], ["Zhang", "Haimin", ""], ["Wen", "Shiping", ""], ["Yang", "Shuo", ""], ["Xu", "Min", ""]]}, {"id": "2103.07620", "submitter": "Zeyu Jiao", "authors": "Zeyu Jiao, Huan Lei, Hengshan Zong, Yingjie Cai, Zhenyu Zhong", "title": "Potential Escalator-related Injury Identification and Prevention Based\n  on Multi-module Integrated System for Public Health", "comments": "Please excuse me for taking some of your time. But that we have not\n  yet studied our work completely and some new great results are discovered. So\n  after carefully thinking, we are going to rearrange this manuscript and try\n  to give more precise model. Thus, we decided to withdraw this manuscript with\n  great pity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Escalator-related injuries threaten public health with the widespread use of\nescalators. The existing studies tend to focus on after-the-fact statistics,\nreflecting on the original design and use of defects to reduce the impact of\nescalator-related injuries, but few attention has been paid to ongoing and\nimpending injuries. In this study, a multi-module escalator safety monitoring\nsystem based on computer vision is designed and proposed to simultaneously\nmonitor and deal with three major injury triggers, including losing balance,\nnot holding on to handrails and carrying large items. The escalator\nidentification module is utilized to determine the escalator region, namely the\nregion of interest. The passenger monitoring module is leveraged to estimate\nthe passengers' pose to recognize unsafe behaviors on the escalator. The\ndangerous object detection module detects large items that may enter the\nescalator and raises alarms. The processing results of the above three modules\nare summarized in the safety assessment module as the basis for the intelligent\ndecision of the system. The experimental results demonstrate that the proposed\nsystem has good performance and great application potential.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 05:26:18 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 03:39:49 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Jiao", "Zeyu", ""], ["Lei", "Huan", ""], ["Zong", "Hengshan", ""], ["Cai", "Yingjie", ""], ["Zhong", "Zhenyu", ""]]}, {"id": "2103.07622", "submitter": "Jemima Jebaseeli T", "authors": "C. Anand Deva Durai, T Jemima Jebaseeli, Salem Alelyani, Azath\n  Mubharakali", "title": "Early Prediction and Diagnosis of Retinoblastoma Using Deep Learning\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Retinoblastoma is the most prominent childhood primary intraocular malignancy\nthat impacts the vision of children and adults worldwide. In contrasting and\ncomparing with adults it is uveal melanoma. It is an aggressive tumor that can\nfill and destroy the eye and the surrounding structures. Therefore early\ndetection of retinoblastoma in childhood is the key. The major impact of the\nresearch is to identify the tumor cells in the retina. Also is to find out the\nstages of the tumor and its corresponding group. The proposed systems assist\nthe ophthalmologists for accurate prediction and diagnosis of retinoblastoma\ncancer disease at the earliest. The contribution of the proposed approach is to\nsave the life of infants and the grown-up children from vision impairment. The\nproposed methodology consists of three phases namely, preprocessing,\nsegmentation, and classification. Initially, the fundus images are preprocessed\nusing the Liner Predictive Decision based Median Filter (LPDMF). It removes the\nnoise introduced in the image due to illumination while capturing or scanning\nthe eye of the patients. The preprocessed images are segmented using the\nConvolutional Neural Network (CNN) to distinguish the foreground tumor cells\nfrom the background.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 05:37:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Durai", "C. Anand Deva", ""], ["Jebaseeli", "T Jemima", ""], ["Alelyani", "Salem", ""], ["Mubharakali", "Azath", ""]]}, {"id": "2103.07640", "submitter": "Hanieh Naderi", "authors": "Hanieh Naderi and Leili Goli and Shohreh Kasaei", "title": "Generating Unrestricted Adversarial Examples via Three Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to be vulnerable to adversarial examples\ndeliberately constructed to misclassify victim models. As most adversarial\nexamples have restricted their perturbations to $L_{p}$-norm, existing defense\nmethods have focused on these types of perturbations and less attention has\nbeen paid to unrestricted adversarial examples; which can create more realistic\nattacks, able to deceive models without affecting human predictions. To address\nthis problem, the proposed adversarial attack generates an unrestricted\nadversarial example with a limited number of parameters. The attack selects\nthree points on the input image and based on their locations transforms the\nimage into an adversarial example. By limiting the range of movement and\nlocation of these three points and using a discriminatory network, the proposed\nunrestricted adversarial example preserves the image appearance. Experimental\nresults show that the proposed adversarial examples obtain an average success\nrate of 93.5% in terms of human evaluation on the MNIST and SVHN datasets. It\nalso reduces the model accuracy by an average of 73% on six datasets MNIST,\nFMNIST, SVHN, CIFAR10, CIFAR100, and ImageNet. It should be noted that, in the\ncase of attacks, lower accuracy in the victim model denotes a more successful\nattack. The adversarial train of the attack also improves model robustness\nagainst a randomly transformed image.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 07:20:14 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Naderi", "Hanieh", ""], ["Goli", "Leili", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "2103.07641", "submitter": "Hao-Chiang Shao", "authors": "Hao-Chiang Shao, Hsin-Chieh Wang, Weng-Tai Su, and Chia-Wen Lin", "title": "Ensemble Learning with Manifold-Based Data Splitting for Noisy Label\n  Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Label noise in training data can significantly degrade a model's\ngeneralization performance for supervised learning tasks. Here we focus on the\nproblem that noisy labels are primarily mislabeled samples, which tend to be\nconcentrated near decision boundaries, rather than uniformly distributed, and\nwhose features should be equivocal. To address the problem, we propose an\nensemble learning method to correct noisy labels by exploiting the local\nstructures of feature manifolds. Different from typical ensemble strategies\nthat increase the prediction diversity among sub-models via certain loss terms,\nour method trains sub-models on disjoint subsets, each being a union of the\nnearest-neighbors of randomly selected seed samples on the data manifold. As a\nresult, each sub-model can learn a coarse representation of the data manifold\nalong with a corresponding graph. Moreover, only a limited number of sub-models\nwill be affected by locally-concentrated noisy labels. The constructed graphs\nare used to suggest a series of label correction candidates, and accordingly,\nour method derives label correction results by voting down inconsistent\nsuggestions. Our experiments on real-world noisy label datasets demonstrate the\nsuperiority of the proposed method over existing state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 07:24:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shao", "Hao-Chiang", ""], ["Wang", "Hsin-Chieh", ""], ["Su", "Weng-Tai", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "2103.07658", "submitter": "Mallikarjun Byrasandra Ramalinga Reddy", "authors": "Mallikarjun B R, Ayush Tewari, Abdallah Dib, Tim Weyrich, Bernd\n  Bickel, Hans-Peter Seidel, Hanspeter Pfister, Wojciech Matusik, Louis\n  Chevallier, Mohamed Elgharib, Christian Theobalt", "title": "PhotoApp: Photorealistic Appearance Editing of Head Portraits", "comments": "http://gvv.mpi-inf.mpg.de/projects/PhotoApp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic editing of portraits is a challenging task as humans are very\nsensitive to inconsistencies in faces. We present an approach for high-quality\nintuitive editing of the camera viewpoint and scene illumination in a portrait\nimage. This requires our method to capture and control the full reflectance\nfield of the person in the image. Most editing approaches rely on supervised\nlearning using training data captured with setups such as light and camera\nstages. Such datasets are expensive to acquire, not readily available and do\nnot capture all the rich variations of in-the-wild portrait images. In\naddition, most supervised approaches only focus on relighting, and do not allow\ncamera viewpoint editing. Thus, they only capture and control a subset of the\nreflectance field. Recently, portrait editing has been demonstrated by\noperating in the generative model space of StyleGAN. While such approaches do\nnot require direct supervision, there is a significant loss of quality when\ncompared to the supervised approaches. In this paper, we present a method which\nlearns from limited supervised training data. The training images only include\npeople in a fixed neutral expression with eyes closed, without much hair or\nbackground variations. Each person is captured under 150 one-light-at-a-time\nconditions and under 8 camera poses. Instead of training directly in the image\nspace, we design a supervised problem which learns transformations in the\nlatent space of StyleGAN. This combines the best of supervised learning and\ngenerative adversarial modeling. We show that the StyleGAN prior allows for\ngeneralisation to different expressions, hairstyles and backgrounds. This\nproduces high-quality photorealistic results for in-the-wild images and\nsignificantly outperforms existing methods. Our approach can edit the\nillumination and pose simultaneously, and runs at interactive rates.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 08:59:49 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 17:59:43 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["R", "Mallikarjun B", ""], ["Tewari", "Ayush", ""], ["Dib", "Abdallah", ""], ["Weyrich", "Tim", ""], ["Bickel", "Bernd", ""], ["Seidel", "Hans-Peter", ""], ["Pfister", "Hanspeter", ""], ["Matusik", "Wojciech", ""], ["Chevallier", "Louis", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2103.07672", "submitter": "Jingshuai Liu", "authors": "Jingshuai Liu, Mehrdad Yaghoobi", "title": "Fine-grained MRI Reconstruction using Attentive Selection Generative\n  Adversarial Networks", "comments": "5 pages, 2 figures, 1 table, 22 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Compressed sensing (CS) leverages the sparsity prior to provide the\nfoundation for fast magnetic resonance imaging (fastMRI). However, iterative\nsolvers for ill-posed problems hinder their adaption to time-critical\napplications. Moreover, such a prior can be neither rich to capture complicated\nanatomical structures nor applicable to meet the demand of high-fidelity\nreconstructions in modern MRI. Inspired by the state-of-the-art methods in\nimage generation, we propose a novel attention-based deep learning framework to\nprovide high-quality MRI reconstruction. We incorporate large-field contextual\nfeature integration and attention selection in a generative adversarial network\n(GAN) framework. We demonstrate that the proposed model can produce superior\nresults compared to other deep learning-based methods in terms of image\nquality, and relevance to the MRI reconstruction in an extremely low sampling\nrate diet.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 09:58:32 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liu", "Jingshuai", ""], ["Yaghoobi", "Mehrdad", ""]]}, {"id": "2103.07676", "submitter": "Noushath Shaffi Dr", "authors": "Noushath Shaffi, Faizal Hajamohideen", "title": "uTHCD: A New Benchmarking for Tamil Handwritten OCR", "comments": "30 pages, 18 figures, in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3096823", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handwritten character recognition is a challenging research in the field of\ndocument image analysis over many decades due to numerous reasons such as large\nwriting styles variation, inherent noise in data, expansive applications it\noffers, non-availability of benchmark databases etc. There has been\nconsiderable work reported in literature about creation of the database for\nseveral Indic scripts but the Tamil script is still in its infancy as it has\nbeen reported only in one database [5]. In this paper, we present the work done\nin the creation of an exhaustive and large unconstrained Tamil Handwritten\nCharacter Database (uTHCD). Database consists of around 91000 samples with\nnearly 600 samples in each of 156 classes. The database is a unified collection\nof both online and offline samples. Offline samples were collected by asking\nvolunteers to write samples on a form inside a specified grid. For online\nsamples, we made the volunteers write in a similar grid using a digital writing\npad. The samples collected encompass a vast variety of writing styles, inherent\ndistortions arising from offline scanning process viz stroke discontinuity,\nvariable thickness of stroke, distortion etc. Algorithms which are resilient to\nsuch data can be practically deployed for real time applications. The samples\nwere generated from around 650 native Tamil volunteers including school going\nkids, homemakers, university students and faculty. The isolated character\ndatabase will be made publicly available as raw images and Hierarchical Data\nFile (HDF) compressed file. With this database, we expect to set a new\nbenchmark in Tamil handwritten character recognition and serve as a launchpad\nfor many avenues in document image analysis domain. Paper also presents an\nideal experimental set-up using the database on convolutional neural networks\n(CNN) with a baseline accuracy of 88% on test data.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:34:08 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Shaffi", "Noushath", ""], ["Hajamohideen", "Faizal", ""]]}, {"id": "2103.07678", "submitter": "Ehsan Farahbakhsh", "authors": "Hojat Shirmard, Ehsan Farahbakhsh, Dietmar Muller, Rohitash Chandra", "title": "A review of machine learning in processing remote sensing data for\n  mineral exploration", "comments": "74 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a primary step in mineral exploration, a variety of features are mapped\nsuch as lithological units, alteration types, structures, and minerals. These\nfeatures are extracted to aid decision-making in targeting ore deposits.\nDifferent types of remote sensing data including satellite optical and radar,\nairborne, and drone-based data make it possible to overcome problems associated\nwith mapping these important parameters on the field. The rapid increase in the\nvolume of remote sensing data obtained from different platforms has allowed\nscientists to develop advanced, innovative, and powerful data processing\nmethodologies. Machine learning methods can help in processing a wide range of\nremote sensing data and in determining the relationship between the reflectance\ncontinuum and features of interest. Moreover, these methods are robust in\nprocessing spectral and ground truth measurements against noise and\nuncertainties. In recent years, many studies have been carried out by\nsupplementing geological surveys with remote sensing data, and this area is now\nconsidered a hotspot in geoscience research. This paper reviews the\nimplementation and adaptation of some popular and recently established machine\nlearning methods for remote sensing data processing and investigates their\napplications for exploring different ore deposits. Lastly, the challenges and\nfuture directions in this critical interdisciplinary field are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:36:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shirmard", "Hojat", ""], ["Farahbakhsh", "Ehsan", ""], ["Muller", "Dietmar", ""], ["Chandra", "Rohitash", ""]]}, {"id": "2103.07679", "submitter": "Yun-Hsuan Liu", "authors": "Ke-Jyun Wang, Yun-Hsuan Liu, Hung-Ting Su, Jen-Wei Wang, Yu-Siang\n  Wang, Winston H. Hsu, Wen-Chin Chen", "title": "OCID-Ref: A 3D Robotic Dataset with Embodied Language for Clutter Scene\n  Grounding", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To effectively apply robots in working environments and assist humans, it is\nessential to develop and evaluate how visual grounding (VG) can affect machine\nperformance on occluded objects. However, current VG works are limited in\nworking environments, such as offices and warehouses, where objects are usually\noccluded due to space utilization issues. In our work, we propose a novel\nOCID-Ref dataset featuring a referring expression segmentation task with\nreferring expressions of occluded objects. OCID-Ref consists of 305,694\nreferring expressions from 2,300 scenes with providing RGB image and point\ncloud inputs. To resolve challenging occlusion issues, we argue that it's\ncrucial to take advantage of both 2D and 3D signals to resolve challenging\nocclusion issues. Our experimental results demonstrate the effectiveness of\naggregating 2D and 3D signals but referring to occluded objects still remains\nchallenging for the modern visual grounding systems. OCID-Ref is publicly\navailable at https://github.com/lluma/OCID-Ref\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:38:15 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 09:03:46 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wang", "Ke-Jyun", ""], ["Liu", "Yun-Hsuan", ""], ["Su", "Hung-Ting", ""], ["Wang", "Jen-Wei", ""], ["Wang", "Yu-Siang", ""], ["Hsu", "Winston H.", ""], ["Chen", "Wen-Chin", ""]]}, {"id": "2103.07700", "submitter": "Xin Suo", "authors": "Xin Suo and Yuheng Jiang and Pei Lin and Yingliang Zhang and Kaiwen\n  Guo and Minye Wu and Lan Xu", "title": "NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering\n  using RGB Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  4D reconstruction and rendering of human activities is critical for immersive\nVR/AR experience.Recent advances still fail to recover fine geometry and\ntexture results with the level of detail present in the input images from\nsparse multi-view RGB cameras. In this paper, we propose NeuralHumanFVV, a\nreal-time neural human performance capture and rendering system to generate\nboth high-quality geometry and photo-realistic texture of human activities in\narbitrary novel views. We propose a neural geometry generation scheme with a\nhierarchical sampling strategy for real-time implicit geometry inference, as\nwell as a novel neural blending scheme to generate high resolution (e.g., 1k)\nand photo-realistic texture results in the novel views. Furthermore, we adopt\nneural normal blending to enhance geometry details and formulate our neural\ngeometry and texture rendering into a multi-task learning framework. Extensive\nexperiments demonstrate the effectiveness of our approach to achieve\nhigh-quality geometry and photo-realistic free view-point reconstruction for\nchallenging human performances.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 12:03:38 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Suo", "Xin", ""], ["Jiang", "Yuheng", ""], ["Lin", "Pei", ""], ["Zhang", "Yingliang", ""], ["Guo", "Kaiwen", ""], ["Wu", "Minye", ""], ["Xu", "Lan", ""]]}, {"id": "2103.07733", "submitter": "Jiaming Han", "authors": "Jiaming Han and Jian Ding and Nan Xue and Gui-Song Xia", "title": "ReDet: A Rotation-equivariant Detector for Aerial Object Detection", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, object detection in aerial images has gained much attention in\ncomputer vision. Different from objects in natural images, aerial objects are\noften distributed with arbitrary orientation. Therefore, the detector requires\nmore parameters to encode the orientation information, which are often highly\nredundant and inefficient. Moreover, as ordinary CNNs do not explicitly model\nthe orientation variation, large amounts of rotation augmented data is needed\nto train an accurate object detector. In this paper, we propose a\nRotation-equivariant Detector (ReDet) to address these issues, which explicitly\nencodes rotation equivariance and rotation invariance. More precisely, we\nincorporate rotation-equivariant networks into the detector to extract\nrotation-equivariant features, which can accurately predict the orientation and\nlead to a huge reduction of model size. Based on the rotation-equivariant\nfeatures, we also present Rotation-invariant RoI Align (RiRoI Align), which\nadaptively extracts rotation-invariant features from equivariant features\naccording to the orientation of RoI. Extensive experiments on several\nchallenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and HRSC2016, show that\nour method can achieve state-of-the-art performance on the task of aerial\nobject detection. Compared with previous best results, our ReDet gains 1.2, 3.5\nand 2.6 mAP on DOTA-v1.0, DOTA-v1.5 and HRSC2016 respectively while reducing\nthe number of parameters by 60\\% (313 Mb vs. 121 Mb). The code is available at:\n\\url{https://github.com/csuhan/ReDet}.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 15:37:36 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Han", "Jiaming", ""], ["Ding", "Jian", ""], ["Xue", "Nan", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2103.07738", "submitter": "Daniel J. Trosten", "authors": "Daniel J. Trosten, Sigurd L{\\o}kse, Robert Jenssen, Michael\n  Kampffmeyer", "title": "Reconsidering Representation Alignment for Multi-view Clustering", "comments": "To appear in CVPR 2021. Code available at\n  https://github.com/DanielTrosten/mvc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aligning distributions of view representations is a core component of today's\nstate of the art models for deep multi-view clustering. However, we identify\nseveral drawbacks with na\\\"ively aligning representation distributions. We\ndemonstrate that these drawbacks both lead to less separable clusters in the\nrepresentation space, and inhibit the model's ability to prioritize views.\nBased on these observations, we develop a simple baseline model for deep\nmulti-view clustering. Our baseline model avoids representation alignment\naltogether, while performing similar to, or better than, the current state of\nthe art. We also expand our baseline model by adding a contrastive learning\ncomponent. This introduces a selective alignment procedure that preserves the\nmodel's ability to prioritize views. Our experiments show that the contrastive\nlearning component enhances the baseline model, improving on the current state\nof the art by a large margin on several datasets.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 15:58:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Trosten", "Daniel J.", ""], ["L\u00f8kse", "Sigurd", ""], ["Jenssen", "Robert", ""], ["Kampffmeyer", "Michael", ""]]}, {"id": "2103.07751", "submitter": "Kaiwen Zha", "authors": "Kaiwen Zha, Yujun Shen, Bolei Zhou", "title": "Unsupervised Image Transformation Learning via Generative Adversarial\n  Networks", "comments": "14 pages, 15 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the image transformation problem by learning the\nunderlying transformations from a collection of images using Generative\nAdversarial Networks (GANs). Specifically, we propose an unsupervised learning\nframework, termed as TrGAN, to project images onto a transformation space that\nis shared by the generator and the discriminator. Any two points in this\nprojected space define a transformation that can guide the image generation\nprocess, leading to continuous semantic change. By projecting a pair of images\nonto the transformation space, we are able to adequately extract the semantic\nvariation between them and further apply the extracted semantic to facilitating\nimage editing, including not only transferring image styles (e.g., changing day\nto night) but also manipulating image contents (e.g., adding clouds in the\nsky). Code and models are available at https://genforce.github.io/trgan.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 17:08:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zha", "Kaiwen", ""], ["Shen", "Yujun", ""], ["Zhou", "Bolei", ""]]}, {"id": "2103.07754", "submitter": "EL-Hachemi Guerrout", "authors": "EL-Hachemi Guerrout, Ramdane Mahiou, Randa Boukabene, and Assia Ouali", "title": "Image Segmentation Methods for Non-destructive testing Applications", "comments": "10 pages, 3 figures, the article is just accepted in the conference\n  JERI 2020 but the conference stopped because of Covid so the article non\n  published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present new image segmentation methods based on hidden\nMarkov random fields (HMRFs) and cuckoo search (CS) variants. HMRFs model the\nsegmentation problem as a minimization of an energy function. CS algorithm is\none of the recent powerful optimization techniques. Therefore, five variants of\nthe CS algorithm are used to compute a solution. Through tests, we conduct a\nstudy to choose the CS variant with parameters that give good results\n(execution time and quality of segmentation). CS variants are evaluated and\ncompared with non-destructive testing (NDT) images using a misclassification\nerror (ME) criterion.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 17:13:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Guerrout", "EL-Hachemi", ""], ["Mahiou", "Ramdane", ""], ["Boukabene", "Randa", ""], ["Ouali", "Assia", ""]]}, {"id": "2103.07756", "submitter": "Pengxiang Wu", "authors": "Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, Chao Chen", "title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "comments": "ICLR 2021 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label noise is frequently observed in real-world large-scale datasets. The\nnoise is introduced due to a variety of reasons; it is heterogeneous and\nfeature-dependent. Most existing approaches to handling noisy labels fall into\ntwo categories: they either assume an ideal feature-independent noise, or\nremain heuristic without theoretical guarantees. In this paper, we propose to\ntarget a new family of feature-dependent label noise, which is much more\ngeneral than commonly used i.i.d. label noise and encompasses a broad spectrum\nof noise patterns. Focusing on this general noise family, we propose a\nprogressive label correction algorithm that iteratively corrects labels and\nrefines the model. We provide theoretical guarantees showing that for a wide\nvariety of (unknown) noise patterns, a classifier trained with this strategy\nconverges to be consistent with the Bayes classifier. In experiments, our\nmethod outperforms SOTA baselines and is robust to various noise types and\nlevels.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 17:34:22 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 07:28:12 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 05:34:18 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Yikai", ""], ["Zheng", "Songzhu", ""], ["Wu", "Pengxiang", ""], ["Goswami", "Mayank", ""], ["Chen", "Chao", ""]]}, {"id": "2103.07758", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "Learning Novel Objects Continually Through Curiosity", "comments": "Accepted at IEEE ICRA 2021 (Workshop titled, Towards Curious Robots:\n  Modern Approaches for Intrinsically-Motivated Intelligent Behavior)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Children learn continually by asking questions about the concepts they are\nmost curious about. With robots becoming an integral part of our society, they\nmust also learn unknown concepts continually by asking humans questions. The\npaper analyzes a recent state-of-the-art approach for continual learning. The\npaper further develops a self-supervised technique to find most of the\nuncertain objects in an environment by utilizing the cluster representation of\nthe previously learned classes. We test our approach on a benchmark dataset for\ncontinual learning on robots. Our results show that our curiosity-driven\ncontinual learning approach beats random sampling and softmax-based uncertainty\nsampling in terms of classification accuracy and the total number of classes\nlearned.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 17:42:09 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 21:53:49 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2103.07770", "submitter": "Pankaj Topiwala", "authors": "Pankaj Topiwala, Wei Dai, Jiangfeng Pian, Katalina Biondi, Arvind\n  Krovvidi", "title": "VMAF And Variants: Towards A Unified VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality assessment (VQA) is now a fastgrowing subject, beginning to\nmature in the full reference (FR) case, while the burgeoning no reference (NR)\ncase remains challenging. We investigate variants of the popular VMAF video\nquality assessment algorithm for the FR case, using support vector regression\nand feedforward neural networks, and extend it to the NR case, using the same\nlearning architectures, to develop a partially unified framework for VQA. When\nheavily trained, algorithms such as VMAF perform well on test datasets, with\n90%+ match; but predicting performance in the wild is better done by\ntraining/testing from scratch, as we do. Even from scratch, we achieve 90%+\nperformance in FR, with gains over VMAF. And we greatly reduce complexity vs.\nleading recent NR algorithms, VIDEVAL, RAPIQUE, yet exceed 80% in SRCC. In our\npreliminary testing, we find the improvements in trainability, while also\nconstraining computational complexity, as quite encouraging, suggesting further\nstudy and analysis.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:41:51 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 20:30:46 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 01:54:59 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Topiwala", "Pankaj", ""], ["Dai", "Wei", ""], ["Pian", "Jiangfeng", ""], ["Biondi", "Katalina", ""], ["Krovvidi", "Arvind", ""]]}, {"id": "2103.07783", "submitter": "Su Pang", "authors": "Su Pang and Hayder Radha", "title": "Multi-Object Tracking using Poisson Multi-Bernoulli Mixture Filtering\n  for Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of an autonomous vehicle to perform 3D tracking is essential for\nsafe planing and navigation in cluttered environments. The main challenges for\nmulti-object tracking (MOT) in autonomous driving applications reside in the\ninherent uncertainties regarding the number of objects, when and where the\nobjects may appear and disappear, and uncertainties regarding objects' states.\nRandom finite set (RFS) based approaches can naturally model these\nuncertainties accurately and elegantly, and they have been widely used in\nradar-based tracking applications. In this work, we developed an RFS-based MOT\nframework for 3D LiDAR data. In partiuclar, we propose a Poisson\nmulti-Bernoulli mixture (PMBM) filter to solve the amodal MOT problem for\nautonomous driving applications. To the best of our knowledge, this represents\na first attempt for employing an RFS-based approach in conjunction with 3D\nLiDAR data for MOT applications with comprehensive validation using challenging\ndatasets made available by industry leaders. The superior experimental results\nof our PMBM tracker on public Waymo and Argoverse datasets clearly illustrate\nthat an RFS-based tracker outperforms many state-of-the-art deep learning-based\nand Kalman filter-based methods, and consequently, these results indicate a\ngreat potential for further exploration of RFS-based frameworks for 3D MOT\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 20:24:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Pang", "Su", ""], ["Radha", "Hayder", ""]]}, {"id": "2103.07790", "submitter": "Salman Ul Hassan Dar", "authors": "Salman Ul Hassan Dar, Mahmut Yurt, Tolga \\c{C}ukur", "title": "A Few-Shot Learning Approach for Accelerated MRI via Fusion of\n  Data-Driven and Subject-Driven Priors", "comments": "Accepted for presentation at the 29th Annual Meeting of the\n  International Society of Magnetic Resonance in Medicine (ISMRM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have recently found emerging use in accelerated\nMRI reconstruction. DNNs typically learn data-driven priors from large datasets\nconstituting pairs of undersampled and fully-sampled acquisitions. Acquiring\nsuch large datasets, however, might be impractical. To mitigate this\nlimitation, we propose a few-shot learning approach for accelerated MRI that\nmerges subject-driven priors obtained via physical signal models with\ndata-driven priors obtained from a few training samples. Demonstrations on\nbrain MR images from the NYU fastMRI dataset indicate that the proposed\napproach requires just a few samples to outperform traditional parallel imaging\nand DNN algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 20:51:31 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Dar", "Salman Ul Hassan", ""], ["Yurt", "Mahmut", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "2103.07798", "submitter": "Yaoyu Hu", "authors": "Yaoyu Hu, Wenshan Wang, Huai Yu, Weikun Zhen, Sebastian Scherer", "title": "ORStereo: Occlusion-Aware Recurrent Stereo Matching for 4K-Resolution\n  Images", "comments": "Submitted to International Conference on Intelligent Robots and\n  Systems (IROS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Stereo reconstruction models trained on small images do not generalize well\nto high-resolution data. Training a model on high-resolution image size faces\ndifficulties of data availability and is often infeasible due to limited\ncomputing resources. In this work, we present the Occlusion-aware Recurrent\nbinocular Stereo matching (ORStereo), which deals with these issues by only\ntraining on available low disparity range stereo images. ORStereo generalizes\nto unseen high-resolution images with large disparity ranges by formulating the\ntask as residual updates and refinements of an initial prediction. ORStereo is\ntrained on images with disparity ranges limited to 256 pixels, yet it can\noperate 4K-resolution input with over 1000 disparities using limited GPU\nmemory. We test the model's capability on both synthetic and real-world\nhigh-resolution images. Experimental results demonstrate that ORStereo achieves\ncomparable performance on 4K-resolution images compared to state-of-the-art\nmethods trained on large disparity ranges. Compared to other methods that are\nonly trained on low-resolution images, our method is 70% more accurate on\n4K-resolution images.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 21:46:06 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hu", "Yaoyu", ""], ["Wang", "Wenshan", ""], ["Yu", "Huai", ""], ["Zhen", "Weikun", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2103.07825", "submitter": "Xu Dong", "authors": "Xu Dong, Binnan Zhuang, Yunxiang Mao, Langechuan Liu", "title": "Radar Camera Fusion via Representation Learning in Autonomous Driving", "comments": null, "journal-ref": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition, pp. 1672-1681. 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radars and cameras are mature, cost-effective, and robust sensors and have\nbeen widely used in the perception stack of mass-produced autonomous driving\nsystems. Due to their complementary properties, outputs from radar detection\n(radar pins) and camera perception (2D bounding boxes) are usually fused to\ngenerate the best perception results. The key to successful radar-camera fusion\nis the accurate data association. The challenges in the radar-camera\nassociation can be attributed to the complexity of driving scenes, the noisy\nand sparse nature of radar measurements, and the depth ambiguity from 2D\nbounding boxes. Traditional rule-based association methods are susceptible to\nperformance degradation in challenging scenarios and failure in corner cases.\nIn this study, we propose to address radar-camera association via deep\nrepresentation learning, to explore feature-level interaction and global\nreasoning. Additionally, we design a loss sampling mechanism and an innovative\nordinal loss to overcome the difficulty of imperfect labeling and to enforce\ncritical human-like reasoning. Despite being trained with noisy labels\ngenerated by a rule-based algorithm, our proposed method achieves a performance\nof 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Moreover,\nthis data-driven method also lends itself to continuous improvement via corner\ncase mining.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 01:32:03 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 21:02:47 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 15:48:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Dong", "Xu", ""], ["Zhuang", "Binnan", ""], ["Mao", "Yunxiang", ""], ["Liu", "Langechuan", ""]]}, {"id": "2103.07838", "submitter": "Xin Wen", "authors": "Xin Wen and Zhizhong Han and Yan-Pei Cao and Pengfei Wan and Wen Zheng\n  and Yu-Shen Liu", "title": "Cycle4Completion: Unpaired Point Cloud Completion using Cycle\n  Transformation with Missing Region Coding", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel unpaired point cloud completion network,\nnamed Cycle4Completion, to infer the complete geometries from a partial 3D\nobject. Previous unpaired completion methods merely focus on the learning of\ngeometric correspondence from incomplete shapes to complete shapes, and ignore\nthe learning in the reverse direction, which makes them suffer from low\ncompletion accuracy due to the limited 3D shape understanding ability. To\naddress this problem, we propose two simultaneous cycle transformations between\nthe latent spaces of complete shapes and incomplete ones. The insight of cycle\ntransformation is to promote networks to understand 3D shapes by learning to\ngenerate complete or incomplete shapes from their complementary ones.\nSpecifically, the first cycle transforms shapes from incomplete domain to\ncomplete domain, and then projects them back to the incomplete domain. This\nprocess learns the geometric characteristic of complete shapes, and maintains\nthe shape consistency between the complete prediction and the incomplete input.\nSimilarly, the inverse cycle transformation starts from complete domain to\nincomplete domain, and goes back to complete domain to learn the characteristic\nof incomplete shapes. We provide a comprehensive evaluation in experiments,\nwhich shows that our model with the learned bidirectional geometry\ncorrespondence outperforms state-of-the-art unpaired completion methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 03:52:53 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 13:55:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wen", "Xin", ""], ["Han", "Zhizhong", ""], ["Cao", "Yan-Pei", ""], ["Wan", "Pengfei", ""], ["Zheng", "Wen", ""], ["Liu", "Yu-Shen", ""]]}, {"id": "2103.07854", "submitter": "Jianhua Sun", "authors": "Jianhua Sun, Yuxuan Li, Hao-Shu Fang, Cewu Lu", "title": "Three Steps to Multimodal Trajectory Prediction: Modality Clustering,\n  Classification and Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal prediction results are essential for trajectory prediction task as\nthere is no single correct answer for the future. Previous frameworks can be\ndivided into three categories: regression, generation and classification\nframeworks. However, these frameworks have weaknesses in different aspects so\nthat they cannot model the multimodal prediction task comprehensively. In this\npaper, we present a novel insight along with a brand-new prediction framework\nby formulating multimodal prediction into three steps: modality clustering,\nclassification and synthesis, and address the shortcomings of earlier\nframeworks. Exhaustive experiments on popular benchmarks have demonstrated that\nour proposed method surpasses state-of-the-art works even without introducing\nsocial and map information. Specifically, we achieve 19.2% and 20.8%\nimprovement on ADE and FDE respectively on ETH/UCY dataset. Our code will be\nmade publicly availabe.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 06:21:03 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 15:22:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sun", "Jianhua", ""], ["Li", "Yuxuan", ""], ["Fang", "Hao-Shu", ""], ["Lu", "Cewu", ""]]}, {"id": "2103.07883", "submitter": "Fabio Poiesi", "authors": "M. Bortolon, L. Bazzanella, F. Poiesi", "title": "Multi-view data capture for dynamic object reconstruction using handheld\n  augmented reality mobiles", "comments": "Accepted in Journal of Real-Time Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a system to capture nearly-synchronous frame streams from multiple\nand moving handheld mobiles that is suitable for dynamic object 3D\nreconstruction. Each mobile executes Simultaneous Localisation and Mapping\non-board to estimate its pose, and uses a wireless communication channel to\nsend or receive synchronisation triggers. Our system can harvest frames and\nmobile poses in real time using a decentralised triggering strategy and a\ndata-relay architecture that can be deployed either at the Edge or in the\nCloud. We show the effectiveness of our system by employing it for 3D skeleton\nand volumetric reconstructions. Our triggering strategy achieves equal\nperformance to that of an NTP-based synchronisation approach, but offers higher\nflexibility, as it can be adjusted online based on application needs. We\ncreated a challenging new dataset, namely 4DM, that involves six handheld\naugmented reality mobiles recording an actor performing sports actions\noutdoors. We validate our system on 4DM, analyse its strengths and limitations,\nand compare its modules with alternative ones.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 10:26:50 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 06:08:59 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bortolon", "M.", ""], ["Bazzanella", "L.", ""], ["Poiesi", "F.", ""]]}, {"id": "2103.07889", "submitter": "Peng Dai", "authors": "Peng Dai and Renliang Weng and Wongun Choi and Changshui Zhang and\n  Zhangping He and Wei Ding", "title": "Learning a Proposal Classifier for Multiple Object Tracking", "comments": "Accepted at CVPR 2021, Poster, EEE/CVF Conference on Computer Vision\n  and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent trend in multiple object tracking (MOT) is heading towards\nleveraging deep learning to boost the tracking performance. However, it is not\ntrivial to solve the data-association problem in an end-to-end fashion. In this\npaper, we propose a novel proposal-based learnable framework, which models MOT\nas a proposal generation, proposal scoring and trajectory inference paradigm on\nan affinity graph. This framework is similar to the two-stage object detector\nFaster RCNN, and can solve the MOT problem in a data-driven way. For proposal\ngeneration, we propose an iterative graph clustering method to reduce the\ncomputational cost while maintaining the quality of the generated proposals.\nFor proposal scoring, we deploy a trainable graph-convolutional-network (GCN)\nto learn the structural patterns of the generated proposals and rank them\naccording to the estimated quality scores. For trajectory inference, a simple\ndeoverlapping strategy is adopted to generate tracking output while complying\nwith the constraints that no detection can be assigned to more than one track.\nWe experimentally demonstrate that the proposed method achieves a clear\nperformance improvement in both MOTA and IDF1 with respect to previous\nstate-of-the-art on two public benchmarks. Our code is available at\nhttps://github.com/daip13/LPC_MOT.git.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 10:46:54 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 06:54:41 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 02:08:53 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Dai", "Peng", ""], ["Weng", "Renliang", ""], ["Choi", "Wongun", ""], ["Zhang", "Changshui", ""], ["He", "Zhangping", ""], ["Ding", "Wei", ""]]}, {"id": "2103.07893", "submitter": "Rui Liu", "authors": "Rui Liu, Yixiao Ge, Ching Lam Choi, Xiaogang Wang, Hongsheng Li", "title": "DivCo: Diverse Conditional Image Synthesis via Contrastive Generative\n  Adversarial Network", "comments": "To appear at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional generative adversarial networks (cGANs) target at synthesizing\ndiverse images given the input conditions and latent codes, but unfortunately,\nthey usually suffer from the issue of mode collapse. To solve this issue,\nprevious works mainly focused on encouraging the correlation between the latent\ncodes and their generated images, while ignoring the relations between images\ngenerated from various latent codes. The recent MSGAN tried to encourage the\ndiversity of the generated image but only considers \"negative\" relations\nbetween the image pairs. In this paper, we propose a novel DivCo framework to\nproperly constrain both \"positive\" and \"negative\" relations between the\ngenerated images specified in the latent space. To the best of our knowledge,\nthis is the first attempt to use contrastive learning for diverse conditional\nimage synthesis. A novel latent-augmented contrastive loss is introduced, which\nencourages images generated from adjacent latent codes to be similar and those\ngenerated from distinct latent codes to be dissimilar. The proposed\nlatent-augmented contrastive loss is well compatible with various cGAN\narchitectures. Extensive experiments demonstrate that the proposed DivCo can\nproduce more diverse images than state-of-the-art methods without sacrificing\nvisual quality in multiple unpaired and paired image generation tasks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 11:11:15 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 14:37:23 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liu", "Rui", ""], ["Ge", "Yixiao", ""], ["Choi", "Ching Lam", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "2103.07894", "submitter": "Haolin Liu", "authors": "Haolin Liu, Anran Lin, Xiaoguang Han, Lei Yang, Yizhou Yu, Shuguang\n  Cui", "title": "Refer-it-in-RGBD: A Bottom-up Approach for 3D Visual Grounding in RGBD\n  Images", "comments": "CVPR 2021, project page:\n  https://unclemedm.github.io/Refer-it-in-RGBD/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding referring expressions in RGBD image has been an emerging field. We\npresent a novel task of 3D visual grounding in single-view RGBD image where the\nreferred objects are often only partially scanned due to occlusion. In contrast\nto previous works that directly generate object proposals for grounding in the\n3D scenes, we propose a bottom-up approach to gradually aggregate context-aware\ninformation, effectively addressing the challenge posed by the partial\ngeometry. Our approach first fuses the language and the visual features at the\nbottom level to generate a heatmap that coarsely localizes the relevant regions\nin the RGBD image. Then our approach conducts an adaptive feature learning\nbased on the heatmap and performs the object-level matching with another\nvisio-linguistic fusion to finally ground the referred object. We evaluate the\nproposed method by comparing to the state-of-the-art methods on both the RGBD\nimages extracted from the ScanRefer dataset and our newly collected SUNRefer\ndataset. Experiments show that our method outperforms the previous methods by a\nlarge margin (by 11.2% and 15.6% Acc@0.5) on both datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 11:18:50 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 02:38:57 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 06:35:20 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Haolin", ""], ["Lin", "Anran", ""], ["Han", "Xiaoguang", ""], ["Yang", "Lei", ""], ["Yu", "Yizhou", ""], ["Cui", "Shuguang", ""]]}, {"id": "2103.07895", "submitter": "Lok Hin Lee", "authors": "Lok Hin Lee and Yuan Gao and J. Alison Noble", "title": "Principled Ultrasound Data Augmentation for Classification of Standard\n  Planes", "comments": "Information Processing in Medical Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models with large learning capacities often overfit to medical\nimaging datasets. This is because training sets are often relatively small due\nto the significant time and financial costs incurred in medical data\nacquisition and labelling. Data augmentation is therefore often used to expand\nthe availability of training data and to increase generalization. However,\naugmentation strategies are often chosen on an ad-hoc basis without\njustification. In this paper, we present an augmentation policy search method\nwith the goal of improving model classification performance. We include in the\naugmentation policy search additional transformations that are often used in\nmedical image analysis and evaluate their performance. In addition, we extend\nthe augmentation policy search to include non-linear mixed-example data\naugmentation strategies. Using these learned policies, we show that principled\ndata augmentation for medical image model training can lead to significant\nimprovements in ultrasound standard plane detection, with an an average\nF1-score improvement of 7.0% overall over naive data augmentation strategies in\nultrasound fetal standard plane classification. We find that the learned\nrepresentations of ultrasound images are better clustered and defined with\noptimized data augmentation.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 11:20:31 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lee", "Lok Hin", ""], ["Gao", "Yuan", ""], ["Noble", "J. Alison", ""]]}, {"id": "2103.07905", "submitter": "Md Fahim Sikder", "authors": "Md Fahim Sikder", "title": "Bangla Handwritten Digit Recognition and Generation", "comments": "Proceedings of International Joint Conference on Computational\n  Intelligence", "journal-ref": null, "doi": "10.1007/978-981-13-7564-4_46", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 12:11:21 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sikder", "Md Fahim", ""]]}, {"id": "2103.07915", "submitter": "Changtao Miao", "authors": "Changtao Miao, Qi Chu, Weihai Li, Tao Gong, Wanyi Zhuang and Nenghai\n  Yu", "title": "Towards Generalizable and Robust Face Manipulation Detection via\n  Bag-of-local-feature", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past several years, in order to solve the problem of malicious abuse\nof facial manipulation technology, face manipulation detection technology has\nobtained considerable attention and achieved remarkable progress. However, most\nexisting methods have very impoverished generalization ability and robustness.\nIn this paper, we propose a novel method for face manipulation detection, which\ncan improve the generalization ability and robustness by bag-of-local-feature.\nSpecifically, we extend Transformers using bag-of-feature approach to encode\ninter-patch relationships, allowing it to learn local forgery features without\nany explicit supervision. Extensive experiments demonstrate that our method can\noutperform competing state-of-the-art methods on FaceForensics++, Celeb-DF and\nDeeperForensics-1.0 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 12:50:48 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Miao", "Changtao", ""], ["Chu", "Qi", ""], ["Li", "Weihai", ""], ["Gong", "Tao", ""], ["Zhuang", "Wanyi", ""], ["Yu", "Nenghai", ""]]}, {"id": "2103.07935", "submitter": "Libo Wang", "authors": "Libo Wang, Shenghui Fang, Ce Zhang, Rui Li, Chenxi Duan, Xiaoliang\n  Meng, Peter M. Atkinson", "title": "SaNet: Scale-aware Neural Network for Semantic Labelling of Multiple\n  Spatial Resolution Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning geospatial objects of aerial images with specific categories at the\npixel level is a fundamental task in urban scene interpretation. Along with\nrapid developments in sensor technologies, aerial images can be captured at\nmultiple spatial resolutions (MSR) with information content manifested at\ndifferent scales. Extracting information from these MSR aerial images\nrepresents huge opportunities for enhanced feature representation and\ncharacterisation. However, MSR images suffer from two critical issues: 1)\nincreased variation in the sizes of geospatial objects and 2) information and\ninformative feature loss at coarse spatial resolutions. In this paper, we\npropose a novel scale-aware neural network (SaNet) for semantic labelling of\nMSR aerial images to address these two issues. SaNet deploys a densely\nconnected feature network (DCFPN) module to capture high-quality multi-scale\ncontext, such as to address the scale variation issue and increase the quality\nof segmentation for both large and small objects simultaneously. A spatial\nfeature recalibration (SFR) module is further incorporated into the network to\nlearn complete semantic features with enhanced spatial relationships, where the\neffects of information and informative feature loss are addressed. The\ncombination of DCFPN and SFR allows the proposed SaNet to learn scale-aware\nfeatures from MSR aerial images. Extensive experiments undertaken on ISPRS\nsemantic segmentation datasets demonstrated the outstanding accuracy of the\nproposed SaNet in cross-resolution segmentation, with an average OA of 83.4% on\nthe Vaihingen dataset and an average F1 score of 80.4% on the Potsdam dataset,\noutperforming state-of-the-art deep learning approaches, including FPN (80.2%\nand 76.6%), PSPNet (79.8% and 76.2%) and Deeplabv3+ (80.8% and 76.1%) as well\nas DDCM-Net (81.7% and 77.6%) and EaNet (81.5% and 78.3%).\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 14:19:46 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 12:51:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Libo", ""], ["Fang", "Shenghui", ""], ["Zhang", "Ce", ""], ["Li", "Rui", ""], ["Duan", "Chenxi", ""], ["Meng", "Xiaoliang", ""], ["Atkinson", "Peter M.", ""]]}, {"id": "2103.07939", "submitter": "Zongsheng Yue", "authors": "Zongsheng Yue, Jianwen Xie, Qian Zhao, Deyu Meng", "title": "Semi-Supervised Video Deraining with Dynamical Rain Generator", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While deep learning (DL)-based video deraining methods have achieved\nsignificant success recently, they still exist two major drawbacks. Firstly,\nmost of them do not sufficiently model the characteristics of rain layers of\nrainy videos. In fact, the rain layers exhibit strong physical properties\n(e.g., direction, scale and thickness) in spatial dimension and natural\ncontinuities in temporal dimension, and thus can be generally modelled by the\nspatial-temporal process in statistics. Secondly, current DL-based methods\nseriously depend on the labeled synthetic training data, whose rain types are\nalways deviated from those in unlabeled real data. Such gap between synthetic\nand real data sets leads to poor performance when applying them in real\nscenarios. Against these issues, this paper proposes a new semi-supervised\nvideo deraining method, in which a dynamic rain generator is employed to fit\nthe rain layer, expecting to better depict its insightful characteristics.\nSpecifically, such dynamic generator consists of one emission model and one\ntransition model to simultaneously encode the spatially physical structure and\ntemporally continuous changes of rain streaks, respectively, which both are\nparameterized as deep neural networks (DNNs). Further more, different prior\nformats are designed for the labeled synthetic and unlabeled real data, so as\nto fully exploit the common knowledge underlying them. Last but not least, we\nalso design a Monte Carlo EM algorithm to solve this model. Extensive\nexperiments are conducted to verify the superiorities of the proposed\nsemi-supervised deraining model.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 14:28:57 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:29:42 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 01:11:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yue", "Zongsheng", ""], ["Xie", "Jianwen", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""]]}, {"id": "2103.07941", "submitter": "Ho Kei Cheng", "authors": "Ho Kei Cheng, Yu-Wing Tai, Chi-Keung Tang", "title": "Modular Interactive Video Object Segmentation: Interaction-to-Mask,\n  Propagation and Difference-Aware Fusion", "comments": "Accepted to CVPR 2021. Project page:\n  https://hkchengrex.github.io/MiVOS/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present Modular interactive VOS (MiVOS) framework which decouples\ninteraction-to-mask and mask propagation, allowing for higher generalizability\nand better performance. Trained separately, the interaction module converts\nuser interactions to an object mask, which is then temporally propagated by our\npropagation module using a novel top-$k$ filtering strategy in reading the\nspace-time memory. To effectively take the user's intent into account, a novel\ndifference-aware module is proposed to learn how to properly fuse the masks\nbefore and after each interaction, which are aligned with the target frames by\nemploying the space-time memory. We evaluate our method both qualitatively and\nquantitatively with different forms of user interactions (e.g., scribbles,\nclicks) on DAVIS to show that our method outperforms current state-of-the-art\nalgorithms while requiring fewer frame interactions, with the additional\nadvantage in generalizing to different types of user interactions. We\ncontribute a large-scale synthetic VOS dataset with pixel-accurate segmentation\nof 4.8M frames to accompany our source codes to facilitate future research.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 14:39:08 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 03:02:55 GMT"}, {"version": "v3", "created": "Sun, 21 Mar 2021 14:14:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Cheng", "Ho Kei", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "2103.07969", "submitter": "Sinisa Stekovic", "authors": "Shreyas Hampali, Sinisa Stekovic, Sayan Deb Sarkar, Chetan Srinivasa\n  Kumar, Friedrich Fraundorfer, Vincent Lepetit", "title": "Monte Carlo Scene Search for 3D Scene Understanding", "comments": "To be presented at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how a general AI algorithm can be used for 3D scene understanding\nto reduce the need for training data. More exactly, we propose a modification\nof the Monte Carlo Tree Search (MCTS) algorithm to retrieve objects and room\nlayouts from noisy RGB-D scans. While MCTS was developed as a game-playing\nalgorithm, we show it can also be used for complex perception problems. Our\nadapted MCTS algorithm has few easy-to-tune hyperparameters and can optimise\ngeneral losses. We use it to optimise the posterior probability of objects and\nroom layout hypotheses given the RGB-D data. This results in an\nanalysis-by-synthesis approach that explores the solution space by rendering\nthe current solution and comparing it to the RGB-D observations. To perform\nthis exploration even more efficiently, we propose simple changes to the\nstandard MCTS' tree construction and exploration policy. We demonstrate our\napproach on the ScanNet dataset. Our method often retrieves configurations that\nare better than some manual annotations, especially on layouts.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:33:28 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 09:39:56 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 10:03:03 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Hampali", "Shreyas", ""], ["Stekovic", "Sinisa", ""], ["Sarkar", "Sayan Deb", ""], ["Kumar", "Chetan Srinivasa", ""], ["Fraundorfer", "Friedrich", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2103.07973", "submitter": "Yudong Liang", "authors": "Yudong Liang, Bin Wang, Jiaying Liu, Deyu Li, Yuhua Qian and Wenqi Ren", "title": "Progressive residual learning for single image dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent physical model-free dehazing methods have achieved\nstate-of-the-art performances. However, without the guidance of physical\nmodels, the performances degrade rapidly when applied to real scenarios due to\nthe unavailable or insufficient data problems. On the other hand, the physical\nmodel-based methods have better interpretability but suffer from\nmulti-objective optimizations of parameters, which may lead to sub-optimal\ndehazing results. In this paper, a progressive residual learning strategy has\nbeen proposed to combine the physical model-free dehazing process with\nreformulated scattering model-based dehazing operations, which enjoys the\nmerits of dehazing methods in both categories. Specifically, the global\natmosphere light and transmission maps are interactively optimized with the aid\nof accurate residual information and preliminary dehazed restorations from the\ninitial physical model-free dehazing process. The proposed method performs\nfavorably against the state-of-the-art methods on public dehazing benchmarks\nwith better model interpretability and adaptivity for complex hazy data.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:54:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liang", "Yudong", ""], ["Wang", "Bin", ""], ["Liu", "Jiaying", ""], ["Li", "Deyu", ""], ["Qian", "Yuhua", ""], ["Ren", "Wenqi", ""]]}, {"id": "2103.07976", "submitter": "Ju He", "authors": "Ju He, Jie-Neng Chen, Shuai Liu, Adam Kortylewski, Cheng Yang, Yutong\n  Bai, Changhu Wang, Alan Yuille", "title": "TransFG: A Transformer Architecture for Fine-grained Recognition", "comments": "Release official PyTorch implementation of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual classification (FGVC) which aims at recognizing objects\nfrom subcategories is a very challenging task due to the inherently subtle\ninter-class differences. Recent works mainly tackle this problem by focusing on\nhow to locate the most discriminative image regions and rely on them to improve\nthe capability of networks to capture subtle variances. Most of these works\nachieve this by re-using the backbone network to extract features of selected\nregions. However, this strategy inevitably complicates the pipeline and pushes\nthe proposed regions to contain most parts of the objects. Recently, vision\ntransformer (ViT) shows its strong performance in the traditional\nclassification task. The self-attention mechanism of the transformer links\nevery patch token to the classification token. The strength of the attention\nlink can be intuitively considered as an indicator of the importance of tokens.\nIn this work, we propose a novel transformer-based framework TransFG where we\nintegrate all raw attention weights of the transformer into an attention map\nfor guiding the network to effectively and accurately select discriminative\nimage patches and compute their relations. A contrastive loss is applied to\nfurther enlarge the distance between feature representations of similar\nsub-classes. We demonstrate the value of TransFG by conducting experiments on\nfive popular fine-grained benchmarks: CUB-200-2011, Stanford Cars, Stanford\nDogs, NABirds and iNat2017 where we achieve state-of-the-art performance.\nQualitative results are presented for better understanding of our model. Code\nis available at https://github.com/TACJu/TransFG.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 17:03:53 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 13:27:48 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 04:03:30 GMT"}, {"version": "v4", "created": "Sun, 28 Mar 2021 11:39:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["He", "Ju", ""], ["Chen", "Jie-Neng", ""], ["Liu", "Shuai", ""], ["Kortylewski", "Adam", ""], ["Yang", "Cheng", ""], ["Bai", "Yutong", ""], ["Wang", "Changhu", ""], ["Yuille", "Alan", ""]]}, {"id": "2103.07985", "submitter": "Anas Tahir", "authors": "Anas M. Tahir, Muhammad E. H. Chowdhury, Amith Khandakar, Tawsifur\n  Rahman, Yazan Qiblawey, Uzair Khurshid, Serkan Kiranyaz, Nabil Ibtehaz, M\n  Shohel Rahman, Somaya Al-Madeed, Khaled Hameed, Tahir Hamid, Sakib Mahmud,\n  Maymouna Ezeddin", "title": "COVID-19 Infection Localization and Severity Grading from Chest X-ray\n  Images", "comments": "30 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) has been the main agenda of the whole\nworld, since it came into sight in December 2019 as it has significantly\naffected the world economy and healthcare system. Given the effects of COVID-19\non pulmonary tissues, chest radiographic imaging has become a necessity for\nscreening and monitoring the disease. Numerous studies have proposed Deep\nLearning approaches for the automatic diagnosis of COVID-19. Although these\nmethods achieved astonishing performance in detection, they have used limited\nchest X-ray (CXR) repositories for evaluation, usually with a few hundred\nCOVID-19 CXR images only. Thus, such data scarcity prevents reliable evaluation\nwith the potential of overfitting. In addition, most studies showed no or\nlimited capability in infection localization and severity grading of COVID-19\npneumonia. In this study, we address this urgent need by proposing a systematic\nand unified approach for lung segmentation and COVID-19 localization with\ninfection quantification from CXR images. To accomplish this, we have\nconstructed the largest benchmark dataset with 33,920 CXR images, including\n11,956 COVID-19 samples, where the annotation of ground-truth lung segmentation\nmasks is performed on CXRs by a novel human-machine collaborative approach. An\nextensive set of experiments was performed using the state-of-the-art\nsegmentation networks, U-Net, U-Net++, and Feature Pyramid Networks (FPN). The\ndeveloped network, after an extensive iterative process, reached a superior\nperformance for lung region segmentation with Intersection over Union (IoU) of\n96.11% and Dice Similarity Coefficient (DSC) of 97.99%. Furthermore, COVID-19\ninfections of various shapes and types were reliably localized with 83.05% IoU\nand 88.21% DSC. Finally, the proposed approach has achieved an outstanding\nCOVID-19 detection performance with both sensitivity and specificity values\nabove 99%.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 18:06:06 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Tahir", "Anas M.", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Khandakar", "Amith", ""], ["Rahman", "Tawsifur", ""], ["Qiblawey", "Yazan", ""], ["Khurshid", "Uzair", ""], ["Kiranyaz", "Serkan", ""], ["Ibtehaz", "Nabil", ""], ["Rahman", "M Shohel", ""], ["Al-Madeed", "Somaya", ""], ["Hameed", "Khaled", ""], ["Hamid", "Tahir", ""], ["Mahmud", "Sakib", ""], ["Ezeddin", "Maymouna", ""]]}, {"id": "2103.07992", "submitter": "Vasilis Toulatzis", "authors": "Vasilis Toulatzis, Ioannis Fudos", "title": "Deep Tiling: Texture Tile Synthesis Using a Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texturing is a fundamental process in computer graphics. Texture is leveraged\nto enhance the visualization outcome for a 3D scene. In many cases a texture\nimage cannot cover a large 3D model surface because of its small resolution.\nConventional techniques like repeating, mirror repeating or clamp to edge do\nnot yield visually acceptable results. Deep learning based texture synthesis\nhas proven to be very effective in such cases. All deep texture synthesis\nmethods trying to create larger resolution textures are limited in terms of GPU\nmemory resources. In this paper, we propose a novel approach to example-based\ntexture synthesis by using a robust deep learning process for creating tiles of\narbitrary resolutions that resemble the structural components of an input\ntexture. In this manner, our method is firstly much less memory limited owing\nto the fact that a new texture tile of small size is synthesized and merged\nwith the original texture and secondly can easily produce missing parts of a\nlarge texture.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 18:17:37 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Toulatzis", "Vasilis", ""], ["Fudos", "Ioannis", ""]]}, {"id": "2103.08064", "submitter": "Wassim Swaileh", "authors": "Wassim Swaileh, Dimitrios Kotzinos, Suman Ghosh, Michel Jordan, Son\n  Vu, and Yaguan Qian", "title": "Versailles-FP dataset: Wall Detection in Ancient", "comments": "16 pages, submitted to ICDAR2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Access to historical monuments' floor plans over a time period is necessary\nto understand the architectural evolution and history. Such knowledge bases\nalso helps to rebuild the history by establishing connection between different\nevent, person and facts which are once part of the buildings. Since the\ntwo-dimensional plans do not capture the entire space, 3D modeling sheds new\nlight on the reading of these unique archives and thus opens up great\nperspectives for understanding the ancient states of the monument. Since the\nfirst step in the building's or monument's 3D model is the wall detection in\nthe floor plan, we introduce in this paper the new and unique Versailles FP\ndataset of wall groundtruthed images of the Versailles Palace dated between\n17th and 18th century. The dataset's wall masks are generated using an\nautomatic approach based on multi directional steerable filters. The generated\nwall masks are then validated and corrected manually. We validate our approach\nof wall mask generation in state-of-the-art modern datasets. Finally we propose\na U net based convolutional framework for wall detection. Our method achieves\nstate of the art result surpassing fully connected network based approach.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 23:27:46 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Swaileh", "Wassim", ""], ["Kotzinos", "Dimitrios", ""], ["Ghosh", "Suman", ""], ["Jordan", "Michel", ""], ["Vu", "Son", ""], ["Qian", "Yaguan", ""]]}, {"id": "2103.08074", "submitter": "Stefan Keselj", "authors": "Prem Nair, Rohan Doshi, Stefan Keselj", "title": "Pushing the Limits of Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks use pooling and other downscaling operations to\nmaintain translational invariance for detection of features, but in their\narchitecture they do not explicitly maintain a representation of the locations\nof the features relative to each other. This means they do not represent two\ninstances of the same object in different orientations the same way, like\nhumans do, and so training them often requires extensive data augmentation and\nexceedingly deep networks. A team at Google Brain recently made news with an\nattempt to fix this problem: Capsule Networks. While a normal CNN works with\nscalar outputs representing feature presence, a CapsNet works with vector\noutputs representing entity presence. We want to stress test CapsNet in various\nincremental ways to better understand their performance and expressiveness. In\nbroad terms, the goals of our investigation are: (1) test CapsNets on datasets\nthat are like MNIST but harder in a specific way, and (2) explore the internal\nembedding space and sources of error for CapsNets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 00:30:34 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Nair", "Prem", ""], ["Doshi", "Rohan", ""], ["Keselj", "Stefan", ""]]}, {"id": "2103.08082", "submitter": "Ruchi Chauhan", "authors": "Ruchi Chauhan, PK Vinod, CV Jawahar", "title": "Exploring Genetic-histologic Relationships in Breast Cancer", "comments": "Accepted at International Symposium of Biomedical Imaging (ISBI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of digital pathology presents opportunities for computer vision\nfor fast, accurate, and objective solutions for histopathological images and\naid in knowledge discovery. This work uses deep learning to predict genomic\nbiomarkers - TP53 mutation, PIK3CA mutation, ER status, PR status, HER2 status,\nand intrinsic subtypes, from breast cancer histopathology images. Furthermore,\nwe attempt to understand the underlying morphology as to how these genomic\nbiomarkers manifest in images. Since gene sequencing is expensive, not always\navailable, or even feasible, predicting these biomarkers from images would help\nin diagnosis, prognosis, and effective treatment planning. We outperform the\nexisting works with a minimum improvement of 0.02 and a maximum of 0.13 AUROC\nscores across all tasks. We also gain insights that can serve as hypotheses for\nfurther experimentations, including the presence of lymphocytes and\nkaryorrhexis. Moreover, our fully automated workflow can be extended to other\ntasks across other cancer subtypes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 00:53:47 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chauhan", "Ruchi", ""], ["Vinod", "PK", ""], ["Jawahar", "CV", ""]]}, {"id": "2103.08102", "submitter": "Jayson Haebich", "authors": "Jayson Haebich, Christian Sandor and Alvaro Cassinelli", "title": "Classifying Cycling Hazards in Egocentric Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This proposal is for the creation and annotation of an egocentric video data\nset of hazardous cycling situations. The resulting data set will facilitate\nprojects to improve the safety and experience of cyclists. Since cyclists are\nhighly sensitive to road surface conditions and hazards they require more\ndetail about road conditions when navigating their route. Features such as tram\ntracks, cobblestones, gratings, and utility access points can pose hazards or\nuncomfortable riding conditions for their journeys. Possible uses for the data\nset are identifying existing hazards in cycling infrastructure for municipal\nauthorities, real time hazard and surface condition warnings for cyclists, and\nthe identification of conditions that cause cyclists to make sudden changes in\ntheir immediate route.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 02:37:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Haebich", "Jayson", ""], ["Sandor", "Christian", ""], ["Cassinelli", "Alvaro", ""]]}, {"id": "2103.08105", "submitter": "Murilo Marques Marinho", "authors": "Masakazu Yoshimura and Murilo Marques Marinho and Kanako Harada and\n  Mamoru Mitsuishi", "title": "MBAPose: Mask and Bounding-Box Aware Pose Estimation of Surgical\n  Instruments with Photorealistic Domain Randomization", "comments": "8 pages, submitted to IROS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical robots are controlled using a priori models based on robots'\ngeometric parameters, which are calibrated before the surgical procedure. One\nof the challenges in using robots in real surgical settings is that parameters\nchange over time, consequently deteriorating control accuracy. In this context,\nour group has been investigating online calibration strategies without added\nsensors. In one step toward that goal, we have developed an algorithm to\nestimate the pose of the instruments' shafts in endoscopic images. In this\nstudy, we build upon that earlier work and propose a new framework to more\nprecisely estimate the pose of a rigid surgical instrument. Our strategy is\nbased on a novel pose estimation model called MBAPose and the use of synthetic\ntraining data. Our experiments demonstrated an improvement of 21 % for\ntranslation error and 26 % for orientation error on synthetic test data with\nrespect to our previous work. Results with real test data provide a baseline\nfor further research.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 02:53:41 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Yoshimura", "Masakazu", ""], ["Marinho", "Murilo Marques", ""], ["Harada", "Kanako", ""], ["Mitsuishi", "Mamoru", ""]]}, {"id": "2103.08109", "submitter": "Shaoning Xiao", "authors": "Shaoning Xiao, Long Chen, Songyang Zhang, Wei Ji, Jian Shao, Lu Ye,\n  Jun Xiao", "title": "Boundary Proposal Network for Two-Stage Natural Language Video\n  Localization", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to address the problem of Natural Language Video Localization\n(NLVL)-localizing the video segment corresponding to a natural language\ndescription in a long and untrimmed video. State-of-the-art NLVL methods are\nalmost in one-stage fashion, which can be typically grouped into two\ncategories: 1) anchor-based approach: it first pre-defines a series of video\nsegment candidates (e.g., by sliding window), and then does classification for\neach candidate; 2) anchor-free approach: it directly predicts the probabilities\nfor each video frame as a boundary or intermediate frame inside the positive\nsegment. However, both kinds of one-stage approaches have inherent drawbacks:\nthe anchor-based approach is susceptible to the heuristic rules, further\nlimiting the capability of handling videos with variant length. While the\nanchor-free approach fails to exploit the segment-level interaction thus\nachieving inferior results. In this paper, we propose a novel Boundary Proposal\nNetwork (BPNet), a universal two-stage framework that gets rid of the issues\nmentioned above. Specifically, in the first stage, BPNet utilizes an\nanchor-free model to generate a group of high-quality candidate video segments\nwith their boundaries. In the second stage, a visual-language fusion layer is\nproposed to jointly model the multi-modal interaction between the candidate and\nthe language query, followed by a matching score rating layer that outputs the\nalignment score for each candidate. We evaluate our BPNet on three challenging\nNLVL benchmarks (i.e., Charades-STA, TACoS and ActivityNet-Captions). Extensive\nexperiments and ablative studies on these datasets demonstrate that the BPNet\noutperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 03:06:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Xiao", "Shaoning", ""], ["Chen", "Long", ""], ["Zhang", "Songyang", ""], ["Ji", "Wei", ""], ["Shao", "Jian", ""], ["Ye", "Lu", ""], ["Xiao", "Jun", ""]]}, {"id": "2103.08116", "submitter": "Laura Zheng", "authors": "Shivam Akhauri, Laura Zheng, Tom Goldstein, Ming Lin", "title": "Improving Generalization of Transfer Learning Across Domains Using\n  Spatio-Temporal Features in Autonomous Driving", "comments": "6 pages, 3 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training vision-based autonomous driving in the real world can be inefficient\nand impractical. Vehicle simulation can be used to learn in the virtual world,\nand the acquired skills can be transferred to handle real-world scenarios more\neffectively. Between virtual and real visual domains, common features such as\nrelative distance to road edges and other vehicles over time are consistent.\nThese visual elements are intuitively crucial for human decision making during\ndriving. We hypothesize that these spatio-temporal factors can also be used in\ntransfer learning to improve generalization across domains. First, we propose a\nCNN+LSTM transfer learning framework to extract the spatio-temporal features\nrepresenting vehicle dynamics from scenes. Next, we conduct an ablation study\nto quantitatively estimate the significance of various features in the\ndecisions of driving systems. We observe that physically interpretable factors\nare highly correlated with network decisions, while representational\ndifferences between scenes are not. Finally, based on the results of our\nablation study, we propose a transfer learning pipeline that uses saliency maps\nand physical features extracted from a source model to enhance the performance\nof a target model. Training of our network is initialized with the learned\nweights from CNN and LSTM latent features (capturing the intrinsic physics of\nthe moving vehicle w.r.t. its surroundings) transferred from one domain to\nanother. Our experiments show that this proposed transfer learning framework\nbetter generalizes across unseen domains compared to a baseline CNN model on a\nbinary classification learning task.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 03:26:06 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Akhauri", "Shivam", ""], ["Zheng", "Laura", ""], ["Goldstein", "Tom", ""], ["Lin", "Ming", ""]]}, {"id": "2103.08129", "submitter": "Pranav Kadam", "authors": "Pranav Kadam, Min Zhang, Shan Liu, C.-C. Jay Kuo", "title": "R-PointHop: A Green, Accurate and Unsupervised Point Cloud Registration\n  Method", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by the recent PointHop classification method, an unsupervised 3D\npoint cloud registration method, called R-PointHop, is proposed in this work.\nR-PointHop first determines a local reference frame (LRF) for every point using\nits nearest neighbors and finds its local attributes. Next, R-PointHop obtains\nlocal-to-global hierarchical features by point downsampling, neighborhood\nexpansion, attribute construction and dimensionality reduction steps. Thus, we\ncan build the correspondence of points in the hierarchical feature space using\nthe nearest neighbor rule. Afterwards, a subset of salient points of good\ncorrespondence is selected to estimate the 3D transformation. The use of LRF\nallows for hierarchical features of points to be invariant with respect to\nrotation and translation, thus making R-PointHop more robust in building point\ncorrespondence even when rotation angles are large. Experiments are conducted\non the ModelNet40 and the Stanford Bunny dataset, which demonstrate the\neffectiveness of R-PointHop on the 3D point cloud registration task. R-PointHop\nis a green and accurate solution since its model size and training time are\nsmaller than those of deep learning methods by an order of magnitude while its\nregistration errors are smaller. Our codes are available on GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 04:12:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kadam", "Pranav", ""], ["Zhang", "Min", ""], ["Liu", "Shan", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2103.08134", "submitter": "Ghazal Mazaheri", "authors": "Ghazal Mazaheri, Amit K. Roy-Chowdhury", "title": "Detection and Localization of Facial Expression Manipulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concern regarding the wide-spread use of fraudulent images/videos in social\nmedia necessitates precise detection of such fraud. The importance of facial\nexpressions in communication is widely known, and adversarial attacks often\nfocus on manipulating the expression related features. Thus, it is important to\ndevelop methods that can detect manipulations in facial expressions, and\nlocalize the manipulated regions. To address this problem, we propose a\nframework that is able to detect manipulations in facial expression using a\nclose combination of facial expression recognition and image manipulation\nmethods. With the addition of feature maps extracted from the facial expression\nrecognition framework, our manipulation detector is able to localize the\nmanipulated region. We show that, on the Face2Face dataset, where there is\nabundant expression manipulation, our method achieves over 3% higher accuracy\nfor both classification and localization of manipulations compared to\nstate-of-the-art methods. In addition, results on the NeuralTextures dataset\nwhere the facial expressions corresponding to the mouth regions have been\nmodified, show 2% higher accuracy in both classification and localization of\nmanipulation. We demonstrate that the method performs at-par with the\nstate-of-the-art methods in cases where the expression is not manipulated, but\nrather the identity is changed, thus ensuring generalizability of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 04:35:56 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mazaheri", "Ghazal", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2103.08147", "submitter": "Xiaolong Yang", "authors": "Xiaolong Yang, Xiaohong Jia, Dihong Gong, Dong-Ming Yan, Zhifeng Li,\n  Wei Liu", "title": "LARNet: Lie Algebra Residual Network for Face Recognition", "comments": "Accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is an important yet challenging problem in computer vision.\nA major challenge in practical face recognition applications lies in\nsignificant variations between profile and frontal faces. Traditional\ntechniques address this challenge either by synthesizing frontal faces or by\npose invariant learning. In this paper, we propose a novel method with Lie\nalgebra theory to explore how face rotation in the 3D space affects the deep\nfeature generation process of convolutional neural networks (CNNs). We prove\nthat face rotation in the image space is equivalent to an additive residual\ncomponent in the feature space of CNNs, which is determined solely by the\nrotation. Based on this theoretical finding, we further design a Lie Algebraic\nResidual Network (LARNet) for tackling pose robust face recognition. Our LARNet\nconsists of a residual subnet for decoding rotation information from input face\nimages, and a gating subnet to learn rotation magnitude for controlling the\nstrength of the residual component contributing to the feature learning\nprocess. Comprehensive experimental evaluations on both frontal-profile face\ndatasets and general face recognition datasets convincingly demonstrate that\nour method consistently outperforms the state-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 05:44:54 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 11:29:26 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Yang", "Xiaolong", ""], ["Jia", "Xiaohong", ""], ["Gong", "Dihong", ""], ["Yan", "Dong-Ming", ""], ["Li", "Zhifeng", ""], ["Liu", "Wei", ""]]}, {"id": "2103.08160", "submitter": "Yang Liu", "authors": "Yang Liu, Tu Zheng, Jie Song, Deng Cai, Xiaofei He", "title": "DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor\n  Neural Network", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) aims to classify images under low-data regimes, where\nthe conventional pooled global representation is likely to lose useful local\ncharacteristics. Recent work has achieved promising performances by using deep\ndescriptors. They generally take all deep descriptors from neural networks into\nconsideration while ignoring that some of them are useless in classification\ndue to their limited receptive field, e.g., task-irrelevant descriptors could\nbe misleading and multiple aggregative descriptors from background clutter\ncould even overwhelm the object's presence. In this paper, we argue that a\nMutual Nearest Neighbor (MNN) relation should be established to explicitly\nselect the query descriptors that are most relevant to each task and discard\nless relevant ones from aggregative clutters in FSL. Specifically, we propose\nDiscriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive\nexperiments demonstrate that our method not only qualitatively selects\ntask-relevant descriptors but also quantitatively outperforms the existing\nstate-of-the-arts by a large margin of 1.8~4.9% on fine-grained CUB, a\nconsiderable margin of 1.4~2.2% on both supervised and semi-supervised\nminiImagenet, and ~1.4% on challenging tieredimagenet.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 06:57:09 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 02:42:26 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Yang", ""], ["Zheng", "Tu", ""], ["Song", "Jie", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "2103.08201", "submitter": "Adil Rasheed Professor", "authors": "Tiril Sundby, Julia Maria Graham, Adil Rasheed, Mandar Tabib, Omer San", "title": "Geometric Change Detection in Digital Twins using 3D Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital twins are meant to bridge the gap between real-world physical systems\nand virtual representations. Both stand-alone and descriptive digital twins\nincorporate 3D geometric models, which are the physical representations of\nobjects in the digital replica. Digital twin applications are required to\nrapidly update internal parameters with the evolution of their physical\ncounterpart. Due to an essential need for having high-quality geometric models\nfor accurate physical representations, the storage and bandwidth requirements\nfor storing 3D model information can quickly exceed the available storage and\nbandwidth capacity. In this work, we demonstrate a novel approach to geometric\nchange detection in the context of a digital twin. We address the issue through\na combined solution of Dynamic Mode Decomposition (DMD) for motion detection,\nYOLOv5 for object detection, and 3D machine learning for pose estimation. DMD\nis applied for background subtraction, enabling detection of moving foreground\nobjects in real-time. The video frames containing detected motion are extracted\nand used as input to the change detection network. The object detection\nalgorithm YOLOv5 is applied to extract the bounding boxes of detected objects\nin the video frames. Furthermore, the rotational pose of each object is\nestimated in a 3D pose estimation network. A series of convolutional neural\nnetworks conducts feature extraction from images and 3D model shapes. Then, the\nnetwork outputs the estimated Euler angles of the camera orientation with\nrespect to the object in the input image. By only storing data associated with\na detected change in pose, we minimize necessary storage and bandwidth\nrequirements while still being able to recreate the 3D scene on demand.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:20:16 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sundby", "Tiril", ""], ["Graham", "Julia Maria", ""], ["Rasheed", "Adil", ""], ["Tabib", "Mandar", ""], ["San", "Omer", ""]]}, {"id": "2103.08204", "submitter": "Yuda Qiu", "authors": "Yuda Qiu, Xiaojie Xu, Lingteng Qiu, Yan Pan, Yushuang Wu, Weikai Chen,\n  Xiaoguang Han", "title": "3DCaricShop: A Dataset and A Baseline Method for Single-view 3D\n  Caricature Face Reconstruction", "comments": "CVPR 2021. Project page:https://qiuyuda.github.io/3DCaricShop/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature is an artistic representation that deliberately exaggerates the\ndistinctive features of a human face to convey humor or sarcasm. However,\nreconstructing a 3D caricature from a 2D caricature image remains a challenging\ntask, mostly due to the lack of data. We propose to fill this gap by\nintroducing 3DCaricShop, the first large-scale 3D caricature dataset that\ncontains 2000 high-quality diversified 3D caricatures manually crafted by\nprofessional artists. 3DCaricShop also provides rich annotations including a\npaired 2D caricature image, camera parameters and 3D facial landmarks. To\ndemonstrate the advantage of 3DCaricShop, we present a novel baseline approach\nfor single-view 3D caricature reconstruction. To ensure a faithful\nreconstruction with plausible face deformations, we propose to connect the good\nends of the detailrich implicit functions and the parametric mesh\nrepresentations. In particular, we first register a template mesh to the output\nof the implicit generator and iteratively project the registration result onto\na pre-trained PCA space to resolve artifacts and self-intersections. To deal\nwith the large deformation during non-rigid registration, we propose a novel\nview-collaborative graph convolution network (VCGCN) to extract key points from\nthe implicit mesh for accurate alignment. Our method is able to generate\nhighfidelity 3D caricature in a pre-defined mesh topology that is\nanimation-ready. Extensive experiments have been conducted on 3DCaricShop to\nverify the significance of the database and the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:24:29 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Qiu", "Yuda", ""], ["Xu", "Xiaojie", ""], ["Qiu", "Lingteng", ""], ["Pan", "Yan", ""], ["Wu", "Yushuang", ""], ["Chen", "Weikai", ""], ["Han", "Xiaoguang", ""]]}, {"id": "2103.08213", "submitter": "Liutong Zhang", "authors": "Liutong Zhang, Lei Zhou, Ruiyang Li, Xianyu Wang, Boxuan Han, Hongen\n  Liao", "title": "Cascaded Feature Warping Network for Unsupervised Medical Image\n  Registration", "comments": "Accepted by ISBI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deformable image registration is widely utilized in medical image analysis,\nbut most proposed methods fail in the situation of complex deformations. In\nthis paper, we pre-sent a cascaded feature warping network to perform the\ncoarse-to-fine registration. To achieve this, a shared-weights encoder network\nis adopted to generate the feature pyramids for the unaligned images. The\nfeature warping registration module is then used to estimate the deformation\nfield at each level. The coarse-to-fine manner is implemented by cascading the\nmodule from the bottom level to the top level. Furthermore, the multi-scale\nloss is also introduced to boost the registration performance. We employ two\npublic benchmark datasets and conduct various experiments to evaluate our\nmethod. The results show that our method outperforms the state-of-the-art\nmethods, which also demonstrates that the cascaded feature warping network can\nperform the coarse-to-fine registration effectively and efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:50:06 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhang", "Liutong", ""], ["Zhou", "Lei", ""], ["Li", "Ruiyang", ""], ["Wang", "Xianyu", ""], ["Han", "Boxuan", ""], ["Liao", "Hongen", ""]]}, {"id": "2103.08214", "submitter": "Zhi Hou", "authors": "Zhi Hou, Baosheng Yu, Yu Qiao, Xiaojiang Peng, Dacheng Tao", "title": "Detecting Human-Object Interaction via Fabricated Compositional Learning", "comments": "Accepted to CVPR2021; update code, figures, appendix(Object Detector\n  Analysis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) detection, inferring the relationships between\nhuman and objects from images/videos, is a fundamental task for high-level\nscene understanding. However, HOI detection usually suffers from the open\nlong-tailed nature of interactions with objects, while human has extremely\npowerful compositional perception ability to cognize rare or unseen HOI\nsamples. Inspired by this, we devise a novel HOI compositional learning\nframework, termed as Fabricated Compositional Learning (FCL), to address the\nproblem of open long-tailed HOI detection. Specifically, we introduce an object\nfabricator to generate effective object representations, and then combine verbs\nand fabricated objects to compose new HOI samples. With the proposed object\nfabricator, we are able to generate large-scale HOI samples for rare and unseen\ncategories to alleviate the open long-tailed issues in HOI detection. Extensive\nexperiments on the most popular HOI detection dataset, HICO-DET, demonstrate\nthe effectiveness of the proposed method for imbalanced HOI detection and\nsignificantly improve the state-of-the-art performance on rare and unseen HOI\ncategories. Code is available at https://github.com/zhihou7/HOI-CL.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:52:56 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 11:03:15 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Hou", "Zhi", ""], ["Yu", "Baosheng", ""], ["Qiao", "Yu", ""], ["Peng", "Xiaojiang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2103.08219", "submitter": "Sulaiman Vesal", "authors": "Sulaiman Vesal, Mingxuan Gu, Ronak Kosti, Andreas Maier, Nishant\n  Ravikumar", "title": "Adapt Everywhere: Unsupervised Adaptation of Point-Clouds and Entropy\n  Minimisation for Multi-modal Cardiac Image Segmentation", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging\n  (IEEE TMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep learning models are sensitive to domain shift phenomena. A model trained\non images from one domain cannot generalise well when tested on images from a\ndifferent domain, despite capturing similar anatomical structures. It is mainly\nbecause the data distribution between the two domains is different. Moreover,\ncreating annotation for every new modality is a tedious and time-consuming\ntask, which also suffers from high inter- and intra- observer variability.\nUnsupervised domain adaptation (UDA) methods intend to reduce the gap between\nsource and target domains by leveraging source domain labelled data to generate\nlabels for the target domain. However, current state-of-the-art (SOTA) UDA\nmethods demonstrate degraded performance when there is insufficient data in\nsource and target domains. In this paper, we present a novel UDA method for\nmulti-modal cardiac image segmentation. The proposed method is based on\nadversarial learning and adapts network features between source and target\ndomain in different spaces. The paper introduces an end-to-end framework that\nintegrates: a) entropy minimisation, b) output feature space alignment and c) a\nnovel point-cloud shape adaptation based on the latent features learned by the\nsegmentation model. We validated our method on two cardiac datasets by adapting\nfrom the annotated source domain, bSSFP-MRI (balanced Steady-State Free\nProcession-MRI), to the unannotated target domain, LGE-MRI (Late-gadolinium\nenhance-MRI), for the multi-sequence dataset; and from MRI (source) to CT\n(target) for the cross-modality dataset. The results highlighted that by\nenforcing adversarial learning in different parts of the network, the proposed\nmethod delivered promising performance, compared to other SOTA methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:59:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Vesal", "Sulaiman", ""], ["Gu", "Mingxuan", ""], ["Kosti", "Ronak", ""], ["Maier", "Andreas", ""], ["Ravikumar", "Nishant", ""]]}, {"id": "2103.08236", "submitter": "Lars V\\\"ogtlin", "authors": "Lars V\\\"ogtlin, Manuel Drazyk, Vinaychandran Pondenkandath, Michele\n  Alberti, Rolf Ingold", "title": "Generating Synthetic Handwritten Historical Documents With OCR\n  Constrained GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to generate synthetic historical documents with\nprecise ground truth using nothing more than a collection of unlabeled\nhistorical images. Obtaining large labeled datasets is often the limiting\nfactor to effectively use supervised deep learning methods for Document Image\nAnalysis (DIA). Prior approaches towards synthetic data generation either\nrequire expertise or result in poor accuracy in the synthetic documents. To\nachieve high precision transformations without requiring expertise, we tackle\nthe problem in two steps. First, we create template documents with\nuser-specified content and structure. Second, we transfer the style of a\ncollection of unlabeled historical images to these template documents while\npreserving their text and layout. We evaluate the use of our synthetic\nhistorical documents in a pre-training setting and find that we outperform the\nbaselines (randomly initialized and pre-trained). Additionally, with visual\nexamples, we demonstrate a high-quality synthesis that makes it possible to\ngenerate large labeled historical document datasets with precise ground truth.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 09:39:17 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 18:52:10 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["V\u00f6gtlin", "Lars", ""], ["Drazyk", "Manuel", ""], ["Pondenkandath", "Vinaychandran", ""], ["Alberti", "Michele", ""], ["Ingold", "Rolf", ""]]}, {"id": "2103.08251", "submitter": "Xueshuang Xiang", "authors": "Wei Bao, Meiyu Huang, Yaqin Zhang, Yao Xu, Xuejiao Liu, Xueshuang\n  Xiang", "title": "Boosting ship detection in SAR images with complementary pretraining\n  techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have made significant progress in ship detection in\nsynthetic aperture radar (SAR) images. The pretraining technique is usually\nadopted to support deep neural networks-based SAR ship detectors due to the\nscarce labeled SAR images. However, directly leveraging ImageNet pretraining is\nhardly to obtain a good ship detector because of different imaging perspective\nand geometry. In this paper, to resolve the problem of inconsistent imaging\nperspective between ImageNet and earth observations, we propose an optical ship\ndetector (OSD) pretraining technique, which transfers the characteristics of\nships in earth observations to SAR images from a large-scale aerial image\ndataset. On the other hand, to handle the problem of different imaging geometry\nbetween optical and SAR images, we propose an optical-SAR matching (OSM)\npretraining technique, which transfers plentiful texture features from optical\nimages to SAR images by common representation learning on the optical-SAR\nmatching task. Finally, observing that the OSD pretraining based SAR ship\ndetector has a better recall on sea area while the OSM pretraining based SAR\nship detector can reduce false alarms on land area, we combine the predictions\nof the two detectors through weighted boxes fusion to further improve detection\nresults. Extensive experiments on four SAR ship detection datasets and two\nrepresentative CNN-based detection benchmarks are conducted to show the\neffectiveness and complementarity of the two proposed detectors, and the\nstate-of-the-art performance of the combination of the two detectors. The\nproposed method won the sixth place of ship detection in SAR images in 2020\nGaofen challenge.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 10:03:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Bao", "Wei", ""], ["Huang", "Meiyu", ""], ["Zhang", "Yaqin", ""], ["Xu", "Yao", ""], ["Liu", "Xuejiao", ""], ["Xiang", "Xueshuang", ""]]}, {"id": "2103.08259", "submitter": "Xueshuang Xiang", "authors": "Meiyu Huang, Yao Xu, Lixin Qian, Weili Shi, Yaqin Zhang, Wei Bao, Nan\n  Wang, Xuejiao Liu, Xueshuang Xiang", "title": "The QXS-SAROPT Dataset for Deep Learning in SAR-Optical Data Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have made an increasing impact on the field of\nremote sensing. However, deep neural networks based fusion of multimodal data\nfrom different remote sensors with heterogenous characteristics has not been\nfully explored, due to the lack of availability of big amounts of perfectly\naligned multi-sensor image data with diverse scenes of high resolutions,\nespecially for synthetic aperture radar (SAR) data and optical imagery. To\npromote the development of deep learning based SAR-optical fusion approaches,\nwe release the QXS-SAROPT dataset, which contains 20,000 pairs of SAR-optical\nimage patches. We obtain the SAR patches from SAR satellite GaoFen-3 images and\nthe optical patches from Google Earth images. These images cover three port\ncities: San Diego, Shanghai and Qingdao. Here, we present a detailed\nintroduction of the construction of the dataset, and show its two\nrepresentative exemplary applications, namely SAR-optical image matching and\nSAR ship detection boosted by cross-modal information from optical images. As a\nlarge open SAR-optical dataset with multiple scenes of a high resolution, we\nbelieve QXS-SAROPT will be of potential value for further research in\nSAR-optical data fusion technology based on deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 10:22:46 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 04:11:55 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Meiyu", ""], ["Xu", "Yao", ""], ["Qian", "Lixin", ""], ["Shi", "Weili", ""], ["Zhang", "Yaqin", ""], ["Bao", "Wei", ""], ["Wang", "Nan", ""], ["Liu", "Xuejiao", ""], ["Xiang", "Xueshuang", ""]]}, {"id": "2103.08271", "submitter": "Boyan Jiang", "authors": "Boyan Jiang, Yinda Zhang, Xingkui Wei, Xiangyang Xue, Yanwei Fu", "title": "Learning Compositional Representation for 4D Captures with Neural ODE", "comments": "Accepted by CVPR2021. Project webpage with video and code:\n  https://boyanjiang.github.io/4D-CR/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based representation has become the key to the success of many\ncomputer vision systems. While many 3D representations have been proposed, it\nis still an unaddressed problem how to represent a dynamically changing 3D\nobject. In this paper, we introduce a compositional representation for 4D\ncaptures, i.e. a deforming 3D object over a temporal span, that disentangles\nshape, initial state, and motion respectively. Each component is represented by\na latent code via a trained encoder. To model the motion, a neural Ordinary\nDifferential Equation (ODE) is trained to update the initial state conditioned\non the learned motion code, and a decoder takes the shape code and the updated\nstate code to reconstruct the 3D model at each time stamp. To this end, we\npropose an Identity Exchange Training (IET) strategy to encourage the network\nto learn effectively decoupling each component. Extensive experiments\ndemonstrate that the proposed method outperforms existing state-of-the-art deep\nlearning based methods on 4D reconstruction, and significantly improves on\nvarious tasks, including motion transfer and completion.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 10:55:55 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 13:13:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jiang", "Boyan", ""], ["Zhang", "Yinda", ""], ["Wei", "Xingkui", ""], ["Xue", "Xiangyang", ""], ["Fu", "Yanwei", ""]]}, {"id": "2103.08273", "submitter": "Mingi Ji", "authors": "Mingi Ji, Seungjae Shin, Seunghyun Hwang, Gibeom Park, Il-Chul Moon", "title": "Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge\n  Distillation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Knowledge distillation is a method of transferring the knowledge from a\npretrained complex teacher model to a student model, so a smaller network can\nreplace a large teacher network at the deployment stage. To reduce the\nnecessity of training a large teacher model, the recent literatures introduced\na self-knowledge distillation, which trains a student network progressively to\ndistill its own knowledge without a pretrained teacher network. While\nSelf-knowledge distillation is largely divided into a data augmentation based\napproach and an auxiliary network based approach, the data augmentation\napproach looses its local information in the augmentation process, which\nhinders its applicability to diverse vision tasks, such as semantic\nsegmentation. Moreover, these knowledge distillation approaches do not receive\nthe refined feature maps, which are prevalent in the object detection and\nsemantic segmentation community. This paper proposes a novel self-knowledge\ndistillation method, Feature Refinement via Self-Knowledge Distillation\n(FRSKD), which utilizes an auxiliary self-teacher network to transfer a refined\nknowledge for the classifier network. Our proposed method, FRSKD, can utilize\nboth soft label and feature-map distillations for the self-knowledge\ndistillation. Therefore, FRSKD can be applied to classification, and semantic\nsegmentation, which emphasize preserving the local information. We demonstrate\nthe effectiveness of FRSKD by enumerating its performance improvements in\ndiverse tasks and benchmark datasets. The implemented code is available at\nhttps://github.com/MingiJi/FRSKD.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 10:59:43 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ji", "Mingi", ""], ["Shin", "Seungjae", ""], ["Hwang", "Seunghyun", ""], ["Park", "Gibeom", ""], ["Moon", "Il-Chul", ""]]}, {"id": "2103.08286", "submitter": "Marcus Valtonen \\\"Ornhag", "authors": "Marcus Valtonen \\\"Ornhag and Patrik Persson and M{\\aa}rten Wadenb\\\"ack\n  and Kalle {\\AA}str\\\"om and Anders Heyden", "title": "Trust Your IMU: Consequences of Ignoring the IMU Drift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue that modern pre-integration methods for inertial\nmeasurement units (IMUs) are accurate enough to ignore the drift for short time\nintervals. This allows us to consider a simplified camera model, which in turn\nadmits further intrinsic calibration. We develop the first-ever solver to\njointly solve the relative pose problem with unknown and equal focal length and\nradial distortion profile while utilizing the IMU data. Furthermore, we show\nsignificant speed-up compared to state-of-the-art algorithms, with small or\nnegligible loss in accuracy for partially calibrated setups. The proposed\nalgorithms are tested on both synthetic and real data, where the latter is\nfocused on navigation using unmanned aerial vehicles (UAVs). We evaluate the\nproposed solvers on different commercially available low-cost UAVs, and\ndemonstrate that the novel assumption on IMU drift is feasible in real-life\napplications. The extended intrinsic auto-calibration enables us to use\ndistorted input images, making tedious calibration processes obsolete, compared\nto current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:24:54 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 20:25:39 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["\u00d6rnhag", "Marcus Valtonen", ""], ["Persson", "Patrik", ""], ["Wadenb\u00e4ck", "M\u00e5rten", ""], ["\u00c5str\u00f6m", "Kalle", ""], ["Heyden", "Anders", ""]]}, {"id": "2103.08290", "submitter": "Tzu-Ming Harry Hsu", "authors": "Tzu-Ming Harry Hsu, Yin-Chih Chelsea Wang", "title": "DeepOPG: Improving Orthopantomogram Finding Summarization with Weak\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Clinical finding summaries from an orthopantomogram, or a dental panoramic\nradiograph, have significant potential to improve patient communication and\nspeed up clinical judgments. While orthopantomogram is a first-line tool for\ndental examinations, no existing work has explored the summarization of\nfindings from it. A finding summary has to find teeth in the imaging study and\nlabel the teeth with several types of past treatments. To tackle the problem,\nwe developDeepOPG that breaks the summarization process into functional\nsegmentation and tooth localization, the latter of which is further refined by\na novel dental coherence module. We also leverage weak supervision labels to\nimprove detection results in a reinforcement learning scenario. Experiments\nshow high efficacy of DeepOPG on finding summarization, achieving an overall\nAUC of 88.2% in detecting six types of findings. The proposed dental coherence\nand weak supervision both are shown to improve DeepOPG by adding 5.9% and 0.4%\nto AP@IoU=0.5.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:28:45 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 15:36:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Hsu", "Tzu-Ming Harry", ""], ["Wang", "Yin-Chih Chelsea", ""]]}, {"id": "2103.08292", "submitter": "Shin-Fang Chng", "authors": "\\'Alvaro Parra, Shin-Fang Chng, Tat-Jun Chin, Anders Eriksson, Ian\n  Reid", "title": "Rotation Coordinate Descent for Fast Globally Optimal Rotation Averaging", "comments": "Accepted to CVPR 2021 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under mild conditions on the noise level of the measurements, rotation\naveraging satisfies strong duality, which enables global solutions to be\nobtained via semidefinite programming (SDP) relaxation. However, generic\nsolvers for SDP are rather slow in practice, even on rotation averaging\ninstances of moderate size, thus developing specialised algorithms is vital. In\nthis paper, we present a fast algorithm that achieves global optimality called\nrotation coordinate descent (RCD). Unlike block coordinate descent (BCD) which\nsolves SDP by updating the semidefinite matrix in a row-by-row fashion, RCD\ndirectly maintains and updates all valid rotations throughout the iterations.\nThis obviates the need to store a large dense semidefinite matrix. We\nmathematically prove the convergence of our algorithm and empirically show its\nsuperior efficiency over state-of-the-art global methods on a variety of\nproblem configurations. Maintaining valid rotations also facilitates\nincorporating local optimisation routines for further speed-ups. Moreover, our\nalgorithm is simple to implement; see supplementary material for a\ndemonstration program.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:31:34 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 02:06:53 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Parra", "\u00c1lvaro", ""], ["Chng", "Shin-Fang", ""], ["Chin", "Tat-Jun", ""], ["Eriksson", "Anders", ""], ["Reid", "Ian", ""]]}, {"id": "2103.08294", "submitter": "Tasin Ishmam", "authors": "Aniruddha Ganguly, Tasin Ishmam, Khandker Aftarul Islam, Md Zahidur\n  Rahman and Md. Shamsuzzoha Bayzid", "title": "3D-FFS: Faster 3D object detection with Focused Frustum Search in sensor\n  fusion based networks", "comments": "Contains 6 pages and 2 figures. Manuscript submitted to the IEEE\n  International Conference on Intelligent Robots and Systems (IROS) 2021 and is\n  currently pending review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose 3D-FFS, a novel approach to make sensor fusion based\n3D object detection networks significantly faster using a class of\ncomputationally inexpensive heuristics. Existing sensor fusion based networks\ngenerate 3D region proposals by leveraging inferences from 2D object detectors.\nHowever, as images have no depth information, these networks rely on extracting\nsemantic features of points from the entire scene to locate the object. By\nleveraging aggregated intrinsic properties (e.g. point density) of the 3D point\ncloud data, 3D-FFS can substantially constrain the 3D search space and thereby\nsignificantly reduce training time, inference time and memory consumption\nwithout sacrificing accuracy. To demonstrate the efficacy of 3D-FFS, we have\nintegrated it with Frustum ConvNet (F-ConvNet), a prominent sensor fusion based\n3D object detection model. We assess the performance of 3D-FFS on the KITTI\ndataset. Compared to F-ConvNet, we achieve improvements in training and\ninference times by up to 62.84% and 56.46%, respectively, while reducing the\nmemory usage by up to 58.53%. Additionally, we achieve 0.59%, 2.03% and 3.34%\nimprovements in accuracy for the Car, Pedestrian and Cyclist classes,\nrespectively. 3D-FFS shows a lot of promise in domains with limited computing\npower, such as autonomous vehicles, drones and robotics where LiDAR-Camera\nbased sensor fusion perception systems are widely used.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:32:21 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ganguly", "Aniruddha", ""], ["Ishmam", "Tasin", ""], ["Islam", "Khandker Aftarul", ""], ["Rahman", "Md Zahidur", ""], ["Bayzid", "Md. Shamsuzzoha", ""]]}, {"id": "2103.08297", "submitter": "Chiranjoy Chattopadhyay", "authors": "Shreya Goyal, Naimul Khan, Chiranjoy Chattopadhyay, Gaurav Bhatnagar", "title": "GRIHA: Synthesizing 2-Dimensional Building Layouts from Images Captured\n  using a Smart Phone", "comments": "19 pages, 22 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing an indoor scene and generating a layout/floor plan in 3D or 2D\nis a widely known problem. Quite a few algorithms have been proposed in the\nliterature recently. However, most existing methods either use RGB-D images,\nthus requiring a depth camera, or depending on panoramic photos, assuming that\nthere is little to no occlusion in the rooms. In this work, we proposed GRIHA\n(Generating Room Interior of a House using ARCore), a framework for generating\na layout using an RGB image captured using a simple mobile phone camera. We\ntake advantage of Simultaneous Localization and Mapping (SLAM) to assess the 3D\ntransformations required for layout generation. SLAM technology is built-in in\nrecent mobile libraries such as ARCore by Google. Hence, the proposed method is\nfast and efficient. It gives the user freedom to generate layout by merely\ntaking a few conventional photos, rather than relying on specialized depth\nhardware or occlusion-free panoramic images. We have compared GRIHA with other\nexisting methods and obtained superior results. Also, the system is tested on\nmultiple hardware platforms to test the dependency and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:48:45 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Goyal", "Shreya", ""], ["Khan", "Naimul", ""], ["Chattopadhyay", "Chiranjoy", ""], ["Bhatnagar", "Gaurav", ""]]}, {"id": "2103.08298", "submitter": "Chiranjoy Chattopadhyay", "authors": "Shreya Goyal, Chiranjoy Chattopadhyay, Gaurav Bhatnagar", "title": "Knowledge driven Description Synthesis for Floor Plan Interpretation", "comments": "19 pages, 18 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning is a widely known problem in the area of AI. Caption\ngeneration from floor plan images has applications in indoor path planning,\nreal estate, and providing architectural solutions. Several methods have been\nexplored in literature for generating captions or semi-structured descriptions\nfrom floor plan images. Since only the caption is insufficient to capture\nfine-grained details, researchers also proposed descriptive paragraphs from\nimages. However, these descriptions have a rigid structure and lack\nflexibility, making it difficult to use them in real-time scenarios. This paper\noffers two models, Description Synthesis from Image Cue (DSIC) and Transformer\nBased Description Generation (TBDG), for the floor plan image to text\ngeneration to fill the gaps in existing methods. These two models take\nadvantage of modern deep neural networks for visual feature extraction and text\ngeneration. The difference between both models is in the way they take input\nfrom the floor plan image. The DSIC model takes only visual features\nautomatically extracted by a deep neural network, while the TBDG model learns\ntextual captions extracted from input floor plan images with paragraphs. The\nspecific keywords generated in TBDG and understanding them with paragraphs make\nit more robust in a general floor plan image. Experiments were carried out on a\nlarge-scale publicly available dataset and compared with state-of-the-art\ntechniques to show the proposed model's superiority.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:57:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Goyal", "Shreya", ""], ["Chattopadhyay", "Chiranjoy", ""], ["Bhatnagar", "Gaurav", ""]]}, {"id": "2103.08335", "submitter": "Xuequan Lu", "authors": "Xuequan Lu, Yihao Wang, Sheldon Fung, and Xue Qing", "title": "I-Nema: A Biological Image Dataset for Nematode Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nematode worms are one of most abundant metazoan groups on the earth,\noccupying diverse ecological niches. Accurate recognition or identification of\nnematodes are of great importance for pest control, soil ecology,\nbio-geography, habitat conservation and against climate changes. Computer\nvision and image processing have witnessed a few successes in species\nrecognition of nematodes; however, it is still in great demand. In this paper,\nwe identify two main bottlenecks: (1) the lack of a publicly available imaging\ndataset for diverse species of nematodes (especially the species only found in\nnatural environment) which requires considerable human resources in field work\nand experts in taxonomy, and (2) the lack of a standard benchmark of\nstate-of-the-art deep learning techniques on this dataset which demands the\ndiscipline background in computer science. With these in mind, we propose an\nimage dataset consisting of diverse nematodes (both laboratory cultured and\nnaturally isolated), which, to our knowledge, is the first time in the\ncommunity. We further set up a species recognition benchmark by employing\nstate-of-the-art deep learning networks on this dataset. We discuss the\nexperimental results, compare the recognition accuracy of different networks,\nand show the challenges of our dataset. We make our dataset publicly available\nat: https://github.com/xuequanlu/I-Nema\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 12:29:37 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lu", "Xuequan", ""], ["Wang", "Yihao", ""], ["Fung", "Sheldon", ""], ["Qing", "Xue", ""]]}, {"id": "2103.08357", "submitter": "Dehua Song", "authors": "Wenbin Xie, Dehua Song, Chang Xu, Chunjing Xu, Hui Zhang, Yunhe Wang", "title": "Learning Frequency-aware Dynamic Network for Efficient Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods, especially convolutional neural networks (CNNs)\nhave been successfully applied in the field of single image super-resolution\n(SISR). To obtain better fidelity and visual quality, most of existing networks\nare of heavy design with massive computation. However, the computation\nresources of modern mobile devices are limited, which cannot easily support the\nexpensive cost. To this end, this paper explores a novel frequency-aware\ndynamic network for dividing the input into multiple parts according to its\ncoefficients in the discrete cosine transform (DCT) domain. In practice, the\nhigh-frequency part will be processed using expensive operations and the\nlower-frequency part is assigned with cheap operations to relieve the\ncomputation burden. Since pixels or image patches belong to low-frequency areas\ncontain relatively few textural details, this dynamic network will not affect\nthe quality of resulting super-resolution images. In addition, we embed\npredictors into the proposed dynamic network to end-to-end fine-tune the\nhandcrafted frequency-aware masks. Extensive experiments conducted on benchmark\nSISR models and datasets show that the frequency-aware dynamic network can be\nemployed for various SISR neural architectures to obtain the better tradeoff\nbetween visual quality and computational complexity. For instance, we can\nreduce the FLOPs of EDSR model by approximate $50\\%$ while preserving\nstate-of-the-art SISR performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 12:54:26 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Xie", "Wenbin", ""], ["Song", "Dehua", ""], ["Xu", "Chang", ""], ["Xu", "Chunjing", ""], ["Zhang", "Hui", ""], ["Wang", "Yunhe", ""]]}, {"id": "2103.08366", "submitter": "Stefan Schubert", "authors": "Stefan Schubert, Peer Neubert, Peter Protzel", "title": "Beyond ANN: Exploiting Structural Knowledge for Efficient Place\n  Recognition", "comments": "Accepted for publication at International Conference on Robotics and\n  Automation (ICRA) 2021. This is the submitted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is the task of recognizing same places of query\nimages in a set of database images, despite potential condition changes due to\ntime of day, weather or seasons. It is important for loop closure detection in\nSLAM and candidate selection for global localization. Many approaches in the\nliterature perform computationally inefficient full image comparisons between\nqueries and all database images. There is still a lack of suited methods for\nefficient place recognition that allow a fast, sparse comparison of only the\nmost promising image pairs without any loss in performance. While this is\npartially given by ANN-based methods, they trade speed for precision and\nadditional memory consumption, and many cannot find arbitrary numbers of\nmatching database images in case of loops in the database. In this paper, we\npropose a novel fast sequence-based method for efficient place recognition that\ncan be applied online. It uses relocalization to recover from sequence losses,\nand exploits usually available but often unused intra-database similarities for\na potential detection of all matching database images for each query in case of\nloops or stops in the database. We performed extensive experimental evaluations\nover five datasets and 21 sequence combinations, and show that our method\noutperforms two state-of-the-art approaches and even full image comparisons in\nmany cases, while providing a good tradeoff between performance and percentage\nof evaluated image pairs. Source code for Matlab will be provided with\npublication of this paper.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:10:57 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Schubert", "Stefan", ""], ["Neubert", "Peer", ""], ["Protzel", "Peter", ""]]}, {"id": "2103.08397", "submitter": "Qin Zou", "authors": "Shenhao Cao and Qin Zou and Xiuqing Mao and Zhongyuan Wang", "title": "Metric Learning for Anti-Compression Facial Forgery Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Detecting facial forgery images and videos is an increasingly important topic\nin multimedia forensics. As forgery images and videos are usually compressed\ninto different formats such as JPEG and H264 when circulating on the Internet,\nexisting forgery-detection methods trained on uncompressed data often suffer\nfrom significant performance degradation in identifying them. To solve this\nproblem, we propose a novel anti-compression facial forgery detection\nframework, which learns a compression-insensitive embedding feature space\nutilizing both original and compressed forgeries. Specifically, our approach\nconsists of three ideas: (i) extracting compression-insensitive features from\nboth uncompressed and compressed forgeries using an adversarial learning\nstrategy; (ii) learning a robust partition by constructing a metric loss that\ncan reduce the distance of the paired original and compressed images in the\nembedding space; (iii) improving the accuracy of tampered localization with an\nattention-transfer module. Experimental results demonstrate that, the proposed\nmethod is highly effective in handling both compressed and uncompressed facial\nforgery images.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 14:11:14 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 05:43:36 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cao", "Shenhao", ""], ["Zou", "Qin", ""], ["Mao", "Xiuqing", ""], ["Wang", "Zhongyuan", ""]]}, {"id": "2103.08439", "submitter": "Chenfei Wang", "authors": "Li Wang, Chenfei Wang, Xinyu Zhang, Tianwei Lan, Jun Li", "title": "S-AT GCN: Spatial-Attention Graph Convolution Network based Feature\n  Enhancement for 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection plays a crucial role in environmental perception for\nautonomous vehicles, which is the prerequisite of decision and control. This\npaper analyses partition-based methods' inherent drawbacks. In the partition\noperation, a single instance such as a pedestrian is sliced into several\npieces, which we call it the partition effect. We propose the Spatial-Attention\nGraph Convolution (S-AT GCN), forming the Feature Enhancement (FE) layers to\novercome this drawback. The S-AT GCN utilizes the graph convolution and the\nspatial attention mechanism to extract local geometrical structure features.\nThis allows the network to have more meaningful features for the foreground.\nOur experiments on the KITTI 3D object and bird's eye view detection show that\nS-AT Conv and FE layers are effective, especially for small objects. FE layers\nboost the pedestrian class performance by 3.62\\% and cyclist class by 4.21\\% 3D\nmAP. The time cost of these extra FE layers are limited. PointPillars with FE\nlayers can achieve 48 PFS, satisfying the real-time requirement.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:08:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wang", "Li", ""], ["Wang", "Chenfei", ""], ["Zhang", "Xinyu", ""], ["Lan", "Tianwei", ""], ["Li", "Jun", ""]]}, {"id": "2103.08454", "submitter": "Zhizhe Liu", "authors": "Zhizhe Liu, Zhenfeng Zhu, Shuai Zheng, Yang Liu, Jiayu Zhou and Yao\n  Zhao", "title": "Margin Preserving Self-paced Contrastive Learning Towards Domain\n  Adaptation for Medical Image Segmentation", "comments": "12 pages, 10 figures, submitted to JBHI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To bridge the gap between the source and target domains in unsupervised\ndomain adaptation (UDA), the most common strategy puts focus on matching the\nmarginal distributions in the feature space through adversarial learning.\nHowever, such category-agnostic global alignment lacks of exploiting the\nclass-level joint distributions, causing the aligned distribution less\ndiscriminative. To address this issue, we propose in this paper a novel margin\npreserving self-paced contrastive Learning (MPSCL) model for cross-modal\nmedical image segmentation. Unlike the conventional construction of contrastive\npairs in contrastive learning, the domain-adaptive category prototypes are\nutilized to constitute the positive and negative sample pairs. With the\nguidance of progressively refined semantic prototypes, a novel margin\npreserving contrastive loss is proposed to boost the discriminability of\nembedded representation space. To enhance the supervision for contrastive\nlearning, more informative pseudo-labels are generated in target domain in a\nself-paced way, thus benefiting the category-aware distribution alignment for\nUDA. Furthermore, the domain-invariant representations are learned through\njoint contrastive learning between the two domains. Extensive experiments on\ncross-modal cardiac segmentation tasks demonstrate that MPSCL significantly\nimproves semantic segmentation performance, and outperforms a wide variety of\nstate-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:23:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liu", "Zhizhe", ""], ["Zhu", "Zhenfeng", ""], ["Zheng", "Shuai", ""], ["Liu", "Yang", ""], ["Zhou", "Jiayu", ""], ["Zhao", "Yao", ""]]}, {"id": "2103.08457", "submitter": "Zhiwei Xu", "authors": "Zhiwei Xu, Thalaiyasingam Ajanthan, Vibhav Vineet, Richard Hartley", "title": "RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs", "comments": "this is an extension of our 3DV2020 conference paper RANP. arXiv\n  admin note: substantial text overlap with arXiv:2010.02488", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although 3D Convolutional Neural Networks are essential for most learning\nbased applications involving dense 3D data, their applicability is limited due\nto excessive memory and computational requirements. Compressing such networks\nby pruning therefore becomes highly desirable. However, pruning 3D CNNs is\nlargely unexplored possibly because of the complex nature of typical pruning\nalgorithms that embeds pruning into an iterative optimization paradigm. In this\nwork, we introduce a Resource Aware Neuron Pruning (RANP) algorithm that prunes\n3D CNNs at initialization to high sparsity levels. Specifically, the core idea\nis to obtain an importance score for each neuron based on their sensitivity to\nthe loss function. This neuron importance is then reweighted according to the\nneuron resource consumption related to FLOPs or memory. We demonstrate the\neffectiveness of our pruning method on 3D semantic segmentation with widely\nused 3D-UNets on ShapeNet and BraTS'18 datasets, video classification with\nMobileNetV2 and I3D on UCF101 dataset, and two-view stereo matching with\nPyramid Stereo Matching (PSM) network on SceneFlow dataset. In these\nexperiments, our RANP leads to roughly 50%-95% reduction in FLOPs and 35%-80%\nreduction in memory with negligible loss in accuracy compared to the unpruned\nnetworks. This significantly reduces the computational resources required to\ntrain 3D CNNs. The pruned network obtained by our algorithm can also be easily\nscaled up and transferred to another dataset for training.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 04:35:29 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Xu", "Zhiwei", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Vineet", "Vibhav", ""], ["Hartley", "Richard", ""]]}, {"id": "2103.08468", "submitter": "Kranti Kumar Parida", "authors": "Kranti Kumar Parida, Siddharth Srivastava, Gaurav Sharma", "title": "Beyond Image to Depth: Improving Depth Prediction using Echoes", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of estimating depth with multi modal audio visual\ndata. Inspired by the ability of animals, such as bats and dolphins, to infer\ndistance of objects with echolocation, some recent methods have utilized echoes\nfor depth estimation. We propose an end-to-end deep learning based pipeline\nutilizing RGB images, binaural echoes and estimated material properties of\nvarious objects within a scene. We argue that the relation between image,\nechoes and depth, for different scene elements, is greatly influenced by the\nproperties of those elements, and a method designed to leverage this\ninformation can lead to significantly improved depth estimation from audio\nvisual inputs. We propose a novel multi modal fusion technique, which\nincorporates the material properties explicitly while combining audio (echoes)\nand visual modalities to predict the scene depth. We show empirically, with\nexperiments on Replica dataset, that the proposed method obtains 28%\nimprovement in RMSE compared to the state-of-the-art audio-visual depth\nprediction method. To demonstrate the effectiveness of our method on larger\ndataset, we report competitive performance on Matterport3D, proposing to use it\nas a multimodal depth prediction benchmark with echoes for the first time. We\nalso analyse the proposed method with exhaustive ablation experiments and\nqualitative results. The code and models are available at\nhttps://krantiparida.github.io/projects/bimgdepth.html\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:45:24 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 16:52:43 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Parida", "Kranti Kumar", ""], ["Srivastava", "Siddharth", ""], ["Sharma", "Gaurav", ""]]}, {"id": "2103.08475", "submitter": "Tianfu Wu", "authors": "Wei Sun and Tianfu Wu", "title": "Deep Consensus Learning", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both generative learning and discriminative learning have recently witnessed\nremarkable progress using Deep Neural Networks (DNNs). For structured input\nsynthesis and structured output prediction problems (e.g., layout-to-image\nsynthesis and image semantic segmentation respectively), they often are studied\nseparately. This paper proposes deep consensus learning (DCL) for joint\nlayout-to-image synthesis and weakly-supervised image semantic segmentation.\nThe former is realized by a recently proposed LostGAN approach, and the latter\nby introducing an inference network as the third player joining the two-player\ngame of LostGAN. Two deep consensus mappings are exploited to facilitate\ntraining the three networks end-to-end: Given an input layout (a list of object\nbounding boxes), the generator generates a mask (label map) and then use it to\nhelp synthesize an image. The inference network infers the mask for the\nsynthesized image. Then, the latent consensus is measured between the mask\ngenerated by the generator and the one inferred by the inference network. For\nthe real image corresponding to the input layout, its mask also is computed by\nthe inference network, and then used by the generator to reconstruct the real\nimage. Then, the data consensus is measured between the real image and its\nreconstructed image. The discriminator still plays the role of an adversary by\ncomputing the realness scores for a real image, its reconstructed image and a\nsynthesized image. In experiments, our DCL is tested in the COCO-Stuff dataset.\nIt obtains compelling layout-to-image synthesis results and weakly-supervised\nimage semantic segmentation results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:51:14 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sun", "Wei", ""], ["Wu", "Tianfu", ""]]}, {"id": "2103.08482", "submitter": "Christoph Angermann", "authors": "Christoph Angermann, Steinbj\\\"orn J\\'onsson, Markus Haltmeier, Ad\\'ela\n  Moravov\\'a, Christian Laubichler, Constantin Kiesling, Martin Kober, Wolfgang\n  Fimml", "title": "Machine Learning for Nondestructive Wear Assessment in Large Internal\n  Combustion Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitalization offers a large number of promising tools for large internal\ncombustion engines such as condition monitoring or condition-based maintenance.\nThis includes the status evaluation of key engine components such as cylinder\nliners, whose inner surfaces are subject to constant wear due to their movement\nrelative to the pistons. Existing state-of-the-art methods for quantifying wear\nrequire disassembly and cutting of the examined liner followed by a\nhigh-resolution microscopic surface depth measurement that quantitatively\nevaluates wear based on bearing load curves (also known as Abbott-Firestone\ncurves). Such reference methods are destructive, time-consuming and costly. The\ngoal of the research presented here is to develop nondestructive yet reliable\nmethods for quantifying the surface condition. A deep-learning framework is\nproposed that allows computation of the bearing load curves from reflection RGB\nimages of the liner surface that can be collected with a wide variety of simple\nimaging devices, without the need to remove and destroy the investigated liner.\nFor this purpose, a convolutional neural network is trained to predict the\nbearing load curve of the corresponding depth profile from the collected RGB\nimages, which in turn can be used for further wear evaluation. Training of the\nnetwork is performed using a custom-built database containing depth profiles\nand reflection images of liner surfaces of large gas engines. The results of\nthe proposed method are visually examined and quantified considering several\nprobabilistic distance metrics and comparison of roughness indicators between\nground truth and model predictions. The observed success of the proposed method\nsuggests its great potential for quantitative wear assessment on engines during\nservice directly on site.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:01:17 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 08:44:05 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Angermann", "Christoph", ""], ["J\u00f3nsson", "Steinbj\u00f6rn", ""], ["Haltmeier", "Markus", ""], ["Moravov\u00e1", "Ad\u00e9la", ""], ["Laubichler", "Christian", ""], ["Kiesling", "Constantin", ""], ["Kober", "Martin", ""], ["Fimml", "Wolfgang", ""]]}, {"id": "2103.08491", "submitter": "Sherif Abdulatif", "authors": "Karim Armanious, Sherif Abdulatif, Wenbin Shi, Tobias Hepp, Sergios\n  Gatidis, Bin Yang", "title": "Uncertainty-Based Biological Age Estimation of Brain MRI Scans", "comments": "Accepted in ICASSP 2021. 5 pages, 4 figures. arXiv admin note:\n  substantial text overlap with arXiv:2009.10765", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9414112", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age is an essential factor in modern diagnostic procedures. However,\nassessment of the true biological age (BA) remains a daunting task due to the\nlack of reference ground-truth labels. Current BA estimation approaches are\neither restricted to skeletal images or rely on non-imaging modalities that\nyield a whole-body BA assessment. However, various organ systems may exhibit\ndifferent aging characteristics due to lifestyle and genetic factors. In this\ninitial study, we propose a new framework for organ-specific BA estimation\nutilizing 3D magnetic resonance image (MRI) scans. As a first step, this\nframework predicts the chronological age (CA) together with the corresponding\npatient-dependent aleatoric uncertainty. An iterative training algorithm is\nthen utilized to segregate atypical aging patients from the given population\nbased on the predicted uncertainty scores. In this manner, we hypothesize that\ntraining a new model on the remaining population should approximate the true BA\nbehavior. We apply the proposed methodology on a brain MRI dataset containing\nhealthy individuals as well as Alzheimer's patients. We demonstrate the\ncorrelation between the predicted BAs and the expected cognitive deterioration\nin Alzheimer's patients.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:08:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Armanious", "Karim", ""], ["Abdulatif", "Sherif", ""], ["Shi", "Wenbin", ""], ["Hepp", "Tobias", ""], ["Gatidis", "Sergios", ""], ["Yang", "Bin", ""]]}, {"id": "2103.08497", "submitter": "Jannik Schmitt", "authors": "Jannik Schmitt and Stefan Roth", "title": "Sampling-free Variational Inference for Neural Networks with\n  Multiplicative Activation Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To adopt neural networks in safety critical domains, knowing whether we can\ntrust their predictions is crucial. Bayesian neural networks (BNNs) provide\nuncertainty estimates by averaging predictions with respect to the posterior\nweight distribution. Variational inference methods for BNNs approximate the\nintractable weight posterior with a tractable distribution, yet mostly rely on\nsampling from the variational distribution during training and inference.\nRecent sampling-free approaches offer an alternative, but incur a significant\nparameter overhead. We here propose a more efficient parameterization of the\nposterior approximation for sampling-free variational inference that relies on\nthe distribution induced by multiplicative Gaussian activation noise. This\nallows us to combine parameter efficiency with the benefits of sampling-free\nvariational inference. Our approach yields competitive results for standard\nregression problems and scales well to large-scale image classification tasks\nincluding ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:16:18 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 07:46:18 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Schmitt", "Jannik", ""], ["Roth", "Stefan", ""]]}, {"id": "2103.08501", "submitter": "Samyak Prajapati", "authors": "Shaswat Patel, Maithili Lohakare, Samyak Prajapati, Shaanya Singh,\n  Nancy Patel", "title": "DiaRet: A browser-based application for the grading of Diabetic\n  Retinopathy with Integrated Gradients", "comments": "Modified abstract and replaced figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Patients with long-standing diabetes often fall prey to Diabetic Retinopathy\n(DR) resulting in changes in the retina of the human eye, which may lead to\nloss of vision in extreme cases. The aim of this study is two-fold: (a) create\ndeep learning models that were trained to grade degraded retinal fundus images\nand (b) to create a browser-based application that will aid in diagnostic\nprocedures by highlighting the key features of the fundus image. In this\nresearch work, we have emulated the images plagued by distortions by degrading\nthe images based on multiple different combinations of Light Transmission\nDisturbance, Image Blurring and insertion of Retinal Artifacts. InceptionV3,\nResNet-50 and InceptionResNetV2 were trained and used to classify retinal\nfundus images based on their severity level and then further used in the\ncreation of a browser-based application, which implements the Integration\nGradient (IG) Attribution Mask on the input image and demonstrates the\npredictions made by the model and the probability associated with each class.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:19:56 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 19:26:55 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 16:21:45 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Patel", "Shaswat", ""], ["Lohakare", "Maithili", ""], ["Prajapati", "Samyak", ""], ["Singh", "Shaanya", ""], ["Patel", "Nancy", ""]]}, {"id": "2103.08504", "submitter": "Mohammad Reza Mohebbian", "authors": "Mohammad Reza Mohebbian, Seyed Shahim Vedaei, Khan A. Wahid and Paul\n  Babyn", "title": "Siamese Network Features for Endoscopy Image and Video Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional Endoscopy (CE) and Wireless Capsule Endoscopy (WCE) are known\ntools for diagnosing gastrointestinal (GI) tract disorders. Localizing frames\nprovide valuable information about the anomaly location and also can help\nclinicians determine a more appropriate treatment plan. There are many\nautomated algorithms to detect the anomaly. However, very few of the existing\nworks address the issue of localization. In this study, we present a\ncombination of meta-learning and deep learning for localizing both endoscopy\nimages and video. A dataset is collected from 10 different anatomical positions\nof human GI tract. In the meta-learning section, the system was trained using\n78 CE and 27 WCE annotated frames with a modified Siamese Neural Network (SNN)\nto predict the location of one single image/frame. Then, a postprocessing\nsection using bidirectional long short-term memory is proposed for localizing a\nsequence of frames. Here, we have employed feature vector, distance and\npredicted location obtained from a trained SNN. The postprocessing section is\ntrained and tested on 1,028 and 365 seconds of CE and WCE videos using hold-out\nvalidation (50%), and achieved F1-score of 86.3% and 83.0%, respectively. In\naddition, we performed subjective evaluation using nine gastroenterologists.\nThe results show that the computer-aided methods can outperform\ngastroenterologists assessment of localization. The proposed method is compared\nwith various approaches, such as support vector machine with hand-crafted\nfeatures, convolutional neural network and the transfer learning-based methods,\nand showed better results. Therefore, it can be used in frame localization,\nwhich can help in video summarization and anomaly detection.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:24:30 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mohebbian", "Mohammad Reza", ""], ["Vedaei", "Seyed Shahim", ""], ["Wahid", "Khan A.", ""], ["Babyn", "Paul", ""]]}, {"id": "2103.08508", "submitter": "Mohammad Reza Mohebbian", "authors": "Mohammad Reza Mohebbian, Seyed Shahim Vedaei, Khan A. Wahid and Paul\n  Babyn", "title": "Multiclass Anomaly Detection in GI Endoscopic Images using Optimized\n  Deep One-class Classification in an Imbalanced Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wireless Capsule Endoscopy helps physicians examine the gastrointestinal (GI)\ntract noninvasively, with the cost of generating many images. Many available\ndatasets, such as KID2 and Kvasir, suffer from imbalance issue which make it\ndifficult to train an effective artificial intelligence (AI) system. Moreover,\nincreasing number of classes makes the problem worse. In this study, an\nensemble of one-class classifiers is used for detecting anomaly. This method\nfocuses on learning single models using samples from only one class, and\nensemble all models for multiclass classification. A total of 1,778 normal, 227\ninflammation, 303 vascular diseases, and 44 polyp images have been used from\nthe KID2 dataset. In the first step, deep features are extracted based on an\nautoencoder architecture from the preprocessed images. Then, these features are\noversampled using Synthetic Minority Over-sampling Technique and clustered\nusing Ordering Points to Identify the Clustering Structure. To create one-class\nclassification model, the Support Vector Data Descriptions are trained on each\ncluster with the help of Ant Colony Optimization, which is also used for tuning\nclustering parameters for improving F1-score. This process is applied on each\nclasses and ensemble of final models used for multiclass classification. The\nentire algorithm ran 5 times and obtained F1-score 96.3 +- 0.2% and\nmacro-average F1-score 85.0 +- 0.4%, for anomaly detection and multiclass\nclassification, respectively. The results are compared with GoogleNet, AlexNet,\nResnet50, VGG16 and other published algorithms, and demonstrate that the\nproposed method is a competitive choice for multiclass class anomaly detection\nin GI images.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:28:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mohebbian", "Mohammad Reza", ""], ["Vedaei", "Seyed Shahim", ""], ["Wahid", "Khan A.", ""], ["Babyn", "Paul", ""]]}, {"id": "2103.08516", "submitter": "Mohammad Reza Mohebbian", "authors": "Mohammad Reza Mohebbian, Ekta Walia, Khan A. Wahid", "title": "Which K-Space Sampling Schemes is good for Motion Artifact Detection in\n  Magnetic Resonance Imaging?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motion artifacts are a common occurrence in the Magnetic Resonance Imaging\n(MRI) exam. Motion during acquisition has a profound impact on workflow\nefficiency, often requiring a repeat of sequences. Furthermore, motion\nartifacts may escape notice by technologists, only to be revealed at the time\nof reading by the radiologists, affecting their diagnostic quality. Designing a\ncomputer-aided tool for automatic motion detection and elimination can improve\nthe diagnosis, however, it needs a deep understanding of motion\ncharacteristics. Motion artifacts in MRI have a complex nature and it is\ndirectly related to the k-space sampling scheme. In this study we investigate\nthe effect of three conventional k-space samplers, including Cartesian, Uniform\nSpiral and Radial on motion induced image distortion. In this regard, various\nsynthetic motions with different trajectories of displacement and rotation are\napplied to T1 and T2-weighted MRI images, and a convolutional neural network is\ntrained to show the difficulty of motion classification. The results show that\nthe spiral k-space sampling method get less effect of motion artifact in image\nspace as compared to radial k-space sampled images, and radial k-space sampled\nimages are more robust than Cartesian ones. Cartesian samplers, on the other\nhand, are the best in terms of deep learning motion detection because they can\nbetter reflect motion.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:38:40 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mohebbian", "Mohammad Reza", ""], ["Walia", "Ekta", ""], ["Wahid", "Khan A.", ""]]}, {"id": "2103.08533", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Andreas Themelis, Panagiotis Patrinos", "title": "Lasry-Lions Envelopes and Nonconvex Optimization: A Homotopy Approach", "comments": "29th Eur. Signal Process. Conf. (EUSIPCO 2021), accepted. 5 pages, 2\n  figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale optimization, the presence of nonsmooth and nonconvex terms in\na given problem typically makes it hard to solve. A popular approach to address\nnonsmooth terms in convex optimization is to approximate them with their\nrespective Moreau envelopes. In this work, we study the use of Lasry-Lions\ndouble envelopes to approximate nonsmooth terms that are also not convex. These\nenvelopes are an extension of the Moreau ones but exhibit an additional\nsmoothness property that makes them amenable to fast optimization algorithms.\nLasry-Lions envelopes can also be seen as an \"intermediate\" between a given\nfunction and its convex envelope, and we make use of this property to develop a\nmethod that builds a sequence of approximate subproblems that are easier to\nsolve than the original problem. We discuss convergence properties of this\nmethod when used to address composite minimization problems; additionally,\nbased on a number of experiments, we discuss settings where it may be more\nuseful than classical alternatives in two domains: signal decoding and spectral\nunmixing.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:55:11 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 09:21:35 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Themelis", "Andreas", ""], ["Patrinos", "Panagiotis", ""]]}, {"id": "2103.08562", "submitter": "Kai Packh\\\"auser", "authors": "Kai Packh\\\"auser, Sebastian G\\\"undel, Nicolas M\\\"unster, Christopher\n  Syben, Vincent Christlein, Andreas Maier", "title": "Is Medical Chest X-ray Data Anonymous?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rise and ever-increasing potential of deep learning techniques in\nrecent years, publicly available medical datasets became a key factor to enable\nreproducible development of diagnostic algorithms in the medical domain.\nMedical data contains sensitive patient-related information and is therefore\nusually anonymized by removing patient identifiers, e.g., patient names before\npublication. To the best of our knowledge, we are the first to show that a\nwell-trained deep learning system is able to recover the patient identity from\nchest X-ray data. We demonstrate this using the publicly available large-scale\nChestX-ray14 dataset, a collection of 112,120 frontal-view chest X-ray images\nfrom 30,805 unique patients. Our verification system is able to identify\nwhether two frontal chest X-ray images are from the same person with an AUC of\n0.9940 and a classification accuracy of 95.55%. We further highlight that the\nproposed system is able to reveal the same person even ten and more years after\nthe initial scan. When pursuing a retrieval approach, we observe an mAP@R of\n0.9748 and a precision@1 of 0.9963. Furthermore, we achieve an AUC of up to\n0.9870 and a precision@1 of up to 0.9444 when evaluating our trained networks\non CheXpert and the COVID-19 Image Data Collection. Based on this high\nidentification rate, a potential attacker may leak patient-related information\nand additionally cross-reference images to obtain more information. Thus, there\nis a great risk of sensitive content falling into unauthorized hands or being\ndisseminated against the will of the concerned patients. Especially during the\nCOVID-19 pandemic, numerous chest X-ray datasets have been published to advance\nresearch. Therefore, such data may be vulnerable to potential attacks by deep\nlearning-based re-identification algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:26:43 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 17:22:04 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 10:36:57 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Packh\u00e4user", "Kai", ""], ["G\u00fcndel", "Sebastian", ""], ["M\u00fcnster", "Nicolas", ""], ["Syben", "Christopher", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""]]}, {"id": "2103.08573", "submitter": "Udit Singh Parihar", "authors": "Udit Singh Parihar, Aniket Gujarathi, Kinal Mehta, Satyajit Tourani,\n  Sourav Garg, Michael Milford and K. Madhava Krishna", "title": "RoRD: Rotation-Robust Descriptors and Orthographic Views for Local\n  Feature Matching", "comments": "Accepted to IROS 2021. Project Page:\n  https://uditsinghparihar.github.io/RoRD/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of local detectors and descriptors in typical computer vision\npipelines work well until variations in viewpoint and appearance change become\nextreme. Past research in this area has typically focused on one of two\napproaches to this challenge: the use of projections into spaces more suitable\nfor feature matching under extreme viewpoint changes, and attempting to learn\nfeatures that are inherently more robust to viewpoint change. In this paper, we\npresent a novel framework that combines learning of invariant descriptors\nthrough data augmentation and orthographic viewpoint projection. We propose\nrotation-robust local descriptors, learnt through training data augmentation\nbased on rotation homographies, and a correspondence ensemble technique that\ncombines vanilla feature correspondences with those obtained through\nrotation-robust features. Using a range of benchmark datasets as well as\ncontributing a new bespoke dataset for this research domain, we evaluate the\neffectiveness of the proposed approach on key tasks including pose estimation\nand visual place recognition. Our system outperforms a range of baseline and\nstate-of-the-art techniques, including enabling higher levels of place\nrecognition precision across opposing place viewpoints and achieves\npractically-useful performance levels even under extreme viewpoint changes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:40:25 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 20:28:12 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Parihar", "Udit Singh", ""], ["Gujarathi", "Aniket", ""], ["Mehta", "Kinal", ""], ["Tourani", "Satyajit", ""], ["Garg", "Sourav", ""], ["Milford", "Michael", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2103.08590", "submitter": "Adrianna Janik", "authors": "Adrianna Janik, Jonathan Dodd, Georgiana Ifrim, Kris Sankaran,\n  Kathleen Curran", "title": "Interpretability of a Deep Learning Model in the Application of Cardiac\n  MRI Segmentation with an ACDC Challenge Dataset", "comments": null, "journal-ref": null, "doi": "10.1117/12.2582227", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardiac Magnetic Resonance (CMR) is the most effective tool for the\nassessment and diagnosis of a heart condition, which malfunction is the world's\nleading cause of death. Software tools leveraging Artificial Intelligence\nalready enhance radiologists and cardiologists in heart condition assessment\nbut their lack of transparency is a problem. This project investigates if it is\npossible to discover concepts representative for different cardiac conditions\nfrom the deep network trained to segment crdiac structures: Left Ventricle\n(LV), Right Ventricle (RV) and Myocardium (MYO), using explainability methods\nthat enhances classification system by providing the score-based values of\nqualitative concepts, along with the key performance metrics. With introduction\nof a need of explanations in GDPR explainability of AI systems is necessary.\nThis study applies Discovering and Testing with Concept Activation Vectors\n(D-TCAV), an interpretaibilty method to extract underlying features important\nfor cardiac disease diagnosis from MRI data. The method provides a quantitative\nnotion of concept importance for disease classified. In previous studies, the\nbase method is applied to the classification of cardiac disease and provides\nclinically meaningful explanations for the predictions of a black-box deep\nlearning classifier. This study applies a method extending TCAV with a\nDiscovering phase (D-TCAV) to cardiac MRI analysis. The advantage of the D-TCAV\nmethod over the base method is that it is user-independent. The contribution of\nthis study is a novel application of the explainability method D-TCAV for\ncardiac MRI anlysis. D-TCAV provides a shorter pre-processing time for\nclinicians than the base method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:57:40 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Janik", "Adrianna", ""], ["Dodd", "Jonathan", ""], ["Ifrim", "Georgiana", ""], ["Sankaran", "Kris", ""], ["Curran", "Kathleen", ""]]}, {"id": "2103.08637", "submitter": "Nikhil Churamani", "authors": "Nikhil Churamani, Ozgur Kara and Hatice Gunes", "title": "Domain-Incremental Continual Learning for Mitigating Bias in Facial\n  Expression and Action Unit Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Facial Expression Recognition (FER) systems become integrated into our\ndaily lives, these systems need to prioritise making fair decisions instead of\naiming at higher individual accuracy scores. Ranging from surveillance systems\nto diagnosing mental and emotional health conditions of individuals, these\nsystems need to balance the accuracy vs fairness trade-off to make decisions\nthat do not unjustly discriminate against specific under-represented\ndemographic groups. Identifying bias as a critical problem in facial analysis\nsystems, different methods have been proposed that aim to mitigate bias both at\ndata and algorithmic levels. In this work, we propose the novel usage of\nContinual Learning (CL), in particular, using Domain-Incremental Learning\n(Domain-IL) settings, as a potent bias mitigation method to enhance the\nfairness of FER systems while guarding against biases arising from skewed data\ndistributions. We compare different non-CL-based and CL-based methods for their\nclassification accuracy and fairness scores on expression recognition and\nAction Unit (AU) detection tasks using two popular benchmarks, the RAF-DB and\nBP4D datasets, respectively. Our experimental results show that CL-based\nmethods, on average, outperform other popular bias mitigation techniques on\nboth accuracy and fairness metrics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 18:22:17 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Churamani", "Nikhil", ""], ["Kara", "Ozgur", ""], ["Gunes", "Hatice", ""]]}, {"id": "2103.08640", "submitter": "Ching-Hsun Tseng", "authors": "Ching-Hsun Tseng, Shin-Jye Lee, Jia-Nan Feng, Shengzhong Mao, Yu-Ping\n  Wu, Jia-Yu Shang, Mou-Chung Tseng, and Xiao-Jun Zeng", "title": "UPANets: Learning from the Universal Pixel Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among image classification, skip and densely-connection-based networks have\ndominated most leaderboards. Recently, from the successful development of\nmulti-head attention in natural language processing, it is sure that now is a\ntime of either using a Transformer-like model or hybrid CNNs with attention.\nHowever, the former need a tremendous resource to train, and the latter is in\nthe perfect balance in this direction. In this work, to make CNNs handle global\nand local information, we proposed UPANets, which equips channel-wise attention\nwith a hybrid skip-densely-connection structure. Also, the extreme-connection\nstructure makes UPANets robust with a smoother loss landscape. In experiments,\nUPANets surpassed most well-known and widely-used SOTAs with an accuracy of\n96.47% in Cifar-10, 80.29% in Cifar-100, and 67.67% in Tiny Imagenet. Most\nimportantly, these performances have high parameters efficiency and only\ntrained in one customer-based GPU. We share implementing code of UPANets in\nhttps://github.com/hanktseng131415go/UPANets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 18:27:59 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 13:29:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tseng", "Ching-Hsun", ""], ["Lee", "Shin-Jye", ""], ["Feng", "Jia-Nan", ""], ["Mao", "Shengzhong", ""], ["Wu", "Yu-Ping", ""], ["Shang", "Jia-Yu", ""], ["Tseng", "Mou-Chung", ""], ["Zeng", "Xiao-Jun", ""]]}, {"id": "2103.08657", "submitter": "Anouar Ben Mabrouk", "authors": "Malika Jallouli, Makerem Zemni, Anouar Ben Mabrouk and Mohamed Ali\n  Mahjoub", "title": "Towards New Multiwavelets: Associated Filters and Algorithms. Part I:\n  Theoretical Framework and Investigation of Biomedical Signals, ECG and\n  Coronavirus Cases", "comments": "28 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV physics.med-ph", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Biosignals are nowadays important subjects for scientific researches from\nboth theory and applications especially with the appearance of new pandemics\nthreatening humanity such as the new Coronavirus. One aim in the present work\nis to prove that Wavelets may be successful machinery to understand such\nphenomena by applying a step forward extension of wavelets to multiwavelets. We\nproposed in a first step to improve the multiwavelet notion by constructing\nmore general families using independent components for multi-scaling and\nmultiwavelet mother functions. A special multiwavelet is then introduced,\ncontinuous and discrete multiwavelet transforms are associated, as well as new\nfilters and algorithms of decomposition and reconstruction. The constructed\nmultiwavelet framework is applied for some experimentations showing fast\nalgorithms, ECG signal, and a strain of Coronavirus processing.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:49:21 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Jallouli", "Malika", ""], ["Zemni", "Makerem", ""], ["Mabrouk", "Anouar Ben", ""], ["Mahjoub", "Mohamed Ali", ""]]}, {"id": "2103.08700", "submitter": "Ecem Sogancioglu", "authors": "Ecem Sogancioglu, Erdi \\c{C}all{\\i}, Bram van Ginneken, Kicky G. van\n  Leeuwen, Keelin Murphy", "title": "Deep Learning for Chest X-ray Analysis: A Survey", "comments": "Under review in Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2021.102125", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent advances in deep learning have led to a promising performance in many\nmedical image analysis tasks. As the most commonly performed radiological exam,\nchest radiographs are a particularly important modality for which a variety of\napplications have been researched. The release of multiple, large, publicly\navailable chest X-ray datasets in recent years has encouraged research interest\nand boosted the number of publications. In this paper, we review all studies\nusing deep learning on chest radiographs, categorizing works by task:\nimage-level prediction (classification and regression), segmentation,\nlocalization, image generation and domain adaptation. Commercially available\napplications are detailed, and a comprehensive discussion of the current state\nof the art and potential future directions are provided.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 20:28:16 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sogancioglu", "Ecem", ""], ["\u00c7all\u0131", "Erdi", ""], ["van Ginneken", "Bram", ""], ["van Leeuwen", "Kicky G.", ""], ["Murphy", "Keelin", ""]]}, {"id": "2103.08741", "submitter": "Lichao Mou", "authors": "Lichao Mou and Sudipan Saha and Yuansheng Hua and Francesca Bovolo and\n  Lorenzo Bruzzone and Xiao Xiang Zhu", "title": "Deep Reinforcement Learning for Band Selection in Hyperspectral Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Band selection refers to the process of choosing the most relevant bands in a\nhyperspectral image. By selecting a limited number of optimal bands, we aim at\nspeeding up model training, improving accuracy, or both. It reduces redundancy\namong spectral bands while trying to preserve the original information of the\nimage. By now many efforts have been made to develop unsupervised band\nselection approaches, of which the majority are heuristic algorithms devised by\ntrial and error. In this paper, we are interested in training an intelligent\nagent that, given a hyperspectral image, is capable of automatically learning\npolicy to select an optimal band subset without any hand-engineered reasoning.\nTo this end, we frame the problem of unsupervised band selection as a Markov\ndecision process, propose an effective method to parameterize it, and finally\nsolve the problem by deep reinforcement learning. Once the agent is trained, it\nlearns a band-selection policy that guides the agent to sequentially select\nbands by fully exploiting the hyperspectral image and previously picked bands.\nFurthermore, we propose two different reward schemes for the environment\nsimulation of deep reinforcement learning and compare them in experiments.\nThis, to the best of our knowledge, is the first study that explores a deep\nreinforcement learning model for hyperspectral image analysis, thus opening a\nnew door for future research and showcasing the great potential of deep\nreinforcement learning in remote sensing applications. Extensive experiments\nare carried out on four hyperspectral data sets, and experimental results\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 22:06:15 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Mou", "Lichao", ""], ["Saha", "Sudipan", ""], ["Hua", "Yuansheng", ""], ["Bovolo", "Francesca", ""], ["Bruzzone", "Lorenzo", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2103.08745", "submitter": "Ryan Razani", "authors": "Ran Cheng, Ryan Razani, Yuan Ren and Liu Bingbing", "title": "S3Net: 3D LiDAR Sparse Semantic Segmentation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic Segmentation is a crucial component in the perception systems of\nmany applications, such as robotics and autonomous driving that rely on\naccurate environmental perception and understanding. In literature, several\napproaches are introduced to attempt LiDAR semantic segmentation task, such as\nprojection-based (range-view or birds-eye-view), and voxel-based approaches.\nHowever, they either abandon the valuable 3D topology and geometric relations\nand suffer from information loss introduced in the projection process or are\ninefficient. Therefore, there is a need for accurate models capable of\nprocessing the 3D driving-scene point cloud in 3D space. In this paper, we\npropose S3Net, a novel convolutional neural network for LiDAR point cloud\nsemantic segmentation. It adopts an encoder-decoder backbone that consists of\nSparse Intra-channel Attention Module (SIntraAM), and Sparse Inter-channel\nAttention Module (SInterAM) to emphasize the fine details of both within each\nfeature map and among nearby feature maps. To extract the global contexts in\ndeeper layers, we introduce Sparse Residual Tower based upon sparse convolution\nthat suits varying sparsity of LiDAR point cloud. In addition, geo-aware\nanisotrophic loss is leveraged to emphasize the semantic boundaries and\npenalize the noise within each predicted regions, leading to a robust\nprediction. Our experimental results show that the proposed method leads to a\nlarge improvement (12\\%) compared to its baseline counterpart (MinkNet42\n\\cite{choy20194d}) on SemanticKITTI \\cite{DBLP:conf/iccv/BehleyGMQBSG19} test\nset and achieves state-of-the-art mIoU accuracy of semantic segmentation\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 22:15:24 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Cheng", "Ran", ""], ["Razani", "Ryan", ""], ["Ren", "Yuan", ""], ["Bingbing", "Liu", ""]]}, {"id": "2103.08756", "submitter": "Yunsheng Li", "authors": "Yunsheng Li, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Ye\n  Yu, Lu Yuan, Zicheng Liu, Mei Chen, Nuno Vasconcelos", "title": "Revisiting Dynamic Convolution via Matrix Decomposition", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research in dynamic convolution shows substantial performance boost\nfor efficient CNNs, due to the adaptive aggregation of K static convolution\nkernels. It has two limitations: (a) it increases the number of convolutional\nweights by K-times, and (b) the joint optimization of dynamic attention and\nstatic convolution kernels is challenging. In this paper, we revisit it from a\nnew perspective of matrix decomposition and reveal the key issue is that\ndynamic convolution applies dynamic attention over channel groups after\nprojecting into a higher dimensional latent space. To address this issue, we\npropose dynamic channel fusion to replace dynamic attention over channel\ngroups. Dynamic channel fusion not only enables significant dimension reduction\nof the latent space, but also mitigates the joint optimization difficulty. As a\nresult, our method is easier to train and requires significantly fewer\nparameters without sacrificing accuracy. Source code is at\nhttps://github.com/liyunsheng13/dcd.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 23:03:18 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Li", "Yunsheng", ""], ["Chen", "Yinpeng", ""], ["Dai", "Xiyang", ""], ["Liu", "Mengchen", ""], ["Chen", "Dongdong", ""], ["Yu", "Ye", ""], ["Yuan", "Lu", ""], ["Liu", "Zicheng", ""], ["Chen", "Mei", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2103.08764", "submitter": "Yu Feng", "authors": "Yu Feng, Patrick Hansen, Paul N. Whatmough, Guoyu Lu, and Yuhao Zhu", "title": "A LiDAR-Guided Framework for Video Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a general framework that simultaneously improves the\nquality and the execution speed of a range of video enhancement tasks, such as\nsuper-sampling, deblurring, and denoising. The key to our framework is a pixel\nmotion estimation algorithm that generates accurate motion from low-quality\nvideos while being computationally very lightweight. Our motion estimation\nalgorithm leverages point cloud information, which is readily available in\ntoday's autonomous devices and will only become more common in the future. We\ndemonstrate a generic framework that leverages the motion information to guide\nhigh-quality image reconstruction. Experiments show that our framework\nconsistently outperforms the state-of-the-art video enhancement algorithms\nwhile improving the execution speed by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 23:25:56 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Feng", "Yu", ""], ["Hansen", "Patrick", ""], ["Whatmough", "Paul N.", ""], ["Lu", "Guoyu", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2103.08773", "submitter": "Fevziye Irem Eyiokur", "authors": "Fevziye Irem Eyiokur, Haz{\\i}m Kemal Ekenel, Alexander Waibel", "title": "A Computer Vision System to Help Prevent the Transmission of COVID-19", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic affects every area of daily life globally. To avoid the\nspread of coronavirus and retrieve the daily normal worldwide, health\norganizations advise social distancing, wearing face mask, and avoiding\ntouching face. Based on these recommended protective measures, we developed a\ndeep learning-based computer vision system to help prevent the transmission of\nCOVID-19. Specifically, the developed system performs face mask detection,\nface-hand interaction detection, and measures social distance. For these\npurposes, we collected and annotated images that represent face mask usage and\nface-hand interaction in the real world. We introduce two different face\ndatasets, namely, Interactive Systems Labs Unconstrained Face Mask Dataset\n(ISL-UFMD) and Interactive Systems Labs Unconstrained Face Hand Interaction\nDataset (ISL-UFHD). We trained the proposed models on our own datasets and\nevaluated them on both our datasets and already existing datasets in the\nliterature without performing any adaptation on these target datasets. Besides,\nwe proposed a distance measurement module to track social distance between\npeople. Experimental results indicate that ISL-UFMD and ISL-UFHD represent the\nreal-world's diversity well. The proposed system achieved very high performance\nand generalization capacity in a real-world scenario for unseen data from\noutside the training data to detect face mask usage, face-hand interaction\ndetection, and measuring social distance. The ISL-UFMD and ISL-UFHD datasets\nwill be available at\nhttps://github.com/iremeyiokur/COVID-19-Preventions-Control-System.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 00:00:04 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 13:53:06 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Eyiokur", "Fevziye Irem", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["Waibel", "Alexander", ""]]}, {"id": "2103.08784", "submitter": "Yen-Chun Chen", "authors": "Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, Yuwei Fang,\n  Jingjing Liu", "title": "LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time\n  Image-Text Retrieval", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal pre-training has propelled great advancement in\nvision-and-language research. These large-scale pre-trained models, although\nsuccessful, fatefully suffer from slow inference speed due to enormous\ncomputation cost mainly from cross-modal attention in Transformer architecture.\nWhen applied to real-life applications, such latency and computation demand\nseverely deter the practical use of pre-trained models. In this paper, we study\nImage-text retrieval (ITR), the most mature scenario of V+L application, which\nhas been widely studied even prior to the emergence of recent pre-trained\nmodels. We propose a simple yet highly effective approach, LightningDOT that\naccelerates the inference time of ITR by thousands of times, without\nsacrificing accuracy. LightningDOT removes the time-consuming cross-modal\nattention by pre-training on three novel learning objectives, extracting\nfeature indexes offline, and employing instant dot-product matching with\nfurther re-ranking, which significantly speeds up retrieval process. In fact,\nLightningDOT achieves new state of the art across multiple ITR benchmarks such\nas Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that\nconsume 1000x magnitude of computational hours. Code and pre-training\ncheckpoints are available at https://github.com/intersun/LightningDOT.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 00:35:28 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 21:53:08 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sun", "Siqi", ""], ["Chen", "Yen-Chun", ""], ["Li", "Linjie", ""], ["Wang", "Shuohang", ""], ["Fang", "Yuwei", ""], ["Liu", "Jingjing", ""]]}, {"id": "2103.08808", "submitter": "Jialian Wu", "authors": "Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, Junsong\n  Yuan", "title": "Track to Detect and Segment: An Online Multi-Object Tracker", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most online multi-object trackers perform object detection stand-alone in a\nneural net without any input from tracking. In this paper, we present a new\nonline joint detection and tracking model, TraDeS (TRAck to DEtect and\nSegment), exploiting tracking clues to assist detection end-to-end. TraDeS\ninfers object tracking offset by a cost volume, which is used to propagate\nprevious object features for improving current object detection and\nsegmentation. Effectiveness and superiority of TraDeS are shown on 4 datasets,\nincluding MOT (2D tracking), nuScenes (3D tracking), MOTS and Youtube-VIS\n(instance segmentation tracking). Project page:\nhttps://jialianwu.com/projects/TraDeS.html.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 02:34:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Wu", "Jialian", ""], ["Cao", "Jiale", ""], ["Song", "Liangchen", ""], ["Wang", "Yu", ""], ["Yang", "Ming", ""], ["Yuan", "Junsong", ""]]}, {"id": "2103.08829", "submitter": "Ryan Mukherjee", "authors": "Ryan Mukherjee, Derek Rollend, Gordon Christie, Armin Hadzic, Sally\n  Matson, Anshu Saksena, Marisa Hughes", "title": "Towards Indirect Top-Down Road Transport Emissions Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road transportation is one of the largest sectors of greenhouse gas (GHG)\nemissions affecting climate change. Tackling climate change as a global\ncommunity will require new capabilities to measure and inventory road transport\nemissions. However, the large scale and distributed nature of vehicle emissions\nmake this sector especially challenging for existing inventory methods. In this\nwork, we develop machine learning models that use satellite imagery to perform\nindirect top-down estimation of road transport emissions. Our initial\nexperiments focus on the United States, where a bottom-up inventory was\navailable for training our models. We achieved a mean absolute error (MAE) of\n39.5 kg CO$_{2}$ of annual road transport emissions, calculated on a\npixel-by-pixel (100 m$^{2}$) basis in Sentinel-2 imagery. We also discuss key\nmodel assumptions and challenges that need to be addressed to develop models\ncapable of generalizing to global geography. We believe this work is the first\npublished approach for automated indirect top-down estimation of road transport\nsector emissions using visual imagery and represents a critical step towards\nscalable, global, near-real-time road transportation emissions inventories that\nare measured both independently and objectively.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:30:53 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Mukherjee", "Ryan", ""], ["Rollend", "Derek", ""], ["Christie", "Gordon", ""], ["Hadzic", "Armin", ""], ["Matson", "Sally", ""], ["Saksena", "Anshu", ""], ["Hughes", "Marisa", ""]]}, {"id": "2103.08833", "submitter": "Songyao Jiang", "authors": "Songyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kunpeng Li, Yun Fu", "title": "Skeleton Aware Multi-modal Sign Language Recognition", "comments": "This is a preprint version of our work SAM-SLR that ranked 1st at\n  CVPR2021 Challenge on Large Scale Signer Independent Isolated Sign Language\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sign language is commonly used by deaf or speech impaired people to\ncommunicate but requires significant effort to master. Sign Language\nRecognition (SLR) aims to bridge the gap between sign language users and others\nby recognizing signs from given videos. It is an essential yet challenging task\nsince sign language is performed with the fast and complex movement of hand\ngestures, body posture, and even facial expressions. Recently, skeleton-based\naction recognition attracts increasing attention due to the independence\nbetween the subject and background variation. However, skeleton-based SLR is\nstill under exploration due to the lack of annotations on hand keypoints. Some\nefforts have been made to use hand detectors with pose estimators to extract\nhand key points and learn to recognize sign language via Neural Networks, but\nnone of them outperforms RGB-based methods. To this end, we propose a novel\nSkeleton Aware Multi-modal SLR framework (SAM-SLR) to take advantage of\nmulti-modal information towards a higher recognition rate. Specifically, we\npropose a Sign Language Graph Convolution Network (SL-GCN) to model the\nembedded dynamics and a novel Separable Spatial-Temporal Convolution Network\n(SSTCN) to exploit skeleton features. RGB and depth modalities are also\nincorporated and assembled into our framework to provide global information\nthat is complementary to the skeleton-based methods SL-GCN and SSTCN. As a\nresult, SAM-SLR achieves the highest performance in both RGB (98.42\\%) and\nRGB-D (98.53\\%) tracks in 2021 Looking at People Large Scale Signer Independent\nIsolated SLR Challenge. Our code is available at\nhttps://github.com/jackyjsy/CVPR21Chal-SLR\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:38:17 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 04:26:11 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 15:43:22 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 19:37:31 GMT"}, {"version": "v5", "created": "Sun, 2 May 2021 20:49:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jiang", "Songyao", ""], ["Sun", "Bin", ""], ["Wang", "Lichen", ""], ["Bai", "Yue", ""], ["Li", "Kunpeng", ""], ["Fu", "Yun", ""]]}, {"id": "2103.08834", "submitter": "Shih-Po Lee", "authors": "Shih-Po Lee, Si-Cun Chen, Wen-Hsiao Peng", "title": "GSVNet: Guided Spatially-Varying Convolution for Fast Semantic\n  Segmentation on Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses fast semantic segmentation on video.Video segmentation\noften calls for real-time, or even fasterthan real-time, processing. One common\nrecipe for conserving computation arising from feature extraction is to\npropagate features of few selected keyframes. However, recent advances in fast\nimage segmentation make these solutions less attractive. To leverage fast image\nsegmentation for furthering video segmentation, we propose a simple yet\nefficient propagation framework. Specifically, we perform lightweight flow\nestimation in 1/8-downscaled image space for temporal warping in segmentation\noutpace space. Moreover, we introduce a guided spatially-varying convolution\nfor fusing segmentations derived from the previous and current frames, to\nmitigate propagation error and enable lightweight feature extraction on\nnon-keyframes. Experimental results on Cityscapes and CamVid show that our\nscheme achieves the state-of-the-art accuracy-throughput trade-off on video\nsegmentation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:38:59 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 02:02:18 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Lee", "Shih-Po", ""], ["Chen", "Si-Cun", ""], ["Peng", "Wen-Hsiao", ""]]}, {"id": "2103.08849", "submitter": "Po-Yao Huang", "authors": "Po-Yao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze\n  and Alexander Hauptmann", "title": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual\n  Transfer of Vision-Language Models", "comments": "accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies zero-shot cross-lingual transfer of vision-language\nmodels. Specifically, we focus on multilingual text-to-video search and propose\na Transformer-based model that learns contextualized multilingual multimodal\nembeddings. Under a zero-shot setting, we empirically demonstrate that\nperformance degrades significantly when we query the multilingual text-video\nmodel with non-English sentences. To address this problem, we introduce a\nmultilingual multimodal pre-training strategy, and collect a new multilingual\ninstructional video dataset (MultiHowTo100M) for pre-training. Experiments on\nVTT show that our method significantly improves video search in non-English\nlanguages without additional annotations. Furthermore, when multilingual\nannotations are available, our method outperforms recent baselines by a large\nmargin in multilingual text-to-video search on VTT and VATEX; as well as in\nmultilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is\navailable at http://github.com/berniebear/Multi-HT100M.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 04:37:40 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 17:40:09 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 02:01:38 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Huang", "Po-Yao", ""], ["Patrick", "Mandela", ""], ["Hu", "Junjie", ""], ["Neubig", "Graham", ""], ["Metze", "Florian", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2103.08852", "submitter": "Ryan Razani", "authors": "Ryan Razani, Ran Cheng, Ehsan Taghavi, and Liu Bingbing", "title": "Lite-HDSeg: LiDAR Semantic Segmentation Using Lite Harmonic Dense\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous driving vehicles and robotic systems rely on accurate perception\nof their surroundings. Scene understanding is one of the crucial components of\nperception modules. Among all available sensors, LiDARs are one of the\nessential sensing modalities of autonomous driving systems due to their active\nsensing nature with high resolution of sensor readings. Accurate and fast\nsemantic segmentation methods are needed to fully utilize LiDAR sensors for\nscene understanding. In this paper, we present Lite-HDSeg, a novel real-time\nconvolutional neural network for semantic segmentation of full $3$D LiDAR point\nclouds. Lite-HDSeg can achieve the best accuracy vs. computational complexity\ntrade-off in SemanticKitti benchmark and is designed on the basis of a new\nencoder-decoder architecture with light-weight harmonic dense convolutions as\nits core. Moreover, we introduce ICM, an improved global contextual module to\ncapture multi-scale contextual features, and MCSPN, a multi-class Spatial\nPropagation Network to further refine the semantic boundaries. Our experimental\nresults show that the proposed method outperforms state-of-the-art semantic\nsegmentation approaches which can run real-time, thus is suitable for robotic\nand autonomous driving applications.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 04:54:57 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Razani", "Ryan", ""], ["Cheng", "Ran", ""], ["Taghavi", "Ehsan", ""], ["Bingbing", "Liu", ""]]}, {"id": "2103.08860", "submitter": "Xueshuang Xiang", "authors": "Nan Ji, YanFei Feng, Haidong Xie, Xueshuang Xiang and Naijin Liu", "title": "Adversarial YOLO: Defense Human Detection Patch Attacks via Detecting\n  Adversarial Patches", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The security of object detection systems has attracted increasing attention,\nespecially when facing adversarial patch attacks. Since patch attacks change\nthe pixels in a restricted area on objects, they are easy to implement in the\nphysical world, especially for attacking human detection systems. The existing\ndefenses against patch attacks are mostly applied for image classification\nproblems and have difficulty resisting human detection attacks. Towards this\ncritical issue, we propose an efficient and effective plug-in defense component\non the YOLO detection system, which we name Ad-YOLO. The main idea is to add a\npatch class on the YOLO architecture, which has a negligible inference\nincrement. Thus, Ad-YOLO is expected to directly detect both the objects of\ninterest and adversarial patches. To the best of our knowledge, our approach is\nthe first defense strategy against human detection attacks.\n  We investigate Ad-YOLO's performance on the YOLOv2 baseline. To improve the\nability of Ad-YOLO to detect variety patches, we first use an adversarial\ntraining process to develop a patch dataset based on the Inria dataset, which\nwe name Inria-Patch. Then, we train Ad-YOLO by a combination of Pascal VOC,\nInria, and Inria-Patch datasets. With a slight drop of $0.70\\%$ mAP on VOC 2007\ntest set, Ad-YOLO achieves $80.31\\%$ AP of persons, which highly outperforms\n$33.93\\%$ AP for YOLOv2 when facing white-box patch attacks. Furthermore,\ncompared with YOLOv2, the results facing a physical-world attack are also\nincluded to demonstrate Ad-YOLO's excellent generalization ability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 05:41:39 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ji", "Nan", ""], ["Feng", "YanFei", ""], ["Xie", "Haidong", ""], ["Xiang", "Xueshuang", ""], ["Liu", "Naijin", ""]]}, {"id": "2103.08863", "submitter": "Peike Li", "authors": "Peike Li, Xin Yu, Yi Yang", "title": "Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot\n  Exemplar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conventional face super-resolution methods usually assume testing\nlow-resolution (LR) images lie in the same domain as the training ones. Due to\ndifferent lighting conditions and imaging hardware, domain gaps between\ntraining and testing images inevitably occur in many real-world scenarios.\nNeglecting those domain gaps would lead to inferior face super-resolution (FSR)\nperformance. However, how to transfer a trained FSR model to a target domain\nefficiently and effectively has not been investigated. To tackle this problem,\nwe develop a Domain-Aware Pyramid-based Face Super-Resolution network, named\nDAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces\nfrom a target domain by exploiting only a pair of high-resolution (HR) and LR\nexemplar in the target domain. To be specific, our DAP-FSR firstly employs its\nencoder to extract the multi-scale latent representations of the input LR face.\nConsidering only one target domain example is available, we propose to augment\nthe target domain data by mixing the latent representations of the target\ndomain face and source domain ones, and then feed the mixed representations to\nthe decoder of our DAP-FSR. The decoder will generate new face images\nresembling the target domain image style. The generated HR faces in turn are\nused to optimize our decoder to reduce the domain gap. By iteratively updating\nthe latent representations and our decoder, our DAP-FSR will be adapted to the\ntarget domain, thus achieving authentic and high-quality upsampled HR faces.\nExtensive experiments on three newly constructed benchmarks validate the\neffectiveness and superior performance of our DAP-FSR compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 05:47:26 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Li", "Peike", ""], ["Yu", "Xin", ""], ["Yang", "Yi", ""]]}, {"id": "2103.08877", "submitter": "Djordje Miladinovic", "authors": "{\\DJ}or{\\dj}e Miladinovi\\'c, Aleksandar Stani\\'c, Stefan Bauer,\n  J\\\"urgen Schmidhuber, Joachim M. Buhmann", "title": "Spatial Dependency Networks: Neural Layers for Improved Generative Image\n  Modeling", "comments": null, "journal-ref": "International Conference on Learning Representations (2021);", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to improve generative modeling by better exploiting spatial regularities\nand coherence in images? We introduce a novel neural network for building image\ngenerators (decoders) and apply it to variational autoencoders (VAEs). In our\nspatial dependency networks (SDNs), feature maps at each level of a deep neural\nnet are computed in a spatially coherent way, using a sequential gating-based\nmechanism that distributes contextual information across 2-D space. We show\nthat augmenting the decoder of a hierarchical VAE by spatial dependency layers\nconsiderably improves density estimation over baseline convolutional\narchitectures and the state-of-the-art among the models within the same class.\nFurthermore, we demonstrate that SDN can be applied to large images by\nsynthesizing samples of high quality and coherence. In a vanilla VAE setting,\nwe find that a powerful SDN decoder also improves learning disentangled\nrepresentations, indicating that neural architectures play an important role in\nthis task. Our results suggest favoring spatial dependency over convolutional\nlayers in various VAE settings. The accompanying source code is given at\nhttps://github.com/djordjemila/sdn.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:01:08 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Miladinovi\u0107", "\u0110or\u0111e", ""], ["Stani\u0107", "Aleksandar", ""], ["Bauer", "Stefan", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "2103.08896", "submitter": "Jungbeom Lee", "authors": "Jungbeom Lee, Eunji Kim, Sungroh Yoon", "title": "Anti-Adversarially Manipulated Attributions for Weakly and\n  Semi-Supervised Semantic Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Weakly supervised semantic segmentation produces a pixel-level localization\nfrom a classifier, but it is likely to restrict its focus to a small\ndiscriminative region of the target object. AdvCAM is an attribution map of an\nimage that is manipulated to increase the classification score. This\nmanipulation is realized in an anti-adversarial manner, which perturbs the\nimages along pixel gradients in the opposite direction from those used in an\nadversarial attack. It forces regions initially considered not to be\ndiscriminative to become involved in subsequent classifications, and produces\nattribution maps that successively identify more regions of the target object.\nIn addition, we introduce a new regularization procedure that inhibits the\nincorrect attribution of regions unrelated to the target object and limits the\nattributions of the regions that already have high scores. On PASCAL VOC 2012\ntest images, we achieve mIoUs of 68.0 and 76.9 for weakly and semi-supervised\nsemantic segmentation respectively, which represent a new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:39:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lee", "Jungbeom", ""], ["Kim", "Eunji", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2103.08907", "submitter": "Jungbeom Lee", "authors": "Jungbeom Lee, Jihun Yi, Chaehun Shin, Sungroh Yoon", "title": "BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and\n  Instance Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Weakly supervised segmentation methods using bounding box annotations focus\non obtaining a pixel-level mask from each box containing an object. Existing\nmethods typically depend on a class-agnostic mask generator, which operates on\nthe low-level information intrinsic to an image. In this work, we utilize\nhigher-level information from the behavior of a trained object detector, by\nseeking the smallest areas of the image from which the object detector produces\nalmost the same result as it does from the whole image. These areas constitute\na bounding-box attribution map (BBAM), which identifies the target object in\nits bounding box and thus serves as pseudo ground-truth for weakly supervised\nsemantic and instance segmentation. This approach significantly outperforms\nrecent comparable techniques on both the PASCAL VOC and MS COCO benchmarks in\nweakly supervised semantic and instance segmentation. In addition, we provide a\ndetailed analysis of our method, offering deeper insight into the behavior of\nthe BBAM.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 08:29:33 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lee", "Jungbeom", ""], ["Yi", "Jihun", ""], ["Shin", "Chaehun", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2103.08914", "submitter": "Qihang Yang", "authors": "Qihang Yang and Tao Chen and Jiayuan Fan and Ye Lu and Chongyan Zuo\n  and Qinghua Chi", "title": "EADNet: Efficient Asymmetric Dilated Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to real-time image semantic segmentation needs on power constrained edge\ndevices, there has been an increasing desire to design lightweight semantic\nsegmentation neural network, to simultaneously reduce computational cost and\nincrease inference speed. In this paper, we propose an efficient asymmetric\ndilated semantic segmentation network, named EADNet, which consists of multiple\ndeveloped asymmetric convolution branches with different dilation rates to\ncapture the variable shapes and scales information of an image. Specially, a\nmulti-scale multi-shape receptive field convolution (MMRFC) block with only a\nfew parameters is designed to capture such information. Experimental results on\nthe Cityscapes dataset demonstrate that our proposed EADNet achieves\nsegmentation mIoU of 67.1 with smallest number of parameters (only 0.35M) among\nmainstream lightweight semantic segmentation networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 08:46:57 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Yang", "Qihang", ""], ["Chen", "Tao", ""], ["Fan", "Jiayuan", ""], ["Lu", "Ye", ""], ["Zuo", "Chongyan", ""], ["Chi", "Qinghua", ""]]}, {"id": "2103.08922", "submitter": "Pit Schneider", "authors": "Pit Schneider", "title": "Combining Morphological and Histogram based Text Line Segmentation in\n  the OCR Context", "comments": "Journal of Data Mining and Digital Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text line segmentation is one of the pre-stages of modern optical character\nrecognition systems. The algorithmic approach proposed by this paper has been\ndesigned for this exact purpose. Its main characteristic is the combination of\ntwo different techniques, morphological image operations and horizontal\nhistogram projections. The method was developed to be applied on a historic\ndata collection that commonly features quality issues, such as degraded paper,\nblurred text, or curved text lines. For that reason, the segmenter in question\ncould be of particular interest for cultural institutions, such as libraries,\narchives, museums, ..., that want access to robust line bounding boxes for a\ngiven historic document. Because of the promising segmentation results that are\njoined by low computational cost, the algorithm was incorporated into the OCR\npipeline of the National Library of Luxembourg, in the context of the\ninitiative of reprocessing their historic newspaper collection. The general\ncontribution of this paper is to outline the approach and to evaluate the gains\nin terms of accuracy and speed, comparing it to the segmentation algorithm\nbundled with the used open source OCR software.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 09:06:25 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Schneider", "Pit", ""]]}, {"id": "2103.08945", "submitter": "Milda Pocevi\\v{c}i\\=ut\\.e", "authors": "Milda Pocevi\\v{c}i\\=ut\\.e, Gabriel Eilertsen, Claes Lundstr\\\"om", "title": "Unsupervised anomaly detection in digital pathology using GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) algorithms are optimized for the distribution\nrepresented by the training data. For outlier data, they often deliver\npredictions with equal confidence, even though these should not be trusted. In\norder to deploy ML-based digital pathology solutions in clinical practice,\neffective methods for detecting anomalous data are crucial to avoid incorrect\ndecisions in the outlier scenario. We propose a new unsupervised learning\napproach for anomaly detection in histopathology data based on generative\nadversarial networks (GANs). Compared to the existing GAN-based methods that\nhave been used in medical imaging, the proposed approach improves significantly\non performance for pathology data. Our results indicate that histopathology\nimagery is substantially more complex than the data targeted by the previous\nmethods. This complexity requires not only a more advanced GAN architecture but\nalso an appropriate anomaly metric to capture the quality of the reconstructed\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:10:12 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Pocevi\u010di\u016bt\u0117", "Milda", ""], ["Eilertsen", "Gabriel", ""], ["Lundstr\u00f6m", "Claes", ""]]}, {"id": "2103.08958", "submitter": "Qiaoyong Zhong", "authors": "Taiheng Zhang, Qiaoyong Zhong, Shiliang Pu, Di Xie", "title": "Modulating Localization and Classification for Harmonized Object\n  Detection", "comments": "Accepted by ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection involves two sub-tasks, i.e. localizing objects in an image\nand classifying them into various categories. For existing CNN-based detectors,\nwe notice the widespread divergence between localization and classification,\nwhich leads to degradation in performance. In this work, we propose a mutual\nlearning framework to modulate the two tasks. In particular, the two tasks are\nforced to learn from each other with a novel mutual labeling strategy. Besides,\nwe introduce a simple yet effective IoU rescoring scheme, which further reduces\nthe divergence. Moreover, we define a Spearman rank correlation-based metric to\nquantify the divergence, which correlates well with the detection performance.\nThe proposed approach is general-purpose and can be easily injected into\nexisting detectors such as FCOS and RetinaNet. We achieve a significant\nperformance gain over the baseline detectors on the COCO dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:36:02 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 07:53:10 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhang", "Taiheng", ""], ["Zhong", "Qiaoyong", ""], ["Pu", "Shiliang", ""], ["Xie", "Di", ""]]}, {"id": "2103.09002", "submitter": "Gabriele Lagani", "authors": "Gabriele Lagani, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato", "title": "Hebbian Semi-Supervised Learning in a Sample Efficiency Setting", "comments": "15 pages, 8 figures, 3 tables, submitted to Elsevier Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose to address the issue of sample efficiency, in Deep Convolutional\nNeural Networks (DCNN), with a semisupervised training strategy that combines\nHebbian learning with gradient descent: all internal layers (both convolutional\nand fully connected) are pre-trained using an unsupervised approach based on\nHebbian learning, and the last fully connected layer (the classification layer)\nis using Stochastic Gradient Descent (SGD). In fact, as Hebbian learning is an\nunsupervised learning method, its potential lies in the possibility of training\nthe internal layers of a DCNN without labeled examples. Only the final fully\nconnected layer has to be trained with labeled examples. We performed\nexperiments on various object recognition datasets, in different regimes of\nsample efficiency, comparing our semi-supervised (Hebbian for internal layers +\nSGD for the final fully layer) approach with end-to-end supervised\nbackpropagation training. The results show that, in regimes where the number of\navailable labeled samples is low, our semi-supervised approach outperforms full\nbackpropagation in almost all the cases.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 11:57:52 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lagani", "Gabriele", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2103.09009", "submitter": "Tianyu Luan Mr.", "authors": "Tianyu Luan, Yali Wang, Junhao Zhang, Zhe Wang, Zhipeng Zhou, Yu Qiao", "title": "PC-HMR: Pose Calibration for 3D Human Mesh Recovery from 2D\n  Images/Videos", "comments": "9 pages, 7 figures. AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The end-to-end Human Mesh Recovery (HMR) approach has been successfully used\nfor 3D body reconstruction. However, most HMR-based frameworks reconstruct\nhuman body by directly learning mesh parameters from images or videos, while\nlacking explicit guidance of 3D human pose in visual data. As a result, the\ngenerated mesh often exhibits incorrect pose for complex activities. To tackle\nthis problem, we propose to exploit 3D pose to calibrate human mesh.\nSpecifically, we develop two novel Pose Calibration frameworks, i.e., Serial\nPC-HMR and Parallel PC-HMR. By coupling advanced 3D pose estimators and HMR in\na serial or parallel manner, these two frameworks can effectively correct human\nmesh with guidance of a concise pose calibration module. Furthermore, since the\ncalibration module is designed via non-rigid pose transformation, our PC-HMR\nframeworks can flexibly tackle bone length variations to alleviate misplacement\nin the calibrated mesh. Finally, our frameworks are based on generic and\ncomplementary integration of data-driven learning and geometrical modeling. Via\nplug-and-play modules, they can be efficiently adapted for both\nimage/video-based human mesh recovery. Additionally, they have no requirement\nof extra 3D pose annotations in the testing phase, which releases inference\ndifficulties in practice. We perform extensive experiments on the popular\nbench-marks, i.e., Human3.6M, 3DPW and SURREAL, where our PC-HMR frameworks\nachieve the SOTA results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 12:12:45 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 17:13:37 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Luan", "Tianyu", ""], ["Wang", "Yali", ""], ["Zhang", "Junhao", ""], ["Wang", "Zhe", ""], ["Zhou", "Zhipeng", ""], ["Qiao", "Yu", ""]]}, {"id": "2103.09013", "submitter": "Tianyu He", "authors": "Tianyu He, Xin Jin, Xu Shen, Jianqiang Huang, Zhibo Chen, Xian-Sheng\n  Hua", "title": "Dense Interaction Learning for Video-based Person Re-identification", "comments": "Technical report, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video-based person re-identification (re-ID) aims at matching the same person\nacross video clips. Efficiently exploiting multi-scale fine-grained features\nwhile building the structural interaction among them is pivotal for its\nsuccess. In this paper, we propose a hybrid framework, Dense Interaction\nLearning (DenseIL), that takes the principal advantages of both CNN-based and\nAttention-based architectures to tackle video-based person re-ID difficulties.\nDenseIL contains a CNN encoder and a Dense Interaction (DI) decoder. The CNN\nencoder is responsible for efficiently extracting discriminative spatial\nfeatures while the DI decoder is designed to densely model spatial-temporal\ninherent interaction across frames. Different from previous works, we\nadditionally let the DI decoder densely attends to intermediate fine-grained\nCNN features and that naturally yields multi-grained spatial-temporal\nrepresentation for each video clip. Moreover, we introduce Spatio-TEmporal\nPositional Embedding (STEP-Emb) into the DI decoder to investigate the\npositional relation among the spatial-temporal inputs. Our experiments\nconsistently and significantly outperform all the state-of-the-art methods on\nmultiple standard video-based re-ID datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 12:22:08 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 07:03:30 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["He", "Tianyu", ""], ["Jin", "Xin", ""], ["Shen", "Xu", ""], ["Huang", "Jianqiang", ""], ["Chen", "Zhibo", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2103.09022", "submitter": "Jong Chul Ye", "authors": "Hyungjin Chung, Jaeyoung Huh, Geon Kim, Yong Keun Park, Jong Chul Ye", "title": "Missing Cone Artifacts Removal in ODT using Unsupervised Deep Learning\n  in Projection Domain", "comments": "This will appear in IEEE Trans. on Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Optical diffraction tomography (ODT) produces three dimensional distribution\nof refractive index (RI) by measuring scattering fields at various angles.\nAlthough the distribution of RI index is highly informative, due to the missing\ncone problem stemming from the limited-angle acquisition of holograms,\nreconstructions have very poor resolution along axial direction compared to the\nhorizontal imaging plane. To solve this issue, here we present a novel\nunsupervised deep learning framework, which learns the probability distribution\nof missing projection views through optimal transport driven cycleGAN.\nExperimental results show that missing cone artifact in ODT can be\nsignificantly resolved by the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 12:41:33 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 09:27:43 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chung", "Hyungjin", ""], ["Huh", "Jaeyoung", ""], ["Kim", "Geon", ""], ["Park", "Yong Keun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2103.09027", "submitter": "Namyeong Kwon", "authors": "Namyeong Kwon, Hwidong Na, Gabriel Huang, Simon Lacoste-Julien", "title": "Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning", "comments": "Appears in: Proceedings of the Ninth International Conference on\n  Learning Representations (ICLR 2021). 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-agnostic meta-learning (MAML) is a popular method for few-shot learning\nbut assumes that we have access to the meta-training set. In practice, training\non the meta-training set may not always be an option due to data privacy\nconcerns, intellectual property issues, or merely lack of computing resources.\nIn this paper, we consider the novel problem of repurposing pretrained MAML\ncheckpoints to solve new few-shot classification tasks. Because of the\npotential distribution mismatch, the original MAML steps may no longer be\noptimal. Therefore we propose an alternative meta-testing procedure and combine\nMAML gradient steps with adversarial training and uncertainty-based stepsize\nadaptation. Our method outperforms \"vanilla\" MAML on same-domain and\ncross-domains benchmarks using both SGD and Adam optimizers and shows improved\nrobustness to the choice of base stepsize.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 12:53:09 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Kwon", "Namyeong", ""], ["Na", "Hwidong", ""], ["Huang", "Gabriel", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "2103.09030", "submitter": "Jianbang Liu", "authors": "Jianbang Liu, Yuqi Fang, Delong Zhu, Nachuan Ma, Jin Pan, Max Q.-H.\n  Meng", "title": "A Large-Scale Dataset for Benchmarking Elevator Button Segmentation and\n  Character Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activities are hugely restricted by COVID-19, recently. Robots that can\nconduct inter-floor navigation attract much public attention, since they can\nsubstitute human workers to conduct the service work. However, current robots\neither depend on human assistance or elevator retrofitting, and fully\nautonomous inter-floor navigation is still not available. As the very first\nstep of inter-floor navigation, elevator button segmentation and recognition\nhold an important position. Therefore, we release the first large-scale\npublicly available elevator panel dataset in this work, containing 3,718 panel\nimages with 35,100 button labels, to facilitate more powerful algorithms on\nautonomous elevator operation. Together with the dataset, a number of deep\nlearning based implementations for button segmentation and recognition are also\nreleased to benchmark future methods in the community. The dataset will be\navailable at \\url{https://github.com/zhudelong/elevator_button_recognition\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 12:57:58 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 07:17:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Liu", "Jianbang", ""], ["Fang", "Yuqi", ""], ["Zhu", "Delong", ""], ["Ma", "Nachuan", ""], ["Pan", "Jin", ""], ["Meng", "Max Q. -H.", ""]]}, {"id": "2103.09042", "submitter": "Kashu Yamazaki", "authors": "Kashu Yamazaki, Vidhiwar Singh Rathour, T.Hoang Ngan Le", "title": "Invertible Residual Network with Regularization for Effective Medical\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) i.e. Residual Networks (ResNets)\nhave been used successfully for many computer vision tasks, but are difficult\nto scale to 3D volumetric medical data. Memory is increasingly often the\nbottleneck when training 3D Convolutional Neural Networks (CNNs). Recently,\ninvertible neural networks have been applied to significantly reduce activation\nmemory footprint when training neural networks with backpropagation thanks to\nthe invertible functions that allow retrieving its input from its output\nwithout storing intermediate activations in memory to perform the\nbackpropagation.\n  Among many successful network architectures, 3D Unet has been established as\na standard architecture for volumetric medical segmentation. Thus, we choose 3D\nUnet as a baseline for a non-invertible network and we then extend it with the\ninvertible residual network. In this paper, we proposed two versions of the\ninvertible Residual Network, namely Partially Invertible Residual Network\n(Partially-InvRes) and Fully Invertible Residual Network (Fully-InvRes). In\nPartially-InvRes, the invertible residual layer is defined by a technique\ncalled additive coupling whereas in Fully-InvRes, both invertible upsampling\nand downsampling operations are learned based on squeezing (known as pixel\nshuffle). Furthermore, to avoid the overfitting problem because of less\ntraining data, a variational auto-encoder (VAE) branch is added to reconstruct\nthe input volumetric data itself. Our results indicate that by using\npartially/fully invertible networks as the central workhorse in volumetric\nsegmentation, we not only reduce memory overhead but also achieve compatible\nsegmentation performance compared against the non-invertible 3D Unet. We have\ndemonstrated the proposed networks on various volumetric datasets such as iSeg\n2019 and BraTS 2020.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 13:19:59 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Yamazaki", "Kashu", ""], ["Rathour", "Vidhiwar Singh", ""], ["Le", "T. Hoang Ngan", ""]]}, {"id": "2103.09094", "submitter": "Chenxin Li", "authors": "Chenxin Li, Yunlong Zhang, Jiongcheng Li, Yue Huang, Xinghao Ding", "title": "Unsupervised Anomaly Segmentation using Image-Semantic Cycle Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of unsupervised anomaly segmentation (UAS) is to detect the\npixel-level anomalies unseen during training. It is a promising field in the\nmedical imaging community, e.g, we can use the model trained with only healthy\ndata to segment the lesions of rare diseases. Existing methods are mainly based\non Information Bottleneck, whose underlying principle is modeling the\ndistribution of normal anatomy via learning to compress and recover the healthy\ndata with a low-dimensional manifold, and then detecting lesions as the outlier\nfrom this learned distribution. However, this dimensionality reduction\ninevitably damages the localization information, which is especially essential\nfor pixel-level anomaly detection. In this paper, to alleviate this issue, we\nintroduce the semantic space of healthy anatomy in the process of modeling\nhealthy-data distribution. More precisely, we view the couple of segmentation\nand synthesis as a special Autoencoder, and propose a novel cycle translation\nframework with a journey of 'image->semantic->image'. Experimental results on\nthe BraTS and ISLES databases show that the proposed approach achieves\nsignificantly superior performance compared to several prior methods and\nsegments the anomalies more accurately.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 14:15:30 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Li", "Chenxin", ""], ["Zhang", "Yunlong", ""], ["Li", "Jiongcheng", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""]]}, {"id": "2103.09096", "submitter": "Jiaming Li", "authors": "Jiaming Li, Hongtao Xie, Jiahong Li, Zhongyuan Wang, Yongdong Zhang", "title": "Frequency-aware Discriminative Feature Learning Supervised by\n  Single-Center Loss for Face Forgery Detection", "comments": "10 pages,6 figures;cvpr accept", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face forgery detection is raising ever-increasing interest in computer vision\nsince facial manipulation technologies cause serious worries. Though recent\nworks have reached sound achievements, there are still unignorable problems: a)\nlearned features supervised by softmax loss are separable but not\ndiscriminative enough, since softmax loss does not explicitly encourage\nintra-class compactness and interclass separability; and b) fixed filter banks\nand hand-crafted features are insufficient to capture forgery patterns of\nfrequency from diverse inputs. To compensate for such limitations, a novel\nfrequency-aware discriminative feature learning framework is proposed in this\npaper. Specifically, we design a novel single-center loss (SCL) that only\ncompresses intra-class variations of natural faces while boosting inter-class\ndifferences in the embedding space. In such a case, the network can learn more\ndiscriminative features with less optimization difficulty. Besides, an adaptive\nfrequency feature generation module is developed to mine frequency clues in a\ncompletely data-driven fashion. With the above two modules, the whole framework\ncan learn more discriminative features in an end-to-end manner. Extensive\nexperiments demonstrate the effectiveness and superiority of our framework on\nthree versions of the FF++ dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 14:17:17 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Li", "Jiaming", ""], ["Xie", "Hongtao", ""], ["Li", "Jiahong", ""], ["Wang", "Zhongyuan", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2103.09097", "submitter": "Chenxin Li", "authors": "Chenxin Li, Yunlong Zhang, Zhehan Liang, Wenao Ma, Yue Huang, Xinghao\n  Ding", "title": "Consistent Posterior Distributions under Vessel-Mixing: A Regularization\n  for Cross-Domain Retinal Artery/Vein Classification", "comments": "Accepted to ICIP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retinal artery/vein (A/V) classification is a critical technique for\ndiagnosing diabetes and cardiovascular diseases. Although deep learning based\nmethods achieve impressive results in A/V classification, their performances\nusually degrade severely when being directly applied to another database, due\nto the domain shift, e.g., caused by the variations in imaging protocols. In\nthis paper, we propose a novel vessel-mixing based consistency regularization\nframework, for cross-domain learning in retinal A/V classification. Specially,\nto alleviate the severe bias to source domain, based on the label smooth prior,\nthe model is regularized to give consistent predictions for unlabeled\ntarget-domain inputs that are under perturbation. This consistency\nregularization implicitly introduces a mechanism where the model and the\nperturbation is opponent to each other, where the model is pushed to be robust\nenough to cope with the perturbation. Thus, we investigate a more difficult\nopponent to further inspire the robustness of model, in the scenario of retinal\nA/V, called vessel-mixing perturbation. Specially, it effectively disturbs the\nfundus images especially the vessel structures by mixing two images regionally.\nWe conduct extensive experiments on cross-domain A/V classification using four\npublic datasets, which are collected by diverse institutions and imaging\ndevices. The results demonstrate that our method achieves the state-of-the-art\ncross-domain performance, which is also close to the upper bound obtained by\nfully supervised learning on target domain.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 14:18:35 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:55:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Li", "Chenxin", ""], ["Zhang", "Yunlong", ""], ["Liang", "Zhehan", ""], ["Ma", "Wenao", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""]]}, {"id": "2103.09108", "submitter": "Lukas Tuggener", "authors": "Lukas Tuggener, J\\\"urgen Schmidhuber, Thilo Stadelmann", "title": "Is it Enough to Optimize CNN Architectures on ImageNet?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An implicit but pervasive hypothesis of modern computer vision research is\nthat convolutional neural network (CNN) architectures that perform better on\nImageNet will also perform better on other vision datasets. We challenge this\nhypothesis through an extensive empirical study for which we train 500 sampled\nCNN architectures on ImageNet as well as 8 other image classification datasets\nfrom a wide array of application domains. The relationship between architecture\nand performance varies wildly, depending on the datasets. For some of them, the\nperformance correlation with ImageNet is even negative. Clearly, it is not\nenough to optimize architectures solely for ImageNet when aiming for progress\nthat is relevant for all applications. Therefore, we identify two\ndataset-specific performance indicators: the cumulative width across layers as\nwell as the total depth of the network. Lastly, we show that the range of\ndataset variability covered by ImageNet can be significantly extended by adding\nImageNet subsets restricted to few classes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 14:42:01 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:23:38 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Tuggener", "Lukas", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Stadelmann", "Thilo", ""]]}, {"id": "2103.09118", "submitter": "Joseph Robinson", "authors": "Joseph P Robinson and Can Qin and Yann Henon and Samson Timoner and\n  Yun Fu", "title": "Balancing Biases and Preserving Privacy on Balanced Faces in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are demographic biases in current models used for facial recognition\n(FR). Our Balanced Faces In the Wild (BFW) dataset serves as a proxy to measure\nbias across ethnicity and gender subgroups, allowing one to characterize FR\nperformances per subgroup. We show performances are non-optimal when a single\nscore threshold is used to determine whether sample pairs are genuine or\nimposter. Across subgroups, performance ratings vary from the reported across\nthe entire dataset. Thus, claims of specific error rates only hold true for\npopulations matching that of the validation data. We mitigate the imbalanced\nperformances using a novel domain adaptation learning scheme on the facial\nfeatures extracted using state-of-the-art. Not only does this technique balance\nperformance, but it also boosts the overall performance. A benefit of the\nproposed is to preserve identity information in facial features while removing\ndemographic knowledge in the lower dimensional features. The removal of\ndemographic knowledge prevents future potential biases from being injected into\ndecision-making. This removal satisfies privacy concerns. We explore why this\nworks qualitatively; we also show quantitatively that subgroup classifiers can\nno longer learn from the features mapped by the proposed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:05:49 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 12:21:18 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Robinson", "Joseph P", ""], ["Qin", "Can", ""], ["Henon", "Yann", ""], ["Timoner", "Samson", ""], ["Fu", "Yun", ""]]}, {"id": "2103.09136", "submitter": "Chenhongyi Yang", "authors": "Chenhongyi Yang, Zehao Huang and Naiyan Wang", "title": "QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While general object detection with deep learning has achieved great success\nin the past few years, the performance and efficiency of detecting small\nobjects are far from satisfactory. The most common and effective way to promote\nsmall object detection is to use high-resolution images or feature maps.\nHowever, both approaches induce costly computation since the computational cost\ngrows squarely as the size of images and features increases. To get the best of\ntwo worlds, we propose QueryDet that uses a novel query mechanism to accelerate\nthe inference speed of feature-pyramid based object detectors. The pipeline\ncomposes two steps: it first predicts the coarse locations of small objects on\nlow-resolution features and then computes the accurate detection results using\nhigh-resolution features sparsely guided by those coarse positions. In this\nway, we can not only harvest the benefit of high-resolution feature maps but\nalso avoid useless computation for the background area. On the popular COCO\ndataset, the proposed method improves the detection mAP by 1.0 and mAP-small by\n2.0, and the high-resolution inference speed is improved to 3.0x on average. On\nVisDrone dataset, which contains more small objects, we create a new\nstate-of-the-art while gaining a 2.3x high-resolution acceleration on average.\nCode is available at: https://github.com/ChenhongyiYang/QueryDet-PyTorch\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:30:20 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Yang", "Chenhongyi", ""], ["Huang", "Zehao", ""], ["Wang", "Naiyan", ""]]}, {"id": "2103.09141", "submitter": "Hamid Sarmadi", "authors": "Hamid Sarmadi, Rafael Mu\\~noz-Salinas, M.A. Berb\\'is, R.\n  Medina-Carnicer", "title": "Simultaneous Multi-View Camera Pose Estimation and Object Tracking with\n  Square Planar Markers", "comments": "Some errors in the IEEE Access version (regarding object's rotational\n  accuracy, and the definition of Equation 14) have been corrected in this\n  version. IEEE Access paper: https://doi.org/10.1109/ACCESS.2019.2896648", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2896648", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is a key aspect in many applications such as augmented\nreality in medicine (e.g. tracking a surgical instrument) or robotics. Squared\nplanar markers have become popular tools for tracking since their pose can be\nestimated from their four corners. While using a single marker and a single\ncamera limits the working area considerably, using multiple markers attached to\nan object requires estimating their relative position, which is not trivial,\nfor high accuracy tracking. Likewise, using multiple cameras requires\nestimating their extrinsic parameters, also a tedious process that must be\nrepeated whenever a camera is moved.\n  This work proposes a novel method to simultaneously solve the above-mentioned\nproblems. From a video sequence showing a rigid set of planar markers recorded\nfrom multiple cameras, the proposed method is able to automatically obtain the\nthree-dimensional configuration of the markers, the extrinsic parameters of the\ncameras, and the relative pose between the markers and the cameras at each\nframe. Our experiments show that our approach can obtain highly accurate\nresults for estimating these parameters using low resolution cameras.\n  Once the parameters are obtained, tracking of the object can be done in real\ntime with a low computational cost. The proposed method is a step forward in\nthe development of cost-effective solutions for object tracking.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:33:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sarmadi", "Hamid", ""], ["Mu\u00f1oz-Salinas", "Rafael", ""], ["Berb\u00eds", "M. A.", ""], ["Medina-Carnicer", "R.", ""]]}, {"id": "2103.09151", "submitter": "Han Wu", "authors": "Han Wu, Wenjie Ruan", "title": "Adversarial Driving: Attacking End-to-End Autonomous Driving Systems", "comments": "3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As the research in deep neural networks advances, deep convolutional networks\nbecome feasible for automated driving tasks. There is an emerging trend of\nemploying end-to-end models in the automation of driving tasks. However,\nprevious research unveils that deep neural networks are vulnerable to\nadversarial attacks in classification tasks. While for regression tasks such as\nautonomous driving, the effect of these attacks remains rarely explored. In\nthis research, we devise two white-box targeted attacks against end-to-end\nautonomous driving systems. The driving model takes an image as input and\noutputs the steering angle. Our attacks can manipulate the behaviour of the\nautonomous driving system only by perturbing the input image. Both attacks can\nbe initiated in real-time on CPUs without employing GPUs. This demo aims to\nraise concerns over applications of end-to-end models in safety-critical\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:47:34 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 14:04:36 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wu", "Han", ""], ["Ruan", "Wenjie", ""]]}, {"id": "2103.09154", "submitter": "Liam Schoneveld", "authors": "Liam Schoneveld and Alice Othmani and Hazem Abdelkawy", "title": "Leveraging Recent Advances in Deep Learning for Audio-Visual Emotion\n  Recognition", "comments": "8 pages, 3 figures, Pattern Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2021.03.007", "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotional expressions are the behaviors that communicate our emotional state\nor attitude to others. They are expressed through verbal and non-verbal\ncommunication. Complex human behavior can be understood by studying physical\nfeatures from multiple modalities; mainly facial, vocal and physical gestures.\nRecently, spontaneous multi-modal emotion recognition has been extensively\nstudied for human behavior analysis. In this paper, we propose a new deep\nlearning-based approach for audio-visual emotion recognition. Our approach\nleverages recent advances in deep learning like knowledge distillation and\nhigh-performing deep architectures. The deep feature representations of the\naudio and visual modalities are fused based on a model-level fusion strategy. A\nrecurrent neural network is then used to capture the temporal dynamics. Our\nproposed approach substantially outperforms state-of-the-art approaches in\npredicting valence on the RECOLA dataset. Moreover, our proposed visual facial\nexpression feature extraction network outperforms state-of-the-art results on\nthe AffectNet and Google Facial Expression Comparison datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:49:15 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Schoneveld", "Liam", ""], ["Othmani", "Alice", ""], ["Abdelkawy", "Hazem", ""]]}, {"id": "2103.09160", "submitter": "Jingdao Chen", "authors": "Jingdao Chen, Zsolt Kira, and Yong K. Cho", "title": "LRGNet: Learnable Region Growing for Class-Agnostic Point Cloud\n  Segmentation", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters 2021", "doi": "10.1109/LRA.2021.3062607", "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D point cloud segmentation is an important function that helps robots\nunderstand the layout of their surrounding environment and perform tasks such\nas grasping objects, avoiding obstacles, and finding landmarks. Current\nsegmentation methods are mostly class-specific, many of which are tuned to work\nwith specific object categories and may not be generalizable to different types\nof scenes. This research proposes a learnable region growing method for\nclass-agnostic point cloud segmentation, specifically for the task of instance\nlabel prediction. The proposed method is able to segment any class of objects\nusing a single deep neural network without any assumptions about their shapes\nand sizes. The deep neural network is trained to predict how to add or remove\npoints from a point cloud region to morph it into incrementally more complete\nregions of an object instance. Segmentation results on the S3DIS and ScanNet\ndatasets show that the proposed method outperforms competing methods by 1%-9%\non 6 different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:58:01 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chen", "Jingdao", ""], ["Kira", "Zsolt", ""], ["Cho", "Yong K.", ""]]}, {"id": "2103.09174", "submitter": "Meher Shashwat Nigam", "authors": "Meher Shashwat Nigam, Avinash Prabhu, Anurag Sahu, Puru Gupta, Tanvi\n  Karandikar, N. Sai Shankar, Ravi Kiran Sarvadevabhatla, K. Madhava Krishna", "title": "RackLay: Multi-Layer Layout Estimation for Warehouse Racks", "comments": "Visit our project repository at\n  https://github.com/Avinash2468/RackLay", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a monocular colour image of a warehouse rack, we aim to predict the\nbird's-eye view layout for each shelf in the rack, which we term as multi-layer\nlayout prediction. To this end, we present RackLay, a deep neural network for\nreal-time shelf layout estimation from a single image. Unlike previous layout\nestimation methods, which provide a single layout for the dominant ground plane\nalone, RackLay estimates the top-view and front-view layout for each shelf in\nthe considered rack populated with objects. RackLay's architecture and its\nvariants are versatile and estimate accurate layouts for diverse scenes\ncharacterized by varying number of visible shelves in an image, large range in\nshelf occupancy factor and varied background clutter. Given the extreme paucity\nof datasets in this space and the difficulty involved in acquiring real data\nfrom warehouses, we additionally release a flexible synthetic dataset\ngeneration pipeline WareSynth which allows users to control the generation\nprocess and tailor the dataset according to contingent application. The\nablations across architectural variants and comparison with strong prior\nbaselines vindicate the efficacy of RackLay as an apt architecture for the\nnovel problem of multi-layered layout estimation. We also show that fusing the\ntop-view and front-view enables 3D reasoning applications such as metric free\nspace estimation for the considered rack.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:22:31 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 10:58:36 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Nigam", "Meher Shashwat", ""], ["Prabhu", "Avinash", ""], ["Sahu", "Anurag", ""], ["Gupta", "Puru", ""], ["Karandikar", "Tanvi", ""], ["Shankar", "N. Sai", ""], ["Sarvadevabhatla", "Ravi Kiran", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2103.09179", "submitter": "Chenwei Cui", "authors": "Chenwei Cui, Liangfu Lu, Zhiyuan Tan, Amir Hussain", "title": "Conceptual Text Region Network: Cognition-Inspired Accurate Scene Text\n  Detection", "comments": "Preprint submitted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation-based methods are widely used for scene text detection due to\ntheir superiority in describing arbitrary-shaped text instances. However, two\nmajor problems still exist: 1) current label generation techniques are mostly\nempirical and lack theoretical support, discouraging elaborate label design; 2)\nas a result, most methods rely heavily on text kernel segmentation which is\nunstable and requires deliberate tuning. To address these challenges, we\npropose a human cognition-inspired framework, termed, Conceptual Text Region\nNetwork (CTRNet). The framework utilizes Conceptual Text Regions (CTRs), which\nis a class of cognition-based tools inheriting good mathematical properties,\nallowing for sophisticated label design. Another component of CTRNet is an\ninference pipeline that, with the help of CTRs, completely omits the need for\ntext kernel segmentation. Compared with previous segmentation-based methods,\nour approach is not only more interpretable but also more accurate.\nExperimental results show that CTRNet achieves state-of-the-art performance on\nbenchmark CTW1500, Total-Text, MSRA-TD500, and ICDAR 2015 datasets, yielding\nperformance gains of up to 2.0%. Notably, to the best of our knowledge, CTRNet\nis among the first detection models to achieve F-measures higher than 85.0% on\nall four of the benchmarks, with remarkable consistency and stability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:28:33 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Cui", "Chenwei", ""], ["Lu", "Liangfu", ""], ["Tan", "Zhiyuan", ""], ["Hussain", "Amir", ""]]}, {"id": "2103.09189", "submitter": "Pranav Agarwal", "authors": "Pranav Agarwal, Pierre de Beaucorps and Raoul de Charette", "title": "Sparse Curriculum Reinforcement Learning for End-to-End Driving", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement Learning for end-to-end driving is limited by the need of\ncomplex reward engineering. Sparse rewards can circumvent this challenge but\nsuffers from long training time and leads to sub-optimal policy. In this work,\nwe explore driving using only goal conditioned sparse rewards and propose a\ncurriculum learning approach for end to end driving using only navigation view\nmaps that benefit from small virtual-to-real domain gap. To address the\ncomplexity of multiple driving policies, we learn concurrent individual\npolicies which are selected at inference by a navigation system. We demonstrate\nthe ability of our proposal to generalize on unseen road layout, and to drive\nlonger than in the training.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:39:09 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Agarwal", "Pranav", ""], ["de Beaucorps", "Pierre", ""], ["de Charette", "Raoul", ""]]}, {"id": "2103.09213", "submitter": "Paul-Edouard Sarlin", "authors": "Paul-Edouard Sarlin, Ajaykumar Unagar, M{\\aa}ns Larsson, Hugo Germain,\n  Carl Toft, Viktor Larsson, Marc Pollefeys, Vincent Lepetit, Lars\n  Hammarstrand, Fredrik Kahl, Torsten Sattler", "title": "Back to the Feature: Learning Robust Camera Localization from Pixels to\n  Pose", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera pose estimation in known scenes is a 3D geometry task recently tackled\nby multiple learning algorithms. Many regress precise geometric quantities,\nlike poses or 3D points, from an input image. This either fails to generalize\nto new viewpoints or ties the model parameters to a specific scene. In this\npaper, we go Back to the Feature: we argue that deep networks should focus on\nlearning robust and invariant visual features, while the geometric estimation\nshould be left to principled algorithms. We introduce PixLoc, a scene-agnostic\nneural network that estimates an accurate 6-DoF pose from an image and a 3D\nmodel. Our approach is based on the direct alignment of multiscale deep\nfeatures, casting camera localization as metric learning. PixLoc learns strong\ndata priors by end-to-end training from pixels to pose and exhibits exceptional\ngeneralization to new scenes by separating model parameters and scene geometry.\nThe system can localize in large environments given coarse pose priors but also\nimprove the accuracy of sparse feature matching by jointly refining keypoints\nand poses with little overhead. The code will be publicly available at\nhttps://github.com/cvg/pixloc.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 17:40:12 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 21:48:06 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Sarlin", "Paul-Edouard", ""], ["Unagar", "Ajaykumar", ""], ["Larsson", "M\u00e5ns", ""], ["Germain", "Hugo", ""], ["Toft", "Carl", ""], ["Larsson", "Viktor", ""], ["Pollefeys", "Marc", ""], ["Lepetit", "Vincent", ""], ["Hammarstrand", "Lars", ""], ["Kahl", "Fredrik", ""], ["Sattler", "Torsten", ""]]}, {"id": "2103.09229", "submitter": "Aniket Gujarathi", "authors": "Aniket Gujarathi, Akshay Kulkarni, Unmesh Patil, Yogesh Phalak,\n  Rajeshree Deotalu, Aman Jain, Navid Panchi, Ashwin Dhabale, Shital Chiddarwar", "title": "Design and Development of Autonomous Delivery Robot", "comments": "56 pages, Bachelor Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of autonomous robotics is growing at a rapid rate. The trend to use\nincreasingly more sensors in vehicles is driven both by legislation and\nconsumer demands for higher safety and reliable service. Nowadays, robots are\nfound everywhere, ranging from homes, hospitals to industries, and military\noperations. Autonomous robots are developed to be robust enough to work beside\nhumans and to carry out jobs efficiently. Humans have a natural sense of\nunderstanding of the physical forces acting around them like gravity, sense of\nmotion, etc. which are not taught explicitly but are developed naturally.\nHowever, this is not the case with robots. To make the robot fully autonomous\nand competent to work with humans, the robot must be able to perceive the\nsituation and devise a plan for smooth operation, considering all the\nadversities that may occur while carrying out the tasks. In this thesis, we\npresent an autonomous mobile robot platform that delivers the package within\nthe VNIT campus without any human intercommunication. From an initial\nuser-supplied geographic target location, the system plans an optimized path\nand autonomously navigates through it. The entire pipeline of an autonomous\nrobot working in outdoor environments is explained in detail in this thesis.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 17:57:44 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Gujarathi", "Aniket", ""], ["Kulkarni", "Akshay", ""], ["Patil", "Unmesh", ""], ["Phalak", "Yogesh", ""], ["Deotalu", "Rajeshree", ""], ["Jain", "Aman", ""], ["Panchi", "Navid", ""], ["Dhabale", "Ashwin", ""], ["Chiddarwar", "Shital", ""]]}, {"id": "2103.09233", "submitter": "Nikhil Churamani", "authors": "Ozgur Kara, Nikhil Churamani and Hatice Gunes", "title": "Towards Fair Affective Robotics: Continual Learning for Mitigating Bias\n  in Facial Expression and Action Unit Recognition", "comments": "Accepted at the Workshop on Lifelong Learning and Personalization in\n  Long-Term Human-Robot Interaction (LEAP-HRI) at the 16th ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI), 2021. arXiv admin\n  note: substantial text overlap with arXiv:2103.08637", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As affective robots become integral in human life, these agents must be able\nto fairly evaluate human affective expressions without discriminating against\nspecific demographic groups. Identifying bias in Machine Learning (ML) systems\nas a critical problem, different approaches have been proposed to mitigate such\nbiases in the models both at data and algorithmic levels. In this work, we\npropose Continual Learning (CL) as an effective strategy to enhance fairness in\nFacial Expression Recognition (FER) systems, guarding against biases arising\nfrom imbalances in data distributions. We compare different state-of-the-art\nbias mitigation approaches with CL-based strategies for fairness on expression\nrecognition and Action Unit (AU) detection tasks using popular benchmarks for\neach; RAF-DB and BP4D. Our experiments show that CL-based methods, on average,\noutperform popular bias mitigation techniques, strengthening the need for\nfurther investigation into CL for the development of fairer FER algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 18:36:14 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Kara", "Ozgur", ""], ["Churamani", "Nikhil", ""], ["Gunes", "Hatice", ""]]}, {"id": "2103.09265", "submitter": "Harshitha Machiraju", "authors": "Harshitha Machiraju, Oh-Hyeon Choung, Pascal Frossard, Michael. H\n  Herzog", "title": "Bio-inspired Robustness: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have revolutionized computer\nvision and are often advocated as good models of the human visual system.\nHowever, there are currently many shortcomings of DCNNs, which preclude them as\na model of human vision. For example, in the case of adversarial attacks, where\nadding small amounts of noise to an image, including an object, can lead to\nstrong misclassification of that object. But for humans, the noise is often\ninvisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot\nbe taken as serious models of human vision. Many studies have tried to add\nfeatures of the human visual system to DCNNs to make them robust against\nadversarial attacks. However, it is not fully clear whether human vision\ninspired components increase robustness because performance evaluations of\nthese novel components in DCNNs are often inconclusive. We propose a set of\ncriteria for proper evaluation and analyze different models according to these\ncriteria. We finally sketch future efforts to make DCCNs one step closer to the\nmodel of human vision.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 18:20:29 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Machiraju", "Harshitha", ""], ["Choung", "Oh-Hyeon", ""], ["Frossard", "Pascal", ""], ["Herzog", "Michael. H", ""]]}, {"id": "2103.09276", "submitter": "Megha Kalia", "authors": "Megha Kalia, Tajwar Abrar Aleef, Nassir Navab, and Septimiu E.\n  Salcudean", "title": "Co-Generation and Segmentation for Generalized Surgical Instrument\n  Segmentation on Unlabelled Data", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Surgical instrument segmentation for robot-assisted surgery is needed for\naccurate instrument tracking and augmented reality overlays. Therefore, the\ntopic has been the subject of a number of recent papers in the CAI community.\nDeep learning-based methods have shown state-of-the-art performance for\nsurgical instrument segmentation, but their results depend on labelled data.\nHowever, labelled surgical data is of limited availability and is a bottleneck\nin surgical translation of these methods. In this paper, we demonstrate the\nlimited generalizability of these methods on different datasets, including\nhuman robot-assisted surgeries. We then propose a novel joint generation and\nsegmentation strategy to learn a segmentation model with better generalization\ncapability to domains that have no labelled data. The method leverages the\navailability of labelled data in a different domain. The generator does the\ndomain translation from the labelled domain to the unlabelled domain and\nsimultaneously, the segmentation model learns using the generated data while\nregularizing the generative model. We compared our method with state-of-the-art\nmethods and showed its generalizability on publicly available datasets and on\nour own recorded video frames from robot-assisted prostatectomies. Our method\nshows consistently high mean Dice scores on both labelled and unlabelled\ndomains when data is available only for one of the domains.\n  *M. Kalia and T. Aleef contributed equally to the manuscript\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 18:41:18 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Kalia", "Megha", ""], ["Aleef", "Tajwar Abrar", ""], ["Navab", "Nassir", ""], ["Salcudean", "Septimiu E.", ""]]}, {"id": "2103.09289", "submitter": "Nisarg Shah", "authors": "Nisarg A. Shah, Divij Gupta, Romil Lodaya, Ujjwal Baid, and Sanjay\n  Talbar", "title": "Colorectal Cancer Segmentation using Atrous Convolution and Residual\n  Enhanced UNet", "comments": "5th IAPR International Conference on Computer Vision and Image\n  Processing, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer is a leading cause of death worldwide. However, early\ndiagnosis dramatically increases the chances of survival, for which it is\ncrucial to identify the tumor in the body. Since its imaging uses\nhigh-resolution techniques, annotating the tumor is time-consuming and requires\nparticular expertise. Lately, methods built upon Convolutional Neural\nNetworks(CNNs) have proven to be at par, if not better in many biomedical\nsegmentation tasks. For the task at hand, we propose another CNN-based\napproach, which uses atrous convolutions and residual connections besides the\nconventional filters. The training and inference were made using an efficient\npatch-based approach, which significantly reduced unnecessary computations. The\nproposed AtResUNet was trained on the DigestPath 2019 Challenge dataset for\ncolorectal cancer segmentation with results having a Dice Coefficient of 0.748.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 19:20:20 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Shah", "Nisarg A.", ""], ["Gupta", "Divij", ""], ["Lodaya", "Romil", ""], ["Baid", "Ujjwal", ""], ["Talbar", "Sanjay", ""]]}, {"id": "2103.09300", "submitter": "Hongjie He", "authors": "Hongjie He, Ke Yang, Yuwei Cai, Zijian Jiang, Qiutong Yu, Kun Zhao,\n  Junbo Wang, Sarah Narges Fatholahi, Yan Liu, Hasti Andon Petrosians, Bingxu\n  Hu, Liyuan Qing, Zhehan Zhang, Hongzhang Xu, Siyu Li, Linlin Xu, Jonathan Li", "title": "A comparative study of deep learning methods for building footprints\n  detection using high spatial resolution aerial images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Building footprints data is of importance in several urban applications and\nnatural disaster management. In contrast to traditional surveying and mapping,\nusing high spatial resolution aerial images, deep learning-based building\nfootprints extraction methods can extract building footprints accurately and\nefficiently. With rapidly development of deep learning methods, it is hard for\nnovice to harness the powerful tools in building footprints extraction. The\npaper aims at providing the whole process of building footprints extraction\nfrom high spatial resolution images using deep learning-based methods. In\naddition, we also compare the commonly used methods, including Fully\nConvolutional Networks (FCN)-8s, U-Net and DeepLabv3+. At the end of the work,\nwe change the data size used in models training to explore the influence of\ndata size to the performance of the algorithms. The experiments show that, in\ndifferent data size, DeepLabv3+ is the best algorithm among them with the\nhighest accuracy and moderate efficiency; FCN-8s has the worst accuracy and\nhighest efficiency; U-Net shows the moderate accuracy and lowest efficiency. In\naddition, with more training data, algorithms converged faster with higher\naccuracy in extraction results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 20:03:50 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["He", "Hongjie", ""], ["Yang", "Ke", ""], ["Cai", "Yuwei", ""], ["Jiang", "Zijian", ""], ["Yu", "Qiutong", ""], ["Zhao", "Kun", ""], ["Wang", "Junbo", ""], ["Fatholahi", "Sarah Narges", ""], ["Liu", "Yan", ""], ["Petrosians", "Hasti Andon", ""], ["Hu", "Bingxu", ""], ["Qing", "Liyuan", ""], ["Zhang", "Zhehan", ""], ["Xu", "Hongzhang", ""], ["Li", "Siyu", ""], ["Xu", "Linlin", ""], ["Li", "Jonathan", ""]]}, {"id": "2103.09354", "submitter": "Denis Dimitrov", "authors": "Mark Potanin, Denis Dimitrov, Alex Shonenkov, Vladimir Bataev, Denis\n  Karachev and Maxim Novopoltsev", "title": "Digital Peter: Dataset, Competition and Handwriting Recognition Methods", "comments": "17 pages, 7 figures, submitted to ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new dataset of Peter the Great's manuscripts and\ndescribes a segmentation procedure that converts initial images of documents\ninto the lines. The new dataset may be useful for researchers to train\nhandwriting text recognition models as a benchmark for comparing different\nmodels. It consists of 9 694 images and text files corresponding to lines in\nhistorical documents. The open machine learning competition Digital Peter was\nheld based on the considered dataset. The baseline solution for this\ncompetition as well as more advanced methods on handwritten text recognition\nare described in the article. Full dataset and all code are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 22:37:22 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Potanin", "Mark", ""], ["Dimitrov", "Denis", ""], ["Shonenkov", "Alex", ""], ["Bataev", "Vladimir", ""], ["Karachev", "Denis", ""], ["Novopoltsev", "Maxim", ""]]}, {"id": "2103.09369", "submitter": "Aayush Chaudhary", "authors": "Aayush K. Chaudhary, Prashnna K. Gyawali, Linwei Wang, Jeff B. Pelz", "title": "Semi-Supervised Learning for Eye Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in appearance-based models have shown improved eye tracking\nperformance in difficult scenarios like occlusion due to eyelashes, eyelids or\ncamera placement, and environmental reflections on the cornea and glasses. The\nkey reason for the improvement is the accurate and robust identification of eye\nparts (pupil, iris, and sclera regions). The improved accuracy often comes at\nthe cost of labeling an enormous dataset, which is complex and time-consuming.\nThis work presents two semi-supervised learning frameworks to identify\neye-parts by taking advantage of unlabeled images where labeled datasets are\nscarce. With these frameworks, leveraging the domain-specific augmentation and\nnovel spatially varying transformations for image segmentation, we show\nimproved performance on various test cases. For instance, for a model trained\non just 48 labeled images, these frameworks achieved an improvement of 0.38%\nand 0.65% in segmentation performance over the baseline model, which is trained\nonly with the labeled dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 00:05:19 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Chaudhary", "Aayush K.", ""], ["Gyawali", "Prashnna K.", ""], ["Wang", "Linwei", ""], ["Pelz", "Jeff B.", ""]]}, {"id": "2103.09377", "submitter": "James Diffenderfer", "authors": "James Diffenderfer, Bhavya Kailkhura", "title": "Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural\n  Networks by Pruning A Randomly Weighted Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Frankle & Carbin (2019) demonstrated that randomly-initialized\ndense networks contain subnetworks that once found can be trained to reach test\naccuracy comparable to the trained dense network. However, finding these high\nperforming trainable subnetworks is expensive, requiring iterative process of\ntraining and pruning weights. In this paper, we propose (and prove) a stronger\nMulti-Prize Lottery Ticket Hypothesis:\n  A sufficiently over-parameterized neural network with random weights contains\nseveral subnetworks (winning tickets) that (a) have comparable accuracy to a\ndense target network with learned weights (prize 1), (b) do not require any\nfurther training to achieve prize 1 (prize 2), and (c) is robust to extreme\nforms of quantization (i.e., binary weights and/or activation) (prize 3).\n  This provides a new paradigm for learning compact yet highly accurate binary\nneural networks simply by pruning and quantizing randomly weighted full\nprecision neural networks. We also propose an algorithm for finding multi-prize\ntickets (MPTs) and test it by performing a series of experiments on CIFAR-10\nand ImageNet datasets. Empirical results indicate that as models grow deeper\nand wider, multi-prize tickets start to reach similar (and sometimes even\nhigher) test accuracy compared to their significantly larger and full-precision\ncounterparts that have been weight-trained. Without ever updating the weight\nvalues, our MPTs-1/32 not only set new binary weight network state-of-the-art\n(SOTA) Top-1 accuracy -- 94.8% on CIFAR-10 and 74.03% on ImageNet -- but also\noutperform their full-precision counterparts by 1.78% and 0.76%, respectively.\nFurther, our MPT-1/1 achieves SOTA Top-1 accuracy (91.9%) for binary neural\nnetworks on CIFAR-10. Code and pre-trained models are available at:\nhttps://github.com/chrundle/biprop.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 00:31:24 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Diffenderfer", "James", ""], ["Kailkhura", "Bhavya", ""]]}, {"id": "2103.09382", "submitter": "Chuang Niu", "authors": "Chuang Niu and Ge Wang", "title": "SPICE: Semantic Pseudo-labeling for Image Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents SPICE, a Semantic Pseudo-labeling framework for Image\nClustEring. Instead of using indirect loss functions required by the recently\nproposed methods, SPICE generates pseudo-labels via self-learning and directly\nuses the pseudo-label-based classification loss to train a deep clustering\nnetwork. The basic idea of SPICE is to synergize the discrepancy among semantic\nclusters, the similarity among instance samples, and the semantic consistency\nof local samples in an embedding space to optimize the clustering network in a\nsemantically-driven paradigm. Specifically, a semantic-similarity-based\npseudo-labeling algorithm is first proposed to train a clustering network\nthrough unsupervised representation learning. Given the initial clustering\nresults, a local semantic consistency principle is used to select a set of\nreliably labeled samples, and a semi-pseudo-labeling algorithm is adapted for\nperformance boosting. Extensive experiments demonstrate that SPICE clearly\noutperforms the state-of-the-art methods on six common benchmark datasets\nincluding STL10, Cifar10, Cifar100-20, ImageNet-10, ImageNet-Dog, and\nTiny-ImageNet. On average, our SPICE method improves the current best results\nby about 10% in terms of adjusted rand index, normalized mutual information,\nand clustering accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 00:52:27 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Niu", "Chuang", ""], ["Wang", "Ge", ""]]}, {"id": "2103.09384", "submitter": "Aditya Challa Dr", "authors": "Aditya Challa, Sravan Danda, B.S.Daya Sagar and Laurent Najman", "title": "Triplet-Watershed for Hyperspectral Image Classification", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hyperspectral images (HSI) consist of rich spatial and spectral information,\nwhich can potentially be used for several applications. However, noise, band\ncorrelations and high dimensionality restrict the applicability of such data.\nThis is recently addressed using creative deep learning network architectures\nsuch as ResNet, SSRN, and A2S2K. However, the last layer, i.e the\nclassification layer, remains unchanged and is taken to be the softmax\nclassifier. In this article, we propose to use a watershed classifier.\nWatershed classifier extends the watershed operator from Mathematical\nMorphology for classification. In its vanilla form, the watershed classifier\ndoes not have any trainable parameters. In this article, we propose a novel\napproach to train deep learning networks to obtain representations suitable for\nthe watershed classifier. The watershed classifier exploits the connectivity\npatterns, a characteristic of HSI datasets, for better inference. We show that\nexploiting such characteristics allows the Triplet-Watershed to achieve\nstate-of-art results in supervised and semi-supervised contexts. These results\nare validated on Indianpines (IP), University of Pavia (UP), Kennedy Space\nCenter (KSC) and University of Houston (UH) datasets, relying on simple convnet\narchitecture using a quarter of parameters compared to previous\nstate-of-the-art networks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 01:06:49 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 06:02:17 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Challa", "Aditya", ""], ["Danda", "Sravan", ""], ["Sagar", "B. S. Daya", ""], ["Najman", "Laurent", ""]]}, {"id": "2103.09396", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Pros and Cons of GAN Evaluation Measures: New Developments", "comments": "NA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 01:48:34 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 07:51:09 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2103.09404", "submitter": "Kartikeya Bhardwaj", "authors": "Kartikeya Bhardwaj, Milos Milosavljevic, Alex Chalfin, Naveen Suda,\n  Liam O'Neil, Dibakar Gope, Lingchuan Meng, Ramon Matas, Danny Loh", "title": "Collapsible Linear Blocks for Super-Efficient Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of smart devices that support 4K and 8K resolution, Single\nImage Super Resolution (SISR) has become an important computer vision problem.\nHowever, most super resolution deep networks are computationally very\nexpensive. In this paper, we propose SESR, a new class of Super-Efficient Super\nResolution networks that significantly improve image quality and reduce\ncomputational complexity. Detailed experiments across six benchmark datasets\ndemonstrate that SESR achieves similar or better image quality than\nstate-of-the-art models while requiring 2x to 330x fewer Multiply-Accumulate\n(MAC) operations. As a result, SESR can be used on constrained hardware to\nperform x2 (1080p to 4K) and x4 SISR (1080p to 8K). Towards this, we simulate\nhardware performance numbers for a commercial mobile Neural Processing Unit\n(NPU) for 1080p to 4K (x2) and 1080p to 8K (x4) SISR. Our results highlight the\nchallenges faced by super resolution on AI accelerators and demonstrate that\nSESR is significantly faster than existing models. Overall, SESR establishes a\nnew Pareto frontier on the quality (PSNR)-computation relationship for the\nsuper resolution task.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 02:16:31 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bhardwaj", "Kartikeya", ""], ["Milosavljevic", "Milos", ""], ["Chalfin", "Alex", ""], ["Suda", "Naveen", ""], ["O'Neil", "Liam", ""], ["Gope", "Dibakar", ""], ["Meng", "Lingchuan", ""], ["Matas", "Ramon", ""], ["Loh", "Danny", ""]]}, {"id": "2103.09408", "submitter": "Saeed Khaki", "authors": "Saeed Khaki, Nima Safaei, Hieu Pham and Lizhi Wang", "title": "WheatNet: A Lightweight Convolutional Neural Network for High-throughput\n  Image-based Wheat Head Detection and Counting", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For a globally recognized planting breeding organization, manually-recorded\nfield observation data is crucial for plant breeding decision making. However,\ncertain phenotypic traits such as plant color, height, kernel counts, etc. can\nonly be collected during a specific time-window of a crop's growth cycle. Due\nto labor-intensive requirements, only a small subset of possible field\nobservations are recorded each season. To help mitigate this data collection\nbottleneck in wheat breeding, we propose a novel deep learning framework to\naccurately and efficiently count wheat heads to aid in the gathering of\nreal-time data for decision making. We call our model WheatNet and show that\nour approach is robust and accurate for a wide range of environmental\nconditions of the wheat field. WheatNet uses a truncated MobileNetV2 as a\nlightweight backbone feature extractor which merges feature maps with different\nscales to counter image scale variations. Then, extracted multi-scale features\ngo to two parallel sub-networks for simultaneous density-based counting and\nlocalization tasks. Our proposed method achieves an MAE and RMSE of 3.85 and\n5.19 in our wheat head counting task, respectively, while having significantly\nfewer parameters when compared to other state-of-the-art methods. Our\nexperiments and comparisons with other state-of-the-art methods demonstrate the\nsuperiority and effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 02:38:58 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 00:45:25 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Khaki", "Saeed", ""], ["Safaei", "Nima", ""], ["Pham", "Hieu", ""], ["Wang", "Lizhi", ""]]}, {"id": "2103.09422", "submitter": "Yuxuan Liu", "authors": "Yuxuan Liu, Lujia Wang, Ming Liu", "title": "YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection", "comments": "Accepcted by ICRA 2021. The arxiv version contains slightly more\n  information than the final ICRA version due to limit in the page number", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in 3D with stereo cameras is an important problem in\ncomputer vision, and is particularly crucial in low-cost autonomous mobile\nrobots without LiDARs.\n  Nowadays, most of the best-performing frameworks for stereo 3D object\ndetection are based on dense depth reconstruction from disparity estimation,\nmaking them extremely computationally expensive.\n  To enable real-world deployments of vision detection with binocular images,\nwe take a step back to gain insights from 2D image-based detection frameworks\nand enhance them with stereo features.\n  We incorporate knowledge and the inference structure from real-time one-stage\n2D/3D object detector and introduce a light-weight stereo matching module.\n  Our proposed framework, YOLOStereo3D, is trained on one single GPU and runs\nat more than ten fps. It demonstrates performance comparable to\nstate-of-the-art stereo 3D detection frameworks without usage of LiDAR data.\nThe code will be published in https://github.com/Owen-Liuyuxuan/visualDet3D.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 03:43:54 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Yuxuan", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""]]}, {"id": "2103.09435", "submitter": "Ahmed Elmoogy", "authors": "Ahmed Elmoogy, Xiaodai Dong, Tao Lu, Robert Westendorp, Kishore Reddy", "title": "Pose-GNN : Camera Pose Estimation System Using Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel image based localization system using graph neural\nnetworks (GNN). The pretrained ResNet50 convolutional neural network (CNN)\narchitecture is used to extract the important features for each image.\nFollowing, the extracted features are input to GNN to find the pose of each\nimage by either using the image features as a node in a graph and formulate the\npose estimation problem as node pose regression or modelling the image features\nthemselves as a graph and the problem becomes graph pose regression. We do an\nextensive comparison between the proposed two approaches and the state of the\nart single image localization methods and show that using GNN leads to enhanced\nperformance for both indoor and outdoor environments.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 04:40:02 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Elmoogy", "Ahmed", ""], ["Dong", "Xiaodai", ""], ["Lu", "Tao", ""], ["Westendorp", "Robert", ""], ["Reddy", "Kishore", ""]]}, {"id": "2103.09442", "submitter": "Ming Zhang", "authors": "Ming Zhang, Hong Yan", "title": "Improved Deep Classwise Hashing With Centers Similarity Learning for\n  Image Retrieval", "comments": "Accepted at ICPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep supervised hashing for image retrieval has attracted researchers'\nattention due to its high efficiency and superior retrieval performance. Most\nexisting deep supervised hashing works, which are based on pairwise/triplet\nlabels, suffer from the expensive computational cost and insufficient\nutilization of the semantics information. Recently, deep classwise hashing\nintroduced a classwise loss supervised by class labels information\nalternatively; however, we find it still has its drawback. In this paper, we\npropose an improved deep classwise hashing, which enables hashing learning and\nclass centers learning simultaneously. Specifically, we design a two-step\nstrategy on center similarity learning. It interacts with the classwise loss to\nattract the class center to concentrate on the intra-class samples while\npushing other class centers as far as possible. The centers similarity learning\ncontributes to generating more compact and discriminative hashing codes. We\nconduct experiments on three benchmark datasets. It shows that the proposed\nmethod effectively surpasses the original method and outperforms\nstate-of-the-art baselines under various commonly-used evaluation metrics for\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 05:01:13 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Zhang", "Ming", ""], ["Yan", "Hong", ""]]}, {"id": "2103.09448", "submitter": "Mazen Abdelfattah Mr", "authors": "Mazen Abdelfattah, Kaiwen Yuan, Z. Jane Wang, and Rabab Ward", "title": "Adversarial Attacks on Camera-LiDAR Models for 3D Car Detection", "comments": "arXiv admin note: text overlap with arXiv:2101.10747", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most autonomous vehicles (AVs) rely on LiDAR and RGB camera sensors for\nperception. Using these point cloud and image data, perception models based on\ndeep neural nets (DNNs) have achieved state-of-the-art performance in 3D\ndetection. The vulnerability of DNNs to adversarial attacks have been heavily\ninvestigated in the RGB image domain and more recently in the point cloud\ndomain, but rarely in both domains simultaneously. Multi-modal perception\nsystems used in AVs can be divided into two broad types: cascaded models which\nuse each modality independently, and fusion models which learn from different\nmodalities simultaneously. We propose a universal and physically realizable\nadversarial attack for each type, and study and contrast their respective\nvulnerabilities to attacks. We place a single adversarial object with specific\nshape and texture on top of a car with the objective of making this car evade\ndetection. Evaluating on the popular KITTI benchmark, our adversarial object\nmade the host vehicle escape detection by each model type nearly 50% of the\ntime. The dense RGB input contributed more to the success of the adversarial\nattacks on both cascaded and fusion models. We found that the fusion model was\nrelatively more robust to adversarial attacks than the cascaded model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 05:24:48 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Abdelfattah", "Mazen", ""], ["Yuan", "Kaiwen", ""], ["Wang", "Z. Jane", ""], ["Ward", "Rabab", ""]]}, {"id": "2103.09455", "submitter": "Wang Shen", "authors": "Wang Shen, Wenbo Bao, Guangtao Zhai, Charlie L Wang, Jerry W Hu,\n  Zhiyong Gao", "title": "Prediction-assistant Frame Super-Resolution for Video Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame transmission delay is critical in real-time applications such as\nonline video gaming, live show, etc. The receiving deadline of a new frame must\ncatch up with the frame rendering time. Otherwise, the system will buffer a\nwhile, and the user will encounter a frozen screen, resulting in unsatisfactory\nuser experiences. An effective approach is to transmit frames in lower-quality\nunder poor bandwidth conditions, such as using scalable video coding. In this\npaper, we propose to enhance video quality using lossy frames in two\nsituations. First, when current frames are too late to receive before rendering\ndeadline (i.e., lost), we propose to use previously received high-resolution\nimages to predict the future frames. Second, when the quality of the currently\nreceived frames is low~(i.e., lossy), we propose to use previously received\nhigh-resolution frames to enhance the low-quality current ones. For the first\ncase, we propose a small yet effective video frame prediction network. For the\nsecond case, we improve the video prediction network to a video enhancement\nnetwork to associate current frames as well as previous frames to restore\nhigh-quality images. Extensive experimental results demonstrate that our method\nperforms favorably against state-of-the-art algorithms in the lossy video\nstreaming environment.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 06:05:27 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Shen", "Wang", ""], ["Bao", "Wenbo", ""], ["Zhai", "Guangtao", ""], ["Wang", "Charlie L", ""], ["Hu", "Jerry W", ""], ["Gao", "Zhiyong", ""]]}, {"id": "2103.09458", "submitter": "Xiaobin Chang", "authors": "Xiaobin Chang, Frederick Tung, Greg Mori", "title": "Learning Discriminative Prototypes with Dynamic Time Warping", "comments": "CVPR'21 preview, 10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dynamic Time Warping (DTW) is widely used for temporal data processing.\nHowever, existing methods can neither learn the discriminative prototypes of\ndifferent classes nor exploit such prototypes for further analysis. We propose\nDiscriminative Prototype DTW (DP-DTW), a novel method to learn class-specific\ndiscriminative prototypes for temporal recognition tasks. DP-DTW shows superior\nperformance compared to conventional DTWs on time series classification\nbenchmarks. Combined with end-to-end deep learning, DP-DTW can handle\nchallenging weakly supervised action segmentation problems and achieves state\nof the art results on standard benchmarks. Moreover, detailed reasoning on the\ninput video is enabled by the learned action prototypes. Specifically, an\naction-based video summarization can be obtained by aligning the input sequence\nwith action prototypes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 06:11:11 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Chang", "Xiaobin", ""], ["Tung", "Frederick", ""], ["Mori", "Greg", ""]]}, {"id": "2103.09460", "submitter": "Qiang Chen", "authors": "Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, Jian\n  Sun", "title": "You Only Look One-level Feature", "comments": "Accepted by CVPR2021. We provide more details and add additional\n  comparison with YOLOv4 in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits feature pyramids networks (FPN) for one-stage detectors\nand points out that the success of FPN is due to its divide-and-conquer\nsolution to the optimization problem in object detection rather than\nmulti-scale feature fusion. From the perspective of optimization, we introduce\nan alternative way to address the problem instead of adopting the complex\nfeature pyramids - {\\em utilizing only one-level feature for detection}. Based\non the simple and efficient solution, we present You Only Look One-level\nFeature (YOLOF). In our method, two key components, Dilated Encoder and Uniform\nMatching, are proposed and bring considerable improvements. Extensive\nexperiments on the COCO benchmark prove the effectiveness of the proposed\nmodel. Our YOLOF achieves comparable results with its feature pyramids\ncounterpart RetinaNet while being $2.5\\times$ faster. Without transformer\nlayers, YOLOF can match the performance of DETR in a single-level feature\nmanner with $7\\times$ less training epochs. With an image size of\n$608\\times608$, YOLOF achieves 44.3 mAP running at 60 fps on 2080Ti, which is\n$13\\%$ faster than YOLOv4. Code is available at\n\\url{https://github.com/megvii-model/YOLOF}.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 06:24:10 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Chen", "Qiang", ""], ["Wang", "Yingming", ""], ["Yang", "Tong", ""], ["Zhang", "Xiangyu", ""], ["Cheng", "Jian", ""], ["Sun", "Jian", ""]]}, {"id": "2103.09468", "submitter": "Qizhou Wang", "authors": "Qizhou Wang, Jiangchao Yao, Chen Gong, Tongliang Liu, Mingming Gong,\n  Hongxia Yang, and Bo Han", "title": "Learning with Group Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning in the context of noise is a challenging but practical\nsetting to plenty of real-world applications. Most of the previous approaches\nin this area focus on the pairwise relation (casual or correlational\nrelationship) with noise, such as learning with noisy labels. However, the\ngroup noise, which is parasitic on the coarse-grained accurate relation with\nthe fine-grained uncertainty, is also universal and has not been well\ninvestigated. The challenge under this setting is how to discover true pairwise\nconnections concealed by the group relation with its fine-grained noise. To\novercome this issue, we propose a novel Max-Matching method for learning with\ngroup noise. Specifically, it utilizes a matching mechanism to evaluate the\nrelation confidence of each object w.r.t. the target, meanwhile considering the\nNon-IID characteristics among objects in the group. Only the most confident\nobject is considered to learn the model, so that the fine-grained noise is\nmostly dropped. The performance on arange of real-world datasets in the area of\nseveral learning paradigms demonstrates the effectiveness of Max-Matching\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 06:57:10 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wang", "Qizhou", ""], ["Yao", "Jiangchao", ""], ["Gong", "Chen", ""], ["Liu", "Tongliang", ""], ["Gong", "Mingming", ""], ["Yang", "Hongxia", ""], ["Han", "Bo", ""]]}, {"id": "2103.09475", "submitter": "Odar Zeynal", "authors": "Odar Zeynal, Saber Malekzadeh", "title": "Virtual Dress Swap Using Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online shopping has gained popularity recently. This paper addresses one\ncrucial problem of buying dress online, which has not been solved yet. This\nresearch tries to implement the idea of clothes swapping with the help of\nDeepFashion dataset where 6,223 images with eight landmarks each used. Deep\nConvolutional Neural Network has been built for Landmark detection.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 07:11:38 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Zeynal", "Odar", ""], ["Malekzadeh", "Saber", ""]]}, {"id": "2103.09479", "submitter": "Chongjian Ge", "authors": "Chongjian Ge, Yibing Song, Yuying Ge, Han Yang, Wei Liu and Ping Luo", "title": "Disentangled Cycle Consistency for Highly-realistic Virtual Try-On", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image virtual try-on replaces the clothes on a person image with a desired\nin-shop clothes image. It is challenging because the person and the in-shop\nclothes are unpaired. Existing methods formulate virtual try-on as either\nin-painting or cycle consistency. Both of these two formulations encourage the\ngeneration networks to reconstruct the input image in a self-supervised manner.\nHowever, existing methods do not differentiate clothing and non-clothing\nregions. A straight-forward generation impedes virtual try-on quality because\nof the heavily coupled image contents. In this paper, we propose a Disentangled\nCycle-consistency Try-On Network (DCTON). The DCTON is able to produce\nhighly-realistic try-on images by disentangling important components of virtual\ntry-on including clothes warping, skin synthesis, and image composition. To\nthis end, DCTON can be naturally trained in a self-supervised manner following\ncycle consistency learning. Extensive experiments on challenging benchmarks\nshow that DCTON outperforms state-of-the-art approaches favorably.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 07:18:55 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 08:08:17 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ge", "Chongjian", ""], ["Song", "Yibing", ""], ["Ge", "Yuying", ""], ["Yang", "Han", ""], ["Liu", "Wei", ""], ["Luo", "Ping", ""]]}, {"id": "2103.09488", "submitter": "Wenxin Yu", "authors": "Wenxin Yu, Bin Hu, Yucheng Hu, Tianxiang Lan, Yuanfan You, Dong Yin", "title": "Revisiting the Loss Weight Adjustment in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By definition, object detection requires a multi-task loss in order to solve\nclassification and regression tasks simultaneously. However, loss weight tends\nto be set manually in actuality. Therefore, a very practical problem that has\nnot been studied so far arises: how to quickly find the loss weight that fits\nthe current loss functions. In addition, when we choose different regression\nloss functions, whether the loss weight need to be adjusted and if so, how\nshould it be adjusted still is a problem demanding prompt solution. In this\npaper, through experiments and theoretical analysis of prediction box shifting,\nwe firstly find out three important conclusions about optimal loss weight\nallocation strategy, including (1) the classification loss curve decays faster\nthan regression loss curve; (2) loss weight is less than 1; (3) the gap between\nclassification and regression loss weight should not be too large. Then, based\non the above conclusions, we propose an Adaptive Loss Weight Adjustment(ALWA)\nto solve the above two problems by dynamically adjusting the loss weight in the\ntraining process, according to statistical characteristics of loss values. By\nincorporating ALWA into both one-stage and two-stage object detectors, we show\na consistent improvement on their performance using L1, SmoothL1 and CIoU loss,\nperformance measures on popular object detection benchmarks including PASCAL\nVOC and MS COCO. The code is available at https://github.com/ywx-hub/ALWA.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 07:45:06 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 05:48:04 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Yu", "Wenxin", ""], ["Hu", "Bin", ""], ["Hu", "Yucheng", ""], ["Lan", "Tianxiang", ""], ["You", "Yuanfan", ""], ["Yin", "Dong", ""]]}, {"id": "2103.09504", "submitter": "Yunbo Wang", "authors": "Yunbo Wang, Haixu Wu, Jianjin Zhang, Zhifeng Gao, Jianmin Wang, Philip\n  S. Yu, Mingsheng Long", "title": "PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The predictive learning of spatiotemporal sequences aims to generate future\nimages by learning from the historical context, where the visual dynamics are\nbelieved to have modular structures that can be learned with compositional\nsubsystems. This paper models these structures by presenting PredRNN, a new\nrecurrent network, in which a pair of memory cells are explicitly decoupled,\noperate in nearly independent transition manners, and finally form unified\nrepresentations of the complex environment. Concretely, besides the original\nmemory cell of LSTM, this network is featured by a zigzag memory flow that\npropagates in both bottom-up and top-down directions across all layers,\nenabling the learned visual dynamics at different levels of RNNs to\ncommunicate. It also leverages a memory decoupling loss to keep the memory\ncells from learning redundant features. We further improve PredRNN with a new\ncurriculum learning strategy, which can be generalized to most\nsequence-to-sequence RNNs in predictive learning scenarios. We provide detailed\nablation studies, gradient analyses, and visualizations to verify the\neffectiveness of each component. We show that our approach obtains highly\ncompetitive results on three standard datasets: the synthetic Moving MNIST\ndataset, the KTH human action dataset, and a radar echo dataset for\nprecipitation forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 08:28:30 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 07:38:07 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wang", "Yunbo", ""], ["Wu", "Haixu", ""], ["Zhang", "Jianjin", ""], ["Gao", "Zhifeng", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S.", ""], ["Long", "Mingsheng", ""]]}, {"id": "2103.09512", "submitter": "Rahul Vishwakarma", "authors": "Rahul Vishwakarma and Ravigopal Vennelakanti (Hitachi America Ltd.\n  R&D)", "title": "CNN Model & Tuning for Global Road Damage Detection", "comments": "7 pages, 13 figures, 4 tables, Invited submission to IEEE BigData Cup\n  Global Road Damage Detection Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper provides a report on our solution including model selection,\ntuning strategy and results obtained for Global Road Damage Detection\nChallenge. This Big Data Cup Challenge was held as a part of IEEE International\nConference on Big Data 2020. We assess single and multi-stage network\narchitectures for object detection and provide a benchmark using popular\nstate-of-the-art open-source PyTorch frameworks like Detectron2 and Yolov5.\nData preparation for provided Road Damage training dataset, captured using\nsmartphone camera from Czech, India and Japan is discussed. We studied the\neffect of training on a per country basis with respect to a single\ngeneralizable model. We briefly describe the tuning strategy for the\nexperiments conducted on two-stage Faster R-CNN with Deep Residual Network\n(Resnet) and Feature Pyramid Network (FPN) backbone. Additionally, we compare\nthis to a one-stage Yolov5 model with Cross Stage Partial Network (CSPNet)\nbackbone. We show a mean F1 score of 0.542 on Test2 and 0.536 on Test1 datasets\nusing a multi-stage Faster R-CNN model, with Resnet-50 and Resnet-101 backbones\nrespectively. This shows the generalizability of the Resnet-50 model when\ncompared to its more complex counterparts. Experiments were conducted using\nGoogle Colab having K80 and a Linux PC with 1080Ti, NVIDIA consumer grade GPU.\nA PyTorch based Detectron2 code to preprocess, train, test and submit the Avg\nF1 score to is made available at https://github.com/vishwakarmarhl/rdd2020\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:01:23 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Vishwakarma", "Rahul", "", "Hitachi America Ltd.\n  R&D"], ["Vennelakanti", "Ravigopal", "", "Hitachi America Ltd.\n  R&D"]]}, {"id": "2103.09528", "submitter": "Hideaki Hayashi D.Eng.", "authors": "Takato Otsuzuki, Heon Song, Seiichi Uchida, Hideaki Hayashi", "title": "Meta-learning of Pooling Layers for Character Recognition", "comments": "Accepted at ICDAR2021, 16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In convolutional neural network-based character recognition, pooling layers\nplay an important role in dimensionality reduction and deformation\ncompensation. However, their kernel shapes and pooling operations are\nempirically predetermined; typically, a fixed-size square kernel shape and max\npooling operation are used. In this paper, we propose a meta-learning framework\nfor pooling layers. As part of our framework, a parameterized pooling layer is\nproposed in which the kernel shape and pooling operation are trainable using\ntwo parameters, thereby allowing flexible pooling of the input data. We also\npropose a meta-learning algorithm for the parameterized pooling layer, which\nallows us to acquire a suitable pooling layer across multiple tasks. In the\nexperiment, we applied the proposed meta-learning framework to character\nrecognition tasks. The results demonstrate that a pooling layer that is\nsuitable across character recognition tasks was obtained via meta-learning, and\nthe obtained pooling layer improved the performance of the model in both\nfew-shot character recognition and noisy image recognition tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:25:47 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 05:16:13 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Otsuzuki", "Takato", ""], ["Song", "Heon", ""], ["Uchida", "Seiichi", ""], ["Hayashi", "Hideaki", ""]]}, {"id": "2103.09553", "submitter": "Bo Wei", "authors": "Bo Wei, Mulin Chen, Qi Wang, Xuelong Li", "title": "Multi-channel Deep Supervision for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is a task worth exploring in modern society because of its\nwide applications such as public safety and video monitoring. Many CNN-based\napproaches have been proposed to improve the accuracy of estimation, but there\nare some inherent issues affect the performance, such as overfitting and\ndetails lost caused by pooling layers. To tackle these problems, in this paper,\nwe propose an effective network called MDSNet, which introduces a novel\nsupervision framework called Multi-channel Deep Supervision (MDS). The MDS\nconducts channel-wise supervision on the decoder of the estimation model to\nhelp generate the density maps. To obtain the accurate supervision information\nof different channels, the MDSNet employs an auxiliary network called\nSupervisionNet (SN) to generate abundant supervision maps based on existing\ngroundtruth. Besides the traditional density map supervision, we also use the\nSN to convert the dot annotations into continuous supervision information and\nconduct dot supervision in the MDSNet. Extensive experiments on several\nmainstream benchmarks show that the proposed MDSNet achieves competitive\nresults and the MDS significantly improves the performance without changing the\nnetwork structure.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 10:33:58 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wei", "Bo", ""], ["Chen", "Mulin", ""], ["Wang", "Qi", ""], ["Li", "Xuelong", ""]]}, {"id": "2103.09564", "submitter": "Dominik Drees", "authors": "Dominik Drees and Xiaoyi Jiang", "title": "Hierarchical Random Walker Segmentation for Large Volumetric Biomedical\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random walker method for image segmentation is a popular tool for\nsemi-automatic image segmentation, especially in the biomedical field. However,\nits linear asymptotic run time and memory requirements make application to 3D\ndatasets of increasing sizes impractical. We propose a hierarchical framework\nthat, to the best of our knowledge, is the first attempt to overcome these\nrestrictions for the random walker algorithm and achieves sublinear run time\nand constant memory complexity. The method is evaluated on synthetic data and\nreal data from current biomedical research, where high segmentation quality is\nquantitatively confirmed and visually observed, respectively. The incremental\n(i.e., interaction update) run time is demonstrated to be in seconds on a\nstandard PC even for volumes of hundreds of Gigabytes in size. An\nimplementation of the presented method is publicly available in version 5.2 of\nthe widely used volume rendering and processing software Voreen\n(https://www.uni-muenster.de/Voreen/).\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 11:02:44 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Drees", "Dominik", ""], ["Jiang", "Xiaoyi", ""]]}, {"id": "2103.09576", "submitter": "Saem Park", "authors": "Saem Park, Donghoon Han, Nojun Kwak", "title": "The U-Net based GLOW for Optical-Flow-free Video Interframe Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame interpolation is the task of creating an interframe between two\nadjacent frames along the time axis. So, instead of simply averaging two\nadjacent frames to create an intermediate image, this operation should maintain\nsemantic continuity with the adjacent frames. Most conventional methods use\noptical flow, and various tools such as occlusion handling and object smoothing\nare indispensable. Since the use of these various tools leads to complex\nproblems, we tried to tackle the video interframe generation problem without\nusing problematic optical flow . To enable this , we have tried to use a deep\nneural network with an invertible structure, and developed an U-Net based\nGenerative Flow which is a modified normalizing flow. In addition, we propose a\nlearning method with a new consistency loss in the latent space to maintain\nsemantic temporal consistency between frames. The resolution of the generated\nimage is guaranteed to be identical to that of the original images by using an\ninvertible network. Furthermore, as it is not a random image like the ones by\ngenerative models, our network guarantees stable outputs without flicker.\nThrough experiments, we \\sam {confirmed the feasibility of the proposed\nalgorithm and would like to suggest the U-Net based Generative Flow as a new\npossibility for baseline in video frame interpolation. This paper is meaningful\nin that it is the world's first attempt to use invertible networks instead of\noptical flows for video interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 11:37:10 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 11:19:17 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 15:06:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Park", "Saem", ""], ["Han", "Donghoon", ""], ["Kwak", "Nojun", ""]]}, {"id": "2103.09577", "submitter": "Justyna P. Zwolak", "authors": "Brian J. Weber, Sandesh S. Kalantre, Thomas McJunkin, Jacob M. Taylor,\n  Justyna P. Zwolak", "title": "Theoretical bounds on data requirements for the ray-based classification", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of classifying high-dimensional shapes in real-world data grows\nin complexity as the dimension of the space increases. For the case of\nidentifying convex shapes of different geometries, a new classification\nframework has recently been proposed in which the intersections of a set of\none-dimensional representations, called rays, with the boundaries of the shape\nare used to identify the specific geometry. This ray-based classification (RBC)\nhas been empirically verified using a synthetic dataset of two- and\nthree-dimensional shapes [1] and, more recently, has also been validated\nexperimentally [2]. Here, we establish a bound on the number of rays necessary\nfor shape classification, defined by key angular metrics, for arbitrary convex\nshapes. For two dimensions, we derive a lower bound on the number of rays in\nterms of the shape's length, diameter, and exterior angles. For convex\npolytopes in R^N, we generalize this result to a similar bound given as a\nfunction of the dihedral angle and the geometrical parameters of polygonal\nfaces. This result enables a different approach for estimating high-dimensional\nshapes using substantially fewer data elements than volumetric or surface-based\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 11:38:45 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Weber", "Brian J.", ""], ["Kalantre", "Sandesh S.", ""], ["McJunkin", "Thomas", ""], ["Taylor", "Jacob M.", ""], ["Zwolak", "Justyna P.", ""]]}, {"id": "2103.09588", "submitter": "Houtan Ghaffari", "authors": "Houtan Ghaffari", "title": "An Efficient Method for the Classification of Croplands in Scarce-Label\n  Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two of the main challenges for cropland classification by satellite\ntime-series images are insufficient ground-truth data and inaccessibility of\nhigh-quality hyperspectral images for under-developed areas. Unlabeled\nmedium-resolution satellite images are abundant, but how to benefit from them\nis an open question. We will show how to leverage their potential for cropland\nclassification using self-supervised tasks. Self-supervision is an approach\nwhere we provide simple training signals for the samples, which are apparent\nfrom the data's structure. Hence, they are cheap to acquire and explain a\nsimple concept about the data. We introduce three self-supervised tasks for\ncropland classification. They reduce epistemic uncertainty, and the resulting\nmodel shows superior accuracy in a wide range of settings compared to SVM and\nRandom Forest. Subsequently, we use the self-supervised tasks to perform\nunsupervised domain adaptation and benefit from the labeled samples in other\nregions. It is crucial to know what information to transfer to avoid degrading\nthe performance. We show how to automate the information selection and transfer\nprocess in cropland classification even when the source and target areas have a\nvery different feature distribution. We improved the model by about 24%\ncompared to a baseline architecture without any labeled sample in the target\ndomain. Our method is amenable to gradual improvement, works with\nmedium-resolution satellite images, and does not require complicated models.\nCode and data are available.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:10:11 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ghaffari", "Houtan", ""]]}, {"id": "2103.09591", "submitter": "Yonatan Bitton", "authors": "Yonatan Bitton, Gabriel Stanovsky, Roy Schwartz, Michael Elhadad", "title": "Automatic Generation of Contrast Sets from Scene Graphs: Probing the\n  Compositional Consistency of GQA", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that supervised models often exploit data artifacts\nto achieve good test scores while their performance severely degrades on\nsamples outside their training distribution. Contrast sets (Gardneret al.,\n2020) quantify this phenomenon by perturbing test samples in a minimal way such\nthat the output label is modified. While most contrast sets were created\nmanually, requiring intensive annotation effort, we present a novel method\nwhich leverages rich semantic input representation to automatically generate\ncontrast sets for the visual question answering task. Our method computes the\nanswer of perturbed questions, thus vastly reducing annotation cost and\nenabling thorough evaluation of models' performance on various semantic aspects\n(e.g., spatial or relational reasoning). We demonstrate the effectiveness of\nour approach on the GQA dataset and its semantic scene graph image\nrepresentation. We find that, despite GQA's compositionality and carefully\nbalanced label distribution, two high-performing models drop 13-17% in accuracy\ncompared to the original test set. Finally, we show that our automatic\nperturbation can be applied to the training set to mitigate the degradation in\nperformance, opening the door to more robust models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:19:25 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bitton", "Yonatan", ""], ["Stanovsky", "Gabriel", ""], ["Schwartz", "Roy", ""], ["Elhadad", "Michael", ""]]}, {"id": "2103.09602", "submitter": "Gullal Singh Cheema", "authors": "Gullal S. Cheema and Sherzod Hakimov and Eric M\\\"uller-Budack and\n  Ralph Ewerth", "title": "On the Role of Images for Analyzing Claims in Social Media", "comments": "CLEOPATRA-2021 Workshop co-located with The Web Conf 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news is a severe problem in social media. In this paper, we present an\nempirical study on visual, textual, and multimodal models for the tasks of\nclaim, claim check-worthiness, and conspiracy detection, all of which are\nrelated to fake news detection. Recent work suggests that images are more\ninfluential than text and often appear alongside fake text. To this end,\nseveral multimodal models have been proposed in recent years that use images\nalong with text to detect fake news on social media sites like Twitter.\nHowever, the role of images is not well understood for claim detection,\nspecifically using transformer-based textual and multimodal models. We\ninvestigate state-of-the-art models for images, text (Transformer-based), and\nmultimodal information for four different datasets across two languages to\nunderstand the role of images in the task of claim and conspiracy detection.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:40:27 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Cheema", "Gullal S.", ""], ["Hakimov", "Sherzod", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2103.09669", "submitter": "Sebastian Bujwid", "authors": "Sebastian Bujwid, Josephine Sullivan", "title": "Large-Scale Zero-Shot Image Classification from Rich and Diverse Textual\n  Descriptions", "comments": "Accepted to LANTERN 2021. Project website:\n  https://bujwid.eu/p/zsl-imagenet-wiki", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the impact of using rich and diverse textual descriptions of classes\nfor zero-shot learning (ZSL) on ImageNet. We create a new dataset ImageNet-Wiki\nthat matches each ImageNet class to its corresponding Wikipedia article. We\nshow that merely employing these Wikipedia articles as class descriptions\nyields much higher ZSL performance than prior works. Even a simple model using\nthis type of auxiliary data outperforms state-of-the-art models that rely on\nstandard features of word embedding encodings of class names. These results\nhighlight the usefulness and importance of textual descriptions for ZSL, as\nwell as the relative importance of auxiliary data type compared to algorithmic\nprogress. Our experimental results also show that standard zero-shot learning\napproaches generalize poorly across categories of classes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:06:56 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bujwid", "Sebastian", ""], ["Sullivan", "Josephine", ""]]}, {"id": "2103.09671", "submitter": "Ario Sadafi", "authors": "Ario Sadafi, Luc\\'ia Mar\\'ia Moya Sans, Asya Makhro, Leonid Livshits,\n  Nassir Navab, Anna Bogdanova, Shadi Albarqouni, Carsten Marr", "title": "Fourier Transform of Percoll Gradients Boosts CNN Classification of\n  Hereditary Hemolytic Anemias", "comments": "Accepted for publication at the 2021 IEEE International Symposium on\n  Biomedical Imaging (ISBI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hereditary hemolytic anemias are genetic disorders that affect the shape and\ndensity of red blood cells. Genetic tests currently used to diagnose such\nanemias are expensive and unavailable in the majority of clinical labs. Here,\nwe propose a method for identifying hereditary hemolytic anemias based on a\nstandard biochemistry method, called Percoll gradient, obtained by centrifuging\na patient's blood. Our hybrid approach consists on using spatial data-driven\nfeatures, extracted with a convolutional neural network and spectral\nhandcrafted features obtained from fast Fourier transform. We compare late and\nearly feature fusion with AlexNet and VGG16 architectures. AlexNet with late\nfusion of spectral features performs better compared to other approaches. We\nachieved an average F1-score of 88% on different classes suggesting the\npossibility of diagnosing of hereditary hemolytic anemias from Percoll\ngradients. Finally, we utilize Grad-CAM to explore the spatial features used\nfor classification.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:09:53 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Sadafi", "Ario", ""], ["Sans", "Luc\u00eda Mar\u00eda Moya", ""], ["Makhro", "Asya", ""], ["Livshits", "Leonid", ""], ["Navab", "Nassir", ""], ["Bogdanova", "Anna", ""], ["Albarqouni", "Shadi", ""], ["Marr", "Carsten", ""]]}, {"id": "2103.09696", "submitter": "Paul Koch", "authors": "Paul Koch, Marian Schl\\\"uter, Serge Thill", "title": "Generating Annotated Training Data for 6D Object Pose Estimation in\n  Operational Environments with Minimal User Interaction", "comments": "This is a preprint and currently under peer review at IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently developed deep neural networks achieved state-of-the-art results in\nthe subject of 6D object pose estimation for robot manipulation. However, those\nsupervised deep learning methods require expensive annotated training data.\nCurrent methods for reducing those costs frequently use synthetic data from\nsimulations, but rely on expert knowledge and suffer from the \"domain gap\" when\nshifting to the real world. Here, we present a proof of concept for a novel\napproach of autonomously generating annotated training data for 6D object pose\nestimation. This approach is designed for learning new objects in operational\nenvironments while requiring little interaction and no expertise on the part of\nthe user. We evaluate our autonomous data generation approach in two grasping\nexperiments, where we archive a similar grasping success rate as related work\non a non autonomously generated data set.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:46:21 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Koch", "Paul", ""], ["Schl\u00fcter", "Marian", ""], ["Thill", "Serge", ""]]}, {"id": "2103.09697", "submitter": "Junlin Han", "authors": "Junlin Han and Mehrdad Shoeiby and Tim Malthus and Elizabeth Botha and\n  Janet Anstee and Saeed Anwar and Ran Wei and Lars Petersson and Mohammad Ali\n  Armin", "title": "Single Underwater Image Restoration by Contrastive Learning", "comments": "Accepted to IGARSS 2021 as oral presentation. Code is available at\n  https://github.com/JunlinHan/CWR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater image restoration attracts significant attention due to its\nimportance in unveiling the underwater world. This paper elaborates on a novel\nmethod that achieves state-of-the-art results for underwater image restoration\nbased on the unsupervised image-to-image translation framework. We design our\nmethod by leveraging from contrastive learning and generative adversarial\nnetworks to maximize mutual information between raw and restored images.\nAdditionally, we release a large-scale real underwater image dataset to support\nboth paired and unpaired training modules. Extensive experiments with\ncomparisons to recent approaches further demonstrate the superiority of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:47:03 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:10:50 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Han", "Junlin", ""], ["Shoeiby", "Mehrdad", ""], ["Malthus", "Tim", ""], ["Botha", "Elizabeth", ""], ["Anstee", "Janet", ""], ["Anwar", "Saeed", ""], ["Wei", "Ran", ""], ["Petersson", "Lars", ""], ["Armin", "Mohammad Ali", ""]]}, {"id": "2103.09699", "submitter": "Yingqian Wang", "authors": "Shitian He, Huanxin Zou, Yingqian Wang, Runlin Li, Fei Cheng", "title": "ShipSRDet: An End-to-End Remote Sensing Ship Detector Using\n  Super-Resolved Feature Representation", "comments": "Accepted to IGARSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-resolution remote sensing images can provide abundant appearance\ninformation for ship detection. Although several existing methods use image\nsuper-resolution (SR) approaches to improve the detection performance, they\nconsider image SR and ship detection as two separate processes and overlook the\ninternal coherence between these two correlated tasks. In this paper, we\nexplore the potential benefits introduced by image SR to ship detection, and\npropose an end-to-end network named ShipSRDet. In our method, we not only feed\nthe super-resolved images to the detector but also integrate the intermediate\nfeatures of the SR network with those of the detection network. In this way,\nthe informative feature representation extracted by the SR network can be fully\nused for ship detection. Experimental results on the HRSC dataset validate the\neffectiveness of our method. Our ShipSRDet can recover the missing details from\nthe input image and achieves promising ship detection performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:51:45 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["He", "Shitian", ""], ["Zou", "Huanxin", ""], ["Wang", "Yingqian", ""], ["Li", "Runlin", ""], ["Cheng", "Fei", ""]]}, {"id": "2103.09708", "submitter": "Pierre Dellenbach PDell", "authors": "Pierre Dellenbach, Jean-Emmanuel Deschaud, Bastien Jacquet,\n  Fran\\c{c}ois Goulette", "title": "What s in My LiDAR Odometry Toolbox?", "comments": "This work was realised in the context of the PhD thesis of Pierre\n  Dellenbach, financed by Kitware\n  (https://www.kitware.fr/equipe-vision-par-odinateur/), under the supervision\n  of Bastien Jacquet (Kitware), Jean-Emmanuel Deschaud and Fran\\c{c}ois\n  Goulette (Mines ParisTech)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the democratization of 3D LiDAR sensors, precise LiDAR odometries and\nSLAM are in high demand. New methods regularly appear, proposing solutions\nranging from small variations in classical algorithms to radically new\nparadigms based on deep learning. Yet it is often difficult to compare these\nmethods, notably due to the few datasets on which the methods can be evaluated\nand compared. Furthermore, their weaknesses are rarely examined, often letting\nthe user discover the hard way whether a method would be appropriate for a use\ncase. In this paper, we review and organize the main 3D LiDAR odometries into\ndistinct categories. We implemented several approaches (geometric based, deep\nlearning based, and hybrid methods) to conduct an in-depth analysis of their\nstrengths and weaknesses on multiple datasets, guiding the reader through the\ndifferent LiDAR odometries available. Implementation of the methods has been\nmade publicly available at\nhttps://gitlab.kitware.com/keu-computervision/pylidar-slam.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:04:23 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Dellenbach", "Pierre", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Jacquet", "Bastien", ""], ["Goulette", "Fran\u00e7ois", ""]]}, {"id": "2103.09712", "submitter": "Xiaojie Gao", "authors": "Xiaojie Gao, Yueming Jin, Yonghao Long, Qi Dou, Pheng-Ann Heng", "title": "Trans-SVNet: Accurate Phase Recognition from Surgical Videos via Hybrid\n  Embedding Aggregation Transformer", "comments": "MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time surgical phase recognition is a fundamental task in modern\noperating rooms. Previous works tackle this task relying on architectures\narranged in spatio-temporal order, however, the supportive benefits of\nintermediate spatial features are not considered. In this paper, we introduce,\nfor the first time in surgical workflow analysis, Transformer to reconsider the\nignored complementary effects of spatial and temporal features for accurate\nsurgical phase recognition. Our hybrid embedding aggregation Transformer fuses\ncleverly designed spatial and temporal embeddings by allowing for active\nqueries based on spatial information from temporal embedding sequences. More\nimportantly, our framework processes the hybrid embeddings in parallel to\nachieve a high inference speed. Our method is thoroughly validated on two large\nsurgical video datasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and\noutperforms the state-of-the-art approaches at a processing speed of 91 fps.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:12:55 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:18:39 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gao", "Xiaojie", ""], ["Jin", "Yueming", ""], ["Long", "Yonghao", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2103.09714", "submitter": "Boxiang Dong", "authors": "Boxiang Dong, Aparna S. Varde, Danilo Stevanovic, Jiayin Wang, Liang\n  Zhao", "title": "Interpretable Distance Metric Learning for Handwritten Chinese Character\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwriting recognition is of crucial importance to both Human Computer\nInteraction (HCI) and paperwork digitization. In the general field of Optical\nCharacter Recognition (OCR), handwritten Chinese character recognition faces\ntremendous challenges due to the enormously large character sets and the\namazing diversity of writing styles. Learning an appropriate distance metric to\nmeasure the difference between data inputs is the foundation of accurate\nhandwritten character recognition. Existing distance metric learning approaches\neither produce unacceptable error rates, or provide little interpretability in\nthe results. In this paper, we propose an interpretable distance metric\nlearning approach for handwritten Chinese character recognition. The learned\nmetric is a linear combination of intelligible base metrics, and thus provides\nmeaningful insights to ordinary users. Our experimental results on a benchmark\ndataset demonstrate the superior efficiency, accuracy and interpretability of\nour proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:17:02 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Dong", "Boxiang", ""], ["Varde", "Aparna S.", ""], ["Stevanovic", "Danilo", ""], ["Wang", "Jiayin", ""], ["Zhao", "Liang", ""]]}, {"id": "2103.09716", "submitter": "Yang Zhao", "authors": "Yang Zhao and Hao Zhang", "title": "Quantitative Effectiveness Assessment and Role Categorization of\n  Individual Units in Convolutional Neural Networks", "comments": "ICML submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying the roles of individual units is critical for understanding the\nmechanism of convolutional neural networks (CNNs). However, it is challenging\nto give the fully automatic and quantitative measures for effectiveness\nassessment of individual units in CNN. To this end, we propose a novel method\nfor quantitatively clarifying the status and usefulness of single unit of CNN\nin image classification tasks. The technical substance of our method is ranking\nthe importance of unit for each class in classification based on calculation of\nspecifically defined entropy using algebraic topological tools. It could be\nimplemented totally by machine without any human intervention. Some interesting\nphenomena including certain kind of phase transition are observed via the\nevolution of accuracy and loss of network in the successive ablation process of\nunits. All of the network units are divided into four categories according to\ntheir performance on training and testing data. The role categorization is\nexcellent startpoint for network construction and simplification. The diverse\nutility and contribution to the network generalization of units in\nclassification tasks are thoroughly illustrated by extensive experiments on\nnetwork (VGG) and dataset (ImageNet) with considerable scale. It is easy for\nour method to have extensional applications on other network models and tasks\nwithout essential difficulties.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:18:18 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Zhao", "Yang", ""], ["Zhang", "Hao", ""]]}, {"id": "2103.09720", "submitter": "Georgios Tziafas", "authors": "Giorgos Tziafas and Hamidreza Kasaei", "title": "Few-Shot Visual Grounding for Natural Human-Robot Interaction", "comments": "6 pages, 4 figures, ICARSC2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Human-Robot Interaction (HRI) is one of the key components for\nservice robots to be able to work in human-centric environments. In such\ndynamic environments, the robot needs to understand the intention of the user\nto accomplish a task successfully. Towards addressing this point, we propose a\nsoftware architecture that segments a target object from a crowded scene,\nindicated verbally by a human user. At the core of our system, we employ a\nmulti-modal deep neural network for visual grounding. Unlike most grounding\nmethods that tackle the challenge using pre-trained object detectors via a\ntwo-stepped process, we develop a single stage zero-shot model that is able to\nprovide predictions in unseen data. We evaluate the performance of the proposed\nmodel on real RGB-D data collected from public scene datasets. Experimental\nresults showed that the proposed model performs well in terms of accuracy and\nspeed, while showcasing robustness to variation in the natural language input.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:24:02 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 14:13:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tziafas", "Giorgos", ""], ["Kasaei", "Hamidreza", ""]]}, {"id": "2103.09742", "submitter": "Jongheon Jeong", "authors": "Jongheon Jeong and Jinwoo Shin", "title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "comments": "23 pages; ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in Generative Adversarial Networks (GANs) are actively\nrevisiting various data augmentation techniques as an effective way to prevent\ndiscriminator overfitting. It is still unclear, however, that which\naugmentations could actually improve GANs, and in particular, how to apply a\nwider range of augmentations in training. In this paper, we propose a novel way\nto address these questions by incorporating a recent contrastive representation\nlearning scheme into the GAN discriminator, coined ContraD. This \"fusion\"\nenables the discriminators to work with much stronger augmentations without\nincreasing their training instability, thereby preventing the discriminator\noverfitting issue in GANs more effectively. Even better, we observe that the\ncontrastive learning itself also benefits from our GAN training, i.e., by\nmaintaining discriminative features between real and fake samples, suggesting a\nstrong coherence between the two worlds: good contrastive representations are\nalso good for GAN discriminators, and vice versa. Our experimental results show\nthat GANs with ContraD consistently improve FID and IS compared to other recent\ntechniques incorporating data augmentations, still maintaining highly\ndiscriminative features in the discriminator in terms of the linear evaluation.\nFinally, as a byproduct, we also show that our GANs trained in an unsupervised\nmanner (without labels) can induce many conditional generative models via a\nsimple latent sampling, leveraging the learned features of ContraD. Code is\navailable at https://github.com/jh-jeong/ContraD.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:04:54 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Jeong", "Jongheon", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2103.09748", "submitter": "Steven Damelin Dr", "authors": "Steven B. Damelin", "title": "On the Whitney extension problem for near isometries and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this memoir, we develop a general framework which allows for a\nsimultaneous study of labeled and unlabeled near alignment data problems in\n$\\mathbb R^D$ and the Whitney near isometry extension problem for discrete and\nnon-discrete subsets of $\\mathbb R^D$ with certain geometries. In addition, we\nsurvey related work of ours on clustering, dimension reduction, manifold\nlearning, vision as well as minimal energy partitions, discrepancy and min-max\noptimization. Numerous open problems in harmonic analysis, computer vision,\nmanifold learning and signal processing connected to our work are given.\n  A significant portion of the work in this memoir is based on joint research\nwith Charles Fefferman in the papers [48], [49], [50], [51].\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:12:53 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 00:18:01 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Damelin", "Steven B.", ""]]}, {"id": "2103.09755", "submitter": "Kd Lv", "authors": "Zhenguang Liu, Kedi Lyu, Shuang Wu, Haipeng Chen, Yanbin Hao, Shouling\n  Ji", "title": "Aggregated Multi-GANs for Controlled 3D Human Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human motion prediction from historical pose sequence is at the core of many\napplications in machine intelligence. However, in current state-of-the-art\nmethods, the predicted future motion is confined within the same activity. One\ncan neither generate predictions that differ from the current activity, nor\nmanipulate the body parts to explore various future possibilities. Undoubtedly,\nthis greatly limits the usefulness and applicability of motion prediction. In\nthis paper, we propose a generalization of the human motion prediction task in\nwhich control parameters can be readily incorporated to adjust the forecasted\nmotion. Our method is compelling in that it enables manipulable motion\nprediction across activity types and allows customization of the human movement\nin a variety of fine-grained ways. To this aim, a simple yet effective\ncomposite GAN structure, consisting of local GANs for different body parts and\naggregated via a global GAN is presented. The local GANs game in lower\ndimensions, while the global GAN adjusts in high dimensional space to avoid\nmode collapse. Extensive experiments show that our method outperforms\nstate-of-the-art. The codes are available at\nhttps://github.com/herolvkd/AM-GAN.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:22:36 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Zhenguang", ""], ["Lyu", "Kedi", ""], ["Wu", "Shuang", ""], ["Chen", "Haipeng", ""], ["Hao", "Yanbin", ""], ["Ji", "Shouling", ""]]}, {"id": "2103.09762", "submitter": "Gobinda Saha", "authors": "Gobinda Saha, Isha Garg, Kaushik Roy", "title": "Gradient Projection Memory for Continual Learning", "comments": "Accepted for Oral Presentation at ICLR 2021\n  https://openreview.net/forum?id=3AOj0RCNC2", "journal-ref": "International Conference on Learning Representations (ICLR), 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn continually without forgetting the past tasks is a\ndesired attribute for artificial learning systems. Existing approaches to\nenable such learning in artificial neural networks usually rely on network\ngrowth, importance based weight update or replay of old data from the memory.\nIn contrast, we propose a novel approach where a neural network learns new\ntasks by taking gradient steps in the orthogonal direction to the gradient\nsubspaces deemed important for the past tasks. We find the bases of these\nsubspaces by analyzing network representations (activations) after learning\neach task with Singular Value Decomposition (SVD) in a single shot manner and\nstore them in the memory as Gradient Projection Memory (GPM). With qualitative\nand quantitative analyses, we show that such orthogonal gradient descent\ninduces minimum to no interference with the past tasks, thereby mitigates\nforgetting. We evaluate our algorithm on diverse image classification datasets\nwith short and long sequences of tasks and report better or on-par performance\ncompared to the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:31:29 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Saha", "Gobinda", ""], ["Garg", "Isha", ""], ["Roy", "Kaushik", ""]]}, {"id": "2103.09764", "submitter": "Yanlun Tu", "authors": "Yanlun Tu, Houchao Lei, Wei Long, Yang Yang", "title": "HAMIL: Hierarchical Aggregation-Based Multi-Instance Learning for\n  Microscopy Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance learning is common for computer vision tasks, especially in\nbiomedical image processing. Traditional methods for multi-instance learning\nfocus on designing feature aggregation methods and multi-instance classifiers,\nwhere the aggregation operation is performed either in feature extraction or\nlearning phase. As deep neural networks (DNNs) achieve great success in image\nprocessing via automatic feature learning, certain feature aggregation\nmechanisms need to be incorporated into common DNN architecture for\nmulti-instance learning. Moreover, flexibility and reliability are crucial\nconsiderations to deal with varying quality and number of instances.\n  In this study, we propose a hierarchical aggregation network for\nmulti-instance learning, called HAMIL. The hierarchical aggregation protocol\nenables feature fusion in a defined order, and the simple convolutional\naggregation units lead to an efficient and flexible architecture. We assess the\nmodel performance on two microscopy image classification tasks, namely protein\nsubcellular localization using immunofluorescence images and gene annotation\nusing spatial gene expression images. The experimental results show that HAMIL\noutperforms the state-of-the-art feature aggregation methods and the existing\nmodels for addressing these two tasks. The visualization analyses also\ndemonstrate the ability of HAMIL to focus on high-quality instances.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:34:08 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Tu", "Yanlun", ""], ["Lei", "Houchao", ""], ["Long", "Wei", ""], ["Yang", "Yang", ""]]}, {"id": "2103.09776", "submitter": "Dan Ruta", "authors": "Dan Ruta, Saeid Motiian, Baldo Faieta, Zhe Lin, Hailin Jin, Alex\n  Filipkowski, Andrew Gilbert, John Collomosse", "title": "ALADIN: All Layer Adaptive Instance Normalization for Fine-grained Style\n  Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ALADIN (All Layer AdaIN); a novel architecture for searching\nimages based on the similarity of their artistic style. Representation learning\nis critical to visual search, where distance in the learned search embedding\nreflects image similarity. Learning an embedding that discriminates\nfine-grained variations in style is hard, due to the difficulty of defining and\nlabelling style. ALADIN takes a weakly supervised approach to learning a\nrepresentation for fine-grained style similarity of digital artworks,\nleveraging BAM-FG, a novel large-scale dataset of user generated content\ngroupings gathered from the web. ALADIN sets a new state of the art accuracy\nfor style-based visual search over both coarse labelled style data (BAM) and\nBAM-FG; a new 2.62 million image dataset of 310,000 fine-grained style\ngroupings also contributed by this work.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:03:27 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ruta", "Dan", ""], ["Motiian", "Saeid", ""], ["Faieta", "Baldo", ""], ["Lin", "Zhe", ""], ["Jin", "Hailin", ""], ["Filipkowski", "Alex", ""], ["Gilbert", "Andrew", ""], ["Collomosse", "John", ""]]}, {"id": "2103.09787", "submitter": "Caleb Robinson", "authors": "Caleb Robinson, Anthony Ortiz, Juan M. Lavista Ferres, Brandon\n  Anderson, Daniel E. Ho", "title": "Temporal Cluster Matching for Change Detection of Structures from\n  Satellite Imagery", "comments": "Published in ACM COMPASS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Longitudinal studies are vital to understanding dynamic changes of the\nplanet, but labels (e.g., buildings, facilities, roads) are often available\nonly for a single point in time. We propose a general model, Temporal Cluster\nMatching (TCM), for detecting building changes in time series of remotely\nsensed imagery when footprint labels are observed only once. The intuition\nbehind the model is that the relationship between spectral values inside and\noutside of building's footprint will change when a building is constructed (or\ndemolished). For instance, in rural settings, the pre-construction area may\nlook similar to the surrounding environment until the building is constructed.\nSimilarly, in urban settings, the pre-construction areas will look different\nfrom the surrounding environment until construction. We further propose a\nheuristic method for selecting the parameters of our model which allows it to\nbe applied in novel settings without requiring data labeling efforts (to fit\nthe parameters). We apply our model over a dataset of poultry barns from\n2016/2017 high-resolution aerial imagery in the Delmarva Peninsula and a\ndataset of solar farms from a 2020 mosaic of Sentinel 2 imagery in India. Our\nresults show that our model performs as well when fit using the proposed\nheuristic as it does when fit with labeled data, and further, that supervised\nversions of our model perform the best among all the baselines we test against.\nFinally, we show that our proposed approach can act as an effective data\naugmentation strategy -- it enables researchers to augment existing structure\nfootprint labels along the time dimension and thus use imagery from multiple\npoints in time to train deep learning models. We show that this improves the\nspatial generalization of such models when evaluated on the same change\ndetection task.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:20:16 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:02:03 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Robinson", "Caleb", ""], ["Ortiz", "Anthony", ""], ["Ferres", "Juan M. Lavista", ""], ["Anderson", "Brandon", ""], ["Ho", "Daniel E.", ""]]}, {"id": "2103.09876", "submitter": "Vaikkunth Mugunthan", "authors": "Vaikkunth Mugunthan, Vignesh Gokul, Lalana Kagal, Shlomo Dubnov", "title": "Bias-Free FedGAN: A Federated Approach to Generate Bias-Free Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated Generative Adversarial Network (FedGAN) is a\ncommunication-efficient approach to train a GAN across distributed clients\nwithout clients having to share their sensitive training data. In this paper,\nwe experimentally show that FedGAN generates biased data points under\nnon-independent-and-identically-distributed (non-iid) settings. Also, we\npropose Bias-Free FedGAN, an approach to generate bias-free synthetic datasets\nusing FedGAN. Our approach generates metadata at the aggregator using the\nmodels received from clients and retrains the federated model to achieve\nbias-free results for image synthesis. Bias-Free FedGAN has the same\ncommunication cost as that of FedGAN. Experimental results on image datasets\n(MNIST and FashionMNIST) validate our claims.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 19:27:08 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 03:58:13 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Mugunthan", "Vaikkunth", ""], ["Gokul", "Vignesh", ""], ["Kagal", "Lalana", ""], ["Dubnov", "Shlomo", ""]]}, {"id": "2103.09882", "submitter": "Shakediel Hiba", "authors": "Shakediel Hiba and Yosi Keller", "title": "Hierarchical Attention-based Age Estimation and Bias Estimation", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we propose a novel deep-learning approach for age estimation\nbased on face images. We first introduce a dual image augmentation-aggregation\napproach based on attention. This allows the network to jointly utilize\nmultiple face image augmentations whose embeddings are aggregated by a\nTransformer-Encoder. The resulting aggregated embedding is shown to better\nencode the face image attributes. We then propose a probabilistic hierarchical\nregression framework that combines a discrete probabilistic estimate of age\nlabels, with a corresponding ensemble of regressors. Each regressor is\nparticularly adapted and trained to refine the probabilistic estimate over a\nrange of ages. Our scheme is shown to outperform contemporary schemes and\nprovide a new state-of-the-art age estimation accuracy, when applied to the\nMORPH II dataset for age estimation. Last, we introduce a bias analysis of\nstate-of-the-art age estimation results.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 19:41:34 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hiba", "Shakediel", ""], ["Keller", "Yosi", ""]]}, {"id": "2103.09891", "submitter": "Matthew Inkawhich", "authors": "Matthew Inkawhich, Nathan Inkawhich, Eric Davis, Hai Li and Yiran Chen", "title": "The Untapped Potential of Off-the-Shelf Convolutional Neural Networks", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over recent years, a myriad of novel convolutional network architectures have\nbeen developed to advance state-of-the-art performance on challenging\nrecognition tasks. As computational resources improve, a great deal of effort\nhas been placed in efficiently scaling up existing designs and generating new\narchitectures with Neural Architecture Search (NAS) algorithms. While network\ntopology has proven to be a critical factor for model performance, we show that\nsignificant gains are being left on the table by keeping topology static at\ninference-time. Due to challenges such as scale variation, we should not expect\nstatic models configured to perform well across a training dataset to be\noptimally configured to handle all test data. In this work, we seek to expose\nthe exciting potential of inference-time-dynamic models. By allowing just four\nlayers to dynamically change configuration at inference-time, we show that\nexisting off-the-shelf models like ResNet-50 are capable of over 95% accuracy\non ImageNet. This level of performance currently exceeds that of models with\nover 20x more parameters and significantly more complex training procedures.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 20:04:46 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Inkawhich", "Matthew", ""], ["Inkawhich", "Nathan", ""], ["Davis", "Eric", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "2103.09939", "submitter": "Mathias Ibsen", "authors": "Mathias Ibsen, Christian Rathgeb, Thomas Fink, Pawel Drozdowski,\n  Christoph Busch", "title": "Impact of Facial Tattoos and Paintings on Face Recognition Systems", "comments": "Accepted to IET Biometrics", "journal-ref": null, "doi": "10.1049/bme2.12032", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past years, face recognition technologies have shown impressive\nrecognition performance, mainly due to recent developments in deep\nconvolutional neural networks. Notwithstanding those improvements, several\nchallenges which affect the performance of face recognition systems remain. In\nthis work, we investigate the impact that facial tattoos and paintings have on\ncurrent face recognition systems. To this end, we first collected an\nappropriate database containing image-pairs of individuals with and without\nfacial tattoos or paintings. The assembled database was used to evaluate how\nfacial tattoos and paintings affect the detection, quality estimation, as well\nas the feature extraction and comparison modules of a face recognition system.\nThe impact on these modules was evaluated using state-of-the-art open-source\nand commercial systems. The obtained results show that facial tattoos and\npaintings affect all the tested modules, especially for images where a large\narea of the face is covered with tattoos or paintings. Our work is an initial\ncase-study and indicates a need to design algorithms which are robust to the\nvisual changes caused by facial tattoos and paintings.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 22:38:13 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 21:26:07 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Ibsen", "Mathias", ""], ["Rathgeb", "Christian", ""], ["Fink", "Thomas", ""], ["Drozdowski", "Pawel", ""], ["Busch", "Christoph", ""]]}, {"id": "2103.09942", "submitter": "Shreyansh Daftry", "authors": "Shreyansh Daftry, Barry Ridge, William Seto, Tu-Hoa Pham, Peter\n  Ilhardt, Gerard Maggiolino, Mark Van der Merwe, Alex Brinkman, John Mayo,\n  Eric Kulczyski and Renaud Detry", "title": "Machine Vision based Sample-Tube Localization for Mars Sample Return", "comments": "IEEE Aerospace Conference, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A potential Mars Sample Return (MSR) architecture is being jointly studied by\nNASA and ESA. As currently envisioned, the MSR campaign consists of a series of\n3 missions: sample cache, fetch and return to Earth. In this paper, we focus on\nthe fetch part of the MSR, and more specifically the problem of autonomously\ndetecting and localizing sample tubes deposited on the Martian surface. Towards\nthis end, we study two machine-vision based approaches: First, a\ngeometry-driven approach based on template matching that uses hard-coded\nfilters and a 3D shape model of the tube; and second, a data-driven approach\nbased on convolutional neural networks (CNNs) and learned features.\nFurthermore, we present a large benchmark dataset of sample-tube images,\ncollected in representative outdoor environments and annotated with ground\ntruth segmentation masks and locations. The dataset was acquired systematically\nacross different terrain, illumination conditions and dust-coverage; and\nbenchmarking was performed to study the feasibility of each approach, their\nrelative strengths and weaknesses, and robustness in the presence of adverse\nenvironmental conditions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 23:09:28 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Daftry", "Shreyansh", ""], ["Ridge", "Barry", ""], ["Seto", "William", ""], ["Pham", "Tu-Hoa", ""], ["Ilhardt", "Peter", ""], ["Maggiolino", "Gerard", ""], ["Van der Merwe", "Mark", ""], ["Brinkman", "Alex", ""], ["Mayo", "John", ""], ["Kulczyski", "Eric", ""], ["Detry", "Renaud", ""]]}, {"id": "2103.09943", "submitter": "Lantao Yu", "authors": "Lantao Yu, Dehong Liu, Hassan Mansour, Petros T. Boufounos", "title": "Fast and High-Quality Blind Multi-Spectral Image Pansharpening", "comments": "17 pages, 47 figures, journal, accepted by IEEE Transactions on\n  Geoscience and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind pansharpening addresses the problem of generating a high\nspatial-resolution multi-spectral (HRMS) image given a low spatial-resolution\nmulti-spectral (LRMS) image with the guidance of its associated spatially\nmisaligned high spatial-resolution panchromatic (PAN) image without parametric\nside information. In this paper, we propose a fast approach to blind\npansharpening and achieve state-of-the-art image reconstruction quality.\nTypical blind pansharpening algorithms are often computationally intensive\nsince the blur kernel and the target HRMS image are often computed using\niterative solvers and in an alternating fashion. To achieve fast blind\npansharpening, we decouple the solution of the blur kernel and of the HRMS\nimage. First, we estimate the blur kernel by computing the kernel coefficients\nwith minimum total generalized variation that blur a downsampled version of the\nPAN image to approximate a linear combination of the LRMS image channels. Then,\nwe estimate each channel of the HRMS image using local Laplacian prior to\nregularize the relationship between each HRMS channel and the PAN image.\nSolving the HRMS image is accelerated by both parallelizing across the channels\nand by fast numerical algorithms for each channel. Due to the fast scheme and\nthe powerful priors we used on the blur kernel coefficients (total generalized\nvariation) and on the cross-channel relationship (local Laplacian prior),\nnumerical experiments demonstrate that our algorithm outperforms\nstate-of-the-art model-based counterparts in terms of both computational time\nand reconstruction quality of the HRMS images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 23:12:14 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 18:25:31 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 03:41:47 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yu", "Lantao", ""], ["Liu", "Dehong", ""], ["Mansour", "Hassan", ""], ["Boufounos", "Petros T.", ""]]}, {"id": "2103.09950", "submitter": "Hossein Talebi", "authors": "Hossein Talebi, Peyman Milanfar", "title": "Learning to Resize Images for Computer Vision Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For all the ways convolutional neural nets have revolutionized computer\nvision in recent years, one important aspect has received surprisingly little\nattention: the effect of image size on the accuracy of tasks being trained for.\nTypically, to be efficient, the input images are resized to a relatively small\nspatial resolution (e.g. 224x224), and both training and inference are carried\nout at this resolution. The actual mechanism for this re-scaling has been an\nafterthought: Namely, off-the-shelf image resizers such as bilinear and bicubic\nare commonly used in most machine learning software frameworks. But do these\nresizers limit the on task performance of the trained networks? The answer is\nyes. Indeed, we show that the typical linear resizer can be replaced with\nlearned resizers that can substantially improve performance. Importantly, while\nthe classical resizers typically result in better perceptual quality of the\ndownscaled images, our proposed learned resizers do not necessarily give better\nvisual quality, but instead improve task performance. Our learned image resizer\nis jointly trained with a baseline vision model. This learned CNN-based resizer\ncreates machine friendly visual manipulations that lead to a consistent\nimprovement of the end task metric over the baseline model. Specifically, here\nwe focus on the classification task with the ImageNet dataset, and experiment\nwith four different models to learn resizers adapted to each model. Moreover,\nwe show that the proposed resizer can also be useful for fine-tuning the\nclassification baselines for other vision tasks. To this end, we experiment\nwith three different baselines to develop image quality assessment (IQA) models\non the AVA dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 23:43:44 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Talebi", "Hossein", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2103.09957", "submitter": "Andy Kim", "authors": "Emma Chen, Andy Kim, Rayan Krishnan, Jin Long, Andrew Y. Ng, Pranav\n  Rajpurkar", "title": "CheXbreak: Misclassification Identification for Deep Learning Models\n  Interpreting Chest X-rays", "comments": "In Proceedings of the 2021 Conference on Machine Learning for Health\n  Care, 2021. In ACM Conference on Health, Inference, and Learning (ACM-CHIL)\n  Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major obstacle to the integration of deep learning models for chest x-ray\ninterpretation into clinical settings is the lack of understanding of their\nfailure modes. In this work, we first investigate whether there are patient\nsubgroups that chest x-ray models are likely to misclassify. We find that\npatient age and the radiographic finding of lung lesion, pneumothorax or\nsupport devices are statistically relevant features for predicting\nmisclassification for some chest x-ray models. Second, we develop\nmisclassification predictors on chest x-ray models using their outputs and\nclinical features. We find that our best performing misclassification\nidentifier achieves an AUROC close to 0.9 for most diseases. Third, employing\nour misclassification identifiers, we develop a corrective algorithm to\nselectively flip model predictions that have high likelihood of\nmisclassification at inference time. We observe F1 improvement on the\nprediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,\n[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and\nhigh-performing chest x-ray models, we are able to derive insights across model\narchitectures and offer a generalizable framework applicable to other medical\nimaging tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 00:30:19 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 20:10:14 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 17:20:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chen", "Emma", ""], ["Kim", "Andy", ""], ["Krishnan", "Rayan", ""], ["Long", "Jin", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2103.09962", "submitter": "Jiangxin Dong", "authors": "Jiangxin Dong, Stefan Roth, Bernt Schiele", "title": "Deep Wiener Deconvolution: Wiener Meets Deep Learning for Image\n  Deblurring", "comments": "Accepted to NeurIPS 2020 as an oral presentation. Project page:\n  https://gitlab.mpi-klsb.mpg.de/jdong/dwdn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective approach for non-blind image deblurring,\ncombining classical techniques and deep learning. In contrast to existing\nmethods that deblur the image directly in the standard image space, we propose\nto perform an explicit deconvolution process in a feature space by integrating\na classical Wiener deconvolution framework with learned deep features. A\nmulti-scale feature refinement module then predicts the deblurred image from\nthe deconvolved deep features, progressively recovering detail and small-scale\nstructures. The proposed model is trained in an end-to-end manner and evaluated\non scenarios with both simulated and real-world image blur. Our extensive\nexperimental results show that the proposed deep Wiener deconvolution network\nfacilitates deblurred results with visibly fewer artifacts. Moreover, our\napproach quantitatively outperforms state-of-the-art non-blind image deblurring\nmethods by a wide margin.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 00:38:11 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Dong", "Jiangxin", ""], ["Roth", "Stefan", ""], ["Schiele", "Bernt", ""]]}, {"id": "2103.09992", "submitter": "Xiaoling Hu Mr", "authors": "Xiaoling Hu, Yusu Wang, Li Fuxin, Dimitris Samaras, Chao Chen", "title": "Topology-Aware Segmentation Using Discrete Morse Theory", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the segmentation of fine-scale structures from natural and biomedical\nimages, per-pixel accuracy is not the only metric of concern. Topological\ncorrectness, such as vessel connectivity and membrane closure, is crucial for\ndownstream analysis tasks. In this paper, we propose a new approach to train\ndeep image segmentation networks for better topological accuracy. In\nparticular, leveraging the power of discrete Morse theory (DMT), we identify\nglobal structures, including 1D skeletons and 2D patches, which are important\nfor topological accuracy. Trained with a novel loss based on these global\nstructures, the network performance is significantly improved especially near\ntopologically challenging locations (such as weak spots of connections and\nmembranes). On diverse datasets, our method achieves superior performance on\nboth the DICE score and topological metrics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 02:47:21 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hu", "Xiaoling", ""], ["Wang", "Yusu", ""], ["Fuxin", "Li", ""], ["Samaras", "Dimitris", ""], ["Chen", "Chao", ""]]}, {"id": "2103.09996", "submitter": "Tajwar Abrar Aleef", "authors": "Tajwar Abrar Aleef, Ingrid T. Spadinger, Michael D. Peacock, Septimiu\n  E. Salcudean, S. Sara Mahdavi", "title": "Rapid treatment planning for low-dose-rate prostate brachytherapy with\n  TP-GAN", "comments": "10 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment planning in low-dose-rate prostate brachytherapy (LDR-PB) aims to\nproduce arrangement of implantable radioactive seeds that deliver a minimum\nprescribed dose to the prostate whilst minimizing toxicity to healthy tissues.\nThere can be multiple seed arrangements that satisfy this dosimetric criterion,\nnot all deemed 'acceptable' for implant from a physician's perspective. This\nleads to plans that are subjective to the physician's/centre's preference,\nplanning style, and expertise. We propose a method that aims to reduce this\nvariability by training a model to learn from a large pool of successful\nretrospective LDR-PB data (961 patients) and create consistent plans that mimic\nthe high-quality manual plans. Our model is based on conditional generative\nadversarial networks that use a novel loss function for penalizing the model on\nspatial constraints of the seeds. An optional optimizer based on a simulated\nannealing (SA) algorithm can be used to further fine-tune the plans if\nnecessary (determined by the treating physician). Performance analysis was\nconducted on 150 test cases demonstrating comparable results to that of the\nmanual prehistorical plans. On average, the clinical target volume covering\n100% of the prescribed dose was 98.9% for our method compared to 99.4% for\nmanual plans. Moreover, using our model, the planning time was significantly\nreduced to an average of 2.5 mins/plan with SA, and less than 3 seconds without\nSA. Compared to this, manual planning at our centre takes around 20 mins/plan.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 03:02:45 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Aleef", "Tajwar Abrar", ""], ["Spadinger", "Ingrid T.", ""], ["Peacock", "Michael D.", ""], ["Salcudean", "Septimiu E.", ""], ["Mahdavi", "S. Sara", ""]]}, {"id": "2103.10003", "submitter": "Ashkan Ebadi", "authors": "Ashkan Ebadi, Pengcheng Xi, Alexander MacLean, St\\'ephane Tremblay,\n  Sonny Kohli, Alexander Wong", "title": "COVIDx-US -- An open-access benchmark dataset of ultrasound imaging data\n  for AI-driven COVID-19 analytics", "comments": "12 pages, 5 figures, to be submitted to Nature Scientific Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic continues to have a devastating effect on the health\nand well-being of the global population. Apart from the global health crises,\nthe pandemic has also caused significant economic and financial difficulties\nand socio-physiological implications. Effective screening, triage, treatment\nplanning, and prognostication of outcome plays a key role in controlling the\npandemic. Recent studies have highlighted the role of point-of-care ultrasound\nimaging for COVID-19 screening and prognosis, particularly given that it is\nnon-invasive, globally available, and easy-to-sanitize. Motivated by these\nattributes and the promise of artificial intelligence tools to aid clinicians,\nwe introduce COVIDx-US, an open-access benchmark dataset of COVID-19 related\nultrasound imaging data. The COVIDx-US dataset was curated from multiple\nsources and its current version, i.e., v1.2., consists of 150 lung ultrasound\nvideos and 12,943 processed images of patients infected with COVID-19\ninfection, non-COVID-19 infection, other lung diseases/conditions, as well as\nnormal control cases. The COVIDx-US is the largest open-access fully-curated\ndataset of its kind that has been systematically curated, processed, and\nvalidated specifically for the purpose of building and evaluating artificial\nintelligence algorithms and models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 03:31:33 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 13:51:52 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ebadi", "Ashkan", ""], ["Xi", "Pengcheng", ""], ["MacLean", "Alexander", ""], ["Tremblay", "St\u00e9phane", ""], ["Kohli", "Sonny", ""], ["Wong", "Alexander", ""]]}, {"id": "2103.10022", "submitter": "Jialun Peng", "authors": "Jialun Peng, Dong Liu, Songcen Xu, Houqiang Li", "title": "Generating Diverse Structure for Image Inpainting With Hierarchical\n  VQ-VAE", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an incomplete image without additional constraint, image inpainting\nnatively allows for multiple solutions as long as they appear plausible.\nRecently, multiplesolution inpainting methods have been proposed and shown the\npotential of generating diverse results. However, these methods have difficulty\nin ensuring the quality of each solution, e.g. they produce distorted structure\nand/or blurry texture. We propose a two-stage model for diverse inpainting,\nwhere the first stage generates multiple coarse results each of which has a\ndifferent structure, and the second stage refines each coarse result separately\nby augmenting texture. The proposed model is inspired by the hierarchical\nvector quantized variational auto-encoder (VQ-VAE), whose hierarchical\narchitecture isentangles structural and textural information. In addition, the\nvector quantization in VQVAE enables autoregressive modeling of the discrete\ndistribution over the structural information. Sampling from the distribution\ncan easily generate diverse and high-quality structures, making up the first\nstage of our model. In the second stage, we propose a structural attention\nmodule inside the texture generation network, where the module utilizes the\nstructural information to capture distant correlations. We further reuse the\nVQ-VAE to calculate two feature losses, which help improve structure coherence\nand texture realism, respectively. Experimental results on CelebA-HQ, Places2,\nand ImageNet datasets show that our method not only enhances the diversity of\nthe inpainting solutions but also improves the visual quality of the generated\nmultiple images. Code and models are available at:\nhttps://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 05:10:49 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Peng", "Jialun", ""], ["Liu", "Dong", ""], ["Xu", "Songcen", ""], ["Li", "Houqiang", ""]]}, {"id": "2103.10023", "submitter": "Yuxin Tian", "authors": "Yuxin Tian, Jinyu MIao, Xingming Wu, Haosong Yue, Zhong Liu, Weihai\n  Chen", "title": "Discriminative and Semantic Feature Selection for Place Recognition\n  towards Dynamic Environments", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Features play an important role in various visual tasks, especially in visual\nplace recognition applied in perceptual changing environments. In this paper,\nwe address the challenges of place recognition due to dynamics and confusable\npatterns by proposing a discriminative and semantic feature selection network,\ndubbed as DSFeat. Supervised by both semantic information and attention\nmechanism, we can estimate pixel-wise stability of features, indicating the\nprobability of a static and stable region from which features are extracted,\nand then select features that are insensitive to dynamic interference and\ndistinguishable to be correctly matched. The designed feature selection model\nis evaluated in place recognition and SLAM system in several public datasets\nwith varying appearances and viewpoints. Experimental results conclude that the\neffectiveness of the proposed method. It should be noticed that our proposal\ncan be readily pluggable into any feature-based SLAM system.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 05:11:46 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 03:35:24 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tian", "Yuxin", ""], ["MIao", "Jinyu", ""], ["Wu", "Xingming", ""], ["Yue", "Haosong", ""], ["Liu", "Zhong", ""], ["Chen", "Weihai", ""]]}, {"id": "2103.10024", "submitter": "Yihong Dong", "authors": "Yihong Dong, Lunchen Xie and Qingjiang Shi", "title": "Efficient Algorithms for Rotation Averaging Problems", "comments": "Rotation Averaging, Reconstruction, BCD, SUM", "journal-ref": null, "doi": "10.1145/3451263.", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rotation averaging problem is a fundamental task in computer vision\napplications. It is generally very difficult to solve due to the nonconvex\nrotation constraints. While a sufficient optimality condition is available in\nthe literature, there is a lack of \\yhedit{a} fast convergent algorithm to\nachieve stationary points. In this paper, by exploring the problem structure,\nwe first propose a block coordinate descent (BCD)-based rotation averaging\nalgorithm with guaranteed convergence to stationary points. Afterwards, we\nfurther propose an alternative rotation averaging algorithm by applying\nsuccessive upper-bound minimization (SUM) method. The SUM-based rotation\naveraging algorithm can be implemented in parallel and thus is more suitable\nfor addressing large-scale rotation averaging problems. Numerical examples\nverify that the proposed rotation averaging algorithms have superior\nconvergence performance as compared to the state-of-the-art algorithm.\nMoreover, by checking the sufficient optimality condition, we find from\nextensive numerical experiments that the proposed two algorithms can achieve\nglobally optimal solutions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 05:22:45 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Dong", "Yihong", ""], ["Xie", "Lunchen", ""], ["Shi", "Qingjiang", ""]]}, {"id": "2103.10029", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang, Wei Sui, Xinggang Wang, Wenming Meng, Hongmei Zhu, Qian\n  Zhang", "title": "Deep Online Correction for Monocular Visual Odometry", "comments": "Accepted at 2021 IEEE International Conference on Robotics and\n  Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel deep online correction (DOC) framework for\nmonocular visual odometry. The whole pipeline has two stages: First, depth maps\nand initial poses are obtained from convolutional neural networks (CNNs)\ntrained in self-supervised manners. Second, the poses predicted by CNNs are\nfurther improved by minimizing photometric errors via gradient updates of poses\nduring inference phases. The benefits of our proposed method are twofold: 1)\nDifferent from online-learning methods, DOC does not need to calculate gradient\npropagation for parameters of CNNs. Thus, it saves more computation resources\nduring inference phases. 2) Unlike hybrid methods that combine CNNs with\ntraditional methods, DOC fully relies on deep learning (DL) frameworks. Though\nwithout complex back-end optimization modules, our method achieves outstanding\nperformance with relative transform error (RTE) = 2.0% on KITTI Odometry\nbenchmark for Seq. 09, which outperforms traditional monocular VO frameworks\nand is comparable to hybrid methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 05:55:51 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Sui", "Wei", ""], ["Wang", "Xinggang", ""], ["Meng", "Wenming", ""], ["Zhu", "Hongmei", ""], ["Zhang", "Qian", ""]]}, {"id": "2103.10031", "submitter": "Aditya Jonnalagadda", "authors": "Aditya Jonnalagadda, Iuri Frosio, Seth Schneider, Morgan McGuire, and\n  Joohwan Kim", "title": "Robust Vision-Based Cheat Detection in Competitive Gaming", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game publishers and anti-cheat companies have been unsuccessful in blocking\ncheating in online gaming. We propose a novel, vision-based approach that\ncaptures the final state of the frame buffer and detects illicit overlays. To\nthis aim, we train and evaluate a DNN detector on a new dataset, collected\nusing two first-person shooter games and three cheating software. We study the\nadvantages and disadvantages of different DNN architectures operating on a\nlocal or global scale. We use output confidence analysis to avoid unreliable\ndetections and inform when network retraining is required. In an ablation\nstudy, we show how to use Interval Bound Propagation to build a detector that\nis also resistant to potential adversarial attacks and study its interaction\nwith confidence analysis. Our results show that robust and effective\nanti-cheating through machine learning is practically feasible and can be used\nto guarantee fair play in online gaming.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 06:06:52 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 06:15:43 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jonnalagadda", "Aditya", ""], ["Frosio", "Iuri", ""], ["Schneider", "Seth", ""], ["McGuire", "Morgan", ""], ["Kim", "Joohwan", ""]]}, {"id": "2103.10036", "submitter": "Seiya Matsuda", "authors": "Seiya Matsuda, Akisato Kimura, Seiichi Uchida", "title": "Impressions2Font: Generating Fonts by Specifying Impressions", "comments": "accepted at ICDAR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various fonts give us various impressions, which are often represented by\nwords. This paper proposes Impressions2Font (Imp2Font) that generates font\nimages with specific impressions. Imp2Font is an extended version of\nconditional generative adversarial networks (GANs). More precisely, Imp2Font\naccepts an arbitrary number of impression words as the condition to generate\nthe font images. These impression words are converted into a soft-constraint\nvector by an impression embedding module built on a word embedding technique.\nQualitative and quantitative evaluations prove that Imp2Font generates font\nimages with higher quality than comparative methods by providing multiple\nimpression words or even unlearned words.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 06:10:26 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 07:53:42 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Matsuda", "Seiya", ""], ["Kimura", "Akisato", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2103.10039", "submitter": "Naiyan Wang", "authors": "Lue Fan, Xuan Xiong, Feng Wang, Naiyan Wang, Zhaoxiang Zhang", "title": "RangeDet:In Defense of Range View for LiDAR-based 3D Object Detection", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an anchor-free single-stage LiDAR-based 3D object\ndetector -- RangeDet. The most notable difference with previous works is that\nour method is purely based on the range view representation. Compared with the\ncommonly used voxelized or Bird's Eye View (BEV) representations, the range\nview representation is more compact and without quantization error. Although\nthere are works adopting it for semantic segmentation, its performance in\nobject detection is largely behind voxelized or BEV counterparts. We first\nanalyze the existing range-view-based methods and find two issues overlooked by\nprevious works: 1) the scale variation between nearby and far away objects; 2)\nthe inconsistency between the 2D range image coordinates used in feature\nextraction and the 3D Cartesian coordinates used in output. Then we\ndeliberately design three components to address these issues in our RangeDet.\nWe test our RangeDet in the large-scale Waymo Open Dataset (WOD). Our best\nmodel achieves 72.9/75.9/65.8 3D AP on vehicle/pedestrian/cyclist. These\nresults outperform other range-view-based methods by a large margin (~20 3D AP\nin vehicle detection), and are overall comparable with the state-of-the-art\nmulti-view-based methods. Codes will be public.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 06:18:51 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Fan", "Lue", ""], ["Xiong", "Xuan", ""], ["Wang", "Feng", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "2103.10042", "submitter": "Zili Liu", "authors": "Zili Liu, Guodong Xu, Honghui Yang, Haifeng Liu, Deng Cai", "title": "SparsePoint: Fully End-to-End Sparse 3D Object Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detectors based on sparse object proposals have recently been proven\nto be successful in the 2D domain, which makes it possible to establish a fully\nend-to-end detector without time-consuming post-processing. This development is\nalso attractive for 3D object detectors. However, considering the remarkably\nlarger search space in the 3D domain, whether it is feasible to adopt the\nsparse method in the 3D object detection setting is still an open question. In\nthis paper, we propose SparsePoint, the first sparse method for 3D object\ndetection. Our SparsePoint adopts a number of learnable proposals to encode\nmost likely potential positions of 3D objects and a foreground embedding to\nencode shared semantic features of all objects. Besides, with the attention\nmodule to provide object-level interaction for redundant proposal removal and\nHungarian algorithm to supply one-one label assignment, our method can produce\nsparse and accurate predictions. SparsePoint sets a new state-of-the-art on\nfour public datasets, including ScanNetV2, SUN RGB-D, S3DIS, and Matterport3D.\nOur code will be publicly available soon.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 06:36:44 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Liu", "Zili", ""], ["Xu", "Guodong", ""], ["Yang", "Honghui", ""], ["Liu", "Haifeng", ""], ["Cai", "Deng", ""]]}, {"id": "2103.10043", "submitter": "Palash Goyal", "authors": "Saurabh Sahu and Palash Goyal", "title": "Enhancing Transformer for Video Understanding Using Gated Multi-Level\n  Attention and Temporal Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The introduction of Transformer model has led to tremendous advancements in\nsequence modeling, especially in text domain. However, the use of\nattention-based models for video understanding is still relatively unexplored.\nIn this paper, we introduce Gated Adversarial Transformer (GAT) to enhance the\napplicability of attention-based models to videos. GAT uses a multi-level\nattention gate to model the relevance of a frame based on local and global\ncontexts. This enables the model to understand the video at various\ngranularities. Further, GAT uses adversarial training to improve model\ngeneralization. We propose temporal attention regularization scheme to improve\nthe robustness of attention modules to adversarial examples. We illustrate the\nperformance of GAT on the large-scale YoutTube-8M data set on the task of video\ncategorization. We further show ablation studies along with quantitative and\nqualitative analysis to showcase the improvement.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 06:39:09 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Sahu", "Saurabh", ""], ["Goyal", "Palash", ""]]}, {"id": "2103.10047", "submitter": "Haoran Zhao", "authors": "Haoran Zhao, Kun Gong, Xin Sun, Junyu Dong and Hui Yu", "title": "Similarity Transfer for Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation is a popular paradigm for learning portable neural\nnetworks by transferring the knowledge from a large model into a smaller one.\nMost existing approaches enhance the student model by utilizing the similarity\ninformation between the categories of instance level provided by the teacher\nmodel. However, these works ignore the similarity correlation between different\ninstances that plays an important role in confidence prediction. To tackle this\nissue, we propose a novel method in this paper, called similarity transfer for\nknowledge distillation (STKD), which aims to fully utilize the similarities\nbetween categories of multiple samples. Furthermore, we propose to better\ncapture the similarity correlation between different instances by the mixup\ntechnique, which creates virtual samples by a weighted linear interpolation.\nNote that, our distillation loss can fully utilize the incorrect classes\nsimilarities by the mixed labels. The proposed approach promotes the\nperformance of student model as the virtual sample created by multiple images\nproduces a similar probability distribution in the teacher and student\nnetworks. Experiments and ablation studies on several public classification\ndatasets including CIFAR-10,CIFAR-100,CINIC-10 and Tiny-ImageNet verify that\nthis light-weight method can effectively boost the performance of the compact\nstudent model. It shows that STKD substantially has outperformed the vanilla\nknowledge distillation and has achieved superior accuracy over the\nstate-of-the-art knowledge distillation methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 06:54:59 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhao", "Haoran", ""], ["Gong", "Kun", ""], ["Sun", "Xin", ""], ["Dong", "Junyu", ""], ["Yu", "Hui", ""]]}, {"id": "2103.10050", "submitter": "Muhammad Usman Qadeer", "authors": "Muhammad Usman Qadeer, Salar Saeed, Murtaza Taj and Abubakr Muhammad", "title": "Spatio-temporal Crop Classification On Volumetric Data", "comments": "Submitted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-area crop classification using multi-spectral imagery is a widely\nstudied problem for several decades and is generally addressed using classical\nRandom Forest classifier. Recently, deep convolutional neural networks (DCNN)\nhave been proposed. However, these methods only achieved results comparable\nwith Random Forest. In this work, we present a novel CNN based architecture for\nlarge-area crop classification. Our methodology combines both spatio-temporal\nanalysis via 3D CNN as well as temporal analysis via 1D CNN. We evaluated the\nefficacy of our approach on Yolo and Imperial county benchmark datasets. Our\ncombined strategy outperforms both classical as well as recent DCNN based\nmethods in terms of classification accuracy by 2% while maintaining a minimum\nnumber of parameters and the lowest inference time.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 07:13:53 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Qadeer", "Muhammad Usman", ""], ["Saeed", "Salar", ""], ["Taj", "Murtaza", ""], ["Muhammad", "Abubakr", ""]]}, {"id": "2103.10051", "submitter": "Donghyun Lee", "authors": "Donghyun Lee, Minkyoung Cho, Seungwon Lee, Joonho Song and Changkyu\n  Choi", "title": "Data-free mixed-precision quantization using novel sensitivity metric", "comments": "Submission to ICIP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-training quantization is a representative technique for compressing\nneural networks, making them smaller and more efficient for deployment on edge\ndevices. However, an inaccessible user dataset often makes it difficult to\nensure the quality of the quantized neural network in practice. In addition,\nexisting approaches may use a single uniform bit-width across the network,\nresulting in significant accuracy degradation at extremely low bit-widths. To\nutilize multiple bit-width, sensitivity metric plays a key role in balancing\naccuracy and compression. In this paper, we propose a novel sensitivity metric\nthat considers the effect of quantization error on task loss and interaction\nwith other layers. Moreover, we develop labeled data generation methods that\nare not dependent on a specific operation of the neural network. Our\nexperiments show that the proposed metric better represents quantization\nsensitivity, and generated data are more feasible to be applied to\nmixed-precision quantization.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 07:23:21 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Lee", "Donghyun", ""], ["Cho", "Minkyoung", ""], ["Lee", "Seungwon", ""], ["Song", "Joonho", ""], ["Choi", "Changkyu", ""]]}, {"id": "2103.10056", "submitter": "Soroush Farghadani", "authors": "Reza Shirkavand, Sana Ayromlou, Soroush Farghadani, Maedeh-sadat\n  Tahaei, Fattane Pourakpour, Bahareh Siahlou, Zeynab Khodakarami, Mohammad H.\n  Rohban, Mansoor Fatehi, and Hamid R. Rabiee", "title": "Dementia Severity Classification under Small Sample Size and Weak\n  Supervision in Thick Slice MRI", "comments": "12 pages, 5 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Early detection of dementia through specific biomarkers in MR images plays a\ncritical role in developing support strategies proactively. Fazekas scale\nfacilitates an accurate quantitative assessment of the severity of white matter\nlesions and hence the disease. Imaging Biomarkers of dementia are multiple and\ncomprehensive documentation of them is time-consuming. Therefore, any effort to\nautomatically extract these biomarkers will be of clinical value while reducing\ninter-rater discrepancies. To tackle this problem, we propose to classify the\ndisease severity based on the Fazekas scale through the visual biomarkers,\nnamely the Periventricular White Matter (PVWM) and the Deep White Matter (DWM)\nchanges, in the real-world setting of thick-slice MRI. Small training sample\nsize and weak supervision in form of assigning severity labels to the whole MRI\nstack are among the main challenges. To combat the mentioned issues, we have\ndeveloped a deep learning pipeline that employs self-supervised representation\nlearning, multiple instance learning, and appropriate pre-processing steps. We\nuse pretext tasks such as non-linear transformation, local shuffling, in- and\nout-painting for self-supervised learning of useful features in this domain.\nFurthermore, an attention model is used to determine the relevance of each MRI\nslice for predicting the Fazekas scale in an unsupervised manner. We show the\nsignificant superiority of our method in distinguishing different classes of\ndementia compared to state-of-the-art methods in our mentioned setting, which\nimproves the macro averaged F1-score of state-of-the-art from 61% to 76% in\nPVWM, and from 58% to 69.2% in DWM.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 07:33:57 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Shirkavand", "Reza", ""], ["Ayromlou", "Sana", ""], ["Farghadani", "Soroush", ""], ["Tahaei", "Maedeh-sadat", ""], ["Pourakpour", "Fattane", ""], ["Siahlou", "Bahareh", ""], ["Khodakarami", "Zeynab", ""], ["Rohban", "Mohammad H.", ""], ["Fatehi", "Mansoor", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "2103.10081", "submitter": "Jinsu Yoo", "authors": "Jinsu Yoo and Tae Hyun Kim", "title": "Self-Supervised Adaptation for Video Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent single-image super-resolution (SISR) networks, which can adapt their\nnetwork parameters to specific input images, have shown promising results by\nexploiting the information available within the input data as well as large\nexternal datasets. However, the extension of these self-supervised SISR\napproaches to video handling has yet to be studied. Thus, we present a new\nlearning algorithm that allows conventional video super-resolution (VSR)\nnetworks to adapt their parameters to test video frames without using the\nground-truth datasets. By utilizing many self-similar patches across space and\ntime, we improve the performance of fully pre-trained VSR networks and produce\ntemporally consistent video frames. Moreover, we present a test-time knowledge\ndistillation technique that accelerates the adaptation speed with less hardware\nresources. In our experiments, we demonstrate that our novel learning algorithm\ncan fine-tune state-of-the-art VSR networks and substantially elevate\nperformance on numerous benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:30:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Yoo", "Jinsu", ""], ["Kim", "Tae Hyun", ""]]}, {"id": "2103.10084", "submitter": "Hao Chen", "authors": "Hao Chen, Xiaohua Li, Jiliu Zhou", "title": "TPPI-Net: Towards Efficient and Practical Hyperspectral Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral Image(HSI) classification is the most vibrant field of research\nin the hyperspectral community, which aims to assign each pixel in the image to\none certain category based on its spectral-spatial characteristics. Recently,\nsome spectral-spatial-feature based DCNNs have been proposed and demonstrated\nremarkable classification performance. When facing a real HSI, however, these\nNetworks have to deal with the pixels in the image one by one. The pixel-wise\nprocessing strategy is inefficient since there are numerous repeated\ncalculations between adjacent pixels. In this paper, firstly, a brand new\nNetwork design mechanism TPPI (training based on pixel and prediction based on\nimage) is proposed for HSI classification, which makes it possible to provide\nefficient and practical HSI classification with the restrictive conditions\nattached to the hyperspectral dataset. And then, according to the TPPI\nmechanism, TPPI-Net is derived based on the state of the art networks for HSI\nclassification. Experimental results show that the proposed TPPI-Net can not\nonly obtain high classification accuracy equivalent to the state of the art\nnetworks for HSI classification, but also greatly reduce the computational\ncomplexity of hyperspectral image prediction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:35:37 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chen", "Hao", ""], ["Li", "Xiaohua", ""], ["Zhou", "Jiliu", ""]]}, {"id": "2103.10089", "submitter": "Jinghao Zhou", "authors": "Jinghao Zhou, Bo Li, Lei Qiao, Peng Wang, Weihao Gan, Wei Wu, Junjie\n  Yan, Wanli Ouyang", "title": "Higher Performance Visual Tracking with Dual-Modal Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Object Tracking (VOT) has synchronous needs for both robustness and\naccuracy. While most existing works fail to operate simultaneously on both, we\ninvestigate in this work the problem of conflicting performance between\naccuracy and robustness. We first conduct a systematic comparison among\nexisting methods and analyze their restrictions in terms of accuracy and\nrobustness. Specifically, 4 formulations-offline classification (OFC), offline\nregression (OFR), online classification (ONC), and online regression (ONR)-are\nconsidered, categorized by the existence of online update and the types of\nsupervision signal. To account for the problem, we resort to the idea of\nensemble and propose a dual-modal framework for target localization, consisting\nof robust localization suppressing distractors via ONR and the accurate\nlocalization attending to the target center precisely via OFC. To yield a final\nrepresentation (i.e, bounding box), we propose a simple but effective score\nvoting strategy to involve adjacent predictions such that the final\nrepresentation does not commit to a single location. Operating beyond the\nreal-time demand, our proposed method is further validated on 8\ndatasets-VOT2018, VOT2019, OTB2015, NFS, UAV123, LaSOT, TrackingNet, and\nGOT-10k, achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:47:56 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhou", "Jinghao", ""], ["Li", "Bo", ""], ["Qiao", "Lei", ""], ["Wang", "Peng", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2103.10091", "submitter": "Yan Luo", "authors": "Yan Luo, Chongyang Zhang, Muming Zhao, Hao Zhou, Jun Sun", "title": "Which to Match? Selecting Consistent GT-Proposal Assignment for\n  Pedestrian Detection", "comments": "This manuscript is waiting for further improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate pedestrian classification and localization have received\nconsiderable attention due to their wide applications such as security\nmonitoring, autonomous driving, etc. Although pedestrian detectors have made\ngreat progress in recent years, the fixed Intersection over Union (IoU) based\nassignment-regression manner still limits their performance. Two main factors\nare responsible for this: 1) the IoU threshold faces a dilemma that a lower one\nwill result in more false positives, while a higher one will filter out the\nmatched positives; 2) the IoU-based GT-Proposal assignment suffers from the\ninconsistent supervision problem that spatially adjacent proposals with similar\nfeatures are assigned to different ground-truth boxes, which means some very\nsimilar proposals may be forced to regress towards different targets, and thus\nconfuses the bounding-box regression when predicting the location results. In\nthis paper, we first put forward the question that \\textbf{Regression\nDirection} would affect the performance for pedestrian detection. Consequently,\nwe address the weakness of IoU by introducing one geometric sensitive search\nalgorithm as a new assignment and regression metric. Different from the\nprevious IoU-based \\textbf{one-to-one} assignment manner of one proposal to one\nground-truth box, the proposed method attempts to seek a reasonable matching\nbetween the sets of proposals and ground-truth boxes. Specifically, we boost\nthe MR-FPPI under R$_{75}$ by 8.8\\% on Citypersons dataset. Furthermore, by\nincorporating this method as a metric into the state-of-the-art pedestrian\ndetectors, we show a consistent improvement.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:54:51 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Luo", "Yan", ""], ["Zhang", "Chongyang", ""], ["Zhao", "Muming", ""], ["Zhou", "Hao", ""], ["Sun", "Jun", ""]]}, {"id": "2103.10094", "submitter": "Gyeongsu Chae", "authors": "Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, Gyeongsu Chae", "title": "KoDF: A Large-scale Korean DeepFake Detection Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of effective face-swap and face-reenactment methods have been\npublicized in recent years, democratizing the face synthesis technology to a\ngreat extent. Videos generated as such have come to be collectively called\ndeepfakes with a negative connotation, for various social problems they have\ncaused. Facing the emerging threat of deepfakes, we have built the Korean\nDeepFake Detection Dataset (KoDF), a large-scale collection of synthesized and\nreal videos focused on Korean subjects. In this paper, we provide a detailed\ndescription of methods used to construct the dataset, experimentally show the\ndiscrepancy between the distributions of KoDF and existing deepfake detection\ndatasets, and underline the importance of using multiple datasets for\nreal-world generalization. KoDF is publicly available at\nhttps://moneybrain-research.github.io/kodf in its entirety (i.e. real clips,\nsynthesized clips, clips with additive noise, and their corresponding\nmetadata).\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 09:04:02 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Kwon", "Patrick", ""], ["You", "Jaeseong", ""], ["Nam", "Gyuhyeon", ""], ["Park", "Sungwoo", ""], ["Chae", "Gyeongsu", ""]]}, {"id": "2103.10095", "submitter": "Michael Wray", "authors": "Michael Wray, Hazel Doughty, Dima Damen", "title": "On Semantic Similarity in Video Retrieval", "comments": "Accepted in CVPR 2021. Project Page: https://mwray.github.io/SSVR/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current video retrieval efforts all found their evaluation on an\ninstance-based assumption, that only a single caption is relevant to a query\nvideo and vice versa. We demonstrate that this assumption results in\nperformance comparisons often not indicative of models' retrieval capabilities.\nWe propose a move to semantic similarity video retrieval, where (i) multiple\nvideos/captions can be deemed equally relevant, and their relative ranking does\nnot affect a method's reported performance and (ii) retrieved videos/captions\nare ranked by their similarity to a query. We propose several proxies to\nestimate semantic similarities in large-scale retrieval datasets, without\nadditional annotations. Our analysis is performed on three commonly used video\nretrieval datasets (MSR-VTT, YouCook2 and EPIC-KITCHENS).\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 09:12:40 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wray", "Michael", ""], ["Doughty", "Hazel", ""], ["Damen", "Dima", ""]]}, {"id": "2103.10107", "submitter": "Luk\\'a\\v{s} Picek", "authors": "Luk\\'a\\v{s} Picek, Milan \\v{S}ulc, Ji\\v{r}\\'i Matas, Jacob\n  Heilmann-Clausen, Thomas S. Jeppesen, Thomas L{\\ae}ss{\\o}e, Tobias Fr{\\o}slev", "title": "Danish Fungi 2020 -- Not Just Another Image Recognition Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel fine-grained dataset and benchmark, the Danish Fungi\n2020 (DF20). The dataset, constructed from observations submitted to the Danish\nFungal Atlas, is unique in its taxonomy-accurate class labels, small number of\nerrors, highly unbalanced long-tailed class distribution, rich observation\nmetadata, and well-defined class hierarchy. DF20 has zero overlap with\nImageNet, allowing unbiased comparison of models fine-tuned from publicly\navailable ImageNet checkpoints. The proposed evaluation protocol enables\ntesting the ability to improve classification using metadata -- e.g. precise\ngeographic location, habitat, and substrate, facilitates classifier calibration\ntesting, and finally allows to study the impact of the device settings on the\nclassification performance. Experiments using Convolutional Neural Networks\n(CNN) and the recent Vision Transformers (ViT) show that DF20 presents a\nchallenging task. Interestingly, ViT achieves results superior to CNN baselines\nwith 81.25% accuracy, reducing the CNN error by 13%. A baseline procedure for\nincluding metadata into the decision process improves the classification\naccuracy by more than 3.5 percentage points, reducing the error rate by 20%.\nThe source code for all methods and experiments is available at\nhttps://sites.google.com/view/danish-fungi-dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 09:33:11 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 12:15:47 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 08:43:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Picek", "Luk\u00e1\u0161", ""], ["\u0160ulc", "Milan", ""], ["Matas", "Ji\u0159\u00ed", ""], ["Heilmann-Clausen", "Jacob", ""], ["Jeppesen", "Thomas S.", ""], ["L\u00e6ss\u00f8e", "Thomas", ""], ["Fr\u00f8slev", "Tobias", ""]]}, {"id": "2103.10130", "submitter": "Jinghao Zhou", "authors": "Jinghao Zhou, Bo Li, Peng Wang, Peixia Li, Weihao Gan, Wei Wu, Junjie\n  Yan, Wanli Ouyang", "title": "Real-Time Visual Object Tracking via Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Object Tracking (VOT) can be seen as an extended task of Few-Shot\nLearning (FSL). While the concept of FSL is not new in tracking and has been\npreviously applied by prior works, most of them are tailored to fit specific\ntypes of FSL algorithms and may sacrifice running speed. In this work, we\npropose a generalized two-stage framework that is capable of employing a large\nvariety of FSL algorithms while presenting faster adaptation speed. The first\nstage uses a Siamese Regional Proposal Network to efficiently propose the\npotential candidates and the second stage reformulates the task of classifying\nthese candidates to a few-shot classification problem. Following such a\ncoarse-to-fine pipeline, the first stage proposes informative sparse samples\nfor the second stage, where a large variety of FSL algorithms can be conducted\nmore conveniently and efficiently. As substantiation of the second stage, we\nsystematically investigate several forms of optimization-based few-shot\nlearners from previous works with different objective functions, optimization\nmethods, or solution space. Beyond that, our framework also entails a direct\napplication of the majority of other FSL algorithms to visual tracking,\nenabling mutual communication between researchers on these two topics.\nExtensive experiments on the major benchmarks, VOT2018, OTB2015, NFS, UAV123,\nTrackingNet, and GOT-10k are conducted, demonstrating a desirable performance\ngain and a real-time speed.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:02:03 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhou", "Jinghao", ""], ["Li", "Bo", ""], ["Wang", "Peng", ""], ["Li", "Peixia", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Yan", "Junjie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2103.10139", "submitter": "Or Perel", "authors": "Or Perel, Oron Anschel, Omri Ben-Eliezer, Shai Mazor, Hadar\n  Averbuch-Elor", "title": "Learning Multimodal Affinities for Textual Editing in Images", "comments": "ACM Transactions on Graphics 2021, to be presented in SIGGRAPH 2021", "journal-ref": null, "doi": "10.1145/3451340", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, as cameras are rapidly adopted in our daily routine, images of\ndocuments are becoming both abundant and prevalent. Unlike natural images that\ncapture physical objects, document-images contain a significant amount of text\nwith critical semantics and complicated layouts. In this work, we devise a\ngeneric unsupervised technique to learn multimodal affinities between textual\nentities in a document-image, considering their visual style, the content of\ntheir underlying text and their geometric context within the image. We then use\nthese learned affinities to automatically cluster the textual entities in the\nimage into different semantic groups. The core of our approach is a deep\noptimization scheme dedicated for an image provided by the user that detects\nand leverages reliable pairwise connections in the multimodal representation of\nthe textual elements in order to properly learn the affinities. We show that\nour technique can operate on highly varying images spanning a wide range of\ndocuments and demonstrate its applicability for various editing operations\nmanipulating the content, appearance and geometry of the image.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:09:57 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Perel", "Or", ""], ["Anschel", "Oron", ""], ["Ben-Eliezer", "Omri", ""], ["Mazor", "Shai", ""], ["Averbuch-Elor", "Hadar", ""]]}, {"id": "2103.10148", "submitter": "Zhengjia Li", "authors": "Zhengjia Li, Duoqian Miao", "title": "Sequential End-to-end Network for Efficient Person Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search aims at jointly solving Person Detection and Person\nRe-identification (re-ID). Existing works have designed end-to-end networks\nbased on Faster R-CNN. However, due to the parallel structure of Faster R-CNN,\nthe extracted features come from the low-quality proposals generated by the\nRegion Proposal Network, rather than the detected high-quality bounding boxes.\nPerson search is a fine-grained task and such inferior features will\nsignificantly reduce re-ID performance. To address this issue, we propose a\nSequential End-to-end Network (SeqNet) to extract superior features. In SeqNet,\ndetection and re-ID are considered as a progressive process and tackled with\ntwo sub-networks sequentially. In addition, we design a robust Context\nBipartite Graph Matching (CBGM) algorithm to effectively employ context\ninformation as an important complementary cue for person matching. Extensive\nexperiments on two widely used person search benchmarks, CUHK-SYSU and PRW,\nhave shown that our method achieves state-of-the-art results. Also, our model\nruns at 11.5 fps on a single GPU and can be integrated into the existing\nend-to-end framework easily.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:28:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Li", "Zhengjia", ""], ["Miao", "Duoqian", ""]]}, {"id": "2103.10158", "submitter": "Samuel M\\\"uller", "authors": "Samuel G. M\\\"uller, Frank Hutter", "title": "TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic augmentation methods have recently become a crucial pillar for\nstrong model performance in vision tasks. Current methods are mostly a\ntrade-off between being simple, in-expensive or well-performing. We present a\nmost simple automatic augmentation baseline, TrivialAugment, that outperforms\nprevious methods almost for free. It is parameter-free and only applies a\nsingle augmentation to each image. To us, TrivialAugment's effectiveness is\nvery unexpected. Thus, we performed very thorough experiments on its\nperformance. First, we compare TrivialAugment to previous state-of-the-art\nmethods in a plethora of scenarios. Then, we perform multiple ablation studies\nwith different augmentation spaces, augmentation methods and setups to\nunderstand the crucial requirements for its performance. We condensate our\nlearnings into recommendations to automatic augmentation users. Additionally,\nwe provide a simple interface to use multiple automatic augmentation methods in\nany codebase, as well as, our full code base for reproducibility. Since our\nwork reveals a stagnation in many parts of automatic augmentation research, we\nend with a short proposal of best practices for sustained future progress in\nautomatic augmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:48:02 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["M\u00fcller", "Samuel G.", ""], ["Hutter", "Frank", ""]]}, {"id": "2103.10178", "submitter": "Qinji Yu", "authors": "Qinji Yu, Kang Dang, Nima Tajbakhsh, Demetri Terzopoulos, Xiaowei Ding", "title": "A Location-Sensitive Local Prototype Network for Few-Shot Medical Image\n  Segmentation", "comments": "ISBI2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the tremendous success of deep neural networks in medical image\nsegmentation, they typically require a large amount of costly, expert-level\nannotated data. Few-shot segmentation approaches address this issue by learning\nto transfer knowledge from limited quantities of labeled examples.\nIncorporating appropriate prior knowledge is critical in designing\nhigh-performance few-shot segmentation algorithms. Since strong spatial priors\nexist in many medical imaging modalities, we propose a prototype-based method\n-- namely, the location-sensitive local prototype network -- that leverages\nspatial priors to perform few-shot medical image segmentation. Our approach\ndivides the difficult problem of segmenting the entire image with global\nprototypes into easily solvable subproblems of local region segmentation with\nlocal prototypes. For organ segmentation experiments on the VISCERAL CT image\ndataset, our method outperforms the state-of-the-art approaches by 10% in the\nmean Dice coefficient. Extensive ablation studies demonstrate the substantial\nbenefits of incorporating spatial information and confirm the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:27:19 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Yu", "Qinji", ""], ["Dang", "Kang", ""], ["Tajbakhsh", "Nima", ""], ["Terzopoulos", "Demetri", ""], ["Ding", "Xiaowei", ""]]}, {"id": "2103.10179", "submitter": "Maximilian Schambach", "authors": "Maximilian Schambach, Jiayang Shi, Michael Heizmann", "title": "Spectral Reconstruction and Disparity from Spatio-Spectrally Coded Light\n  Fields via Multi-Task Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to reconstruct a spectral central view and its\naligned disparity map from spatio-spectrally coded light fields. Since we do\nnot reconstruct an intermediate full light field from the coded measurement, we\nrefer to this as principal reconstruction. The coded light fields correspond to\nthose captured by a light field camera in the unfocused design with a\nspectrally coded microlens array. In this application, the spectrally coded\nlight field camera can be interpreted as a single-shot spectral depth camera.\n  We investigate several multi-task deep learning methods and propose a new\nauxiliary loss-based training strategy to enhance the reconstruction\nperformance. The results are evaluated using a synthetic as well as a new\nreal-world spectral light field dataset that we captured using a custom-built\ncamera. The results are compared to state-of-the art compressed sensing\nreconstruction and disparity estimation.\n  We achieve a high reconstruction quality for both synthetic and real-world\ncoded light fields. The disparity estimation quality is on par with or even\noutperforms state-of-the-art disparity estimation from uncoded RGB light\nfields.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:28:05 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Schambach", "Maximilian", ""], ["Shi", "Jiayang", ""], ["Heizmann", "Michael", ""]]}, {"id": "2103.10180", "submitter": "Bruno Artacho", "authors": "Bruno Artacho and Andreas Savakis", "title": "OmniPose: A Multi-Scale Framework for Multi-Person Pose Estimation", "comments": "arXiv admin note: text overlap with arXiv:2001.08095", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose OmniPose, a single-pass, end-to-end trainable framework, that\nachieves state-of-the-art results for multi-person pose estimation. Using a\nnovel waterfall module, the OmniPose architecture leverages multi-scale feature\nrepresentations that increase the effectiveness of backbone feature extractors,\nwithout the need for post-processing. OmniPose incorporates contextual\ninformation across scales and joint localization with Gaussian heatmap\nmodulation at the multi-scale feature extractor to estimate human pose with\nstate-of-the-art accuracy. The multi-scale representations, obtained by the\nimproved waterfall module in OmniPose, leverage the efficiency of progressive\nfiltering in the cascade architecture, while maintaining multi-scale\nfields-of-view comparable to spatial pyramid configurations. Our results on\nmultiple datasets demonstrate that OmniPose, with an improved HRNet backbone\nand waterfall module, is a robust and efficient architecture for multi-person\npose estimation that achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:30:31 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Artacho", "Bruno", ""], ["Savakis", "Andreas", ""]]}, {"id": "2103.10182", "submitter": "Matthew Holden", "authors": "Matthew Holden, Marcelo Pereyra, Konstantinos C. Zygalakis", "title": "Bayesian Imaging With Data-Driven Priors Encoded by Neural Networks:\n  Theory, Methods, and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new methodology for performing Bayesian inference in\nimaging inverse problems where the prior knowledge is available in the form of\ntraining data. Following the manifold hypothesis and adopting a generative\nmodelling approach, we construct a data-driven prior that is supported on a\nsub-manifold of the ambient space, which we can learn from the training data by\nusing a variational autoencoder or a generative adversarial network. We\nestablish the existence and well-posedness of the associated posterior\ndistribution and posterior moments under easily verifiable conditions,\nproviding a rigorous underpinning for Bayesian estimators and uncertainty\nquantification analyses. Bayesian computation is performed by using a parallel\ntempered version of the preconditioned Crank-Nicolson algorithm on the\nmanifold, which is shown to be ergodic and robust to the non-convex nature of\nthese data-driven models. In addition to point estimators and uncertainty\nquantification analyses, we derive a model misspecification test to\nautomatically detect situations where the data-driven prior is unreliable, and\nexplain how to identify the dimension of the latent space directly from the\ntraining data. The proposed approach is illustrated with a range of experiments\nwith the MNIST dataset, where it outperforms alternative image reconstruction\napproaches from the state of the art. A model accuracy analysis suggests that\nthe Bayesian probabilities reported by the data-driven models are also\nremarkably accurate under a frequentist definition of probability.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:34:08 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Holden", "Matthew", ""], ["Pereyra", "Marcelo", ""], ["Zygalakis", "Konstantinos C.", ""]]}, {"id": "2103.10189", "submitter": "Jiawei Shi", "authors": "Jiawei Shi and Songhao Zhu and Zhiwei Liang", "title": "Learning to Amend Facial Expression Representation via De-albino and\n  Affinity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Expression Recognition (FER) is a classification task that points to\nface variants. Hence, there are certain affinity features between facial\nexpressions, receiving little attention in the FER literature. Convolution\npadding, despite helping capture the edge information, causes erosion of the\nfeature map simultaneously. After multi-layer filling convolution, the output\nfeature map named albino feature definitely weakens the representation of the\nexpression. To tackle these challenges, we propose a novel architecture named\nAmending Representation Module (ARM). ARM is a substitute for the pooling\nlayer. Theoretically, it can be embedded in the back end of any network to deal\nwith the Padding Erosion. ARM efficiently enhances facial expression\nrepresentation from two different directions: 1) reducing the weight of eroded\nfeatures to offset the side effect of padding, and 2) sharing affinity features\nover mini-batch to strengthen the representation learning. Experiments on\npublic benchmarks prove that our ARM boosts the performance of FER remarkably.\nThe validation accuracies are respectively 92.05% on RAF-DB, 65.2% on\nAffect-Net, and 58.71% on SFEW, exceeding current state-of-the-art methods. Our\nimplementation and trained models are available at\nhttps://github.com/JiaweiShiCV/Amend-Representation-Module.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:54:13 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 11:06:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Shi", "Jiawei", ""], ["Zhu", "Songhao", ""], ["Liang", "Zhiwei", ""]]}, {"id": "2103.10191", "submitter": "Qianyu Feng", "authors": "Qianyu Feng, Yunchao Wei, Mingming Cheng, Yi Yang", "title": "Decoupled Spatial Temporal Graphs for Generic Visual Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual grounding is a long-lasting problem in vision-language understanding\ndue to its diversity and complexity. Current practices concentrate mostly on\nperforming visual grounding in still images or well-trimmed video clips. This\nwork, on the other hand, investigates into a more general setting, generic\nvisual grounding, aiming to mine all the objects satisfying the given\nexpression, which is more challenging yet practical in real-world scenarios.\nImportantly, grounding results are expected to accurately localize targets in\nboth space and time. Whereas, it is tricky to make trade-offs between the\nappearance and motion features. In real scenarios, model tends to fail in\ndistinguishing distractors with similar attributes. Motivated by these\nconsiderations, we propose a simple yet effective approach, named DSTG, which\ncommits to 1) decomposing the spatial and temporal representations to collect\nall-sided cues for precise grounding; 2) enhancing the discriminativeness from\ndistractors and the temporal consistency with a contrastive learning routing\nstrategy. We further elaborate a new video dataset, GVG, that consists of\nchallenging referring cases with far-ranging videos. Empirical experiments well\ndemonstrate the superiority of DSTG over state-of-the-art on Charades-STA,\nActivityNet-Caption and GVG datasets. Code and dataset will be made available.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:56:29 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Feng", "Qianyu", ""], ["Wei", "Yunchao", ""], ["Cheng", "Mingming", ""], ["Yang", "Yi", ""]]}, {"id": "2103.10206", "submitter": "Buyu Li", "authors": "Buyu Li, Yongchi Zhao, Lu Sheng", "title": "DanceNet3D: Music Based Dance Generation with Parametric Motion\n  Transformer", "comments": "Add project link in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a novel deep learning framework that can generate a\nvivid dance from a whole piece of music. In contrast to previous works that\ndefine the problem as generation of frames of motion state parameters, we\nformulate the task as a prediction of motion curves between key poses, which is\ninspired by the animation industry practice. The proposed framework, named\nDanceNet3D, first generates key poses on beats of the given music and then\npredicts the in-between motion curves. DanceNet3D adopts the encoder-decoder\narchitecture and the adversarial schemes for training. The decoders in\nDanceNet3D are constructed on MoTrans, a transformer tailored for motion\ngeneration. In MoTrans we introduce the kinematic correlation by the Kinematic\nChain Networks, and we also propose the Learned Local Attention module to take\nthe temporal local correlation of human motion into consideration. Furthermore,\nwe propose PhantomDance, the first large-scale dance dataset produced by\nprofessional animatiors, with accurate synchronization with music. Extensive\nexperiments demonstrate that the proposed approach can generate fluent,\nelegant, performative and beat-synchronized 3D dances, which significantly\nsurpasses previous works quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 12:17:38 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 07:28:26 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 09:27:22 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Li", "Buyu", ""], ["Zhao", "Yongchi", ""], ["Sheng", "Lu", ""]]}, {"id": "2103.10211", "submitter": "Yuki Asano", "authors": "Mandela Patrick, Yuki M. Asano, Bernie Huang, Ishan Misra, Florian\n  Metze, Joao Henriques, Andrea Vedaldi", "title": "Space-Time Crop & Attend: Improving Cross-modal Video Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The quality of the image representations obtained from self-supervised\nlearning depends strongly on the type of data augmentations used in the\nlearning formulation. Recent papers have ported these methods from still images\nto videos and found that leveraging both audio and video signals yields strong\ngains; however, they did not find that spatial augmentations such as cropping,\nwhich are very important for still images, work as well for videos. In this\npaper, we improve these formulations in two ways unique to the spatio-temporal\naspect of videos. First, for space, we show that spatial augmentations such as\ncropping do work well for videos too, but that previous implementations, due to\nthe high processing and memory cost, could not do this at a scale sufficient\nfor it to work well. To address this issue, we first introduce Feature Crop, a\nmethod to simulate such augmentations much more efficiently directly in feature\nspace. Second, we show that as opposed to naive average pooling, the use of\ntransformer-based attention improves performance significantly, and is well\nsuited for processing feature crops. Combining both of our discoveries into a\nnew method, Space-time Crop & Attend (STiCA) we achieve state-of-the-art\nperformance across multiple video-representation learning benchmarks. In\nparticular, we achieve new state-of-the-art accuracies of 67.0% on HMDB-51 and\n93.1% on UCF-101 when pre-training on Kinetics-400.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 12:32:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Patrick", "Mandela", ""], ["Asano", "Yuki M.", ""], ["Huang", "Bernie", ""], ["Misra", "Ishan", ""], ["Metze", "Florian", ""], ["Henriques", "Joao", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2103.10226", "submitter": "Pau Rodr\\'iguez L\\'opez", "authors": "Pau Rodriguez, Massimo Caccia, Alexandre Lacoste, Lee Zamparo, Issam\n  Laradji, Laurent Charlin, David Vazquez", "title": "Beyond Trivial Counterfactual Explanations with Diverse Valuable\n  Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainability for machine learning models has gained considerable attention\nwithin our research community given the importance of deploying more reliable\nmachine-learning systems. In computer vision applications, generative\ncounterfactual methods indicate how to perturb a model's input to change its\nprediction, providing details about the model's decision-making. Current\ncounterfactual methods make ambiguous interpretations as they combine multiple\nbiases of the model and the data in a single counterfactual interpretation of\nthe model's decision. Moreover, these methods tend to generate trivial\ncounterfactuals about the model's decision, as they often suggest to exaggerate\nor remove the presence of the attribute being classified. For the machine\nlearning practitioner, these types of counterfactuals offer little value, since\nthey provide no new information about undesired model or data biases. In this\nwork, we propose a counterfactual method that learns a perturbation in a\ndisentangled latent space that is constrained using a diversity-enforcing loss\nto uncover multiple valuable explanations about the model's prediction.\nFurther, we introduce a mechanism to prevent the model from producing trivial\nexplanations. Experiments on CelebA and Synbols demonstrate that our model\nimproves the success rate of producing high-quality valuable explanations when\ncompared to previous state-of-the-art methods. We will publish the code.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 12:57:34 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Rodriguez", "Pau", ""], ["Caccia", "Massimo", ""], ["Lacoste", "Alexandre", ""], ["Zamparo", "Lee", ""], ["Laradji", "Issam", ""], ["Charlin", "Laurent", ""], ["Vazquez", "David", ""]]}, {"id": "2103.10230", "submitter": "Jaeyeon Jang Dr.", "authors": "Jaeyeon Jang and Chang Ouk Kim", "title": "Collective Decision of One-vs-Rest Networks for Open Set Recognition", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unknown examples that are unseen during training often appear in real-world\nmachine learning tasks, and an intelligent self-learning system should be able\nto distinguish between known and unknown examples. Accordingly, open set\nrecognition (OSR), which addresses the problem of classifying knowns and\nidentifying unknowns, has recently been highlighted. However, conventional deep\nneural networks using a softmax layer are vulnerable to overgeneralization,\nproducing high confidence scores for unknowns. In this paper, we propose a\nsimple OSR method based on the intuition that OSR performance can be maximized\nby setting strict and sophisticated decision boundaries that reject unknowns\nwhile maintaining satisfactory classification performance on knowns. For this\npurpose, a novel network structure is proposed, in which multiple one-vs-rest\nnetworks (OVRNs) follow a convolutional neural network feature extractor. Here,\nthe OVRN is a simple feed-forward neural network that enhances the ability to\nreject nonmatches by learning class-specific discriminative features.\nFurthermore, the collective decision score is modeled by combining the multiple\ndecisions reached by the OVRNs to alleviate overgeneralization. Extensive\nexperiments were conducted on various datasets, and the experimental results\nshowed that the proposed method performed significantly better than the\nstate-of-the-art methods by effectively reducing overgeneralization.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 13:06:46 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 22:00:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jang", "Jaeyeon", ""], ["Kim", "Chang Ouk", ""]]}, {"id": "2103.10234", "submitter": "Yue Cao", "authors": "Yue Cao and Xiaohe Wu and Shuran Qi and Xiao Liu and Zhongqin Wu and\n  Wangmeng Zuo", "title": "Pseudo-ISP: Learning Pseudo In-camera Signal Processing Pipeline from A\n  Color Image Denoiser", "comments": "The source code and pre-trained model are available at\n  https://github.com/happycaoyue/Pseudo-ISP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep denoisers on real-world color photographs usually relies\non the modeling of sensor noise and in-camera signal processing (ISP) pipeline.\nPerformance drop will inevitably happen when the sensor and ISP pipeline of\ntest images are different from those for training the deep denoisers (i.e.,\nnoise discrepancy). In this paper, we present an unpaired learning scheme to\nadapt a color image denoiser for handling test images with noise discrepancy.\nWe consider a practical training setting, i.e., a pre-trained denoiser, a set\nof test noisy images, and an unpaired set of clean images. To begin with, the\npre-trained denoiser is used to generate the pseudo clean images for the test\nimages. Pseudo-ISP is then suggested to jointly learn the pseudo ISP pipeline\nand signal-dependent rawRGB noise model using the pairs of test and pseudo\nclean images. We further apply the learned pseudo ISP and rawRGB noise model to\nclean color images to synthesize realistic noisy images for denoiser adaption.\nPseudo-ISP is effective in synthesizing realistic noisy sRGB images, and\nimproved denoising performance can be achieved by alternating between\nPseudo-ISP training and denoiser adaption. Experiments show that our Pseudo-ISP\nnot only can boost simple Gaussian blurring-based denoiser to achieve\ncompetitive performance against CBDNet, but also is effective in improving\nstate-of-the-art deep denoisers, e.g., CBDNet and RIDNet.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 13:11:28 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Cao", "Yue", ""], ["Wu", "Xiaohe", ""], ["Qi", "Shuran", ""], ["Liu", "Xiao", ""], ["Wu", "Zhongqin", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2103.10255", "submitter": "Daniel Moyer", "authors": "Daniel Moyer, Esra Abaci Turk, P Ellen Grant, William M. Wells, and\n  Polina Golland", "title": "Equivariant Filters for Efficient Tracking in 3D Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an object tracking method for {3D} images with fixed\ncomputational cost and state-of-the-art performance. Previous methods predicted\ntransformation parameters from convolutional layers. We instead propose an\narchitecture that does not include either flattening of convolutional features\nor fully connected layers, but instead relies on equivariant filters to\npreserve transformations between inputs and outputs (e.g. rot./trans. of inputs\nrotate/translate outputs). The transformation is then derived in closed form\nfrom the outputs of the filters. This method is useful for applications\nrequiring low latency, such as real-time tracking. We demonstrate our model on\nsynthetically augmented adult brain MRI, as well as fetal brain MRI, which is\nthe intended use-case.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 13:47:27 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Moyer", "Daniel", ""], ["Turk", "Esra Abaci", ""], ["Grant", "P Ellen", ""], ["Wells", "William M.", ""], ["Golland", "Polina", ""]]}, {"id": "2103.10284", "submitter": "Yiming Cui", "authors": "Dongfang Liu, Yiming Cui, Wenbo Tan, Yingjie Chen", "title": "SG-Net: Spatial Granularity Network for One-Stage Video Instance\n  Segmentation", "comments": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video instance segmentation (VIS) is a new and critical task in computer\nvision. To date, top-performing VIS methods extend the two-stage Mask R-CNN by\nadding a tracking branch, leaving plenty of room for improvement. In contrast,\nwe approach the VIS task from a new perspective and propose a one-stage spatial\ngranularity network (SG-Net). Compared to the conventional two-stage methods,\nSG-Net demonstrates four advantages: 1) Our method has a one-stage compact\narchitecture and each task head (detection, segmentation, and tracking) is\ncrafted interdependently so they can effectively share features and enjoy the\njoint optimization; 2) Our mask prediction is dynamically performed on the\nsub-regions of each detected instance, leading to high-quality masks of fine\ngranularity; 3) Each of our task predictions avoids using expensive\nproposal-based RoI features, resulting in much reduced runtime complexity per\ninstance; 4) Our tracking head models objects centerness movements for\ntracking, which effectively enhances the tracking robustness to different\nobject appearances. In evaluation, we present state-of-the-art comparisons on\nthe YouTube-VIS dataset. Extensive experiments demonstrate that our compact\none-stage method can achieve improved performance in both accuracy and\ninference speed. We hope our SG-Net could serve as a strong and flexible\nbaseline for the VIS task. Our code will be available.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:31:15 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 17:54:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liu", "Dongfang", ""], ["Cui", "Yiming", ""], ["Tan", "Wenbo", ""], ["Chen", "Yingjie", ""]]}, {"id": "2103.10292", "submitter": "Veronika Cheplygina", "authors": "Ga\\\"el Varoquaux and Veronika Cheplygina", "title": "How I failed machine learning in medical imaging -- shortcomings and\n  recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Medical imaging is an important research field with many opportunities for\nimproving patients' health. However, there are a number of challenges that are\nslowing down the progress of the field as a whole, such optimizing for\npublication. In this paper we reviewed several problems related to choosing\ndatasets, methods, evaluation metrics, and publication strategies. With a\nreview of literature and our own analysis, we show that at every step,\npotential biases can creep in. On a positive note, we also see that initiatives\nto counteract these problems are already being started. Finally we provide a\nbroad range of recommendations on how to further these address problems in the\nfuture. For reproducibility, data and code for our analyses are available on\n\\url{https://github.com/GaelVaroquaux/ml_med_imaging_failures}\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:46:35 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Varoquaux", "Ga\u00ebl", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "2103.10308", "submitter": "Xiaojie Gao", "authors": "Xiaojie Gao, Yueming Jin, Zixu Zhao, Qi Dou, Pheng-Ann Heng", "title": "Future Frame Prediction for Robot-assisted Surgery", "comments": "IPMI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future frames for robotic surgical video is an interesting,\nimportant yet extremely challenging problem, given that the operative tasks may\nhave complex dynamics. Existing approaches on future prediction of natural\nvideos were based on either deterministic models or stochastic models,\nincluding deep recurrent neural networks, optical flow, and latent space\nmodeling. However, the potential in predicting meaningful movements of robots\nwith dual arms in surgical scenarios has not been tapped so far, which is\ntypically more challenging than forecasting independent motions of one arm\nrobots in natural scenarios. In this paper, we propose a ternary prior guided\nvariational autoencoder (TPG-VAE) model for future frame prediction in robotic\nsurgical video sequences. Besides content distribution, our model learns motion\ndistribution, which is novel to handle the small movements of surgical tools.\nFurthermore, we add the invariant prior information from the gesture class into\nthe generation process to constrain the latent space of our model. To our best\nknowledge, this is the first time that the future frames of dual arm robots are\npredicted considering their unique characteristics relative to general robotic\nvideos. Experiments demonstrate that our model gains more stable and realistic\nfuture frame prediction scenes with the suturing task on the public JIGSAWS\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:12:06 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Gao", "Xiaojie", ""], ["Jin", "Yueming", ""], ["Zhao", "Zixu", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2103.10310", "submitter": "Yubo Zhang", "authors": "Yubo Zhang, Shuxian Wang, Ruibin Ma, Sarah K. McGill, Julian G.\n  Rosenman, Stephen M. Pizer", "title": "Lighting Enhancement Aids Reconstruction of Colonoscopic Surfaces", "comments": "Accepted at IPMI 2021 (The 27th international conference on\n  Information Processing in Medical Imaging)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High screening coverage during colonoscopy is crucial to effectively prevent\ncolon cancer. Previous work has allowed alerting the doctor to unsurveyed\nregions by reconstructing the 3D colonoscopic surface from colonoscopy videos\nin real-time. However, the lighting inconsistency of colonoscopy videos can\ncause a key component of the colonoscopic reconstruction system, the SLAM\noptimization, to fail. In this work we focus on the lighting problem in\ncolonoscopy videos. To successfully improve the lighting consistency of\ncolonoscopy videos, we have found necessary a lighting correction that adapts\nto the intensity distribution of recent video frames. To achieve this in\nreal-time, we have designed and trained an RNN network. This network adapts the\ngamma value in a gamma-correction process. Applied in the colonoscopic surface\nreconstruction system, our light-weight model significantly boosts the\nreconstruction success rate, making a larger proportion of colonoscopy video\nsegments reconstructable and improving the reconstruction quality of the\nalready reconstructed segments.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:12:59 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhang", "Yubo", ""], ["Wang", "Shuxian", ""], ["Ma", "Ruibin", ""], ["McGill", "Sarah K.", ""], ["Rosenman", "Julian G.", ""], ["Pizer", "Stephen M.", ""]]}, {"id": "2103.10312", "submitter": "Isaac Gerg", "authors": "Isaac D. Gerg and Vishal Monga", "title": "Real-Time, Deep Synthetic Aperture Sonar (SAS) Autofocus", "comments": "Four pages. Accepted to IGARSS 2021. Fixed Eq 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Synthetic aperture sonar (SAS) requires precise time-of-flight measurements\nof the transmitted/received waveform to produce well-focused imagery. It is not\nuncommon for errors in these measurements to be present resulting in image\ndefocusing. To overcome this, an \\emph{autofocus} algorithm is employed as a\npost-processing step after image reconstruction to improve image focus. A\nparticular class of these algorithms can be framed as a sharpness/contrast\nmetric-based optimization. To improve convergence, a hand-crafted weighting\nfunction to remove \"bad\" areas of the image is sometimes applied to the\nimage-under-test before the optimization procedure. Additionally, dozens of\niterations are necessary for convergence which is a large compute burden for\nlow size, weight, and power (SWaP) systems. We propose a deep learning\ntechnique to overcome these limitations and implicitly learn the weighting\nfunction in a data-driven manner. Our proposed method, which we call Deep\nAutofocus, uses features from the single-look-complex (SLC) to estimate the\nphase correction which is applied in $k$-space. Furthermore, we train our\nalgorithm on batches of training imagery so that during deployment, only a\nsingle iteration of our method is sufficient to autofocus. We show results\ndemonstrating the robustness of our technique by comparing our results to four\ncommonly used image sharpness metrics. Our results demonstrate Deep Autofocus\ncan produce imagery perceptually better than common iterative techniques but at\na lower computational cost. We conclude that Deep Autofocus can provide a more\nfavorable cost-quality trade-off than alternatives with significant potential\nof future research.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:16:29 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 01:09:02 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 15:19:25 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gerg", "Isaac D.", ""], ["Monga", "Vishal", ""]]}, {"id": "2103.10339", "submitter": "Junhao Zhang", "authors": "Mingye Xu, Zhipeng Zhou, Junhao Zhang, Yu Qiao", "title": "Investigate Indistinguishable Points in Semantic Segmentation of 3D\n  Point Cloud", "comments": "AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the indistinguishable points (difficult to predict\nlabel) in semantic segmentation for large-scale 3D point clouds. The\nindistinguishable points consist of those located in complex boundary, points\nwith similar local textures but different categories, and points in isolate\nsmall hard areas, which largely harm the performance of 3D semantic\nsegmentation. To address this challenge, we propose a novel Indistinguishable\nArea Focalization Network (IAF-Net), which selects indistinguishable points\nadaptively by utilizing the hierarchical semantic features and enhances\nfine-grained features for points especially those indistinguishable points. We\nalso introduce multi-stage loss to improve the feature representation in a\nprogressive way. Moreover, in order to analyze the segmentation performances of\nindistinguishable areas, we propose a new evaluation metric called\nIndistinguishable Points Based Metric (IPBM). Our IAF-Net achieves the\ncomparable results with state-of-the-art performance on several popular 3D\npoint cloud datasets e.g. S3DIS and ScanNet, and clearly outperforms other\nmethods on IPBM.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:54:59 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Xu", "Mingye", ""], ["Zhou", "Zhipeng", ""], ["Zhang", "Junhao", ""], ["Qiao", "Yu", ""]]}, {"id": "2103.10350", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini, Mahmudul Hasan", "title": "The Case for High-Accuracy Classification: Think Small, Think Many!", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate implementation of high-accuracy deep neural networks especially\non resource-constrained devices, maintaining low computation requirements is\ncrucial. Using very deep models for classification purposes not only decreases\nthe neural network training speed and increases the inference time, but also\nneed more data for higher prediction accuracy and to mitigate false positives.\n  In this paper, we propose an efficient and lightweight deep classification\nensemble structure based on a combination of simple color features, which is\nparticularly designed for \"high-accuracy\" image classifications with low false\npositives. We designed, implemented, and evaluated our approach for explosion\ndetection use-case applied to images and videos. Our evaluation results based\non a large test test show considerable improvements on the prediction accuracy\ncompared to the popular ResNet-50 model, while benefiting from 7.64x faster\ninference and lower computation cost.\n  While we applied our approach to explosion detection, our approach is general\nand can be applied to other similar classification use cases as well. Given the\ninsight gained from our experiments, we hence propose a \"think small, think\nmany\" philosophy in classification scenarios: that transforming a single,\nlarge, monolithic deep model into a verification-based step model ensemble of\nmultiple small, simple, lightweight models with narrowed-down color spaces can\npossibly lead to predictions with higher accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:15:31 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hosseini", "Mohammad", ""], ["Hasan", "Mahmudul", ""]]}, {"id": "2103.10368", "submitter": "Pablo G\\'omez", "authors": "Pablo G\\'omez and Gabriele Meoni", "title": "MSMatch: Semi-Supervised Multispectral Scene Classification with Few\n  Labels", "comments": "6 pages, 4 figures, submitted to IEEE Transactions on Geoscience and\n  Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised learning techniques are at the center of many tasks in remote\nsensing. Unfortunately, these methods, especially recent deep learning methods,\noften require large amounts of labeled data for training. Even though\nsatellites acquire large amounts of data, labeling the data is often tedious,\nexpensive and requires expert knowledge. Hence, improved methods that require\nfewer labeled samples are needed. We present MSMatch, the first semi-supervised\nlearning approach competitive with supervised methods on scene classification\non the EuroSAT benchmark dataset. We test both RGB and multispectral images and\nperform various ablation studies to identify the critical parts of the model.\nThe trained neural network achieves state-of-the-art results on EuroSAT with an\naccuracy that is between 1.98% and 19.76% better than previous methods\ndepending on the number of labeled training examples. With just five labeled\nexamples per class we reach 94.53% and 95.86% accuracy on the EuroSAT RGB and\nmultispectral datasets, respectively. With 50 labels per class we reach 97.62%\nand 98.23% accuracy. Our results show that MSMatch is capable of greatly\nreducing the requirements for labeled data. It translates well to multispectral\ndata and should enable various applications that are currently infeasible due\nto a lack of labeled data. We provide the source code of MSMatch online to\nenable easy reproduction and quick adoption.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:47:21 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["G\u00f3mez", "Pablo", ""], ["Meoni", "Gabriele", ""]]}, {"id": "2103.10374", "submitter": "Chen Chen", "authors": "Weiping Yu, Sijie Zhu, Taojiannan Yang, Chen Chen, Mengyuan Liu", "title": "Consistency-based Active Learning for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to improve the performance of task model by selecting\nthe most informative samples with a limited budget. Unlike most recent works\nthat focused on applying active learning for image classification, we propose\nan effective Consistency-based Active Learning method for object Detection\n(CALD), which fully explores the consistency between original and augmented\ndata. CALD has three appealing benefits. (i) CALD is systematically designed by\ninvestigating the weaknesses of existing active learning methods, which do not\ntake the unique challenges of object detection into account. (ii) CALD unifies\nbox regression and classification with a single metric, which is not concerned\nby active learning methods for classification. CALD also focuses on the most\ninformative local region rather than the whole image, which is beneficial for\nobject detection. (iii) CALD not only gauges individual information for sample\nselection, but also leverages mutual information to encourage a balanced data\ndistribution. Extensive experiments show that CALD significantly outperforms\nexisting state-of-the-art task-agnostic and detection-specific active learning\nmethods on general object detection datasets. Based on the Faster R-CNN\ndetector, CALD consistently surpasses the baseline method (random selection) by\n2.9/2.8/0.8 mAP on average on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO.\nCode is available at \\url{https://github.com/we1pingyu/CALD}\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:00:34 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 15:53:32 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Yu", "Weiping", ""], ["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""], ["Liu", "Mengyuan", ""]]}, {"id": "2103.10380", "submitter": "Marek Kowalski", "authors": "Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton,\n  Julien Valentin", "title": "FastNeRF: High-Fidelity Neural Rendering at 200FPS", "comments": "main paper: 10 pages, 6 figures; supplementary: 10 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on Neural Radiance Fields (NeRF) showed how neural networks can\nbe used to encode complex 3D environments that can be rendered\nphotorealistically from novel viewpoints. Rendering these images is very\ncomputationally demanding and recent improvements are still a long way from\nenabling interactive rates, even on high-end hardware. Motivated by scenarios\non mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based\nsystem capable of rendering high fidelity photorealistic images at 200Hz on a\nhigh-end consumer GPU. The core of our method is a graphics-inspired\nfactorization that allows for (i) compactly caching a deep radiance map at each\nposition in space, (ii) efficiently querying that map using ray directions to\nestimate the pixel values in the rendered image. Extensive experiments show\nthat the proposed method is 3000 times faster than the original NeRF algorithm\nand at least an order of magnitude faster than existing work on accelerating\nNeRF, while maintaining visual quality and extensibility.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:09:12 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 11:01:16 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Garbin", "Stephan J.", ""], ["Kowalski", "Marek", ""], ["Johnson", "Matthew", ""], ["Shotton", "Jamie", ""], ["Valentin", "Julien", ""]]}, {"id": "2103.10390", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "Challenges of 3D Surface Reconstruction in Capsule Endoscopy", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are currently many challenges related to three-dimensional (3D) surface\nreconstruction using capsule endoscopy (CE) images. There are also challenges\nrelated to viewing the content of reconstructed 3D surfaces. In this\npreliminary investigation, the author focuses on the latter and evaluates their\neffects on the content of reconstructed 3D surfaces using CE images. The\nevaluation of such challenges is preliminarily conducted into two parts. The\nfirst part focuses on the comparison of the content of 3D surfaces\nreconstructed using both preprocessed and non-preprocessed CE images. The\nsecond part focuses on the comparison of the content of 3D surfaces viewed at\nthe same azimuth angles and different elevation angles of the line-of-sight.\nThe experiments demonstrated the need for generalizable line-of-sight and\nadvanced CE image preprocessing means as well as further research in 3D surface\nreconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:18:48 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "2103.10391", "submitter": "Jia Zheng", "authors": "Zhaoyuan Yin, Jia Zheng, Weixin Luo, Shenhan Qian, Hanling Zhang,\n  Shenghua Gao", "title": "Learning to Recommend Frame for Interactive Video Object Segmentation in\n  the Wild", "comments": "To appear in CVPR 2021. Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a framework for the interactive video object segmentation\n(VOS) in the wild where users can choose some frames for annotations\niteratively. Then, based on the user annotations, a segmentation algorithm\nrefines the masks. The previous interactive VOS paradigm selects the frame with\nsome worst evaluation metric, and the ground truth is required for calculating\nthe evaluation metric, which is impractical in the testing phase. In contrast,\nin this paper, we advocate that the frame with the worst evaluation metric may\nnot be exactly the most valuable frame that leads to the most performance\nimprovement across the video. Thus, we formulate the frame selection problem in\nthe interactive VOS as a Markov Decision Process, where an agent is learned to\nrecommend the frame under a deep reinforcement learning framework. The learned\nagent can automatically determine the most valuable frame, making the\ninteractive setting more practical in the wild. Experimental results on the\npublic datasets show the effectiveness of our learned agent without any changes\nto the underlying VOS algorithms. Our data, code, and models are available at\nhttps://github.com/svip-lab/IVOS-W.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:19:47 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 02:45:20 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Yin", "Zhaoyuan", ""], ["Zheng", "Jia", ""], ["Luo", "Weixin", ""], ["Qian", "Shenhan", ""], ["Zhang", "Hanling", ""], ["Gao", "Shenghua", ""]]}, {"id": "2103.10400", "submitter": "Karnik Ram", "authors": "Karnik Ram, Chaitanya Kharyal, Sudarshan S. Harithas, K. Madhava\n  Krishna", "title": "RP-VIO: Robust Plane-based Visual-Inertial Odometry for Dynamic\n  Environments", "comments": "Submitted to IROS 21, code and dataset available at\n  https://github.com/karnikram/rp-vio", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern visual-inertial navigation systems (VINS) are faced with a critical\nchallenge in real-world deployment: they need to operate reliably and robustly\nin highly dynamic environments. Current best solutions merely filter dynamic\nobjects as outliers based on the semantics of the object category. Such an\napproach does not scale as it requires semantic classifiers to encompass all\npossibly-moving object classes; this is hard to define, let alone deploy. On\nthe other hand, many real-world environments exhibit strong structural\nregularities in the form of planes such as walls and ground surfaces, which are\nalso crucially static. We present RP-VIO, a monocular visual-inertial odometry\nsystem that leverages the simple geometry of these planes for improved\nrobustness and accuracy in challenging dynamic environments. Since existing\ndatasets have a limited number of dynamic elements, we also present a\nhighly-dynamic, photorealistic synthetic dataset for a more effective\nevaluation of the capabilities of modern VINS systems. We evaluate our approach\non this dataset, and three diverse sequences from standard datasets including\ntwo real-world dynamic sequences and show a significant improvement in\nrobustness and accuracy over a state-of-the-art monocular visual-inertial\nodometry system. We also show in simulation an improvement over a simple\ndynamic-features masking approach. Our code and dataset are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:33:07 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ram", "Karnik", ""], ["Kharyal", "Chaitanya", ""], ["Harithas", "Sudarshan S.", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2103.10419", "submitter": "Ahmed Alkhateeb", "authors": "Muhammad Alrabeiah, Umut Demirhan, Andrew Hredzak, and Ahmed Alkhateeb", "title": "Computer Vision Aided URLL Communications: Proactive Service\n  Identification and Coexistence", "comments": "The datasets and code files will be available soon on the ViWi\n  Dataset website https://viwi-dataset.net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The support of coexisting ultra-reliable and low-latency (URLL) and enhanced\nMobile BroadBand (eMBB) services is a key challenge for the current and future\nwireless communication networks. Those two types of services introduce strict,\nand in some time conflicting, resource allocation requirements that may result\nin a power-struggle between reliability, latency, and resource utilization in\nwireless networks. The difficulty in addressing that challenge could be traced\nback to the predominant reactive approach in allocating the wireless resources.\nThis allocation operation is carried out based on received service requests and\nglobal network statistics, which may not incorporate a sense of\n\\textit{proaction}. Therefore, this paper proposes a novel framework termed\n\\textit{service identification} to develop novel proactive resource allocation\nalgorithms. The developed framework is based on visual data (captured for\nexample by RGB cameras) and deep learning (e.g., deep neural networks). The\nultimate objective of this framework is to equip future wireless networks with\nthe ability to analyze user behavior, anticipate incoming services, and perform\nproactive resource allocation. To demonstrate the potential of the proposed\nframework, a wireless network scenario with two coexisting URLL and eMBB\nservices is considered, and two deep learning algorithms are designed to\nutilize RGB video frames and predict incoming service type and its request\ntime. An evaluation dataset based on the considered scenario is developed and\nused to evaluate the performance of the two algorithms. The results confirm the\nanticipated value of proaction to wireless networks; the proposed models enable\nefficient network performance ensuring more than $85\\%$ utilization of the\nnetwork resources at $\\sim 98\\%$ reliability. This highlights a promising\ndirection for the future vision-aided wireless communication networks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:53:29 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Alrabeiah", "Muhammad", ""], ["Demirhan", "Umut", ""], ["Hredzak", "Andrew", ""], ["Alkhateeb", "Ahmed", ""]]}, {"id": "2103.10426", "submitter": "Lucy Chai", "authors": "Lucy Chai, Jonas Wulff, Phillip Isola", "title": "Using latent space regression to analyze and leverage compositionality\n  in GANs", "comments": "Update to ICLR 2021 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, Generative Adversarial Networks have become ubiquitous in\nboth research and public perception, but how GANs convert an unstructured\nlatent code to a high quality output is still an open question. In this work,\nwe investigate regression into the latent space as a probe to understand the\ncompositional properties of GANs. We find that combining the regressor and a\npretrained generator provides a strong image prior, allowing us to create\ncomposite images from a collage of random image parts at inference time while\nmaintaining global consistency. To compare compositional properties across\ndifferent generators, we measure the trade-offs between reconstruction of the\nunrealistic input and image quality of the regenerated samples. We find that\nthe regression approach enables more localized editing of individual image\nparts compared to direct editing in the latent space, and we conduct\nexperiments to quantify this independence effect. Our method is agnostic to the\nsemantics of edits, and does not require labels or predefined concepts during\ntraining. Beyond image composition, our method extends to a number of related\napplications, such as image inpainting or example-based image editing, which we\ndemonstrate on several GANs and datasets, and because it uses only a single\nforward pass, it can operate in real-time. Code is available on our project\npage: https://chail.github.io/latent-composition/.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:58:01 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 19:52:33 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chai", "Lucy", ""], ["Wulff", "Jonas", ""], ["Isola", "Phillip", ""]]}, {"id": "2103.10427", "submitter": "Minyoung Huh", "authors": "Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit\n  Agrawal, Phillip Isola", "title": "The Low-Rank Simplicity Bias in Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern deep neural networks are highly over-parameterized compared to the\ndata on which they are trained, yet they often generalize remarkably well. A\nflurry of recent work has asked: why do deep networks not overfit to their\ntraining data? We investigate the hypothesis that deeper nets are implicitly\nbiased to find lower rank solutions and that these are the solutions that\ngeneralize well. We prove for the asymptotic case that the percent volume of\nlow effective-rank solutions increases monotonically as linear neural networks\nare made deeper. We then show empirically that our claim holds true on finite\nwidth models. We further empirically find that a similar result holds for\nnon-linear networks: deeper non-linear networks learn a feature space whose\nkernel has a lower rank. We further demonstrate how linear\nover-parameterization of deep non-linear models can be used to induce low-rank\nbias, improving generalization performance without changing the effective model\ncapacity. We evaluate on various model architectures and demonstrate that\nlinearly over-parameterized models outperform existing baselines on image\nclassification tasks, including ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:58:02 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Huh", "Minyoung", ""], ["Mobahi", "Hossein", ""], ["Zhang", "Richard", ""], ["Cheung", "Brian", ""], ["Agrawal", "Pulkit", ""], ["Isola", "Phillip", ""]]}, {"id": "2103.10428", "submitter": "Shengyu Zhao", "authors": "Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I\n  Chang, Yan Xu", "title": "Large Scale Image Completion via Co-Modulated Generative Adversarial\n  Networks", "comments": "ICLR 2021 (Spotlight). Code: https://github.com/zsyzzsoft/co-mod-gan\n  Demo: https://comodgan.ml/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous task-specific variants of conditional generative adversarial\nnetworks have been developed for image completion. Yet, a serious limitation\nremains that all existing algorithms tend to fail when handling large-scale\nmissing regions. To overcome this challenge, we propose a generic new approach\nthat bridges the gap between image-conditional and recent modulated\nunconditional generative architectures via co-modulation of both conditional\nand stochastic style representations. Also, due to the lack of good\nquantitative metrics for image completion, we propose the new Paired/Unpaired\nInception Discriminative Score (P-IDS/U-IDS), which robustly measures the\nperceptual fidelity of inpainted images compared to real images via linear\nseparability in a feature space. Experiments demonstrate superior performance\nin terms of both quality and diversity over state-of-the-art methods in\nfree-form image completion and easy generalization to image-to-image\ntranslation. Code is available at https://github.com/zsyzzsoft/co-mod-gan.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:59:11 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhao", "Shengyu", ""], ["Cui", "Jonathan", ""], ["Sheng", "Yilun", ""], ["Dong", "Yue", ""], ["Liang", "Xiao", ""], ["Chang", "Eric I", ""], ["Xu", "Yan", ""]]}, {"id": "2103.10429", "submitter": "Despoina Paschalidou", "authors": "Despoina Paschalidou and Angelos Katharopoulos and Andreas Geiger and\n  Sanja Fidler", "title": "Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible\n  Neural Networks", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impressive progress in 3D shape extraction led to representations that can\ncapture object geometries with high fidelity. In parallel, primitive-based\nmethods seek to represent objects as semantically consistent part arrangements.\nHowever, due to the simplicity of existing primitive representations, these\nmethods fail to accurately reconstruct 3D shapes using a small number of\nprimitives/parts. We address the trade-off between reconstruction quality and\nnumber of parts with Neural Parts, a novel 3D primitive representation that\ndefines primitives using an Invertible Neural Network (INN) which implements\nhomeomorphic mappings between a sphere and the target object. The INN allows us\nto compute the inverse mapping of the homeomorphism, which in turn, enables the\nefficient computation of both the implicit surface function of a primitive and\nits mesh, without any additional post-processing. Our model learns to parse 3D\nobjects into semantically consistent part arrangements without any part-level\nsupervision. Evaluations on ShapeNet, D-FAUST and FreiHAND demonstrate that our\nprimitives can capture complex geometries and thus simultaneously achieve\ngeometrically accurate as well as interpretable reconstructions using an order\nof magnitude fewer primitives than state-of-the-art shape abstraction methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:59:31 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Paschalidou", "Despoina", ""], ["Katharopoulos", "Angelos", ""], ["Geiger", "Andreas", ""], ["Fidler", "Sanja", ""]]}, {"id": "2103.10434", "submitter": "Hendrik Hachmann", "authors": "Hendrik Hachmann, Benjamin Kr\\\"uger, Bodo Rosenhahn and Waldo Nogueira", "title": "Localization of Cochlear Implant Electrodes from Cone Beam Computed\n  Tomography using Particle Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cochlear implants (CIs) are implantable medical devices that can restore the\nhearing sense of people suffering from profound hearing loss. The CI uses a set\nof electrode contacts placed inside the cochlea to stimulate the auditory nerve\nwith current pulses. The exact location of these electrodes may be an important\nparameter to improve and predict the performance with these devices. Currently\nthe methods used in clinics to characterize the geometry of the cochlea as well\nas to estimate the electrode positions are manual, error-prone and time\nconsuming. We propose a Markov random field (MRF) model for CI electrode\nlocalization for cone beam computed tomography (CBCT) data-sets. Intensity and\nshape of electrodes are included as prior knowledge as well as distance and\nangles between contacts. MRF inference is based on slice sampling particle\nbelief propagation and guided by several heuristics. A stochastic search finds\nthe best maximum a posteriori estimation among sampled MRF realizations. We\nevaluate our algorithm on synthetic and real CBCT data-sets and compare its\nperformance with two state of the art algorithms. An increase of localization\nprecision up to 31.5% (mean), or 48.6% (median) respectively, on real CBCT\ndata-sets is shown.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:39:23 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Hachmann", "Hendrik", ""], ["Kr\u00fcger", "Benjamin", ""], ["Rosenhahn", "Bodo", ""], ["Nogueira", "Waldo", ""]]}, {"id": "2103.10451", "submitter": "Dominik D\\\"urrschnabel", "authors": "Lena Stubbemann, Dominik D\\\"urrschnabel, Robert Refflinghaus", "title": "Neural Networks for Semantic Gaze Analysis in XR Settings", "comments": "16 pages, 6 figures, 1 table, Accepted to: ETRA2021, ACM Symposium on\n  Eye Tracking Research and Applications", "journal-ref": null, "doi": "10.1145/3448017.3457380", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual-reality (VR) and augmented-reality (AR) technology is increasingly\ncombined with eye-tracking. This combination broadens both fields and opens up\nnew areas of application, in which visual perception and related cognitive\nprocesses can be studied in interactive but still well controlled settings.\nHowever, performing a semantic gaze analysis of eye-tracking data from\ninteractive three-dimensional scenes is a resource-intense task, which so far\nhas been an obstacle to economic use. In this paper we present a novel approach\nwhich minimizes time and information necessary to annotate volumes of interest\n(VOIs) by using techniques from object recognition. To do so, we train\nconvolutional neural networks (CNNs) on synthetic data sets derived from\nvirtual models using image augmentation techniques. We evaluate our method in\nreal and virtual environments, showing that the method can compete with\nstate-of-the-art approaches, while not relying on additional markers or\npreexisting databases but instead offering cross-platform use.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:05:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Stubbemann", "Lena", ""], ["D\u00fcrrschnabel", "Dominik", ""], ["Refflinghaus", "Robert", ""]]}, {"id": "2103.10455", "submitter": "Chen Chen", "authors": "Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen,\n  Zhengming Ding", "title": "3D Human Pose Estimation with Spatial and Temporal Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer architectures have become the model of choice in natural language\nprocessing and are now being introduced into computer vision tasks such as\nimage classification, object detection, and semantic segmentation. However, in\nthe field of human pose estimation, convolutional architectures still remain\ndominant. In this work, we present PoseFormer, a purely transformer-based\napproach for 3D human pose estimation in videos without convolutional\narchitectures involved. Inspired by recent developments in vision transformers,\nwe design a spatial-temporal transformer structure to comprehensively model the\nhuman joint relations within each frame as well as the temporal correlations\nacross frames, then output an accurate 3D human pose of the center frame. We\nquantitatively and qualitatively evaluate our method on two popular and\nstandard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments\nshow that PoseFormer achieves state-of-the-art performance on both datasets.\nCode is available at \\url{https://github.com/zczcwh/PoseFormer}\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:14:37 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 16:54:14 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zheng", "Ce", ""], ["Zhu", "Sijie", ""], ["Mendieta", "Matias", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""], ["Ding", "Zhengming", ""]]}, {"id": "2103.10480", "submitter": "David Noever", "authors": "David A. Noever, Samantha E. Miller Noever", "title": "Reading Isn't Believing: Adversarial Attacks On Multi-Modal Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With Open AI's publishing of their CLIP model (Contrastive Language-Image\nPre-training), multi-modal neural networks now provide accessible models that\ncombine reading with visual recognition. Their network offers novel ways to\nprobe its dual abilities to read text while classifying visual objects. This\npaper demonstrates several new categories of adversarial attacks, spanning\nbasic typographical, conceptual, and iconographic inputs generated to fool the\nmodel into making false or absurd classifications. We demonstrate that\ncontradictory text and image signals can confuse the model into choosing false\n(visual) options. Like previous authors, we show by example that the CLIP model\ntends to read first, look later, a phenomenon we describe as reading isn't\nbelieving.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:56:51 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Noever", "David A.", ""], ["Noever", "Samantha E. Miller", ""]]}, {"id": "2103.10484", "submitter": "James Fox", "authors": "James Fox, Bo Zhao, Sivasankaran Rajamanickam, Rampi Ramprasad, Le\n  Song", "title": "Concentric Spherical GNN for 3D Representation Learning", "comments": "This paper has been submitted for conference review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning 3D representations that generalize well to arbitrarily oriented\ninputs is a challenge of practical importance in applications varying from\ncomputer vision to physics and chemistry. We propose a novel multi-resolution\nconvolutional architecture for learning over concentric spherical feature maps,\nof which the single sphere representation is a special case. Our hierarchical\narchitecture is based on alternatively learning to incorporate both\nintra-sphere and inter-sphere information. We show the applicability of our\nmethod for two different types of 3D inputs, mesh objects, which can be\nregularly sampled, and point clouds, which are irregularly distributed. We also\npropose an efficient mapping of point clouds to concentric spherical images,\nthereby bridging spherical convolutions on grids with general point clouds. We\ndemonstrate the effectiveness of our approach in improving state-of-the-art\nperformance on 3D classification tasks with rotated data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 19:05:04 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Fox", "James", ""], ["Zhao", "Bo", ""], ["Rajamanickam", "Sivasankaran", ""], ["Ramprasad", "Rampi", ""], ["Song", "Le", ""]]}, {"id": "2103.10492", "submitter": "Jakaria Rabbi", "authors": "Md. Tahmid Hasan Fuad, Awal Ahmed Fime, Delowar Sikder, Md. Akil\n  Raihan Iftee, Jakaria Rabbi, Mabrook S. Al-rakhami, Abdu Gumae, Ovishake Sen,\n  Mohtasim Fuad, and Md. Nazrul Islam", "title": "Recent Advances in Deep Learning Techniques for Face Recognition", "comments": "32 pages and citation: M. T. H. Fuad et al., \"Recent Advances in Deep\n  Learning Techniques for Face Recognition,\" in IEEE Access, vol. 9, pp.\n  99112-99142, 2021, doi: 10.1109/ACCESS.2021.3096136", "journal-ref": "in IEEE Access, vol. 9, pp. 99112-99142, 2021", "doi": "10.1109/ACCESS.2021.3096136", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, researchers have proposed many deep learning (DL) methods\nfor various tasks, and particularly face recognition (FR) made an enormous leap\nusing these techniques. Deep FR systems benefit from the hierarchical\narchitecture of the DL methods to learn discriminative face representation.\nTherefore, DL techniques significantly improve state-of-the-art performance on\nFR systems and encourage diverse and efficient real-world applications. In this\npaper, we present a comprehensive analysis of various FR systems that leverage\nthe different types of DL techniques, and for the study, we summarize 168\nrecent contributions from this area. We discuss the papers related to different\nalgorithms, architectures, loss functions, activation functions, datasets,\nchallenges, improvement ideas, current and future trends of DL-based FR\nsystems. We provide a detailed discussion of various DL methods to understand\nthe current state-of-the-art, and then we discuss various activation and loss\nfunctions for the methods. Additionally, we summarize different datasets used\nwidely for FR tasks and discuss challenges related to illumination, expression,\npose variations, and occlusion. Finally, we discuss improvement ideas, current\nand future trends of FR tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 19:39:12 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 16:31:53 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Fuad", "Md. Tahmid Hasan", ""], ["Fime", "Awal Ahmed", ""], ["Sikder", "Delowar", ""], ["Iftee", "Md. Akil Raihan", ""], ["Rabbi", "Jakaria", ""], ["Al-rakhami", "Mabrook S.", ""], ["Gumae", "Abdu", ""], ["Sen", "Ovishake", ""], ["Fuad", "Mohtasim", ""], ["Islam", "Md. Nazrul", ""]]}, {"id": "2103.10493", "submitter": "Arjun Krishna", "authors": "Arjun Krishna, Kedar Bartake, Chuang Niu, Ge Wang, Youfang Lai, Xun\n  Jia, Klaus Mueller", "title": "Image Synthesis for Data Augmentation in Medical CT using Deep\n  Reinforcement Learning", "comments": "Fully3D 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has shown great promise for CT image reconstruction, in\nparticular to enable low dose imaging and integrated diagnostics. These merits,\nhowever, stand at great odds with the low availability of diverse image data\nwhich are needed to train these neural networks. We propose to overcome this\nbottleneck via a deep reinforcement learning (DRL) approach that is integrated\nwith a style-transfer (ST) methodology, where the DRL generates the anatomical\nshapes and the ST synthesizes the texture detail. We show that our method bears\nhigh promise for generating novel and anatomically accurate high resolution CT\nimages at large and diverse quantities. Our approach is specifically designed\nto work with even small image datasets which is desirable given the often low\namount of image data many researchers have available to them.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 19:47:11 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 01:00:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Krishna", "Arjun", ""], ["Bartake", "Kedar", ""], ["Niu", "Chuang", ""], ["Wang", "Ge", ""], ["Lai", "Youfang", ""], ["Jia", "Xun", ""], ["Mueller", "Klaus", ""]]}, {"id": "2103.10502", "submitter": "Mohammadreza Salehi Dehnavi", "authors": "Masoud Pourreza, Mohammadreza Salehi, Mohammad Sabokrou", "title": "Ano-Graph: Learning Normal Scene Contextual Graphs to Detect Video\n  Anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anomaly detection has proved to be a challenging task owing to its\nunsupervised training procedure and high spatio-temporal complexity existing in\nreal-world scenarios. In the absence of anomalous training samples,\nstate-of-the-art methods try to extract features that fully grasp normal\nbehaviors in both space and time domains using different approaches such as\nautoencoders, or generative adversarial networks. However, these approaches\ncompletely ignore or, by using the ability of deep networks in the hierarchical\nmodeling, poorly model the spatio-temporal interactions that exist between\nobjects. To address this issue, we propose a novel yet efficient method named\nAno-Graph for learning and modeling the interaction of normal objects. Towards\nthis end, a Spatio-Temporal Graph (STG) is made by considering each node as an\nobject's feature extracted from a real-time off-the-shelf object detector, and\nedges are made based on their interactions. After that, a self-supervised\nlearning method is employed on the STG in such a way that encapsulates\ninteractions in a semantic space. Our method is data-efficient, significantly\nmore robust against common real-world variations such as illumination, and\npasses SOTA by a large margin on the challenging datasets ADOC and Street Scene\nwhile stays competitive on Avenue, ShanghaiTech, and UCSD.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:08:53 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 12:55:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Pourreza", "Masoud", ""], ["Salehi", "Mohammadreza", ""], ["Sabokrou", "Mohammad", ""]]}, {"id": "2103.10504", "submitter": "Ali Hatamizadeh", "authors": "Ali Hatamizadeh, Dong Yang, Holger Roth and Daguang Xu", "title": "UNETR: Transformers for 3D Medical Image Segmentation", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fully Convolutional Neural Networks (FCNNs) with contracting and expansive\npaths (e.g. encoder and decoder) have shown prominence in various medical image\nsegmentation applications during the recent years. In these architectures, the\nencoder plays an integral role by learning global contextual representations\nwhich will be further utilized for semantic output prediction by the decoder.\nDespite their success, the locality of convolutional layers , as the main\nbuilding block of FCNNs limits the capability of learning long-range spatial\ndependencies in such networks. Inspired by the recent success of transformers\nin Natural Language Processing (NLP) in long-range sequence learning, we\nreformulate the task of volumetric (3D) medical image segmentation as a\nsequence-to-sequence prediction problem. In particular, we introduce a novel\narchitecture, dubbed as UNEt TRansformers (UNETR), that utilizes a pure\ntransformer as the encoder to learn sequence representations of the input\nvolume and effectively capture the global multi-scale information. The\ntransformer encoder is directly connected to a decoder via skip connections at\ndifferent resolutions to compute the final semantic segmentation output. We\nhave extensively validated the performance of our proposed model across\ndifferent imaging modalities(i.e. MR and CT) on volumetric brain tumour and\nspleen segmentation tasks using the Medical Segmentation Decathlon (MSD)\ndataset, and our results consistently demonstrate favorable benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:17:15 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Hatamizadeh", "Ali", ""], ["Yang", "Dong", ""], ["Roth", "Holger", ""], ["Xu", "Daguang", ""]]}, {"id": "2103.10559", "submitter": "Tianyu Ding", "authors": "Tianyu Ding, Luming Liang, Zhihui Zhu, Ilya Zharkov", "title": "CDFI: Compression-Driven Network Design for Frame Interpolation", "comments": "To appear in the proceedings of 2021 IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN-based frame interpolation--that generates the intermediate frames given\ntwo consecutive frames--typically relies on heavy model architectures with a\nhuge number of features, preventing them from being deployed on systems with\nlimited resources, e.g., mobile devices. We propose a compression-driven\nnetwork design for frame interpolation (CDFI), that leverages model pruning\nthrough sparsity-inducing optimization to significantly reduce the model size\nwhile achieving superior performance. Concretely, we first compress the\nrecently proposed AdaCoF model and show that a 10X compressed AdaCoF performs\nsimilarly as its original counterpart; then we further improve this compressed\nmodel by introducing a multi-resolution warping module, which boosts visual\nconsistencies with multi-level details. As a consequence, we achieve a\nsignificant performance gain with only a quarter in size compared with the\noriginal AdaCoF. Moreover, our model performs favorably against other\nstate-of-the-arts in a broad range of datasets. Finally, the proposed\ncompression-driven framework is generic and can be easily transferred to other\nDNN-based frame interpolation algorithm. Our source code is available at\nhttps://github.com/tding1/CDFI.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 22:59:42 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 02:52:52 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ding", "Tianyu", ""], ["Liang", "Luming", ""], ["Zhu", "Zhihui", ""], ["Zharkov", "Ilya", ""]]}, {"id": "2103.10567", "submitter": "Yang Bo", "authors": "Yang Bo, Yangdi Lu and Wenbo He", "title": "CLTA: Contents and Length-based Temporal Attention for Few-shot Action\n  Recognition", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot action recognition has attracted increasing attention due to the\ndifficulty in acquiring the properly labelled training samples. Current works\nhave shown that preserving spatial information and comparing video descriptors\nare crucial for few-shot action recognition. However, the importance of\npreserving temporal information is not well discussed. In this paper, we\npropose a Contents and Length-based Temporal Attention (CLTA) model, which\nlearns customized temporal attention for the individual video to tackle the\nfew-shot action recognition problem. CLTA utilizes the Gaussian likelihood\nfunction as the template to generate temporal attention and trains the learning\nmatrices to study the mean and standard deviation based on both frame contents\nand length. We show that even a not fine-tuned backbone with an ordinary\nsoftmax classifier can still achieve similar or better results compared to the\nstate-of-the-art few-shot action recognition with precisely captured temporal\nattention.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 23:40:28 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Bo", "Yang", ""], ["Lu", "Yangdi", ""], ["He", "Wenbo", ""]]}, {"id": "2103.10571", "submitter": "Chunhua Shen", "authors": "Yifan Liu, Hao Chen, Yu Chen, Wei Yin, Chunhua Shen", "title": "Generic Perceptual Loss for Modeling Structured Output Dependencies", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The perceptual loss has been widely used as an effective loss term in image\nsynthesis tasks including image super-resolution, and style transfer. It was\nbelieved that the success lies in the high-level perceptual feature\nrepresentations extracted from CNNs pretrained with a large set of images. Here\nwe reveal that, what matters is the network structure instead of the trained\nweights. Without any learning, the structure of a deep network is sufficient to\ncapture the dependencies between multiple levels of variable statistics using\nmultiple layers of CNNs. This insight removes the requirements of pre-training\nand a particular network structure (commonly, VGG) that are previously assumed\nfor the perceptual loss, thus enabling a significantly wider range of\napplications. To this end, we demonstrate that a randomly-weighted deep CNN can\nbe used to model the structured dependencies of outputs. On a few dense\nper-pixel prediction tasks such as semantic segmentation, depth estimation and\ninstance segmentation, we show improved results of using the extended\nrandomized perceptual loss, compared to the baselines using pixel-wise loss\nalone. We hope that this simple, extended perceptual loss may serve as a\ngeneric structured-output loss that is applicable to most structured output\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 23:56:07 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Yifan", ""], ["Chen", "Hao", ""], ["Chen", "Yu", ""], ["Yin", "Wei", ""], ["Shen", "Chunhua", ""]]}, {"id": "2103.10574", "submitter": "Honglu Zhou", "authors": "Honglu Zhou, Asim Kadav, Farley Lai, Alexandru Niculescu-Mizil, Martin\n  Renqiang Min, Mubbasir Kapadia, Hans Peter Graf", "title": "Hopper: Multi-hop Transformer for Spatiotemporal Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of spatiotemporal object-centric reasoning\nin videos. Central to our approach is the notion of object permanence, i.e.,\nthe ability to reason about the location of objects as they move through the\nvideo while being occluded, contained or carried by other objects. Existing\ndeep learning based approaches often suffer from spatiotemporal biases when\napplied to video reasoning problems. We propose Hopper, which uses a Multi-hop\nTransformer for reasoning object permanence in videos. Given a video and a\nlocalization query, Hopper reasons over image and object tracks to\nautomatically hop over critical frames in an iterative fashion to predict the\nfinal position of the object of interest. We demonstrate the effectiveness of\nusing a contrastive loss to reduce spatiotemporal biases. We evaluate over\nCATER dataset and find that Hopper achieves 73.2% Top-1 accuracy using just 1\nFPS by hopping through just a few critical frames. We also demonstrate Hopper\ncan perform long-term reasoning by building a CATER-h dataset that requires\nmulti-step reasoning to localize objects of interest correctly.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 00:13:04 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 02:00:23 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Honglu", ""], ["Kadav", "Asim", ""], ["Lai", "Farley", ""], ["Niculescu-Mizil", "Alexandru", ""], ["Min", "Martin Renqiang", ""], ["Kapadia", "Mubbasir", ""], ["Graf", "Hans Peter", ""]]}, {"id": "2103.10583", "submitter": "Yunsheng Li", "authors": "Yunsheng Li, Lu Yuan, Yinpeng Chen, Pei Wang, Nuno Vasconcelos", "title": "Dynamic Transfer for Multi-Source Domain Adaptation", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works of multi-source domain adaptation focus on learning a\ndomain-agnostic model, of which the parameters are static. However, such a\nstatic model is difficult to handle conflicts across multiple domains, and\nsuffers from a performance degradation in both source domains and target\ndomain. In this paper, we present dynamic transfer to address domain conflicts,\nwhere the model parameters are adapted to samples. The key insight is that\nadapting model across domains is achieved via adapting model across samples.\nThus, it breaks down source domain barriers and turns multi-source domains into\na single-source domain. This also simplifies the alignment between source and\ntarget domains, as it only requires the target domain to be aligned with any\npart of the union of source domains. Furthermore, we find dynamic transfer can\nbe simply modeled by aggregating residual matrices and a static convolution\nmatrix. Experimental results show that, without using domain labels, our\ndynamic transfer outperforms the state-of-the-art method by more than 3% on the\nlarge multi-source domain adaptation datasets -- DomainNet. Source code is at\nhttps://github.com/liyunsheng13/DRT.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 01:22:12 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Yunsheng", ""], ["Yuan", "Lu", ""], ["Chen", "Yinpeng", ""], ["Wang", "Pei", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2103.10592", "submitter": "Chankyu Lee", "authors": "Chankyu Lee, Adarsh Kumar Kosta and Kaushik Roy", "title": "Fusion-FlowNet: Energy-Efficient Optical Flow Estimation using Sensor\n  Fusion and Deep Fused Spiking-Analog Network Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard frame-based cameras that sample light intensity frames are heavily\nimpacted by motion blur for high-speed motion and fail to perceive scene\naccurately when the dynamic range is high. Event-based cameras, on the other\nhand, overcome these limitations by asynchronously detecting the variation in\nindividual pixel intensities. However, event cameras only provide information\nabout pixels in motion, leading to sparse data. Hence, estimating the overall\ndense behavior of pixels is difficult. To address such issues associated with\nthe sensors, we present Fusion-FlowNet, a sensor fusion framework for\nenergy-efficient optical flow estimation using both frame- and event-based\nsensors, leveraging their complementary characteristics. Our proposed network\narchitecture is also a fusion of Spiking Neural Networks (SNNs) and Analog\nNeural Networks (ANNs) where each network is designed to simultaneously process\nasynchronous event streams and regular frame-based images, respectively. Our\nnetwork is end-to-end trained using unsupervised learning to avoid expensive\nvideo annotations. The method generalizes well across distinct environments\n(rapid motion and challenging lighting conditions) and demonstrates\nstate-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event\nCamera (MVSEC) dataset. Furthermore, our network offers substantial savings in\nterms of the number of network parameters and computational energy cost.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:03:33 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Lee", "Chankyu", ""], ["Kosta", "Adarsh Kumar", ""], ["Roy", "Kaushik", ""]]}, {"id": "2103.10596", "submitter": "Xiaohong Liu", "authors": "Xiaohong Liu, Yaojie Liu, Jun Chen, Xiaoming Liu", "title": "PSCC-Net: Progressive Spatio-Channel Correlation Network for Image\n  Manipulation Detection and Localization", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To defend against manipulation of image content, such as splicing, copy-move,\nand removal, we develop a Progressive Spatio-Channel Correlation Network\n(PSCC-Net) to detect and localize image manipulations. PSCC-Net processes the\nimage in a two-path procedure: a top-down path that extracts local and global\nfeatures and a bottom-up path that detects whether the input image is\nmanipulated, and estimates its manipulation masks at 4 scales, where each mask\nis conditioned on the previous one. Different from the conventional\nencoder-decoder and no-pooling structures, PSCC-Net leverages features at\ndifferent scales with dense cross-connections to produce manipulation masks in\na coarse-to-fine fashion. Moreover, a Spatio-Channel Correlation Module (SCCM)\ncaptures both spatial and channel-wise correlations in the bottom-up path,\nwhich endows features with holistic cues, enabling the network to cope with a\nwide range of manipulation attacks. Thanks to the light-weight backbone and\nprogressive mechanism, PSCC-Net can process 1,080P images at 50+ FPS. Extensive\nexperiments demonstrate the superiority of PSCC-Net over the state-of-the-art\nmethods on both detection and localization.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:22:53 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Xiaohong", ""], ["Liu", "Yaojie", ""], ["Chen", "Jun", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2103.10603", "submitter": "Haoyang Li", "authors": "Haoyang Li and Xinggang Wang", "title": "Noise Modulation: Let Your Model Interpret Itself", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given the great success of Deep Neural Networks(DNNs) and the black-box\nnature of it,the interpretability of these models becomes an important\nissue.The majority of previous research works on the post-hoc interpretation of\na trained model.But recently, adversarial training shows that it is possible\nfor a model to have an interpretable input-gradient through\ntraining.However,adversarial training lacks efficiency for interpretability.To\nresolve this problem, we construct an approximation of the adversarial\nperturbations and discover a connection between adversarial training and\namplitude modulation. Based on a digital analogy,we propose noise modulation as\nan efficient and model-agnostic alternative to train a model that interprets\nitself with input-gradients.Experiment results show that noise modulation can\neffectively increase the interpretability of input-gradients model-agnosticly.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:55:33 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Haoyang", ""], ["Wang", "Xinggang", ""]]}, {"id": "2103.10607", "submitter": "Xizhe Xue", "authors": "Xizhe Xue, Ying Li, Xiaoyue Yin, Qiang Shen", "title": "DCF-ASN: Coarse-to-fine Real-time Visual Tracking via Discriminative\n  Correlation Filter and Attentional Siamese Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discriminative correlation filters (DCF) and siamese networks have achieved\npromising performance on visual tracking tasks thanks to their superior\ncomputational efficiency and reliable similarity metric learning, respectively.\nHowever, how to effectively take advantages of powerful deep networks, while\nmaintaining the real-time response of DCF, remains a challenging problem.\nEmbedding the cross-correlation operator as a separate layer into siamese\nnetworks is a popular choice to enhance the tracking accuracy. Being a key\ncomponent of such a network, the correlation layer is updated online together\nwith other parts of the network. Yet, when facing serious disturbance, fused\ntrackers may still drift away from the target completely due to accumulated\nerrors. To address these issues, we propose a coarse-to-fine tracking\nframework, which roughly infers the target state via an online-updating DCF\nmodule first and subsequently, finely locates the target through an\noffline-training asymmetric siamese network (ASN). Benefitting from the\nguidance of DCF and the learned channel weights obtained through exploiting the\ngiven ground-truth template, ASN refines feature representation and implements\nprecise target localization. Systematic experiments on five popular tracking\ndatasets demonstrate that the proposed DCF-ASN achieves the state-of-the-art\nperformance while exhibiting good tracking efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 03:01:21 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Xue", "Xizhe", ""], ["Li", "Ying", ""], ["Yin", "Xiaoyue", ""], ["Shen", "Qiang", ""]]}, {"id": "2103.10608", "submitter": "Ankit Parag Shah", "authors": "Anxiang Zhang, Ankit Shah, Bhiksha Raj", "title": "Training image classifiers using Semi-Weak Label Data", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In Multiple Instance learning (MIL), weak labels are provided at the bag\nlevel with only presence/absence information known. However, there is a\nconsiderable gap in performance in comparison to a fully supervised model,\nlimiting the practical applicability of MIL approaches. Thus, this paper\nintroduces a novel semi-weak label learning paradigm as a middle ground to\nmitigate the problem. We define semi-weak label data as data where we know the\npresence or absence of a given class and the exact count of each class as\nopposed to knowing the label proportions. We then propose a two-stage framework\nto address the problem of learning from semi-weak labels. It leverages the fact\nthat counting information is non-negative and discrete. Experiments are\nconducted on generated samples from CIFAR-10. We compare our model with a\nfully-supervised setting baseline, a weakly-supervised setting baseline and\nlearning from pro-portion (LLP) baseline. Our framework not only outperforms\nboth baseline models for MIL-based weakly super-vised setting and learning from\nproportion setting, but also gives comparable results compared to the fully\nsupervised model. Further, we conduct thorough ablation studies to analyze\nacross datasets and variation with batch size, losses architectural changes,\nbag size and regularization\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 03:06:07 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhang", "Anxiang", ""], ["Shah", "Ankit", ""], ["Raj", "Bhiksha", ""]]}, {"id": "2103.10609", "submitter": "Xiaosen Wang", "authors": "Xiaosen Wang, Jiadong Lin, Han Hu, Jingdong Wang, Kun He", "title": "Boosting Adversarial Transferability through Enhanced Momentum", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are known to be vulnerable to adversarial examples\ncrafted by adding human-imperceptible perturbations on benign images. Many\nexisting adversarial attack methods have achieved great white-box attack\nperformance, but exhibit low transferability when attacking other models.\nVarious momentum iterative gradient-based methods are shown to be effective to\nimprove the adversarial transferability. In what follows, we propose an\nenhanced momentum iterative gradient-based method to further enhance the\nadversarial transferability. Specifically, instead of only accumulating the\ngradient during the iterative process, we additionally accumulate the average\ngradient of the data points sampled in the gradient direction of the previous\niteration so as to stabilize the update direction and escape from poor local\nmaxima. Extensive experiments on the standard ImageNet dataset demonstrate that\nour method could improve the adversarial transferability of momentum-based\nmethods by a large margin of 11.1% on average. Moreover, by incorporating with\nvarious input transformation methods, the adversarial transferability could be\nfurther improved significantly. We also attack several extra advanced defense\nmodels under the ensemble-model setting, and the enhancements are remarkable\nwith at least 7.8% on average.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 03:10:32 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Wang", "Xiaosen", ""], ["Lin", "Jiadong", ""], ["Hu", "Han", ""], ["Wang", "Jingdong", ""], ["He", "Kun", ""]]}, {"id": "2103.10611", "submitter": "Jinyang Yuan", "authors": "Jinyang Yuan, Bin Li, Xiangyang Xue", "title": "Knowledge-Guided Object Discovery with Acquired Deep Impressions", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework called Acquired Deep Impressions (ADI) which\ncontinuously learns knowledge of objects as \"impressions\" for compositional\nscene understanding. In this framework, the model first acquires knowledge from\nscene images containing a single object in a supervised manner, and then\ncontinues to learn from novel multi-object scene images which may contain\nobjects that have not been seen before without any further supervision, under\nthe guidance of the learned knowledge as humans do. By memorizing impressions\nof objects into parameters of neural networks and applying the generative\nreplay strategy, the learned knowledge can be reused to generate images with\npseudo-annotations and in turn assist the learning of novel scenes. The\nproposed ADI framework focuses on the acquisition and utilization of knowledge,\nand is complementary to existing deep generative models proposed for\ncompositional scene representation. We adapt a base model to make it fall\nwithin the ADI framework and conduct experiments on two types of datasets.\nEmpirical results suggest that the proposed framework is able to effectively\nutilize the acquired impressions and improve the scene decomposition\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 03:17:57 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Yuan", "Jinyang", ""], ["Li", "Bin", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2103.10614", "submitter": "Zhongyang Zhang", "authors": "Zhongyang Zhang, Zhiyang Xu, Zia Ahmed, Asif Salekin, Tauhidur Rahman", "title": "Hyperspectral Image Super-Resolution in Arbitrary Input-Output Band\n  Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images (HSIs) with narrow spectral bands can capture rich\nspectral information, making them suitable for many computer vision tasks. One\nof the fundamental limitations of HSI is its low spatial resolution, and\nseveral recent works on super-resolution(SR) have been proposed to tackle this\nchallenge. However, due to HSI cameras' diversity, different cameras capture\nimages with different spectral response functions and the number of total\nchannels. The existing HSI datasets are usually small and consequently\ninsufficient for modeling. We propose a Meta-Learning-Based\nSuper-Resolution(MLSR) model, which can take in HSI images at an arbitrary\nnumber of input bands' peak wavelengths and generate super-resolved HSIs with\nan arbitrary number of output bands' peak wavelengths. We artificially create\nsub-datasets by sampling the bands from NTIRE2020 and ICVL datasets to simulate\nthe cross-dataset settings and perform HSI SR with spectral interpolation and\nextrapolation on them. We train a single MLSR model for all sub-datasets and\ntrain dedicated baseline models for each sub-dataset. The results show the\nproposed model has the same level or better performance compared to\nthe-state-of-the-art HSI SR methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 03:32:28 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhang", "Zhongyang", ""], ["Xu", "Zhiyang", ""], ["Ahmed", "Zia", ""], ["Salekin", "Asif", ""], ["Rahman", "Tauhidur", ""]]}, {"id": "2103.10619", "submitter": "Bohan Zhuang", "authors": "Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, Jianfei Cai", "title": "Scalable Visual Transformers with Hierarchical Pooling", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recently proposed Visual image Transformers (ViT) with pure attention\nhave achieved promising performance on image recognition tasks, such as image\nclassification. However, the routine of the current ViT model is to maintain a\nfull-length patch sequence during inference, which is redundant and lacks\nhierarchical representation. To this end, we propose a Hierarchical Visual\nTransformer (HVT) which progressively pools visual tokens to shrink the\nsequence length and hence reduces the computational cost, analogous to the\nfeature maps downsampling in Convolutional Neural Networks (CNNs). It brings a\ngreat benefit that we can increase the model capacity by scaling dimensions of\ndepth/width/resolution/patch size without introducing extra computational\ncomplexity due to the reduced sequence length. Moreover, we empirically find\nthat the average pooled visual tokens contain more discriminative information\nthan the single class token. To demonstrate the improved scalability of our\nHVT, we conduct extensive experiments on the image classification task. With\ncomparable FLOPs, our HVT outperforms the competitive baselines on ImageNet and\nCIFAR-100 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 03:55:58 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Pan", "Zizheng", ""], ["Zhuang", "Bohan", ""], ["Liu", "Jing", ""], ["He", "Haoyu", ""], ["Cai", "Jianfei", ""]]}, {"id": "2103.10621", "submitter": "Chen Chen", "authors": "Kui Jiang, Zhongyuan Wang, Zheng Wang, Peng Yi, Xiao Wang, Yansheng\n  Qiu, Chen Chen, Chia-Wen Lin", "title": "Degrade is Upgrade: Learning Degradation for Low-light Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light image enhancement aims to improve an image's visibility while\nkeeping its visual naturalness. Different from existing methods, which tend to\naccomplish the enhancement task directly, we investigate the intrinsic\ndegradation and relight the low-light image while refining the details and\ncolor in two steps. Inspired by the color image formulation (diffuse\nillumination color plus environment illumination color), we first estimate the\ndegradation from low-light inputs to simulate the distortion of environment\nillumination color, and then refine the content to recover the loss of diffuse\nillumination color. To this end, we propose a novel Degradation-to-Refinement\nGeneration Network (DRGN). Its distinctive features can be summarized as 1) A\nnovel two-step generation network for degradation learning and content\nrefinement. It is not only superior to one-step methods, but also is capable of\nsynthesizing sufficient paired samples to benefit the model training; 2) A\nmulti-resolution fusion network to represent the target information\n(degradation or contents) in a multi-scale cooperative manner, which is more\neffective to address the complex unmixing problems. Extensive experiments on\nboth the enhancement task and the joint detection task have verified the\neffectiveness and efficiency of our proposed method, surpassing the SOTA by\n0.95dB in PSNR on LOL1000 dataset and 3.18\\% in mAP on ExDark dataset. Our code\nis available at \\url{https://github.com/kuijiang0802/DRGN}\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 04:00:27 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 03:35:49 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jiang", "Kui", ""], ["Wang", "Zhongyuan", ""], ["Wang", "Zheng", ""], ["Yi", "Peng", ""], ["Wang", "Xiao", ""], ["Qiu", "Yansheng", ""], ["Chen", "Chen", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "2103.10626", "submitter": "Yash Sharma", "authors": "Yash Sharma, Aman Shrivastava, Lubaina Ehsan, Christopher A. Moskaluk,\n  Sana Syed, Donald E. Brown", "title": "Cluster-to-Conquer: A Framework for End-to-End Multi-Instance Learning\n  for Whole Slide Image Classification", "comments": "Accepted at MIDL, 2021 - https://openreview.net/forum?id=7i1-2oKIELU", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the availability of digitized Whole Slide Images (WSIs) has\nenabled the use of deep learning-based computer vision techniques for automated\ndisease diagnosis. However, WSIs present unique computational and algorithmic\nchallenges. WSIs are gigapixel-sized ($\\sim$100K pixels), making them\ninfeasible to be used directly for training deep neural networks. Also, often\nonly slide-level labels are available for training as detailed annotations are\ntedious and can be time-consuming for experts. Approaches using\nmultiple-instance learning (MIL) frameworks have been shown to overcome these\nchallenges. Current state-of-the-art approaches divide the learning framework\ninto two decoupled parts: a convolutional neural network (CNN) for encoding the\npatches followed by an independent aggregation approach for slide-level\nprediction. In this approach, the aggregation step has no bearing on the\nrepresentations learned by the CNN encoder. We have proposed an end-to-end\nframework that clusters the patches from a WSI into ${k}$-groups, samples\n${k}'$ patches from each group for training, and uses an adaptive attention\nmechanism for slide level prediction; Cluster-to-Conquer (C2C). We have\ndemonstrated that dividing a WSI into clusters can improve the model training\nby exposing it to diverse discriminative features extracted from the patches.\nWe regularized the clustering mechanism by introducing a KL-divergence loss\nbetween the attention weights of patches in a cluster and the uniform\ndistribution. The framework is optimized end-to-end on slide-level\ncross-entropy, patch-level cross-entropy, and KL-divergence loss\n(Implementation: https://github.com/YashSharma/C2C).\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 04:24:01 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 18:48:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sharma", "Yash", ""], ["Shrivastava", "Aman", ""], ["Ehsan", "Lubaina", ""], ["Moskaluk", "Christopher A.", ""], ["Syed", "Sana", ""], ["Brown", "Donald E.", ""]]}, {"id": "2103.10643", "submitter": "Yihao Luo", "authors": "Yihao Luo, Xiang Cao, Juntao Zhang, Xiang Cao, Jingjuan Guo, Haibo\n  Shen, Tianjiang Wang and Qi Feng", "title": "CE-FPN: Enhancing Channel Information for Object Detection", "comments": "9pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pyramid network (FPN) has been an effective framework to extract\nmulti-scale features in object detection. However, current FPN-based methods\nmostly suffer from the intrinsic flaw of channel reduction, which brings about\nthe loss of semantical information. And the miscellaneous fused feature maps\nmay cause serious aliasing effects. In this paper, we present a novel channel\nenhancement feature pyramid network (CE-FPN) with three simple yet effective\nmodules to alleviate these problems. Specifically, inspired by sub-pixel\nconvolution, we propose a sub-pixel skip fusion method to perform both channel\nenhancement and upsampling. Instead of the original 1x1 convolution and linear\nupsampling, it mitigates the information loss due to channel reduction. Then we\npropose a sub-pixel context enhancement module for extracting more feature\nrepresentations, which is superior to other context methods due to the\nutilization of rich channel information by sub-pixel convolution. Furthermore,\na channel attention guided module is introduced to optimize the final\nintegrated features on each level, which alleviates the aliasing effect only\nwith a few computational burdens. Our experiments show that CE-FPN achieves\ncompetitive performance compared to state-of-the-art FPN-based detectors on MS\nCOCO benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 05:51:53 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Luo", "Yihao", ""], ["Cao", "Xiang", ""], ["Zhang", "Juntao", ""], ["Cao", "Xiang", ""], ["Guo", "Jingjuan", ""], ["Shen", "Haibo", ""], ["Wang", "Tianjiang", ""], ["Feng", "Qi", ""]]}, {"id": "2103.10656", "submitter": "Nicolas Gillis", "authors": "Maryam Abdolali, Nicolas Gillis", "title": "Beyond Linear Subspace Clustering: A Comparative Study of Nonlinear\n  Manifold Clustering Algorithms", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Subspace clustering is an important unsupervised clustering approach. It is\nbased on the assumption that the high-dimensional data points are approximately\ndistributed around several low-dimensional linear subspaces. The majority of\nthe prominent subspace clustering algorithms rely on the representation of the\ndata points as linear combinations of other data points, which is known as a\nself-expressive representation. To overcome the restrictive linearity\nassumption, numerous nonlinear approaches were proposed to extend successful\nsubspace clustering approaches to data on a union of nonlinear manifolds. In\nthis comparative study, we provide a comprehensive overview of nonlinear\nsubspace clustering approaches proposed in the last decade. We introduce a new\ntaxonomy to classify the state-of-the-art approaches into three categories,\nnamely locality preserving, kernel based, and neural network based. The major\nrepresentative algorithms within each category are extensively compared on\ncarefully designed synthetic and real-world data sets. The detailed analysis of\nthese approaches unfolds potential research directions and unsolved challenges\nin this field.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 06:34:34 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Abdolali", "Maryam", ""], ["Gillis", "Nicolas", ""]]}, {"id": "2103.10663", "submitter": "Eunji Kim", "authors": "Eunji Kim, Siwon Kim, Minji Seo, Sungroh Yoon", "title": "XProtoNet: Diagnosis in Chest Radiography with Global and Local\n  Explanations", "comments": "10 pages, 7 figures. Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automated diagnosis using deep neural networks in chest radiography can help\nradiologists detect life-threatening diseases. However, existing methods only\nprovide predictions without accurate explanations, undermining the\ntrustworthiness of the diagnostic methods. Here, we present XProtoNet, a\nglobally and locally interpretable diagnosis framework for chest radiography.\nXProtoNet learns representative patterns of each disease from X-ray images,\nwhich are prototypes, and makes a diagnosis on a given X-ray image based on the\npatterns. It predicts the area where a sign of the disease is likely to appear\nand compares the features in the predicted area with the prototypes. It can\nprovide a global explanation, the prototype, and a local explanation, how the\nprototype contributes to the prediction of a single image. Despite the\nconstraint for interpretability, XProtoNet achieves state-of-the-art\nclassification performance on the public NIH chest X-ray dataset.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 07:18:21 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Kim", "Eunji", ""], ["Kim", "Siwon", ""], ["Seo", "Minji", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2103.10670", "submitter": "Zhengwen Li", "authors": "Zhengwen Li, Xiabi Liu", "title": "Improving Image co-segmentation via Deep Metric Learning", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Metric Learning (DML) is helpful in computer vision tasks. In this\npaper, we firstly introduce DML into image co-segmentation. We propose a novel\nTriplet loss for Image Segmentation, called IS-Triplet loss for short, and\ncombine it with traditional image segmentation loss. Different from the general\nDML task which learns the metric between pictures, we treat each pixel as a\nsample, and use their embedded features in high-dimensional space to form\ntriples, then we tend to force the distance between pixels of different\ncategories greater than of the same category by optimizing IS-Triplet loss so\nthat the pixels from different categories are easier to be distinguished in the\nhigh-dimensional feature space. We further present an efficient triple sampling\nstrategy to make a feasible computation of IS-Triplet loss. Finally, the\nIS-Triplet loss is combined with 3 traditional image segmentation losses to\nperform image segmentation. We apply the proposed approach to image\nco-segmentation and test it on the SBCoseg dataset and the Internet dataset.\nThe experimental result shows that our approach can effectively improve the\ndiscrimination of pixels' categories in high-dimensional space and thus help\ntraditional loss achieve better performance of image segmentation with fewer\ntraining epochs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 07:30:42 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Zhengwen", ""], ["Liu", "Xiabi", ""]]}, {"id": "2103.10674", "submitter": "Honghong Zhou", "authors": "Honghong Zhou, Caili Guo, Hao Zhang and Yanjun Wang", "title": "Learning Multiscale Correlations for Human Motion Prediction", "comments": "The paper has submitted to IEEE ICDL 2021, The codes will be\n  available after the paper was accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In spite of the great progress in human motion prediction, it is still a\nchallenging task to predict those aperiodic and complicated motions. We believe\nthat to capture the correlations among human body components is the key to\nunderstand the human motion. In this paper, we propose a novel multiscale graph\nconvolution network (MGCN) to address this problem. Firstly, we design an\nadaptive multiscale interactional encoding module (MIEM) which is composed of\ntwo sub modules: scale transformation module and scale interaction module to\nlearn the human body correlations. Secondly, we apply a coarse-to-fine decoding\nstrategy to decode the motions sequentially. We evaluate our approach on two\nstandard benchmark datasets for human motion prediction: Human3.6M and CMU\nmotion capture dataset. The experiments show that the proposed approach\nachieves the state-of-the-art performance for both short-term and long-term\nprediction especially in those complicated action category.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 07:58:16 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 07:09:23 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhou", "Honghong", ""], ["Guo", "Caili", ""], ["Zhang", "Hao", ""], ["Wang", "Yanjun", ""]]}, {"id": "2103.10681", "submitter": "Lei Zhu", "authors": "Lei Zhu, Qi She, Bin Zhang, Yanye Lu, Zhilin Lu, Duo Li, Jie Hu", "title": "Learning the Superpixel in a Non-iterative and Lifelong Manner", "comments": "Accept by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Superpixel is generated by automatically clustering pixels in an image into\nhundreds of compact partitions, which is widely used to perceive the object\ncontours for its excellent contour adherence. Although some works use the\nConvolution Neural Network (CNN) to generate high-quality superpixel, we\nchallenge the design principles of these networks, specifically for their\ndependence on manual labels and excess computation resources, which limits\ntheir flexibility compared with the traditional unsupervised segmentation\nmethods. We target at redefining the CNN-based superpixel segmentation as a\nlifelong clustering task and propose an unsupervised CNN-based method called\nLNS-Net. The LNS-Net can learn superpixel in a non-iterative and lifelong\nmanner without any manual labels. Specifically, a lightweight feature embedder\nis proposed for LNS-Net to efficiently generate the cluster-friendly features.\nWith those features, seed nodes can be automatically assigned to cluster pixels\nin a non-iterative way. Additionally, our LNS-Net can adapt the sequentially\nlifelong learning by rescaling the gradient of weight based on both channel and\nspatial context to avoid overfitting. Experiments show that the proposed\nLNS-Net achieves significantly better performance on three benchmarks with\nnearly ten times lower complexity compared with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 08:19:37 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 10:04:32 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhu", "Lei", ""], ["She", "Qi", ""], ["Zhang", "Bin", ""], ["Lu", "Yanye", ""], ["Lu", "Zhilin", ""], ["Li", "Duo", ""], ["Hu", "Jie", ""]]}, {"id": "2103.10697", "submitter": "St\\'ephane d'Ascoli", "authors": "St\\'ephane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio\n  Biroli, Levent Sagun", "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive\n  Biases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional architectures have proven extremely successful for vision\ntasks. Their hard inductive biases enable sample-efficient learning, but come\nat the cost of a potentially lower performance ceiling. Vision Transformers\n(ViTs) rely on more flexible self-attention layers, and have recently\noutperformed CNNs for image classification. However, they require costly\npre-training on large external datasets or distillation from pre-trained\nconvolutional networks. In this paper, we ask the following question: is it\npossible to combine the strengths of these two architectures while avoiding\ntheir respective limitations? To this end, we introduce gated positional\nself-attention (GPSA), a form of positional self-attention which can be\nequipped with a ``soft\" convolutional inductive bias. We initialise the GPSA\nlayers to mimic the locality of convolutional layers, then give each attention\nhead the freedom to escape locality by adjusting a gating parameter regulating\nthe attention paid to position versus content information. The resulting\nconvolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet,\nwhile offering a much improved sample efficiency. We further investigate the\nrole of locality in learning by first quantifying how it is encouraged in\nvanilla self-attention layers, then analysing how it is escaped in GPSA layers.\nWe conclude by presenting various ablations to better understand the success of\nthe ConViT. Our code and models are released publicly at\nhttps://github.com/facebookresearch/convit.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:11:20 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 08:44:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["d'Ascoli", "St\u00e9phane", ""], ["Touvron", "Hugo", ""], ["Leavitt", "Matthew", ""], ["Morcos", "Ari", ""], ["Biroli", "Giulio", ""], ["Sagun", "Levent", ""]]}, {"id": "2103.10699", "submitter": "Aleksandr Petiushko", "authors": "Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, Aleksandr\n  Petiushko", "title": "MDMMT: Multidomain Multimodal Transformer for Video Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new state-of-the-art on the text to video retrieval task on\nMSRVTT and LSMDC benchmarks where our model outperforms all previous solutions\nby a large margin. Moreover, state-of-the-art results are achieved with a\nsingle model on two datasets without finetuning. This multidomain\ngeneralisation is achieved by a proper combination of different video caption\ndatasets. We show that training on different datasets can improve test results\nof each other. Additionally we check intersection between many popular datasets\nand found that MSRVTT has a significant overlap between the test and the train\nparts, and the same situation is observed for ActivityNet.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:16:39 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Dzabraev", "Maksim", ""], ["Kalashnikov", "Maksim", ""], ["Komkov", "Stepan", ""], ["Petiushko", "Aleksandr", ""]]}, {"id": "2103.10702", "submitter": "Chen Liang", "authors": "Chen Liang, Yu Wu, Yawei Luo and Yi Yang", "title": "ClawCraneNet: Leveraging Object-level Relation for Text-based Video\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based video segmentation is a challenging task that segments out the\nnatural language referred objects in videos. It essentially requires semantic\ncomprehension and fine-grained video understanding. Existing methods introduce\nlanguage representation into segmentation models in a bottom-up manner, which\nmerely conducts vision-language interaction within local receptive fields of\nConvNets. We argue that such interaction is not fulfilled since the model can\nbarely construct region-level relationships given partial observations, which\nis contrary to the description logic of natural language/referring expressions.\nIn fact, people usually describe a target object using relations with other\nobjects, which may not be easily understood without seeing the whole video. To\naddress the issue, we introduce a novel top-down approach by imitating how we\nhuman segment an object with the language guidance. We first figure out all\ncandidate objects in videos and then choose the refereed one by parsing\nrelations among those high-level objects. Three kinds of object-level relations\nare investigated for precise relationship understanding, i.e., positional\nrelation, text-guided semantic relation, and temporal relation. Extensive\nexperiments on A2D Sentences and J-HMDB Sentences show our method outperforms\nstate-of-the-art methods by a large margin. Qualitative results also show our\nresults are more explainable. Besides, based on the inspiration, we win the\nfirst place in CVPR2021 Referring Youtube-VOS challenge.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:31:08 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 07:15:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liang", "Chen", ""], ["Wu", "Yu", ""], ["Luo", "Yawei", ""], ["Yang", "Yi", ""]]}, {"id": "2103.10729", "submitter": "Dimitri Gominski", "authors": "Dimitri Gominski and Val\\'erie Gouet-Brunet and Liming Chen", "title": "Connecting Images through Time and Sources: Introducing Low-data,\n  Heterogeneous Instance Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With impressive results in applications relying on feature learning, deep\nlearning has also blurred the line between algorithm and data. Pick a training\ndataset, pick a backbone network for feature extraction, and voil\\`a ; this\nusually works for a variety of use cases. But the underlying hypothesis that\nthere exists a training dataset matching the use case is not always met.\nMoreover, the demand for interconnections regardless of the variations of the\ncontent calls for increasing generalization and robustness in features.\n  An interesting application characterized by these problematics is the\nconnection of historical and cultural databases of images. Through the\nseemingly simple task of instance retrieval, we propose to show that it is not\ntrivial to pick features responding well to a panel of variations and semantic\ncontent. Introducing a new enhanced version of the Alegoria benchmark, we\ncompare descriptors using the detailed annotations. We further give insights\nabout the core problems in instance retrieval, testing four state-of-the-art\nadditional techniques to increase performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 10:54:51 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Gominski", "Dimitri", ""], ["Gouet-Brunet", "Val\u00e9rie", ""], ["Chen", "Liming", ""]]}, {"id": "2103.10738", "submitter": "Lijun Gou", "authors": "Lijun Gou, Shengkai Wu, Jinrong Yang, Hangcheng Yu, Chenxi Lin,\n  Xiaoping Li, Chao Deng", "title": "Carton dataset synthesis method for domain shift based on foreground\n  texture decoupling and replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One major impediment in rapidly deploying object detection models for\nindustrial applications is the lack of large annotated datasets. We currently\nhave presented the Sacked Carton Dataset(SCD) that contains carton images from\nthree scenarios, such as comprehensive pharmaceutical logistics company(CPLC),\ne-commerce logistics company(ECLC), fruit market(FM). However, due to domain\nshift, the model trained with one of the three scenarios in SCD has poor\ngeneralization ability when applied to the rest scenarios. To solve this\nproblem, a novel image synthesis method is proposed to replace the foreground\ntexture of the source datasets with the texture of the target datasets. Our\nmethod can keep the context relationship of foreground objects and backgrounds\nunchanged and greatly augment the target datasets. We firstly propose a surface\nsegmentation algorithm to achieve texture decoupling of each instance.\nSecondly, a contour reconstruction algorithm is proposed to keep the occlusion\nand truncation relationship of the instance unchanged. Finally, the Gaussian\nfusion algorithm is used to replace the foreground texture from the source\ndatasets with the texture from the target datasets. The novel image synthesis\nmethod can largely boost AP by at least 4.3%~6.5% on RetinaNet and 3.4%~6.8% on\nFaster R-CNN for the target domain. Code is available at\nhttps://github.com/hustgetlijun/RCAN.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:21:31 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 08:59:02 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 14:00:28 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 11:13:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Gou", "Lijun", ""], ["Wu", "Shengkai", ""], ["Yang", "Jinrong", ""], ["Yu", "Hangcheng", ""], ["Lin", "Chenxi", ""], ["Li", "Xiaoping", ""], ["Deng", "Chao", ""]]}, {"id": "2103.10741", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Sethupathy Parameswaran, Ashish Mishra, Suresh\n  Sundaram", "title": "Online Lifelong Generalized Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods proposed in the literature for zero-shot learning (ZSL) are typically\nsuitable for offline learning and cannot continually learn from sequential\nstreaming data. The sequential data comes in the form of tasks during training.\nRecently, a few attempts have been made to handle this issue and develop\ncontinual ZSL (CZSL) methods. However, these CZSL methods require clear\ntask-boundary information between the tasks during training, which is not\npractically possible. This paper proposes a task-free (i.e., task-agnostic)\nCZSL method, which does not require any task information during continual\nlearning. The proposed task-free CZSL method employs a variational autoencoder\n(VAE) for performing ZSL. To develop the CZSL method, we combine the concept of\nexperience replay with knowledge distillation and regularization. Here,\nknowledge distillation is performed using the training sample's dark knowledge,\nwhich essentially helps overcome the catastrophic forgetting issue. Further, it\nis enabled for task-free learning using short-term memory. Finally, a\nclassifier is trained on the synthetic features generated at the latent space\nof the VAE. Moreover, the experiments are conducted in a challenging and\npractical ZSL setup, i.e., generalized ZSL (GZSL). These experiments are\nconducted for two kinds of single-head continual learning settings: (i) mild\nsetting-: task-boundary is known only during training but not during testing;\n(ii) strict setting-: task-boundary is not known at training, as well as\ntesting. Experimental results on five benchmark datasets exhibit the validity\nof the approach for CZSL.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:24:05 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 03:05:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Gautam", "Chandan", ""], ["Parameswaran", "Sethupathy", ""], ["Mishra", "Ashish", ""], ["Sundaram", "Suresh", ""]]}, {"id": "2103.10764", "submitter": "Bonan Li", "authors": "Bonan Li and Xuecheng Nie and Congying Han", "title": "DFS: A Diverse Feature Synthesis Model for Generalized Zero-Shot\n  Learning", "comments": "11 pages,5 figures,conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative based strategy has shown great potential in the Generalized\nZero-Shot Learning task. However, it suffers severe generalization problem due\nto lacking of feature diversity for unseen classes to train a good classifier.\nIn this paper, we propose to enhance the generalizability of GZSL models via\nimproving feature diversity of unseen classes. For this purpose, we present a\nnovel Diverse Feature Synthesis (DFS) model. Different from prior works that\nsolely utilize semantic knowledge in the generation process, DFS leverages\nvisual knowledge with semantic one in a unified way, thus deriving\nclass-specific diverse feature samples and leading to robust classifier for\nrecognizing both seen and unseen classes in the testing phase. To simplify the\nlearning, DFS represents visual and semantic knowledge in the aligned space,\nmaking it able to produce good feature samples with a low-complexity\nimplementation. Accordingly, DFS is composed of two consecutive generators: an\naligned feature generator, transferring semantic and visual representations\ninto aligned features; a synthesized feature generator, producing diverse\nfeature samples of unseen classes in the aligned space. We conduct\ncomprehensive experiments to verify the efficacy of DFS. Results demonstrate\nits effectiveness to generate diverse features for unseen classes, leading to\nsuperior performance on multiple benchmarks. Code will be released upon\nacceptance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 12:24:42 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Bonan", ""], ["Nie", "Xuecheng", ""], ["Han", "Congying", ""]]}, {"id": "2103.10768", "submitter": "Celyn Walters", "authors": "Celyn Walters (1), Oscar Mendez (1), Mark Johnson, Richard Bowden (1)\n  ((1) CVSSP, University of Surrey)", "title": "There and Back Again: Self-supervised Multispectral Correspondence\n  Estimation", "comments": "To be published in IEEE/RSJ International Conference on Robot and\n  Automation (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across a wide range of applications, from autonomous vehicles to medical\nimaging, multi-spectral images provide an opportunity to extract additional\ninformation not present in color images. One of the most important steps in\nmaking this information readily available is the accurate estimation of dense\ncorrespondences between different spectra.\n  Due to the nature of cross-spectral images, most correspondence solving\ntechniques for the visual domain are simply not applicable. Furthermore, most\ncross-spectral techniques utilize spectra-specific characteristics to perform\nthe alignment. In this work, we aim to address the dense correspondence\nestimation problem in a way that generalizes to more than one spectrum. We do\nthis by introducing a novel cycle-consistency metric that allows us to\nself-supervise. This, combined with our spectra-agnostic loss functions, allows\nus to train the same network across multiple spectra.\n  We demonstrate our approach on the challenging task of dense RGB-FIR\ncorrespondence estimation. We also show the performance of our unmodified\nnetwork on the cases of RGB-NIR and RGB-RGB, where we achieve higher accuracy\nthan similar self-supervised approaches. Our work shows that cross-spectral\ncorrespondence estimation can be solved in a common framework that learns to\ngeneralize alignment across spectra.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 12:33:56 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:53:22 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Walters", "Celyn", "", "CVSSP, University of Surrey"], ["Mendez", "Oscar", "", "CVSSP, University of Surrey"], ["Johnson", "Mark", "", "CVSSP, University of Surrey"], ["Bowden", "Richard", "", "CVSSP, University of Surrey"]]}, {"id": "2103.10773", "submitter": "Zhigang Dai", "authors": "Zhigang Dai, Bolun Cai, Yugeng Lin, Junying Chen", "title": "UniMoCo: Unsupervised, Semi-Supervised and Full-Supervised Visual\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Momentum Contrast (MoCo) achieves great success for unsupervised visual\nrepresentation. However, there are a lot of supervised and semi-supervised\ndatasets, which are already labeled. To fully utilize the label annotations, we\npropose Unified Momentum Contrast (UniMoCo), which extends MoCo to support\narbitrary ratios of labeled data and unlabeled data training. Compared with\nMoCo, UniMoCo has two modifications as follows: (1) Different from a single\npositive pair in MoCo, we maintain multiple positive pairs on-the-fly by\ncomparing the query label to a label queue. (2) We propose a Unified\nContrastive(UniCon) loss to support an arbitrary number of positives and\nnegatives in a unified pair-wise optimization perspective. Our UniCon is more\nreasonable and powerful than the supervised contrastive loss in theory and\npractice. In our experiments, we pre-train multiple UniMoCo models with\ndifferent ratios of ImageNet labels and evaluate the performance on various\ndownstream tasks. Experiment results show that UniMoCo generalizes well for\nunsupervised, semi-supervised and supervised visual representation learning.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 12:42:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Dai", "Zhigang", ""], ["Cai", "Bolun", ""], ["Lin", "Yugeng", ""], ["Chen", "Junying", ""]]}, {"id": "2103.10787", "submitter": "Ashkan Esmaeili", "authors": "Ashkan Esmaeili, Marzieh Edraki, Nazanin Rahnavard, Mubarak Shah,\n  Ajmal Mian", "title": "LSDAT: Low-Rank and Sparse Decomposition for Decision-based Adversarial\n  Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose LSDAT, an image-agnostic decision-based black-box attack that\nexploits low-rank and sparse decomposition (LSD) to dramatically reduce the\nnumber of queries and achieve superior fooling rates compared to the\nstate-of-the-art decision-based methods under given imperceptibility\nconstraints. LSDAT crafts perturbations in the low-dimensional subspace formed\nby the sparse component of the input sample and that of an adversarial sample\nto obtain query-efficiency. The specific perturbation of interest is obtained\nby traversing the path between the input and adversarial sparse components. It\nis set forth that the proposed sparse perturbation is the most aligned sparse\nperturbation with the shortest path from the input sample to the decision\nboundary for some initial adversarial sample (the best sparse approximation of\nshortest path, likely to fool the model). Theoretical analyses are provided to\njustify the functionality of LSDAT. Unlike other dimensionality reduction based\ntechniques aimed at improving query efficiency (e.g, ones based on FFT), LSD\nworks directly in the image pixel domain to guarantee that non-$\\ell_2$\nconstraints, such as sparsity, are satisfied. LSD offers better control over\nthe number of queries and provides computational efficiency as it performs\nsparse decomposition of the input and adversarial images only once to generate\nall queries. We demonstrate $\\ell_0$, $\\ell_2$ and $\\ell_\\infty$ bounded\nattacks with LSDAT to evince its efficiency compared to baseline decision-based\nattacks in diverse low-query budget scenarios as outlined in the experiments.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:10:47 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 16:07:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Esmaeili", "Ashkan", ""], ["Edraki", "Marzieh", ""], ["Rahnavard", "Nazanin", ""], ["Shah", "Mubarak", ""], ["Mian", "Ajmal", ""]]}, {"id": "2103.10796", "submitter": "Arthur Moreau", "authors": "Arthur Moreau, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu,\n  Arnaud de La Fortelle", "title": "CoordiNet: uncertainty-aware pose regressor for reliable vehicle\n  localization", "comments": "8 pages, 8 figures. Submitted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we investigate visual-based camera localization with neural\nnetworks for robotics and autonomous vehicles applications. Our solution is a\nCNN-based algorithm which predicts camera pose (3D translation and 3D rotation)\ndirectly from a single image. It also provides an uncertainty estimate of the\npose. Pose and uncertainty are learned together with a single loss function.\nFurthermore, we propose a new fully convolutional architecture, named\nCoordiNet, designed to embed some of the scene geometry.\n  Our framework outperforms comparable methods on the largest available\nbenchmark, the Oxford RobotCar dataset, with an average error of 8 meters where\nprevious best was 19 meters. We have also investigated the performance of our\nmethod on large scenes for real time (18 fps) vehicle localization. In this\nsetup, structure-based methods require a large database, and we show that our\nproposal is a reliable alternative, achieving 29cm median error in a 1.9km loop\nin a busy urban area.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:32:40 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Moreau", "Arthur", ""], ["Piasco", "Nathan", ""], ["Tsishkou", "Dzmitry", ""], ["Stanciulescu", "Bogdan", ""], ["de La Fortelle", "Arnaud", ""]]}, {"id": "2103.10798", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Quanwei Huang, Youbao Tang, Xingxu Yao, Jufeng Yang,\n  Guiguang Ding, Bj\\\"orn W. Schuller", "title": "Computational Emotion Analysis From Images: Recent Advances and Future\n  Directions", "comments": "Accepted chapter in the book \"Human Perception of Visual Information\n  Psychological and Computational Perspective\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions are usually evoked in humans by images. Recently, extensive research\nefforts have been dedicated to understanding the emotions of images. In this\nchapter, we aim to introduce image emotion analysis (IEA) from a computational\nperspective with the focus on summarizing recent advances and suggesting future\ndirections. We begin with commonly used emotion representation models from\npsychology. We then define the key computational problems that the researchers\nhave been trying to solve and provide supervised frameworks that are generally\nused for different IEA tasks. After the introduction of major challenges in\nIEA, we present some representative methods on emotion feature extraction,\nsupervised classifier learning, and domain adaptation. Furthermore, we\nintroduce available datasets for evaluation and summarize some main results.\nFinally, we discuss some open questions and future directions that researchers\ncan pursue.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:33:34 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhao", "Sicheng", ""], ["Huang", "Quanwei", ""], ["Tang", "Youbao", ""], ["Yao", "Xingxu", ""], ["Yang", "Jufeng", ""], ["Ding", "Guiguang", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2103.10814", "submitter": "Ruoxi Shi", "authors": "Ruoxi Shi, Zhengrong Xue, Yang You, Cewu Lu", "title": "Skeleton Merger: an Unsupervised Aligned Keypoint Detector", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting aligned 3D keypoints is essential under many scenarios such as\nobject tracking, shape retrieval and robotics. However, it is generally hard to\nprepare a high-quality dataset for all types of objects due to the ambiguity of\nkeypoint itself. Meanwhile, current unsupervised detectors are unable to\ngenerate aligned keypoints with good coverage. In this paper, we propose an\nunsupervised aligned keypoint detector, Skeleton Merger, which utilizes\nskeletons to reconstruct objects. It is based on an Autoencoder architecture.\nThe encoder proposes keypoints and predicts activation strengths of edges\nbetween keypoints. The decoder performs uniform sampling on the skeleton and\nrefines it into small point clouds with pointwise offsets. Then the activation\nstrengths are applied and the sub-clouds are merged. Composite Chamfer Distance\n(CCD) is proposed as a distance between the input point cloud and the\nreconstruction composed of sub-clouds masked by activation strengths. We\ndemonstrate that Skeleton Merger is capable of detecting semantically-rich\nsalient keypoints with good alignment, and shows comparable performance to\nsupervised methods on the KeypointNet dataset. It is also shown that the\ndetector is robust to noise and subsampling. Our code is available at\nhttps://github.com/eliphatfs/SkeletonMerger.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 14:00:39 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Shi", "Ruoxi", ""], ["Xue", "Zhengrong", ""], ["You", "Yang", ""], ["Lu", "Cewu", ""]]}, {"id": "2103.10825", "submitter": "Tom van Sonsbeek", "authors": "Tom van Sonsbeek, Xiantong Zhen, Marcel Worring and Ling Shao", "title": "Variational Knowledge Distillation for Disease Classification in Chest\n  X-Rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease classification relying solely on imaging data attracts great interest\nin medical image analysis. Current models could be further improved, however,\nby also employing Electronic Health Records (EHRs), which contain rich\ninformation on patients and findings from clinicians. It is challenging to\nincorporate this information into disease classification due to the high\nreliance on clinician input in EHRs, limiting the possibility for automated\ndiagnosis. In this paper, we propose \\textit{variational knowledge\ndistillation} (VKD), which is a new probabilistic inference framework for\ndisease classification based on X-rays that leverages knowledge from EHRs.\nSpecifically, we introduce a conditional latent variable model, where we infer\nthe latent representation of the X-ray image with the variational posterior\nconditioning on the associated EHR text. By doing so, the model acquires the\nability to extract the visual features relevant to the disease during learning\nand can therefore perform more accurate classification for unseen patients at\ninference based solely on their X-ray scans. We demonstrate the effectiveness\nof our method on three public benchmark datasets with paired X-ray images and\nEHRs. The results show that the proposed variational knowledge distillation can\nconsistently improve the performance of medical image classification and\nsignificantly surpasses current methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 14:13:56 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["van Sonsbeek", "Tom", ""], ["Zhen", "Xiantong", ""], ["Worring", "Marcel", ""], ["Shao", "Ling", ""]]}, {"id": "2103.10858", "submitter": "Seul-Ki Yeom", "authors": "Seul-Ki Yeom, Kyung-Hwan Shim, Jee-Hyun Hwang", "title": "Toward Compact Deep Neural Networks via Energy-Aware Pruning", "comments": "11 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite of the remarkable performance, modern deep neural networks are\ninevitably accompanied with a significant amount of computational cost for\nlearning and deployment, which may be incompatible with their usage on edge\ndevices. Recent efforts to reduce these overheads involves pruning and\ndecomposing the parameters of various layers without performance deterioration.\nInspired by several decomposition studies, in this paper, we propose a novel\nenergy-aware pruning method that quantifies the importance of each filter in\nthe network using nuclear-norm (NN). Proposed energy-aware pruning leads to\nstate-of-the art performance for Top-1 accuracy, FLOPs, and parameter reduction\nacross a wide range of scenarios with multiple network architectures on\nCIFAR-10 and ImageNet after fine-grained classification tasks. On toy\nexperiment, despite of no fine-tuning, we can visually observe that NN not only\nhas little change in decision boundaries across classes, but also clearly\noutperforms previous popular criteria. We achieve competitive results with\n40.4/49.8% of FLOPs and 45.9/52.9% of parameter reduction with 94.13/94.61% in\nthe Top-1 accuracy with ResNet-56/110 on CIFAR-10, respectively. In addition,\nour observations are consistent for a variety of different pruning setting in\nterms of data size as well as data quality which can be emphasized in the\nstability of the acceleration and compression with negligible accuracy loss.\nOur code is available at https://github.com/nota-github/nota-pruning_rank.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 15:33:16 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Yeom", "Seul-Ki", ""], ["Shim", "Kyung-Hwan", ""], ["Hwang", "Jee-Hyun", ""]]}, {"id": "2103.10868", "submitter": "Seong Tae Kim", "authors": "Aadhithya Sankar, Matthias Keicher, Rami Eisawy, Abhijeet Parida,\n  Franz Pfister, Seong Tae Kim, Nassir Navab", "title": "GLOWin: A Flow-based Invertible Generative Framework for Learning\n  Disentangled Feature Representations in Medical Images", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangled representations can be useful in many downstream tasks, help to\nmake deep learning models more interpretable, and allow for control over\nfeatures of synthetically generated images that can be useful in training other\nmodels that require a large number of labelled or unlabelled data. Recently,\nflow-based generative models have been proposed to generate realistic images by\ndirectly modeling the data distribution with invertible functions. In this\nwork, we propose a new flow-based generative model framework, named GLOWin,\nthat is end-to-end invertible and able to learn disentangled representations.\nFeature disentanglement is achieved by factorizing the latent space into\ncomponents such that each component learns the representation for one\ngenerative factor. Comprehensive experiments have been conducted to evaluate\nthe proposed method on a public brain tumor MR dataset. Quantitative and\nqualitative results suggest that the proposed method is effective in\ndisentangling the features from complex medical images.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 15:47:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Sankar", "Aadhithya", ""], ["Keicher", "Matthias", ""], ["Eisawy", "Rami", ""], ["Parida", "Abhijeet", ""], ["Pfister", "Franz", ""], ["Kim", "Seong Tae", ""], ["Navab", "Nassir", ""]]}, {"id": "2103.10869", "submitter": "G\\\"orkem Algan", "authors": "G\\\"orkem Algan, Ilkay Ulusoy", "title": "MetaLabelNet: Learning to Generate Soft-Labels from Noisy-Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Real-world datasets commonly have noisy labels, which negatively affects the\nperformance of deep neural networks (DNNs). In order to address this problem,\nwe propose a label noise robust learning algorithm, in which the base\nclassifier is trained on soft-labels that are produced according to a\nmeta-objective. In each iteration, before conventional training, the\nmeta-objective reshapes the loss function by changing soft-labels, so that\nresulting gradient updates would lead to model parameters with minimum loss on\nmeta-data. Soft-labels are generated from extracted features of data instances,\nand the mapping function is learned by a single layer perceptron (SLP) network,\nwhich is called MetaLabelNet. Following, base classifier is trained by using\nthese generated soft-labels. These iterations are repeated for each batch of\ntraining data. Our algorithm uses a small amount of clean data as meta-data,\nwhich can be obtained effortlessly for many cases. We perform extensive\nexperiments on benchmark datasets with both synthetic and real-world noises.\nResults show that our approach outperforms existing baselines.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 15:47:44 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Algan", "G\u00f6rkem", ""], ["Ulusoy", "Ilkay", ""]]}, {"id": "2103.10892", "submitter": "Long Xie", "authors": "Long Xie, Laura E.M. Wisse, Jiancong Wang, Sadhana Ravikumar, Trevor\n  Glenn, Anica Luther, Sydney Lim, David A. Wolk, and Paul A. Yushkevich", "title": "Deep Label Fusion: A 3D End-to-End Hybrid Multi-Atlas Segmentation and\n  Deep Learning Pipeline", "comments": "12 pages paper accepted by the international conference of\n  Information Processing in Medical Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning (DL) is the state-of-the-art methodology in various medical\nimage segmentation tasks. However, it requires relatively large amounts of\nmanually labeled training data, which may be infeasible to generate in some\napplications. In addition, DL methods have relatively poor generalizability to\nout-of-sample data. Multi-atlas segmentation (MAS), on the other hand, has\npromising performance using limited amounts of training data and good\ngeneralizability. A hybrid method that integrates the high accuracy of DL and\ngood generalizability of MAS is highly desired and could play an important role\nin segmentation problems where manually labeled data is hard to generate. Most\nof the prior work focuses on improving single components of MAS using DL rather\nthan directly optimizing the final segmentation accuracy via an end-to-end\npipeline. Only one study explored this idea in binary segmentation of 2D\nimages, but it remains unknown whether it generalizes well to multi-class 3D\nsegmentation problems. In this study, we propose a 3D end-to-end hybrid\npipeline, named deep label fusion (DLF), that takes advantage of the strengths\nof MAS and DL. Experimental results demonstrate that DLF yields significant\nimprovements over conventional label fusion methods and U-Net, a direct DL\napproach, in the context of segmenting medial temporal lobe subregions using 3T\nT1-weighted and T2-weighted MRI. Further, when applied to an unseen similar\ndataset acquired in 7T, DLF maintains its superior performance, which\ndemonstrates its good generalizability.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 16:25:38 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Xie", "Long", ""], ["Wisse", "Laura E. M.", ""], ["Wang", "Jiancong", ""], ["Ravikumar", "Sadhana", ""], ["Glenn", "Trevor", ""], ["Luther", "Anica", ""], ["Lim", "Sydney", ""], ["Wolk", "David A.", ""], ["Yushkevich", "Paul A.", ""]]}, {"id": "2103.10895", "submitter": "Joakim Bruslund Haurum", "authors": "Joakim Bruslund Haurum and Thomas B. Moeslund", "title": "Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and\n  Benchmark", "comments": "CVPR 2021. Project webpage: https://vap.aau.dk/sewer-ml/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perhaps surprisingly sewerage infrastructure is one of the most costly\ninfrastructures in modern society. Sewer pipes are manually inspected to\ndetermine whether the pipes are defective. However, this process is limited by\nthe number of qualified inspectors and the time it takes to inspect a pipe.\nAutomatization of this process is therefore of high interest. So far, the\nsuccess of computer vision approaches for sewer defect classification has been\nlimited when compared to the success in other fields mainly due to the lack of\npublic datasets. To this end, in this work we present a large novel and\npublicly available multi-label classification dataset for image-based sewer\ndefect classification called Sewer-ML.\n  The Sewer-ML dataset consists of 1.3 million images annotated by professional\nsewer inspectors from three different utility companies across nine years.\nTogether with the dataset, we also present a benchmark algorithm and a novel\nmetric for assessing performance. The benchmark algorithm is a result of\nevaluating 12 state-of-the-art algorithms, six from the sewer defect\nclassification domain and six from the multi-label classification domain, and\ncombining the best performing algorithms. The novel metric is a\nclass-importance weighted F2 score, $\\text{F}2_{\\text{CIW}}$, reflecting the\neconomic impact of each class, used together with the normal pipe F1 score,\n$\\text{F}1_{\\text{Normal}}$. The benchmark algorithm achieves an\n$\\text{F}2_{\\text{CIW}}$ score of 55.11% and $\\text{F}1_{\\text{Normal}}$ score\nof 90.94%, leaving ample room for improvement on the Sewer-ML dataset. The\ncode, models, and dataset are available at the project page\nhttps://vap.aau.dk/sewer-ml/\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 16:32:37 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Haurum", "Joakim Bruslund", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2103.10919", "submitter": "O\\u{g}uzhan Fatih Kar", "authors": "Teresa Yeo, O\\u{g}uzhan Fatih Kar, Amir Zamir", "title": "Robustness via Cross-Domain Ensembles", "comments": "Project website at https://crossdomain-ensembles.epfl.ch/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for making neural network predictions robust to shifts\nfrom the training data distribution. The proposed method is based on making\npredictions via a diverse set of cues (called 'middle domains') and ensembling\nthem into one strong prediction. The premise of the idea is that predictions\nmade via different cues respond differently to a distribution shift, hence one\nshould be able to merge them into one robust final prediction. We perform the\nmerging in a straightforward but principled manner based on the uncertainty\nassociated with each prediction. The evaluations are performed using multiple\ntasks and datasets (Taskonomy, Replica, ImageNet, CIFAR) under a wide range of\nadversarial and non-adversarial distribution shifts which demonstrate the\nproposed method is considerably more robust than its standard learning\ncounterpart, conventional deep ensembles, and several other baselines.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:28:03 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Yeo", "Teresa", ""], ["Kar", "O\u011fuzhan Fatih", ""], ["Zamir", "Amir", ""]]}, {"id": "2103.10951", "submitter": "David Bau iii", "authors": "David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian,\n  Aude Oliva, Antonio Torralba", "title": "Paint by Word", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of zero-shot semantic image painting. Instead of\npainting modifications into an image using only concrete colors or a finite set\nof semantic concepts, we ask how to create semantic paint based on open\nfull-text descriptions: our goal is to be able to point to a location in a\nsynthesized image and apply an arbitrary new concept such as \"rustic\" or\n\"opulent\" or \"happy dog.\" To do this, our method combines a state-of-the art\ngenerative model of realistic images with a state-of-the-art text-image\nsemantic similarity network. We find that, to make large changes, it is\nimportant to use non-gradient methods to explore latent space, and it is\nimportant to relax the computations of the GAN to target changes to a specific\nregion. We conduct user studies to compare our methods to several baselines.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:59:08 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 05:46:17 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Bau", "David", ""], ["Andonian", "Alex", ""], ["Cui", "Audrey", ""], ["Park", "YeonHwan", ""], ["Jahanian", "Ali", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "2103.10957", "submitter": "Olivier H\\'enaff", "authors": "Olivier J. H\\'enaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van\n  den Oord, Oriol Vinyals, Jo\\~ao Carreira", "title": "Efficient Visual Pretraining with Contrastive Detection", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised pretraining has been shown to yield powerful representations\nfor transfer learning. These performance gains come at a large computational\ncost however, with state-of-the-art methods requiring an order of magnitude\nmore computation than supervised pretraining. We tackle this computational\nbottleneck by introducing a new self-supervised objective, contrastive\ndetection, which tasks representations with identifying object-level features\nacross augmentations. This objective extracts a rich learning signal per image,\nleading to state-of-the-art transfer performance from ImageNet to COCO, while\nrequiring up to 5x less pretraining. In particular, our strongest\nImageNet-pretrained model performs on par with SEER, one of the largest\nself-supervised systems to date, which uses 1000x more pretraining data.\nFinally, our objective seamlessly handles pretraining on more complex images\nsuch as those in COCO, closing the gap with supervised transfer learning from\nCOCO to PASCAL.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 14:05:12 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["H\u00e9naff", "Olivier J.", ""], ["Koppula", "Skanda", ""], ["Alayrac", "Jean-Baptiste", ""], ["Oord", "Aaron van den", ""], ["Vinyals", "Oriol", ""], ["Carreira", "Jo\u00e3o", ""]]}, {"id": "2103.10978", "submitter": "Akash Sengupta", "authors": "Akash Sengupta, Ignas Budvytis, Roberto Cipolla", "title": "Probabilistic 3D Human Shape and Pose Estimation from Multiple\n  Unconstrained Images in the Wild", "comments": "Accepted at CVPR 2021, 16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of 3D human body shape and pose estimation\nfrom RGB images. Recent progress in this field has focused on single images,\nvideo or multi-view images as inputs. In contrast, we propose a new task: shape\nand pose estimation from a group of multiple images of a human subject, without\nconstraints on subject pose, camera viewpoint or background conditions between\nimages in the group. Our solution to this task predicts distributions over SMPL\nbody shape and pose parameters conditioned on the input images in the group. We\nprobabilistically combine predicted body shape distributions from each image to\nobtain a final multi-image shape prediction. We show that the additional body\nshape information present in multi-image input groups improves 3D human shape\nestimation metrics compared to single-image inputs on the SSP-3D dataset and a\nprivate dataset of tape-measured humans. In addition, predicting distributions\nover 3D bodies allows us to quantify pose prediction uncertainty, which is\nuseful when faced with challenging input images with significant occlusion. Our\nmethod demonstrates meaningful pose uncertainty on the 3DPW dataset and is\ncompetitive with the state-of-the-art in terms of pose estimation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 18:32:16 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:17:30 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sengupta", "Akash", ""], ["Budvytis", "Ignas", ""], ["Cipolla", "Roberto", ""]]}, {"id": "2103.10982", "submitter": "Inchang Choi", "authors": "Yitong Jiang, Inchang Choi, Jun Jiang, Jinwei Gu", "title": "HDR Video Reconstruction with Tri-Exposure Quad-Bayer Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel high dynamic range (HDR) video reconstruction method with\nnew tri-exposure quad-bayer sensors. Thanks to the larger number of exposure\nsets and their spatially uniform deployment over a frame, they are more robust\nto noise and spatial artifacts than previous spatially varying exposure (SVE)\nHDR video methods. Nonetheless, the motion blur from longer exposures, the\nnoise from short exposures, and inherent spatial artifacts of the SVE methods\nremain huge obstacles. Additionally, temporal coherence must be taken into\naccount for the stability of video reconstruction. To tackle these challenges,\nwe introduce a novel network architecture that divides-and-conquers these\nproblems. In order to better adapt the network to the large dynamic range, we\nalso propose LDR-reconstruction loss that takes equal contributions from both\nthe highlighted and the shaded pixels of HDR frames. Through a series of\ncomparisons and ablation studies, we show that the tri-exposure quad-bayer with\nour solution is more optimal to capture than previous reconstruction methods,\nparticularly for the scenes with larger dynamic range and objects with motion.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 18:40:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jiang", "Yitong", ""], ["Choi", "Inchang", ""], ["Jiang", "Jun", ""], ["Gu", "Jinwei", ""]]}, {"id": "2103.10994", "submitter": "Elad Amrani", "authors": "Elad Amrani, Alex Bronstein", "title": "Self-Supervised Classification Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Self-Classifier -- a novel self-supervised end-to-end\nclassification neural network. Self-Classifier learns labels and\nrepresentations simultaneously in a single-stage end-to-end manner by\noptimizing for same-class prediction of two augmented views of the same sample.\nTo guarantee non-degenerate solutions (i.e., solutions where all labels are\nassigned to the same class), a uniform prior is asserted on the labels. We show\nmathematically that unlike the regular cross-entropy loss, our approach avoids\nsuch solutions. Self-Classifier is simple to implement and is scalable to\npractically unlimited amounts of data. Unlike other unsupervised classification\napproaches, it does not require any form of pre-training or the use of\nexpectation maximization algorithms, pseudo-labelling or external clustering.\nUnlike other contrastive learning representation learning approaches, it does\nnot require a memory bank or a second network. Despite its relative simplicity,\nour approach achieves comparable results to state-of-the-art performance with\nImageNet, CIFAR10 and CIFAR100 for its two objectives: unsupervised\nclassification and unsupervised representation learning. Furthermore, it is the\nfirst unsupervised end-to-end classification network to perform well on the\nlarge-scale ImageNet dataset. Code will be made available.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 19:29:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Amrani", "Elad", ""], ["Bronstein", "Alex", ""]]}, {"id": "2103.11002", "submitter": "Lakshmanan Nataraj", "authors": "Michael Goebel, Jason Bunk, Srinjoy Chattopadhyay, Lakshmanan Nataraj,\n  Shivkumar Chandrasekaran and B.S. Manjunath", "title": "Attribution of Gradient Based Adversarial Attacks for Reverse\n  Engineering of Deceptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) algorithms are susceptible to adversarial attacks and\ndeception both during training and deployment. Automatic reverse engineering of\nthe toolchains behind these adversarial machine learning attacks will aid in\nrecovering the tools and processes used in these attacks. In this paper, we\npresent two techniques that support automated identification and attribution of\nadversarial ML attack toolchains using Co-occurrence Pixel statistics and\nLaplacian Residuals. Our experiments show that the proposed techniques can\nidentify parameters used to generate adversarial samples. To the best of our\nknowledge, this is the first approach to attribute gradient based adversarial\nattacks and estimate their parameters. Source code and data is available at:\nhttps://github.com/michael-goebel/ei_red\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 19:55:00 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Goebel", "Michael", ""], ["Bunk", "Jason", ""], ["Chattopadhyay", "Srinjoy", ""], ["Nataraj", "Lakshmanan", ""], ["Chandrasekaran", "Shivkumar", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2103.11006", "submitter": "Mariano Rivera", "authors": "Hanna Ehrlich and Mariano Rivera", "title": "AxonNet: A self-supervised Deep Neural Network for Intravoxel Structure\n  Estimation from DW-MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a method for estimating intravoxel parameters from a DW-MRI based\non deep learning techniques. We show that neural networks (DNNs) have the\npotential to extract information from diffusion-weighted signals to reconstruct\ncerebral tracts. We present two DNN models: one that estimates the axonal\nstructure in the form of a voxel and the other to calculate the structure of\nthe central voxel using the voxel neighborhood. Our methods are based on a\nproposed parameter representation suitable for the problem. Since it is\npractically impossible to have real tagged data for any acquisition protocol,\nwe used a self-supervised strategy. Experiments with synthetic data and real\ndata show that our approach is competitive, and the computational times show\nthat our approach is faster than the SOTA methods, even if training times are\nconsidered. This computational advantage increases if we consider the\nprediction of multiple images with the same acquisition protocol.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 20:11:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ehrlich", "Hanna", ""], ["Rivera", "Mariano", ""]]}, {"id": "2103.11015", "submitter": "Mennatullah Siam M.S.", "authors": "Mennatullah Siam, Alex Kendall, Martin Jagersand", "title": "Video Class Agnostic Segmentation Benchmark for Autonomous Driving", "comments": "Accepted in WAD workshop, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation approaches are typically trained on large-scale data\nwith a closed finite set of known classes without considering unknown objects.\nIn certain safety-critical robotics applications, especially autonomous\ndriving, it is important to segment all objects, including those unknown at\ntraining time. We formalize the task of video class agnostic segmentation from\nmonocular video sequences in autonomous driving to account for unknown objects.\nVideo class agnostic segmentation can be formulated as an open-set or a motion\nsegmentation problem. We discuss both formulations and provide datasets and\nbenchmark different baseline approaches for both tracks. In the\nmotion-segmentation track we benchmark real-time joint panoptic and motion\ninstance segmentation, and evaluate the effect of ego-flow suppression. In the\nopen-set segmentation track we evaluate baseline methods that combine\nappearance, and geometry to learn prototypes per semantic class. We then\ncompare it to a model that uses an auxiliary contrastive loss to improve the\ndiscrimination between known and unknown objects. Datasets and models are\npublicly released at https://msiam.github.io/vca/.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 20:41:40 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 03:19:47 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Siam", "Mennatullah", ""], ["Kendall", "Alex", ""], ["Jagersand", "Martin", ""]]}, {"id": "2103.11017", "submitter": "Filiz Gurkan Golcuk", "authors": "Filiz Gurkan, Llukman Cerkezi, Ozgun Cirakman, Bilge Gunsel", "title": "TDIOT: Target-driven Inference for Deep Video Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent tracking-by-detection approaches use deep object detectors as target\ndetection baseline, because of their high performance on still images. For\neffective video object tracking, object detection is integrated with a data\nassociation step performed by either a custom design inference architecture or\nan end-to-end joint training for tracking purpose. In this work, we adopt the\nformer approach and use the pre-trained Mask R-CNN deep object detector as the\nbaseline. We introduce a novel inference architecture placed on top of\nFPN-ResNet101 backbone of Mask R-CNN to jointly perform detection and tracking,\nwithout requiring additional training for tracking purpose. The proposed single\nobject tracker, TDIOT, applies an appearance similarity-based temporal matching\nfor data association. In order to tackle tracking discontinuities, we\nincorporate a local search and matching module into the inference head layer\nthat exploits SiamFC for short term tracking. Moreover, in order to improve\nrobustness to scale changes, we introduce a scale adaptive region proposal\nnetwork that enables to search the target at an adaptively enlarged spatial\nneighborhood specified by the trace of the target. In order to meet long term\ntracking requirements, a low cost verification layer is incorporated into the\ninference architecture to monitor presence of the target based on its LBP\nhistogram model. Performance evaluation on videos from VOT2016, VOT2018 and\nVOT-LT2018 datasets demonstrate that TDIOT achieves higher accuracy compared to\nthe state-of-the-art short-term trackers while it provides comparable\nperformance in long term tracking.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 20:45:06 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:51:19 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Gurkan", "Filiz", ""], ["Cerkezi", "Llukman", ""], ["Cirakman", "Ozgun", ""], ["Gunsel", "Bilge", ""]]}, {"id": "2103.11031", "submitter": "Yihao Zhang", "authors": "Yihao Zhang and John J. Leonard", "title": "Bootstrapped Self-Supervised Training with Monocular Video for Semantic\n  Segmentation and Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a robot deployed in the world, it is desirable to have the ability of\nautonomous learning to improve its initial pre-set knowledge. We formalize this\nas a bootstrapped self-supervised learning problem where a system is initially\nbootstrapped with supervised training on a labeled dataset and we look for a\nself-supervised training method that can subsequently improve the system over\nthe supervised training baseline using only unlabeled data. In this work, we\nleverage temporal consistency between frames in monocular video to perform this\nbootstrapped self-supervised training. We show that a well-trained\nstate-of-the-art semantic segmentation network can be further improved through\nour method. In addition, we show that the bootstrapped self-supervised training\nframework can help a network learn depth estimation better than pure supervised\ntraining or self-supervised training.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 21:28:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Yihao", ""], ["Leonard", "John J.", ""]]}, {"id": "2103.11052", "submitter": "Piotr Tynecki", "authors": "Mateusz Choinski, Mateusz Rogowski, Piotr Tynecki, Dries P.J. Kuijper,\n  Marcin Churski, Jakub W. Bubnicki", "title": "A first step towards automated species recognition from camera trap\n  images of mammals using AI in a European temperate forest", "comments": "CISIM 2021 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camera traps are used worldwide to monitor wildlife. Despite the increasing\navailability of Deep Learning (DL) models, the effective usage of this\ntechnology to support wildlife monitoring is limited. This is mainly due to the\ncomplexity of DL technology and high computing requirements. This paper\npresents the implementation of the light-weight and state-of-the-art YOLOv5\narchitecture for automated labeling of camera trap images of mammals in the\nBialowieza Forest (BF), Poland. The camera trapping data were organized and\nharmonized using TRAPPER software, an open source application for managing\nlarge-scale wildlife monitoring projects. The proposed image recognition\npipeline achieved an average accuracy of 85% F1-score in the identification of\nthe 12 most commonly occurring medium-size and large mammal species in BF using\na limited set of training and testing data (a total 2659 images with animals).\n  Based on the preliminary results, we concluded that the YOLOv5 object\ndetection and classification model is a promising light-weight DL solution\nafter the adoption of transfer learning technique. It can be efficiently\nplugged in via an API into existing web-based camera trapping data processing\nplatforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage\nand classify (manually) camera trapping datasets by many research groups in\nEurope, the implementation of AI-based automated species classification may\nsignificantly speed up the data processing workflow and thus better support\ndata-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers\nperform better performance on edge devices which may open a new chapter in\nanimal population monitoring in real time directly from camera trap devices.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 22:48:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Choinski", "Mateusz", ""], ["Rogowski", "Mateusz", ""], ["Tynecki", "Piotr", ""], ["Kuijper", "Dries P. J.", ""], ["Churski", "Marcin", ""], ["Bubnicki", "Jakub W.", ""]]}, {"id": "2103.11056", "submitter": "Abu Md Niamul Taufique", "authors": "Abu Md Niamul Taufique, Chowdhury Sadman Jahan, Andreas Savakis", "title": "ConDA: Continual Unsupervised Domain Adaptation", "comments": "10pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation (DA) techniques are important for overcoming the domain\nshift between the source domain used for training and the target domain where\ntesting takes place. However, current DA methods assume that the entire target\ndomain is available during adaptation, which may not hold in practice. This\npaper considers a more realistic scenario, where target data become available\nin smaller batches and adaptation on the entire target domain is not feasible.\nIn our work, we introduce a new, data-constrained DA paradigm where unlabeled\ntarget samples are received in batches and adaptation is performed continually.\nWe propose a novel source-free method for continual unsupervised domain\nadaptation that utilizes a buffer for selective replay of previously seen\nsamples. In our continual DA framework, we selectively mix samples from\nincoming batches with data stored in a buffer using buffer management\nstrategies and use the combination to incrementally update our model. We\nevaluate the classification performance of the continual DA approach with\nstate-of-the-art DA methods based on the entire target domain. Our results on\nthree popular DA datasets demonstrate that our method outperforms many existing\nstate-of-the-art DA methods with access to the entire target domain during\nadaptation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 23:20:41 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 23:28:11 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Taufique", "Abu Md Niamul", ""], ["Jahan", "Chowdhury Sadman", ""], ["Savakis", "Andreas", ""]]}, {"id": "2103.11059", "submitter": "Abu Md Niamul Taufique", "authors": "Abu Md Niamul Taufique, Andreas Savakis, Jonathan Leckenby", "title": "Automatic Quantification of Facial Asymmetry using Facial Landmarks", "comments": "5 pages, 4 figures", "journal-ref": "2019 IEEE Western New York Image and Signal Processing Workshop\n  (WNYISPW)", "doi": "10.1109/WNYIPW.2019.8923078", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-sided facial paralysis causes uneven movements of facial muscles on the\nsides of the face. Physicians currently assess facial asymmetry in a subjective\nmanner based on their clinical experience. This paper proposes a novel method\nto provide an objective and quantitative asymmetry score for frontal faces. Our\nmetric has the potential to help physicians for diagnosis as well as monitoring\nthe rehabilitation of patients with one-sided facial paralysis. A deep learning\nbased landmark detection technique is used to estimate style invariant facial\nlandmark points and dense optical flow is used to generate motion maps from a\nshort sequence of frames. Six face regions are considered corresponding to the\nleft and right parts of the forehead, eyes, and mouth. Motion is computed and\ncompared between the left and the right parts of each region of interest to\nestimate the symmetry score. For testing, asymmetric sequences are\nsynthetically generated from a facial expression dataset. A score equation is\ndeveloped to quantify symmetry in both symmetric and asymmetric face sequences.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 00:08:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Taufique", "Abu Md Niamul", ""], ["Savakis", "Andreas", ""], ["Leckenby", "Jonathan", ""]]}, {"id": "2103.11061", "submitter": "Abu Md Niamul Taufique", "authors": "Abu Md Niamul Taufique, Navya Nagananda, Andreas Savakis", "title": "Visualization of Deep Transfer Learning In SAR Imagery", "comments": "4 pages, 5 figures", "journal-ref": "IGARSS 2020 - 2020 IEEE International Geoscience and Remote\n  Sensing Symposium", "doi": "10.1109/IGARSS39084.2020.9324490", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic Aperture Radar (SAR) imagery has diverse applications in land and\nmarine surveillance. Unlike electro-optical (EO) systems, these systems are not\naffected by weather conditions and can be used in the day and night times. With\nthe growing importance of SAR imagery, it would be desirable if models trained\non widely available EO datasets can also be used for SAR images. In this work,\nwe consider transfer learning to leverage deep features from a network trained\non an EO ships dataset and generate predictions on SAR imagery. Furthermore, by\nexploring the network activations in the form of class-activation maps (CAMs),\nwe visualize the transfer learning process to SAR imagery and gain insight on\nhow a deep network interprets a new modality.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 00:16:15 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Taufique", "Abu Md Niamul", ""], ["Nagananda", "Navya", ""], ["Savakis", "Andreas", ""]]}, {"id": "2103.11071", "submitter": "Yuguang Shi", "authors": "Yuguang Shi, Zhenqiang Mi, Yu Guo, Xinjie Li", "title": "Stereo CenterNet based 3D Object Detection for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, 3D detection based on stereo cameras has made great\nprogress, but most state-of-the-art methods use anchor-based 2D detection or\ndepth estimation to solve this problem. However, the high computational cost\nmakes these methods difficult to meet real-time performance. In this work, we\npropose a 3D object detection method using geometric information in stereo\nimages, called Stereo CenterNet. Stereo CenterNet predicts the four semantic\nkey points of the 3D bounding box of the object in space and uses 2D left right\nboxes, 3D dimension, orientation and key points to restore the bounding box of\nthe object in the 3D space. Then, we use an improved photometric alignment\nmodule to further optimize the position of the 3D bounding box. Experiments\nconducted on the KITTI dataset show that our method achieves the best\nspeed-accuracy trade-off compared with the state-of-the-art methods that\nwithout extra required data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 02:18:49 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 16:16:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shi", "Yuguang", ""], ["Mi", "Zhenqiang", ""], ["Guo", "Yu", ""], ["Li", "Xinjie", ""]]}, {"id": "2103.11078", "submitter": "Yudong Guo", "authors": "Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun Bao, Juyong Zhang", "title": "AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis", "comments": "Project: https://yudongguo.github.io/ADNeRF/ Code:\n  https://github.com/YudongGuo/AD-NeRF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating high-fidelity talking head video by fitting with the input audio\nsequence is a challenging problem that receives considerable attentions\nrecently. In this paper, we address this problem with the aid of neural scene\nrepresentation networks. Our method is completely different from existing\nmethods that rely on intermediate representations like 2D landmarks or 3D face\nmodels to bridge the gap between audio input and video output. Specifically,\nthe feature of input audio signal is directly fed into a conditional implicit\nfunction to generate a dynamic neural radiance field, from which a\nhigh-fidelity talking-head video corresponding to the audio signal is\nsynthesized using volume rendering. Another advantage of our framework is that\nnot only the head (with hair) region is synthesized as previous methods did,\nbut also the upper body is generated via two individual neural radiance fields.\nExperimental results demonstrate that our novel framework can (1) produce\nhigh-fidelity and natural results, and (2) support free adjustment of audio\nsignals, viewing directions, and background images.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 02:58:13 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 07:53:49 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Guo", "Yudong", ""], ["Chen", "Keyu", ""], ["Liang", "Sen", ""], ["Liu", "Yongjin", ""], ["Bao", "Hujun", ""], ["Zhang", "Juyong", ""]]}, {"id": "2103.11084", "submitter": "Jihua Zhu", "authors": "Jihua Zhu and Di Wang and Jiaxi Mu and Huimin Lu and Zhiqiang Tian and\n  Zhongyu Li", "title": "3DMNDT:3D multi-view registration method based on the normal\n  distributions transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The normal distributions transform (NDT) is an effective paradigm for the\npoint set registration. This method is originally designed for pair-wise\nregistration and it will suffer from great challenges when applied to\nmulti-view registration. Under the NDT framework, this paper proposes a novel\nmulti-view registration method, named 3D multi-view registration based on the\nnormal distributions transform (3DMNDT), which integrates the K-means\nclustering and Lie algebra solver to achieve multi-view registration. More\nspecifically, the multi-view registration is cast into the problem of maximum\nlikelihood estimation. Then, the K-means algorithm is utilized to divide all\ndata points into different clusters, where a normal distribution is computed to\nlocally models the probability of measuring a data point in each cluster.\nSubsequently, the registration problem is formulated by the NDT-based\nlikelihood function. To maximize this likelihood function, the Lie algebra\nsolver is developed to sequentially optimize each rigid transformation. The\nproposed method alternately implements data point clustering, NDT computing,\nand likelihood maximization until desired registration results are obtained.\nExperimental results tested on benchmark data sets illustrate that the proposed\nmethod can achieve state-of-the-art performance for multi-view registration.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 03:20:31 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhu", "Jihua", ""], ["Wang", "Di", ""], ["Mu", "Jiaxi", ""], ["Lu", "Huimin", ""], ["Tian", "Zhiqiang", ""], ["Li", "Zhongyu", ""]]}, {"id": "2103.11093", "submitter": "Ziqiang Li", "authors": "Ziqiang Li, Pengfei Xia, Xue Rui, Yanghui Hu, Bin Li", "title": "Are High-Frequency Components Beneficial for Training of Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advancements in Generative Adversarial Networks (GANs) have the ability to\ngenerate realistic images that are visually indistinguishable from real images.\nHowever, recent studies of the image spectrum have demonstrated that generated\nand real images share significant differences at high frequency. Furthermore,\nthe high-frequency components invisible to human eyes affect the decision of\nCNNs and are related to the robustness of it. Similarly, whether the\ndiscriminator will be sensitive to the high-frequency differences, thus\nreducing the fitting ability of the generator to the low-frequency components\nis an open problem. In this paper, we demonstrate that the discriminator in\nGANs is sensitive to such high-frequency differences that can not be\ndistinguished by humans and the high-frequency components of images are not\nconducive to the training of GANs. Based on these, we propose two preprocessing\nmethods eliminating high-frequency differences in GANs training: High-Frequency\nConfusion (HFC) and High-Frequency Filter (HFF). The proposed methods are\ngeneral and can be easily applied to most existing GANs frameworks with a\nfraction of the cost. The advanced performance of the proposed method is\nverified on multiple loss functions, network architectures, and datasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 04:37:06 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Ziqiang", ""], ["Xia", "Pengfei", ""], ["Rui", "Xue", ""], ["Hu", "Yanghui", ""], ["Li", "Bin", ""]]}, {"id": "2103.11099", "submitter": "Shiqi Lin", "authors": "Shiqi Lin, Tao Yu, Ruoyu Feng, Zhibo Chen", "title": "Patch AutoAugment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation (DA) plays a critical role in training deep neural networks\nfor improving the generalization of models. Recent work has shown that\nautomatic DA policy, such as AutoAugment (AA), significantly improves model\nperformance. However, most automatic DA methods search for DA policies at the\nimage-level without considering that the optimal policies for different regions\nin an image may be diverse. In this paper, we propose a patch-level automatic\nDA algorithm called Patch AutoAugment (PAA). PAA divides an image into a grid\nof patches and searches for the optimal DA policy of each patch. Specifically,\nPAA allows each patch DA operation to be controlled by an agent and models it\nas a Multi-Agent Reinforcement Learning (MARL) problem. At each step, PAA\nsamples the most effective operation for each patch based on its content and\nthe semantics of the whole image. The agents cooperate as a team and share a\nunified team reward for achieving the joint optimal DA policy of the whole\nimage. The experiment shows that PAA consistently improves the target network\nperformance on many benchmark datasets of image classification and fine-grained\nimage recognition. PAA also achieves remarkable computational efficiency, i.e\n2.3x faster than FastAA and 56.1x faster than AA on ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 05:10:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lin", "Shiqi", ""], ["Yu", "Tao", ""], ["Feng", "Ruoyu", ""], ["Chen", "Zhibo", ""]]}, {"id": "2103.11110", "submitter": "Khwaja Monib Sediqi", "authors": "Khwaja Monib Sediqi, and Hyo Jong Lee", "title": "A Novel Upsampling and Context Convolution for Image Semantic\n  Segmentation", "comments": "11 pages, published in sensors journal", "journal-ref": "Sensors 2021, 21, 2170", "doi": "10.3390/s21062170", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation, which refers to pixel-wise classification of an image,\nis a fundamental topic in computer vision owing to its growing importance in\nrobot vision and autonomous driving industries. It provides rich information\nabout objects in the scene such as object boundary, category, and location.\nRecent methods for semantic segmentation often employ an encoder-decoder\nstructure using deep convolutional neural networks. The encoder part extracts\nfeature of the image using several filters and pooling operations, whereas the\ndecoder part gradually recovers the low-resolution feature maps of the encoder\ninto a full input resolution feature map for pixel-wise prediction. However,\nthe encoder-decoder variants for semantic segmentation suffer from severe\nspatial information loss, caused by pooling operations or convolutions with\nstride, and does not consider the context in the scene. In this paper, we\npropose a dense upsampling convolution method based on guided filtering to\neffectively preserve the spatial information of the image in the network. We\nfurther propose a novel local context convolution method that not only covers\nlarger-scale objects in the scene but covers them densely for precise object\nboundary delineation. Theoretical analyses and experimental results on several\nbenchmark datasets verify the effectiveness of our method. Qualitatively, our\napproach delineates object boundaries at a level of accuracy that is beyond the\ncurrent excellent methods. Quantitatively, we report a new record of 82.86% and\n81.62% of pixel accuracy on ADE20K and Pascal-Context benchmark datasets,\nrespectively. In comparison with the state-of-the-art methods, the proposed\nmethod offers promising improvements.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 06:16:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sediqi", "Khwaja Monib", ""], ["Lee", "Hyo Jong", ""]]}, {"id": "2103.11112", "submitter": "Jacopo Cavazza", "authors": "Jacopo Cavazza", "title": "Classifier Crafting: Turn Your ConvNet into a Zero-Shot Learner!", "comments": "8 pages (excluding references), 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Zero-shot learning (ZSL), we classify unseen categories using textual\ndescriptions about their expected appearance when observed (class embeddings)\nand a disjoint pool of seen classes, for which annotated visual data are\naccessible. We tackle ZSL by casting a \"vanilla\" convolutional neural network\n(e.g. AlexNet, ResNet-101, DenseNet-201 or DarkNet-53) into a zero-shot\nlearner. We do so by crafting the softmax classifier: we freeze its weights\nusing fixed seen classification rules, either semantic (seen class embeddings)\nor visual (seen class prototypes). Then, we learn a data-driven and\nZSL-tailored feature representation on seen classes only to match these fixed\nclassification rules. Given that the latter seamlessly generalize towards\nunseen classes, while requiring not actual unseen data to be computed, we can\nperform ZSL inference by augmenting the pool of classification rules at test\ntime while keeping the very same representation we learnt: nowhere re-training\nor fine-tuning on unseen data is performed. The combination of semantic and\nvisual crafting (by simply averaging softmax scores) improves prior\nstate-of-the-art methods in benchmark datasets for standard, inductive ZSL.\nAfter rebalancing predictions to better handle the joint inference over seen\nand unseen classes, we outperform prior generalized, inductive ZSL methods as\nwell. Also, we gain interpretability at no additional cost, by using neural\nattention methods (e.g., grad-CAM) as they are. Code will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 06:26:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Cavazza", "Jacopo", ""]]}, {"id": "2103.11114", "submitter": "Zhenhong Zou", "authors": "Zhenhong Zou, Xinyu Zhang, Huaping Liu, Zhiwei Li, Amir Hussain and\n  Jun Li", "title": "A novel multimodal fusion network based on a joint coding model for lane\n  line segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been growing interest in utilizing multimodal sensors to\nachieve robust lane line segmentation. In this paper, we introduce a novel\nmultimodal fusion architecture from an information theory perspective, and\ndemonstrate its practical utility using Light Detection and Ranging (LiDAR)\ncamera fusion networks. In particular, we develop, for the first time, a\nmultimodal fusion network as a joint coding model, where each single node,\nlayer, and pipeline is represented as a channel. The forward propagation is\nthus equal to the information transmission in the channels. Then, we can\nqualitatively and quantitatively analyze the effect of different fusion\napproaches. We argue the optimal fusion architecture is related to the\nessential capacity and its allocation based on the source and channel. To test\nthis multimodal fusion hypothesis, we progressively determine a series of\nmultimodal models based on the proposed fusion methods and evaluate them on the\nKITTI and the A2D2 datasets. Our optimal fusion network achieves 85%+ lane line\naccuracy and 98.7%+ overall. The performance gap among the models will inform\ncontinuing future research into development of optimal fusion algorithms for\nthe deep multimodal learning community.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 06:47:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zou", "Zhenhong", ""], ["Zhang", "Xinyu", ""], ["Liu", "Huaping", ""], ["Li", "Zhiwei", ""], ["Hussain", "Amir", ""], ["Li", "Jun", ""]]}, {"id": "2103.11119", "submitter": "Yiwei Bao", "authors": "Yiwei Bao, Yihua Cheng, Yunfei Liu and Feng Lu", "title": "Adaptive Feature Fusion Network for Gaze Tracking in Mobile Tablets", "comments": "Accepted at International Conference on Pattern Recognition 2020\n  (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many multi-stream gaze estimation methods have been proposed. They\nestimate gaze from eye and face appearances and achieve reasonable accuracy.\nHowever, most of the methods simply concatenate the features extracted from eye\nand face appearance. The feature fusion process has been ignored. In this\npaper, we propose a novel Adaptive Feature Fusion Network (AFF-Net), which\nperforms gaze tracking task in mobile tablets. We stack two-eye feature maps\nand utilize Squeeze-and-Excitation layers to adaptively fuse two-eye features\naccording to their similarity on appearance. Meanwhile, we also propose\nAdaptive Group Normalization to recalibrate eye features with the guidance of\nfacial feature. Extensive experiments on both GazeCapture and MPIIFaceGaze\ndatasets demonstrate consistently superior performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 07:16:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bao", "Yiwei", ""], ["Cheng", "Yihua", ""], ["Liu", "Yunfei", ""], ["Lu", "Feng", ""]]}, {"id": "2103.11135", "submitter": "Martin Pernu\\v{s}", "authors": "Martin Pernu\\v{s}, Vitomir \\v{S}truc, Simon Dobri\\v{s}ek", "title": "High Resolution Face Editing with Masked GAN Latent Code Optimization", "comments": "The updated paper will be submitted to IEEE Transactions on Image\n  Processing. Added more qualitative and quantitative results to the main part\n  of the paper. This version now also includes the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face editing represents a popular research topic within the computer vision\nand image processing communities. While significant progress has been made\nrecently in this area, existing solutions: (i) are still largely focused on\nlow-resolution images, (ii) often generate editing results with visual\nartefacts, or (iii) lack fine-grained control and alter multiple (entangled)\nattributes at once, when trying to generate the desired facial semantics. In\nthis paper, we aim to address these issues though a novel attribute editing\napproach called MaskFaceGAN. The proposed approach is based on an optimization\nprocedure that directly optimizes the latent code of a pre-trained\n(state-of-the-art) Generative Adversarial Network (i.e., StyleGAN2) with\nrespect to several constraints that ensure: (i) preservation of relevant image\ncontent, (ii) generation of the targeted facial attributes, and (iii)\nspatially--selective treatment of local image areas. The constraints are\nenforced with the help of an (differentiable) attribute classifier and face\nparser that provide the necessary reference information for the optimization\nprocedure. MaskFaceGAN is evaluated in extensive experiments on the CelebA-HQ,\nHelen and SiblingsDB-HQf datasets and in comparison with several\nstate-of-the-art techniques from the literature, i.e., StarGAN, AttGAN, STGAN,\nand two versions of InterFaceGAN. Our experimental results show that the\nproposed approach is able to edit face images with respect to several facial\nattributes with unprecedented image quality and at high-resolutions\n(1024x1024), while exhibiting considerably less problems with attribute\nentanglement than competing solutions. The source code is made freely available\nfrom: https://github.com/MartinPernus/MaskFaceGAN.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 08:39:41 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 09:35:33 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Pernu\u0161", "Martin", ""], ["\u0160truc", "Vitomir", ""], ["Dobri\u0161ek", "Simon", ""]]}, {"id": "2103.11139", "submitter": "Yang Liu", "authors": "Yang Liu, Fei Wang, Baigui Sun, Hao Li", "title": "MogFace: Rethinking Scale Augmentation on the Face Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face detector frequently confronts extreme scale variance challenge. The\nfamous solutions are Multi-scale training, Data-anchor-sampling and Random crop\nstrategy. In this paper, we indicate 2 significant elements to resolve extreme\nscale variance problem by investigating the difference among the previous\nsolutions, including the fore-ground and back-ground information of an image\nand the scale information. However, current excellent solutions can only\nutilize the former information while neglecting to absorb the latter one\neffectively. In order to help the detector utilize the scale information\nefficiently, we analyze the relationship between the detector performance and\nthe scale distribution of the training data. Based on this analysis, we propose\na Selective Scale Enhancement (SSE) strategy which can assimilate these two\ninformation efficiently and simultaneously. Finally, our method achieves\nstate-of-the-art detection performance on all common face detection benchmarks,\nincluding AFW, PASCAL face, FDDB and Wider Face datasets. Note that our result\nachieves six champions on the Wider Face dataset.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 09:17:04 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 03:08:44 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 07:32:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Yang", ""], ["Wang", "Fei", ""], ["Sun", "Baigui", ""], ["Li", "Hao", ""]]}, {"id": "2103.11145", "submitter": "Alberto Testoni", "authors": "Alberto Testoni, Raffaella Bernardi", "title": "Overprotective Training Environments Fall Short at Testing Time: Let\n  Models Contribute to Their Own Training", "comments": "This paper has been published in the Proceedings of the Seventh\n  Italian Conference on Computational Linguistics, CLiC-it 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite important progress, conversational systems often generate dialogues\nthat sound unnatural to humans. We conjecture that the reason lies in their\ndifferent training and testing conditions: agents are trained in a controlled\n\"lab\" setting but tested in the \"wild\". During training, they learn to generate\nan utterance given the human dialogue history. On the other hand, during\ntesting, they must interact with each other, and hence deal with noisy data. We\npropose to fill this gap by training the model with mixed batches containing\nboth samples of human and machine-generated dialogues. We assess the validity\nof the proposed method on GuessWhat?!, a visual referential game.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 09:55:50 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:16:52 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Testoni", "Alberto", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "2103.11161", "submitter": "Sinisa Stekovic", "authors": "Sinisa Stekovic, Mahdi Rad, Friedrich Fraundorfer, Vincent Lepetit", "title": "MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor\n  Plans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for reconstructing floor plans from noisy 3D point\nclouds. Our main contribution is a principled approach that relies on the Monte\nCarlo Tree Search (MCTS) algorithm to maximize a suitable objective function\nefficiently despite the complexity of the problem. Like previous work, we first\nproject the input point cloud to a top view to create a density map and extract\nroom proposals from it. Our method selects and optimizes the polygonal shapes\nof these room proposals jointly to fit the density map and outputs an accurate\nvectorized floor map even for large complex scenes. To do this, we adapted\nMCTS, an algorithm originally designed to learn to play games, to select the\nroom proposals by maximizing an objective function combining the fitness with\nthe density map as predicted by a deep network and regularizing terms on the\nroom shapes. We also introduce a refinement step to MCTS that adjusts the shape\nof the room proposals. For this step, we propose a novel differentiable method\nfor rendering the polygonal shapes of these proposals. We evaluate our method\non the recent and challenging Structured3D and Floor-SP datasets and show a\nsignificant improvement over the state-of-the-art, without imposing any hard\nconstraints nor assumptions on the floor plan configurations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 11:36:49 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Stekovic", "Sinisa", ""], ["Rad", "Mahdi", ""], ["Fraundorfer", "Friedrich", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2103.11166", "submitter": "Xin Ding", "authors": "Xin Ding, Yongwei Wang, Z. Jane Wang, William J. Welch", "title": "Efficient Subsampling for Generating High-Quality Images from\n  Conditional Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Subsampling unconditional generative adversarial networks (GANs) to improve\nthe overall image quality has been studied recently. However, these methods\noften require high training costs (e.g., storage space, parameter tuning) and\nmay be inefficient or even inapplicable for subsampling conditional GANs, such\nas class-conditional GANs and continuous conditional GANs (CcGANs), when the\ncondition has many distinct values. In this paper, we propose an efficient\nmethod called conditional density ratio estimation in feature space with\nconditional Softplus loss (cDRE-F-cSP). With cDRE-F-cSP, we estimate an image's\nconditional density ratio based on a novel conditional Softplus (cSP) loss in\nthe feature space learned by a specially designed ResNet-34 or sparse\nautoencoder. We then derive the error bound of a conditional density ratio\nmodel trained with the proposed cSP loss. Finally, we propose a rejection\nsampling scheme, termed cDRE-F-cSP+RS, which can subsample both\nclass-conditional GANs and CcGANs efficiently. An extra filtering scheme is\nalso developed for CcGANs to increase the label consistency. Experiments on\nCIFAR-10 and Tiny-ImageNet datasets show that cDRE-F-cSP+RS can substantially\nimprove the Intra-FID and FID scores of BigGAN. Experiments on RC-49 and\nUTKFace datasets demonstrate that cDRE-F-cSP+RS also improves Intra-FID,\nDiversity, and Label Score of CcGANs. Moreover, to show the high efficiency of\ncDRE-F-cSP+RS, we compare it with the state-of-the-art unconditional\nsubsampling method (i.e., DRE-F-SP+RS). With comparable or even better\nperformance, cDRE-F-cSP+RS only requires about \\textbf{10}\\% and \\textbf{1.7}\\%\nof the training costs spent respectively on CIFAR-10 and UTKFace by\nDRE-F-SP+RS.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 12:19:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ding", "Xin", ""], ["Wang", "Yongwei", ""], ["Wang", "Z. Jane", ""], ["Welch", "William J.", ""]]}, {"id": "2103.11169", "submitter": "Jogendra Nath Kundu", "authors": "Naveen Venkat, Jogendra Nath Kundu, Durgesh Kumar Singh, Ambareesh\n  Revanur, R. Venkatesh Babu", "title": "Your Classifier can Secretly Suffice Multi-Source Domain Adaptation", "comments": "NeurIPS 2020. Project page: https://sites.google.com/view/simpal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-Source Domain Adaptation (MSDA) deals with the transfer of task\nknowledge from multiple labeled source domains to an unlabeled target domain,\nunder a domain-shift. Existing methods aim to minimize this domain-shift using\nauxiliary distribution alignment objectives. In this work, we present a\ndifferent perspective to MSDA wherein deep models are observed to implicitly\nalign the domains under label supervision. Thus, we aim to utilize implicit\nalignment without additional training objectives to perform adaptation. To this\nend, we use pseudo-labeled target samples and enforce a classifier agreement on\nthe pseudo-labels, a process called Self-supervised Implicit Alignment\n(SImpAl). We find that SImpAl readily works even under category-shift among the\nsource domains. Further, we propose classifier agreement as a cue to determine\nthe training convergence, resulting in a simple training algorithm. We provide\na thorough evaluation of our approach on five benchmarks, along with detailed\ninsights into each component of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 12:44:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Venkat", "Naveen", ""], ["Kundu", "Jogendra Nath", ""], ["Singh", "Durgesh Kumar", ""], ["Revanur", "Ambareesh", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2103.11186", "submitter": "Brent Harrison", "authors": "Chengxi Li and Brent Harrison", "title": "3M: Multi-style image caption generation using Multi-modality features\n  under Multi-UPDOWN model", "comments": "To be published at FLAIRS-34", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we build a multi-style generative model for stylish image\ncaptioning which uses multi-modality image features, ResNeXt features and text\nfeatures generated by DenseCap. We propose the 3M model, a Multi-UPDOWN caption\nmodel that encodes multi-modality features and decode them to captions. We\ndemonstrate the effectiveness of our model on generating human-like captions by\nexamining its performance on two datasets, the PERSONALITY-CAPTIONS dataset and\nthe FlickrStyle10K dataset. We compare against a variety of state-of-the-art\nbaselines on various automatic NLP metrics such as BLEU, ROUGE-L, CIDEr, SPICE,\netc. A qualitative study has also been done to verify our 3M model can be used\nfor generating different stylized captions.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 14:12:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Chengxi", ""], ["Harrison", "Brent", ""]]}, {"id": "2103.11190", "submitter": "Yue Lu", "authors": "Congqi Cao, Yue Lu, Yifan Zhang, Dongmei Jiang and Yanning Zhang", "title": "Efficient Spatialtemporal Context Modeling for Action Recognition", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information plays an important role in action recognition. Local\noperations have difficulty to model the relation between two elements with a\nlong-distance interval. However, directly modeling the contextual information\nbetween any two points brings huge cost in computation and memory, especially\nfor action recognition, where there is an additional temporal dimension.\nInspired from 2D criss-cross attention used in segmentation task, we propose a\nrecurrent 3D criss-cross attention (RCCA-3D) module to model the dense\nlong-range spatiotemporal contextual information in video for action\nrecognition. The global context is factorized into sparse relation maps. We\nmodel the relationship between points in the same line along the direction of\nhorizon, vertical and depth at each time, which forms a 3D criss-cross\nstructure, and duplicate the same operation with recurrent mechanism to\ntransmit the relation between points in a line to a plane finally to the whole\nspatiotemporal space. Compared with the non-local method, the proposed RCCA-3D\nmodule reduces the number of parameters and FLOPs by 25% and 30% for video\ncontext modeling. We evaluate the performance of RCCA-3D with two latest action\nrecognition networks on three datasets and make a thorough analysis of the\narchitecture, obtaining the optimal way to factorize and fuse the relation\nmaps. Comparisons with other state-of-the-art methods demonstrate the\neffectiveness and efficiency of our model.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 14:48:12 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 04:40:12 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Cao", "Congqi", ""], ["Lu", "Yue", ""], ["Zhang", "Yifan", ""], ["Jiang", "Dongmei", ""], ["Zhang", "Yanning", ""]]}, {"id": "2103.11204", "submitter": "Patrick Wenzel", "authors": "Qadeer Khan, Patrick Wenzel, Daniel Cremers", "title": "Self-Supervised Steering Angle Prediction for Vehicle Control Using\n  Visual Odometry", "comments": "Accepted at International Conference on Artificial Intelligence and\n  Statistics (AISTATS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based learning methods for self-driving cars have primarily used\nsupervised approaches that require a large number of labels for training.\nHowever, those labels are usually difficult and expensive to obtain. In this\npaper, we demonstrate how a model can be trained to control a vehicle's\ntrajectory using camera poses estimated through visual odometry methods in an\nentirely self-supervised fashion. We propose a scalable framework that\nleverages trajectory information from several different runs using a camera\nsetup placed at the front of a car. Experimental results on the CARLA simulator\ndemonstrate that our proposed approach performs at par with the model trained\nwith supervision.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 16:29:01 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Khan", "Qadeer", ""], ["Wenzel", "Patrick", ""], ["Cremers", "Daniel", ""]]}, {"id": "2103.11210", "submitter": "Maria H\\\"anel Dr.", "authors": "Maria L. H\\\"anel and Carola-B. Sch\\\"onlieb", "title": "Efficient Global Optimization of Non-differentiable, Symmetric\n  Objectives for Multi Camera Placement", "comments": "Submitted to be reviewed, 10 pages, 6 figures, 2 tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a novel iterative method for optimally placing and orienting\nmultiple cameras in a 3D scene. Sample applications include improving the\naccuracy of 3D reconstruction, maximizing the covered area for surveillance, or\nimproving the coverage in multi-viewpoint pedestrian tracking. Our algorithm is\nbased on a block-coordinate ascent combined with a surrogate function and an\nexclusion area technique. This allows to flexibly handle difficult objective\nfunctions that are often expensive and quantized or non-differentiable. The\nsolver is globally convergent and easily parallelizable. We show how to\naccelerate the optimization by exploiting special properties of the objective\nfunction, such as symmetry. Additionally, we discuss the trade-off between\nnon-optimal stationary points and the cost reduction when optimizing the\nviewpoints consecutively.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 17:01:15 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["H\u00e4nel", "Maria L.", ""], ["Sch\u00f6nlieb", "Carola-B.", ""]]}, {"id": "2103.11211", "submitter": "Maria H\\\"anel Dr.", "authors": "Maria L. H\\\"anel and Johannes V\\\"olkel and Dominik Henrich", "title": "Multi Camera Placement via Z-buffer Rendering for the Optimization of\n  the Coverage and the Visual Hull", "comments": "8 pages, 9 figures, not reviewed yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We can only allow human-robot-cooperation in a common work cell if the human\nintegrity is guaranteed. A surveillance system with multiple cameras can detect\ncollisions without contact to the human collaborator. A failure safe system\nneeds to optimally cover the important areas of the robot work cell with safety\noverlap. We propose an efficient algorithm for optimally placing and orienting\nthe cameras in a 3D CAD model of the work cell. In order to evaluate the\nquality of the camera constellation in each step, our method simulates the\nvision system using a z-buffer rendering technique for image acquisition, a\nvoxel space for the overlap and a refined visual hull method for a conservative\nhuman reconstruction. The simulation allows to evaluate the quality with\nrespect to the distortion of images and advanced image analysis in the presence\nof static and dynamic visual obstacles such as tables, racks, walls, robots and\npeople. Our method is ideally suited for maximizing the coverage of multiple\ncameras or minimizing an error made by the visual hull and can be extended to\nprobabilistic space carving.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 17:04:00 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["H\u00e4nel", "Maria L.", ""], ["V\u00f6lkel", "Johannes", ""], ["Henrich", "Dominik", ""]]}, {"id": "2103.11216", "submitter": "Steven Damelin Dr", "authors": "Gurpreet S. Kalsi and Steven B. Damelin", "title": "Preprocessing power weighted shortest path data using a s-Well Separated\n  Pair Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $s$ $>$ 0, we consider an algorithm that computes all $s$-well separated\npairs in certain point sets in $\\mathbb{R}^{n}$, $n$ $>1$. For an integer $K$\n$>1$, we also consider an algorithm that is a permutation of Dijkstra's\nalgorithm, that computes $K$-nearest neighbors using a certain power weighted\nshortest path metric in $\\mathbb{R}^{n}$, $n$ $>$ $1$. We describe each\nalgorithm and their respective dependencies on the input data. We introduce a\nway to combine both algorithms into a fused algorithm. Several open problems\nare given for future research.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 17:38:13 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 20:27:09 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kalsi", "Gurpreet S.", ""], ["Damelin", "Steven B.", ""]]}, {"id": "2103.11241", "submitter": "Alvaro Leandro Cavalcante Carneiro", "authors": "Alvaro Leandro Cavalcante Carneiro, Lucas de Brito Silva, Marisa\n  Silveira Almeida Renaud Faulin", "title": "Artificial intelligence for detection and quantification of rust and\n  leaf miner in coffee crop", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pest and disease control plays a key role in agriculture since the damage\ncaused by these agents are responsible for a huge economic loss every year.\nBased on this assumption, we create an algorithm capable of detecting rust\n(Hemileia vastatrix) and leaf miner (Leucoptera coffeella) in coffee leaves\n(Coffea arabica) and quantify disease severity using a mobile application as a\nhigh-level interface for the model inferences. We used different convolutional\nneural network architectures to create the object detector, besides the OpenCV\nlibrary, k-means, and three treatments: the RGB and value to quantification,\nand the AFSoft software, in addition to the analysis of variance, where we\ncompare the three methods. The results show an average precision of 81,5% in\nthe detection and that there was no significant statistical difference between\ntreatments to quantify the severity of coffee leaves, proposing a\ncomputationally less costly method. The application, together with the trained\nmodel, can detect the pest and disease over different image conditions and\ninfection stages and also estimate the disease infection stage.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 20:52:11 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 22:41:10 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Carneiro", "Alvaro Leandro Cavalcante", ""], ["Silva", "Lucas de Brito", ""], ["Faulin", "Marisa Silveira Almeida Renaud", ""]]}, {"id": "2103.11247", "submitter": "Aviad Moreshet", "authors": "Aviad Moreshet, Yosi Keller", "title": "Paying Attention to Multiscale Feature Maps in Multimodal Image Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an attention-based approach for multimodal image patch matching\nusing a Transformer encoder attending to the feature maps of a multiscale\nSiamese CNN. Our encoder is shown to efficiently aggregate multiscale image\nembeddings while emphasizing task-specific appearance-invariant image cues. We\nalso introduce an attention-residual architecture, using a residual connection\nbypassing the encoder. This additional learning signal facilitates end-to-end\ntraining from scratch. Our approach is experimentally shown to achieve new\nstate-of-the-art accuracy on both multimodal and single modality benchmarks,\nillustrating its general applicability. To the best of our knowledge, this is\nthe first successful implementation of the Transformer encoder architecture to\nthe multimodal image patch matching task.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 21:14:24 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Moreshet", "Aviad", ""], ["Keller", "Yosi", ""]]}, {"id": "2103.11257", "submitter": "Zifan Wang", "authors": "Zifan Wang, Matt Fredrikson, Anupam Datta", "title": "Boundary Attributions Provide Normal (Vector) Explanations", "comments": "\\", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on explaining Deep Neural Networks (DNNs) focuses on attributing\nthe model's output scores to input features. However, when it comes to\nclassification problems, a more fundamental question is how much does each\nfeature contributes to the model's decision to classify an input instance into\na specific class. Our first contribution is Boundary Attribution, a new\nexplanation method to address this question. BA leverages an understanding of\nthe geometry of activation regions. Specifically, they involve computing (and\naggregating) normal vectors of the local decision boundaries for the target\ninput. Our second contribution is a set of analytical results connecting the\nadversarial robustness of the network and the quality of gradient-based\nexplanations. Specifically, we prove two theorems for ReLU networks: BA of\nrandomized smoothed networks or robustly trained networks is much closer to\nnon-boundary attribution methods than that in standard networks. These\nanalytics encourage users to improve model robustness for high-quality\nexplanations. Finally, we evaluate the proposed methods on ImageNet and show\nBAs produce more concentrated and sharper visualizations compared with\nnon-boundary ones. We further demonstrate that our method also helps to reduce\nthe sensitivity of attributions to the baseline input if one is required.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 22:36:39 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 16:06:51 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wang", "Zifan", ""], ["Fredrikson", "Matt", ""], ["Datta", "Anupam", ""]]}, {"id": "2103.11264", "submitter": "M. Saquib Sarfraz", "authors": "M. Saquib Sarfraz, Naila Murray, Vivek Sharma, Ali Diba, Luc Van Gool,\n  Rainer Stiefelhagen", "title": "Temporally-Weighted Hierarchical Clustering for Unsupervised Action\n  Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action segmentation refers to inferring boundaries of semantically consistent\nvisual concepts in videos and is an important requirement for many video\nunderstanding tasks. For this and other video understanding tasks, supervised\napproaches have achieved encouraging performance but require a high volume of\ndetailed frame-level annotations. We present a fully automatic and unsupervised\napproach for segmenting actions in a video that does not require any training.\nOur proposal is an effective temporally-weighted hierarchical clustering\nalgorithm that can group semantically consistent frames of the video. Our main\nfinding is that representing a video with a 1-nearest neighbor graph by taking\ninto account the time progression is sufficient to form semantically and\ntemporally consistent clusters of frames where each cluster may represent some\naction in the video. Additionally, we establish strong unsupervised baselines\nfor action segmentation and show significant performance improvements over\npublished unsupervised methods on five challenging action segmentation\ndatasets. Our code is available at\nhttps://github.com/ssarfraz/FINCH-Clustering/tree/master/TW-FINCH\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 23:30:01 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:16:53 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 12:39:28 GMT"}, {"version": "v4", "created": "Sat, 27 Mar 2021 16:40:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sarfraz", "M. Saquib", ""], ["Murray", "Naila", ""], ["Sharma", "Vivek", ""], ["Diba", "Ali", ""], ["Van Gool", "Luc", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2103.11271", "submitter": "Vuong M. Ngo", "authors": "Vuong M. Ngo and Sven Helmer and Nhien-An Le-Khac and M-Tahar Kechadi", "title": "Structural Textile Pattern Recognition and Processing Based on\n  Hypergraphs", "comments": "38 pages, 23 figures", "journal-ref": "Information Retrieval Journal, Springer, 2021", "doi": "10.1007/s10791-020-09384-y", "report-no": null, "categories": "cs.IR cs.CC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The humanities, like many other areas of society, are currently undergoing\nmajor changes in the wake of digital transformation. However, in order to make\ncollection of digitised material in this area easily accessible, we often still\nlack adequate search functionality. For instance, digital archives for textiles\noffer keyword search, which is fairly well understood, and arrange their\ncontent following a certain taxonomy, but search functionality at the level of\nthread structure is still missing. To facilitate the clustering and search, we\nintroduce an approach for recognising similar weaving patterns based on their\nstructures for textile archives. We first represent textile structures using\nhypergraphs and extract multisets of k-neighbourhoods describing weaving\npatterns from these graphs. Then, the resulting multisets are clustered using\nvarious distance measures and various clustering algorithms (K-Means for\nsimplicity and hierarchical agglomerative algorithms for precision). We\nevaluate the different variants of our approach experimentally, showing that\nthis can be implemented efficiently (meaning it has linear complexity), and\ndemonstrate its quality to query and cluster datasets containing large textile\nsamples. As, to the est of our knowledge, this is the first practical approach\nfor explicitly modelling complex and irregular weaving patterns usable for\nretrieval, we aim at establishing a solid baseline.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 00:44:40 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ngo", "Vuong M.", ""], ["Helmer", "Sven", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "2103.11276", "submitter": "Erkan Kayacan", "authors": "Zhongzhong Zhang, Erkan Kayacan, Benjamin Thompson and Girish\n  Chowdhary", "title": "High precision control and deep learning-based corn stand counting\n  algorithms for agricultural robot", "comments": "14 pages, 9 figures", "journal-ref": "Autonomous Robots, volume 44, pages 1289-1302, 2020", "doi": "10.1007/s10514-020-09915-y", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents high precision control and deep learning-based corn stand\ncounting algorithms for a low-cost, ultra-compact 3D printed and autonomous\nfield robot for agricultural operations. Currently, plant traits, such as\nemergence rate, biomass, vigor, and stand counting, are measured manually. This\nis highly labor-intensive and prone to errors. The robot, termed TerraSentia,\nis designed to automate the measurement of plant traits for efficient\nphenotyping as an alternative to manual measurements. In this paper, we\nformulate a Nonlinear Moving Horizon Estimator (NMHE) that identifies key\nterrain parameters using onboard robot sensors and a learning-based Nonlinear\nModel Predictive Control (NMPC) that ensures high precision path tracking in\nthe presence of unknown wheel-terrain interaction. Moreover, we develop a\nmachine vision algorithm designed to enable an ultra-compact ground robot to\ncount corn stands by driving through the fields autonomously. The algorithm\nleverages a deep network to detect corn plants in images, and a visual tracking\nmodel to re-identify detected objects at different time steps. We collected\ndata from 53 corn plots in various fields for corn plants around 14 days after\nemergence (stage V3 - V4). The robot predictions have agreed well with the\nground truth with $C_{robot}=1.02 \\times C_{human}-0.86$ and a correlation\ncoefficient $R=0.96$. The mean relative error given by the algorithm is\n$-3.78\\%$, and the standard deviation is $6.76\\%$. These results indicate a\nfirst and significant step towards autonomous robot-based real-time phenotyping\nusing low-cost, ultra-compact ground robots for corn and potentially other\ncrops.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 01:13:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Zhongzhong", ""], ["Kayacan", "Erkan", ""], ["Thompson", "Benjamin", ""], ["Chowdhary", "Girish", ""]]}, {"id": "2103.11285", "submitter": "Charles (A.) Kantor", "authors": "Charles A. Kantor, Marta Skreta, Brice Rauby, L\\'eonard Boussioux,\n  Emmanuel Jehanno, Alexandra Luccioni, David Rolnick, Hugues Talbot", "title": "Geo-Spatiotemporal Features and Shape-Based Prior Knowledge for\n  Fine-grained Imbalanced Data Classification", "comments": "Copyright by the authors. All rights reserved to authors only.\n  Correspondence to: ckantor (at) stanford [dot] edu", "journal-ref": "Proc. IJCAI 2021, Workshop on AI for Social Good, Harvard\n  University (2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fine-grained classification aims at distinguishing between items with similar\nglobal perception and patterns, but that differ by minute details. Our primary\nchallenges come from both small inter-class variations and large intra-class\nvariations. In this article, we propose to combine several innovations to\nimprove fine-grained classification within the use-case of wildlife, which is\nof practical interest for experts. We utilize geo-spatiotemporal data to enrich\nthe picture information and further improve the performance. We also\ninvestigate state-of-the-art methods for handling the imbalanced data issue.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 02:01:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kantor", "Charles A.", ""], ["Skreta", "Marta", ""], ["Rauby", "Brice", ""], ["Boussioux", "L\u00e9onard", ""], ["Jehanno", "Emmanuel", ""], ["Luccioni", "Alexandra", ""], ["Rolnick", "David", ""], ["Talbot", "Hugues", ""]]}, {"id": "2103.11298", "submitter": "Kaihao Zhang", "authors": "Kaihao Zhang, Rongqing Li, Yanjiang Yu, Wenhan Luo, Changsheng Li,\n  Hongdong Li", "title": "Deep Dense Multi-scale Network for Snow Removal Using Semantic and\n  Geometric Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured in snowy days suffer from noticeable degradation of scene\nvisibility, which degenerates the performance of current vision-based\nintelligent systems. Removing snow from images thus is an important topic in\ncomputer vision. In this paper, we propose a Deep Dense Multi-Scale Network\n(\\textbf{DDMSNet}) for snow removal by exploiting semantic and geometric\npriors. As images captured in outdoor often share similar scenes and their\nvisibility varies with depth from camera, such semantic and geometric\ninformation provides a strong prior for snowy image restoration. We incorporate\nthe semantic and geometric maps as input and learn the semantic-aware and\ngeometry-aware representation to remove snow. In particular, we first create a\ncoarse network to remove snow from the input images. Then, the coarsely\ndesnowed images are fed into another network to obtain the semantic and\ngeometric labels. Finally, we design a DDMSNet to learn semantic-aware and\ngeometry-aware representation via a self-attention mechanism to produce the\nfinal clean images. Experiments evaluated on public synthetic and real-world\nsnowy images verify the superiority of the proposed method, offering better\nresults both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 03:30:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Kaihao", ""], ["Li", "Rongqing", ""], ["Yu", "Yanjiang", ""], ["Luo", "Wenhan", ""], ["Li", "Changsheng", ""], ["Li", "Hongdong", ""]]}, {"id": "2103.11299", "submitter": "Keval Doshi", "authors": "Keval Doshi and Yasin Yilmaz", "title": "A Modular and Unified Framework for Detecting and Localizing Video\n  Anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection in videos has been attracting an increasing amount of\nattention. Despite the competitive performance of recent methods on benchmark\ndatasets, they typically lack desirable features such as modularity,\ncross-domain adaptivity, interpretability, and real-time anomalous event\ndetection. Furthermore, current state-of-the-art approaches are evaluated using\nthe standard instance-based detection metric by considering video frames as\nindependent instances, which is not ideal for video anomaly detection.\nMotivated by these research gaps, we propose a modular and unified approach to\nthe online video anomaly detection and localization problem, called MOVAD,\nwhich consists of a novel transfer learning based plug-and-play architecture, a\nsequential anomaly detector, a mathematical framework for selecting the\ndetection threshold, and a suitable performance metric for real-time anomalous\nevent detection in videos. Extensive performance evaluations on benchmark\ndatasets show that the proposed framework significantly outperforms the current\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 04:16:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Doshi", "Keval", ""], ["Yilmaz", "Yasin", ""]]}, {"id": "2103.11313", "submitter": "Bo Pang", "authors": "Bo Pang, Gao Peng, Yizhuo Li, Cewu Lu", "title": "PGT: A Progressive Method for Training Models on Long Videos", "comments": "CVPR21, Oral", "journal-ref": "CVPR2021 oral", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional video models have an order of magnitude larger computational\ncomplexity than their counterpart image-level models. Constrained by\ncomputational resources, there is no model or training method that can train\nlong video sequences end-to-end. Currently, the main-stream method is to split\na raw video into clips, leading to incomplete fragmentary temporal information\nflow. Inspired by natural language processing techniques dealing with long\nsentences, we propose to treat videos as serial fragments satisfying Markov\nproperty, and train it as a whole by progressively propagating information\nthrough the temporal dimension in multiple steps. This progressive training\n(PGT) method is able to train long videos end-to-end with limited resources and\nensures the effective transmission of information. As a general and robust\ntraining method, we empirically demonstrate that it yields significant\nperformance improvements on different models and datasets. As an illustrative\nexample, the proposed method improves SlowOnly network by 3.7 mAP on Charades\nand 1.9 top-1 accuracy on Kinetics with negligible parameter and computation\noverhead. Code is available at https://github.com/BoPang1996/PGT.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 06:15:20 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pang", "Bo", ""], ["Peng", "Gao", ""], ["Li", "Yizhuo", ""], ["Lu", "Cewu", ""]]}, {"id": "2103.11314", "submitter": "Menghan Xia", "authors": "Menghan Xia, Jose Echevarria, Minshan Xie and Tien-Tsin Wong", "title": "A Learned Compact and Editable Light Field Representation", "comments": "submitted to TIP since 2020.08.03", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light fields are 4D scene representation typically structured as arrays of\nviews, or several directional samples per pixel in a single view. This highly\ncorrelated structure is not very efficient to transmit and manipulate\n(especially for editing), though. To tackle these problems, we present a novel\ncompact and editable light field representation, consisting of a set of visual\nchannels (i.e. the central RGB view) and a complementary meta channel that\nencodes the residual geometric and appearance information. The visual channels\nin this representation can be edited using existing 2D image editing tools,\nbefore accurately reconstructing the whole edited light field back. We propose\nto learn this representation via an autoencoder framework, consisting of an\nencoder for learning the representation, and a decoder for reconstructing the\nlight field. To handle the challenging occlusions and propagation of edits, we\nspecifically designed an editing-aware decoding network and its associated\ntraining strategy, so that the edits to the visual channels can be consistently\npropagated to the whole light field upon reconstruction.Experimental results\nshow that our proposed method outperforms related existing methods in\nreconstruction accuracy, and achieves visually pleasant performance in editing\npropagation.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 06:16:11 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Xia", "Menghan", ""], ["Echevarria", "Jose", ""], ["Xie", "Minshan", ""], ["Wong", "Tien-Tsin", ""]]}, {"id": "2103.11319", "submitter": "Guoqing Zhang", "authors": "Guoqing Zhang, Yuhao Chen, Yang Dai, Yuhui Zheng, Yi Wu", "title": "Reference-Aided Part-Aligned Feature Disentangling for Video Person\n  Re-Identification", "comments": "6 pages, accepted by ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, video-based person re-identification (re-ID) has drawn increasing\nattention in compute vision community because of its practical application\nprospects. Due to the inaccurate person detections and pose changes, pedestrian\nmisalignment significantly increases the difficulty of feature extraction and\nmatching. To address this problem, in this paper, we propose a\n\\textbf{R}eference-\\textbf{A}ided \\textbf{P}art-\\textbf{A}ligned\n(\\textbf{RAPA}) framework to disentangle robust features of different parts.\nFirstly, in order to obtain better references between different videos, a\npose-based reference feature learning module is introduced. Secondly, an\neffective relation-based part feature disentangling module is explored to align\nframes within each video. By means of using both modules, the informative parts\nof pedestrian in videos are well aligned and more discriminative feature\nrepresentation is generated. Comprehensive experiments on three widely-used\nbenchmarks, i.e. iLIDS-VID, PRID-2011 and MARS datasets verify the\neffectiveness of the proposed framework. Our code will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 06:53:57 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Guoqing", ""], ["Chen", "Yuhao", ""], ["Dai", "Yang", ""], ["Zheng", "Yuhui", ""], ["Wu", "Yi", ""]]}, {"id": "2103.11322", "submitter": "Sundara Tejaswi Digumarti", "authors": "S. Tejaswi Digumarti (1 and 2), Joseph Daniel (1), Ahalya Ravendran (1\n  and 2), Donald G. Dansereau (1 and 2) ((1) School of Aerospace, Mechanical\n  and Mechatronic Engineering, The University of Sydney, (2) Sydney Institute\n  for Robotics and Intelligent Systems)", "title": "Unsupervised Learning of Depth Estimation and Visual Odometry for Sparse\n  Light Field Cameras", "comments": "Submitted to IROS 2021, 8 pages, 6 figures, 2 tables, for associated\n  project page, see https://roboticimaging.org/Projects/LearnLFOdo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While an exciting diversity of new imaging devices is emerging that could\ndramatically improve robotic perception, the challenges of calibrating and\ninterpreting these cameras have limited their uptake in the robotics community.\nIn this work we generalise techniques from unsupervised learning to allow a\nrobot to autonomously interpret new kinds of cameras. We consider emerging\nsparse light field (LF) cameras, which capture a subset of the 4D LF function\ndescribing the set of light rays passing through a plane. We introduce a\ngeneralised encoding of sparse LFs that allows unsupervised learning of\nodometry and depth. We demonstrate the proposed approach outperforming\nmonocular and conventional techniques for dealing with 4D imagery, yielding\nmore accurate odometry and depth maps and delivering these with metric scale.\nWe anticipate our technique to generalise to a broad class of LF and sparse LF\ncameras, and to enable unsupervised recalibration for coping with shifts in\ncamera behaviour over the lifetime of a robot. This work represents a first\nstep toward streamlining the integration of new kinds of imaging devices in\nrobotics applications.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 07:13:14 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Digumarti", "S. Tejaswi", "", "1 and 2"], ["Daniel", "Joseph", "", "1\n  and 2"], ["Ravendran", "Ahalya", "", "1\n  and 2"], ["Dansereau", "Donald G.", "", "1 and 2"]]}, {"id": "2103.11351", "submitter": "Yousong Zhu", "authors": "Li Wang, Dong Li, Yousong Zhu, Lu Tian, Yi Shan", "title": "Cross-Dataset Collaborative Learning for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work attempts to improve semantic segmentation performance by\nexploring well-designed architectures on a target dataset. However, it remains\nchallenging to build a unified system that simultaneously learns from various\ndatasets due to the inherent distribution shift across different datasets. In\nthis paper, we present a simple, flexible, and general method for semantic\nsegmentation, termed Cross-Dataset Collaborative Learning (CDCL). Given\nmultiple labeled datasets, we aim to improve the generalization and\ndiscrimination of feature representations on each dataset. Specifically, we\nfirst introduce a family of Dataset-Aware Blocks (DAB) as the fundamental\ncomputing units of the network, which help capture homogeneous representations\nand heterogeneous statistics across different datasets. Second, we propose a\nDataset Alternation Training (DAT) mechanism to efficiently facilitate the\noptimization procedure. We conduct extensive evaluations on four diverse\ndatasets, i.e., Cityscapes, BDD100K, CamVid, and COCO Stuff, with\nsingle-dataset and cross-dataset settings. Experimental results demonstrate our\nmethod consistently achieves notable improvements over prior single-dataset and\ncross-dataset training methods without introducing extra FLOPs. Particularly,\nwith the same architecture of PSPNet (ResNet-18), our method outperforms the\nsingle-dataset baseline by 5.65\\%, 6.57\\%, and 5.79\\% of mIoU on the validation\nsets of Cityscapes, BDD100K, CamVid, respectively. Code and models will be\nreleased.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 09:59:47 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 01:48:51 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Li", ""], ["Li", "Dong", ""], ["Zhu", "Yousong", ""], ["Tian", "Lu", ""], ["Shan", "Yi", ""]]}, {"id": "2103.11362", "submitter": "Jelica Vasiljevi\\'c", "authors": "Jelica Vasiljevi\\'c, Friedrich Feuerhake, C\\'edric Wemmert, Thomas\n  Lampert", "title": "Self adversarial attack as an augmentation method for\n  immunohistochemical stainings", "comments": "Accepted to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It has been shown that unpaired image-to-image translation methods\nconstrained by cycle-consistency hide the information necessary for accurate\ninput reconstruction as imperceptible noise. We demonstrate that, when applied\nto histopathology data, this hidden noise appears to be related to stain\nspecific features and show that this is the case with two immunohistochemical\nstainings during translation to Periodic acid- Schiff (PAS), a histochemical\nstaining method commonly applied in renal pathology. Moreover, by perturbing\nthis hidden information, the translation models produce different, plausible\noutputs. We demonstrate that this property can be used as an augmentation\nmethod which, in a case of supervised glomeruli segmentation, leads to improved\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:48:40 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Vasiljevi\u0107", "Jelica", ""], ["Feuerhake", "Friedrich", ""], ["Wemmert", "C\u00e9dric", ""], ["Lampert", "Thomas", ""]]}, {"id": "2103.11372", "submitter": "Sadaf Gulshad", "authors": "Sadaf Gulshad and Arnold Smeulders", "title": "Natural Perturbed Training for General Robustness of Neural Network\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the robustness of neural networks for classification. To permit a\nfair comparison between methods to achieve robustness, we first introduce a\nstandard based on the mensuration of a classifier's degradation. Then, we\npropose natural perturbed training to robustify the network. Natural\nperturbations will be encountered in practice: the difference of two images of\nthe same object may be approximated by an elastic deformation (when they have\nslightly different viewing angles), by occlusions (when they hide differently\nbehind objects), or by saturation, Gaussian noise etc. Training some fraction\nof the epochs on random versions of such variations will help the classifier to\nlearn better. We conduct extensive experiments on six datasets of varying sizes\nand granularity. Natural perturbed learning show better and much faster\nperformance than adversarial training on clean, adversarial as well as natural\nperturbed images. It even improves general robustness on perturbations not seen\nduring the training. For Cifar-10 and STL-10 natural perturbed training even\nimproves the accuracy for clean data and reaches the state of the art\nperformance. Ablation studies verify the effectiveness of natural perturbed\ntraining.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 11:47:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Gulshad", "Sadaf", ""], ["Smeulders", "Arnold", ""]]}, {"id": "2103.11373", "submitter": "Praveen Chopra Mr", "authors": "Praveen Chopra", "title": "ProgressiveSpinalNet architecture for FC layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In deeplearning models the FC (fully connected) layer has biggest important\nrole for classification of the input based on the learned features from\nprevious layers. The FC layers has highest numbers of parameters and\nfine-tuning these large numbers of parameters, consumes most of the\ncomputational resources, so in this paper it is aimed to reduce these large\nnumbers of parameters significantly with improved performance. The motivation\nis inspired from SpinalNet and other biological architecture. The proposed\narchitecture has a gradient highway between input to output layers and this\nsolves the problem of diminishing gradient in deep networks. In this all the\nlayers receives the input from previous layers as well as the CNN layer output\nand this way all layers contribute in decision making with last layer. This\napproach has improved classification performance over the SpinalNet\narchitecture and has SOTA performance on many datasets such as Caltech101,\nKMNIST, QMNIST and EMNIST. The source code is available at\nhttps://github.com/praveenchopra/ProgressiveSpinalNet.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 11:54:50 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chopra", "Praveen", ""]]}, {"id": "2103.11374", "submitter": "Kowshik Thopalli", "authors": "Zachary Seymour, Kowshik Thopalli, Niluthpol Mithun, Han-Pang Chiu,\n  Supun Samarasekera, Rakesh Kumar", "title": "MaAST: Map Attention with Semantic Transformersfor Efficient Visual\n  Navigation", "comments": "6 pages, 5 figures, accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual navigation for autonomous agents is a core task in the fields of\ncomputer vision and robotics. Learning-based methods, such as deep\nreinforcement learning, have the potential to outperform the classical\nsolutions developed for this task; however, they come at a significantly\nincreased computational load. Through this work, we design a novel approach\nthat focuses on performing better or comparable to the existing learning-based\nsolutions but under a clear time/computational budget. To this end, we propose\na method to encode vital scene semantics such as traversable paths, unexplored\nareas, and observed scene objects -- alongside raw visual streams such as RGB,\ndepth, and semantic segmentation masks -- into a semantically informed,\ntop-down egocentric map representation. Further, to enable the effective use of\nthis information, we introduce a novel 2-D map attention mechanism, based on\nthe successful multi-layer Transformer networks. We conduct experiments on 3-D\nreconstructed indoor PointGoal visual navigation and demonstrate the\neffectiveness of our approach. We show that by using our novel attention schema\nand auxiliary rewards to better utilize scene semantics, we outperform multiple\nbaselines trained with only raw inputs or implicit semantic information while\noperating with an 80% decrease in the agent's experience.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 12:01:23 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Seymour", "Zachary", ""], ["Thopalli", "Kowshik", ""], ["Mithun", "Niluthpol", ""], ["Chiu", "Han-Pang", ""], ["Samarasekera", "Supun", ""], ["Kumar", "Rakesh", ""]]}, {"id": "2103.11383", "submitter": "Huaxiong Li", "authors": "Haoxing Chen and Huaxiong Li and Yaohui Li and Chunlin Chen", "title": "Multi-level Metric Learning for Few-shot Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning is devoted to training a model on few samples. Recently,\nthe method based on local descriptor metric-learning has achieved great\nperformance. Most of these approaches learn a model based on a pixel-level\nmetric. However, such works can only measure the relations between them on a\nsingle level, which is not comprehensive and effective. We argue that if query\nimages can simultaneously be well classified via three distinct level\nsimilarity metrics, the query images within a class can be more tightly\ndistributed in a smaller feature space, generating more discriminative feature\nmaps. Motivated by this, we propose a novel Multi-level Metric Learning (MML)\nmethod for few-shot learning, which not only calculates the pixel-level\nsimilarity but also considers the similarity of part-level features and the\nsimilarity of distributions. First, we use a feature extractor to get the\nfeature maps of images. Second, a multi-level metric module is proposed to\ncalculate the part-level, pixel-level, and distribution-level similarities\nsimultaneously. Specifically, the distribution-level similarity metric\ncalculates the distribution distance (i.e., Wasserstein distance,\nKullback-Leibler divergence) between query images and the support set, the\npixel-level, and the part-level metric calculates the pixel-level and\npart-level similarities respectively. Finally, the fusion layer fuses three\nkinds of relation scores to obtain the final similarity score. Extensive\nexperiments on popular benchmarks demonstrate that the MML method significantly\noutperforms the current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 12:49:07 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 05:04:27 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 05:06:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Haoxing", ""], ["Li", "Huaxiong", ""], ["Li", "Yaohui", ""], ["Chen", "Chunlin", ""]]}, {"id": "2103.11384", "submitter": "Huaxiong Li", "authors": "Yaohui Li and Huaxiong Li and Haoxing Chen and Chunlin Chen", "title": "Hierarchical Representation based Query-Specific Prototypical Network\n  for Few-Shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot image classification aims at recognizing unseen categories with a\nsmall number of labeled training data. Recent metric-based frameworks tend to\nrepresent a support class by a fixed prototype (e.g., the mean of the support\ncategory) and make classification according to the similarities between query\ninstances and support prototypes. However, discriminative dominant regions may\nlocate uncertain areas of images and have various scales, which leads to the\nmisaligned metric. Besides, a fixed prototype for one support category cannot\nfit for all query instances to accurately reflect their distances with this\ncategory, which lowers the efficiency of metric. Therefore, query-specific\ndominant regions in support samples should be extracted for a high-quality\nmetric. To address these problems, we propose a Hierarchical Representation\nbased Query-Specific Prototypical Network (QPN) to tackle the limitations by\ngenerating a region-level prototype for each query sample, which achieves both\npositional and dimensional semantic alignment simultaneously. Extensive\nexperiments conducted on five benchmark datasets (including three fine-grained\ndatasets) show that our proposed method outperforms the current\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 12:50:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Yaohui", ""], ["Li", "Huaxiong", ""], ["Chen", "Haoxing", ""], ["Chen", "Chunlin", ""]]}, {"id": "2103.11390", "submitter": "Gijs Van Tulder", "authors": "Gijs van Tulder, Yao Tong, Elena Marchiori", "title": "Multi-view analysis of unregistered medical images using cross-view\n  transformers", "comments": "Submitted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view medical image analysis often depends on the combination of\ninformation from multiple views. However, differences in perspective or other\nforms of misalignment can make it difficult to combine views effectively, as\nregistration is not always possible. Without registration, views can only be\ncombined at a global feature level, by joining feature vectors after global\npooling. We present a novel cross-view transformer method to transfer\ninformation between unregistered views at the level of spatial feature maps. We\ndemonstrate this method on multi-view mammography and chest X-ray datasets. On\nboth datasets, we find that a cross-view transformer that links spatial feature\nmaps can outperform a baseline model that joins feature vectors after global\npooling.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 13:29:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["van Tulder", "Gijs", ""], ["Tong", "Yao", ""], ["Marchiori", "Elena", ""]]}, {"id": "2103.11395", "submitter": "Ragav Sachdeva", "authors": "Ragav Sachdeva, Filipe R Cordeiro, Vasileios Belagiannis, Ian Reid,\n  Gustavo Carneiro", "title": "ScanMix: Learning from Severe Label Noise via Semantic Clustering and\n  Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of training deep neural networks in the\npresence of severe label noise. Our proposed training algorithm ScanMix,\ncombines semantic clustering with semi-supervised learning (SSL) to improve the\nfeature representations and enable an accurate identification of noisy samples,\neven in severe label noise scenarios. To be specific, ScanMix is designed based\non the expectation maximisation (EM) framework, where the E-step estimates the\nvalue of a latent variable to cluster the training images based on their\nappearance representations and classification results, and the M-step optimises\nthe SSL classification and learns effective feature representations via\nsemantic clustering. In our evaluations, we show state-of-the-art results on\nstandard benchmarks for symmetric, asymmetric and semantic label noise on\nCIFAR-10 and CIFAR-100, as well as large scale real label noise on WebVision.\nMost notably, for the benchmarks contaminated with large noise rates (80% and\nabove), our results are up to 27% better than the related work. The code is\navailable at https://github.com/ragavsachdeva/ScanMix.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 13:43:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sachdeva", "Ragav", ""], ["Cordeiro", "Filipe R", ""], ["Belagiannis", "Vasileios", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2103.11399", "submitter": "Zongqi Wei", "authors": "Dong Liang, Zongqi Wei, Dong Zhang, Qixiang Geng, Liyan Zhang, Han\n  Sun, Huiyu Zhou, Mingqiang Wei, Pan Gao", "title": "Learning Calibrated-Guidance for Object Detection in Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the study on object detection in aerial images has made tremendous\nprogress in the community of computer vision. However, most state-of-the-art\nmethods tend to develop elaborate attention mechanisms for the space-time\nfeature calibrations with high computational complexity, while surprisingly\nignoring the importance of feature calibrations in channels. In this work, we\npropose a simple yet effective Calibrated-Guidance (CG) scheme to enhance\nchannel communications in a feature transformer fashion, which can adaptively\ndetermine the calibration weights for each channel based on the global feature\naffinity-pairs. Specifically, given a set of feature maps, CG first computes\nthe feature similarity between each channel and the remaining channels as the\nintermediary calibration guidance. Then, re-representing each channel by\naggregating all the channels weighted together via the guidance. Our CG can be\nplugged into any deep neural network, which is named as CG-Net. To demonstrate\nits effectiveness and efficiency, extensive experiments are carried out on both\noriented and horizontal object detection tasks of aerial images. Results on two\nchallenging benchmarks (i.e., DOTA and HRSC2016) demonstrate that our CG-Net\ncan achieve state-of-the-art performance in accuracy with a fair computational\noverhead. https://github.com/WeiZongqi/CG-Net\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 13:55:46 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liang", "Dong", ""], ["Wei", "Zongqi", ""], ["Zhang", "Dong", ""], ["Geng", "Qixiang", ""], ["Zhang", "Liyan", ""], ["Sun", "Han", ""], ["Zhou", "Huiyu", ""], ["Wei", "Mingqiang", ""], ["Gao", "Pan", ""]]}, {"id": "2103.11402", "submitter": "Qiang Zhou", "authors": "Qiang Zhou, Chaohui Yu, Zhibin Wang, Qi Qian, Hao Li", "title": "Instant-Teaching: An End-to-End Semi-Supervised Object Detection\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised learning based object detection frameworks demand plenty of\nlaborious manual annotations, which may not be practical in real applications.\nSemi-supervised object detection (SSOD) can effectively leverage unlabeled data\nto improve the model performance, which is of great significance for the\napplication of object detection models. In this paper, we revisit SSOD and\npropose Instant-Teaching, a completely end-to-end and effective SSOD framework,\nwhich uses instant pseudo labeling with extended weak-strong data augmentations\nfor teaching during each training iteration. To alleviate the confirmation bias\nproblem and improve the quality of pseudo annotations, we further propose a\nco-rectify scheme based on Instant-Teaching, denoted as Instant-Teaching$^*$.\nExtensive experiments on both MS-COCO and PASCAL VOC datasets substantiate the\nsuperiority of our framework. Specifically, our method surpasses\nstate-of-the-art methods by 4.2 mAP on MS-COCO when using $2\\%$ labeled data.\nEven with full supervised information of MS-COCO, the proposed method still\noutperforms state-of-the-art methods by about 1.0 mAP. On PASCAL VOC, we can\nachieve more than 5 mAP improvement by applying VOC07 as labeled data and VOC12\nas unlabeled data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 14:03:36 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Qiang", ""], ["Yu", "Chaohui", ""], ["Wang", "Zhibin", ""], ["Qian", "Qi", ""], ["Li", "Hao", ""]]}, {"id": "2103.11424", "submitter": "Mingjie Luo", "authors": "Mingjie Luo, Siwei Wang, Xinwang Liu, Wenxuan Tu, Yi Zhang, Xifeng\n  Guo, Sihang Zhou and En Zhu", "title": "Deep Distribution-preserving Incomplete Clustering with Optimal\n  Transport", "comments": "Data are provided at\n  https://github.com/wangsiwei2010/Single-view-incomplete-datasets-for-deep-clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering is a fundamental task in the computer vision and machine learning\ncommunity. Although various methods have been proposed, the performance of\nexisting approaches drops dramatically when handling incomplete\nhigh-dimensional data (which is common in real world applications). To solve\nthe problem, we propose a novel deep incomplete clustering method, named Deep\nDistribution-preserving Incomplete Clustering with Optimal Transport (DDIC-OT).\nTo avoid insufficient sample utilization in existing methods limited by few\nfully-observed samples, we propose to measure distribution distance with the\noptimal transport for reconstruction evaluation instead of traditional\npixel-wise loss function. Moreover, the clustering loss of the latent feature\nis introduced to regularize the embedding with more discrimination capability.\nAs a consequence, the network becomes more robust against missing features and\nthe unified framework which combines clustering and sample imputation enables\nthe two procedures to negotiate to better serve for each other. Extensive\nexperiments demonstrate that the proposed network achieves superior and stable\nclustering performance improvement against existing state-of-the-art incomplete\nclustering methods over different missing ratios.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 15:43:17 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Luo", "Mingjie", ""], ["Wang", "Siwei", ""], ["Liu", "Xinwang", ""], ["Tu", "Wenxuan", ""], ["Zhang", "Yi", ""], ["Guo", "Xifeng", ""], ["Zhou", "Sihang", ""], ["Zhu", "En", ""]]}, {"id": "2103.11436", "submitter": "Gholamreza Anbarjafari", "authors": "Artem Domnich and Gholamreza Anbarjafari", "title": "Responsible AI: Gender bias assessment in emotion recognition", "comments": "19 pages, 31 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid development of artificial intelligence (AI) systems amplify many\nconcerns in society. These AI algorithms inherit different biases from humans\ndue to mysterious operational flow and because of that it is becoming adverse\nin usage. As a result, researchers have started to address the issue by\ninvestigating deeper in the direction towards Responsible and Explainable AI.\nAmong variety of applications of AI, facial expression recognition might not be\nthe most important one, yet is considered as a valuable part of human-AI\ninteraction. Evolution of facial expression recognition from the feature based\nmethods to deep learning drastically improve quality of such algorithms. This\nresearch work aims to study a gender bias in deep learning methods for facial\nexpression recognition by investigating six distinct neural networks, training\nthem, and further analysed on the presence of bias, according to the three\ndefinition of fairness. The main outcomes show which models are gender biased,\nwhich are not and how gender of subject affects its emotion recognition. More\nbiased neural networks show bigger accuracy gap in emotion recognition between\nmale and female test sets. Furthermore, this trend keeps for true positive and\nfalse positive rates. In addition, due to the nature of the research, we can\nobserve which types of emotions are better classified for men and which for\nwomen. Since the topic of biases in facial expression recognition is not well\nstudied, a spectrum of continuation of this research is truly extensive, and\nmay comprise detail analysis of state-of-the-art methods, as well as targeting\nother biases.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 17:00:21 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Domnich", "Artem", ""], ["Anbarjafari", "Gholamreza", ""]]}, {"id": "2103.11438", "submitter": "Viktor Kocur", "authors": "Viktor Kocur, Milan Ft\\'a\\v{c}nik", "title": "Traffic Camera Calibration via Vehicle Vanishing Point Detection", "comments": "submitted to ICANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a traffic surveillance camera calibration method\nbased on detection of pairs of vanishing points associated with vehicles in the\ntraffic surveillance footage. To detect the vanishing points we propose a CNN\nwhich outputs heatmaps in which the positions of vanishing points are\nrepresented using the diamond space parametrization which enables us to detect\nvanishing points from the whole infinite projective space. From the detected\npairs of vanishing points for multiple vehicles in a scene we establish the\nscene geometry by estimating the focal length of the camera and the orientation\nof the road plane. We show that our method achieves competitive results on the\nBrnoCarPark dataset while having fewer requirements than the current state of\nthe art approach.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 17:11:59 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kocur", "Viktor", ""], ["Ft\u00e1\u010dnik", "Milan", ""]]}, {"id": "2103.11460", "submitter": "\\.Ibrahim Deliba\\c{s}o\\u{g}lu", "authors": "Ibrahim Delibasoglu", "title": "UAV Images Dataset for Moving Object Detection from Moving Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new high resolution aerial images dataset in which\nmoving objects are labelled manually. It aims to contribute to the evaluation\nof the moving object detection methods for moving cameras. The problem of\nrecognizing moving objects from aerial images is one of the important issues in\ncomputer vision. The biggest problem in the images taken by UAV is that the\nbackground is constantly variable due to camera movement. There are various\ndatasets in the literature in which proposed methods for motion detection are\nevaluated. Prepared dataset consists of challenging images containing small\ntargets compared to other datasets. Two methods in the literature have been\ntested for the prepared dataset. In addition, a simpler method compared to\nthese methods has been proposed for moving object object in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 18:44:38 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 09:42:45 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Delibasoglu", "Ibrahim", ""]]}, {"id": "2103.11468", "submitter": "Yoli Shavit", "authors": "Yoli Shavit, Ron Ferens, Yosi Keller", "title": "Learning Multi-Scene Absolute Pose Regression with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Absolute camera pose regressors estimate the position and orientation of a\ncamera from the captured image alone. Typically, a convolutional backbone with\na multi-layer perceptron head is trained with images and pose labels to embed a\nsingle reference scene at a time. Recently, this scheme was extended for\nlearning multiple scenes by replacing the MLP head with a set of fully\nconnected layers. In this work, we propose to learn multi-scene absolute camera\npose regression with Transformers, where encoders are used to aggregate\nactivation maps with self-attention and decoders transform latent features and\nscenes encoding into candidate pose predictions. This mechanism allows our\nmodel to focus on general features that are informative for localization while\nembedding multiple scenes in parallel. We evaluate our method on commonly\nbenchmarked indoor and outdoor datasets and show that it surpasses both\nmulti-scene and state-of-the-art single-scene absolute pose regressors. We make\nour code publicly available from\nhttps://github.com/yolish/multi-scene-pose-transformer.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 19:21:44 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 10:11:11 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Shavit", "Yoli", ""], ["Ferens", "Ron", ""], ["Keller", "Yosi", ""]]}, {"id": "2103.11471", "submitter": "Sahib Julka", "authors": "Sahib Julka, Vishal Sowrirajan, Joerg Schloetterer, Michael Granitzer", "title": "Conditional Generative Adversarial Networks for Speed Control in\n  Trajectory Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motion behaviour is driven by several factors -- goals, presence and actions\nof neighbouring agents, social relations, physical and social norms, the\nenvironment with its variable characteristics, and further. Most factors are\nnot directly observable and must be modelled from context. Trajectory\nprediction, is thus a hard problem, and has seen increasing attention from\nresearchers in the recent years. Prediction of motion, in application, must be\nrealistic, diverse and controllable. In spite of increasing focus on multimodal\ntrajectory generation, most methods still lack means for explicitly controlling\ndifferent modes of the data generation. Further, most endeavours invest heavily\nin designing special mechanisms to learn the interactions in latent space. We\npresent Conditional Speed GAN (CSG), that allows controlled generation of\ndiverse and socially acceptable trajectories, based on user controlled speed.\nDuring prediction, CSG forecasts future speed from latent space and conditions\nits generation based on it. CSG is comparable to state-of-the-art GAN methods\nin terms of the benchmark distance metrics, while being simple and useful for\nsimulation and data augmentation for different contexts such as fast or slow\npaced environments. Additionally, we compare the effect of different\naggregation mechanisms and show that a naive approach of concatenation works\ncomparable to its attention and pooling alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 19:47:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Julka", "Sahib", ""], ["Sowrirajan", "Vishal", ""], ["Schloetterer", "Joerg", ""], ["Granitzer", "Michael", ""]]}, {"id": "2103.11474", "submitter": "Sandra Avila", "authors": "Gabriel Oliveira dos Santos and Esther Luna Colombini and Sandra Avila", "title": "#PraCegoVer: A Large Dataset for Image Captioning in Portuguese", "comments": "19 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatically describing images using natural sentences is an important task\nto support visually impaired people's inclusion onto the Internet. It is still\na big challenge that requires understanding the relation of the objects present\nin the image and their attributes and actions they are involved in. Then,\nvisual interpretation methods are needed, but linguistic models are also\nnecessary to verbally describe the semantic relations. This problem is known as\nImage Captioning. Although many datasets were proposed in the literature, the\nmajority contains only English captions, whereas datasets with captions\ndescribed in other languages are scarce. Recently, a movement called PraCegoVer\narose on the Internet, stimulating users from social media to publish images,\ntag #PraCegoVer and add a short description of their content. Thus, inspired by\nthis movement, we have proposed the #PraCegoVer, a multi-modal dataset with\nPortuguese captions based on posts from Instagram. It is the first large\ndataset for image captioning in Portuguese with freely annotated images.\nFurther, the captions in our dataset bring additional challenges to the\nproblem: first, in contrast to popular datasets such as MS COCO Captions,\n#PraCegoVer has only one reference to each image; also, both mean and variance\nof our reference sentence length are significantly greater than those in the MS\nCOCO Captions. These two characteristics contribute to making our dataset\ninteresting due to the linguistic aspect and the challenges that it introduces\nto the image captioning problem. We publicly-share the dataset at\nhttps://github.com/gabrielsantosrv/PraCegoVer.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 19:55:46 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Santos", "Gabriel Oliveira dos", ""], ["Colombini", "Esther Luna", ""], ["Avila", "Sandra", ""]]}, {"id": "2103.11477", "submitter": "Yoli Shavit", "authors": "Yoli Shavit, Ron Ferens, Yosi Keller", "title": "Paying Attention to Activation Maps in Camera Pose Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Camera pose regression methods apply a single forward pass to the query image\nto estimate the camera pose. As such, they offer a fast and light-weight\nalternative to traditional localization schemes based on image retrieval. Pose\nregression approaches simultaneously learn two regression tasks, aiming to\njointly estimate the camera position and orientation using a single embedding\nvector computed by a convolutional backbone. We propose an attention-based\napproach for pose regression, where the convolutional activation maps are used\nas sequential inputs. Transformers are applied to encode the sequential\nactivation maps as latent vectors, used for camera pose regression. This allows\nus to pay attention to spatially-varying deep features. Using two Transformer\nheads, we separately focus on the features for camera position and orientation,\nbased on how informative they are per task. Our proposed approach is shown to\ncompare favorably to contemporary pose regressors schemes and achieves\nstate-of-the-art accuracy across multiple outdoor and indoor benchmarks. In\nparticular, to the best of our knowledge, our approach is the only method to\nattain sub-meter average accuracy across outdoor scenes. We make our code\npublicly available from here.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 20:10:15 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 19:55:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Shavit", "Yoli", ""], ["Ferens", "Ron", ""], ["Keller", "Yosi", ""]]}, {"id": "2103.11511", "submitter": "Dan Kondratyuk", "authors": "Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan,\n  Matthew Brown, Boqing Gong", "title": "MoViNets: Mobile Video Networks for Efficient Video Recognition", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Mobile Video Networks (MoViNets), a family of computation and\nmemory efficient video networks that can operate on streaming video for online\ninference. 3D convolutional neural networks (CNNs) are accurate at video\nrecognition but require large computation and memory budgets and do not support\nonline inference, making them difficult to work on mobile devices. We propose a\nthree-step approach to improve computational efficiency while substantially\nreducing the peak memory usage of 3D CNNs. First, we design a video network\nsearch space and employ neural architecture search to generate efficient and\ndiverse 3D CNN architectures. Second, we introduce the Stream Buffer technique\nthat decouples memory from video clip duration, allowing 3D CNNs to embed\narbitrary-length streaming video sequences for both training and inference with\na small constant memory footprint. Third, we propose a simple ensembling\ntechnique to improve accuracy further without sacrificing efficiency. These\nthree progressive techniques allow MoViNets to achieve state-of-the-art\naccuracy and efficiency on the Kinetics, Moments in Time, and Charades video\naction recognition datasets. For instance, MoViNet-A5-Stream achieves the same\naccuracy as X3D-XL on Kinetics 600 while requiring 80% fewer FLOPs and 65% less\nmemory. Code will be made available at\nhttps://github.com/tensorflow/models/tree/master/official/vision.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:06:38 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 18:04:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kondratyuk", "Dan", ""], ["Yuan", "Liangzhe", ""], ["Li", "Yandong", ""], ["Zhang", "Li", ""], ["Tan", "Mingxing", ""], ["Brown", "Matthew", ""], ["Gong", "Boqing", ""]]}, {"id": "2103.11520", "submitter": "Gabriel Bertocco", "authors": "Gabriel Bertocco and Fernanda Andal\\'o and Anderson Rocha", "title": "Unsupervised and self-adaptative techniques for cross-domain person\n  re-identification", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (ReID) across non-overlapping cameras is a\nchallenging task and, for this reason, most works in the prior art rely on\nsupervised feature learning from a labeled dataset to match the same person in\ndifferent views. However, it demands the time-consuming task of labeling the\nacquired data, prohibiting its fast deployment, specially in forensic\nscenarios. Unsupervised Domain Adaptation (UDA) emerges as a promising\nalternative, as it performs feature-learning adaptation from a model trained on\na source to a target domain without identity-label annotation. However, most\nUDA-based algorithms rely upon a complex loss function with several\nhyper-parameters, which hinders the generalization to different scenarios.\nMoreover, as UDA depends on the translation between domains, it is important to\nselect the most reliable data from the unseen domain, thus avoiding error\npropagation caused by noisy examples on the target data -- an often overlooked\nproblem. In this sense, we propose a novel UDA-based ReID method that optimizes\na simple loss function with only one hyper-parameter and that takes advantage\nof triplets of samples created by a new offline strategy based on the diversity\nof cameras within a cluster. This new strategy adapts the model and also\nregularizes it, avoiding overfitting on the target domain. We also introduce a\nnew self-ensembling strategy, in which weights from different iterations are\naggregated to create a final model combining knowledge from distinct moments of\nthe adaptation. For evaluation, we consider three well-known deep learning\narchitectures and combine them for final decision-making. The proposed method\ndoes not use person re-ranking nor any label on the target domain, and\noutperforms the state of the art, with a much simpler setup, on the Market to\nDuke, the challenging Market1501 to MSMT17, and Duke to MSMT17 adaptation\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:58:39 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 18:22:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bertocco", "Gabriel", ""], ["Andal\u00f3", "Fernanda", ""], ["Rocha", "Anderson", ""]]}, {"id": "2103.11521", "submitter": "Michael Soloveitchik", "authors": "Michael Soloveitchik, Tzvi Diskin, Efrat Morin and Ami Wiesel", "title": "Conditional Frechet Inception Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider distance functions between conditional distributions functions.\nWe focus on the Wasserstein metric and its Gaussian case known as the Frechet\nInception Distance (FID).We develop conditional versions of these metrics, and\nanalyze their relations. Then, we numerically compare the metrics inthe context\nof performance evaluation of conditional generative models. Our results show\nthat the metrics are similar in classical models which are less susceptible to\nconditional collapse. But the conditional distances are more informative in\nmodern unsuper-vised, semisupervised and unpaired models where learning the\nrelations between the inputs and outputs is the main challenge.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:59:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Soloveitchik", "Michael", ""], ["Diskin", "Tzvi", ""], ["Morin", "Efrat", ""], ["Wiesel", "Ami", ""]]}, {"id": "2103.11537", "submitter": "Karan Samel", "authors": "Karan Samel, Zelin Zhao, Binghong Chen, Kuan Wang, Robin Luo, Le Song", "title": "How to Design Sample and Computationally Efficient VQA Models", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In multi-modal reasoning tasks, such as visual question answering (VQA),\nthere have been many modeling and training paradigms tested. Previous models\npropose different methods for the vision and language tasks, but which ones\nperform the best while being sample and computationally efficient? Based on our\nexperiments, we find that representing the text as probabilistic programs and\nimages as object-level scene graphs best satisfy these desiderata. We extend\nexisting models to leverage these soft programs and scene graphs to train on\nquestion answer pairs in an end-to-end manner. Empirical results demonstrate\nthat this differentiable end-to-end program executor is able to maintain\nstate-of-the-art accuracy while being sample and computationally efficient.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 01:48:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Samel", "Karan", ""], ["Zhao", "Zelin", ""], ["Chen", "Binghong", ""], ["Wang", "Kuan", ""], ["Luo", "Robin", ""], ["Song", "Le", ""]]}, {"id": "2103.11554", "submitter": "Jian Zhang", "authors": "Di You, Jingfen Xie, Jian Zhang", "title": "ISTA-Net++: Flexible Deep Unfolding Network for Compressive Sensing", "comments": "ICME 2021 ORAL accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While deep neural networks have achieved impressive success in image\ncompressive sensing (CS), most of them lack flexibility when dealing with\nmulti-ratio tasks and multi-scene images in practical applications. To tackle\nthese challenges, we propose a novel end-to-end flexible ISTA-unfolding deep\nnetwork, dubbed ISTA-Net++, with superior performance and strong flexibility.\nSpecifically, by developing a dynamic unfolding strategy, our model enjoys the\nadaptability of handling CS problems with different ratios, i.e., multi-ratio\ntasks, through a single model. A cross-block strategy is further utilized to\nreduce blocking artifacts and enhance the CS recovery quality. Furthermore, we\nadopt a balanced dataset for training, which brings more robustness when\nreconstructing images of multiple scenes. Extensive experiments on four\ndatasets show that ISTA-Net++ achieves state-of-the-art results in terms of\nboth quantitative metrics and visual quality. Considering its flexibility,\neffectiveness and practicability, our model is expected to serve as a suitable\nbaseline in future CS research. The source code is available on\nhttps://github.com/jianzhangcs/ISTA-Netpp.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 03:09:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["You", "Di", ""], ["Xie", "Jingfen", ""], ["Zhang", "Jian", ""]]}, {"id": "2103.11555", "submitter": "Daizong Liu", "authors": "Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Yu Cheng, Wei Wei,\n  Zichuan Xu, Yulai Xie", "title": "Context-aware Biaffine Localizing Network for Temporal Sentence\n  Grounding", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of temporal sentence grounding (TSG), which\naims to identify the temporal boundary of a specific segment from an untrimmed\nvideo by a sentence query. Previous works either compare pre-defined candidate\nsegments with the query and select the best one by ranking, or directly regress\nthe boundary timestamps of the target segment. In this paper, we propose a\nnovel localization framework that scores all pairs of start and end indices\nwithin the video simultaneously with a biaffine mechanism. In particular, we\npresent a Context-aware Biaffine Localizing Network (CBLN) which incorporates\nboth local and global contexts into features of each start/end position for\nbiaffine-based localization. The local contexts from the adjacent frames help\ndistinguish the visually similar appearance, and the global contexts from the\nentire video contribute to reasoning the temporal relation. Besides, we also\ndevelop a multi-modal self-attention module to provide fine-grained\nquery-guided video representation for this biaffine strategy. Extensive\nexperiments show that our CBLN significantly outperforms state-of-the-arts on\nthree public datasets (ActivityNet Captions, TACoS, and Charades-STA),\ndemonstrating the effectiveness of the proposed localization framework.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 03:13:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Daizong", ""], ["Qu", "Xiaoye", ""], ["Dong", "Jianfeng", ""], ["Zhou", "Pan", ""], ["Cheng", "Yu", ""], ["Wei", "Wei", ""], ["Xu", "Zichuan", ""], ["Xie", "Yulai", ""]]}, {"id": "2103.11568", "submitter": "Zuozhuo Dai", "authors": "Zuozhuo Dai, Guangyuan Wang, Weihao Yuan, Siyu Zhu, Ping Tan", "title": "Cluster Contrast for Unsupervised Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised person re-identification (re-ID) attracts increasing attention\ndue to its practical applications in industry. State-of-the-art unsupervised\nre-ID methods train the neural networks using a memory-based non-parametric\nsoftmax loss. They store the pre-computed instance feature vectors inside the\nmemory, assign pseudo labels to them us-ing clustering algorithm, and compare\nthe query instances to the cluster using a form of contrastive loss. During\ntraining, the instance feature vectors are updated. How-ever, due to the\nvarying cluster size, the updating progress for each cluster is inconsistent.\nTo solve this problem, we present Cluster Contrast which stores feature vectors\nand computes contrast loss in the cluster level. We demonstrate that the\ninconsistency problem for cluster feature representation can be solved by the\ncluster-level memory dictionary.By straightforwardly applying Cluster Contrast\nto a standard unsupervised re-ID pipeline, it achieves considerable\nimprovements of 9.5%, 7.5%, 6.6% compared to state-of-the-art purely\nunsupervised re-ID methods and 5.1%, 4.0%,6.5% mAP compared to the\nstate-of-the-art unsupervised domain adaptation re-ID methods on the Market,\nDuke, andMSMT17 datasets.Our source code is available at\nhttps://github.com/alibaba/cluster-contrast.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 03:41:19 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 09:18:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dai", "Zuozhuo", ""], ["Wang", "Guangyuan", ""], ["Yuan", "Weihao", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""]]}, {"id": "2103.11571", "submitter": "Petr Kellnhofer", "authors": "Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli and\n  Gordon Wetzstein", "title": "Neural Lumigraph Rendering", "comments": "Project website:\n  http://www.computationalimaging.org/publications/nlr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel view synthesis is a challenging and ill-posed inverse rendering\nproblem. Neural rendering techniques have recently achieved photorealistic\nimage quality for this task. State-of-the-art (SOTA) neural volume rendering\napproaches, however, are slow to train and require minutes of inference (i.e.,\nrendering) time for high image resolutions. We adopt high-capacity neural scene\nrepresentations with periodic activations for jointly optimizing an implicit\nsurface and a radiance field of a scene supervised exclusively with posed 2D\nimages. Our neural rendering pipeline accelerates SOTA neural volume rendering\nby about two orders of magnitude and our implicit surface representation is\nunique in allowing us to export a mesh with view-dependent texture information.\nThus, like other implicit surface representations, ours is compatible with\ntraditional graphics pipelines, enabling real-time rendering rates, while\nachieving unprecedented image quality compared to other surface methods. We\nassess the quality of our approach using existing datasets as well as\nhigh-quality 3D face data captured with a custom multi-camera rig.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 03:46:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kellnhofer", "Petr", ""], ["Jebe", "Lars", ""], ["Jones", "Andrew", ""], ["Spicer", "Ryan", ""], ["Pulli", "Kari", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2103.11575", "submitter": "Jonathan Francis", "authors": "James Herman, Jonathan Francis, Siddha Ganju, Bingqing Chen, Anirudh\n  Koul, Abhinav Gupta, Alexey Skabelkin, Ivan Zhukov, Max Kumskoy, Eric Nyberg", "title": "Learn-to-Race: A Multimodal Control Environment for Autonomous Racing", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing research on autonomous driving primarily focuses on urban driving,\nwhich is insufficient for characterising the complex driving behaviour\nunderlying high-speed racing. At the same time, existing racing simulation\nframeworks struggle in capturing realism, with respect to visual rendering,\nvehicular dynamics, and task objectives, inhibiting the transfer of learning\nagents to real-world contexts. We introduce a new environment, where agents\nLearn-to-Race (L2R) in simulated competition-style racing, using multimodal\ninformation--from virtual cameras to a comprehensive array of inertial\nmeasurement sensors. Our environment, which includes a simulator and an\ninterfacing training framework, accurately models vehicle dynamics and racing\nconditions. In this paper, we release the Arrival simulator for autonomous\nracing. Next, we propose the L2R task with challenging metrics, inspired by\nlearning-to-drive challenges, Formula-style racing, and multimodal trajectory\nprediction for autonomous driving. Additionally, we provide the L2R framework\nsuite, facilitating simulated racing on high-precision models of real-world\ntracks, such as the famed Thruxton Circuit and the Las Vegas Motor Speedway.\nFinally, we provide an official L2R task dataset of expert demonstrations, as\nwell as a series of baseline experiments and reference implementations. We make\nall code available: https://github.com/hermgerm29/learn-to-race\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 04:03:06 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 19:52:52 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Herman", "James", ""], ["Francis", "Jonathan", ""], ["Ganju", "Siddha", ""], ["Chen", "Bingqing", ""], ["Koul", "Anirudh", ""], ["Gupta", "Abhinav", ""], ["Skabelkin", "Alexey", ""], ["Zhukov", "Ivan", ""], ["Kumskoy", "Max", ""], ["Nyberg", "Eric", ""]]}, {"id": "2103.11587", "submitter": "Yawen Huang", "authors": "Yawen Huang, Feng Zheng, Danyang Wang, Weilin Huang, Matthew R. Scott,\n  Ling Shao", "title": "Brain Image Synthesis with Unsupervised Multivariate Canonical\n  CSC$\\ell_4$Net", "comments": "10 pages, 5 figures CVPR2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neuroscience have highlighted the effectiveness of\nmulti-modal medical data for investigating certain pathologies and\nunderstanding human cognition. However, obtaining full sets of different\nmodalities is limited by various factors, such as long acquisition times, high\nexamination costs and artifact suppression. In addition, the complexity, high\ndimensionality and heterogeneity of neuroimaging data remains another key\nchallenge in leveraging existing randomized scans effectively, as data of the\nsame modality is often measured differently by different machines. There is a\nclear need to go beyond the traditional imaging-dependent process and\nsynthesize anatomically specific target-modality data from a source input. In\nthis paper, we propose to learn dedicated features that cross both intre- and\nintra-modal variations using a novel CSC$\\ell_4$Net. Through an initial\nunification of intra-modal data in the feature maps and multivariate canonical\nadaptation, CSC$\\ell_4$Net facilitates feature-level mutual transformation. The\npositive definite Riemannian manifold-penalized data fidelity term further\nenables CSC$\\ell_4$Net to reconstruct missing measurements according to\ntransformed features. Finally, the maximization $\\ell_4$-norm boils down to a\ncomputationally efficient optimization problem. Extensive experiments validate\nthe ability and robustness of our CSC$\\ell_4$Net compared to the\nstate-of-the-art methods on multiple datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 05:19:40 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Huang", "Yawen", ""], ["Zheng", "Feng", ""], ["Wang", "Danyang", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""], ["Shao", "Ling", ""]]}, {"id": "2103.11589", "submitter": "Srinjoy Chattopadhyay", "authors": "Jason Bunk, Srinjoy Chattopadhyay, B. S. Manjunath, Shivkumar\n  Chandrasekaran", "title": "Adversarially Optimized Mixup for Robust Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixup is a procedure for data augmentation that trains networks to make\nsmoothly interpolated predictions between datapoints. Adversarial training is a\nstrong form of data augmentation that optimizes for worst-case predictions in a\ncompact space around each data-point, resulting in neural networks that make\nmuch more robust predictions. In this paper, we bring these ideas together by\nadversarially probing the space between datapoints, using projected gradient\ndescent (PGD). The fundamental approach in this work is to leverage\nbackpropagation through the mixup interpolation during training to optimize for\nplaces where the network makes unsmooth and incongruous predictions.\nAdditionally, we also explore several modifications and nuances, like\noptimization of the mixup ratio and geometrical label assignment, and discuss\ntheir impact on enhancing network robustness. Through these ideas, we have been\nable to train networks that robustly generalize better; experiments on CIFAR-10\nand CIFAR-100 demonstrate consistent improvements in accuracy against strong\nadversaries, including the recent strong ensemble attack AutoAttack. Our source\ncode would be released for reproducibility.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 05:37:59 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bunk", "Jason", ""], ["Chattopadhyay", "Srinjoy", ""], ["Manjunath", "B. S.", ""], ["Chandrasekaran", "Shivkumar", ""]]}, {"id": "2103.11590", "submitter": "Yuxiang Liu", "authors": "Yuxiang Liu, Jidong Ge, Chuanyi Li, and Jie Gui", "title": "Delving into Variance Transmission and Normalization: Shift of Average\n  Gradient Makes the Network Collapse", "comments": "This paper has been accepted by AAAI21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization operations are essential for state-of-the-art neural networks\nand enable us to train a network from scratch with a large learning rate (LR).\nWe attempt to explain the real effect of Batch Normalization (BN) from the\nperspective of variance transmission by investigating the relationship between\nBN and Weights Normalization (WN). In this work, we demonstrate that the\nproblem of the shift of the average gradient will amplify the variance of every\nconvolutional (conv) layer. We propose Parametric Weights Standardization\n(PWS), a fast and robust to mini-batch size module used for conv filters, to\nsolve the shift of the average gradient. PWS can provide the speed-up of BN.\nBesides, it has less computation and does not change the output of a conv\nlayer. PWS enables the network to converge fast without normalizing the\noutputs. This result enhances the persuasiveness of the shift of the average\ngradient and explains why BN works from the perspective of variance\ntransmission. The code and appendix will be made available on\nhttps://github.com/lyxzzz/PWSConv.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 05:40:46 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Yuxiang", ""], ["Ge", "Jidong", ""], ["Li", "Chuanyi", ""], ["Gui", "Jie", ""]]}, {"id": "2103.11594", "submitter": "Yaoru Luo", "authors": "Yaoru Luo, Guole Liu, Wenjing Li, Yuanhao Guo and Ge Yang", "title": "Deep Neural Networks Learn Meta-Structures to Segment Fluorescence\n  Microscopy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy images play the critical role of capturing spatial or\nspatiotemporal information of biomedical processes in life sciences. Their\nsimple structures and semantics provide unique advantages in elucidating\nlearning behavior of deep neural networks (DNNs). It is generally assumed that\naccurate image annotation is required to train DNNs for accurate image\nsegmentation. In this study, however, we find that DNNs trained by label images\nin which nearly half (49%) of the binary pixel labels are randomly flipped\nprovide largely the same segmentation performance. This suggests that DNNs\nlearn high-level structures rather than pixel-level labels per se to segment\nfluorescence microscopy images. We refer to these structures as\nmeta-structures. In support of the existence of the meta-structures, when DNNs\nare trained by a series of label images with progressively less meta-structure\ninformation, we find progressive degradation in their segmentation performance.\nMotivated by the learning behavior of DNNs trained by random labels and the\ncharacteristics of meta-structures, we propose an unsupervised segmentation\nmodel. Experiments show that it achieves remarkably competitive performance in\ncomparison to supervised segmentation models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 05:43:34 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Luo", "Yaoru", ""], ["Liu", "Guole", ""], ["Li", "Wenjing", ""], ["Guo", "Yuanhao", ""], ["Yang", "Ge", ""]]}, {"id": "2103.11597", "submitter": "Qiang Zhou", "authors": "Qiang Zhou, Shiyin Wang, Yitong Wang, Zilong Huang, Xinggang Wang", "title": "Human De-occlusion: Invisible Perception and Recovery for Humans", "comments": "11 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we tackle the problem of human de-occlusion which reasons\nabout occluded segmentation masks and invisible appearance content of humans.\nIn particular, a two-stage framework is proposed to estimate the invisible\nportions and recover the content inside. For the stage of mask completion, a\nstacked network structure is devised to refine inaccurate masks from a general\ninstance segmentation model and predict integrated masks simultaneously.\nAdditionally, the guidance from human parsing and typical pose masks are\nleveraged to bring prior information. For the stage of content recovery, a\nnovel parsing guided attention module is applied to isolate body parts and\ncapture context information across multiple scales. Besides, an Amodal Human\nPerception dataset (AHP) is collected to settle the task of human de-occlusion.\nAHP has advantages of providing annotations from real-world scenes and the\nnumber of humans is comparatively larger than other amodal perception datasets.\nBased on this dataset, experiments demonstrate that our method performs over\nthe state-of-the-art techniques in both tasks of mask completion and content\nrecovery. Our AHP dataset is available at\n\\url{https://sydney0zq.github.io/ahp/}.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 05:54:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Qiang", ""], ["Wang", "Shiyin", ""], ["Wang", "Yitong", ""], ["Huang", "Zilong", ""], ["Wang", "Xinggang", ""]]}, {"id": "2103.11600", "submitter": "Gyeongsu Chae", "authors": "Wai Ting Cheung, Gyeongsu Chae", "title": "PriorityCut: Occlusion-guided Regularization for Warp-based Image\n  Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image animation generates a video of a source image following the motion of a\ndriving video. State-of-the-art self-supervised image animation approaches warp\nthe source image according to the motion of the driving video and recover the\nwarping artifacts by inpainting. These approaches mostly use vanilla\nconvolution for inpainting, and vanilla convolution does not distinguish\nbetween valid and invalid pixels. As a result, visual artifacts are still\nnoticeable after inpainting. CutMix is a state-of-the-art regularization\nstrategy that cuts and mixes patches of images and is widely studied in\ndifferent computer vision tasks. Among the remaining computer vision tasks,\nwarp-based image animation is one of the fields that the effects of CutMix have\nyet to be studied. This paper first presents a preliminary study on the effects\nof CutMix on warp-based image animation. We observed in our study that CutMix\nhelps improve only pixel values, but disturbs the spatial relationships between\npixels. Based on such observation, we propose PriorityCut, a novel augmentation\napproach that uses the top-k percent occluded pixels of the foreground to\nregularize warp-based image animation. By leveraging the domain knowledge in\nwarp-based image animation, PriorityCut significantly reduces the warping\nartifacts in state-of-the-art warp-based image animation models on diverse\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 06:05:20 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Cheung", "Wai Ting", ""], ["Chae", "Gyeongsu", ""]]}, {"id": "2103.11616", "submitter": "Muhammad Uzair", "authors": "Saira Kanwal, Muhammad Uzair, Habib Ullah", "title": "A Survey of Hand Crafted and Deep Learning Methods for Image Aesthetic\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic image aesthetics assessment is a computer vision problem that deals\nwith the categorization of images into different aesthetic levels. The\ncategorization is usually done by analyzing an input image and computing some\nmeasure of the degree to which the image adhere to the key principles of\nphotography (balance, rhythm, harmony, contrast, unity, look, feel, tone and\ntexture). Owing to its diverse applications in many areas, automatic image\naesthetic assessment has gained significant research attention in recent years.\nThis paper presents a literature review of the recent techniques of automatic\nimage aesthetics assessment. A large number of traditional hand crafted and\ndeep learning based approaches are reviewed. Key problem aspects are discussed\nsuch as why some features or models perform better than others and what are the\nlimitations. A comparison of the quantitative results of different methods is\nalso provided at the end.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 07:00:56 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kanwal", "Saira", ""], ["Uzair", "Muhammad", ""], ["Ullah", "Habib", ""]]}, {"id": "2103.11617", "submitter": "Yichao Yan", "authors": "Yichao Yan, Jinpeng Li, Jie Qin, Song Bai, Shengcai Liao, Li Liu, Fan\n  Zhu, and Ling Shao", "title": "Anchor-Free Person Search", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search aims to simultaneously localize and identify a query person\nfrom realistic, uncropped images, which can be regarded as the unified task of\npedestrian detection and person re-identification (re-id). Most existing works\nemploy two-stage detectors like Faster-RCNN, yielding encouraging accuracy but\nwith high computational overhead. In this work, we present the Feature-Aligned\nPerson Search Network (AlignPS), the first anchor-free framework to efficiently\ntackle this challenging task. AlignPS explicitly addresses the major\nchallenges, which we summarize as the misalignment issues in different levels\n(i.e., scale, region, and task), when accommodating an anchor-free detector for\nthis task. More specifically, we propose an aligned feature aggregation module\nto generate more discriminative and robust feature embeddings by following a\n\"re-id first\" principle. Such a simple design directly improves the baseline\nanchor-free model on CUHK-SYSU by more than 20% in mAP. Moreover, AlignPS\noutperforms state-of-the-art two-stage methods, with a higher speed. Code is\navailable at https://github.com/daodaofr/AlignPS\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 07:04:29 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 08:25:26 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yan", "Yichao", ""], ["Li", "Jinpeng", ""], ["Qin", "Jie", ""], ["Bai", "Song", ""], ["Liao", "Shengcai", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Shao", "Ling", ""]]}, {"id": "2103.11622", "submitter": "Zhen Zhu", "authors": "Zhen Zhu, Tengteng Huang, Mengde Xu, Baoguang Shi, Wenqing Cheng,\n  Xiang Bai", "title": "Progressive and Aligned Pose Attention Transfer for Person Image\n  Generation", "comments": "Accepted by TPAMI. An extension of the conference version. Conference\n  version at arXiv:1904.03349", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new generative adversarial network for pose transfer,\ni.e., transferring the pose of a given person to a target pose. We design a\nprogressive generator which comprises a sequence of transfer blocks. Each block\nperforms an intermediate transfer step by modeling the relationship between the\ncondition and the target poses with attention mechanism. Two types of blocks\nare introduced, namely Pose-Attentional Transfer Block (PATB) and Aligned\nPose-Attentional Transfer Bloc ~(APATB). Compared with previous works, our\nmodel generates more photorealistic person images that retain better appearance\nconsistency and shape consistency compared with input images. We verify the\nefficacy of the model on the Market-1501 and DeepFashion datasets, using\nquantitative and qualitative measures. Furthermore, we show that our method can\nbe used for data augmentation for the person re-identification task,\nalleviating the issue of data insufficiency. Code and pretrained models are\navailable at https://github.com/tengteng95/Pose-Transfer.git.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 07:24:57 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhu", "Zhen", ""], ["Huang", "Tengteng", ""], ["Xu", "Mengde", ""], ["Shi", "Baoguang", ""], ["Cheng", "Wenqing", ""], ["Bai", "Xiang", ""]]}, {"id": "2103.11624", "submitter": "Liu Yicheng", "authors": "Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, Bolei Zhou", "title": "Multimodal Motion Prediction with Stacked Transformers", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting multiple plausible future trajectories of the nearby vehicles is\ncrucial for the safety of autonomous driving. Recent motion prediction\napproaches attempt to achieve such multimodal motion prediction by implicitly\nregularizing the feature or explicitly generating multiple candidate proposals.\nHowever, it remains challenging since the latent features may concentrate on\nthe most frequent mode of the data while the proposal-based methods depend\nlargely on the prior knowledge to generate and select the proposals. In this\nwork, we propose a novel transformer framework for multimodal motion\nprediction, termed as mmTransformer. A novel network architecture based on\nstacked transformers is designed to model the multimodality at feature level\nwith a set of fixed independent proposals. A region-based training strategy is\nthen developed to induce the multimodality of the generated proposals.\nExperiments on Argoverse dataset show that the proposed model achieves the\nstate-of-the-art performance on motion prediction, substantially improving the\ndiversity and the accuracy of the predicted trajectories. Demo video and code\nare available at https://decisionforce.github.io/mmTransformer.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 07:25:54 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 06:37:16 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Yicheng", ""], ["Zhang", "Jinghuai", ""], ["Fang", "Liangji", ""], ["Jiang", "Qinhong", ""], ["Zhou", "Bolei", ""]]}, {"id": "2103.11636", "submitter": "Qi Ming", "authors": "Qi Ming, Zhiqiang Zhou, Lingjuan Miao, Xue Yang, Yunpeng Dong", "title": "Optimization for Oriented Object Detection via Representation Invariance\n  Loss", "comments": "The code and models are available at https://github.com/ming71/RIDet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary-oriented objects exist widely in natural scenes, and thus the\noriented object detection has received extensive attention in recent years. The\nmainstream rotation detectors use oriented bounding boxes (OBB) or\nquadrilateral bounding boxes (QBB) to represent the rotating objects. However,\nthese methods suffer from the representation ambiguity for oriented object\ndefinition, which leads to suboptimal regression optimization and the\ninconsistency between the loss metric and the localization accuracy of the\npredictions. In this paper, we propose a Representation Invariance Loss (RIL)\nto optimize the bounding box regression for the rotating objects. Specifically,\nRIL treats multiple representations of an oriented object as multiple\nequivalent local minima, and hence transforms bounding box regression into an\nadaptive matching process with these local minima. Then, the Hungarian matching\nalgorithm is adopted to obtain the optimal regression strategy. We also propose\na normalized rotation loss to alleviate the weak correlation between different\nvariables and their unbalanced loss contribution in OBB representation.\nExtensive experiments on remote sensing datasets and scene text datasets show\nthat our method achieves consistent and substantial improvement. The source\ncode and trained models are available at https://github.com/ming71/RIDet.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 07:55:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ming", "Qi", ""], ["Zhou", "Zhiqiang", ""], ["Miao", "Lingjuan", ""], ["Yang", "Xue", ""], ["Dong", "Yunpeng", ""]]}, {"id": "2103.11642", "submitter": "Matthew R Behrend", "authors": "Matthew R. Behrend and Sean M. Robinson", "title": "A Batch Normalization Classifier for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adapting a model to perform well on unforeseen data outside its training set\nis a common problem that continues to motivate new approaches. We demonstrate\nthat application of batch normalization in the output layer, prior to softmax\nactivation, results in improved generalization across visual data domains in a\nrefined ResNet model. The approach adds negligible computational complexity yet\noutperforms many domain adaptation methods that explicitly learn to align data\ndomains. We benchmark this technique on the Office-Home dataset and show that\nbatch normalization is competitive with other leading methods. We show that\nthis method is not sensitive to presence of source data during adaptation and\nfurther present the impact on trained tensor distributions tends toward\nsparsity. Code is available at https://github.com/matthewbehrend/BNC\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:03:44 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Behrend", "Matthew R.", ""], ["Robinson", "Sean M.", ""]]}, {"id": "2103.11645", "submitter": "Chang Liu", "authors": "Chang Liu, Xiaojuan Qi, Edmund Lam, Ngai Wong", "title": "AET-EFN: A Versatile Design for Static and Dynamic Event-Based Vision", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The neuromorphic event cameras, which capture the optical changes of a scene,\nhave drawn increasing attention due to their high speed and low power\nconsumption. However, the event data are noisy, sparse, and nonuniform in the\nspatial-temporal domain with an extremely high temporal resolution, making it\nchallenging to design backend algorithms for event-based vision. Existing\nmethods encode events into point-cloud-based or voxel-based representations,\nbut suffer from noise and/or information loss. Additionally, there is little\nresearch that systematically studies how to handle static and dynamic scenes\nwith one universal design for event-based vision. This work proposes the\nAligned Event Tensor (AET) as a novel event data representation, and a neat\nframework called Event Frame Net (EFN), which enables our model for event-based\nvision under static and dynamic scenes. The proposed AET and EFN are evaluated\non various datasets, and proved to surpass existing state-of-the-art methods by\nlarge margins. Our method is also efficient and achieves the fastest inference\nspeed among others.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:09:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Chang", ""], ["Qi", "Xiaojuan", ""], ["Lam", "Edmund", ""], ["Wong", "Ngai", ""]]}, {"id": "2103.11651", "submitter": "Karin van Garderen", "authors": "Karin A. van Garderen, Sebastian R. van der Voort, Maarten M.J.\n  Wijnenga, Fatih Incekara, Georgios Kapsas, Renske Gahrmann, Ahmad Alafandi,\n  Marion Smits, Stefan Klein", "title": "Evaluating glioma growth predictions as a forward ranking problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The problem of tumor growth prediction is challenging, but promising results\nhave been achieved with both model-driven and statistical methods. In this\nwork, we present a framework for the evaluation of growth predictions that\nfocuses on the spatial infiltration patterns, and specifically evaluating a\nprediction of future growth. We propose to frame the problem as a ranking\nproblem rather than a segmentation problem. Using the average precision as a\nmetric, we can evaluate the results with segmentations while using the full\nspatiotemporal prediction. Furthermore, by separating the model goodness-of-fit\nfrom future predictive performance, we show that in some cases, a better fit of\nmodel parameters does not guarantee a better the predictive power.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:21:21 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["van Garderen", "Karin A.", ""], ["van der Voort", "Sebastian R.", ""], ["Wijnenga", "Maarten M. J.", ""], ["Incekara", "Fatih", ""], ["Kapsas", "Georgios", ""], ["Gahrmann", "Renske", ""], ["Alafandi", "Ahmad", ""], ["Smits", "Marion", ""], ["Klein", "Stefan", ""]]}, {"id": "2103.11652", "submitter": "Sijia Wen", "authors": "Sijia Wen, Yingqiang Zheng, Feng Lu", "title": "Polarization Guided Specular Reflection Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since specular reflection often exists in the real captured images and causes\ndeviation between the recorded color and intrinsic color, specular reflection\nseparation can bring advantages to multiple applications that require\nconsistent object surface appearance. However, due to the color of an object is\nsignificantly influenced by the color of the illumination, the existing\nresearches still suffer from the near-duplicate challenge, that is, the\nseparation becomes unstable when the illumination color is close to the surface\ncolor. In this paper, we derive a polarization guided model to incorporate the\npolarization information into a designed iteration optimization separation\nstrategy to separate the specular reflection. Based on the analysis of\npolarization, we propose a polarization guided model to generate a polarization\nchromaticity image, which is able to reveal the geometrical profile of the\ninput image in complex scenarios, such as diversity of illumination. The\npolarization chromaticity image can accurately cluster the pixels with similar\ndiffuse color. We further use the specular separation of all these clusters as\nan implicit prior to ensure that the diffuse components will not be mistakenly\nseparated as the specular components. With the polarization guided model, we\nreformulate the specular reflection separation into a unified optimization\nfunction which can be solved by the ADMM strategy. The specular reflection will\nbe detected and separated jointly by RGB and polarimetric information. Both\nqualitative and quantitative experimental results have shown that our method\ncan faithfully separate the specular reflection, especially in some challenging\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:22:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wen", "Sijia", ""], ["Zheng", "Yingqiang", ""], ["Lu", "Feng", ""]]}, {"id": "2103.11658", "submitter": "Shiyu Xuan", "authors": "Shiyu Xuan, Shiliang Zhang", "title": "Intra-Inter Camera Similarity for Unsupervised Person Re-Identification", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of unsupervised person Re-Identification (Re-ID) works produce\npseudo-labels by measuring the feature similarity without considering the\ndistribution discrepancy among cameras, leading to degraded accuracy in label\ncomputation across cameras. This paper targets to address this challenge by\nstudying a novel intra-inter camera similarity for pseudo-label generation. We\ndecompose the sample similarity computation into two stage, i.e., the\nintra-camera and inter-camera computations, respectively. The intra-camera\ncomputation directly leverages the CNN features for similarity computation\nwithin each camera. Pseudo-labels generated on different cameras train the\nre-id model in a multi-branch network. The second stage considers the\nclassification scores of each sample on different cameras as a new feature\nvector. This new feature effectively alleviates the distribution discrepancy\namong cameras and generates more reliable pseudo-labels. We hence train our\nre-id model in two stages with intra-camera and inter-camera pseudo-labels,\nrespectively. This simple intra-inter camera similarity produces surprisingly\ngood performance on multiple datasets, e.g., achieves rank-1 accuracy of 89.5%\non the Market1501 dataset, outperforming the recent unsupervised works by 9+%,\nand is comparable with the latest transfer learning works that leverage extra\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:29:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Xuan", "Shiyu", ""], ["Zhang", "Shiliang", ""]]}, {"id": "2103.11661", "submitter": "Xin Jin", "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "title": "Re-energizing Domain Discriminator with Sample Relabeling for\n  Adversarial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Many unsupervised domain adaptation (UDA) methods exploit domain adversarial\ntraining to align the features to reduce domain gap, where a feature extractor\nis trained to fool a domain discriminator in order to have aligned feature\ndistributions. The discrimination capability of the domain classifier w.r.t the\nincreasingly aligned feature distributions deteriorates as training goes on,\nthus cannot effectively further drive the training of feature extractor. In\nthis work, we propose an efficient optimization strategy named Re-enforceable\nAdversarial Domain Adaptation (RADA) which aims to re-energize the domain\ndiscriminator during the training by using dynamic domain labels. Particularly,\nwe relabel the well aligned target domain samples as source domain samples on\nthe fly. Such relabeling makes the less separable distributions more separable,\nand thus leads to a more powerful domain classifier w.r.t. the new data\ndistributions, which in turn further drives feature alignment. Extensive\nexperiments on multiple UDA benchmarks demonstrate the effectiveness and\nsuperiority of our RADA.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:32:55 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jin", "Xin", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2103.11671", "submitter": "Yunfei Liu", "authors": "Yunfei Liu, Chaoqun Zhuang, Feng Lu", "title": "Unsupervised Two-Stage Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection from a single image is challenging since anomaly data is\nalways rare and can be with highly unpredictable types. With only anomaly-free\ndata available, most existing methods train an AutoEncoder to reconstruct the\ninput image and find the difference between the input and output to identify\nthe anomalous region. However, such methods face a potential problem - a coarse\nreconstruction generates extra image differences while a high-fidelity one may\ndraw in the anomaly. In this paper, we solve this contradiction by proposing a\ntwo-stage approach, which generates high-fidelity yet anomaly-free\nreconstructions. Our Unsupervised Two-stage Anomaly Detection (UTAD) relies on\ntwo technical components, namely the Impression Extractor (IE-Net) and the\nExpert-Net. The IE-Net and Expert-Net accomplish the two-stage anomaly-free\nimage reconstruction task while they also generate intuitive intermediate\nresults, making the whole UTAD interpretable. Extensive experiments show that\nour method outperforms state-of-the-arts on four anomaly detection datasets\nwith different types of real-world objects and textures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:57:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Yunfei", ""], ["Zhuang", "Chaoqun", ""], ["Lu", "Feng", ""]]}, {"id": "2103.11681", "submitter": "Ning Wang", "authors": "Ning Wang and Wengang Zhou and Jie Wang and Houqaing Li", "title": "Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual\n  Tracking", "comments": "To appear in CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video object tracking, there exist rich temporal contexts among successive\nframes, which have been largely overlooked in existing trackers. In this work,\nwe bridge the individual video frames and explore the temporal contexts across\nthem via a transformer architecture for robust object tracking. Different from\nclassic usage of the transformer in natural language processing tasks, we\nseparate its encoder and decoder into two parallel branches and carefully\ndesign them within the Siamese-like tracking pipelines. The transformer encoder\npromotes the target templates via attention-based feature reinforcement, which\nbenefits the high-quality tracking model generation. The transformer decoder\npropagates the tracking cues from previous templates to the current frame,\nwhich facilitates the object searching process. Our transformer-assisted\ntracking framework is neat and trained in an end-to-end manner. With the\nproposed transformer, a simple Siamese matching approach is able to outperform\nthe current top-performing trackers. By combining our transformer with the\nrecent discriminative tracking pipeline, our method sets several new\nstate-of-the-art records on prevalent tracking benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:20:05 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 09:23:57 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wang", "Ning", ""], ["Zhou", "Wengang", ""], ["Wang", "Jie", ""], ["Li", "Houqaing", ""]]}, {"id": "2103.11695", "submitter": "Lukas Fisch", "authors": "Lukas Fisch, Jan Ernsting, Nils R. Winter, Vincent Holstein, Ramona\n  Leenings, Marie Beisemann, Kelvin Sarink, Daniel Emden, Nils Opel, Ronny\n  Redlich, Jonathan Repple, Dominik Grotegerd, Susanne Meinert, Niklas Wulms,\n  Heike Minnerup, Jochen G. Hirsch, Thoralf Niendorf, Beate Endemann, Fabian\n  Bamberg, Thomas Kr\\\"oncke, Annette Peters, Robin B\\\"ulow, Henry V\\\"olzke,\n  Oyunbileg von Stackelberg, Ramona Felizitas Sowade, Lale Umutlu, B\\\"orge\n  Schmidt, Svenja Caspers, German National Cohort Study Center Consortium,\n  Harald Kugel, Bernhard T. Baune, Tilo Kircher, Benjamin Risse, Udo\n  Dannlowski, Klaus Berger, Tim Hahn", "title": "Predicting brain-age from raw T 1 -weighted Magnetic Resonance Imaging\n  data using 3D Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Age prediction based on Magnetic Resonance Imaging (MRI) data of the brain is\na biomarker to quantify the progress of brain diseases and aging. Current\napproaches rely on preparing the data with multiple preprocessing steps, such\nas registering voxels to a standardized brain atlas, which yields a significant\ncomputational overhead, hampers widespread usage and results in the predicted\nbrain-age to be sensitive to preprocessing parameters. Here we describe a 3D\nConvolutional Neural Network (CNN) based on the ResNet architecture being\ntrained on raw, non-registered T$_ 1$-weighted MRI data of N=10,691 samples\nfrom the German National Cohort and additionally applied and validated in\nN=2,173 samples from three independent studies using transfer learning. For\ncomparison, state-of-the-art models using preprocessed neuroimaging data are\ntrained and validated on the same samples. The 3D CNN using raw neuroimaging\ndata predicts age with a mean average deviation of 2.84 years, outperforming\nthe state-of-the-art brain-age models using preprocessed data. Since our\napproach is invariant to preprocessing software and parameter choices, it\nenables faster, more robust and more accurate brain-age modeling.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:48:34 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Fisch", "Lukas", ""], ["Ernsting", "Jan", ""], ["Winter", "Nils R.", ""], ["Holstein", "Vincent", ""], ["Leenings", "Ramona", ""], ["Beisemann", "Marie", ""], ["Sarink", "Kelvin", ""], ["Emden", "Daniel", ""], ["Opel", "Nils", ""], ["Redlich", "Ronny", ""], ["Repple", "Jonathan", ""], ["Grotegerd", "Dominik", ""], ["Meinert", "Susanne", ""], ["Wulms", "Niklas", ""], ["Minnerup", "Heike", ""], ["Hirsch", "Jochen G.", ""], ["Niendorf", "Thoralf", ""], ["Endemann", "Beate", ""], ["Bamberg", "Fabian", ""], ["Kr\u00f6ncke", "Thomas", ""], ["Peters", "Annette", ""], ["B\u00fclow", "Robin", ""], ["V\u00f6lzke", "Henry", ""], ["von Stackelberg", "Oyunbileg", ""], ["Sowade", "Ramona Felizitas", ""], ["Umutlu", "Lale", ""], ["Schmidt", "B\u00f6rge", ""], ["Caspers", "Svenja", ""], ["Consortium", "German National Cohort Study Center", ""], ["Kugel", "Harald", ""], ["Baune", "Bernhard T.", ""], ["Kircher", "Tilo", ""], ["Risse", "Benjamin", ""], ["Dannlowski", "Udo", ""], ["Berger", "Klaus", ""], ["Hahn", "Tim", ""]]}, {"id": "2103.11696", "submitter": "Dong Chen", "authors": "Dong Chen and Duoqian Miao", "title": "Control Distance IoU and Control Distance IoU Loss Function for Better\n  Bounding Box Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous improvements for feedback mechanisms have contributed to the great\nprogress in object detection. In this paper, we first present an\nevaluation-feedback module, which is proposed to consist of evaluation system\nand feedback mechanism. Then we analyze and summarize the disadvantages and\nimprovements of traditional evaluation-feedback module. Finally, we focus on\nboth the evaluation system and the feedback mechanism, and propose Control\nDistance IoU and Control Distance IoU loss function (or CDIoU and CDIoU loss\nfor short) without increasing parameters or FLOPs in models, which show\ndifferent significant enhancements on several classical and emerging models.\nSome experiments and comparative tests show that coordinated\nevaluation-feedback module can effectively improve model performance. CDIoU and\nCDIoU loss have different excellent performances in several models such as\nFaster R-CNN, YOLOv4, RetinaNet and ATSS. There is a maximum AP improvement of\n1.9% and an average AP of 0.8% improvement on MS COCO dataset, compared to\ntraditional evaluation-feedback modules.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:57:25 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Dong", ""], ["Miao", "Duoqian", ""]]}, {"id": "2103.11703", "submitter": "Yujin Chen", "authors": "Yujin Chen, Zhigang Tu, Di Kang, Linchao Bao, Ying Zhang, Xuefei Zhe,\n  Ruizhi Chen, Junsong Yuan", "title": "Model-based 3D Hand Reconstruction via Self-Supervised Learning", "comments": "Accepted by CVPR21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing a 3D hand from a single-view RGB image is challenging due to\nvarious hand configurations and depth ambiguity. To reliably reconstruct a 3D\nhand from a monocular image, most state-of-the-art methods heavily rely on 3D\nannotations at the training stage, but obtaining 3D annotations is expensive.\nTo alleviate reliance on labeled training data, we propose S2HAND, a\nself-supervised 3D hand reconstruction network that can jointly estimate pose,\nshape, texture, and the camera viewpoint. Specifically, we obtain geometric\ncues from the input image through easily accessible 2D detected keypoints. To\nlearn an accurate hand reconstruction model from these noisy geometric cues, we\nutilize the consistency between 2D and 3D representations and propose a set of\nnovel losses to rationalize outputs of the neural network. For the first time,\nwe demonstrate the feasibility of training an accurate 3D hand reconstruction\nnetwork without relying on manual annotations. Our experiments show that the\nproposed method achieves comparable performance with recent fully-supervised\nmethods while using fewer supervision data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:12:43 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Yujin", ""], ["Tu", "Zhigang", ""], ["Kang", "Di", ""], ["Bao", "Linchao", ""], ["Zhang", "Ying", ""], ["Zhe", "Xuefei", ""], ["Chen", "Ruizhi", ""], ["Yuan", "Junsong", ""]]}, {"id": "2103.11704", "submitter": "Yuiko Sakuma", "authors": "Yuiko Sakuma, Hiroshi Sumihiro, Jun Nishikawa, Toshiki Nakamura and\n  Ryoji Ikegaya", "title": "n-hot: Efficient bit-level sparsity for powers-of-two neural network\n  quantization", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powers-of-two (PoT) quantization reduces the number of bit operations of deep\nneural networks on resource-constrained hardware. However, PoT quantization\ntriggers a severe accuracy drop because of its limited representation ability.\nSince DNN models have been applied for relatively complex tasks (e.g.,\nclassification for large datasets and object detection), improvement in\naccuracy for the PoT quantization method is required. Although some previous\nworks attempt to improve the accuracy of PoT quantization, there is no work\nthat balances accuracy and computation costs in a memory-efficient way. To\naddress this problem, we propose an efficient PoT quantization scheme.\nBit-level sparsity is introduced; weights (or activations) are rounded to\nvalues that can be calculated by n shift operations in multiplication. We also\nallow not only addition but also subtraction as each operation. Moreover, we\nuse a two-stage fine-tuning algorithm to recover the accuracy drop that is\ntriggered by introducing the bit-level sparsity. The experimental results on an\nobject detection model (CenterNet, MobileNet-v2 backbone) on the COCO dataset\nshow that our proposed method suppresses the accuracy drop by 0.3% at most\nwhile reducing the number of operations by about 75% and model size by 11.5%\ncompared to the uniform method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:13:12 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sakuma", "Yuiko", ""], ["Sumihiro", "Hiroshi", ""], ["Nishikawa", "Jun", ""], ["Nakamura", "Toshiki", ""], ["Ikegaya", "Ryoji", ""]]}, {"id": "2103.11713", "submitter": "Jo\\~ao B. S. Carvalho", "authors": "Jo\\~ao B. S. Carvalho, Jo\\~ao A. Santinha, {\\DJ}or{\\dj}e\n  Miladinovi\\'c, Joachim M. Buhmann", "title": "Spatially Dependent U-Nets: Highly Accurate Architectures for Medical\n  Imaging Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In clinical practice, regions of interest in medical imaging often need to be\nidentified through a process of precise image segmentation. The quality of this\nimage segmentation step critically affects the subsequent clinical assessment\nof the patient status. To enable high accuracy, automatic image segmentation,\nwe introduce a novel deep neural network architecture that exploits the\ninherent spatial coherence of anatomical structures and is well equipped to\ncapture long-range spatial dependencies in the segmented pixel/voxel space. In\ncontrast to the state-of-the-art solutions based on convolutional layers, our\napproach leverages on recently introduced spatial dependency layers that have\nan unbounded receptive field and explicitly model the inductive bias of spatial\ncoherence. Our method performs favourably to commonly used U-Net and U-Net++\narchitectures as demonstrated by improved Dice and Jaccardscore in three\ndifferent medical segmentation tasks: nuclei segmentation in microscopy images,\npolyp segmentation in colonoscopy videos, and liver segmentation in abdominal\nCT scans.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:37:20 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Carvalho", "Jo\u00e3o B. S.", ""], ["Santinha", "Jo\u00e3o A.", ""], ["Miladinovi\u0107", "\u0110or\u0111e", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "2103.11716", "submitter": "Effrosyni Doutsi", "authors": "Effrosyni Doutsi, Lionel Fillatre, Marc Antonini, Julien Gaulmin", "title": "Retinal-inspired Filtering for Dynamic Image Coding", "comments": null, "journal-ref": null, "doi": "10.1109/ICIP.2015.7351456", "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper introduces a novel non-Separable sPAtioteMporal filter (non-SPAM)\nwhich enables the spatiotemporal decomposition of a still-image. The\nconstruction of this filter is inspired by the model of the retina which is\nable to selectively transmit information to the brain. The non-SPAM filter\nmimics the retinal-way to extract necessary information for a dynamic\nencoding/decoding system. We applied the non-SPAM filter on a still image which\nis flashed for a long time. We prove that the non-SPAM filter decomposes the\nstill image over a set of time-varying difference of Gaussians, which form a\nframe. We simulate the analysis and synthesis system based on this frame. This\nsystem results in a progressive reconstruction of the input image. Both the\ntheoretical and numerical results show that the quality of the reconstruction\nimproves while the time increases.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:39:47 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Doutsi", "Effrosyni", ""], ["Fillatre", "Lionel", ""], ["Antonini", "Marc", ""], ["Gaulmin", "Julien", ""]]}, {"id": "2103.11719", "submitter": "Jigyasa Katrolia", "authors": "Jigyasa Singh Katrolia, Bruno Mirbach, Ahmed El-Sherif, Hartmut Feld,\n  Jason Rambach, Didier Stricker", "title": "TICaM: A Time-of-flight In-car Cabin Monitoring Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present TICaM, a Time-of-flight In-car Cabin Monitoring dataset for\nvehicle interior monitoring using a single wide-angle depth camera. Our dataset\naddresses the deficiencies of currently available in-car cabin datasets in\nterms of the ambit of labeled classes, recorded scenarios and provided\nannotations; all at the same time. We record an exhaustive list of actions\nperformed while driving and provide for them multi-modal labeled images (depth,\nRGB and IR), with complete annotations for 2D and 3D object detection, instance\nand semantic segmentation as well as activity annotations for RGB frames.\nAdditional to real recordings, we provide a synthetic dataset of in-car cabin\nimages with same multi-modality of images and annotations, providing a unique\nand extremely beneficial combination of synthetic and real data for effectively\ntraining cabin monitoring systems and evaluating domain adaptation approaches.\nThe dataset is available at https://vizta-tof.kl.dfki.de/.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:48:45 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 12:40:17 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Katrolia", "Jigyasa Singh", ""], ["Mirbach", "Bruno", ""], ["El-Sherif", "Ahmed", ""], ["Feld", "Hartmut", ""], ["Rambach", "Jason", ""], ["Stricker", "Didier", ""]]}, {"id": "2103.11727", "submitter": "Youssef Chahir", "authors": "Messaoud Mostefai, Salah Khodja and Youssef Chahir", "title": "A New Efficient Numbering System : Application to Numbers Generation and\n  Visual Markers Design", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This short paper introduces a recently patented line based numbering system.\nThe last allows a best concordance with decimal digits values, and open up new\nopportunities, which are not possible with the classical decimal numeration\nsystem. Proposed OILU symbolic allows generating a new type of number series,\nbased on multi facets numbers splitting process. On the other hand, this new\nsymbolic is used in the development of new visual markers, highly required in\naugmented reality and UAV's navigation applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:06:54 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 20:06:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Mostefai", "Messaoud", ""], ["Khodja", "Salah", ""], ["Chahir", "Youssef", ""]]}, {"id": "2103.11731", "submitter": "Gongjie Zhang", "authors": "Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, Shijian Lu", "title": "Meta-DETR: Few-Shot Object Detection via Unified Image-Level\n  Meta-Learning", "comments": "Codes and data will be made publicly available in the future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Few-shot object detection aims at detecting novel objects with only a few\nannotated examples. Prior works have proved meta-learning a promising solution,\nand most of them essentially address detection by meta-learning over regions\nfor their classification and location fine-tuning. However, these methods\nsubstantially rely on initially well-located region proposals, which are\nusually hard to obtain under the few-shot settings. This paper presents a novel\nmeta-detector framework, namely Meta-DETR, which eliminates region-wise\nprediction and instead meta-learns object localization and classification at\nimage level in a unified and complementary manner. Specifically, it first\nencodes both support and query images into category-specific features and then\nfeeds them into a category-agnostic decoder to directly generate predictions\nfor specific categories. To facilitate meta-learning with deep networks, we\ndesign a simple but effective Semantic Alignment Mechanism (SAM), which aligns\nhigh-level and low-level feature semantics to improve the generalization of\nmeta-learned representations. Experiments over multiple few-shot object\ndetection benchmarks show that Meta-DETR outperforms state-of-the-art methods\nby large margins.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:14:00 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 04:54:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhang", "Gongjie", ""], ["Luo", "Zhipeng", ""], ["Cui", "Kaiwen", ""], ["Lu", "Shijian", ""]]}, {"id": "2103.11736", "submitter": "Yaoyong Zheng", "authors": "Lin Pan, Yaoyong Zheng, Liqin Huang, Liuqing Chen, Zhen Zhang, Rongda\n  Fu, Bin Zheng, Shaohua Zheng", "title": "Automatic Pulmonary Artery-Vein Separation in CT Images using Twin-Pipe\n  Network and Topology Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of medical computer-aided diagnostic systems, pulmonary\nartery-vein(A/V) separation plays a crucial role in assisting doctors in\npreoperative planning for lung cancer surgery. However, distinguishing arterial\nfrom venous irrigation in chest CT images remains a challenge due to the\nsimilarity and complex structure of the arteries and veins. We propose a novel\nmethod for automatic separation of pulmonary arteries and veins from chest CT\nimages. The method consists of three parts. First, global connection\ninformation and local feature information are used to construct a complete\ntopological tree and ensure the continuity of vessel reconstruction. Second,\nthe Twin-Pipe network proposed can automatically learn the differences between\narteries and veins at different levels to reduce classification errors caused\nby changes in terminal vessel characteristics. Finally, the topology optimizer\nconsiders interbranch and intrabranch topological relationships to maintain\nspatial consistency to avoid the misclassification of A/V irrigations. We\nvalidate the performance of the method on chest CT images. Compared with manual\nclassification, the proposed method achieves an average accuracy of 96.2% on\nnoncontrast chest CT. In addition, the method has been proven to have good\ngeneralization, that is, the accuracies of 93.8% and 94.8% are obtained for CT\nscans from other devices and other modes, respectively. The result of pulmonary\nartery-vein obtained by the proposed method can provide better assistance for\npreoperative planning of lung cancer surgery.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:25:45 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 11:51:56 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Pan", "Lin", ""], ["Zheng", "Yaoyong", ""], ["Huang", "Liqin", ""], ["Chen", "Liuqing", ""], ["Zhang", "Zhen", ""], ["Fu", "Rongda", ""], ["Zheng", "Bin", ""], ["Zheng", "Shaohua", ""]]}, {"id": "2103.11744", "submitter": "Peng Zhao", "authors": "Hongying Liu, Peng Zhao, Zhubo Ruan, Fanhua Shang, and Yuanyuan Liu", "title": "Large Motion Video Super-Resolution with Dual Subnet and Multi-Stage\n  Communicated Upsampling", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) aims at restoring a video in low-resolution (LR)\nand improving it to higher-resolution (HR). Due to the characteristics of video\ntasks, it is very important that motion information among frames should be well\nconcerned, summarized and utilized for guidance in a VSR algorithm. Especially,\nwhen a video contains large motion, conventional methods easily bring\nincoherent results or artifacts. In this paper, we propose a novel deep neural\nnetwork with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for\nsuper-resolution of videos with large motion. We design a new module named\nU-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit\nmotion estimation and motion compensation (MEMC) as well as coarse spatial\nfeature extraction. And we present a new Multi-Stage Communicated Upsampling\n(MSCU) module to make full use of the intermediate results of upsampling for\nguiding the VSR. Moreover, a novel dual subnet is devised to aid the training\nof our DSMC, whose dual loss helps to reduce the solution space as well as\nenhance the generalization ability. Our experimental results confirm that our\nmethod achieves superior performance on videos with large motion compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:52:12 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Hongying", ""], ["Zhao", "Peng", ""], ["Ruan", "Zhubo", ""], ["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""]]}, {"id": "2103.11747", "submitter": "Stefan Becker", "authors": "Stefan Becker, Ronny Hug, Wolfgang H\\\"ubner, Michael Arens, and\n  Brendan T. Morris", "title": "Handling Missing Observations with an RNN-based Prediction-Update Cycle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In tasks such as tracking, time-series data inevitably carry missing\nobservations. While traditional tracking approaches can handle missing\nobservations, recurrent neural networks (RNNs) are designed to receive input\ndata in every step. Furthermore, current solutions for RNNs, like omitting the\nmissing data or data imputation, are not sufficient to account for the\nresulting increased uncertainty. Towards this end, this paper introduces an\nRNN-based approach that provides a full temporal filtering cycle for motion\nstate estimation. The Kalman filter inspired approach, enables to deal with\nmissing observations and outliers. For providing a full temporal filtering\ncycle, a basic RNN is extended to take observations and the associated belief\nabout its accuracy into account for updating the current state. An RNN\nprediction model, which generates a parametrized distribution to capture the\npredicted states, is combined with an RNN update model, which relies on the\nprediction model output and the current observation. By providing the model\nwith masking information, binary-encoded missing events, the model can overcome\nlimitations of standard techniques for dealing with missing input values. The\nmodel abilities are demonstrated on synthetic data reflecting prototypical\npedestrian tracking scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:55:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Becker", "Stefan", ""], ["Hug", "Ronny", ""], ["H\u00fcbner", "Wolfgang", ""], ["Arens", "Michael", ""], ["Morris", "Brendan T.", ""]]}, {"id": "2103.11770", "submitter": "Lei Shi", "authors": "Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu", "title": "AdaSGN: Adapting Joint Number and Model Size for Efficient\n  Skeleton-Based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing methods for skeleton-based action recognition mainly focus on\nimproving the recognition accuracy, whereas the efficiency of the model is\nrarely considered. Recently, there are some works trying to speed up the\nskeleton modeling by designing light-weight modules. However, in addition to\nthe model size, the amount of the data involved in the calculation is also an\nimportant factor for the running speed, especially for the skeleton data where\nmost of the joints are redundant or non-informative to identify a specific\nskeleton. Besides, previous works usually employ one fix-sized model for all\nthe samples regardless of the difficulty of recognition, which wastes\ncomputations for easy samples. To address these limitations, a novel approach,\ncalled AdaSGN, is proposed in this paper, which can reduce the computational\ncost of the inference process by adaptively controlling the input number of the\njoints of the skeleton on-the-fly. Moreover, it can also adaptively select the\noptimal model size for each sample to achieve a better trade-off between\naccuracy and efficiency. We conduct extensive experiments on three challenging\ndatasets, namely, NTU-60, NTU-120 and SHREC, to verify the superiority of the\nproposed approach, where AdaSGN achieves comparable or even higher performance\nwith much lower GFLOPs compared with the baseline method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:36:39 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Shi", "Lei", ""], ["Zhang", "Yifan", ""], ["Cheng", "Jian", ""], ["Lu", "Hanqing", ""]]}, {"id": "2103.11775", "submitter": "Alexander Mathis", "authors": "S\\'ebastien B. Hausmann and Alessandro Marin Vargas and Alexander\n  Mathis and Mackenzie W. Mathis", "title": "Measuring and modeling the motor system with machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utility of machine learning in understanding the motor system is\npromising a revolution in how to collect, measure, and analyze data. The field\nof movement science already elegantly incorporates theory and engineering\nprinciples to guide experimental work, and in this review we discuss the\ngrowing use of machine learning: from pose estimation, kinematic analyses,\ndimensionality reduction, and closed-loop feedback, to its use in understanding\nneural correlates and untangling sensorimotor systems. We also give our\nperspective on new avenues where markerless motion capture combined with\nbiomechanical modeling and neural networks could be a new platform for\nhypothesis-driven research.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:42:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hausmann", "S\u00e9bastien B.", ""], ["Vargas", "Alessandro Marin", ""], ["Mathis", "Alexander", ""], ["Mathis", "Mackenzie W.", ""]]}, {"id": "2103.11781", "submitter": "Sun Yifan", "authors": "Yifan Sun, Yuke Zhu, Yuhan Zhang, Pengkun Zheng, Xi Qiu, Chi Zhang,\n  Yichen Wei", "title": "Dynamic Metric Learning: Towards a Scalable Metric Space to Accommodate\n  Multiple Semantic Scales", "comments": "8pages, accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new fundamental characteristic, \\ie, the dynamic\nrange, from real-world metric tools to deep visual recognition. In metrology,\nthe dynamic range is a basic quality of a metric tool, indicating its\nflexibility to accommodate various scales. Larger dynamic range offers higher\nflexibility. In visual recognition, the multiple scale problem also exist.\nDifferent visual concepts may have different semantic scales. For example,\n``Animal'' and ``Plants'' have a large semantic scale while ``Elk'' has a much\nsmaller one. Under a small semantic scale, two different elks may look quite\n\\emph{different} to each other . However, under a large semantic scale (\\eg,\nanimals and plants), these two elks should be measured as being \\emph{similar}.\n%We argue that such flexibility is also important for deep metric learning,\nbecause different visual concepts indeed correspond to different semantic\nscales.\n  Introducing the dynamic range to deep metric learning, we get a novel\ncomputer vision task, \\ie, the Dynamic Metric Learning. It aims to learn a\nscalable metric space to accommodate visual concepts across multiple semantic\nscales. Based on three types of images, \\emph{i.e.}, vehicle, animal and online\nproducts, we construct three datasets for Dynamic Metric Learning. We benchmark\nthese datasets with popular deep metric learning methods and find Dynamic\nMetric Learning to be very challenging. The major difficulty lies in a conflict\nbetween different scales: the discriminative ability under a small scale\nusually compromises the discriminative ability under a large one, and vice\nversa. As a minor contribution, we propose Cross-Scale Learning (CSL) to\nalleviate such conflict. We show that CSL consistently improves the baseline on\nall the three datasets. The datasets and the code will be publicly available at\nhttps://github.com/SupetZYK/DynamicMetricLearning.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:46:12 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sun", "Yifan", ""], ["Zhu", "Yuke", ""], ["Zhang", "Yuhan", ""], ["Zheng", "Pengkun", ""], ["Qiu", "Xi", ""], ["Zhang", "Chi", ""], ["Wei", "Yichen", ""]]}, {"id": "2103.11784", "submitter": "Zhe Chen", "authors": "Zhe Chen, Wenhai Wang, Enze Xie, Tong Lu, Ping Luo", "title": "Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance\n  Normalization", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extremely simple Ultra-Resolution Style Transfer framework,\ntermed URST, to flexibly process arbitrary high-resolution images (e.g.,\n10000x10000 pixels) style transfer for the first time. Most of the existing\nstate-of-the-art methods would fall short due to massive memory cost and small\nstroke size when processing ultra-high resolution images. URST completely\navoids the memory problem caused by ultra-high resolution images by 1) dividing\nthe image into small patches and 2) performing patch-wise style transfer with a\nnovel Thumbnail Instance Normalization (TIN). Specifically, TIN can extract\nthumbnail's normalization statistics and apply them to small patches, ensuring\nthe style consistency among different patches. Overall, the URST framework has\nthree merits compared to prior arts. 1) We divide input image into small\npatches and adopt TIN, successfully transferring image style with arbitrary\nhigh-resolution. 2) Experiments show that our URST surpasses existing SOTA\nmethods on ultra-high resolution images benefiting from the effectiveness of\nthe proposed stroke perceptual loss in enlarging the stroke size. 3) Our URST\ncan be easily plugged into most existing style transfer methods and directly\nimprove their performance even without training. Code is available at\nhttps://github.com/czczup/URST.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:54:01 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Zhe", ""], ["Wang", "Wenhai", ""], ["Xie", "Enze", ""], ["Lu", "Tong", ""], ["Luo", "Ping", ""]]}, {"id": "2103.11816", "submitter": "Kun Yuan", "authors": "Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu and Wei Wu", "title": "Incorporating Convolution Designs into Visual Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the success of Transformers in natural language processing (NLP)\ntasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to\nthe vision domain. However, pure Transformer architectures often require a\nlarge amount of training data or extra supervision to obtain comparable\nperformance with convolutional neural networks (CNNs). To overcome these\nlimitations, we analyze the potential drawbacks when directly borrowing\nTransformer architectures from NLP. Then we propose a new\n\\textbf{Convolution-enhanced image Transformer (CeiT)} which combines the\nadvantages of CNNs in extracting low-level features, strengthening locality,\nand the advantages of Transformers in establishing long-range dependencies.\nThree modifications are made to the original Transformer: \\textbf{1)} instead\nof the straightforward tokenization from raw input images, we design an\n\\textbf{Image-to-Tokens (I2T)} module that extracts patches from generated\nlow-level features; \\textbf{2)} the feed-froward network in each encoder block\nis replaced with a \\textbf{Locally-enhanced Feed-Forward (LeFF)} layer that\npromotes the correlation among neighboring tokens in the spatial dimension;\n\\textbf{3)} a \\textbf{Layer-wise Class token Attention (LCA)} is attached at\nthe top of the Transformer that utilizes the multi-level representations.\n  Experimental results on ImageNet and seven downstream tasks show the\neffectiveness and generalization ability of CeiT compared with previous\nTransformers and state-of-the-art CNNs, without requiring a large amount of\ntraining data and extra CNN teachers. Besides, CeiT models also demonstrate\nbetter convergence with $3\\times$ fewer training iterations, which can reduce\nthe training cost significantly\\footnote{Code and models will be released upon\nacceptance.}.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:16:12 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 11:03:32 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yuan", "Kun", ""], ["Guo", "Shaopeng", ""], ["Liu", "Ziwei", ""], ["Zhou", "Aojun", ""], ["Yu", "Fengwei", ""], ["Wu", "Wei", ""]]}, {"id": "2103.11820", "submitter": "Dige Ai", "authors": "Dige Ai, Hong Zhang", "title": "GPNAS: A Neural Network Architecture Search Framework Based on Graphical\n  Predictor", "comments": "12 pages,15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, the problems encountered in Neural Architecture Search (NAS)\ntraining are not simple problems, but often a series of difficult combinations\n(wrong compensation estimation, curse of dimension, overfitting, high\ncomplexity, etc.). In this paper, we propose a framework to decouple network\nstructure from operator search space, and use two BOHBs to search\nalternatively. Considering that activation function and initialization are also\nimportant parts of neural network, the generalization ability of the model will\nbe affected. We introduce an activation function and an initialization method\ndomain, and add them into the operator search space to form a generalized\nsearch space, so as to improve the generalization ability of the child model.\nWe then trained a GCN-based predictor using feedback from the child model. This\ncan not only improve the search efficiency, but also solve the problem of\ndimension curse. Next, unlike other NAS studies, we used predictors to analyze\nthe stability of different network structures. Finally, we applied our\nframework to neural structure search and achieved significant improvements on\nmultiple datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 06:51:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 01:21:39 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 09:18:04 GMT"}, {"version": "v4", "created": "Wed, 31 Mar 2021 02:18:39 GMT"}, {"version": "v5", "created": "Thu, 8 Apr 2021 06:42:34 GMT"}, {"version": "v6", "created": "Thu, 8 Jul 2021 09:15:05 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ai", "Dige", ""], ["Zhang", "Hong", ""]]}, {"id": "2103.11832", "submitter": "Xi Li", "authors": "Peng Sun, Wenhu Zhang, Huanyu Wang, Songyuan Li, Xi Li", "title": "Deep RGB-D Saliency Detection with Depth-Sensitive Attention and\n  Automatic Multi-Modal Fusion", "comments": "Accepted by CVPR2021, Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-D salient object detection (SOD) is usually formulated as a problem of\nclassification or regression over two modalities, i.e., RGB and depth. Hence,\neffective RGBD feature modeling and multi-modal feature fusion both play a\nvital role in RGB-D SOD. In this paper, we propose a depth-sensitive RGB\nfeature modeling scheme using the depth-wise geometric prior of salient\nobjects. In principle, the feature modeling scheme is carried out in a\ndepth-sensitive attention module, which leads to the RGB feature enhancement as\nwell as the background distraction reduction by capturing the depth geometry\nprior. Moreover, to perform effective multi-modal feature fusion, we further\npresent an automatic architecture search approach for RGB-D SOD, which does\nwell in finding out a feasible architecture from our specially designed\nmulti-modal multi-scale search space. Extensive experiments on seven standard\nbenchmarks demonstrate the effectiveness of the proposed approach against the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:28:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sun", "Peng", ""], ["Zhang", "Wenhu", ""], ["Wang", "Huanyu", ""], ["Li", "Songyuan", ""], ["Li", "Xi", ""]]}, {"id": "2103.11833", "submitter": "Zhou Daquan", "authors": "Daquan Zhou, Xiaojie Jin, Xiaochen Lian, Linjie Yang, Yujing Xue,\n  Qibin Hou, Jiashi Feng", "title": "AutoSpace: Neural Architecture Search with Less Human Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current neural architecture search (NAS) algorithms still require expert\nknowledge and effort to design a search space for network construction. In this\npaper, we consider automating the search space design to minimize human\ninterference, which however faces two challenges: the explosive complexity of\nthe exploration space and the expensive computation cost to evaluate the\nquality of different search spaces. To solve them, we propose a novel\ndifferentiable evolutionary framework named AutoSpace, which evolves the search\nspace to an optimal one with following novel techniques: a differentiable\nfitness scoring function to efficiently evaluate the performance of cells and a\nreference architecture to speedup the evolution procedure and avoid falling\ninto sub-optimal solutions. The framework is generic and compatible with\nadditional computational constraints, making it feasible to learn specialized\nsearch spaces that fit different computational budgets. With the learned search\nspace, the performance of recent NAS algorithms can be improved significantly\ncompared with using previously manually designed spaces. Remarkably, the models\ngenerated from the new search space achieve 77.8% top-1 accuracy on ImageNet\nunder the mobile setting (MAdds < 500M), out-performing previous SOTA\nEfficientNet-B0 by 0.7%. All codes will be made public.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:28:56 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Daquan", ""], ["Jin", "Xiaojie", ""], ["Lian", "Xiaochen", ""], ["Yang", "Linjie", ""], ["Xue", "Yujing", ""], ["Hou", "Qibin", ""], ["Feng", "Jiashi", ""]]}, {"id": "2103.11834", "submitter": "Christoph Reich", "authors": "Christoph Reich", "title": "Generation and Simulation of Yeast Microscopy Imagery with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Time-lapse fluorescence microscopy (TLFM) is an important and powerful tool\nin synthetic biological research. Modeling TLFM experiments based on real data\nmay enable researchers to repeat certain experiments with minor effort. This\nthesis is a study towards deep learning-based modeling of TLFM experiments on\nthe image level. The modeling of TLFM experiments, by way of the example of\ntrapped yeast cells, is split into two tasks. The first task is to generate\nsynthetic image data based on real image data. To approach this problem, a\nnovel generative adversarial network, for conditionalized and unconditionalized\nimage generation, is proposed. The second task is the simulation of brightfield\nmicroscopy images over multiple discrete time-steps. To tackle this simulation\ntask an advanced future frame prediction model is introduced. The proposed\nmodels are trained and tested on a novel dataset that is presented in this\nthesis. The obtained results showed that the modeling of TLFM experiments, with\ndeep learning, is a proper approach, but requires future research to\neffectively model real-world experiments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:30:24 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 01:27:34 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 13:28:37 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Reich", "Christoph", ""]]}, {"id": "2103.11886", "submitter": "Zhou Daquan", "authors": "Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian,\n  Zihang Jiang, Qibin Hou, Jiashi Feng", "title": "DeepViT: Towards Deeper Vision Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformers (ViTs) have been successfully applied in image\nclassification tasks recently. In this paper, we show that, unlike convolution\nneural networks (CNNs)that can be improved by stacking more convolutional\nlayers, the performance of ViTs saturate fast when scaled to be deeper. More\nspecifically, we empirically observe that such scaling difficulty is caused by\nthe attention collapse issue: as the transformer goes deeper, the attention\nmaps gradually become similar and even much the same after certain layers. In\nother words, the feature maps tend to be identical in the top layers of deep\nViT models. This fact demonstrates that in deeper layers of ViTs, the\nself-attention mechanism fails to learn effective concepts for representation\nlearning and hinders the model from getting expected performance gain. Based on\nabove observation, we propose a simple yet effective method, named\nRe-attention, to re-generate the attention maps to increase their diversity at\ndifferent layers with negligible computation and memory cost. The pro-posed\nmethod makes it feasible to train deeper ViT models with consistent performance\nimprovements via minor modification to existing ViT models. Notably, when\ntraining a deep ViT model with 32 transformer blocks, the Top-1 classification\naccuracy can be improved by 1.6% on ImageNet. Code is publicly available at\nhttps://github.com/zhoudaquan/dvit_repo.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:32:07 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 14:45:44 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 03:49:56 GMT"}, {"version": "v4", "created": "Mon, 19 Apr 2021 07:06:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhou", "Daquan", ""], ["Kang", "Bingyi", ""], ["Jin", "Xiaojie", ""], ["Yang", "Linjie", ""], ["Lian", "Xiaochen", ""], ["Jiang", "Zihang", ""], ["Hou", "Qibin", ""], ["Feng", "Jiashi", ""]]}, {"id": "2103.11887", "submitter": "Wandong Zhang", "authors": "Yimin Yang, Wandong Zhang, Jonathan Wu, Will Zhao, Ao Chen", "title": "Deconvolution-and-convolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D Convolutional neural network (CNN) has arguably become the de facto\nstandard for computer vision tasks. Recent findings, however, suggest that CNN\nmay not be the best option for 1D pattern recognition, especially for datasets\nwith over 1 M training samples, e.g., existing CNN-based methods for 1D signals\nare highly reliant on human pre-processing. Common practices include utilizing\ndiscrete Fourier transform (DFT) to reconstruct 1D signal into 2D array. To add\nto extant knowledge, in this paper, a novel 1D data processing algorithm is\nproposed for 1D big data analysis through learning a deep\ndeconvolutional-convolutional network. Rather than resorting to human-based\ntechniques, we employed deconvolution layers to convert 1 D signals into 2D\ndata. On top of the deconvolution model, the data was identified by a 2D CNN.\nCompared with the existing 1D signal processing algorithms, DCNet boasts the\nadvantages of less human-made inference and higher generalization performance.\nOur experimental results from a varying number of training patterns (50 K to 11\nM) from classification and regression demonstrate the desirability of our new\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:32:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yang", "Yimin", ""], ["Zhang", "Wandong", ""], ["Wu", "Jonathan", ""], ["Zhao", "Will", ""], ["Chen", "Ao", ""]]}, {"id": "2103.11895", "submitter": "Ruben Hemelings", "authors": "Ruben Hemelings, Bart Elen, Jo\\~ao Barbosa Breda, Matthew B. Blaschko,\n  Patrick De Boever, Ingeborg Stalmans", "title": "Glaucoma detection beyond the optic disc: The importance of the\n  peripapillary region using explainable deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Today, a large number of glaucoma cases remain undetected, resulting in\nirreversible blindness. In a quest for cost-effective screening, deep\nlearning-based methods are being evaluated to detect glaucoma from color fundus\nimages. Although unprecedented sensitivity and specificity values are reported,\nrecent glaucoma detection deep learning models lack in decision transparency.\nHere, we propose a methodology that advances explainable deep learning in the\nfield of glaucoma detection and vertical cup-disc ratio (VCDR), an important\nrisk factor. We trained and evaluated a total of 64 deep learning models using\nfundus images that undergo a certain cropping policy. We defined the circular\ncrop radius as a percentage of image size, centered on the optic nerve head\n(ONH), with an equidistant spaced range from 10%-60% (ONH crop policy). The\ninverse of the cropping mask was also applied to quantify the performance of\nmodels trained on ONH information exclusively (periphery crop policy). The\nperformance of the models evaluated on original images resulted in an area\nunder the curve (AUC) of 0.94 [95% CI: 0.92-0.96] for glaucoma detection, and a\ncoefficient of determination (R^2) equal to 77% [95% CI: 0.77-0.79] for VCDR\nestimation. Models that were trained on images with absence of the ONH are\nstill able to obtain significant performance (0.88 [95% CI: 0.85-0.90] AUC for\nglaucoma detection and 37% [95% CI: 0.35-0.40] R^2 score for VCDR estimation in\nthe most extreme setup of 60% ONH crop). We validated our glaucoma detection\nmodels on a recent public data set (REFUGE) that contains images captured with\na different camera, still achieving an AUC of 0.80 [95% CI: 0.76-0.84] when ONH\ncrop policy of 60% image size was applied. Our findings provide the first\nirrefutable evidence that deep learning can detect glaucoma from fundus image\nregions outside the ONH.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:42:02 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hemelings", "Ruben", ""], ["Elen", "Bart", ""], ["Breda", "Jo\u00e3o Barbosa", ""], ["Blaschko", "Matthew B.", ""], ["De Boever", "Patrick", ""], ["Stalmans", "Ingeborg", ""]]}, {"id": "2103.11897", "submitter": "Sen He", "authors": "Sen He, Wentong Liao, Michael Ying Yang, Yongxin Yang, Yi-Zhe Song,\n  Bodo Rosenhahn, Tao Xiang", "title": "Context-Aware Layout to Image Generation with Enhanced Object Appearance", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A layout to image (L2I) generation model aims to generate a complicated image\ncontaining multiple objects (things) against natural background (stuff),\nconditioned on a given layout. Built upon the recent advances in generative\nadversarial networks (GANs), existing L2I models have made great progress.\nHowever, a close inspection of their generated images reveals two major\nlimitations: (1) the object-to-object as well as object-to-stuff relations are\noften broken and (2) each object's appearance is typically distorted lacking\nthe key defining characteristics associated with the object class. We argue\nthat these are caused by the lack of context-aware object and stuff feature\nencoding in their generators, and location-sensitive appearance representation\nin their discriminators. To address these limitations, two new modules are\nproposed in this work. First, a context-aware feature transformation module is\nintroduced in the generator to ensure that the generated feature encoding of\neither object or stuff is aware of other co-existing objects/stuff in the\nscene. Second, instead of feeding location-insensitive image features to the\ndiscriminator, we use the Gram matrix computed from the feature maps of the\ngenerated object images to preserve location-sensitive information, resulting\nin much enhanced object appearance. Extensive experiments show that the\nproposed method achieves state-of-the-art performance on the COCO-Thing-Stuff\nand Visual Genome benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:43:25 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["He", "Sen", ""], ["Liao", "Wentong", ""], ["Yang", "Michael Ying", ""], ["Yang", "Yongxin", ""], ["Song", "Yi-Zhe", ""], ["Rosenhahn", "Bodo", ""], ["Xiang", "Tao", ""]]}, {"id": "2103.11912", "submitter": "Gon\\c{c}alo Mordido", "authors": "Gon\\c{c}alo Mordido, Haojin Yang, Christoph Meinel", "title": "Evaluating Post-Training Compression in GANs using Locality-Sensitive\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of the compression effects in generative adversarial networks\n(GANs) after training, i.e. without any fine-tuning, remains an unstudied,\nalbeit important, topic with the increasing trend of their computation and\nmemory requirements. While existing works discuss the difficulty of compressing\nGANs during training, requiring novel methods designed with the instability of\nGANs training in mind, we show that existing compression methods (namely\nclipping and quantization) may be directly applied to compress GANs\npost-training, without any additional changes. High compression levels may\ndistort the generated set, likely leading to an increase of outliers that may\nnegatively affect the overall assessment of existing k-nearest neighbor (KNN)\nbased metrics. We propose two new precision and recall metrics based on\nlocality-sensitive hashing (LSH), which, on top of increasing the outlier\nrobustness, decrease the complexity of assessing an evaluation sample against\n$n$ reference samples from $O(n)$ to $O(\\log(n))$, if using LSH and KNN, and to\n$O(1)$, if only applying LSH. We show that low-bit compression of several\npre-trained GANs on multiple datasets induces a trade-off between precision and\nrecall, retaining sample quality while sacrificing sample diversity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:55:24 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Mordido", "Gon\u00e7alo", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "2103.11920", "submitter": "Jonas Pfeiffer", "authors": "Gregor Geigle, Jonas Pfeiffer, Nils Reimers, Ivan Vuli\\'c, Iryna\n  Gurevych", "title": "Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for\n  Improved Cross-Modal Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art approaches to cross-modal retrieval process text and\nvisual input jointly, relying on Transformer-based architectures with\ncross-attention mechanisms that attend over all words and objects in an image.\nWhile offering unmatched retrieval performance, such models: 1) are typically\npretrained from scratch and thus less scalable, 2) suffer from huge retrieval\nlatency and inefficiency issues, which makes them impractical in realistic\napplications. To address these crucial gaps towards both improved and efficient\ncross-modal retrieval, we propose a novel fine-tuning framework which turns any\npretrained text-image multi-modal model into an efficient retrieval model. The\nframework is based on a cooperative retrieve-and-rerank approach which\ncombines: 1) twin networks to separately encode all items of a corpus, enabling\nefficient initial retrieval, and 2) a cross-encoder component for a more\nnuanced (i.e., smarter) ranking of the retrieved small set of items. We also\npropose to jointly fine-tune the two components with shared weights, yielding a\nmore parameter-efficient model. Our experiments on a series of standard\ncross-modal retrieval benchmarks in monolingual, multilingual, and zero-shot\nsetups, demonstrate improved accuracy and huge efficiency benefits over the\nstate-of-the-art cross-encoders.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:08:06 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Geigle", "Gregor", ""], ["Pfeiffer", "Jonas", ""], ["Reimers", "Nils", ""], ["Vuli\u0107", "Ivan", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2103.11922", "submitter": "Shan You", "authors": "Xiu Su, Tao Huang, Yanxi Li, Shan You, Fei Wang, Chen Qian, Changshui\n  Zhang, Chang Xu", "title": "Prioritized Architecture Sampling with Monto-Carlo Tree Search", "comments": "Accepted by CVPR2021. We also release a NAS benchmark on the one-shot\n  MobileNetV2 search space", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot neural architecture search (NAS) methods significantly reduce the\nsearch cost by considering the whole search space as one network, which only\nneeds to be trained once. However, current methods select each operation\nindependently without considering previous layers. Besides, the historical\ninformation obtained with huge computation cost is usually used only once and\nthen discarded. In this paper, we introduce a sampling strategy based on Monte\nCarlo tree search (MCTS) with the search space modeled as a Monte Carlo tree\n(MCT), which captures the dependency among layers. Furthermore, intermediate\nresults are stored in the MCT for the future decision and a better\nexploration-exploitation balance. Concretely, MCT is updated using the training\nloss as a reward to the architecture performance; for accurately evaluating the\nnumerous nodes, we propose node communication and hierarchical node selection\nmethods in the training and search stages, respectively, which make better uses\nof the operation rewards and hierarchical information. Moreover, for a fair\ncomparison of different NAS methods, we construct an open-source NAS benchmark\nof a macro search space evaluated on CIFAR-10, namely NAS-Bench-Macro.\nExtensive experiments on NAS-Bench-Macro and ImageNet demonstrate that our\nmethod significantly improves search efficiency and performance. For example,\nby only searching $20$ architectures, our obtained architecture achieves\n$78.0\\%$ top-1 accuracy with 442M FLOPs on ImageNet. Code (Benchmark) is\navailable at: \\url{https://github.com/xiusu/NAS-Bench-Macro}.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:09:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Su", "Xiu", ""], ["Huang", "Tao", ""], ["Li", "Yanxi", ""], ["You", "Shan", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Zhang", "Changshui", ""], ["Xu", "Chang", ""]]}, {"id": "2103.12002", "submitter": "Purvi Goel", "authors": "Purvi Goel and Li Chen", "title": "On the Robustness of Monte Carlo Dropout Trained with Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The memorization effect of deep learning hinders its performance to\neffectively generalize on test set when learning with noisy labels. Prior study\nhas discovered that epistemic uncertainty techniques are robust when trained\nwith noisy labels compared with neural networks without uncertainty estimation.\nThey obtain prolonged memorization effect and better generalization performance\nunder the adversarial setting of noisy labels. Due to its superior performance\namongst other selected epistemic uncertainty methods under noisy labels, we\nfocus on Monte Carlo Dropout (MCDropout) and investigate why it is robust when\ntrained with noisy labels. Through empirical studies on datasets MNIST,\nCIFAR-10, Animal-10n, we deep dive into three aspects of MCDropout under noisy\nlabel setting: 1. efficacy: understanding the learning behavior and test\naccuracy of MCDropout when training set contains artificially generated or\nnaturally embedded label noise; 2. representation volatility: studying the\nresponsiveness of neurons by examining the mean and standard deviation on each\nneuron's activation; 3. network sparsity: investigating the network support of\nMCDropout in comparison with deterministic neural networks. Our findings\nsuggest that MCDropout further sparsifies and regularizes the deterministic\nneural networks and thus provides higher robustness against noisy labels.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 16:52:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Goel", "Purvi", ""], ["Chen", "Li", ""]]}, {"id": "2103.12032", "submitter": "Zachary Teed", "authors": "Zachary Teed and Jia Deng", "title": "Tangent Space Backpropagation for 3D Transformation Groups", "comments": "fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of performing backpropagation for computation graphs\ninvolving 3D transformation groups SO(3), SE(3), and Sim(3). 3D transformation\ngroups are widely used in 3D vision and robotics, but they do not form vector\nspaces and instead lie on smooth manifolds. The standard backpropagation\napproach, which embeds 3D transformations in Euclidean spaces, suffers from\nnumerical difficulties. We introduce a new library, which exploits the group\nstructure of 3D transformations and performs backpropagation in the tangent\nspaces of manifolds. We show that our approach is numerically more stable,\neasier to implement, and beneficial to a diverse set of tasks. Our\nplug-and-play PyTorch library is available at\nhttps://github.com/princeton-vl/lietorch.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:33:30 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 17:26:27 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Teed", "Zachary", ""], ["Deng", "Jia", ""]]}, {"id": "2103.12040", "submitter": "Akshay Rangesh", "authors": "Hala Abualsaud, Sean Liu, David Lu, Kenny Situ, Akshay Rangesh and\n  Mohan M. Trivedi", "title": "LaneAF: Robust Multi-Lane Detection with Affinity Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents an approach to lane detection involving the prediction of\nbinary segmentation masks and per-pixel affinity fields. These affinity fields,\nalong with the binary masks, can then be used to cluster lane pixels\nhorizontally and vertically into corresponding lane instances in a\npost-processing step. This clustering is achieved through a simple row-by-row\ndecoding process with little overhead; such an approach allows LaneAF to detect\na variable number of lanes without assuming a fixed or maximum number of lanes.\nMoreover, this form of clustering is more interpretable in comparison to\nprevious visual clustering approaches, and can be analyzed to identify and\ncorrect sources of error. Qualitative and quantitative results obtained on\npopular lane detection datasets demonstrate the model's ability to detect and\ncluster lanes effectively and robustly. Our proposed approach sets a new\nstate-of-the-art on the challenging CULane dataset and the recently introduced\nUnsupervised LLAMAS dataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:43:19 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 00:52:49 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 21:37:12 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Abualsaud", "Hala", ""], ["Liu", "Sean", ""], ["Lu", "David", ""], ["Situ", "Kenny", ""], ["Rangesh", "Akshay", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "2103.12051", "submitter": "Vikash Sehwag", "authors": "Vikash Sehwag, Mung Chiang, Prateek Mittal", "title": "SSD: A Unified Framework for Self-Supervised Outlier Detection", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We ask the following question: what training information is required to\ndesign an effective outlier/out-of-distribution (OOD) detector, i.e., detecting\nsamples that lie far away from the training distribution? Since unlabeled data\nis easily accessible for many applications, the most compelling approach is to\ndevelop detectors based on only unlabeled in-distribution data. However, we\nobserve that most existing detectors based on unlabeled data perform poorly,\noften equivalent to a random prediction. In contrast, existing state-of-the-art\nOOD detectors achieve impressive performance but require access to fine-grained\ndata labels for supervised training. We propose SSD, an outlier detector based\non only unlabeled in-distribution data. We use self-supervised representation\nlearning followed by a Mahalanobis distance based detection in the feature\nspace. We demonstrate that SSD outperforms most existing detectors based on\nunlabeled data by a large margin. Additionally, SSD even achieves performance\non par, and sometimes even better, with supervised training based detectors.\nFinally, we expand our detection framework with two key extensions. First, we\nformulate few-shot OOD detection, in which the detector has access to only one\nto five samples from each class of the targeted OOD dataset. Second, we extend\nour framework to incorporate training data labels, if available. We find that\nour novel detection framework based on SSD displays enhanced performance with\nthese extensions, and achieves state-of-the-art performance. Our code is\npublicly available at https://github.com/inspire-group/SSD.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:51:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sehwag", "Vikash", ""], ["Chiang", "Mung", ""], ["Mittal", "Prateek", ""]]}, {"id": "2103.12068", "submitter": "Aqsa Saeed Qureshi", "authors": "Aqsa Saeed Qureshi and Teemu Roos", "title": "Transfer Learning with Ensembles of Deep Neural Networks for Skin Cancer\n  Detection in Imbalanced Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several machine learning techniques for accurate detection of skin cancer\nfrom medical images have been reported. Many of these techniques are based on\npre-trained convolutional neural networks (CNNs), which enable training the\nmodels based on limited amounts of training data. However, the classification\naccuracy of these models still tends to be severely limited by the scarcity of\nrepresentative images from malignant tumours. We propose a novel ensemble-based\nCNN architecture where multiple CNN models, some of which are pre-trained and\nsome are trained only on the data at hand, along with auxiliary data in the\nform of metadata associated with the input images, are combined using a\nmeta-learner. The proposed approach improves the model's ability to handle\nlimited and imbalanced data. We demonstrate the benefits of the proposed\ntechnique using a dataset with 33126 dermoscopic images from 2056 patients. We\nevaluate the performance of the proposed technique in terms of the F1-measure,\narea under the ROC curve (AUC-ROC), and area under the PR-curve (AUC-PR), and\ncompare it with that of seven different benchmark methods, including two recent\nCNN-based techniques. The proposed technique compares favourably in terms of\nall the evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 06:04:45 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 15:05:48 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 18:06:28 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 09:33:03 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Qureshi", "Aqsa Saeed", ""], ["Roos", "Teemu", ""]]}, {"id": "2103.12091", "submitter": "Hao Tang", "authors": "Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, Elisa Ricci", "title": "Transformers Solve the Limited Receptive Field for Monocular Depth\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convolutional neural networks have shown a tremendous impact on various\ncomputer vision tasks, they generally demonstrate limitations in explicitly\nmodeling long-range dependencies due to the intrinsic locality of the\nconvolution operation. Transformers, initially designed for natural language\nprocessing tasks, have emerged as alternative architectures with innate global\nself-attention mechanisms to capture long-range dependencies. In this paper, we\npropose TransDepth, an architecture which benefits from both convolutional\nneural networks and transformers. To avoid the network to loose its ability to\ncapture local-level details due to the adoption of transformers, we propose a\nnovel decoder which employs on attention mechanisms based on gates. Notably,\nthis is the first paper which applies transformers into pixel-wise prediction\nproblems involving continuous labels (i.e., monocular depth prediction and\nsurface normal estimation). Extensive experiments demonstrate that the proposed\nTransDepth achieves state-of-the-art performance on three challenging datasets.\nThe source code and trained models are available at\nhttps://github.com/ygjwd12345/TransDepth.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:00:13 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Yang", "Guanglei", ""], ["Tang", "Hao", ""], ["Ding", "Mingli", ""], ["Sebe", "Nicu", ""], ["Ricci", "Elisa", ""]]}, {"id": "2103.12106", "submitter": "David Honz\\'atko", "authors": "David Honz\\'atko, Engin T\\\"uretken, Pascal Fua, L. Andrea Dunbar", "title": "Leveraging Spatial and Photometric Context for Calibrated Non-Lambertian\n  Photometric Stereo", "comments": "Submitted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating a surface shape from its observed reflectance\nproperties still remains a challenging task in computer vision. The presence of\nglobal illumination effects such as inter-reflections or cast shadows makes the\ntask particularly difficult for non-convex real-world surfaces.\nState-of-the-art methods for calibrated photometric stereo address these issues\nusing convolutional neural networks (CNNs) that primarily aim to capture either\nthe spatial context among adjacent pixels or the photometric one formed by\nilluminating a sample from adjacent directions.\n  In this paper, we bridge these two objectives and introduce an efficient\nfully-convolutional architecture that can leverage both spatial and photometric\ncontext simultaneously. In contrast to existing approaches that rely on\nstandard 2D CNNs and regress directly to surface normals, we argue that using\nseparable 4D convolutions and regressing to 2D Gaussian heat-maps severely\nreduces the size of the network and makes inference more efficient. Our\nexperimental results on a real-world photometric stereo benchmark show that the\nproposed approach outperforms the existing methods both in efficiency and\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:06:58 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Honz\u00e1tko", "David", ""], ["T\u00fcretken", "Engin", ""], ["Fua", "Pascal", ""], ["Dunbar", "L. Andrea", ""]]}, {"id": "2103.12115", "submitter": "Alexander Mathis", "authors": "Lucas Stoffl and Maxime Vidal and Alexander Mathis", "title": "End-to-End Trainable Multi-Instance Pose Estimation with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new end-to-end trainable approach for multi-instance pose\nestimation by combining a convolutional neural network with a transformer. We\ncast multi-instance pose estimation from images as a direct set prediction\nproblem. Inspired by recent work on end-to-end trainable object detection with\ntransformers, we use a transformer encoder-decoder architecture together with a\nbipartite matching scheme to directly regress the pose of all individuals in a\ngiven image. Our model, called POse Estimation Transformer (POET), is trained\nusing a novel set-based global loss that consists of a keypoint loss, a\nkeypoint visibility loss, a center loss and a class loss. POET reasons about\nthe relations between detected humans and the full image context to directly\npredict the poses in parallel. We show that POET can achieve high accuracy on\nthe challenging COCO keypoint detection task. To the best of our knowledge,\nthis model is the first end-to-end trainable multi-instance human pose\nestimation method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:19:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Stoffl", "Lucas", ""], ["Vidal", "Maxime", ""], ["Mathis", "Alexander", ""]]}, {"id": "2103.12155", "submitter": "Satvik Garg", "authors": "Satvik Garg and Somya Garg", "title": "Prediction of lung and colon cancer through analysis of\n  histopathological images by utilizing Pre-trained CNN models with\n  visualization of class activation and saliency maps", "comments": "This Paper has been accepted and presented in 2nd Asia Pacific\n  Digital Image Processing (ADIP) Workshop of 3rd Artificial Intelligence and\n  Cloud Computing Conference (AICCC 2020), ACM, Japan. 2020 December 3-5. The\n  publication can be accessed from the proceedings of the AICCC 2020\n  conference. (https://dl.acm.org/doi/10.1145/3442536.3442543)", "journal-ref": null, "doi": "10.1145/3442536.3442543", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colon and Lung cancer is one of the most perilous and dangerous ailments that\nindividuals are enduring worldwide and has become a general medical problem. To\nlessen the risk of death, a legitimate and early finding is particularly\nrequired. In any case, it is a truly troublesome task that depends on the\nexperience of histopathologists. If a histologist is under-prepared it may even\nhazard the life of a patient. As of late, deep learning has picked up energy,\nand it is being valued in the analysis of Medical Imaging. This paper intends\nto utilize and alter the current pre-trained CNN-based model to identify lung\nand colon cancer utilizing histopathological images with better augmentation\ntechniques. In this paper, eight distinctive Pre-trained CNN models, VGG16,\nNASNetMobile, InceptionV3, InceptionResNetV2, ResNet50, Xception, MobileNet,\nand DenseNet169 are trained on LC25000 dataset. The model performances are\nassessed on precision, recall, f1score, accuracy, and auroc score. The results\nexhibit that all eight models accomplished noteworthy results ranging from 96%\nto 100% accuracy. Subsequently, GradCAM and SmoothGrad are also used to picture\nthe attention images of Pre-trained CNN models classifying malignant and benign\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:06:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Garg", "Satvik", ""], ["Garg", "Somya", ""]]}, {"id": "2103.12162", "submitter": "Hamid Sarmadi", "authors": "Hamid Sarmadi, Rafael Mu\\~noz-Salinas, M.\\'Alvaro Berb\\'is, Antonio\n  Luna, Rafael Medina-Carnicer", "title": "3D Reconstruction and Alignment by Consumer RGB-D Sensors and Fiducial\n  Planar Markers for Patient Positioning in Radiation Therapy", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.cmpb.2019.105004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND AND OBJECTIVE: Patient positioning is a crucial step in radiation\ntherapy, for which non-invasive methods have been developed based on surface\nreconstruction using optical 3D imaging. However, most solutions need expensive\nspecialized hardware and a careful calibration procedure that must be repeated\nover time.This paper proposes a fast and cheap patient positioning method based\non inexpensive consumer level RGB-D sensors.\n  METHODS: The proposed method relies on a 3D reconstruction approach that\nfuses, in real-time, artificial and natural visual landmarks recorded from a\nhand-held RGB-D sensor. The video sequence is transformed into a set of\nkeyframes with known poses, that are later refined to obtain a realistic 3D\nreconstruction of the patient. The use of artificial landmarks allows our\nmethod to automatically align the reconstruction to a reference one, without\nthe need of calibrating the system with respect to the linear accelerator\ncoordinate system.\n  RESULTS:The experiments conducted show that our method obtains a median of 1\ncm in translational error, and 1 degree of rotational error with respect to\nreference pose. Additionally, the proposed method shows as visual output\noverlayed poses (from the reference and the current scene) and an error map\nthat can be used to correct the patient's current pose to match the reference\npose.\n  CONCLUSIONS: A novel approach to obtain 3D body reconstructions for patient\npositioning without requiring expensive hardware or dedicated graphic cards is\nproposed. The method can be used to align in real time the patient's current\npose to a preview pose, which is a relevant step in radiation therapy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:20:59 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Sarmadi", "Hamid", ""], ["Mu\u00f1oz-Salinas", "Rafael", ""], ["Berb\u00eds", "M. \u00c1lvaro", ""], ["Luna", "Antonio", ""], ["Medina-Carnicer", "Rafael", ""]]}, {"id": "2103.12171", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Yu Cheng, Zhe Gan, Jianfeng Wang, Lijuan Wang,\n  Zhangyang Wang, Jingjing Liu", "title": "Adversarial Feature Augmentation and Normalization for Visual\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision take advantage of adversarial data\naugmentation to ameliorate the generalization ability of classification models.\nHere, we present an effective and efficient alternative that advocates\nadversarial augmentation on intermediate feature embeddings, instead of relying\non computationally-expensive pixel-level perturbations. We propose Adversarial\nFeature Augmentation and Normalization (A-FAN), which (i) first augments visual\nrecognition models with adversarial features that integrate flexible scales of\nperturbation strengths, (ii) then extracts adversarial feature statistics from\nbatch normalization, and re-injects them into clean features through feature\nnormalization. We validate the proposed approach across diverse visual\nrecognition tasks with representative backbone networks, including ResNets and\nEfficientNets for classification, Faster-RCNN for detection, and Deeplab V3+\nfor segmentation. Extensive experiments show that A-FAN yields consistent\ngeneralization improvement over strong baselines across various datasets for\nclassification, detection and segmentation tasks, such as CIFAR-10, CIFAR-100,\nImageNet, Pascal VOC2007, Pascal VOC2012, COCO2017, and Cityspaces.\nComprehensive ablation studies and detailed analyses also demonstrate that\nadding perturbations to specific modules and layers of\nclassification/detection/segmentation backbones yields optimal performance.\nCodes and pre-trained models will be made available at:\nhttps://github.com/VITA-Group/CV_A-FAN.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:36:34 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chen", "Tianlong", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Wang", "Jianfeng", ""], ["Wang", "Lijuan", ""], ["Wang", "Zhangyang", ""], ["Liu", "Jingjing", ""]]}, {"id": "2103.12201", "submitter": "Shlok Mishra", "authors": "Shlok Kumar Mishra and Kuntal Sengupta and Max Horowitz-Gelb and\n  Wen-Sheng Chu and Sofien Bouaziz and David Jacobs", "title": "Improved Detection of Face Presentation Attacks Using Image\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Presentation attack detection (PAD) is a critical component in secure face\nauthentication. We present a PAD algorithm to distinguish face spoofs generated\nby a photograph of a subject from live images. Our method uses an image\ndecomposition network to extract albedo and normal. The domain gap between the\nreal and spoof face images leads to easily identifiable differences, especially\nbetween the recovered albedo maps. We enhance this domain gap by retraining\nexisting methods using supervised contrastive loss. We present empirical and\ntheoretical analysis that demonstrates that the contrast and lighting effects\ncan play a significant role in PAD; these show up particularly in the recovered\nalbedo. Finally, we demonstrate that by combining all of these methods we\nachieve state-of-the-art results on datasets such as CelebA-Spoof, OULU and\nCASIA-SURF.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:15:17 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Mishra", "Shlok Kumar", ""], ["Sengupta", "Kuntal", ""], ["Horowitz-Gelb", "Max", ""], ["Chu", "Wen-Sheng", ""], ["Bouaziz", "Sofien", ""], ["Jacobs", "David", ""]]}, {"id": "2103.12204", "submitter": "Long Chen", "authors": "Long Chen, Zhihong Jiang, Jun Xiao, Wei Liu", "title": "Human-like Controllable Image Captioning with Verb-specific Semantic\n  Roles", "comments": "Accepted by CVPR 2021. The code is available at:\n  https://github.com/mad-red/VSR-guided-CIC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controllable Image Captioning (CIC) -- generating image descriptions\nfollowing designated control signals -- has received unprecedented attention\nover the last few years. To emulate the human ability in controlling caption\ngeneration, current CIC studies focus exclusively on control signals concerning\nobjective properties, such as contents of interest or descriptive patterns.\nHowever, we argue that almost all existing objective control signals have\noverlooked two indispensable characteristics of an ideal control signal: 1)\nEvent-compatible: all visual contents referred to in a single sentence should\nbe compatible with the described activity. 2) Sample-suitable: the control\nsignals should be suitable for a specific image sample. To this end, we propose\na new control signal for CIC: Verb-specific Semantic Roles (VSR). VSR consists\nof a verb and some semantic roles, which represents a targeted activity and the\nroles of entities involved in this activity. Given a designated VSR, we first\ntrain a grounded semantic role labeling (GSRL) model to identify and ground all\nentities for each role. Then, we propose a semantic structure planner (SSP) to\nlearn human-like descriptive semantic structures. Lastly, we use a role-shift\ncaptioning model to generate the captions. Extensive experiments and ablations\ndemonstrate that our framework can achieve better controllability than several\nstrong baselines on two challenging CIC benchmarks. Besides, we can generate\nmulti-level diverse captions easily. The code is available at:\nhttps://github.com/mad-red/VSR-guided-CIC.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:17:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chen", "Long", ""], ["Jiang", "Zhihong", ""], ["Xiao", "Jun", ""], ["Liu", "Wei", ""]]}, {"id": "2103.12209", "submitter": "Akhil Gurram", "authors": "Akhil Gurram, Ahmet Faruk Tuna, Fengyi Shen, Onay Urfalioglu, and\n  Antonio M. L\\'opez", "title": "Monocular Depth Estimation through Virtual-world Supervision and\n  Real-world SfM Self-Supervision", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth information is essential for on-board perception in autonomous driving\nand driver assistance. Monocular depth estimation (MDE) is very appealing since\nit allows for appearance and depth being on direct pixelwise correspondence\nwithout further calibration. Best MDE models are based on Convolutional Neural\nNetworks (CNNs) trained in a supervised manner, i.e., assuming pixelwise ground\ntruth (GT). Usually, this GT is acquired at training time through a calibrated\nmulti-modal suite of sensors. However, also using only a monocular system at\ntraining time is cheaper and more scalable. This is possible by relying on\nstructure-from-motion (SfM) principles to generate self-supervision.\nNevertheless, problems of camouflaged objects, visibility changes,\nstatic-camera intervals, textureless areas, and scale ambiguity, diminish the\nusefulness of such self-supervision. In this paper, we perform monocular depth\nestimation by virtual-world supervision (MonoDEVS) and real-world SfM\nself-supervision. We compensate the SfM self-supervision limitations by\nleveraging virtual-world images with accurate semantic and depth supervision\nand addressing the virtual-to-real domain gap. Our MonoDEVSNet outperforms\nprevious MDE CNNs trained on monocular and even stereo sequences.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:33:49 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Gurram", "Akhil", ""], ["Tuna", "Ahmet Faruk", ""], ["Shen", "Fengyi", ""], ["Urfalioglu", "Onay", ""], ["L\u00f3pez", "Antonio M.", ""]]}, {"id": "2103.12212", "submitter": "Ange Lou", "authors": "Ange Lou, Murray Loew", "title": "CFPNet: Channel-wise Feature Pyramid for Real-Time Semantic Segmentation", "comments": "Accepted by ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic segmentation is playing a more important role in computer\nvision, due to the growing demand for mobile devices and autonomous driving.\nTherefore, it is very important to achieve a good trade-off among performance,\nmodel size and inference speed. In this paper, we propose a Channel-wise\nFeature Pyramid (CFP) module to balance those factors. Based on the CFP module,\nwe built CFPNet for real-time semantic segmentation which applied a series of\ndilated convolution channels to extract effective features. Experiments on\nCityscapes and CamVid datasets show that the proposed CFPNet achieves an\neffective combination of those factors. For the Cityscapes test dataset, CFPNet\nachieves 70.1% class-wise mIoU with only 0.55 million parameters and 2.5 MB\nmemory. The inference speed can reach 30 FPS on a single RTX 2080Ti GPU with a\n1024x2048-pixel image.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:39:30 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 19:45:09 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Lou", "Ange", ""], ["Loew", "Murray", ""]]}, {"id": "2103.12213", "submitter": "Michael Weber", "authors": "Michael Weber, Tassilo Wald, J. Marius Z\\\"ollner", "title": "Temporal Feature Networks for CNN based Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For reliable environment perception, the use of temporal information is\nessential in some situations. Especially for object detection, sometimes a\nsituation can only be understood in the right perspective through temporal\ninformation. Since image-based object detectors are currently based almost\nexclusively on CNN architectures, an extension of their feature extraction with\ntemporal features seems promising.\n  Within this work we investigate different architectural components for a\nCNN-based temporal information extraction. We present a Temporal Feature\nNetwork which is based on the insights gained from our architectural\ninvestigations. This network is trained from scratch without any ImageNet\ninformation based pre-training as these images are not available with temporal\ninformation. The object detector based on this network is evaluated against the\nnon-temporal counterpart as baseline and achieves competitive results in an\nevaluation on the KITTI object detection dataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:39:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Weber", "Michael", ""], ["Wald", "Tassilo", ""], ["Z\u00f6llner", "J. Marius", ""]]}, {"id": "2103.12216", "submitter": "Mohammad Sabokrou", "authors": "Mozhgan PourKeshavarz, Mohammad Sabokrou", "title": "ZS-IL: Looking Back on Learned Experiences For Zero-Shot Incremental\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Classical deep neural networks are limited in their ability to learn from\nemerging streams of training data. When trained sequentially on new or evolving\ntasks, their performance degrades sharply, making them inappropriate in\nreal-world use cases. Existing methods tackle it by either storing old data\nsamples or only updating a parameter set of DNNs, which, however, demands a\nlarge memory budget or spoils the flexibility of models to learn the\nincremented class distribution. In this paper, we shed light on an on-call\ntransfer set to provide past experiences whenever a new class arises in the\ndata stream. In particular, we propose a Zero-Shot Incremental Learning not\nonly to replay past experiences the model has learned but also to perform this\nin a zero-shot manner. Towards this end, we introduced a memory recovery\nparadigm in which we query the network to synthesize past exemplars whenever a\nnew task (class) emerges. Thus, our method needs no fixed-sized memory, besides\ncalls the proposed memory recovery paradigm to provide past exemplars, named a\ntransfer set in order to mitigate catastrophically forgetting the former\nclasses. Moreover, in contrast with recently proposed methods, the suggested\nparadigm does not desire a parallel architecture since it only relies on the\nlearner network. Compared to the state-of-the-art data techniques without\nbuffering past data samples, ZS-IL demonstrates significantly better\nperformance on the well-known datasets (CIFAR-10, Tiny-ImageNet) in both\nTask-IL and Class-IL settings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:43:20 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["PourKeshavarz", "Mozhgan", ""], ["Sabokrou", "Mohammad", ""]]}, {"id": "2103.12228", "submitter": "Ken C. L. Wong", "authors": "Ken C. L. Wong, Satyananda Kashyap, Mehdi Moradi", "title": "Channel Scaling: A Scale-and-Select Approach for Transfer Learning", "comments": "This paper was accepted by the IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning with pre-trained neural networks is a common strategy for\ntraining classifiers in medical image analysis. Without proper channel\nselections, this often results in unnecessarily large models that hinder\ndeployment and explainability. In this paper, we propose a novel approach to\nefficiently build small and well performing networks by introducing the\nchannel-scaling layers. A channel-scaling layer is attached to each frozen\nconvolutional layer, with the trainable scaling weights inferring the\nimportance of the corresponding feature channels. Unlike the fine-tuning\napproaches, we maintain the weights of the original channels and large datasets\nare not required. By imposing L1 regularization and thresholding on the scaling\nweights, this framework iteratively removes unnecessary feature channels from a\npre-trained model. Using an ImageNet pre-trained VGG16 model, we demonstrate\nthe capabilities of the proposed framework on classifying opacity from chest\nX-ray images. The results show that we can reduce the number of parameters by\n95% while delivering a superior performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 23:26:57 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wong", "Ken C. L.", ""], ["Kashyap", "Satyananda", ""], ["Moradi", "Mehdi", ""]]}, {"id": "2103.12233", "submitter": "Alvaro Leandro Cavalcante Carneiro", "authors": "Alvaro Leandro Cavalcante Carneiro, Lucas de Brito Silva, Denis\n  Henrique Pinheiro Salvadeo", "title": "Efficient sign language recognition system and dataset creation method\n  based on deep learning and image processing", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New deep-learning architectures are created every year, achieving\nstate-of-the-art results in image recognition and leading to the belief that,\nin a few years, complex tasks such as sign language translation will be\nconsiderably easier, serving as a communication tool for the hearing-impaired\ncommunity. On the other hand, these algorithms still need a lot of data to be\ntrained and the dataset creation process is expensive, time-consuming, and\nslow. Thereby, this work aims to investigate techniques of digital image\nprocessing and machine learning that can be used to create a sign language\ndataset effectively. We argue about data acquisition, such as the frames per\nsecond rate to capture or subsample the videos, the background type,\npreprocessing, and data augmentation, using convolutional neural networks and\nobject detection to create an image classifier and comparing the results based\non statistical tests. Different datasets were created to test the hypotheses,\ncontaining 14 words used daily and recorded by different smartphones in the RGB\ncolor system. We achieved an accuracy of 96.38% on the test set and 81.36% on\nthe validation set containing more challenging conditions, showing that 30 FPS\nis the best frame rate subsample to train the classifier, geometric\ntransformations work better than intensity transformations, and artificial\nbackground creation is not effective to model generalization. These trade-offs\nshould be considered in future work as a cost-benefit guideline between\ncomputational cost and accuracy gain when creating a dataset and training a\nsign recognition model.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 23:36:49 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 22:36:18 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Carneiro", "Alvaro Leandro Cavalcante", ""], ["Silva", "Lucas de Brito", ""], ["Salvadeo", "Denis Henrique Pinheiro", ""]]}, {"id": "2103.12236", "submitter": "Fuwen Tan", "authors": "Fuwen Tan, Jiangbo Yuan, Vicente Ordonez", "title": "Instance-level Image Retrieval using Reranking Transformers", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Instance-level image retrieval is the task of searching in a large database\nfor images that match an object in a query image. To address this task, systems\nusually rely on a retrieval step that uses global image descriptors, and a\nsubsequent step that performs domain-specific refinements or reranking by\nleveraging operations such as geometric verification based on local features.\nIn this work, we propose Reranking Transformers (RRTs) as a general model to\nincorporate both local and global features to rerank the matching images in a\nsupervised fashion and thus replace the relatively expensive process of\ngeometric verification. RRTs are lightweight and can be easily parallelized so\nthat reranking a set of top matching results can be performed in a single\nforward-pass. We perform extensive experiments on the Revisited Oxford and\nParis datasets, and the Google Landmark v2 dataset, showing that RRTs\noutperform previous reranking approaches while using much fewer local\ndescriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs\ncan be optimized jointly with the feature extractor, which can lead to feature\nrepresentations tailored to downstream tasks and further accuracy improvements.\nTraining code and pretrained models will be made public.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 23:58:38 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tan", "Fuwen", ""], ["Yuan", "Jiangbo", ""], ["Ordonez", "Vicente", ""]]}, {"id": "2103.12242", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "F-SIOL-310: A Robotic Dataset and Benchmark for Few-Shot Incremental\n  Object Learning", "comments": "Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has achieved remarkable success in object recognition tasks\nthrough the availability of large scale datasets like ImageNet. However, deep\nlearning systems suffer from catastrophic forgetting when learning\nincrementally without replaying old data. For real-world applications, robots\nalso need to incrementally learn new objects. Further, since robots have\nlimited human assistance available, they must learn from only a few examples.\nHowever, very few object recognition datasets and benchmarks exist to test\nincremental learning capability for robotic vision. Further, there is no\ndataset or benchmark specifically designed for incremental object learning from\na few examples. To fill this gap, we present a new dataset termed F-SIOL-310\n(Few-Shot Incremental Object Learning) which is specifically captured for\ntesting few-shot incremental object learning capability for robotic vision. We\nalso provide benchmarks and evaluations of 8 incremental learning algorithms on\nF-SIOL-310 for future comparisons. Our results demonstrate that the few-shot\nincremental object learning problem for robotic vision is far from being\nsolved.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 00:25:50 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2103.12245", "submitter": "Ken C. L. Wong", "authors": "Ken C. L. Wong, Elena S. Sinkovskaya, Alfred Z. Abuhamad, Tanveer\n  Syeda-Mahmood", "title": "Multiview and Multiclass Image Segmentation using Deep Learning in Fetal\n  Echocardiography", "comments": "This paper was accepted by SPIE Medical Imaging 2021", "journal-ref": null, "doi": "10.1117/12.2582191", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congenital heart disease (CHD) is the most common congenital abnormality\nassociated with birth defects in the United States. Despite training efforts\nand substantial advancement in ultrasound technology over the past years, CHD\nremains an abnormality that is frequently missed during prenatal\nultrasonography. Therefore, computer-aided detection of CHD can play a critical\nrole in prenatal care by improving screening and diagnosis. Since many CHDs\ninvolve structural abnormalities, automatic segmentation of anatomical\nstructures is an important step in the analysis of fetal echocardiograms. While\nexisting methods mainly focus on the four-chamber view with a small number of\nstructures, here we present a more comprehensive deep learning segmentation\nframework covering 14 anatomical structures in both three-vessel trachea and\nfour-chamber views. Specifically, our framework enhances the V-Net with spatial\ndropout, group normalization, and deep supervision to train a segmentation\nmodel that can be applied on both views regardless of abnormalities. By\nidentifying the pitfall of using the Dice loss when some labels are unavailable\nin some images, this framework integrates information from multiple views and\nis robust to missing structures due to anatomical anomalies, achieving an\naverage Dice score of 79%.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 00:33:23 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wong", "Ken C. L.", ""], ["Sinkovskaya", "Elena S.", ""], ["Abuhamad", "Alfred Z.", ""], ["Syeda-Mahmood", "Tanveer", ""]]}, {"id": "2103.12248", "submitter": "Roozbeh Mottaghi", "authors": "Jialin Wu, Jiasen Lu, Ashish Sabharwal, Roozbeh Mottaghi", "title": "Multi-Modal Answer Validation for Knowledge-Based VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of knowledge-based visual question answering involves answering\nquestions that require external knowledge in addition to the content of the\nimage. Such knowledge typically comes in a variety of forms, including visual,\ntextual, and commonsense knowledge. The use of more knowledge sources, however,\nalso increases the chance of retrieving more irrelevant or noisy facts, making\nit difficult to comprehend the facts and find the answer. To address this\nchallenge, we propose Multi-modal Answer Validation using External knowledge\n(MAVEx), where the idea is to validate a set of promising answer candidates\nbased on answer-specific knowledge retrieval. This is in contrast to existing\napproaches that search for the answer in a vast collection of often irrelevant\nfacts. Our approach aims to learn which knowledge source should be trusted for\neach answer candidate and how to validate the candidate using that source. We\nconsider a multi-modal setting, relying on both textual and visual knowledge\nresources, including images searched using Google, sentences from Wikipedia\narticles, and concepts from ConceptNet. Our experiments with OK-VQA, a\nchallenging knowledge-based VQA dataset, demonstrate that MAVEx achieves new\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 00:49:36 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 19:57:46 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Wu", "Jialin", ""], ["Lu", "Jiasen", ""], ["Sabharwal", "Ashish", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "2103.12266", "submitter": "Peng-Shuai Wang", "authors": "Shi-Lin Liu, Hao-Xiang Guo, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang\n  Liu", "title": "Deep Implicit Moving Least-Squares Functions for 3D Reconstruction", "comments": "Accepted by CVPR 2021, Code: https://github.com/Andy97/DeepMLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point set is a flexible and lightweight representation widely used for 3D\ndeep learning. However, their discrete nature prevents them from representing\ncontinuous and fine geometry, posing a major issue for learning-based shape\ngeneration. In this work, we turn the discrete point sets into smooth surfaces\nby introducing the well-known implicit moving least-squares (IMLS) surface\nformulation, which naturally defines locally implicit functions on point sets.\nWe incorporate IMLS surface generation into deep neural networks for inheriting\nboth the flexibility of point sets and the high quality of implicit surfaces.\nOur IMLSNet predicts an octree structure as a scaffold for generating MLS\npoints where needed and characterizes shape geometry with learned local priors.\nFurthermore, our implicit function evaluation is independent of the neural\nnetwork once the MLS points are predicted, thus enabling fast runtime\nevaluation. Our experiments on 3D object reconstruction demonstrate that\nIMLSNets outperform state-of-the-art learning-based methods in terms of\nreconstruction quality and computational efficiency. Extensive ablation tests\nalso validate our network design and loss functions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 02:26:07 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 03:14:20 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Liu", "Shi-Lin", ""], ["Guo", "Hao-Xiang", ""], ["Pan", "Hao", ""], ["Wang", "Peng-Shuai", ""], ["Tong", "Xin", ""], ["Liu", "Yang", ""]]}, {"id": "2103.12270", "submitter": "Xianzhi Du", "authors": "Abdullah Rashwan and Xianzhi Du and Xiaoqi Yin and Jing Li", "title": "Dilated SpineNet for Semantic Segmentation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scale-permuted networks have shown promising results on object bounding box\ndetection and instance segmentation. Scale permutation and cross-scale fusion\nof features enable the network to capture multi-scale semantics while\npreserving spatial resolution. In this work, we evaluate this meta-architecture\ndesign on semantic segmentation - another vision task that benefits from high\nspatial resolution and multi-scale feature fusion at different network stages.\nBy further leveraging dilated convolution operations, we propose SpineNet-Seg,\na network discovered by NAS that is searched from the DeepLabv3 system.\nSpineNet-Seg is designed with a better scale-permuted network topology with\ncustomized dilation ratios per block on a semantic segmentation task.\nSpineNet-Seg models outperform the DeepLabv3/v3+ baselines at all model scales\non multiple popular benchmarks in speed and accuracy. In particular, our\nSpineNet-S143+ model achieves the new state-of-the-art on the popular\nCityscapes benchmark at 83.04% mIoU and attained strong performance on the\nPASCAL VOC2012 benchmark at 85.56% mIoU. SpineNet-Seg models also show\npromising results on a challenging Street View segmentation dataset. Code and\ncheckpoints will be open-sourced.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 02:39:04 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Rashwan", "Abdullah", ""], ["Du", "Xianzhi", ""], ["Yin", "Xiaoqi", ""], ["Li", "Jing", ""]]}, {"id": "2103.12277", "submitter": "Han Li", "authors": "Han Li, Long Chen, Hu Han, S. Kevin Zhou", "title": "Conditional Training with Bounding Map for Universal Lesion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal Lesion Detection (ULD) in computed tomography plays an essential\nrole in computer-aided diagnosis. Promising ULD results have been reported by\ncoarse-to-fine two-stage detection approaches, but such two-stage ULD methods\nstill suffer from issues like imbalance of positive v.s. negative anchors\nduring object proposal and insufficient supervision problem during localization\nregression and classification of the region of interest (RoI) proposals. While\nleveraging pseudo segmentation masks such as bounding map (BM) can reduce the\nabove issues to some degree, it is still an open problem to effectively handle\nthe diverse lesion shapes and sizes in ULD. In this paper, we propose a\nBM-based conditional training for two-stage ULD, which can (i) reduce positive\nvs. negative anchor imbalance via BM-based conditioning (BMC) mechanism for\nanchor sampling instead of traditional IoU-based rule; and (ii) adaptively\ncompute size-adaptive BM (ABM) from lesion bounding box, which is used for\nimproving lesion localization accuracy via ABMsupervised segmentation.\nExperiments with four state-of-the-art methods show that the proposed approach\ncan bring an almost free detection accuracy improvement without requiring\nexpensive lesion mask annotations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:04:13 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Li", "Han", ""], ["Chen", "Long", ""], ["Han", "Hu", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2103.12278", "submitter": "Boyuan Jiang", "authors": "Mingyu Wu, Boyuan Jiang, Donghao Luo, Junchi Yan, Yabiao Wang, Ying\n  Tai, Chengjie Wang, Jilin Li, Feiyue Huang, Xiaokang Yang", "title": "Learning Comprehensive Motion Representation for Action Recognition", "comments": "Accepted by AAAI21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For action recognition learning, 2D CNN-based methods are efficient but may\nyield redundant features due to applying the same 2D convolution kernel to each\nframe. Recent efforts attempt to capture motion information by establishing\ninter-frame connections while still suffering the limited temporal receptive\nfield or high latency. Moreover, the feature enhancement is often only\nperformed by channel or space dimension in action recognition. To address these\nissues, we first devise a Channel-wise Motion Enhancement (CME) module to\nadaptively emphasize the channels related to dynamic information with a\nchannel-wise gate vector. The channel gates generated by CME incorporate the\ninformation from all the other frames in the video. We further propose a\nSpatial-wise Motion Enhancement (SME) module to focus on the regions with the\ncritical target in motion, according to the point-to-point similarity between\nadjacent feature maps. The intuition is that the change of background is\ntypically slower than the motion area. Both CME and SME have clear physical\nmeaning in capturing action clues. By integrating the two modules into the\noff-the-shelf 2D network, we finally obtain a Comprehensive Motion\nRepresentation (CMR) learning method for action recognition, which achieves\ncompetitive performance on Something-Something V1 & V2 and Kinetics-400. On the\ntemporal reasoning datasets Something-Something V1 and V2, our method\noutperforms the current state-of-the-art by 2.3% and 1.9% when using 16 frames\nas input, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:06:26 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wu", "Mingyu", ""], ["Jiang", "Boyuan", ""], ["Luo", "Donghao", ""], ["Yan", "Junchi", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2103.12287", "submitter": "Darren Tsai Mr", "authors": "Darren Tsai, Stewart Worrall, Mao Shan, Anton Lohr, Eduardo Nebot", "title": "Optimising the selection of samples for robust lidar camera calibration", "comments": "Submitted to ITSC Conference 2021, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust calibration pipeline that optimises the selection of\ncalibration samples for the estimation of calibration parameters that fit the\nentire scene. We minimise user error by automating the data selection process\naccording to a metric, called Variability of Quality (VOQ) that gives a score\nto each calibration set of samples. We show that this VOQ score is correlated\nwith the estimated calibration parameter's ability to generalise well to the\nentire scene, thereby overcoming the overfitting problems of existing\ncalibration algorithms. Our approach has the benefits of simplifying the\ncalibration process for practitioners of any calibration expertise level and\nproviding an objective measure of the quality for our calibration pipeline's\ninput and output data. We additionally use a novel method of assessing the\naccuracy of the calibration parameters. It involves computing reprojection\nerrors for the entire scene to ensure that the parameters are well fitted to\nall features in the scene. Our proposed calibration pipeline takes 90s, and\nobtains an average reprojection error of 1-1.2cm, with standard deviation of\n0.4-0.5cm over 46 poses evenly distributed in a scene. This process has been\nvalidated by experimentation on a high resolution, software definable lidar,\nBaraja Spectrum-Scan; and a low, fixed resolution lidar, Velodyne VLP-16. We\nhave shown that despite the vast differences in lidar technologies, our\nproposed approach manages to estimate robust calibration parameters for both.\nOur code and data set used for this paper are made available as open-source.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:46:15 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tsai", "Darren", ""], ["Worrall", "Stewart", ""], ["Shan", "Mao", ""], ["Lohr", "Anton", ""], ["Nebot", "Eduardo", ""]]}, {"id": "2103.12292", "submitter": "Li Sun Dr", "authors": "Zhicheng Zhou, Cheng Zhao, Daniel Adolfsson, Songzhi Su, Yang Gao, and\n  Tom Duckett, Li Sun", "title": "NDT-Transformer: Large-Scale 3D Point Cloud Localisation using the\n  Normal Distribution Transform Representation", "comments": "To be appear in ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D point cloud-based place recognition is highly demanded by autonomous\ndriving in GPS-challenged environments and serves as an essential component\n(i.e. loop-closure detection) in lidar-based SLAM systems. This paper proposes\na novel approach, named NDT-Transformer, for realtime and large-scale place\nrecognition using 3D point clouds. Specifically, a 3D Normal Distribution\nTransform (NDT) representation is employed to condense the raw, dense 3D point\ncloud as probabilistic distributions (NDT cells) to provide the geometrical\nshape description. Then a novel NDT-Transformer network learns a global\ndescriptor from a set of 3D NDT cell representations. Benefiting from the NDT\nrepresentation and NDT-Transformer network, the learned global descriptors are\nenriched with both geometrical and contextual information. Finally, descriptor\nretrieval is achieved using a query-database for place recognition. Compared to\nthe state-of-the-art methods, the proposed approach achieves an improvement of\n7.52% on average top 1 recall and 2.73% on average top 1% recall on the Oxford\nRobotcar benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 04:04:38 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zhou", "Zhicheng", ""], ["Zhao", "Cheng", ""], ["Adolfsson", "Daniel", ""], ["Su", "Songzhi", ""], ["Gao", "Yang", ""], ["Duckett", "Tom", ""], ["Sun", "Li", ""]]}, {"id": "2103.12294", "submitter": "Shixiang Tang", "authors": "Shixiang Tang, Peng Su, Dapeng Chen and Wanli Ouyang", "title": "Gradient Regularized Contrastive Learning for Continual Domain\n  Adaptation", "comments": "Accepted by AAAI2021 (poster). arXiv admin note: text overlap with\n  arXiv:2007.12942", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings can quickly adapt to environmental changes by leveraging\nlearning experience. However, adapting deep neural networks to dynamic\nenvironments by machine learning algorithms remains a challenge. To better\nunderstand this issue, we study the problem of continual domain adaptation,\nwhere the model is presented with a labelled source domain and a sequence of\nunlabelled target domains. The obstacles in this problem are both domain shift\nand catastrophic forgetting. We propose Gradient Regularized Contrastive\nLearning (GRCL) to solve the obstacles. At the core of our method, gradient\nregularization plays two key roles: (1) enforcing the gradient not to harm the\ndiscriminative ability of source features which can, in turn, benefit the\nadaptation ability of the model to target domains; (2) constraining the\ngradient not to increase the classification loss on old target domains, which\nenables the model to preserve the performance on old target domains when\nadapting to an in-coming target domain. Experiments on Digits, DomainNet and\nOffice-Caltech benchmarks demonstrate the strong performance of our approach\nwhen compared to the other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 04:10:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tang", "Shixiang", ""], ["Su", "Peng", ""], ["Chen", "Dapeng", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2103.12297", "submitter": "Qiqin Dai", "authors": "Qiqin Dai, Fengqiang Li, Oliver Cossairt, and Aggelos K Katsaggelos", "title": "Adaptive Illumination based Depth Sensing using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense depth map capture is challenging in existing active sparse illumination\nbased depth acquisition techniques, such as LiDAR. Various techniques have been\nproposed to estimate a dense depth map based on fusion of the sparse depth map\nmeasurement with the RGB image. Recent advances in hardware enable adaptive\ndepth measurements resulting in further improvement of the dense depth map\nestimation. In this paper, we study the topic of estimating dense depth from\ndepth sampling. The adaptive sparse depth sampling network is jointly trained\nwith a fusion network of an RGB image and sparse depth, to generate optimal\nadaptive sampling masks. We show that such adaptive sampling masks can\ngeneralize well to many RGB and sparse depth fusion algorithms under a variety\nof sampling rates (as low as $0.0625\\%$). The proposed adaptive sampling method\nis fully differentiable and flexible to be trained end-to-end with upstream\nperception algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 04:21:07 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Dai", "Qiqin", ""], ["Li", "Fengqiang", ""], ["Cossairt", "Oliver", ""], ["Katsaggelos", "Aggelos K", ""]]}, {"id": "2103.12308", "submitter": "Alina Jade Barnett", "authors": "Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao, Chaofan Chen,\n  Yinhao Ren, Joseph Y. Lo and Cynthia Rudin", "title": "IAIA-BL: A Case-based Interpretable Deep Learning Model for\n  Classification of Mass Lesions in Digital Mammography", "comments": "24 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability in machine learning models is important in high-stakes\ndecisions, such as whether to order a biopsy based on a mammographic exam.\nMammography poses important challenges that are not present in other computer\nvision tasks: datasets are small, confounding information is present, and it\ncan be difficult even for a radiologist to decide between watchful waiting and\nbiopsy based on a mammogram alone. In this work, we present a framework for\ninterpretable machine learning-based mammography. In addition to predicting\nwhether a lesion is malignant or benign, our work aims to follow the reasoning\nprocesses of radiologists in detecting clinically relevant semantic features of\neach image, such as the characteristics of the mass margins. The framework\nincludes a novel interpretable neural network algorithm that uses case-based\nreasoning for mammography. Our algorithm can incorporate a combination of data\nwith whole image labelling and data with pixel-wise annotations, leading to\nbetter accuracy and interpretability even with a small number of images. Our\ninterpretable models are able to highlight the classification-relevant parts of\nthe image, whereas other methods highlight healthy tissue and confounding\ninformation. Our models are decision aids, rather than decision makers, aimed\nat better overall human-machine collaboration. We do not observe a loss in mass\nmargin classification accuracy over a black box neural network trained on the\nsame data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:00:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Barnett", "Alina Jade", ""], ["Schwartz", "Fides Regina", ""], ["Tao", "Chaofan", ""], ["Chen", "Chaofan", ""], ["Ren", "Yinhao", ""], ["Lo", "Joseph Y.", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2103.12311", "submitter": "Hanwen Cao", "authors": "Hanwen Cao, Hao-Shu Fang, Wenhai Liu, Cewu Lu", "title": "SuctionNet-1Billion: A Large-Scale Benchmark for Suction Grasping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Suction is an important solution for the longstanding robotic grasping\nproblem. Compared with other kinds of grasping, suction grasping is easier to\nrepresent and often more reliable in practice. Though preferred in many\nscenarios, it is not fully investigated and lacks sufficient training data and\nevaluation benchmarks. To address that, firstly, we propose a new physical\nmodel to analytically evaluate seal formation and wrench resistance of a\nsuction grasping, which are two key aspects of grasp success. Secondly, a\ntwo-step methodology is adopted to generate annotations on a large-scale\ndataset collected in real-world cluttered scenarios. Thirdly, a standard online\nevaluation system is proposed to evaluate suction poses in continuous operation\nspace, which can benchmark different algorithms fairly without the need of\nexhaustive labeling. Real-robot experiments are conducted to show that our\nannotations align well with real world. Meanwhile, we propose a method to\npredict numerous suction poses from an RGB-D image of a cluttered scene and\ndemonstrate our superiority against several previous methods. Result analyses\nare further provided to help readers better understand the challenges in this\narea. Data and source code are publicly available at www.graspnet.net.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:02:52 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Cao", "Hanwen", ""], ["Fang", "Hao-Shu", ""], ["Liu", "Wenhai", ""], ["Lu", "Cewu", ""]]}, {"id": "2103.12318", "submitter": "Kaihao Zhang", "authors": "Kaihao Zhang, Dongxu Li, Wenhan Luo, Wen-Yan Lin, Fang Zhao, Wenqi\n  Ren, Wei Liu, Hongdong Li", "title": "Enhanced Spatio-Temporal Interaction Learning for Video Deraining: A\n  Faster and Better Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video deraining is an important task in computer vision as the unwanted rain\nhampers the visibility of videos and deteriorates the robustness of most\noutdoor vision systems. Despite the significant success which has been achieved\nfor video deraining recently, two major challenges remain: 1) how to exploit\nthe vast information among continuous frames to extract powerful\nspatio-temporal features across both the spatial and temporal domains, and 2)\nhow to restore high-quality derained videos with a high-speed approach. In this\npaper, we present a new end-to-end video deraining framework, named Enhanced\nSpatio-Temporal Interaction Network (ESTINet), which considerably boosts\ncurrent state-of-the-art video deraining quality and speed. The ESTINet takes\nthe advantage of deep residual networks and convolutional long short-term\nmemory, which can capture the spatial features and temporal correlations among\ncontinuing frames at the cost of very little computational source. Extensive\nexperiments on three public datasets show that the proposed ESTINet can achieve\nfaster speed than the competitors, while maintaining better performance than\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:19:35 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zhang", "Kaihao", ""], ["Li", "Dongxu", ""], ["Luo", "Wenhan", ""], ["Lin", "Wen-Yan", ""], ["Zhao", "Fang", ""], ["Ren", "Wenqi", ""], ["Liu", "Wei", ""], ["Li", "Hongdong", ""]]}, {"id": "2103.12322", "submitter": "Mohit Prabhushankar", "authors": "Mohit Prabhushankar and Ghassan AlRegib", "title": "Extracting Causal Visual Features for Limited label Classification", "comments": "Submitted to IEEE International Conference on Image Processing (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks trained to classify images do so by identifying features that\nallow them to distinguish between classes. These sets of features are either\ncausal or context dependent. Grad-CAM is a popular method of visualizing both\nsets of features. In this paper, we formalize this feature divide and provide a\nmethodology to extract causal features from Grad-CAM. We do so by defining\ncontext features as those features that allow contrast between predicted class\nand any contrast class. We then apply a set theoretic approach to separate\ncausal from contrast features for COVID-19 CT scans. We show that on average,\nthe image regions with the proposed causal features require 15% less bits when\nencoded using Huffman encoding, compared to Grad-CAM, for an average increase\nof 3% classification accuracy, over Grad-CAM. Moreover, we validate the\ntransfer-ability of causal features between networks and comment on the\nnon-human interpretable causal nature of current networks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:42:20 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Prabhushankar", "Mohit", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2103.12328", "submitter": "Kazuma Kobayashi", "authors": "Kazuma Kobayashi, Ryuichiro Hataya, Yusuke Kurose, Mototaka Miyake,\n  Masamichi Takahashi, Akiko Nakagawa, Tatsuya Harada, Ryuji Hamamoto", "title": "Decomposing Normal and Abnormal Features of Medical Images into Discrete\n  Latent Codes for Content-Based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, the characteristics purely derived from a disease should\nreflect the extent to which abnormal findings deviate from the normal features.\nIndeed, physicians often need corresponding images without abnormal findings of\ninterest or, conversely, images that contain similar abnormal findings\nregardless of normal anatomical context. This is called comparative diagnostic\nreading of medical images, which is essential for a correct diagnosis. To\nsupport comparative diagnostic reading, content-based image retrieval (CBIR),\nwhich can selectively utilize normal and abnormal features in medical images as\ntwo separable semantic components, will be useful. Therefore, we propose a\nneural network architecture to decompose the semantic components of medical\nimages into two latent codes: normal anatomy code and abnormal anatomy code.\nThe normal anatomy code represents normal anatomies that should have existed if\nthe sample is healthy, whereas the abnormal anatomy code attributes to abnormal\nchanges that reflect deviation from the normal baseline. These latent codes are\ndiscretized through vector quantization to enable binary hashing, which can\nreduce the computational burden at the time of similarity search. By\ncalculating the similarity based on either normal or abnormal anatomy codes or\nthe combination of the two codes, our algorithm can retrieve images according\nto the selected semantic component from a dataset consisting of brain magnetic\nresonance images of gliomas. Our CBIR system qualitatively and quantitatively\nachieves remarkable results.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:53:53 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kobayashi", "Kazuma", ""], ["Hataya", "Ryuichiro", ""], ["Kurose", "Yusuke", ""], ["Miyake", "Mototaka", ""], ["Takahashi", "Masamichi", ""], ["Nakagawa", "Akiko", ""], ["Harada", "Tatsuya", ""], ["Hamamoto", "Ryuji", ""]]}, {"id": "2103.12329", "submitter": "Mohit Prabhushankar", "authors": "Mohit Prabhushankar and Ghassan AlRegib", "title": "Contrastive Reasoning in Neural Networks", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks represent data as projections on trained weights in a high\ndimensional manifold. The trained weights act as a knowledge base consisting of\ncausal class dependencies. Inference built on features that identify these\ndependencies is termed as feed-forward inference. Such inference mechanisms are\njustified based on classical cause-to-effect inductive reasoning models.\nInductive reasoning based feed-forward inference is widely used due to its\nmathematical simplicity and operational ease. Nevertheless, feed-forward models\ndo not generalize well to untrained situations. To alleviate this\ngeneralization challenge, we propose using an effect-to-cause inference model\nthat reasons abductively. Here, the features represent the change from existing\nweight dependencies given a certain effect. We term this change as contrast and\nthe ensuing reasoning mechanism as contrastive reasoning. In this paper, we\nformalize the structure of contrastive reasoning and propose a methodology to\nextract a neural network's notion of contrast. We demonstrate the value of\ncontrastive reasoning in two stages of a neural network's reasoning pipeline :\nin inferring and visually explaining decisions for the application of object\nrecognition. We illustrate the value of contrastively recognizing images under\ndistortions by reporting an improvement of 3.47%, 2.56%, and 5.48% in average\naccuracy under the proposed contrastive framework on CIFAR-10C, noisy STL-10,\nand VisDA datasets respectively.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:54:36 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Prabhushankar", "Mohit", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2103.12337", "submitter": "Rishab Sharma Mr.", "authors": "Rahul Deora, Rishab Sharma and Dinesh Samuel Sathia Raj", "title": "Salient Image Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an image matting framework called Salient Image\nMatting to estimate the per-pixel opacity value of the most salient foreground\nin an image. To deal with a large amount of semantic diversity in images, a\ntrimap is conventionally required as it provides important guidance about\nobject semantics to the matting process. However, creating a good trimap is\noften expensive and timeconsuming. The SIM framework simultaneously deals with\nthe challenge of learning a wide range of semantics and salient object types in\na fully automatic and an end to end manner. Specifically, our framework is able\nto produce accurate alpha mattes for a wide range of foreground objects and\ncases where the foreground class, such as human, appears in a very different\ncontext than the train data directly from an RGB input. This is done by\nemploying a salient object detection model to produce a trimap of the most\nsalient object in the image in order to guide the matting model about\nhigher-level object semantics. Our framework leverages large amounts of coarse\nannotations coupled with a heuristic trimap generation scheme to train the\ntrimap prediction network so it can produce trimaps for arbitrary foregrounds.\nMoreover, we introduce a multi-scale fusion architecture for the task of\nmatting to better capture finer, low-level opacity semantics. With high-level\nguidance provided by the trimap network, our framework requires only a fraction\nof expensive matting data as compared to other automatic methods while being\nable to produce alpha mattes for a diverse range of inputs. We demonstrate our\nframework on a range of diverse images and experimental results show our\nframework compares favourably against state of art matting methods without the\nneed for a trimap\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 06:22:33 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Deora", "Rahul", ""], ["Sharma", "Rishab", ""], ["Raj", "Dinesh Samuel Sathia", ""]]}, {"id": "2103.12339", "submitter": "Binhui Xie", "authors": "Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang and\n  Guoren Wang", "title": "Generalized Domain Conditioned Adaptation Network", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI). Journal version of arXiv:2005.06717 (AAAI 2020). Code\n  is available at https://github.com/BIT-DA/GDCAN", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3062644", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation (DA) attempts to transfer knowledge learned in the labeled\nsource domain to the unlabeled but related target domain without requiring\nlarge amounts of target supervision. Recent advances in DA mainly proceed by\naligning the source and target distributions. Despite the significant success,\nthe adaptation performance still degrades accordingly when the source and\ntarget domains encounter a large distribution discrepancy. We consider this\nlimitation may attribute to the insufficient exploration of domain-specialized\nfeatures because most studies merely concentrate on domain-general feature\nlearning in task-specific layers and integrate totally-shared convolutional\nnetworks (convnets) to generate common features for both domains. In this\npaper, we relax the completely-shared convnets assumption adopted by previous\nDA methods and propose Domain Conditioned Adaptation Network (DCAN), which\nintroduces domain conditioned channel attention module with a multi-path\nstructure to separately excite channel activation for each domain. Such a\npartially-shared convnets module allows domain-specialized features in\nlow-level to be explored appropriately. Further, given the knowledge\ntransferability varying along with convolutional layers, we develop Generalized\nDomain Conditioned Adaptation Network (GDCAN) to automatically determine\nwhether domain channel activations should be separately modeled in each\nattention module. Afterward, the critical domain-specialized knowledge could be\nadaptively extracted according to the domain statistic gaps. As far as we know,\nthis is the first work to explore the domain-wise convolutional channel\nactivations separately for deep DA networks. Additionally, to effectively match\nhigh-level feature distributions across domains, we consider deploying feature\nadaptation blocks after task-specific layers, which can explicitly mitigate the\ndomain discrepancy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 06:24:26 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Binhui", ""], ["Lin", "Qiuxia", ""], ["Liu", "Chi Harold", ""], ["Huang", "Gao", ""], ["Wang", "Guoren", ""]]}, {"id": "2103.12340", "submitter": "Lei Ke", "authors": "Lei Ke, Yu-Wing Tai and Chi-Keung Tang", "title": "Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers", "comments": "Accepted by CVPR2021. BCNet Code: https://github.com/lkeab/BCNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmenting highly-overlapping objects is challenging, because typically no\ndistinction is made between real object contours and occlusion boundaries.\nUnlike previous two-stage instance segmentation methods, we model image\nformation as composition of two overlapping layers, and propose Bilayer\nConvolutional Network (BCNet), where the top GCN layer detects the occluding\nobjects (occluder) and the bottom GCN layer infers partially occluded instance\n(occludee). The explicit modeling of occlusion relationship with bilayer\nstructure naturally decouples the boundaries of both the occluding and occluded\ninstances, and considers the interaction between them during mask regression.\nWe validate the efficacy of bilayer decoupling on both one-stage and two-stage\nobject detectors with different backbones and network layer choices. Despite\nits simplicity, extensive experiments on COCO and KINS show that our\nocclusion-aware BCNet achieves large and consistent performance gain especially\nfor heavy occlusion cases. Code is available at https://github.com/lkeab/BCNet.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 06:25:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ke", "Lei", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "2103.12344", "submitter": "Siyuan Zhu", "authors": "JingWei Xu, Siyuan Zhu, Zenan Li, Chang Xu", "title": "Joint Distribution across Representation Space for Out-of-Distribution\n  Detection", "comments": "Submitted to a conference of computer vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) have become a key part of many modern software\napplications. After training and validating, the DNN is deployed as an\nirrevocable component and applied in real-world scenarios. Although most DNNs\nare built meticulously with huge volumes of training data, data in the real\nworld still remain unknown to the DNN model, which leads to the crucial\nrequirement of runtime out-of-distribution (OOD) detection. However, many\nexisting approaches 1) need OOD data for classifier training or parameter\ntuning, or 2) simply combine the scores of each hidden layer as an ensemble of\nfeatures for OOD detection. In this paper, we present a novel outlook on\nin-distribution data in a generative manner, which takes their latent features\ngenerated from each hidden layer as a joint distribution across representation\nspaces. Since only the in-distribution latent features are comprehensively\nunderstood in representation space, the internal difference between\nin-distribution and OOD data can be naturally revealed without the intervention\nof any OOD data. Specifically, We construct a generative model, called Latent\nSequential Gaussian Mixture (LSGM), to depict how the in-distribution latent\nfeatures are generated in terms of the trace of DNN inference across\nrepresentation spaces. We first construct the Gaussian Mixture Model (GMM)\nbased on in-distribution latent features for each hidden layer, and then\nconnect GMMs via the transition probabilities of the inference traces.\nExperimental evaluations on popular benchmark OOD datasets and models validate\nthe superiority of the proposed method over the state-of-the-art methods in OOD\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 06:39:29 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 03:47:32 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Xu", "JingWei", ""], ["Zhu", "Siyuan", ""], ["Li", "Zenan", ""], ["Xu", "Chang", ""]]}, {"id": "2103.12346", "submitter": "Sijie Song", "authors": "Sijie Song, Xudong Lin, Jiaying Liu, Zongming Guo and Shih-Fu Chang", "title": "Co-Grounding Networks with Semantic Attention for Referring Expression\n  Comprehension in Videos", "comments": "Accepted to CVPR2021. The project page is at\n  https://sijiesong.github.io/co-grounding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of referring expression comprehension\nin videos, which is challenging due to complex expression and scene dynamics.\nUnlike previous methods which solve the problem in multiple stages (i.e.,\ntracking, proposal-based matching), we tackle the problem from a novel\nperspective, \\textbf{co-grounding}, with an elegant one-stage framework. We\nenhance the single-frame grounding accuracy by semantic attention learning and\nimprove the cross-frame grounding consistency with co-grounding feature\nlearning. Semantic attention learning explicitly parses referring cues in\ndifferent attributes to reduce the ambiguity in the complex expression.\nCo-grounding feature learning boosts visual feature representations by\nintegrating temporal correlation to reduce the ambiguity caused by scene\ndynamics. Experiment results demonstrate the superiority of our framework on\nthe video grounding datasets VID and LiOTB in generating accurate and stable\nresults across frames. Our model is also applicable to referring expression\ncomprehension in images, illustrated by the improved performance on the RefCOCO\ndataset. Our project is available at https://sijiesong.github.io/co-grounding.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 06:42:49 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Song", "Sijie", ""], ["Lin", "Xudong", ""], ["Liu", "Jiaying", ""], ["Guo", "Zongming", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2103.12347", "submitter": "Jihun Kang", "authors": "Jihun Kang, Daichi Haraguchi, Akisato Kimura, Seiichi Uchida", "title": "Shared Latent Space of Font Shapes and Impressions", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have specific impressions from the style of a typeface (font), suggesting\nthat there are correlations between font shape and its impressions. Based on\nthis hypothesis, we realize a shared latent space where a font shape image and\nits impression words are embedded in a cross-modal manner. This latent space is\nuseful to understand the style-impression correlation and generate font images\nby specifying several impression words. Experimental results with a large\nstyle-impression dataset prove that it is possible to accurately realize the\nshared latent space, especially for shape-relevant impression words, and then\nuse the space to generate font images with various impressions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 06:54:45 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:06:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kang", "Jihun", ""], ["Haraguchi", "Daichi", ""], ["Kimura", "Akisato", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2103.12350", "submitter": "Vidhiwar Rathour", "authors": "Vidhiwar Singh Rathour, Kashu Yamakazi and T. Hoang Ngan Le", "title": "Roughness Index and Roughness Distance for Benchmarking Medical\n  Segmentation", "comments": "Paper has been accepted at BIOIMAGING2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is one of the most challenging tasks in medical\nimage analysis and has been widely developed for many clinical applications.\nMost of the existing metrics have been first designed for natural images and\nthen extended to medical images. While object surface plays an important role\nin medical segmentation and quantitative analysis i.e. analyze brain tumor\nsurface, measure gray matter volume, most of the existing metrics are limited\nwhen it comes to analyzing the object surface, especially to tell about surface\nsmoothness or roughness of a given volumetric object or to analyze the\ntopological errors. In this paper, we first analysis both pros and cons of all\nexisting medical image segmentation metrics, specially on volumetric data. We\nthen propose an appropriate roughness index and roughness distance for medical\nimage segmentation analysis and evaluation. Our proposed method addresses two\nkinds of segmentation errors, i.e. (i)topological errors on boundary/surface\nand (ii)irregularities on the boundary/surface. The contribution of this work\nis four-fold: (i) detect irregular spikes/holes on a surface, (ii) propose\nroughness index to measure surface roughness of a given object, (iii) propose a\nroughness distance to measure the distance of two boundaries/surfaces by\nutilizing the proposed roughness index and (iv) suggest an algorithm which\nhelps to remove the irregular spikes/holes to smooth the surface. Our proposed\nroughness index and roughness distance are built upon the solid surface\nroughness parameter which has been successfully developed in the civil\nengineering.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 07:19:32 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Rathour", "Vidhiwar Singh", ""], ["Yamakazi", "Kashu", ""], ["Le", "T. Hoang Ngan", ""]]}, {"id": "2103.12352", "submitter": "Edgar Sucar", "authors": "Edgar Sucar, Shikun Liu, Joseph Ortiz, Andrew J. Davison", "title": "iMAP: Implicit Mapping and Positioning in Real-Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show for the first time that a multilayer perceptron (MLP) can serve as\nthe only scene representation in a real-time SLAM system for a handheld RGB-D\ncamera. Our network is trained in live operation without prior data, building a\ndense, scene-specific implicit 3D model of occupancy and colour which is also\nimmediately used for tracking.\n  Achieving real-time SLAM via continual training of a neural network against a\nlive image stream requires significant innovation. Our iMAP algorithm uses a\nkeyframe structure and multi-processing computation flow, with dynamic\ninformation-guided pixel sampling for speed, with tracking at 10 Hz and global\nmap updating at 2 Hz. The advantages of an implicit MLP over standard dense\nSLAM techniques include efficient geometry representation with automatic detail\ncontrol and smooth, plausible filling-in of unobserved regions such as the back\nsurfaces of objects.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 07:21:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Sucar", "Edgar", ""], ["Liu", "Shikun", ""], ["Ortiz", "Joseph", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2103.12362", "submitter": "Henrique Siqueira", "authors": "Henrique Siqueira, Pablo Barros, Sven Magg, Cornelius Weber and Stefan\n  Wermter", "title": "A Sub-Layered Hierarchical Pyramidal Neural Architecture for Facial\n  Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In domains where computational resources and labeled data are limited, such\nas in robotics, deep networks with millions of weights might not be the optimal\nsolution. In this paper, we introduce a connectivity scheme for pyramidal\narchitectures to increase their capacity for learning features. Experiments on\nfacial expression recognition of unseen people demonstrate that our approach is\na potential candidate for applications with restricted resources, due to good\ngeneralization performance and low computational cost. We show that our\napproach generalizes as well as convolutional architectures in this task but\nuses fewer trainable parameters and is more robust for low-resolution faces.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 07:50:33 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Siqueira", "Henrique", ""], ["Barros", "Pablo", ""], ["Magg", "Sven", ""], ["Weber", "Cornelius", ""], ["Wermter", "Stefan", ""]]}, {"id": "2103.12366", "submitter": "Kecheng Zheng", "authors": "Kecheng Zheng, Wu Liu, Lingxiao He, Tao Mei, Jiebo Luo, Zheng-Jun Zha", "title": "Group-aware Label Transfer for Domain Adaptive Person Re-identification", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised Domain Adaptive (UDA) person re-identification (ReID) aims at\nadapting the model trained on a labeled source-domain dataset to a\ntarget-domain dataset without any further annotations. Most successful UDA-ReID\napproaches combine clustering-based pseudo-label prediction with representation\nlearning and perform the two steps in an alternating fashion. However, offline\ninteraction between these two steps may allow noisy pseudo labels to\nsubstantially hinder the capability of the model. In this paper, we propose a\nGroup-aware Label Transfer (GLT) algorithm, which enables the online\ninteraction and mutual promotion of pseudo-label prediction and representation\nlearning. Specifically, a label transfer algorithm simultaneously uses pseudo\nlabels to train the data while refining the pseudo labels as an online\nclustering algorithm. It treats the online label refinery problem as an optimal\ntransport problem, which explores the minimum cost for assigning M samples to N\npseudo labels. More importantly, we introduce a group-aware strategy to assign\nimplicit attribute group IDs to samples. The combination of the online label\nrefining algorithm and the group-aware strategy can better correct the noisy\npseudo label in an online fashion and narrow down the search space of the\ntarget identity. The effectiveness of the proposed GLT is demonstrated by the\nexperimental results (Rank-1 accuracy) for Market1501$\\to$DukeMTMC (82.0\\%) and\nDukeMTMC$\\to$Market1501 (92.2\\%), remarkably closing the gap between\nunsupervised and supervised performance on person re-identification.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 07:57:39 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zheng", "Kecheng", ""], ["Liu", "Wu", ""], ["He", "Lingxiao", ""], ["Mei", "Tao", ""], ["Luo", "Jiebo", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "2103.12369", "submitter": "Zihan Xu", "authors": "Zihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, Ling Shao, Yue Gao,\n  Yonghong Tian, Rongrong Ji", "title": "ReCU: Reviving the Dead Weights in Binary Neural Networks", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Binary neural networks (BNNs) have received increasing attention due to their\nsuperior reductions of computation and memory. Most existing works focus on\neither lessening the quantization error by minimizing the gap between the\nfull-precision weights and their binarization or designing a gradient\napproximation to mitigate the gradient mismatch, while leaving the \"dead\nweights\" untouched. This leads to slow convergence when training BNNs. In this\npaper, for the first time, we explore the influence of \"dead weights\" which\nrefer to a group of weights that are barely updated during the training of\nBNNs, and then introduce rectified clamp unit (ReCU) to revive the \"dead\nweights\" for updating. We prove that reviving the \"dead weights\" by ReCU can\nresult in a smaller quantization error. Besides, we also take into account the\ninformation entropy of the weights, and then mathematically analyze why the\nweight standardization can benefit BNNs. We demonstrate the inherent\ncontradiction between minimizing the quantization error and maximizing the\ninformation entropy, and then propose an adaptive exponential scheduler to\nidentify the range of the \"dead weights\". By considering the \"dead weights\",\nour method offers not only faster BNN training, but also state-of-the-art\nperformance on CIFAR-10 and ImageNet, compared with recent methods. Code can be\navailable at [this https URL](https://github.com/z-hXu/ReCU).\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 08:11:20 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Xu", "Zihan", ""], ["Lin", "Mingbao", ""], ["Liu", "Jianzhuang", ""], ["Chen", "Jie", ""], ["Shao", "Ling", ""], ["Gao", "Yue", ""], ["Tian", "Yonghong", ""], ["Ji", "Rongrong", ""]]}, {"id": "2103.12371", "submitter": "Shiyu Tang", "authors": "Shiyu Tang, Peijun Tang, Yanxiang Gong, Zheng Ma, Mei Xie", "title": "Unsupervised domain adaptation via coarse-to-fine feature alignment\n  method using contrastive learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous feature alignment methods in Unsupervised domain adaptation(UDA)\nmostly only align global features without considering the mismatch between\nclass-wise features. In this work, we propose a new coarse-to-fine feature\nalignment method using contrastive learning called CFContra. It draws\nclass-wise features closer than coarse feature alignment or class-wise feature\nalignment only, therefore improves the model's performance to a great extent.\nWe build it upon one of the most effective methods of UDA called entropy\nminimization to further improve performance. In particular, to prevent\nexcessive memory occupation when applying contrastive loss in semantic\nsegmentation, we devise a new way to build and update the memory bank. In this\nway, we make the algorithm more efficient and viable with limited memory.\nExtensive experiments show the effectiveness of our method and model trained on\nthe GTA5 to Cityscapes dataset has boost mIOU by 3.5 compared to the MinEnt\nalgorithm. Our code will be publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 08:12:28 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tang", "Shiyu", ""], ["Tang", "Peijun", ""], ["Gong", "Yanxiang", ""], ["Ma", "Zheng", ""], ["Xie", "Mei", ""]]}, {"id": "2103.12376", "submitter": "Yuchen Luo", "authors": "Yuchen Luo, Yong Zhang, Junchi Yan, Wei Liu", "title": "Generalizing Face Forgery Detection with High-frequency Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current face forgery detection methods achieve high accuracy under the\nwithin-database scenario where training and testing forgeries are synthesized\nby the same algorithm. However, few of them gain satisfying performance under\nthe cross-database scenario where training and testing forgeries are\nsynthesized by different algorithms. In this paper, we find that current\nCNN-based detectors tend to overfit to method-specific color textures and thus\nfail to generalize. Observing that image noises remove color textures and\nexpose discrepancies between authentic and tampered regions, we propose to\nutilize the high-frequency noises for face forgery detection. We carefully\ndevise three functional modules to take full advantage of the high-frequency\nfeatures. The first is the multi-scale high-frequency feature extraction module\nthat extracts high-frequency noises at multiple scales and composes a novel\nmodality. The second is the residual-guided spatial attention module that\nguides the low-level RGB feature extractor to concentrate more on forgery\ntraces from a new perspective. The last is the cross-modality attention module\nthat leverages the correlation between the two complementary modalities to\npromote feature learning for each other. Comprehensive evaluations on several\nbenchmark databases corroborate the superior generalization performance of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 08:19:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Luo", "Yuchen", ""], ["Zhang", "Yong", ""], ["Yan", "Junchi", ""], ["Liu", "Wei", ""]]}, {"id": "2103.12417", "submitter": "P B Sujit Dr", "authors": "Kasi Viswanath, Kartikeya Singh, Peng Jiang, Sujit P.B. and Srikanth\n  Saripalli", "title": "OFFSEG: A Semantic Segmentation Framework For Off-Road Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-road image semantic segmentation is challenging due to the presence of\nuneven terrains, unstructured class boundaries, irregular features and strong\ntextures. These aspects affect the perception of the vehicle from which the\ninformation is used for path planning. Current off-road datasets exhibit\ndifficulties like class imbalance and understanding of varying environmental\ntopography. To overcome these issues we propose a framework for off-road\nsemantic segmentation called as OFFSEG that involves (i) a pooled class\nsemantic segmentation with four classes (sky, traversable region,\nnon-traversable region and obstacle) using state-of-the-art deep learning\narchitectures (ii) a colour segmentation methodology to segment out specific\nsub-classes (grass, puddle, dirt, gravel, etc.) from the traversable region for\nbetter scene understanding. The evaluation of the framework is carried out on\ntwo off-road driving datasets, namely, RELLIS-3D and RUGD. We have also tested\nproposed framework in IISERB campus frames. The results show that OFFSEG\nachieves good performance and also provides detailed information on the\ntraversable region.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 09:45:41 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Viswanath", "Kasi", ""], ["Singh", "Kartikeya", ""], ["Jiang", "Peng", ""], ["B.", "Sujit P.", ""], ["Saripalli", "Srikanth", ""]]}, {"id": "2103.12424", "submitter": "Changlin Li", "authors": "Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan\n  Liang and Xiaojun Chang", "title": "BossNAS: Exploring Hybrid CNN-transformers with Block-wisely\n  Self-supervised Neural Architecture Search", "comments": "Our searched hybrid CNN-transformer models achieve up to 82.2%\n  accuracy on ImageNet, surpassing EfficientNet by 2.1% with comparable compute\n  time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A myriad of recent breakthroughs in hand-crafted neural architectures for\nvisual recognition have highlighted the urgent need to explore hybrid\narchitectures consisting of diversified building blocks. Meanwhile, neural\narchitecture search methods are surging with an expectation to reduce human\nefforts. However, whether NAS methods can efficiently and effectively handle\ndiversified search spaces with disparate candidates (e.g. CNNs and\ntransformers) is still an open question. In this work, we present Block-wisely\nSelf-supervised Neural Architecture Search (BossNAS), an unsupervised NAS\nmethod that addresses the problem of inaccurate architecture rating caused by\nlarge weight-sharing space and biased supervision in previous methods. More\nspecifically, we factorize the search space into blocks and utilize a novel\nself-supervised training scheme, named ensemble bootstrapping, to train each\nblock separately before searching them as a whole towards the population\ncenter. Additionally, we present HyTra search space, a fabric-like hybrid\nCNN-transformer search space with searchable down-sampling positions. On this\nchallenging search space, our searched model, BossNet-T, achieves up to 82.2%\naccuracy on ImageNet, surpassing EfficientNet by 2.1% with comparable compute\ntime. Moreover, our method achieves superior architecture rating accuracy with\n0.78 and 0.76 Spearman correlation on the canonical MBConv search space with\nImageNet and on NATS-Bench size search space with CIFAR-100, respectively,\nsurpassing state-of-the-art NAS methods. Code and pretrained models are\navailable at https://github.com/changlin31/BossNAS .\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:05:58 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 16:35:30 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Li", "Changlin", ""], ["Tang", "Tao", ""], ["Wang", "Guangrun", ""], ["Peng", "Jiefeng", ""], ["Wang", "Bing", ""], ["Liang", "Xiaodan", ""], ["Chang", "Xiaojun", ""]]}, {"id": "2103.12434", "submitter": "Manu Tom", "authors": "Manu Tom and Tianyu Wu and Emmanuel Baltsavias and Konrad Schindler", "title": "Recent Ice Trends in Swiss Mountain Lakes: 20-year Analysis of MODIS\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depleting lake ice can serve as an indicator for climate change, just like\nsea level rise or glacial retreat. Several Lake Ice Phenological (LIP) events\nserve as sentinels to understand the regional and global climate change. Hence,\nmonitoring the long-term lake freezing and thawing patterns can prove very\nuseful. In this paper, we focus on observing the LIP events such as freeze-up,\nbreak-up and temporal freeze extent in the Oberengadin region of Switzerland,\nwhere there are several small- and medium-sized mountain lakes, across two\ndecades (2000-2020) from optical satellite images. We analyse time-series of\nMODIS imagery (and additionally cross-check with VIIRS data when available), by\nestimating spatially resolved maps of lake ice for these Alpine lakes with\nsupervised machine learning. To train the classifier we rely on reference data\nannotated manually based on publicly available webcam images. From the ice maps\nwe derive long-term LIP trends. Since the webcam data is only available for two\nwinters, we also validate our results against the operational MODIS and VIIRS\nsnow products. We find a change in Complete Freeze Duration (CFD) of -0.76 and\n-0.89 days per annum (d/a) for lakes Sils and Silvaplana respectively.\nFurthermore, we correlate the lake freezing and thawing trends with climate\ndata such as temperature, sunshine, precipitation and wind measured at nearby\nmeteorological stations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:25:02 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tom", "Manu", ""], ["Wu", "Tianyu", ""], ["Baltsavias", "Emmanuel", ""], ["Schindler", "Konrad", ""]]}, {"id": "2103.12437", "submitter": "Federico Marmoreo", "authors": "Federico Marmoreo, Julio Ivan Davila Carrazco, Vittorio Murino, Jacopo\n  Cavazza", "title": "Learning without Seeing nor Knowing: Towards Open Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Generalized Zero-Shot Learning (GZSL), unseen categories (for which no\nvisual data are available at training time) can be predicted by leveraging\ntheir class embeddings (e.g., a list of attributes describing them) together\nwith a complementary pool of seen classes (paired with both visual data and\nclass embeddings). Despite GZSL is arguably challenging, we posit that knowing\nin advance the class embeddings, especially for unseen categories, is an actual\nlimit of the applicability of GZSL towards real-world scenarios. To relax this\nassumption, we propose Open Zero-Shot Learning (OZSL) to extend GZSL towards\nthe open-world settings. We formalize OZSL as the problem of recognizing seen\nand unseen classes (as in GZSL) while also rejecting instances from unknown\ncategories, for which neither visual data nor class embeddings are provided. We\nformalize the OZSL problem introducing evaluation protocols, error metrics and\nbenchmark datasets. We also suggest to tackle the OZSL problem by proposing the\nidea of performing unknown feature generation (instead of only unseen features\ngeneration as done in GZSL). We achieve this by optimizing a generative process\nto sample unknown class embeddings as complementary to the seen and the unseen.\nWe intend these results to be the ground to foster future research, extending\nthe standard closed-world zero-shot learning (GZSL) with the novel open-world\ncounterpart (OZSL).\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:30:50 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Marmoreo", "Federico", ""], ["Carrazco", "Julio Ivan Davila", ""], ["Murino", "Vittorio", ""], ["Cavazza", "Jacopo", ""]]}, {"id": "2103.12441", "submitter": "Pierre-Louis Antonsanti", "authors": "Pierre-Louis Antonsanti, Joan Glaun\\`es, Thomas Benseghir, Vincent\n  Jugnon, Ir\\`ene Kaltenmark", "title": "Partial Matching in the Space of Varifolds", "comments": "12 pages, 3 figures, The 27th international conference on Information\n  Processing in Medical Imaging (June, 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In computer vision and medical imaging, the problem of matching structures\nfinds numerous applications from automatic annotation to data reconstruction.\nThe data however, while corresponding to the same anatomy, are often very\ndifferent in topology or shape and might only partially match each other. We\nintroduce a new asymmetric data dissimilarity term for various geometric shapes\nlike sets of curves or surfaces. This term is based on the Varifold shape\nrepresentation and assesses the embedding of a shape into another one without\nrelying on correspondences between points. It is designed as data attachment\nfor the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework,\nallowing to compute meaningful deformation of one shape onto a subset of the\nother. Registrations are illustrated on sets of synthetic 3D curves, real\nvascular trees and livers' surfaces from two different modalities: Computed\nTomography (CT) and Cone Beam Computed Tomography (CBCT). All experiments show\nthat this data dissimilarity term leads to coherent partial matching despite\nthe topological differences.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:44:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Antonsanti", "Pierre-Louis", ""], ["Glaun\u00e8s", "Joan", ""], ["Benseghir", "Thomas", ""], ["Jugnon", "Vincent", ""], ["Kaltenmark", "Ir\u00e8ne", ""]]}, {"id": "2103.12459", "submitter": "Nitika Verma", "authors": "Nitika Verma, Adnane Boukhayma, Jakob Verbeek, Edmond Boyer", "title": "DualConv: Dual Mesh Convolutional Networks for Shape Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks have been extremely successful for 2D images\nand are readily extended to handle 3D voxel data. Meshes are a more common 3D\nshape representation that quantize the shape surface instead of the ambient\nspace as with voxels, hence giving access to surface properties such as normals\nor appearances. The formulation of deep neural networks on meshes is, however,\nmore complex since they are irregular data structures where the number of\nneighbors varies across vertices. While graph convolutional networks have\npreviously been proposed over mesh vertex data, in this paper we explore how\nthese networks can be extended to the dual face-based representation of\ntriangular meshes, where nodes represent triangular faces in place of vertices.\nIn comparison to the primal vertex mesh, its face dual offers several\nadvantages, including, importantly, that the dual mesh is regular in the sense\nthat each triangular face has exactly three neighbors. Moreover, the dual mesh\nsuggests the use of a number of input features that are naturally defined over\nfaces, such as surface normals and face areas. We evaluate the dual approach on\nthe shape correspondence task on the FAUST human shape dataset and other\nversions of it with varying mesh topology. While applying generic graph\nconvolutions to the dual mesh shows already improvements over primal mesh\ninputs, our experiments demonstrate that building additionally convolutional\nmodels that explicitly leverage the neighborhood size regularity of dual meshes\nenables learning shape representations that perform on par or better than\nprevious approaches in terms of correspondence accuracy and mean geodesic\nerror, while being more robust to topological changes in the meshes between\ntraining and testing shapes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:22:47 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Verma", "Nitika", ""], ["Boukhayma", "Adnane", ""], ["Verbeek", "Jakob", ""], ["Boyer", "Edmond", ""]]}, {"id": "2103.12462", "submitter": "Nan Pu", "authors": "Nan Pu, Wei Chen, Yu Liu, Erwin M. Bakker and Michael S. Lew", "title": "Lifelong Person Re-Identification via Adaptive Knowledge Accumulation", "comments": "10 pages, 5 figures, Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person ReID methods always learn through a stationary domain that is fixed by\nthe choice of a given dataset. In many contexts (e.g., lifelong learning),\nthose methods are ineffective because the domain is continually changing in\nwhich case incremental learning over multiple domains is required potentially.\nIn this work we explore a new and challenging ReID task, namely lifelong person\nre-identification (LReID), which enables to learn continuously across multiple\ndomains and even generalise on new and unseen domains. Following the cognitive\nprocesses in the human brain, we design an Adaptive Knowledge Accumulation\n(AKA) framework that is endowed with two crucial abilities: knowledge\nrepresentation and knowledge operation. Our method alleviates catastrophic\nforgetting on seen domains and demonstrates the ability to generalize to unseen\ndomains. Correspondingly, we also provide a new and large-scale benchmark for\nLReID. Extensive experiments demonstrate our method outperforms other\ncompetitors by a margin of 5.8% mAP in generalising evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:30:38 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Pu", "Nan", ""], ["Chen", "Wei", ""], ["Liu", "Yu", ""], ["Bakker", "Erwin M.", ""], ["Lew", "Michael S.", ""]]}, {"id": "2103.12469", "submitter": "Hao Huang", "authors": "Hao Huang, Yongtao Wang, Zhaoyu Chen, Zhi Tang, Wenqiang Zhang and\n  Kai-Kuang Ma", "title": "RPATTACK: Refined Patch Attack on General Object Detectors", "comments": "6 pages, 4 figures, IEEE International Conference on Multimedia and\n  Expo (ICME) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Nowadays, general object detectors like YOLO and Faster R-CNN as well as\ntheir variants are widely exploited in many applications. Many works have\nrevealed that these detectors are extremely vulnerable to adversarial patch\nattacks. The perturbed regions generated by previous patch-based attack works\non object detectors are very large which are not necessary for attacking and\nperceptible for human eyes. To generate much less but more efficient\nperturbation, we propose a novel patch-based method for attacking general\nobject detectors. Firstly, we propose a patch selection and refining scheme to\nfind the pixels which have the greatest importance for attack and remove the\ninconsequential perturbations gradually. Then, for a stable ensemble attack, we\nbalance the gradients of detectors to avoid over-optimizing one of them during\nthe training phase. Our RPAttack can achieve an amazing missed detection rate\nof 100% for both Yolo v4 and Faster R-CNN while only modifies 0.32% pixels on\nVOC 2007 test set. Our code is available at\nhttps://github.com/VDIGPKU/RPAttack.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:45:41 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Huang", "Hao", ""], ["Wang", "Yongtao", ""], ["Chen", "Zhaoyu", ""], ["Tang", "Zhi", ""], ["Zhang", "Wenqiang", ""], ["Ma", "Kai-Kuang", ""]]}, {"id": "2103.12474", "submitter": "Osama Makansi", "authors": "Osama Makansi, \\\"Ozg\\\"un Cicek, Yassine Marrakchi, and Thomas Brox", "title": "On Exposing the Challenging Long Tail in Future Prediction of Traffic\n  Actors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Predicting the states of dynamic traffic actors into the future is important\nfor autonomous systems to operate safelyand efficiently. Remarkably, the most\ncritical scenarios aremuch less frequent and more complex than the\nuncriticalones. Therefore, uncritical cases dominate the prediction. In this\npaper, we address specifically the challenging scenarios at the long tail of\nthe dataset distribution. Our analysis shows that the common losses tend to\nplace challenging cases suboptimally in the embedding space. As a consequence,\nwe propose to supplement the usual loss with aloss that places challenging\ncases closer to each other. This triggers sharing information among challenging\ncases andlearning specific predictive features. We show on four public datasets\nthat this leads to improved performance on the challenging scenarios while the\noverall performance stays stable. The approach is agnostic w.r.t. the used\nnetwork architecture, input modality or viewpoint, and can be integrated into\nexisting solutions easily.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:56:15 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 10:29:42 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Makansi", "Osama", ""], ["Cicek", "\u00d6zg\u00fcn", ""], ["Marrakchi", "Yassine", ""], ["Brox", "Thomas", ""]]}, {"id": "2103.12489", "submitter": "Chenguo Lin", "authors": "Ruowei Wang, Chenguo Lin, Qijun Zhao, Feiyu Zhu", "title": "Watermark Faker: Towards Forgery of Digital Image Watermarking", "comments": "6 pages; accepted by ICME2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital watermarking has been widely used to protect the copyright and\nintegrity of multimedia data. Previous studies mainly focus on designing\nwatermarking techniques that are robust to attacks of destroying the embedded\nwatermarks. However, the emerging deep learning based image generation\ntechnology raises new open issues that whether it is possible to generate fake\nwatermarked images for circumvention. In this paper, we make the first attempt\nto develop digital image watermark fakers by using generative adversarial\nlearning. Suppose that a set of paired images of original and watermarked\nimages generated by the targeted watermarker are available, we use them to\ntrain a watermark faker with U-Net as the backbone, whose input is an original\nimage, and after a domain-specific preprocessing, it outputs a fake watermarked\nimage. Our experiments show that the proposed watermark faker can effectively\ncrack digital image watermarkers in both spatial and frequency domains,\nsuggesting the risk of such forgery attacks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 12:28:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wang", "Ruowei", ""], ["Lin", "Chenguo", ""], ["Zhao", "Qijun", ""], ["Zhu", "Feiyu", ""]]}, {"id": "2103.12496", "submitter": "Uehwan Kim", "authors": "Ue-Hwan Kim, Jong-Hwan Kim", "title": "Revisiting Self-Supervised Monocular Depth Estimation", "comments": "14 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning of depth map prediction and motion estimation from\nmonocular video sequences is of vital importance -- since it realizes a broad\nrange of tasks in robotics and autonomous vehicles. A large number of research\nefforts have enhanced the performance by tackling illumination variation,\nocclusions, and dynamic objects, to name a few. However, each of those efforts\ntargets individual goals and endures as separate works. Moreover, most of\nprevious works have adopted the same CNN architecture, not reaping\narchitectural benefits. Therefore, the need to investigate the inter-dependency\nof the previous methods and the effect of architectural factors remains. To\nachieve these objectives, we revisit numerous previously proposed\nself-supervised methods for joint learning of depth and motion, perform a\ncomprehensive empirical study, and unveil multiple crucial insights.\nFurthermore, we remarkably enhance the performance as a result of our study --\noutperforming previous state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 12:45:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kim", "Ue-Hwan", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "2103.12498", "submitter": "Jaesung Choe", "authors": "Jaesung Choe, Kyungdon Joo, Francois Rameau, In So Kweon", "title": "Stereo Object Matching Network", "comments": "Accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a stereo object matching method that exploits both 2D\ncontextual information from images as well as 3D object-level information.\nUnlike existing stereo matching methods that exclusively focus on the\npixel-level correspondence between stereo images within a volumetric space\n(i.e., cost volume), we exploit this volumetric structure in a different\nmanner. The cost volume explicitly encompasses 3D information along its\ndisparity axis, therefore it is a privileged structure that can encapsulate the\n3D contextual information from objects. However, it is not straightforward\nsince the disparity values map the 3D metric space in a non-linear fashion.\nThus, we present two novel strategies to handle 3D objectness in the cost\nvolume space: selective sampling (RoISelect) and 2D-3D fusion\n(fusion-by-occupancy), which allow us to seamlessly incorporate 3D object-level\ninformation and achieve accurate depth performance near the object boundary\nregions. Our depth estimation achieves competitive performance in the KITTI\ndataset and the Virtual-KITTI 2.0 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 12:54:43 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Choe", "Jaesung", ""], ["Joo", "Kyungdon", ""], ["Rameau", "Francois", ""], ["Kweon", "In So", ""]]}, {"id": "2103.12511", "submitter": "Xuewu Lin", "authors": "Xuewu Lin, Yu-ang Guo, Jianqiang Wang", "title": "Global Correlation Network: End-to-End Joint Multi-Object Detection and\n  Tracking", "comments": "9 pages, 5 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-object tracking (MOT) has made great progress in recent years, but\nthere are still some problems. Most MOT algorithms follow tracking-by-detection\nframework, which separates detection and tracking into two independent parts.\nEarly tracking-by-detection algorithms need to do two feature extractions for\ndetection and tracking. Recently, some algorithms make the feature extraction\ninto one network, but the tracking part still relies on data association and\nneeds complex post-processing for life cycle management. Those methods do not\ncombine detection and tracking well. In this paper, we present a novel network\nto realize joint multi-object detection and tracking in an end-to-end way,\ncalled Global Correlation Network (GCNet). Different from most object detection\nmethods, GCNet introduces the global correlation layer for regression of\nabsolute size and coordinates of bounding boxes instead of offsets prediction.\nThe pipeline of detection and tracking by GCNet is conceptually simple, which\ndoes not need non-maximum suppression, data association, and other complicated\ntracking strategies. GCNet was evaluated on a multi-vehicle tracking dataset,\nUA-DETRAC, and demonstrates promising performance compared to the\nstate-of-the-art detectors and trackers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:16:42 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 09:25:45 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lin", "Xuewu", ""], ["Guo", "Yu-ang", ""], ["Wang", "Jianqiang", ""]]}, {"id": "2103.12523", "submitter": "Anshul Pundhir", "authors": "Anshul Pundhir, Deepak Verma, Puneet Kumar, Balasubramanian Raman", "title": "Region extraction based approach for cigarette usage classification\n  using deep learning", "comments": "5 pages, 16 figures. To appear in the proceedings of the 28th IEEE\n  International Conference on Image Processing (IEEE - ICIP), September 19-22,\n  2021, Anchorage, Alaska, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has proposed a novel approach to classify the subjects' smoking\nbehavior by extracting relevant regions from a given image using deep learning.\nAfter the classification, we have proposed a conditional detection module based\non Yolo-v3, which improves model's performance and reduces its complexity. As\nper the best of our knowledge, we are the first to work on this dataset. This\ndataset contains a total of 2,400 images that include smokers and non-smokers\nequally in various environmental settings. We have evaluated the proposed\napproach's performance using quantitative and qualitative measures, which\nconfirms its effectiveness in challenging situations. The proposed approach has\nachieved a classification accuracy of 96.74% on this dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:19:43 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Pundhir", "Anshul", ""], ["Verma", "Deepak", ""], ["Kumar", "Puneet", ""], ["Raman", "Balasubramanian", ""]]}, {"id": "2103.12529", "submitter": "HaiChao Zhang", "authors": "Haichao Zhang, Kuangrong Hao, Lei Gao, Xuesong Tang, and Bing Wei", "title": "Enhanced Gradient for Differentiable Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural architecture search (NAS) methods have been proposed\nfor the automatic generation of task-oriented network architecture in image\nclassification. However, the architectures obtained by existing NAS approaches\nare optimized only for classification performance and do not adapt to devices\nwith limited computational resources. To address this challenge, we propose a\nneural network architecture search algorithm aiming to simultaneously improve\nnetwork performance (e.g., classification accuracy) and reduce network\ncomplexity. The proposed framework automatically builds the network\narchitecture at two stages: block-level search and network-level search. At the\nstage of block-level search, a relaxation method based on the gradient is\nproposed, using an enhanced gradient to design high-performance and\nlow-complexity blocks. At the stage of network-level search, we apply an\nevolutionary multi-objective algorithm to complete the automatic design from\nblocks to the target network. The experiment results demonstrate that our\nmethod outperforms all evaluated hand-crafted networks in image classification,\nwith an error rate of on CIFAR10 and an error rate of on CIFAR100, both at\nnetwork parameter size less than one megabit. Moreover, compared with other\nneural architecture search methods, our method offers a tremendous reduction in\ndesigned network architecture parameters.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:27:24 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zhang", "Haichao", ""], ["Hao", "Kuangrong", ""], ["Gao", "Lei", ""], ["Tang", "Xuesong", ""], ["Wei", "Bing", ""]]}, {"id": "2103.12532", "submitter": "Quentin Jodelet", "authors": "Quentin Jodelet, Xin Liu and Tsuyoshi Murata", "title": "Balanced Softmax Cross-Entropy for Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are prone to catastrophic forgetting when incrementally\ntrained on new classes or new tasks as adaptation to the new data leads to a\ndrastic decrease of the performance on the old classes and tasks. By using a\nsmall memory for rehearsal and knowledge distillation, recent methods have\nproven to be effective to mitigate catastrophic forgetting. However due to the\nlimited size of the memory, large imbalance between the amount of data\navailable for the old and new classes still remains which results in a\ndeterioration of the overall accuracy of the model. To address this problem, we\npropose the use of the Balanced Softmax Cross-Entropy loss and show that it can\nbe combined with exiting methods for incremental learning to improve their\nperformances while also decreasing the computational cost of the training\nprocedure in some cases. Complete experiments on the competitive ImageNet,\nsubImageNet and CIFAR100 datasets show states-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:30:26 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 10:05:32 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Jodelet", "Quentin", ""], ["Liu", "Xin", ""], ["Murata", "Tsuyoshi", ""]]}, {"id": "2103.12544", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri, Anupam Biswas and Sabuzima Nayak", "title": "deepBF: Malicious URL detection using Learned Bloom Filter and\n  Evolutionary Deep Learning", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malicious URL detection is an emerging research area due to continuous\nmodernization of various systems, for instance, Edge Computing. In this\narticle, we present a novel malicious URL detection technique, called deepBF\n(deep learning and Bloom Filter). deepBF is presented in two-fold. Firstly, we\npropose a learned Bloom Filter using 2-dimensional Bloom Filter. We\nexperimentally decide the best non-cryptography string hash function. Then, we\nderive a modified non-cryptography string hash function from the selected hash\nfunction for deepBF by introducing biases in the hashing method and compared\namong the string hash functions. The modified string hash function is compared\nto other variants of diverse non-cryptography string hash functions. It is also\ncompared with various filters, particularly, counting Bloom Filter, Kirsch\n\\textit{et al.}, and Cuckoo Filter using various use cases. The use cases\nunearth weakness and strength of the filters. Secondly, we propose a malicious\nURL detection mechanism using deepBF. We apply the evolutionary convolutional\nneural network to identify the malicious URLs. The evolutionary convolutional\nneural network is trained and tested with malicious URL datasets. The output is\ntested in deepBF for accuracy. We have achieved many conclusions from our\nexperimental evaluation and results and are able to reach various conclusive\ndecisions which are presented in the article.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:53:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Patgiri", "Ripon", ""], ["Biswas", "Anupam", ""], ["Nayak", "Sabuzima", ""]]}, {"id": "2103.12545", "submitter": "Edwin Pan", "authors": "Edwin Pan, Anthony Vento", "title": "MetaHDR: Model-Agnostic Meta-Learning for HDR Image Reconstruction", "comments": "7 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Capturing scenes with a high dynamic range is crucial to reproducing images\nthat appear similar to those seen by the human visual system. Despite progress\nin developing data-driven deep learning approaches for converting low dynamic\nrange images to high dynamic range images, existing approaches are limited by\nthe assumption that all conversions are governed by the same nonlinear mapping.\nTo address this problem, we propose \"Model-Agnostic Meta-Learning for HDR Image\nReconstruction\" (MetaHDR), which applies meta-learning to the LDR-to-HDR\nconversion problem using existing HDR datasets. Our key novelty is the\nreinterpretation of LDR-to-HDR conversion scenes as independently sampled tasks\nfrom a common LDR-to-HDR conversion task distribution. Naturally, we use a\nmeta-learning framework that learns a set of meta-parameters which capture the\ncommon structure consistent across all LDR-to-HDR conversion tasks. Finally, we\nperform experimentation with MetaHDR to demonstrate its capacity to tackle\nchallenging LDR-to-HDR image conversions. Code and pretrained models are\navailable at https://github.com/edwin-pan/MetaHDR.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 07:56:45 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Pan", "Edwin", ""], ["Vento", "Anthony", ""]]}, {"id": "2103.12547", "submitter": "Achraf Djerida", "authors": "Achraf Djerida, Khelifa Djerriri, Moussa Sofiane Karoui and Mohammed\n  El Amin larabi", "title": "A new public Alsat-2B dataset for single-image super-resolution", "comments": "This paper has been Accepted for publication in the International\n  Geoscience and Remote Sensing Symposium (IGARSS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Currently, when reliable training datasets are available, deep learning\nmethods dominate the proposed solutions for image super-resolution. However,\nfor remote sensing benchmarks, it is very expensive to obtain high spatial\nresolution images. Most of the super-resolution methods use down-sampling\ntechniques to simulate low and high spatial resolution pairs and construct the\ntraining samples. To solve this issue, the paper introduces a novel public\nremote sensing dataset (Alsat2B) of low and high spatial resolution images (10m\nand 2.5m respectively) for the single-image super-resolution task. The\nhigh-resolution images are obtained through pan-sharpening. Besides, the\nperformance of some super-resolution methods on the dataset is assessed based\non common criteria. The obtained results reveal that the proposed scheme is\npromising and highlight the challenges in the dataset which shows the need for\nadvanced methods to grasp the relationship between the low and high-resolution\npatches.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:47:38 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Djerida", "Achraf", ""], ["Djerriri", "Khelifa", ""], ["Karoui", "Moussa Sofiane", ""], ["larabi", "Mohammed El Amin", ""]]}, {"id": "2103.12562", "submitter": "Mixue Xie", "authors": "Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Wei\n  Li", "title": "Transferable Semantic Augmentation for Domain Adaptation", "comments": "Accepted as CVPR 2021. The code is publicly available at\n  https://github.com/BIT-DA/TSA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation has been widely explored by transferring the knowledge from\na label-rich source domain to a related but unlabeled target domain. Most\nexisting domain adaptation algorithms attend to adapting feature\nrepresentations across two domains with the guidance of a shared\nsource-supervised classifier. However, such classifier limits the\ngeneralization ability towards unlabeled target recognition. To remedy this, we\npropose a Transferable Semantic Augmentation (TSA) approach to enhance the\nclassifier adaptation ability through implicitly generating source features\ntowards target semantics. Specifically, TSA is inspired by the fact that deep\nfeature transformation towards a certain direction can be represented as\nmeaningful semantic altering in the original input space. Thus, source features\ncan be augmented to effectively equip with target semantics to train a more\ntransferable classifier. To achieve this, for each class, we first use the\ninter-domain feature mean difference and target intra-class feature covariance\nto construct a multivariate normal distribution. Then we augment source\nfeatures with random directions sampled from the distribution class-wisely.\nInterestingly, such source augmentation is implicitly implemented through an\nexpected transferable cross-entropy loss over the augmented source\ndistribution, where an upper bound of the expected loss is derived and\nminimized, introducing negligible computational overhead. As a light-weight and\ngeneral technique, TSA can be easily plugged into various domain adaptation\nmethods, bringing remarkable improvements. Comprehensive experiments on\ncross-domain benchmarks validate the efficacy of TSA.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:04:11 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Mixue", ""], ["Gong", "Kaixiong", ""], ["Liu", "Chi Harold", ""], ["Wang", "Yulin", ""], ["Li", "Wei", ""]]}, {"id": "2103.12575", "submitter": "Shunji Itani", "authors": "S. Itani, S. Kita and Y. Kajikawa", "title": "Multimodal Personal Ear Authentication Using Smartphones", "comments": "9 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, biometric authentication technology for smartphones has\nbecome widespread, with the mainstream methods being fingerprint authentication\nand face recognition. However, fingerprint authentication cannot be used when\nhands are wet, and face recognition cannot be used when a person is wearing a\nmask. Therefore, we examine a personal authentication system using the pinna as\na new approach for biometric authentication on smartphones. Authentication\nsystems based on the acoustic transfer function of the pinna (PRTF: Pinna\nRelated Transfer Function) have been investigated. However, the authentication\naccuracy decreases due to the positional fluctuation across each measurement.\nIn this paper, we propose multimodal personal authentication on smartphones\nusing PRTF. The pinna image and positional sensor information are used with the\nPRTF, and the effectiveness of the authentication method is examined. We\ndemonstrate that the proposed authentication system can compensate for the\npositional changes in each measurement and improve robustness.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:19:15 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Itani", "S.", ""], ["Kita", "S.", ""], ["Kajikawa", "Y.", ""]]}, {"id": "2103.12579", "submitter": "Kaixiong Gong", "authors": "Shuang Li, Kaixiong Gong, Chi Harold Liu, Yulin Wang, Feng Qiao,\n  Xinjing Cheng", "title": "MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world training data usually exhibits long-tailed distribution, where\nseveral majority classes have a significantly larger number of samples than the\nremaining minority classes. This imbalance degrades the performance of typical\nsupervised learning algorithms designed for balanced training sets. In this\npaper, we address this issue by augmenting minority classes with a recently\nproposed implicit semantic data augmentation (ISDA) algorithm, which produces\ndiversified augmented samples by translating deep features along many\nsemantically meaningful directions. Importantly, given that ISDA estimates the\nclass-conditional statistics to obtain semantic directions, we find it\nineffective to do this on minority classes due to the insufficient training\ndata. To this end, we propose a novel approach to learn transformed semantic\ndirections with meta-learning automatically. In specific, the augmentation\nstrategy during training is dynamically optimized, aiming to minimize the loss\non a small balanced validation set, which is approximated via a meta update\nstep. Extensive empirical results on CIFAR-LT-10/100, ImageNet-LT, and\niNaturalist 2017/2018 validate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:31:33 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 04:49:57 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 12:01:14 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Li", "Shuang", ""], ["Gong", "Kaixiong", ""], ["Liu", "Chi Harold", ""], ["Wang", "Yulin", ""], ["Qiao", "Feng", ""], ["Cheng", "Xinjing", ""]]}, {"id": "2103.12595", "submitter": "Maria Ines Meyer", "authors": "Maria Ines Meyer, Ezequiel de la Rosa, Nuno Barros, Roberto Paolella,\n  Koen Van Leemput and Diana M. Sima", "title": "An augmentation strategy to mimic multi-scanner variability in MRI", "comments": "5 pages, 2 figures. accepted for presentation at the International\n  Symposium on Biomedical Imaging (ISBI) 2021. Code available at\n  https://github.com/icometrix/gmm-augmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most publicly available brain MRI datasets are very homogeneous in terms of\nscanner and protocols, and it is difficult for models that learn from such data\nto generalize to multi-center and multi-scanner data. We propose a novel data\naugmentation approach with the aim of approximating the variability in terms of\nintensities and contrasts present in real world clinical data. We use a\nGaussian Mixture Model based approach to change tissue intensities\nindividually, producing new contrasts while preserving anatomical information.\nWe train a deep learning model on a single scanner dataset and evaluate it on a\nmulti-center and multi-scanner dataset. The proposed approach improves the\ngeneralization capability of the model to other scanners not present in the\ntraining data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:49:38 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Meyer", "Maria Ines", ""], ["de la Rosa", "Ezequiel", ""], ["Barros", "Nuno", ""], ["Paolella", "Roberto", ""], ["Van Leemput", "Koen", ""], ["Sima", "Diana M.", ""]]}, {"id": "2103.12605", "submitter": "Hansheng Chen", "authors": "Hansheng Chen, Yuyao Huang, Wei Tian, Zhong Gao, Lu Xiong", "title": "MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty\n  Propagation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object localization in 3D space is a challenging aspect in monocular 3D\nobject detection. Recent advances in 6DoF pose estimation have shown that\npredicting dense 2D-3D correspondence maps between image and object 3D model\nand then estimating object pose via Perspective-n-Point (PnP) algorithm can\nachieve remarkable localization accuracy. Yet these methods rely on training\nwith ground truth of object geometry, which is difficult to acquire in real\noutdoor scenes. To address this issue, we propose MonoRUn, a novel detection\nframework that learns dense correspondences and geometry in a self-supervised\nmanner, with simple 3D bounding box annotations. To regress the pixel-related\n3D object coordinates, we employ a regional reconstruction network with\nuncertainty awareness. For self-supervised training, the predicted 3D\ncoordinates are projected back to the image plane. A Robust KL loss is proposed\nto minimize the uncertainty-weighted reprojection error. During testing phase,\nwe exploit the network uncertainty by propagating it through all downstream\nmodules. More specifically, the uncertainty-driven PnP algorithm is leveraged\nto estimate object pose and its covariance. Extensive experiments demonstrate\nthat our proposed approach outperforms current state-of-the-art methods on\nKITTI benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 15:03:08 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 12:28:15 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chen", "Hansheng", ""], ["Huang", "Yuyao", ""], ["Tian", "Wei", ""], ["Gao", "Zhong", ""], ["Xiong", "Lu", ""]]}, {"id": "2103.12609", "submitter": "Sixiao Zheng", "authors": "Sixiao Zheng, Yanwei Fu, Yanxi Hou", "title": "Incrementally Zero-Shot Detection by an Extreme Value Analyzer", "comments": "International Conference on Pattern Recognition (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human beings not only have the ability to recognize novel unseen classes, but\nalso can incrementally incorporate the new classes to existing knowledge\npreserved. However, zero-shot learning models assume that all seen classes\nshould be known beforehand, while incremental learning models cannot recognize\nunseen classes. This paper introduces a novel and challenging task of\nIncrementally Zero-Shot Detection (IZSD), a practical strategy for both\nzero-shot learning and class-incremental learning in real-world object\ndetection. An innovative end-to-end model -- IZSD-EVer was proposed to tackle\nthis task that requires incrementally detecting new classes and detecting the\nclasses that have never been seen. Specifically, we propose a novel extreme\nvalue analyzer to detect objects from old seen, new seen, and unseen classes,\nsimultaneously. Additionally and technically, we propose two innovative losses,\ni.e., background-foreground mean squared error loss alleviating the extreme\nimbalance of the background and foreground of images, and projection distance\nloss aligning the visual space and semantic spaces of old seen classes.\nExperiments demonstrate the efficacy of our model in detecting objects from\nboth the seen and unseen classes, outperforming the alternative models on\nPascal VOC and MSCOCO datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 15:06:30 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 16:12:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zheng", "Sixiao", ""], ["Fu", "Yanwei", ""], ["Hou", "Yanxi", ""]]}, {"id": "2103.12622", "submitter": "Julio Marco", "authors": "Julio Marco, Adrian Jarabo, Ji Hyun Nam, Xiaochun Liu, Miguel \\'Angel\n  Cosculluela, Andreas Velten, Diego Gutierrez", "title": "Virtual light transport matrices for non-line-of-sight imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The light transport matrix (LTM) is an instrumental tool in line-of-sight\n(LOS) imaging, describing how light interacts with the scene and enabling\napplications such as relighting or separation of illumination components. We\nintroduce a framework to estimate the LTM of non-line-of-sight (NLOS)\nscenarios, coupling recent virtual forward light propagation models for NLOS\nimaging with the LOS light transport equation. We design computational\nprojector-camera setups, and use these virtual imaging systems to estimate the\ntransport matrix of hidden scenes. We introduce the specific illumination\nfunctions to compute the different elements of the matrix, overcoming the\nchallenging wide-aperture conditions of NLOS setups. Our NLOS light transport\nmatrix allows us to (re)illuminate specific locations of a hidden scene, and\nseparate direct, first-order indirect, and higher-order indirect illumination\nof complex cluttered hidden scenes, similar to existing LOS techniques.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 15:17:45 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Marco", "Julio", ""], ["Jarabo", "Adrian", ""], ["Nam", "Ji Hyun", ""], ["Liu", "Xiaochun", ""], ["Cosculluela", "Miguel \u00c1ngel", ""], ["Velten", "Andreas", ""], ["Gutierrez", "Diego", ""]]}, {"id": "2103.12650", "submitter": "Oscar J. Pellicer-Valero", "authors": "Oscar J. Pellicer-Valero, Jos\\'e L. Marenco Jim\\'enez, Victor\n  Gonzalez-Perez, Juan Luis Casanova Ram\\'on-Borja, Isabel Mart\\'in Garc\\'ia,\n  Mar\\'ia Barrios Benito, Paula Pelechano G\\'omez, Jos\\'e Rubio-Briones,\n  Mar\\'ia Jos\\'e Rup\\'erez, Jos\\'e D. Mart\\'in-Guerrero", "title": "Deep Learning for fully automatic detection, segmentation, and Gleason\n  Grade estimation of prostate cancer in multiparametric Magnetic Resonance\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of multi-parametric magnetic resonance imaging (mpMRI) has had\na profound impact on the diagnosis of prostate cancers (PCa), which is the most\nprevalent malignancy in males in the western world, enabling a better selection\nof patients for confirmation biopsy. However, analyzing these images is complex\neven for experts, hence opening an opportunity for computer-aided diagnosis\nsystems to seize. This paper proposes a fully automatic system based on Deep\nLearning that takes a prostate mpMRI from a PCa-suspect patient and, by\nleveraging the Retina U-Net detection framework, locates PCa lesions, segments\nthem, and predicts their most likely Gleason grade group (GGG). It uses 490\nmpMRIs for training/validation, and 75 patients for testing from two different\ndatasets: ProstateX and IVO (Valencia Oncology Institute Foundation). In the\ntest set, it achieves an excellent lesion-level AUC/sensitivity/specificity for\nthe GGG$\\geq$2 significance criterion of 0.96/1.00/0.79 for the ProstateX\ndataset, and 0.95/1.00/0.80 for the IVO dataset. Evaluated at a patient level,\nthe results are 0.87/1.00/0.375 in ProstateX, and 0.91/1.00/0.762 in IVO.\nFurthermore, on the online ProstateX grand challenge, the model obtained an AUC\nof 0.85 (0.87 when trained only on the ProstateX data, tying up with the\noriginal winner of the challenge). For expert comparison, IVO radiologist's\nPI-RADS 4 sensitivity/specificity were 0.88/0.56 at a lesion level, and\n0.85/0.58 at a patient level. Additional subsystems for automatic prostate\nzonal segmentation and mpMRI non-rigid sequence registration were also employed\nto produce the final fully automated system. The code for the ProstateX-trained\nsystem has been made openly available at\nhttps://github.com/OscarPellicer/prostate_lesion_detection. We hope that this\nwill represent a landmark for future research to use, compare and improve upon.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 16:08:43 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 11:56:41 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Pellicer-Valero", "Oscar J.", ""], ["Jim\u00e9nez", "Jos\u00e9 L. Marenco", ""], ["Gonzalez-Perez", "Victor", ""], ["Ram\u00f3n-Borja", "Juan Luis Casanova", ""], ["Garc\u00eda", "Isabel Mart\u00edn", ""], ["Benito", "Mar\u00eda Barrios", ""], ["G\u00f3mez", "Paula Pelechano", ""], ["Rubio-Briones", "Jos\u00e9", ""], ["Rup\u00e9rez", "Mar\u00eda Jos\u00e9", ""], ["Mart\u00edn-Guerrero", "Jos\u00e9 D.", ""]]}, {"id": "2103.12672", "submitter": "Amaan Valiuddin", "authors": "M.M.A. Valiuddin, C.G.A. Viviers", "title": "Out-of-Distribution Detection of Melanoma using Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative modelling has been a topic at the forefront of machine learning\nresearch for a substantial amount of time. With the recent success in the field\nof machine learning, especially in deep learning, there has been an increased\ninterest in explainable and interpretable machine learning. The ability to\nmodel distributions and provide insight in the density estimation and exact\ndata likelihood is an example of such a feature. Normalizing Flows (NFs), a\nrelatively new research field of generative modelling, has received substantial\nattention since it is able to do exactly this at a relatively low cost whilst\nenabling competitive generative results. While the generative abilities of NFs\nare typically explored, we focus on exploring the data distribution modelling\nfor Out-of-Distribution (OOD) detection. Using one of the state-of-the-art NF\nmodels, GLOW, we attempt to detect OOD examples in the ISIC dataset. We notice\nthat this model under performs in conform related research. To improve the OOD\ndetection, we explore the masking methods to inhibit co-adaptation of the\ncoupling layers however find no substantial improvement. Furthermore, we\nutilize Wavelet Flow which uses wavelets that can filter particular frequency\ncomponents, thus simplifying the modeling process to data-driven conditional\nwavelet coefficients instead of complete images. This enables us to efficiently\nmodel larger resolution images in the hopes that it would capture more relevant\nfeatures for OOD. The paper that introduced Wavelet Flow mainly focuses on its\nability of sampling high resolution images and did not treat OOD detection. We\npresent the results and propose several ideas for improvement such as\ncontrolling frequency components, using different wavelets and using other\nstate-of-the-art NF architectures.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 16:47:19 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Valiuddin", "M. M. A.", ""], ["Viviers", "C. G. A.", ""]]}, {"id": "2103.12703", "submitter": "Alexander Ku", "authors": "Alexander Ku and Peter Anderson and Jordi Pont-Tuset and Jason\n  Baldridge", "title": "PanGEA: The Panoramic Graph Environment Annotation Toolkit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight\ntoolkit for collecting speech and text annotations in photo-realistic 3D\nenvironments. PanGEA immerses annotators in a web-based simulation and allows\nthem to move around easily as they speak and/or listen. It includes database\nand cloud storage integration, plus utilities for automatically aligning\nrecorded speech with manual transcriptions and the virtual pose of the\nannotators. Out of the box, PanGEA supports two tasks -- collecting navigation\ninstructions and navigation instruction following -- and it could be easily\nadapted for annotating walking tours, finding and labeling landmarks or\nobjects, and similar tasks. We share best practices learned from using PanGEA\nin a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We\nhope that our open-source annotation toolkit and insights will both expedite\nfuture data collection efforts and spur innovation on the kinds of grounded\nlanguage tasks such environments can support.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:24:12 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ku", "Alexander", ""], ["Anderson", "Peter", ""], ["Pont-Tuset", "Jordi", ""], ["Baldridge", "Jason", ""]]}, {"id": "2103.12710", "submitter": "Jimmy Wu", "authors": "Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Szymon Rusinkiewicz,\n  Thomas Funkhouser", "title": "Spatial Intention Maps for Multi-Agent Mobile Manipulation", "comments": "To appear at IEEE International Conference on Robotics and Automation\n  (ICRA), 2021. Project page: https://spatial-intention-maps.cs.princeton.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to communicate intention enables decentralized multi-agent robots\nto collaborate while performing physical tasks. In this work, we present\nspatial intention maps, a new intention representation for multi-agent\nvision-based deep reinforcement learning that improves coordination between\ndecentralized mobile manipulators. In this representation, each agent's\nintention is provided to other agents, and rendered into an overhead 2D map\naligned with visual observations. This synergizes with the recently proposed\nspatial action maps framework, in which state and action representations are\nspatially aligned, providing inductive biases that encourage emergent\ncooperative behaviors requiring spatial coordination, such as passing objects\nto each other or avoiding collisions. Experiments across a variety of\nmulti-agent environments, including heterogeneous robot teams with different\nabilities (lifting, pushing, or throwing), show that incorporating spatial\nintention maps improves performance for different mobile manipulation tasks\nwhile significantly enhancing cooperative behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:31:14 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wu", "Jimmy", ""], ["Sun", "Xingyuan", ""], ["Zeng", "Andy", ""], ["Song", "Shuran", ""], ["Rusinkiewicz", "Szymon", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2103.12716", "submitter": "Xingqian Xu", "authors": "Xingqian Xu, Zhangyang Wang, Humphrey Shi", "title": "UltraSR: Spatial Encoding is a Missing Key for Implicit Image\n  Function-based Arbitrary-Scale Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent success of NeRF and other related implicit neural representation\nmethods has opened a new path for continuous image representation, where pixel\nvalues no longer need to be looked up from stored discrete 2D arrays but can be\ninferred from neural network models on a continuous spatial domain. Although\nthe recent work LIIF has demonstrated that such novel approach can achieve good\nperformance on the arbitrary-scale super-resolution task, their upscaled images\nfrequently show structural distortion due to the faulty prediction on\nhigh-frequency textures. In this work, we propose UltraSR, a simple yet\neffective new network design based on implicit image functions in which spatial\ncoordinates and periodic encoding are deeply integrated with the implicit\nneural representation. We show that spatial encoding is indeed a missing key\ntowards the next-stage high-accuracy implicit image function through extensive\nexperiments and ablation studies. Our UltraSR sets new state-of-the-art\nperformance on the DIV2K benchmark under all super-resolution scales comparing\nto previous state-of-the-art methods. UltraSR also achieves superior\nperformance on other standard benchmark datasets in which it outperforms prior\nworks in almost all experiments. Our code will be released at\nhttps://github.com/SHI-Labs/UltraSR-Arbitrary-Scale-Super-Resolution.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:36:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Xu", "Xingqian", ""], ["Wang", "Zhangyang", ""], ["Shi", "Humphrey", ""]]}, {"id": "2103.12718", "submitter": "Colorado J Reed", "authors": "Colorado J. Reed and Xiangyu Yue and Ani Nrusimha and Sayna Ebrahimi\n  and Vivek Vijaykumar and Richard Mao and Bo Li and Shanghang Zhang and Devin\n  Guillory and Sean Metzger and Kurt Keutzer and Trevor Darrell", "title": "Self-Supervised Pretraining Improves Self-Supervised Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While self-supervised pretraining has proven beneficial for many computer\nvision tasks, it requires expensive and lengthy computation, large amounts of\ndata, and is sensitive to data augmentation. Prior work demonstrates that\nmodels pretrained on datasets dissimilar to their target data, such as chest\nX-ray models trained on ImageNet, underperform models trained from scratch.\nUsers that lack the resources to pretrain must use existing models with lower\nperformance. This paper explores Hierarchical PreTraining (HPT), which\ndecreases convergence time and improves accuracy by initializing the\npretraining process with an existing pretrained model. Through experimentation\non 16 diverse vision datasets, we show HPT converges up to 80x faster, improves\naccuracy across tasks, and improves the robustness of the self-supervised\npretraining process to changes in the image augmentation policy or amount of\npretraining data. Taken together, HPT provides a simple framework for obtaining\nbetter pretrained representations with less computational resources.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:37:51 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 00:33:47 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Reed", "Colorado J.", ""], ["Yue", "Xiangyu", ""], ["Nrusimha", "Ani", ""], ["Ebrahimi", "Sayna", ""], ["Vijaykumar", "Vivek", ""], ["Mao", "Richard", ""], ["Li", "Bo", ""], ["Zhang", "Shanghang", ""], ["Guillory", "Devin", ""], ["Metzger", "Sean", ""], ["Keutzer", "Kurt", ""], ["Darrell", "Trevor", ""]]}, {"id": "2103.12719", "submitter": "Chaitanya Ryali", "authors": "Chaitanya K. Ryali, David J. Schwab, Ari S. Morcos", "title": "Leveraging background augmentations to encourage semantic focus in\n  self-supervised contrastive learning", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning is an important challenge in computer\nvision, with self-supervised learning methods recently closing the gap to\nsupervised representation learning. An important ingredient in high-performing\nself-supervised methods is the use of data augmentation by training models to\nplace different augmented views of the same image nearby in embedding space.\nHowever, commonly used augmentation pipelines treat images holistically,\ndisregarding the semantic relevance of parts of an image-e.g. a subject vs. a\nbackground-which can lead to the learning of spurious correlations. Our work\naddresses this problem by investigating a class of simple, yet highly effective\n\"background augmentations\", which encourage models to focus on\nsemantically-relevant content by discouraging them from focusing on image\nbackgrounds. Background augmentations lead to substantial improvements (+1-2%\non ImageNet-1k) in performance across a spectrum of state-of-the art\nself-supervised methods (MoCov2, BYOL, SwAV) on a variety of tasks, allowing us\nto reach within 0.3% of supervised performance. We also demonstrate that\nbackground augmentations improve robustness to a number of out of distribution\nsettings, including natural adversarial examples, the backgrounds challenge,\nadversarial attacks, and ReaL ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:39:16 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ryali", "Chaitanya K.", ""], ["Schwab", "David J.", ""], ["Morcos", "Ari S.", ""]]}, {"id": "2103.12723", "submitter": "HongYu Liu", "authors": "Hongyu Liu, Ziyu Wan, Wei Huang, Yibing Song, Xintong Han, Jing Liao,\n  Bing Jiang, Wei Liu", "title": "DeFLOCNet: Deep Image Editing via Flexible Low-level Controls", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-intended visual content fills the hole regions of an input image in the\nimage editing scenario. The coarse low-level inputs, which typically consist of\nsparse sketch lines and color dots, convey user intentions for content creation\n(\\ie, free-form editing). While existing methods combine an input image and\nthese low-level controls for CNN inputs, the corresponding feature\nrepresentations are not sufficient to convey user intentions, leading to\nunfaithfully generated content. In this paper, we propose DeFLOCNet which\nrelies on a deep encoder-decoder CNN to retain the guidance of these controls\nin the deep feature representations. In each skip-connection layer, we design a\nstructure generation block. Instead of attaching low-level controls to an input\nimage, we inject these controls directly into each structure generation block\nfor sketch line refinement and color propagation in the CNN feature space. We\nthen concatenate the modulated features with the original decoder features for\nstructure generation. Meanwhile, DeFLOCNet involves another decoder branch for\ntexture generation and detail enhancement. Both structures and textures are\nrendered in the decoder, leading to user-intended editing results. Experiments\non benchmarks demonstrate that DeFLOCNet effectively transforms different user\nintentions to create visually pleasing content.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:47:23 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Liu", "Hongyu", ""], ["Wan", "Ziyu", ""], ["Huang", "Wei", ""], ["Song", "Yibing", ""], ["Han", "Xintong", ""], ["Liao", "Jing", ""], ["Jiang", "Bing", ""], ["Liu", "Wei", ""]]}, {"id": "2103.12731", "submitter": "Ashish Vaswani", "authors": "Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar,\n  Blake Hechtman, Jonathon Shlens", "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-attention has the promise of improving computer vision systems due to\nparameter-independent scaling of receptive fields and content-dependent\ninteractions, in contrast to parameter-dependent scaling and\ncontent-independent interactions of convolutions. Self-attention models have\nrecently been shown to have encouraging improvements on accuracy-parameter\ntrade-offs compared to baseline convolutional models such as ResNet-50. In this\nwork, we aim to develop self-attention models that can outperform not just the\ncanonical baseline models, but even the high-performing convolutional models.\nWe propose two extensions to self-attention that, in conjunction with a more\nefficient implementation of self-attention, improve the speed, memory usage,\nand accuracy of these models. We leverage these improvements to develop a new\nself-attention model family, HaloNets, which reach state-of-the-art accuracies\non the parameter-limited setting of the ImageNet classification benchmark. In\npreliminary transfer learning experiments, we find that HaloNet models\noutperform much larger models and have better inference performance. On harder\ntasks such as object detection and instance segmentation, our simple local\nself-attention and convolutional hybrids show improvements over very strong\nbaselines. These results mark another step in demonstrating the efficacy of\nself-attention models on settings traditionally dominated by convolutional\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:56:06 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 17:46:25 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 05:42:10 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Vaswani", "Ashish", ""], ["Ramachandran", "Prajit", ""], ["Srinivas", "Aravind", ""], ["Parmar", "Niki", ""], ["Hechtman", "Blake", ""], ["Shlens", "Jonathon", ""]]}, {"id": "2103.12768", "submitter": "Mirco Planamente", "authors": "Mirco Planamente and Chiara Plizzari and Marco Cannici and Marco\n  Ciccone and Francesco Strada and Andrea Bottino and Matteo Matteucci and\n  Barbara Caputo", "title": "DA4Event: towards bridging the Sim-to-Real Gap for Event Cameras using\n  Domain Adaptation", "comments": "This paper is currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are novel bio-inspired sensors, which asynchronously capture\npixel-level intensity changes in the form of \"events\". The innovative way they\nacquire data presents several advantages over standard devices, especially in\npoor lighting and high-speed motion conditions. However, the novelty of these\nsensors results in the lack of a large amount of training data capable of fully\nunlocking their potential. The most common approach implemented by researchers\nto address this issue is to leverage simulated event data. Yet, this approach\ncomes with an open research question: how well simulated data generalize to\nreal data? To answer this, we propose to exploit, in the event-based context,\nrecent Domain Adaptation (DA) advances in traditional computer vision, showing\nthat DA techniques applied to event data help reduce the sim-to-real gap. To\nthis purpose, we propose a novel architecture, which we call Multi-View DA4E\n(MV-DA4E), that better exploits the peculiarities of frame-based event\nrepresentations while also promoting domain invariant characteristics in\nfeatures. Through extensive experiments, we prove the effectiveness of DA\nmethods and MV-DA4E on N-Caltech101. Moreover, we validate their soundness in a\nreal-world scenario through a cross-domain analysis on the popular RGB-D Object\nDataset (ROD), which we extended to the event modality (RGB-E).\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 18:09:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Planamente", "Mirco", ""], ["Plizzari", "Chiara", ""], ["Cannici", "Marco", ""], ["Ciccone", "Marco", ""], ["Strada", "Francesco", ""], ["Bottino", "Andrea", ""], ["Matteucci", "Matteo", ""], ["Caputo", "Barbara", ""]]}, {"id": "2103.12770", "submitter": "Patrick Geneva", "authors": "Pengxiang Zhu, Patrick Geneva, Wei Ren, and Guoquan Huang", "title": "Distributed Visual-Inertial Cooperative Localization", "comments": "8 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a consistent and distributed state estimator for\nmulti-robot cooperative localization (CL) which efficiently fuses environmental\nfeatures and loop-closure constraints across time and robots. In particular, we\nleverage covariance intersection (CI) to allow each robot to only track its own\nstate and autocovariance and compensate for the unknown correlations between\nrobots. Two novel different methods for utilizing common environmental temporal\nSLAM features are introduced and evaluated in terms of accuracy and efficiency.\nMoreover, we adapt CI to enable drift-free estimation through the use of\nloop-closure measurement constraints to other robots' historical poses without\na significant increase in computational cost. The proposed distributed CL\nestimator is validated against its naive non-realtime centralized counterpart\nextensively in both simulations and real-world experiments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 18:12:07 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhu", "Pengxiang", ""], ["Geneva", "Patrick", ""], ["Ren", "Wei", ""], ["Huang", "Guoquan", ""]]}, {"id": "2103.12814", "submitter": "Yangdi Lu", "authors": "Yangdi Lu, Yang Bo, Wenbo He", "title": "Co-matching: Combating Noisy Labels by Augmentation Anchoring", "comments": "13 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:2003.02752 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning with noisy labels is challenging as deep neural networks have\nthe high capacity to memorize the noisy labels. In this paper, we propose a\nlearning algorithm called Co-matching, which balances the consistency and\ndivergence between two networks by augmentation anchoring. Specifically, we\nhave one network generate anchoring label from its prediction on a\nweakly-augmented image. Meanwhile, we force its peer network, taking the\nstrongly-augmented version of the same image as input, to generate prediction\nclose to the anchoring label. We then update two networks simultaneously by\nselecting small-loss instances to minimize both unsupervised matching loss\n(i.e., measure the consistency of the two networks) and supervised\nclassification loss (i.e. measure the classification performance). Besides, the\nunsupervised matching loss makes our method not heavily rely on noisy labels,\nwhich prevents memorization of noisy labels. Experiments on three benchmark\ndatasets demonstrate that Co-matching achieves results comparable to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:00:13 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lu", "Yangdi", ""], ["Bo", "Yang", ""], ["He", "Wenbo", ""]]}, {"id": "2103.12815", "submitter": "Paul Horton", "authors": "Paul Horton, Hannah R. Kerner, Samantha Jacob, Ernest Cisneros, Kiri\n  L. Wagstaff, James Bell", "title": "Integrating Novelty Detection Capabilities with MSL Mastcam Operations\n  to Enhance Data Analysis", "comments": "8 pages, 5 figure, accepted and presented at IEEE Aerospace\n  Conference 2021", "journal-ref": null, "doi": "10.1109/AERO50100.2021.9438280", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While innovations in scientific instrumentation have pushed the boundaries of\nMars rover mission capabilities, the increase in data complexity has pressured\nMars Science Laboratory (MSL) and future Mars rover operations staff to quickly\nanalyze complex data sets to meet progressively shorter tactical and strategic\nplanning timelines. MSLWEB is an internal data tracking tool used by operations\nstaff to perform first pass analysis on MSL image sequences, a series of\nproducts taken by the Mast camera, Mastcam. Mastcam's multiband multispectral\nimage sequences require more complex analysis compared to standard 3-band RGB\nimages. Typically, these are analyzed using traditional methods to identify\nunique features within the sequence. Given the short time frame of tactical\nplanning in which downlinked images might need to be analyzed (within 5-10\nhours before the next uplink), there exists a need to triage analysis time to\nfocus on the most important sequences and parts of a sequence. We address this\nneed by creating products for MSLWEB that use novelty detection to help\noperations staff identify unusual data that might be diagnostic of new or\natypical compositions or mineralogies detected within an imaging scene. This\nwas achieved in two ways: 1) by creating products for each sequence to identify\nnovel regions in the image, and 2) by assigning multispectral sequences a\nsortable novelty score. These new products provide colorized heat maps of\ninferred novelty that operations staff can use to rapidly review downlinked\ndata and focus their efforts on analyzing potentially new kinds of diagnostic\nmultispectral signatures. This approach has the potential to guide scientists\nto new discoveries by quickly drawing their attention to often subtle\nvariations not detectable with simple color composites.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:01:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Horton", "Paul", ""], ["Kerner", "Hannah R.", ""], ["Jacob", "Samantha", ""], ["Cisneros", "Ernest", ""], ["Wagstaff", "Kiri L.", ""], ["Bell", "James", ""]]}, {"id": "2103.12871", "submitter": "Jaeyeon Jang Dr.", "authors": "Jaeyeon Jang and Chang Ouk Kim", "title": "Teacher-Explorer-Student Learning: A Novel Learning Method for Open Set\n  Recognition", "comments": "12 pages, 13 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If an unknown example that is not seen during training appears, most\nrecognition systems usually produce overgeneralized results and determine that\nthe example belongs to one of the known classes. To address this problem,\nteacher-explorer-student (T/E/S) learning, which adopts the concept of open set\nrecognition (OSR) that aims to reject unknown samples while minimizing the loss\nof classification performance on known samples, is proposed in this study. In\nthis novel learning method, overgeneralization of deep learning classifiers is\nsignificantly reduced by exploring various possibilities of unknowns. Here, the\nteacher network extracts some hints about unknowns by distilling the pretrained\nknowledge about knowns and delivers this distilled knowledge to the student.\nAfter learning the distilled knowledge, the student network shares the learned\ninformation with the explorer network. Then, the explorer network shares its\nexploration results by generating unknown-like samples and feeding the samples\nto the student network. By repeating this alternating learning process, the\nstudent network experiences a variety of synthetic unknowns, reducing\novergeneralization. Extensive experiments were conducted, and the experimental\nresults showed that each component proposed in this paper significantly\ncontributes to the improvement in OSR performance. As a result, the proposed\nT/E/S learning method outperformed current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:32:32 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Jang", "Jaeyeon", ""], ["Kim", "Chang Ouk", ""]]}, {"id": "2103.12886", "submitter": "Qing Liu", "authors": "Qing Liu, Vignesh Ramanathan, Dhruv Mahajan, Alan Yuille, Zhenheng\n  Yang", "title": "Weakly Supervised Instance Segmentation for Videos with Temporal Mask\n  Consistency", "comments": "14 pages, 8 figures, accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised instance segmentation reduces the cost of annotations\nrequired to train models. However, existing approaches which rely only on\nimage-level class labels predominantly suffer from errors due to (a) partial\nsegmentation of objects and (b) missing object predictions. We show that these\nissues can be better addressed by training with weakly labeled videos instead\nof images. In videos, motion and temporal consistency of predictions across\nframes provide complementary signals which can help segmentation. We are the\nfirst to explore the use of these video signals to tackle weakly supervised\ninstance segmentation. We propose two ways to leverage this information in our\nmodel. First, we adapt inter-pixel relation network (IRN) to effectively\nincorporate motion information during training. Second, we introduce a new\nMaskConsist module, which addresses the problem of missing object instances by\ntransferring stable predictions between neighboring frames during training. We\ndemonstrate that both approaches together improve the instance segmentation\nmetric $AP_{50}$ on video frames of two datasets: Youtube-VIS and Cityscapes by\n$5\\%$ and $3\\%$ respectively.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 23:20:46 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Qing", ""], ["Ramanathan", "Vignesh", ""], ["Mahajan", "Dhruv", ""], ["Yuille", "Alan", ""], ["Yang", "Zhenheng", ""]]}, {"id": "2103.12896", "submitter": "Nitthilan Kannappan Jayakodi", "authors": "Nitthilan Kannappan Jayakodi, Janardhan Rao Doppa, Partha Pratim Pande", "title": "SETGAN: Scale and Energy Trade-off GANs for Image Applications on Mobile\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the task of photo-realistic unconditional image generation\n(generate high quality, diverse samples that carry the same visual content as\nthe image) on mobile platforms using Generative Adversarial Networks (GANs). In\nthis paper, we propose a novel approach to trade-off image generation accuracy\nof a GAN for the energy consumed (compute) at run-time called Scale-Energy\nTradeoff GAN (SETGAN). GANs usually take a long time to train and consume a\nhuge memory hence making it difficult to run on edge devices. The key idea\nbehind SETGAN for an image generation task is for a given input image, we train\na GAN on a remote server and use the trained model on edge devices. We use\nSinGAN, a single image unconditional generative model, that contains a pyramid\nof fully convolutional GANs, each responsible for learning the patch\ndistribution at a different scale of the image. During the training process, we\ndetermine the optimal number of scales for a given input image and the energy\nconstraint from the target edge device. Results show that with SETGAN's unique\nclient-server-based architecture, we were able to achieve a 56% gain in energy\nfor a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale\ntraining, we obtain around 4x gain in training time on the server.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 23:51:22 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Jayakodi", "Nitthilan Kannappan", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""]]}, {"id": "2103.12902", "submitter": "Tete Xiao", "authors": "Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer, Trevor\n  Darrell", "title": "Region Similarity Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Region Similarity Representation Learning (ReSim), a new approach\nto self-supervised representation learning for localization-based tasks such as\nobject detection and segmentation. While existing work has largely focused on\nsolely learning global representations for an entire image, ReSim learns both\nregional representations for localization as well as semantic image-level\nrepresentations. ReSim operates by sliding a fixed-sized window across the\noverlapping area between two views (e.g., image crops), aligning these areas\nwith their corresponding convolutional feature map regions, and then maximizing\nthe feature similarity across views. As a result, ReSim learns spatially and\nsemantically consistent feature representation throughout the convolutional\nfeature maps of a neural network. A shift or scale of an image region, e.g., a\nshift or scale of an object, has a corresponding change in the feature maps;\nthis allows downstream tasks to leverage these representations for\nlocalization. Through object detection, instance segmentation, and dense pose\nestimation experiments, we illustrate how ReSim learns representations which\nsignificantly improve the localization and classification performance compared\nto a competitive MoCo-v2 baseline: $+2.7$ AP$^{\\text{bb}}_{75}$ VOC, $+1.1$\nAP$^{\\text{bb}}_{75}$ COCO, and $+1.9$ AP$^{\\text{mk}}$ Cityscapes. Code and\npre-trained models are released at: \\url{https://github.com/Tete-Xiao/ReSim}\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 00:42:37 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Xiao", "Tete", ""], ["Reed", "Colorado J", ""], ["Wang", "Xiaolong", ""], ["Keutzer", "Kurt", ""], ["Darrell", "Trevor", ""]]}, {"id": "2103.12924", "submitter": "Abu Md Niamul Taufique", "authors": "Abu Md Niamul Taufique, Breton Minnehan, Andreas Savakis", "title": "Benchmarking Deep Trackers on Aerial Videos", "comments": "25 pages, 10 figures, 7 tables", "journal-ref": "Sensors 2020, 20(2), 547", "doi": "10.3390/s20020547", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning-based visual object trackers have achieved\nstate-of-the-art performance on several visual object tracking benchmarks.\nHowever, most tracking benchmarks are focused on ground level videos, whereas\naerial tracking presents a new set of challenges. In this paper, we compare ten\ntrackers based on deep learning techniques on four aerial datasets. We choose\ntop performing trackers utilizing different approaches, specifically tracking\nby detection, discriminative correlation filters, Siamese networks and\nreinforcement learning. In our experiments, we use a subset of OTB2015 dataset\nwith aerial style videos; the UAV123 dataset without synthetic sequences; the\nUAV20L dataset, which contains 20 long sequences; and DTB70 dataset as our\nbenchmark datasets. We compare the advantages and disadvantages of different\ntrackers in different tracking situations encountered in aerial data. Our\nfindings indicate that the trackers perform significantly worse in aerial\ndatasets compared to standard ground level videos. We attribute this effect to\nsmaller target size, camera motion, significant camera rotation with respect to\nthe target, out of view movement, and clutter in the form of occlusions or\nsimilar looking distractors near tracked object.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 01:45:19 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Taufique", "Abu Md Niamul", ""], ["Minnehan", "Breton", ""], ["Savakis", "Andreas", ""]]}, {"id": "2103.12926", "submitter": "Wei Wei", "authors": "Wei Wei, Li Guan, Yue Liu, Hao Kang, Haoxiang Li, Ying Wu, Gang Hua", "title": "Beyond Visual Attractiveness: Physically Plausible Single Image HDR\n  Reconstruction for Spherical Panoramas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HDR reconstruction is an important task in computer vision with many\nindustrial needs. The traditional approaches merge multiple exposure shots to\ngenerate HDRs that correspond to the physical quantity of illuminance of the\nscene. However, the tedious capturing process makes such multi-shot approaches\ninconvenient in practice. In contrast, recent single-shot methods predict a\nvisually appealing HDR from a single LDR image through deep learning. But it is\nnot clear whether the previously mentioned physical properties would still\nhold, without training the network to explicitly model them. In this paper, we\nintroduce the physical illuminance constraints to our single-shot HDR\nreconstruction framework, with a focus on spherical panoramas. By the proposed\nphysical regularization, our method can generate HDRs which are not only\nvisually appealing but also physically plausible. For evaluation, we collect a\nlarge dataset of LDR and HDR images with ground truth illuminance measures.\nExtensive experiments show that our HDR images not only maintain high visual\nquality but also top all baseline methods in illuminance prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 01:51:19 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wei", "Wei", ""], ["Guan", "Li", ""], ["Liu", "Yue", ""], ["Kang", "Hao", ""], ["Li", "Haoxiang", ""], ["Wu", "Ying", ""], ["Hua", "Gang", ""]]}, {"id": "2103.12934", "submitter": "Haozhe Xie", "authors": "Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping Zhang, Wenxiu Sun", "title": "Efficient Regional Memory Network for Video Object Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several Space-Time Memory based networks have shown that the object\ncues (e.g. video frames as well as the segmented object masks) from the past\nframes are useful for segmenting objects in the current frame. However, these\nmethods exploit the information from the memory by global-to-global matching\nbetween the current and past frames, which lead to mismatching to similar\nobjects and high computational complexity. To address these problems, we\npropose a novel local-to-local matching solution for semi-supervised VOS,\nnamely Regional Memory Network (RMNet). In RMNet, the precise regional memory\nis constructed by memorizing local regions where the target objects appear in\nthe past frames. For the current query frame, the query regions are tracked and\npredicted based on the optical flow estimated from the previous frame. The\nproposed local-to-local matching effectively alleviates the ambiguity of\nsimilar objects in both memory and query frames, which allows the information\nto be passed from the regional memory to the query region efficiently and\neffectively. Experimental results indicate that the proposed RMNet performs\nfavorably against state-of-the-art methods on the DAVIS and YouTube-VOS\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 02:08:46 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 23:02:25 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Xie", "Haozhe", ""], ["Yao", "Hongxun", ""], ["Zhou", "Shangchen", ""], ["Zhang", "Shengping", ""], ["Sun", "Wenxiu", ""]]}, {"id": "2103.12944", "submitter": "Guanbin Li", "authors": "Xiangru Lin, Guanbin Li, Yizhou Yu", "title": "Scene-Intuitive Agent for Remote Embodied Visual Grounding", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans learn from life events to form intuitions towards the understanding of\nvisual environments and languages. Envision that you are instructed by a\nhigh-level instruction, \"Go to the bathroom in the master bedroom and replace\nthe blue towel on the left wall\", what would you possibly do to carry out the\ntask? Intuitively, we comprehend the semantics of the instruction to form an\noverview of where a bathroom is and what a blue towel is in mind; then, we\nnavigate to the target location by consistently matching the bathroom\nappearance in mind with the current scene. In this paper, we present an agent\nthat mimics such human behaviors. Specifically, we focus on the Remote Embodied\nVisual Referring Expression in Real Indoor Environments task, called REVERIE,\nwhere an agent is asked to correctly localize a remote target object specified\nby a concise high-level natural language instruction, and propose a two-stage\ntraining pipeline. In the first stage, we pretrain the agent with two\ncross-modal alignment sub-tasks, namely the Scene Grounding task and the Object\nGrounding task. The agent learns where to stop in the Scene Grounding task and\nwhat to attend to in the Object Grounding task respectively. Then, to generate\naction sequences, we propose a memory-augmented attentive action decoder to\nsmoothly fuse the pre-trained vision and language representations with the\nagent's past memory experiences. Without bells and whistles, experimental\nresults show that our method outperforms previous state-of-the-art(SOTA)\nsignificantly, demonstrating the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 02:37:48 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lin", "Xiangru", ""], ["Li", "Guanbin", ""], ["Yu", "Yizhou", ""]]}, {"id": "2103.12955", "submitter": "Baoli Sun", "authors": "Baoli Sun, Xinchen Ye, Baopu Li, Haojie Li, Zhihui Wang, Rui Xu", "title": "Learning Scene Structure Guidance via Cross-Task Knowledge Transfer for\n  Single Depth Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing color-guided depth super-resolution (DSR) approaches require paired\nRGB-D data as training samples where the RGB image is used as structural\nguidance to recover the degraded depth map due to their geometrical similarity.\nHowever, the paired data may be limited or expensive to be collected in actual\ntesting environment. Therefore, we explore for the first time to learn the\ncross-modality knowledge at training stage, where both RGB and depth modalities\nare available, but test on the target dataset, where only single depth modality\nexists. Our key idea is to distill the knowledge of scene structural guidance\nfrom RGB modality to the single DSR task without changing its network\narchitecture. Specifically, we construct an auxiliary depth estimation (DE)\ntask that takes an RGB image as input to estimate a depth map, and train both\nDSR task and DE task collaboratively to boost the performance of DSR. Upon\nthis, a cross-task interaction module is proposed to realize bilateral cross\ntask knowledge transfer. First, we design a cross-task distillation scheme that\nencourages DSR and DE networks to learn from each other in a teacher-student\nrole-exchanging fashion. Then, we advance a structure prediction (SP) task that\nprovides extra structure regularization to help both DSR and DE networks learn\nmore informative structure representations for depth recovery. Extensive\nexperiments demonstrate that our scheme achieves superior performance in\ncomparison with other DSR methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:08:25 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sun", "Baoli", ""], ["Ye", "Xinchen", ""], ["Li", "Baopu", ""], ["Li", "Haojie", ""], ["Wang", "Zhihui", ""], ["Xu", "Rui", ""]]}, {"id": "2103.12957", "submitter": "Xinrui Cui", "authors": "Dan Wang, Xinrui Cui, Xun Chen, Zhengxia Zou, Tianyang Shi, Septimiu\n  Salcudean, Z. Jane Wang, Rabab Ward", "title": "Multi-view 3D Reconstruction with Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep CNN-based methods have so far achieved the state of the art results in\nmulti-view 3D object reconstruction. Despite the considerable progress, the two\ncore modules of these methods - multi-view feature extraction and fusion, are\nusually investigated separately, and the object relations in different views\nare rarely explored. In this paper, inspired by the recent great success in\nself-attention-based Transformer models, we reformulate the multi-view 3D\nreconstruction as a sequence-to-sequence prediction problem and propose a new\nframework named 3D Volume Transformer (VolT) for such a task. Unlike previous\nCNN-based methods using a separate design, we unify the feature extraction and\nview fusion in a single Transformer network. A natural advantage of our design\nlies in the exploration of view-to-view relationships using self-attention\namong multiple unordered inputs. On ShapeNet - a large-scale 3D reconstruction\nbenchmark dataset, our method achieves a new state-of-the-art accuracy in\nmulti-view reconstruction with fewer parameters ($70\\%$ less) than other\nCNN-based methods. Experimental results also suggest the strong scaling\ncapability of our method. Our code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:14:49 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wang", "Dan", ""], ["Cui", "Xinrui", ""], ["Chen", "Xun", ""], ["Zou", "Zhengxia", ""], ["Shi", "Tianyang", ""], ["Salcudean", "Septimiu", ""], ["Wang", "Z. Jane", ""], ["Ward", "Rabab", ""]]}, {"id": "2103.12964", "submitter": "Jaesung Choe", "authors": "Jaesung Choe, Kyungdon Joo, Tooba Imtiaz, In So Kweon", "title": "Volumetric Propagation Network: Stereo-LiDAR Fusion for Long-Range Depth\n  Estimation", "comments": "This is a presentation paper for ICRA 2021. Accepted at RA-L 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo-LiDAR fusion is a promising task in that we can utilize two different\ntypes of 3D perceptions for practical usage -- dense 3D information (stereo\ncameras) and highly-accurate sparse point clouds (LiDAR). However, due to their\ndifferent modalities and structures, the method of aligning sensor data is the\nkey for successful sensor fusion. To this end, we propose a geometry-aware\nstereo-LiDAR fusion network for long-range depth estimation, called volumetric\npropagation network. The key idea of our network is to exploit sparse and\naccurate point clouds as a cue for guiding correspondences of stereo images in\na unified 3D volume space. Unlike existing fusion strategies, we directly embed\npoint clouds into the volume, which enables us to propagate valid information\ninto nearby voxels in the volume, and to reduce the uncertainty of\ncorrespondences. Thus, it allows us to fuse two different input modalities\nseamlessly and regress a long-range depth map. Our fusion is further enhanced\nby a newly proposed feature extraction layer for point clouds guided by images:\nFusionConv. FusionConv extracts point cloud features that consider both\nsemantic (2D image domain) and geometric (3D domain) relations and aid fusion\nat the volume. Our network achieves state-of-the-art performance on the KITTI\nand the Virtual-KITTI datasets among recent stereo-LiDAR fusion methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:24:46 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Choe", "Jaesung", ""], ["Joo", "Kyungdon", ""], ["Imtiaz", "Tooba", ""], ["Kweon", "In So", ""]]}, {"id": "2103.12972", "submitter": "Bolin Lai", "authors": "Bolin Lai, Yuhsuan Wu, Xiao-Yun Zhou, Peng Wang, Le Lu, Lingyun Huang,\n  Mei Han, Jing Xiao, Heping Hu, Adam P. Harrison", "title": "Hetero-Modal Learning and Expansive Consistency Constraints for\n  Semi-Supervised Detection from Multi-Sequence Data", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion detection serves a critical role in early diagnosis and has been well\nexplored in recent years due to methodological advancesand increased data\navailability. However, the high costs of annotations hinder the collection of\nlarge and completely labeled datasets, motivating semi-supervised detection\napproaches. In this paper, we introduce mean teacher hetero-modal detection\n(MTHD), which addresses two important gaps in current semi-supervised\ndetection. First, it is not obvious how to enforce unlabeled consistency\nconstraints across the very different outputs of various detectors, which has\nresulted in various compromises being used in the state of the art. Using an\nanchor-free framework, MTHD formulates a mean teacher approach without such\ncompromises, enforcing consistency on the soft-output of object centers and\nsize. Second, multi-sequence data is often critical, e.g., for abdominal lesion\ndetection, but unlabeled data is often missing sequences. To deal with this,\nMTHD incorporates hetero-modal learning in its framework. Unlike prior art,\nMTHD is able to incorporate an expansive set of consistency constraints that\ninclude geometric transforms and random sequence combinations. We train and\nevaluate MTHD on liver lesion detection using the largest MR lesion dataset to\ndate (1099 patients with >5000 volumes). MTHD surpasses the best\nfully-supervised and semi-supervised competitors by 10.1% and 3.5%,\nrespectively, in average sensitivity.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:52:06 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lai", "Bolin", ""], ["Wu", "Yuhsuan", ""], ["Zhou", "Xiao-Yun", ""], ["Wang", "Peng", ""], ["Lu", "Le", ""], ["Huang", "Lingyun", ""], ["Han", "Mei", ""], ["Xiao", "Jing", ""], ["Hu", "Heping", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2103.12975", "submitter": "Yining Hong", "authors": "Yining Hong, Qing Li, Song-Chun Zhu, Siyuan Huang", "title": "VLGrammar: Grounded Grammar Induction of Vision and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive grammar suggests that the acquisition of language grammar is\ngrounded within visual structures. While grammar is an essential representation\nof natural language, it also exists ubiquitously in vision to represent the\nhierarchical part-whole structure. In this work, we study grounded grammar\ninduction of vision and language in a joint learning framework. Specifically,\nwe present VLGrammar, a method that uses compound probabilistic context-free\ngrammars (compound PCFGs) to induce the language grammar and the image grammar\nsimultaneously. We propose a novel contrastive learning framework to guide the\njoint learning of both modules. To provide a benchmark for the grounded grammar\ninduction task, we collect a large-scale dataset, \\textsc{PartIt}, which\ncontains human-written sentences that describe part-level semantics for 3D\nobjects. Experiments on the \\textsc{PartIt} dataset show that VLGrammar\noutperforms all baselines in image grammar induction and language grammar\ninduction. The learned VLGrammar naturally benefits related downstream tasks.\nSpecifically, it improves the image unsupervised clustering accuracy by 30\\%,\nand performs well in image retrieval and text retrieval. Notably, the induced\ngrammar shows superior generalizability by easily generalizing to unseen\ncategories.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:05:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Hong", "Yining", ""], ["Li", "Qing", ""], ["Zhu", "Song-Chun", ""], ["Huang", "Siyuan", ""]]}, {"id": "2103.12978", "submitter": "Jianyun Xu", "authors": "Jianyun Xu, Ruixiang Zhang, Jian Dou, Yushi Zhu, Jie Sun, Shiliang Pu", "title": "RPVNet: A Deep and Efficient Range-Point-Voxel Fusion Network for LiDAR\n  Point Cloud Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds can be represented in many forms (views), typically, point-based\nsets, voxel-based cells or range-based images(i.e., panoramic view). The\npoint-based view is geometrically accurate, but it is disordered, which makes\nit difficult to find local neighbors efficiently. The voxel-based view is\nregular, but sparse, and computation grows cubically when voxel resolution\nincreases. The range-based view is regular and generally dense, however\nspherical projection makes physical dimensions distorted. Both voxel- and\nrange-based views suffer from quantization loss, especially for voxels when\nfacing large-scale scenes. In order to utilize different view's advantages and\nalleviate their own shortcomings in fine-grained segmentation task, we propose\na novel range-point-voxel fusion network, namely RPVNet. In this network, we\ndevise a deep fusion framework with multiple and mutual information\ninteractions among these three views and propose a gated fusion module (termed\nas GFM), which can adaptively merge the three features based on concurrent\ninputs. Moreover, the proposed RPV interaction mechanism is highly efficient,\nand we summarize it into a more general formulation. By leveraging this\nefficient interaction and relatively lower voxel resolution, our method is also\nproved to be more efficient. Finally, we evaluated the proposed model on two\nlarge-scale datasets, i.e., SemanticKITTI and nuScenes, and it shows\nstate-of-the-art performance on both of them. Note that, our method currently\nranks 1st on SemanticKITTI leaderboard without any extra tricks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:24:12 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Xu", "Jianyun", ""], ["Zhang", "Ruixiang", ""], ["Dou", "Jian", ""], ["Zhu", "Yushi", ""], ["Sun", "Jie", ""], ["Pu", "Shiliang", ""]]}, {"id": "2103.12980", "submitter": "Steven Damelin Dr", "authors": "Steven B. Damelin, David L. Ragozin and Michael Werman", "title": "On a realization of motion and similarity group equivalence classes of\n  labeled points in $\\mathbb R^k$ with applications to computer vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.GR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a realization of motion and similarity group equivalence classes of\n$n\\geq 1$ labeled points in $\\mathbb R^k,\\, k\\geq 1$ as a metric space with a\ncomputable metric. Our study is motivated by applications in computer vision.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:25:12 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Damelin", "Steven B.", ""], ["Ragozin", "David L.", ""], ["Werman", "Michael", ""]]}, {"id": "2103.12981", "submitter": "Brevin Tilmon", "authors": "Brevin Tilmon and Sanjeev J. Koppal", "title": "SaccadeCam: Adaptive Visual Attention for Monocular Depth Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most monocular depth sensing methods use conventionally captured images that\nare created without considering scene content. In contrast, animal eyes have\nfast mechanical motions, called saccades, that control how the scene is imaged\nby the fovea, where resolution is highest. In this paper, we present the\nSaccadeCam framework for adaptively distributing resolution onto regions of\ninterest in the scene. Our algorithm for adaptive resolution is a\nself-supervised network and we demonstrate results for end-to-end learning for\nmonocular depth estimation. We also show preliminary results with a real\nSaccadeCam hardware prototype.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:36:18 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 03:44:54 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Tilmon", "Brevin", ""], ["Koppal", "Sanjeev J.", ""]]}, {"id": "2103.12988", "submitter": "Zixu Zhao", "authors": "Zixu Zhao, Yueming Jin, Bo Lu, Chi-Fai Ng, Qi Dou, Yun-Hui Liu, and\n  Pheng-Ann Heng", "title": "One to Many: Adaptive Instrument Segmentation via Meta Learning and\n  Dynamic Online Adaptation in Robotic Surgical Video", "comments": "Accepted by ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical instrument segmentation in robot-assisted surgery (RAS) - especially\nthat using learning-based models - relies on the assumption that training and\ntesting videos are sampled from the same domain. However, it is impractical and\nexpensive to collect and annotate sufficient data from every new domain. To\ngreatly increase the label efficiency, we explore a new problem, i.e., adaptive\ninstrument segmentation, which is to effectively adapt one source model to new\nrobotic surgical videos from multiple target domains, only given the annotated\ninstruments in the first frame. We propose MDAL, a meta-learning based dynamic\nonline adaptive learning scheme with a two-stage framework to fast adapt the\nmodel parameters on the first frame and partial subsequent frames while\npredicting the results. MDAL learns the general knowledge of instruments and\nthe fast adaptation ability through the video-specific meta-learning paradigm.\nThe added gradient gate excludes the noisy supervision from pseudo masks for\ndynamic online adaptation on target videos. We demonstrate empirically that\nMDAL outperforms other state-of-the-art methods on two datasets (including a\nreal-world RAS dataset). The promising performance on ex-vivo scenes also\nbenefits the downstream tasks such as robot-assisted suturing and camera\ncontrol.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 05:02:18 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhao", "Zixu", ""], ["Jin", "Yueming", ""], ["Lu", "Bo", ""], ["Ng", "Chi-Fai", ""], ["Dou", "Qi", ""], ["Liu", "Yun-Hui", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2103.12989", "submitter": "Yongfei Liu", "authors": "Yongfei Liu, Bo Wan, Lin Ma, Xuming He", "title": "Relation-aware Instance Refinement for Weakly Supervised Visual\n  Grounding", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual grounding, which aims to build a correspondence between visual objects\nand their language entities, plays a key role in cross-modal scene\nunderstanding. One promising and scalable strategy for learning visual\ngrounding is to utilize weak supervision from only image-caption pairs.\nPrevious methods typically rely on matching query phrases directly to a\nprecomputed, fixed object candidate pool, which leads to inaccurate\nlocalization and ambiguous matching due to lack of semantic relation\nconstraints.\n  In our paper, we propose a novel context-aware weakly-supervised learning\nmethod that incorporates coarse-to-fine object refinement and entity relation\nmodeling into a two-stage deep network, capable of producing more accurate\nobject representation and matching. To effectively train our network, we\nintroduce a self-taught regression loss for the proposal locations and a\nclassification loss based on parsed entity relations.\n  Extensive experiments on two public benchmarks Flickr30K Entities and\nReferItGame demonstrate the efficacy of our weakly grounding framework. The\nresults show that we outperform the previous methods by a considerable margin,\nachieving 59.27\\% top-1 accuracy in Flickr30K Entities and 37.68\\% in the\nReferItGame dataset respectively (Code is available at\nhttps://github.com/youngfly11/ReIR-WeaklyGrounding.pytorch.git).\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 05:03:54 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Yongfei", ""], ["Wan", "Bo", ""], ["Ma", "Lin", ""], ["He", "Xuming", ""]]}, {"id": "2103.12991", "submitter": "Jiaxing Huang", "authors": "Jiaxing Huang, Dayan Guan, Shijian Lu, Aoran Xiao", "title": "MLAN: Multi-Level Adversarial Network for Domain Adaptive Semantic\n  Segmentation", "comments": "Submitted to PR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progresses in domain adaptive semantic segmentation demonstrate the\neffectiveness of adversarial learning (AL) in unsupervised domain adaptation.\nHowever, most adversarial learning based methods align source and target\ndistributions at a global image level but neglect the inconsistency around\nlocal image regions. This paper presents a novel multi-level adversarial\nnetwork (MLAN) that aims to address inter-domain inconsistency at both global\nimage level and local region level optimally. MLAN has two novel designs,\nnamely, region-level adversarial learning (RL-AL) and co-regularized\nadversarial learning (CR-AL). Specifically, RL-AL models prototypical regional\ncontext-relations explicitly in the feature space of a labelled source domain\nand transfers them to an unlabelled target domain via adversarial learning.\nCR-AL fuses region-level AL and image-level AL optimally via mutual\nregularization. In addition, we design a multi-level consistency map that can\nguide domain adaptation in both input space ($i.e.$, image-to-image\ntranslation) and output space ($i.e.$, self-training) effectively. Extensive\nexperiments show that MLAN outperforms the state-of-the-art with a large margin\nconsistently across multiple datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 05:13:23 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Huang", "Jiaxing", ""], ["Guan", "Dayan", ""], ["Lu", "Shijian", ""], ["Xiao", "Aoran", ""]]}, {"id": "2103.12992", "submitter": "YeongHyeon Park", "authors": "YeongHyeon Park and JongHee Jung", "title": "Non-Compression Auto-Encoder for Detecting Road Surface Abnormality via\n  Vehicle Driving Noise", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road accident can be triggered by wet road because it decreases skid\nresistance. To prevent the road accident, detecting road surface abnomality is\nhighly useful. In this paper, we propose the deep learning based cost-effective\nreal-time anomaly detection architecture, naming with non-compression\nauto-encoder (NCAE). The proposed architecture can reflect forward and backward\ncausality of time series information via convolutional operation. Moreover, the\nabove architecture shows higher anomaly detection performance of published\nanomaly detection model via experiments. We conclude that NCAE as a\ncutting-edge model for road surface anomaly detection with 4.20\\% higher AUROC\nand 2.99 times faster decision than before.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 05:13:50 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 03:31:47 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 01:25:39 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Park", "YeongHyeon", ""], ["Jung", "JongHee", ""]]}, {"id": "2103.12995", "submitter": "Yi Zhang", "authors": "Zexin Lu, Wenjun Xia, Yongqiang Huang, Hongming Shan, Hu Chen, Jiliu\n  Zhou, Yi Zhang", "title": "MANAS: Multi-Scale and Multi-Level Neural Architecture Search for\n  Low-Dose CT Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lowering the radiation dose in computed tomography (CT) can greatly reduce\nthe potential risk to public health. However, the reconstructed images from the\ndose-reduced CT or low-dose CT (LDCT) suffer from severe noise, compromising\nthe subsequent diagnosis and analysis. Recently, convolutional neural networks\nhave achieved promising results in removing noise from LDCT images; the network\narchitectures used are either handcrafted or built on top of conventional\nnetworks such as ResNet and U-Net. Recent advance on neural network\narchitecture search (NAS) has proved that the network architecture has a\ndramatic effect on the model performance, which indicates that current network\narchitectures for LDCT may be sub-optimal. Therefore, in this paper, we make\nthe first attempt to apply NAS to LDCT and propose a multi-scale and\nmulti-level NAS for LDCT denoising, termed MANAS. On the one hand, the proposed\nMANAS fuses features extracted by different scale cells to capture multi-scale\nimage structural details. On the other hand, the proposed MANAS can search a\nhybrid cell- and network-level structure for better performance. Extensively\nexperimental results on three different dose levels demonstrate that the\nproposed MANAS can achieve better performance in terms of preserving image\nstructural details than several state-of-the-art methods. In addition, we also\nvalidate the effectiveness of the multi-scale and multi-level architecture for\nLDCT denoising.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 05:41:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lu", "Zexin", ""], ["Xia", "Wenjun", ""], ["Huang", "Yongqiang", ""], ["Shan", "Hongming", ""], ["Chen", "Hu", ""], ["Zhou", "Jiliu", ""], ["Zhang", "Yi", ""]]}, {"id": "2103.12997", "submitter": "Zhihao Liu", "authors": "Zhihao Liu, Hui Yin, Xinyi Wu, Zhenyao Wu, Yang Mi, Song Wang", "title": "From Shadow Generation to Shadow Removal", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow removal is a computer-vision task that aims to restore the image\ncontent in shadow regions. While almost all recent shadow-removal methods\nrequire shadow-free images for training, in ECCV 2020 Le and Samaras introduces\nan innovative approach without this requirement by cropping patches with and\nwithout shadows from shadow images as training samples. However, it is still\nlaborious and time-consuming to construct a large amount of such unpaired\npatches. In this paper, we propose a new G2R-ShadowNet which leverages shadow\ngeneration for weakly-supervised shadow removal by only using a set of shadow\nimages and their corresponding shadow masks for training. The proposed\nG2R-ShadowNet consists of three sub-networks for shadow generation, shadow\nremoval and refinement, respectively and they are jointly trained in an\nend-to-end fashion. In particular, the shadow generation sub-net stylises\nnon-shadow regions to be shadow ones, leading to paired data for training the\nshadow-removal sub-net. Extensive experiments on the ISTD dataset and the Video\nShadow Removal dataset show that the proposed G2R-ShadowNet achieves\ncompetitive performances against the current state of the arts and outperforms\nLe and Samaras' patch-based shadow-removal method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 05:49:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Zhihao", ""], ["Yin", "Hui", ""], ["Wu", "Xinyi", ""], ["Wu", "Zhenyao", ""], ["Mi", "Yang", ""], ["Wang", "Song", ""]]}, {"id": "2103.13001", "submitter": "Liang Xie", "authors": "Liang Xie, Guodong Xu, Deng Cai, Xiaofei He", "title": "X-view: Non-egocentric Multi-View 3D Object Detector", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection algorithms for autonomous driving reason about 3D\nobstacles either from 3D birds-eye view or perspective view or both. Recent\nworks attempt to improve the detection performance via mining and fusing from\nmultiple egocentric views. Although the egocentric perspective view alleviates\nsome weaknesses of the birds-eye view, the sectored grid partition becomes so\ncoarse in the distance that the targets and surrounding context mix together,\nwhich makes the features less discriminative. In this paper, we generalize the\nresearch on 3D multi-view learning and propose a novel multi-view-based 3D\ndetection method, named X-view, to overcome the drawbacks of the multi-view\nmethods. Specifically, X-view breaks through the traditional limitation about\nthe perspective view whose original point must be consistent with the 3D\nCartesian coordinate. X-view is designed as a general paradigm that can be\napplied on almost any 3D detectors based on LiDAR with only little increment of\nrunning time, no matter it is voxel/grid-based or raw-point-based. We conduct\nexperiments on KITTI and NuScenes datasets to demonstrate the robustness and\neffectiveness of our proposed X-view. The results show that X-view obtains\nconsistent improvements when combined with four mainstream state-of-the-art 3D\nmethods: SECOND, PointRCNN, Part-A^2, and PV-RCNN.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 06:13:35 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Xie", "Liang", ""], ["Xu", "Guodong", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "2103.13003", "submitter": "Tobias Schlagenhauf", "authors": "Tobias Schlagenhauf, Magnus Landwehr, Juergen Fleischer", "title": "Industrial Machine Tool Component Surface Defect Dataset", "comments": "7 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Using machine learning (ML) techniques in general and deep learning\ntechniques in specific needs a certain amount of data often not available in\nlarge quantities in technical domains. The manual inspection of machine tool\ncomponents and the manual end-of-line check of products are labor-intensive\ntasks in industrial applications that companies often want to automate. To\nautomate classification processes and develop reliable and robust machine\nlearning-based classification and wear prognostics models, one needs real-world\ndatasets to train and test the models. The dataset is available under\nhttps://doi.org/10.5445/IR/1000129520.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 06:17:21 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Schlagenhauf", "Tobias", ""], ["Landwehr", "Magnus", ""], ["Fleischer", "Juergen", ""]]}, {"id": "2103.13013", "submitter": "Chuan-Shen Hu", "authors": "Yu-Min Chung, Sarah Day, Chuan-Shen Hu", "title": "A Multi-parameter Persistence Framework for Mathematical Morphology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV math.AT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The field of mathematical morphology offers well-studied techniques for image\nprocessing. In this work, we view morphological operations through the lens of\npersistent homology, a tool at the heart of the field of topological data\nanalysis. We demonstrate that morphological operations naturally form a\nmultiparameter filtration and that persistent homology can then be used to\nextract information about both topology and geometry in the images as well as\nto automate methods for optimizing the study and rendering of structure in\nimages. For illustration, we apply this framework to analyze noisy binary,\ngrayscale, and color images.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 06:46:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chung", "Yu-Min", ""], ["Day", "Sarah", ""], ["Hu", "Chuan-Shen", ""]]}, {"id": "2103.13021", "submitter": "Sourangshu Bhattacharya", "authors": "Soumi Das, Harikrishna Patibandla, Suparna Bhattacharya, Kshounis\n  Bera, Niloy Ganguly, Sourangshu Bhattacharya", "title": "Convex Online Video Frame Subset Selection using Multiple Criteria for\n  Data Efficient Autonomous Driving", "comments": "Submitted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training vision-based Urban Autonomous driving models is a challenging\nproblem, which is highly researched in recent times. Training such models is a\ndata-intensive task requiring the storage and processing of vast volumes of\n(possibly redundant) driving video data. In this paper, we study the problem of\ndeveloping data-efficient autonomous driving systems. In this context, we study\nthe problem of multi-criteria online video frame subset selection. We study\nconvex optimization-based solutions and show that they are unable to provide\nsolutions with high weightage to the loss of selected video frames. We design a\nnovel convex optimization-based multi-criteria online subset selection\nalgorithm that uses a thresholded concave function of selection variables. We\nalso propose and study a submodular optimization-based algorithm. Extensive\nexperiments using the driving simulator CARLA show that we are able to drop 80%\nof the frames while succeeding to complete 100% of the episodes w.r.t. the\nmodel trained on 100% data, in the most difficult task of taking turns. This\nresults in a training time of less than 30% compared to training on the whole\ndataset. We also perform detailed experiments on prediction performances of\nvarious affordances used by the Conditional Affordance Learning (CAL) model and\nshow that our subset selection improves performance on the crucial affordance\n\"Relative Angle\" during turns.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:02:41 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Das", "Soumi", ""], ["Patibandla", "Harikrishna", ""], ["Bhattacharya", "Suparna", ""], ["Bera", "Kshounis", ""], ["Ganguly", "Niloy", ""], ["Bhattacharya", "Sourangshu", ""]]}, {"id": "2103.13023", "submitter": "Kodai Nakashima", "authors": "Kodai Nakashima, Hirokatsu Kataoka, Asato Matsumoto, Kenji Iwata and\n  Nakamasa Inoue", "title": "Can Vision Transformers Learn without Natural Images?", "comments": "Project page:\n  https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we complete pre-training of Vision Transformers (ViT) without natural\nimages and human-annotated labels? Although a pre-trained ViT seems to heavily\nrely on a large-scale dataset and human-annotated labels, recent large-scale\ndatasets contain several problems in terms of privacy violations, inadequate\nfairness protection, and labor-intensive annotation. In the present paper, we\npre-train ViT without any image collections and annotation labor. We\nexperimentally verify that our proposed framework partially outperforms\nsophisticated Self-Supervised Learning (SSL) methods like SimCLRv2 and MoCov2\nwithout using any natural images in the pre-training phase. Moreover, although\nthe ViT pre-trained without natural images produces some different\nvisualizations from ImageNet pre-trained ViT, it can interpret natural image\ndatasets to a large extent. For example, the performance rates on the CIFAR-10\ndataset are as follows: our proposal 97.6 vs. SimCLRv2 97.4 vs. ImageNet 98.0.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:09:21 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Nakashima", "Kodai", ""], ["Kataoka", "Hirokatsu", ""], ["Matsumoto", "Asato", ""], ["Iwata", "Kenji", ""], ["Inoue", "Nakamasa", ""]]}, {"id": "2103.13027", "submitter": "Siyuan Li", "authors": "Zicheng Liu, Siyuan Li, Di Wu, Zhiyuan Chen, Lirong Wu, Jianzhu Guo,\n  Stan Z. Li", "title": "AutoMix: Unveiling the Power of Mixup", "comments": "The first version of AutoMix. 13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixup-based data augmentation has achieved great success as regularizer for\ndeep neural networks. However, existing mixup methods require explicitly\ndesigned mixup policies. In this paper, we present a flexible, general\nAutomatic Mixup (AutoMix) framework which utilizes discriminative features to\nlearn a sample mixing policy adaptively. We regard mixup as a pretext task and\nsplit it into two sub-problems: mixed samples generation and mixup\nclassification. To this end, we design a lightweight mix block to generate\nsynthetic samples based on feature maps and mix labels. Since the two\nsub-problems are in the nature of Expectation-Maximization (EM), we also\npropose a momentum training pipeline to optimize the mixup process and mixup\nclassification process alternatively in an end-to-end fashion. Extensive\nexperiments on six popular classification benchmarks show that AutoMix\nconsistently outperforms other leading mixup methods and improves\ngeneralization abilities to downstream tasks. We hope AutoMix will motivate the\ncommunity to rethink the role of mixup in representation learning. The code\nwill be released soon.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:21:53 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Zicheng", ""], ["Li", "Siyuan", ""], ["Wu", "Di", ""], ["Chen", "Zhiyuan", ""], ["Wu", "Lirong", ""], ["Guo", "Jianzhu", ""], ["Li", "Stan Z.", ""]]}, {"id": "2103.13028", "submitter": "Guangwei Gao", "authors": "Zhengxue Wang, Guangwei Gao, Juncheng Li, Yi Yu, Huimin Lu", "title": "Lightweight Image Super-Resolution with Multi-scale Feature Interaction\n  Network", "comments": "ICME2021, https://ieeexplore.ieee.org/abstract/document/9428136", "journal-ref": null, "doi": "10.1109/ICME51207.2021.9428136", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the single image super-resolution (SISR) approaches with deep and\ncomplex convolutional neural network structures have achieved promising\nperformance. However, those methods improve the performance at the cost of\nhigher memory consumption, which is difficult to be applied for some mobile\ndevices with limited storage and computing resources. To solve this problem, we\npresent a lightweight multi-scale feature interaction network (MSFIN). For\nlightweight SISR, MSFIN expands the receptive field and adequately exploits the\ninformative features of the low-resolution observed images from various scales\nand interactive connections. In addition, we design a lightweight recurrent\nresidual channel attention block (RRCAB) so that the network can benefit from\nthe channel attention mechanism while being sufficiently lightweight. Extensive\nexperiments on some benchmarks have confirmed that our proposed MSFIN can\nachieve comparable performance against the state-of-the-arts with a more\nlightweight model.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:25:21 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 03:04:34 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Zhengxue", ""], ["Gao", "Guangwei", ""], ["Li", "Juncheng", ""], ["Yu", "Yi", ""], ["Lu", "Huimin", ""]]}, {"id": "2103.13029", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Zeren Sun, Chuanyi Zhang, Fumin Shen, Qi Wu, Jian Zhang,\n  and Zhenmin Tang", "title": "Jo-SRC: A Contrastive Approach for Combating Noisy Labels", "comments": "accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the memorization effect in Deep Neural Networks (DNNs), training with\nnoisy labels usually results in inferior model performance. Existing\nstate-of-the-art methods primarily adopt a sample selection strategy, which\nselects small-loss samples for subsequent training. However, prior literature\ntends to perform sample selection within each mini-batch, neglecting the\nimbalance of noise ratios in different mini-batches. Moreover, valuable\nknowledge within high-loss samples is wasted. To this end, we propose a\nnoise-robust approach named Jo-SRC (Joint Sample Selection and Model\nRegularization based on Consistency). Specifically, we train the network in a\ncontrastive learning manner. Predictions from two different views of each\nsample are used to estimate its \"likelihood\" of being clean or\nout-of-distribution. Furthermore, we propose a joint loss to advance the model\ngeneralization performance by introducing consistency regularization. Extensive\nexperiments have validated the superiority of our approach over existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:26:07 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Yao", "Yazhou", ""], ["Sun", "Zeren", ""], ["Zhang", "Chuanyi", ""], ["Shen", "Fumin", ""], ["Wu", "Qi", ""], ["Zhang", "Jian", ""], ["Tang", "Zhenmin", ""]]}, {"id": "2103.13030", "submitter": "Xiaogang Wang", "authors": "Xiaogang Wang, Xun Sun, Xinyu Cao, Kai Xu, Bin Zhou", "title": "Learning Fine-Grained Segmentation of 3D Shapes without Part Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based 3D shape segmentation is usually formulated as a semantic\nlabeling problem, assuming that all parts of training shapes are annotated with\na given set of tags. This assumption, however, is impractical for learning\nfine-grained segmentation. Although most off-the-shelf CAD models are, by\nconstruction, composed of fine-grained parts, they usually miss semantic tags\nand labeling those fine-grained parts is extremely tedious. We approach the\nproblem with deep clustering, where the key idea is to learn part priors from a\nshape dataset with fine-grained segmentation but no part labels. Given point\nsampled 3D shapes, we model the clustering priors of points with a similarity\nmatrix and achieve part segmentation through minimizing a novel low rank loss.\nTo handle highly densely sampled point sets, we adopt a divide-and-conquer\nstrategy. We partition the large point set into a number of blocks. Each block\nis segmented using a deep-clustering-based part prior network trained in a\ncategory-agnostic manner. We then train a graph convolution network to merge\nthe segments of all blocks to form the final segmentation result. Our method is\nevaluated with a challenging benchmark of fine-grained segmentation, showing\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:27:07 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wang", "Xiaogang", ""], ["Sun", "Xun", ""], ["Cao", "Xinyu", ""], ["Xu", "Kai", ""], ["Zhou", "Bin", ""]]}, {"id": "2103.13041", "submitter": "Haoyu Ma", "authors": "Haoyu Ma, Xiangru Lin, Zifeng Wu, Yizhou Yu", "title": "Coarse-to-Fine Domain Adaptive Semantic Segmentation with Photometric\n  Alignment and Category-Center Regularization", "comments": "Accepted to appear in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) in semantic segmentation is a\nfundamental yet promising task relieving the need for laborious annotation\nworks. However, the domain shifts/discrepancies problem in this task compromise\nthe final segmentation performance. Based on our observation, the main causes\nof the domain shifts are differences in imaging conditions, called image-level\ndomain shifts, and differences in object category configurations called\ncategory-level domain shifts. In this paper, we propose a novel UDA pipeline\nthat unifies image-level alignment and category-level feature distribution\nregularization in a coarse-to-fine manner. Specifically, on the coarse side, we\npropose a photometric alignment module that aligns an image in the source\ndomain with a reference image from the target domain using a set of image-level\noperators; on the fine side, we propose a category-oriented triplet loss that\nimposes a soft constraint to regularize category centers in the source domain\nand a self-supervised consistency regularization method in the target domain.\nExperimental results show that our proposed pipeline improves the\ngeneralization capability of the final segmentation model and significantly\noutperforms all previous state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 08:04:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ma", "Haoyu", ""], ["Lin", "Xiangru", ""], ["Wu", "Zifeng", ""], ["Yu", "Yizhou", ""]]}, {"id": "2103.13043", "submitter": "Gaochang Wu", "authors": "Gaochang Wu, Yebin Liu, Lu Fang, Qionghai Dai, Tianyou Chai", "title": "Light Field Reconstruction Using Convolutional Network on EPI and\n  Extended Applications", "comments": "Published in IEEE TPAMI, 2019", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019", "doi": "10.1109/TPAMI.2018.2845393", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel convolutional neural network (CNN)-based framework is\ndeveloped for light field reconstruction from a sparse set of views. We\nindicate that the reconstruction can be efficiently modeled as angular\nrestoration on an epipolar plane image (EPI). The main problem in direct\nreconstruction on the EPI involves an information asymmetry between the spatial\nand angular dimensions, where the detailed portion in the angular dimensions is\ndamaged by undersampling. Directly upsampling or super-resolving the light\nfield in the angular dimensions causes ghosting effects. To suppress these\nghosting effects, we contribute a novel \"blur-restoration-deblur\" framework.\nFirst, the \"blur\" step is applied to extract the low-frequency components of\nthe light field in the spatial dimensions by convolving each EPI slice with a\nselected blur kernel. Then, the \"restoration\" step is implemented by a CNN,\nwhich is trained to restore the angular details of the EPI. Finally, we use a\nnon-blind \"deblur\" operation to recover the spatial high frequencies suppressed\nby the EPI blur. We evaluate our approach on several datasets, including\nsynthetic scenes, real-world scenes and challenging microscope light field\ndata. We demonstrate the high performance and robustness of the proposed\nframework compared with state-of-the-art algorithms. We further show extended\napplications, including depth enhancement and interpolation for unstructured\ninput. More importantly, a novel rendering approach is presented by combining\nthe proposed framework and depth information to handle large disparities.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 08:16:32 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wu", "Gaochang", ""], ["Liu", "Yebin", ""], ["Fang", "Lu", ""], ["Dai", "Qionghai", ""], ["Chai", "Tianyou", ""]]}, {"id": "2103.13044", "submitter": "Guangwei Gao", "authors": "Guangwei Gao, Guoan Xu, Yi Yu, Jin Xie, Jian Yang, Dong Yue", "title": "MSCFNet: A Lightweight Network With Multi-Scale Context Fusion for\n  Real-Time Semantic Segmentation", "comments": "IEEE Transactions on Intelligent Transportation Systems, 11 pages, 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, how to strike a good trade-off between accuracy and\ninference speed has become the core issue for real-time semantic segmentation\napplications, which plays a vital role in real-world scenarios such as\nautonomous driving systems and drones. In this study, we devise a novel\nlightweight network using a multi-scale context fusion (MSCFNet) scheme, which\nexplores an asymmetric encoder-decoder architecture to dispose this problem.\nMore specifically, the encoder adopts some developed efficient asymmetric\nresidual (EAR) modules, which are composed of factorization depth-wise\nconvolution and dilation convolution. Meanwhile, instead of complicated\ncomputation, simple deconvolution is applied in the decoder to further reduce\nthe amount of parameters while still maintaining high segmentation accuracy.\nAlso, MSCFNet has branches with efficient attention modules from different\nstages of the network to well capture multi-scale contextual information. Then\nwe combine them before the final classification to enhance the expression of\nthe features and improve the segmentation efficiency. Comprehensive experiments\non challenging datasets have demonstrated that the proposed MSCFNet, which\ncontains only 1.15M parameters, achieves 71.9\\% Mean IoU on the Cityscapes\ntesting dataset and can run at over 50 FPS on a single Titan XP GPU\nconfiguration.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 08:28:26 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 09:10:51 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Gao", "Guangwei", ""], ["Xu", "Guoan", ""], ["Yu", "Yi", ""], ["Xie", "Jin", ""], ["Yang", "Jian", ""], ["Yue", "Dong", ""]]}, {"id": "2103.13061", "submitter": "Amaia Salvador", "authors": "Amaia Salvador, Erhan Gundogdu, Loris Bazzani, Michael Donoser", "title": "Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers\n  and Self-supervised Learning", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal recipe retrieval has recently gained substantial attention due to\nthe importance of food in people's lives, as well as the availability of vast\namounts of digital cooking recipes and food images to train machine learning\nmodels. In this work, we revisit existing approaches for cross-modal recipe\nretrieval and propose a simplified end-to-end model based on well established\nand high performing encoders for text and images. We introduce a hierarchical\nrecipe Transformer which attentively encodes individual recipe components\n(titles, ingredients and instructions). Further, we propose a self-supervised\nloss function computed on top of pairs of individual recipe components, which\nis able to leverage semantic relationships within recipes, and enables training\nusing both image-recipe and recipe-only samples. We conduct a thorough analysis\nand ablation studies to validate our design choices. As a result, our proposed\nmethod achieves state-of-the-art performance in the cross-modal recipe\nretrieval task on the Recipe1M dataset. We make code and models publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:17:09 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Salvador", "Amaia", ""], ["Gundogdu", "Erhan", ""], ["Bazzani", "Loris", ""], ["Donoser", "Michael", ""]]}, {"id": "2103.13080", "submitter": "Luo Chunjie", "authors": "Chunjie Luo, Jianfeng Zhan, Tianshu Hao, Lei Wang, Wanling Gao", "title": "Shift-and-Balance Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention is an effective mechanism to improve the deep model capability.\nSqueeze-and-Excite (SE) introduces a light-weight attention branch to enhance\nthe network's representational power. The attention branch is gated using the\nSigmoid function and multiplied by the feature map's trunk branch. It is too\nsensitive to coordinate and balance the trunk and attention branches'\ncontributions. To control the attention branch's influence, we propose a new\nattention method, called Shift-and-Balance (SB). Different from\nSqueeze-and-Excite, the attention branch is regulated by the learned control\nfactor to control the balance, then added into the feature map's trunk branch.\nExperiments show that Shift-and-Balance attention significantly improves the\naccuracy compared to Squeeze-and-Excite when applied in more layers, increasing\nmore size and capacity of a network. Moreover, Shift-and-Balance attention\nachieves better or close accuracy compared to the state-of-art Dynamic\nConvolution.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:54:25 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Luo", "Chunjie", ""], ["Zhan", "Jianfeng", ""], ["Hao", "Tianshu", ""], ["Wang", "Lei", ""], ["Gao", "Wanling", ""]]}, {"id": "2103.13090", "submitter": "Jianhao Jiao", "authors": "Jianhao Jiao and Yilong Zhu and Haoyang Ye and Huaiyang Huang and Peng\n  Yun and Linxin Jiang and Lujia Wang and Ming Liu", "title": "Greedy-Based Feature Selection for Efficient LiDAR SLAM", "comments": "7 pages, 6 figures, accepted at 2021 International Conference on\n  Robotics and Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern LiDAR-SLAM (L-SLAM) systems have shown excellent results in\nlarge-scale, real-world scenarios. However, they commonly have a high latency\ndue to the expensive data association and nonlinear optimization. This paper\ndemonstrates that actively selecting a subset of features significantly\nimproves both the accuracy and efficiency of an L-SLAM system. We formulate the\nfeature selection as a combinatorial optimization problem under a cardinality\nconstraint to preserve the information matrix's spectral attributes. The\nstochastic-greedy algorithm is applied to approximate the optimal results in\nreal-time. To avoid ill-conditioned estimation, we also propose a general\nstrategy to evaluate the environment's degeneracy and modify the feature number\nonline. The proposed feature selector is integrated into a multi-LiDAR SLAM\nsystem. We validate this enhanced system with extensive experiments covering\nvarious scenarios on two sensor setups and computation platforms. We show that\nour approach exhibits low localization error and speedup compared to the\nstate-of-the-art L-SLAM systems. To benefit the community, we have released the\nsource code: https://ram-lab.com/file/site/m-loam.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 11:03:16 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Jiao", "Jianhao", ""], ["Zhu", "Yilong", ""], ["Ye", "Haoyang", ""], ["Huang", "Huaiyang", ""], ["Yun", "Peng", ""], ["Jiang", "Linxin", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""]]}, {"id": "2103.13096", "submitter": "Zhang Yunhua", "authors": "Yunhua Zhang, Ling Shao, Cees G.M. Snoek", "title": "Repetitive Activity Counting by Sight and Sound", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper strives for repetitive activity counting in videos. Different from\nexisting works, which all analyze the visual video content only, we incorporate\nfor the first time the corresponding sound into the repetition counting\nprocess. This benefits accuracy in challenging vision conditions such as\nocclusion, dramatic camera view changes, low resolution, etc. We propose a\nmodel that starts with analyzing the sight and sound streams separately. Then\nan audiovisual temporal stride decision module and a reliability estimation\nmodule are introduced to exploit cross-modal temporal interaction. For learning\nand evaluation, an existing dataset is repurposed and reorganized to allow for\nrepetition counting with sight and sound. We also introduce a variant of this\ndataset for repetition counting under challenging vision conditions.\nExperiments demonstrate the benefit of sound, as well as the other introduced\nmodules, for repetition counting. Our sight-only model already outperforms the\nstate-of-the-art by itself, when we add sound, results improve notably,\nespecially under harsh vision conditions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 11:15:33 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 18:43:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhang", "Yunhua", ""], ["Shao", "Ling", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2103.13107", "submitter": "Francesco Ponzio", "authors": "Francesco Ponzio, Enrico Macii, Elisa Ficarra, Santa Di Cataldo", "title": "W2WNet: a two-module probabilistic Convolutional Neural Network with\n  embedded data cleansing functionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) are supposed to be fed with only\nhigh-quality annotated datasets. Nonetheless, in many real-world scenarios,\nsuch high quality is very hard to obtain, and datasets may be affected by any\nsort of image degradation and mislabelling issues. This negatively impacts the\nperformance of standard CNNs, both during the training and the inference phase.\nTo address this issue we propose Wise2WipedNet (W2WNet), a new two-module\nConvolutional Neural Network, where a Wise module exploits Bayesian inference\nto identify and discard spurious images during the training, and a Wiped module\ntakes care of the final classification while broadcasting information on the\nprediction confidence at inference time. The goodness of our solution is\ndemonstrated on a number of public benchmarks addressing different image\nclassification tasks, as well as on a real-world case study on histological\nimage analysis. Overall, our experiments demonstrate that W2WNet is able to\nidentify image degradation and mislabelling issues both at training and at\ninference time, with a positive impact on the final classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 11:28:59 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ponzio", "Francesco", ""], ["Macii", "Enrico", ""], ["Ficarra", "Elisa", ""], ["Di Cataldo", "Santa", ""]]}, {"id": "2103.13109", "submitter": "Peter Mortimer", "authors": "Kai A. Metzger, Peter Mortimer, Hans-Joachim Wuensche", "title": "A Fine-Grained Dataset and its Efficient Semantic Segmentation for\n  Unstructured Driving Scenarios", "comments": "Accepted at International Conference on Pattern Recognition 2020\n  (ICPR). For the associated project page, see\n  https://www.mucar3.de/icpr2020-tas500/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in autonomous driving for unstructured environments suffers from a\nlack of semantically labeled datasets compared to its urban counterpart. Urban\nand unstructured outdoor environments are challenging due to the varying\nlighting and weather conditions during a day and across seasons. In this paper,\nwe introduce TAS500, a novel semantic segmentation dataset for autonomous\ndriving in unstructured environments. TAS500 offers fine-grained vegetation and\nterrain classes to learn drivable surfaces and natural obstacles in outdoor\nscenes effectively. We evaluate the performance of modern semantic segmentation\nmodels with an additional focus on their efficiency. Our experiments\ndemonstrate the advantages of fine-grained semantic classes to improve the\noverall prediction accuracy, especially along the class boundaries. The dataset\nand pretrained model are available at mucar3.de/icpr2020-tas500.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 11:30:43 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Metzger", "Kai A.", ""], ["Mortimer", "Peter", ""], ["Wuensche", "Hans-Joachim", ""]]}, {"id": "2103.13124", "submitter": "Faqiang Liu", "authors": "Faqiang Liu, Rong Zhao, Luping Shi", "title": "Adversarial Feature Stacking for Accurate and Robust Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have achieved remarkable performance on a variety\nof applications but are extremely vulnerable to adversarial perturbation. To\naddress this issue, various defense methods have been proposed to enhance model\nrobustness. Unfortunately, the most representative and promising methods, such\nas adversarial training and its variants, usually degrade model accuracy on\nbenign samples, limiting practical utility. This indicates that it is difficult\nto extract both robust and accurate features using a single network under\ncertain conditions, such as limited training data, resulting in a trade-off\nbetween accuracy and robustness. To tackle this problem, we propose an\nAdversarial Feature Stacking (AFS) model that can jointly take advantage of\nfeatures with varied levels of robustness and accuracy, thus significantly\nalleviating the aforementioned trade-off. Specifically, we adopt multiple\nnetworks adversarially trained with different perturbation budgets to extract\neither more robust features or more accurate features. These features are then\nfused by a learnable merger to give final predictions. We evaluate the AFS\nmodel on CIFAR-10 and CIFAR-100 datasets with strong adaptive attack methods,\nwhich significantly advances the state-of-the-art in terms of the trade-off.\nWithout extra training data, the AFS model achieves a benign accuracy\nimprovement of 6% on CIFAR-10 and 9% on CIFAR-100 with comparable or even\nstronger robustness than the state-of-the-art adversarial training methods.\nThis work demonstrates the feasibility to obtain both accurate and robust\nmodels under the circumstances of limited training data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:01:24 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Faqiang", ""], ["Zhao", "Rong", ""], ["Shi", "Luping", ""]]}, {"id": "2103.13127", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang\n  Su, Jun Zhu", "title": "Black-box Detection of Backdoor Attacks with Limited Information and\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although deep neural networks (DNNs) have made rapid progress in recent\nyears, they are vulnerable in adversarial environments. A malicious backdoor\ncould be embedded in a model by poisoning the training dataset, whose intention\nis to make the infected model give wrong predictions during inference when the\nspecific trigger appears. To mitigate the potential threats of backdoor\nattacks, various backdoor detection and defense methods have been proposed.\nHowever, the existing techniques usually require the poisoned training data or\naccess to the white-box model, which is commonly unavailable in practice. In\nthis paper, we propose a black-box backdoor detection (B3D) method to identify\nbackdoor attacks with only query access to the model. We introduce a\ngradient-free optimization algorithm to reverse-engineer the potential trigger\nfor each class, which helps to reveal the existence of backdoor attacks. In\naddition to backdoor detection, we also propose a simple strategy for reliable\npredictions using the identified backdoored models. Extensive experiments on\nhundreds of DNN models trained on several datasets corroborate the\neffectiveness of our method under the black-box setting against various\nbackdoor attacks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:06:40 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Dong", "Yinpeng", ""], ["Yang", "Xiao", ""], ["Deng", "Zhijie", ""], ["Pang", "Tianyu", ""], ["Xiao", "Zihao", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2103.13134", "submitter": "Mingjie Xu", "authors": "Mingjie Xu, Haofei Wang, Yunfei Liu, Feng Lu", "title": "Vulnerability of Appearance-based Gaze Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance-based gaze estimation has achieved significant improvement by\nusing deep learning. However, many deep learning-based methods suffer from the\nvulnerability property, i.e., perturbing the raw image using noise confuses the\ngaze estimation models. Although the perturbed image visually looks similar to\nthe original image, the gaze estimation models output the wrong gaze direction.\nIn this paper, we investigate the vulnerability of appearance-based gaze\nestimation. To our knowledge, this is the first time that the vulnerability of\ngaze estimation to be found. We systematically characterized the vulnerability\nproperty from multiple aspects, the pixel-based adversarial attack, the\npatch-based adversarial attack and the defense strategy. Our experimental\nresults demonstrate that the CA-Net shows superior performance against attack\namong the four popular appearance-based gaze estimation networks, Full-Face,\nGaze-Net, CA-Net and RT-GENE. This study draws the attention of researchers in\nthe appearance-based gaze estimation community to defense from adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:19:59 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Xu", "Mingjie", ""], ["Wang", "Haofei", ""], ["Liu", "Yunfei", ""], ["Lu", "Feng", ""]]}, {"id": "2103.13137", "submitter": "Chuming Lin", "authors": "Chuming Lin, Chengming Xu, Donghao Luo, Yabiao Wang, Ying Tai,\n  Chengjie Wang, Jilin Li, Feiyue Huang, Yanwei Fu", "title": "Learning Salient Boundary Feature for Anchor-free Temporal Action\n  Localization", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal action localization is an important yet challenging task in video\nunderstanding. Typically, such a task aims at inferring both the action\ncategory and localization of the start and end frame for each action instance\nin a long, untrimmed video.While most current models achieve good results by\nusing pre-defined anchors and numerous actionness, such methods could be\nbothered with both large number of outputs and heavy tuning of locations and\nsizes corresponding to different anchors. Instead, anchor-free methods is\nlighter, getting rid of redundant hyper-parameters, but gains few attention. In\nthis paper, we propose the first purely anchor-free temporal localization\nmethod, which is both efficient and effective. Our model includes (i) an\nend-to-end trainable basic predictor, (ii) a saliency-based refinement module\nto gather more valuable boundary features for each proposal with a novel\nboundary pooling, and (iii) several consistency constraints to make sure our\nmodel can find the accurate boundary given arbitrary proposals. Extensive\nexperiments show that our method beats all anchor-based and actionness-guided\nmethods with a remarkable margin on THUMOS14, achieving state-of-the-art\nresults, and comparable ones on ActivityNet v1.3. Code is available at\nhttps://github.com/TencentYoutuResearch/ActionDetection-AFSD.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:28:32 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lin", "Chuming", ""], ["Xu", "Chengming", ""], ["Luo", "Donghao", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Fu", "Yanwei", ""]]}, {"id": "2103.13141", "submitter": "Zhiwu Qing", "authors": "Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang, Wei Wu, Xiang\n  Wang, Yu Qiao, Junjie Yan, Changxin Gao, Nong Sang", "title": "Temporal Context Aggregation Network for Temporal Action Proposal\n  Refinement", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action proposal generation aims to estimate temporal intervals of\nactions in untrimmed videos, which is a challenging yet important task in the\nvideo understanding field. The proposals generated by current methods still\nsuffer from inaccurate temporal boundaries and inferior confidence used for\nretrieval owing to the lack of efficient temporal modeling and effective\nboundary context utilization. In this paper, we propose Temporal Context\nAggregation Network (TCANet) to generate high-quality action proposals through\n\"local and global\" temporal context aggregation and complementary as well as\nprogressive boundary refinement. Specifically, we first design a Local-Global\nTemporal Encoder (LGTE), which adopts the channel grouping strategy to\nefficiently encode both \"local and global\" temporal inter-dependencies.\nFurthermore, both the boundary and internal context of proposals are adopted\nfor frame-level and segment-level boundary regressions, respectively. Temporal\nBoundary Regressor (TBR) is designed to combine these two regression\ngranularities in an end-to-end fashion, which achieves the precise boundaries\nand reliable confidence of proposals through progressive refinement. Extensive\nexperiments are conducted on three challenging datasets: HACS,\nActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high\nprecision and recall. By combining with the existing action classifier, TCANet\ncan obtain remarkable temporal action detection performance compared with other\nmethods. Not surprisingly, the proposed TCANet won the 1$^{st}$ place in the\nCVPR 2020 - HACS challenge leaderboard on temporal action localization task.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:34:49 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Qing", "Zhiwu", ""], ["Su", "Haisheng", ""], ["Gan", "Weihao", ""], ["Wang", "Dongliang", ""], ["Wu", "Wei", ""], ["Wang", "Xiang", ""], ["Qiao", "Yu", ""], ["Yan", "Junjie", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""]]}, {"id": "2103.13151", "submitter": "Zhongshan Sun", "authors": "Yishan He, Fei Gao, Jun Wang, Amir Hussain, Erfu Yang, Huiyu Zhou", "title": "Learning Polar Encodings for Arbitrary-Oriented Ship Detection in SAR\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Common horizontal bounding box (HBB)-based methods are not capable of\naccurately locating slender ship targets with arbitrary orientations in\nsynthetic aperture radar (SAR) images. Therefore, in recent years, methods\nbased on oriented bounding box (OBB) have gradually received attention from\nresearchers. However, most of the recently proposed deep learning-based methods\nfor OBB detection encounter the boundary discontinuity problem in angle or key\npoint regression. In order to alleviate this problem, researchers propose to\nintroduce some manually set parameters or extra network branches for\ndistinguishing the boundary cases, which make training more diffcult and lead\nto performance degradation. In this paper, in order to solve the boundary\ndiscontinuity problem in OBB regression, we propose to detect SAR ships by\nlearning polar encodings. The encoding scheme uses a group of vectors pointing\nfrom the center of the ship target to the boundary points to represent an OBB.\nThe boundary discontinuity problem is avoided by training and inference\ndirectly according to the polar encodings. In addition, we propose an Intersect\nover Union (IOU) -weighted regression loss, which further guides the training\nof polar encodings through the IOU metric and improves the detection\nperformance. Experiments on the Rotating SAR Ship Detection Dataset (RSSDD)\nshow that the proposed method can achieve better detection performance over\nother comparison algorithms and other OBB encoding schemes, demonstrating the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:52:54 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["He", "Yishan", ""], ["Gao", "Fei", ""], ["Wang", "Jun", ""], ["Hussain", "Amir", ""], ["Yang", "Erfu", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2103.13164", "submitter": "Shujie Luo", "authors": "Shujie Luo, Hang Dai, Ling Shao, Yong Ding", "title": "M3DSSD: Monocular 3D Single Stage Object Detector", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Monocular 3D Single Stage object Detector\n(M3DSSD) with feature alignment and asymmetric non-local attention. Current\nanchor-based monocular 3D object detection methods suffer from feature\nmismatching. To overcome this, we propose a two-step feature alignment\napproach. In the first step, the shape alignment is performed to enable the\nreceptive field of the feature map to focus on the pre-defined anchors with\nhigh confidence scores. In the second step, the center alignment is used to\nalign the features at 2D/3D centers. Further, it is often difficult to learn\nglobal information and capture long-range relationships, which are important\nfor the depth prediction of objects. Therefore, we propose a novel asymmetric\nnon-local attention block with multi-scale sampling to extract depth-wise\nfeatures. The proposed M3DSSD achieves significantly better performance than\nthe monocular 3D object detection methods on the KITTI dataset, in both 3D\nobject detection and bird's eye view tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 13:09:11 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Luo", "Shujie", ""], ["Dai", "Hang", ""], ["Shao", "Ling", ""], ["Ding", "Yong", ""]]}, {"id": "2103.13173", "submitter": "Yihua Cheng", "authors": "Yihua Cheng, Yiwei Bao, Feng Lu", "title": "PureGaze: Purifying Gaze Feature for Generalizable Gaze Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze estimation methods learn eye gaze from facial features. However, among\nrich information in the facial image, real gaze-relevant features only\ncorrespond to subtle changes in eye region, while other gaze-irrelevant\nfeatures like illumination, personal appearance and even facial expression may\naffect the learning in an unexpected way. This is a major reason why existing\nmethods show significant performance degradation in cross-domain/dataset\nevaluation. In this paper, we tackle the domain generalization problem in\ncross-domain gaze estimation for unknown target domains. To be specific, we\nrealize the domain generalization by gaze feature purification. We eliminate\ngaze-irrelevant factors such as illumination and identity to improve the\ncross-dataset performance without knowing the target dataset. We design a\nplug-and-play self-adversarial framework for the gaze feature purification. The\nframework enhances not only our baseline but also existing gaze estimation\nmethods directly and significantly. Our method achieves the state-of-the-art\nperformance in different benchmarks. Meanwhile, the purification is easily\nexplainable via visualization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 13:22:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Cheng", "Yihua", ""], ["Bao", "Yiwei", ""], ["Lu", "Feng", ""]]}, {"id": "2103.13183", "submitter": "Yuan Liu", "authors": "Yuan Liu, Jingyuan Chen, Zhenfang Chen, Bing Deng, Jianqiang Huang,\n  Hanwang Zhang", "title": "The Blessings of Unlabeled Background in Untrimmed Videos", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Weakly-supervised Temporal Action Localization (WTAL) aims to detect the\naction segments with only video-level action labels in training. The key\nchallenge is how to distinguish the action of interest segments from the\nbackground, which is unlabelled even on the video-level. While previous works\ntreat the background as \"curses\", we consider it as \"blessings\". Specifically,\nwe first use causal analysis to point out that the common localization errors\nare due to the unobserved confounder that resides ubiquitously in visual\nrecognition. Then, we propose a Temporal Smoothing PCA-based (TS-PCA)\ndeconfounder, which exploits the unlabelled background to model an observed\nsubstitute for the unobserved confounder, to remove the confounding effect.\nNote that the proposed deconfounder is model-agnostic and non-intrusive, and\nhence can be applied in any WTAL method without model re-designs. Through\nextensive experiments on four state-of-the-art WTAL methods, we show that the\ndeconfounder can improve all of them on the public datasets: THUMOS-14 and\nActivityNet-1.3.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 13:34:42 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 02:34:57 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Liu", "Yuan", ""], ["Chen", "Jingyuan", ""], ["Chen", "Zhenfang", ""], ["Deng", "Bing", ""], ["Huang", "Jianqiang", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2103.13201", "submitter": "Xiaodong Gu", "authors": "Xiaodong Gu, Weihao Yuan, Zuozhuo Dai, Chengzhou Tang, Siyu Zhu, Ping\n  Tan", "title": "DRO: Deep Recurrent Optimizer for Structure-from-Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are increasing interests of studying the structure-from-motion (SfM)\nproblem with machine learning techniques. While earlier methods directly learn\na mapping from images to depth maps and camera poses, more recent works enforce\nmulti-view geometry constraints through optimization embedded in the learning\nframework. This paper presents a novel optimization method based on recurrent\nneural networks to further exploit the potential of neural networks in SfM.\nSpecifically, our neural optimizer alternately updates the depth and camera\nposes through iterations to minimize a feature-metric cost, and two gated\nrecurrent units iteratively improve the results by tracing historical\ninformation. In this way, our network is a gradient-free zeroth-order optimizer\ndesigned for SfM and can be applied to both supervised and self-supervised SfM.\nExtensive experimental results demonstrate that our method outperforms previous\nmethods and is more efficient in computation and memory consumption than\ncost-volume-based methods. In particular, our self-supervised method\noutperforms previous supervised methods on the KITTI and ScanNet datasets. Our\nsource code is available at https://github.com/aliyun/dro-sfm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 13:59:40 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 09:23:23 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 12:38:55 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gu", "Xiaodong", ""], ["Yuan", "Weihao", ""], ["Dai", "Zuozhuo", ""], ["Tang", "Chengzhou", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""]]}, {"id": "2103.13225", "submitter": "Shuai Shen", "authors": "Shuai Shen, Wanhua Li, Zheng Zhu, Guan Huang, Dalong Du, Jiwen Lu, Jie\n  Zhou", "title": "Structure-Aware Face Clustering on a Large-Scale Graph with\n  $\\bf{10^{7}}$ Nodes", "comments": "Accepted by the CVPR 2021. Project: https://sstzal.github.io/STAR-FC/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face clustering is a promising method for annotating unlabeled face images.\nRecent supervised approaches have boosted the face clustering accuracy greatly,\nhowever their performance is still far from satisfactory. These methods can be\nroughly divided into global-based and local-based ones. Global-based methods\nsuffer from the limitation of training data scale, while local-based ones are\ndifficult to grasp the whole graph structure information and usually take a\nlong time for inference. Previous approaches fail to tackle these two\nchallenges simultaneously. To address the dilemma of large-scale training and\nefficient inference, we propose the STructure-AwaRe Face Clustering (STAR-FC)\nmethod. Specifically, we design a structure-preserved subgraph sampling\nstrategy to explore the power of large-scale training data, which can increase\nthe training data scale from ${10^{5}}$ to ${10^{7}}$. During inference, the\nSTAR-FC performs efficient full-graph clustering with two steps: graph parsing\nand graph refinement. And the concept of node intimacy is introduced in the\nsecond step to mine the local structural information. The STAR-FC gets 91.97\npairwise F-score on partial MS1M within 310s which surpasses the\nstate-of-the-arts. Furthermore, we are the first to train on very large-scale\ngraph with 20M nodes, and achieve superior inference results on 12M testing\ndata. Overall, as a simple and effective method, the proposed STAR-FC provides\na strong baseline for large-scale face clustering. Code is available at\n\\url{https://sstzal.github.io/STAR-FC/}.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:34:26 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 03:25:52 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Shen", "Shuai", ""], ["Li", "Wanhua", ""], ["Zhu", "Zheng", ""], ["Huang", "Guan", ""], ["Du", "Dalong", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2103.13246", "submitter": "Gabrielle Flood", "authors": "Gabrielle Flood, David Gillsj\\\"o, Patrik Persson, Anders Heyden, Kalle\n  \\r{A}str\\\"om", "title": "Generic Merging of Structure from Motion Maps with a Low Memory\n  Footprint", "comments": "Accepted at ICPR2020, 9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of cheap image sensors, the amount of available image\ndata have increased enormously, and the possibility of using crowdsourced\ncollection methods has emerged. This calls for development of ways to handle\nall these data. In this paper, we present new tools that will enable efficient,\nflexible and robust map merging. Assuming that separate optimisations have been\nperformed for the individual maps, we show how only relevant data can be stored\nin a low memory footprint representation. We use these representations to\nperform map merging so that the algorithm is invariant to the merging order and\nindependent of the choice of coordinate system. The result is a robust\nalgorithm that can be applied to several maps simultaneously. The result of a\nmerge can also be represented with the same type of low-memory footprint\nformat, which enables further merging and updating of the map in a hierarchical\nway. Furthermore, the method can perform loop closing and also detect changes\nin the scene between the capture of the different image sequences. Using both\nsimulated and real data - from both a hand held mobile phone and from a drone -\nwe verify the performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:03:25 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Flood", "Gabrielle", ""], ["Gillsj\u00f6", "David", ""], ["Persson", "Patrik", ""], ["Heyden", "Anders", ""], ["\u00c5str\u00f6m", "Kalle", ""]]}, {"id": "2103.13253", "submitter": "Mingyu Ding", "authors": "Mingyu Ding, Yuqi Huo, Haoyu Lu, Linjie Yang, Zhe Wang, Zhiwu Lu,\n  Jingdong Wang, Ping Luo", "title": "Learning Versatile Neural Architectures by Propagating Network Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores how to design a single neural network that is capable of\nadapting to multiple heterogeneous tasks of computer vision, such as image\nsegmentation, 3D detection, and video recognition. This goal is challenging\nbecause network architecture designs in different tasks are inconsistent. We\nsolve this challenge by proposing Network Coding Propagation (NCP), a novel\n\"neural predictor\", which is able to predict an architecture's performance in\nmultiple datasets and tasks. Unlike prior arts of neural architecture search\n(NAS) that typically focus on a single task, NCP has several unique benefits.\n(1) NCP can be trained on different NAS benchmarks, such as NAS-Bench-201 and\nNAS-Bench-MR, which contains a novel network space designed by us for jointly\nsearching an architecture among multiple tasks, including ImageNet, Cityscapes,\nKITTI, and HMDB51. (2) NCP learns from network codes but not original data,\nenabling it to update the architecture efficiently across datasets. (3)\nExtensive experiments evaluate NCP on object classification, detection,\nsegmentation, and video recognition. For example, with 17\\% fewer FLOPs, a\nsingle architecture returned by NCP achieves 86\\% and 77.16\\% on\nImageNet-50-1000 and Cityscapes respectively, outperforming its counterparts.\nMore interestingly, NCP enables a single architecture applicable to both image\nsegmentation and video recognition, which achieves competitive performance on\nboth HMDB51 and ADE20K compared to the singular counterparts. Code is available\nat https://github.com/dingmyu/NCP}{https://github.com/dingmyu/NCP.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:20:38 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ding", "Mingyu", ""], ["Huo", "Yuqi", ""], ["Lu", "Haoyu", ""], ["Yang", "Linjie", ""], ["Wang", "Zhe", ""], ["Lu", "Zhiwu", ""], ["Wang", "Jingdong", ""], ["Luo", "Ping", ""]]}, {"id": "2103.13258", "submitter": "Changlin Li", "authors": "Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang, Zhihui Li and\n  Xiaojun Chang", "title": "Dynamic Slimmable Network", "comments": "Accepted to CVPR 2021 as an Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Current dynamic networks and dynamic pruning methods have shown their\npromising capability in reducing theoretical computation complexity. However,\ndynamic sparse patterns on convolutional filters fail to achieve actual\nacceleration in real-world implementation, due to the extra burden of indexing,\nweight-copying, or zero-masking. Here, we explore a dynamic network slimming\nregime, named Dynamic Slimmable Network (DS-Net), which aims to achieve good\nhardware-efficiency via dynamically adjusting filter numbers of networks at\ntest time with respect to different inputs, while keeping filters stored\nstatically and contiguously in hardware to prevent the extra burden. Our DS-Net\nis empowered with the ability of dynamic inference by the proposed\ndouble-headed dynamic gate that comprises an attention head and a slimming head\nto predictively adjust network width with negligible extra computation cost. To\nensure generality of each candidate architecture and the fairness of gate, we\npropose a disentangled two-stage training scheme inspired by one-shot NAS. In\nthe first stage, a novel training technique for weight-sharing networks named\nIn-place Ensemble Bootstrapping is proposed to improve the supernet training\nefficacy. In the second stage, Sandwich Gate Sparsification is proposed to\nassist the gate training by identifying easy and hard samples in an online way.\nExtensive experiments demonstrate our DS-Net consistently outperforms its\nstatic counterparts as well as state-of-the-art static and dynamic model\ncompression methods by a large margin (up to 5.9%). Typically, DS-Net achieves\n2-4x computation reduction and 1.62x real-world acceleration over ResNet-50 and\nMobileNet with minimal accuracy drops on ImageNet. Code release:\nhttps://github.com/changlin31/DS-Net .\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:25:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Li", "Changlin", ""], ["Wang", "Guangrun", ""], ["Wang", "Bing", ""], ["Liang", "Xiaodan", ""], ["Li", "Zhihui", ""], ["Chang", "Xiaojun", ""]]}, {"id": "2103.13279", "submitter": "Yang Cao", "authors": "Yang Cao, Zhengqiang Zhang, Enze Xie, Qibin Hou, Kai Zhao, Xiangui\n  Luo, Jian Tuo", "title": "FakeMix Augmentation Improves Transparent Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting transparent objects in natural scenes is challenging due to the low\ncontrast in texture, brightness and colors. Recent deep-learning-based works\nreveal that it is effective to leverage boundaries for transparent object\ndetection (TOD). However, these methods usually encounter boundary-related\nimbalance problem, leading to limited generation capability. Detailly, a kind\nof boundaries in the background, which share the same characteristics with\nboundaries of transparent objects but have much smaller amounts, usually hurt\nthe performance. To conquer the boundary-related imbalance problem, we propose\na novel content-dependent data augmentation method termed FakeMix. Considering\ncollecting these trouble-maker boundaries in the background is hard without\ncorresponding annotations, we elaborately generate them by appending the\nboundaries of transparent objects from other samples into the current image\nduring training, which adjusts the data space and improves the generalization\nof the models. Further, we present AdaptiveASPP, an enhanced version of ASPP,\nthat can capture multi-scale and cross-modality features dynamically. Extensive\nexperiments demonstrate that our methods clearly outperform the\nstate-of-the-art methods. We also show that our approach can also transfer well\non related tasks, in which the model meets similar troubles, such as mirror\ndetection, glass detection, and camouflaged object detection. Code will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:51:37 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Cao", "Yang", ""], ["Zhang", "Zhengqiang", ""], ["Xie", "Enze", ""], ["Hou", "Qibin", ""], ["Zhao", "Kai", ""], ["Luo", "Xiangui", ""], ["Tuo", "Jian", ""]]}, {"id": "2103.13282", "submitter": "Alexander Mathis", "authors": "Daniel Joska and Liam Clark and Naoya Muramatsu and Ricardo Jericevich\n  and Fred Nicolls and Alexander Mathis and Mackenzie W. Mathis and Amir Patel", "title": "AcinoSet: A 3D Pose Estimation Dataset and Baseline Models for Cheetahs\n  in the Wild", "comments": "Code and data can be found at:\n  https://github.com/African-Robotics-Unit/AcinoSet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY eess.SY q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals are capable of extreme agility, yet understanding their complex\ndynamics, which have ecological, biomechanical and evolutionary implications,\nremains challenging. Being able to study this incredible agility will be\ncritical for the development of next-generation autonomous legged robots. In\nparticular, the cheetah (acinonyx jubatus) is supremely fast and maneuverable,\nyet quantifying its whole-body 3D kinematic data during locomotion in the wild\nremains a challenge, even with new deep learning-based methods. In this work we\npresent an extensive dataset of free-running cheetahs in the wild, called\nAcinoSet, that contains 119,490 frames of multi-view synchronized high-speed\nvideo footage, camera calibration files and 7,588 human-annotated frames. We\nutilize markerless animal pose estimation to provide 2D keypoints. Then, we use\nthree methods that serve as strong baselines for 3D pose estimation tool\ndevelopment: traditional sparse bundle adjustment, an Extended Kalman Filter,\nand a trajectory optimization-based method we call Full Trajectory Estimation.\nThe resulting 3D trajectories, human-checked 3D ground truth, and an\ninteractive tool to inspect the data is also provided. We believe this dataset\nwill be useful for a diverse range of fields such as ecology, neuroscience,\nrobotics, biomechanics as well as computer vision.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:54:11 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Joska", "Daniel", ""], ["Clark", "Liam", ""], ["Muramatsu", "Naoya", ""], ["Jericevich", "Ricardo", ""], ["Nicolls", "Fred", ""], ["Mathis", "Alexander", ""], ["Mathis", "Mackenzie W.", ""], ["Patel", "Amir", ""]]}, {"id": "2103.13283", "submitter": "Lianrui Zuo", "authors": "Lianrui Zuo, Blake E. Dewey, Aaron Carass, Yihao Liu, Yufan He, Peter\n  A. Calabresi, Jerry L. Prince", "title": "Information-based Disentangled Representation Learning for Unsupervised\n  MR Harmonization", "comments": "Accepted in the 27th International Conference on Information\n  Processing in Medical Imaging (IPMI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy and consistency are two key factors in computer-assisted magnetic\nresonance (MR) image analysis. However, contrast variation from site to site\ncaused by lack of standardization in MR acquisition impedes consistent\nmeasurements. In recent years, image harmonization approaches have been\nproposed to compensate for contrast variation in MR images. Current\nharmonization approaches either require cross-site traveling subjects for\nsupervised training or heavily rely on site-specific harmonization models to\nencourage harmonization accuracy. These requirements potentially limit the\napplication of current harmonization methods in large-scale multi-site studies.\nIn this work, we propose an unsupervised MR harmonization framework, CALAMITI\n(Contrast Anatomy Learning and Analysis for MR Intensity Translation and\nIntegration), based on information bottleneck theory. CALAMITI learns a\ndisentangled latent space using a unified structure for multi-site\nharmonization without the need for traveling subjects. Our model is also able\nto adapt itself to harmonize MR images from a new site with fine tuning solely\non images from the new site. Both qualitative and quantitative results show\nthat the proposed method achieves superior performance compared with other\nunsupervised harmonization approaches.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:54:27 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zuo", "Lianrui", ""], ["Dewey", "Blake E.", ""], ["Carass", "Aaron", ""], ["Liu", "Yihao", ""], ["He", "Yufan", ""], ["Calabresi", "Peter A.", ""], ["Prince", "Jerry L.", ""]]}, {"id": "2103.13314", "submitter": "Marta Bianca Maria Ranzini Dr", "authors": "Marta B.M. Ranzini, Lucas Fidon, S\\'ebastien Ourselin, Marc Modat and\n  Tom Vercauteren", "title": "MONAIfbs: MONAI-based fetal brain MRI deep learning segmentation", "comments": "Abstract accepted at IEEE International Symposium on Biomedical\n  Imaging (ISBI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fetal Magnetic Resonance Imaging, Super Resolution Reconstruction (SRR)\nalgorithms are becoming popular tools to obtain high-resolution 3D volume\nreconstructions from low-resolution stacks of 2D slices, acquired at different\norientations. To be effective, these algorithms often require accurate\nsegmentation of the region of interest, such as the fetal brain in suspected\npathological cases. In the case of Spina Bifida, Ebner, Wang et al.\n(NeuroImage, 2020) combined their SRR algorithm with a 2-step segmentation\npipeline (2D localisation followed by a 2D segmentation network). However, if\nthe localisation step fails, the second network is not able to recover a\ncorrect brain mask, thus requiring manual corrections for an effective SRR. In\nthis work, we aim at improving the fetal brain segmentation for SRR in Spina\nBifida. We hypothesise that a well-trained single-step UNet can achieve\naccurate performance, avoiding the need of a 2-step approach. We propose a new\ntool for fetal brain segmentation called MONAIfbs, which takes advantage of the\nMedical Open Network for Artificial Intelligence (MONAI) framework. Our network\nis based on the dynamic UNet (dynUNet), an adaptation of the nnU-Net framework.\nWhen compared to the original 2-step approach proposed in Ebner-Wang, and the\nsame Ebner-Wang approach retrained with the expanded dataset available for this\nwork, the dynUNet showed to achieve higher performance using a single step\nonly. It also showed to reduce the number of outliers, as only 28 stacks\nobtained Dice score less than 0.9, compared to 68 for Ebner-Wang and 53\nEbner-Wang expanded. The proposed dynUNet model thus provides an improvement of\nthe state-of-the-art fetal brain segmentation techniques, reducing the need for\nmanual correction in automated SRR pipelines. Our code and our trained model\nare made publicly available at https://github.com/gift-surg/MONAIfbs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 18:35:25 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ranzini", "Marta B. M.", ""], ["Fidon", "Lucas", ""], ["Ourselin", "S\u00e9bastien", ""], ["Modat", "Marc", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2103.13318", "submitter": "Thomas Mensink", "authors": "Thomas Mensink, Jasper Uijlings, Alina Kuznetsova, Michael Gygli,\n  Vittorio Ferrari", "title": "Factors of Influence for Transfer Learning across Diverse Appearance\n  Domains and Task Types", "comments": "submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning enables to re-use knowledge learned on a source task to\nhelp learning a target task. A simple form of transfer learning is common in\ncurrent state-of-the-art computer vision models, i.e. pre-training a model for\nimage classification on the ILSVRC dataset, and then fine-tune on any target\ntask. However, previous systematic studies of transfer learning have been\nlimited and the circumstances in which it is expected to work are not fully\nunderstood. In this paper we carry out an extensive experimental exploration of\ntransfer learning across vastly different image domains (consumer photos,\nautonomous driving, aerial imagery, underwater, indoor scenes, synthetic,\nclose-ups) and task types (semantic segmentation, object detection, depth\nestimation, keypoint detection). Importantly, these are all complex, structured\noutput tasks types relevant to modern computer vision applications. In total we\ncarry out over 1200 transfer experiments, including many where the source and\ntarget come from different image domains, task types, or both. We\nsystematically analyze these experiments to understand the impact of image\ndomain, task type, and dataset size on transfer learning performance. Our study\nleads to several insights and concrete recommendations for practitioners.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:24:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Mensink", "Thomas", ""], ["Uijlings", "Jasper", ""], ["Kuznetsova", "Alina", ""], ["Gygli", "Michael", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "2103.13322", "submitter": "Ghouthi Boukli Hacene", "authors": "Ghouthi Boukli Hacene, Lukas Mauch, Stefan Uhlich, Fabien Cardinaux", "title": "DNN Quantization with Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-bit quantization of network weights and activations can drastically\nreduce the memory footprint, complexity, energy consumption and latency of Deep\nNeural Networks (DNNs). However, low-bit quantization can also cause a\nconsiderable drop in accuracy, in particular when we apply it to complex\nlearning tasks or lightweight DNN architectures. In this paper, we propose a\ntraining procedure that relaxes the low-bit quantization. We call this\nprocedure \\textit{DNN Quantization with Attention} (DQA). The relaxation is\nachieved by using a learnable linear combination of high, medium and low-bit\nquantizations. Our learning procedure converges step by step to a low-bit\nquantization using an attention mechanism with temperature scheduling. In\nexperiments, our approach outperforms other low-bit quantization techniques on\nvarious object recognition benchmarks such as CIFAR10, CIFAR100 and ImageNet\nILSVRC 2012, achieves almost the same accuracy as a full precision DNN, and\nconsiderably reduces the accuracy drop when quantizing lightweight DNN\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:24:59 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Hacene", "Ghouthi Boukli", ""], ["Mauch", "Lukas", ""], ["Uhlich", "Stefan", ""], ["Cardinaux", "Fabien", ""]]}, {"id": "2103.13339", "submitter": "Faraz Lotfi Dr", "authors": "Faraz Lotfi, Farnoosh Faraji, Hamid D. Taghirad", "title": "Object Localization Through a Single Multiple-Model Convolutional Neural\n  Network with a Specific Training Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object localization has a vital role in any object detector, and therefore,\nhas been the focus of attention by many researchers. In this article, a special\ntraining approach is proposed for a light convolutional neural network (CNN) to\ndetermine the region of interest (ROI) in an image while effectively reducing\nthe number of probable anchor boxes. Almost all CNN-based detectors utilize a\nfixed input size image, which may yield poor performance when dealing with\nvarious object sizes. In this paper, a different CNN structure is proposed\ntaking three different input sizes, to enhance the performance. In order to\ndemonstrate the effectiveness of the proposed method, two common data set are\nused for training while tracking by localization application is considered to\ndemonstrate its final performance. The promising results indicate the\napplicability of the presented structure and the training method in practice.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:52:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lotfi", "Faraz", ""], ["Faraji", "Farnoosh", ""], ["Taghirad", "Hamid D.", ""]]}, {"id": "2103.13361", "submitter": "Junyeong Kim", "authors": "Junyeong Kim and Sunjae Yoon and Dahyun Kim and Chang D. Yoo", "title": "Structured Co-reference Graph Attention for Video-grounded Dialogue", "comments": "Accepted to AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A video-grounded dialogue system referred to as the Structured Co-reference\nGraph Attention (SCGA) is presented for decoding the answer sequence to a\nquestion regarding a given video while keeping track of the dialogue context.\nAlthough recent efforts have made great strides in improving the quality of the\nresponse, performance is still far from satisfactory. The two main challenging\nissues are as follows: (1) how to deduce co-reference among multiple modalities\nand (2) how to reason on the rich underlying semantic structure of video with\ncomplex spatial and temporal dynamics. To this end, SCGA is based on (1)\nStructured Co-reference Resolver that performs dereferencing via building a\nstructured graph over multiple modalities, (2) Spatio-temporal Video Reasoner\nthat captures local-to-global dynamics of video via gradually neighboring graph\nattention. SCGA makes use of pointer network to dynamically replicate parts of\nthe question for decoding the answer sequence. The validity of the proposed\nSCGA is demonstrated on AVSD@DSTC7 and AVSD@DSTC8 datasets, a challenging\nvideo-grounded dialogue benchmarks, and TVQA dataset, a large-scale videoQA\nbenchmark. Our empirical results show that SCGA outperforms other\nstate-of-the-art dialogue systems on both benchmarks, while extensive ablation\nstudy and qualitative analysis reveal performance gain and improved\ninterpretability.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:36:33 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kim", "Junyeong", ""], ["Yoon", "Sunjae", ""], ["Kim", "Dahyun", ""], ["Yoo", "Chang D.", ""]]}, {"id": "2103.13372", "submitter": "Enrique Sanchez", "authors": "Enrique Sanchez and Mani Kumar Tellamekala and Michel Valstar and\n  Georgios Tzimiropoulos", "title": "Affective Processes: stochastic modelling of temporal context for\n  emotion and facial expression recognition", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal context is key to the recognition of expressions of emotion.\nExisting methods, that rely on recurrent or self-attention models to enforce\ntemporal consistency, work on the feature level, ignoring the task-specific\ntemporal dependencies, and fail to model context uncertainty. To alleviate\nthese issues, we build upon the framework of Neural Processes to propose a\nmethod for apparent emotion recognition with three key novel components: (a)\nprobabilistic contextual representation with a global latent variable model;\n(b) temporal context modelling using task-specific predictions in addition to\nfeatures; and (c) smart temporal context selection. We validate our approach on\nfour databases, two for Valence and Arousal estimation (SEWA and AffWild2), and\ntwo for Action Unit intensity estimation (DISFA and BP4D). Results show a\nconsistent improvement over a series of strong baselines as well as over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:48:19 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sanchez", "Enrique", ""], ["Tellamekala", "Mani Kumar", ""], ["Valstar", "Michel", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2103.13389", "submitter": "Vadim Sushko", "authors": "Vadim Sushko, Juergen Gall, Anna Khoreva", "title": "One-Shot GAN: Learning to Generate Samples from Single Images and Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large number of training samples, GANs can achieve remarkable\nperformance for the image synthesis task. However, training GANs in extremely\nlow-data regimes remains a challenge, as overfitting often occurs, leading to\nmemorization or training divergence. In this work, we introduce One-Shot GAN,\nan unconditional generative model that can learn to generate samples from a\nsingle training image or a single video clip. We propose a two-branch\ndiscriminator architecture, with content and layout branches designed to judge\ninternal content and scene layout realism separately from each other. This\nallows synthesis of visually plausible, novel compositions of a scene, with\nvarying content and layout, while preserving the context of the original\nsample. Compared to previous single-image GAN models, One-Shot GAN generates\nmore diverse, higher quality images, while also not being restricted to a\nsingle image setting. We show that our model successfully deals with other\none-shot regimes, and introduce a new task of learning generative models from a\nsingle video.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:59:07 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sushko", "Vadim", ""], ["Gall", "Juergen", ""], ["Khoreva", "Anna", ""]]}, {"id": "2103.13413", "submitter": "Ren\\'e Ranftl", "authors": "Ren\\'e Ranftl, Alexey Bochkovskiy, Vladlen Koltun", "title": "Vision Transformers for Dense Prediction", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce dense vision transformers, an architecture that leverages vision\ntransformers in place of convolutional networks as a backbone for dense\nprediction tasks. We assemble tokens from various stages of the vision\ntransformer into image-like representations at various resolutions and\nprogressively combine them into full-resolution predictions using a\nconvolutional decoder. The transformer backbone processes representations at a\nconstant and relatively high resolution and has a global receptive field at\nevery stage. These properties allow the dense vision transformer to provide\nfiner-grained and more globally coherent predictions when compared to\nfully-convolutional networks. Our experiments show that this architecture\nyields substantial improvements on dense prediction tasks, especially when a\nlarge amount of training data is available. For monocular depth estimation, we\nobserve an improvement of up to 28% in relative performance when compared to a\nstate-of-the-art fully-convolutional network. When applied to semantic\nsegmentation, dense vision transformers set a new state of the art on ADE20K\nwith 49.02% mIoU. We further show that the architecture can be fine-tuned on\nsmaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets\nthe new state of the art. Our models are available at\nhttps://github.com/intel-isl/DPT.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:01:17 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ranftl", "Ren\u00e9", ""], ["Bochkovskiy", "Alexey", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2103.13415", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman,\n  Ricardo Martin-Brualla, Pratul P. Srinivasan", "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance\n  Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rendering procedure used by neural radiance fields (NeRF) samples a scene\nwith a single ray per pixel and may therefore produce renderings that are\nexcessively blurred or aliased when training or testing images observe scene\ncontent at different resolutions. The straightforward solution of supersampling\nby rendering with multiple rays per pixel is impractical for NeRF, because\nrendering each ray requires querying a multilayer perceptron hundreds of times.\nOur solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to\nrepresent the scene at a continuously-valued scale. By efficiently rendering\nanti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable\naliasing artifacts and significantly improves NeRF's ability to represent fine\ndetails, while also being 7% faster than NeRF and half the size. Compared to\nNeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with\nNeRF and by 60% on a challenging multiscale variant of that dataset that we\npresent. Mip-NeRF is also able to match the accuracy of a brute-force\nsupersampled NeRF on our multiscale dataset while being 22x faster.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:02:11 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 22:17:41 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Barron", "Jonathan T.", ""], ["Mildenhall", "Ben", ""], ["Tancik", "Matthew", ""], ["Hedman", "Peter", ""], ["Martin-Brualla", "Ricardo", ""], ["Srinivasan", "Pratul P.", ""]]}, {"id": "2103.13423", "submitter": "Sebastian Lutz", "authors": "Sebastian Lutz, Aljosa Smolic", "title": "Foreground color prediction through inverse compositing", "comments": "To be published in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural image matting, the goal is to estimate the opacity of the\nforeground object in the image. This opacity controls the way the foreground\nand background is blended in transparent regions. In recent years, advances in\ndeep learning have led to many natural image matting algorithms that have\nachieved outstanding performance in a fully automatic manner. However, most of\nthese algorithms only predict the alpha matte from the image, which is not\nsufficient to create high-quality compositions. Further, it is not possible to\nmanually interact with these algorithms in any way except by directly changing\ntheir input or output. We propose a novel recurrent neural network that can be\nused as a post-processing method to recover the foreground and background\ncolors of an image, given an initial alpha estimation. Our method outperforms\nthe state-of-the-art in color estimation for natural image matting and show\nthat the recurrent nature of our method allows users to easily change candidate\nsolutions that lead to superior color estimations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:10:15 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lutz", "Sebastian", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2103.13425", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Xiangyu Zhang, Jungong Han, Guiguang Ding", "title": "Diverse Branch Block: Building a Convolution as an Inception-like Unit", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a universal building block of Convolutional Neural Network\n(ConvNet) to improve the performance without any inference-time costs. The\nblock is named Diverse Branch Block (DBB), which enhances the representational\ncapacity of a single convolution by combining diverse branches of different\nscales and complexities to enrich the feature space, including sequences of\nconvolutions, multi-scale convolutions, and average pooling. After training, a\nDBB can be equivalently converted into a single conv layer for deployment.\nUnlike the advancements of novel ConvNet architectures, DBB complicates the\ntraining-time microstructure while maintaining the macro architecture, so that\nit can be used as a drop-in replacement for regular conv layers of any\narchitecture. In this way, the model can be trained to reach a higher level of\nperformance and then transformed into the original inference-time structure for\ninference. DBB improves ConvNets on image classification (up to 1.9% higher\ntop-1 accuracy on ImageNet), object detection and semantic segmentation. The\nPyTorch code and models are released at\nhttps://github.com/DingXiaoH/DiverseBranchBlock.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:12:00 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 13:00:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ding", "Xiaohan", ""], ["Zhang", "Xiangyu", ""], ["Han", "Jungong", ""], ["Ding", "Guiguang", ""]]}, {"id": "2103.13428", "submitter": "Songtao He", "authors": "Songtao He, Favyen Bastani, Mohammad Alizadeh, Hari Balakrishnan,\n  Michael Cafarella, Tim Kraska, Sam Madden", "title": "TagMe: GPS-Assisted Automatic Object Annotation in Videos", "comments": "https://people.csail.mit.edu/songtao/tagme.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training high-accuracy object detection models requires large and diverse\nannotated datasets. However, creating these data-sets is time-consuming and\nexpensive since it relies on human annotators. We design, implement, and\nevaluate TagMe, a new approach for automatic object annotation in videos that\nuses GPS data. When the GPS trace of an object is available, TagMe matches the\nobject's motion from GPS trace and the pixels' motions in the video to find the\npixels belonging to the object in the video and creates the bounding box\nannotations of the object. TagMe works using passive data collection and can\ncontinuously generate new object annotations from outdoor video streams without\nany human annotators. We evaluate TagMe on a dataset of 100 video clips. We\nshow TagMe can produce high-quality object annotations in a fully-automatic and\nlow-cost way. Compared with the traditional human-in-the-loop solution, TagMe\ncan produce the same amount of annotations at a much lower cost, e.g., up to\n110x.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:15:32 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["He", "Songtao", ""], ["Bastani", "Favyen", ""], ["Alizadeh", "Mohammad", ""], ["Balakrishnan", "Hari", ""], ["Cafarella", "Michael", ""], ["Kraska", "Tim", ""], ["Madden", "Sam", ""]]}, {"id": "2103.13430", "submitter": "Faraz Lotfi Dr", "authors": "Faraz Lotfi, Hamid D. Taghirad", "title": "A Framework for 3D Tracking of Frontal Dynamic Objects in Autonomous\n  Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both recognition and 3D tracking of frontal dynamic objects are crucial\nproblems in an autonomous vehicle, while depth estimation as an essential issue\nbecomes a challenging problem using a monocular camera. Since both camera and\nobjects are moving, the issue can be formed as a structure from motion (SFM)\nproblem. In this paper, to elicit features from an image, the YOLOv3 approach\nis utilized beside an OpenCV tracker. Subsequently, to obtain the lateral and\nlongitudinal distances, a nonlinear SFM model is considered alongside a\nstate-dependent Riccati equation (SDRE) filter and a newly developed\nobservation model. Additionally, a switching method in the form of switching\nestimation error covariance is proposed to enhance the robust performance of\nthe SDRE filter. The stability analysis of the presented filter is conducted on\na class of discrete nonlinear systems. Furthermore, the ultimate bound of\nestimation error caused by model uncertainties is analytically obtained to\ninvestigate the switching significance. Simulations are reported to validate\nthe performance of the switched SDRE filter. Finally, real-time experiments are\nperformed through a multi-thread framework implemented on a Jetson TX2 board,\nwhile radar data is used for the evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:21:29 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lotfi", "Faraz", ""], ["Taghirad", "Hamid D.", ""]]}, {"id": "2103.13447", "submitter": "Seunghun Lee", "authors": "Seunghun Lee, Sunghyun Cho, Sunghoon Im", "title": "DRANet: Disentangling Representation and Adaptation Networks for\n  Unsupervised Cross-Domain Adaptation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present DRANet, a network architecture that disentangles\nimage representations and transfers the visual attributes in a latent space for\nunsupervised cross-domain adaptation. Unlike the existing domain adaptation\nmethods that learn associated features sharing a domain, DRANet preserves the\ndistinctiveness of each domain's characteristics. Our model encodes individual\nrepresentations of content (scene structure) and style (artistic appearance)\nfrom both source and target images. Then, it adapts the domain by incorporating\nthe transferred style factor into the content factor along with learnable\nweights specified for each domain. This learning framework allows\nbi-/multi-directional domain adaptation with a single encoder-decoder network\nand aligns their domain shift. Additionally, we propose a content-adaptive\ndomain transfer module that helps retain scene structure while transferring\nstyle. Extensive experiments show our model successfully separates\ncontent-style factors and synthesizes visually pleasing domain-transferred\nimages. The proposed method demonstrates state-of-the-art performance on\nstandard digit classification tasks as well as semantic segmentation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:54:23 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 07:14:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lee", "Seunghun", ""], ["Cho", "Sunghyun", ""], ["Im", "Sunghoon", ""]]}, {"id": "2103.13455", "submitter": "Chandan Singh", "authors": "Chandan Singh, Guha Balakrishnan, Pietro Perona", "title": "Matched sample selection with GANs for mitigating attribute confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring biases of vision systems with respect to protected attributes like\ngender and age is critical as these systems gain widespread use in society.\nHowever, significant correlations between attributes in benchmark datasets make\nit difficult to separate algorithmic bias from dataset bias. To mitigate such\nattribute confounding during bias analysis, we propose a matching approach that\nselects a subset of images from the full dataset with balanced attribute\ndistributions across protected attributes. Our matching approach first projects\nreal images onto a generative adversarial network (GAN)'s latent space in a\nmanner that preserves semantic attributes. It then finds image matches in this\nlatent space across a chosen protected attribute, yielding a dataset where\nsemantic and perceptual attributes are balanced across the protected attribute.\nWe validate projection and matching strategies with qualitative, quantitative,\nand human annotation experiments. We demonstrate our work in the context of\ngender bias in multiple open-source facial-recognition classifiers and find\nthat bias persists after removing key confounders via matching. Code and\ndocumentation to reproduce the results here and apply the methods to new data\nis available at https://github.com/csinva/matching-with-gans .\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 19:18:44 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Singh", "Chandan", ""], ["Balakrishnan", "Guha", ""], ["Perona", "Pietro", ""]]}, {"id": "2103.13477", "submitter": "Zijian Kuang", "authors": "Zijian Kuang and Xinran Tie", "title": "A Survey of Multimedia Technologies and Robust Algorithms", "comments": "arXiv admin note: text overlap with arXiv:2010.12968", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia technologies are now more practical and deployable in real life,\nand the algorithms are widely used in various researching areas such as deep\nlearning, signal processing, haptics, computer vision, robotics, and medical\nmultimedia processing. This survey provides an overview of multimedia\ntechnologies and robust algorithms in multimedia data processing, medical\nmultimedia processing, human facial expression tracking and pose recognition,\nand multimedia in education and training. This survey will also analyze and\npropose a future research direction based on the overview of current robust\nalgorithms and multimedia technologies. We want to thank the research and\nprevious work done by the Multimedia Research Centre (MRC), the University of\nAlberta, which is the inspiration and starting point for future research.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 20:52:23 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 02:49:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kuang", "Zijian", ""], ["Tie", "Xinran", ""]]}, {"id": "2103.13482", "submitter": "Kang Zheng", "authors": "Kang Zheng, Yirui Wang, Xiaoyun Zhou, Fakai Wang, Le Lu, Chihung Lin,\n  Lingyun Huang, Guotong Xie, Jing Xiao, Chang-Fu Kuo, Shun Miao", "title": "Semi-Supervised Learning for Bone Mineral Density Estimation in Hip\n  X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bone mineral density (BMD) is a clinically critical indicator of\nosteoporosis, usually measured by dual-energy X-ray absorptiometry (DEXA). Due\nto the limited accessibility of DEXA machines and examinations, osteoporosis is\noften under-diagnosed and under-treated, leading to increased fragility\nfracture risks. Thus it is highly desirable to obtain BMDs with alternative\ncost-effective and more accessible medical imaging examinations such as X-ray\nplain films. In this work, we formulate the BMD estimation from plain hip X-ray\nimages as a regression problem. Specifically, we propose a new semi-supervised\nself-training algorithm to train the BMD regression model using images coupled\nwith DEXA measured BMDs and unlabeled images with pseudo BMDs. Pseudo BMDs are\ngenerated and refined iteratively for unlabeled images during self-training. We\nalso present a novel adaptive triplet loss to improve the model's regression\naccuracy. On an in-house dataset of 1,090 images (819 unique patients), our BMD\nestimation method achieves a high Pearson correlation coefficient of 0.8805 to\nground-truth BMDs. It offers good feasibility to use the more accessible and\ncheaper X-ray imaging for opportunistic osteoporosis screening.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 20:59:54 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 19:27:29 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zheng", "Kang", ""], ["Wang", "Yirui", ""], ["Zhou", "Xiaoyun", ""], ["Wang", "Fakai", ""], ["Lu", "Le", ""], ["Lin", "Chihung", ""], ["Huang", "Lingyun", ""], ["Xie", "Guotong", ""], ["Xiao", "Jing", ""], ["Kuo", "Chang-Fu", ""], ["Miao", "Shun", ""]]}, {"id": "2103.13497", "submitter": "Alex Chang Mr.", "authors": "Alex Chang, Vinith Suriyakumar, Abhishek Moturu, James Tu, Nipaporn\n  Tewattanarat, Sayali Joshi, Andrea Doria and Anna Goldenberg", "title": "3D Reasoning for Unsupervised Anomaly Detection in Pediatric WbMRI", "comments": "10 pages, 2 tables, 3 figures, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern deep unsupervised learning methods have shown great promise for\ndetecting diseases across a variety of medical imaging modalities. While\nprevious generative modeling approaches successfully perform anomaly detection\nby learning the distribution of healthy 2D image slices, they process such\nslices independently and ignore the fact that they are correlated, all being\nsampled from a 3D volume. We show that incorporating the 3D context and\nprocessing whole-body MRI volumes is beneficial to distinguishing anomalies\nfrom their benign counterparts. In our work, we introduce a multi-channel\nsliding window generative model to perform lesion detection in whole-body MRI\n(wbMRI). Our experiments demonstrate that our proposed method significantly\noutperforms processing individual images in isolation and our ablations clearly\nshow the importance of 3D reasoning. Moreover, our work also shows that it is\nbeneficial to include additional patient-specific features to further improve\nanomaly detection in pediatric scans.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 21:37:01 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Chang", "Alex", ""], ["Suriyakumar", "Vinith", ""], ["Moturu", "Abhishek", ""], ["Tu", "James", ""], ["Tewattanarat", "Nipaporn", ""], ["Joshi", "Sayali", ""], ["Doria", "Andrea", ""], ["Goldenberg", "Anna", ""]]}, {"id": "2103.13511", "submitter": "Praveer Singh", "authors": "Sharut Gupta, Praveer Singh, Ken Chang, Liangqiong Qu, Mehak Aggarwal,\n  Nishanth Arun, Ashwin Vaswani, Shruti Raghavan, Vibha Agarwal, Mishka\n  Gidwani, Katharina Hoebel, Jay Patel, Charles Lu, Christopher P. Bridge,\n  Daniel L. Rubin, Jayashree Kalpathy-Cramer", "title": "Addressing catastrophic forgetting for medical domain expansion", "comments": "First three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model brittleness is a key concern when deploying deep learning models in\nreal-world medical settings. A model that has high performance at one\ninstitution may suffer a significant decline in performance when tested at\nother institutions. While pooling datasets from multiple institutions and\nretraining may provide a straightforward solution, it is often infeasible and\nmay compromise patient privacy. An alternative approach is to fine-tune the\nmodel on subsequent institutions after training on the original institution.\nNotably, this approach degrades model performance at the original institution,\na phenomenon known as catastrophic forgetting. In this paper, we develop an\napproach to address catastrophic forget-ting based on elastic weight\nconsolidation combined with modulation of batch normalization statistics under\ntwo scenarios: first, for expanding the domain from one imaging system's data\nto another imaging system's, and second, for expanding the domain from a large\nmulti-institutional dataset to another single institution dataset. We show that\nour approach outperforms several other state-of-the-art approaches and provide\ntheoretical justification for the efficacy of batch normalization modulation.\nThe results of this study are generally applicable to the deployment of any\nclinical deep learning model which requires domain expansion.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 22:33:38 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gupta", "Sharut", ""], ["Singh", "Praveer", ""], ["Chang", "Ken", ""], ["Qu", "Liangqiong", ""], ["Aggarwal", "Mehak", ""], ["Arun", "Nishanth", ""], ["Vaswani", "Ashwin", ""], ["Raghavan", "Shruti", ""], ["Agarwal", "Vibha", ""], ["Gidwani", "Mishka", ""], ["Hoebel", "Katharina", ""], ["Patel", "Jay", ""], ["Lu", "Charles", ""], ["Bridge", "Christopher P.", ""], ["Rubin", "Daniel L.", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "2103.13512", "submitter": "Frank Guerin", "authors": "Frank Guerin", "title": "Projection: A Mechanism for Human-like Reasoning in Artificial\n  Intelligence", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial Intelligence systems cannot yet match human abilities to apply\nknowledge to situations that vary from what they have been programmed for, or\ntrained for. In visual object recognition methods of inference exploiting\ntop-down information (from a model) have been shown to be effective for\nrecognising entities in difficult conditions. Here this type of inference,\ncalled `projection', is shown to be a key mechanism to solve the problem of\napplying knowledge to varied or challenging situations, across a range of AI\ndomains, such as vision, robotics, or language. Finally the relevance of\nprojection to tackling the commonsense knowledge problem is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 22:33:51 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Guerin", "Frank", ""]]}, {"id": "2103.13516", "submitter": "Ramana Subramanyam Sundararaman", "authors": "Ramana Sundararaman, Cedric De Almeida Braga, Eric Marchand, Julien\n  Pettre", "title": "Tracking Pedestrian Heads in Dense Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tracking humans in crowded video sequences is an important constituent of\nvisual scene understanding. Increasing crowd density challenges visibility of\nhumans, limiting the scalability of existing pedestrian trackers to higher\ncrowd densities. For that reason, we propose to revitalize head tracking with\nCrowd of Heads Dataset (CroHD), consisting of 9 sequences of 11,463 frames with\nover 2,276,838 heads and 5,230 tracks annotated in diverse scenes. For\nevaluation, we proposed a new metric, IDEucl, to measure an algorithm's\nefficacy in preserving a unique identity for the longest stretch in image\ncoordinate space, thus building a correspondence between pedestrian crowd\nmotion and the performance of a tracking algorithm. Moreover, we also propose a\nnew head detector, HeadHunter, which is designed for small head detection in\ncrowded scenes. We extend HeadHunter with a Particle Filter and a color\nhistogram based re-identification module for head tracking. To establish this\nas a strong baseline, we compare our tracker with existing state-of-the-art\npedestrian trackers on CroHD and demonstrate superiority, especially in\nidentity preserving tracking metrics. With a light-weight head detector and a\ntracker which is efficient at identity preservation, we believe our\ncontributions will serve useful in advancement of pedestrian tracking in dense\ncrowds.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 22:51:17 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Sundararaman", "Ramana", ""], ["Braga", "Cedric De Almeida", ""], ["Marchand", "Eric", ""], ["Pettre", "Julien", ""]]}, {"id": "2103.13517", "submitter": "Ashraful Islam", "authors": "Ashraful Islam, Chun-Fu Chen, Rameswar Panda, Leonid Karlinsky,\n  Richard Radke, Rogerio Feris", "title": "A Broad Study on the Transferability of Visual Representations with\n  Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tremendous progress has been made in visual representation learning, notably\nwith the recent success of self-supervised contrastive learning methods.\nSupervised contrastive learning has also been shown to outperform its\ncross-entropy counterparts by leveraging labels for choosing where to contrast.\nHowever, there has been little work to explore the transfer capability of\ncontrastive learning to a different domain. In this paper, we conduct a\ncomprehensive study on the transferability of learned representations of\ndifferent contrastive approaches for linear evaluation, full-network transfer,\nand few-shot recognition on 12 downstream datasets from different domains, and\nobject detection tasks on MSCOCO and VOC0712. The results show that the\ncontrastive approaches learn representations that are easily transferable to a\ndifferent downstream task. We further observe that the joint objective of\nself-supervised contrastive loss with cross-entropy/supervised-contrastive loss\nleads to better transferability of these models over their supervised\ncounterparts. Our analysis reveals that the representations learned from the\ncontrastive approaches contain more low/mid-level semantics than cross-entropy\nmodels, which enables them to quickly adapt to a new task. Our codes and models\nwill be publicly available to facilitate future research on transferability of\nvisual representations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 22:55:04 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 21:44:16 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Islam", "Ashraful", ""], ["Chen", "Chun-Fu", ""], ["Panda", "Rameswar", ""], ["Karlinsky", "Leonid", ""], ["Radke", "Richard", ""], ["Feris", "Rogerio", ""]]}, {"id": "2103.13538", "submitter": "Zhibo Yang", "authors": "Zhibo Yang, Muhammet Bastan, Xinliang Zhu, Doug Gray, Dimitris Samaras", "title": "Hierarchical Proxy-based Loss for Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Proxy-based metric learning losses are superior to pair-based losses due to\ntheir fast convergence and low training complexity. However, existing\nproxy-based losses focus on learning class-discriminative features while\noverlooking the commonalities shared across classes which are potentially\nuseful in describing and matching samples. Moreover, they ignore the implicit\nhierarchy of categories in real-world datasets, where similar subordinate\nclasses can be grouped together. In this paper, we present a framework that\nleverages this implicit hierarchy by imposing a hierarchical structure on the\nproxies and can be used with any existing proxy-based loss. This allows our\nmodel to capture both class-discriminative features and class-shared\ncharacteristics without breaking the implicit data hierarchy. We evaluate our\nmethod on five established image retrieval datasets such as In-Shop and SOP.\nResults demonstrate that our hierarchical proxy-based loss framework improves\nthe performance of existing proxy-based losses, especially on large datasets\nwhich exhibit strong hierarchical structure.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 00:38:33 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yang", "Zhibo", ""], ["Bastan", "Muhammet", ""], ["Zhu", "Xinliang", ""], ["Gray", "Doug", ""], ["Samaras", "Dimitris", ""]]}, {"id": "2103.13544", "submitter": "Zheng Tong", "authors": "Zheng Tong, Philippe Xu, Thierry Den{\\oe}ux", "title": "Evidential fully convolutional network for semantic segmentation", "comments": "34 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid architecture composed of a fully convolutional network\n(FCN) and a Dempster-Shafer layer for image semantic segmentation. In the\nso-called evidential FCN (E-FCN), an encoder-decoder architecture first\nextracts pixel-wise feature maps from an input image. A Dempster-Shafer layer\nthen computes mass functions at each pixel location based on distances to\nprototypes. Finally, a utility layer performs semantic segmentation from mass\nfunctions and allows for imprecise classification of ambiguous pixels and\noutliers. We propose an end-to-end learning strategy for jointly updating the\nnetwork parameters, which can make use of soft (imprecise) labels. Experiments\nusing three databases (Pascal VOC 2011, MIT-scene Parsing and SIFT Flow) show\nthat the proposed combination improves the accuracy and calibration of semantic\nsegmentation by assigning confusing pixels to multi-class sets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:21:22 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Tong", "Zheng", ""], ["Xu", "Philippe", ""], ["Den\u0153ux", "Thierry", ""]]}, {"id": "2103.13557", "submitter": "Jiajin Zhang", "authors": "Jiajin Zhang, Hanqing Chao, Xuanang Xu, Chuang Niu, Ge Wang and\n  Pingkun Yan", "title": "Task-Oriented Low-Dose CT Image Denoising", "comments": "Paper accepted by MICCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extensive use of medical CT has raised a public concern over the\nradiation dose to the patient. Reducing the radiation dose leads to increased\nCT image noise and artifacts, which can adversely affect not only the\nradiologists judgement but also the performance of downstream medical image\nanalysis tasks. Various low-dose CT denoising methods, especially the recent\ndeep learning based approaches, have produced impressive results. However, the\nexisting denoising methods are all downstream-task-agnostic and neglect the\ndiverse needs of the downstream applications. In this paper, we introduce a\nnovel Task-Oriented Denoising Network (TOD-Net) with a task-oriented loss\nleveraging knowledge from the downstream tasks. Comprehensive empirical\nanalysis shows that the task-oriented loss complements other task agnostic\nlosses by steering the denoiser to enhance the image quality in the task\nrelated regions of interest. Such enhancement in turn brings general boosts on\nthe performance of various methods for the downstream task. The presented work\nmay shed light on the future development of context-aware image denoising\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:47:55 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 15:24:17 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Jiajin", ""], ["Chao", "Hanqing", ""], ["Xu", "Xuanang", ""], ["Niu", "Chuang", ""], ["Wang", "Ge", ""], ["Yan", "Pingkun", ""]]}, {"id": "2103.13558", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Kevin J Liang, Nikhil Mehta, Piyush Rai, Lawrence\n  Carin", "title": "Efficient Feature Transformations for Discriminative and Generative\n  Continual Learning", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As neural networks are increasingly being applied to real-world applications,\nmechanisms to address distributional shift and sequential task learning without\nforgetting are critical. Methods incorporating network expansion have shown\npromise by naturally adding model capacity for learning new tasks while\nsimultaneously avoiding catastrophic forgetting. However, the growth in the\nnumber of additional parameters of many of these types of methods can be\ncomputationally expensive at larger scales, at times prohibitively so. Instead,\nwe propose a simple task-specific feature map transformation strategy for\ncontinual learning, which we call Efficient Feature Transformations (EFTs).\nThese EFTs provide powerful flexibility for learning new tasks, achieved with\nminimal parameters added to the base architecture. We further propose a feature\ndistance maximization strategy, which significantly improves task prediction in\nclass incremental settings, without needing expensive generative models. We\ndemonstrate the efficacy and efficiency of our method with an extensive set of\nexperiments in discriminative (CIFAR-100 and ImageNet-1K) and generative (LSUN,\nCUB-200, Cats) sequences of tasks. Even with low single-digit parameter growth\nrates, EFTs can outperform many other continual learning methods in a wide\nrange of settings.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:48:14 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Liang", "Kevin J", ""], ["Mehta", "Nikhil", ""], ["Rai", "Piyush", ""], ["Carin", "Lawrence", ""]]}, {"id": "2103.13559", "submitter": "Yun-Hao Cao", "authors": "Yun-Hao Cao and Jianxin Wu", "title": "Rethinking Self-Supervised Learning: Small is Beautiful", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning (SSL), in particular contrastive learning, has made\ngreat progress in recent years. However, a common theme in these methods is\nthat they inherit the learning paradigm from the supervised deep learning\nscenario. Current SSL methods are often pretrained for many epochs on\nlarge-scale datasets using high resolution images, which brings heavy\ncomputational cost and lacks flexibility. In this paper, we demonstrate that\nthe learning paradigm for SSL should be different from supervised learning and\nthe information encoded by the contrastive loss is expected to be much less\nthan that encoded in the labels in supervised learning via the cross entropy\nloss. Hence, we propose scaled-down self-supervised learning (S3L), which\ninclude 3 parts: small resolution, small architecture and small data. On a\ndiverse set of datasets, SSL methods and backbone architectures, S3L achieves\nhigher accuracy consistently with much less training cost when compared to\nprevious SSL learning paradigm. Furthermore, we show that even without a large\npretraining dataset, S3L can achieve impressive results on small data alone.\nOur code has been made publically available at\nhttps://github.com/CupidJay/Scaled-down-self-supervised-learning.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:48:52 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Cao", "Yun-Hao", ""], ["Wu", "Jianxin", ""]]}, {"id": "2103.13561", "submitter": "Kekai Sheng", "authors": "Kekai Sheng, Ke Li, Xiawu Zheng, Jian Liang, Weiming Dong, Feiyue\n  Huang, Rongrong Ji, Xing Sun", "title": "On Evolving Attention Towards Domain Adaptation", "comments": "Among the first to study arbitrary domain adaptation from the\n  perspective of network architecture design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Towards better unsupervised domain adaptation (UDA). Recently, researchers\npropose various domain-conditioned attention modules and make promising\nprogresses. However, considering that the configuration of attention, i.e., the\ntype and the position of attention module, affects the performance\nsignificantly, it is more generalized to optimize the attention configuration\nautomatically to be specialized for arbitrary UDA scenario. For the first time,\nthis paper proposes EvoADA: a novel framework to evolve the attention\nconfiguration for a given UDA task without human intervention. In particular,\nwe propose a novel search space containing diverse attention configurations.\nThen, to evaluate the attention configurations and make search procedure\nUDA-oriented (transferability + discrimination), we apply a simple and\neffective evaluation strategy: 1) training the network weights on two domains\nwith off-the-shelf domain adaptation methods; 2) evolving the attention\nconfigurations under the guide of the discriminative ability on the target\ndomain. Experiments on various kinds of cross-domain benchmarks, i.e.,\nOffice-31, Office-Home, CUB-Paintings, and Duke-Market-1510, reveal that the\nproposed EvoADA consistently boosts multiple state-of-the-art domain adaptation\napproaches, and the optimal attention configurations help them achieve better\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:50:28 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Sheng", "Kekai", ""], ["Li", "Ke", ""], ["Zheng", "Xiawu", ""], ["Liang", "Jian", ""], ["Dong", "Weiming", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""], ["Sun", "Xing", ""]]}, {"id": "2103.13567", "submitter": "Yiwen Guo", "authors": "Zhi Wang, Yiwen Guo, Wangmeng Zuo", "title": "Deepfake Forensics via An Adversarial Game", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the progress in AI-based facial forgery (i.e., deepfake), people are\nincreasingly concerned about its abuse. Albeit effort has been made for\ntraining classification (also known as deepfake detection) models to recognize\nsuch forgeries, existing models suffer from poor generalization to unseen\nforgery technologies and high sensitivity to changes in image/video quality. In\nthis paper, we advocate adversarial training for improving the generalization\nability to both unseen facial forgeries and unseen image/video qualities. We\nbelieve training with samples that are adversarially crafted to attack the\nclassification models improves the generalization ability considerably.\nConsidering that AI-based face manipulation often leads to high-frequency\nartifacts that can be easily spotted by models yet difficult to generalize, we\nfurther propose a new adversarial training method that attempts to blur out\nthese specific artifacts, by introducing pixel-wise Gaussian blurring models.\nWith adversarial training, the classification models are forced to learn more\ndiscriminative and generalizable features, and the effectiveness of our method\ncan be verified by plenty of empirical evidence. Our code will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 02:20:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Wang", "Zhi", ""], ["Guo", "Yiwen", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2103.13575", "submitter": "Guoqiang Wei", "authors": "Guoqiang Wei, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "title": "MetaAlign: Coordinating Domain Alignment and Classification for\n  Unsupervised Domain Adaptation", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For unsupervised domain adaptation (UDA), to alleviate the effect of domain\nshift, many approaches align the source and target domains in the feature space\nby adversarial learning or by explicitly aligning their statistics. However,\nthe optimization objective of such domain alignment is generally not\ncoordinated with that of the object classification task itself such that their\ndescent directions for optimization may be inconsistent. This will reduce the\neffectiveness of domain alignment in improving the performance of UDA. In this\npaper, we aim to study and alleviate the optimization inconsistency problem\nbetween the domain alignment and classification tasks. We address this by\nproposing an effective meta-optimization based strategy dubbed MetaAlign, where\nwe treat the domain alignment objective and the classification objective as the\nmeta-train and meta-test tasks in a meta-learning scheme. MetaAlign encourages\nboth tasks to be optimized in a coordinated way, which maximizes the inner\nproduct of the gradients of the two tasks during training. Experimental results\ndemonstrate the effectiveness of our proposed method on top of various\nalignment-based baseline approaches, for tasks of object classification and\nobject detection. MetaAlign helps achieve the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:16:05 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Wei", "Guoqiang", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2103.13578", "submitter": "Wentao Zhu", "authors": "Wentao Zhu and Yufang Huang and Daguang Xu and Zhen Qian and Wei Fan\n  and Xiaohui Xie", "title": "Test-Time Training for Deformable Multi-Scale Image Registration", "comments": "ICRA 2021; 8 pages, 4 figures, 2 big tables", "journal-ref": "ICRA 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Registration is a fundamental task in medical robotics and is often a crucial\nstep for many downstream tasks such as motion analysis, intra-operative\ntracking and image segmentation. Popular registration methods such as ANTs and\nNiftyReg optimize objective functions for each pair of images from scratch,\nwhich are time-consuming for 3D and sequential images with complex\ndeformations. Recently, deep learning-based registration approaches such as\nVoxelMorph have been emerging and achieve competitive performance. In this\nwork, we construct a test-time training for deep deformable image registration\nto improve the generalization ability of conventional learning-based\nregistration model. We design multi-scale deep networks to consecutively model\nthe residual deformations, which is effective for high variational\ndeformations. Extensive experiments validate the effectiveness of multi-scale\ndeep registration with test-time training based on Dice coefficient for image\nsegmentation and mean square error (MSE), normalized local cross-correlation\n(NLCC) for tissue dense tracking tasks. Two videos are in\nhttps://www.youtube.com/watch?v=NvLrCaqCiAE and\nhttps://www.youtube.com/watch?v=pEA6ZmtTNuQ\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:22:59 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhu", "Wentao", ""], ["Huang", "Yufang", ""], ["Xu", "Daguang", ""], ["Qian", "Zhen", ""], ["Fan", "Wei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2103.13580", "submitter": "Feng Lu", "authors": "Feng Lu, Baifan Chen, Xiang-Dong Zhou and Dezhen Song", "title": "STA-VPR: Spatio-temporal Alignment for Visual Place Recognition", "comments": "Accepted for publication in IEEE RA-L 2021", "journal-ref": "IEEE Robotics and Automation Letters, 2021", "doi": "10.1109/LRA.2021.3067623", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the methods based on Convolutional Neural Networks (CNNs) have\ngained popularity in the field of visual place recognition (VPR). In\nparticular, the features from the middle layers of CNNs are more robust to\ndrastic appearance changes than handcrafted features and high-layer features.\nUnfortunately, the holistic mid-layer features lack robustness to large\nviewpoint changes. Here we split the holistic mid-layer features into local\nfeatures, and propose an adaptive dynamic time warping (DTW) algorithm to align\nlocal features from the spatial domain while measuring the distance between two\nimages. This realizes viewpoint-invariant and condition-invariant place\nrecognition. Meanwhile, a local matching DTW (LM-DTW) algorithm is applied to\nperform image sequence matching based on temporal alignment, which achieves\nfurther improvements and ensures linear time complexity. We perform extensive\nexperiments on five representative VPR datasets. The results show that the\nproposed method significantly improves the CNN-based methods. Moreover, our\nmethod outperforms several state-of-the-art methods while maintaining good\nrun-time performance. This work provides a novel way to boost the performance\nof CNN methods without any re-training for VPR. The code is available at\nhttps://github.com/Lu-Feng/STA-VPR.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:27:42 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 09:00:03 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Lu", "Feng", ""], ["Chen", "Baifan", ""], ["Zhou", "Xiang-Dong", ""], ["Song", "Dezhen", ""]]}, {"id": "2103.13582", "submitter": "Chengming Xu", "authors": "Chengming Xu, Chen Liu, Li Zhang, Chengjie Wang, Jilin Li, Feiyue\n  Huang, Xiangyang Xue, Yanwei Fu", "title": "Learning Dynamic Alignment via Meta-filter for Few-shot Learning", "comments": "accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning (FSL), which aims to recognise new classes by adapting the\nlearned knowledge with extremely limited few-shot (support) examples, remains\nan important open problem in computer vision. Most of the existing methods for\nfeature alignment in few-shot learning only consider image-level or\nspatial-level alignment while omitting the channel disparity. Our insight is\nthat these methods would lead to poor adaptation with redundant matching, and\nleveraging channel-wise adjustment is the key to well adapting the learned\nknowledge to new classes. Therefore, in this paper, we propose to learn a\ndynamic alignment, which can effectively highlight both query regions and\nchannels according to different local support information. Specifically, this\nis achieved by first dynamically sampling the neighbourhood of the feature\nposition conditioned on the input few shot, based on which we further predict a\nboth position-dependent and channel-dependent Dynamic Meta-filter. The filter\nis used to align the query feature with position-specific and channel-specific\nknowledge. Moreover, we adopt Neural Ordinary Differential Equation (ODE) to\nenable a more accurate control of the alignment. In such a sense our model is\nable to better capture fine-grained semantic context of the few-shot example\nand thus facilitates dynamical knowledge adaptation for few-shot learning. The\nresulting framework establishes the new state-of-the-arts on major few-shot\nvisual recognition benchmarks, including miniImageNet and tieredImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:29:33 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Xu", "Chengming", ""], ["Liu", "Chen", ""], ["Zhang", "Li", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Xue", "Xiangyang", ""], ["Fu", "Yanwei", ""]]}, {"id": "2103.13588", "submitter": "Tonghe Wang", "authors": "Mingquan Lin, Jacob Wynne, Yang Lei, Tonghe Wang, Walter J. Curran,\n  Tian Liu, Xiaofeng Yang", "title": "Artificial Intelligence in Tumor Subregion Analysis Based on Medical\n  Imaging: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging is widely used in cancer diagnosis and treatment, and\nartificial intelligence (AI) has achieved tremendous success in various tasks\nof medical image analysis. This paper reviews AI-based tumor subregion analysis\nin medical imaging. We summarize the latest AI-based methods for tumor\nsubregion analysis and their applications. Specifically, we categorize the\nAI-based methods by training strategy: supervised and unsupervised. A detailed\nreview of each category is presented, highlighting important contributions and\nachievements. Specific challenges and potential AI applications in tumor\nsubregion analysis are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:41:21 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lin", "Mingquan", ""], ["Wynne", "Jacob", ""], ["Lei", "Yang", ""], ["Wang", "Tonghe", ""], ["Curran", "Walter J.", ""], ["Liu", "Tian", ""], ["Yang", "Xiaofeng", ""]]}, {"id": "2103.13598", "submitter": "Yiwen Guo", "authors": "Yiwen Guo, Changshui Zhang", "title": "Recent Advances in Large Margin Learning", "comments": "Accepted by TPAMI, 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper serves as a survey of recent advances in large margin training and\nits theoretical foundations, mostly for (nonlinear) deep neural networks (DNNs)\nthat are probably the most prominent machine learning models for large-scale\ndata in the community over the past decade. We generalize the formulation of\nclassification margins from classical research to latest DNNs, summarize\ntheoretical connections between the margin, network generalization, and\nrobustness, and introduce recent efforts in enlarging the margins for DNNs\ncomprehensively. Since the viewpoint of different methods is discrepant, we\ncategorize them into groups for ease of comparison and discussion in the paper.\nHopefully, our discussions and overview inspire new research work in the\ncommunity that aim to improve the performance of DNNs, and we also point to\ndirections where the large margin principle can be verified to provide\ntheoretical evidence why certain regularizations for DNNs function well in\npractice. We managed to shorten the paper such that the crucial spirit of large\nmargin learning and related methods are better emphasized.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 04:12:00 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 05:41:45 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Guo", "Yiwen", ""], ["Zhang", "Changshui", ""]]}, {"id": "2103.13607", "submitter": "Gautam Gare", "authors": "Gautam Rajendrakumar Gare and John Michael Galeotti", "title": "Exploiting Class Similarity for Machine Learning with Confidence Labels\n  and Projective Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class labels used for machine learning are relatable to each other, with\ncertain class labels being more similar to each other than others (e.g. images\nof cats and dogs are more similar to each other than those of cats and cars).\nSuch similarity among classes is often the cause of poor model performance due\nto the models confusing between them. Current labeling techniques fail to\nexplicitly capture such similarity information. In this paper, we instead\nexploit the similarity between classes by capturing the similarity information\nwith our novel confidence labels. Confidence labels are probabilistic labels\ndenoting the likelihood of similarity, or confusability, between the classes.\nOften even after models are trained to differentiate between classes in the\nfeature space, the similar classes' latent space still remains clustered. We\nview this type of clustering as valuable information and exploit it with our\nnovel projective loss functions. Our projective loss functions are designed to\nwork with confidence labels with an ability to relax the loss penalty for\nerrors that confuse similar classes. We use our approach to train neural\nnetworks with noisy labels, as we believe noisy labels are partly a result of\nconfusability arising from class similarity. We show improved performance\ncompared to the use of standard loss functions. We conduct a detailed analysis\nusing the CIFAR-10 dataset and show our proposed methods' applicability to\nlarger datasets, such as ImageNet and Food-101N.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 04:49:44 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gare", "Gautam Rajendrakumar", ""], ["Galeotti", "John Michael", ""]]}, {"id": "2103.13612", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Tom Goldstein, Larry S. Davis, Ser-Nam Lim", "title": "THAT: Two Head Adversarial Training for Improving Robustness at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many variants of adversarial training have been proposed, with most research\nfocusing on problems with relatively few classes. In this paper, we propose Two\nHead Adversarial Training (THAT), a two-stream adversarial learning network\nthat is designed to handle the large-scale many-class ImageNet dataset. The\nproposed method trains a network with two heads and two loss functions; one to\nminimize feature-space domain shift between natural and adversarial images, and\none to promote high classification accuracy. This combination delivers a\nhardened network that achieves state of the art robust accuracy while\nmaintaining high natural accuracy on ImageNet. Through extensive experiments,\nwe demonstrate that the proposed framework outperforms alternative methods\nunder both standard and \"free\" adversarial training settings.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 05:32:38 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Wu", "Zuxuan", ""], ["Goldstein", "Tom", ""], ["Davis", "Larry S.", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2103.13613", "submitter": "Jinrong Yang", "authors": "Shengkai Wu, Jinrong Yang, Hangcheng Yu, Lijun Gou, Xiaoping Li", "title": "Gaussian Guided IoU: A Better Metric for Balanced Learning on Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most of the anchor-based detectors, Intersection over Union(IoU) is\nwidely utilized to assign targets for the anchors during training. However, IoU\npays insufficient attention to the closeness of the anchor's center to the\ntruth box's center. This results in two problems: (1) only one anchor is\nassigned to most of the slender objects which leads to insufficient supervision\ninformation for the slender objects during training and the performance on the\nslender objects is hurt; (2) IoU can not accurately represent the alignment\ndegree between the receptive field of the feature at the anchor's center and\nthe object. Thus during training, some features whose receptive field aligns\nbetter with objects are missing while some features whose receptive field\naligns worse with objects are adopted. This hurts the localization accuracy of\nmodels. To solve these problems, we firstly design Gaussian Guided IoU(GGIoU)\nwhich focuses more attention on the closeness of the anchor's center to the\ntruth box's center. Then we propose GGIoU-balanced learning method including\nGGIoU-guided assignment strategy and GGIoU-balanced localization loss. The\nmethod can assign multiple anchors for each slender object and bias the\ntraining process to the features well-aligned with objects. Extensive\nexperiments on the popular benchmarks such as PASCAL VOC and MS COCO\ndemonstrate GGIoU-balanced learning can solve the above problems and\nsubstantially improve the performance of the object detection model, especially\nin the localization accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 05:36:55 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Wu", "Shengkai", ""], ["Yang", "Jinrong", ""], ["Yu", "Hangcheng", ""], ["Gou", "Lijun", ""], ["Li", "Xiaoping", ""]]}, {"id": "2103.13622", "submitter": "Muyi Sun", "authors": "Muyi Sun, Guanhong Zhang", "title": "Contextual Information Enhanced Convolutional Neural Networks for\n  Retinal Vessel Segmentation in Color Fundus Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate retinal vessel segmentation is a challenging problem in color fundus\nimage analysis. An automatic retinal vessel segmentation system can effectively\nfacilitate clinical diagnosis and ophthalmological research. Technically, this\nproblem suffers from various degrees of vessel thickness, perception of\ndetails, and contextual feature fusion. For addressing these challenges, a deep\nlearning based method has been proposed and several customized modules have\nbeen integrated into the well-known encoder-decoder architecture U-net, which\nis mainly employed in medical image segmentation. Structurally, cascaded\ndilated convolutional modules have been integrated into the intermediate\nlayers, for obtaining larger receptive field and generating denser encoded\nfeature maps. Also, the advantages of the pyramid module with spatial\ncontinuity have been taken, for multi-thickness perception, detail refinement,\nand contextual feature fusion. Additionally, the effectiveness of different\nnormalization approaches has been discussed in network training for different\ndatasets with specific properties. Experimentally, sufficient comparative\nexperiments have been enforced on three retinal vessel segmentation datasets,\nDRIVE, CHASEDB1, and the unhealthy dataset STARE. As a result, the proposed\nmethod outperforms the work of predecessors and achieves state-of-the-art\nperformance in Sensitivity/Recall, F1-score and MCC.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 06:10:47 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Sun", "Muyi", ""], ["Zhang", "Guanhong", ""]]}, {"id": "2103.13629", "submitter": "Wanhua Li", "authors": "Wanhua Li, Xiaoke Huang, Jiwen Lu, Jianjiang Feng, Jie Zhou", "title": "Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware\n  Regression", "comments": "Accepted by CVPR2021. Code is available at\n  https://github.com/Li-Wanhua/POEs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty is the only certainty there is. Modeling data uncertainty is\nessential for regression, especially in unconstrained settings. Traditionally\nthe direct regression formulation is considered and the uncertainty is modeled\nby modifying the output space to a certain family of probabilistic\ndistributions. On the other hand, classification based regression and ranking\nbased solutions are more popular in practice while the direct regression\nmethods suffer from the limited performance. How to model the uncertainty\nwithin the present-day technologies for regression remains an open issue. In\nthis paper, we propose to learn probabilistic ordinal embeddings which\nrepresent each data as a multivariate Gaussian distribution rather than a\ndeterministic point in the latent space. An ordinal distribution constraint is\nproposed to exploit the ordinal nature of regression. Our probabilistic ordinal\nembeddings can be integrated into popular regression approaches and empower\nthem with the ability of uncertainty estimation. Experimental results show that\nour approach achieves competitive performance. Code is available at\nhttps://github.com/Li-Wanhua/POEs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 06:56:09 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Li", "Wanhua", ""], ["Huang", "Xiaoke", ""], ["Lu", "Jiwen", ""], ["Feng", "Jianjiang", ""], ["Zhou", "Jie", ""]]}, {"id": "2103.13630", "submitter": "Amir Gholami", "authors": "Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney,\n  Kurt Keutzer", "title": "A Survey of Quantization Methods for Efficient Neural Network Inference", "comments": "Book Chapter: Low-Power Computer Vision: Improving the Efficiency of\n  Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As soon as abstract mathematical computations were adapted to computation on\ndigital computers, the problem of efficient representation, manipulation, and\ncommunication of the numerical values in those computations arose. Strongly\nrelated to the problem of numerical representation is the problem of\nquantization: in what manner should a set of continuous real-valued numbers be\ndistributed over a fixed discrete set of numbers to minimize the number of bits\nrequired and also to maximize the accuracy of the attendant computations? This\nperennial problem of quantization is particularly relevant whenever memory\nand/or computational resources are severely restricted, and it has come to the\nforefront in recent years due to the remarkable performance of Neural Network\nmodels in computer vision, natural language processing, and related areas.\nMoving from floating-point representations to low-precision fixed integer\nvalues represented in four bits or less holds the potential to reduce the\nmemory footprint and latency by a factor of 16x; and, in fact, reductions of 4x\nto 8x are often realized in practice in these applications. Thus, it is not\nsurprising that quantization has emerged recently as an important and very\nactive sub-area of research in the efficient implementation of computations\nassociated with Neural Networks. In this article, we survey approaches to the\nproblem of quantizing the numerical values in deep Neural Network computations,\ncovering the advantages/disadvantages of current methods. With this survey and\nits organization, we hope to have presented a useful snapshot of the current\nresearch in quantization for Neural Networks and to have given an intelligent\norganization to ease the evaluation of future research in this area.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 06:57:11 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 23:59:25 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 21:01:12 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gholami", "Amir", ""], ["Kim", "Sehoon", ""], ["Dong", "Zhen", ""], ["Yao", "Zhewei", ""], ["Mahoney", "Michael W.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2103.13634", "submitter": "Chunwei Tian", "authors": "Chunwei Tian, Yong Xu, Wangmeng Zuo, Chia-Wen Lin and David Zhang", "title": "Asymmetric CNN for image super-resolution", "comments": "Blind Super-resolution; Blind Super-resolution with unknown noise", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep convolutional neural networks (CNNs) have been widely applied for\nlow-level vision over the past five years. According to nature of different\napplications, designing appropriate CNN architectures is developed. However,\ncustomized architectures gather different features via treating all pixel\npoints as equal to improve the performance of given application, which ignores\nthe effects of local power pixel points and results in low training efficiency.\nIn this paper, we propose an asymmetric CNN (ACNet) comprising an asymmetric\nblock (AB), a memory enhancement block (MEB) and a high-frequency feature\nenhancement block (HFFEB) for image super-resolution. The AB utilizes\none-dimensional asymmetric convolutions to intensify the square convolution\nkernels in horizontal and vertical directions for promoting the influences of\nlocal salient features for SISR. The MEB fuses all hierarchical low-frequency\nfeatures from the AB via residual learning (RL) technique to resolve the\nlong-term dependency problem and transforms obtained low-frequency features\ninto high-frequency features. The HFFEB exploits low- and high-frequency\nfeatures to obtain more robust super-resolution features and address excessive\nfeature enhancement problem. Addditionally, it also takes charge of\nreconstructing a high-resolution (HR) image. Extensive experiments show that\nour ACNet can effectively address single image super-resolution (SISR), blind\nSISR and blind SISR of blind noise problems. The code of the ACNet is shown at\nhttps://github.com/hellloxiaotian/ACNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 07:10:46 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 06:18:56 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Tian", "Chunwei", ""], ["Xu", "Yong", ""], ["Zuo", "Wangmeng", ""], ["Lin", "Chia-Wen", ""], ["Zhang", "David", ""]]}, {"id": "2103.13646", "submitter": "Evgenii Zheltonozhskii", "authors": "Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex M.\n  Bronstein, Or Litany", "title": "Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of learning with noisy labels (LNL) methods relies heavily on the\nsuccess of a warm-up stage where standard supervised training is performed\nusing the full (noisy) training set. In this paper, we identify a \"warm-up\nobstacle\": the inability of standard warm-up stages to train high quality\nfeature extractors and avert memorization of noisy labels. We propose \"Contrast\nto Divide\" (C2D), a simple framework that solves this problem by pre-training\nthe feature extractor in a self-supervised fashion. Using self-supervised\npre-training boosts the performance of existing LNL approaches by drastically\nreducing the warm-up stage's susceptibility to noise level, shortening its\nduration, and increasing extracted feature quality. C2D works out of the box\nwith existing methods and demonstrates markedly improved performance,\nespecially in the high noise regime, where we get a boost of more than 27% for\nCIFAR-100 with 90% noise over the previous state of the art. In real-life noise\nsettings, C2D trained on mini-WebVision outperforms previous works both in\nWebVision and ImageNet validation sets by 3% top-1 accuracy. We perform an\nin-depth analysis of the framework, including investigating the performance of\ndifferent pre-training approaches and estimating the effective upper bound of\nthe LNL performance with semi-supervised learning. Code for reproducing our\nexperiments is available at https://github.com/ContrastToDivide/C2D\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 07:40:51 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zheltonozhskii", "Evgenii", ""], ["Baskin", "Chaim", ""], ["Mendelson", "Avi", ""], ["Bronstein", "Alex M.", ""], ["Litany", "Or", ""]]}, {"id": "2103.13660", "submitter": "Chang Yi", "authors": "Yuntong Ye, Yi Chang, Hanyu Zhou, Luxin Yan", "title": "Closing the Loop: Joint Rain Generation and Removal via Disentangled\n  Image Translation", "comments": "10 pages, Accepted by 2021 CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing deep learning-based image deraining methods have achieved promising\nperformance for synthetic rainy images, typically rely on the pairs of sharp\nimages and simulated rainy counterparts. However, these methods suffer from\nsignificant performance drop when facing the real rain, because of the huge gap\nbetween the simplified synthetic rain and the complex real rain. In this work,\nwe argue that the rain generation and removal are the two sides of the same\ncoin and should be tightly coupled. To close the loop, we propose to jointly\nlearn real rain generation and removal procedure within a unified disentangled\nimage translation framework. Specifically, we propose a bidirectional\ndisentangled translation network, in which each unidirectional network contains\ntwo loops of joint rain generation and removal for both the real and synthetic\nrain image, respectively. Meanwhile, we enforce the disentanglement strategy by\ndecomposing the rainy image into a clean background and rain layer (rain\nremoval), in order to better preserve the identity background via both the\ncycle-consistency loss and adversarial loss, and ease the rain layer\ntranslating between the real and synthetic rainy image. A counterpart\ncomposition with the entanglement strategy is symmetrically applied for rain\ngeneration. Extensive experiments on synthetic and real-world rain datasets\nshow the superiority of proposed method compared to state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 08:21:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ye", "Yuntong", ""], ["Chang", "Yi", ""], ["Zhou", "Hanyu", ""], ["Yan", "Luxin", ""]]}, {"id": "2103.13674", "submitter": "Seung-Hun Nam", "authors": "Minseok Yoon, Seung-Hun Nam, In-Jae Yu, Wonhyuk Ahn, Myung-Joon Kwon,\n  Heung-Kyu Lee", "title": "Frame-rate Up-conversion Detection Based on Convolutional Neural Network\n  for Learning Spatiotemporal Features", "comments": "preprint; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance in user-friendly and powerful video editing tools, anyone\ncan easily manipulate videos without leaving prominent visual traces.\nFrame-rate up-conversion (FRUC), a representative temporal-domain operation,\nincreases the motion continuity of videos with a lower frame-rate and is used\nby malicious counterfeiters in video tampering such as generating fake\nframe-rate video without improving the quality or mixing temporally spliced\nvideos. FRUC is based on frame interpolation schemes and subtle artifacts that\nremain in interpolated frames are often difficult to distinguish. Hence,\ndetecting such forgery traces is a critical issue in video forensics. This\npaper proposes a frame-rate conversion detection network (FCDNet) that learns\nforensic features caused by FRUC in an end-to-end fashion. The proposed network\nuses a stack of consecutive frames as the input and effectively learns\ninterpolation artifacts using network blocks to learn spatiotemporal features.\nThis study is the first attempt to apply a neural network to the detection of\nFRUC. Moreover, it can cover the following three types of frame interpolation\nschemes: nearest neighbor interpolation, bilinear interpolation, and\nmotion-compensated interpolation. In contrast to existing methods that exploit\nall frames to verify integrity, the proposed approach achieves a high detection\nspeed because it observes only six frames to test its authenticity. Extensive\nexperiments were conducted with conventional forensic methods and neural\nnetworks for video forensic tasks to validate our research. The proposed\nnetwork achieved state-of-the-art performance in terms of detecting the\ninterpolated artifacts of FRUC. The experimental results also demonstrate that\nour trained model is robust for an unseen dataset, unlearned frame-rate, and\nunlearned quality factor.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 08:47:46 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yoon", "Minseok", ""], ["Nam", "Seung-Hun", ""], ["Yu", "In-Jae", ""], ["Ahn", "Wonhyuk", ""], ["Kwon", "Myung-Joon", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "2103.13676", "submitter": "Guangwei Gao", "authors": "Guangwei Gao, Lei Tang, Yi Yu, Fei Wu, Huimin Lu, Jian Yang", "title": "JDSR-GAN: Constructing A Joint and Collaborative Learning Network for\n  Masked Face Super-Resolution", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing importance of preventing the COVID-19 virus, face images\nobtained in most video surveillance scenarios are low resolution with mask\nsimultaneously. However, most of the previous face super-resolution solutions\ncan not handle both tasks in one model. In this work, we treat the mask\nocclusion as image noise and construct a joint and collaborative learning\nnetwork, called JDSR-GAN, for the masked face super-resolution task. Given a\nlow-quality face image with the mask as input, the role of the generator\ncomposed of a denoising module and super-resolution module is to acquire a\nhigh-quality high-resolution face image. The discriminator utilizes some\ncarefully designed loss functions to ensure the quality of the recovered face\nimages. Moreover, we incorporate the identity information and attention\nmechanism into our network for feasible correlated feature expression and\ninformative feature learning. By jointly performing denoising and face\nsuper-resolution, the two tasks can complement each other and attain promising\nperformance. Extensive qualitative and quantitative results show the\nsuperiority of our proposed JDSR-GAN over some comparable methods which perform\nthe previous two tasks separately.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 08:50:40 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gao", "Guangwei", ""], ["Tang", "Lei", ""], ["Yu", "Yi", ""], ["Wu", "Fei", ""], ["Lu", "Huimin", ""], ["Yang", "Jian", ""]]}, {"id": "2103.13677", "submitter": "Tal Shaharabany", "authors": "Ameen Ali, Tal Shaharabany, Lior Wolf", "title": "Explainability Guided Multi-Site COVID-19 CT Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiologist examination of chest CT is an effective way for screening\nCOVID-19 cases. In this work, we overcome three challenges in the automation of\nthis process: (i) the limited number of supervised positive cases, (ii) the\nlack of region-based supervision, and (iii) the variability across acquisition\nsites. These challenges are met by incorporating a recent augmentation solution\ncalled SnapMix, by a new patch embedding technique, and by performing a\ntest-time stability analysis. The three techniques are complementary and are\nall based on utilizing the heatmaps produced by the Class Activation Mapping\n(CAM) explainability method. Compared to the current state of the art, we\nobtain an increase of five percent in the F1 score on a site with a relatively\nhigh number of cases, and a gap twice as large for a site with much fewer\ntraining images.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 08:56:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ali", "Ameen", ""], ["Shaharabany", "Tal", ""], ["Wolf", "Lior", ""]]}, {"id": "2103.13684", "submitter": "Peidong Liu Mr.", "authors": "Peidong Liu, Xingxing Zuo, Viktor Larsson and Marc Pollefeys", "title": "MBA-VO: Motion Blur Aware Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motion blur is one of the major challenges remaining for visual odometry\nmethods. In low-light conditions where longer exposure times are necessary,\nmotion blur can appear even for relatively slow camera motions. In this paper\nwe present a novel hybrid visual odometry pipeline with direct approach that\nexplicitly models and estimates the camera's local trajectory within the\nexposure time. This allows us to actively compensate for any motion blur that\noccurs due to the camera motion. In addition, we also contribute a novel\nbenchmarking dataset for motion blur aware visual odometry. In experiments we\nshow that by directly modeling the image formation process, we are able to\nimprove robustness of the visual odometry, while keeping comparable accuracy as\nthat for images without motion blur.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:02:56 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Liu", "Peidong", ""], ["Zuo", "Xingxing", ""], ["Larsson", "Viktor", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2103.13689", "submitter": "Shunquan Tan", "authors": "Xianbo Mo and Shunquan Tan and Bin Li and Jiwu Huang", "title": "MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning\n  Framework for Universal Non-additive Steganography", "comments": "submitted to TIFS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:12:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Mo", "Xianbo", ""], ["Tan", "Shunquan", ""], ["Li", "Bin", ""], ["Huang", "Jiwu", ""]]}, {"id": "2103.13696", "submitter": "Phi Vu Tran", "authors": "Phi Vu Tran", "title": "SSLayout360: Semi-Supervised Indoor Layout Estimation from 360-Degree\n  Panorama", "comments": "CVPR 2021. File size 37MB. Project page at\n  https://github.com/FlyreelAI/sslayout360", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent years have seen flourishing research on both semi-supervised learning\nand 3D room layout reconstruction. In this work, we explore the intersection of\nthese two fields to advance the research objective of enabling more accurate 3D\nindoor scene modeling with less labeled data. We propose the first approach to\nlearn representations of room corners and boundaries by using a combination of\nlabeled and unlabeled data for improved layout estimation in a 360-degree\npanoramic scene. Through extensive comparative experiments, we demonstrate that\nour approach can advance layout estimation of complex indoor scenes using as\nfew as 20 labeled examples. When coupled with a layout predictor pre-trained on\nsynthetic data, our semi-supervised method matches the fully supervised\ncounterpart using only 12% of the labels. Our work takes an important first\nstep towards robust semi-supervised layout estimation that can enable many\napplications in 3D perception with limited labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:19:13 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 06:22:43 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 02:13:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tran", "Phi Vu", ""]]}, {"id": "2103.13701", "submitter": "Frederik Hvilsh{\\o}j", "authors": "Frederik Hvilsh{\\o}j, Alexandros Iosifidis, and Ira Assent", "title": "ECINN: Efficient Counterfactuals from Invertible Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual examples identify how inputs can be altered to change the\npredicted class of a classifier, thus opening up the black-box nature of, e.g.,\ndeep neural networks. We propose a method, ECINN, that utilizes the generative\ncapacities of invertible neural networks for image classification to generate\ncounterfactual examples efficiently. In contrast to competing methods that\nsometimes need a thousand evaluations or more of the classifier, ECINN has a\nclosed-form expression and generates a counterfactual in the time of only two\nevaluations. Arguably, the main challenge of generating counterfactual examples\nis to alter only input features that affect the predicted outcome, i.e.,\nclass-dependent features. Our experiments demonstrate how ECINN alters\nclass-dependent image regions to change the perceptual and predicted class of\nthe counterfactuals. Additionally, we extend ECINN to also produce heatmaps\n(ECINNh) for easy inspection of, e.g., pairwise class-dependent changes in the\ngenerated counterfactual examples. Experimentally, we find that ECINNh\noutperforms established methods that generate heatmap-based explanations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:23:24 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 18:55:56 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Hvilsh\u00f8j", "Frederik", ""], ["Iosifidis", "Alexandros", ""], ["Assent", "Ira", ""]]}, {"id": "2103.13710", "submitter": "Yanling Miao", "authors": "Yanling Miao, Qi Wang, Mulin Chen, Xuelong Li", "title": "Spatial-spectral Hyperspectral Image Classification via Multiple Random\n  Anchor Graphs Ensemble Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based semi-supervised learning methods, which deal well with the\nsituation of limited labeled data, have shown dominant performance in practical\napplications. However, the high dimensionality of hyperspectral images (HSI)\nmakes it hard to construct the pairwise adjacent graph. Besides, the fine\nspatial features that help improve the discriminability of the model are often\noverlooked. To handle the problems, this paper proposes a novel\nspatial-spectral HSI classification method via multiple random anchor graphs\nensemble learning (RAGE). Firstly, the local binary pattern is adopted to\nextract the more descriptive features on each selected band, which preserves\nlocal structures and subtle changes of a region. Secondly, the adaptive\nneighbors assignment is introduced in the construction of anchor graph, to\nreduce the computational complexity. Finally, an ensemble model is built by\nutilizing multiple anchor graphs, such that the diversity of HSI is learned.\nExtensive experiments show that RAGE is competitive against the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:31:41 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Miao", "Yanling", ""], ["Wang", "Qi", ""], ["Chen", "Mulin", ""], ["Li", "Xuelong", ""]]}, {"id": "2103.13716", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Yongxin Yang, Timothy M.\n  Hospedales, Tao Xiang, Yi-Zhe Song", "title": "Vectorization and Rasterization: Self-Supervised Learning for Sketch and\n  Handwriting", "comments": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021\n  Code : https://github.com/AyanKumarBhunia/Self-Supervised-Learning-for-Sketch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning has gained prominence due to its efficacy at\nlearning powerful representations from unlabelled data that achieve excellent\nperformance on many challenging downstream tasks. However supervision-free\npre-text tasks are challenging to design and usually modality specific.\nAlthough there is a rich literature of self-supervised methods for either\nspatial (such as images) or temporal data (sound or text) modalities, a common\npre-text task that benefits both modalities is largely missing. In this paper,\nwe are interested in defining a self-supervised pre-text task for sketches and\nhandwriting data. This data is uniquely characterised by its existence in dual\nmodalities of rasterized images and vector coordinate sequences. We address and\nexploit this dual representation by proposing two novel cross-modal translation\npre-text tasks for self-supervised feature learning: Vectorization and\nRasterization. Vectorization learns to map image space to vector coordinates\nand rasterization maps vector coordinates to image space. We show that the our\nlearned encoder modules benefit both raster-based and vector-based downstream\napproaches to analysing hand-drawn data. Empirical evidence shows that our\nnovel pre-text tasks surpass existing single and multi-modal self-supervision\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:47:18 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Chowdhury", "Pinaki Nath", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2103.13722", "submitter": "Stanislav Frolov", "authors": "Stanislav Frolov, Avneesh Sharma, J\\\"orn Hees, Tushar Karayil,\n  Federico Raue, Andreas Dengel", "title": "AttrLostGAN: Attribute Controlled Image Synthesis from Reconfigurable\n  Layout and Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional image synthesis from layout has recently attracted much interest.\nPrevious approaches condition the generator on object locations as well as\nclass labels but lack fine-grained control over the diverse appearance aspects\nof individual objects. Gaining control over the image generation process is\nfundamental to build practical applications with a user-friendly interface. In\nthis paper, we propose a method for attribute controlled image synthesis from\nlayout which allows to specify the appearance of individual objects without\naffecting the rest of the image. We extend a state-of-the-art approach for\nlayout-to-image generation to additionally condition individual objects on\nattributes. We create and experiment on a synthetic, as well as the challenging\nVisual Genome dataset. Our qualitative and quantitative results show that our\nmethod can successfully control the fine-grained details of individual objects\nwhen modelling complex scenes with multiple objects.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:09:45 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Frolov", "Stanislav", ""], ["Sharma", "Avneesh", ""], ["Hees", "J\u00f6rn", ""], ["Karayil", "Tushar", ""], ["Raue", "Federico", ""], ["Dengel", "Andreas", ""]]}, {"id": "2103.13725", "submitter": "Haipeng Li", "authors": "Haipeng Li and Kunming Luo and Shuaicheng Liu", "title": "GyroFlow: Gyroscope-Guided Unsupervised Optical Flow Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing optical flow methods are erroneous in challenging scenes, such as\nfog, rain, and night because the basic optical flow assumptions such as\nbrightness and gradient constancy are broken. To address this problem, we\npresent an unsupervised learning approach that fuses gyroscope into optical\nflow learning. Specifically, we first convert gyroscope readings into motion\nfields named gyro field. Then, we design a self-guided fusion module to fuse\nthe background motion extracted from the gyro field with the optical flow and\nguide the network to focus on motion details. To the best of our knowledge,\nthis is the first deep learning-based framework that fuses gyroscope data and\nimage content for optical flow learning. To validate our method, we propose a\nnew dataset that covers regular and challenging scenes. Experiments show that\nour method outperforms the state-of-art methods in both regular and challenging\nscenes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:14:57 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Li", "Haipeng", ""], ["Luo", "Kunming", ""], ["Liu", "Shuaicheng", ""]]}, {"id": "2103.13733", "submitter": "Zhi Yuan Wu", "authors": "Zhiyuan Wu, Yu Jiang, Chupeng Cui, Zongmin Yang, Xinhui Xue, Hong Qi", "title": "Spirit Distillation: Precise Real-time Semantic Segmentation of Road\n  Scenes with Insufficient Data", "comments": "12 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of road scenes is one of the key technologies for\nrealizing autonomous driving scene perception, and the effectiveness of deep\nConvolutional Neural Networks(CNNs) for this task has been demonstrated.\nState-of-art CNNs for semantic segmentation suffer from excessive computations\nas well as large-scale training data requirement. Inspired by the ideas of\nFine-tuning-based Transfer Learning (FTT) and feature-based knowledge\ndistillation, we propose a new knowledge distillation method for cross-domain\nknowledge transference and efficient data-insufficient network training, named\nSpirit Distillation(SD), which allow the student network to mimic the teacher\nnetwork to extract general features, so that a compact and accurate student\nnetwork can be trained for real-time semantic segmentation of road scenes.\nThen, in order to further alleviate the trouble of insufficient data and\nimprove the robustness of the student, an Enhanced Spirit Distillation (ESD)\nmethod is proposed, which commits to exploit a more comprehensive general\nfeatures extraction capability by considering images from both the target and\nthe proximity domains as input. To our knowledge, this paper is a pioneering\nwork on the application of knowledge distillation to few-shot learning.\nPersuasive experiments conducted on Cityscapes semantic segmentation with the\nprior knowledge transferred from COCO2017 and KITTI demonstrate that our\nmethods can train a better student network (mIOU and high-precision accuracy\nboost by 1.4% and 8.2% respectively, with 78.2% segmentation variance) with\nonly 41.8% FLOPs (see Fig. 1).\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:23:30 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 00:40:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Zhiyuan", ""], ["Jiang", "Yu", ""], ["Cui", "Chupeng", ""], ["Yang", "Zongmin", ""], ["Xue", "Xinhui", ""], ["Qi", "Hong", ""]]}, {"id": "2103.13744", "submitter": "Christian Reiser", "authors": "Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger", "title": "KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  NeRF synthesizes novel views of a scene with unprecedented quality by fitting\na neural radiance field to RGB images. However, NeRF requires querying a deep\nMulti-Layer Perceptron (MLP) millions of times, leading to slow rendering\ntimes, even on modern GPUs. In this paper, we demonstrate that significant\nspeed-ups are possible by utilizing thousands of tiny MLPs instead of one\nsingle large MLP. In our setting, each individual MLP only needs to represent\nparts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By\ncombining this divide-and-conquer strategy with further optimizations,\nrendering is accelerated by two orders of magnitude compared to the original\nNeRF model without incurring high storage costs. Further, using teacher-student\ndistillation for training, we show that this speed-up can be achieved without\nsacrificing visual quality.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:53:05 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Reiser", "Christian", ""], ["Peng", "Songyou", ""], ["Liao", "Yiyi", ""], ["Geiger", "Andreas", ""]]}, {"id": "2103.13746", "submitter": "Huaijia Lin", "authors": "Huaijia Lin, Ruizheng Wu, Shu Liu, Jiangbo Lu, Jiaya Jia", "title": "Video Instance Segmentation with a Propose-Reduce Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video instance segmentation (VIS) aims to segment and associate all instances\nof predefined classes for each frame in videos. Prior methods usually obtain\nsegmentation for a frame or clip first, and then merge the incomplete results\nby tracking or matching. These methods may cause error accumulation in the\nmerging step. Contrarily, we propose a new paradigm -- Propose-Reduce, to\ngenerate complete sequences for input videos by a single step. We further build\na sequence propagation head on the existing image-level instance segmentation\nnetwork for long-term propagation. To ensure robustness and high recall of our\nproposed framework, multiple sequences are proposed where redundant sequences\nof the same instance are reduced. We achieve state-of-the-art performance on\ntwo representative benchmark datasets -- we obtain 47.6% in terms of AP on\nYouTube-VIS validation set and 70.4% for J&F on DAVIS-UVOS validation set.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:58:36 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lin", "Huaijia", ""], ["Wu", "Ruizheng", ""], ["Liu", "Shu", ""], ["Lu", "Jiangbo", ""], ["Jia", "Jiaya", ""]]}, {"id": "2103.13757", "submitter": "Chaoqi Chen", "authors": "Chaoqi Chen, Zebiao Zheng, Yue Huang, Xinghao Ding, Yizhou Yu", "title": "I3Net: Implicit Instance-Invariant Network for Adapting One-Stage Object\n  Detectors", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent works on two-stage cross-domain detection have widely explored the\nlocal feature patterns to achieve more accurate adaptation results. These\nmethods heavily rely on the region proposal mechanisms and ROI-based\ninstance-level features to design fine-grained feature alignment modules with\nrespect to the foreground objects. However, for one-stage detectors, it is hard\nor even impossible to obtain explicit instance-level features in the detection\npipelines. Motivated by this, we propose an Implicit Instance-Invariant Network\n(I3Net), which is tailored for adapting one-stage detectors and implicitly\nlearns instance-invariant features via exploiting the natural characteristics\nof deep features in different layers. Specifically, we facilitate the\nadaptation from three aspects: (1) Dynamic and Class-Balanced Reweighting\n(DCBR) strategy, which considers the coexistence of intra-domain and\nintra-class variations to assign larger weights to those sample-scarce\ncategories and easy-to-adapt samples; (2) Category-aware Object Pattern\nMatching (COPM) module, which boosts the cross-domain foreground objects\nmatching guided by the categorical information and suppresses the uninformative\nbackground features; (3) Regularized Joint Category Alignment (RJCA) module,\nwhich jointly enforces the category alignment at different domain-specific\nlayers with a consistency regularization. Experiments reveal that I3Net exceeds\nthe state-of-the-art performance on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 11:14:36 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 08:12:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Chaoqi", ""], ["Zheng", "Zebiao", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""], ["Yu", "Yizhou", ""]]}, {"id": "2103.13767", "submitter": "Gregory Vaksman", "authors": "Gregory Vaksman, Michael Elad and Peyman Milanfar", "title": "Patch Craft: Video Denoising by Deep Modeling and Patch Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-local self-similarity property of natural images has been exploited\nextensively for solving various image processing problems. When it comes to\nvideo sequences, harnessing this force is even more beneficial due to the\ntemporal redundancy. In the context of image and video denoising, many\nclassically-oriented algorithms employ self-similarity, splitting the data into\noverlapping patches, gathering groups of similar ones and processing these\ntogether somehow. With the emergence of convolutional neural networks (CNN),\nthe patch-based framework has been abandoned. Most CNN denoisers operate on the\nwhole image, leveraging non-local relations only implicitly by using a large\nreceptive field. This work proposes a novel approach for leveraging\nself-similarity in the context of video denoising, while still relying on a\nregular convolutional architecture. We introduce a concept of patch-craft\nframes - artificial frames that are similar to the real ones, built by tiling\nmatched patches. Our algorithm augments video sequences with patch-craft frames\nand feeds them to a CNN. We demonstrate the substantial boost in denoising\nperformance obtained with the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 11:45:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Vaksman", "Gregory", ""], ["Elad", "Michael", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2103.13778", "submitter": "Kireeti Bodduna", "authors": "Kireeti Bodduna and Joachim Weickert, Marcelo C\\'ardenas", "title": "Multi-frame Super-resolution from Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining high resolution images from low resolution data with clipped noise\nis algorithmically challenging due to the ill-posed nature of the problem. So\nfar such problems have hardly been tackled, and the few existing approaches use\nsimplistic regularisers. We show the usefulness of two adaptive regularisers\nbased on anisotropic diffusion ideas: Apart from evaluating the classical\nedge-enhancing anisotropic diffusion regulariser, we introduce a novel\nnon-local one with one-sided differences and superior performance. It is termed\nsector diffusion. We combine it with all six variants of the classical\nsuper-resolution observational model that arise from permutations of its three\noperators for warping, blurring, and downsampling. Surprisingly, the evaluation\nin a practically relevant noisy scenario produces a different ranking than the\none in the noise-free setting in our previous work (SSVM 2017).\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 12:07:08 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Bodduna", "Kireeti", ""], ["Weickert", "Joachim", ""], ["C\u00e1rdenas", "Marcelo", ""]]}, {"id": "2103.13808", "submitter": "Florian Tschopp", "authors": "Dominic Streiff, Lukas Bernreiter, Florian Tschopp, Marius Fehr,\n  Roland Siegwart", "title": "3D3L: Deep Learned 3D Keypoint Detection and Description for LiDARs", "comments": "Accepted for IEEE International Conference on Robotics and\n  Automation, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of powerful, light-weight 3D LiDARs, they have become the\nhearth of many navigation and SLAM algorithms on various autonomous systems.\nPointcloud registration methods working with unstructured pointclouds such as\nICP are often computationally expensive or require a good initial guess.\nFurthermore, 3D feature-based registration methods have never quite reached the\nrobustness of 2D methods in visual SLAM. With the continuously increasing\nresolution of LiDAR range images, these 2D methods not only become applicable\nbut should exploit the illumination-independent modalities that come with it,\nsuch as depth and intensity. In visual SLAM, deep learned 2D features and\ndescriptors perform exceptionally well compared to traditional methods. In this\npublication, we use a state-of-the-art 2D feature network as a basis for 3D3L,\nexploiting both intensity and depth of LiDAR range images to extract powerful\n3D features. Our results show that these keypoints and descriptors extracted\nfrom LiDAR scan images outperform state-of-the-art on different benchmark\nmetrics and allow for robust scan-to-scan alignment as well as global\nlocalization.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:08:07 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 12:51:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Streiff", "Dominic", ""], ["Bernreiter", "Lukas", ""], ["Tschopp", "Florian", ""], ["Fehr", "Marius", ""], ["Siegwart", "Roland", ""]]}, {"id": "2103.13813", "submitter": "Adnan Siraj Rakin", "authors": "Adnan Siraj Rakin, Li Yang, Jingtao Li, Fan Yao, Chaitali Chakrabarti,\n  Yu Cao, Jae-sun Seo, and Deliang Fan", "title": "RA-BNN: Constructing Robust & Accurate Binary Neural Network to\n  Simultaneously Defend Adversarial Bit-Flip Attack and Improve Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently developed adversarial weight attack, a.k.a. bit-flip attack (BFA),\nhas shown enormous success in compromising Deep Neural Network (DNN)\nperformance with an extremely small amount of model parameter perturbation. To\ndefend against this threat, we propose RA-BNN that adopts a complete binary\n(i.e., for both weights and activation) neural network (BNN) to significantly\nimprove DNN model robustness (defined as the number of bit-flips required to\ndegrade the accuracy to as low as a random guess). However, such an aggressive\nlow bit-width model suffers from poor clean (i.e., no attack) inference\naccuracy. To counter this, we propose a novel and efficient two-stage network\ngrowing method, named Early-Growth. It selectively grows the channel size of\neach BNN layer based on channel-wise binary masks training with Gumbel-Sigmoid\nfunction. Apart from recovering the inference accuracy, our RA-BNN after\ngrowing also shows significantly higher resistance to BFA. Our evaluation of\nthe CIFAR-10 dataset shows that the proposed RA-BNN can improve the clean model\naccuracy by ~2-8 %, compared with a baseline BNN, while simultaneously\nimproving the resistance to BFA by more than 125 x. Moreover, on ImageNet, with\na sufficiently large (e.g., 5,000) amount of bit-flips, the baseline BNN\naccuracy drops to 4.3 % from 51.9 %, while our RA-BNN accuracy only drops to\n37.1 % from 60.9 % (9 % clean accuracy improvement).\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:50:30 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["Yang", "Li", ""], ["Li", "Jingtao", ""], ["Yao", "Fan", ""], ["Chakrabarti", "Chaitali", ""], ["Cao", "Yu", ""], ["Seo", "Jae-sun", ""], ["Fan", "Deliang", ""]]}, {"id": "2103.13833", "submitter": "Erdi \\c{C}alli", "authors": "Erdi \\c{C}all{\\i}, Keelin Murphy, Steef Kurstjens, Tijs Samson, Robert\n  Herpers, Henk Smits, Matthieu Rutten and Bram van Ginneken", "title": "Deep Learning with robustness to missing data: A novel approach to the\n  detection of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the current global pandemic and the limitations of the\nRT-PCR test, we propose a novel deep learning architecture, DFCN, (Denoising\nFully Connected Network) for the detection of COVID-19 using laboratory tests\nand chest x-rays. Since medical facilities around the world differ enormously\nin what laboratory tests or chest imaging may be available, DFCN is designed to\nbe robust to missing input data. An ablation study extensively evaluates the\nperformance benefits of the DFCN architecture as well as its robustness to\nmissing inputs. Data from 1088 patients with confirmed RT-PCR results are\nobtained from two independent medical facilities. The data collected includes\nresults from 27 laboratory tests and a chest x-ray scored by a deep learning\nnetwork. Training and test datasets are defined based on the source medical\nfacility. Data is made publicly available. The performance of DFCN in\npredicting the RT-PCR result is compared with 3 related architectures as well\nas a Random Forest baseline. All models are trained with varying levels of\nmasked input data to encourage robustness to missing inputs. Missing data is\nsimulated at test time by masking inputs randomly. Using area under the\nreceiver operating curve (AUC) as a metric, DFCN outperforms all other models\nwith statistical significance using random subsets of input data with 2-27\navailable inputs. When all 28 inputs are available DFCN obtains an AUC of\n0.924, higher than achieved by any other model. Furthermore, with clinically\nmeaningful subsets of parameters consisting of just 6 and 7 inputs\nrespectively, DFCN also achieves higher AUCs than any other model, with values\nof 0.909 and 0.919.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:21:53 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["\u00c7all\u0131", "Erdi", ""], ["Murphy", "Keelin", ""], ["Kurstjens", "Steef", ""], ["Samson", "Tijs", ""], ["Herpers", "Robert", ""], ["Smits", "Henk", ""], ["Rutten", "Matthieu", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2103.13841", "submitter": "Wei-Hong Li", "authors": "Wei-Hong Li, Xialei Liu, Hakan Bilen", "title": "Universal Representation Learning from Multiple Domains for Few-shot\n  Classification", "comments": "Code will be available at https://github.com/VICO-UoE/URL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we look at the problem of few-shot classification that aims to\nlearn a classifier for previously unseen classes and domains from few labeled\nsamples. Recent methods use adaptation networks for aligning their features to\nnew domains or select the relevant features from multiple domain-specific\nfeature extractors. In this work, we propose to learn a single set of universal\ndeep representations by distilling knowledge of multiple separately trained\nnetworks after co-aligning their features with the help of adapters and\ncentered kernel alignment. We show that the universal representations can be\nfurther refined for previously unseen domains by an efficient adaptation step\nin a similar spirit to distance learning methods. We rigorously evaluate our\nmodel in the recent Meta-Dataset benchmark and demonstrate that it\nsignificantly outperforms the previous methods while being more efficient. Our\ncode will be available at https://github.com/VICO-UoE/URL.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:49:12 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Li", "Wei-Hong", ""], ["Liu", "Xialei", ""], ["Bilen", "Hakan", ""]]}, {"id": "2103.13843", "submitter": "Yang Tan", "authors": "Yang Tan, Yang Li, Shao-Lun Huang", "title": "OTCE: A Transferability Metric for Cross-Domain Cross-Task\n  Representations", "comments": "13 pages, accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transfer learning across heterogeneous data distributions (a.k.a. domains)\nand distinct tasks is a more general and challenging problem than conventional\ntransfer learning, where either domains or tasks are assumed to be the same.\nWhile neural network based feature transfer is widely used in transfer learning\napplications, finding the optimal transfer strategy still requires\ntime-consuming experiments and domain knowledge. We propose a transferability\nmetric called Optimal Transport based Conditional Entropy (OTCE), to\nanalytically predict the transfer performance for supervised classification\ntasks in such cross-domain and cross-task feature transfer settings. Our OTCE\nscore characterizes transferability as a combination of domain difference and\ntask difference, and explicitly evaluates them from data in a unified\nframework. Specifically, we use optimal transport to estimate domain difference\nand the optimal coupling between source and target distributions, which is then\nused to derive the conditional entropy of the target task (task difference).\nExperiments on the largest cross-domain dataset DomainNet and Office31\ndemonstrate that OTCE shows an average of 21% gain in the correlation with the\nground truth transfer accuracy compared to state-of-the-art methods. We also\ninvestigate two applications of the OTCE score including source model selection\nand multi-source feature fusion.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:51:33 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Tan", "Yang", ""], ["Li", "Yang", ""], ["Huang", "Shao-Lun", ""]]}, {"id": "2103.13851", "submitter": "Guangwei Gao", "authors": "Guangwei Gao, Yi Yu, Jian Yang, Guo-Jun Qi, Meng Yang", "title": "Hierarchical Deep CNN Feature Set-Based Representation Learning for\n  Robust Cross-Resolution Face Recognition", "comments": "IEEE Transactions on Circuits and Systems for Video Technology, 11\n  pages, 9 figures", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3042178", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-resolution face recognition (CRFR), which is important in intelligent\nsurveillance and biometric forensics, refers to the problem of matching a\nlow-resolution (LR) probe face image against high-resolution (HR) gallery face\nimages. Existing shallow learning-based and deep learning-based methods focus\non mapping the HR-LR face pairs into a joint feature space where the resolution\ndiscrepancy is mitigated. However, little works consider how to extract and\nutilize the intermediate discriminative features from the noisy LR query faces\nto further mitigate the resolution discrepancy due to the resolution\nlimitations. In this study, we desire to fully exploit the multi-level deep\nconvolutional neural network (CNN) feature set for robust CRFR. In particular,\nour contributions are threefold. (i) To learn more robust and discriminative\nfeatures, we desire to adaptively fuse the contextual features from different\nlayers. (ii) To fully exploit these contextual features, we design a feature\nset-based representation learning (FSRL) scheme to collaboratively represent\nthe hierarchical features for more accurate recognition. Moreover, FSRL\nutilizes the primitive form of feature maps to keep the latent structural\ninformation, especially in noisy cases. (iii) To further promote the\nrecognition performance, we desire to fuse the hierarchical recognition outputs\nfrom different stages. Meanwhile, the discriminability from different scales\ncan also be fully integrated. By exploiting these advantages, the efficiency of\nthe proposed method can be delivered. Experimental results on several face\ndatasets have verified the superiority of the presented algorithm to the other\ncompetitive CRFR approaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:03:42 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gao", "Guangwei", ""], ["Yu", "Yi", ""], ["Yang", "Jian", ""], ["Qi", "Guo-Jun", ""], ["Yang", "Meng", ""]]}, {"id": "2103.13858", "submitter": "Yuchen He", "authors": "Yuchen He, Yibing Chen, Hui Chen, Huaibin Zheng, Jianbin Liu, Shitao\n  Zhu and Zhuo Xu", "title": "Generative-Adversarial-Networks-based Ghost Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, target recognition technique plays an important role in many\nfields. However, the existing image information based methods suffer from the\ninfluence of target image quality. In addition, some methods also need image\nreconstruction, which will bring additional time cost. In this paper, we\npropose a novel coincidence recognition method combining ghost imaging (GI) and\ngenerative adversarial networks (GAN). Based on the mechanism of GI, a set of\nrandom speckles sequence is employed to illuminate target, and a bucket\ndetector without resolution is utilized to receive echo signal. The bucket\nsignal sequence formed after continuous detections is constructed into a bucket\nsignal array, which is regarded as the sample of GAN. Then, conditional GAN is\nused to map bucket signal array and target category. In practical application,\nthe speckles sequence in training step is still employed to illuminate target,\nand the bucket signal array is input GAN for recognition. The proposed method\ncan improve the problems caused by existing recognition methods that based on\nimage information, and provide a certain turbulence-free ability. Extensive\nexperiments are show that the proposed method achieves promising performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:15:56 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["He", "Yuchen", ""], ["Chen", "Yibing", ""], ["Chen", "Hui", ""], ["Zheng", "Huaibin", ""], ["Liu", "Jianbin", ""], ["Zhu", "Shitao", ""], ["Xu", "Zhuo", ""]]}, {"id": "2103.13859", "submitter": "Qing-Long Zhang", "authors": "Qinglong Zhang, Lu Rao, Yubin Yang", "title": "Group-CAM: Group Score-Weighted Visual Explanations for Deep\n  Convolutional Networks", "comments": "Group-CAM is an efficient region-based saliency method, which can be\n  used as an effective data augmentation trick", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an efficient saliency map generation method, called\nGroup score-weighted Class Activation Mapping (Group-CAM), which adopts the\n\"split-transform-merge\" strategy to generate saliency maps. Specifically, for\nan input image, the class activations are firstly split into groups. In each\ngroup, the sub-activations are summed and de-noised as an initial mask. After\nthat, the initial masks are transformed with meaningful perturbations and then\napplied to preserve sub-pixels of the input (i.e., masked inputs), which are\nthen fed into the network to calculate the confidence scores. Finally, the\ninitial masks are weighted summed to form the final saliency map, where the\nweights are confidence scores produced by the masked inputs. Group-CAM is\nefficient yet effective, which only requires dozens of queries to the network\nwhile producing target-related saliency maps. As a result, Group-CAM can be\nserved as an effective data augment trick for fine-tuning the networks. We\ncomprehensively evaluate the performance of Group-CAM on common-used\nbenchmarks, including deletion and insertion tests on ImageNet-1k, and pointing\ngame tests on COCO2017. Extensive experimental results demonstrate that\nGroup-CAM achieves better visual performance than the current state-of-the-art\nexplanation approaches. The code is available at\nhttps://github.com/wofmanaf/Group-CAM.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:16:02 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 08:56:42 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 14:51:34 GMT"}, {"version": "v4", "created": "Sat, 19 Jun 2021 09:40:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhang", "Qinglong", ""], ["Rao", "Lu", ""], ["Yang", "Yubin", ""]]}, {"id": "2103.13872", "submitter": "Rumeng Yi", "authors": "Rumeng Yi, Yaping Huang", "title": "Transform consistency for learning with noisy labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is crucial to distinguish mislabeled samples for dealing with noisy\nlabels. Previous methods such as Coteaching and JoCoR introduce two different\nnetworks to select clean samples out of the noisy ones and only use these clean\nones to train the deep models. Different from these methods which require to\ntrain two networks simultaneously, we propose a simple and effective method to\nidentify clean samples only using one single network. We discover that the\nclean samples prefer to reach consistent predictions for the original images\nand the transformed images while noisy samples usually suffer from inconsistent\npredictions. Motivated by this observation, we introduce to constrain the\ntransform consistency between the original images and the transformed images\nfor network training, and then select small-loss samples to update the\nparameters of the network. Furthermore, in order to mitigate the negative\ninfluence of noisy labels, we design a classification loss by using the\noff-line hard labels and on-line soft labels to provide more reliable\nsupervisions for training a robust model. We conduct comprehensive experiments\non CIFAR-10, CIFAR-100 and Clothing1M datasets. Compared with the baselines, we\nachieve the state-of-the-art performance. Especially, in most cases, our\nproposed method outperforms the baselines by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:33:13 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yi", "Rumeng", ""], ["Huang", "Yaping", ""]]}, {"id": "2103.13873", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Lorenzo Porzi, Samuel Rota Bul\\`o, Barbara\n  Caputo and Elisa Ricci", "title": "Inferring Latent Domains for Unsupervised Deep Domain Adaptation", "comments": "IEEE T-PAMI, https://ieeexplore.ieee.org/document/8792192", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2933829", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) refers to the problem of learning a\nmodel in a target domain where labeled data are not available by leveraging\ninformation from annotated data in a source domain. Most deep UDA approaches\noperate in a single-source, single-target scenario, i.e. they assume that the\nsource and the target samples arise from a single distribution. However, in\npractice most datasets can be regarded as mixtures of multiple domains. In\nthese cases, exploiting traditional single-source, single-target methods for\nlearning classification models may lead to poor results. Furthermore, it is\noften difficult to provide the domain labels for all data points, i.e. latent\ndomains should be automatically discovered. This paper introduces a novel deep\narchitecture which addresses the problem of UDA by automatically discovering\nlatent domains in visual datasets and exploiting this information to learn\nrobust target classifiers. Specifically, our architecture is based on two main\ncomponents, i.e. a side branch that automatically computes the assignment of\neach sample to its latent domain and novel layers that exploit domain\nmembership information to appropriately align the distribution of the CNN\ninternal feature representations to a reference distribution. We evaluate our\napproach on publicly available benchmarks, showing that it outperforms\nstate-of-the-art domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:33:33 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Porzi", "Lorenzo", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Caputo", "Barbara", ""], ["Ricci", "Elisa", ""]]}, {"id": "2103.13875", "submitter": "Daniel Barath", "authors": "Daniel Barath, Denys Rozumny, Ivan Eichhardt, Levente Hajder, Jiri\n  Matas", "title": "Progressive-X+: Clustering in the Consensus Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Progressive-X+, a new algorithm for finding an unknown number of\ngeometric models, e.g., homographies. The problem is formalized as finding\ndominant model instances progressively without forming crisp point-to-model\nassignments. Dominant instances are found via RANSAC-like sampling and a\nconsolidation process driven by a model quality function considering previously\nproposed instances. New ones are found by clustering in the consensus space.\nThis new formulation leads to a simple iterative algorithm with\nstate-of-the-art accuracy while running in real-time on a number of vision\nproblems. Also, we propose a sampler reflecting the fact that real-world data\ntend to form spatially coherent structures. The sampler returns connected\ncomponents in a progressively growing neighborhood-graph. We present a number\nof applications where the use of multiple geometric models improves accuracy.\nThese include using multiple homographies to estimate relative poses for global\nSfM; pose estimation from generalized homographies; and trajectory estimation\nof fast-moving objects.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:35:07 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Barath", "Daniel", ""], ["Rozumny", "Denys", ""], ["Eichhardt", "Ivan", ""], ["Hajder", "Levente", ""], ["Matas", "Jiri", ""]]}, {"id": "2103.13886", "submitter": "Xiangning Chen", "authors": "Xiangning Chen, Cihang Xie, Mingxing Tan, Li Zhang, Cho-Jui Hsieh,\n  Boqing Gong", "title": "Robust and Accurate Object Detection via Adversarial Learning", "comments": "CVPR 2021. Models are available at\n  https://github.com/google/automl/tree/master/efficientdet/Det-AdvProp.md", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has become a de facto component for training\nhigh-performance deep image classifiers, but its potential is under-explored\nfor object detection. Noting that most state-of-the-art object detectors\nbenefit from fine-tuning a pre-trained classifier, we first study how the\nclassifiers' gains from various data augmentations transfer to object\ndetection. The results are discouraging; the gains diminish after fine-tuning\nin terms of either accuracy or robustness. This work instead augments the\nfine-tuning stage for object detectors by exploring adversarial examples, which\ncan be viewed as a model-dependent data augmentation. Our method dynamically\nselects the stronger adversarial images sourced from a detector's\nclassification and localization branches and evolves with the detector to\nensure the augmentation policy stays current and relevant. This model-dependent\naugmentation generalizes to different object detectors better than AutoAugment,\na model-agnostic augmentation policy searched based on one particular detector.\nOur approach boosts the performance of state-of-the-art EfficientDets by +1.1\nmAP on the COCO object detection benchmark. It also improves the detectors'\nrobustness against natural distortions by +3.8 mAP and against domain shift by\n+1.3 mAP. Models are available at\nhttps://github.com/google/automl/tree/master/efficientdet/Det-AdvProp.md\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 19:45:26 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 17:34:31 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Xiangning", ""], ["Xie", "Cihang", ""], ["Tan", "Mingxing", ""], ["Zhang", "Li", ""], ["Hsieh", "Cho-Jui", ""], ["Gong", "Boqing", ""]]}, {"id": "2103.13894", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Elisa Ricci, Barbara Caputo and Samuel Rota\n  Bul\\'o", "title": "Boosting Binary Masks for Multi-Domain Learning through Affine\n  Transformations", "comments": "Accepted for publication by Machine Vision and Applications on May\n  21, 2020. arXiv admin note: substantial text overlap with arXiv:1805.11119", "journal-ref": null, "doi": "10.1007/S00138-020-01090-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new, algorithm for multi-domain learning. Given a\npretrained architecture and a set of visual domains received sequentially, the\ngoal of multi-domain learning is to produce a single model performing a task in\nall the domains together. Recent works showed how we can address this problem\nby masking the internal weights of a given original conv-net through learned\nbinary variables. In this work, we provide a general formulation of binary mask\nbased models for multi-domain learning by affine transformations of the\noriginal network parameters. Our formulation obtains significantly higher\nlevels of adaptation to new domains, achieving performances comparable to\ndomain-specific models while requiring slightly more than 1 bit per network\nparameter per additional domain. Experiments on two popular benchmarks showcase\nthe power of our approach, achieving performances close to state-of-the-art\nmethods on the Visual Decathlon Challenge.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:54:37 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Ricci", "Elisa", ""], ["Caputo", "Barbara", ""], ["Bul\u00f3", "Samuel Rota", ""]]}, {"id": "2103.13905", "submitter": "Julien Rebut", "authors": "Julien Rebut, Andrei Bursuc, and Patrick P\\'erez", "title": "StyleLess layer: Improving robustness for real-world driving", "comments": "6 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are a critical component for self-driving\nvehicles. They achieve impressive performance by reaping information from high\namounts of labeled data. Yet, the full complexity of the real world cannot be\nencapsulated in the training data, no matter how big the dataset, and DNNs can\nhardly generalize to unseen conditions. Robustness to various image\ncorruptions, caused by changing weather conditions or sensor degradation and\naging, is crucial for safety when such vehicles are deployed in the real world.\nWe address this problem through a novel type of layer, dubbed StyleLess, which\nenables DNNs to learn robust and informative features that can cope with\nvarying external conditions. We propose multiple variations of this layer that\ncan be integrated in most of the architectures and trained jointly with the\nmain task. We validate our contribution on typical autonomous-driving tasks\n(detection, semantic segmentation), showing that in most cases, this approach\nimproves predictive performance on unseen conditions (fog, rain), while\npreserving performance on seen conditions and objects.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:15:39 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Rebut", "Julien", ""], ["Bursuc", "Andrei", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "2103.13915", "submitter": "Gilad Sharir", "authors": "Gilad Sharir, Asaf Noy, Lihi Zelnik-Manor", "title": "An Image is Worth 16x16 Words, What is a Video Worth?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leading methods in the domain of action recognition try to distill\ninformation from both the spatial and temporal dimensions of an input video.\nMethods that reach State of the Art (SotA) accuracy, usually make use of 3D\nconvolution layers as a way to abstract the temporal information from video\nframes. The use of such convolutions requires sampling short clips from the\ninput video, where each clip is a collection of closely sampled frames. Since\neach short clip covers a small fraction of an input video, multiple clips are\nsampled at inference in order to cover the whole temporal length of the video.\nThis leads to increased computational load and is impractical for real-world\napplications. We address the computational bottleneck by significantly reducing\nthe number of frames required for inference. Our approach relies on a temporal\ntransformer that applies global attention over video frames, and thus better\nexploits the salient information in each frame. Therefore our approach is very\ninput efficient, and can achieve SotA results (on Kinetics dataset) with a\nfraction of the data (frames per video), computation and latency. Specifically\non Kinetics-400, we reach $80.5$ top-1 accuracy with $\\times 30$ less frames\nper video, and $\\times 40$ faster inference than the current leading method.\nCode is available at: https://github.com/Alibaba-MIIL/STAM\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:25:17 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 13:17:38 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Sharir", "Gilad", ""], ["Noy", "Asaf", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "2103.13917", "submitter": "Zhizheng Zhang", "authors": "Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Quanzeng You, Zicheng Liu,\n  Kecheng Zheng, Zhibo Chen", "title": "Disentanglement-based Cross-Domain Feature Augmentation for Effective\n  Unsupervised Domain Adaptive Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptive (UDA) person re-identification (ReID) aims to\ntransfer the knowledge from the labeled source domain to the unlabeled target\ndomain for person matching. One challenge is how to generate target domain\nsamples with reliable labels for training. To address this problem, we propose\na Disentanglement-based Cross-Domain Feature Augmentation (DCDFA) strategy,\nwhere the augmented features characterize well the target and source domain\ndata distributions while inheriting reliable identity labels. Particularly, we\ndisentangle each sample feature into a robust domain-invariant/shared feature\nand a domain-specific feature, and perform cross-domain feature recomposition\nto enhance the diversity of samples used in the training, with the constraints\nof cross-domain ReID loss and domain classification loss. Each recomposed\nfeature, obtained based on the domain-invariant feature (which enables a\nreliable inheritance of identity) and an enhancement from a domain specific\nfeature (which enables the approximation of real distributions), is thus an\n\"ideal\" augmentation. Extensive experimental results demonstrate the\neffectiveness of our method, which achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:28:41 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhang", "Zhizheng", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["You", "Quanzeng", ""], ["Liu", "Zicheng", ""], ["Zheng", "Kecheng", ""], ["Chen", "Zhibo", ""]]}, {"id": "2103.13922", "submitter": "Daniel Martin", "authors": "Daniel Martin, Ana Serrano, Alexander W. Bergman, Gordon Wetzstein,\n  Belen Masia", "title": "ScanGAN360: A Generative Model of Realistic Scanpaths for 360$^{\\circ}$\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding and modeling the dynamics of human gaze behavior in 360$^\\circ$\nenvironments is a key challenge in computer vision and virtual reality.\nGenerative adversarial approaches could alleviate this challenge by generating\na large number of possible scanpaths for unseen images. Existing methods for\nscanpath generation, however, do not adequately predict realistic scanpaths for\n360$^\\circ$ images. We present ScanGAN360, a new generative adversarial\napproach to address this challenging problem. Our network generator is tailored\nto the specifics of 360$^\\circ$ images representing immersive environments.\nSpecifically, we accomplish this by leveraging the use of a spherical\nadaptation of dynamic-time warping as a loss function and proposing a novel\nparameterization of 360$^\\circ$ scanpaths. The quality of our scanpaths\noutperforms competing approaches by a large margin and is almost on par with\nthe human baseline. ScanGAN360 thus allows fast simulation of large numbers of\nvirtual observers, whose behavior mimics real users, enabling a better\nunderstanding of gaze behavior and novel applications in virtual scene design.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:34:18 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Martin", "Daniel", ""], ["Serrano", "Ana", ""], ["Bergman", "Alexander W.", ""], ["Wetzstein", "Gordon", ""], ["Masia", "Belen", ""]]}, {"id": "2103.13933", "submitter": "Hubert P. H. Shum", "authors": "Brian K. S. Isaac-Medina, Matt Poyser, Daniel Organisciak, Chris G.\n  Willcocks, Toby P. Breckon, Hubert P. H. Shum", "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural\n  Networks: A Performance Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due\nto both negligent and malicious use. For this reason, the automated detection\nand tracking of UAV is a fundamental task in aerial security systems. Common\ntechnologies for UAV detection include visible-band and thermal infrared\nimaging, radio frequency and radar. Recent advances in deep neural networks\n(DNNs) for image-based object detection open the possibility to use visual\ninformation for this detection and tracking task. Furthermore, these detection\narchitectures can be implemented as backbones for visual tracking systems,\nthereby enabling persistent tracking of UAV incursions. To date, no\ncomprehensive performance benchmark exists that applies DNNs to visible-band\nimagery for UAV detection and tracking. To this end, three datasets with varied\nenvironmental conditions for UAV detection and tracking, comprising a total of\n241 videos (331,486 images), are assessed using four detection architectures\nand three tracking frameworks. The best performing detector architecture\nobtains an mAP of 98.6% and the best performing tracking framework obtains a\nMOTA of 96.3%. Cross-modality evaluation is carried out between visible and\ninfrared spectrums, achieving a maximal 82.8% mAP on visible images when\ntraining in the infrared modality. These results provide the first public\nmulti-approach benchmark for state-of-the-art deep learning-based methods and\ngive insight into which detection and tracking architectures are effective in\nthe UAV domain.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:51:53 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 13:50:11 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Isaac-Medina", "Brian K. S.", ""], ["Poyser", "Matt", ""], ["Organisciak", "Daniel", ""], ["Willcocks", "Chris G.", ""], ["Breckon", "Toby P.", ""], ["Shum", "Hubert P. H.", ""]]}, {"id": "2103.13970", "submitter": "Ting Sun", "authors": "Ting Sun and Jinlin Chen and Francis Ng", "title": "Multi-Target Domain Adaptation via Unsupervised Domain Classification\n  for Weather Invariant Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Object detection is an essential technique for autonomous driving. The\nperformance of an object detector significantly degrades if the weather of the\ntraining images is different from that of test images. Domain adaptation can be\nused to address the domain shift problem so as to improve the robustness of an\nobject detector. However, most existing domain adaptation methods either handle\nsingle target domain or require domain labels. We propose a novel unsupervised\ndomain classification method which can be used to generalize single-target\ndomain adaptation methods to multi-target domains, and design a\nweather-invariant object detector training framework based on it. We conduct\nthe experiments on Cityscapes dataset and its synthetic variants, i.e. foggy,\nrainy, and night. The experimental results show that the object detector\ntrained by our proposed method realizes robust object detection under different\nweather conditions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 16:59:35 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Sun", "Ting", ""], ["Chen", "Jinlin", ""], ["Ng", "Francis", ""]]}, {"id": "2103.13990", "submitter": "Ayan Kumar Bhunia", "authors": "Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Yongxin Yang,\n  Tao Xiang, Yi-Zhe Song", "title": "More Photos are All You Need: Semi-Supervised Learning for Fine-Grained\n  Sketch Based Image Retrieval", "comments": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021\n  Code : https://github.com/AyanKumarBhunia/semisupervised-FGSBIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fundamental challenge faced by existing Fine-Grained Sketch-Based Image\nRetrieval (FG-SBIR) models is the data scarcity -- model performances are\nlargely bottlenecked by the lack of sketch-photo pairs. Whilst the number of\nphotos can be easily scaled, each corresponding sketch still needs to be\nindividually produced. In this paper, we aim to mitigate such an upper-bound on\nsketch data, and study whether unlabelled photos alone (of which they are many)\ncan be cultivated for performances gain. In particular, we introduce a novel\nsemi-supervised framework for cross-modal retrieval that can additionally\nleverage large-scale unlabelled photos to account for data scarcity. At the\ncentre of our semi-supervision design is a sequential photo-to-sketch\ngeneration model that aims to generate paired sketches for unlabelled photos.\nImportantly, we further introduce a discriminator guided mechanism to guide\nagainst unfaithful generation, together with a distillation loss based\nregularizer to provide tolerance against noisy training samples. Last but not\nleast, we treat generation and retrieval as two conjugate problems, where a\njoint learning procedure is devised for each module to mutually benefit from\neach other. Extensive experiments show that our semi-supervised model yields\nsignificant performance boost over the state-of-the-art supervised\nalternatives, as well as existing methods that can exploit unlabelled photos\nfor FG-SBIR.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:27:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Bhunia", "Ayan Kumar", ""], ["Chowdhury", "Pinaki Nath", ""], ["Sain", "Aneeshan", ""], ["Yang", "Yongxin", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2103.13998", "submitter": "Xiaohong Liu", "authors": "Xiaohong Liu, Zhihao Shi, Zijun Wu, Jun Chen", "title": "GridDehazeNet+: An Enhanced Multi-Scale Network with Intra-Task\n  Knowledge Transfer for Single Image Dehazing", "comments": "13 pages, 12 figures, submitted to IEEE Transactions on Image\n  Processing. arXiv admin note: text overlap with arXiv:1908.03245", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an enhanced multi-scale network, dubbed GridDehazeNet+, for single\nimage dehazing. It consists of three modules: pre-processing, backbone, and\npost-processing. The trainable pre-processing module can generate learned\ninputs with better diversity and more pertinent features as compared to those\nderived inputs produced by hand-selected pre-processing methods. The backbone\nmodule implements multi-scale estimation with two major enhancements: 1) a\nnovel grid structure that effectively alleviates the bottleneck issue via dense\nconnections across different scales; 2) a spatial-channel attention block that\ncan facilitate adaptive fusion by consolidating dehazing-relevant features. The\npost-processing module helps to reduce the artifacts in the final output. To\nalleviate domain shift between network training and testing, we convert\nsynthetic data to so-called translated data with the distribution shaped to\nmatch that of real data. Moreover, to further improve the dehazing performance\nin real-world scenarios, we propose a novel intra-task knowledge transfer\nmechanism that leverages the distilled knowledge from synthetic data to assist\nthe learning process on translated data. Experimental results indicate that the\nproposed GridDehazeNet+ outperforms the state-of-the-art methods on several\ndehazing benchmarks. The proposed dehazing method does not rely on the\natmosphere scattering model, and we provide a possible explanation as to why it\nis not necessarily beneficial to take advantage of the dimension reduction\noffered by this model, even if only the dehazing results on synthetic images\nare concerned.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:35:36 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Liu", "Xiaohong", ""], ["Shi", "Zhihao", ""], ["Wu", "Zijun", ""], ["Chen", "Jun", ""]]}, {"id": "2103.14003", "submitter": "Haozhi Zhang", "authors": "Haozhi Zhang, Xun Wang, Weilin Huang, Matthew R. Scott", "title": "Rethinking Deep Contrastive Learning with Embedding Memory", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pair-wise loss functions have been extensively studied and shown to\ncontinuously improve the performance of deep metric learning (DML). However,\nthey are primarily designed with intuition based on simple toy examples, and\nexperimentally identifying the truly effective design is difficult in\ncomplicated, real-world cases. In this paper, we provide a new methodology for\nsystematically studying weighting strategies of various pair-wise loss\nfunctions, and rethink pair weighting with an embedding memory. We delve into\nthe weighting mechanisms by decomposing the pair-wise functions, and study\npositive and negative weights separately using direct weight assignment. This\nallows us to study various weighting functions deeply and systematically via\nweight curves, and identify a number of meaningful, comprehensive and\ninsightful facts, which come up with our key observation on memory-based DML:\nit is critical to mine hard negatives and discard easy negatives which are less\ninformative and redundant, but weighting on positive pairs is not helpful. This\nresults in an efficient but surprisingly simple rule to design the weighting\nscheme, making it significantly different from existing mini-batch based\nmethods which design various sophisticated loss functions to weight pairs\ncarefully. Finally, we conduct extensive experiments on three large-scale\nvisual retrieval benchmarks, and demonstrate the superiority of memory-based\nDML over recent mini-batch based approaches, by using a simple contrastive loss\nwith momentum-updated memory.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:39:34 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhang", "Haozhi", ""], ["Wang", "Xun", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""]]}, {"id": "2103.14005", "submitter": "Roozbeh Mottaghi", "authors": "Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, Roozbeh\n  Mottaghi", "title": "Contrasting Contrastive Self-Supervised Representation Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, we have witnessed remarkable breakthroughs in\nself-supervised representation learning. Despite the success and adoption of\nrepresentations learned through this paradigm, much is yet to be understood\nabout how different training methods and datasets influence performance on\ndownstream tasks. In this paper, we analyze contrastive approaches as one of\nthe most successful and popular variants of self-supervised representation\nlearning. We perform this analysis from the perspective of the training\nalgorithms, pre-training datasets and end tasks. We examine over 700 training\nexperiments including 30 encoders, 4 pre-training datasets and 20 diverse\ndownstream tasks. Our experiments address various questions regarding the\nperformance of self-supervised models compared to their supervised\ncounterparts, current benchmarks used for evaluation, and the effect of the\npre-training data on end task performance. We hope the insights and empirical\nevidence provided by this work will help future research in learning better\nvisual representations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:40:38 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kotar", "Klemen", ""], ["Ilharco", "Gabriel", ""], ["Schmidt", "Ludwig", ""], ["Ehsani", "Kiana", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "2103.14006", "submitter": "Kai Zhang", "authors": "Kai Zhang, Jingyun Liang, Luc Van Gool, Radu Timofte", "title": "Designing a Practical Degradation Model for Deep Blind Image\n  Super-Resolution", "comments": "Code: https://github.com/cszn/BSRGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is widely acknowledged that single image super-resolution (SISR) methods\nwould not perform well if the assumed degradation model deviates from those in\nreal images. Although several degradation models take additional factors into\nconsideration, such as blur, they are still not effective enough to cover the\ndiverse degradations of real images. To address this issue, this paper proposes\nto design a more complex but practical degradation model that consists of\nrandomly shuffled blur, downsampling and noise degradations. Specifically, the\nblur is approximated by two convolutions with isotropic and anisotropic\nGaussian kernels; the downsampling is randomly chosen from nearest, bilinear\nand bicubic interpolations; the noise is synthesized by adding Gaussian noise\nwith different noise levels, adopting JPEG compression with different quality\nfactors, and generating processed camera sensor noise via reverse-forward\ncamera image signal processing (ISP) pipeline model and RAW image noise model.\nTo verify the effectiveness of the new degradation model, we have trained a\ndeep blind ESRGAN super-resolver and then applied it to super-resolve both\nsynthetic and real images with diverse degradations. The experimental results\ndemonstrate that the new degradation model can help to significantly improve\nthe practicability of deep super-resolvers, thus providing a powerful\nalternative solution for real SISR applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:40:53 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhang", "Kai", ""], ["Liang", "Jingyun", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2103.14010", "submitter": "Gianmarco Jhair Gallardo Callalli", "authors": "Jhair Gallardo, Tyler L. Hayes, Christopher Kanan", "title": "Self-Supervised Training Enhances Online Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In continual learning, a system must incrementally learn from a\nnon-stationary data stream without catastrophic forgetting. Recently, multiple\nmethods have been devised for incrementally learning classes on large-scale\nimage classification tasks, such as ImageNet. State-of-the-art continual\nlearning methods use an initial supervised pre-training phase, in which the\nfirst 10% - 50% of the classes in a dataset are used to learn representations\nin an offline manner before continual learning of new classes begins. We\nhypothesize that self-supervised pre-training could yield features that\ngeneralize better than supervised learning, especially when the number of\nsamples used for pre-training is small. We test this hypothesis using the\nself-supervised MoCo-V2, Barlow Twins, and SwAV algorithms. On ImageNet, we\nfind that these methods outperform supervised pre-training considerably for\nonline continual learning, and the gains are larger when fewer samples are\navailable. Our findings are consistent across three online continual learning\nalgorithms. Our best system achieves a 14.95% relative increase in top-1\naccuracy on class incremental ImageNet over the prior state of the art for\nonline continual learning.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:45:27 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 23:54:17 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Gallardo", "Jhair", ""], ["Hayes", "Tyler L.", ""], ["Kanan", "Christopher", ""]]}, {"id": "2103.14015", "submitter": "Agnieszka Szczotka", "authors": "Agnieszka Barbara Szczotka, Dzhoshkun Ismail Shakir, Matthew J.\n  Clarkson, Stephen P. Pereira, Tom Vercauteren", "title": "Zero-shot super-resolution with a physically-motivated downsampling\n  kernel for endomicroscopy", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, 2021", "doi": "10.1109/TMI.2021.3067512", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Super-resolution (SR) methods have seen significant advances thanks to the\ndevelopment of convolutional neural networks (CNNs). CNNs have been\nsuccessfully employed to improve the quality of endomicroscopy imaging. Yet,\nthe inherent limitation of research on SR in endomicroscopy remains the lack of\nground truth high-resolution (HR) images, commonly used for both supervised\ntraining and reference-based image quality assessment (IQA). Therefore,\nalternative methods, such as unsupervised SR are being explored. To address the\nneed for non-reference image quality improvement, we designed a novel zero-shot\nsuper-resolution (ZSSR) approach that relies only on the endomicroscopy data to\nbe processed in a self-supervised manner without the need for ground-truth HR\nimages. We tailored the proposed pipeline to the idiosyncrasies of\nendomicroscopy by introducing both: a physically-motivated Voronoi downscaling\nkernel accounting for the endomicroscope's irregular fibre-based sampling\npattern, and realistic noise patterns. We also took advantage of video\nsequences to exploit a sequence of images for self-supervised zero-shot image\nquality improvement. We run ablation studies to assess our contribution in\nregards to the downscaling kernel and noise simulation. We validate our\nmethodology on both synthetic and original data. Synthetic experiments were\nassessed with reference-based IQA, while our results for original images were\nevaluated in a user study conducted with both expert and non-expert observers.\nThe results demonstrated superior performance in image quality of ZSSR\nreconstructions in comparison to the baseline method. The ZSSR is also\ncompetitive when compared to supervised single-image SR, especially being the\npreferred reconstruction technique by experts.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:47:02 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Szczotka", "Agnieszka Barbara", ""], ["Shakir", "Dzhoshkun Ismail", ""], ["Clarkson", "Matthew J.", ""], ["Pereira", "Stephen P.", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2103.14017", "submitter": "Aviv Gabbay", "authors": "Aviv Gabbay and Yedid Hoshen", "title": "Scaling-up Disentanglement for Image Translation", "comments": "Project page: http://www.vision.huji.ac.il/overlord", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image translation methods typically aim to manipulate a set of labeled\nattributes (given as supervision at training time e.g. domain label) while\nleaving the unlabeled attributes intact. Current methods achieve either: (i)\ndisentanglement, which exhibits low visual fidelity and can only be satisfied\nwhere the attributes are perfectly uncorrelated. (ii) visually-plausible\ntranslations, which are clearly not disentangled. In this work, we propose\nOverLORD, a single framework for disentangling labeled and unlabeled attributes\nas well as synthesizing high-fidelity images, which is composed of two stages;\n(i) Disentanglement: Learning disentangled representations with latent\noptimization. Differently from previous approaches, we do not rely on\nadversarial training or any architectural biases. (ii) Synthesis: Training\nfeed-forward encoders for inferring the learned attributes and tuning the\ngenerator in an adversarial manner to increase the perceptual quality. When the\nlabeled and unlabeled attributes are correlated, we model an additional\nrepresentation that accounts for the correlated attributes and improves\ndisentanglement. We highlight that our flexible framework covers multiple image\ntranslation settings e.g. attribute manipulation, pose-appearance translation,\nsegmentation-guided synthesis and shape-texture transfer. In an extensive\nevaluation, we present significantly better disentanglement with higher\ntranslation quality and greater output diversity than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:52:38 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gabbay", "Aviv", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2103.14021", "submitter": "Kanchana Ranasinghe Mr", "authors": "Kanchana Ranasinghe, Muzammal Naseer, Munawar Hayat, Salman Khan,\n  Fahad Shahbaz Khan", "title": "Orthogonal Projection Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have achieved remarkable performance on a range of\nclassification tasks, with softmax cross-entropy (CE) loss emerging as the\nde-facto objective function. The CE loss encourages features of a class to have\na higher projection score on the true class-vector compared to the negative\nclasses. However, this is a relative constraint and does not explicitly force\ndifferent class features to be well-separated. Motivated by the observation\nthat ground-truth class representations in CE loss are orthogonal (one-hot\nencoded vectors), we develop a novel loss function termed `Orthogonal\nProjection Loss' (OPL) which imposes orthogonality in the feature space. OPL\naugments the properties of CE loss and directly enforces inter-class separation\nalongside intra-class clustering in the feature space through orthogonality\nconstraints on the mini-batch level. As compared to other alternatives of CE,\nOPL offers unique advantages e.g., no additional learnable parameters, does not\nrequire careful negative mining and is not sensitive to the batch size. Given\nthe plug-and-play nature of OPL, we evaluate it on a diverse range of tasks\nincluding image recognition (CIFAR-100), large-scale classification (ImageNet),\ndomain generalization (PACS) and few-shot learning (miniImageNet, CIFAR-FS,\ntiered-ImageNet and Meta-dataset) and demonstrate its effectiveness across the\nboard. Furthermore, OPL offers better robustness against practical nuisances\nsuch as adversarial attacks and label noise. Code is available at:\nhttps://github.com/kahnchana/opl.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:58:00 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ranasinghe", "Kanchana", ""], ["Naseer", "Muzammal", ""], ["Hayat", "Munawar", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "2103.14023", "submitter": "Ye Yuan", "authors": "Ye Yuan, Xinshuo Weng, Yanglan Ou, Kris Kitani", "title": "AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent\n  Forecasting", "comments": "Project page: https://www.ye-yuan.com/agentformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting accurate future trajectories of multiple agents is essential for\nautonomous systems, but is challenging due to the complex agent interaction and\nthe uncertainty in each agent's future behavior. Forecasting multi-agent\ntrajectories requires modeling two key dimensions: (1) time dimension, where we\nmodel the influence of past agent states over future states; (2) social\ndimension, where we model how the state of each agent affects others. Most\nprior methods model these two dimensions separately; e.g., first using a\ntemporal model to summarize features over time for each agent independently and\nthen modeling the interaction of the summarized features with a social model.\nThis approach is suboptimal since independent feature encoding over either the\ntime or social dimension can result in a loss of information. Instead, we would\nprefer a method that allows an agent's state at one time to directly affect\nanother agent's state at a future time. To this end, we propose a new\nTransformer, AgentFormer, that jointly models the time and social dimensions.\nThe model leverages a sequence representation of multi-agent trajectories by\nflattening trajectory features across time and agents. Since standard attention\noperations disregard the agent identity of each element in the sequence,\nAgentFormer uses a novel agent-aware attention mechanism that preserves agent\nidentities by attending to elements of the same agent differently than elements\nof other agents. Based on AgentFormer, we propose a stochastic multi-agent\ntrajectory prediction model that can attend to features of any agent at any\nprevious timestep when inferring an agent's future position. The latent intent\nof all agents is also jointly modeled, allowing the stochasticity in one\nagent's behavior to affect other agents. Our method significantly improves the\nstate of the art on well-established pedestrian and autonomous driving\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yuan", "Ye", ""], ["Weng", "Xinshuo", ""], ["Ou", "Yanglan", ""], ["Kitani", "Kris", ""]]}, {"id": "2103.14024", "submitter": "Alex Yu", "authors": "Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa", "title": "PlenOctrees for Real-time Rendering of Neural Radiance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a method to render Neural Radiance Fields (NeRFs) in real time\nusing PlenOctrees, an octree-based 3D representation which supports\nview-dependent effects. Our method can render 800x800 images at more than 150\nFPS, which is over 3000 times faster than conventional NeRFs. We do so without\nsacrificing quality while preserving the ability of NeRFs to perform\nfree-viewpoint rendering of scenes with arbitrary geometry and view-dependent\neffects. Real-time performance is achieved by pre-tabulating the NeRF into a\nPlenOctree. In order to preserve view-dependent effects such as specularities,\nwe factorize the appearance via closed-form spherical basis functions.\nSpecifically, we show that it is possible to train NeRFs to predict a spherical\nharmonic representation of radiance, removing the viewing direction as an input\nto the neural network. Furthermore, we show that PlenOctrees can be directly\noptimized to further minimize the reconstruction loss, which leads to equal or\nbetter quality compared to competing methods. Moreover, this octree\noptimization step can be used to reduce the training time, as we no longer need\nto wait for the NeRF training to converge fully. Our real-time neural rendering\napproach may potentially enable new applications such as 6-DOF industrial and\nproduct visualizations, as well as next generation AR/VR systems. PlenOctrees\nare amenable to in-browser rendering as well; please visit the project page for\nthe interactive online demo, as well as video and code:\nhttps://alexyu.net/plenoctrees\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:06 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yu", "Alex", ""], ["Li", "Ruilong", ""], ["Tancik", "Matthew", ""], ["Li", "Hao", ""], ["Ng", "Ren", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2103.14025", "submitter": "Chuang Gan", "authors": "Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek\n  Bhandwaldar, Dan Gutfreund, Daniel L.K. Yamins, James J DiCarlo, Josh\n  McDermott, Antonio Torralba, Joshua B. Tenenbaum", "title": "The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion\n  Planning Benchmark for Physically Realistic Embodied AI", "comments": "Project page: http://tdw-transport.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a visually-guided and physics-driven task-and-motion planning\nbenchmark, which we call the ThreeDWorld Transport Challenge. In this\nchallenge, an embodied agent equipped with two 9-DOF articulated arms is\nspawned randomly in a simulated physical home environment. The agent is\nrequired to find a small set of objects scattered around the house, pick them\nup, and transport them to a desired final location. We also position containers\naround the house that can be used as tools to assist with transporting objects\nefficiently. To complete the task, an embodied agent must plan a sequence of\nactions to change the state of a large number of objects in the face of\nrealistic physical constraints. We build this benchmark challenge using the\nThreeDWorld simulation: a virtual 3D environment where all objects respond to\nphysics, and where can be controlled using fully physics-driven navigation and\ninteraction API. We evaluate several existing agents on this benchmark.\nExperimental results suggest that: 1) a pure RL model struggles on this\nchallenge; 2) hierarchical planning-based agents can transport some objects but\nstill far from solving this task. We anticipate that this benchmark will\nempower researchers to develop more intelligent physics-driven robots for the\nphysical world.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gan", "Chuang", ""], ["Zhou", "Siyuan", ""], ["Schwartz", "Jeremy", ""], ["Alter", "Seth", ""], ["Bhandwaldar", "Abhishek", ""], ["Gutfreund", "Dan", ""], ["Yamins", "Daniel L. K.", ""], ["DiCarlo", "James J", ""], ["McDermott", "Josh", ""], ["Torralba", "Antonio", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2103.14026", "submitter": "Jifeng Dai", "authors": "Hao Li, Tianwen Fu, Jifeng Dai, Hongsheng Li, Gao Huang, Xizhou Zhu", "title": "AutoLoss-Zero: Searching Loss Functions from Scratch for Generic Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been achieved in automating the design of various\ncomponents in deep networks. However, the automatic design of loss functions\nfor generic tasks with various evaluation metrics remains under-investigated.\nPrevious works on handcrafting loss functions heavily rely on human expertise,\nwhich limits their extendibility. Meanwhile, existing efforts on searching loss\nfunctions mainly focus on specific tasks and particular metrics, with\ntask-specific heuristics. Whether such works can be extended to generic tasks\nis not verified and questionable. In this paper, we propose AutoLoss-Zero, the\nfirst general framework for searching loss functions from scratch for generic\ntasks. Specifically, we design an elementary search space composed only of\nprimitive mathematical operators to accommodate the heterogeneous tasks and\nevaluation metrics. A variant of the evolutionary algorithm is employed to\ndiscover loss functions in the elementary search space. A loss-rejection\nprotocol and a gradient-equivalence-check strategy are developed so as to\nimprove the search efficiency, which are applicable to generic tasks. Extensive\nexperiments on various computer vision tasks demonstrate that our searched loss\nfunctions are on par with or superior to existing loss functions, which\ngeneralize well to different datasets and networks. Code shall be released.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:09 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Li", "Hao", ""], ["Fu", "Tianwen", ""], ["Dai", "Jifeng", ""], ["Li", "Hongsheng", ""], ["Huang", "Gao", ""], ["Zhu", "Xizhou", ""]]}, {"id": "2103.14027", "submitter": "Yosuke Shinya", "authors": "Yosuke Shinya", "title": "USB: Universal-Scale Object Detection Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmarks, such as COCO, play a crucial role in object detection. However,\nexisting benchmarks are insufficient in scale variation, and their protocols\nare inadequate for fair comparison. In this paper, we introduce the\nUniversal-Scale object detection Benchmark (USB). USB has variations in object\nscales and image domains by incorporating COCO with the recently proposed Waymo\nOpen Dataset and Manga109-s dataset. To enable fair comparison, we propose USB\nprotocols by defining multiple thresholds for training epochs and evaluation\nimage resolutions. By analyzing methods on the proposed benchmark, we designed\nfast and accurate object detectors called UniverseNets, which surpassed all\nbaselines on USB and achieved state-of-the-art results on existing benchmarks.\nSpecifically, UniverseNets achieved 54.1% AP on COCO test-dev with 20 epochs\ntraining, the top result among single-stage detectors on the Waymo Open Dataset\nChallenge 2020 2D detection, and the first place in the NightOwls Detection\nChallenge 2020 all objects track. The code is available at\nhttps://github.com/shinya7y/UniverseNet .\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:15 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Shinya", "Yosuke", ""]]}, {"id": "2103.14030", "submitter": "Han Hu", "authors": "Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng\n  Zhang and Stephen Lin and Baining Guo", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "comments": "The first 4 authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new vision Transformer, called Swin Transformer, that\ncapably serves as a general-purpose backbone for computer vision. Challenges in\nadapting Transformer from language to vision arise from differences between the\ntwo domains, such as large variations in the scale of visual entities and the\nhigh resolution of pixels in images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose representation is\ncomputed with shifted windows. The shifted windowing scheme brings greater\nefficiency by limiting self-attention computation to non-overlapping local\nwindows while also allowing for cross-window connection. This hierarchical\narchitecture has the flexibility to model at various scales and has linear\ncomputational complexity with respect to image size. These qualities of Swin\nTransformer make it compatible with a broad range of vision tasks, including\nimage classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction\ntasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev)\nand semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses\nthe previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP\non COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of\nTransformer-based models as vision backbones. The code and models will be made\npublicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:31 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Liu", "Ze", ""], ["Lin", "Yutong", ""], ["Cao", "Yue", ""], ["Hu", "Han", ""], ["Wei", "Yixuan", ""], ["Zhang", "Zheng", ""], ["Lin", "Stephen", ""], ["Guo", "Baining", ""]]}, {"id": "2103.14031", "submitter": "Dongdong Chen", "authors": "Ziyu Wan and Jingbo Zhang and Dongdong Chen and Jing Liao", "title": "High-Fidelity Pluralistic Image Completion with Transformers", "comments": "Project Page: http://raywzy.com/ICT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image completion has made tremendous progress with convolutional neural\nnetworks (CNNs), because of their powerful texture modeling capacity. However,\ndue to some inherent properties (e.g., local inductive prior, spatial-invariant\nkernels), CNNs do not perform well in understanding global structures or\nnaturally support pluralistic completion. Recently, transformers demonstrate\ntheir power in modeling the long-term relationship and generating diverse\nresults, but their computation complexity is quadratic to input length, thus\nhampering the application in processing high-resolution images. This paper\nbrings the best of both worlds to pluralistic image completion: appearance\nprior reconstruction with transformer and texture replenishment with CNN. The\nformer transformer recovers pluralistic coherent structures together with some\ncoarse textures, while the latter CNN enhances the local texture details of\ncoarse priors guided by the high-resolution masked images. The proposed method\nvastly outperforms state-of-the-art methods in terms of three aspects: 1) large\nperformance boost on image fidelity even compared to deterministic completion\nmethods; 2) better diversity and higher fidelity for pluralistic completion; 3)\nexceptional generalization ability on large masks and generic dataset, like\nImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:46 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Wan", "Ziyu", ""], ["Zhang", "Jingbo", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""]]}, {"id": "2103.14051", "submitter": "Hadi Jamali-Rad", "authors": "Attila Szabo, Hadi Jamali-Rad, Siva-Datta Mannava", "title": "Tilted Cross Entropy (TCE): Promoting Fairness in Semantic Segmentation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional empirical risk minimization (ERM) for semantic segmentation can\ndisproportionately advantage or disadvantage certain target classes in favor of\nan (unfair but) improved overall performance. Inspired by the recently\nintroduced tilted ERM (TERM), we propose tilted cross-entropy (TCE) loss and\nadapt it to the semantic segmentation setting to minimize performance disparity\namong target classes and promote fairness. Through quantitative and qualitative\nperformance analyses, we demonstrate that the proposed Stochastic TCE for\nsemantic segmentation can efficiently improve the low-performing classes of\nCityscapes and ADE20k datasets trained with multi-class cross-entropy (MCCE),\nand also results in improved overall fairness.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:00:50 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Szabo", "Attila", ""], ["Jamali-Rad", "Hadi", ""], ["Mannava", "Siva-Datta", ""]]}, {"id": "2103.14076", "submitter": "Andreas Bock", "authors": "Andreas Bock, Colin J. Cotter", "title": "Learning landmark geodesics using Kalman ensembles", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of diffeomorphometric geodesic landmark matching where\nthe objective is to find a diffeomorphism that via its group action maps\nbetween two sets of landmarks. It is well-known that the motion of the\nlandmarks, and thereby the diffeomorphism, can be encoded by an initial\nmomentum leading to a formulation where the landmark matching problem can be\nsolved as an optimisation problem over such momenta. The novelty of our work\nlies in the application of a derivative-free Bayesian inverse method for\nlearning the optimal momentum encoding the diffeomorphic mapping between the\ntemplate and the target. The method we apply is the ensemble Kalman filter, an\nextension of the Kalman filter to nonlinear observation operators. We describe\nan efficient implementation of the algorithm and show several numerical results\nfor various target shapes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:52:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bock", "Andreas", ""], ["Cotter", "Colin J.", ""]]}, {"id": "2103.14098", "submitter": "Qing Liu", "authors": "Qing Liu, Adam Kortylewski, Zhishuai Zhang, Zizhang Li, Mengqi Guo,\n  Qihao Liu, Xiaoding Yuan, Jiteng Mu, Weichao Qiu, Alan Yuille", "title": "CGPart: A Part Segmentation Dataset Based on 3D Computer Graphics Models", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part segmentations provide a rich and detailed part-level description of\nobjects, but their annotation requires an enormous amount of work. In this\npaper, we introduce CGPart, a comprehensive part segmentation dataset that\nprovides detailed annotations on 3D CAD models, synthetic images, and real test\nimages. CGPart includes $21$ 3D CAD models covering $5$ vehicle categories,\neach with detailed per-mesh part labeling. The average number of parts per\ncategory is $24$, which is larger than any existing datasets for part\nsegmentation on vehicle objects. By varying the rendering parameters, we make\n$168,000$ synthetic images from these CAD models, each with automatically\ngenerated part segmentation ground-truth. We also annotate part segmentations\non $200$ real images for evaluation purposes. To illustrate the value of\nCGPart, we apply it to image part segmentation through unsupervised domain\nadaptation (UDA). We evaluate several baseline methods by adapting\ntop-performing UDA algorithms from related tasks to part segmentation.\nMoreover, we introduce a new method called Geometric-Matching Guided domain\nadaptation (GMG), which leverages the spatial object structure to guide the\nknowledge transfer from the synthetic to the real images. Experimental results\ndemonstrate the advantage of our new algorithm and reveal insights for future\nimprovement. We will release our data and code.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 19:34:21 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Liu", "Qing", ""], ["Kortylewski", "Adam", ""], ["Zhang", "Zhishuai", ""], ["Li", "Zizhang", ""], ["Guo", "Mengqi", ""], ["Liu", "Qihao", ""], ["Yuan", "Xiaoding", ""], ["Mu", "Jiteng", ""], ["Qiu", "Weichao", ""], ["Yuille", "Alan", ""]]}, {"id": "2103.14103", "submitter": "Kranti Kumar Parida", "authors": "Kranti Kumar Parida, Gaurav Sharma", "title": "Discriminative Semantic Transitive Consistency for Cross-Modal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-modal retrieval is generally performed by projecting and aligning the\ndata from two different modalities onto a shared representation space. This\nshared space often also acts as a bridge for translating the modalities. We\naddress the problem of learning such representation space by proposing and\nexploiting the property of Discriminative Semantic Transitive Consistency --\nensuring that the data points are correctly classified even after being\ntransferred to the other modality. Along with semantic transitive consistency,\nwe also enforce the traditional distance minimizing constraint which makes the\nprojections of the corresponding data points from both the modalities to come\ncloser in the representation space. We analyze and compare the contribution of\nboth the loss terms and their interaction, for the task. In addition, we\nincorporate semantic cycle-consistency for each of the modality. We empirically\ndemonstrate better performance owing to the different components with clear\nablation studies. We also provide qualitative results to support the proposals.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 19:45:24 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Parida", "Kranti Kumar", ""], ["Sharma", "Gaurav", ""]]}, {"id": "2103.14107", "submitter": "Chuhua Wang", "authors": "Chuhua Wang, Yuchen Wang, Mingze Xu, David J. Crandall", "title": "Stepwise Goal-Driven Networks for Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to predict the future trajectories of observed agents (e.g.,\npedestrians or vehicles) by estimating and using their goals at multiple time\nscales. We argue that the goal of a moving agent may change over time, and\nmodeling goals continuously provides more accurate and detailed information for\nfuture trajectory estimation. In this paper, we present a novel recurrent\nnetwork for trajectory prediction, called Stepwise Goal-Driven Network (SGNet).\nUnlike prior work that models only a single, long-term goal, SGNet estimates\nand uses goals at multiple temporal scales. In particular, the framework\nincorporates an encoder module that captures historical information, a stepwise\ngoal estimator that predicts successive goals into the future, and a decoder\nmodule that predicts future trajectory. We evaluate our model on three\nfirst-person traffic datasets (HEV-I, JAAD, and PIE) as well as on two bird's\neye view datasets (ETH and UCY), and show that our model outperforms the\nstate-of-the-art methods in terms of both average and final displacement errors\non all datasets. Code has been made available at:\nhttps://github.com/ChuhuaW/SGNet.pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 19:51:54 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wang", "Chuhua", ""], ["Wang", "Yuchen", ""], ["Xu", "Mingze", ""], ["Crandall", "David J.", ""]]}, {"id": "2103.14113", "submitter": "Manh Huynh", "authors": "Manh Huynh, Gita Alaghband", "title": "GPRAR: Graph Convolutional Network based Pose Reconstruction and Action\n  Recognition for Human Trajectory Prediction", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prediction with high accuracy is essential for various applications such as\nautonomous driving. Existing prediction models are easily prone to errors in\nreal-world settings where observations (e.g. human poses and locations) are\noften noisy. To address this problem, we introduce GPRAR, a graph convolutional\nnetwork based pose reconstruction and action recognition for human trajectory\nprediction. The key idea of GPRAR is to generate robust features: human poses\nand actions, under noisy scenarios. To this end, we design GPRAR using two\nnovel sub-networks: PRAR (Pose Reconstruction and Action Recognition) and FA\n(Feature Aggregator). PRAR aims to simultaneously reconstruct human poses and\naction features from the coherent and structural properties of human skeletons.\nIt is a network of an encoder and two decoders, each of which comprises\nmultiple layers of spatiotemporal graph convolutional networks. Moreover, we\npropose a Feature Aggregator (FA) to channel-wise aggregate the learned\nfeatures: human poses, actions, locations, and camera motion using\nencoder-decoder based temporal convolutional neural networks to predict future\nlocations. Extensive experiments on the commonly used datasets: JAAD [13] and\nTITAN [19] show accuracy improvements of GPRAR over state-of-theart models.\nSpecifically, GPRAR improves the prediction accuracy up to 22% and 50% under\nnoisy observations on JAAD and TITAN datasets, respectively\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 20:12:14 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Huynh", "Manh", ""], ["Alaghband", "Gita", ""]]}, {"id": "2103.14127", "submitter": "Martin Sundermeyer", "authors": "Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, Dieter Fox", "title": "Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes", "comments": "ICRA 2021. Video of the real world experiments and code are available\n  at\n  https://research.nvidia.com/publication/2021-03_Contact-GraspNet%3A--Efficient", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Grasping unseen objects in unconstrained, cluttered environments is an\nessential skill for autonomous robotic manipulation. Despite recent progress in\nfull 6-DoF grasp learning, existing approaches often consist of complex\nsequential pipelines that possess several potential failure points and\nrun-times unsuitable for closed-loop grasping. Therefore, we propose an\nend-to-end network that efficiently generates a distribution of 6-DoF\nparallel-jaw grasps directly from a depth recording of a scene. Our novel grasp\nrepresentation treats 3D points of the recorded point cloud as potential grasp\ncontacts. By rooting the full 6-DoF grasp pose and width in the observed point\ncloud, we can reduce the dimensionality of our grasp representation to 4-DoF\nwhich greatly facilitates the learning process. Our class-agnostic approach is\ntrained on 17 million simulated grasps and generalizes well to real world\nsensor data. In a robotic grasping study of unseen objects in structured\nclutter we achieve over 90% success rate, cutting the failure rate in half\ncompared to a recent state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 20:33:29 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sundermeyer", "Martin", ""], ["Mousavian", "Arsalan", ""], ["Triebel", "Rudolph", ""], ["Fox", "Dieter", ""]]}, {"id": "2103.14146", "submitter": "Yue Qiu", "authors": "Yue Qiu and Shintaro Yamamoto and Kodai Nakashima and Ryota Suzuki and\n  Kenji Iwata and Hirokatsu Kataoka and Yutaka Satoh", "title": "Describing and Localizing Multiple Changes with Transformers", "comments": "18 pages, 15 figures, project page:\n  https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change captioning tasks aim to detect changes in image pairs observed before\nand after a scene change and generate a natural language description of the\nchanges. Existing change captioning studies have mainly focused on scenes with\na single change. However, detecting and describing multiple changed parts in\nimage pairs is essential for enhancing adaptability to complex scenarios. We\nsolve the above issues from three aspects: (i) We propose a CG-based\nmulti-change captioning dataset; (ii) We benchmark existing state-of-the-art\nmethods of single change captioning on multi-change captioning; (iii) We\nfurther propose Multi-Change Captioning transformers (MCCFormers) that identify\nchange regions by densely correlating different regions in image pairs and\ndynamically determines the related change regions with words in sentences. The\nproposed method obtained the highest scores on four conventional change\ncaptioning evaluation metrics for multi-change captioning. In addition,\nexisting methods generate a single attention map for multiple changes and lack\nthe ability to distinguish change regions. In contrast, our proposed method can\nseparate attention maps for each change and performs well with respect to\nchange localization. Moreover, the proposed framework outperformed the previous\nstate-of-the-art methods on an existing change captioning benchmark,\nCLEVR-Change, by a large margin (+6.1 on BLEU-4 and +9.7 on CIDEr scores),\nindicating its general ability in change captioning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 21:52:03 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Qiu", "Yue", ""], ["Yamamoto", "Shintaro", ""], ["Nakashima", "Kodai", ""], ["Suzuki", "Ryota", ""], ["Iwata", "Kenji", ""], ["Kataoka", "Hirokatsu", ""], ["Satoh", "Yutaka", ""]]}, {"id": "2103.14147", "submitter": "Haiwei Chen", "authors": "Haiwei Chen and Shichen Liu and Weikai Chen and Hao Li", "title": "Equivariant Point Network for 3D Point Cloud Analysis", "comments": "10 pages, to be published in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Features that are equivariant to a larger group of symmetries have been shown\nto be more discriminative and powerful in recent studies. However, higher-order\nequivariant features often come with an exponentially-growing computational\ncost. Furthermore, it remains relatively less explored how rotation-equivariant\nfeatures can be leveraged to tackle 3D shape alignment tasks. While many past\napproaches have been based on either non-equivariant or invariant descriptors\nto align 3D shapes, we argue that such tasks may benefit greatly from an\nequivariant framework. In this paper, we propose an effective and practical\nSE(3) (3D translation and rotation) equivariant network for point cloud\nanalysis that addresses both problems. First, we present SE(3) separable point\nconvolution, a novel framework that breaks down the 6D convolution into two\nseparable convolutional operators alternatively performed in the 3D Euclidean\nand SO(3) spaces. This significantly reduces the computational cost without\ncompromising the performance. Second, we introduce an attention layer to\neffectively harness the expressiveness of the equivariant features. While\njointly trained with the network, the attention layer implicitly derives the\nintrinsic local frame in the feature space and generates attention vectors that\ncan be integrated into different alignment tasks. We evaluate our approach\nthrough extensive studies and visual interpretations. The empirical results\ndemonstrate that our proposed model outperforms strong baselines in a variety\nof benchmarks\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 21:57:10 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 10:22:01 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Chen", "Haiwei", ""], ["Liu", "Shichen", ""], ["Chen", "Weikai", ""], ["Li", "Hao", ""]]}, {"id": "2103.14162", "submitter": "Amirreza Shaban", "authors": "Amirreza Shaban, Amir Rahimi, Thalaiyasingam Ajanthan, Byron Boots,\n  Richard Hartley", "title": "Few-shot Weakly-Supervised Object Detection via Directional Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting novel objects from few examples has become an emerging topic in\ncomputer vision recently. However, these methods need fully annotated training\nimages to learn new object categories which limits their applicability in real\nworld scenarios such as field robotics. In this work, we propose a\nprobabilistic multiple instance learning approach for few-shot Common Object\nLocalization (COL) and few-shot Weakly Supervised Object Detection (WSOD). In\nthese tasks, only image-level labels, which are much cheaper to acquire, are\navailable. We find that operating on features extracted from the last layer of\na pre-trained Faster-RCNN is more effective compared to previous episodic\nlearning based few-shot COL methods. Our model simultaneously learns the\ndistribution of the novel objects and localizes them via\nexpectation-maximization steps. As a probabilistic model, we employ von\nMises-Fisher (vMF) distribution which captures the semantic information better\nthan Gaussian distribution when applied to the pre-trained embedding space.\nWhen the novel objects are localized, we utilize them to learn a linear\nappearance model to detect novel classes in new images. Our extensive\nexperiments show that the proposed method, despite being simple, outperforms\nstrong baselines in few-shot COL and WSOD, as well as large-scale WSOD tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 22:34:16 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Shaban", "Amirreza", ""], ["Rahimi", "Amir", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Boots", "Byron", ""], ["Hartley", "Richard", ""]]}, {"id": "2103.14167", "submitter": "Wei Jiang", "authors": "Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, Kwang Moo\n  Yi", "title": "COTR: Correspondence Transformer for Matching Across Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for finding correspondences in images based on a\ndeep neural network that, given two images and a query point in one of them,\nfinds its correspondence in the other. By doing so, one has the option to query\nonly the points of interest and retrieve sparse correspondences, or to query\nall points in an image and obtain dense mappings. Importantly, in order to\ncapture both local and global priors, and to let our model relate between image\nregions using the most relevant among said priors, we realize our network using\na transformer. At inference time, we apply our correspondence network by\nrecursively zooming in around the estimates, yielding a multiscale pipeline\nable to provide highly-accurate correspondences. Our method significantly\noutperforms the state of the art on both sparse and dense correspondence\nproblems on multiple datasets and tasks, ranging from wide-baseline stereo to\noptical flow, without any retraining for a specific dataset. We commit to\nreleasing data, code, and all the tools necessary to train from scratch and\nensure reproducibility.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 22:47:02 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Jiang", "Wei", ""], ["Trulls", "Eduard", ""], ["Hosang", "Jan", ""], ["Tagliasacchi", "Andrea", ""], ["Yi", "Kwang Moo", ""]]}, {"id": "2103.14182", "submitter": "Yun-Chun Chen", "authors": "Yun-Chun Chen, Marco Piccirilli, Robinson Piramuthu, Ming-Hsuan Yang", "title": "Self-Attentive 3D Human Pose and Shape Estimation from Videos", "comments": "This paper is under consideration at Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of estimating 3D human pose and shape from videos. While\nexisting frame-based approaches have made significant progress, these methods\nare independently applied to each image, thereby often leading to inconsistent\npredictions. In this work, we present a video-based learning algorithm for 3D\nhuman pose and shape estimation. The key insights of our method are two-fold.\nFirst, to address the inconsistent temporal prediction issue, we exploit\ntemporal information in videos and propose a self-attention module that jointly\nconsiders short-range and long-range dependencies across frames, resulting in\ntemporally coherent estimations. Second, we model human motion with a\nforecasting module that allows the transition between adjacent frames to be\nsmooth. We evaluate our method on the 3DPW, MPI-INF-3DHP, and Human3.6M\ndatasets. Extensive experimental results show that our algorithm performs\nfavorably against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 00:02:19 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Chen", "Yun-Chun", ""], ["Piccirilli", "Marco", ""], ["Piramuthu", "Robinson", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2103.14184", "submitter": "Wenbo Zhang", "authors": "Wenbo Zhang, Karl Schmeckpeper, Pratik Chaudhari, Kostas Daniilidis", "title": "Deformable Linear Object Prediction Using Locally Linear Latent Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a framework for deformable linear object prediction. Prediction of\ndeformable objects (e.g., rope) is challenging due to their non-linear dynamics\nand infinite-dimensional configuration spaces. By mapping the dynamics from a\nnon-linear space to a linear space, we can use the good properties of linear\ndynamics for easier learning and more efficient prediction. We learn a locally\nlinear, action-conditioned dynamics model that can be used to predict future\nlatent states. Then, we decode the predicted latent state into the predicted\nstate. We also apply a sampling-based optimization algorithm to select the\noptimal control action. We empirically demonstrate that our approach can\npredict the rope state accurately up to ten steps into the future and that our\nalgorithm can find the optimal action given an initial state and a goal state.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 00:29:31 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhang", "Wenbo", ""], ["Schmeckpeper", "Karl", ""], ["Chaudhari", "Pratik", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "2103.14198", "submitter": "Yurong You", "authors": "Yurong You, Carlos Andres Diaz-Ruiz, Yan Wang, Wei-Lun Chao, Bharath\n  Hariharan, Mark Campbell, Kilian Q Weinberger", "title": "Exploiting Playbacks in Unsupervised Domain Adaptation for 3D Object\n  Detection", "comments": "RAL-IROS 2021 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-driving cars must detect other vehicles and pedestrians in 3D to plan\nsafe routes and avoid collisions. State-of-the-art 3D object detectors, based\non deep learning, have shown promising accuracy but are prone to over-fit to\ndomain idiosyncrasies, making them fail in new environments -- a serious\nproblem if autonomous vehicles are meant to operate freely. In this paper, we\npropose a novel learning approach that drastically reduces this gap by\nfine-tuning the detector on pseudo-labels in the target domain, which our\nmethod generates while the vehicle is parked, based on replays of previously\nrecorded driving sequences. In these replays, objects are tracked over time,\nand detections are interpolated and extrapolated -- crucially, leveraging\nfuture information to catch hard cases. We show, on five autonomous driving\ndatasets, that fine-tuning the object detector on these pseudo-labels\nsubstantially reduces the domain gap to new driving environments, yielding\ndrastic improvements in accuracy and detection reliability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 01:18:11 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["You", "Yurong", ""], ["Diaz-Ruiz", "Carlos Andres", ""], ["Wang", "Yan", ""], ["Chao", "Wei-Lun", ""], ["Hariharan", "Bharath", ""], ["Campbell", "Mark", ""], ["Weinberger", "Kilian Q", ""]]}, {"id": "2103.14201", "submitter": "Nikhil Singh", "authors": "Nikhil Singh and Jeff Mentch and Jerry Ng and Matthew Beveridge and\n  Iddo Drori", "title": "Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis", "comments": "23 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring the acoustic characteristics of a space is often done by capturing\nits impulse response (IR), a representation of how a full-range stimulus sound\nexcites it. This is the first work that generates an IR from a single image,\nwhich we call Image2Reverb. This IR is then applied to other signals using\nconvolution, simulating the reverberant characteristics of the space shown in\nthe image. Recording these IRs is both time-intensive and expensive, and often\ninfeasible for inaccessible locations. We use an end-to-end neural network\narchitecture to generate plausible audio impulse responses from single images\nof acoustic environments. We evaluate our method both by comparisons to ground\ntruth data and by human expert evaluation. We demonstrate our approach by\ngenerating plausible impulse responses from diverse settings and formats\nincluding well known places, musical halls, rooms in paintings, images from\nanimations and computer games, synthetic environments generated from text,\npanoramic images, and video conference backgrounds.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 01:25:58 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Singh", "Nikhil", ""], ["Mentch", "Jeff", ""], ["Ng", "Jerry", ""], ["Beveridge", "Matthew", ""], ["Drori", "Iddo", ""]]}, {"id": "2103.14204", "submitter": "Xiaohong Liu", "authors": "Xiaohong Liu, Yongrui Ma, Zhihao Shi, Linhui Dai, Jun Chen", "title": "Towards a Unified Approach to Single Image Deraining and Dehazing", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new physical model for the rain effect and show that the\nwell-known atmosphere scattering model (ASM) for the haze effect naturally\nemerges as its homogeneous continuous limit. Via depth-aware fusion of\nmulti-layer rain streaks according to the camera imaging mechanism, the new\nmodel can better capture the sophisticated non-deterministic degradation\npatterns commonly seen in real rainy images. We also propose a Densely\nScale-Connected Attentive Network (DSCAN) that is suitable for both deraining\nand dehazing tasks. Our design alleviates the bottleneck issue existent in\nconventional multi-scale networks and enables more effective information\nexchange and aggregation. Extensive experimental results demonstrate that the\nproposed DSCAN is able to deliver superior derained/dehazed results on both\nsynthetic and real images as compared to the state-of-the-art. Moreover, it is\nshown that for our DSCAN, the synthetic dataset built using the new physical\nmodel yields better generalization performance on real images in comparison\nwith the existing datasets based on over-simplified models.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 01:35:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Liu", "Xiaohong", ""], ["Ma", "Yongrui", ""], ["Shi", "Zhihao", ""], ["Dai", "Linhui", ""], ["Chen", "Jun", ""]]}, {"id": "2103.14210", "submitter": "Guangwei Gao", "authors": "Guangwei Gao, Hao Shao, Yi Yu, Fei Wu, Meng Yang", "title": "Leaning Compact and Representative Features for Cross-Modality Person\n  Re-Identification", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper pays close attention to the cross-modality visible-infrared person\nre-identification (VI Re-ID) task, which aims to match human samples between\nvisible and infrared modes. In order to reduce the discrepancy between features\nof different modalities, most existing works usually use constraints based on\nEuclidean metric. Since the Euclidean based distance metric cannot effectively\nmeasure the internal angles between the embedded vectors, the above methods\ncannot learn the angularly discriminative feature embedding. Because the most\nimportant factor affecting the classification task based on embedding vector is\nwhether there is an angularly discriminativ feature space, in this paper, we\npropose a new loss function called Enumerate Angular Triplet (EAT) loss. Also,\nmotivated by the knowledge distillation, to narrow down the features between\ndifferent modalities before feature embedding, we further present a new\nCross-Modality Knowledge Distillation (CMKD) loss. The experimental results on\nRegDB and SYSU-MM01 datasets have shown that the proposed method is superior to\nthe other most advanced methods in terms of impressive performance.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 01:53:16 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Gao", "Guangwei", ""], ["Shao", "Hao", ""], ["Yu", "Yi", ""], ["Wu", "Fei", ""], ["Yang", "Meng", ""]]}, {"id": "2103.14211", "submitter": "Zhikai Chen", "authors": "Zhikai Chen and Lingxi Xie and Shanmin Pang and Yong He and Bo Zhang", "title": "MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfakes raised serious concerns on the authenticity of visual contents.\nPrior works revealed the possibility to disrupt deepfakes by adding adversarial\nperturbations to the source data, but we argue that the threat has not been\neliminated yet. This paper presents MagDR, a mask-guided detection and\nreconstruction pipeline for defending deepfakes from adversarial attacks. MagDR\nstarts with a detection module that defines a few criteria to judge the\nabnormality of the output of deepfakes, and then uses it to guide a learnable\nreconstruction procedure. Adaptive masks are extracted to capture the change in\nlocal facial regions. In experiments, MagDR defends three main tasks of\ndeepfakes, and the learned reconstruction pipeline transfers across input data,\nshowing promising performance in defending both black-box and white-box\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 01:57:04 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Chen", "Zhikai", ""], ["Xie", "Lingxi", ""], ["Pang", "Shanmin", ""], ["He", "Yong", ""], ["Zhang", "Bo", ""]]}, {"id": "2103.14212", "submitter": "Arghya Pal", "authors": "Arghya Pal, Rapha Phan, KokSheik Wong", "title": "Synthesize-It-Classifier: Learning a Generative Classifier through\n  RecurrentSelf-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show the generative capability of an image classifier\nnetwork by synthesizing high-resolution, photo-realistic, and diverse images at\nscale. The overall methodology, called Synthesize-It-Classifier (STIC), does\nnot require an explicit generator network to estimate the density of the data\ndistribution and sample images from that, but instead uses the classifier's\nknowledge of the boundary to perform gradient ascent w.r.t. class logits and\nthen synthesizes images using Gram Matrix Metropolis Adjusted Langevin\nAlgorithm (GRMALA) by drawing on a blank canvas. During training, the\nclassifier iteratively uses these synthesized images as fake samples and\nre-estimates the class boundary in a recurrent fashion to improve both the\nclassification accuracy and quality of synthetic images. The STIC shows the\nmixing of the hard fake samples (i.e. those synthesized by the one hot class\nconditioning), and the soft fake samples (which are synthesized as a convex\ncombination of classes, i.e. a mixup of classes) improves class interpolation.\nWe demonstrate an Attentive-STIC network that shows an iterative drawing of\nsynthesized images on the ImageNet dataset that has thousands of classes. In\naddition, we introduce the synthesis using a class conditional score classifier\n(Score-STIC) instead of a normal image classifier and show improved results on\nseveral real-world datasets, i.e. ImageNet, LSUN, and CIFAR 10.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:00:29 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Pal", "Arghya", ""], ["Phan", "Rapha", ""], ["Wong", "KokSheik", ""]]}, {"id": "2103.14216", "submitter": "Masaya Ueda", "authors": "Masaya Ueda, Akisato Kimura, Seiichi Uchida", "title": "Which Parts Determine the Impression of the Font?", "comments": "Accepted at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various fonts give different impressions, such as legible, rough, and\ncomic-text.This paper aims to analyze the correlation between the local shapes,\nor parts, and the impression of fonts. By focusing on local shapes instead of\nthe whole letter shape, we can realize letter-shape independent and more\ngeneral analysis. The analysis is performed by newly combining SIFT and\nDeepSets, to extract an arbitrary number of essential parts from a particular\nfont and aggregate them to infer the font impressions by nonlinear regression.\nOur qualitative and quantitative analyses prove that (1)fonts with similar\nparts have similar impressions, (2)many impressions, such as legible and rough,\nlargely depend on specific parts, (3)several impressions are very irrelevant to\nparts.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:13:24 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 05:21:52 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 03:55:31 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ueda", "Masaya", ""], ["Kimura", "Akisato", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2103.14222", "submitter": "Chengzhi Mao", "authors": "Chengzhi Mao, Mia Chiquier, Hao Wang, Junfeng Yang, Carl Vondrick", "title": "Adversarial Attacks are Reversible with Natural Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find that images contain intrinsic structure that enables the reversal of\nmany adversarial attacks. Attack vectors cause not only image classifiers to\nfail, but also collaterally disrupt incidental structure in the image. We\ndemonstrate that modifying the attacked image to restore the natural structure\nwill reverse many types of attacks, providing a defense. Experiments\ndemonstrate significantly improved robustness for several state-of-the-art\nmodels across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results\nshow that our defense is still effective even if the attacker is aware of the\ndefense mechanism. Since our defense is deployed during inference instead of\ntraining, it is compatible with pre-trained networks as well as most other\ndefenses. Our results suggest deep networks are vulnerable to adversarial\nexamples partly because their representations do not enforce the natural\nstructure of images.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:21:40 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 02:34:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mao", "Chengzhi", ""], ["Chiquier", "Mia", ""], ["Wang", "Hao", ""], ["Yang", "Junfeng", ""], ["Vondrick", "Carl", ""]]}, {"id": "2103.14230", "submitter": "Chi Zhang", "authors": "Chi Zhang, Baoxiong Jia, Song-Chun Zhu, Yixin Zhu", "title": "Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and\n  Execution", "comments": "CVPR 2021 paper. Supplementary:\n  http://wellyzhang.github.io/attach/cvpr21zhang_prae_supp.pdf Project:\n  http://wellyzhang.github.io/project/prae.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial-temporal reasoning is a challenging task in Artificial Intelligence\n(AI) due to its demanding but unique nature: a theoretic requirement on\nrepresenting and reasoning based on spatial-temporal knowledge in mind, and an\napplied requirement on a high-level cognitive system capable of navigating and\nacting in space and time. Recent works have focused on an abstract reasoning\ntask of this kind -- Raven's Progressive Matrices (RPM). Despite the\nencouraging progress on RPM that achieves human-level performance in terms of\naccuracy, modern approaches have neither a treatment of human-like reasoning on\ngeneralization, nor a potential to generate answers. To fill in this gap, we\npropose a neuro-symbolic Probabilistic Abduction and Execution (PrAE) learner;\ncentral to the PrAE learner is the process of probabilistic abduction and\nexecution on a probabilistic scene representation, akin to the mental\nmanipulation of objects. Specifically, we disentangle perception and reasoning\nfrom a monolithic model. The neural visual perception frontend predicts\nobjects' attributes, later aggregated by a scene inference engine to produce a\nprobabilistic scene representation. In the symbolic logical reasoning backend,\nthe PrAE learner uses the representation to abduce the hidden rules. An answer\nis predicted by executing the rules on the probabilistic representation. The\nentire system is trained end-to-end in an analysis-by-synthesis manner without\nany visual attribute annotations. Extensive experiments demonstrate that the\nPrAE learner improves cross-configuration generalization and is capable of\nrendering an answer, in contrast to prior works that merely make a categorical\nchoice from candidates.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:42:18 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 01:47:55 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhang", "Chi", ""], ["Jia", "Baoxiong", ""], ["Zhu", "Song-Chun", ""], ["Zhu", "Yixin", ""]]}, {"id": "2103.14231", "submitter": "Chi Zhang", "authors": "Xu Xie, Chi Zhang, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu", "title": "Congestion-aware Multi-agent Trajectory Prediction for Collision\n  Avoidance", "comments": "ICRA 2021 paper. Project:\n  https://xuxie1031.github.io/projects/GTA/GTAProj.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting agents' future trajectories plays a crucial role in modern AI\nsystems, yet it is challenging due to intricate interactions exhibited in\nmulti-agent systems, especially when it comes to collision avoidance. To\naddress this challenge, we propose to learn congestion patterns as contextual\ncues explicitly and devise a novel \"Sense--Learn--Reason--Predict\" framework by\nexploiting advantages of three different doctrines of thought, which yields the\nfollowing desirable benefits: (i) Representing congestion as contextual cues\nvia latent factors subsumes the concept of social force commonly used in\nphysics-based approaches and implicitly encodes the distance as a cost, similar\nto the way a planning-based method models the environment. (ii) By decomposing\nthe learning phases into two stages, a \"student\" can learn contextual cues from\na \"teacher\" while generating collision-free trajectories. To make the framework\ncomputationally tractable, we formulate it as an optimization problem and\nderive an upper bound by leveraging the variational parametrization. In\nexperiments, we demonstrate that the proposed model is able to generate\ncollision-free trajectory predictions in a synthetic dataset designed for\ncollision avoidance evaluation and remains competitive on the commonly used\nNGSIM US-101 highway dataset.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:42:33 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Xie", "Xu", ""], ["Zhang", "Chi", ""], ["Zhu", "Yixin", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2103.14232", "submitter": "Chi Zhang", "authors": "Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, Yixin Zhu", "title": "ACRE: Abstract Causal REasoning Beyond Covariation", "comments": "CVPR 2021 paper. Supplementary:\n  http://wellyzhang.github.io/attach/cvpr21zhang_acre_supp.pdf Project:\n  http://wellyzhang.github.io/project/acre.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal induction, i.e., identifying unobservable mechanisms that lead to the\nobservable relations among variables, has played a pivotal role in modern\nscientific discovery, especially in scenarios with only sparse and limited\ndata. Humans, even young toddlers, can induce causal relationships surprisingly\nwell in various settings despite its notorious difficulty. However, in contrast\nto the commonplace trait of human cognition is the lack of a diagnostic\nbenchmark to measure causal induction for modern Artificial Intelligence (AI)\nsystems. Therefore, in this work, we introduce the Abstract Causal REasoning\n(ACRE) dataset for systematic evaluation of current vision systems in causal\ninduction. Motivated by the stream of research on causal discovery in Blicket\nexperiments, we query a visual reasoning system with the following four types\nof questions in either an independent scenario or an interventional scenario:\ndirect, indirect, screening-off, and backward-blocking, intentionally going\nbeyond the simple strategy of inducing causal relationships by covariation. By\nanalyzing visual reasoning architectures on this testbed, we notice that pure\nneural models tend towards an associative strategy under their chance-level\nperformance, whereas neuro-symbolic combinations struggle in backward-blocking\nreasoning. These deficiencies call for future research in models with a more\ncomprehensive capability of causal induction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:42:38 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhang", "Chi", ""], ["Jia", "Baoxiong", ""], ["Edmonds", "Mark", ""], ["Zhu", "Song-Chun", ""], ["Zhu", "Yixin", ""]]}, {"id": "2103.14242", "submitter": "Rumeng Yi", "authors": "Rumeng Yi, Yaping Huang, Qingji Guan, Mengyang Pu, Runsheng Zhang", "title": "Learning from Pixel-Level Label Noise: A New Perspective for\n  Semi-Supervised Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses semi-supervised semantic segmentation by exploiting a\nsmall set of images with pixel-level annotations (strong supervisions) and a\nlarge set of images with only image-level annotations (weak supervisions). Most\nexisting approaches aim to generate accurate pixel-level labels from weak\nsupervisions. However, we observe that those generated labels still inevitably\ncontain noisy labels. Motivated by this observation, we present a novel\nperspective and formulate this task as a problem of learning with pixel-level\nlabel noise. Existing noisy label methods, nevertheless, mainly aim at\nimage-level tasks, which can not capture the relationship between neighboring\nlabels in one image. Therefore, we propose a graph based label noise detection\nand correction framework to deal with pixel-level noisy labels. In particular,\nfor the generated pixel-level noisy labels from weak supervisions by Class\nActivation Map (CAM), we train a clean segmentation model with strong\nsupervisions to detect the clean labels from these noisy labels according to\nthe cross-entropy loss. Then, we adopt a superpixel-based graph to represent\nthe relations of spatial adjacency and semantic similarity between pixels in\none image. Finally we correct the noisy labels using a Graph Attention Network\n(GAT) supervised by detected clean labels. We comprehensively conduct\nexperiments on PASCAL VOC 2012, PASCAL-Context and MS-COCO datasets. The\nexperimental results show that our proposed semi supervised method achieves the\nstate-of-the-art performances and even outperforms the fully-supervised models\non PASCAL VOC 2012 and MS-COCO datasets in some cases.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 03:23:21 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yi", "Rumeng", ""], ["Huang", "Yaping", ""], ["Guan", "Qingji", ""], ["Pu", "Mengyang", ""], ["Zhang", "Runsheng", ""]]}, {"id": "2103.14247", "submitter": "Dewang Hou", "authors": "Dewang Hou, Yang Zhao, Yuyao Ye, Jiayu Yang, Jian Zhang, Ronggang Wang", "title": "Super-Resolving Compressed Video in Coding Chain", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling and lossy coding are widely used in video transmission and storage.\nPrevious methods for enhancing the resolution of such videos often ignore the\ninherent interference between resolution loss and compression artifacts, which\ncompromises perceptual video quality. To address this problem, we present a\nmixed-resolution coding framework, which cooperates with a reference-based\nDCNN. In this novel coding chain, the reference-based DCNN learns the direct\nmapping from low-resolution (LR) compressed video to their high-resolution (HR)\nclean version at the decoder side. We further improve reconstruction quality by\ndevising an efficient deformable alignment module with receptive field block to\nhandle various motion distances and introducing a disentangled loss that helps\nnetworks distinguish the artifact patterns from texture. Extensive experiments\ndemonstrate the effectiveness of proposed innovations by comparing with\nstate-of-the-art single image, video and reference-based restoration methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 03:39:54 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hou", "Dewang", ""], ["Zhao", "Yang", ""], ["Ye", "Yuyao", ""], ["Yang", "Jiayu", ""], ["Zhang", "Jian", ""], ["Wang", "Ronggang", ""]]}, {"id": "2103.14249", "submitter": "Yuichi Tanaka", "authors": "Yuya Sato, Takumi Ueda, Yuichi Tanaka", "title": "Marine Snow Removal Benchmarking Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces a new benchmarking dataset for marine snow removal of\nunderwater images. Marine snow is one of the main degradation sources of\nunderwater images that are caused by small particles, e.g., organic matter and\nsand, between the underwater scene and photosensors. We mathematically model\ntwo typical types of marine snow from the observations of real underwater\nimages. The modeled artifacts are synthesized with underwater images to\nconstruct large-scale pairs of ground-truth and degraded images to calculate\nobjective qualities for marine snow removal and to train a deep neural network.\nWe propose two marine snow removal tasks using the dataset and show the first\nbenchmarking results of marine snow removal. The Marine Snow Removal\nBenchmarking Dataset is publicly available online.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 03:54:43 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 22:34:11 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sato", "Yuya", ""], ["Ueda", "Takumi", ""], ["Tanaka", "Yuichi", ""]]}, {"id": "2103.14255", "submitter": "Myeongkyun Kang", "authors": "Myeongkyun Kang, Philip Chikontwe, Miguel Luna, Kyung Soo Hong, June\n  Hong Ahn, Sang Hyun Park", "title": "Mixing-AdaSIN: Constructing a de-biased dataset using Adaptive\n  Structural Instance Normalization and texture Mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the pandemic outbreak, several works have proposed to diagnose\nCOVID-19 with deep learning in computed tomography (CT); reporting performance\non-par with experts. However, models trained/tested on the same in-distribution\ndata may rely on the inherent data biases for successful prediction, failing to\ngeneralize on out-of-distribution samples or CT with different scanning\nprotocols. Early attempts have partly addressed bias-mitigation and\ngeneralization through augmentation or re-sampling, but are still limited by\ncollection costs and the difficulty of quantifying bias in medical images. In\nthis work, we propose Mixing-AdaSIN; a bias mitigation method that uses a\ngenerative model to generate de-biased images by mixing texture information\nbetween different labeled CT scans with semantically similar features. Here, we\nuse Adaptive Structural Instance Normalization (AdaSIN) to enhance de-biasing\ngeneration quality and guarantee structural consistency. Following, a\nclassifier trained with the generated images learns to correctly predict the\nlabel without bias and generalizes better. To demonstrate the efficacy of our\nmethod, we construct a biased COVID-19 vs. bacterial pneumonia dataset based on\nCT protocols and compare with existing state-of-the-art de-biasing methods. Our\nexperiments show that classifiers trained with de-biased generated images\nreport improved in-distribution performance and generalization on an external\nCOVID-19 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 04:40:14 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kang", "Myeongkyun", ""], ["Chikontwe", "Philip", ""], ["Luna", "Miguel", ""], ["Hong", "Kyung Soo", ""], ["Ahn", "June Hong", ""], ["Park", "Sang Hyun", ""]]}, {"id": "2103.14258", "submitter": "Pavel Tokmakov", "authors": "Pavel Tokmakov, Jie Li, Wolfram Burgard, Adrien Gaidon", "title": "Learning to Track with Object Permanence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tracking by detection, the dominant approach for online multi-object\ntracking, alternates between localization and re-identification steps. As a\nresult, it strongly depends on the quality of instantaneous observations, often\nfailing when objects are not fully visible. In contrast, tracking in humans is\nunderlined by the notion of object permanence: once an object is recognized, we\nare aware of its physical existence and can approximately localize it even\nunder full occlusions. In this work, we introduce an end-to-end trainable\napproach for joint object detection and tracking that is capable of such\nreasoning. We build on top of the recent CenterTrack architecture, which takes\npairs of frames as input, and extend it to videos of arbitrary length. To this\nend, we augment the model with a spatio-temporal, recurrent memory module,\nallowing it to reason about object locations and identities in the current\nframe using all the previous history. It is, however, not obvious how to train\nsuch an approach. We study this question on a new, large-scale, synthetic\ndataset for multi-object tracking, which provides ground truth annotations for\ninvisible objects, and propose several approaches for supervising tracking\nbehind occlusions. Our model, trained jointly on synthetic and real data,\noutperforms the state of the art on KITTI, and MOT17 datasets thanks to its\nrobustness to occlusions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 04:43:04 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Tokmakov", "Pavel", ""], ["Li", "Jie", ""], ["Burgard", "Wolfram", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2103.14259", "submitter": "Zheng Ge", "authors": "Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, Jian Sun", "title": "OTA: Optimal Transport Assignment for Object Detection", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in label assignment in object detection mainly seek to\nindependently define positive/negative training samples for each ground-truth\n(gt) object. In this paper, we innovatively revisit the label assignment from a\nglobal perspective and propose to formulate the assigning procedure as an\nOptimal Transport (OT) problem -- a well-studied topic in Optimization Theory.\nConcretely, we define the unit transportation cost between each demander\n(anchor) and supplier (gt) pair as the weighted summation of their\nclassification and regression losses. After formulation, finding the best\nassignment solution is converted to solve the optimal transport plan at minimal\ntransportation costs, which can be solved via Sinkhorn-Knopp Iteration. On\nCOCO, a single FCOS-ResNet-50 detector equipped with Optimal Transport\nAssignment (OTA) can reach 40.7% mAP under 1X scheduler, outperforming all\nother existing assigning methods. Extensive experiments conducted on COCO and\nCrowdHuman further validate the effectiveness of our proposed OTA, especially\nits superiority in crowd scenarios. The code is available at\nhttps://github.com/Megvii-BaseDetection/OTA.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 04:45:12 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Ge", "Zheng", ""], ["Liu", "Songtao", ""], ["Li", "Zeming", ""], ["Yoshie", "Osamu", ""], ["Sun", "Jian", ""]]}, {"id": "2103.14267", "submitter": "Peng Wang", "authors": "Peng Wang, Kai Han, Xiu-Shen Wei, Lei Zhang, Lei Wang", "title": "Contrastive Learning based Hybrid Networks for Long-Tailed Image\n  Classification", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Learning discriminative image representations plays a vital role in\nlong-tailed image classification because it can ease the classifier learning in\nimbalanced cases. Given the promising performance contrastive learning has\nshown recently in representation learning, in this work, we explore effective\nsupervised contrastive learning strategies and tailor them to learn better\nimage representations from imbalanced data in order to boost the classification\naccuracy thereon. Specifically, we propose a novel hybrid network structure\nbeing composed of a supervised contrastive loss to learn image representations\nand a cross-entropy loss to learn classifiers, where the learning is\nprogressively transited from feature learning to the classifier learning to\nembody the idea that better features make better classifiers. We explore two\nvariants of contrastive loss for feature learning, which vary in the forms but\nshare a common idea of pulling the samples from the same class together in the\nnormalized embedding space and pushing the samples from different classes\napart. One of them is the recently proposed supervised contrastive (SC) loss,\nwhich is designed on top of the state-of-the-art unsupervised contrastive loss\nby incorporating positive samples from the same class. The other is a\nprototypical supervised contrastive (PSC) learning strategy which addresses the\nintensive memory consumption in standard SC loss and thus shows more promise\nunder limited memory budget. Extensive experiments on three long-tailed\nclassification datasets demonstrate the advantage of the proposed contrastive\nlearning based hybrid networks in long-tailed classification.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 05:22:36 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wang", "Peng", ""], ["Han", "Kai", ""], ["Wei", "Xiu-Shen", ""], ["Zhang", "Lei", ""], ["Wang", "Lei", ""]]}, {"id": "2103.14268", "submitter": "Zhongwen Zhang", "authors": "Zhongwen Zhang, Dmitrii Marin, Maria Drangova, Yuri Boykov", "title": "Confluent Vessel Trees with Accurate Bifurcations", "comments": "13 pages, 14 figures, CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in unsupervised reconstruction of complex near-capillary\nvasculature with thousands of bifurcations where supervision and learning are\ninfeasible. Unsupervised methods can use many structural constraints, e.g.\ntopology, geometry, physics. Common techniques use variants of MST on geodesic\ntubular graphs minimizing symmetric pairwise costs, i.e. distances. We show\nlimitations of such standard undirected tubular graphs producing typical errors\nat bifurcations where flow \"directedness\" is critical. We introduce a new\ngeneral concept of confluence for continuous oriented curves forming vessel\ntrees and show how to enforce it on discrete tubular graphs. While confluence\nis a high-order property, we present an efficient practical algorithm for\nreconstructing confluent vessel trees using minimum arborescence on a directed\ngraph enforcing confluence via simple flow-extrapolating arc construction.\nEmpirical tests on large near-capillary sub-voxel vasculature volumes\ndemonstrate significantly improved reconstruction accuracy at bifurcations. Our\ncode has also been made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 05:22:56 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhang", "Zhongwen", ""], ["Marin", "Dmitrii", ""], ["Drangova", "Maria", ""], ["Boykov", "Yuri", ""]]}, {"id": "2103.14269", "submitter": "Peishan Cong", "authors": "Peishan Cong, Xinge Zhu, Yuexin Ma", "title": "Input-Output Balanced Framework for Long-tailed LiDAR Semantic\n  Segmentation", "comments": "Accepted by ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A thorough and holistic scene understanding is crucial for autonomous\nvehicles, where LiDAR semantic segmentation plays an indispensable role.\nHowever, most existing methods focus on the network design while neglecting the\ninherent difficulty, imbalanced data distribution in the realistic dataset\n(also named long-tailed distribution), which narrows down the capability of\nstate-of-the-art methods. In this paper, we propose an input-output balanced\nframework to handle the issue of long-tailed distribution. Specifically, for\nthe input space, we synthesize these tailed instances from mesh models and well\nsimulate the position and density distribution of LiDAR scan, which enhances\nthe input data balance and improves the data diversity. For the output space, a\nmulti-head block is proposed to group different categories based on their\nshapes and instance amounts, which alleviates the biased representation of\ndominating category during the feature learning. We evaluate the proposed model\non two large-scale datasets, SemanticKITTI and nuScenes, where state-of-the-art\nresults demonstrate its effectiveness. The proposed new modules can also be\nused as a plug-and-play, and we apply them on various backbones and datasets,\nshowing its good generalization ability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 05:42:11 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Cong", "Peishan", ""], ["Zhu", "Xinge", ""], ["Ma", "Yuexin", ""]]}, {"id": "2103.14273", "submitter": "Jani Boutellier", "authors": "Abol Basher, Muhammad Sarmad, Jani Boutellier", "title": "LightSAL: Lightweight Sign Agnostic Learning for Implicit Surface\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several works have addressed modeling of 3D shapes using deep\nneural networks to learn implicit surface representations. Up to now, the\nmajority of works have concentrated on reconstruction quality, paying little or\nno attention to model size or training time. This work proposes LightSAL, a\nnovel deep convolutional architecture for learning 3D shapes; the proposed work\nconcentrates on efficiency both in network training time and resulting model\nsize. We build on the recent concept of Sign Agnostic Learning for training the\nproposed network, relying on signed distance fields, with unsigned distance as\nground truth. In the experimental section of the paper, we demonstrate that the\nproposed architecture outperforms previous work in model size and number of\nrequired training iterations, while achieving equivalent accuracy. Experiments\nare based on the D-Faust dataset that contains 41k 3D scans of human shapes.\nThe proposed model has been implemented in PyTorch.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 05:50:14 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Basher", "Abol", ""], ["Sarmad", "Muhammad", ""], ["Boutellier", "Jani", ""]]}, {"id": "2103.14275", "submitter": "Shengkun Tang", "authors": "Puyuan Yi, Shengkun Tang and Jian Yao", "title": "DDR-Net: Learning Multi-Stage Multi-View Stereo With Dynamic Depth Range", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To obtain high-resolution depth maps, some previous learning-based multi-view\nstereo methods build a cost volume pyramid in a coarse-to-fine manner. These\napproaches leverage fixed depth range hypotheses to construct cascaded plane\nsweep volumes. However, it is inappropriate to set identical range hypotheses\nfor each pixel since the uncertainties of previous per-pixel depth predictions\nare spatially varying. Distinct from these approaches, we propose a Dynamic\nDepth Range Network (DDR-Net) to determine the depth range hypotheses\ndynamically by applying a range estimation module (REM) to learn the\nuncertainties of range hypotheses in the former stages. Specifically, in our\nDDR-Net, we first build an initial depth map at the coarsest resolution of an\nimage across the entire depth range. Then the range estimation module (REM)\nleverages the probability distribution information of the initial depth to\nestimate the depth range hypotheses dynamically for the following stages.\nMoreover, we develop a novel loss strategy, which utilizes learned dynamic\ndepth ranges to generate refined depth maps, to keep the ground truth value of\neach pixel covered in the range hypotheses of the next stage. Extensive\nexperimental results show that our method achieves superior performance over\nother state-of-the-art methods on the DTU benchmark and obtains comparable\nresults on the Tanks and Temples benchmark. The code is available at\nhttps://github.com/Tangshengku/DDR-Net.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 05:52:38 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yi", "Puyuan", ""], ["Tang", "Shengkun", ""], ["Yao", "Jian", ""]]}, {"id": "2103.14283", "submitter": "Lin Shao", "authors": "Yifan You, Lin Shao, Toki Migimatsu, Jeannette Bohg", "title": "OmniHang: Learning to Hang Arbitrary Objects using Contact Point\n  Correspondences and Neural Collision Estimation", "comments": "Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we explore whether a robot can learn to hang arbitrary objects\nonto a diverse set of supporting items such as racks or hooks. Endowing robots\nwith such an ability has applications in many domains such as domestic\nservices, logistics, or manufacturing. Yet, it is a challenging manipulation\ntask due to the large diversity of geometry and topology of everyday objects.\nIn this paper, we propose a system that takes partial point clouds of an object\nand a supporting item as input and learns to decide where and how to hang the\nobject stably. Our system learns to estimate the contact point correspondences\nbetween the object and supporting item to get an estimated stable pose. We then\nrun a deep reinforcement learning algorithm to refine the predicted stable\npose. Then, the robot needs to find a collision-free path to move the object\nfrom its initial pose to stable hanging pose. To this end, we train a neural\nnetwork based collision estimator that takes as input partial point clouds of\nthe object and supporting item. We generate a new and challenging, large-scale,\nsynthetic dataset annotated with stable poses of objects hung on various\nsupporting items and their contact point correspondences. In this dataset, we\nshow that our system is able to achieve a 68.3% success rate of predicting\nstable object poses and has a 52.1% F1 score in terms of finding feasible\npaths. Supplemental material and videos are available on our project webpage.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 06:11:05 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["You", "Yifan", ""], ["Shao", "Lin", ""], ["Migimatsu", "Toki", ""], ["Bohg", "Jeannette", ""]]}, {"id": "2103.14286", "submitter": "Ming Zhang", "authors": "Ming Zhang, Mingming Zhang, Yiming Chen, Mingyang Li", "title": "IMU Data Processing For Inertial Aided Navigation: A Recurrent Neural\n  Network Based Approach", "comments": "IEEE International Conference on Robotics and Automation (ICRA),\n  Xi'an, China, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a novel method for performing inertial aided\nnavigation, by using deep neural networks (DNNs). To date, most DNN inertial\nnavigation methods focus on the task of inertial odometry, by taking gyroscope\nand accelerometer readings as input and regressing for integrated IMU poses\n(i.e., position and orientation). While this design has been successfully\napplied on a number of applications, it is not of theoretical performance\nguarantee unless patterned motion is involved. This inevitably leads to\nsignificantly reduced accuracy and robustness in certain use cases. To solve\nthis problem, we design a framework to compute observable IMU integration terms\nusing DNNs, followed by the numerical pose integration and sensor fusion to\nachieve the performance gain. Specifically, we perform detailed analysis on the\nmotion terms in IMU kinematic equations, propose a dedicated network design,\nloss functions, and training strategies for the IMU data processing, and\nconduct extensive experiments. The results show that our method is generally\napplicable and outperforms both traditional and DNN methods by wide margins.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 06:21:37 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhang", "Ming", ""], ["Zhang", "Mingming", ""], ["Chen", "Yiming", ""], ["Li", "Mingyang", ""]]}, {"id": "2103.14301", "submitter": "Kaleem Nawaz Khan Mr.", "authors": "Muhammad Islam, Kaleem Nawaz Khan, Muhammad Salman Khan", "title": "Evaluation of Preprocessing Techniques for U-Net Based Automated Liver\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To extract liver from medical images is a challenging task due to similar\nintensity values of liver with adjacent organs, various contrast levels,\nvarious noise associated with medical images and irregular shape of liver. To\naddress these issues, it is important to preprocess the medical images, i.e.,\ncomputerized tomography (CT) and magnetic resonance imaging (MRI) data prior to\nliver analysis and quantification. This paper investigates the impact of\npermutation of various preprocessing techniques for CT images, on the automated\nliver segmentation using deep learning, i.e., U-Net architecture. The study\nfocuses on Hounsfield Unit (HU) windowing, contrast limited adaptive histogram\nequalization (CLAHE), z-score normalization, median filtering and\nBlock-Matching and 3D (BM3D) filtering. The segmented results show that\ncombination of three techniques; HU-windowing, median filtering and z-score\nnormalization achieve optimal performance with Dice coefficient of 96.93%,\n90.77% and 90.84% for training, validation and testing respectively.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 07:31:25 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Islam", "Muhammad", ""], ["Khan", "Kaleem Nawaz", ""], ["Khan", "Muhammad Salman", ""]]}, {"id": "2103.14304", "submitter": "Wenhao Li", "authors": "Wenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, Pichao Wang, Wenming\n  Yang", "title": "Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite great progress in 3D human pose estimation from videos, it is still\nan open problem to take full advantage of redundant 2D pose sequences to learn\nrepresentative representation for generating one single 3D pose. To this end,\nwe propose an improved Transformer-based architecture, called Strided\nTransformer, for 3D human pose estimation in videos to lift a sequence of 2D\njoint locations to a 3D pose. Specifically, a vanilla Transformer encoder (VTE)\nis adopted to model long-range dependencies of 2D pose sequences. To reduce\nredundancy of the sequence and aggregate information from local context,\nstrided convolutions are incorporated into VTE to progressively reduce the\nsequence length. The modified VTE is termed as strided Transformer encoder\n(STE) which is built upon the outputs of VTE. STE not only effectively\naggregates long-range information to a single-vector representation in a\nhierarchical global and local fashion but also significantly reduces the\ncomputation cost. Furthermore, a full-to-single supervision scheme is designed\nat both the full sequence scale and single target frame scale, applied to the\noutputs of VTE and STE, respectively. This scheme imposes extra temporal\nsmoothness constraints in conjunction with the single target frame supervision\nand improves the representation ability of features for the target frame. The\nproposed architecture is evaluated on two challenging benchmark datasets,\nHuman3.6M and HumanEva-I, and achieves state-of-the-art results with much fewer\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 07:35:08 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:04:12 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 13:09:46 GMT"}, {"version": "v4", "created": "Sat, 10 Apr 2021 17:23:03 GMT"}, {"version": "v5", "created": "Wed, 14 Apr 2021 01:23:08 GMT"}, {"version": "v6", "created": "Fri, 16 Apr 2021 02:33:47 GMT"}, {"version": "v7", "created": "Thu, 22 Jul 2021 10:15:11 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Li", "Wenhao", ""], ["Liu", "Hong", ""], ["Ding", "Runwei", ""], ["Liu", "Mengyuan", ""], ["Wang", "Pichao", ""], ["Yang", "Wenming", ""]]}, {"id": "2103.14314", "submitter": "Zi Jian Yew", "authors": "Zi Jian Yew and Gim Hee Lee", "title": "City-scale Scene Change Detection using Point Clouds", "comments": "8 pages, 10 figures. To be presented at ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for detecting structural changes in a city using images\ncaptured from vehicular mounted cameras over traversals at two different times.\nWe first generate 3D point clouds for each traversal from the images and\napproximate GNSS/INS readings using Structure-from-Motion (SfM). A direct\ncomparison of the two point clouds for change detection is not ideal due to\ninaccurate geo-location information and possible drifts in the SfM. To\ncircumvent this problem, we propose a deep learning-based non-rigid\nregistration on the point clouds which allows us to compare the point clouds\nfor structural change detection in the scene. Furthermore, we introduce a dual\nthresholding check and post-processing step to enhance the robustness of our\nmethod. We collect two datasets for the evaluation of our approach. Experiments\nshow that our method is able to detect scene changes effectively, even in the\npresence of viewpoint and illumination differences.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 08:04:13 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yew", "Zi Jian", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2103.14326", "submitter": "Wenbo Hu", "authors": "Wenbo Hu, Hengshuang Zhao, Li Jiang, Jiaya Jia, Tien-Tsin Wong", "title": "Bidirectional Projection Network for Cross Dimension Scene Understanding", "comments": "CVPR 2021 (Oral)", "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D image representations are in regular grids and can be processed\nefficiently, whereas 3D point clouds are unordered and scattered in 3D space.\nThe information inside these two visual domains is well complementary, e.g., 2D\nimages have fine-grained texture while 3D point clouds contain plentiful\ngeometry information. However, most current visual recognition systems process\nthem individually. In this paper, we present a \\emph{bidirectional projection\nnetwork (BPNet)} for joint 2D and 3D reasoning in an end-to-end manner. It\ncontains 2D and 3D sub-networks with symmetric architectures, that are\nconnected by our proposed \\emph{bidirectional projection module (BPM)}. Via the\n\\emph{BPM}, complementary 2D and 3D information can interact with each other in\nmultiple architectural levels, such that advantages in these two visual domains\ncan be combined for better scene recognition. Extensive quantitative and\nqualitative experimental evaluations show that joint reasoning over 2D and 3D\nvisual domains can benefit both 2D and 3D scene understanding simultaneously.\nOur \\emph{BPNet} achieves top performance on the ScanNetV2 benchmark for both\n2D and 3D semantic segmentation. Code is available at\n\\url{https://github.com/wbhu/BPNet}.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 08:31:39 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hu", "Wenbo", ""], ["Zhao", "Hengshuang", ""], ["Jiang", "Li", ""], ["Jia", "Jiaya", ""], ["Wong", "Tien-Tsin", ""]]}, {"id": "2103.14332", "submitter": "Dohun Lim", "authors": "Dohun Lim, Hyeonseok Lee and Sungchan Kim", "title": "Building Reliable Explanations of Unreliable Neural Networks: Locally\n  Smoothing Perspective of Model Interpretation", "comments": "Accepted to CVPR 2021. The supplementary materials are included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for reliably explaining the predictions of neural\nnetworks. We consider an explanation reliable if it identifies input features\nrelevant to the model output by considering the input and the neighboring data\npoints. Our method is built on top of the assumption of smooth landscape in a\nloss function of the model prediction: locally consistent loss and gradient\nprofile. A theoretical analysis established in this study suggests that those\nlocally smooth model explanations are learned using a batch of noisy copies of\nthe input with the L1 regularization for a saliency map. Extensive experiments\nsupport the analysis results, revealing that the proposed saliency maps\nretrieve the original classes of adversarial examples crafted against both\nnaturally and adversarially trained models, significantly outperforming\nprevious methods. We further demonstrated that such good performance results\nfrom the learning capability of this method to identify input features that are\ntruly relevant to the model output of the input and the neighboring data\npoints, fulfilling the requirements of a reliable explanation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 08:52:11 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 01:59:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lim", "Dohun", ""], ["Lee", "Hyeonseok", ""], ["Kim", "Sungchan", ""]]}, {"id": "2103.14333", "submitter": "Hiroki Sakuma", "authors": "Hiroki Sakuma and Yoshinori Konishi", "title": "Geometry-Aware Unsupervised Domain Adaptation for Stereo Matching", "comments": "Accepted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently proposed DNN-based stereo matching methods that learn priors\ndirectly from data are known to suffer a drastic drop in accuracy in new\nenvironments. Although supervised approaches with ground truth disparity maps\noften work well, collecting them in each deployment environment is cumbersome\nand costly. For this reason, many unsupervised domain adaptation methods based\non image-to-image translation have been proposed, but these methods do not\npreserve the geometric structure of a stereo image pair because the\nimage-to-image translation is applied to each view separately. To address this\nproblem, in this paper, we propose an attention mechanism that aggregates\nfeatures in the left and right views, called Stereoscopic Cross Attention\n(SCA). Incorporating SCA to an image-to-image translation network makes it\npossible to preserve the geometric structure of a stereo image pair in the\nprocess of the image-to-image translation. We empirically demonstrate the\neffectiveness of the proposed unsupervised domain adaptation based on the\nimage-to-image translation with SCA.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 08:53:36 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sakuma", "Hiroki", ""], ["Konishi", "Yoshinori", ""]]}, {"id": "2103.14337", "submitter": "Yangyang Qin", "authors": "Yangyang Qin, Hefei Ling, Zhenghai He, Yuxuan Shi, Lei Wu", "title": "Hands-on Guidance for Distilling Object Detectors", "comments": "Accepted at ICME2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation can lead to deploy-friendly networks against the\nplagued computational complexity problem, but previous methods neglect the\nfeature hierarchy in detectors. Motivated by this, we propose a general\nframework for detection distillation. Our method, called Hands-on Guidance\nDistillation, distills the latent knowledge of all stage features for imposing\nmore comprehensive supervision, and focuses on the essence simultaneously for\npromoting more intense knowledge absorption. Specifically, a series of novel\nmechanisms are designed elaborately, including correspondence establishment for\nconsistency, hands-on imitation loss measure and re-weighted optimization from\nboth micro and macro perspectives. We conduct extensive evaluations with\ndifferent distillation configurations over VOC and COCO datasets, which show\nbetter performance on accuracy and speed trade-offs. Meanwhile, feasibility\nexperiments on different structural networks further prove the robustness of\nour HGD.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:00:23 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 09:14:27 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Qin", "Yangyang", ""], ["Ling", "Hefei", ""], ["He", "Zhenghai", ""], ["Shi", "Yuxuan", ""], ["Wu", "Lei", ""]]}, {"id": "2103.14338", "submitter": "Zhichao Huang", "authors": "Zhichao Huang, Xintong Han, Jia Xu, Tong Zhang", "title": "Few-Shot Human Motion Transfer by Personalized Geometry and Texture\n  Modeling", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for few-shot human motion transfer that achieves\nrealistic human image generation with only a small number of appearance inputs.\nDespite recent advances in single person motion transfer, prior methods often\nrequire a large number of training images and take long training time. One\npromising direction is to perform few-shot human motion transfer, which only\nneeds a few of source images for appearance transfer. However, it is\nparticularly challenging to obtain satisfactory transfer results. In this\npaper, we address this issue by rendering a human texture map to a surface\ngeometry (represented as a UV map), which is personalized to the source person.\nOur geometry generator combines the shape information from source images, and\nthe pose information from 2D keypoints to synthesize the personalized UV map. A\ntexture generator then generates the texture map conditioned on the texture of\nsource images to fill out invisible parts. Furthermore, we may fine-tune the\ntexture map on the manifold of the texture generator from a few source images\nat the test time, which improves the quality of the texture map without\nover-fitting or artifacts. Extensive experiments show the proposed method\noutperforms state-of-the-art methods both qualitatively and quantitatively. Our\ncode is available at https://github.com/HuangZhiChao95/FewShotMotionTransfer.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:01:33 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Huang", "Zhichao", ""], ["Han", "Xintong", ""], ["Xu", "Jia", ""], ["Zhang", "Tong", ""]]}, {"id": "2103.14339", "submitter": "Akshay Smit", "authors": "Akshay Smit, Damir Vrabac, Yujie He, Andrew Y. Ng, Andrew L. Beam,\n  Pranav Rajpurkar", "title": "MedSelect: Selective Labeling for Medical Image Classification Combining\n  Meta-Learning with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a selective learning method using meta-learning and deep\nreinforcement learning for medical image interpretation in the setting of\nlimited labeling resources. Our method, MedSelect, consists of a trainable deep\nlearning selector that uses image embeddings obtained from contrastive\npretraining for determining which images to label, and a non-parametric\nselector that uses cosine similarity to classify unseen images. We demonstrate\nthat MedSelect learns an effective selection strategy outperforming baseline\nselection strategies across seen and unseen medical conditions for chest X-ray\ninterpretation. We also perform an analysis of the selections performed by\nMedSelect comparing the distribution of latent embeddings and clinical\nfeatures, and find significant differences compared to the strongest performing\nbaseline. We believe that our method may be broadly applicable across medical\nimaging settings where labels are expensive to acquire.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:09:34 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Smit", "Akshay", ""], ["Vrabac", "Damir", ""], ["He", "Yujie", ""], ["Ng", "Andrew Y.", ""], ["Beam", "Andrew L.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2103.14341", "submitter": "Baoquan Zhang", "authors": "Baoquan Zhang, Xutao Li, Yunming Ye, Shanshan Feng, Rui Ye", "title": "MetaNODE: Prototype Optimization as a Neural ODE for Few-Shot Learning", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-Shot Learning (FSL) is a challenging task, i.e., how to recognize novel\nclasses with few examples? Pre-training based methods effectively tackle the\nproblem by pre-training a feature extractor and then predict novel classes via\na nearest neighbor classifier with mean-based prototypes. Nevertheless, due to\nthe data scarcity, the mean-based prototypes are usually biased. In this paper,\nwe diminish the bias by regarding it as a prototype optimization problem.\nAlthough the existing meta-optimizers can also be applied for the optimization,\nthey all overlook a crucial gradient bias issue, i.e., the mean-based gradient\nestimation is also biased on scarce data. Consequently, we regard the gradient\nitself as meta-knowledge and then propose a novel prototype optimization-based\nmeta-learning framework, called MetaNODE. Specifically, we first regard the\nmean-based prototypes as initial prototypes, and then model the process of\nprototype optimization as continuous-time dynamics specified by a Neural\nOrdinary Differential Equation (Neural ODE). A gradient flow inference network\nis carefully designed to learn to estimate the continuous gradients for\nprototype dynamics. Finally, the optimal prototypes can be obtained by solving\nthe Neural ODE using the Runge-Kutta method. Extensive experiments demonstrate\nthat our proposed method obtains superior performance over the previous\nstate-of-the-art methods. Our code will be publicly available upon acceptance.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:16:46 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhang", "Baoquan", ""], ["Li", "Xutao", ""], ["Ye", "Yunming", ""], ["Feng", "Shanshan", ""], ["Ye", "Rui", ""]]}, {"id": "2103.14347", "submitter": "Motasem Alfarra Alfarra M", "authors": "Motasem Alfarra, Juan C. P\\'erez, Ali Thabet, Adel Bibi, Philip H. S.\n  Torr, Bernard Ghanem", "title": "Combating Adversaries with Anti-Adversaries", "comments": "15 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks are vulnerable to small input perturbations known as\nadversarial attacks. Inspired by the fact that these adversaries are\nconstructed by iteratively minimizing the confidence of a network for the true\nclass label, we propose the anti-adversary layer, aimed at countering this\neffect. In particular, our layer generates an input perturbation in the\nopposite direction of the adversarial one, and feeds the classifier a perturbed\nversion of the input. Our approach is training-free and theoretically\nsupported. We verify the effectiveness of our approach by combining our layer\nwith both nominally and robustly trained models, and conduct large scale\nexperiments from black-box to adaptive attacks on CIFAR10, CIFAR100 and\nImageNet. Our anti-adversary layer significantly enhances model robustness\nwhile coming at no cost on clean accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:36:59 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Alfarra", "Motasem", ""], ["P\u00e9rez", "Juan C.", ""], ["Thabet", "Ali", ""], ["Bibi", "Adel", ""], ["Torr", "Philip H. S.", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2103.14357", "submitter": "Jiayi Tian", "authors": "Jiayi Tian, Jing Zhang, Wen Li, Dong Xu", "title": "VDM-DA: Virtual Domain Modeling for Source Data-free Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims to leverage a label-rich domain (the source domain) to\nhelp model learning in a label-scarce domain (the target domain). Most domain\nadaptation methods require the co-existence of source and target domain samples\nto reduce the distribution mismatch, however, access to the source domain\nsamples may not always be feasible in the real world applications due to\ndifferent problems (e.g., storage, transmission, and privacy issues). In this\nwork, we deal with the source data-free unsupervised domain adaptation problem,\nand propose a novel approach referred to as Virtual Domain Modeling (VDM-DA).\nThe virtual domain acts as a bridge between the source and target domains. On\none hand, we generate virtual domain samples based on an approximated Gaussian\nMixture Model (GMM) in the feature space with the pre-trained source model,\nsuch that the virtual domain maintains a similar distribution with the source\ndomain without accessing to the original source data. On the other hand, we\nalso design an effective distribution alignment method to reduce the\ndistribution divergence between the virtual domain and the target domain by\ngradually improving the compactness of the target domain distribution through\nmodel learning. In this way, we successfully achieve the goal of distribution\nalignment between the source and target domains by training deep networks\nwithout accessing to the source domain data. We conduct extensive experiments\non benchmark datasets for both 2D image-based and 3D point cloud-based\ncross-domain object recognition tasks, where the proposed method referred to\nDomain Adaptation with Virtual Domain Modeling (VDM-DA) achieves the\nstate-of-the-art performances on all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:56:40 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Tian", "Jiayi", ""], ["Zhang", "Jing", ""], ["Li", "Wen", ""], ["Xu", "Dong", ""]]}, {"id": "2103.14373", "submitter": "Lanpeng Jia", "authors": "Youwei Li, Haibin Huang, Lanpeng Jia, Haoqiang Fan and Shuaicheng Liu", "title": "D2C-SR: A Divergence to Convergence Approach for Image Super-Resolution", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present D2C-SR, a novel framework for the task of image\nsuper-resolution(SR). As an ill-posed problem, the key challenge for\nsuper-resolution related tasks is there can be multiple predictions for a given\nlow-resolution input. Most classical methods and early deep learning based\napproaches ignored this fundamental fact and modeled this problem as a\ndeterministic processing which often lead to unsatisfactory results. Inspired\nby recent works like SRFlow, we tackle this problem in a semi-probabilistic\nmanner and propose a two-stage pipeline: a divergence stage is used to learn\nthe distribution of underlying high-resolution outputs in a discrete form, and\na convergence stage is followed to fuse the learned predictions into a final\noutput. More specifically, we propose a tree-based structure deep network,\nwhere each branch is designed to learn a possible high-resolution prediction.\nAt the divergence stage, each branch is trained separately to fit ground truth,\nand a triple loss is used to enforce the outputs from different branches\ndivergent. Subsequently, we add a fuse module to combine the multiple\npredictions as the outputs from the first stage can be sub-optimal. The fuse\nmodule can be trained to converge w.r.t the final high-resolution image in an\nend-to-end manner. We conduct evaluations on several benchmarks, including a\nnew proposed dataset with 8x upscaling factor. Our experiments demonstrate that\nD2C-SR can achieve state-of-the-art performance on PSNR and SSIM, with a\nsignificantly less computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:20:28 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Li", "Youwei", ""], ["Huang", "Haibin", ""], ["Jia", "Lanpeng", ""], ["Fan", "Haoqiang", ""], ["Liu", "Shuaicheng", ""]]}, {"id": "2103.14417", "submitter": "Emanuela Haller", "authors": "Emanuela Haller, Elena Burceanu, Marius Leordeanu", "title": "Unsupervised Domain Adaptation through Iterative Consensus Shift in a\n  Multi-Task Graph", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Babies learn with very little supervision by observing the surrounding world.\nThey synchronize the feedback from all their senses and learn to maintain\nconsistency and stability among their internal states. Such observations\ninspired recent works in multi-task and multi-modal learning, but existing\nmethods rely on expensive manual supervision. In contrast, our proposed\nmulti-task graph, with consensus shift learning, relies only on pseudo-labels\nprovided by expert models. In our graph, every node represents a task, and\nevery edge learns to transform one input node into another. Once initialized,\nthe graph learns by itself on virtually any novel target domain. An adaptive\nselection mechanism finds consensus among multiple paths reaching a given node\nand establishes the pseudo-ground truth at that node. Such pseudo-labels, given\nby ensemble pathways in the graph, are used during the next learning iteration\nwhen single edges distill this distributed knowledge. We validate our key\ncontributions experimentally and demonstrate strong performance on the Replica\ndataset, superior to the very few published methods on multi-task learning with\nminimal supervision.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 11:57:42 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Haller", "Emanuela", ""], ["Burceanu", "Elena", ""], ["Leordeanu", "Marius", ""]]}, {"id": "2103.14420", "submitter": "Annika Meyer", "authors": "Annika Meyer, Philipp Skudlik, Jan-Hendrik Pauls, Christoph Stiller", "title": "YOLinO: Generic Single Shot Polyline Detection in Real Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of polylines in images is usually either bound to branchless\npolylines or formulated in a recurrent way, prohibiting their use in real-time\nsystems.\n  We propose an approach that transfers the idea of single shot object\ndetection. Reformulating the problem of polyline detection as bottom-up\ncomposition of small line segments allows to detect bounded, dashed and\ncontinuous polylines with a single head. This has several major advantages over\nprevious methods. Not only is the method at 187 fps more than suited for\nreal-time applications with virtually any restriction on the shapes of the\ndetected polylines. By predicting multiple line segments for each spatial cell,\neven branching or crossing polylines can be detected.\n  We evaluate our approach on three different applications for road marking,\nlane border and center line detection. Hereby, we demonstrate the ability to\ngeneralize to different domains as well as both implicit and explicit polyline\ndetection tasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:00:26 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Meyer", "Annika", ""], ["Skudlik", "Philipp", ""], ["Pauls", "Jan-Hendrik", ""], ["Stiller", "Christoph", ""]]}, {"id": "2103.14422", "submitter": "Tamir Blum", "authors": "Tamir Blum, Gabin Paillet, Watcharawut Masawat, Mickael Laine and\n  Kazuya Yoshida", "title": "SegVisRL: Visuomotor Development for a Lunar Rover for Hazard Avoidance\n  using Camera Images", "comments": "9 pages including references. 8 images, 2 tables. Workshop submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visuomotor system of any animal is critical for its survival, and the\ndevelopment of a complex one within humans is large factor in our success as a\nspecies on Earth. This system is an essential part of our ability to adapt to\nour environment. We use this system continuously throughout the day, when\npicking something up, or walking around while avoiding bumping into objects.\nEquipping robots with such capabilities will help produce more intelligent\nlocomotion with the ability to more easily understand their surroundings and to\nmove safely. In particular, such capabilities are desirable for traversing the\nlunar surface, as it is full of hazardous obstacles, such as rocks. These\nobstacles need to be identified and avoided in real time. This paper seeks to\ndemonstrate the development of a visuomotor system within a robot for\nnavigation and obstacle avoidance, with complex rock shaped objects\nrepresenting hazards. Our approach uses deep reinforcement learning with only\nimage data. In this paper, we compare the results from several neural network\narchitectures and a preprocessing methodology which includes producing a\nsegmented image and downsampling.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:01:42 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Blum", "Tamir", ""], ["Paillet", "Gabin", ""], ["Masawat", "Watcharawut", ""], ["Laine", "Mickael", ""], ["Yoshida", "Kazuya", ""]]}, {"id": "2103.14431", "submitter": "Zihui Xue", "authors": "Zihui Xue, Sucheng Ren, Zhengqi Gao and Hang Zhao", "title": "Multimodal Knowledge Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of multimodal sensors and the accessibility of the Internet\nhave brought us a massive amount of unlabeled multimodal data. Since existing\ndatasets and well-trained models are primarily unimodal, the modality gap\nbetween a unimodal network and unlabeled multimodal data poses an interesting\nproblem: how to transfer a pre-trained unimodal network to perform the same\ntask on unlabeled multimodal data? In this work, we propose multimodal\nknowledge expansion (MKE), a knowledge distillation-based framework to\neffectively utilize multimodal data without requiring labels. Opposite to\ntraditional knowledge distillation, where the student is designed to be\nlightweight and inferior to the teacher, we observe that a multimodal student\nmodel consistently denoises pseudo labels and generalizes better than its\nteacher. Extensive experiments on four tasks and different modalities verify\nthis finding. Furthermore, we connect the mechanism of MKE to semi-supervised\nlearning and offer both empirical and theoretical explanations to understand\nthe denoising capability of a multimodal student.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:32:07 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 03:10:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xue", "Zihui", ""], ["Ren", "Sucheng", ""], ["Gao", "Zhengqi", ""], ["Zhao", "Hang", ""]]}, {"id": "2103.14441", "submitter": "Youngeun Kim", "authors": "Youngeun Kim, Priyadarshini Panda", "title": "Visual Explanations from Spiking Neural Networks using Interspike\n  Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spiking Neural Networks (SNNs) compute and communicate with asynchronous\nbinary temporal events that can lead to significant energy savings with\nneuromorphic hardware. Recent algorithmic efforts on training SNNs have shown\ncompetitive performance on a variety of classification tasks. However, a\nvisualization tool for analysing and explaining the internal spike behavior of\nsuch temporal deep SNNs has not been explored. In this paper, we propose a new\nconcept of bio-plausible visualization for SNNs, called Spike Activation Map\n(SAM). The proposed SAM circumvents the non-differentiable characteristic of\nspiking neurons by eliminating the need for calculating gradients to obtain\nvisual explanations. Instead, SAM calculates a temporal visualization map by\nforward propagating input spikes over different time-steps. SAM yields an\nattention map corresponding to each time-step of input data by highlighting\nneurons with short inter-spike interval activity. Interestingly, without both\nthe backpropagation process and the class label, SAM highlights the\ndiscriminative region of the image while capturing fine-grained details. With\nSAM, for the first time, we provide a comprehensive analysis on how internal\nspikes work in various SNN training configurations depending on optimization\ntypes, leak behavior, as well as when faced with adversarial examples.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:49:46 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kim", "Youngeun", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2103.14470", "submitter": "Zhanghui Kuang", "authors": "Hongbin Sun, Zhanghui Kuang, Xiaoyu Yue, Chenhao Lin and Wayne Zhang", "title": "Spatial Dual-Modality Graph Reasoning for Key Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key information extraction from document images is of paramount importance in\noffice automation. Conventional template matching based approaches fail to\ngeneralize well to document images of unseen templates, and are not robust\nagainst text recognition errors. In this paper, we propose an end-to-end\nSpatial Dual-Modality Graph Reasoning method (SDMG-R) to extract key\ninformation from unstructured document images. We model document images as\ndual-modality graphs, nodes of which encode both the visual and textual\nfeatures of detected text regions, and edges of which represent the spatial\nrelations between neighboring text regions. The key information extraction is\nsolved by iteratively propagating messages along graph edges and reasoning the\ncategories of graph nodes. In order to roundly evaluate our proposed method as\nwell as boost the future research, we release a new dataset named WildReceipt,\nwhich is collected and annotated tailored for the evaluation of key information\nextraction from document images of unseen templates in the wild. It contains 25\nkey information categories, a total of about 69000 text boxes, and is about 2\ntimes larger than the existing public datasets. Extensive experiments validate\nthat all information including visual features, textual features and spatial\nrelations can benefit key information extraction. It has been shown that SDMG-R\ncan effectively extract key information from document images of unseen\ntemplates, and obtain new state-of-the-art results on the recent popular\nbenchmark SROIE and our WildReceipt. Our code and dataset will be publicly\nreleased.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:46:00 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sun", "Hongbin", ""], ["Kuang", "Zhanghui", ""], ["Yue", "Xiaoyu", ""], ["Lin", "Chenhao", ""], ["Zhang", "Wayne", ""]]}, {"id": "2103.14471", "submitter": "Taewon Kang", "authors": "Taewon Kang", "title": "Multiple GAN Inversion for Exemplar-based Image-to-Image Translation", "comments": "8 pages, 8 figures, extended version of arXiv:2011.09330", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing state-of-the-art techniques in exemplar-based image-to-image\ntranslation have several critical problems. Existing method related to\nexemplar-based image-to-image translation is impossible to translate on an\nimage tuple input(source, target) that is not aligned. Also, we can confirm\nthat the existing method has limited generalization ability to unseen images.\nTo overcome this limitation, we propose Multiple GAN Inversion for\nExemplar-based Image-to-Image Translation. Our novel Multiple GAN Inversion\navoids human intervention using a self-deciding algorithm in choosing the\nnumber of layers using Fr\\'echet Inception Distance(FID), which selects more\nplausible image reconstruction result among multiple hypotheses without any\ntraining or supervision. Experimental results shows the advantage of the\nproposed method compared to existing state-of-the-art exemplar-based\nimage-to-image translation methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:46:14 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kang", "Taewon", ""]]}, {"id": "2103.14473", "submitter": "Shaojie Li", "authors": "Shaojie Li, Mingbao Lin, Yan Wang, Feiyue Huang, Yongjian Wu, Yonghong\n  Tian, Ling Shao, Rongrong Ji", "title": "Distilling a Powerful Student Model via Online Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing online knowledge distillation approaches either adopt the student\nwith the best performance or construct an ensemble model for better holistic\nperformance. However, the former strategy ignores other students' information,\nwhile the latter increases the computational complexity. In this paper, we\npropose a novel method for online knowledge distillation, termed FFSD, which\ncomprises two key components: Feature Fusion and Self-Distillation, towards\nsolving the above problems in a unified framework. Different from previous\nworks, where all students are treated equally, the proposed FFSD splits them\ninto a student leader and a common student set. Then, the feature fusion module\nconverts the concatenation of feature maps from all common students into a\nfused feature map. The fused representation is used to assist the learning of\nthe student leader. To enable the student leader to absorb more diverse\ninformation, we design an enhancement strategy to increase the diversity among\nstudents. Besides, a self-distillation module is adopted to convert the feature\nmap of deeper layers into a shallower one. Then, the shallower layers are\nencouraged to mimic the transformed feature maps of the deeper layers, which\nhelps the students to generalize better. After training, we simply adopt the\nstudent leader, which achieves superior performance, over the common students,\nwithout increasing the storage or inference cost. Extensive experiments on\nCIFAR-100 and ImageNet demonstrate the superiority of our FFSD over existing\nworks. The code is available at https://github.com/SJLeo/FFSD.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:54:24 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 07:04:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Shaojie", ""], ["Lin", "Mingbao", ""], ["Wang", "Yan", ""], ["Huang", "Feiyue", ""], ["Wu", "Yongjian", ""], ["Tian", "Yonghong", ""], ["Shao", "Ling", ""], ["Ji", "Rongrong", ""]]}, {"id": "2103.14475", "submitter": "Jianyuan Guo", "authors": "Jianyuan Guo, Kai Han, Yunhe Wang, Han Wu, Xinghao Chen, Chunjing Xu\n  and Chang Xu", "title": "Distilling Object Detectors via Decoupled Features", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is a widely used paradigm for inheriting information\nfrom a complicated teacher network to a compact student network and maintaining\nthe strong performance. Different from image classification, object detectors\nare much more sophisticated with multiple loss functions in which features that\nsemantic information rely on are tangled. In this paper, we point out that the\ninformation of features derived from regions excluding objects are also\nessential for distilling the student detector, which is usually ignored in\nexisting approaches. In addition, we elucidate that features from different\nregions should be assigned with different importance during distillation. To\nthis end, we present a novel distillation algorithm via decoupled features\n(DeFeat) for learning a better student detector. Specifically, two levels of\ndecoupled features will be processed for embedding useful information into the\nstudent, i.e., decoupled features from neck and decoupled proposals from\nclassification head. Extensive experiments on various detectors with different\nbackbones show that the proposed DeFeat is able to surpass the state-of-the-art\ndistillation methods for object detection. For example, DeFeat improves\nResNet50 based Faster R-CNN from 37.4% to 40.9% mAP, and improves ResNet50\nbased RetinaNet from 36.5% to 39.7% mAP on COCO benchmark. Our implementation\nis available at https://github.com/ggjy/DeFeat.pytorch.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:58:49 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Guo", "Jianyuan", ""], ["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Wu", "Han", ""], ["Chen", "Xinghao", ""], ["Xu", "Chunjing", ""], ["Xu", "Chang", ""]]}, {"id": "2103.14496", "submitter": "Matteo Dunnhofer", "authors": "Matteo Dunnhofer, Niki Martinel, Christian Micheloni", "title": "Weakly-Supervised Domain Adaptation of Deep Regression Trackers via\n  Reinforced Knowledge Distillation", "comments": "IEEE Robotics and Automation Letters (RA-L)", "journal-ref": null, "doi": "10.1109/LRA.2021.3070816", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep regression trackers are among the fastest tracking algorithms available,\nand therefore suitable for real-time robotic applications. However, their\naccuracy is inadequate in many domains due to distribution shift and\noverfitting. In this paper we overcome such limitations by presenting the first\nmethodology for domain adaption of such a class of trackers. To reduce the\nlabeling effort we propose a weakly-supervised adaptation strategy, in which\nreinforcement learning is used to express weak supervision as a scalar\napplication-dependent and temporally-delayed feedback. At the same time,\nknowledge distillation is employed to guarantee learning stability and to\ncompress and transfer knowledge from more powerful but slower trackers.\nExtensive experiments on five different robotic vision domains demonstrate the\nrelevance of our methodology. Real-time speed is achieved on embedded devices\nand on machines without GPUs, while accuracy reaches significant results.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 14:37:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dunnhofer", "Matteo", ""], ["Martinel", "Niki", ""], ["Micheloni", "Christian", ""]]}, {"id": "2103.14502", "submitter": "Yuhao Huang", "authors": "Xin Yang, Haoran Dou, Ruobing Huang, Wufeng Xue, Yuhao Huang, Jikuan\n  Qian, Yuanji Zhang, Huanjia Luo, Huizhi Guo, Tianfu Wang, Yi Xiong, Dong Ni", "title": "Agent with Warm Start and Adaptive Dynamic Termination for Plane\n  Localization in 3D Ultrasound", "comments": "Accepted by IEEE Transactions on Medical Imaging (12 pages, 8\n  figures, 11 tabels)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate standard plane (SP) localization is the fundamental step for\nprenatal ultrasound (US) diagnosis. Typically, dozens of US SPs are collected\nto determine the clinical diagnosis. 2D US has to perform scanning for each SP,\nwhich is time-consuming and operator-dependent. While 3D US containing multiple\nSPs in one shot has the inherent advantages of less user-dependency and more\nefficiency. Automatically locating SP in 3D US is very challenging due to the\nhuge search space and large fetal posture variations. Our previous study\nproposed a deep reinforcement learning (RL) framework with an alignment module\nand active termination to localize SPs in 3D US automatically. However,\ntermination of agent search in RL is important and affects the practical\ndeployment. In this study, we enhance our previous RL framework with a newly\ndesigned adaptive dynamic termination to enable an early stop for the agent\nsearching, saving at most 67% inference time, thus boosting the accuracy and\nefficiency of the RL framework at the same time. Besides, we validate the\neffectiveness and generalizability of our algorithm extensively on our in-house\nmulti-organ datasets containing 433 fetal brain volumes, 519 fetal abdomen\nvolumes, and 683 uterus volumes. Our approach achieves localization error of\n2.52mm/10.26 degrees, 2.48mm/10.39 degrees, 2.02mm/10.48 degrees, 2.00mm/14.57\ndegrees, 2.61mm/9.71 degrees, 3.09mm/9.58 degrees, 1.49mm/7.54 degrees for the\ntranscerebellar, transventricular, transthalamic planes in fetal brain,\nabdominal plane in fetal abdomen, and mid-sagittal, transverse and coronal\nplanes in uterus, respectively. Experimental results show that our method is\ngeneral and has the potential to improve the efficiency and standardization of\nUS scanning.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 14:57:26 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yang", "Xin", ""], ["Dou", "Haoran", ""], ["Huang", "Ruobing", ""], ["Xue", "Wufeng", ""], ["Huang", "Yuhao", ""], ["Qian", "Jikuan", ""], ["Zhang", "Yuanji", ""], ["Luo", "Huanjia", ""], ["Guo", "Huizhi", ""], ["Wang", "Tianfu", ""], ["Xiong", "Yi", ""], ["Ni", "Dong", ""]]}, {"id": "2103.14517", "submitter": "Deniz Engin", "authors": "Deniz Engin, Yannis Avrithis, Ngoc Q. K. Duong, Fran\\c{c}ois\n  Schnitzler", "title": "On the hidden treasure of dialog in video question answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  High-level understanding of stories in video such as movies and TV shows from\nraw data is extremely challenging. Modern video question answering (VideoQA)\nsystems often use additional human-made sources like plot synopses, scripts,\nvideo descriptions or knowledge bases. In this work, we present a new approach\nto understand the whole story without such external sources. The secret lies in\nthe dialog: unlike any prior work, we treat dialog as a noisy source to be\nconverted into text description via dialog summarization, much like recent\nmethods treat video. The input of each modality is encoded by transformers\nindependently, and a simple fusion method combines all modalities, using soft\ntemporal attention for localization over long inputs. Our model outperforms the\nstate of the art on the KnowIT VQA dataset by a large margin, without using\nquestion-specific human annotation or human-made plot summaries. It even\noutperforms human evaluators who have never watched any whole episode before.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:17:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Engin", "Deniz", ""], ["Avrithis", "Yannis", ""], ["Duong", "Ngoc Q. K.", ""], ["Schnitzler", "Fran\u00e7ois", ""]]}, {"id": "2103.14528", "submitter": "Zhishen Huang", "authors": "Zhishen Huang and Siqi Ye and Michael T. McCann and Saiprasad\n  Ravishankar", "title": "Model-based Reconstruction with Learning: From Unsupervised to\n  Supervised and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many techniques have been proposed for image reconstruction in medical\nimaging that aim to recover high-quality images especially from limited or\ncorrupted measurements. Model-based reconstruction methods have been\nparticularly popular (e.g., in magnetic resonance imaging and tomographic\nmodalities) and exploit models of the imaging system's physics together with\nstatistical models of measurements, noise and often relatively simple object\npriors or regularizers. For example, sparsity or low-rankness based\nregularizers have been widely used for image reconstruction from limited data\nsuch as in compressed sensing. Learning-based approaches for image\nreconstruction have garnered much attention in recent years and have shown\npromise across biomedical imaging applications. These methods include synthesis\ndictionary learning, sparsifying transform learning, and different forms of\ndeep learning involving complex neural networks. We briefly discuss classical\nmodel-based reconstruction methods and then review reconstruction methods at\nthe intersection of model-based and learning-based paradigms in detail. This\nreview includes many recent methods based on unsupervised learning, and\nsupervised learning, as well as a framework to combine multiple types of\nlearned models together.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:33:59 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Huang", "Zhishen", ""], ["Ye", "Siqi", ""], ["McCann", "Michael T.", ""], ["Ravishankar", "Saiprasad", ""]]}, {"id": "2103.14529", "submitter": "Xinggang Wang", "authors": "Xinggang Wang, Zhaojin Huang, Bencheng Liao, Lichao Huang, Yongchao\n  Gong, Chang Huang", "title": "Real-Time and Accurate Object Detection in Compressed Video by Long\n  Short-term Feature Aggregation", "comments": null, "journal-ref": "Computer Vision and Image Understanding,Volume 206, May 2021", "doi": "10.1016/j.cviu.2021.103188", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video object detection is a fundamental problem in computer vision and has a\nwide spectrum of applications. Based on deep networks, video object detection\nis actively studied for pushing the limits of detection speed and accuracy. To\nreduce the computation cost, we sparsely sample key frames in video and treat\nthe rest frames are non-key frames; a large and deep network is used to extract\nfeatures for key frames and a tiny network is used for non-key frames. To\nenhance the features of non-key frames, we propose a novel short-term feature\naggregation method to propagate the rich information in key frame features to\nnon-key frame features in a fast way. The fast feature aggregation is enabled\nby the freely available motion cues in compressed videos. Further, key frame\nfeatures are also aggregated based on optical flow. The propagated deep\nfeatures are then integrated with the directly extracted features for object\ndetection. The feature extraction and feature integration parameters are\noptimized in an end-to-end manner. The proposed video object detection network\nis evaluated on the large-scale ImageNet VID benchmark and achieves 77.2\\% mAP,\nwhich is on-par with state-of-the-art accuracy, at the speed of 30 FPS using a\nTitan X GPU. The source codes are available at\n\\url{https://github.com/hustvl/LSFA}.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:38:31 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wang", "Xinggang", ""], ["Huang", "Zhaojin", ""], ["Liao", "Bencheng", ""], ["Huang", "Lichao", ""], ["Gong", "Yongchao", ""], ["Huang", "Chang", ""]]}, {"id": "2103.14533", "submitter": "Sofiane Horache", "authors": "Sofiane Horache and Jean-Emmanuel Deschaud and Fran\\c{c}ois Goulette", "title": "3D Point Cloud Registration with Multi-Scale Architecture and\n  Self-supervised Fine-tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present MS-SVConv, a fast multi-scale deep neural network that outputs\nfeatures from point clouds for 3D registration between two scenes. We compute\nfeatures using a 3D sparse voxel convolutional network on a point cloud at\ndifferent scales and then fuse the features through fully-connected layers.\nWith supervised learning, we show significant improvements compared to\nstate-of-the-art methods on the competitive and well-known 3DMatch benchmark.\nWe also achieve a better generalization through different source and target\ndatasets, with very fast computation. Finally, we present a strategy to\nfine-tune MS-SVConv on unknown datasets in a self-supervised way, which leads\nto state-of-the-art results on ETH and TUM datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:38:33 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Horache", "Sofiane", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Goulette", "Fran\u00e7ois", ""]]}, {"id": "2103.14537", "submitter": "Xavier Rafael-Palou", "authors": "Xavier Rafael-Palou (1 and 2), Anton Aubanell (3), Mario Ceresa (2),\n  Vicent Ribas (1), Gemma Piella (2) and Miguel A. Gonz\\'alez Ballester (2 and\n  4) ((1) Eurecat Centre Tecnol\\`ogic de Catalunya, eHealth Unit, Barcelona,\n  Spain (2) BCN MedTech, Dept. of Information and Communication Technologies,\n  Universitat Pompeu Fabra, Barcelona, Spain (3) Vall d'Hebron University\n  Hospital, Barcelona, Spain (4) ICREA, Barcelona, Spain)", "title": "Detection, growth quantification and malignancy prediction of pulmonary\n  nodules using deep convolutional networks in follow-up CT scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of supporting radiologists in the longitudinal\nmanagement of lung cancer. Therefore, we proposed a deep learning pipeline,\ncomposed of four stages that completely automatized from the detection of\nnodules to the classification of cancer, through the detection of growth in the\nnodules. In addition, the pipeline integrated a novel approach for nodule\ngrowth detection, which relied on a recent hierarchical probabilistic U-Net\nadapted to report uncertainty estimates. Also, a second novel method was\nintroduced for lung cancer nodule classification, integrating into a two stream\n3D-CNN network the estimated nodule malignancy probabilities derived from a\npretrained nodule malignancy network. The pipeline was evaluated in a\nlongitudinal cohort and reported comparable performances to the state of art.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:41:37 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Rafael-Palou", "Xavier", "", "1 and 2"], ["Aubanell", "Anton", "", "2 and\n  4"], ["Ceresa", "Mario", "", "2 and\n  4"], ["Ribas", "Vicent", "", "2 and\n  4"], ["Piella", "Gemma", "", "2 and\n  4"], ["Ballester", "Miguel A. Gonz\u00e1lez", "", "2 and\n  4"]]}, {"id": "2103.14545", "submitter": "Zirui Liu", "authors": "Zirui Liu, Haifeng Jin, Ting-Hsiang Wang, Kaixiong Zhou, Xia Hu", "title": "DivAug: Plug-in Automated Data Augmentation with Explicit Diversity\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-designed data augmentation strategies have been replaced by\nautomatically learned augmentation policy in the past two years. Specifically,\nrecent work has empirically shown that the superior performance of the\nautomated data augmentation methods stems from increasing the diversity of\naugmented data. However, two factors regarding the diversity of augmented data\nare still missing: 1) the explicit definition (and thus measurement) of\ndiversity and 2) the quantifiable relationship between diversity and its\nregularization effects. To bridge this gap, we propose a diversity measure\ncalled Variance Diversity and theoretically show that the regularization effect\nof data augmentation is promised by Variance Diversity. We validate in\nexperiments that the relative gain from automated data augmentation in test\naccuracy is highly correlated to Variance Diversity. An unsupervised\nsampling-based framework, DivAug, is designed to directly maximize Variance\nDiversity and hence strengthen the regularization effect. Without requiring a\nseparate search process, the performance gain from DivAug is comparable with\nthe state-of-the-art method with better efficiency. Moreover, under the\nsemi-supervised setting, our framework can further improve the performance of\nsemi-supervised learning algorithms when compared to RandAugment, making it\nhighly applicable to real-world problems, where labeled data is scarce.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:00:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Liu", "Zirui", ""], ["Jin", "Haifeng", ""], ["Wang", "Ting-Hsiang", ""], ["Zhou", "Kaixiong", ""], ["Hu", "Xia", ""]]}, {"id": "2103.14572", "submitter": "Adrian Wolny", "authors": "Adrian Wolny, Qin Yu, Constantin Pape, Anna Kreshuk", "title": "Sparse Object-level Supervision for Instance Segmentation with Pixel\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art instance segmentation methods have to be trained on\ndensely annotated images. While difficult in general, this requirement is\nespecially daunting for biomedical images, where domain expertise is often\nrequired for annotation. We propose to address the dense annotation bottleneck\nby introducing a proposal-free segmentation approach based on non-spatial\nembeddings, which exploits the structure of the learned embedding space to\nextract individual instances in a differentiable way. The segmentation loss can\nthen be applied directly on the instances and the overall method can be trained\non ground truth images where only a few objects are annotated, from scratch or\nin a semi-supervised transfer learning setting. In addition to the segmentation\nloss, our setup allows to apply self-supervised consistency losses on the\nunlabeled parts of the training data. We evaluate the proposed method on\nchallenging 2D and 3D segmentation problems in different microscopy modalities\nas well as on the popular CVPPP instance segmentation benchmark where we\nachieve state-of-the-art results.\n  The code is available at: https://github.com/kreshuklab/spoco\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:36:56 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Wolny", "Adrian", ""], ["Yu", "Qin", ""], ["Pape", "Constantin", ""], ["Kreshuk", "Anna", ""]]}, {"id": "2103.14577", "submitter": "Peshal Agarwal", "authors": "Peshal Agarwal, Danda Pani Paudel, Jan-Nico Zaech and Luc Van Gool", "title": "Unsupervised Robust Domain Adaptation without Source Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of robust domain adaptation in the context of\nunavailable target labels and source data. The considered robustness is against\nadversarial perturbations. This paper aims at answering the question of finding\nthe right strategy to make the target model robust and accurate in the setting\nof unsupervised domain adaptation without source data. The major findings of\nthis paper are: (i) robust source models can be transferred robustly to the\ntarget; (ii) robust domain adaptation can greatly benefit from non-robust\npseudo-labels and the pair-wise contrastive loss. The proposed method of using\nnon-robust pseudo-labels performs surprisingly well on both clean and\nadversarial samples, for the task of image classification. We show a consistent\nperformance improvement of over $10\\%$ in accuracy against the tested baselines\non four benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:42:28 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Agarwal", "Peshal", ""], ["Paudel", "Danda Pani", ""], ["Zaech", "Jan-Nico", ""], ["Van Gool", "Luc", ""]]}, {"id": "2103.14579", "submitter": "Narciso L\\'opez-L\\'opez", "authors": "Narciso L\\'opez-L\\'opez, Andrea V\\'azquez, Cyril Poupon,\n  Jean-Fran\\c{c}ois Mangin, Susana Ladra, and Pamela Guevara", "title": "GeoSP: A parallel method for a cortical surface parcellation based on\n  geodesic distance", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, ANID PFCHA/DOCTORADO\n  NACIONAL/2016-21160342, ANID FONDECYT 1190701, ANID PIA/Anillo de\n  Investigaci\\'on en Ciencia y Tecnolog\\'ia ACT172121, and ANID Basal Project\n  FB0008", "journal-ref": null, "doi": "10.1109/EMBC44109.2020.9175779", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GeoSP, a parallel method that creates a parcellation of the\ncortical mesh based on a geodesic distance, in order to consider gyri and sulci\ntopology. The method represents the mesh with a graph and performs a K-means\nclustering in parallel. It has two modes of use, by default, it performs the\ngeodesic cortical parcellation based on the boundaries of the anatomical\nparcels provided by the Desikan-Killiany atlas. The other mode performs the\ncomplete parcellation of the cortex. Results for both modes and with different\nvalues for the total number of sub-parcels show homogeneous sub-parcels.\nFurthermore, the execution time is 82 s for the whole cortex mode and 18 s for\nthe Desikan-Killiany atlas subdivision, for a parcellation into 350\nsub-parcels. The proposed method will be available to the community to perform\nthe evaluation of data-driven cortical parcellations. As an example, we\ncompared GeoSP parcellation with Desikan-Killiany and Destrieux atlases in 50\nsubjects, obtaining more homogeneous parcels for GeoSP and minor differences in\nstructural connectivity reproducibility across subjects.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:43:04 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["L\u00f3pez-L\u00f3pez", "Narciso", ""], ["V\u00e1zquez", "Andrea", ""], ["Poupon", "Cyril", ""], ["Mangin", "Jean-Fran\u00e7ois", ""], ["Ladra", "Susana", ""], ["Guevara", "Pamela", ""]]}, {"id": "2103.14581", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Tao Chen, Guosen Xie, Chuanyi Zhang, Fumin Shen, Qi Wu,\n  Zhenmin Tang, and Jian Zhang", "title": "Non-Salient Region Object Mining for Weakly Supervised Semantic\n  Segmentation", "comments": "accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation aims to classify every pixel of an input image.\nConsidering the difficulty of acquiring dense labels, researchers have recently\nbeen resorting to weak labels to alleviate the annotation burden of\nsegmentation. However, existing works mainly concentrate on expanding the seed\nof pseudo labels within the image's salient region. In this work, we propose a\nnon-salient region object mining approach for weakly supervised semantic\nsegmentation. We introduce a graph-based global reasoning unit to strengthen\nthe classification network's ability to capture global relations among disjoint\nand distant regions. This helps the network activate the object features\noutside the salient area. To further mine the non-salient region objects, we\npropose to exert the segmentation network's self-correction ability.\nSpecifically, a potential object mining module is proposed to reduce the\nfalse-negative rate in pseudo labels. Moreover, we propose a non-salient region\nmasking module for complex images to generate masked pseudo labels. Our\nnon-salient region masking module helps further discover the objects in the\nnon-salient region. Extensive experiments on the PASCAL VOC dataset demonstrate\nstate-of-the-art results compared to current methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:44:03 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yao", "Yazhou", ""], ["Chen", "Tao", ""], ["Xie", "Guosen", ""], ["Zhang", "Chuanyi", ""], ["Shen", "Fumin", ""], ["Wu", "Qi", ""], ["Tang", "Zhenmin", ""], ["Zhang", "Jian", ""]]}, {"id": "2103.14586", "submitter": "Daniel Glasner", "authors": "Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li,\n  Thomas Unterthiner, Andreas Veit", "title": "Understanding Robustness of Transformers for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have long been the architecture of\nchoice for computer vision tasks. Recently, Transformer-based architectures\nlike Vision Transformer (ViT) have matched or even surpassed ResNets for image\nclassification. However, details of the Transformer architecture -- such as the\nuse of non-overlapping patches -- lead one to wonder whether these networks are\nas robust. In this paper, we perform an extensive study of a variety of\ndifferent measures of robustness of ViT models and compare the findings to\nResNet baselines. We investigate robustness to input perturbations as well as\nrobustness to model perturbations. We find that when pre-trained with a\nsufficient amount of data, ViT models are at least as robust as the ResNet\ncounterparts on a broad range of perturbations. We also find that Transformers\nare robust to the removal of almost any single layer, and that while\nactivations from later layers are highly correlated with each other, they\nnevertheless play an important role in classification.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:47:55 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Chakrabarti", "Ayan", ""], ["Glasner", "Daniel", ""], ["Li", "Daliang", ""], ["Unterthiner", "Thomas", ""], ["Veit", "Andreas", ""]]}, {"id": "2103.14602", "submitter": "Md Sahidullah", "authors": "Bhusan Chettri, Rosa Gonz\\'alez Hautam\\\"aki, Md Sahidullah, Tomi\n  Kinnunen", "title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization", "comments": "INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Voice anti-spoofing aims at classifying a given utterance either as a\nbonafide human sample, or a spoofing attack (e.g. synthetic or replayed\nsample). Many anti-spoofing methods have been proposed but most of them fail to\ngeneralize across domains (corpora) -- and we do not know \\emph{why}. We\noutline a novel interpretative framework for gauging the impact of data quality\nupon anti-spoofing performance. Our within- and between-domain experiments pool\ndata from seven public corpora and three anti-spoofing methods based on\nGaussian mixture and convolutive neural network models. We assess the impacts\nof long-term spectral information, speaker population (through x-vector speaker\nembeddings), signal-to-noise ratio, and selected voice quality features.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:09:06 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 20:53:23 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chettri", "Bhusan", ""], ["Hautam\u00e4ki", "Rosa Gonz\u00e1lez", ""], ["Sahidullah", "Md", ""], ["Kinnunen", "Tomi", ""]]}, {"id": "2103.14616", "submitter": "Aamir Mustafa", "authors": "Aamir Mustafa, Aliaksei Mikhailiuk, Dan Andrei Iliescu, Varun Babbar\n  and Rafal K. Mantiuk", "title": "Training a Better Loss Function for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Central to the application of neural networks in image restoration problems,\nsuch as single image super resolution, is the choice of a loss function that\nencourages natural and perceptually pleasing results. A popular choice for a\nloss function is a pre-trained network, such as VGG and LPIPS, which is used as\na feature extractor for computing the difference between restored and reference\nimages. However, such an approach has multiple drawbacks: it is computationally\nexpensive, requires regularization and hyper-parameter tuning, and involves a\nlarge network trained on an unrelated task. In this work, we explore the\nquestion of what makes a good loss function for an image restoration task.\nFirst, we observe that a single natural image is sufficient to train a\nlightweight feature extractor that outperforms state-of-the-art loss functions\nin single image super resolution, denoising, and JPEG artefact removal. We\npropose a novel Multi-Scale Discriminative Feature (MDF) loss comprising a\nseries of discriminators, trained to penalize errors introduced by a generator.\nSecond, we show that an effective loss function does not have to be a good\npredictor of perceived image quality, but instead needs to be specialized in\nidentifying the distortions for a given restoration method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:29:57 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Mustafa", "Aamir", ""], ["Mikhailiuk", "Aliaksei", ""], ["Iliescu", "Dan Andrei", ""], ["Babbar", "Varun", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "2103.14633", "submitter": "Michael S. Ryoo", "authors": "Iretiayo Akinola, Anelia Angelova, Yao Lu, Yevgen Chebotar, Dmitry\n  Kalashnikov, Jacob Varley, Julian Ibarz, Michael S. Ryoo", "title": "Visionary: Vision architecture discovery for robot learning", "comments": null, "journal-ref": "ICRA 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a vision-based architecture search algorithm for robot\nmanipulation learning, which discovers interactions between low dimension\naction inputs and high dimensional visual inputs. Our approach automatically\ndesigns architectures while training on the task - discovering novel ways of\ncombining and attending image feature representations with actions as well as\nfeatures from previous layers. The obtained new architectures demonstrate\nbetter task success rates, in some cases with a large margin, compared to a\nrecent high performing baseline. Our real robot experiments also confirm that\nit improves grasping performance by 6%. This is the first approach to\ndemonstrate a successful neural architecture search and attention connectivity\nsearch for a real-robot task.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:51:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Akinola", "Iretiayo", ""], ["Angelova", "Anelia", ""], ["Lu", "Yao", ""], ["Chebotar", "Yevgen", ""], ["Kalashnikov", "Dmitry", ""], ["Varley", "Jacob", ""], ["Ibarz", "Julian", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "2103.14635", "submitter": "Mutian Xu", "authors": "Mutian Xu, Runyu Ding, Hengshuang Zhao, Xiaojuan Qi", "title": "PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on\n  Point Clouds", "comments": "To be appear in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Position Adaptive Convolution (PAConv), a generic convolution\noperation for 3D point cloud processing. The key of PAConv is to construct the\nconvolution kernel by dynamically assembling basic weight matrices stored in\nWeight Bank, where the coefficients of these weight matrices are\nself-adaptively learned from point positions through ScoreNet. In this way, the\nkernel is built in a data-driven manner, endowing PAConv with more flexibility\nthan 2D convolutions to better handle the irregular and unordered point cloud\ndata. Besides, the complexity of the learning process is reduced by combining\nweight matrices instead of brutally predicting kernels from point positions.\n  Furthermore, different from the existing point convolution operators whose\nnetwork architectures are often heavily engineered, we integrate our PAConv\ninto classical MLP-based point cloud pipelines without changing network\nconfigurations. Even built on simple networks, our method still approaches or\neven surpasses the state-of-the-art models, and significantly improves baseline\nperformance on both classification and segmentation tasks, yet with decent\nefficiency. Thorough ablation studies and visualizations are provided to\nunderstand PAConv. Code is released on https://github.com/CVMI-Lab/PAConv.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:52:38 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 06:50:57 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Xu", "Mutian", ""], ["Ding", "Runyu", ""], ["Zhao", "Hengshuang", ""], ["Qi", "Xiaojuan", ""]]}, {"id": "2103.14641", "submitter": "Muzammal Naseer", "authors": "Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and\n  Fatih Porikli", "title": "On Generating Transferable Targeted Perturbations", "comments": "Source Code is available at https://github.com/Muzammal-Naseer/TTP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the untargeted black-box transferability of adversarial perturbations\nhas been extensively studied before, changing an unseen model's decisions to a\nspecific `targeted' class remains a challenging feat. In this paper, we propose\na new generative approach for highly transferable targeted perturbations\n(\\ours). We note that the existing methods are less suitable for this task due\nto their reliance on class-boundary information that changes from one model to\nanother, thus reducing transferability. In contrast, our approach matches the\nperturbed image `distribution' with that of the target class, leading to high\ntargeted transferability rates. To this end, we propose a new objective\nfunction that not only aligns the global distributions of source and target\nimages, but also matches the local neighbourhood structure between the two\ndomains. Based on the proposed objective, we train a generator function that\ncan adaptively synthesize perturbations specific to a given input. Our\ngenerative approach is independent of the source or target domain labels, while\nconsistently performs well against state-of-the-art methods on a wide range of\nattack settings. As an example, we achieve $32.63\\%$ target transferability\nfrom (an adversarially weak) VGG19$_{BN}$ to (a strong) WideResNet on ImageNet\nval. set, which is 4$\\times$ higher than the previous best generative attack\nand 16$\\times$ better than instance-specific iterative attack. Code is\navailable at: {\\small\\url{https://github.com/Muzammal-Naseer/TTP}}.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:55:28 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Naseer", "Muzammal", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Porikli", "Fatih", ""]]}, {"id": "2103.14644", "submitter": "Linyi Jin", "authors": "Linyi Jin, Shengyi Qian, Andrew Owens, David F. Fouhey", "title": "Planar Surface Reconstruction from Sparse Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies planar surface reconstruction of indoor scenes from two\nviews with unknown camera poses. While prior approaches have successfully\ncreated object-centric reconstructions of many scenes, they fail to exploit\nother structures, such as planes, which are typically the dominant components\nof indoor scenes. In this paper, we reconstruct planar surfaces from multiple\nviews, while jointly estimating camera pose. Our experiments demonstrate that\nour method is able to advance the state of the art of reconstruction from\nsparse views, on challenging scenes from Matterport3D. Project site:\nhttps://jinlinyi.github.io/SparsePlanes/\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:59:20 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Jin", "Linyi", ""], ["Qian", "Shengyi", ""], ["Owens", "Andrew", ""], ["Fouhey", "David F.", ""]]}, {"id": "2103.14645", "submitter": "Ben Mildenhall", "authors": "Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T.\n  Barron, Paul Debevec", "title": "Baking Neural Radiance Fields for Real-Time View Synthesis", "comments": "Project page: https://nerf.live", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural volumetric representations such as Neural Radiance Fields (NeRF) have\nemerged as a compelling technique for learning to represent 3D scenes from\nimages with the goal of rendering photorealistic images of the scene from\nunobserved viewpoints. However, NeRF's computational requirements are\nprohibitive for real-time applications: rendering views from a trained NeRF\nrequires querying a multilayer perceptron (MLP) hundreds of times per ray. We\npresent a method to train a NeRF, then precompute and store (i.e. \"bake\") it as\na novel representation called a Sparse Neural Radiance Grid (SNeRG) that\nenables real-time rendering on commodity hardware. To achieve this, we\nintroduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid\nrepresentation with learned feature vectors. The resulting scene representation\nretains NeRF's ability to render fine geometric details and view-dependent\nappearance, is compact (averaging less than 90 MB per scene), and can be\nrendered in real-time (higher than 30 frames per second on a laptop GPU).\nActual screen captures are shown in our video.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:59:52 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hedman", "Peter", ""], ["Srinivasan", "Pratul P.", ""], ["Mildenhall", "Ben", ""], ["Barron", "Jonathan T.", ""], ["Debevec", "Paul", ""]]}, {"id": "2103.14653", "submitter": "Benjamin Jaderberg", "authors": "Ben Jaderberg, Lewis W. Anderson, Weidi Xie, Samuel Albanie, Martin\n  Kiffner, Dieter Jaksch", "title": "Quantum Self-Supervised Learning", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularisation of neural networks has seen incredible advances in pattern\nrecognition, driven by the supervised learning of human annotations. However,\nthis approach is unsustainable in relation to the dramatically increasing size\nof real-world datasets. This has led to a resurgence in self-supervised\nlearning, a paradigm whereby the model generates its own supervisory signal\nfrom the data. Here we propose a hybrid quantum-classical neural network\narchitecture for contrastive self-supervised learning and test its\neffectiveness in proof-of-principle experiments. Interestingly, we observe a\nnumerical advantage for the learning of visual representations using\nsmall-scale quantum neural networks over equivalently structured classical\nnetworks, even when the quantum circuits are sampled with only 100 shots.\nFurthermore, we apply our best quantum model to classify unseen images on the\nibmq_paris quantum computer and find that current noisy devices can already\nachieve equal accuracy to the equivalent classical model on downstream tasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:00:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jaderberg", "Ben", ""], ["Anderson", "Lewis W.", ""], ["Xie", "Weidi", ""], ["Albanie", "Samuel", ""], ["Kiffner", "Martin", ""], ["Jaksch", "Dieter", ""]]}, {"id": "2103.14672", "submitter": "Silvia Bucci", "authors": "Andrea Ferreri and Silvia Bucci and Tatiana Tommasi", "title": "Translate to Adapt: RGB-D Scene Recognition across Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene classification is one of the basic problems in computer vision research\nwith extensive applications in robotics. When available, depth images provide\nhelpful geometric cues that complement the RGB texture information and help to\nidentify more discriminative scene image features. Depth sensing technology\ndeveloped fast in the last years and a great variety of 3D cameras have been\nintroduced, each with different acquisition properties. However, when targeting\nbig data collections, often multi-modal images are gathered disregarding their\noriginal nature. In this work we put under the spotlight the existence of a\npossibly severe domain shift issue within multi-modality scene recognition\ndatasets. We design an experimental testbed to study this problem and present a\nmethod based on self-supervised inter-modality translation able to adapt across\ndifferent camera domains. Our extensive experimental analysis confirms the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:20:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ferreri", "Andrea", ""], ["Bucci", "Silvia", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2103.14675", "submitter": "Anindita Ghosh", "authors": "Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt,\n  Philipp Slusallek", "title": "Synthesis of Compositional Animations from Textual Descriptions", "comments": "13 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"How can we animate 3D-characters from a movie script or move robots by\nsimply telling them what we would like them to do?\" \"How unstructured and\ncomplex can we make a sentence and still generate plausible movements from it?\"\nThese are questions that need to be answered in the long-run, as the field is\nstill in its infancy. Inspired by these problems, we present a new technique\nfor generating compositional actions, which handles complex input sentences.\nOur output is a 3D pose sequence depicting the actions in the input sentence.\nWe propose a hierarchical two-stream sequential model to explore a finer\njoint-level mapping between natural language sentences and 3D pose sequences\ncorresponding to the given motion. We learn two manifold representations of the\nmotion -- one each for the upper body and the lower body movements. Our model\ncan generate plausible pose sequences for short sentences describing single\nactions as well as long compositional sentences describing multiple sequential\nand superimposed actions. We evaluate our proposed model on the publicly\navailable KIT Motion-Language Dataset containing 3D pose data with\nhuman-annotated sentences. Experimental results show that our model advances\nthe state-of-the-art on text-based motion synthesis in objective evaluations by\na margin of 50%. Qualitative evaluations based on a user study indicate that\nour synthesized motions are perceived to be the closest to the ground-truth\nmotion captures for both short and compositional sentences.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:23:29 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 13:37:04 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ghosh", "Anindita", ""], ["Cheema", "Noshaba", ""], ["Oguz", "Cennet", ""], ["Theobalt", "Christian", ""], ["Slusallek", "Philipp", ""]]}, {"id": "2103.14697", "submitter": "Clemens Seibold", "authors": "Clemens Seibold, Anna Hilsmann, Peter Eisert", "title": "Focused LRP: Explainable AI for Face Morphing Attack Detection", "comments": "Published at WACVW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of detecting morphed face images has become highly relevant in\nrecent years to ensure the security of automatic verification systems based on\nfacial images, e.g. automated border control gates. Detection methods based on\nDeep Neural Networks (DNN) have been shown to be very suitable to this end.\nHowever, they do not provide transparency in the decision making and it is not\nclear how they distinguish between genuine and morphed face images. This is\nparticularly relevant for systems intended to assist a human operator, who\nshould be able to understand the reasoning. In this paper, we tackle this\nproblem and present Focused Layer-wise Relevance Propagation (FLRP). This\nframework explains to a human inspector on a precise pixel level, which image\nregions are used by a Deep Neural Network to distinguish between a genuine and\na morphed face image. Additionally, we propose another framework to objectively\nanalyze the quality of our method and compare FLRP to other DNN\ninterpretability methods. This evaluation framework is based on removing\ndetected artifacts and analyzing the influence of these changes on the decision\nof the DNN. Especially, if the DNN is uncertain in its decision or even\nincorrect, FLRP performs much better in highlighting visible artifacts compared\nto other methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:05:01 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Seibold", "Clemens", ""], ["Hilsmann", "Anna", ""], ["Eisert", "Peter", ""]]}, {"id": "2103.14708", "submitter": "Bo Sun", "authors": "Bo Sun, Junchi Yan, Xiao Zhou, and Yinqiang Zheng", "title": "Tuning IR-cut Filter for Illumination-aware Spectral Reconstruction from\n  RGB", "comments": "CVPR 2021 - Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To reconstruct spectral signals from multi-channel observations, in\nparticular trichromatic RGBs, has recently emerged as a promising alternative\nto traditional scanning-based spectral imager. It has been proven that the\nreconstruction accuracy relies heavily on the spectral response of the RGB\ncamera in use. To improve accuracy, data-driven algorithms have been proposed\nto retrieve the best response curves of existing RGB cameras, or even to design\nbrand new three-channel response curves. Instead, this paper explores the\nfilter-array based color imaging mechanism of existing RGB cameras, and\nproposes to design the IR-cut filter properly for improved spectral recovery,\nwhich stands out as an in-between solution with better trade-off between\nreconstruction accuracy and implementation complexity. We further propose a\ndeep learning based spectral reconstruction method, which allows to recover the\nillumination spectrum as well. Experiment results with both synthetic and real\nimages under daylight illumination have shown the benefits of our IR-cut filter\ntuning method and our illumination-aware spectral reconstruction method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:42:21 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sun", "Bo", ""], ["Yan", "Junchi", ""], ["Zhou", "Xiao", ""], ["Zheng", "Yinqiang", ""]]}, {"id": "2103.14712", "submitter": "Arijit Ray", "authors": "Arijit Ray, Michael Cogswell, Xiao Lin, Kamran Alipour, Ajay\n  Divakaran, Yi Yao, Giedrius Burachas", "title": "Knowing What VQA Does Not: Pointing to Error-Inducing Regions to Improve\n  Explanation Helpfulness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention maps, a popular heatmap-based explanation method for Visual\nQuestion Answering (VQA), are supposed to help users understand the model by\nhighlighting portions of the image/question used by the model to infer answers.\nHowever, we see that users are often misled by current attention map\nvisualizations that point to relevant regions despite the model producing an\nincorrect answer. Hence, we propose Error Maps that clarify the error by\nhighlighting image regions where the model is prone to err. Error maps can\nindicate when a correctly attended region may be processed incorrectly leading\nto an incorrect answer, and hence, improve users' understanding of those cases.\nTo evaluate our new explanations, we further introduce a metric that simulates\nusers' interpretation of explanations to evaluate their potential helpfulness\nto understand model correctness. We finally conduct user studies to see that\nour new explanations help users understand model correctness better than\nbaselines by an expected 30% and that our proxy helpfulness metrics correlate\nstrongly ($\\rho$>0.97) with how well users can predict model correctness.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:52:32 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 21:15:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Ray", "Arijit", ""], ["Cogswell", "Michael", ""], ["Lin", "Xiao", ""], ["Alipour", "Kamran", ""], ["Divakaran", "Ajay", ""], ["Yao", "Yi", ""], ["Burachas", "Giedrius", ""]]}, {"id": "2103.14724", "submitter": "Zhongjie Yu", "authors": "Zhongjie Yu, Gaoang Wang, Lin Chen, Sebastian Raschka, and Jiebo Luo", "title": "Few-Shot Learning for Video Object Detection in a Transfer-Learning\n  Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from static images, videos contain additional temporal and spatial\ninformation for better object detection. However, it is costly to obtain a\nlarge number of videos with bounding box annotations that are required for\nsupervised deep learning. Although humans can easily learn to recognize new\nobjects by watching only a few video clips, deep learning usually suffers from\noverfitting. This leads to an important question: how to effectively learn a\nvideo object detector from only a few labeled video clips? In this paper, we\nstudy the new problem of few-shot learning for video object detection. We first\ndefine the few-shot setting and create a new benchmark dataset for few-shot\nvideo object detection derived from the widely used ImageNet VID dataset. We\nemploy a transfer-learning framework to effectively train the video object\ndetector on a large number of base-class objects and a few video clips of\nnovel-class objects. By analyzing the results of two methods under this\nframework (Joint and Freeze) on our designed weak and strong base datasets, we\nreveal insufficiency and overfitting problems. A simple but effective method,\ncalled Thaw, is naturally developed to trade off the two problems and validate\nour analysis.\n  Extensive experiments on our proposed benchmark datasets with different\nscenarios demonstrate the effectiveness of our novel analysis in this new\nfew-shot video object detection problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 20:37:55 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 01:35:36 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Yu", "Zhongjie", ""], ["Wang", "Gaoang", ""], ["Chen", "Lin", ""], ["Raschka", "Sebastian", ""], ["Luo", "Jiebo", ""]]}, {"id": "2103.14734", "submitter": "Oumaima Hamila", "authors": "Oumaima Hamila, Sheela Ramanna, Christopher J. Henry, Serkan Kiranyaz,\n  Ridha Hamila, Rashid Mazhar, Tahir Hamid", "title": "Fully Automated 2D and 3D Convolutional Neural Networks Pipeline for\n  Video Segmentation and Myocardial Infarction Detection in Echocardiography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac imaging known as echocardiography is a non-invasive tool utilized to\nproduce data including images and videos, which cardiologists use to diagnose\ncardiac abnormalities in general and myocardial infarction (MI) in particular.\nEchocardiography machines can deliver abundant amounts of data that need to be\nquickly analyzed by cardiologists to help them make a diagnosis and treat\ncardiac conditions. However, the acquired data quality varies depending on the\nacquisition conditions and the patient's responsiveness to the setup\ninstructions. These constraints are challenging to doctors especially when\npatients are facing MI and their lives are at stake. In this paper, we propose\nan innovative real-time end-to-end fully automated model based on convolutional\nneural networks (CNN) to detect MI depending on regional wall motion\nabnormalities (RWMA) of the left ventricle (LV) from videos produced by\nechocardiography. Our model is implemented as a pipeline consisting of a 2D CNN\nthat performs data preprocessing by segmenting the LV chamber from the apical\nfour-chamber (A4C) view, followed by a 3D CNN that performs a binary\nclassification to detect if the segmented echocardiography shows signs of MI.\nWe trained both CNNs on a dataset composed of 165 echocardiography videos each\nacquired from a distinct patient. The 2D CNN achieved an accuracy of 97.18% on\ndata segmentation while the 3D CNN achieved 90.9% of accuracy, 100% of\nprecision and 95% of recall on MI detection. Our results demonstrate that\ncreating a fully automated system for MI detection is feasible and propitious.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 21:03:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Hamila", "Oumaima", ""], ["Ramanna", "Sheela", ""], ["Henry", "Christopher J.", ""], ["Kiranyaz", "Serkan", ""], ["Hamila", "Ridha", ""], ["Mazhar", "Rashid", ""], ["Hamid", "Tahir", ""]]}, {"id": "2103.14756", "submitter": "Dongdong Chen", "authors": "Dongdong Chen, Juli\\'an Tachella, Mike E. Davies", "title": "Equivariant Imaging: Learning Beyond the Range Space", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various imaging problems, we only have access to compressed measurements\nof the underlying signals, hindering most learning-based strategies which\nusually require pairs of signals and associated measurements for training.\nLearning only from compressed measurements is impossible in general, as the\ncompressed observations do not contain information outside the range of the\nforward sensing operator. We propose a new end-to-end self-supervised framework\nthat overcomes this limitation by exploiting the equivariances present in\nnatural signals. Our proposed learning strategy performs as well as fully\nsupervised methods. Experiments demonstrate the potential of this framework on\ninverse problems including sparse-view X-ray computed tomography on real\nclinical data and image inpainting on natural images. Code will be released.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 22:38:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Dongdong", ""], ["Tachella", "Juli\u00e1n", ""], ["Davies", "Mike E.", ""]]}, {"id": "2103.14785", "submitter": "Jesus Perez-Martin", "authors": "Jesus Perez-Martin and Benjamin Bustos and Silvio Jamil F. Guimar\\~aes\n  and Ivan Sipiran and Jorge P\\'erez and Grethel Coello Said", "title": "Bridging Vision and Language from the Video-to-Text Perspective: A\n  Comprehensive Review", "comments": "66 pages, 5 figures. Submitted to Artificial Intelligence Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in the area of Vision and Language encompasses challenging topics\nthat seek to connect visual and textual information. The video-to-text problem\nis one of these topics, in which the goal is to connect an input video with its\ntextual description. This connection can be mainly made by retrieving the most\nsignificant descriptions from a corpus or generating a new one given a context\nvideo. These two ways represent essential tasks for Computer Vision and Natural\nLanguage Processing communities, called text retrieval from video task and\nvideo captioning/description task. These two tasks are substantially more\ncomplex than predicting or retrieving a single sentence from an image. The\nspatiotemporal information present in videos introduces diversity and\ncomplexity regarding the visual content and the structure of associated\nlanguage descriptions. This review categorizes and describes the\nstate-of-the-art techniques for the video-to-text problem. It covers the main\nvideo-to-text methods and the ways to evaluate their performance. We analyze\nhow the most reported benchmark datasets have been created, showing their\ndrawbacks and strengths for the problem requirements. We also show the\nimpressive progress that researchers have made on each dataset, and we analyze\nwhy, despite this progress, the video-to-text conversion is still unsolved.\nState-of-the-art techniques are still a long way from achieving human-like\nperformance in generating or retrieving video descriptions. We cover several\nsignificant challenges in the field and discuss future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 02:12:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Perez-Martin", "Jesus", ""], ["Bustos", "Benjamin", ""], ["Guimar\u00e3es", "Silvio Jamil F.", ""], ["Sipiran", "Ivan", ""], ["P\u00e9rez", "Jorge", ""], ["Said", "Grethel Coello", ""]]}, {"id": "2103.14793", "submitter": "Ganning Zhao", "authors": "Ganning Zhao, Jiesi Hu, Suya You and C.-C. Jay Kuo", "title": "CalibDNN: Multimodal Sensor Calibration for Perception Using Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current perception systems often carry multimodal imagers and sensors such as\n2D cameras and 3D LiDAR sensors. To fuse and utilize the data for downstream\nperception tasks, robust and accurate calibration of the multimodal sensor data\nis essential. We propose a novel deep learning-driven technique (CalibDNN) for\naccurate calibration among multimodal sensor, specifically LiDAR-Camera pairs.\nThe key innovation of the proposed work is that it does not require any\nspecific calibration targets or hardware assistants, and the entire processing\nis fully automatic with a single model and single iteration. Results comparison\namong different methods and extensive experiments on different datasets\ndemonstrates the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 02:43:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Ganning", ""], ["Hu", "Jiesi", ""], ["You", "Suya", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2103.14794", "submitter": "Kaizhang Kang", "authors": "Kaizhang Kang, Cihui Xie, Ruisheng Zhu, Xiaohe Ma, Ping Tan, Hongzhi\n  Wu and Kun Zhou", "title": "Learning Efficient Photometric Feature Transform for Multi-view Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework to learn to convert the perpixel photometric\ninformation at each view into spatially distinctive and view-invariant\nlow-level features, which can be plugged into existing multi-view stereo\npipeline for enhanced 3D reconstruction. Both the illumination conditions\nduring acquisition and the subsequent per-pixel feature transform can be\njointly optimized in a differentiable fashion. Our framework automatically\nadapts to and makes efficient use of the geometric information available in\ndifferent forms of input data. High-quality 3D reconstructions of a variety of\nchallenging objects are demonstrated on the data captured with an illumination\nmultiplexing device, as well as a point light. Our results compare favorably\nwith state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 02:53:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kang", "Kaizhang", ""], ["Xie", "Cihui", ""], ["Zhu", "Ruisheng", ""], ["Ma", "Xiaohe", ""], ["Tan", "Ping", ""], ["Wu", "Hongzhi", ""], ["Zhou", "Kun", ""]]}, {"id": "2103.14795", "submitter": "Yi Cai", "authors": "Yi Cai, Xuefei Ning, Huazhong Yang, Yu Wang", "title": "Ensemble-in-One: Learning Ensemble within Random Gated Networks for\n  Enhanced Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks have rendered high security risks on modern deep learning\nsystems. Adversarial training can significantly enhance the robustness of\nneural network models by suppressing the non-robust features. However, the\nmodels often suffer from significant accuracy loss on clean data. Ensemble\ntraining methods have emerged as promising solutions for defending against\nadversarial attacks by diversifying the vulnerabilities among the sub-models,\nsimultaneously maintaining comparable accuracy as standard training. However,\nexisting ensemble methods are with poor scalability, owing to the rapid\ncomplexity increase when including more sub-models in the ensemble. Moreover,\nin real-world applications, it is difficult to deploy an ensemble with multiple\nsub-models, owing to the tight hardware resource budget and latency\nrequirement. In this work, we propose ensemble-in-one (EIO), a simple but\nefficient way to train an ensemble within one random gated network (RGN). EIO\naugments the original model by replacing the parameterized layers with\nmulti-path random gated blocks (RGBs) to construct a RGN. By diversifying the\nvulnerability of the numerous paths within the RGN, better robustness can be\nachieved. It provides high scalability because the paths within an EIO network\nexponentially increase with the network depth. Our experiments demonstrate that\nEIO consistently outperforms previous ensemble training methods with even less\ncomputational overhead.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 03:13:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cai", "Yi", ""], ["Ning", "Xuefei", ""], ["Yang", "Huazhong", ""], ["Wang", "Yu", ""]]}, {"id": "2103.14799", "submitter": "Shuren Qi", "authors": "Shuren Qi, Yushu Zhang, Chao Wang, Jiantao Zhou, Xiaochun Cao", "title": "A Survey of Orthogonal Moments for Image Representation: Theory,\n  Implementation, and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image representation is an important topic in computer vision and pattern\nrecognition. It plays a fundamental role in a range of applications towards\nunderstanding visual contents. Moment-based image representation has been\nreported to be effective in satisfying the core conditions of semantic\ndescription due to its beneficial mathematical properties, especially geometric\ninvariance and independence. This paper presents a comprehensive survey of the\northogonal moments for image representation, covering recent advances in\nfast/accurate calculation, robustness/invariance optimization, and definition\nextension. We also create a software package for a variety of widely-used\northogonal moments and evaluate such methods in a same base. The presented\ntheory analysis, software implementation, and evaluation results can support\nthe community, particularly in developing novel techniques and promoting\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 03:41:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Qi", "Shuren", ""], ["Zhang", "Yushu", ""], ["Wang", "Chao", ""], ["Zhou", "Jiantao", ""], ["Cao", "Xiaochun", ""]]}, {"id": "2103.14803", "submitter": "Yaoyao Zhong", "authors": "Yaoyao Zhong and Weihong Deng", "title": "Face Transformer for Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a growing interest in Transformer not only in NLP but\nalso in computer vision. We wonder if transformer can be used in face\nrecognition and whether it is better than CNNs. Therefore, we investigate the\nperformance of Transformer models in face recognition. Considering the original\nTransformer may neglect the inter-patch information, we modify the patch\ngeneration process and make the tokens with sliding patches which overlaps with\neach others. The models are trained on CASIA-WebFace and MS-Celeb-1M databases,\nand evaluated on several mainstream benchmarks, including LFW, SLLFW, CALFW,\nCPLFW, TALFW, CFP-FP, AGEDB and IJB-C databases. We demonstrate that Face\nTransformer models trained on a large-scale database, MS-Celeb-1M, achieve\ncomparable performance as CNN with similar number of parameters and MACs. To\nfacilitate further researches, Face Transformer models and codes are available\nat https://github.com/zhongyy/Face-Transformer.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 03:53:29 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 02:55:36 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhong", "Yaoyao", ""], ["Deng", "Weihong", ""]]}, {"id": "2103.14811", "submitter": "Yiqun Liu", "authors": "Yiqun Liu, Yi Zeng, Jian Pu, Hongming Shan, Peiyang He, Junping Zhang", "title": "SelfGait: A Spatiotemporal Representation Learning Method for\n  Self-supervised Gait Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gait recognition plays a vital role in human identification since gait is a\nunique biometric feature that can be perceived at a distance. Although existing\ngait recognition methods can learn gait features from gait sequences in\ndifferent ways, the performance of gait recognition suffers from insufficient\nlabeled data, especially in some practical scenarios associated with short gait\nsequences or various clothing styles. It is unpractical to label the numerous\ngait data. In this work, we propose a self-supervised gait recognition method,\ntermed SelfGait, which takes advantage of the massive, diverse, unlabeled gait\ndata as a pre-training process to improve the representation abilities of\nspatiotemporal backbones. Specifically, we employ the horizontal pyramid\nmapping (HPM) and micro-motion template builder (MTB) as our spatiotemporal\nbackbones to capture the multi-scale spatiotemporal representations.\nExperiments on CASIA-B and OU-MVLP benchmark gait datasets demonstrate the\neffectiveness of the proposed SelfGait compared with four state-of-the-art gait\nrecognition methods. The source code has been released at\nhttps://github.com/EchoItLiu/SelfGait.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 05:15:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Yiqun", ""], ["Zeng", "Yi", ""], ["Pu", "Jian", ""], ["Shan", "Hongming", ""], ["He", "Peiyang", ""], ["Zhang", "Junping", ""]]}, {"id": "2103.14829", "submitter": "Tianyu Zhu", "authors": "Tianyu Zhu, Markus Hiller, Mahsa Ehsanpour, Rongkai Ma, Tom Drummond,\n  Hamid Rezatofighi", "title": "Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using\n  Spatial and Temporal Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tracking a time-varying indefinite number of objects in a video sequence over\ntime remains a challenge despite recent advances in the field. Ignoring\nlong-term temporal information, most existing approaches are not able to\nproperly handle multi-object tracking challenges such as occlusion. To address\nthese shortcomings, we present MO3TR: a truly end-to-end Transformer-based\nonline multi-object tracking (MOT) framework that learns to handle occlusions,\ntrack initiation and termination without the need for an explicit data\nassociation module or any heuristics/post-processing. MO3TR encodes object\ninteractions into long-term temporal embeddings using a combination of spatial\nand temporal Transformers, and recursively uses the information jointly with\nthe input data to estimate the states of all tracked objects over time. The\nspatial attention mechanism enables our framework to learn implicit\nrepresentations between all the objects and the objects to the measurements,\nwhile the temporal attention mechanism focuses on specific parts of past\ninformation, allowing our approach to resolve occlusions over multiple frames.\nOur experiments demonstrate the potential of this new approach, reaching new\nstate-of-the-art results on multiple MOT metrics for two popular multi-object\ntracking benchmarks. Our code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 07:23:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhu", "Tianyu", ""], ["Hiller", "Markus", ""], ["Ehsanpour", "Mahsa", ""], ["Ma", "Rongkai", ""], ["Drummond", "Tom", ""], ["Rezatofighi", "Hamid", ""]]}, {"id": "2103.14835", "submitter": "Zhijie Deng", "authors": "Zhijie Deng, Xiao Yang, Shizhen Xu, Hang Su, Jun Zhu", "title": "LiBRe: A Practical Bayesian Approach to Adversarial Detection", "comments": "IEEE/ CVF International Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite their appealing flexibility, deep neural networks (DNNs) are\nvulnerable against adversarial examples. Various adversarial defense strategies\nhave been proposed to resolve this problem, but they typically demonstrate\nrestricted practicability owing to unsurmountable compromise on universality,\neffectiveness, or efficiency. In this work, we propose a more practical\napproach, Lightweight Bayesian Refinement (LiBRe), in the spirit of leveraging\nBayesian neural networks (BNNs) for adversarial detection. Empowered by the\ntask and attack agnostic modeling under Bayes principle, LiBRe can endow a\nvariety of pre-trained task-dependent DNNs with the ability of defending\nheterogeneous adversarial attacks at a low cost. We develop and integrate\nadvanced learning techniques to make LiBRe appropriate for adversarial\ndetection. Concretely, we build the few-layer deep ensemble variational and\nadopt the pre-training & fine-tuning workflow to boost the effectiveness and\nefficiency of LiBRe. We further provide a novel insight to realise adversarial\ndetection-oriented uncertainty quantification without inefficiently crafting\nadversarial examples during training. Extensive empirical studies covering a\nwide range of scenarios verify the practicability of LiBRe. We also conduct\nthorough ablation studies to evidence the superiority of our modeling and\nlearning strategies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 07:48:58 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 06:42:21 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Deng", "Zhijie", ""], ["Yang", "Xiao", ""], ["Xu", "Shizhen", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2103.14843", "submitter": "Chen Li", "authors": "Chen Li, Gim Hee Lee", "title": "From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose\n  Estimation", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal pose estimation is an important field that has received increasing\nattention in the recent years. The main challenge for this task is the lack of\nlabeled data. Existing works circumvent this problem with pseudo labels\ngenerated from data of other easily accessible domains such as synthetic data.\nHowever, these pseudo labels are noisy even with consistency check or\nconfidence-based filtering due to the domain shift in the data. To solve this\nproblem, we design a multi-scale domain adaptation module (MDAM) to reduce the\ndomain gap between the synthetic and real data. We further introduce an online\ncoarse-to-fine pseudo label updating strategy. Specifically, we propose a\nself-distillation module in an inner coarse-update loop and a mean-teacher in\nan outer fine-update loop to generate new pseudo labels that gradually replace\nthe old ones. Consequently, our model is able to learn from the old pseudo\nlabels at the early stage, and gradually switch to the new pseudo labels to\nprevent overfitting in the later stage. We evaluate our approach on the TigDog\nand VisDA 2019 datasets, where we outperform existing approaches by a large\nmargin. We also demonstrate the generalization ability of our model by testing\nextensively on both unseen domains and unseen animal categories. Our code is\navailable at the project website.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 08:39:43 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Chen", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2103.14845", "submitter": "Naoki Okamoto", "authors": "Naoki Okamoto, Soma Minami, Tsubasa Hirakawa, Takayoshi Yamashita,\n  Hironobu Fujiyoshi", "title": "Deep Ensemble Collaborative Learning by using Knowledge-transfer Graph\n  for Fine-grained Object Classification", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual learning, in which multiple networks learn by sharing their knowledge,\nimproves the performance of each network. However, the performance of ensembles\nof networks that have undergone mutual learning does not improve significantly\nfrom that of normal ensembles without mutual learning, even though the\nperformance of each network has improved significantly. This may be due to the\nrelationship between the knowledge in mutual learning and the individuality of\nthe networks in the ensemble. In this study, we propose an ensemble method\nusing knowledge transfer to improve the accuracy of ensembles by introducing a\nloss design that promotes diversity among networks in mutual learning. We use\nan attention map as knowledge, which represents the probability distribution\nand information in the middle layer of a network. There are many ways to\ncombine networks and loss designs for knowledge transfer methods. Therefore, we\nuse the automatic optimization of knowledge-transfer graphs to consider a\nvariety of knowledge-transfer methods by graphically representing conventional\nmutual-learning and distillation methods and optimizing each element through\nhyperparameter search. The proposed method consists of a mechanism for\nconstructing an ensemble in a knowledge-transfer graph, attention loss, and a\nloss design that promotes diversity among networks. We explore optimal ensemble\nlearning by optimizing a knowledge-transfer graph to maximize ensemble\naccuracy. From exploration of graphs and evaluation experiments using the\ndatasets of Stanford Dogs, Stanford Cars, and CUB-200-2011, we confirm that the\nproposed method is more accurate than a conventional ensemble method.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 08:56:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Okamoto", "Naoki", ""], ["Minami", "Soma", ""], ["Hirakawa", "Tsubasa", ""], ["Yamashita", "Takayoshi", ""], ["Fujiyoshi", "Hironobu", ""]]}, {"id": "2103.14846", "submitter": "Rui Huang", "authors": "Rui Huang, Chuan Fang, Kejie Qiu, Le Cui, Zilong Dong, Siyu Zhu, Ping\n  Tan", "title": "AR Mapping: Accurate and Efficient Mapping for Augmented Reality", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR) has gained increasingly attention from both research\nand industry communities. By overlaying digital information and content onto\nthe physical world, AR enables users to experience the world in a more\ninformative and efficient manner. As a major building block for AR systems,\nlocalization aims at determining the device's pose from a pre-built \"map\"\nconsisting of visual and depth information in a known environment. While the\nlocalization problem has been widely studied in the literature, the \"map\" for\nAR systems is rarely discussed. In this paper, we introduce the AR Map for a\nspecific scene to be composed of 1) color images with 6-DOF poses; 2) dense\ndepth maps for each image and 3) a complete point cloud map. We then propose an\nefficient end-to-end solution to generating and evaluating AR Maps. Firstly,\nfor efficient data capture, a backpack scanning device is presented with a\nunified calibration pipeline. Secondly, we propose an AR mapping pipeline which\ntakes the input from the scanning device and produces accurate AR Maps.\nFinally, we present an approach to evaluating the accuracy of AR Maps with the\nhelp of the highly accurate reconstruction result from a high-end laser\nscanner. To the best of our knowledge, it is the first time to present an\nend-to-end solution to efficient and accurate mapping for AR applications.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 08:57:48 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Huang", "Rui", ""], ["Fang", "Chuan", ""], ["Qiu", "Kejie", ""], ["Cui", "Le", ""], ["Dong", "Zilong", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""]]}, {"id": "2103.14858", "submitter": "Yi-Hsin Chen", "authors": "Yan-Cheng Huang, Yi-Hsin Chen, Cheng-You Lu, Hui-Po Wang, Wen-Hsiao\n  Peng and Ching-Chun Huang", "title": "Video Rescaling Networks with Joint Optimization Strategies for\n  Downscaling and Upscaling", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the video rescaling task, which arises from the needs of\nadapting the video spatial resolution to suit individual viewing devices. We\naim to jointly optimize video downscaling and upscaling as a combined task.\nMost recent studies focus on image-based solutions, which do not consider\ntemporal information. We present two joint optimization approaches based on\ninvertible neural networks with coupling layers. Our Long Short-Term Memory\nVideo Rescaling Network (LSTM-VRN) leverages temporal information in the\nlow-resolution video to form an explicit prediction of the missing\nhigh-frequency information for upscaling. Our Multi-input Multi-output Video\nRescaling Network (MIMO-VRN) proposes a new strategy for downscaling and\nupscaling a group of video frames simultaneously. Not only do they outperform\nthe image-based invertible model in terms of quantitative and qualitative\nresults, but also show much improved upscaling quality than the video rescaling\nmethods without joint optimization. To our best knowledge, this work is the\nfirst attempt at the joint optimization of video downscaling and upscaling.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 09:35:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Huang", "Yan-Cheng", ""], ["Chen", "Yi-Hsin", ""], ["Lu", "Cheng-You", ""], ["Wang", "Hui-Po", ""], ["Peng", "Wen-Hsiao", ""], ["Huang", "Ching-Chun", ""]]}, {"id": "2103.14862", "submitter": "Wei Gao", "authors": "Wei Gao, Fang Wan, Xingjia Pan, Zhiliang Peng, Qi Tian, Zhenjun Han,\n  Bolei Zhou, Qixiang Ye", "title": "TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised\n  Object Localization", "comments": "10 pages, 9 figures. For appendix, 8 pages, 6 figures. arXiv admin\n  note: text overlap with arXiv:2103.04523", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object localization (WSOL) is a challenging problem when\ngiven image category labels but requires to learn object localization models.\nOptimizing a convolutional neural network (CNN) for classification tends to\nactivate local discriminative regions while ignoring complete object extent,\ncausing the partial activation issue. In this paper, we argue that partial\nactivation is caused by the intrinsic characteristics of CNN, where the\nconvolution operations produce local receptive fields and experience difficulty\nto capture long-range feature dependency among pixels. We introduce the token\nsemantic coupled attention map (TS-CAM) to take full advantage of the\nself-attention mechanism in visual transformer for long-range dependency\nextraction. TS-CAM first splits an image into a sequence of patch tokens for\nspatial embedding, which produce attention maps of long-range visual dependency\nto avoid partial activation. TS-CAM then re-allocates category-related\nsemantics for patch tokens, enabling each of them to be aware of object\ncategories. TS-CAM finally couples the patch tokens with the semantic-agnostic\nattention map to achieve semantic-aware localization. Experiments on the\nILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM\ncounterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 09:43:16 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 09:09:01 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 09:45:10 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 14:19:14 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Gao", "Wei", ""], ["Wan", "Fang", ""], ["Pan", "Xingjia", ""], ["Peng", "Zhiliang", ""], ["Tian", "Qi", ""], ["Han", "Zhenjun", ""], ["Zhou", "Bolei", ""], ["Ye", "Qixiang", ""]]}, {"id": "2103.14869", "submitter": "Jianfeng Cao", "authors": "Jianfeng Cao and Hong Yan", "title": "Instance segmentation with the number of clusters incorporated in\n  embedding learning", "comments": "Accepted by ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic and instance segmentation algorithms are two general yet distinct\nimage segmentation solutions powered by Convolution Neural Network. While\nsemantic segmentation benefits extensively from the end-to-end training\nstrategy, instance segmentation is frequently framed as a multi-stage task,\nsupported by learning-based discrimination and post-process clustering.\nIndependent optimizations on substages instigate the accumulation of\nsegmentation errors. In this work, we propose to embed prior clustering\ninformation into an embedding learning framework FCRNet, stimulating the\none-stage instance segmentation. FCRNet relieves the complexity of post process\nby incorporating the number of clustering groups into the embedding space. The\nsuperior performance of FCRNet is verified and compared with other methods on\nthe nucleus dataset BBBC006.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 10:03:19 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cao", "Jianfeng", ""], ["Yan", "Hong", ""]]}, {"id": "2103.14872", "submitter": "Kun Hu", "authors": "Kun Hu, Zhiyong Wang, Guy Coleman, Asher Bender, Tingting Yao, Shan\n  Zeng, Dezhen Song, Arnold Schumann, Michael Walsh", "title": "Deep Learning Techniques for In-Crop Weed Identification: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weeds are a significant threat to the agricultural productivity and the\nenvironment. The increasing demand for sustainable agriculture has driven\ninnovations in accurate weed control technologies aimed at reducing the\nreliance on herbicides. With the great success of deep learning in various\nvision tasks, many promising image-based weed detection algorithms have been\ndeveloped. This paper reviews recent developments of deep learning techniques\nin the field of image-based weed detection. The review begins with an\nintroduction to the fundamentals of deep learning related to weed detection.\nNext, recent progresses on deep weed detection are reviewed with the discussion\nof the research materials including public weed datasets. Finally, the\nchallenges of developing practically deployable weed detection methods are\nsummarized, together with the discussions of the opportunities for future\nresearch.We hope that this review will provide a timely survey of the field and\nattract more researchers to address this inter-disciplinary research problem.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 10:08:41 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Hu", "Kun", ""], ["Wang", "Zhiyong", ""], ["Coleman", "Guy", ""], ["Bender", "Asher", ""], ["Yao", "Tingting", ""], ["Zeng", "Shan", ""], ["Song", "Dezhen", ""], ["Schumann", "Arnold", ""], ["Walsh", "Michael", ""]]}, {"id": "2103.14877", "submitter": "Yuki Endo", "authors": "Yuki Endo and Yoshihiro Kanamori", "title": "Few-shot Semantic Image Synthesis Using StyleGAN Prior", "comments": "The source codes are available at\n  https://github.com/endo-yuki-t/Fewshot-SMIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles a challenging problem of generating photorealistic images\nfrom semantic layouts in few-shot scenarios where annotated training pairs are\nhardly available but pixel-wise annotation is quite costly. We present a\ntraining strategy that performs pseudo labeling of semantic masks using the\nStyleGAN prior. Our key idea is to construct a simple mapping between the\nStyleGAN feature and each semantic class from a few examples of semantic masks.\nWith such mappings, we can generate an unlimited number of pseudo semantic\nmasks from random noise to train an encoder for controlling a pre-trained\nStyleGAN generator. Although the pseudo semantic masks might be too coarse for\nprevious approaches that require pixel-aligned masks, our framework can\nsynthesize high-quality images from not only dense semantic masks but also\nsparse inputs such as landmarks and scribbles. Qualitative and quantitative\nresults with various datasets demonstrate improvement over previous approaches\nwith respect to layout fidelity and visual quality in as few as one- or\nfive-shot settings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 11:04:22 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 09:37:56 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Endo", "Yuki", ""], ["Kanamori", "Yoshihiro", ""]]}, {"id": "2103.14878", "submitter": "Erfan Taghvaei", "authors": "Shayan Khosravipour, Erfan Taghvaei, Nasrollah Moghadam Charkari", "title": "COVID-19 personal protective equipment detection using real-time deep\n  learning methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential spread of COVID-19 in over 215 countries has led WHO to\nrecommend face masks and gloves for a safe return to school or work. We used\nartificial intelligence and deep learning algorithms for automatic face masks\nand gloves detection in public areas. We investigated and assessed the efficacy\nof two popular deep learning algorithms of YOLO (You Only Look Once) and SSD\nMobileNet for the detection and proper wearing of face masks and gloves trained\nover a data set of 8250 images imported from the internet. YOLOv3 is\nimplemented using the DarkNet framework, and the SSD MobileNet algorithm is\napplied for the development of accurate object detection. The proposed models\nhave been developed to provide accurate multi-class detection (Mask vs. No-Mask\nvs. Gloves vs. No-Gloves vs. Improper). When people wear their masks\nimproperly, the method detects them as an improper class. The introduced models\nprovide accuracies of (90.6% for YOLO and 85.5% for SSD) for multi-class\ndetection. The systems' results indicate the efficiency and validity of\ndetecting people who do not wear masks and gloves in public.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 11:07:11 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Khosravipour", "Shayan", ""], ["Taghvaei", "Erfan", ""], ["Charkari", "Nasrollah Moghadam", ""]]}, {"id": "2103.14884", "submitter": "Yufeng Zheng", "authors": "Yufeng Zheng, Yunkai Zhang, Zeyu Zheng", "title": "Continuous Conditional Generative Adversarial Networks (cGAN) with\n  Generator Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Generative Adversarial Networks are known to be difficult to\ntrain, especially when the conditions are continuous and high-dimensional. To\npartially alleviate this difficulty, we propose a simple generator\nregularization term on the GAN generator loss in the form of Lipschitz penalty.\nThus, when the generator is fed with neighboring conditions in the continuous\nspace, the regularization term will leverage the neighbor information and push\nthe generator to generate samples that have similar conditional distributions\nfor each neighboring condition. We analyze the effect of the proposed\nregularization term and demonstrate its robust performance on a range of\nsynthetic and real-world tasks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 12:01:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zheng", "Yufeng", ""], ["Zhang", "Yunkai", ""], ["Zheng", "Zeyu", ""]]}, {"id": "2103.14887", "submitter": "Navdeep Dahiya", "authors": "Martin Mueller and Navdeep Dahiya and Anthony Yezzi", "title": "An Efficiently Coupled Shape and Appearance Prior for Active Contour\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel training model based on shape and appearance\nfeatures for object segmentation in images and videos. Whereas most such models\nrely on two-dimensional appearance templates or a finite set of descriptors,\nour appearance-based feature is a one-dimensional function, which is\nefficiently coupled with the object's shape by integrating intensities along\nthe object's iso-contours. Joint PCA training on these shape and appearance\nfeatures further exploits shape-appearance correlations and the resulting\ntraining model is incorporated in an active-contour-type energy functional for\nrecognition-segmentation tasks. Experiments on synthetic and infrared images\ndemonstrate how this shape and appearance training model improves accuracy\ncompared to methods based on the Chan-Vese energy.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 12:14:04 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 00:45:20 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Mueller", "Martin", ""], ["Dahiya", "Navdeep", ""], ["Yezzi", "Anthony", ""]]}, {"id": "2103.14896", "submitter": "Ningbo Zhu", "authors": "Ningbo Zhu and Fei Yang", "title": "Representation, Analysis of Bayesian Refinement Approximation Network: A\n  Survey", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  After an artificial model background subtraction, the pixels have been\nlabelled as foreground and background. Previous approaches to secondary\nprocessing the output for denoising usually use traditional methods such as the\nBayesian refinement method. In this paper, we focus on using a modified U-Net\nmodel to approximate the result of the Bayesian refinement method and improve\nthe result. In our modified U-Net model, the result of background subtraction\nfrom other models will be combined with the source image as input for learning\nthe statistical distribution. Thus, the losing information caused by the\nbackground subtraction model can be restored from the source image. Moreover,\nsince the part of the input image is already the output of the other background\nsubtraction model, the feature extraction should be convenient, it only needs\nto change the labels of the noise pixels. Compare with traditional methods,\nusing deep learning methods superiority in keeping details.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 12:55:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhu", "Ningbo", ""], ["Yang", "Fei", ""]]}, {"id": "2103.14898", "submitter": "Shun-Cheng Wu", "authors": "Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab and Federico\n  Tombari", "title": "SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D\n  Sequences", "comments": "Proceedings IEEE Computer Vision and Pattern Recognition (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scene graphs are a compact and explicit representation successfully used in a\nvariety of 2D scene understanding tasks. This work proposes a method to\nincrementally build up semantic scene graphs from a 3D environment given a\nsequence of RGB-D frames. To this end, we aggregate PointNet features from\nprimitive scene components by means of a graph neural network. We also propose\na novel attention mechanism well suited for partial and missing graph data\npresent in such an incremental reconstruction scenario. Although our proposed\nmethod is designed to run on submaps of the scene, we show it also transfers to\nentire 3D scenes. Experiments show that our approach outperforms 3D scene graph\nprediction methods by a large margin and its accuracy is on par with other 3D\nsemantic and panoptic segmentation methods while running at 35 Hz.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 13:00:36 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 08:56:11 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 08:05:08 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wu", "Shun-Cheng", ""], ["Wald", "Johanna", ""], ["Tateno", "Keisuke", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "2103.14899", "submitter": "Chun-Fu (Richard) Chen", "authors": "Chun-Fu Chen, Quanfu Fan, Rameswar Panda", "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recently developed vision transformer (ViT) has achieved promising\nresults on image classification compared to convolutional neural networks.\nInspired by this, in this paper, we study how to learn multi-scale feature\nrepresentations in transformer models for image classification. To this end, we\npropose a dual-branch transformer to combine image patches (i.e., tokens in a\ntransformer) of different sizes to produce stronger image features. Our\napproach processes small-patch and large-patch tokens with two separate\nbranches of different computational complexity and these tokens are then fused\npurely by attention multiple times to complement each other. Furthermore, to\nreduce computation, we develop a simple yet effective token fusion module based\non cross attention, which uses a single token for each branch as a query to\nexchange information with other branches. Our proposed cross-attention only\nrequires linear time for both computational and memory complexity instead of\nquadratic time otherwise. Extensive experiments demonstrate that the proposed\napproach performs better than or on par with several concurrent works on vision\ntransformer, in addition to efficient CNN models. For example, on the\nImageNet1K dataset, with some architectural changes, our approach outperforms\nthe recent DeiT by a large margin of 2\\%\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 13:03:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Chun-Fu", ""], ["Fan", "Quanfu", ""], ["Panda", "Rameswar", ""]]}, {"id": "2103.14907", "submitter": "Zhiguo Luo", "authors": "Zhiguo Luo, Ling-Li Zeng, Hui Shen and Dewen Hu", "title": "Frequency-specific segregation and integration of human cerebral cortex:\n  an intrinsic functional atlas", "comments": "43 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The frequency-specific coupling mechanism of the functional human brain\nnetworks underpins its complex cognitive and behavioral functions.\nNevertheless, it is not well unveiled what are the frequency-specific\nsubdivisions and network topologies of the human brain. In this study, we\nestimated functional connectivity of the human cerebral cortex using spectral\nconnection, and conducted frequency-specific parcellation using\neigen-clustering and gradient-based methods, and then explored their\ntopological structures. 7T fMRI data of 184 subjects in the HCP dataset were\nused for parcellation and exploring the topological properties of the\nfunctional networks, and 3T fMRI data of another 890 subjects were used to\nconfirm the stability of the frequency-specific topologies. Seven to ten\nfunctional networks were stably integrated by two to four dissociable hub\ncategories at specific frequencies, and we proposed an intrinsic functional\natlas containing 456 parcels according to the parcellations across frequencies.\nThe results revealed that the functional networks contained stable\nfrequency-specific topologies, which may imply more abundant roles of the\nfunctional units and more complex interactions among them.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 13:26:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Luo", "Zhiguo", ""], ["Zeng", "Ling-Li", ""], ["Shen", "Hui", ""], ["Hu", "Dewen", ""]]}, {"id": "2103.14908", "submitter": "Sungyeon Kim", "authors": "Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak", "title": "Embedding Transfer with Label Relaxation for Improved Metric Learning", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel method for embedding transfer, a task of\ntransferring knowledge of a learned embedding model to another. Our method\nexploits pairwise similarities between samples in the source embedding space as\nthe knowledge, and transfers them through a loss used for learning target\nembedding models. To this end, we design a new loss called relaxed contrastive\nloss, which employs the pairwise similarities as relaxed labels for\ninter-sample relations. Our loss provides a rich supervisory signal beyond\nclass equivalence, enables more important pairs to contribute more to training,\nand imposes no restriction on manifolds of target embedding spaces. Experiments\non metric learning benchmarks demonstrate that our method largely improves\nperformance, or reduces sizes and output dimensions of target models\neffectively. We further show that it can be also used to enhance quality of\nself-supervised representation and performance of classification models. In all\nthe experiments, our method clearly outperforms existing embedding transfer\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 13:35:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kim", "Sungyeon", ""], ["Kim", "Dongwon", ""], ["Cho", "Minsu", ""], ["Kwak", "Suha", ""]]}, {"id": "2103.14910", "submitter": "Jiaxin Li", "authors": "Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee\n  Lee", "title": "NeMI: Unifying Neural Radiance Fields with Multiplane Images for Novel\n  View Synthesis", "comments": "Main paper and supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose an approach to perform novel view synthesis and\ndepth estimation via dense 3D reconstruction from a single image. Our NeMI\nunifies Neural radiance fields (NeRF) with Multiplane Images (MPI).\nSpecifically, our NeMI is a general two-dimensional and image-conditioned\nextension of NeRF, and a continuous depth generalization of MPI. Given a single\nimage as input, our method predicts a 4-channel image (RGB and volume density)\nat arbitrary depth values to jointly reconstruct the camera frustum and fill in\noccluded contents. The reconstructed and inpainted frustum can then be easily\nrendered into novel RGB or depth views using differentiable rendering.\nExtensive experiments on RealEstate10K, KITTI and Flowers Light Fields show\nthat our NeMI outperforms state-of-the-art by a large margin in novel view\nsynthesis. We also achieve competitive results in depth estimation on iBims-1\nand NYU-v2 without annotated depth supervision. Project page available at\nhttps://vincentfung13.github.io/projects/nemi/\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 13:41:00 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 02:28:33 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Jiaxin", ""], ["Feng", "Zijian", ""], ["She", "Qi", ""], ["Ding", "Henghui", ""], ["Wang", "Changhu", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2103.14938", "submitter": "Shuai Jia", "authors": "Shuai Jia, Yibing Song, Chao Ma, Xiaokang Yang", "title": "IoU Attack: Towards Temporally Coherent Black-Box Adversarial Attack for\n  Visual Object Tracking", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attack arises due to the vulnerability of deep neural networks to\nperceive input samples injected with imperceptible perturbations. Recently,\nadversarial attack has been applied to visual object tracking to evaluate the\nrobustness of deep trackers. Assuming that the model structures of deep\ntrackers are known, a variety of white-box attack approaches to visual tracking\nhave demonstrated promising results. However, the model knowledge about deep\ntrackers is usually unavailable in real applications. In this paper, we propose\na decision-based black-box attack method for visual object tracking. In\ncontrast to existing black-box adversarial attack methods that deal with static\nimages for image classification, we propose IoU attack that sequentially\ngenerates perturbations based on the predicted IoU scores from both current and\nhistorical frames. By decreasing the IoU scores, the proposed attack method\ndegrades the accuracy of temporal coherent bounding boxes (i.e., object\nmotions) accordingly. In addition, we transfer the learned perturbations to the\nnext few frames to initialize temporal motion attack. We validate the proposed\nIoU attack on state-of-the-art deep trackers (i.e., detection based,\ncorrelation filter based, and long-term trackers). Extensive experiments on the\nbenchmark datasets indicate the effectiveness of the proposed IoU attack\nmethod. The source code is available at\nhttps://github.com/VISION-SJTU/IoUattack.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 16:20:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jia", "Shuai", ""], ["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2103.14943", "submitter": "Guanying Chen", "authors": "Guanying Chen, Chaofeng Chen, Shi Guo, Zhetong Liang, Kwan-Yee K.\n  Wong, Lei Zhang", "title": "HDR Video Reconstruction: A Coarse-to-fine Network and A Real-world\n  Benchmark Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) video reconstruction from sequences captured with\nalternating exposures is a very challenging problem. Existing methods often\nalign low dynamic range (LDR) input sequence in the image space using optical\nflow, and then merge the aligned images to produce HDR output. However,\naccurate alignment and fusion in the image space are difficult due to the\nmissing details in the over-exposed regions and noise in the under-exposed\nregions, resulting in unpleasing ghosting artifacts. To enable more accurate\nalignment and HDR fusion, we introduce a coarse-to-fine deep learning framework\nfor HDR video reconstruction. Firstly, we perform coarse alignment and pixel\nblending in the image space to estimate the coarse HDR video. Secondly, we\nconduct more sophisticated alignment and temporal fusion in the feature space\nof the coarse HDR video to produce better reconstruction. Considering the fact\nthat there is no publicly available dataset for quantitative and comprehensive\nevaluation of HDR video reconstruction methods, we collect such a benchmark\ndataset, which contains $97$ sequences of static scenes and 184 testing pairs\nof dynamic scenes. Extensive experiments show that our method outperforms\nprevious state-of-the-art methods. Our dataset, code and model will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 16:40:05 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Guanying", ""], ["Chen", "Chaofeng", ""], ["Guo", "Shi", ""], ["Liang", "Zhetong", ""], ["Wong", "Kwan-Yee K.", ""], ["Zhang", "Lei", ""]]}, {"id": "2103.14949", "submitter": "Ziheng Jiang", "authors": "Ziheng Jiang, Animesh Jain, Andrew Liu, Josh Fromm, Chengqian Ma,\n  Tianqi Chen, Luis Ceze", "title": "Automated Backend-Aware Post-Training Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization is a key technique to reduce the resource requirement and\nimprove the performance of neural network deployment. However, different\nhardware backends such as x86 CPU, NVIDIA GPU, ARM CPU, and accelerators may\ndemand different implementations for quantized networks. This diversity calls\nfor specialized post-training quantization pipelines to built for each hardware\ntarget, an engineering effort that is often too large for developers to keep up\nwith. We tackle this problem with an automated post-training quantization\nframework called HAGO. HAGO provides a set of general quantization graph\ntransformations based on a user-defined hardware specification and implements a\nsearch mechanism to find the optimal quantization strategy while satisfying\nhardware constraints for any model. We observe that HAGO achieves speedups of\n2.09x, 1.97x, and 2.48x on Intel Xeon Cascade Lake CPUs, NVIDIA Tesla T4 GPUs,\nARM Cortex-A CPUs on Raspberry Pi4 relative to full precision respectively,\nwhile maintaining the highest reported post-training quantization accuracy in\neach case.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 17:12:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jiang", "Ziheng", ""], ["Jain", "Animesh", ""], ["Liu", "Andrew", ""], ["Fromm", "Josh", ""], ["Ma", "Chengqian", ""], ["Chen", "Tianqi", ""], ["Ceze", "Luis", ""]]}, {"id": "2103.14953", "submitter": "John Jewell", "authors": "John Taylor Jewell, Vahid Reza Khazaie, Yalda Mohsenzadeh", "title": "OLED: One-Class Learned Encoder-Decoder Network with Adversarial Context\n  Masking for Novelty Detection", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Novelty detection is the task of recognizing samples that do not belong to\nthe distribution of the target class. During training, the novelty class is\nabsent, preventing the use of traditional classification approaches. Deep\nautoencoders have been widely used as a base of many unsupervised novelty\ndetection methods. In particular, context autoencoders have been successful in\nthe novelty detection task because of the more effective representations they\nlearn by reconstructing original images from randomly masked images. However, a\nsignificant drawback of context autoencoders is that random masking fails to\nconsistently cover important structures of the input image, leading to\nsuboptimal representations - especially for the novelty detection task. In this\npaper, to optimize input masking, we have designed a framework consisting of\ntwo competing networks, a Mask Module and a Reconstructor. The Mask Module is a\nconvolutional autoencoder that learns to generate optimal masks that cover the\nmost important parts of images. Alternatively, the Reconstructor is a\nconvolutional encoder-decoder that aims to reconstruct unperturbed images from\nmasked images. The networks are trained in an adversarial manner in which the\nMask Module generates masks that are applied to images given to the\nReconstructor. In this way, the Mask Module seeks to maximize the\nreconstruction error that the Reconstructor is minimizing. When applied to\nnovelty detection, the proposed approach learns semantically richer\nrepresentations compared to context autoencoders and enhances novelty detection\nat test time through more optimal masking. Novelty detection experiments on the\nMNIST and CIFAR-10 image datasets demonstrate the proposed approach's\nsuperiority over cutting-edge methods. In a further experiment on the UCSD\nvideo dataset for novelty detection, the proposed approach achieves\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 17:59:40 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 07:29:35 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Jewell", "John Taylor", ""], ["Khazaie", "Vahid Reza", ""], ["Mohsenzadeh", "Yalda", ""]]}, {"id": "2103.14955", "submitter": "Alvaro Fernandez-Quilez", "authors": "Alvaro Fernandez-Quilez and Steinar Valle Larsen and Morten Goodwin\n  and Thor Ole Gulsurd and Svein Reidar Kjosavik and Ketil Oppedal", "title": "Improving prostate whole gland segmentation in t2-weighted MRI with\n  synthetically generated data", "comments": "5 pages. Accepted as a full paper at the International Symposium on\n  Biomedical Imaging (ISBI) 2021", "journal-ref": null, "doi": "10.1109/ISBI48211.2021.9433793", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Whole gland (WG) segmentation of the prostate plays a crucial role in\ndetection, staging and treatment planning of prostate cancer (PCa). Despite\npromise shown by deep learning (DL) methods, they rely on the availability of a\nconsiderable amount of annotated data. Augmentation techniques such as\ntranslation and rotation of images present an alternative to increase data\navailability. Nevertheless, the amount of information provided by the\ntransformed data is limited due to the correlation between the generated data\nand the original. Based on the recent success of generative adversarial\nnetworks (GAN) in producing synthetic images for other domains as well as in\nthe medical domain, we present a pipeline to generate WG segmentation masks and\nsynthesize T2-weighted MRI of the prostate based on a publicly available\nmulti-center dataset. Following, we use the generated data as a form of data\naugmentation. Results show an improvement in the quality of the WG segmentation\nwhen compared to standard augmentation techniques.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 18:04:11 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Fernandez-Quilez", "Alvaro", ""], ["Larsen", "Steinar Valle", ""], ["Goodwin", "Morten", ""], ["Gulsurd", "Thor Ole", ""], ["Kjosavik", "Svein Reidar", ""], ["Oppedal", "Ketil", ""]]}, {"id": "2103.14962", "submitter": "Zixiang Zhou", "authors": "Zixiang Zhou, Yang Zhang, Hassan Foroosh", "title": "Panoptic-PolarNet: Proposal-free LiDAR Point Cloud Panoptic Segmentation", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation presents a new challenge in exploiting the merits of\nboth detection and segmentation, with the aim of unifying instance segmentation\nand semantic segmentation in a single framework. However, an efficient solution\nfor panoptic segmentation in the emerging domain of LiDAR point cloud is still\nan open research problem and is very much under-explored. In this paper, we\npresent a fast and robust LiDAR point cloud panoptic segmentation framework,\nreferred to as Panoptic-PolarNet. We learn both semantic segmentation and\nclass-agnostic instance clustering in a single inference network using a polar\nBird's Eye View (BEV) representation, enabling us to circumvent the issue of\nocclusion among instances in urban street scenes. To improve our network's\nlearnability, we also propose an adapted instance augmentation technique and a\nnovel adversarial point cloud pruning method. Our experiments show that\nPanoptic-PolarNet outperforms the baseline methods on SemanticKITTI and\nnuScenes datasets with an almost real-time inference speed. Panoptic-PolarNet\nachieved 54.1% PQ in the public SemanticKITTI panoptic segmentation leaderboard\nand leading performance for the validation set of nuScenes.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 18:31:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Zixiang", ""], ["Zhang", "Yang", ""], ["Foroosh", "Hassan", ""]]}, {"id": "2103.14963", "submitter": "Carl Ringqvist Mr", "authors": "Adam Lindhe, Carl Ringqvist and Henrik Hult", "title": "Particle Filter Bridge Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Auto encoding models have been extensively studied in recent years. They\nprovide an efficient framework for sample generation, as well as for analysing\nfeature learning. Furthermore, they are efficient in performing interpolations\nbetween data-points in semantically meaningful ways. In this paper, we build\nfurther on a previously introduced method for generating canonical, dimension\nindependent, stochastic interpolations. Here, the distribution of interpolation\npaths is represented as the distribution of a bridge process constructed from\nan artificial random data generating process in the latent space, having the\nprior distribution as its invariant distribution. As a result the stochastic\ninterpolation paths tend to reside in regions of the latent space where the\nprior has high mass. This is a desirable feature since, generally, such areas\nproduce semantically meaningful samples. In this paper, we extend the bridge\nprocess method by introducing a discriminator network that accurately\nidentifies areas of high latent representation density. The discriminator\nnetwork is incorporated as a change of measure of the underlying bridge process\nand sampling of interpolation paths is implemented using sequential Monte\nCarlo. The resulting sampling procedure allows for greater variability in\ninterpolation paths and stronger drift towards areas of high data density.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 18:33:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lindhe", "Adam", ""], ["Ringqvist", "Carl", ""], ["Hult", "Henrik", ""]]}, {"id": "2103.14968", "submitter": "Rameen Abdal", "authors": "Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka", "title": "Labels4Free: Unsupervised Segmentation using StyleGAN", "comments": "\"Project Page: https://rameenabdal.github.io/Labels4Free/\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose an unsupervised segmentation framework for StyleGAN generated\nobjects. We build on two main observations. First, the features generated by\nStyleGAN hold valuable information that can be utilized towards training\nsegmentation networks. Second, the foreground and background can often be\ntreated to be largely independent and be composited in different ways. For our\nsolution, we propose to augment the StyleGAN2 generator architecture with a\nsegmentation branch and to split the generator into a foreground and background\nnetwork. This enables us to generate soft segmentation masks for the foreground\nobject in an unsupervised fashion. On multiple object classes, we report\ncomparable results against state-of-the-art supervised segmentation networks,\nwhile against the best unsupervised segmentation approach we demonstrate a\nclear improvement, both in qualitative and quantitative metrics.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 18:59:22 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Abdal", "Rameen", ""], ["Zhu", "Peihao", ""], ["Mitra", "Niloy", ""], ["Wonka", "Peter", ""]]}, {"id": "2103.14969", "submitter": "Teofilo Zosa", "authors": "Teofilo E. Zosa", "title": "Catalyzing Clinical Diagnostic Pipelines Through Volumetric Medical\n  Image Segmentation Using Deep Neural Networks: Past, Present, & Future", "comments": "Review paper written for the UCSD PhD Research Mastery Exam; June 7,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has made a remarkable impact in the field of natural image\nprocessing over the past decade. Consequently, there is a great deal of\ninterest in replicating this success across unsolved tasks in related domains,\nsuch as medical image analysis. Core to medical image analysis is the task of\nsemantic segmentation which enables various clinical workflows. Due to the\nchallenges inherent in manual segmentation, many decades of research have been\ndevoted to discovering extensible, automated, expert-level segmentation\ntechniques. Given the groundbreaking performance demonstrated by recent neural\nnetwork-based techniques, deep learning seems poised to achieve what classic\nmethods have historically been unable. This paper will briefly overview some of\nthe state-of-the-art (SoTA) neural network-based segmentation algorithms with a\nparticular emphasis on the most recent architectures, comparing and contrasting\nthe contributions and characteristics of each network topology. Using\nultrasonography as a motivating example, it will also demonstrate important\nclinical implications of effective deep learning-based solutions, articulate\nchallenges unique to the modality, and discuss novel approaches developed in\nresponse to those challenges, concluding with the proposal of future directions\nin the field. Given the generally observed ephemerality of the best deep\nlearning approaches (i.e. the extremely quick succession of the SoTA), the main\ncontributions of the paper are its contextualization of modern deep learning\narchitectures with historical background and the elucidation of the current\ntrajectory of volumetric medical image segmentation research.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 19:05:11 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 00:35:48 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zosa", "Teofilo E.", ""]]}, {"id": "2103.14983", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Ping Luo, Zhe Lin, Kevin Bowyer", "title": "Going Deeper Into Face Detection: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face detection is a crucial first step in many facial recognition and face\nanalysis systems. Early approaches for face detection were mainly based on\nclassifiers built on top of hand-crafted features extracted from local image\nregions, such as Haar Cascades and Histogram of Oriented Gradients. However,\nthese approaches were not powerful enough to achieve a high accuracy on images\nof from uncontrolled environments. With the breakthrough work in image\nclassification using deep neural networks in 2012, there has been a huge\nparadigm shift in face detection. Inspired by the rapid progress of deep\nlearning in computer vision, many deep learning based frameworks have been\nproposed for face detection over the past few years, achieving significant\nimprovements in accuracy. In this work, we provide a detailed overview of some\nof the most representative deep learning based face detection methods by\ngrouping them into a few major categories, and present their core architectural\ndesigns and accuracies on popular benchmarks. We also describe some of the most\npopular face detection datasets. Finally, we discuss some current challenges in\nthe field, and suggest potential future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:18:00 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 18:50:21 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Minaee", "Shervin", ""], ["Luo", "Ping", ""], ["Lin", "Zhe", ""], ["Bowyer", "Kevin", ""]]}, {"id": "2103.14984", "submitter": "Zihao Jian", "authors": "Zihao Jian, Minshan Xie", "title": "Realistic face animation generation from videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  3D face reconstruction and face alignment are two fundamental and highly\nrelated topics in computer vision. Recently, some works start to use deep\nlearning models to estimate the 3DMM coefficients to reconstruct 3D face\ngeometry. However, the performance is restricted due to the limitation of the\npre-defined face templates. To address this problem, some end-to-end methods,\nwhich can completely bypass the calculation of 3DMM coefficients, are proposed\nand attract much attention. In this report, we introduce and analyse three\nstate-of-the-art methods in 3D face reconstruction and face alignment. Some\npotential improvement on PRN are proposed to further enhance its accuracy and\nspeed.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:18:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jian", "Zihao", ""], ["Xie", "Minshan", ""]]}, {"id": "2103.15017", "submitter": "Sergiu Oprea", "authors": "Sergiu Oprea, Giorgos Karvounas, Pablo Martinez-Gonzalez, Nikolaos\n  Kyriazis, Sergio Orts-Escolano, Iason Oikonomidis, Alberto Garcia-Garcia,\n  Aggeliki Tsoli, Jose Garcia-Rodriguez, Antonis Argyros", "title": "H-GAN: the power of GANs in your Hands", "comments": "Paper accepted at The International Joint Conference on Neural\n  Networks (IJCNN) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present HandGAN (H-GAN), a cycle-consistent adversarial learning approach\nimplementing multi-scale perceptual discriminators. It is designed to translate\nsynthetic images of hands to the real domain. Synthetic hands provide complete\nground-truth annotations, yet they are not representative of the target\ndistribution of real-world data. We strive to provide the perfect blend of a\nrealistic hand appearance with synthetic annotations. Relying on image-to-image\ntranslation, we improve the appearance of synthetic hands to approximate the\nstatistical distribution underlying a collection of real images of hands. H-GAN\ntackles not only the cross-domain tone mapping but also structural differences\nin localized areas such as shading discontinuities. Results are evaluated on a\nqualitative and quantitative basis improving previous works. Furthermore, we\nrelied on the hand classification task to claim our generated hands are\nstatistically similar to the real domain of hands.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 23:46:27 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 16:16:41 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Oprea", "Sergiu", ""], ["Karvounas", "Giorgos", ""], ["Martinez-Gonzalez", "Pablo", ""], ["Kyriazis", "Nikolaos", ""], ["Orts-Escolano", "Sergio", ""], ["Oikonomidis", "Iason", ""], ["Garcia-Garcia", "Alberto", ""], ["Tsoli", "Aggeliki", ""], ["Garcia-Rodriguez", "Jose", ""], ["Argyros", "Antonis", ""]]}, {"id": "2103.15027", "submitter": "Xiao Zang", "authors": "Xiao Zang, Yi Xie, Siyu Liao, Jie Chen, Bo Yuan", "title": "Noise Injection-based Regularization for Point Cloud Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Noise injection-based regularization, such as Dropout, has been widely used\nin image domain to improve the performance of deep neural networks (DNNs).\nHowever, efficient regularization in the point cloud domain is rarely\nexploited, and most of the state-of-the-art works focus on data\naugmentation-based regularization. In this paper, we, for the first time,\nperform systematic investigation on noise injection-based regularization for\npoint cloud-domain DNNs. To be specific, we propose a series of regularization\ntechniques, namely DropFeat, DropPoint and DropCluster, to perform noise\ninjection on the point feature maps at the feature level, point level and\ncluster level, respectively. We also empirically analyze the impacts of\ndifferent factors, including dropping rate, cluster size and dropping position,\nto obtain useful insights and general deployment guidelines, which can\nfacilitate the adoption of our approaches across different datasets and DNN\narchitectures.\n  We evaluate our proposed approaches on various DNN models for different point\ncloud processing tasks. Experimental results show our approaches enable\nsignificant performance improvement. Notably, our DropCluster brings 1.5%, 1.3%\nand 0.8% higher overall accuracy for PointNet, PointNet++ and DGCNN,\nrespectively, on ModelNet40 shape classification dataset. On ShapeNet part\nsegmentation dataset, DropCluster brings 0.5%, 0.5% and 0.2% mean\nIntersection-over-union (IoU) increase for PointNet, PointNet++ and DGCNN,\nrespectively. On S3DIS semantic segmentation dataset, DropCluster improves the\nmean IoU of PointNet, PointNet++ and DGCNN by 3.2%, 2.9% and 3.7%,\nrespectively. Meanwhile, DropCluster also enables the overall accuracy increase\nfor these three popular backbone DNNs by 2.4%, 2.2% and 1.8%, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 00:55:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zang", "Xiao", ""], ["Xie", "Yi", ""], ["Liao", "Siyu", ""], ["Chen", "Jie", ""], ["Yuan", "Bo", ""]]}, {"id": "2103.15039", "submitter": "Hongtao Wu", "authors": "Weixiao Liu, Hongtao Wu, Gregory Chirikjian", "title": "LSG-CPD: Coherent Point Drift with Local Surface Geometry for Point\n  Cloud Registration", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic point cloud registration methods are becoming more popular\nbecause of their robustness. However, unlike point-to-plane variants of\niterative closest point (ICP) which incorporate local surface geometric\ninformation such as surface normals, most probabilistic methods (e.g., coherent\npoint drift (CPD)) ignore such information and build Gaussian mixture models\n(GMMs) with isotropic Gaussian covariances. This results in sphere-like GMM\ncomponents which only penalize the point-to-point distance between the two\npoint clouds. In this paper, we propose a novel method called CPD with Local\nSurface Geometry (LSG-CPD) for rigid point cloud registration. Our method\nadaptively adds different levels of point-to-plane penalization on top of the\npoint-to-point penalization based on the flatness of the local surface. This\nresults in GMM components with anisotropic covariances. We formulate point\ncloud registration as a maximum likelihood estimation (MLE) problem and solve\nit with the Expectation-Maximization (EM) algorithm. In the E step, we\ndemonstrate that the computation can be recast into simple matrix manipulations\nand efficiently computed on a GPU. In the M step, we perform an unconstrained\noptimization on a matrix Lie group to efficiently update the rigid\ntransformation of the registration. The proposed method outperforms\nstate-of-the-art algorithms in terms of accuracy and robustness on various\ndatasets captured with range scanners, RGBD cameras, and LiDARs. Also, it is\nsignificantly faster than modern implementations of CPD. The code will be\nreleased.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 03:46:41 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Weixiao", ""], ["Wu", "Hongtao", ""], ["Chirikjian", "Gregory", ""]]}, {"id": "2103.15042", "submitter": "Yinyin He", "authors": "Yin-Yin He, Jianxin Wu, Xiu-Shen Wei", "title": "Distilling Virtual Examples for Long-tailed Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the long-tailed visual recognition problem from the\nknowledge distillation perspective by proposing a Distill the Virtual Examples\n(DiVE) method. Specifically, by treating the predictions of a teacher model as\nvirtual examples, we prove that distilling from these virtual examples is\nequivalent to label distribution learning under certain constraints. We show\nthat when the virtual example distribution becomes flatter than the original\ninput distribution, the under-represented tail classes will receive significant\nimprovements, which is crucial in long-tailed recognition. The proposed DiVE\nmethod can explicitly tune the virtual example distribution to become flat.\nExtensive experiments on three benchmark datasets, including the large-scale\niNaturalist ones, justify that the proposed DiVE method can significantly\noutperform state-of-the-art methods. Furthermore, additional analyses and\nexperiments verify the virtual example interpretation, and demonstrate the\neffectiveness of tailored designs in DiVE for long-tailed problems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 04:25:43 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["He", "Yin-Yin", ""], ["Wu", "Jianxin", ""], ["Wei", "Xiu-Shen", ""]]}, {"id": "2103.15049", "submitter": "Song Liu", "authors": "Song Liu and Haoqi Fan and Shengsheng Qian and Yiru Chen and Wenkui\n  Ding and Zhongyuan Wang", "title": "HiT: Hierarchical Transformer with Momentum Contrast for Video-Text\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video-Text Retrieval has been a hot research topic with the explosion of\nmultimedia data on the Internet. Transformer for video-text learning has\nattracted increasing attention due to the promising performance.However,\nexisting cross-modal transformer approaches typically suffer from two major\nlimitations: 1) Limited exploitation of the transformer architecture where\ndifferent layers have different feature characteristics. 2) End-to-end training\nmechanism limits negative interactions among samples in a mini-batch. In this\npaper, we propose a novel approach named Hierarchical Transformer (HiT) for\nvideo-text retrieval. HiT performs hierarchical cross-modal contrastive\nmatching in feature-level and semantic-level to achieve multi-view and\ncomprehensive retrieval results. Moreover, inspired by MoCo, we propose\nMomentum Cross-modal Contrast for cross-modal learning to enable large-scale\nnegative interactions on-the-fly, which contributes to the generation of more\nprecise and discriminative representations. Experimental results on three major\nVideo-Text Retrieval benchmark datasets demonstrate the advantages of our\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 04:52:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Song", ""], ["Fan", "Haoqi", ""], ["Qian", "Shengsheng", ""], ["Chen", "Yiru", ""], ["Ding", "Wenkui", ""], ["Wang", "Zhongyuan", ""]]}, {"id": "2103.15053", "submitter": "Sophia Abraham", "authors": "Sophia Abraham, Zachariah Carmichael, Sreya Banerjee, Rosaura\n  VidalMata, Ankit Agrawal, Md Nafee Al Islam, Walter Scheirer, Jane\n  Cleland-Huang", "title": "Adaptive Autonomy in Human-on-the-Loop Vision-Based Robotics Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision approaches are widely used by autonomous robotic systems to\nsense the world around them and to guide their decision making as they perform\ndiverse tasks such as collision avoidance, search and rescue, and object\nmanipulation. High accuracy is critical, particularly for Human-on-the-loop\n(HoTL) systems where decisions are made autonomously by the system, and humans\nplay only a supervisory role. Failures of the vision model can lead to\nerroneous decisions with potentially life or death consequences. In this paper,\nwe propose a solution based upon adaptive autonomy levels, whereby the system\ndetects loss of reliability of these models and responds by temporarily\nlowering its own autonomy levels and increasing engagement of the human in the\ndecision-making process. Our solution is applicable for vision-based tasks in\nwhich humans have time to react and provide guidance. When implemented, our\napproach would estimate the reliability of the vision task by considering\nuncertainty in its model, and by performing covariate analysis to determine\nwhen the current operating environment is ill-matched to the model's training\ndata. We provide examples from DroneResponse, in which small Unmanned Aerial\nSystems are deployed for Emergency Response missions, and show how the vision\nmodel's reliability would be used in addition to confidence scores to drive and\nspecify the behavior and adaptation of the system's autonomy. This workshop\npaper outlines our proposed approach and describes open challenges at the\nintersection of Computer Vision and Software Engineering for the safe and\nreliable deployment of vision models in the decision making of autonomous\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 05:43:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Abraham", "Sophia", ""], ["Carmichael", "Zachariah", ""], ["Banerjee", "Sreya", ""], ["VidalMata", "Rosaura", ""], ["Agrawal", "Ankit", ""], ["Islam", "Md Nafee Al", ""], ["Scheirer", "Walter", ""], ["Cleland-Huang", "Jane", ""]]}, {"id": "2103.15055", "submitter": "Yifan Zhou", "authors": "Yifan Zhou, Yifan Ge, Jianxin Wu", "title": "Friends and Foes in Learning from Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from examples with noisy labels has attracted increasing attention\nrecently. But, this paper will show that the commonly used CIFAR-based datasets\nand the accuracy evaluation metric used in the literature are both\ninappropriate in this context. An alternative valid evaluation metric and new\ndatasets are proposed in this paper to promote proper research and evaluation\nin this area. Then, friends and foes are identified from existing methods as\ntechnical components that are either beneficial or detrimental to deep learning\nfrom noisy labeled examples, respectively, and this paper improves and combines\ntechnical components from the friends category, including self-supervised\nlearning, new warmup strategy, instance filtering and label correction. The\nresulting F&F method significantly outperforms existing methods on the proposed\nnCIFAR datasets and the real-world Clothing1M dataset.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 06:05:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Yifan", ""], ["Ge", "Yifan", ""], ["Wu", "Jianxin", ""]]}, {"id": "2103.15061", "submitter": "Yazhou Xing", "authors": "Yazhou Xing, Zian Qian, Qifeng Chen", "title": "Invertible Image Signal Processing", "comments": "Accepted to CVPR2021. Code available at:\n  https://github.com/yzxing87/Invertible-ISP", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unprocessed RAW data is a highly valuable image format for image editing and\ncomputer vision. However, since the file size of RAW data is huge, most users\ncan only get access to processed and compressed sRGB images. To bridge this\ngap, we design an Invertible Image Signal Processing (InvISP) pipeline, which\nnot only enables rendering visually appealing sRGB images but also allows\nrecovering nearly perfect RAW data. Due to our framework's inherent\nreversibility, we can reconstruct realistic RAW data instead of synthesizing\nRAW data from sRGB images without any memory overhead. We also integrate a\ndifferentiable JPEG compression simulator that empowers our framework to\nreconstruct RAW data from JPEG images. Extensive quantitative and qualitative\nexperiments on two DSLR demonstrate that our method obtains much higher quality\nin both rendered sRGB images and reconstructed RAW data than alternative\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 06:30:15 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 03:34:56 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Xing", "Yazhou", ""], ["Qian", "Zian", ""], ["Chen", "Qifeng", ""]]}, {"id": "2103.15068", "submitter": "Yanyan Li", "authors": "Raza Yunus, Yanyan Li and Federico Tombari", "title": "ManhattanSLAM: Robust Planar Tracking and Mapping Leveraging Mixture of\n  Manhattan Frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a robust RGB-D SLAM system is proposed to utilize the\nstructural information in indoor scenes, allowing for accurate tracking and\nefficient dense mapping on a CPU. Prior works have used the Manhattan World\n(MW) assumption to estimate low-drift camera pose, in turn limiting the\napplications of such systems. This paper, in contrast, proposes a novel\napproach delivering robust tracking in MW and non-MW environments. We check\northogonal relations between planes to directly detect Manhattan Frames,\nmodeling the scene as a Mixture of Manhattan Frames. For MW scenes, we decouple\npose estimation and provide a novel drift-free rotation estimation based on\nManhattan Frame observations. For translation estimation in MW scenes and full\ncamera pose estimation in non-MW scenes, we make use of point, line and plane\nfeatures for robust tracking in challenging scenes. %mapping Additionally, by\nexploiting plane features detected in each frame, we also propose an efficient\nsurfel-based dense mapping strategy, which divides each image into planar and\nnon-planar regions. Planar surfels are initialized directly from sparse planes\nin our map while non-planar surfels are built by extracting superpixels. We\nevaluate our method on public benchmarks for pose estimation, drift and\nreconstruction accuracy, achieving superior performance compared to other\nstate-of-the-art methods. We will open-source our code in the future.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 07:11:57 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yunus", "Raza", ""], ["Li", "Yanyan", ""], ["Tombari", "Federico", ""]]}, {"id": "2103.15069", "submitter": "Jie Xu", "authors": "Jie Xu, Yazhou Ren, Huayi Tang, Zhimeng Yang, Lili Pan, Yang Yang,\n  Xiaorong Pu", "title": "Self-supervised Discriminative Feature Learning for Deep Multi-view\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering is an important research topic due to its capability to\nutilize complementary information from multiple views. However, there are few\nmethods to consider the negative impact caused by certain views with unclear\nclustering structures, resulting in poor multi-view clustering performance. To\naddress this drawback, we propose self-supervised discriminative feature\nlearning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders\nare applied to learn embedded features for each view independently. To leverage\nthe multi-view complementary information, we concatenate all views' embedded\nfeatures to form the global features, which can overcome the negative impact of\nsome views' unclear clustering structures. In a self-supervised manner,\npseudo-labels are obtained to build a unified target distribution to perform\nmulti-view discriminative feature learning. During this process, global\ndiscriminative information can be mined to supervise all views to learn more\ndiscriminative features, which in turn are used to update the target\ndistribution. Besides, this unified target distribution can make SDMVC learn\nconsistent cluster assignments, which accomplishes the clustering consistency\nof multiple views while preserving their features' diversity. Experiments on\nvarious types of multi-view datasets show that SDMVC achieves state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 07:18:39 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 07:04:11 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Xu", "Jie", ""], ["Ren", "Yazhou", ""], ["Tang", "Huayi", ""], ["Yang", "Zhimeng", ""], ["Pan", "Lili", ""], ["Yang", "Yang", ""], ["Pu", "Xiaorong", ""]]}, {"id": "2103.15074", "submitter": "Shinnosuke Matsuo", "authors": "Shinnosuke Matsuo, Xiaomeng Wu, Gantugs Atarsaikhan, Akisato Kimura,\n  Kunio Kashino, Brian Kenji Iwana, Seiichi Uchida", "title": "Attention to Warp: Deep Metric Learning for Multivariate Time Series", "comments": "Accepted at ICDAR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep time series metric learning is challenging due to the difficult\ntrade-off between temporal invariance to nonlinear distortion and\ndiscriminative power in identifying non-matching sequences. This paper proposes\na novel neural network-based approach for robust yet discriminative time series\nclassification and verification. This approach adapts a parameterized attention\nmodel to time warping for greater and more adaptive temporal invariance. It is\nrobust against not only local but also large global distortions, so that even\nmatching pairs that do not satisfy the monotonicity, continuity, and boundary\nconditions can still be successfully identified. Learning of this model is\nfurther guided by dynamic time warping to impose temporal constraints for\nstabilized training and higher discriminative power. It can learn to augment\nthe inter-class variation through warping, so that similar but different\nclasses can be effectively distinguished. We experimentally demonstrate the\nsuperiority of the proposed approach over previous non-parametric and deep\nmodels by combining it with a deep online signature verification framework,\nafter confirming its promising behavior in single-letter handwriting\nclassification on the Unipen dataset.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 07:54:01 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 04:31:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Matsuo", "Shinnosuke", ""], ["Wu", "Xiaomeng", ""], ["Atarsaikhan", "Gantugs", ""], ["Kimura", "Akisato", ""], ["Kashino", "Kunio", ""], ["Iwana", "Brian Kenji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2103.15076", "submitter": "Huan Lei", "authors": "Huan Lei, Naveed Akhtar, Ajmal Mian", "title": "Picasso: A CUDA-based Library for Deep Learning over 3D Meshes", "comments": "Accepted to CVPR2021", "journal-ref": "CVPR,2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Picasso, a CUDA-based library comprising novel modules for deep\nlearning over complex real-world 3D meshes. Hierarchical neural architectures\nhave proved effective in multi-scale feature extraction which signifies the\nneed for fast mesh decimation. However, existing methods rely on CPU-based\nimplementations to obtain multi-resolution meshes. We design GPU-accelerated\nmesh decimation to facilitate network resolution reduction efficiently\non-the-fly. Pooling and unpooling modules are defined on the vertex clusters\ngathered during decimation. For feature learning over meshes, Picasso contains\nthree types of novel convolutions namely, facet2vertex, vertex2facet, and\nfacet2facet convolution. Hence, it treats a mesh as a geometric structure\ncomprising vertices and facets, rather than a spatial graph with edges as\nprevious methods do. Picasso also incorporates a fuzzy mechanism in its filters\nfor robustness to mesh sampling (vertex density). It exploits Gaussian mixtures\nto define fuzzy coefficients for the facet2vertex convolution, and barycentric\ninterpolation to define the coefficients for the remaining two convolutions. In\nthis release, we demonstrate the effectiveness of the proposed modules with\ncompetitive segmentation results on S3DIS. The library will be made public\nthrough https://github.com/hlei-ziyan/Picasso.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 08:04:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lei", "Huan", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "2103.15086", "submitter": "Da-Wei Zhou", "authors": "Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan", "title": "Learning Placeholders for Open-Set Recognition", "comments": "Accepted to CVPR 2021 as an Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional classifiers are deployed under closed-set setting, with both\ntraining and test classes belong to the same set. However, real-world\napplications probably face the input of unknown categories, and the model will\nrecognize them as known ones. Under such circumstances, open-set recognition is\nproposed to maintain classification performance on known classes and reject\nunknowns. The closed-set models make overconfident predictions over familiar\nknown class instances, so that calibration and thresholding across categories\nbecome essential issues when extending to an open-set environment. To this end,\nwe proposed to learn PlaceholdeRs for Open-SEt Recognition (Proser), which\nprepares for the unknown classes by allocating placeholders for both data and\nclassifier. In detail, learning data placeholders tries to anticipate open-set\nclass data, thus transforms closed-set training into open-set training.\nBesides, to learn the invariant information between target and non-target\nclasses, we reserve classifier placeholders as the class-specific boundary\nbetween known and unknown. The proposed Proser efficiently generates novel\nclass by manifold mixup, and adaptively sets the value of reserved open-set\nclassifier during training. Experiments on various datasets validate the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 09:18:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Da-Wei", ""], ["Ye", "Han-Jia", ""], ["Zhan", "De-Chuan", ""]]}, {"id": "2103.15087", "submitter": "Chenjie Cao", "authors": "Chenjie Cao, Yanwei Fu", "title": "Learning a Sketch Tensor Space for Image Inpainting of Man-made Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper studies the task of inpainting man-made scenes. It is very\nchallenging due to the difficulty in preserving the visual patterns of images,\nsuch as edges, lines, and junctions. Especially, most previous works are failed\nto restore the object/building structures for images of man-made scenes. To\nthis end, this paper proposes learning a Sketch Tensor (ST) space for\ninpainting man-made scenes. Such a space is learned to restore the edges,\nlines, and junctions in images, and thus makes reliable predictions of the\nholistic image structures. To facilitate the structure refinement, we propose a\nMulti-scale Sketch Tensor inpainting (MST) network, with a novel\nencoder-decoder structure. The encoder extracts lines and edges from the input\nimages to project them into an ST space. From this space, the decoder is\nlearned to restore the input images. Extensive experiments validate the\nefficacy of our model. Furthermore, our model can also achieve competitive\nperformance in inpainting general nature images over the competitors.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 09:18:20 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cao", "Chenjie", ""], ["Fu", "Yanwei", ""]]}, {"id": "2103.15088", "submitter": "Ziyi Liu", "authors": "Ziyi Liu, Le Wang, Qilin Zhang, Wei Tang, Junsong Yuan, Nanning Zheng,\n  Gang Hua", "title": "ACSNet: Action-Context Separation Network for Weakly Supervised Temporal\n  Action Localization", "comments": "Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The object of Weakly-supervised Temporal Action Localization (WS-TAL) is to\nlocalize all action instances in an untrimmed video with only video-level\nsupervision. Due to the lack of frame-level annotations during training,\ncurrent WS-TAL methods rely on attention mechanisms to localize the foreground\nsnippets or frames that contribute to the video-level classification task. This\nstrategy frequently confuse context with the actual action, in the localization\nresult. Separating action and context is a core problem for precise WS-TAL, but\nit is very challenging and has been largely ignored in the literature. In this\npaper, we introduce an Action-Context Separation Network (ACSNet) that\nexplicitly takes into account context for accurate action localization. It\nconsists of two branches (i.e., the Foreground-Background branch and the\nAction-Context branch). The Foreground- Background branch first distinguishes\nforeground from background within the entire video while the Action-Context\nbranch further separates the foreground as action and context. We associate\nvideo snippets with two latent components (i.e., a positive component and a\nnegative component), and their different combinations can effectively\ncharacterize foreground, action and context. Furthermore, we introduce extended\nlabels with auxiliary context categories to facilitate the learning of\naction-context separation. Experiments on THUMOS14 and ActivityNet v1.2/v1.3\ndatasets demonstrate the ACSNet outperforms existing state-of-the-art WS-TAL\nmethods by a large margin.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 09:20:54 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Ziyi", ""], ["Wang", "Le", ""], ["Zhang", "Qilin", ""], ["Tang", "Wei", ""], ["Yuan", "Junsong", ""], ["Zheng", "Nanning", ""], ["Hua", "Gang", ""]]}, {"id": "2103.15089", "submitter": "Chenlin Meng", "authors": "Chenlin Meng, Jiaming Song, Yang Song, Shengjia Zhao, and Stefano\n  Ermon", "title": "Improved Autoregressive Modeling with Distribution Smoothing", "comments": "ICLR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While autoregressive models excel at image compression, their sample quality\nis often lacking. Although not realistic, generated images often have high\nlikelihood according to the model, resembling the case of adversarial examples.\nInspired by a successful adversarial defense method, we incorporate randomized\nsmoothing into autoregressive generative modeling. We first model a smoothed\nversion of the data distribution, and then reverse the smoothing process to\nrecover the original data distribution. This procedure drastically improves the\nsample quality of existing autoregressive models on several synthetic and\nreal-world image datasets while obtaining competitive likelihoods on synthetic\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 09:21:20 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Meng", "Chenlin", ""], ["Song", "Jiaming", ""], ["Song", "Yang", ""], ["Zhao", "Shengjia", ""], ["Ermon", "Stefano", ""]]}, {"id": "2103.15099", "submitter": "Qishang Cheng", "authors": "Qishang Cheng, Hongliang Li, Qingbo Wu and King Ngi Ngan", "title": "BA^2M: A Batch Aware Attention Module for Image Classification", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The attention mechanisms have been employed in Convolutional Neural Network\n(CNN) to enhance the feature representation. However, existing attention\nmechanisms only concentrate on refining the features inside each sample and\nneglect the discrimination between different samples. In this paper, we propose\na batch aware attention module (BA2M) for feature enrichment from a distinctive\nperspective. More specifically, we first get the sample-wise attention\nrepresentation (SAR) by fusing the channel, local spatial and global spatial\nattention maps within each sample. Then, we feed the SARs of the whole batch to\na normalization function to get the weights for each sample. The weights serve\nto distinguish the features' importance between samples in a training batch\nwith different complexity of content. The BA2M could be embedded into different\nparts of CNN and optimized with the network in an end-to-end manner. The design\nof BA2M is lightweight with few extra parameters and calculations. We validate\nBA2M through extensive experiments on CIFAR-100 and ImageNet-1K for the image\nrecognition task. The results show that BA2M can boost the performance of\nvarious network architectures and outperforms many classical attention methods.\nBesides, BA2M exceeds traditional methods of re-weighting samples based on the\nloss value.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 10:04:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cheng", "Qishang", ""], ["Li", "Hongliang", ""], ["Wu", "Qingbo", ""], ["Ngan", "King Ngi", ""]]}, {"id": "2103.15105", "submitter": "Faraz Lotfi Dr", "authors": "Faraz Lotfi, Hamid D. Taghirad", "title": "Single Object Tracking through a Fast and Effective Single-Multiple\n  Model Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking becomes critical especially when similar objects are present\nin the same area. Recent state-of-the-art (SOTA) approaches are proposed based\non taking a matching network with a heavy structure to distinguish the target\nfrom other objects in the area which indeed drastically downgrades the\nperformance of the tracker in terms of speed. Besides, several candidates are\nconsidered and processed to localize the intended object in a region of\ninterest for each frame which is time-consuming. In this article, a special\narchitecture is proposed based on which in contrast to the previous approaches,\nit is possible to identify the object location in a single shot while taking\nits template into account to distinguish it from the similar objects in the\nsame area. In brief, first of all, a window containing the object with twice\nthe target size is considered. This window is then fed into a fully\nconvolutional neural network (CNN) to extract a region of interest (RoI) in a\nform of a matrix for each of the frames. In the beginning, a template of the\ntarget is also taken as the input to the CNN. Considering this RoI matrix, the\nnext movement of the tracker is determined based on a simple and fast method.\nMoreover, this matrix helps to estimate the object size which is crucial when\nit changes over time. Despite the absence of a matching network, the presented\ntracker performs comparatively with the SOTA in challenging situations while\nhaving a super speed compared to them (up to $120 FPS$ on 1080ti). To\ninvestigate this claim, a comparison study is carried out on the GOT-10k\ndataset. Results reveal the outstanding performance of the proposed method in\nfulfilling the task.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 11:02:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lotfi", "Faraz", ""], ["Taghirad", "Hamid D.", ""]]}, {"id": "2103.15108", "submitter": "Wanhua Li", "authors": "Wanhua Li, Shiwei Wang, Jiwen Lu, Jianjiang Feng, Jie Zhou", "title": "Meta-Mining Discriminative Samples for Kinship Verification", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinship verification aims to find out whether there is a kin relation for a\ngiven pair of facial images. Kinship verification databases are born with\nunbalanced data. For a database with N positive kinship pairs, we naturally\nobtain N(N-1) negative pairs. How to fully utilize the limited positive pairs\nand mine discriminative information from sufficient negative samples for\nkinship verification remains an open issue. To address this problem, we propose\na Discriminative Sample Meta-Mining (DSMM) approach in this paper. Unlike\nexisting methods that usually construct a balanced dataset with fixed negative\npairs, we propose to utilize all possible pairs and automatically learn\ndiscriminative information from data. Specifically, we sample an unbalanced\ntrain batch and a balanced meta-train batch for each iteration. Then we learn a\nmeta-miner with the meta-gradient on the balanced meta-train batch. In the end,\nthe samples in the unbalanced train batch are re-weighted by the learned\nmeta-miner to optimize the kinship models. Experimental results on the widely\nused KinFaceW-I, KinFaceW-II, TSKinFace, and Cornell Kinship datasets\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 11:47:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Wanhua", ""], ["Wang", "Shiwei", ""], ["Lu", "Jiwen", ""], ["Feng", "Jianjiang", ""], ["Zhou", "Jie", ""]]}, {"id": "2103.15114", "submitter": "Lifeng Gu", "authors": "Lifeng Gu", "title": "Explaining Representation by Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Science is used to discover the law of world. Machine learning can be used to\ndiscover the law of data. In recent years, there are more and more research\nabout interpretability in machine learning community. We hope the machine\nlearning methods are safe, interpretable, and they can help us to find\nmeaningful pattern in data. In this paper, we focus on interpretability of deep\nrepresentation. We propose a interpretable method of representation based on\nmutual information, which summarizes the interpretation of representation into\nthree types of information between input data and representation. We further\nproposed MI-LR module, which can be inserted into the model to estimate the\namount of information to explain the model's representation. Finally, we verify\nthe method through the visualization of the prototype network.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 12:26:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gu", "Lifeng", ""]]}, {"id": "2103.15136", "submitter": "Darshan Gera", "authors": "Darshan Gera and S. Balasubramanian", "title": "Imponderous Net for Facial Expression Recognition in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the renaissance of deep learning (DL), facial expression recognition\n(FER) has received a lot of interest, with continual improvement in the\nperformance. Hand-in-hand with performance, new challenges have come up. Modern\nFER systems deal with face images captured under uncontrolled conditions (also\ncalled in-the-wild scenario) including occlusions and pose variations. They\nsuccessfully handle such conditions using deep networks that come with various\ncomponents like transfer learning, attention mechanism and local-global context\nextractor. However, these deep networks are highly complex with large number of\nparameters, making them unfit to be deployed in real scenarios. Is it possible\nto build a light-weight network that can still show significantly good\nperformance on FER under in-the-wild scenario? In this work, we methodically\nbuild such a network and call it as Imponderous Net. We leverage on the\naforementioned components of deep networks for FER, and analyse, carefully\nchoose and fit them to arrive at Imponderous Net. Our Imponderous Net is a low\ncalorie net with only 1.45M parameters, which is almost 50x less than that of a\nstate-of-the-art (SOTA) architecture. Further, during inference, it can process\nat the real time rate of 40 frames per second (fps) in an intel-i7 cpu. Though\nit is low calorie, it is still power packed in its performance, overpowering\nother light-weight architectures and even few high capacity architectures.\nSpecifically, Imponderous Net reports 87.09\\%, 88.17\\% and 62.06\\% accuracies\non in-the-wild datasets RAFDB, FERPlus and AffectNet respectively. It also\nexhibits superior robustness under occlusions and pose variations in comparison\nto other light-weight architectures from the literature.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 13:47:34 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gera", "Darshan", ""], ["Balasubramanian", "S.", ""]]}, {"id": "2103.15144", "submitter": "Ben Wycliff Mugalu", "authors": "Ben Wycliff Mugalu, Rodrick Calvin Wamala, Jonathan Serugunda, Andrew\n  Katumba", "title": "Face Recognition as a Method of Authentication in a Web-Based System", "comments": "7 pages, 9 figures, National Conference on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online information systems currently heavily rely on the username and\npassword traditional method for protecting information and controlling access.\nWith the advancement in biometric technology and popularity of fields like AI\nand Machine Learning, biometric security is becoming increasingly popular\nbecause of the usability advantage. This paper reports how machine learning\nbased face recognition can be integrated into a web-based system as a method of\nauthentication to reap the benefits of improved usability. This paper includes\na comparison of combinations of detection and classification algorithms with\nFaceNet for face recognition. The results show that a combination of MTCNN for\ndetection, Facenet for generating embeddings, and LinearSVC for classification\noutperforms other combinations with a 95% accuracy. The resulting classifier is\nintegrated into the web-based system and used for authenticating users.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 14:49:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mugalu", "Ben Wycliff", ""], ["Wamala", "Rodrick Calvin", ""], ["Serugunda", "Jonathan", ""], ["Katumba", "Andrew", ""]]}, {"id": "2103.15145", "submitter": "Yihong Xu", "authors": "Yihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan, Daniela Rus,\n  Xavier Alameda-Pineda", "title": "TransCenter: Transformers with Dense Queries for Multiple-Object\n  Tracking", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transformer networks have proven extremely powerful for a wide variety of\ntasks since they were introduced. Computer vision is not an exception, as the\nuse of transformers has become very popular in the vision community in recent\nyears. Despite this wave, multiple-object tracking (MOT) exhibits for now some\nsort of incompatibility with transformers. We argue that the standard\nrepresentation -- bounding boxes -- is not adapted to learning transformers for\nMOT. Inspired by recent research, we propose TransCenter, the first\ntransformer-based architecture for tracking the centers of multiple targets.\nMethodologically, we propose the use of dense queries in a double-decoder\nnetwork, to be able to robustly infer the heatmap of targets' centers and\nassociate them through time. TransCenter outperforms the current\nstate-of-the-art in multiple-object tracking, both in MOT17 and MOT20. Our\nablation study demonstrates the advantage in the proposed architecture compared\nto more naive alternatives. The code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 14:49:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xu", "Yihong", ""], ["Ban", "Yutong", ""], ["Delorme", "Guillaume", ""], ["Gan", "Chuang", ""], ["Rus", "Daniela", ""], ["Alameda-Pineda", "Xavier", ""]]}, {"id": "2103.15149", "submitter": "Chia-Ni Lu", "authors": "Chia-Ni Lu, Ya-Chu Chang and Wei-Chen Chiu", "title": "Bridging the Visual Gap: Wide-Range Image Blending", "comments": "Accepted to CVPR 2021. Project page:\n  http://github.com/julia0607/Wide-Range-Image-Blending", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new problem scenario in image processing,\nwide-range image blending, which aims to smoothly merge two different input\nphotos into a panorama by generating novel image content for the intermediate\nregion between them. Although such problem is closely related to the topics of\nimage inpainting, image outpainting, and image blending, none of the approaches\nfrom these topics is able to easily address it. We introduce an effective\ndeep-learning model to realize wide-range image blending, where a novel\nBidirectional Content Transfer module is proposed to perform the conditional\nprediction for the feature representation of the intermediate region via\nrecurrent neural networks. In addition to ensuring the spatial and semantic\nconsistency during the blending, we also adopt the contextual attention\nmechanism as well as the adversarial learning scheme in our proposed method for\nimproving the visual quality of the resultant panorama. We experimentally\ndemonstrate that our proposed method is not only able to produce visually\nappealing results for wide-range image blending, but also able to provide\nsuperior performance with respect to several baselines built upon the\nstate-of-the-art image inpainting and outpainting approaches.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 15:07:45 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 08:37:31 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lu", "Chia-Ni", ""], ["Chang", "Ya-Chu", ""], ["Chiu", "Wei-Chen", ""]]}, {"id": "2103.15152", "submitter": "Jacob John", "authors": "Jacob John", "title": "Image Processing Techniques for identifying tumors in an MRI image", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical Resonance Imaging or MRI is a medical image processing technique that\nused radio waves to scan the body. It is a tomographic imaging technique,\nprincipally used in the field of radiology. With the advantage of being a\npainless diagnostic procedure, MRI allows medical personnel to illustrate clear\npictures of the anatomy and the physiological processes occurring in the body,\nthus allowing early detection and treatment of diseases. These images, combined\nwith image processing techniques may be used in the detection of tumors,\ndifficult to identify with the naked eye. This digital assignment surveys the\ndifferent image processing techniques used in Automated Tumor Detection (ATD).\nThis assignment initiates the discussion with a comparison of traditional\ntechniques such as Morphological Tools (MT) and Region Growing Technique (RGT).\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 15:18:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["John", "Jacob", ""]]}, {"id": "2103.15158", "submitter": "Gongjie Zhang", "authors": "Gongjie Zhang, Kaiwen Cui, Tzu-Yi Hung, Shijian Lu", "title": "Defect-GAN: High-Fidelity Defect Synthesis for Automated Defect\n  Inspection", "comments": "Codes will not be released due to confidentiality agreement.\n  Published on WACV 2021.\n  (https://openaccess.thecvf.com/content/WACV2021/papers/Zhang_Defect-GAN_High-Fidelity_Defect_Synthesis_for_Automated_Defect_Inspection_WACV_2021_paper.pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automated defect inspection is critical for effective and efficient\nmaintenance, repair, and operations in advanced manufacturing. On the other\nhand, automated defect inspection is often constrained by the lack of defect\nsamples, especially when we adopt deep neural networks for this task. This\npaper presents Defect-GAN, an automated defect synthesis network that generates\nrealistic and diverse defect samples for training accurate and robust defect\ninspection networks. Defect-GAN learns through defacement and restoration\nprocesses, where the defacement generates defects on normal surface images\nwhile the restoration removes defects to generate normal images. It employs a\nnovel compositional layer-based architecture for generating realistic defects\nwithin various image backgrounds with different textures and appearances. It\ncan also mimic the stochastic variations of defects and offer flexible control\nover the locations and categories of the generated defects within the image\nbackground. Extensive experiments show that Defect-GAN is capable of\nsynthesizing various defects with superior diversity and fidelity. In addition,\nthe synthesized defect samples demonstrate their effectiveness in training\nbetter defect inspection networks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 15:53:34 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Gongjie", ""], ["Cui", "Kaiwen", ""], ["Hung", "Tzu-Yi", ""], ["Lu", "Shijian", ""]]}, {"id": "2103.15208", "submitter": "Fujun Luan", "authors": "Fujun Luan, Shuang Zhao, Kavita Bala, Zhao Dong", "title": "Unified Shape and SVBRDF Recovery using Differentiable Monte Carlo\n  Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing the shape and appearance of real-world objects using measured\n2D images has been a long-standing problem in computer vision. In this paper,\nwe introduce a new analysis-by-synthesis technique capable of producing\nhigh-quality reconstructions through robust coarse-to-fine optimization and\nphysics-based differentiable rendering.\n  Unlike most previous methods that handle geometry and reflectance largely\nseparately, our method unifies the optimization of both by leveraging image\ngradients with respect to both object reflectance and geometry. To obtain\nphysically accurate gradient estimates, we develop a new GPU-based Monte Carlo\ndifferentiable renderer leveraging recent advances in differentiable rendering\ntheory to offer unbiased gradients while enjoying better performance than\nexisting tools like PyTorch3D and redner. To further improve robustness, we\nutilize several shape and material priors as well as a coarse-to-fine\noptimization strategy to reconstruct geometry. We demonstrate that our\ntechnique can produce reconstructions with higher quality than previous methods\nsuch as COLMAP and Kinect Fusion.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 19:44:05 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 19:21:22 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 03:10:06 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Luan", "Fujun", ""], ["Zhao", "Shuang", ""], ["Bala", "Kavita", ""], ["Dong", "Zhao", ""]]}, {"id": "2103.15226", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Gaurav Sharma", "title": "Exploiting Local Geometry for Feature and Graph Construction for Better\n  3D Point Cloud Processing with Graph Neural Networks", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose simple yet effective improvements in point representations and\nlocal neighborhood graph construction within the general framework of graph\nneural networks (GNNs) for 3D point cloud processing. As a first contribution,\nwe propose to augment the vertex representations with important local geometric\ninformation of the points, followed by nonlinear projection using a MLP. As a\nsecond contribution, we propose to improve the graph construction for GNNs for\n3D point clouds. The existing methods work with a k-nn based approach for\nconstructing the local neighborhood graph. We argue that it might lead to\nreduction in coverage in case of dense sampling by sensors in some regions of\nthe scene. The proposed methods aims to counter such problems and improve\ncoverage in such cases. As the traditional GNNs were designed to work with\ngeneral graphs, where vertices may have no geometric interpretations, we see\nboth our proposals as augmenting the general graphs to incorporate the\ngeometric nature of 3D point clouds. While being simple, we demonstrate with\nmultiple challenging benchmarks, with relatively clean CAD models, as well as\nwith real world noisy scans, that the proposed method achieves state of the art\nresults on benchmarks for 3D classification (ModelNet40) , part segmentation\n(ShapeNet) and semantic segmentation (Stanford 3D Indoor Scenes Dataset). We\nalso show that the proposed network achieves faster training convergence, i.e.\n~40% less epochs for classification. The project details are available at\nhttps://siddharthsrivastava.github.io/publication/geomgcnn/\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:34:59 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Sharma", "Gaurav", ""]]}, {"id": "2103.15231", "submitter": "Dominik Bauer", "authors": "Dominik Bauer, Timothy Patten and Markus Vincze", "title": "ReAgent: Point Cloud Registration using Imitation and Reinforcement\n  Learning", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud registration is a common step in many 3D computer vision tasks\nsuch as object pose estimation, where a 3D model is aligned to an observation.\nClassical registration methods generalize well to novel domains but fail when\ngiven a noisy observation or a bad initialization. Learning-based methods, in\ncontrast, are more robust but lack in generalization capacity. We propose to\nconsider iterative point cloud registration as a reinforcement learning task\nand, to this end, present a novel registration agent (ReAgent). We employ\nimitation learning to initialize its discrete registration policy based on a\nsteady expert policy. Integration with policy optimization, based on our\nproposed alignment reward, further improves the agent's registration\nperformance. We compare our approach to classical and learning-based\nregistration methods on both ModelNet40 (synthetic) and ScanObjectNN (real\ndata) and show that our ReAgent achieves state-of-the-art accuracy. The\nlightweight architecture of the agent, moreover, enables reduced inference time\nas compared to related approaches. In addition, we apply our method to the\nobject pose estimation task on real data (LINEMOD), outperforming\nstate-of-the-art pose refinement approaches.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 22:04:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bauer", "Dominik", ""], ["Patten", "Timothy", ""], ["Vincze", "Markus", ""]]}, {"id": "2103.15233", "submitter": "Mengmeng Xu", "authors": "Mengmeng Xu, Juan-Manuel Perez-Rua, Xiatian Zhu, Bernard Ghanem, Brais\n  Martinez", "title": "Low-Fidelity End-to-End Video Encoder Pre-training for Temporal Action\n  Localization", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization (TAL) is a fundamental yet challenging task in\nvideo understanding. Existing TAL methods rely on pre-training a video encoder\nthrough action classification supervision. This results in a task discrepancy\nproblem for the video encoder -- trained for action classification, but used\nfor TAL. Intuitively, end-to-end model optimization is a good solution.\nHowever, this is not operable for TAL subject to the GPU memory constraints,\ndue to the prohibitive computational cost in processing long untrimmed videos.\nIn this paper, we resolve this challenge by introducing a novel low-fidelity\nend-to-end (LoFi) video encoder pre-training method. Instead of always using\nthe full training configurations for TAL learning, we propose to reduce the\nmini-batch composition in terms of temporal, spatial or spatio-temporal\nresolution so that end-to-end optimization for the video encoder becomes\noperable under the memory conditions of a mid-range hardware budget. Crucially,\nthis enables the gradient to flow backward through the video encoder from a TAL\nloss supervision, favourably solving the task discrepancy problem and providing\nmore effective feature representations. Extensive experiments show that the\nproposed LoFi pre-training approach can significantly enhance the performance\nof existing TAL methods. Encouragingly, even with a lightweight ResNet18 based\nvideo encoder in a single RGB stream, our method surpasses two-stream ResNet50\nbased alternatives with expensive optical flow, often by a good margin.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 22:18:14 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 13:21:37 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Xu", "Mengmeng", ""], ["Perez-Rua", "Juan-Manuel", ""], ["Zhu", "Xiatian", ""], ["Ghanem", "Bernard", ""], ["Martinez", "Brais", ""]]}, {"id": "2103.15244", "submitter": "Zhengbo Luo", "authors": "Zhengbo Luo and Zitang Sun and Weilian Zhou and Zizhang Wu and\n  Sei-ichiro Kamata", "title": "Rethinking ResNets: Improved Stacking Strategies With High Order Schemes", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various deep neural network architectures (DNNs) maintain massive vital\nrecords in computer vision. While drawing attention worldwide, the design of\nthe overall structure lacks general guidance. Based on the relationship between\nDNN design and numerical differential equations, we performed a fair comparison\nof the residual design with higher-order perspectives. We show that the widely\nused DNN design strategy, constantly stacking a small design (usually 2-3\nlayers), could be easily improved, supported by solid theoretical knowledge and\nwith no extra parameters needed. We reorganise the residual design in\nhigher-order ways, which is inspired by the observation that many effective\nnetworks can be interpreted as different numerical discretisations of\ndifferential equations. The design of ResNet follows a relatively simple\nscheme, which is Euler forward; however, the situation becomes complicated\nrapidly while stacking. We suppose that stacked ResNet is somehow equalled to a\nhigher-order scheme; then, the current method of forwarding propagation might\nbe relatively weak compared with a typical high-order method such as\nRunge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV\nbenchmarks with sufficient experiments. Stable and noticeable increases in\nperformance are observed, and convergence and robustness are also improved. Our\nstacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per\ncent on CIFAR-10, with the same settings and parameters. The proposed strategy\nis fundamental and theoretical and can therefore be applied to any network as a\ngeneral guideline.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 23:29:57 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 12:31:21 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 01:38:13 GMT"}, {"version": "v4", "created": "Wed, 28 Jul 2021 06:16:32 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Luo", "Zhengbo", ""], ["Sun", "Zitang", ""], ["Zhou", "Weilian", ""], ["Wu", "Zizhang", ""], ["Kamata", "Sei-ichiro", ""]]}, {"id": "2103.15254", "submitter": "Chao Qu `", "authors": "Chao Qu, Wenxin Liu, Camillo J. Taylor", "title": "Bayesian Deep Basis Fitting for Depth Completion with Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we investigate the problem of uncertainty estimation for\nimage-guided depth completion. We extend Deep Basis Fitting (DBF) for depth\ncompletion within a Bayesian evidence framework to provide calibrated per-pixel\nvariance. The DBF approach frames the depth completion problem in terms of a\nnetwork that produces a set of low-dimensional depth bases and a differentiable\nleast squares fitting module that computes the basis weights using the sparse\ndepths. By adopting a Bayesian treatment, our Bayesian Deep Basis Fitting\n(BDBF) approach is able to 1) predict high-quality uncertainty estimates and 2)\nenable depth completion with few or no sparse measurements. We conduct\ncontrolled experiments to compare BDBF against commonly used techniques for\nuncertainty estimation under various scenarios. Results show that our method\nproduces better uncertainty estimates with accurate depth prediction.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 00:40:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Qu", "Chao", ""], ["Liu", "Wenxin", ""], ["Taylor", "Camillo J.", ""]]}, {"id": "2103.15263", "submitter": "Yuang Liu", "authors": "Yuang Liu, Wei Zhang, Jun Wang", "title": "Zero-shot Adversarial Quantization", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model quantization is a promising approach to compress deep neural networks\nand accelerate inference, making it possible to be deployed on mobile and edge\ndevices. To retain the high performance of full-precision models, most existing\nquantization methods focus on fine-tuning quantized model by assuming training\ndatasets are accessible. However, this assumption sometimes is not satisfied in\nreal situations due to data privacy and security issues, thereby making these\nquantization methods not applicable. To achieve zero-short model quantization\nwithout accessing training data, a tiny number of quantization methods adopt\neither post-training quantization or batch normalization statistics-guided data\ngeneration for fine-tuning. However, both of them inevitably suffer from low\nperformance, since the former is a little too empirical and lacks training\nsupport for ultra-low precision quantization, while the latter could not fully\nrestore the peculiarities of original data and is often low efficient for\ndiverse data generation. To address the above issues, we propose a zero-shot\nadversarial quantization (ZAQ) framework, facilitating effective discrepancy\nestimation and knowledge transfer from a full-precision model to its quantized\nmodel. This is achieved by a novel two-level discrepancy modeling to drive a\ngenerator to synthesize informative and diverse data examples to optimize the\nquantized model in an adversarial learning fashion. We conduct extensive\nexperiments on three fundamental vision tasks, demonstrating the superiority of\nZAQ over the strong zero-shot baselines and validating the effectiveness of its\nmain components. Code is available at <https://git.io/Jqc0y>.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 01:33:34 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 14:17:16 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Yuang", ""], ["Zhang", "Wei", ""], ["Wang", "Jun", ""]]}, {"id": "2103.15279", "submitter": "Shunkai Li", "authors": "Shunkai Li, Xin Wu, Yingdian Cao, Hongbin Zha", "title": "Generalizing to the Open World: Deep Visual Odometry with Online\n  Adaptation", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite learning-based visual odometry (VO) has shown impressive results in\nrecent years, the pretrained networks may easily collapse in unseen\nenvironments. The large domain gap between training and testing data makes them\ndifficult to generalize to new scenes. In this paper, we propose an online\nadaptation framework for deep VO with the assistance of scene-agnostic\ngeometric computations and Bayesian inference. In contrast to learning-based\npose estimation, our method solves pose from optical flow and depth while the\nsingle-view depth estimation is continuously improved with new observations by\nonline learned uncertainties. Meanwhile, an online learned photometric\nuncertainty is used for further depth and pose optimization by a differentiable\nGauss-Newton layer. Our method enables fast adaptation of deep VO networks to\nunseen environments in a self-supervised manner. Extensive experiments\nincluding Cityscapes to KITTI and outdoor KITTI to indoor TUM demonstrate that\nour method achieves state-of-the-art generalization ability among\nself-supervised VO methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:13:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Shunkai", ""], ["Wu", "Xin", ""], ["Cao", "Yingdian", ""], ["Zha", "Hongbin", ""]]}, {"id": "2103.15290", "submitter": "Yuanfei Huang", "authors": "Yuanfei Huang, Jie Li, Yanting Hu, Xinbo Gao, Wen Lu", "title": "Transitive Learning: Exploring the Transitivity of Degradations for\n  Blind Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being extremely dependent on the iterative estimation and correction of data\nor models, the existing blind super-resolution (SR) methods are generally\ntime-consuming and less effective. To address it, this paper proposes a\ntransitive learning method for blind SR using an end-to-end network without any\nadditional iterations in inference. To begin with, we analyze and demonstrate\nthe transitivity of degradations, including the widely used additive and\nconvolutive degradations. We then propose a novel Transitive Learning method\nfor blind Super-Resolution on transitive degradations (TLSR), by adaptively\ninferring a transitive transformation function to solve the unknown\ndegradations without any iterative operations in inference. Specifically, the\nend-to-end TLSR network consists of a degree of transitivity (DoT) estimation\nnetwork, a homogeneous feature extraction network, and a transitive learning\nmodule. Quantitative and qualitative evaluations on blind SR tasks demonstrate\nthat the proposed TLSR achieves superior performance and consumes less time\nagainst the state-of-the-art blind SR methods. The code is available at\nhttps://github.com/YuanfeiHuang/TLSR.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:51:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Huang", "Yuanfei", ""], ["Li", "Jie", ""], ["Hu", "Yanting", ""], ["Gao", "Xinbo", ""], ["Lu", "Wen", ""]]}, {"id": "2103.15293", "submitter": "Minghan Zhu", "authors": "Minghan Zhu, Songan Zhang, Yuanxin Zhong, Pingping Lu, Huei Peng and\n  John Lenneman", "title": "Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras\n  through Homography", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a method to extract the position and pose of vehicles in\nthe 3D world from a single traffic camera. Most previous monocular 3D vehicle\ndetection algorithms focused on cameras on vehicles from the perspective of a\ndriver, and assumed known intrinsic and extrinsic calibration. On the contrary,\nthis paper focuses on the same task using uncalibrated monocular traffic\ncameras. We observe that the homography between the road plane and the image\nplane is essential to 3D vehicle detection and the data synthesis for this\ntask, and the homography can be estimated without the camera intrinsics and\nextrinsics. We conduct 3D vehicle detection by estimating the rotated bounding\nboxes (r-boxes) in the bird's eye view (BEV) images generated from inverse\nperspective mapping. We propose a new regression target called\n\\textit{tailed~r-box} and a \\textit{dual-view} network architecture which\nboosts the detection accuracy on warped BEV images. Experiments show that the\nproposed method can generalize to new camera and environment setups despite not\nseeing imaged from them during training.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:57:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhu", "Minghan", ""], ["Zhang", "Songan", ""], ["Zhong", "Yuanxin", ""], ["Lu", "Pingping", ""], ["Peng", "Huei", ""], ["Lenneman", "John", ""]]}, {"id": "2103.15295", "submitter": "Wenbo Li", "authors": "Wenbo Li, Kun Zhou, Lu Qi, Liying Lu, Nianjuan Jiang, Jiangbo Lu,\n  Jiaya Jia", "title": "Best-Buddy GANs for Highly Detailed Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the single image super-resolution (SISR) problem, where a\nhigh-resolution (HR) image is generated based on a low-resolution (LR) input.\nRecently, generative adversarial networks (GANs) become popular to hallucinate\ndetails. Most methods along this line rely on a predefined single-LR-single-HR\nmapping, which is not flexible enough for the SISR task. Also, GAN-generated\nfake details may often undermine the realism of the whole image. We address\nthese issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR.\nRelaxing the immutable one-to-one constraint, we allow the estimated patches to\ndynamically seek the best supervision during training, which is beneficial to\nproducing more reasonable details. Besides, we propose a region-aware\nadversarial learning strategy that directs our model to focus on generating\ndetails for textured areas adaptively. Extensive experiments justify the\neffectiveness of our method. An ultra-high-resolution 4K dataset is also\nconstructed to facilitate future super-resolution research.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:58:27 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Wenbo", ""], ["Zhou", "Kun", ""], ["Qi", "Lu", ""], ["Lu", "Liying", ""], ["Jiang", "Nianjuan", ""], ["Lu", "Jiangbo", ""], ["Jia", "Jiaya", ""]]}, {"id": "2103.15296", "submitter": "Sungwon Han", "authors": "Sungwon Han, Hyeonho Song, Seungeon Lee, Sungwon Park and Meeyoung Cha", "title": "Elsa: Energy-based learning for semi-supervised anomaly detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection aims at identifying deviant instances from the normal data\ndistribution. Many advances have been made in the field, including the\ninnovative use of unsupervised contrastive learning. However, existing methods\ngenerally assume clean training data and are limited when the data contain\nunknown anomalies. This paper presents Elsa, a novel semi-supervised anomaly\ndetection approach that unifies the concept of energy-based models with\nunsupervised contrastive learning. Elsa instills robustness against any data\ncontamination by a carefully designed fine-tuning step based on the new energy\nfunction that forces the normal data to be divided into classes of prototypes.\nExperiments on multiple contamination scenarios show the proposed model\nachieves SOTA performance. Extensive analyses also verify the contribution of\neach component in the proposed model. Beyond the experiments, we also offer a\ntheoretical interpretation of why contrastive learning alone cannot detect\nanomalies under data contamination.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 03:01:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Han", "Sungwon", ""], ["Song", "Hyeonho", ""], ["Lee", "Seungeon", ""], ["Park", "Sungwon", ""], ["Cha", "Meeyoung", ""]]}, {"id": "2103.15297", "submitter": "Zhichao Li", "authors": "Zhichao Li, Feng Wang, Naiyan Wang", "title": "LiDAR R-CNN: An Efficient and Universal 3D Object Detector", "comments": "CVPR 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR-based 3D detection in point cloud is essential in the perception system\nof autonomous driving. In this paper, we present LiDAR R-CNN, a second stage\ndetector that can generally improve any existing 3D detector. To fulfill the\nreal-time and high precision requirement in practice, we resort to point-based\napproach other than the popular voxel-based approach. However, we find an\noverlooked issue in previous work: Naively applying point-based methods like\nPointNet could make the learned features ignore the size of proposals. To this\nend, we analyze this problem in detail and propose several methods to remedy\nit, which bring significant performance improvement. Comprehensive experimental\nresults on real-world datasets like Waymo Open Dataset (WOD) and KITTI dataset\nwith various popular detectors demonstrate the universality and superiority of\nour LiDAR R-CNN. In particular, based on one variant of PointPillars, our\nmethod could achieve new state-of-the-art results with minor cost. Codes will\nbe released at https://github.com/tusimple/LiDAR_RCNN .\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 03:01:21 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Zhichao", ""], ["Wang", "Feng", ""], ["Wang", "Naiyan", ""]]}, {"id": "2103.15306", "submitter": "Dailan He", "authors": "Dailan He, Yaoyan Zheng, Baocheng Sun, Yan Wang, Hongwei Qin", "title": "Checkerboard Context Model for Efficient Learned Image Compression", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For learned image compression, the autoregressive context model is proved\neffective in improving the rate-distortion (RD) performance. Because it helps\nremove spatial redundancies among latent representations. However, the decoding\nprocess must be done in a strict scan order, which breaks the parallelization.\nWe propose a parallelizable checkerboard context model (CCM) to solve the\nproblem. Our two-pass checkerboard context calculation eliminates such\nlimitations on spatial locations by re-organizing the decoding order. Speeding\nup the decoding process more than 40 times in our experiments, it achieves\nsignificantly improved computational efficiency with almost the same\nrate-distortion performance. To the best of our knowledge, this is the first\nexploration on parallelization-friendly spatial context model for learned image\ncompression.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 03:25:41 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 08:33:32 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["He", "Dailan", ""], ["Zheng", "Yaoyan", ""], ["Sun", "Baocheng", ""], ["Wang", "Yan", ""], ["Qin", "Hongwei", ""]]}, {"id": "2103.15307", "submitter": "Dingwen Zhang", "authors": "Dingwen Zhang, Bo Wang, Gerong Wang, Qiang Zhang, Jiajia Zhang,\n  Jungong Han, Zheng You", "title": "Onfocus Detection: Identifying Individual-Camera Eye Contact from\n  Unconstrained Images", "comments": null, "journal-ref": "SCIENCE CHINA Information Sciences, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Onfocus detection aims at identifying whether the focus of the individual\ncaptured by a camera is on the camera or not. Based on the behavioral research,\nthe focus of an individual during face-to-camera communication leads to a\nspecial type of eye contact, i.e., the individual-camera eye contact, which is\na powerful signal in social communication and plays a crucial role in\nrecognizing irregular individual status (e.g., lying or suffering mental\ndisease) and special purposes (e.g., seeking help or attracting fans). Thus,\ndeveloping effective onfocus detection algorithms is of significance for\nassisting the criminal investigation, disease discovery, and social behavior\nanalysis. However, the review of the literature shows that very few efforts\nhave been made toward the development of onfocus detector due to the lack of\nlarge-scale public available datasets as well as the challenging nature of this\ntask. To this end, this paper engages in the onfocus detection research by\naddressing the above two issues. Firstly, we build a large-scale onfocus\ndetection dataset, named as the OnFocus Detection In the Wild (OFDIW). It\nconsists of 20,623 images in unconstrained capture conditions (thus called ``in\nthe wild'') and contains individuals with diverse emotions, ages, facial\ncharacteristics, and rich interactions with surrounding objects and background\nscenes. On top of that, we propose a novel end-to-end deep model, i.e., the\neye-context interaction inferring network (ECIIN), for onfocus detection, which\nexplores eye-context interaction via dynamic capsule routing. Finally,\ncomprehensive experiments are conducted on the proposed OFDIW dataset to\nbenchmark the existing learning models and demonstrate the effectiveness of the\nproposed ECIIN. The project (containing both datasets and codes) is at\nhttps://github.com/wintercho/focus.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 03:29:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Dingwen", ""], ["Wang", "Bo", ""], ["Wang", "Gerong", ""], ["Zhang", "Qiang", ""], ["Zhang", "Jiajia", ""], ["Han", "Jungong", ""], ["You", "Zheng", ""]]}, {"id": "2103.15320", "submitter": "Chunhua Shen", "authors": "Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin\n  Wang", "title": "TFPose: Direct Human Pose Estimation with Transformers", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a human pose estimation framework that solves the task in the\nregression-based fashion. Unlike previous regression-based methods, which often\nfall behind those state-of-the-art methods, we formulate the pose estimation\ntask into a sequence prediction problem that can effectively be solved by\ntransformers. Our framework is simple and direct, bypassing the drawbacks of\nthe heatmap-based pose estimation. Moreover, with the attention mechanism in\ntransformers, our proposed framework is able to adaptively attend to the\nfeatures most relevant to the target keypoints, which largely overcomes the\nfeature misalignment issue of previous regression-based methods and\nconsiderably improves the performance. Importantly, our framework can\ninherently take advantages of the structured relationship between keypoints.\nExperiments on the MS-COCO and MPII datasets demonstrate that our method can\nsignificantly improve the state-of-the-art of regression-based pose estimation\nand perform comparably with the best heatmap-based pose estimation methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 04:18:54 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mao", "Weian", ""], ["Ge", "Yongtao", ""], ["Shen", "Chunhua", ""], ["Tian", "Zhi", ""], ["Wang", "Xinlong", ""], ["Wang", "Zhibin", ""]]}, {"id": "2103.15323", "submitter": "Jiayi Ye", "authors": "Yilin Wang and Jiayi Ye", "title": "Classifying Video based on Automatic Content Detection Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video classification and analysis is always a popular and challenging field\nin computer vision. It is more than just simple image classification due to the\ncorrelation with respect to the semantic contents of subsequent frames brings\ndifficulties for video analysis. In this literature review, we summarized some\nstate-of-the-art methods for multi-label video classification. Our goal is\nfirst to experimentally research the current widely used architectures, and\nthen to develop a method to deal with the sequential data of frames and perform\nmulti-label classification based on automatic content detection of video.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 04:31:45 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Yilin", ""], ["Ye", "Jiayi", ""]]}, {"id": "2103.15326", "submitter": "Yiming Li", "authors": "Yiming Li and Congcong Wen and Felix Juefei-Xu and Chen Feng", "title": "Fooling LiDAR Perception via Adversarial Trajectory Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR point clouds collected from a moving vehicle are functions of its\ntrajectories, because the sensor motion needs to be compensated to avoid\ndistortions. When autonomous vehicles are sending LiDAR point clouds to deep\nnetworks for perception and planning, could the motion compensation\nconsequently become a wide-open backdoor in those networks, due to both the\nadversarial vulnerability of deep learning and GPS-based vehicle trajectory\nestimation that is susceptible to wireless spoofing? We demonstrate such\npossibilities for the first time: instead of directly attacking point cloud\ncoordinates which requires tampering with the raw LiDAR readings, only\nadversarial spoofing of a self-driving car's trajectory with small\nperturbations is enough to make safety-critical objects undetectable or\ndetected with incorrect positions. Moreover, polynomial trajectory perturbation\nis developed to achieve a temporally-smooth and highly-imperceptible attack.\nExtensive experiments on 3D object detection have shown that such attacks not\nonly lower the performance of the state-of-the-art detectors effectively, but\nalso transfer to other detectors, raising a red flag for the community. The\ncode is available on https://ai4ce.github.io/FLAT/.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 04:34:31 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Yiming", ""], ["Wen", "Congcong", ""], ["Juefei-Xu", "Felix", ""], ["Feng", "Chen", ""]]}, {"id": "2103.15331", "submitter": "Zhe Li", "authors": "Zhe Li, Tao Yu, Zerong Zheng, Kaiwen Guo, Yebin Liu", "title": "POSEFusion: Pose-guided Selective Fusion for Single-view Human\n  Volumetric Capture", "comments": "CVPR 2021 (Oral presentation), for more information, please refer to\n  the projectpage http://www.liuyebin.com/posefusion/posefusion.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose POse-guided SElective Fusion (POSEFusion), a single-view human\nvolumetric capture method that leverages tracking-based methods and\ntracking-free inference to achieve high-fidelity and dynamic 3D reconstruction.\nBy contributing a novel reconstruction framework which contains pose-guided\nkeyframe selection and robust implicit surface fusion, our method fully\nutilizes the advantages of both tracking-based methods and tracking-free\ninference methods, and finally enables the high-fidelity reconstruction of\ndynamic surface details even in the invisible regions. We formulate the\nkeyframe selection as a dynamic programming problem to guarantee the temporal\ncontinuity of the reconstructed sequence. Moreover, the novel robust implicit\nsurface fusion involves an adaptive blending weight to preserve high-fidelity\nsurface details and an automatic collision handling method to deal with the\npotential self-collisions. Overall, our method enables high-fidelity and\ndynamic capture in both visible and invisible regions from a single RGBD\ncamera, and the results and experiments show that our method outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 04:56:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Zhe", ""], ["Yu", "Tao", ""], ["Zheng", "Zerong", ""], ["Guo", "Kaiwen", ""], ["Liu", "Yebin", ""]]}, {"id": "2103.15345", "submitter": "Zhao Zhong", "authors": "Yucong Zhou, Yunxiao Sun, Zhao Zhong", "title": "FixNorm: Dissecting Weight Decay for Training Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight decay is a widely used technique for training Deep Neural\nNetworks(DNN). It greatly affects generalization performance but the underlying\nmechanisms are not fully understood. Recent works show that for layers followed\nby normalizations, weight decay mainly affects the effective learning rate.\nHowever, despite normalizations have been extensively adopted in modern DNNs,\nlayers such as the final fully-connected layer do not satisfy this\nprecondition. For these layers, the effects of weight decay are still unclear.\nIn this paper, we comprehensively investigate the mechanisms of weight decay\nand find that except for influencing effective learning rate, weight decay has\nanother distinct mechanism that is equally important: affecting generalization\nperformance by controlling cross-boundary risk. These two mechanisms together\ngive a more comprehensive explanation for the effects of weight decay. Based on\nthis discovery, we propose a new training method called FixNorm, which discards\nweight decay and directly controls the two mechanisms. We also propose a simple\nyet effective method to tune hyperparameters of FixNorm, which can find\nnear-optimal solutions in a few trials. On ImageNet classification task,\ntraining EfficientNet-B0 with FixNorm achieves 77.7%, which outperforms the\noriginal baseline by a clear margin. Surprisingly, when scaling MobileNetV2 to\nthe same FLOPS and applying the same tricks with EfficientNet-B0, training with\nFixNorm achieves 77.4%, which is only 0.3% lower. A series of SOTA results show\nthe importance of well-tuned training procedures, and further verify the\neffectiveness of our approach. We set up more well-tuned baselines using\nFixNorm, to facilitate fair comparisons in the community.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:41:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Yucong", ""], ["Sun", "Yunxiao", ""], ["Zhong", "Zhao", ""]]}, {"id": "2103.15346", "submitter": "Nianjin Ye", "authors": "Nianjin Ye, Chuan Wang, Haoqiang Fan, Shuaicheng Liu", "title": "Motion Basis Learning for Unsupervised Deep Homography Estimation with\n  Subspace Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new framework for unsupervised deep homography\nestimation. Our contributions are 3 folds. First, unlike previous methods that\nregress 4 offsets for a homography, we propose a homography flow\nrepresentation, which can be estimated by a weighted sum of 8 pre-defined\nhomography flow bases. Second, considering a homography contains 8\nDegree-of-Freedoms (DOFs) that is much less than the rank of the network\nfeatures, we propose a Low Rank Representation (LRR) block that reduces the\nfeature rank, so that features corresponding to the dominant motions are\nretained while others are rejected. Last, we propose a Feature Identity Loss\n(FIL) to enforce the learned image feature warp-equivariant, meaning that the\nresult should be identical if the order of warp operation and feature\nextraction is swapped. With this constraint, the unsupervised optimization is\nachieved more effectively and more stable features are learned. Extensive\nexperiments are conducted to demonstrate the effectiveness of all the newly\nproposed components, and results show our approach outperforms the\nstate-of-the-art on the homography benchmark datasets both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:51:34 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ye", "Nianjin", ""], ["Wang", "Chuan", ""], ["Fan", "Haoqiang", ""], ["Liu", "Shuaicheng", ""]]}, {"id": "2103.15348", "submitter": "Zejiang Shen", "authors": "Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain\n  Lee, Jacob Carlson, Weining Li", "title": "LayoutParser: A Unified Toolkit for Deep Learning Based Document Image\n  Analysis", "comments": "Accepted at ICDAR 2021, 16 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in document image analysis (DIA) have been primarily driven\nby the application of neural networks. Ideally, research outcomes could be\neasily deployed in production and extended for further investigation. However,\nvarious factors like loosely organized codebases and sophisticated model\nconfigurations complicate the easy reuse of important innovations by a wide\naudience. Though there have been on-going efforts to improve reusability and\nsimplify deep learning (DL) model development in disciplines like natural\nlanguage processing and computer vision, none of them are optimized for\nchallenges in the domain of DIA. This represents a major gap in the existing\ntoolkit, as DIA is central to academic research across a wide range of\ndisciplines in the social sciences and humanities. This paper introduces\nlayoutparser, an open-source library for streamlining the usage of DL in DIA\nresearch and applications. The core layoutparser library comes with a set of\nsimple and intuitive interfaces for applying and customizing DL models for\nlayout detection, character recognition, and many other document processing\ntasks. To promote extensibility, layoutparser also incorporates a community\nplatform for sharing both pre-trained models and full document digitization\npipelines. We demonstrate that layoutparser is helpful for both lightweight and\nlarge-scale digitization pipelines in real-word use cases. The library is\npublicly available at https://layout-parser.github.io/.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:55:08 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:24:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shen", "Zejiang", ""], ["Zhang", "Ruochen", ""], ["Dell", "Melissa", ""], ["Lee", "Benjamin Charles Germain", ""], ["Carlson", "Jacob", ""], ["Li", "Weining", ""]]}, {"id": "2103.15349", "submitter": "Dorian Tsai", "authors": "Dorian Tsai and Peter Corke and Thierry Peynot and Donald G. Dansereau", "title": "Refractive Light-Field Features for Curved Transparent Objects in\n  Structure from Motion", "comments": "submitted to IROS-RAL 2021. 8 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curved refractive objects are common in the human environment, and have a\ncomplex visual appearance that can cause robotic vision algorithms to fail.\nLight-field cameras allow us to address this challenge by capturing the\nview-dependent appearance of such objects in a single exposure. We propose a\nnovel image feature for light fields that detects and describes the patterns of\nlight refracted through curved transparent objects. We derive characteristic\npoints based on these features allowing them to be used in place of\nconventional 2D features. Using our features, we demonstrate improved\nstructure-from-motion performance in challenging scenes containing refractive\nobjects, including quantitative evaluations that show improved camera pose\nestimates and 3D reconstructions. Additionally, our methods converge 15-35%\nmore frequently than the state-of-the-art. Our method is a critical step\ntowards allowing robots to operate around refractive objects, with applications\nin manufacturing, quality assurance, pick-and-place, and domestic robots\nworking with acrylic, glass and other transparent materials.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:55:32 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 21:19:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tsai", "Dorian", ""], ["Corke", "Peter", ""], ["Peynot", "Thierry", ""], ["Dansereau", "Donald G.", ""]]}, {"id": "2103.15358", "submitter": "Pengchuan Zhang", "authors": "Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei\n  Zhang, Jianfeng Gao", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for\n  High-Resolution Image Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Vision Transformer (ViT) architecture Multi-Scale\nVision Longformer, which significantly enhances the ViT of\n\\cite{dosovitskiy2020image} for encoding high-resolution images using two\ntechniques. The first is the multi-scale model structure, which provides image\nencodings at multiple scales with manageable computational cost. The second is\nthe attention mechanism of vision Longformer, which is a variant of Longformer\n\\cite{beltagy2020longformer}, originally developed for natural language\nprocessing, and achieves a linear complexity w.r.t. the number of input tokens.\nA comprehensive empirical study shows that the new ViT significantly\noutperforms several strong baselines, including the existing ViT models and\ntheir ResNet counterparts, and the Pyramid Vision Transformer from a concurrent\nwork \\cite{wang2021pyramid}, on a range of vision tasks, including image\nclassification, object detection, and segmentation. The models and source code\nare released at \\url{https://github.com/microsoft/vision-longformer}.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:23:20 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 09:02:00 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhang", "Pengchuan", ""], ["Dai", "Xiyang", ""], ["Yang", "Jianwei", ""], ["Xiao", "Bin", ""], ["Yuan", "Lu", ""], ["Zhang", "Lei", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2103.15365", "submitter": "Yuan Yao", "authors": "Yuan Yao, Ao Zhang, Xu Han, Mengdi Li, Cornelius Weber, Zhiyuan Liu,\n  Stefan Wermter, Maosong Sun", "title": "Visual Distant Supervision for Scene Graph Generation", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph generation aims to identify objects and their relations in\nimages, providing structured image representations that can facilitate numerous\napplications in computer vision. However, scene graph models usually require\nsupervised learning on large quantities of labeled data with intensive human\nannotation. In this work, we propose visual distant supervision, a novel\nparadigm of visual relation learning, which can train scene graph models\nwithout any human-labeled data. The intuition is that by aligning commonsense\nknowledge bases and images, we can automatically create large-scale labeled\ndata to provide distant supervision for visual relation learning. To alleviate\nthe noise in distantly labeled data, we further propose a framework that\niteratively estimates the probabilistic relation labels and eliminates the\nnoisy ones. Comprehensive experimental results show that our distantly\nsupervised model outperforms strong weakly supervised and semi-supervised\nbaselines. By further incorporating human-labeled data in a semi-supervised\nfashion, our model outperforms state-of-the-art fully supervised models by a\nlarge margin (e.g., 8.6 micro- and 7.6 macro-recall@50 improvements for\npredicate classification in Visual Genome evaluation). All the data and code\nwill be available to facilitate future research.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:35:24 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yao", "Yuan", ""], ["Zhang", "Ao", ""], ["Han", "Xu", ""], ["Li", "Mengdi", ""], ["Weber", "Cornelius", ""], ["Liu", "Zhiyuan", ""], ["Wermter", "Stefan", ""], ["Sun", "Maosong", ""]]}, {"id": "2103.15368", "submitter": "Xi Zhang", "authors": "Xi Zhang and Xiaolin Wu", "title": "Attention-guided Image Compression by Deep Reconstruction of Compressive\n  Sensed Saliency Skeleton", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning system for attention-guided dual-layer image\ncompression (AGDL). In the AGDL compression system, an image is encoded into\ntwo layers, a base layer and an attention-guided refinement layer. Unlike the\nexisting ROI image compression methods that spend an extra bit budget equally\non all pixels in ROI, AGDL employs a CNN module to predict those pixels on and\nnear a saliency sketch within ROI that are critical to perceptual quality. Only\nthe critical pixels are further sampled by compressive sensing (CS) to form a\nvery compact refinement layer. Another novel CNN method is developed to jointly\ndecode the two compression layers for a much refined reconstruction, while\nstrictly satisfying the transmitted CS constraints on perceptually critical\npixels. Extensive experiments demonstrate that the proposed AGDL system\nadvances the state of the art in perception-aware image compression.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:43:59 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Xi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2103.15369", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Flaviano Christian Reyes, Ritika Shrivastava,\n  Oladapo Afolabi, Luisa Caldas, Allen Y. Yang", "title": "Contextual Scene Augmentation and Synthesis via GSACNet", "comments": "arXiv admin note: text overlap with arXiv:2009.12395 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor scene augmentation has become an emerging topic in the field of\ncomputer vision and graphics with applications in augmented and virtual\nreality. However, current state-of-the-art systems using deep neural networks\nrequire large datasets for training. In this paper we introduce GSACNet, a\ncontextual scene augmentation system that can be trained with limited scene\npriors. GSACNet utilizes a novel parametric data augmentation method combined\nwith a Graph Attention and Siamese network architecture followed by an\nAutoencoder network to facilitate training with small datasets. We show the\neffectiveness of our proposed system by conducting ablation and comparative\nstudies with alternative systems on the Matterport3D dataset. Our results\nindicate that our scene augmentation outperforms prior art in scene synthesis\nwith limited scene priors available.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:47:01 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Reyes", "Flaviano Christian", ""], ["Shrivastava", "Ritika", ""], ["Afolabi", "Oladapo", ""], ["Caldas", "Luisa", ""], ["Yang", "Allen Y.", ""]]}, {"id": "2103.15375", "submitter": "Shashanka Venkataramanan", "authors": "Shashanka Venkataramanan, Yannis Avrithis, Ewa Kijak, Laurent Amsaleg", "title": "AlignMix: Improving representation by interpolating aligned features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixup is a powerful data augmentation method that interpolates between two or\nmore examples in the input or feature space and between the corresponding\ntarget labels. Many recent mixup methods focus on cutting and pasting two or\nmore objects into one image, which is more about efficient processing than\ninterpolation. However, how to best interpolate images is not well defined. In\nthis sense, mixup has been connected to autoencoders, because often\nautoencoders \"interpolate well\", for instance generating an image that\ncontinuously deforms into another.\n  In this work, we revisit mixup from the interpolation perspective and\nintroduce AlignMix, where we geometrically align two images in the feature\nspace. The correspondences allow us to interpolate between two sets of\nfeatures, while keeping the locations of one set. Interestingly, this gives\nrise to a situation where mixup retains mostly the geometry or pose of one\nimage and the texture of the other, connecting it to style transfer. More than\nthat, we show that an autoencoder can still improve representation learning\nunder mixup, without the classifier ever seeing decoded images. AlignMix\noutperforms state-of-the-art mixup methods on five different benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:03:18 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Venkataramanan", "Shashanka", ""], ["Avrithis", "Yannis", ""], ["Kijak", "Ewa", ""], ["Amsaleg", "Laurent", ""]]}, {"id": "2103.15383", "submitter": "Xuan Cheng", "authors": "Xuan Cheng, Tianshu Xie, Xiaomin Wang, Qifeng Weng, Minghui Liu, Jiali\n  Deng, Ming Liu", "title": "Selective Output Smoothing Regularization: Regularize Neural Networks by\n  Softening Output Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Selective Output Smoothing Regularization, a novel\nregularization method for training the Convolutional Neural Networks (CNNs).\nInspired by the diverse effects on training from different samples, Selective\nOutput Smoothing Regularization improves the performance by encouraging the\nmodel to produce equal logits on incorrect classes when dealing with samples\nthat the model classifies correctly and over-confidently. This plug-and-play\nregularization method can be conveniently incorporated into almost any\nCNN-based project without extra hassle. Extensive experiments have shown that\nSelective Output Smoothing Regularization consistently achieves significant\nimprovement in image classification benchmarks, such as CIFAR-100, Tiny\nImageNet, ImageNet, and CUB-200-2011. Particularly, our method obtains\n77.30$\\%$ accuracy on ImageNet with ResNet-50, which gains 1.1$\\%$ than\nbaseline (76.2$\\%$). We also empirically demonstrate the ability of our method\nto make further improvements when combining with other widely used\nregularization techniques. On Pascal detection, using the SOSR-trained ImageNet\nclassifier as the pretrained model leads to better detection performances.\nMoreover, we demonstrate the effectiveness of our method in small sample size\nproblem and imbalanced dataset problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:21:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cheng", "Xuan", ""], ["Xie", "Tianshu", ""], ["Wang", "Xiaomin", ""], ["Weng", "Qifeng", ""], ["Liu", "Minghui", ""], ["Deng", "Jiali", ""], ["Liu", "Ming", ""]]}, {"id": "2103.15385", "submitter": "Mohammad Hossein Rohban", "authors": "Mohammad Azizmalayeri, Mohammad Hossein Rohban", "title": "Lagrangian Objective Function Leads to Improved Unforeseen Attack\n  Generalization in Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent improvements in deep learning models and their practical applications\nhave raised concerns about the robustness of these models against adversarial\nexamples. Adversarial training (AT) has been shown effective to reach a robust\nmodel against the attack that is used during training. However, it usually\nfails against other attacks, i.e. the model overfits to the training attack\nscheme. In this paper, we propose a simple modification to the AT that\nmitigates the mentioned issue. More specifically, we minimize the perturbation\n$\\ell_p$ norm while maximizing the classification loss in the Lagrangian form.\nWe argue that crafting adversarial examples based on this scheme results in\nenhanced attack generalization in the learned model. We compare our final model\nrobust accuracy against attacks that were not used during training to closely\nrelated state-of-the-art AT methods. This comparison demonstrates that our\naverage robust accuracy against unseen attacks is 5.9% higher in the CIFAR-10\ndataset and is 3.2% higher in the ImageNet-100 dataset than corresponding\nstate-of-the-art methods. We also demonstrate that our attack is faster than\nother attack schemes that are designed for unseen attack generalization, and\nconclude that it is feasible for large-scale datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:23:46 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Azizmalayeri", "Mohammad", ""], ["Rohban", "Mohammad Hossein", ""]]}, {"id": "2103.15395", "submitter": "Xin Liu", "authors": "Xin Liu, Silvia L. Pintea, Fatemeh Karimi Nejadasl, Olaf Booij, Jan C.\n  van Gemert", "title": "No frame left behind: Full Video Action Recognition", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Not all video frames are equally informative for recognizing an action. It is\ncomputationally infeasible to train deep networks on all video frames when\nactions develop over hundreds of frames. A common heuristic is uniformly\nsampling a small number of video frames and using these to recognize the\naction. Instead, here we propose full video action recognition and consider all\nvideo frames. To make this computational tractable, we first cluster all frame\nactivations along the temporal dimension based on their similarity with respect\nto the classification task, and then temporally aggregate the frames in the\nclusters into a smaller number of representations. Our method is end-to-end\ntrainable and computationally efficient as it relies on temporally localized\nclustering in combination with fast Hamming distances in feature space. We\nevaluate on UCF101, HMDB51, Breakfast, and Something-Something V1 and V2, where\nwe compare favorably to existing heuristic frame sampling methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:44:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Xin", ""], ["Pintea", "Silvia L.", ""], ["Nejadasl", "Fatemeh Karimi", ""], ["Booij", "Olaf", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2103.15396", "submitter": "Ziyu Li", "authors": "Ziyu Li, Yuncong Yao, Zhibin Quan, Wankou Yang, Jin Xie", "title": "SIENet: Spatial Information Enhancement Network for 3D Object Detection\n  from Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR-based 3D object detection pushes forward an immense influence on\nautonomous vehicles. Due to the limitation of the intrinsic properties of\nLiDAR, fewer points are collected at the objects farther away from the sensor.\nThis imbalanced density of point clouds degrades the detection accuracy but is\ngenerally neglected by previous works. To address the challenge, we propose a\nnovel two-stage 3D object detection framework, named SIENet. Specifically, we\ndesign the Spatial Information Enhancement (SIE) module to predict the spatial\nshapes of the foreground points within proposals, and extract the structure\ninformation to learn the representative features for further box refinement.\nThe predicted spatial shapes are complete and dense point sets, thus the\nextracted structure information contains more semantic representation. Besides,\nwe design the Hybrid-Paradigm Region Proposal Network (HP-RPN) which includes\nmultiple branches to learn discriminate features and generate accurate\nproposals for the SIE module. Extensive experiments on the KITTI 3D object\ndetection benchmark show that our elaborately designed SIENet outperforms the\nstate-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:45:09 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 02:16:52 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Li", "Ziyu", ""], ["Yao", "Yuncong", ""], ["Quan", "Zhibin", ""], ["Yang", "Wankou", ""], ["Xie", "Jin", ""]]}, {"id": "2103.15402", "submitter": "Lihe Yang", "authors": "Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, Yang Gao", "title": "Mining Latent Classes for Few-shot Segmentation", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation (FSS) aims to segment unseen classes given only a few\nannotated samples. Existing methods suffer the problem of feature undermining,\ni.e. potential novel classes are treated as background during training phase.\nOur method aims to alleviate this problem and enhance the feature embedding on\nlatent novel classes. In our work, we propose a novel joint-training framework.\nBased on conventional episodic training on support-query pairs, we add an\nadditional mining branch that exploits latent novel classes via transferable\nsub-clusters, and a new rectification technique on both background and\nforeground categories to enforce more stable prototypes. Over and above that,\nour transferable sub-cluster has the ability to leverage extra unlabeled data\nfor further feature enhancement. Extensive experiments on two FSS benchmarks\ndemonstrate that our method outperforms previous state-of-the-art by a large\nmargin of 3.7% mIOU on PASCAL-5i and 7.0% mIOU on COCO-20i at the cost of 74%\nfewer parameters and 2.5x faster inference speed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:59:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yang", "Lihe", ""], ["Zhuo", "Wei", ""], ["Qi", "Lei", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "2103.15407", "submitter": "Yujiao Shi", "authors": "Yujiao Shi, Hongdong Li, Xin Yu", "title": "Self-Supervised Visibility Learning for Novel View Synthesis", "comments": "accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of novel view synthesis (NVS) from a few sparse source\nview images. Conventional image-based rendering methods estimate scene geometry\nand synthesize novel views in two separate steps. However, erroneous geometry\nestimation will decrease NVS performance as view synthesis highly depends on\nthe quality of estimated scene geometry. In this paper, we propose an\nend-to-end NVS framework to eliminate the error propagation issue. To be\nspecific, we construct a volume under the target view and design a source-view\nvisibility estimation (SVE) module to determine the visibility of the\ntarget-view voxels in each source view. Next, we aggregate the visibility of\nall source views to achieve a consensus volume. Each voxel in the consensus\nvolume indicates a surface existence probability. Then, we present a soft\nray-casting (SRC) mechanism to find the most front surface in the target view\n(i.e. depth). Specifically, our SRC traverses the consensus volume along\nviewing rays and then estimates a depth probability distribution. We then warp\nand aggregate source view pixels to synthesize a novel view based on the\nestimated source-view visibility and target-view depth. At last, our network is\ntrained in an end-to-end self-supervised fashion, thus significantly\nalleviating error accumulation in view synthesis. Experimental results\ndemonstrate that our method generates novel views in higher quality compared to\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:11:25 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 06:13:53 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shi", "Yujiao", ""], ["Li", "Hongdong", ""], ["Yu", "Xin", ""]]}, {"id": "2103.15409", "submitter": "Xudong Chen", "authors": "Xudong Chen, Shugong Xu, Qiaobin Ji, Shan Cao", "title": "A Dataset and Benchmark Towards Multi-Modal Face Anti-Spoofing Under\n  Surveillance Scenarios", "comments": "Published in: IEEE Access", "journal-ref": "IEEE Access, vol. 9, pp. 28140-28155, 2021", "doi": "10.1109/ACCESS.2021.3052728", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Anti-spoofing (FAS) is a challenging problem due to complex serving\nscenarios and diverse face presentation attack patterns. Especially when\ncaptured images are low-resolution, blurry, and coming from different domains,\nthe performance of FAS will degrade significantly. The existing multi-modal FAS\ndatasets rarely pay attention to the cross-domain problems under deployment\nscenarios, which is not conducive to the study of model performance. To solve\nthese problems, we explore the fine-grained differences between multi-modal\ncameras and construct a cross-domain multi-modal FAS dataset under surveillance\nscenarios called GREAT-FASD-S. Besides, we propose an Attention based Face\nAnti-spoofing network with Feature Augment (AFA) to solve the FAS towards\nlow-quality face images. It consists of the depthwise separable attention\nmodule (DAM) and the multi-modal based feature augment module (MFAM). Our model\ncan achieve state-of-the-art performance on the CASIA-SURF dataset and our\nproposed GREAT-FASD-S dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:14:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Xudong", ""], ["Xu", "Shugong", ""], ["Ji", "Qiaobin", ""], ["Cao", "Shan", ""]]}, {"id": "2103.15425", "submitter": "Tianshu Xie", "authors": "Tianshu Xie, Minghui Liu, Jiali Deng, Xuan Cheng, Xiaomin Wang, Ming\n  Liu", "title": "FocusedDropout for Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In convolutional neural network (CNN), dropout cannot work well because\ndropped information is not entirely obscured in convolutional layers where\nfeatures are correlated spatially. Except randomly discarding regions or\nchannels, many approaches try to overcome this defect by dropping influential\nunits. In this paper, we propose a non-random dropout method named\nFocusedDropout, aiming to make the network focus more on the target. In\nFocusedDropout, we use a simple but effective way to search for the\ntarget-related features, retain these features and discard others, which is\ncontrary to the existing methods. We found that this novel method can improve\nnetwork performance by making the network more target-focused. Besides,\nincreasing the weight decay while using FocusedDropout can avoid the\noverfitting and increase accuracy. Experimental results show that even a slight\ncost, 10\\% of batches employing FocusedDropout, can produce a nice performance\nboost over the baselines on multiple datasets of classification, including\nCIFAR10, CIFAR100, Tiny Imagenet, and has a good versatility for different CNN\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:47:55 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xie", "Tianshu", ""], ["Liu", "Minghui", ""], ["Deng", "Jiali", ""], ["Cheng", "Xuan", ""], ["Wang", "Xiaomin", ""], ["Liu", "Ming", ""]]}, {"id": "2103.15428", "submitter": "Fangwen Shu", "authors": "Yaxu Xie, Jason Rambach, Fangwen Shu, Didier Stricker", "title": "PlaneSegNet: Fast and Robust Plane Estimation Using a Single-stage\n  Instance Segmentation CNN", "comments": "accepted to ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Instance segmentation of planar regions in indoor scenes benefits visual SLAM\nand other applications such as augmented reality (AR) where scene understanding\nis required. Existing methods built upon two-stage frameworks show satisfactory\naccuracy but are limited by low frame rates. In this work, we propose a\nreal-time deep neural architecture that estimates piece-wise planar regions\nfrom a single RGB image. Our model employs a variant of a fast single-stage CNN\narchitecture to segment plane instances. Considering the particularity of the\ntarget detected, we propose Fast Feature Non-maximum Suppression (FF-NMS) to\nreduce the suppression errors resulted from overlapping bounding boxes of\nplanes. We also utilize a Residual Feature Augmentation module in the Feature\nPyramid Network (FPN). Our method achieves significantly higher frame-rates and\ncomparable segmentation accuracy against two-stage methods. We automatically\nlabel over 70,000 images as ground truth from the Stanford 2D-3D-Semantics\ndataset. Moreover, we incorporate our method with a state-of-the-art planar\nSLAM and validate its benefits.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:53:05 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xie", "Yaxu", ""], ["Rambach", "Jason", ""], ["Shu", "Fangwen", ""], ["Stricker", "Didier", ""]]}, {"id": "2103.15432", "submitter": "Abdallah Dib", "authors": "Abdallah Dib, Cedric Thebault, Junghyun Ahn, Philippe-Henri Gosselin,\n  Christian Theobalt, Louis Chevallier", "title": "Towards High Fidelity Monocular Face Reconstruction with Rich\n  Reflectance using Self-supervised Learning and Ray Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust face reconstruction from monocular image in general lighting\nconditions is challenging. Methods combining deep neural network encoders with\ndifferentiable rendering have opened up the path for very fast monocular\nreconstruction of geometry, lighting and reflectance. They can also be trained\nin self-supervised manner for increased robustness and better generalization.\nHowever, their differentiable rasterization based image formation models, as\nwell as underlying scene parameterization, limit them to Lambertian face\nreflectance and to poor shape details. More recently, ray tracing was\nintroduced for monocular face reconstruction within a classic\noptimization-based framework and enables state-of-the art results. However\noptimization-based approaches are inherently slow and lack robustness. In this\npaper, we build our work on the aforementioned approaches and propose a new\nmethod that greatly improves reconstruction quality and robustness in general\nscenes. We achieve this by combining a CNN encoder with a differentiable ray\ntracer, which enables us to base the reconstruction on much more advanced\npersonalized diffuse and specular albedos, a more sophisticated illumination\nmodel and a plausible representation of self-shadows. This enables to take a\nbig leap forward in reconstruction quality of shape, appearance and lighting\neven in scenes with difficult illumination. With consistent face attributes\nreconstruction, our method leads to practical applications such as relighting\nand self-shadows removal. Compared to state-of-the-art methods, our results\nshow improved accuracy and validity of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:58:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dib", "Abdallah", ""], ["Thebault", "Cedric", ""], ["Ahn", "Junghyun", ""], ["Gosselin", "Philippe-Henri", ""], ["Theobalt", "Christian", ""], ["Chevallier", "Louis", ""]]}, {"id": "2103.15436", "submitter": "Xin Chen", "authors": "Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang and Huchuan Lu", "title": "Transformer Tracking", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation acts as a critical role in the tracking field, especially in\nrecent popular Siamese-based trackers. The correlation operation is a simple\nfusion manner to consider the similarity between the template and the search\nregion. However, the correlation operation itself is a local linear matching\nprocess, leading to lose semantic information and fall into local optimum\neasily, which may be the bottleneck of designing high-accuracy tracking\nalgorithms. Is there any better feature fusion method than correlation? To\naddress this issue, inspired by Transformer, this work presents a novel\nattention-based feature fusion network, which effectively combines the template\nand search region features solely using attention. Specifically, the proposed\nmethod includes an ego-context augment module based on self-attention and a\ncross-feature augment module based on cross-attention. Finally, we present a\nTransformer tracking (named TransT) method based on the Siamese-like feature\nextraction backbone, the designed attention-based fusion mechanism, and the\nclassification and regression head. Experiments show that our TransT achieves\nvery promising results on six challenging datasets, especially on large-scale\nLaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively\n50 fps on GPU. Code and models are available at\nhttps://github.com/chenxin-dlut/TransT.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:06:55 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Xin", ""], ["Yan", "Bin", ""], ["Zhu", "Jiawen", ""], ["Wang", "Dong", ""], ["Yang", "Xiaoyun", ""], ["Lu", "Huchuan", ""]]}, {"id": "2103.15438", "submitter": "Yufan Liu", "authors": "Yufan Liu, Minglang Qiao, Mai Xu, Bing Li, Weiming Hu, Ali Borji", "title": "Learning to Predict Salient Faces: A Novel Visual-Audio Saliency Model", "comments": "Published as an ECCV2020 paper", "journal-ref": null, "doi": "10.1007/978-3-030-58565-5_25", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, video streams have occupied a large proportion of Internet traffic,\nmost of which contain human faces. Hence, it is necessary to predict saliency\non multiple-face videos, which can provide attention cues for many content\nbased applications. However, most of multiple-face saliency prediction works\nonly consider visual information and ignore audio, which is not consistent with\nthe naturalistic scenarios. Several behavioral studies have established that\nsound influences human attention, especially during the speech turn-taking in\nmultiple-face videos. In this paper, we thoroughly investigate such influences\nby establishing a large-scale eye-tracking database of Multiple-face Video in\nVisual-Audio condition (MVVA). Inspired by the findings of our investigation,\nwe propose a novel multi-modal video saliency model consisting of three\nbranches: visual, audio and face. The visual branch takes the RGB frames as the\ninput and encodes them into visual feature maps. The audio and face branches\nencode the audio signal and multiple cropped faces, respectively. A fusion\nmodule is introduced to integrate the information from three modalities, and to\ngenerate the final saliency map. Experimental results show that the proposed\nmethod outperforms 11 state-of-the-art saliency prediction works. It performs\ncloser to human multi-modal attention.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:09:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Yufan", ""], ["Qiao", "Minglang", ""], ["Xu", "Mai", ""], ["Li", "Bing", ""], ["Hu", "Weiming", ""], ["Borji", "Ali", ""]]}, {"id": "2103.15439", "submitter": "Endel Poder", "authors": "Endel Poder", "title": "CNN-based search model underestimates attention guidance by simple\n  visual features", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, Zhang et al. (2018) proposed an interesting model of attention\nguidance that uses visual features learnt by convolutional neural networks for\nobject recognition. I adapted this model for search experiments with accuracy\nas the measure of performance. Simulation of our previously published feature\nand conjunction search experiments revealed that CNN-based search model\nconsiderably underestimates human attention guidance by simple visual features.\nA simple explanation is that the model has no bottom-up guidance of attention.\nAnother view might be that standard CNNs do not learn features required for\nhuman-like attention guidance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:10:48 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 18:57:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Poder", "Endel", ""]]}, {"id": "2103.15446", "submitter": "Soham Mazumder", "authors": "Shivangi Aneja and Soham Mazumder", "title": "Deep Image Compositing", "comments": "ESSE 2020: Proceedings of the 2020 European Symposium on Software\n  Engineering", "journal-ref": "In Proceedings of the 2020 European Symposium on Software\n  Engineering (pp. 101-104) 2020", "doi": "10.1145/3393822.3432314", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In image editing, the most common task is pasting objects from one image to\nthe other and then eventually adjusting the manifestation of the foreground\nobject with the background object. This task is called image compositing. But\nimage compositing is a challenging problem that requires professional editing\nskills and a considerable amount of time. Not only these professionals are\nexpensive to hire, but the tools (like Adobe Photoshop) used for doing such\ntasks are also expensive to purchase making the overall task of image\ncompositing difficult for people without this skillset. In this work, we aim to\ncater to this problem by making composite images look realistic. To achieve\nthis, we are using Generative Adversarial Networks (GANS). By training the\nnetwork with a diverse range of filters applied to the images and special loss\nfunctions, the model is able to decode the color histogram of the foreground\nand background part of the image and also learns to blend the foreground object\nwith the background. The hue and saturation values of the image play an\nimportant role as discussed in this paper. To the best of our knowledge, this\nis the first work that uses GANs for the task of image compositing. Currently,\nthere is no benchmark dataset available for image compositing. So we created\nthe dataset and will also make the dataset publicly available for benchmarking.\nExperimental results on this dataset show that our method outperforms all\ncurrent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:23:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Aneja", "Shivangi", ""], ["Mazumder", "Soham", ""]]}, {"id": "2103.15449", "submitter": "Benjamin Filtjens", "authors": "Benjamin Filtjens, Pieter Ginis, Alice Nieuwboer, Peter Slaets, and\n  Bart Vanrumste", "title": "Automated freezing of gait assessment with marker-based motion capture\n  and multi-stage graph convolutional neural networks approaches expert-level\n  detection", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freezing of gait (FOG) is a common and debilitating gait impairment in\nParkinson's disease. Further insight in this phenomenon is hampered by the\ndifficulty to objectively assess FOG. To meet this clinical need, this paper\nproposes a motion capture-based FOG assessment method driven by a novel deep\nneural network. The proposed network, termed multi-stage graph convolutional\nnetwork (MS-GCN), combines the spatial-temporal graph convolutional network\n(ST-GCN) and the multi-stage temporal convolutional network (MS-TCN). The\nST-GCN captures the hierarchical motion among the optical markers inherent to\nmotion capture, while the multi-stage component reduces over-segmentation\nerrors by refining the predictions over multiple stages. The proposed model was\nvalidated on a dataset of fourteen freezers, fourteen non-freezers, and\nfourteen healthy control subjects. The experiments indicate that the proposed\nmodel outperforms state-of-the-art baselines. An in-depth quantitative and\nqualitative analysis demonstrates that the proposed model is able to achieve\nclinician-like FOG assessment. The proposed MS-GCN can provide an automated and\nobjective alternative to labor-intensive clinician-based FOG assessment.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:32:45 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 19:24:52 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Filtjens", "Benjamin", ""], ["Ginis", "Pieter", ""], ["Nieuwboer", "Alice", ""], ["Slaets", "Peter", ""], ["Vanrumste", "Bart", ""]]}, {"id": "2103.15454", "submitter": "ByungSoo Ko", "authors": "Geonmo Gu, Byungsoo Ko, Han-Gyu Kim", "title": "Proxy Synthesis: Learning with Synthetic Classes for Deep Metric\n  Learning", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main purposes of deep metric learning is to construct an embedding\nspace that has well-generalized embeddings on both seen (training) classes and\nunseen (test) classes. Most existing works have tried to achieve this using\ndifferent types of metric objectives and hard sample mining strategies with\ngiven training data. However, learning with only the training data can be\noverfitted to the seen classes, leading to the lack of generalization\ncapability on unseen classes. To address this problem, we propose a simple\nregularizer called Proxy Synthesis that exploits synthetic classes for stronger\ngeneralization in deep metric learning. The proposed method generates synthetic\nembeddings and proxies that work as synthetic classes, and they mimic unseen\nclasses when computing proxy-based losses. Proxy Synthesis derives an embedding\nspace considering class relations and smooth decision boundaries for robustness\non unseen classes. Our method is applicable to any proxy-based losses,\nincluding softmax and its variants. Extensive experiments on four famous\nbenchmarks in image retrieval tasks demonstrate that Proxy Synthesis\nsignificantly boosts the performance of proxy-based losses and achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:39:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gu", "Geonmo", ""], ["Ko", "Byungsoo", ""], ["Kim", "Han-Gyu", ""]]}, {"id": "2103.15459", "submitter": "Jindong Gu", "authors": "Jindong Gu, Volker Tresp, Han Hu", "title": "Capsule Network is Not More Robust than Convolutional Network", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Capsule Network is widely believed to be more robust than Convolutional\nNetworks. However, there are no comprehensive comparisons between these two\nnetworks, and it is also unknown which components in the CapsNet affect its\nrobustness. In this paper, we first carefully examine the special designs in\nCapsNet that differ from that of a ConvNet commonly used for image\nclassification. The examination reveals five major new/different components in\nCapsNet: a transformation process, a dynamic routing layer, a squashing\nfunction, a marginal loss other than cross-entropy loss, and an additional\nclass-conditional reconstruction loss for regularization. Along with these\nmajor differences, we conduct comprehensive ablation studies on three kinds of\nrobustness, including affine transformation, overlapping digits, and semantic\nrepresentation. The study reveals that some designs, which are thought critical\nto CapsNet, actually can harm its robustness, i.e., the dynamic routing layer\nand the transformation process, while others are beneficial for the robustness.\nBased on these findings, we propose enhanced ConvNets simply by introducing the\nessential components behind the CapsNet's success. The proposed simple ConvNets\ncan achieve better robustness than the CapsNet.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:47:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gu", "Jindong", ""], ["Tresp", "Volker", ""], ["Hu", "Han", ""]]}, {"id": "2103.15463", "submitter": "Hacer Yalim Keles", "authors": "Ozlem Sen and Hacer Yalim Keles", "title": "A Hierarchical Approach to Remote Sensing Scene Classification", "comments": "This paper is under consideration in PFG - Journal of Photogrammetry,\n  Remote Sensing and Geoinformation Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Remote sensing scene classification deals with the problem of classifying\nland use/cover of a region from images. To predict the development and\nsocioeconomic structures of cities, the status of land use in regions are\ntracked by the national mapping agencies of countries. Many of these agencies\nuse land use types that are arranged in multiple levels. In this paper, we\nexamined the efficiency of a hierarchically designed CNN based framework that\nis suitable for such arrangements. We use NWPU-RESISC45 dataset for our\nexperiments and arranged this data set in a two level nested hierarchy. We have\ntwo cascaded deep CNN models initiated using DenseNet-121 architectures. We\nprovide detailed empirical analysis to compare the performances of this\nhierarchical scheme and its non hierarchical counterpart, together with the\nindividual model performances. We also evaluated the performance of the\nhierarchical structure statistically to validate the presented empirical\nresults. The results of our experiments show that although individual\nclassifiers for different sub-categories in the hierarchical scheme perform\nwell, the accumulation of classification errors in the cascaded structure\nprevents its classification performance from exceeding that of the non\nhierarchical deep model.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:56:57 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sen", "Ozlem", ""], ["Keles", "Hacer Yalim", ""]]}, {"id": "2103.15467", "submitter": "Yantian Luo", "authors": "Yantian Luo, Zhiming Wang, Danlan Huang, Ning Ge and Jianhua Lu", "title": "Get away from Style: Category-Guided Domain Adaptation for Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) becomes more and more popular in\ntackling real-world problems without ground truth of the target domain. Though\na mass of tedious annotation work is not needed, UDA unavoidably faces the\nproblem how to narrow the domain discrepancy to boost the transferring\nperformance. In this paper, we focus on UDA for semantic segmentation task.\nFirstly, we propose a style-independent content feature extraction mechanism to\nkeep the style information of extracted features in the similar space, since\nthe style information plays a extremely slight role for semantic segmentation\ncompared with the content part. Secondly, to keep the balance of pseudo labels\non each category, we propose a category-guided threshold mechanism to choose\ncategory-wise pseudo labels for self-supervised learning. The experiments are\nconducted using GTA5 as the source domain, Cityscapes as the target domain. The\nresults show that our model outperforms the state-of-the-arts with a noticeable\ngain on cross-domain adaptation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:00:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Luo", "Yantian", ""], ["Wang", "Zhiming", ""], ["Huang", "Danlan", ""], ["Ge", "Ning", ""], ["Lu", "Jianhua", ""]]}, {"id": "2103.15469", "submitter": "Jihyong Oh", "authors": "Jihyong Oh, Munchurl Kim", "title": "PeaceGAN: A GAN-based Multi-Task Learning Method for SAR Target Image\n  Generation with a Pose Estimator and an Auxiliary Classifier", "comments": "14 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Networks (GANs) are successfully applied to\ndiverse fields, training GANs on synthetic aperture radar (SAR) data is a\nchallenging task mostly due to speckle noise. On the one hands, in a learning\nperspective of human's perception, it is natural to learn a task by using\nvarious information from multiple sources. However, in the previous GAN works\non SAR target image generation, the information on target classes has only been\nused. Due to the backscattering characteristics of SAR image signals, the\nshapes and structures of SAR target images are strongly dependent on their pose\nangles. Nevertheless, the pose angle information has not been incorporated into\nsuch generative models for SAR target images. In this paper, we firstly propose\na novel GAN-based multi-task learning (MTL) method for SAR target image\ngeneration, called PeaceGAN that uses both pose angle and target class\ninformation, which makes it possible to produce SAR target images of desired\ntarget classes at intended pose angles. For this, the PeaceGAN has two\nadditional structures, a pose estimator and an auxiliary classifier, at the\nside of its discriminator to combine the pose and class information more\nefficiently. In addition, the PeaceGAN is jointly learned in an end-to-end\nmanner as MTL with both pose angle and target class information, thus enhancing\nthe diversity and quality of generated SAR target images The extensive\nexperiments show that taking an advantage of both pose angle and target class\nlearning by the proposed pose estimator and auxiliary classifier can help the\nPeaceGAN's generator effectively learn the distributions of SAR target images\nin the MTL framework, so that it can better generate the SAR target images more\nflexibly and faithfully at intended pose angles for desired target classes\ncompared to the recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:03:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Oh", "Jihyong", ""], ["Kim", "Munchurl", ""]]}, {"id": "2103.15476", "submitter": "Mohammad Hossein Rohban", "authors": "Zeinab Golgooni, Mehrdad Saberi, Masih Eskandar, Mohammad Hossein\n  Rohban", "title": "ZeroGrad : Mitigating and Explaining Catastrophic Overfitting in FGSM\n  Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Making deep neural networks robust to small adversarial noises has recently\nbeen sought in many applications. Adversarial training through iterative\nprojected gradient descent (PGD) has been established as one of the mainstream\nideas to achieve this goal. However, PGD is computationally demanding and often\nprohibitive in case of large datasets and models. For this reason, single-step\nPGD, also known as FGSM, has recently gained interest in the field.\nUnfortunately, FGSM-training leads to a phenomenon called ``catastrophic\noverfitting,\" which is a sudden drop in the adversarial accuracy under the PGD\nattack. In this paper, we support the idea that small input gradients play a\nkey role in this phenomenon, and hence propose to zero the input gradient\nelements that are small for crafting FGSM attacks. Our proposed idea, while\nbeing simple and efficient, achieves competitive adversarial accuracy on\nvarious datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:19:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Golgooni", "Zeinab", ""], ["Saberi", "Mehrdad", ""], ["Eskandar", "Masih", ""], ["Rohban", "Mohammad Hossein", ""]]}, {"id": "2103.15483", "submitter": "Xiaoxiao Long", "authors": "Xiaoxiao Long, Cheng Lin, Lingjie Liu, Wei Li, Christian Theobalt,\n  Ruigang Yang, Wenping Wang", "title": "Adaptive Surface Normal Constraint for Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel method for single image depth estimation using surface\nnormal constraints. Existing depth estimation methods either suffer from the\nlack of geometric constraints, or are limited to the difficulty of reliably\ncapturing geometric context, which leads to a bottleneck of depth estimation\nquality. We therefore introduce a simple yet effective method, named Adaptive\nSurface Normal (ASN) constraint, to effectively correlate the depth estimation\nwith geometric consistency. Our key idea is to adaptively determine the\nreliable local geometry from a set of randomly sampled candidates to derive\nsurface normal constraint, for which we measure the consistency of the\ngeometric contextual features. As a result, our method can faithfully\nreconstruct the 3D geometry and is robust to local shape variations, such as\nboundaries, sharp corners and noises. We conduct extensive evaluations and\ncomparisons using public datasets. The experimental results demonstrate our\nmethod outperforms the state-of-the-art methods and has superior efficiency and\nrobustness.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:36:25 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 16:37:05 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Long", "Xiaoxiao", ""], ["Lin", "Cheng", ""], ["Liu", "Lingjie", ""], ["Li", "Wei", ""], ["Theobalt", "Christian", ""], ["Yang", "Ruigang", ""], ["Wang", "Wenping", ""]]}, {"id": "2103.15486", "submitter": "Bahram Mohammadi", "authors": "Bahram Mohammadi and Mohammad Sabokrou", "title": "ClaRe: Practical Class Incremental Learning By Remembering Previous\n  Class Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a practical and simple yet efficient method to\neffectively deal with the catastrophic forgetting for Class Incremental\nLearning (CIL) tasks. CIL tends to learn new concepts perfectly, but not at the\nexpense of performance and accuracy for old data. Learning new knowledge in the\nabsence of data instances from previous classes or even imbalance samples of\nboth old and new classes makes CIL an ongoing challenging problem. These issues\ncan be tackled by storing exemplars belonging to the previous tasks or by\nutilizing the rehearsal strategy. Inspired by the rehearsal strategy with the\napproach of using generative models, we propose ClaRe, an efficient solution\nfor CIL by remembering the representations of learned classes in each\nincrement. Taking this approach leads to generating instances with the same\ndistribution of the learned classes. Hence, our model is somehow retrained from\nthe scratch using a new training set including both new and the generated\nsamples. Subsequently, the imbalance data problem is also solved. ClaRe has a\nbetter generalization than prior methods thanks to producing diverse instances\nfrom the distribution of previously learned classes. We comprehensively\nevaluate ClaRe on the MNIST benchmark. Results show a very low degradation on\naccuracy against facing new knowledge over time. Furthermore, contrary to the\nmost proposed solutions, the memory limitation is not problematic any longer\nwhich is considered as a consequential issue in this research area.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:39:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mohammadi", "Bahram", ""], ["Sabokrou", "Mohammad", ""]]}, {"id": "2103.15488", "submitter": "Jiajun Zhu", "authors": "Jiajun Zhu, Xiufeng Jiang, Zhiwei Jia, Shugong Xu, Shan Cao", "title": "Tracking Based Semi-Automatic Annotation for Scene Text Videos", "comments": "Published in: IEEE Access ( Early Access )", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3066601", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, video scene text detection has received increasing attention due to\nits comprehensive applications. However, the lack of annotated scene text video\ndatasets has become one of the most important problems, which hinders the\ndevelopment of video scene text detection. The existing scene text video\ndatasets are not large-scale due to the expensive cost caused by manual\nlabeling. In addition, the text instances in these datasets are too clear to be\na challenge. To address the above issues, we propose a tracking based\nsemi-automatic labeling strategy for scene text videos in this paper. We get\nsemi-automatic scene text annotation by labeling manually for the first frame\nand tracking automatically for the subsequent frames, which avoid the huge cost\nof manual labeling. Moreover, a paired low-quality scene text video dataset\nnamed Text-RBL is proposed, consisting of raw videos, blurry videos, and\nlow-resolution videos, labeled by the proposed convenient semi-automatic\nlabeling strategy. Through an averaging operation and bicubic down-sampling\noperation over the raw videos, we can efficiently obtain blurry videos and\nlow-resolution videos paired with raw videos separately. To verify the\neffectiveness of Text-RBL, we propose a baseline model combined with the text\ndetector and tracker for video scene text detection. Moreover, a failure\ndetection scheme is designed to alleviate the baseline model drift issue caused\nby complex scenes. Extensive experiments demonstrate that Text-RBL with paired\nlow-quality videos labeled by the semi-automatic method can significantly\nimprove the performance of the text detector in low-quality scenes.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:42:23 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhu", "Jiajun", ""], ["Jiang", "Xiufeng", ""], ["Jia", "Zhiwei", ""], ["Xu", "Shugong", ""], ["Cao", "Shan", ""]]}, {"id": "2103.15501", "submitter": "Shohei Nobuhara", "authors": "Kosuke Takahashi and Shohei Nobuhara", "title": "Structure of Multiple Mirror System from Kaleidoscopic Projections of\n  Single 3D Point", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel algorithm of discovering the structure of a\nkaleidoscopic imaging system that consists of multiple planar mirrors and a\ncamera. The kaleidoscopic imaging system can be recognized as the virtual\nmulti-camera system and has strong advantages in that the virtual cameras are\nstrictly synchronized and have the same intrinsic parameters. In this paper, we\nfocus on the extrinsic calibration of the virtual multi-camera system. The\nproblems to be solved in this paper are two-fold. The first problem is to\nidentify to which mirror chamber each of the 2D projections of mirrored 3D\npoints belongs. The second problem is to estimate all mirror parameters, i.e.,\nnormals, and distances of the mirrors. The key contribution of this paper is to\npropose novel algorithms for these problems using a single 3D point of unknown\ngeometry by utilizing a kaleidoscopic projection constraint, which is an\nepipolar constraint on mirror reflections. We demonstrate the performance of\nthe proposed algorithm of chamber assignment and estimation of mirror\nparameters with qualitative and quantitative evaluations using synthesized and\nreal data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:12:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Takahashi", "Kosuke", ""], ["Nobuhara", "Shohei", ""]]}, {"id": "2103.15502", "submitter": "Feng Gao", "authors": "Tiange Zhang, Feng Gao, Junyu Dong, Qian Du", "title": "Remote Sensing Image Translation via Style-Based Recalibration Module\n  and Improved Style Discriminator", "comments": "Accepted by IEEE Geoscience and Remote Sensing Letters, Code:\n  https://github.com/summitgao/RSIT_SRM_ISD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing remote sensing change detection methods are heavily affected by\nseasonal variation. Since vegetation colors are different between winter and\nsummer, such variations are inclined to be falsely detected as changes. In this\nletter, we proposed an image translation method to solve the problem. A\nstyle-based recalibration module is introduced to capture seasonal features\neffectively. Then, a new style discriminator is designed to improve the\ntranslation performance. The discriminator can not only produce a decision for\nthe fake or real sample, but also return a style vector according to the\nchannel-wise correlations. Extensive experiments are conducted on\nseason-varying dataset. The experimental results show that the proposed method\ncan effectively perform image translation, thereby consistently improving the\nseason-varying image change detection performance. Our codes and data are\navailable at https://github.com/summitgao/RSIT_SRM_ISD.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:12:43 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Tiange", ""], ["Gao", "Feng", ""], ["Dong", "Junyu", ""], ["Du", "Qian", ""]]}, {"id": "2103.15507", "submitter": "Xiaoxuan Ma", "authors": "Xiaoxuan Ma, Jiajun Su, Chunyu Wang, Hai Ci and Yizhou Wang", "title": "Context Modeling in 3D Human Pose Estimation: A Unified Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D human pose from a single image suffers from severe ambiguity\nsince multiple 3D joint configurations may have the same 2D projection. The\nstate-of-the-art methods often rely on context modeling methods such as\npictorial structure model (PSM) or graph neural network (GNN) to reduce\nambiguity. However, there is no study that rigorously compares them side by\nside. So we first present a general formula for context modeling in which both\nPSM and GNN are its special cases. By comparing the two methods, we found that\nthe end-to-end training scheme in GNN and the limb length constraints in PSM\nare two complementary factors to improve results. To combine their advantages,\nwe propose ContextPose based on attention mechanism that allows enforcing soft\nlimb length constraints in a deep network. The approach effectively reduces the\nchance of getting absurd 3D pose estimates with incorrect limb lengths and\nachieves state-of-the-art results on two benchmark datasets. More importantly,\nthe introduction of limb length constraints into deep networks enables the\napproach to achieve much better generalization performance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:26:03 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 08:56:32 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ma", "Xiaoxuan", ""], ["Su", "Jiajun", ""], ["Wang", "Chunyu", ""], ["Ci", "Hai", ""], ["Wang", "Yizhou", ""]]}, {"id": "2103.15510", "submitter": "Melanie Schellenberg", "authors": "Melanie Schellenberg, Janek Gr\\\"ohl, Kris Dreher, Niklas Holzwarth,\n  Minu D. Tizabi, Alexander Seitel, Lena Maier-Hein", "title": "Data-driven generation of plausible tissue geometries for realistic\n  photoacoustic image synthesis", "comments": "13 pages, 7 figures, 5 tables, update of figures with revised data\n  and corresponding text, overall message remains unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoacoustic tomography (PAT) has the potential to recover morphological and\nfunctional tissue properties such as blood oxygenation with high spatial\nresolution and in an interventional setting. However, decades of research\ninvested in solving the inverse problem of recovering clinically relevant\ntissue properties from spectral measurements have failed to produce solutions\nthat can quantify tissue parameters robustly in a clinical setting. Previous\nattempts to address the limitations of model-based approaches with machine\nlearning were hampered by the absence of labeled reference data needed for\nsupervised algorithm training. While this bottleneck has been tackled by\nsimulating training data, the domain gap between real and simulated images\nremains a huge unsolved challenge. As a first step to address this bottleneck,\nwe propose a novel approach to PAT data simulation, which we refer to as\n\"learning to simulate\". Our approach involves subdividing the challenge of\ngenerating plausible simulations into two disjoint problems: (1) Probabilistic\ngeneration of realistic tissue morphology, represented by semantic segmentation\nmaps and (2) pixel-wise assignment of corresponding optical and acoustic\nproperties. In the present work, we focus on the first challenge. Specifically,\nwe leverage the concept of Generative Adversarial Networks (GANs) trained on\nsemantically annotated medical imaging data to generate plausible tissue\ngeometries. According to an initial in silico feasibility study our approach is\nwell-suited for contributing to realistic PAT image synthesis and could thus\nbecome a fundamental step for deep learning-based quantitative PAT.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:30:18 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:50:48 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Schellenberg", "Melanie", ""], ["Gr\u00f6hl", "Janek", ""], ["Dreher", "Kris", ""], ["Holzwarth", "Niklas", ""], ["Tizabi", "Minu D.", ""], ["Seitel", "Alexander", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2103.15534", "submitter": "Lei Tian", "authors": "Lei Tian, Guoqiang Liang, Peng Wang, Chunhua Shen", "title": "An Adversarial Human Pose Estimation Network Injected with Graph\n  Structure", "comments": "The paper is accepted by Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because of the invisible human keypoints in images caused by illumination,\nocclusion and overlap, it is likely to produce unreasonable human pose\nprediction for most of the current human pose estimation methods. In this\npaper, we design a novel generative adversarial network (GAN) to improve the\nlocalization accuracy of visible joints when some joints are invisible. The\nnetwork consists of two simple but efficient modules, Cascade Feature Network\n(CFN) and Graph Structure Network (GSN). First, the CFN utilizes the prediction\nmaps from the previous stages to guide the prediction maps in the next stage to\nproduce accurate human pose. Second, the GSN is designed to contribute to the\nlocalization of invisible joints by passing message among different joints.\nAccording to GAN, if the prediction pose produced by the generator G cannot be\ndistinguished by the discriminator D, the generator network G has successfully\nobtained the underlying dependence of human joints. We conduct experiments on\nthree widely used human pose estimation benchmark datasets, LSP, MPII and COCO,\nwhose results show the effectiveness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:07:08 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 04:21:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tian", "Lei", ""], ["Liang", "Guoqiang", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""]]}, {"id": "2103.15536", "submitter": "Ayan Das", "authors": "Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang and Yi-Zhe Song", "title": "Cloud2Curve: Generation and Vectorization of Parametric Sketches", "comments": "Accepted at CVPR 2021 (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of human sketches in deep learning has advanced immensely through\nthe use of waypoint-sequences rather than raster-graphic representations. We\nfurther aim to model sketches as a sequence of low-dimensional parametric\ncurves. To this end, we propose an inverse graphics framework capable of\napproximating a raster or waypoint based stroke encoded as a point-cloud with a\nvariable-degree B\\'ezier curve. Building on this module, we present\nCloud2Curve, a generative model for scalable high-resolution vector sketches\nthat can be trained end-to-end using point-cloud data alone. As a consequence,\nour model is also capable of deterministic vectorization which can map novel\nraster or waypoint based sketches to their corresponding high-resolution\nscalable B\\'ezier equivalent. We evaluate the generation and vectorization\ncapabilities of our model on Quick, Draw! and K-MNIST datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:09:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Das", "Ayan", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2103.15537", "submitter": "Xin Jin", "authors": "Xin Jin, Tianyu He, Kecheng Zheng, Zhiheng Yin, Xu Shen, Zhen Huang,\n  Ruoyu Feng, Jianqiang Huang, Xian-Sheng Hua, Zhibo Chen", "title": "Cloth-Changing Person Re-identification from A Single Image with Gait\n  Prediction and Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cloth-Changing person re-identification (CC-ReID) aims at matching the same\nperson across different locations over a long-duration, e.g., over days, and\ntherefore inevitably meets challenge of changing clothing. In this paper, we\nfocus on handling well the CC-ReID problem under a more challenging setting,\ni.e., just from a single image, which enables high-efficiency and latency-free\npedestrian identify for real-time surveillance applications. Specifically, we\nintroduce Gait recognition as an auxiliary task to drive the Image ReID model\nto learn cloth-agnostic representations by leveraging personal unique and\ncloth-independent gait information, we name this framework as GI-ReID. GI-ReID\nadopts a two-stream architecture that consists of a image ReID-Stream and an\nauxiliary gait recognition stream (Gait-Stream). The Gait-Stream, that is\ndiscarded in the inference for high computational efficiency, acts as a\nregulator to encourage the ReID-Stream to capture cloth-invariant biometric\nmotion features during the training. To get temporal continuous motion cues\nfrom a single image, we design a Gait Sequence Prediction (GSP) module for\nGait-Stream to enrich gait information. Finally, a high-level semantics\nconsistency over two streams is enforced for effective knowledge\nregularization. Experiments on multiple image-based Cloth-Changing ReID\nbenchmarks, e.g., LTCC, PRCC, Real28, and VC-Clothes, demonstrate that GI-ReID\nperforms favorably against the state-of-the-arts. Codes are available at\nhttps://github.com/jinx-USTC/GI-ReID.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:10:50 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 02:03:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jin", "Xin", ""], ["He", "Tianyu", ""], ["Zheng", "Kecheng", ""], ["Yin", "Zhiheng", ""], ["Shen", "Xu", ""], ["Huang", "Zhen", ""], ["Feng", "Ruoyu", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-Sheng", ""], ["Chen", "Zhibo", ""]]}, {"id": "2103.15538", "submitter": "Li Xu", "authors": "Li Xu, He Huang and Jun Liu", "title": "SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network\n  for Video Reasoning over Traffic Events", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic event cognition and reasoning in videos is an important task that has\na wide range of applications in intelligent transportation, assisted driving,\nand autonomous vehicles. In this paper, we create a novel dataset,\nSUTD-TrafficQA (Traffic Question Answering), which takes the form of video QA\nbased on the collected 10,080 in-the-wild videos and annotated 62,535 QA pairs,\nfor benchmarking the cognitive capability of causal inference and event\nunderstanding models in complex traffic scenarios. Specifically, we propose 6\nchallenging reasoning tasks corresponding to various traffic scenarios, so as\nto evaluate the reasoning capability over different kinds of complex yet\npractical traffic events. Moreover, we propose Eclipse, a novel Efficient\nglimpse network via dynamic inference, in order to achieve\ncomputation-efficient and reliable video reasoning. The experiments show that\nour method achieves superior performance while reducing the computation cost\nsignificantly. The project page: https://github.com/SUTDCV/SUTD-TrafficQA.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:12:50 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:00:27 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 12:21:03 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Xu", "Li", ""], ["Huang", "He", ""], ["Liu", "Jun", ""]]}, {"id": "2103.15545", "submitter": "Niv Granot", "authors": "Niv Granot, Assaf Shocher, Ben Feinstein, Shai Bagon and Michal Irani", "title": "Drop the GAN: In Defense of Patches Nearest Neighbors as Single Image\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image generative models perform synthesis and manipulation tasks by\ncapturing the distribution of patches within a single image. The classical (pre\nDeep Learning) prevailing approaches for these tasks are based on an\noptimization process that maximizes patch similarity between the input and\ngenerated output. Recently, however, Single Image GANs were introduced both as\na superior solution for such manipulation tasks, but also for remarkable novel\ngenerative tasks. Despite their impressiveness, single image GANs require long\ntraining time (usually hours) for each image and each task. They often suffer\nfrom artifacts and are prone to optimization issues such as mode collapse. In\nthis paper, we show that all of these tasks can be performed without any\ntraining, within several seconds, in a unified, surprisingly simple framework.\nWe revisit and cast the \"good-old\" patch-based methods into a novel\noptimization-free framework. We start with an initial coarse guess, and then\nsimply refine the details coarse-to-fine using patch-nearest-neighbor search.\nThis allows generating random novel images better and much faster than GANs. We\nfurther demonstrate a wide range of applications, such as image editing and\nreshuffling, retargeting to different sizes, structural analogies, image\ncollage and a newly introduced task of conditional inpainting. Not only is our\nmethod faster ($\\times 10^3$-$\\times 10^4$ than a GAN), it produces superior\nresults (confirmed by quantitative and qualitative evaluation), less artifacts\nand more realistic global structure than any of the previous approaches\n(whether GAN-based or classical patch-based).\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:20:46 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Granot", "Niv", ""], ["Shocher", "Assaf", ""], ["Feinstein", "Ben", ""], ["Bagon", "Shai", ""], ["Irani", "Michal", ""]]}, {"id": "2103.15565", "submitter": "Giulia Fracastoro", "authors": "Diego Valsesia, Giulia Fracastoro, Enrico Magli", "title": "RAN-GNNs: breaking the capacity limits of graph neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph neural networks have become a staple in problems addressing learning\nand analysis of data defined over graphs. However, several results suggest an\ninherent difficulty in extracting better performance by increasing the number\nof layers. Recent works attribute this to a phenomenon peculiar to the\nextraction of node features in graph-based tasks, i.e., the need to consider\nmultiple neighborhood sizes at the same time and adaptively tune them. In this\npaper, we investigate the recently proposed randomly wired architectures in the\ncontext of graph neural networks. Instead of building deeper networks by\nstacking many layers, we prove that employing a randomly-wired architecture can\nbe a more effective way to increase the capacity of the network and obtain\nricher representations. We show that such architectures behave like an ensemble\nof paths, which are able to merge contributions from receptive fields of varied\nsize. Moreover, these receptive fields can also be modulated to be wider or\nnarrower through the trainable weights over the paths. We also provide\nextensive experimental evidence of the superior performance of randomly wired\narchitectures over multiple tasks and four graph convolution definitions, using\nrecent benchmarking frameworks that addresses the reliability of previous\ntesting methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:34:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Valsesia", "Diego", ""], ["Fracastoro", "Giulia", ""], ["Magli", "Enrico", ""]]}, {"id": "2103.15566", "submitter": "Georgios Leontidis", "authors": "Mamatha Thota and Georgios Leontidis", "title": "Contrastive Domain Adaptation", "comments": "10 pages, 6 figures, 5 tables", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, 2021, pp. 2209-2218", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, contrastive self-supervised learning has become a key component for\nlearning visual representations across many computer vision tasks and\nbenchmarks. However, contrastive learning in the context of domain adaptation\nremains largely underexplored. In this paper, we propose to extend contrastive\nlearning to a new domain adaptation setting, a particular situation occurring\nwhere the similarity is learned and deployed on samples following different\nprobability distributions without access to labels. Contrastive learning learns\nby comparing and contrasting positive and negative pairs of samples in an\nunsupervised setting without access to source and target labels. We have\ndeveloped a variation of a recently proposed contrastive learning framework\nthat helps tackle the domain adaptation problem, further identifying and\nremoving possible negatives similar to the anchor to mitigate the effects of\nfalse negatives. Extensive experiments demonstrate that the proposed method\nadapts well, and improves the performance on the downstream domain adaptation\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:55:19 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Thota", "Mamatha", ""], ["Leontidis", "Georgios", ""]]}, {"id": "2103.15568", "submitter": "Haolong Li", "authors": "Haolong Li and Joerg Stueckler", "title": "Tracking 6-DoF Object Motion from Events and Frames", "comments": "Accepted by IEEE International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are promising devices for lowlatency tracking and high-dynamic\nrange imaging. In this paper,we propose a novel approach for 6\ndegree-of-freedom (6-DoF)object motion tracking that combines measurements of\neventand frame-based cameras. We formulate tracking from highrate events with a\nprobabilistic generative model of the eventmeasurement process of the object.\nOn a second layer, we refinethe object trajectory in slower rate image frames\nthrough directimage alignment. We evaluate the accuracy of our approach\ninseveral object tracking scenarios with synthetic data, and alsoperform\nexperiments with real data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:39:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Haolong", ""], ["Stueckler", "Joerg", ""]]}, {"id": "2103.15573", "submitter": "Feitong Tan", "authors": "Feitong Tan, Danhang Tang, Mingsong Dou, Kaiwen Guo, Rohit Pandey, Cem\n  Keskin, Ruofei Du, Deqing Sun, Sofien Bouaziz, Sean Fanello, Ping Tan, Yinda\n  Zhang", "title": "HumanGPS: Geodesic PreServing Feature for Dense Human Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of building dense correspondences\nbetween human images under arbitrary camera viewpoints and body poses. Prior\nart either assumes small motion between frames or relies on local descriptors,\nwhich cannot handle large motion or visually ambiguous body parts, e.g., left\nvs. right hand. In contrast, we propose a deep learning framework that maps\neach pixel to a feature space, where the feature distances reflect the geodesic\ndistances among pixels as if they were projected onto the surface of a 3D human\nscan. To this end, we introduce novel loss functions to push features apart\naccording to their geodesic distances on the surface. Without any semantic\nannotation, the proposed embeddings automatically learn to differentiate\nvisually similar parts and align different subjects into an unified feature\nspace. Extensive experiments show that the learned embeddings can produce\naccurate correspondences between images with remarkable generalization\ncapabilities on both intra and inter subjects.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:43:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tan", "Feitong", ""], ["Tang", "Danhang", ""], ["Dou", "Mingsong", ""], ["Guo", "Kaiwen", ""], ["Pandey", "Rohit", ""], ["Keskin", "Cem", ""], ["Du", "Ruofei", ""], ["Sun", "Deqing", ""], ["Bouaziz", "Sofien", ""], ["Fanello", "Sean", ""], ["Tan", "Ping", ""], ["Zhang", "Yinda", ""]]}, {"id": "2103.15578", "submitter": "Venkat Margapuri", "authors": "Venkat Margapuri and Mitchell Neilsen", "title": "Classification of Seeds using Domain Randomization on Self-Supervised\n  Learning Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The first step toward Seed Phenotyping i.e. the comprehensive assessment of\ncomplex seed traits such as growth, development, tolerance, resistance,\necology, yield, and the measurement of pa-rameters that form more complex\ntraits is the identification of seed type. Generally, a plant re-searcher\ninspects the visual attributes of a seed such as size, shape, area, color and\ntexture to identify the seed type, a process that is tedious and\nlabor-intensive. Advances in the areas of computer vision and deep learning\nhave led to the development of convolutional neural networks (CNN) that aid in\nclassification using images. While they classify efficiently, a key bottleneck\nis the need for an extensive amount of labelled data to train the CNN before it\ncan be put to the task of classification. The work leverages the concepts of\nContrastive Learning and Domain Randomi-zation in order to achieve the same.\nBriefly, domain randomization is the technique of applying models trained on\nimages containing simulated objects to real-world objects. The use of synthetic\nimages generated from a representational sample crop of real-world images\nalleviates the need for a large volume of test subjects. As part of the work,\nsynthetic image datasets of five different types of seed images namely, canola,\nrough rice, sorghum, soy and wheat are applied to three different\nself-supervised learning frameworks namely, SimCLR, Momentum Contrast (MoCo)\nand Build Your Own Latent (BYOL) where ResNet-50 is used as the backbone in\neach of the networks. When the self-supervised models are fine-tuned with only\n5% of the labels from the synthetic dataset, results show that MoCo, the model\nthat yields the best performance of the self-supervised learning frameworks in\nquestion, achieves an accuracy of 77% on the test dataset which is only ~13%\nless than the accuracy of 90% achieved by ResNet-50 trained on 100% of the\nlabels.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:50:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Margapuri", "Venkat", ""], ["Neilsen", "Mitchell", ""]]}, {"id": "2103.15584", "submitter": "Guoxi Huang", "authors": "Guoxi Huang and Adrian G. Bors", "title": "Video Classification with FineCoarse Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rich representation of the information in video data can be realized by\nmeans of frequency analysis. Fine motion details from the boundaries of moving\nregions are characterized by high frequencies in the spatio-temporal domain.\nMeanwhile, lower frequencies are encoded with coarse information containing\nsubstantial redundancy, which causes low efficiency for those video models that\ntake as input raw RGB frames. In this work, we propose a Motion Band-pass\nModule (MBPM) for separating the fine-grained information from coarse\ninformation in raw video data. By representing the coarse information with low\nresolution, we can increase the efficiency of video data processing. By\nembedding the MBPM into a two-pathway CNN architecture, we define a FineCoarse\nnetwork. The efficiency of the FineCoarse network is determined by avoiding the\nredundancy in the feature space processed by the two pathways: one operates on\ndownsampled features of low-resolution data, while the other operates on the\nfine-grained motion information captured by the MBPM. The proposed FineCoarse\nnetwork outperforms many recent video processing models on Kinetics400, UCF101\nand HMDB51. Furthermore, our approach achieves the state-of-the-art with 57.0%\ntop-1 accuracy on Something-Something V1.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:03:27 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Huang", "Guoxi", ""], ["Bors", "Adrian G.", ""]]}, {"id": "2103.15587", "submitter": "Soroush Farghadani", "authors": "Anees Kazi, Soroush Farghadani, Nassir Navab", "title": "IA-GCN: Interpretable Attention based Graph Convolutional Network for\n  Disease prediction", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interpretability in Graph Convolutional Networks (GCNs) has been explored to\nsome extent in computer vision in general, yet, in the medical domain, it\nrequires further examination. Moreover, most of the interpretability approaches\nfor GCNs, especially in the medical domain, focus on interpreting the model in\na post hoc fashion. In this paper, we propose an interpretable graph\nlearning-based model which 1) interprets the clinical relevance of the input\nfeatures towards the task, 2) uses the explanation to improve the model\nperformance and, 3) learns a population level latent graph that may be used to\ninterpret the cohort's behavior. In a clinical scenario, such a model can\nassist the clinical experts in better decision-making for diagnosis and\ntreatment planning. The main novelty lies in the interpretable attention module\n(IAM), which directly operates on multi-modal features. Our IAM learns the\nattention for each feature based on the unique interpretability-specific\nlosses. We show the application on two publicly available datasets, Tadpole and\nUKBB, for three tasks of disease, age, and gender prediction. Our proposed\nmodel shows superior performance with respect to compared methods with an\nincrease in an average accuracy of 3.2% for Tadpole, 1.6% for UKBB Gender, and\n2% for the UKBB Age prediction task. Further, we show exhaustive validation and\nclinical interpretation of our results.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:04:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kazi", "Anees", ""], ["Farghadani", "Soroush", ""], ["Navab", "Nassir", ""]]}, {"id": "2103.15595", "submitter": "Anpei Chen", "authors": "Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang,\n  Jingyi Yu and Hao Su", "title": "MVSNeRF: Fast Generalizable Radiance Field Reconstruction from\n  Multi-View Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MVSNeRF, a novel neural rendering approach that can efficiently\nreconstruct neural radiance fields for view synthesis. Unlike prior works on\nneural radiance fields that consider per-scene optimization on densely captured\nimages, we propose a generic deep neural network that can reconstruct radiance\nfields from only three nearby input views via fast network inference. Our\napproach leverages plane-swept cost volumes (widely used in multi-view stereo)\nfor geometry-aware scene reasoning, and combines this with physically based\nvolume rendering for neural radiance field reconstruction. We train our network\non real objects in the DTU dataset, and test it on three different datasets to\nevaluate its effectiveness and generalizability. Our approach can generalize\nacross scenes (even indoor scenes, completely different from our training\nscenes of objects) and generate realistic view synthesis results using only\nthree input images, significantly outperforming concurrent works on\ngeneralizable radiance field reconstruction. Moreover, if dense images are\ncaptured, our estimated radiance field representation can be easily fine-tuned;\nthis leads to fast per-scene reconstruction with higher rendering quality and\nsubstantially less optimization time than NeRF.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:15:23 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Anpei", ""], ["Xu", "Zexiang", ""], ["Zhao", "Fuqiang", ""], ["Zhang", "Xiaoshuai", ""], ["Xiang", "Fanbo", ""], ["Yu", "Jingyi", ""], ["Su", "Hao", ""]]}, {"id": "2103.15596", "submitter": "Thiago Gomes", "authors": "Thiago L. Gomes and Renato Martins and Jo\\~ao Ferreira and Rafael\n  Azevedo and Guilherme Torres and Erickson R. Nascimento", "title": "A Shape-Aware Retargeting Approach to Transfer Human Motion and\n  Appearance in Monocular Videos", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transferring human motion and appearance between videos of human actors\nremains one of the key challenges in Computer Vision. Despite the advances from\nrecent image-to-image translation approaches, there are several transferring\ncontexts where most end-to-end learning-based retargeting methods still perform\npoorly. Transferring human appearance from one actor to another is only ensured\nwhen a strict setup has been complied, which is generally built considering\ntheir training regime's specificities. In this work, we propose a shape-aware\napproach based on a hybrid image-based rendering technique that exhibits\ncompetitive visual retargeting quality compared to state-of-the-art neural\nrendering approaches. The formulation leverages the user body shape into the\nretargeting while considering physical constraints of the motion in 3D and the\n2D image domain. We also present a new video retargeting benchmark dataset\ncomposed of different videos with annotated human motions to evaluate the task\nof synthesizing people's videos, which can be used as a common base to improve\ntracking the progress in the field. The dataset and its evaluation protocols\nare designed to evaluate retargeting methods in more general and challenging\nconditions. Our method is validated in several experiments, comprising publicly\navailable videos of actors with different shapes, motion types, and camera\nsetups. The dataset and retargeting code are publicly available to the\ncommunity at: https://www.verlab.dcc.ufmg.br/retargeting-motion.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:17:41 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 15:56:27 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Gomes", "Thiago L.", ""], ["Martins", "Renato", ""], ["Ferreira", "Jo\u00e3o", ""], ["Azevedo", "Rafael", ""], ["Torres", "Guilherme", ""], ["Nascimento", "Erickson R.", ""]]}, {"id": "2103.15597", "submitter": "Sungha Choi", "authors": "Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne Kim, Seungryong Kim and\n  Jaegul Choo", "title": "RobustNet: Improving Domain Generalization in Urban-Scene Segmentation\n  via Instance Selective Whitening", "comments": "Accepted to CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Enhancing the generalization capability of deep neural networks to unseen\ndomains is crucial for safety-critical applications in the real world such as\nautonomous driving. To address this issue, this paper proposes a novel instance\nselective whitening loss to improve the robustness of the segmentation networks\nfor unseen domains. Our approach disentangles the domain-specific style and\ndomain-invariant content encoded in higher-order statistics (i.e., feature\ncovariance) of the feature representations and selectively removes only the\nstyle information causing domain shift. As shown in Fig. 1, our method provides\nreasonable predictions for (a) low-illuminated, (b) rainy, and (c) unseen\nstructures. These types of images are not included in the training dataset,\nwhere the baseline shows a significant performance drop, contrary to ours.\nBeing simple yet effective, our approach improves the robustness of various\nbackbone networks without additional computational cost. We conduct extensive\nexperiments in urban-scene segmentation and show the superiority of our\napproach to existing work. Our code is available at\nhttps://github.com/shachoi/RobustNet.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:19:37 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 10:56:17 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Choi", "Sungha", ""], ["Jung", "Sanghun", ""], ["Yun", "Huiwon", ""], ["Kim", "Joanne", ""], ["Kim", "Seungryong", ""], ["Choo", "Jaegul", ""]]}, {"id": "2103.15599", "submitter": "Yang Liu", "authors": "Yang Liu, Xingming Zhang, Jinzhao Zhou, Xin Li, Yante Li and Guoying\n  Zhao", "title": "Graph-based Facial Affect Analysis: A Review of Methods, Applications\n  and Challenges", "comments": "20 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial affect analysis (FAA) using visual signals is important in\nhuman-computer interaction. Early methods focus on extracting appearance and\ngeometry features associated with human affects, while ignoring the latent\nsemantic information among individual facial changes, leading to limited\nperformance and generalization. Recent work attempts to establish a graph-based\nrepresentation to model these semantic relationships and develop frameworks to\nleverage them for various FAA tasks. In this paper, we provide a comprehensive\nreview of graph-based FAA, including the evolution of algorithms and their\napplications. First, the FAA background knowledge is introduced, especially on\nthe role of the graph. We then discuss approaches that are widely used for\ngraph-based affective representation in literature and show a trend towards\ngraph construction. For the relational reasoning in graph-based FAA, existing\nstudies are categorized according to their usage of traditional methods or deep\nmodels, with a special emphasis on the latest graph neural networks.\nPerformance comparisons of the state-of-the-art graph-based FAA methods are\nalso summarized. Finally, we discuss the challenges and potential directions.\nAs far as we know, this is the first survey of graph-based FAA methods. Our\nfindings can serve as a reference for future research in this field.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:22:14 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 13:52:14 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 09:31:51 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 08:46:22 GMT"}, {"version": "v5", "created": "Tue, 20 Jul 2021 12:04:18 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Liu", "Yang", ""], ["Zhang", "Xingming", ""], ["Zhou", "Jinzhao", ""], ["Li", "Xin", ""], ["Li", "Yante", ""], ["Zhao", "Guoying", ""]]}, {"id": "2103.15606", "submitter": "Quan Meng", "authors": "Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming\n  He, Jingyi Yu", "title": "GNeRF: GAN-based Neural Radiance Field without Posed Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GNeRF, a framework to marry Generative Adversarial Networks\n(GAN) with Neural Radiance Field reconstruction for the complex scenarios with\nunknown and even randomly initialized camera poses. Recent NeRF-based advances\nhave gained popularity for remarkable realistic novel view synthesis. However,\nmost of them heavily rely on accurate camera poses estimation, while few recent\nmethods can only optimize the unknown camera poses in roughly forward-facing\nscenes with relatively short camera trajectories and require rough camera poses\ninitialization. Differently, our GNeRF only utilizes randomly initialized poses\nfor complex outside-in scenarios. We propose a novel two-phases end-to-end\nframework. The first phase takes the use of GANs into the new realm for coarse\ncamera poses and radiance fields jointly optimization, while the second phase\nrefines them with additional photometric loss. We overcome local minima using a\nhybrid and iterative optimization scheme. Extensive experiments on a variety of\nsynthetic and natural scenes demonstrate the effectiveness of GNeRF. More\nimpressively, our approach outperforms the baselines favorably in those scenes\nwith repeated patterns or even low textures that are regarded as extremely\nchallenging before.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:36:38 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:32:11 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Meng", "Quan", ""], ["Chen", "Anpei", ""], ["Luo", "Haimin", ""], ["Wu", "Minye", ""], ["Su", "Hao", ""], ["Xu", "Lan", ""], ["He", "Xuming", ""], ["Yu", "Jingyi", ""]]}, {"id": "2103.15619", "submitter": "Jinwoo Kim", "authors": "Jinwoo Kim, Jaehoon Yoo, Juho Lee and Seunghoon Hong", "title": "SetVAE: Learning Hierarchical Composition for Generative Modeling of\n  Set-Structured Data", "comments": "19 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling of set-structured data, such as point clouds, requires\nreasoning over local and global structures at various scales. However, adopting\nmulti-scale frameworks for ordinary sequential data to a set-structured data is\nnontrivial as it should be invariant to the permutation of its elements. In\nthis paper, we propose SetVAE, a hierarchical variational autoencoder for sets.\nMotivated by recent progress in set encoding, we build SetVAE upon attentive\nmodules that first partition the set and project the partition back to the\noriginal cardinality. Exploiting this module, our hierarchical VAE learns\nlatent variables at multiple scales, capturing coarse-to-fine dependency of the\nset elements while achieving permutation invariance. We evaluate our model on\npoint cloud generation task and achieve competitive performance to the prior\narts with substantially smaller model capacity. We qualitatively demonstrate\nthat our model generalizes to unseen set sizes and learns interesting subset\nrelations without supervision. Our implementation is available at\nhttps://github.com/jw9730/setvae.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:01:18 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kim", "Jinwoo", ""], ["Yoo", "Jaehoon", ""], ["Lee", "Juho", ""], ["Hong", "Seunghoon", ""]]}, {"id": "2103.15627", "submitter": "Dario Pavllo", "authors": "Dario Pavllo, Jonas Kohler, Thomas Hofmann, Aurelien Lucchi", "title": "Learning Generative Models of Textured 3D Meshes from Real-World Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in differentiable rendering have sparked an interest in\nlearning generative models of textured 3D meshes from image collections. These\nmodels natively disentangle pose and appearance, enable downstream applications\nin computer graphics, and improve the ability of generative models to\nunderstand the concept of image formation. Although there has been prior work\non learning such models from collections of 2D images, these approaches require\na delicate pose estimation step that exploits annotated keypoints, thereby\nrestricting their applicability to a few specific datasets. In this work, we\npropose a GAN framework for generating textured triangle meshes without relying\non such annotations. We show that the performance of our approach is on par\nwith prior work that relies on ground-truth keypoints, and more importantly, we\ndemonstrate the generality of our method by setting new baselines on a larger\nset of categories from ImageNet - for which keypoints are not available -\nwithout any class-specific hyperparameter tuning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:07:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pavllo", "Dario", ""], ["Kohler", "Jonas", ""], ["Hofmann", "Thomas", ""], ["Lucchi", "Aurelien", ""]]}, {"id": "2103.15632", "submitter": "Federico Pernici", "authors": "Federico Pernici and Matteo Bruni and Claudio Baecchi and Alberto Del\n  Bimbo", "title": "Regular Polytope Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1902.10441", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2021", "doi": "10.1109/TNNLS.2021.3056762", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are widely used as a model for classification in a large\nvariety of tasks. Typically, a learnable transformation (i.e. the classifier)\nis placed at the end of such models returning a value for each class used for\nclassification. This transformation plays an important role in determining how\nthe generated features change during the learning process. In this work, we\nargue that this transformation not only can be fixed (i.e. set as\nnon-trainable) with no loss of accuracy and with a reduction in memory usage,\nbut it can also be used to learn stationary and maximally separated embeddings.\nWe show that the stationarity of the embedding and its maximal separated\nrepresentation can be theoretically justified by setting the weights of the\nfixed classifier to values taken from the coordinate vertices of the three\nregular polytopes available in $\\mathbb{R}^d$, namely: the $d$-Simplex, the\n$d$-Cube and the $d$-Orthoplex. These regular polytopes have the maximal amount\nof symmetry that can be exploited to generate stationary features angularly\ncentered around their corresponding fixed weights. Our approach improves and\nbroadens the concept of a fixed classifier, recently proposed in\n\\cite{hoffer2018fix}, to a larger class of fixed classifier models.\nExperimental results confirm the theoretical analysis, the generalization\ncapability, the faster convergence and the improved performance of the proposed\nmethod. Code will be publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:11:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pernici", "Federico", ""], ["Bruni", "Matteo", ""], ["Baecchi", "Claudio", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2103.15662", "submitter": "Anurag Arnab", "authors": "Anurag Arnab, Chen Sun, Cordelia Schmid", "title": "Unified Graph Structured Models for Video Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate video understanding involves reasoning about the relationships\nbetween actors, objects and their environment, often over long temporal\nintervals. In this paper, we propose a message passing graph neural network\nthat explicitly models these spatio-temporal relations and can use explicit\nrepresentations of objects, when supervision is available, and implicit\nrepresentations otherwise. Our formulation generalises previous structured\nmodels for video understanding, and allows us to study how different design\nchoices in graph structure and representation affect the model's performance.\nWe demonstrate our method on two different tasks requiring relational reasoning\nin videos -- spatio-temporal action detection on AVA and UCF101-24, and video\nscene graph classification on the recent Action Genome dataset -- and achieve\nstate-of-the-art results on all three datasets. Furthermore, we show\nquantitatively and qualitatively how our method is able to more effectively\nmodel relationships between relevant entities in the scene.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:37:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Arnab", "Anurag", ""], ["Sun", "Chen", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2103.15670", "submitter": "Rulin Shao", "authors": "Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh", "title": "On the Adversarial Robustness of Visual Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the success in advancing natural language processing and\nunderstanding, transformers are expected to bring revolutionary changes to\ncomputer vision. This work provides the first and comprehensive study on the\nrobustness of vision transformers (ViTs) against adversarial perturbations.\nTested on various white-box and transfer attack settings, we find that ViTs\npossess better adversarial robustness when compared with convolutional neural\nnetworks (CNNs). We summarize the following main observations contributing to\nthe improved robustness of ViTs:\n  1) Features learned by ViTs contain less low-level information and are more\ngeneralizable, which contributes to superior robustness against adversarial\nperturbations.\n  2) Introducing convolutional or tokens-to-token blocks for learning low-level\nfeatures in ViTs can improve classification accuracy but at the cost of\nadversarial robustness.\n  3) Increasing the proportion of transformers in the model structure (when the\nmodel consists of both transformer and CNN blocks) leads to better robustness.\nBut for a pure transformer model, simply increasing the size or adding layers\ncannot guarantee a similar effect.\n  4) Pre-training on larger datasets does not significantly improve adversarial\nrobustness though it is critical for training ViTs.\n  5) Adversarial training is also applicable to ViT for training robust models.\n  Furthermore, feature visualization and frequency analysis are conducted for\nexplanation. The results show that ViTs are less sensitive to high-frequency\nperturbations than CNNs and there is a high correlation between how well the\nmodel learns low-level features and its robustness against different\nfrequency-based perturbations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:48:24 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Shao", "Rulin", ""], ["Shi", "Zhouxing", ""], ["Yi", "Jinfeng", ""], ["Chen", "Pin-Yu", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2103.15679", "submitter": "Hila Chefer", "authors": "Hila Chefer, Shir Gur, and Lior Wolf", "title": "Generic Attention-model Explainability for Interpreting Bi-Modal and\n  Encoder-Decoder Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformers are increasingly dominating multi-modal reasoning tasks, such as\nvisual question answering, achieving state-of-the-art results thanks to their\nability to contextualize information using the self-attention and co-attention\nmechanisms. These attention modules also play a role in other computer vision\ntasks including object detection and image segmentation. Unlike Transformers\nthat only use self-attention, Transformers with co-attention require to\nconsider multiple attention maps in parallel in order to highlight the\ninformation that is relevant to the prediction in the model's input. In this\nwork, we propose the first method to explain prediction by any\nTransformer-based architecture, including bi-modal Transformers and\nTransformers with co-attentions. We provide generic solutions and apply these\nto the three most commonly used of these architectures: (i) pure\nself-attention, (ii) self-attention combined with co-attention, and (iii)\nencoder-decoder attention. We show that our method is superior to all existing\nmethods which are adapted from single modality explainability.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:03:11 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chefer", "Hila", ""], ["Gur", "Shir", ""], ["Wolf", "Lior", ""]]}, {"id": "2103.15683", "submitter": "Peng Yi", "authors": "Peng Yi and Zhongyuan Wang and Kui Jiang and Junjun Jiang and Tao Lu\n  and Xin Tian and Jiayi Ma", "title": "Omniscient Video Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent video super-resolution (SR) methods either adopt an iterative\nmanner to deal with low-resolution (LR) frames from a temporally sliding\nwindow, or leverage the previously estimated SR output to help reconstruct the\ncurrent frame recurrently. A few studies try to combine these two structures to\nform a hybrid framework but have failed to give full play to it. In this paper,\nwe propose an omniscient framework to not only utilize the preceding SR output,\nbut also leverage the SR outputs from the present and future. The omniscient\nframework is more generic because the iterative, recurrent and hybrid\nframeworks can be regarded as its special cases. The proposed omniscient\nframework enables a generator to behave better than its counterparts under\nother frameworks. Abundant experiments on public datasets show that our method\nis superior to the state-of-the-art methods in objective metrics, subjective\nvisual effects and complexity. Our code will be made public.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:09:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yi", "Peng", ""], ["Wang", "Zhongyuan", ""], ["Jiang", "Kui", ""], ["Jiang", "Junjun", ""], ["Lu", "Tao", ""], ["Tian", "Xin", ""], ["Ma", "Jiayi", ""]]}, {"id": "2103.15684", "submitter": "Anouk van Diepen", "authors": "A. van Diepen, T. H. G. F. Bakkes, A. J. R. De Bie, S. Turco, R. A.\n  Bouwman, P. H. Woerlee, M. Mischi", "title": "A Model-Based Approach to Synthetic Data Set Generation for\n  Patient-Ventilator Waveforms for Machine Learning and Educational Use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although mechanical ventilation is a lifesaving intervention in the ICU, it\nhas harmful side-effects, such as barotrauma and volutrauma. These harms can\noccur due to asynchronies. Asynchronies are defined as a mismatch between the\nventilator timing and patient respiratory effort. Automatic detection of these\nasynchronies, and subsequent feedback, would improve lung ventilation and\nreduce the probability of lung damage. Neural networks to detect asynchronies\nprovide a promising new approach but require large annotated data sets, which\nare difficult to obtain and require complex monitoring of inspiratory effort.\nIn this work, we propose a model-based approach to generate a synthetic data\nset for machine learning and educational use by extending an existing lung\nmodel with a first-order ventilator model. The physiological nature of the\nderived lung model allows adaptation to various disease archetypes, resulting\nin a diverse data set. We generated a synthetic data set using 9 different\npatient archetypes, which are derived from measurements in the literature. The\nmodel and synthetic data quality have been verified by comparison with clinical\ndata, review by a clinical expert, and an artificial intelligence model that\nwas trained on experimental data. The evaluation showed it was possible to\ngenerate patient-ventilator waveforms including asynchronies that have the most\nimportant features of experimental patient-ventilator waveforms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:10:17 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 12:05:08 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["van Diepen", "A.", ""], ["Bakkes", "T. H. G. F.", ""], ["De Bie", "A. J. R.", ""], ["Turco", "S.", ""], ["Bouwman", "R. A.", ""], ["Woerlee", "P. H.", ""], ["Mischi", "M.", ""]]}, {"id": "2103.15685", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng and Yi Yang", "title": "Adaptive Boosting for Domain Adaptation: Towards Robust Predictions in\n  Scene Segmentation", "comments": "10 pages, 7 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is to transfer the shared knowledge learned from the source\ndomain to a new environment, i.e., target domain. One common practice is to\ntrain the model on both labeled source-domain data and unlabeled target-domain\ndata. Yet the learned models are usually biased due to the strong supervision\nof the source domain. Most researchers adopt the early-stopping strategy to\nprevent over-fitting, but when to stop training remains a challenging problem\nsince the lack of the target-domain validation set. In this paper, we propose\none efficient bootstrapping method, called Adaboost Student, explicitly\nlearning complementary models during training and liberating users from\nempirical early stopping. Adaboost Student combines the deep model learning\nwith the conventional training strategy, i.e., adaptive boosting, and enables\ninteractions between learned models and the data sampler. We adopt one adaptive\ndata sampler to progressively facilitate learning on hard samples and aggregate\n\"weak\" models to prevent over-fitting. Extensive experiments show that (1)\nWithout the need to worry about the stopping time, AdaBoost Student provides\none robust solution by efficient complementary model learning during training.\n(2) AdaBoost Student is orthogonal to most domain adaptation methods, which can\nbe combined with existing approaches to further improve the state-of-the-art\nperformance. We have achieved competitive results on three widely-used scene\nsegmentation domain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:12:58 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 04:03:28 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zheng", "Zhedong", ""], ["Yang", "Yi", ""]]}, {"id": "2103.15686", "submitter": "Kecheng Zheng", "authors": "Rui Zhao, Kecheng Zheng, Zheng-Jun Zha, Hongtao Xie and Jiebo Luo", "title": "Memory Enhanced Embedding Learning for Cross-Modal Video-Text Retrieval", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-modal video-text retrieval, a challenging task in the field of vision\nand language, aims at retrieving corresponding instance giving sample from\neither modality. Existing approaches for this task all focus on how to design\nencoding model through a hard negative ranking loss, leaving two key problems\nunaddressed during this procedure. First, in the training stage, only a\nmini-batch of instance pairs is available in each iteration. Therefore, this\nkind of hard negatives is locally mined inside a mini-batch while ignoring the\nglobal negative samples among the dataset. Second, there are many text\ndescriptions for one video and each text only describes certain local features\nof a video. Previous works for this task did not consider to fuse the multiply\ntexts corresponding to a video during the training. In this paper, to solve the\nabove two problems, we propose a novel memory enhanced embedding learning\n(MEEL) method for videotext retrieval. To be specific, we construct two kinds\nof memory banks respectively: cross-modal memory module and text center memory\nmodule. The cross-modal memory module is employed to record the instance\nembeddings of all the datasets for global negative mining. To avoid the fast\nevolving of the embedding in the memory bank during training, we utilize a\nmomentum encoder to update the features by a moving-averaging strategy. The\ntext center memory module is designed to record the center information of the\nmultiple textual instances corresponding to a video, and aims at bridging these\ntextual instances together. Extensive experimental results on two challenging\nbenchmarks, i.e., MSR-VTT and VATEX, demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:15:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Rui", ""], ["Zheng", "Kecheng", ""], ["Zha", "Zheng-Jun", ""], ["Xie", "Hongtao", ""], ["Luo", "Jiebo", ""]]}, {"id": "2103.15691", "submitter": "Anurag Arnab", "authors": "Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario\n  Lu\\v{c}i\\'c, Cordelia Schmid", "title": "ViViT: A Video Vision Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present pure-transformer based models for video classification, drawing\nupon the recent success of such models in image classification. Our model\nextracts spatio-temporal tokens from the input video, which are then encoded by\na series of transformer layers. In order to handle the long sequences of tokens\nencountered in video, we propose several, efficient variants of our model which\nfactorise the spatial- and temporal-dimensions of the input. Although\ntransformer-based models are known to only be effective when large training\ndatasets are available, we show how we can effectively regularise the model\nduring training and leverage pretrained image models to be able to train on\ncomparatively small datasets. We conduct thorough ablation studies, and achieve\nstate-of-the-art results on multiple video classification benchmarks including\nKinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in\nTime, outperforming prior methods based on deep 3D convolutional networks. To\nfacilitate further research, we will release code and models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:27:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Arnab", "Anurag", ""], ["Dehghani", "Mostafa", ""], ["Heigold", "Georg", ""], ["Sun", "Chen", ""], ["Lu\u010di\u0107", "Mario", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2103.15706", "submitter": "Ayan Kumar Bhunia", "authors": "Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xiang, Yi-Zhe Song", "title": "StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval", "comments": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sketch-based image retrieval (SBIR) is a cross-modal matching problem which\nis typically solved by learning a joint embedding space where the semantic\ncontent shared between photo and sketch modalities are preserved. However, a\nfundamental challenge in SBIR has been largely ignored so far, that is,\nsketches are drawn by humans and considerable style variations exist amongst\ndifferent users. An effective SBIR model needs to explicitly account for this\nstyle diversity, crucially, to generalise to unseen user styles. To this end, a\nnovel style-agnostic SBIR model is proposed. Different from existing models, a\ncross-modal variational autoencoder (VAE) is employed to explicitly disentangle\neach sketch into a semantic content part shared with the corresponding photo,\nand a style part unique to the sketcher. Importantly, to make our model\ndynamically adaptable to any unseen user styles, we propose to meta-train our\ncross-modal VAE by adding two style-adaptive components: a set of feature\ntransformation layers to its encoder and a regulariser to the disentangled\nsemantic content latent code. With this meta-learning framework, our model can\nnot only disentangle the cross-modal shared semantic content for SBIR, but can\nadapt the disentanglement to any unseen user style as well, making the SBIR\nmodel truly style-agnostic. Extensive experiments show that our style-agnostic\nmodel yields state-of-the-art performance for both category-level and\ninstance-level SBIR.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:44:19 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 10:31:24 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Sain", "Aneeshan", ""], ["Bhunia", "Ayan Kumar", ""], ["Yang", "Yongxin", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2103.15718", "submitter": "Tyler Scott", "authors": "Tyler R. Scott and Andrew C. Gallagher and Michael C. Mozer", "title": "von Mises-Fisher Loss: An Exploration of Embedding Geometries for\n  Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has argued that classification losses utilizing softmax\ncross-entropy are superior not only for fixed-set classification tasks, but\nalso by outperforming losses developed specifically for open-set tasks\nincluding few-shot learning and retrieval. Softmax classifiers have been\nstudied using different embedding geometries -- Euclidean, hyperbolic, and\nspherical -- and claims have been made about the superiority of one or another,\nbut they have not been systematically compared with careful controls. We\nconduct an empirical investigation of embedding geometry on softmax losses for\na variety of fixed-set classification and image retrieval tasks. An interesting\nproperty observed for the spherical losses lead us to propose a probabilistic\nclassifier based on the von Mises-Fisher distribution, and we show that it is\ncompetitive with state-of-the-art methods while producing improved\nout-of-the-box calibration. We provide guidance regarding the trade-offs\nbetween losses and how to choose among them.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:07:09 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 15:10:23 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Scott", "Tyler R.", ""], ["Gallagher", "Andrew C.", ""], ["Mozer", "Michael C.", ""]]}, {"id": "2103.15726", "submitter": "Fei Yang", "authors": "Fei Yang, Luis Herranz, Yongmei Cheng, Mikhail G. Mozerov", "title": "Slimmable Compressive Autoencoders for Practical Neural Image\n  Compression", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural image compression leverages deep neural networks to outperform\ntraditional image codecs in rate-distortion performance. However, the resulting\nmodels are also heavy, computationally demanding and generally optimized for a\nsingle rate, limiting their practical use. Focusing on practical image\ncompression, we propose slimmable compressive autoencoders (SlimCAEs), where\nrate (R) and distortion (D) are jointly optimized for different capacities.\nOnce trained, encoders and decoders can be executed at different capacities,\nleading to different rates and complexities. We show that a successful\nimplementation of SlimCAEs requires suitable capacity-specific RD tradeoffs.\nOur experiments show that SlimCAEs are highly flexible models that provide\nexcellent rate-distortion performance, variable rate, and dynamic adjustment of\nmemory, computational cost and latency, thus addressing the main requirements\nof practical image compression.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:12:04 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yang", "Fei", ""], ["Herranz", "Luis", ""], ["Cheng", "Yongmei", ""], ["Mozerov", "Mikhail G.", ""]]}, {"id": "2103.15727", "submitter": "Dina Bashkirova", "authors": "Dina Bashkirova, Ben Usman and Kate Saenko", "title": "Evaluation of Correctness in Unsupervised Many-to-Many Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an input image from a source domain and a \"guidance\" image from a\ntarget domain, unsupervised many-to-many image-to-image (UMMI2I) translation\nmethods seek to generate a plausible example from the target domain that\npreserves domain-invariant information of the input source image and inherits\nthe domain-specific information from the guidance image. For example, when\ntranslating female faces to male faces, the generated male face should have the\nsame expression, pose and hair color as the input female image, and the same\nfacial hairstyle and other male-specific attributes as the guidance male image.\nCurrent state-of-the art UMMI2I methods generate visually pleasing images, but,\nsince for most pairs of real datasets we do not know which attributes are\ndomain-specific and which are domain-invariant, the semantic correctness of\nexisting approaches has not been quantitatively evaluated yet. In this paper,\nwe propose a set of benchmarks and metrics for the evaluation of semantic\ncorrectness of UMMI2I methods. We provide an extensive study how well the\nexisting state-of-the-art UMMI2I translation methods preserve domain-invariant\nand manipulate domain-specific attributes, and discuss the trade-offs shared by\nall methods, as well as how different architectural choices affect various\naspects of semantic correctness.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:13:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bashkirova", "Dina", ""], ["Usman", "Ben", ""], ["Saenko", "Kate", ""]]}, {"id": "2103.15734", "submitter": "Xiangtai Li", "authors": "Hao He, Xiangtai Li, Guangliang Cheng, Jianping Shi, Yunhai Tong,\n  Gaofeng Meng, V\\'eronique Prinet, Lubin Weng", "title": "Enhanced Boundary Learning for Glass-like Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Glass-like objects such as windows, bottles, and mirrors exist widely in the\nreal world. Sensing these objects has many applications, including robot\nnavigation and grasping. However, this task is very challenging due to the\narbitrary scenes behind glass-like objects. This paper aims to solve the\nglass-like object segmentation problem via enhanced boundary learning. In\nparticular, we first propose a novel refined differential module for generating\nfiner boundary cues. Then an edge-aware point-based graph convolution network\nmodule is proposed to model the global shape representation along the boundary.\nBoth modules are lightweight and effective, which can be embedded into various\nsegmentation models. Moreover, we use these two modules to design a decoder to\nget accurate segmentation results, especially on the boundary. Extensive\nexperiments on three recent glass-like object segmentation datasets, including\nTrans10k, MSD, and GDD, show that our approach establishes new state-of-the-art\nperformances. We also offer the generality and superiority of our approach\ncompared with recent methods on three general segmentation datasets, including\nCityscapes, BDD, and COCO Stuff. Code and models will be available at\n(\\url{https://github.com/hehao13/EBLNet})\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:18:57 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["He", "Hao", ""], ["Li", "Xiangtai", ""], ["Cheng", "Guangliang", ""], ["Shi", "Jianping", ""], ["Tong", "Yunhai", ""], ["Meng", "Gaofeng", ""], ["Prinet", "V\u00e9ronique", ""], ["Weng", "Lubin", ""]]}, {"id": "2103.15756", "submitter": "Baohua Sun", "authors": "Baohua Sun, Tao Zhang, Jiapeng Su, Hao Sha", "title": "GnetDet: Object Detection Optimized on a 224mW CNN Accelerator Chip at\n  the Speed of 106FPS", "comments": "5 pages, 2 figures, and 1 table. arXiv admin note: text overlap with\n  arXiv:2101.10444", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is widely used on embedded devices. With the wide\navailability of CNN (Convolutional Neural Networks) accelerator chips, the\nobject detection applications are expected to run with low power consumption,\nand high inference speed. In addition, the CPU load is expected to be as low as\npossible for a CNN accelerator chip working as a co-processor with a host CPU.\nIn this paper, we optimize the object detection model on the CNN accelerator\nchip by minimizing the CPU load. The resulting model is called GnetDet. The\nexperimental result shows that the GnetDet model running on a 224mW chip\nachieves the speed of 106FPS with excellent accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 06:16:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sun", "Baohua", ""], ["Zhang", "Tao", ""], ["Su", "Jiapeng", ""], ["Sha", "Hao", ""]]}, {"id": "2103.15767", "submitter": "Varun Sundar", "authors": "Varun Sundar, Rajat Vadiraj Dwaraknath", "title": "[Reproducibility Report] Rigging the Lottery: Making All Tickets Winners", "comments": "Under review at ML Reproducibility Challenge 2020. Code available at\n  https://github.com/varun19299/rigl-reproducibility. Training plots and other\n  logs available at https://wandb.ai/ml-reprod-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\textit{RigL}$, a sparse training algorithm, claims to directly train sparse\nnetworks that match or exceed the performance of existing dense-to-sparse\ntraining techniques (such as pruning) for a fixed parameter count and compute\nbudget. We implement $\\textit{RigL}$ from scratch in Pytorch and reproduce its\nperformance on CIFAR-10 within 0.1% of the reported value. On both\nCIFAR-10/100, the central claim holds -- given a fixed training budget,\n$\\textit{RigL}$ surpasses existing dynamic-sparse training methods over a range\nof target sparsities. By training longer, the performance can match or exceed\niterative pruning, while consuming constant FLOPs throughout training. We also\nshow that there is little benefit in tuning $\\textit{RigL}$'s hyper-parameters\nfor every sparsity, initialization pair -- the reference choice of\nhyperparameters is often close to optimal performance. Going beyond the\noriginal paper, we find that the optimal initialization scheme depends on the\ntraining constraint. While the Erdos-Renyi-Kernel distribution outperforms the\nUniform distribution for a fixed parameter count, for a fixed FLOP count, the\nlatter performs better. Finally, redistributing layer-wise sparsity while\ntraining can bridge the performance gap between the two initialization schemes,\nbut increases computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:01:11 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 03:15:56 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sundar", "Varun", ""], ["Dwaraknath", "Rajat Vadiraj", ""]]}, {"id": "2103.15783", "submitter": "Sam Polk", "authors": "Sam L. Polk and James M. Murphy", "title": "Multiscale Clustering of Hyperspectral Images Through Spectral-Spatial\n  Diffusion Geometry", "comments": "(6 pages, 2 figures). To appear in Proceedings of IEEE IGARSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering algorithms partition a dataset into groups of similar points. The\nprimary contribution of this article is the Multiscale Spatially-Regularized\nDiffusion Learning (M-SRDL) clustering algorithm, which uses\nspatially-regularized diffusion distances to efficiently and accurately learn\nmultiple scales of latent structure in hyperspectral images (HSI). The M-SRDL\nclustering algorithm extracts clusterings at many scales from an HSI and\noutputs these clusterings' variation of information-barycenter as an exemplar\nfor all underlying cluster structure. We show that incorporating spatial\nregularization into a multiscale clustering framework corresponds to smoother\nand more coherent clusters when applied to HSI data and leads to more accurate\nclustering labels.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:24:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Polk", "Sam L.", ""], ["Murphy", "James M.", ""]]}, {"id": "2103.15792", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units\n  and a Unified Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Affect recognition based on subjects' facial expressions has been a topic of\nmajor research in the attempt to generate machines that can understand the way\nsubjects feel, act and react. In the past, due to the unavailability of large\namounts of data captured in real-life situations, research has mainly focused\non controlled environments. However, recently, social media and platforms have\nbeen widely used. Moreover, deep learning has emerged as a means to solve\nvisual analysis and recognition problems. This paper exploits these advances\nand presents significant contributions for affect analysis and recognition\nin-the-wild. Affect analysis and recognition can be seen as a dual knowledge\ngeneration problem, involving: i) creation of new, large and rich in-the-wild\ndatabases and ii) design and training of novel deep neural architectures that\nare able to analyse affect over these databases and to successfully generalise\ntheir performance on other datasets. The paper focuses on large in-the-wild\ndatabases, i.e., Aff-Wild and Aff-Wild2 and presents the design of two classes\nof deep neural networks trained with these databases. The first class refers to\nuni-task affect recognition, focusing on prediction of the valence and arousal\ndimensional variables. The second class refers to estimation of all main\nbehavior tasks, i.e. valence-arousal prediction; categorical emotion\nclassification in seven basic facial expressions; facial Action Unit detection.\nA novel multi-task and holistic framework is presented which is able to jointly\nlearn and effectively generalize and perform affect recognition over all\nexisting in-the-wild databases. Large experimental studies illustrate the\nachieved performance improvement over the existing state-of-the-art in affect\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:36:20 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2103.15796", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland and Dhruv Mahajan", "title": "Adaptive Methods for Real-World Domain Generalization", "comments": "To appear as an oral presentation in IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR), 2021. v2 corrects double printing of\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariant approaches have been remarkably successful in tackling the problem\nof domain generalization, where the objective is to perform inference on data\ndistributions different from those used in training. In our work, we\ninvestigate whether it is possible to leverage domain information from the\nunseen test samples themselves. We propose a domain-adaptive approach\nconsisting of two steps: a) we first learn a discriminative domain embedding\nfrom unsupervised training examples, and b) use this domain embedding as\nsupplementary information to build a domain-adaptive model, that takes both the\ninput as well as its domain into account while making predictions. For unseen\ndomains, our method simply uses few unlabelled test examples to construct the\ndomain embedding. This enables adaptive classification on any unseen domain.\nOur approach achieves state-of-the-art performance on various domain\ngeneralization benchmarks. In addition, we introduce the first real-world,\nlarge-scale domain generalization benchmark, Geo-YFCC, containing 1.1M samples\nover 40 training, 7 validation, and 15 test domains, orders of magnitude larger\nthan prior work. We show that the existing approaches either do not scale to\nthis dataset or underperform compared to the simple baseline of training a\nmodel on the union of data from all training domains. In contrast, our approach\nachieves a significant improvement.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:44:35 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 01:36:47 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Ramanathan", "Vignesh", ""], ["Pentland", "Alex", ""], ["Mahajan", "Dhruv", ""]]}, {"id": "2103.15798", "submitter": "Mikhail Khodak", "authors": "Nicholas Roberts and Mikhail Khodak and Tri Dao and Liam Li and\n  Christopher R\\'e and Ameet Talwalkar", "title": "Rethinking Neural Operations for Diverse Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal of neural architecture search (NAS) is to automate-away the\ndesign of neural networks on new tasks in under-explored domains. Motivated by\nthis broader vision for NAS, we study the problem of enabling users to discover\nthe right neural operations given data from their specific domain. We introduce\na search space of neural operations called XD-Operations that mimic the\ninductive bias of standard multichannel convolutions while being much more\nexpressive: we prove that XD-operations include many named operations across\nseveral application areas. Starting with any standard backbone network such as\nLeNet or ResNet, we show how to transform it into an architecture search space\nover XD-operations and how to traverse the space using a simple weight-sharing\nscheme. On a diverse set of applications--image classification, solving partial\ndifferential equations (PDEs), and sequence modeling--our approach consistently\nyields models with lower error than baseline networks and sometimes even lower\nerror than expert-designed domain-specific approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:50:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Roberts", "Nicholas", ""], ["Khodak", "Mikhail", ""], ["Dao", "Tri", ""], ["Li", "Liam", ""], ["R\u00e9", "Christopher", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "2103.15808", "submitter": "Bin Xiao", "authors": "Haiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang\n  Dai and Lu Yuan and Lei Zhang", "title": "CvT: Introducing Convolutions to Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present in this paper a new architecture, named Convolutional vision\nTransformer (CvT), that improves Vision Transformer (ViT) in performance and\nefficiency by introducing convolutions into ViT to yield the best of both\ndesigns. This is accomplished through two primary modifications: a hierarchy of\nTransformers containing a new convolutional token embedding, and a\nconvolutional Transformer block leveraging a convolutional projection. These\nchanges introduce desirable properties of convolutional neural networks (CNNs)\nto the ViT architecture (\\ie shift, scale, and distortion invariance) while\nmaintaining the merits of Transformers (\\ie dynamic attention, global context,\nand better generalization). We validate CvT by conducting extensive\nexperiments, showing that this approach achieves state-of-the-art performance\nover other Vision Transformers and ResNets on ImageNet-1k, with fewer\nparameters and lower FLOPs. In addition, performance gains are maintained when\npretrained on larger datasets (\\eg ImageNet-22k) and fine-tuned to downstream\ntasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of\n87.7\\% on the ImageNet-1k val set. Finally, our results show that the\npositional encoding, a crucial component in existing Vision Transformers, can\nbe safely removed in our model, simplifying the design for higher resolution\nvision tasks. Code will be released at \\url{https://github.com/leoxiaobin/CvT}.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:58:22 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wu", "Haiping", ""], ["Xiao", "Bin", ""], ["Codella", "Noel", ""], ["Liu", "Mengchen", ""], ["Dai", "Xiyang", ""], ["Yuan", "Lu", ""], ["Zhang", "Lei", ""]]}, {"id": "2103.15812", "submitter": "Xingzhe He", "authors": "Xingzhe He, Bastian Wandt, Helge Rhodin", "title": "LatentKeypointGAN: Controlling GANs via Latent Keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generative adversarial networks (GANs) have attained photo-realistic quality.\nHowever, it remains an open challenge of how to best control the image content.\nWe introduce LatentKeypointGAN, a two-stage GAN that is trained end-to-end on\nthe classical GAN objective yet internally conditioned on a set of sparse\nkeypoints with associated appearance embeddings that respectively control the\nposition and style of the generated objects and their parts. A major difficulty\nthat we address with suitable network architectures and training schemes is\ndisentangling the image into spatial and appearance factors without any\nsupervision signals of either nor domain knowledge. We demonstrate that\nLatentKeypointGAN provides an interpretable latent space that can be used to\nre-arrange the generated images by re-positioning and exchanging keypoint\nembeddings, such as combining the eyes, nose, and mouth from different images\nfor generating portraits. In addition, the explicit generation of keypoints and\nmatching images enables a new, GAN-based methodology for unsupervised keypoint\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:59:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["He", "Xingzhe", ""], ["Wandt", "Bastian", ""], ["Rhodin", "Helge", ""]]}, {"id": "2103.15813", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani, Abhinav Gupta", "title": "PixelTransformer: Sample Conditioned Signal Generation", "comments": "Project page: https://shubhtuls.github.io/PixelTransformer/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative model that can infer a distribution for the\nunderlying spatial signal conditioned on sparse samples e.g. plausible images\ngiven a few observed pixels. In contrast to sequential autoregressive\ngenerative models, our model allows conditioning on arbitrary samples and can\nanswer distributional queries for any location. We empirically validate our\napproach across three image datasets and show that we learn to generate diverse\nand meaningful samples, with the distribution variance reducing given more\nobserved pixels. We also show that our approach is applicable beyond images and\ncan allow generating other types of spatial outputs e.g. polynomials, 3D\nshapes, and videos.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:59:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2103.15814", "submitter": "Yue Gao", "authors": "Yue Gao, Fangyun Wei, Jianmin Bao, Shuyang Gu, Dong Chen, Fang Wen,\n  Zhouhui Lian", "title": "High-Fidelity and Arbitrary Face Editing", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cycle consistency is widely used for face editing. However, we observe that\nthe generator tends to find a tricky way to hide information from the original\nimage to satisfy the constraint of cycle consistency, making it impossible to\nmaintain the rich details (e.g., wrinkles and moles) of non-editing areas. In\nthis work, we propose a simple yet effective method named HifaFace to address\nthe above-mentioned problem from two perspectives. First, we relieve the\npressure of the generator to synthesize rich details by directly feeding the\nhigh-frequency information of the input image into the end of the generator.\nSecond, we adopt an additional discriminator to encourage the generator to\nsynthesize rich details. Specifically, we apply wavelet transformation to\ntransform the image into multi-frequency domains, among which the\nhigh-frequency parts can be used to recover the rich details. We also notice\nthat a fine-grained and wider-range control for the attribute is of great\nimportance for face editing. To achieve this goal, we propose a novel attribute\nregression loss. Powered by the proposed framework, we achieve high-fidelity\nand arbitrary face editing, outperforming other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:59:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gao", "Yue", ""], ["Wei", "Fangyun", ""], ["Bao", "Jianmin", ""], ["Gu", "Shuyang", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""], ["Lian", "Zhouhui", ""]]}, {"id": "2103.15858", "submitter": "Yuyin Zhou", "authors": "Junfei Xiao, Lequan Yu, Lei Xing, Alan Yuille, Yuyin Zhou", "title": "DualNorm-UNet: Incorporating Global and Local Statistics for Robust\n  Medical Image Segmentation", "comments": "code available at https://github.com/lambert-x/DualNorm-Unet", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) is one of the key components for accelerating\nnetwork training, and has been widely adopted in the medical image analysis\nfield. However, BN only calculates the global statistics at the batch level,\nand applies the same affine transformation uniformly across all spatial\ncoordinates, which would suppress the image contrast of different semantic\nstructures. In this paper, we propose to incorporate the semantic class\ninformation into normalization layers, so that the activations corresponding to\ndifferent regions (i.e., classes) can be modulated differently. We thus develop\na novel DualNorm-UNet, to concurrently incorporate both global image-level\nstatistics and local region-wise statistics for network normalization.\nSpecifically, the local statistics are integrated by adaptively modulating the\nactivations along different class regions via the learned semantic masks in the\nnormalization layer. Compared with existing methods, our approach exploits\nsemantic knowledge at normalization and yields more discriminative features for\nrobust segmentation results. More importantly, our network demonstrates\nsuperior abilities in capturing domain-invariant information from multiple\ndomains (institutions) of medical data. Extensive experiments show that our\nproposed DualNorm-UNet consistently improves the performance on various\nsegmentation tasks, even in the face of more complex and variable data\ndistributions. Code is available at https://github.com/lambert-x/DualNorm-Unet.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:09:56 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Xiao", "Junfei", ""], ["Yu", "Lequan", ""], ["Xing", "Lei", ""], ["Yuille", "Alan", ""], ["Zhou", "Yuyin", ""]]}, {"id": "2103.15875", "submitter": "Shuaifeng Zhi", "authors": "Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, Andrew J. Davison", "title": "In-Place Scene Labelling and Understanding with Implicit Scene\n  Representation", "comments": "Project page with more videos:\n  https://shuaifengzhi.com/Semantic-NeRF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic labelling is highly correlated with geometry and radiance\nreconstruction, as scene entities with similar shape and appearance are more\nlikely to come from similar classes. Recent implicit neural reconstruction\ntechniques are appealing as they do not require prior training data, but the\nsame fully self-supervised approach is not possible for semantics because\nlabels are human-defined properties.\n  We extend neural radiance fields (NeRF) to jointly encode semantics with\nappearance and geometry, so that complete and accurate 2D semantic labels can\nbe achieved using a small amount of in-place annotations specific to the scene.\nThe intrinsic multi-view consistency and smoothness of NeRF benefit semantics\nby enabling sparse labels to efficiently propagate. We show the benefit of this\napproach when labels are either sparse or very noisy in room-scale scenes. We\ndemonstrate its advantageous properties in various interesting applications\nsuch as an efficient scene labelling tool, novel semantic view synthesis, label\ndenoising, super-resolution, label interpolation and multi-view semantic label\nfusion in visual semantic mapping systems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:30:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhi", "Shuaifeng", ""], ["Laidlow", "Tristan", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2103.15876", "submitter": "Lele Chen", "authors": "Lele Chen, Chen Cao, Fernando De la Torre, Jason Saragih, Chenliang\n  Xu, Yaser Sheikh", "title": "High-fidelity Face Tracking for AR/VR via Deep Lighting Adaptation", "comments": "The paper is accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  3D video avatars can empower virtual communications by providing compression,\nprivacy, entertainment, and a sense of presence in AR/VR. Best 3D\nphoto-realistic AR/VR avatars driven by video, that can minimize uncanny\neffects, rely on person-specific models. However, existing person-specific\nphoto-realistic 3D models are not robust to lighting, hence their results\ntypically miss subtle facial behaviors and cause artifacts in the avatar. This\nis a major drawback for the scalability of these models in communication\nsystems (e.g., Messenger, Skype, FaceTime) and AR/VR. This paper addresses\nprevious limitations by learning a deep learning lighting model, that in\ncombination with a high-quality 3D face tracking algorithm, provides a method\nfor subtle and robust facial motion transfer from a regular video to a 3D\nphoto-realistic avatar. Extensive experimental validation and comparisons to\nother state-of-the-art methods demonstrate the effectiveness of the proposed\nframework in real-world scenarios with variability in pose, expression, and\nillumination. Please visit https://www.youtube.com/watch?v=dtz1LgZR8cc for more\nresults. Our project page can be found at\nhttps://www.cs.rochester.edu/u/lchen63.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:33:49 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Lele", ""], ["Cao", "Chen", ""], ["De la Torre", "Fernando", ""], ["Saragih", "Jason", ""], ["Xu", "Chenliang", ""], ["Sheikh", "Yaser", ""]]}, {"id": "2103.15890", "submitter": "Hanlin Zhang", "authors": "Yi-Fan Zhang, Hanlin Zhang, Zhang Zhang, Da Li, Zhen Jia, Liang Wang,\n  Tieniu Tan", "title": "Learning Domain Invariant Representations for Generalizable Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizable person Re-Identification (ReID) has attracted growing attention\nin recent computer vision community. In this work, we construct a structural\ncausal model among identity labels, identity-specific factors (clothes/shoes\ncolor etc), and domain-specific factors (background, viewpoints etc). According\nto the causal analysis, we propose a novel Domain Invariant Representation\nLearning for generalizable person Re-Identification (DIR-ReID) framework.\nSpecifically, we first propose to disentangle the identity-specific and\ndomain-specific feature spaces, based on which we propose an effective\nalgorithmic implementation for backdoor adjustment, essentially serving as a\ncausal intervention towards the SCM. Extensive experiments have been conducted,\nshowing that DIR-ReID outperforms state-of-the-art methods on large-scale\ndomain generalization ReID benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:59:48 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 08:58:04 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Yi-Fan", ""], ["Zhang", "Hanlin", ""], ["Zhang", "Zhang", ""], ["Li", "Da", ""], ["Jia", "Zhen", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "2103.15897", "submitter": "Matthew Ciolino", "authors": "Josh Kalin, David Noever, Matthew Ciolino, Dominick Hambrick, Gerry\n  Dozier", "title": "Automating Defense Against Adversarial Attacks: Discovery of\n  Vulnerabilities and Application of Multi-INT Imagery to Protect Deployed\n  Models", "comments": "SPIE 2021, 8 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image classification is a common step in image recognition for machine\nlearning in overhead applications. When applying popular model architectures\nlike MobileNetV2, known vulnerabilities expose the model to counter-attacks,\neither mislabeling a known class or altering box location. This work proposes\nan automated approach to defend these models. We evaluate the use of\nmulti-spectral image arrays and ensemble learners to combat adversarial\nattacks. The original contribution demonstrates the attack, proposes a remedy,\nand automates some key outcomes for protecting the model's predictions against\nadversaries. In rough analogy to defending cyber-networks, we combine\ntechniques from both offensive (\"red team\") and defensive (\"blue team\")\napproaches, thus generating a hybrid protective outcome (\"green team\"). For\nmachine learning, we demonstrate these methods with 3-color channels plus\ninfrared for vehicles. The outcome uncovers vulnerabilities and corrects them\nwith supplemental data inputs commonly found in overhead cases particularly.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:07:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kalin", "Josh", ""], ["Noever", "David", ""], ["Ciolino", "Matthew", ""], ["Hambrick", "Dominick", ""], ["Dozier", "Gerry", ""]]}, {"id": "2103.15898", "submitter": "Loris Nanni", "authors": "Loris Nanni, Gianluca Maguolo, Sheryl Brahnam, Michelangelo Paci", "title": "Comparison of different convolutional neural network activation\n  functions and methods for building ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, much attention has been devoted to finding highly efficient and\npowerful activation functions for CNN layers. Because activation functions\ninject different nonlinearities between layers that affect performance, varying\nthem is one method for building robust ensembles of CNNs. The objective of this\nstudy is to examine the performance of CNN ensembles made with different\nactivation functions, including six new ones presented here: 2D Mexican ReLU,\nTanELU, MeLU+GaLU, Symmetric MeLU, Symmetric GaLU, and Flexible MeLU. The\nhighest performing ensemble was built with CNNs having different activation\nlayers that randomly replaced the standard ReLU. A comprehensive evaluation of\nthe proposed approach was conducted across fifteen biomedical data sets\nrepresenting various classification tasks. The proposed method was tested on\ntwo basic CNN architectures: Vgg16 and ResNet50. Results demonstrate the\nsuperiority in performance of this approach. The MATLAB source code for this\nstudy will be available at https://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:12:41 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 02:09:13 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Nanni", "Loris", ""], ["Maguolo", "Gianluca", ""], ["Brahnam", "Sheryl", ""], ["Paci", "Michelangelo", ""]]}, {"id": "2103.15903", "submitter": "Sutanu Bera", "authors": "Sutanu Bera, Prabir Kumar Biswas", "title": "Iterative Gradient Encoding Network with Feature Co-Occurrence Loss for\n  Single Image Reflection Removal", "comments": "Submitted to IEEE International Conference of Image Processing (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing undesired reflections from a photo taken in front of glass is of\ngreat importance for enhancing visual computing systems' efficiency. Previous\nlearning-based approaches have produced visually plausible results for some\nreflections type, however, failed to generalize against other reflection types.\nThere is a dearth of literature for efficient methods concerning single image\nreflection removal, which can generalize well in large-scale reflection types.\nIn this study, we proposed an iterative gradient encoding network for single\nimage reflection removal. Next, to further supervise the network in learning\nthe correlation between the transmission layer features, we proposed a feature\nco-occurrence loss. Extensive experiments on the public benchmark dataset of\nSIR$^2$ demonstrated that our method can remove reflection favorably against\nthe existing state-of-the-art method on all imaging settings, including diverse\nbackgrounds. Moreover, as the reflection strength increases, our method can\nstill remove reflection even where other state of the art methods failed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:29:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bera", "Sutanu", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "2103.15910", "submitter": "Mohammad Sabokrou", "authors": "Razieh Rastgoo, Kourosh Kiani, Sergio Escalera, Mohammad Sabokrou", "title": "Sign Language Production: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language is the dominant yet non-primary form of communication language\nused in the deaf and hearing-impaired community. To make an easy and mutual\ncommunication between the hearing-impaired and the hearing communities,\nbuilding a robust system capable of translating the spoken language into sign\nlanguage and vice versa is fundamental. To this end, sign language recognition\nand production are two necessary parts for making such a two-way system. Sign\nlanguage recognition and production need to cope with some critical challenges.\nIn this survey, we review recent advances in Sign Language Production (SLP) and\nrelated areas using deep learning. This survey aims to briefly summarize recent\nachievements in SLP, discussing their advantages, limitations, and future\ndirections of research.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:38:22 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Rastgoo", "Razieh", ""], ["Kiani", "Kourosh", ""], ["Escalera", "Sergio", ""], ["Sabokrou", "Mohammad", ""]]}, {"id": "2103.15914", "submitter": "Alex Fedorov", "authors": "Alex Fedorov, Eloy Geenjaar, Lei Wu, Thomas P. DeRamus, Vince D.\n  Calhoun, Sergey M. Plis", "title": "Tasting the cake: evaluating self-supervised generalization on\n  out-of-distribution multimodal MRI data", "comments": "Presented as a RobustML workshop paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has enabled significant improvements on natural\nimage benchmarks. However, there is less work in the medical imaging domain in\nthis area. The optimal models have not yet been determined among the various\noptions. Moreover, little work has evaluated the current applicability limits\nof novel self-supervised methods. In this paper, we evaluate a range of current\ncontrastive self-supervised methods on out-of-distribution generalization in\norder to evaluate their applicability to medical imaging. We show that\nself-supervised models are not as robust as expected based on their results in\nnatural imaging benchmarks and can be outperformed by supervised learning with\ndropout. We also show that this behavior can be countered with extensive\naugmentation. Our results highlight the need for out-of-distribution\ngeneralization standards and benchmarks to adopt the self-supervised methods in\nthe medical imaging community.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:49:26 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 15:53:45 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Fedorov", "Alex", ""], ["Geenjaar", "Eloy", ""], ["Wu", "Lei", ""], ["DeRamus", "Thomas P.", ""], ["Calhoun", "Vince D.", ""], ["Plis", "Sergey M.", ""]]}, {"id": "2103.15916", "submitter": "Pedro Morgado", "authors": "Pedro Morgado, Ishan Misra, Nuno Vasconcelos", "title": "Robust Audio-Visual Instance Discrimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised learning method to learn audio and video\nrepresentations. Prior work uses the natural correspondence between audio and\nvideo to define a standard cross-modal instance discrimination task, where a\nmodel is trained to match representations from the two modalities. However, the\nstandard approach introduces two sources of training noise. First, audio-visual\ncorrespondences often produce faulty positives since the audio and video\nsignals can be uninformative of each other. To limit the detrimental impact of\nfaulty positives, we optimize a weighted contrastive learning loss, which\ndown-weighs their contribution to the overall loss. Second, since\nself-supervised contrastive learning relies on random sampling of negative\ninstances, instances that are semantically similar to the base instance can be\nused as faulty negatives. To alleviate the impact of faulty negatives, we\npropose to optimize an instance discrimination loss with a soft target\ndistribution that estimates relationships between instances. We validate our\ncontributions through extensive experiments on action recognition tasks and\nshow that they address the problems of audio-visual instance discrimination and\nimprove transfer learning performance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:52:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Morgado", "Pedro", ""], ["Misra", "Ishan", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2103.15918", "submitter": "Panagiota Kiourti", "authors": "Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, and Susmit\n  Jha", "title": "Online Defense of Trojaned Models using Misattributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach to detecting neural Trojans on Deep Neural\nNetworks during inference. This approach is based on monitoring the inference\nof a machine learning model, computing the attribution of the model's decision\non different features of the input, and then statistically analyzing these\nattributions to detect whether an input sample contains the Trojan trigger. The\nanomalous attributions, aka misattributions, are then accompanied by\nreverse-engineering of the trigger to evaluate whether the input sample is\ntruly poisoned with a Trojan trigger. We evaluate our approach on several\nbenchmarks, including models trained on MNIST, Fashion MNIST, and German\nTraffic Sign Recognition Benchmark, and demonstrate the state of the art\ndetection accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:53:44 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kiourti", "Panagiota", ""], ["Li", "Wenchao", ""], ["Roy", "Anirban", ""], ["Sikka", "Karan", ""], ["Jha", "Susmit", ""]]}, {"id": "2103.15939", "submitter": "Vivek Chalumuri", "authors": "Vivek Chalumuri, Bac Nguyen", "title": "A Simple Approach for Zero-Shot Learning based on Triplet Distribution\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the semantic descriptions of classes, Zero-Shot Learning (ZSL) aims to\nrecognize unseen classes without labeled training data by exploiting semantic\ninformation, which contains knowledge between seen and unseen classes. Existing\nZSL methods mainly use vectors to represent the embeddings to the semantic\nspace. Despite the popularity, such vector representation limits the\nexpressivity in terms of modeling the intra-class variability for each class.\nWe address this issue by leveraging the use of distribution embeddings. More\nspecifically, both image embeddings and class embeddings are modeled as\nGaussian distributions, where their similarity relationships are preserved\nthrough the use of triplet constraints. The key intuition which guides our\napproach is that for each image, the embedding of the correct class label\nshould be closer than that of any other class label. Extensive experiments on\nmultiple benchmark data sets show that the proposed method achieves highly\ncompetitive results for both traditional ZSL and more challenging Generalized\nZero-Shot Learning (GZSL) settings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:26:20 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chalumuri", "Vivek", ""], ["Nguyen", "Bac", ""]]}, {"id": "2103.15954", "submitter": "Yufan He", "authors": "Yufan He, Dong Yang, Holger Roth, Can Zhao, Daguang Xu", "title": "DiNTS: Differentiable Neural Network Topology Search for 3D Medical\n  Image Segmentation", "comments": "CVPR2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural architecture search (NAS) has been applied to automatically\nsearch high-performance networks for medical image segmentation. The NAS search\nspace usually contains a network topology level (controlling connections among\ncells with different spatial scales) and a cell level (operations within each\ncell). Existing methods either require long searching time for large-scale 3D\nimage datasets, or are limited to pre-defined topologies (such as U-shaped or\nsingle-path). In this work, we focus on three important aspects of NAS in 3D\nmedical image segmentation: flexible multi-path network topology, high search\nefficiency, and budgeted GPU memory usage. A novel differentiable search\nframework is proposed to support fast gradient-based search within a highly\nflexible network topology search space. The discretization of the searched\noptimal continuous model in differentiable scheme may produce a sub-optimal\nfinal discrete model (discretization gap). Therefore, we propose a topology\nloss to alleviate this problem. In addition, the GPU memory usage for the\nsearched 3D model is limited with budget constraints during search. Our\nDifferentiable Network Topology Search scheme (DiNTS) is evaluated on the\nMedical Segmentation Decathlon (MSD) challenge, which contains ten challenging\nsegmentation tasks. Our method achieves the state-of-the-art performance and\nthe top ranking on the MSD challenge leaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 21:02:42 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["He", "Yufan", ""], ["Yang", "Dong", ""], ["Roth", "Holger", ""], ["Zhao", "Can", ""], ["Xu", "Daguang", ""]]}, {"id": "2103.15967", "submitter": "Brian Wang", "authors": "Brian H. Wang, Carlos Diaz-Ruiz, Jacopo Banfi, and Mark Campbell", "title": "Detecting and Mapping Trees in Unstructured Environments with a Stereo\n  Camera and Pseudo-Lidar", "comments": "Accepted to the 2021 IEEE International Conference on Robotics and\n  Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for detecting and mapping trees in noisy stereo camera\npoint clouds, using a learned 3-D object detector. Inspired by recent\nadvancements in 3-D object detection using a pseudo-lidar representation for\nstereo data, we train a PointRCNN detector to recognize trees in forest-like\nenvironments. We generate detector training data with a novel automatic\nlabeling process that clusters a fused global point cloud. This process\nannotates large stereo point cloud training data sets with minimal user\nsupervision, and unlike previous pseudo-lidar detection pipelines, requires no\n3-D ground truth from other sensors such as lidar. Our mapping system\nadditionally uses a Kalman filter to associate detections and consistently\nestimate the positions and sizes of trees. We collect a data set for tree\ndetection consisting of 8680 stereo point clouds, and validate our method on an\noutdoors test sequence. Our results demonstrate robust tree recognition in\nnoisy stereo data at ranges of up to 7 meters, on 720p resolution images from a\nStereolabs ZED 2 camera. Code and data are available at\nhttps://github.com/brian-h-wang/pseudolidar-tree-detection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 21:46:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Brian H.", ""], ["Diaz-Ruiz", "Carlos", ""], ["Banfi", "Jacopo", ""], ["Campbell", "Mark", ""]]}, {"id": "2103.15970", "submitter": "Cl\\'ement Pinard", "authors": "Cl\\'ement Pinard, Antoine Manzanera", "title": "Does it work outside this benchmark? Introducing the Rigid Depth\n  Constructor tool, depth validation dataset construction in rigid scenes for\n  the masses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a protocol to construct your own depth validation dataset for\nnavigation. This protocol, called RDC for Rigid Depth Constructor, aims at\nbeing more accessible and cheaper than already existing techniques, requiring\nonly a camera and a Lidar sensor to get started. We also develop a test suite\nto get insightful information from the evaluated algorithm. Finally, we take\nthe example of UAV videos, on which we test two depth algorithms that were\ninitially tested on KITTI and show that the drone context is dramatically\ndifferent from in-car videos. This shows that a single context benchmark should\nnot be considered reliable, and when developing a depth estimation algorithm,\none should benchmark it on a dataset that best fits one's particular needs,\nwhich often means creating a brand new one. Along with this paper we provide\nthe tool with an open source implementation and plan to make it as\nuser-friendly as possible, to make depth dataset creation possible even for\nsmall teams. Our key contributions are the following: We propose a complete,\nopen-source and almost fully automatic software application for creating\nvalidation datasets with densely annotated depth, adaptable to a wide variety\nof image, video and range data. It includes selection tools to adapt the\ndataset to specific validation needs, and conversion tools to other dataset\nformats. Using this application, we propose two new real datasets, outdoor and\nindoor, readily usable in UAV navigation context. Finally as examples, we show\nan evaluation of two depth prediction algorithms, using a collection of\ncomprehensive (e.g. distribution based) metrics.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 22:01:24 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Pinard", "Cl\u00e9ment", ""], ["Manzanera", "Antoine", ""]]}, {"id": "2103.15973", "submitter": "Waqar Ahmed", "authors": "Waqar Ahmed, Pietro Morerio and Vittorio Murino", "title": "Adaptive Pseudo-Label Refinement by Negative Ensemble Learning for\n  Source-Free Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of existing Unsupervised Domain Adaptation (UDA) methods\npresumes source and target domain data to be simultaneously available during\ntraining. Such an assumption may not hold in practice, as source data is often\ninaccessible (e.g., due to privacy reasons). On the contrary, a pre-trained\nsource model is always considered to be available, even though performing\npoorly on target due to the well-known domain shift problem. This translates\ninto a significant amount of misclassifications, which can be interpreted as\nstructured noise affecting the inferred target pseudo-labels. In this work, we\ncast UDA as a pseudo-label refinery problem in the challenging source-free\nscenario. We propose a unified method to tackle adaptive noise filtering and\npseudo-label refinement. A novel Negative Ensemble Learning technique is\ndevised to specifically address noise in pseudo-labels, by enhancing diversity\nin ensemble members with different stochastic (i) input augmentation and (ii)\nfeedback. In particular, the latter is achieved by leveraging the novel concept\nof Disjoint Residual Labels, which allow diverse information to be fed to the\ndifferent members. A single target model is eventually trained with the refined\npseudo-labels, which leads to a robust performance on the target domain.\nExtensive experiments show that the proposed method, named Adaptive\nPseudo-Label Refinement, achieves state-of-the-art performance on major UDA\nbenchmarks, such as Digit5, PACS, Visda-C, and DomainNet, without using source\ndata at all.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 22:18:34 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ahmed", "Waqar", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "2103.15974", "submitter": "Mingda Zhang", "authors": "Mingda Zhang, Tristan Maidment, Ahmad Diab, Adriana Kovashka, Rebecca\n  Hwa", "title": "Domain-robust VQA with diverse datasets and methods but no target labels", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The observation that computer vision methods overfit to dataset specifics has\ninspired diverse attempts to make object recognition models robust to domain\nshifts. However, similar work on domain-robust visual question answering\nmethods is very limited. Domain adaptation for VQA differs from adaptation for\nobject recognition due to additional complexity: VQA models handle multimodal\ninputs, methods contain multiple steps with diverse modules resulting in\ncomplex optimization, and answer spaces in different datasets are vastly\ndifferent. To tackle these challenges, we first quantify domain shifts between\npopular VQA datasets, in both visual and textual space. To disentangle shifts\nbetween datasets arising from different modalities, we also construct synthetic\nshifts in the image and question domains separately. Second, we test the\nrobustness of different families of VQA methods (classic two-stream,\ntransformer, and neuro-symbolic methods) to these shifts. Third, we test the\napplicability of existing domain adaptation methods and devise a new one to\nbridge VQA domain gaps, adjusted to specific VQA models. To emulate the setting\nof real-world generalization, we focus on unsupervised domain adaptation and\nthe open-ended classification task formulation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 22:24:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Mingda", ""], ["Maidment", "Tristan", ""], ["Diab", "Ahmad", ""], ["Kovashka", "Adriana", ""], ["Hwa", "Rebecca", ""]]}, {"id": "2103.15977", "submitter": "Jingyun Liang", "authors": "Jingyun Liang, Kai Zhang, Shuhang Gu, Luc Van Gool, Radu Timofte", "title": "Flow-based Kernel Prior with Application to Blind Super-Resolution", "comments": "Accepted by CVPR2021. Code: https://github.com/JingyunLiang/FKP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Kernel estimation is generally one of the key problems for blind image\nsuper-resolution (SR). Recently, Double-DIP proposes to model the kernel via a\nnetwork architecture prior, while KernelGAN employs the deep linear network and\nseveral regularization losses to constrain the kernel space. However, they fail\nto fully exploit the general SR kernel assumption that anisotropic Gaussian\nkernels are sufficient for image SR. To address this issue, this paper proposes\na normalizing flow-based kernel prior (FKP) for kernel modeling. By learning an\ninvertible mapping between the anisotropic Gaussian kernel distribution and a\ntractable latent distribution, FKP can be easily used to replace the kernel\nmodeling modules of Double-DIP and KernelGAN. Specifically, FKP optimizes the\nkernel in the latent space rather than the network parameter space, which\nallows it to generate reasonable kernel initialization, traverse the learned\nkernel manifold and improve the optimization stability. Extensive experiments\non synthetic and real-world images demonstrate that the proposed FKP can\nsignificantly improve the kernel estimation accuracy with less parameters,\nruntime and memory usage, leading to state-of-the-art blind SR results.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 22:37:06 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liang", "Jingyun", ""], ["Zhang", "Kai", ""], ["Gu", "Shuhang", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2103.15980", "submitter": "Jose-Luis Blanco-Claraco", "authors": "Jos\\'e Luis Blanco-Claraco", "title": "A tutorial on $\\mathbf{SE}(3)$ transformation parameterizations and\n  on-manifold optimization", "comments": "68 pages, 6 figures; v1 in arXiv; see history of document versions on\n  page 3 for full change log of the technical report since 2010", "journal-ref": null, "doi": null, "report-no": "UMA-MAPIR-012010", "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An arbitrary rigid transformation in $\\mathbf{SE}(3)$ can be separated into\ntwo parts, namely, a translation and a rigid rotation. This technical report\nreviews, under a unifying viewpoint, three common alternatives to representing\nthe rotation part: sets of three (yaw-pitch-roll) Euler angles, orthogonal\nrotation matrices from $\\mathbf{SO}(3)$ and quaternions. It will be described:\n(i) the equivalence between these representations and the formulas for\ntransforming one to each other (in all cases considering the translational and\nrotational parts as a whole), (ii) how to compose poses with poses and poses\nwith points in each representation and (iii) how the uncertainty of the poses\n(when modeled as Gaussian distributions) is affected by these transformations\nand compositions. Some brief notes are also given about the Jacobians required\nto implement least-squares optimization on manifolds, an very promising\napproach in recent engineering literature. The text reflects which MRPT C++\nlibrary functions implement each of the described algorithms. All formulas and\ntheir implementation have been thoroughly validated by means of unit testing\nand numerical estimation of the Jacobians\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 22:43:49 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Blanco-Claraco", "Jos\u00e9 Luis", ""]]}, {"id": "2103.15982", "submitter": "Yuqian Zhou", "authors": "Yuqian Zhou, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi", "title": "TransFill: Reference-guided Image Inpainting by Merging Multiple Color\n  and Spatial Transformations", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image inpainting is the task of plausibly restoring missing pixels within a\nhole region that is to be removed from a target image. Most existing\ntechnologies exploit patch similarities within the image, or leverage\nlarge-scale training data to fill the hole using learned semantic and texture\ninformation. However, due to the ill-posed nature of the inpainting task, such\nmethods struggle to complete larger holes containing complicated scenes. In\nthis paper, we propose TransFill, a multi-homography transformed fusion method\nto fill the hole by referring to another source image that shares scene\ncontents with the target image. We first align the source image to the target\nimage by estimating multiple homographies guided by different depth levels. We\nthen learn to adjust the color and apply a pixel-level warping to each\nhomography-warped source image to make it more consistent with the target.\nFinally, a pixel-level fusion module is learned to selectively merge the\ndifferent proposals. Our method achieves state-of-the-art performance on pairs\nof images across a variety of wide baselines and color differences, and\ngeneralizes to user-provided image pairs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 22:45:07 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhou", "Yuqian", ""], ["Barnes", "Connelly", ""], ["Shechtman", "Eli", ""], ["Amirghodsi", "Sohrab", ""]]}, {"id": "2103.15992", "submitter": "Jing Huang", "authors": "Jing Huang, Guan Pang, Rama Kovvuri, Mandy Toh, Kevin J Liang, Praveen\n  Krishnan, Xi Yin, Tal Hassner", "title": "A Multiplexed Network for End-to-End, Multilingual OCR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in OCR have shown that an end-to-end (E2E) training pipeline\nthat includes both detection and recognition leads to the best results.\nHowever, many existing methods focus primarily on Latin-alphabet languages,\noften even only case-insensitive English characters. In this paper, we propose\nan E2E approach, Multiplexed Multilingual Mask TextSpotter, that performs\nscript identification at the word level and handles different scripts with\ndifferent recognition heads, all while maintaining a unified loss that\nsimultaneously optimizes script identification and multiple recognition heads.\nExperiments show that our method outperforms the single-head model with similar\nnumber of parameters in end-to-end recognition tasks, and achieves\nstate-of-the-art results on MLT17 and MLT19 joint text detection and script\nidentification benchmarks. We believe that our work is a step towards the\nend-to-end trainable and scalable multilingual multi-purpose OCR system. Our\ncode and model will be released.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 23:53:49 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Huang", "Jing", ""], ["Pang", "Guan", ""], ["Kovvuri", "Rama", ""], ["Toh", "Mandy", ""], ["Liang", "Kevin J", ""], ["Krishnan", "Praveen", ""], ["Yin", "Xi", ""], ["Hassner", "Tal", ""]]}, {"id": "2103.15997", "submitter": "Gilberto Ochoa-Ruiz", "authors": "Juan Carlos Angeles Ceron, Leonardo Chang, Gilberto Ochoa-Ruiz and\n  Sharib Ali", "title": "Assessing YOLACT++ for real time and robust instance segmentation of\n  medical instruments in endoscopic procedures", "comments": "Preprint under review for EMBC 2021 following IEEE guidelines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-based tracking of laparoscopic instruments plays a fundamental role in\ncomputer and robotic-assisted surgeries by aiding surgeons and increasing\npatient safety. Computer vision contests, such as the Robust Medical Instrument\nSegmentation (ROBUST-MIS) Challenge, seek to encourage the development of\nrobust models for such purposes, providing large, diverse, and annotated\ndatasets. To date, most of the existing models for instance segmentation of\nmedical instruments were based on two-stage detectors, which provide robust\nresults but are nowhere near to the real-time (5 frames-per-second (fps)at\nmost). However, in order for the method to be clinically applicable, real-time\ncapability is utmost required along with high accuracy. In this paper, we\npropose the addition of attention mechanisms to the YOLACT architecture that\nallows real-time instance segmentation of instrument with improved accuracy on\nthe ROBUST-MIS dataset. Our proposed approach achieves competitive performance\ncompared to the winner ofthe 2019 ROBUST-MIS challenge in terms of robustness\nscores,obtaining 0.313 MI_DSC and 0.338 MI_NSD, while achieving real-time\nperformance (37 fps)\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:09:55 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 01:39:43 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ceron", "Juan Carlos Angeles", ""], ["Chang", "Leonardo", ""], ["Ochoa-Ruiz", "Gilberto", ""], ["Ali", "Sharib", ""]]}, {"id": "2103.16002", "submitter": "Madeleine Grunde-McLaughlin", "authors": "Madeleine Grunde-McLaughlin, Ranjay Krishna, Maneesh Agrawala", "title": "AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning", "comments": "8 pages, 15 pages supplementary, 12 figures. To be published in CVPR\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual events are a composition of temporal actions involving actors\nspatially interacting with objects. When developing computer vision models that\ncan reason about compositional spatio-temporal events, we need benchmarks that\ncan analyze progress and uncover shortcomings. Existing video question\nanswering benchmarks are useful, but they often conflate multiple sources of\nerror into one accuracy metric and have strong biases that models can exploit,\nmaking it difficult to pinpoint model weaknesses. We present Action Genome\nQuestion Answering (AGQA), a new benchmark for compositional spatio-temporal\nreasoning. AGQA contains $192M$ unbalanced question answer pairs for $9.6K$\nvideos. We also provide a balanced subset of $3.9M$ question answer pairs, $3$\norders of magnitude larger than existing benchmarks, that minimizes bias by\nbalancing the answer distributions and types of question structures. Although\nhuman evaluators marked $86.02\\%$ of our question-answer pairs as correct, the\nbest model achieves only $47.74\\%$ accuracy. In addition, AGQA introduces\nmultiple training/test splits to test for various reasoning abilities,\nincluding generalization to novel compositions, to indirect references, and to\nmore compositional steps. Using AGQA, we evaluate modern visual reasoning\nsystems, demonstrating that the best models barely perform better than\nnon-visual baselines exploiting linguistic biases and that none of the existing\nmodels generalize to novel compositions unseen during training.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:24:01 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Grunde-McLaughlin", "Madeleine", ""], ["Krishna", "Ranjay", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "2103.16009", "submitter": "Jun He", "authors": "Jun He, Richang Hong, Xueliang Liu, Mingliang Xu and Meng Wang", "title": "Revisiting Deep Local Descriptor for Improved Few-Shot Classification", "comments": "12 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification studies the problem of quickly adapting a deep\nlearner to understanding novel classes based on few support images. In this\ncontext, recent research efforts have been aimed at designing more and more\ncomplex classifiers that measure similarities between query and support images,\nbut left the importance of feature embeddings seldom explored. We show that the\nreliance on sophisticated classifier is not necessary and a simple classifier\napplied directly to improved feature embeddings can outperform state-of-the-art\nmethods. To this end, we present a new method named \\textbf{DCAP} in which we\ninvestigate how one can improve the quality of embeddings by leveraging\n\\textbf{D}ense \\textbf{C}lassification and \\textbf{A}ttentive \\textbf{P}ooling.\nSpecifically, we propose to pre-train a learner on base classes with abundant\nsamples to solve dense classification problem first and then fine-tune the\nlearner on a bunch of randomly sampled few-shot tasks to adapt it to few-shot\nscenerio or the test time scenerio. We suggest to pool feature maps by applying\nattentive pooling instead of the widely used global average pooling (GAP) to\nprepare embeddings for few-shot classification during meta-finetuning.\nAttentive pooling learns to reweight local descriptors, explaining what the\nlearner is looking for as evidence for decision making. Experiments on two\nbenchmark datasets show the proposed method to be superior in multiple few-shot\nsettings while being simpler and more explainable. Code is available at:\n\\url{https://github.com/Ukeyboard/dcap/}.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:48:28 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["He", "Jun", ""], ["Hong", "Richang", ""], ["Liu", "Xueliang", ""], ["Xu", "Mingliang", ""], ["Wang", "Meng", ""]]}, {"id": "2103.16013", "submitter": "Weipeng Li", "authors": "Weipeng Li, Xiaogang Yang, Chuanxiang Li, Ruitao Lu, Xueli Xie", "title": "Training Sparse Neural Network by Constraining Synaptic Weight on Unit\n  Lp Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse deep neural networks have shown their advantages over dense models\nwith fewer parameters and higher computational efficiency. Here we demonstrate\nconstraining the synaptic weights on unit Lp-sphere enables the flexibly\ncontrol of the sparsity with p and improves the generalization ability of\nneural networks. Firstly, to optimize the synaptic weights constrained on unit\nLp-sphere, the parameter optimization algorithm, Lp-spherical gradient descent\n(LpSGD) is derived from the augmented Empirical Risk Minimization condition,\nwhich is theoretically proved to be convergent. To understand the mechanism of\nhow p affects Hoyer's sparsity, the expectation of Hoyer's sparsity under the\nhypothesis of gamma distribution is given and the predictions are verified at\nvarious p under different conditions. In addition, the \"semi-pruning\" and\nthreshold adaptation are designed for topology evolution to effectively screen\nout important connections and lead the neural networks converge from the\ninitial sparsity to the expected sparsity. Our approach is validated by\nexperiments on benchmark datasets covering a wide range of domains. And the\ntheoretical analysis pave the way to future works on training sparse neural\nnetworks with constrained optimization.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 01:02:31 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Weipeng", ""], ["Yang", "Xiaogang", ""], ["Li", "Chuanxiang", ""], ["Lu", "Ruitao", ""], ["Xie", "Xueli", ""]]}, {"id": "2103.16019", "submitter": "Weihong Deng", "authors": "Yuke Fang, Jiani Hu, Weihong Deng", "title": "Identity-Aware CycleGAN for Face Photo-Sketch Synthesis and Recognition", "comments": "36 pages, 11 figures", "journal-ref": "Pattern Recognition, vol.102, pp.107249, 2020", "doi": "10.1016/j.patcog.2020.107249", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face photo-sketch synthesis and recognition has many applications in digital\nentertainment and law enforcement. Recently, generative adversarial networks\n(GANs) based methods have significantly improved the quality of image\nsynthesis, but they have not explicitly considered the purpose of recognition.\nIn this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model\nthat applies a new perceptual loss to supervise the image generation network.\nIt improves CycleGAN on photo-sketch synthesis by paying more attention to the\nsynthesis of key facial regions, such as eyes and nose, which are important for\nidentity recognition. Furthermore, we develop a mutual optimization procedure\nbetween the synthesis model and the recognition model, which iteratively\nsynthesizes better images by IACycleGAN and enhances the recognition model by\nthe triplet loss of the generated and real samples. Extensive experiments are\nperformed on both photo-tosketch and sketch-to-photo tasks using the widely\nused CUFS and CUFSF databases. The results show that the proposed method\nperforms better than several state-of-the-art methods in terms of both\nsynthetic image quality and photo-sketch recognition accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 01:30:08 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Fang", "Yuke", ""], ["Hu", "Jiani", ""], ["Deng", "Weihong", ""]]}, {"id": "2103.16020", "submitter": "Eisa Hedayati", "authors": "Eisa Hedayati, Timothy C. Havens, Jeremy P. Bos", "title": "Machine learning method for light field refocusing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging introduced the capability to refocus an image after\ncapturing. Currently there are two popular methods for refocusing,\nshift-and-sum and Fourier slice methods. Neither of these two methods can\nrefocus the light field in real-time without any pre-processing. In this paper\nwe introduce a machine learning based refocusing technique that is capable of\nextracting 16 refocused images with refocusing parameters of\n\\alpha=0.125,0.250,0.375,...,2.0 in real-time. We have trained our network,\nwhich is called RefNet, in two experiments. Once using the Fourier slice method\nas the training -- i.e., \"ground truth\" -- data and another using the\nshift-and-sum method as the training data. We showed that in both cases, not\nonly is the RefNet method at least 134x faster than previous approaches, but\nalso the color prediction of RefNet is superior to both Fourier slice and\nshift-and-sum methods while having similar depth of field and focus distance\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 01:46:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hedayati", "Eisa", ""], ["Havens", "Timothy C.", ""], ["Bos", "Jeremy P.", ""]]}, {"id": "2103.16022", "submitter": "Xiaosong Wang", "authors": "Xiaosong Wang and Ziyue Xu and Leo Tam and Dong Yang and Daguang Xu", "title": "Self-supervised Image-text Pre-training With Mixed Data In Chest X-rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pre-trained models, e.g., from ImageNet, have proven to be effective in\nboosting the performance of many downstream applications. It is too demanding\nto acquire large-scale annotations to build such models for medical imaging.\nMeanwhile, there are numerous clinical data (in the form of images and text\nreports) stored in the hospital information systems. The paired image-text data\nfrom the same patient study could be utilized for the pre-training task in a\nweakly supervised manner. However, the integrity, accessibility, and amount of\nsuch raw data vary across different institutes, e.g., paired vs. unpaired\n(image-only or text-only). In this work, we introduce an image-text\npre-training framework that can learn from these raw data with mixed data\ninputs, i.e., paired image-text data, a mixture of paired and unpaired data.\nThe unpaired data can be sourced from one or multiple institutes (e.g., images\nfrom one institute coupled with texts from another). Specifically, we propose a\ntransformer-based training framework for jointly learning the representation of\nboth the image and text data. In addition to the existing masked language\nmodeling, multi-scale masked vision modeling is introduced as a self-supervised\ntraining task for image patch regeneration. We not only demonstrate the\nfeasibility of pre-training across mixed data inputs but also illustrate the\nbenefits of adopting such pre-trained models in 3 chest X-ray applications,\ni.e., classification, retrieval, and image regeneration. Superior results are\nreported in comparison to prior art using MIMIC-CXR, NIH14-CXR, and OpenI-CXR\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 01:48:46 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Xiaosong", ""], ["Xu", "Ziyue", ""], ["Tam", "Leo", ""], ["Yang", "Dong", ""], ["Xu", "Daguang", ""]]}, {"id": "2103.16024", "submitter": "Shuning Chang", "authors": "Shuning Chang, Pichao Wang, Fan Wang, Hao Li, Jiashi Feng", "title": "Augmented Transformer with Adaptive Graph for Temporal Action Proposal\n  Generation", "comments": "12 pagess, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action proposal generation (TAPG) is a fundamental and challenging\ntask in video understanding, especially in temporal action detection. Most\nprevious works focus on capturing the local temporal context and can well\nlocate simple action instances with clean frames and clear boundaries. However,\nthey generally fail in complicated scenarios where interested actions involve\nirrelevant frames and background clutters, and the local temporal context\nbecomes less effective. To deal with these problems, we present an augmented\ntransformer with adaptive graph network (ATAG) to exploit both long-range and\nlocal temporal contexts for TAPG. Specifically, we enhance the vanilla\ntransformer by equipping a snippet actionness loss and a front block, dubbed\naugmented transformer, and it improves the abilities of capturing long-range\ndependencies and learning robust feature for noisy action instances.Moreover,\nan adaptive graph convolutional network (GCN) is proposed to build local\ntemporal context by mining the position information and difference between\nadjacent features. The features from the two modules carry rich semantic\ninformation of the video, and are fused for effective sequential proposal\ngeneration. Extensive experiments are conducted on two challenging datasets,\nTHUMOS14 and ActivityNet1.3, and the results demonstrate that our method\noutperforms state-of-the-art TAPG methods. Our code will be released soon.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 02:01:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chang", "Shuning", ""], ["Wang", "Pichao", ""], ["Wang", "Fan", ""], ["Li", "Hao", ""], ["Feng", "Jiashi", ""]]}, {"id": "2103.16026", "submitter": "Shangrong Yang", "authors": "Shangrong Yang, Chunyu Lin, Kang Liao, Chunjie Zhang, Yao Zhao", "title": "Progressively Complementary Network for Fisheye Image Rectification\n  Using Appearance Flow", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distortion rectification is often required for fisheye images. The\ngeneration-based method is one mainstream solution due to its label-free\nproperty, but its naive skip-connection and overburdened decoder will cause\nblur and incomplete correction. First, the skip-connection directly transfers\nthe image features, which may introduce distortion and cause incomplete\ncorrection. Second, the decoder is overburdened during simultaneously\nreconstructing the content and structure of the image, resulting in vague\nperformance. To solve these two problems, in this paper, we focus on the\ninterpretable correction mechanism of the distortion rectification network and\npropose a feature-level correction scheme. We embed a correction layer in\nskip-connection and leverage the appearance flows in different layers to\npre-correct the image features. Consequently, the decoder can easily\nreconstruct a plausible result with the remaining distortion-less information.\nIn addition, we propose a parallel complementary structure. It effectively\nreduces the burden of the decoder by separating content reconstruction and\nstructure correction. Subjective and objective experiment results on different\ndatasets demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 02:11:38 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 01:56:51 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yang", "Shangrong", ""], ["Lin", "Chunyu", ""], ["Liao", "Kang", ""], ["Zhang", "Chunjie", ""], ["Zhao", "Yao", ""]]}, {"id": "2103.16047", "submitter": "Chang Liu", "authors": "Chang Liu and Han Yu and Boyang Li and Zhiqi Shen and Zhanning Gao and\n  Peiran Ren and Xuansong Xie and Lizhen Cui and Chunyan Miao", "title": "Noise-resistant Deep Metric Learning with Ranking-based Instance\n  Selection", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The existence of noisy labels in real-world data negatively impacts the\nperformance of deep learning models. Although much research effort has been\ndevoted to improving robustness to noisy labels in classification tasks, the\nproblem of noisy labels in deep metric learning (DML) remains open. In this\npaper, we propose a noise-resistant training technique for DML, which we name\nProbabilistic Ranking-based Instance Selection with Memory (PRISM). PRISM\nidentifies noisy data in a minibatch using average similarity against image\nfeatures extracted by several previous versions of the neural network. These\nfeatures are stored in and retrieved from a memory bank. To alleviate the high\ncomputational cost brought by the memory bank, we introduce an acceleration\nmethod that replaces individual data points with the class centers. In\nextensive comparisons with 12 existing approaches under both synthetic and\nreal-world label noise, PRISM demonstrates superior performance of up to 6.06%\nin Precision@1.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 03:22:17 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 04:16:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liu", "Chang", ""], ["Yu", "Han", ""], ["Li", "Boyang", ""], ["Shen", "Zhiqi", ""], ["Gao", "Zhanning", ""], ["Ren", "Peiran", ""], ["Xie", "Xuansong", ""], ["Cui", "Lizhen", ""], ["Miao", "Chunyan", ""]]}, {"id": "2103.16050", "submitter": "Lei Li", "authors": "Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi,\n  Zhengze Yu, Xiaoya Li, Boyang xia", "title": "Progressive Domain Expansion Network for Single Domain Generalization", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single domain generalization is a challenging case of model generalization,\nwhere the models are trained on a single domain and tested on other unseen\ndomains. A promising solution is to learn cross-domain invariant\nrepresentations by expanding the coverage of the training domain. These methods\nhave limited generalization performance gains in practical applications due to\nthe lack of appropriate safety and effectiveness constraints. In this paper, we\npropose a novel learning framework called progressive domain expansion network\n(PDEN) for single domain generalization. The domain expansion subnetwork and\nrepresentation learning subnetwork in PDEN mutually benefit from each other by\njoint learning. For the domain expansion subnetwork, multiple domains are\nprogressively generated in order to simulate various photometric and geometric\ntransforms in unseen domains. A series of strategies are introduced to\nguarantee the safety and effectiveness of the expanded domains. For the domain\ninvariant representation learning subnetwork, contrastive learning is\nintroduced to learn the domain invariant representation in which each class is\nwell clustered so that a better decision boundary can be learned to improve\nit's generalization. Extensive experiments on classification and segmentation\nhave shown that PDEN can achieve up to 15.28% improvement compared with the\nstate-of-the-art single-domain generalization methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 03:31:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Lei", ""], ["Gao", "Ke", ""], ["Cao", "Juan", ""], ["Huang", "Ziyao", ""], ["Weng", "Yepeng", ""], ["Mi", "Xiaoyue", ""], ["Yu", "Zhengze", ""], ["Li", "Xiaoya", ""], ["xia", "Boyang", ""]]}, {"id": "2103.16054", "submitter": "Zetong Yang", "authors": "Zetong Yang, Yin Zhou, Zhifeng Chen, Jiquan Ngiam", "title": "3D-MAN: 3D Multi-frame Attention Network for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D object detection is an important module in autonomous driving and\nrobotics. However, many existing methods focus on using single frames to\nperform 3D detection, and do not fully utilize information from multiple\nframes. In this paper, we present 3D-MAN: a 3D multi-frame attention network\nthat effectively aggregates features from multiple perspectives and achieves\nstate-of-the-art performance on Waymo Open Dataset. 3D-MAN first uses a novel\nfast single-frame detector to produce box proposals. The box proposals and\ntheir corresponding feature maps are then stored in a memory bank. We design a\nmulti-view alignment and aggregation module, using attention networks, to\nextract and aggregate the temporal features stored in the memory bank. This\neffectively combines the features coming from different perspectives of the\nscene. We demonstrate the effectiveness of our approach on the large-scale\ncomplex Waymo Open Dataset, achieving state-of-the-art results compared to\npublished single-frame and multi-frame methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 03:44:22 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Yang", "Zetong", ""], ["Zhou", "Yin", ""], ["Chen", "Zhifeng", ""], ["Ngiam", "Jiquan", ""]]}, {"id": "2103.16066", "submitter": "Jun Zhou", "authors": "Jun Zhou, Wei Jin, Mingjie Wang, Xiuping Liu, Zhiyang Li and Zhaobin\n  Liu", "title": "Fast and Accurate Normal Estimation for Point Cloud via Patch Stitching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an effective normal estimation method adopting\nmulti-patch stitching for an unstructured point cloud. The majority of\nlearning-based approaches encode a local patch around each point of a whole\nmodel and estimate the normals in a point-by-point manner. In contrast, we\nsuggest a more efficient pipeline, in which we introduce a patch-level normal\nestimation architecture to process a series of overlapping patches.\nAdditionally, a multi-normal selection method based on weights, dubbed as\nmulti-patch stitching, integrates the normals from the overlapping patches. To\nreduce the adverse effects of sharp corners or noise in a patch, we introduce\nan adaptive local feature aggregation layer to focus on an anisotropic\nneighborhood. We then utilize a multi-branch planar experts module to break the\nmutual influence between underlying piecewise surfaces in a patch. At the\nstitching stage, we use the learned weights of multi-branch planar experts and\ndistance weights between points to select the best normal from the overlapping\nparts. Furthermore, we put forward constructing a sparse matrix representation\nto reduce large-scale retrieval overheads for the loop iterations dramatically.\nExtensive experiments demonstrate that our method achieves SOTA results with\nthe advantage of lower computational costs and higher robustness to noise over\nmost of the existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:30:35 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 05:18:51 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhou", "Jun", ""], ["Jin", "Wei", ""], ["Wang", "Mingjie", ""], ["Liu", "Xiuping", ""], ["Li", "Zhiyang", ""], ["Liu", "Zhaobin", ""]]}, {"id": "2103.16074", "submitter": "Xinke Li", "authors": "Xinke Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim,\n  Joey Tianyi Zhou", "title": "PointBA: Towards Backdoor Attacks in 3D Point Cloud", "comments": "International Conference on Computer Vision, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D deep learning has been increasingly more popular for a variety of tasks\nincluding many safety-critical applications. However, recently several works\nraise the security issues of 3D deep nets. Although most of these works\nconsider adversarial attacks, we identify that backdoor attack is indeed a more\nserious threat to 3D deep learning systems but remains unexplored. We present\nthe backdoor attacks in 3D with a unified framework that exploits the unique\nproperties of 3D data and networks. In particular, we design two attack\napproaches: the poison-label attack and the clean-label attack. The first one\nis straightforward and effective in practice, while the second one is more\nsophisticated assuming there are certain data inspections. The attack\nalgorithms are mainly motivated and developed by 1) the recent discovery of 3D\nadversarial samples which demonstrate the vulnerability of 3D deep nets under\nspatial transformations; 2) the proposed feature disentanglement technique that\nmanipulates the feature of the data through optimization methods and its\npotential to embed a new task. Extensive experiments show the efficacy of the\npoison-label attack with over 95% success rate across several 3D datasets and\nmodels, and the ability of clean-label attack against data filtering with\naround 50% success rate. Our proposed backdoor attack in 3D point cloud is\nexpected to perform as a baseline for improving the robustness of 3D deep\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:49:25 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 06:41:18 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Xinke", ""], ["Chen", "Zhirui", ""], ["Zhao", "Yue", ""], ["Tong", "Zekun", ""], ["Zhao", "Yabang", ""], ["Lim", "Andrew", ""], ["Zhou", "Joey Tianyi", ""]]}, {"id": "2103.16076", "submitter": "Tianfei Zhou", "authors": "Tianfei Zhou, Wenguan Wang, Zhiyuan Liang, Jianbing Shen", "title": "Face Forensics in the Wild", "comments": "CVPR 2021 (Oral). https://github.com/tfzhou/FFIW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  On existing public benchmarks, face forgery detection techniques have\nachieved great success. However, when used in multi-person videos, which often\ncontain many people active in the scene with only a small subset having been\nmanipulated, their performance remains far from being satisfactory. To take\nface forgery detection to a new level, we construct a novel large-scale\ndataset, called FFIW-10K, which comprises 10,000 high-quality forgery videos,\nwith an average of three human faces in each frame. The manipulation procedure\nis fully automatic, controlled by a domain-adversarial quality assessment\nnetwork, making our dataset highly scalable with low human cost. In addition,\nwe propose a novel algorithm to tackle the task of multi-person face forgery\ndetection. Supervised by only video-level label, the algorithm explores\nmultiple instance learning and learns to automatically attend to tampered\nfaces. Our algorithm outperforms representative approaches for both forgery\nclassification and localization on FFIW-10K, and also shows high generalization\nability on existing benchmarks. We hope that our dataset and study will help\nthe community to explore this new field in more depth.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:06:19 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhou", "Tianfei", ""], ["Wang", "Wenguan", ""], ["Liang", "Zhiyuan", ""], ["Shen", "Jianbing", ""]]}, {"id": "2103.16079", "submitter": "Weiping Zheng", "authors": "Weiping Zheng, Dacan Jiang, Gansen Zhao", "title": "Environmental sound analysis with mixup based multitask learning and\n  cross-task fusion", "comments": "5 pages, 1 figue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Environmental sound analysis is currently getting more and more attentions.\nIn the domain, acoustic scene classification and acoustic event classification\nare two closely related tasks. In this letter, a two-stage method is proposed\nfor the above tasks. In the first stage, a mixup based MTL solution is proposed\nto classify both tasks in one single convolutional neural network. Artificial\nmulti-label samples are used in the training of the MTL model, which are mixed\nup using existing single-task datasets. The multi-task model obtained can\neffectively recognize both the acoustic scenes and events. Compared with other\nmethods such as re-annotation or synthesis, the mixup based MTL is low-cost,\nflexible and effective. In the second stage, the MTL model is modified into a\nsingle-task model which is fine-tuned using the original dataset corresponding\nto the specific task. By controlling the frozen layers carefully, the\ntask-specific high level features are fused and the performance of the single\nclassification task is further improved. The proposed method has confirmed the\ncomplementary characteristics of acoustic scene and acoustic event\nclassifications. Finally, enhanced by ensemble learning, a satisfactory\naccuracy of 84.5 percent on TUT acoustic scene 2017 dataset and an accuracy of\n77.5 percent on ESC-50 dataset are achieved respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:11:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zheng", "Weiping", ""], ["Jiang", "Dacan", ""], ["Zhao", "Gansen", ""]]}, {"id": "2103.16083", "submitter": "Hengyue Liu", "authors": "Hengyue Liu, Ning Yan, Masood S. Mortazavi, Bir Bhanu", "title": "Fully Convolutional Scene Graph Generation", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fully convolutional scene graph generation (FCSGG)\nmodel that detects objects and relations simultaneously. Most of the scene\ngraph generation frameworks use a pre-trained two-stage object detector, like\nFaster R-CNN, and build scene graphs using bounding box features. Such pipeline\nusually has a large number of parameters and low inference speed. Unlike these\napproaches, FCSGG is a conceptually elegant and efficient bottom-up approach\nthat encodes objects as bounding box center points, and relationships as 2D\nvector fields which are named as Relation Affinity Fields (RAFs). RAFs encode\nboth semantic and spatial features, and explicitly represent the relationship\nbetween a pair of objects by the integral on a sub-region that points from\nsubject to object. FCSGG only utilizes visual features and still generates\nstrong results for scene graph generation. Comprehensive experiments on the\nVisual Genome dataset demonstrate the efficacy, efficiency, and\ngeneralizability of the proposed method. FCSGG achieves highly competitive\nresults on recall and zero-shot recall with significantly reduced inference\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:25:38 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Hengyue", ""], ["Yan", "Ning", ""], ["Mortazavi", "Masood S.", ""], ["Bhanu", "Bir", ""]]}, {"id": "2103.16086", "submitter": "Xiao Wang", "authors": "Xiao Wang, Zhe Chen, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian,\n  Feng Wu", "title": "Dynamic Attention guided Multi-Trajectory Analysis for Single Object\n  Tracking", "comments": "Accepted by IEEE T-CSVT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing single object trackers track the target in a unitary\nlocal search window, making them particularly vulnerable to challenging factors\nsuch as heavy occlusions and out-of-view movements. Despite the attempts to\nfurther incorporate global search, prevailing mechanisms that cooperate local\nand global search are relatively static, thus are still sub-optimal for\nimproving tracking performance. By further studying the local and global search\nresults, we raise a question: can we allow more dynamics for cooperating both\nresults? In this paper, we propose to introduce more dynamics by devising a\ndynamic attention-guided multi-trajectory tracking strategy. In particular, we\nconstruct dynamic appearance model that contains multiple target templates,\neach of which provides its own attention for locating the target in the new\nframe. Guided by different attention, we maintain diversified tracking results\nfor the target to build multi-trajectory tracking history, allowing more\ncandidates to represent the true target trajectory. After spanning the whole\nsequence, we introduce a multi-trajectory selection network to find the best\ntrajectory that delivers improved tracking performance. Extensive experimental\nresults show that our proposed tracking strategy achieves compelling\nperformance on various large-scale tracking benchmarks. The project page of\nthis paper can be found at https://sites.google.com/view/mt-track/.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:36:31 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Xiao", ""], ["Chen", "Zhe", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""], ["Wang", "Yaowei", ""], ["Tian", "Yonghong", ""], ["Wu", "Feng", ""]]}, {"id": "2103.16095", "submitter": "Muzhi Han", "authors": "Muzhi Han, Zeyu Zhang, Ziyuan Jiao, Xu Xie, Yixin Zhu, Song-Chun Zhu,\n  Hangxin Liu", "title": "Reconstructing Interactive 3D Scenes by Panoptic Mapping and CAD Model\n  Alignments", "comments": "ICRA 2021 paper. Project:\n  https://sites.google.com/view/icra2021-reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we rethink the problem of scene reconstruction from an\nembodied agent's perspective: While the classic view focuses on the\nreconstruction accuracy, our new perspective emphasizes the underlying\nfunctions and constraints such that the reconstructed scenes provide\n\\em{actionable} information for simulating \\em{interactions} with agents. Here,\nwe address this challenging problem by reconstructing an interactive scene\nusing RGB-D data stream, which captures (i) the semantics and geometry of\nobjects and layouts by a 3D volumetric panoptic mapping module, and (ii) object\naffordance and contextual relations by reasoning over physical common sense\namong objects, organized by a graph-based scene representation. Crucially, this\nreconstructed scene replaces the object meshes in the dense panoptic map with\npart-based articulated CAD models for finer-grained robot interactions. In the\nexperiments, we demonstrate that (i) our panoptic mapping module outperforms\nprevious state-of-the-art methods, (ii) a high-performant physical reasoning\nprocedure that matches, aligns, and replaces objects' meshes with best-fitted\nCAD models, and (iii) reconstructed scenes are physically plausible and\nnaturally afford actionable interactions; without any manual labeling, they are\nseamlessly imported to ROS-based simulators and virtual environments for\ncomplex robot task executions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:56:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Han", "Muzhi", ""], ["Zhang", "Zeyu", ""], ["Jiao", "Ziyuan", ""], ["Xie", "Xu", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""], ["Liu", "Hangxin", ""]]}, {"id": "2103.16099", "submitter": "Yanan Wu", "authors": "Zizhang Wu, Man Wang, Jason Wang, Wenkai Zhang, Muqing Fang, Tianhao\n  Xu", "title": "DeepWORD: A GCN-based Approach for Owner-Member Relationship Detection\n  in Autonomous Driving", "comments": "Accepted by IEEE ICME", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It's worth noting that the owner-member relationship between wheels and\nvehicles has an significant contribution to the 3D perception of vehicles,\nespecially in the embedded environment. However, there are currently two main\nchallenges about the above relationship prediction: i) The traditional\nheuristic methods based on IoU can hardly deal with the traffic jam scenarios\nfor the occlusion. ii) It is difficult to establish an efficient applicable\nsolution for the vehicle-mounted system. To address these issues, we propose an\ninnovative relationship prediction method, namely DeepWORD, by designing a\ngraph convolution network (GCN). Specifically, we utilize the feature maps with\nlocal correlation as the input of nodes to improve the information richness.\nBesides, we introduce the graph attention network (GAT) to dynamically amend\nthe prior estimation deviation. Furthermore, we establish an annotated\nowner-member relationship dataset called WORD as a large-scale benchmark, which\nwill be available soon. The experiments demonstrate that our solution achieves\nstate-of-the-art accuracy and real-time in practice.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:12:29 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:13:53 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wu", "Zizhang", ""], ["Wang", "Man", ""], ["Wang", "Jason", ""], ["Zhang", "Wenkai", ""], ["Fang", "Muqing", ""], ["Xu", "Tianhao", ""]]}, {"id": "2103.16101", "submitter": "JIn Fang", "authors": "Jinxin Zhao, Jin Fang, Zhixian Ye and Liangjun Zhang", "title": "Large Scale Autonomous Driving Scenarios Clustering with Self-supervised\n  Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The clustering of autonomous driving scenario data can substantially benefit\nthe autonomous driving validation and simulation systems by improving the\nsimulation tests' completeness and fidelity. This article proposes a\ncomprehensive data clustering framework for a large set of vehicle driving\ndata. Existing algorithms utilize handcrafted features whose quality relies on\nthe judgments of human experts. Additionally, the related feature compression\nmethods are not scalable for a large data-set. Our approach thoroughly\nconsiders the traffic elements, including both in-traffic agent objects and map\ninformation. Meanwhile, we proposed a self-supervised deep learning approach\nfor spatial and temporal feature extraction to avoid biased data\nrepresentation. With the newly designed driving data clustering evaluation\nmetrics based on data-augmentation, the accuracy assessment does not require a\nhuman-labeled data-set, which is subject to human bias. Via such unprejudiced\nevaluation metrics, we have shown our approach surpasses the existing methods\nthat rely on handcrafted feature extractions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:22:40 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhao", "Jinxin", ""], ["Fang", "Jin", ""], ["Ye", "Zhixian", ""], ["Zhang", "Liangjun", ""]]}, {"id": "2103.16106", "submitter": "Petros Spachos", "authors": "Lili Zhu, Petros Spachos, Erica Pensini, and Konstantinos Plataniotis", "title": "Deep Learning and Machine Vision for Food Processing: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The quality and safety of food is an important issue to the whole society,\nsince it is at the basis of human health, social development and stability.\nEnsuring food quality and safety is a complex process, and all stages of food\nprocessing must be considered, from cultivating, harvesting and storage to\npreparation and consumption. However, these processes are often\nlabour-intensive. Nowadays, the development of machine vision can greatly\nassist researchers and industries in improving the efficiency of food\nprocessing. As a result, machine vision has been widely used in all aspects of\nfood processing. At the same time, image processing is an important component\nof machine vision. Image processing can take advantage of machine learning and\ndeep learning models to effectively identify the type and quality of food.\nSubsequently, follow-up design in the machine vision system can address tasks\nsuch as food grading, detecting locations of defective spots or foreign\nobjects, and removing impurities. In this paper, we provide an overview on the\ntraditional machine learning and deep learning methods, as well as the machine\nvision techniques that can be applied to the field of food processing. We\npresent the current approaches and challenges, and the future trends.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:40:19 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhu", "Lili", ""], ["Spachos", "Petros", ""], ["Pensini", "Erica", ""], ["Plataniotis", "Konstantinos", ""]]}, {"id": "2103.16107", "submitter": "Weiqing Min", "authors": "Weiqing Min and Zhiling Wang and Yuxin Liu and Mengjiang Luo and\n  Liping Kang and Xiaoming Wei and Xiaolin Wei and Shuqiang Jiang", "title": "Large Scale Visual Food Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food recognition plays an important role in food choice and intake, which is\nessential to the health and well-being of humans. It is thus of importance to\nthe computer vision community, and can further support many food-oriented\nvision and multimodal tasks. Unfortunately, we have witnessed remarkable\nadvancements in generic visual recognition for released large-scale datasets,\nyet largely lags in the food domain. In this paper, we introduce Food2K, which\nis the largest food recognition dataset with 2,000 categories and over 1\nmillion images.Compared with existing food recognition datasets, Food2K\nbypasses them in both categories and images by one order of magnitude, and thus\nestablishes a new challenging benchmark to develop advanced models for food\nvisual representation learning. Furthermore, we propose a deep progressive\nregion enhancement network for food recognition, which mainly consists of two\ncomponents, namely progressive local feature learning and region feature\nenhancement. The former adopts improved progressive training to learn diverse\nand complementary local features, while the latter utilizes self-attention to\nincorporate richer context with multiple scales into local features for further\nlocal feature enhancement. Extensive experiments on Food2K demonstrate the\neffectiveness of our proposed method. More importantly, we have verified better\ngeneralization ability of Food2K in various tasks, including food recognition,\nfood image retrieval, cross-modal recipe retrieval, food detection and\nsegmentation. Food2K can be further explored to benefit more food-relevant\ntasks including emerging and more complex ones (e.g., nutritional understanding\nof food), and the trained models on Food2K can be expected as backbones to\nimprove the performance of more food-relevant tasks. We also hope Food2K can\nserve as a large scale fine-grained visual recognition benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:41:42 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 05:01:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Min", "Weiqing", ""], ["Wang", "Zhiling", ""], ["Liu", "Yuxin", ""], ["Luo", "Mengjiang", ""], ["Kang", "Liping", ""], ["Wei", "Xiaoming", ""], ["Wei", "Xiaolin", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "2103.16110", "submitter": "Mingchen Zhuge", "authors": "Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen,\n  Haoming Zhou, Minghui Qiu and Ling Shao", "title": "Kaleido-BERT: Vision-Language Pre-training on Fashion Domain", "comments": "CVPR2021 Accepted. Code: https://github.com/mczhuge/Kaleido-BERT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new vision-language (VL) pre-training model dubbed Kaleido-BERT,\nwhich introduces a novel kaleido strategy for fashion cross-modality\nrepresentations from transformers. In contrast to random masking strategy of\nrecent VL models, we design alignment guided masking to jointly focus more on\nimage-text semantic relations. To this end, we carry out five novel tasks,\ni.e., rotation, jigsaw, camouflage, grey-to-color, and blank-to-color for\nself-supervised VL pre-training at patches of different scale. Kaleido-BERT is\nconceptually simple and easy to extend to the existing BERT framework, it\nattains new state-of-the-art results by large margins on four downstream tasks,\nincluding text retrieval (R@1: 4.03% absolute improvement), image retrieval\n(R@1: 7.13% abs imv.), category recognition (ACC: 3.28% abs imv.), and fashion\ncaptioning (Bleu4: 1.2 abs imv.). We validate the efficiency of Kaleido-BERT on\na wide range of e-commerical websites, demonstrating its broader potential in\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:53:00 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 11:37:06 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 05:07:39 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zhuge", "Mingchen", ""], ["Gao", "Dehong", ""], ["Fan", "Deng-Ping", ""], ["Jin", "Linbo", ""], ["Chen", "Ben", ""], ["Zhou", "Haoming", ""], ["Qiu", "Minghui", ""], ["Shao", "Ling", ""]]}, {"id": "2103.16115", "submitter": "Tiange Xiang", "authors": "Tiange Xiang, Hongliang Yuan, Haozhi Huang, Yujin Shi", "title": "Two-Stage Monte Carlo Denoising with Adaptive Sampling and Kernel Pool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Monte Carlo path tracer renders noisy image sequences at low sampling counts.\nAlthough great progress has been made on denoising such sequences, existing\nmethods still suffer from spatial and temporary artifacts. In this paper, we\ntackle the problems in Monte Carlo rendering by proposing a two-stage denoiser\nbased on the adaptive sampling strategy. In the first stage, concurrent to\nadjusting samples per pixel (spp) on-the-fly, we reuse the computations to\ngenerate extra denoising kernels applying on the adaptively rendered image.\nRather than a direct prediction of pixel-wise kernels, we save the overhead\ncomplexity by interpolating such kernels from a public kernel pool, which can\nbe dynamically updated to fit input signals. In the second stage, we design the\nposition-aware pooling and semantic alignment operators to improve\nspatial-temporal stability. Our method was first benchmarked on 10 synthesized\nscenes rendered from the Mitsuba renderer and then validated on 3 additional\nscenes rendered from our self-built RTX-based renderer. Our method outperforms\nstate-of-the-art counterparts in terms of both numerical error and visual\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:05:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Xiang", "Tiange", ""], ["Yuan", "Hongliang", ""], ["Huang", "Haozhi", ""], ["Shi", "Yujin", ""]]}, {"id": "2103.16129", "submitter": "Bingfeng Zhang", "authors": "Bingfeng Zhang, Jimin Xiao and Terry Qin", "title": "Self-Guided and Cross-Guided Learning for Few-Shot Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation has been attracting a lot of attention due to its\neffectiveness to segment unseen object classes with a few annotated samples.\nMost existing approaches use masked Global Average Pooling (GAP) to encode an\nannotated support image to a feature vector to facilitate query image\nsegmentation. However, this pipeline unavoidably loses some discriminative\ninformation due to the average operation. In this paper, we propose a simple\nbut effective self-guided learning approach, where the lost critical\ninformation is mined. Specifically, through making an initial prediction for\nthe annotated support image, the covered and uncovered foreground regions are\nencoded to the primary and auxiliary support vectors using masked GAP,\nrespectively. By aggregating both primary and auxiliary support vectors, better\nsegmentation performances are obtained on query images. Enlightened by our\nself-guided module for 1-shot segmentation, we propose a cross-guided module\nfor multiple shot segmentation, where the final mask is fused using predictions\nfrom multiple annotated samples with high-quality support vectors contributing\nmore and vice versa. This module improves the final prediction in the inference\nstage without re-training. Extensive experiments show that our approach\nachieves new state-of-the-art performances on both PASCAL-5i and COCO-20i\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:36:41 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Bingfeng", ""], ["Xiao", "Jimin", ""], ["Qin", "Terry", ""]]}, {"id": "2103.16130", "submitter": "Jiwoong Choi", "authors": "Jiwoong Choi, Ismail Elezi, Hyuk-Jae Lee, Clement Farabet, Jose M.\n  Alvarez", "title": "Active Learning for Deep Object Detection via Probabilistic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to reduce labeling costs by selecting only the most\ninformative samples on a dataset. Few existing works have addressed active\nlearning for object detection. Most of these methods are based on multiple\nmodels or are straightforward extensions of classification methods, hence\nestimate an image's informativeness using only the classification head. In this\npaper, we propose a novel deep active learning approach for object detection.\nOur approach relies on mixture density networks that estimate a probabilistic\ndistribution for each localization and classification head's output. We\nexplicitly estimate the aleatoric and epistemic uncertainty in a single forward\npass of a single model. Our method uses a scoring function that aggregates\nthese two types of uncertainties for both heads to obtain every image's\ninformativeness score. We demonstrate the efficacy of our approach in PASCAL\nVOC and MS-COCO datasets. Our approach outperforms single-model based methods\nand performs on par with multi-model based methods at a fraction of the\ncomputing cost.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:37:11 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Choi", "Jiwoong", ""], ["Elezi", "Ismail", ""], ["Lee", "Hyuk-Jae", ""], ["Farabet", "Clement", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "2103.16146", "submitter": "Jong Chul Ye", "authors": "Gihyun Kwon, Jong Chul Ye", "title": "Diagonal Attention and Style-based GAN for Content-Style Disentanglement\n  in Image Generation and Translation", "comments": "ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the important research topics in image generative models is to\ndisentangle the spatial contents and styles for their separate control.\nAlthough StyleGAN can generate content feature vectors from random noises, the\nresulting spatial content control is primarily intended for minor spatial\nvariations, and the disentanglement of global content and styles is by no means\ncomplete. Inspired by a mathematical understanding of normalization and\nattention, here we present a novel hierarchical adaptive Diagonal spatial\nATtention (DAT) layers to separately manipulate the spatial contents from\nstyles in a hierarchical manner. Using DAT and AdaIN, our method enables\ncoarse-to-fine level disentanglement of spatial contents and styles. In\naddition, our generator can be easily integrated into the GAN inversion\nframework so that the content and style of translated images from multi-domain\nimage translation tasks can be flexibly controlled. By using various datasets,\nwe confirm that the proposed method not only outperforms the existing models in\ndisentanglement scores, but also provides more flexible control over spatial\nfeatures in the generated images.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:00:13 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 16:28:42 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Kwon", "Gihyun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2103.16148", "submitter": "Bo-Han Kung", "authors": "Pin-Chun Chen, Bo-Han Kung, and Jun-Cheng Chen", "title": "Class-Aware Robust Adversarial Training for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important computer vision task with plenty of\nreal-world applications; therefore, how to enhance its robustness against\nadversarial attacks has emerged as a crucial issue. However, most of the\nprevious defense methods focused on the classification task and had few\nanalysis in the context of the object detection task. In this work, to address\nthe issue, we present a novel class-aware robust adversarial training paradigm\nfor the object detection task. For a given image, the proposed approach\ngenerates an universal adversarial perturbation to simultaneously attack all\nthe occurred objects in the image through jointly maximizing the respective\nloss for each object. Meanwhile, instead of normalizing the total loss with the\nnumber of objects, the proposed approach decomposes the total loss into\nclass-wise losses and normalizes each class loss using the number of objects\nfor the class. The adversarial training based on the class weighted loss can\nnot only balances the influence of each class but also effectively and evenly\nimproves the adversarial robustness of trained models for all the object\nclasses as compared with the previous defense methods. Furthermore, with the\nrecent development of fast adversarial training, we provide a fast version of\nthe proposed algorithm which can be trained faster than the traditional\nadversarial training while keeping comparable performance. With extensive\nexperiments on the challenging PASCAL-VOC and MS-COCO datasets, the evaluation\nresults demonstrate that the proposed defense methods can effectively enhance\nthe robustness of the object detection models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:02:28 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 02:40:24 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chen", "Pin-Chun", ""], ["Kung", "Bo-Han", ""], ["Chen", "Jun-Cheng", ""]]}, {"id": "2103.16150", "submitter": "Rakshith S", "authors": "Rakshith S, Rishabh Khurana, Vibhav Agarwal, Jayesh Rajkumar Vachhani,\n  Guggilla Bhanodai", "title": "FONTNET: On-Device Font Understanding and Prediction Pipeline", "comments": "Accepted for publication in IEEE ICASSP 2021: 46th IEEE International\n  Conference on Acoustics, Speech, & Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fonts are one of the most basic and core design concepts. Numerous use cases\ncan benefit from an in depth understanding of Fonts such as Text Customization\nwhich can change text in an image while maintaining the Font attributes like\nstyle, color, size. Currently, Text recognition solutions can group recognized\ntext based on line breaks or paragraph breaks, if the Font attributes are known\nmultiple text blocks can be combined based on context in a meaningful manner.\nIn this paper, we propose two engines: Font Detection Engine, which identifies\nthe font style, color and size attributes of text in an image and a Font\nPrediction Engine, which predicts similar fonts for a query font. Major\ncontributions of this paper are three-fold: First, we developed a novel CNN\narchitecture for identifying font style of text in images. Second, we designed\na novel algorithm for predicting similar fonts for a given query font. Third,\nwe have optimized and deployed the entire engine On-Device which ensures\nprivacy and improves latency in real time applications such as instant\nmessaging. We achieve a worst case On-Device inference time of 30ms and a model\nsize of 4.5MB for both the engines.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:11:24 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["S", "Rakshith", ""], ["Khurana", "Rishabh", ""], ["Agarwal", "Vibhav", ""], ["Vachhani", "Jayesh Rajkumar", ""], ["Bhanodai", "Guggilla", ""]]}, {"id": "2103.16151", "submitter": "Subhadip Mukherjee", "authors": "Subhadip Mukherjee, Ozan \\\"Oktem, and Carola-Bibiane Sch\\\"onlieb", "title": "Adversarially learned iterative reconstruction for imaging inverse\n  problems", "comments": "Accepted to the Eighth International Conference on Scale Space and\n  Variational Methods in Computer Vision (SSVM), May-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In numerous practical applications, especially in medical image\nreconstruction, it is often infeasible to obtain a large ensemble of\nground-truth/measurement pairs for supervised learning. Therefore, it is\nimperative to develop unsupervised learning protocols that are competitive with\nsupervised approaches in performance. Motivated by the maximum-likelihood\nprinciple, we propose an unsupervised learning framework for solving ill-posed\ninverse problems. Instead of seeking pixel-wise proximity between the\nreconstructed and the ground-truth images, the proposed approach learns an\niterative reconstruction network whose output matches the ground-truth in\ndistribution. Considering tomographic reconstruction as an application, we\ndemonstrate that the proposed unsupervised approach not only performs on par\nwith its supervised variant in terms of objective quality measures but also\nsuccessfully circumvents the issue of over-smoothing that supervised approaches\ntend to suffer from. The improvement in reconstruction quality comes at the\nexpense of higher training complexity, but, once trained, the reconstruction\ntime remains the same as its supervised counterpart.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:18:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Mukherjee", "Subhadip", ""], ["\u00d6ktem", "Ozan", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2103.16155", "submitter": "Ziyi Liu", "authors": "Ziyi Liu, Le Wang, Wei Tang, Junsong Yuan, Nanning Zheng, Gang Hua", "title": "Weakly Supervised Temporal Action Localization Through Learning Explicit\n  Subspaces for Action and Context", "comments": "Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised Temporal Action Localization (WS-TAL) methods learn to\nlocalize temporal starts and ends of action instances in a video under only\nvideo-level supervision. Existing WS-TAL methods rely on deep features learned\nfor action recognition. However, due to the mismatch between classification and\nlocalization, these features cannot distinguish the frequently co-occurring\ncontextual background, i.e., the context, and the actual action instances. We\nterm this challenge action-context confusion, and it will adversely affect the\naction localization accuracy. To address this challenge, we introduce a\nframework that learns two feature subspaces respectively for actions and their\ncontext. By explicitly accounting for action visual elements, the action\ninstances can be localized more precisely without the distraction from the\ncontext. To facilitate the learning of these two feature subspaces with only\nvideo-level categorical labels, we leverage the predictions from both spatial\nand temporal streams for snippets grouping. In addition, an unsupervised\nlearning task is introduced to make the proposed module focus on mining\ntemporal information. The proposed approach outperforms state-of-the-art WS-TAL\nmethods on three benchmarks, i.e., THUMOS14, ActivityNet v1.2 and v1.3\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:26:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Ziyi", ""], ["Wang", "Le", ""], ["Tang", "Wei", ""], ["Yuan", "Junsong", ""], ["Zheng", "Nanning", ""], ["Hua", "Gang", ""]]}, {"id": "2103.16173", "submitter": "Zongyan Han", "authors": "Zongyan Han, Zhenyong Fu, Shuo Chen and Jian Yang", "title": "Contrastive Embedding for Generalized Zero-Shot Learning", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning (GZSL) aims to recognize objects from both\nseen and unseen classes, when only the labeled examples from seen classes are\nprovided. Recent feature generation methods learn a generative model that can\nsynthesize the missing visual features of unseen classes to mitigate the\ndata-imbalance problem in GZSL. However, the original visual feature space is\nsuboptimal for GZSL classification since it lacks discriminative information.\nTo tackle this issue, we propose to integrate the generation model with the\nembedding model, yielding a hybrid GZSL framework. The hybrid GZSL approach\nmaps both the real and the synthetic samples produced by the generation model\ninto an embedding space, where we perform the final GZSL classification.\nSpecifically, we propose a contrastive embedding (CE) for our hybrid GZSL\nframework. The proposed contrastive embedding can leverage not only the\nclass-wise supervision but also the instance-wise supervision, where the latter\nis usually neglected by existing GZSL researches. We evaluate our proposed\nhybrid GZSL framework with contrastive embedding, named CE-GZSL, on five\nbenchmark datasets. The results show that our CEGZSL method can outperform the\nstate-of-the-arts by a significant margin on three datasets. Our codes are\navailable on https://github.com/Hanzy1996/CE-GZSL.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:54:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Han", "Zongyan", ""], ["Fu", "Zhenyong", ""], ["Chen", "Shuo", ""], ["Yang", "Jian", ""]]}, {"id": "2103.16178", "submitter": "Jiawei He", "authors": "Jiawei He, Zehao Huang, Naiyan Wang, Zhaoxiang Zhang", "title": "Learnable Graph Matching: Incorporating Graph Partitioning with Deep\n  Feature Learning for Multiple Object Tracking", "comments": "CVPR 2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data association across frames is at the core of Multiple Object Tracking\n(MOT) task. This problem is usually solved by a traditional graph-based\noptimization or directly learned via deep learning. Despite their popularity,\nwe find some points worth studying in current paradigm: 1) Existing methods\nmostly ignore the context information among tracklets and intra-frame\ndetections, which makes the tracker hard to survive in challenging cases like\nsevere occlusion. 2) The end-to-end association methods solely rely on the data\nfitting power of deep neural networks, while they hardly utilize the advantage\nof optimization-based assignment methods. 3) The graph-based optimization\nmethods mostly utilize a separate neural network to extract features, which\nbrings the inconsistency between training and inference. Therefore, in this\npaper we propose a novel learnable graph matching method to address these\nissues. Briefly speaking, we model the relationships between tracklets and the\nintra-frame detections as a general undirected graph. Then the association\nproblem turns into a general graph matching between tracklet graph and\ndetection graph. Furthermore, to make the optimization end-to-end\ndifferentiable, we relax the original graph matching into continuous quadratic\nprogramming and then incorporate the training of it into a deep graph network\nwith the help of the implicit function theorem. Lastly, our method GMTracker,\nachieves state-of-the-art performance on several standard MOT datasets. Our\ncode will be available at https://github.com/jiaweihe1996/GMTracker .\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:58:45 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["He", "Jiawei", ""], ["Huang", "Zehao", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "2103.16183", "submitter": "Yifan Wang", "authors": "Yifan Wang, Andrew Liu, Richard Tucker, Jiajun Wu, Brian L. Curless,\n  Steven M. Seitz, Noah Snavely", "title": "Repopulating Street Scenes", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a framework for automatically reconfiguring images of street\nscenes by populating, depopulating, or repopulating them with objects such as\npedestrians or vehicles. Applications of this method include anonymizing images\nto enhance privacy, generating data augmentations for perception tasks like\nautonomous driving, and composing scenes to achieve a certain ambiance, such as\nempty streets in the early morning. At a technical level, our work has three\nprimary contributions: (1) a method for clearing images of objects, (2) a\nmethod for estimating sun direction from a single image, and (3) a way to\ncompose objects in scenes that respects scene geometry and illumination. Each\ncomponent is learned from data with minimal ground truth annotations, by making\ncreative use of large-numbers of short image bursts of street scenes. We\ndemonstrate convincing results on a range of street scenes and illustrate\npotential applications.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:04:46 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Yifan", ""], ["Liu", "Andrew", ""], ["Tucker", "Richard", ""], ["Wu", "Jiajun", ""], ["Curless", "Brian L.", ""], ["Seitz", "Steven M.", ""], ["Snavely", "Noah", ""]]}, {"id": "2103.16194", "submitter": "Daniela Mihai", "authors": "Daniela Mihai and Jonathon Hare", "title": "Differentiable Drawing and Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a bottom-up differentiable relaxation of the process of drawing\npoints, lines and curves into a pixel raster. Our approach arises from the\nobservation that rasterising a pixel in an image given parameters of a\nprimitive can be reformulated in terms of the primitive's distance transform,\nand then relaxed to allow the primitive's parameters to be learned. This\nrelaxation allows end-to-end differentiable programs and deep networks to be\nlearned and optimised and provides several building blocks that allow control\nover how a compositional drawing process is modelled. We emphasise the\nbottom-up nature of our proposed approach, which allows for drawing operations\nto be composed in ways that can mimic the physical reality of drawing rather\nthan being tied to, for example, approaches in modern computer graphics. With\nthe proposed approach we demonstrate how sketches can be generated by directly\noptimising against photographs and how auto-encoders can be built to transform\nrasterised handwritten digits into vectors without supervision. Extensive\nexperimental results highlight the power of this approach under different\nmodelling assumptions for drawing tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:25:55 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 13:16:47 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "2103.16201", "submitter": "Alexander Bartler", "authors": "Alexander Bartler, Andre B\\\"uhler, Felix Wiewel, Mario D\\\"obler and\n  Bin Yang", "title": "MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An unresolved problem in Deep Learning is the ability of neural networks to\ncope with domain shifts during test-time, imposed by commonly fixing network\nparameters after training. Our proposed method Meta Test-Time Training (MT3),\nhowever, breaks this paradigm and enables adaption at test-time. We combine\nmeta-learning, self-supervision and test-time training to learn to adapt to\nunseen test distributions. By minimizing the self-supervised loss, we learn\ntask-specific model parameters for different tasks. A meta-model is optimized\nsuch that its adaption to the different task-specific models leads to higher\nperformance on those tasks. During test-time a single unlabeled image is\nsufficient to adapt the meta-model parameters. This is achieved by minimizing\nonly the self-supervised loss component resulting in a better prediction for\nthat image. Our approach significantly improves the state-of-the-art results on\nthe CIFAR-10-Corrupted image classification benchmark. Our implementation is\navailable on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:33:38 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bartler", "Alexander", ""], ["B\u00fchler", "Andre", ""], ["Wiewel", "Felix", ""], ["D\u00f6bler", "Mario", ""], ["Yang", "Bin", ""]]}, {"id": "2103.16204", "submitter": "Lina Zhuang", "authors": "Lianru Gao, Zhicheng Wang, Lina Zhuang, Haoyang Yu, Bing Zhang,\n  Jocelyn Chanussot", "title": "Using Low-rank Representation of Abundance Maps and Nonnegative Tensor\n  Factorization for Hyperspectral Nonlinear Unmixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor-based methods have been widely studied to attack inverse problems in\nhyperspectral imaging since a hyperspectral image (HSI) cube can be naturally\nrepresented as a third-order tensor, which can perfectly retain the spatial\ninformation in the image. In this article, we extend the linear tensor method\nto the nonlinear tensor method and propose a nonlinear low-rank tensor unmixing\nalgorithm to solve the generalized bilinear model (GBM). Specifically, the\nlinear and nonlinear parts of the GBM can both be expressed as tensors.\nFurthermore, the low-rank structures of abundance maps and nonlinear\ninteraction abundance maps are exploited by minimizing their nuclear norm, thus\ntaking full advantage of the high spatial correlation in HSIs. Synthetic and\nreal-data experiments show that the low rank of abundance maps and nonlinear\ninteraction abundance maps exploited in our method can improve the performance\nof the nonlinear unmixing. A MATLAB demo of this work will be available at\nhttps://github.com/LinaZhuang for the sake of reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:37:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Gao", "Lianru", ""], ["Wang", "Zhicheng", ""], ["Zhuang", "Lina", ""], ["Yu", "Haoyang", ""], ["Zhang", "Bing", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "2103.16206", "submitter": "Hyeonjun Sim", "authors": "Hyeonjun Sim, Jihyong Oh, Munchurl Kim", "title": "XVFI: eXtreme Video Frame Interpolation", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000\nfps with the extreme motion to the research community for video frame\ninterpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that\nfirst handles the VFI for 4K videos with large motion. The XVFI-Net is based on\na recursive multi-scale shared structure that consists of two cascaded modules\nfor bidirectional optical flow learning between two input frames (BiOF-I) and\nfor bidirectional optical flow learning from target to input frames (BiOF-T).\nThe optical flows are stably approximated by a complementary flow reversal\n(CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start\nat any scale of input while the BiOF-T module only operates at the original\ninput scale so that the inference can be accelerated while maintaining highly\naccurate VFI performance. Extensive experimental results show that our XVFI-Net\ncan successfully capture the essential information of objects with extremely\nlarge motions and complex textures while the state-of-the-art methods exhibit\npoor performance. Furthermore, our XVFI-Net framework also performs comparably\non the previous lower resolution benchmark dataset, which shows a robustness of\nour algorithm as well. All source codes, pre-trained models, and proposed\nX4K1000FPS datasets are publicly available at\nhttps://github.com/JihyongOh/XVFI.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:38:30 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sim", "Hyeonjun", ""], ["Oh", "Jihyong", ""], ["Kim", "Munchurl", ""]]}, {"id": "2103.16214", "submitter": "Arthur Ouaknine", "authors": "Arthur Ouaknine, Alasdair Newson, Patrick P\\'erez, Florence Tupin,\n  Julien Rebut", "title": "Multi-View Radar Semantic Segmentation", "comments": "15 pages, 8 figures. Preprint. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the scene around the ego-vehicle is key to assisted and\nautonomous driving. Nowadays, this is mostly conducted using cameras and laser\nscanners, despite their reduced performances in adverse weather conditions.\nAutomotive radars are low-cost active sensors that measure properties of\nsurrounding objects, including their relative speed, and have the key advantage\nof not being impacted by rain, snow or fog. However, they are seldom used for\nscene understanding due to the size and complexity of radar raw data and the\nlack of annotated datasets. Fortunately, recent open-sourced datasets have\nopened up research on classification, object detection and semantic\nsegmentation with raw radar signals using end-to-end trainable models. In this\nwork, we propose several novel architectures, and their associated losses,\nwhich analyse multiple \"views\" of the range-angle-Doppler radar tensor to\nsegment it semantically. Experiments conducted on the recent CARRADA dataset\ndemonstrate that our best model outperforms alternative models, derived either\nfrom the semantic segmentation of natural images or from radar scene\nunderstanding, while requiring significantly fewer parameters. Both our code\nand trained models will be released.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:56:41 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ouaknine", "Arthur", ""], ["Newson", "Alasdair", ""], ["P\u00e9rez", "Patrick", ""], ["Tupin", "Florence", ""], ["Rebut", "Julien", ""]]}, {"id": "2103.16219", "submitter": "Xuning Shao", "authors": "Xuning Shao, Weidong Zhang", "title": "SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised\n  Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For unsupervised image-to-image translation, we propose a discriminator\narchitecture which focuses on the statistical features instead of individual\npatches. The network is stabilized by distribution matching of key statistical\nfeatures at multiple scales. Unlike the existing methods which impose more and\nmore constraints on the generator, our method facilitates the shape deformation\nand enhances the fine details with a greatly simplified framework. We show that\nthe proposed method outperforms the existing state-of-the-art models in various\nchallenging applications including selfie-to-anime, male-to-female and glasses\nremoval. The code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:03:07 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shao", "Xuning", ""], ["Zhang", "Weidong", ""]]}, {"id": "2103.16229", "submitter": "Michail Christos Doukas", "authors": "Michail Christos Doukas, Mohammad Rami Koujan, Viktoriia Sharmanska,\n  Stefanos Zafeiriou", "title": "Head2HeadFS: Video-based Head Reenactment with Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, a substantial amount of work has been done on the\nproblem of facial reenactment, with the solutions coming mainly from the\ngraphics community. Head reenactment is an even more challenging task, which\naims at transferring not only the facial expression, but also the entire head\npose from a source person to a target. Current approaches either train\nperson-specific systems, or use facial landmarks to model human heads, a\nrepresentation that might transfer unwanted identity attributes from the source\nto the target. We propose head2headFS, a novel easily adaptable pipeline for\nhead reenactment. We condition synthesis of the target person on dense 3D face\nshape information from the source, which enables high quality expression and\npose transfer. Our video-based rendering network is fine-tuned under a few-shot\nlearning strategy, using only a few samples. This allows for fast adaptation of\na generic generator trained on a multiple-person dataset, into a\nperson-specific one.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:19:41 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Doukas", "Michail Christos", ""], ["Koujan", "Mohammad Rami", ""], ["Sharmanska", "Viktoriia", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2103.16237", "submitter": "Xinzhu Ma", "authors": "Xinzhu Ma, Yinmin Zhang, Dan Xu, Dongzhan Zhou, Shuai Yi, Haojie Li,\n  Wanli Ouyang", "title": "Delving into Localization Errors for Monocular 3D Object Detection", "comments": "CVPR'2021, code will be made available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D bounding boxes from monocular images is an essential component\nin autonomous driving, while accurate 3D object detection from this kind of\ndata is very challenging. In this work, by intensive diagnosis experiments, we\nquantify the impact introduced by each sub-task and found the `localization\nerror' is the vital factor in restricting monocular 3D detection. Besides, we\nalso investigate the underlying reasons behind localization errors, analyze the\nissues they might bring, and propose three strategies. First, we revisit the\nmisalignment between the center of the 2D bounding box and the projected center\nof the 3D object, which is a vital factor leading to low localization accuracy.\nSecond, we observe that accurately localizing distant objects with existing\ntechnologies is almost impossible, while those samples will mislead the learned\nnetwork. To this end, we propose to remove such samples from the training set\nfor improving the overall performance of the detector. Lastly, we also propose\na novel 3D IoU oriented loss for the size estimation of the object, which is\nnot affected by `localization error'. We conduct extensive experiments on the\nKITTI dataset, where the proposed method achieves real-time detection and\noutperforms previous methods by a large margin. The code will be made available\nat: https://github.com/xinzhuma/monodle.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:38:01 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ma", "Xinzhu", ""], ["Zhang", "Yinmin", ""], ["Xu", "Dan", ""], ["Zhou", "Dongzhan", ""], ["Yi", "Shuai", ""], ["Li", "Haojie", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2103.16241", "submitter": "Tonmoy Saikia", "authors": "Tonmoy Saikia, Cordelia Schmid, Thomas Brox", "title": "Improving robustness against common corruptions with frequency biased\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs perform remarkably well when the training and test distributions are\ni.i.d, but unseen image corruptions can cause a surprisingly large drop in\nperformance. In various real scenarios, unexpected distortions, such as random\nnoise, compression artefacts, or weather distortions are common phenomena.\nImproving performance on corrupted images must not result in degraded i.i.d\nperformance - a challenge faced by many state-of-the-art robust approaches.\nImage corruption types have different characteristics in the frequency spectrum\nand would benefit from a targeted type of data augmentation, which, however, is\noften unknown during training. In this paper, we introduce a mixture of two\nexpert models specializing in high and low-frequency robustness, respectively.\nMoreover, we propose a new regularization scheme that minimizes the total\nvariation (TV) of convolution feature-maps to increase high-frequency\nrobustness. The approach improves on corrupted images without degrading\nin-distribution performance. We demonstrate this on ImageNet-C and also for\nreal-world corruptions on an automotive dataset, both for object classification\nand object detection.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:44:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Saikia", "Tonmoy", ""], ["Schmid", "Cordelia", ""], ["Brox", "Thomas", ""]]}, {"id": "2103.16255", "submitter": "Tonmoy Saikia", "authors": "Simon Schrodi, Tonmoy Saikia, Thomas Brox", "title": "What Causes Optical Flow Networks to be Vulnerable to Physical\n  Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work demonstrated the lack of robustness of optical flow networks to\nphysical, patch-based adversarial attacks. The possibility to physically attack\na basic component of automotive systems is a reason for serious concerns. In\nthis paper, we analyze the cause of the problem and show that the lack of\nrobustness is rooted in the classical aperture problem of optical flow\nestimation in combination with bad choices in the details of the network\narchitecture. We show how these mistakes can be rectified in order to make\noptical flow networks robust to physical, patch-based attacks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:12:46 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Schrodi", "Simon", ""], ["Saikia", "Tonmoy", ""], ["Brox", "Thomas", ""]]}, {"id": "2103.16257", "submitter": "Qinbin Li", "authors": "Qinbin Li, Bingsheng He, Dawn Song", "title": "Model-Contrastive Federated Learning", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables multiple parties to collaboratively train a\nmachine learning model without communicating their local data. A key challenge\nin federated learning is to handle the heterogeneity of local data distribution\nacross parties. Although many studies have been proposed to address this\nchallenge, we find that they fail to achieve high performance in image datasets\nwith deep learning models. In this paper, we propose MOON: model-contrastive\nfederated learning. MOON is a simple and effective federated learning\nframework. The key idea of MOON is to utilize the similarity between model\nrepresentations to correct the local training of individual parties, i.e.,\nconducting contrastive learning in model-level. Our extensive experiments show\nthat MOON significantly outperforms the other state-of-the-art federated\nlearning algorithms on various image classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:16:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Qinbin", ""], ["He", "Bingsheng", ""], ["Song", "Dawn", ""]]}, {"id": "2103.16262", "submitter": "Jiahao Lu", "authors": "Jiahao Lu, Johan \\\"Ofverstedt, Joakim Lindblad, Nata\\v{s}a Sladoje", "title": "Is Image-to-Image Translation the Panacea for Multimodal Image\n  Registration? A Comparative Study", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite current advancement in the field of biomedical image processing,\npropelled by the deep learning revolution, multimodal image registration, due\nto its several challenges, is still often performed manually by specialists.\nThe recent success of image-to-image (I2I) translation in computer vision\napplications and its growing use in biomedical areas provide a tempting\npossibility of transforming the multimodal registration problem into a,\npotentially easier, monomodal one. We conduct an empirical study of the\napplicability of modern I2I translation methods for the task of multimodal\nbiomedical image registration. We compare the performance of four Generative\nAdversarial Network (GAN)-based methods and one contrastive representation\nlearning method, subsequently combined with two representative monomodal\nregistration methods, to judge the effectiveness of modality translation for\nmultimodal image registration. We evaluate these method combinations on three\npublicly available multimodal datasets of increasing difficulty, and compare\nwith the performance of registration by Mutual Information maximisation and one\nmodern data-specific multimodal registration method. Our results suggest that,\nalthough I2I translation may be helpful when the modalities to register are\nclearly correlated, registration of modalities which express distinctly\ndifferent properties of the sample are not well handled by the I2I translation\napproach. When less information is shared between the modalities, the I2I\ntranslation methods struggle to provide good predictions, which impairs the\nregistration performance. The evaluated representation learning method, which\naims to find an in-between representation, manages better, and so does the\nMutual Information maximisation approach. We share our complete experimental\nsetup as open-source (https://github.com/Noodles-321/Registration).\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:28:21 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lu", "Jiahao", ""], ["\u00d6fverstedt", "Johan", ""], ["Lindblad", "Joakim", ""], ["Sladoje", "Nata\u0161a", ""]]}, {"id": "2103.16265", "submitter": "Steffen Czolbe", "authors": "Steffen Czolbe, Kasra Arnavaz, Oswin Krause, Aasa Feragen", "title": "Is segmentation uncertainty useful?", "comments": "Published at Information Processing in Medical Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic image segmentation encodes varying prediction confidence and\ninherent ambiguity in the segmentation problem. While different probabilistic\nsegmentation models are designed to capture different aspects of segmentation\nuncertainty and ambiguity, these modelling differences are rarely discussed in\nthe context of applications of uncertainty. We consider two common use cases of\nsegmentation uncertainty, namely assessment of segmentation quality and active\nlearning. We consider four established strategies for probabilistic\nsegmentation, discuss their modelling capabilities, and investigate their\nperformance in these two tasks. We find that for all models and both tasks,\nreturned uncertainty correlates positively with segmentation error, but does\nnot prove to be useful for active learning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:34:28 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Czolbe", "Steffen", ""], ["Arnavaz", "Kasra", ""], ["Krause", "Oswin", ""], ["Feragen", "Aasa", ""]]}, {"id": "2103.16273", "submitter": "Jinbiao Lin", "authors": "Bo Dong, Hao Liu, Yu Bai, Jinbiao Lin, Zhuoran Xu, Xinyu Xu, Qi Kong", "title": "Multi-modal Trajectory Prediction for Autonomous Driving with Semantic\n  Map and Dynamic Graph Attention Network", "comments": "NIPS2020 Workshop on Machine Learning for Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future trajectories of surrounding obstacles is a crucial task for\nautonomous driving cars to achieve a high degree of road safety. There are\nseveral challenges in trajectory prediction in real-world traffic scenarios,\nincluding obeying traffic rules, dealing with social interactions, handling\ntraffic of multi-class movement, and predicting multi-modal trajectories with\nprobability. Inspired by people's natural habit of navigating traffic with\nattention to their goals and surroundings, this paper presents a unique dynamic\ngraph attention network to solve all those challenges. The network is designed\nto model the dynamic social interactions among agents and conform to traffic\nrules with a semantic map. By extending the anchor-based method to multiple\ntypes of agents, the proposed method can predict multi-modal trajectories with\nprobabilities for multi-class movements using a single model. We validate our\napproach on the proprietary autonomous driving dataset for the logistic\ndelivery scenario and two publicly available datasets. The results show that\nour method outperforms state-of-the-art techniques and demonstrates the\npotential for trajectory prediction in real-world traffic.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:53:12 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Dong", "Bo", ""], ["Liu", "Hao", ""], ["Bai", "Yu", ""], ["Lin", "Jinbiao", ""], ["Xu", "Zhuoran", ""], ["Xu", "Xinyu", ""], ["Kong", "Qi", ""]]}, {"id": "2103.16284", "submitter": "Tao Kong", "authors": "Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, Tieniu Tan", "title": "Locate then Segment: A Strong Pipeline for Referring Image Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Referring image segmentation aims to segment the objects referred by a\nnatural language expression. Previous methods usually focus on designing an\nimplicit and recurrent feature interaction mechanism to fuse the\nvisual-linguistic features to directly generate the final segmentation mask\nwithout explicitly modeling the localization information of the referent\ninstances. To tackle these problems, we view this task from another perspective\nby decoupling it into a \"Locate-Then-Segment\" (LTS) scheme. Given a language\nexpression, people generally first perform attention to the corresponding\ntarget image regions, then generate a fine segmentation mask about the object\nbased on its context. The LTS first extracts and fuses both visual and textual\nfeatures to get a cross-modal representation, then applies a cross-model\ninteraction on the visual-textual features to locate the referred object with\nposition prior, and finally generates the segmentation result with a\nlight-weight segmentation network. Our LTS is simple but surprisingly\neffective. On three popular benchmark datasets, the LTS outperforms all the\nprevious state-of-the-art methods by a large margin (e.g., +3.2% on RefCOCO+\nand +3.4% on RefCOCOg). In addition, our model is more interpretable with\nexplicitly locating the object, which is also proved by visualization\nexperiments. We believe this framework is promising to serve as a strong\nbaseline for referring image segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:25:27 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Jing", "Ya", ""], ["Kong", "Tao", ""], ["Wang", "Wei", ""], ["Wang", "Liang", ""], ["Li", "Lei", ""], ["Tan", "Tieniu", ""]]}, {"id": "2103.16285", "submitter": "Sahar Almahfouz Nasser", "authors": "Sahar A. Nasser, Debjani Paul, and Suyash P. Awate", "title": "Single Test Image-Based Automated Machine Learning System for\n  Distinguishing between Trait and Diseased Blood Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a machine learning-based method for fully automated diagnosis of\nsickle cell disease of poor-quality unstained images of a mobile microscope.\nOur method is capable of distinguishing between diseased, trait (carrier), and\nnormal samples unlike the previous methods that are limited to distinguishing\nthe normal from the abnormal samples only. The novelty of this method comes\nfrom distinguishing the trait and the diseased samples from challenging images\nthat have been captured directly in the field. The proposed approach contains\ntwo parts, the segmentation part followed by the classification part. We use a\nrandom forest algorithm to segment such challenging images acquitted through a\nmobile phone-based microscope. Then, we train two classifiers based on a random\nforest (RF) and a support vector machine (SVM) for classification. The results\nshow superior performances of both of the classifiers not only for images which\nhave been captured in the lab, but also for the ones which have been acquired\nin the field itself.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:29:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Nasser", "Sahar A.", ""], ["Paul", "Debjani", ""], ["Awate", "Suyash P.", ""]]}, {"id": "2103.16291", "submitter": "Weizhe Liu", "authors": "Weizhe Liu, Nikita Durasov, Pascal Fua", "title": "Leveraging Self-Supervision for Cross-Domain Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for counting people in crowded scenes rely on deep\nnetworks to estimate crowd density. While effective, these data-driven\napproaches rely on large amount of data annotation to achieve good performance,\nwhich stops these models from being deployed in emergencies during which data\nannotation is either too costly or cannot be obtained fast enough.\n  One popular solution is to use synthetic data for training. Unfortunately,\ndue to domain shift, the resulting models generalize poorly on real imagery. We\nremedy this shortcoming by training with both synthetic images, along with\ntheir associated labels, and unlabeled real images. To this end, we force our\nnetwork to learn perspective-aware features by training it to recognize\nupside-down real images from regular ones and incorporate into it the ability\nto predict its own uncertainty so that it can generate useful pseudo labels for\nfine-tuning purposes. This yields an algorithm that consistently outperforms\nstate-of-the-art cross-domain crowd counting ones without any extra computation\nat inference time.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:37:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Weizhe", ""], ["Durasov", "Nikita", ""], ["Fua", "Pascal", ""]]}, {"id": "2103.16302", "submitter": "Byeongho Heo", "authors": "Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe,\n  Seong Joon Oh", "title": "Rethinking Spatial Dimensions of Vision Transformers", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Vision Transformer (ViT) extends the application range of transformers from\nlanguage processing to computer vision tasks as being an alternative\narchitecture against the existing convolutional neural networks (CNN). Since\nthe transformer-based architecture has been innovative for computer vision\nmodeling, the design convention towards an effective architecture has been less\nstudied yet. From the successful design principles of CNN, we investigate the\nrole of the spatial dimension conversion and its effectiveness on the\ntransformer-based architecture. We particularly attend the dimension reduction\nprinciple of CNNs; as the depth increases, a conventional CNN increases channel\ndimension and decreases spatial dimensions. We empirically show that such a\nspatial dimension reduction is beneficial to a transformer architecture as\nwell, and propose a novel Pooling-based Vision Transformer (PiT) upon the\noriginal ViT model. We show that PiT achieves the improved model capability and\ngeneralization performance against ViT. Throughout the extensive experiments,\nwe further show PiT outperforms the baseline on several tasks such as image\nclassification, object detection and robustness evaluation. Source codes and\nImageNet models are available at https://github.com/naver-ai/pit\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:51:28 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Heo", "Byeongho", ""], ["Yun", "Sangdoo", ""], ["Han", "Dongyoon", ""], ["Chun", "Sanghyuk", ""], ["Choe", "Junsuk", ""], ["Oh", "Seong Joon", ""]]}, {"id": "2103.16317", "submitter": "Romain Br\\'egier", "authors": "Romain Br\\'egier", "title": "Deep regression on manifolds: a 3D rotation case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning involve regressing outputs that do not lie\non a Euclidean space, such as a discrete probability distribution, or the pose\nof an object. An approach to tackle these problems through gradient-based\nlearning consists in including in the deep learning architecture a\ndifferentiable function mapping arbitrary inputs of a Euclidean space onto this\nmanifold. In this work, we establish a set of properties that such mapping\nshould satisfy to allow proper training, and illustrate it in the case of 3D\nrotations. Through theoretical considerations and methodological experiments on\na variety of tasks, we compare various differentiable mappings on the 3D\nrotation space, and conjecture about the importance of the local linearity of\nthe mapping. We notably show that a mapping based on Procrustes\northonormalization of a 3x3 matrix generally performs best among the ones\nconsidered, but that rotation-vector representation might also be suitable when\nrestricted to small angles.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:07:36 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Br\u00e9gier", "Romain", ""]]}, {"id": "2103.16324", "submitter": "Csaba Kert\\'esz", "authors": "Csaba Kert\\'esz", "title": "Automated Cleanup of the ImageNet Dataset by Model Consensus,\n  Explainability and Confident Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The convolutional neural networks (CNNs) trained on ILSVRC12 ImageNet were\nthe backbone of various applications as a generic classifier, a feature\nextractor or a base model for transfer learning. This paper describes automated\nheuristics based on model consensus, explainability and confident learning to\ncorrect labeling mistakes and remove ambiguous images from this dataset. After\nmaking these changes on the training and validation sets, the ImageNet-Clean\nimproves the model performance by 2-2.4 % for SqueezeNet and EfficientNet-B0\nmodels. The results support the importance of larger image corpora and\nsemi-supervised learning, but the original datasets must be fixed to avoid\ntransmitting their mistakes and biases to the student learner. Further\ncontributions describe the training impacts of widescreen input resolutions in\nportrait and landscape orientations. The trained models and scripts are\npublished on Github (https://github.com/kecsap/imagenet-clean) to clean up\nImageNet and ImageNetV2 datasets for reproducible research.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:16:35 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kert\u00e9sz", "Csaba", ""]]}, {"id": "2103.16327", "submitter": "Yueming Jin", "authors": "Yueming Jin, Yonghao Long, Cheng Chen, Zixu Zhao, Qi Dou, Pheng-Ann\n  Heng", "title": "Temporal Memory Relation Network for Workflow Recognition from Surgical\n  Video", "comments": "Accepted at IEEE Transactions on Medical Imaging (IEEE TMI); Code is\n  available at https://github.com/YuemingJin/TMRNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic surgical workflow recognition is a key component for developing\ncontext-aware computer-assisted systems in the operating theatre. Previous\nworks either jointly modeled the spatial features with short fixed-range\ntemporal information, or separately learned visual and long temporal cues. In\nthis paper, we propose a novel end-to-end temporal memory relation network\n(TMRNet) for relating long-range and multi-scale temporal patterns to augment\nthe present features. We establish a long-range memory bank to serve as a\nmemory cell storing the rich supportive information. Through our designed\ntemporal variation layer, the supportive cues are further enhanced by\nmulti-scale temporal-only convolutions. To effectively incorporate the two\ntypes of cues without disturbing the joint learning of spatio-temporal\nfeatures, we introduce a non-local bank operator to attentively relate the past\nto the present. In this regard, our TMRNet enables the current feature to view\nthe long-range temporal dependency, as well as tolerate complex temporal\nextents. We have extensively validated our approach on two benchmark surgical\nvideo datasets, M2CAI challenge dataset and Cholec80 dataset. Experimental\nresults demonstrate the outstanding performance of our method, consistently\nexceeding the state-of-the-art methods by a large margin (e.g., 67.0% v.s.\n78.9% Jaccard on Cholec80 dataset).\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:20:26 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Jin", "Yueming", ""], ["Long", "Yonghao", ""], ["Chen", "Cheng", ""], ["Zhao", "Zixu", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2103.16328", "submitter": "Antonio Garcia-Uceda Juarez", "authors": "A. Garcia-Uceda, R. Selvan, Z. Saghir, H.A.W.M. Tiddens, M. de Bruijne", "title": "Automatic airway segmentation from Computed Tomography using robust and\n  efficient 3-D convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fully automatic and end-to-end optimised airway\nsegmentation method for thoracic computed tomography, based on the U-Net\narchitecture. We use a simple and low-memory 3D U-Net as backbone, which allows\nthe method to process large 3D image patches, often comprising full lungs, in a\nsingle pass through the network. This makes the method simple, robust and\nefficient. We validated the proposed method on three datasets with very\ndifferent characteristics and various airway abnormalities: i) a dataset of\npediatric patients including subjects with cystic fibrosis, ii) a subset of the\nDanish Lung Cancer Screening Trial, including subjects with chronic obstructive\npulmonary disease, and iii) the EXACT'09 public dataset. We compared our method\nwith other state-of-the-art airway segmentation methods, including relevant\nlearning-based methods in the literature evaluated on the EXACT'09 data. We\nshow that our method can extract highly complete airway trees with few false\npositive errors, on scans from both healthy and diseased subjects, and also\nthat the method generalizes well across different datasets. On the EXACT'09\ntest set, our method achieved the second highest sensitivity score among all\nmethods that reported good specificity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:21:02 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Garcia-Uceda", "A.", ""], ["Selvan", "R.", ""], ["Saghir", "Z.", ""], ["Tiddens", "H. A. W. M.", ""], ["de Bruijne", "M.", ""]]}, {"id": "2103.16341", "submitter": "Jiapeng Tang", "authors": "Jiapeng Tang, Dan Xu, Kui Jia, Lei Zhang", "title": "Learning Parallel Dense Correspondence from Spatio-Temporal Descriptors\n  for Efficient and Robust 4D Reconstruction", "comments": "15 pages, 11 figures, CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the task of 4D shape reconstruction from a sequence of\npoint clouds. Despite the recent success achieved by extending deep implicit\nrepresentations into 4D space, it is still a great challenge in two respects,\ni.e. how to design a flexible framework for learning robust spatio-temporal\nshape representations from 4D point clouds, and develop an efficient mechanism\nfor capturing shape dynamics. In this work, we present a novel pipeline to\nlearn a temporal evolution of the 3D human shape through spatially continuous\ntransformation functions among cross-frame occupancy fields. The key idea is to\nparallelly establish the dense correspondence between predicted occupancy\nfields at different time steps via explicitly learning continuous displacement\nvector fields from robust spatio-temporal shape representations. Extensive\ncomparisons against previous state-of-the-arts show the superior accuracy of\nour approach for 4D human reconstruction in the problems of 4D shape\nauto-encoding and completion, and a much faster network inference with about 8\ntimes speedup demonstrates the significant efficiency of our approach. The\ntrained models and implementation code are available at\nhttps://github.com/tangjiapeng/LPDC-Net.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:36:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Tang", "Jiapeng", ""], ["Xu", "Dan", ""], ["Jia", "Kui", ""], ["Zhang", "Lei", ""]]}, {"id": "2103.16344", "submitter": "Hong-Yu Zhou", "authors": "Hong-Yu Zhou, Hualuo Liu, Shilei Cao, Dong Wei, Chixiang Lu, Yizhou\n  Yu, Kai Ma, Yefeng Zheng", "title": "Generalized Organ Segmentation by Imitating One-shot Reasoning using\n  Anatomical Correlation", "comments": "IPMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning by imitation is one of the most significant abilities of human\nbeings and plays a vital role in human's computational neural system. In\nmedical image analysis, given several exemplars (anchors), experienced\nradiologist has the ability to delineate unfamiliar organs by imitating the\nreasoning process learned from existing types of organs. Inspired by this\nobservation, we propose OrganNet which learns a generalized organ concept from\na set of annotated organ classes and then transfer this concept to unseen\nclasses. In this paper, we show that such process can be integrated into the\none-shot segmentation task which is a very challenging but meaningful topic. We\npropose pyramid reasoning modules (PRMs) to model the anatomical correlation\nbetween anchor and target volumes. In practice, the proposed module first\ncomputes a correlation matrix between target and anchor computerized tomography\n(CT) volumes. Then, this matrix is used to transform the feature\nrepresentations of both anchor volume and its segmentation mask. Finally,\nOrganNet learns to fuse the representations from various inputs and predicts\nsegmentation results for target volume. Extensive experiments show that\nOrganNet can effectively resist the wide variations in organ morphology and\nproduce state-of-the-art results in one-shot segmentation task. Moreover, even\nwhen compared with fully-supervised segmentation models, OrganNet is still able\nto produce satisfying segmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:41:12 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhou", "Hong-Yu", ""], ["Liu", "Hualuo", ""], ["Cao", "Shilei", ""], ["Wei", "Dong", ""], ["Lu", "Chixiang", ""], ["Yu", "Yizhou", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2103.16350", "submitter": "Shaopeng Guo", "authors": "Shaopeng Guo, Yujie Wang, Kun Yuan, Quanquan Li", "title": "Differentiable Network Adaption with Elastic Search Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a novel network adaption method called\nDifferentiable Network Adaption (DNA), which can adapt an existing network to a\nspecific computation budget by adjusting the width and depth in a\ndifferentiable manner. The gradient-based optimization allows DNA to achieve an\nautomatic optimization of width and depth rather than previous heuristic\nmethods that heavily rely on human priors. Moreover, we propose a new elastic\nsearch space that can flexibly condense or expand during the optimization\nprocess, allowing the network optimization of width and depth in a bi-direction\nmanner. By DNA, we successfully achieve network architecture optimization by\ncondensing and expanding in both width and depth dimensions. Extensive\nexperiments on ImageNet demonstrate that DNA can adapt the existing network to\nmeet different targeted computation requirements with better performance than\nprevious methods. What's more, DNA can further improve the performance of\nhigh-accuracy networks obtained by state-of-the-art neural architecture search\nmethods such as EfficientNet and MobileNet-v3.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:49:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Guo", "Shaopeng", ""], ["Wang", "Yujie", ""], ["Yuan", "Kun", ""], ["Li", "Quanquan", ""]]}, {"id": "2103.16352", "submitter": "Filippos Kokkinos", "authors": "Filippos Kokkinos, Iasonas Kokkinos", "title": "Learning monocular 3D reconstruction of articulated categories from\n  motion", "comments": "Accepted to CVPR2021. For project website see\n  https://fkokkinos.github.io/video_3d_reconstruction/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Monocular 3D reconstruction of articulated object categories is challenging\ndue to the lack of training data and the inherent ill-posedness of the problem.\nIn this work we use video self-supervision, forcing the consistency of\nconsecutive 3D reconstructions by a motion-based cycle loss. This largely\nimproves both optimization-based and learning-based 3D mesh reconstruction. We\nfurther introduce an interpretable model of 3D template deformations that\ncontrols a 3D surface through the displacement of a small number of local,\nlearnable handles. We formulate this operation as a structured layer relying on\nmesh-laplacian regularization and show that it can be trained in an end-to-end\nmanner. We finally introduce a per-sample numerical optimisation approach that\njointly optimises over mesh displacements and cameras within a video, boosting\naccuracy both for training and also as test time post-processing. While relying\nexclusively on a small set of videos collected per category for supervision, we\nobtain state-of-the-art reconstructions with diverse shapes, viewpoints and\ntextures for multiple articulated object categories.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:50:27 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 15:14:18 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Kokkinos", "Filippos", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "2103.16364", "submitter": "Hao Chen", "authors": "Hao Chen, Benoit Lagadec, Francois Bremond", "title": "ICE: Inter-instance Contrastive Encoding for Unsupervised Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised person re-identification (ReID) aims at learning discriminative\nidentity features without annotations. Recently, self-supervised contrastive\nlearning has gained increasing attention for its effectiveness in unsupervised\nrepresentation learning. The main idea of instance contrastive learning is to\nmatch a same instance in different augmented views. However, the relationship\nbetween different instances of a same identity has not been explored in\nprevious methods, leading to sub-optimal ReID performance. To address this\nissue, we propose Inter-instance Contrastive Encoding (ICE) that leverages\ninter-instance pairwise similarity scores to boost previous class-level\ncontrastive ReID methods. We first use pairwise similarity ranking as one-hot\nhard pseudo labels for hard instance contrast, which aims at reducing\nintra-class variance. Then, we use similarity scores as soft pseudo labels to\nenhance the consistency between augmented and original views, which makes our\nmodel more robust to augmentation perturbations. Experiments on several\nlarge-scale person ReID datasets validate the effectiveness of our proposed\nunsupervised method ICE, which is competitive with even supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:05:09 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Hao", ""], ["Lagadec", "Benoit", ""], ["Bremond", "Francois", ""]]}, {"id": "2103.16365", "submitter": "Qi Sun", "authors": "Nianchen Deng and Zhenyi He and Jiannan Ye and Praneeth Chakravarthula\n  and Xubo Yang and Qi Sun", "title": "Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual\n  Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional high-quality 3D graphics requires large volumes of fine-detailed\nscene data for rendering. This demand compromises computational efficiency and\nlocal storage resources. Specifically, it becomes more concerning for future\nwearable and portable virtual and augmented reality (VR/AR) displays. Recent\napproaches to combat this problem include remote rendering/streaming and neural\nrepresentations of 3D assets. These approaches have redefined the traditional\nlocal storage-rendering pipeline by distributed computing or compression of\nlarge data. However, these methods typically suffer from high latency or low\nquality for practical visualization of large immersive virtual scenes, notably\nwith extra high resolution and refresh rate requirements for VR applications\nsuch as gaming and design.\n  Tailored for the future portable, low-storage, and energy-efficient VR\nplatforms, we present the first gaze-contingent 3D neural representation and\nview synthesis method. We incorporate the human psychophysics of visual- and\nstereo-acuity into an egocentric neural representation of 3D scenery.\nFurthermore, we jointly optimize the latency/performance and visual quality,\nwhile mutually bridging human perception and neural scene synthesis, to achieve\nperceptually high-quality immersive interaction. Both objective analysis and\nsubjective study demonstrate the effectiveness of our approach in significantly\nreducing local storage volume and synthesis latency (up to 99% reduction in\nboth data size and computational time), while simultaneously presenting\nhigh-fidelity rendering, with perceptual quality identical to that of fully\nlocally stored and rendered high-quality imagery.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:05:47 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Deng", "Nianchen", ""], ["He", "Zhenyi", ""], ["Ye", "Jiannan", ""], ["Chakravarthula", "Praneeth", ""], ["Yang", "Xubo", ""], ["Sun", "Qi", ""]]}, {"id": "2103.16367", "submitter": "Jinguo Zhu", "authors": "Jinguo Zhu and Shixiang Tang and Dapeng Chen and Shijie Yu and Yakun\n  Liu and Aijun Yang and Mingzhe Rong and Xiaohua Wang", "title": "Complementary Relation Contrastive Distillation", "comments": "CVPR2021 Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation aims to transfer representation ability from a teacher\nmodel to a student model. Previous approaches focus on either individual\nrepresentation distillation or inter-sample similarity preservation. While we\nargue that the inter-sample relation conveys abundant information and needs to\nbe distilled in a more effective way. In this paper, we propose a novel\nknowledge distillation method, namely Complementary Relation Contrastive\nDistillation (CRCD), to transfer the structural knowledge from the teacher to\nthe student. Specifically, we estimate the mutual relation in an anchor-based\nway and distill the anchor-student relation under the supervision of its\ncorresponding anchor-teacher relation. To make it more robust, mutual relations\nare modeled by two complementary elements: the feature and its gradient.\nFurthermore, the low bound of mutual information between the anchor-teacher\nrelation distribution and the anchor-student relation distribution is maximized\nvia relation contrastive loss, which can distill both the sample representation\nand the inter-sample relations. Experiments on different benchmarks demonstrate\nthe effectiveness of our proposed CRCD.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:43:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhu", "Jinguo", ""], ["Tang", "Shixiang", ""], ["Chen", "Dapeng", ""], ["Yu", "Shijie", ""], ["Liu", "Yakun", ""], ["Yang", "Aijun", ""], ["Rong", "Mingzhe", ""], ["Wang", "Xiaohua", ""]]}, {"id": "2103.16368", "submitter": "Zhenyu Wang", "authors": "Zhenyu Wang, Yali Li, Ye Guo, Lu Fang, Shengjin Wang", "title": "Data-Uncertainty Guided Multi-Phase Learning for Semi-Supervised Object\n  Detection", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we delve into semi-supervised object detection where unlabeled\nimages are leveraged to break through the upper bound of fully-supervised\nobject detection models. Previous semi-supervised methods based on pseudo\nlabels are severely degenerated by noise and prone to overfit to noisy labels,\nthus are deficient in learning different unlabeled knowledge well. To address\nthis issue, we propose a data-uncertainty guided multi-phase learning method\nfor semi-supervised object detection. We comprehensively consider divergent\ntypes of unlabeled images according to their difficulty levels, utilize them in\ndifferent phases and ensemble models from different phases together to generate\nultimate results. Image uncertainty guided easy data selection and region\nuncertainty guided RoI Re-weighting are involved in multi-phase learning and\nenable the detector to concentrate on more certain knowledge. Through extensive\nexperiments on PASCAL VOC and MS COCO, we demonstrate that our method behaves\nextraordinarily compared to baseline approaches and outperforms them by a large\nmargin, more than 3% on VOC and 2% on COCO.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:27:23 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Zhenyu", ""], ["Li", "Yali", ""], ["Guo", "Ye", ""], ["Fang", "Lu", ""], ["Wang", "Shengjin", ""]]}, {"id": "2103.16370", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, Jian Sun", "title": "Distribution Alignment: A Unified Framework for Long-tail Visual\n  Recognition", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the recent success of deep neural networks, it remains challenging to\neffectively model the long-tail class distribution in visual recognition tasks.\nTo address this problem, we first investigate the performance bottleneck of the\ntwo-stage learning framework via ablative study. Motivated by our discovery, we\npropose a unified distribution alignment strategy for long-tail visual\nrecognition. Specifically, we develop an adaptive calibration function that\nenables us to adjust the classification scores for each data point. We then\nintroduce a generalized re-weight method in the two-stage learning to balance\nthe class prior, which provides a flexible and unified solution to diverse\nscenarios in visual recognition tasks. We validate our method by extensive\nexperiments on four tasks, including image classification, semantic\nsegmentation, object detection, and instance segmentation. Our approach\nachieves the state-of-the-art results across all four recognition tasks with a\nsimple and unified framework. The code and models will be made publicly\navailable at: https://github.com/Megvii-BaseDetection/DisAlign\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:09:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Songyang", ""], ["Li", "Zeming", ""], ["Yan", "Shipeng", ""], ["He", "Xuming", ""], ["Sun", "Jian", ""]]}, {"id": "2103.16372", "submitter": "Yuang Liu", "authors": "Yuang Liu, Wei Zhang, Jun Wang", "title": "Source-Free Domain Adaptation for Semantic Segmentation", "comments": "CVPR 2021, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) can tackle the challenge that\nconvolutional neural network(CNN)-based approaches for semantic segmentation\nheavily rely on the pixel-level annotated data, which is labor-intensive.\nHowever, existing UDA approaches in this regard inevitably require the full\naccess to source datasets to reduce the gap between the source and target\ndomains during model adaptation, which are impractical in the real scenarios\nwhere the source datasets are private, and thus cannot be released along with\nthe well-trained source models. To cope with this issue, we propose a\nsource-free domain adaptation framework for semantic segmentation, namely SFDA,\nin which only a well-trained source model and an unlabeled target domain\ndataset are available for adaptation. SFDA not only enables to recover and\npreserve the source domain knowledge from the source model via knowledge\ntransfer during model adaptation, but also distills valuable information from\nthe target domain for self-supervised learning. The pixel- and patch-level\noptimization objectives tailored for semantic segmentation are seamlessly\nintegrated in the framework. The extensive experimental results on numerous\nbenchmark datasets highlight the effectiveness of our framework against the\nexisting UDA approaches relying on source data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:14:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Yuang", ""], ["Zhang", "Wei", ""], ["Wang", "Jun", ""]]}, {"id": "2103.16381", "submitter": "Mingtao Feng", "authors": "Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming\n  Zhu, Hui Zhang, Yaonan Wang and Ajmal Mian", "title": "Free-form Description Guided 3D Visual Graph Network for Object\n  Grounding in Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object grounding aims to locate the most relevant target object in a raw\npoint cloud scene based on a free-form language description. Understanding\ncomplex and diverse descriptions, and lifting them directly to a point cloud is\na new and challenging topic due to the irregular and sparse nature of point\nclouds. There are three main challenges in 3D object grounding: to find the\nmain focus in the complex and diverse description; to understand the point\ncloud scene; and to locate the target object. In this paper, we address all\nthree challenges. Firstly, we propose a language scene graph module to capture\nthe rich structure and long-distance phrase correlations. Secondly, we\nintroduce a multi-level 3D proposal relation graph module to extract the\nobject-object and object-scene co-occurrence relationships, and strengthen the\nvisual features of the initial proposals. Lastly, we develop a description\nguided 3D visual graph module to encode global contexts of phrases and\nproposals by a nodes matching strategy. Extensive experiments on challenging\nbenchmark datasets (ScanRefer and Nr3D) show that our algorithm outperforms\nexisting state-of-the-art. Our code is available at\nhttps://github.com/PNXD/FFL-3DOG.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:22:36 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Feng", "Mingtao", ""], ["Li", "Zhen", ""], ["Li", "Qi", ""], ["Zhang", "Liang", ""], ["Zhang", "XiangDong", ""], ["Zhu", "Guangming", ""], ["Zhang", "Hui", ""], ["Wang", "Yaonan", ""], ["Mian", "Ajmal", ""]]}, {"id": "2103.16385", "submitter": "Tianhan Xu", "authors": "Tianhan Xu, Wataru Takano", "title": "Graph Stacked Hourglass Networks for 3D Human Pose Estimation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel graph convolutional network architecture,\nGraph Stacked Hourglass Networks, for 2D-to-3D human pose estimation tasks. The\nproposed architecture consists of repeated encoder-decoder, in which\ngraph-structured features are processed across three different scales of human\nskeletal representations. This multi-scale architecture enables the model to\nlearn both local and global feature representations, which are critical for 3D\nhuman pose estimation. We also introduce a multi-level feature learning\napproach using different-depth intermediate features and show the performance\nimprovements that result from exploiting multi-scale, multi-level feature\nrepresentations. Extensive experiments are conducted to validate our approach,\nand the results show that our model outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:25:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Xu", "Tianhan", ""], ["Takano", "Wataru", ""]]}, {"id": "2103.16391", "submitter": "Jing Li", "authors": "Jing Li, Botong Wu, Xinwei Sun, Yizhou Wang", "title": "Causal Hidden Markov Model for Time Series Disease Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a causal hidden Markov model to achieve robust prediction of\nirreversible disease at an early stage, which is safety-critical and vital for\nmedical treatment in early stages. Specifically, we introduce the hidden\nvariables which propagate to generate medical data at each time step. To avoid\nlearning spurious correlation (e.g., confounding bias), we explicitly separate\nthese hidden variables into three parts: a) the disease (clinical)-related\npart; b) the disease (non-clinical)-related part; c) others, with only a),b)\ncausally related to the disease however c) may contain spurious correlations\n(with the disease) inherited from the data provided. With personal attributes\nand the disease label respectively provided as side information and\nsupervision, we prove that these disease-related hidden variables can be\ndisentangled from others, implying the avoidance of spurious correlation for\ngeneralization to medical data from other (out-of-) distributions. Guaranteed\nby this result, we propose a sequential variational auto-encoder with a\nreformulated objective function. We apply our model to the early prediction of\nperipapillary atrophy and achieve promising results on out-of-distribution test\ndata. Further, the ablation study empirically shows the effectiveness of each\ncomponent in our method. And the visualization shows the accurate\nidentification of lesion regions from others.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:34:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Jing", ""], ["Wu", "Botong", ""], ["Sun", "Xinwei", ""], ["Wang", "Yizhou", ""]]}, {"id": "2103.16392", "submitter": "Can Zhang", "authors": "Can Zhang, Meng Cao, Dongming Yang, Jie Chen, Yuexian Zou", "title": "CoLA: Weakly-Supervised Temporal Action Localization with Snippet\n  Contrastive Learning", "comments": "accepted by CVPR 2021, typos corrected, code link added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised temporal action localization (WS-TAL) aims to localize\nactions in untrimmed videos with only video-level labels. Most existing models\nfollow the \"localization by classification\" procedure: locate temporal regions\ncontributing most to the video-level classification. Generally, they process\neach snippet (or frame) individually and thus overlook the fruitful temporal\ncontext relation. Here arises the single snippet cheating issue: \"hard\"\nsnippets are too vague to be classified. In this paper, we argue that learning\nby comparing helps identify these hard snippets and we propose to utilize\nsnippet Contrastive learning to Localize Actions, CoLA for short. Specifically,\nwe propose a Snippet Contrast (SniCo) Loss to refine the hard snippet\nrepresentation in feature space, which guides the network to perceive precise\ntemporal boundaries and avoid the temporal interval interruption. Besides,\nsince it is infeasible to access frame-level annotations, we introduce a Hard\nSnippet Mining algorithm to locate the potential hard snippets. Substantial\nanalyses verify that this mining strategy efficaciously captures the hard\nsnippets and SniCo Loss leads to more informative feature representation.\nExtensive experiments show that CoLA achieves state-of-the-art results on\nTHUMOS'14 and ActivityNet v1.2 datasets. CoLA code is publicly available at\nhttps://github.com/zhang-can/CoLA.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:37:15 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 03:28:22 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhang", "Can", ""], ["Cao", "Meng", ""], ["Yang", "Dongming", ""], ["Chen", "Jie", ""], ["Zou", "Yuexian", ""]]}, {"id": "2103.16397", "submitter": "Shengheng Deng", "authors": "Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, Kui Jia", "title": "3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding", "comments": "CVPR2021 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to understand the ways to interact with objects from visual cues,\na.k.a. visual affordance, is essential to vision-guided robotic research. This\ninvolves categorizing, segmenting and reasoning of visual affordance. Relevant\nstudies in 2D and 2.5D image domains have been made previously, however, a\ntruly functional understanding of object affordance requires learning and\nprediction in the 3D physical domain, which is still absent in the community.\nIn this work, we present a 3D AffordanceNet dataset, a benchmark of 23k shapes\nfrom 23 semantic object categories, annotated with 18 visual affordance\ncategories. Based on this dataset, we provide three benchmarking tasks for\nevaluating visual affordance understanding, including full-shape, partial-view\nand rotation-invariant affordance estimations. Three state-of-the-art point\ncloud deep learning networks are evaluated on all tasks. In addition we also\ninvestigate a semi-supervised learning setup to explore the possibility to\nbenefit from unlabeled data. Comprehensive results on our contributed dataset\nshow the promise of visual affordance understanding as a valuable yet\nchallenging benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:46:27 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 09:59:28 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Deng", "Shengheng", ""], ["Xu", "Xun", ""], ["Wu", "Chaozheng", ""], ["Chen", "Ke", ""], ["Jia", "Kui", ""]]}, {"id": "2103.16403", "submitter": "Wenxuan Ma", "authors": "Shuang Li, Jinming Zhang, Wenxuan Ma, Chi Harold Liu, Wei Li", "title": "Dynamic Domain Adaptation for Efficient Inference", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) enables knowledge transfer from a labeled source\ndomain to an unlabeled target domain by reducing the cross-domain distribution\ndiscrepancy. Most prior DA approaches leverage complicated and powerful deep\nneural networks to improve the adaptation capacity and have shown remarkable\nsuccess. However, they may have a lack of applicability to real-world\nsituations such as real-time interaction, where low target inference latency is\nan essential requirement under limited computational budget. In this paper, we\ntackle the problem by proposing a dynamic domain adaptation (DDA) framework,\nwhich can simultaneously achieve efficient target inference in low-resource\nscenarios and inherit the favorable cross-domain generalization brought by DA.\nIn contrast to static models, as a simple yet generic method, DDA can integrate\nvarious domain confusion constraints into any typical adaptive network, where\nmultiple intermediate classifiers can be equipped to infer \"easier\" and\n\"harder\" target data dynamically. Moreover, we present two novel strategies to\nfurther boost the adaptation performance of multiple prediction exits: 1) a\nconfidence score learning strategy to derive accurate target pseudo labels by\nfully exploring the prediction consistency of different classifiers; 2) a\nclass-balanced self-training strategy to explicitly adapt multi-stage\nclassifiers from source to target without losing prediction diversity.\nExtensive experiments on multiple benchmarks are conducted to verify that DDA\ncan consistently improve the adaptation performance and accelerate target\ninference under domain shift and limited resources scenarios\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 08:53:16 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Shuang", ""], ["Zhang", "Jinming", ""], ["Ma", "Wenxuan", ""], ["Liu", "Chi Harold", ""], ["Li", "Wei", ""]]}, {"id": "2103.16411", "submitter": "Chenran Lin", "authors": "Chenran Lin and Lok Ming Lui", "title": "Beltrami Signature: A Novel Invariant 2D Shape Representation for Object\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in shape analysis in recent years and in this\npaper we present a novel contour-based shape representation named Beltrami\nsignature for 2D bounded simple connected domain. The proposed representation\nis based on conformal welding. With suitable normalization, the uniqueness of\nwelding is guaranteed up to a rotation. Then it can be extended to a harmonic\nfunction and finally quasi-conformal theory get rid of the only uncertainty by\ncomputing Beltrami coefficient of harmonic extension. The benifits of the\nproposed signature is it keeps invariant under simple transformations like\nsacling, transformation and rotation and is roubost under slight deformation\nand distortion. Experiments demonstrates the above properties and also shows\nthe excellent classification performance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:09:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lin", "Chenran", ""], ["Lui", "Lok Ming", ""]]}, {"id": "2103.16442", "submitter": "Zoe Landgraf", "authors": "Zoe Landgraf, Raluca Scona, Tristan Laidlow, Stephen James, Stefan\n  Leutenegger, Andrew J. Davison", "title": "SIMstack: A Generative Shape and Instance Model for Unordered Object\n  Stacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By estimating 3D shape and instances from a single view, we can capture\ninformation about an environment quickly, without the need for comprehensive\nscanning and multi-view fusion. Solving this task for composite scenes (such as\nobject stacks) is challenging: occluded areas are not only ambiguous in shape\nbut also in instance segmentation; multiple decompositions could be valid. We\nobserve that physics constrains decomposition as well as shape in occluded\nregions and hypothesise that a latent space learned from scenes built under\nphysics simulation can serve as a prior to better predict shape and instances\nin occluded regions. To this end we propose SIMstack, a depth-conditioned\nVariational Auto-Encoder (VAE), trained on a dataset of objects stacked under\nphysics simulation. We formulate instance segmentation as a centre voting task\nwhich allows for class-agnostic detection and doesn't require setting the\nmaximum number of objects in the scene. At test time, our model can generate 3D\nshape and instance segmentation from a single depth view, probabilistically\nsampling proposals for the occluded region from the learned latent space. Our\nmethod has practical applications in providing robots some of the ability\nhumans have to make rapid intuitive inferences of partially observed scenes. We\ndemonstrate an application for precise (non-disruptive) object grasping of\nunknown objects from a single depth view.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:42:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Landgraf", "Zoe", ""], ["Scona", "Raluca", ""], ["Laidlow", "Tristan", ""], ["James", "Stephen", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2103.16449", "submitter": "Guan Shanyan", "authors": "Shanyan Guan, Jingwei Xu, Yunbo Wang, Bingbing Ni, Xiaokang Yang", "title": "Bilevel Online Adaptation for Out-of-Domain Human Mesh Reconstruction", "comments": "CVPR 2021, the project page:\n  https://sites.google.com/view/humanmeshboa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a new problem of adapting a pre-trained model of human\nmesh reconstruction to out-of-domain streaming videos. However, most previous\nmethods based on the parametric SMPL model \\cite{loper2015smpl} underperform in\nnew domains with unexpected, domain-specific attributes, such as camera\nparameters, lengths of bones, backgrounds, and occlusions. Our general idea is\nto dynamically fine-tune the source model on test video streams with additional\ntemporal constraints, such that it can mitigate the domain gaps without\nover-fitting the 2D information of individual test frames. A subsequent\nchallenge is how to avoid conflicts between the 2D and temporal constraints. We\npropose to tackle this problem using a new training algorithm named Bilevel\nOnline Adaptation (BOA), which divides the optimization process of overall\nmulti-objective into two steps of weight probe and weight update in a training\niteration. We demonstrate that BOA leads to state-of-the-art results on two\nhuman mesh reconstruction benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:47:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Guan", "Shanyan", ""], ["Xu", "Jingwei", ""], ["Wang", "Yunbo", ""], ["Ni", "Bingbing", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2103.16469", "submitter": "Tianyu Zhang", "authors": "Tianyu Zhang, Longhui Wei, Lingxi Xie, Zijie Zhuang, Yongfei Zhang, Bo\n  Li, Qi Tian", "title": "Spatiotemporal Transformer for Video-based Person Re-identification", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the Transformer module has been transplanted from natural language\nprocessing to computer vision. This paper applies the Transformer to\nvideo-based person re-identification, where the key issue is to extract the\ndiscriminative information from a tracklet. We show that, despite the strong\nlearning ability, the vanilla Transformer suffers from an increased risk of\nover-fitting, arguably due to a large number of attention parameters and\ninsufficient training data. To solve this problem, we propose a novel pipeline\nwhere the model is pre-trained on a set of synthesized video data and then\ntransferred to the downstream domains with the perception-constrained\nSpatiotemporal Transformer (STT) module and Global Transformer (GT) module. The\nderived algorithm achieves significant accuracy gain on three popular\nvideo-based person re-identification benchmarks, MARS, DukeMTMC-VideoReID, and\nLS-VID, especially when the training and testing data are from different\ndomains. More importantly, our research sheds light on the application of the\nTransformer on highly-structured visual data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 16:19:27 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Tianyu", ""], ["Wei", "Longhui", ""], ["Xie", "Lingxi", ""], ["Zhuang", "Zijie", ""], ["Zhang", "Yongfei", ""], ["Li", "Bo", ""], ["Tian", "Qi", ""]]}, {"id": "2103.16470", "submitter": "Li Zhang", "authors": "Li Wang, Liang Du, Xiaoqing Ye, Yanwei Fu, Guodong Guo, Xiangyang Xue,\n  Jianfeng Feng, Li Zhang", "title": "Depth-conditioned Dynamic Message Propagation for Monocular 3D Object\n  Detection", "comments": "CVPR 2021. Code at https://github.com/fudan-zvg/DDMP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this paper is to learn context- and depth-aware feature\nrepresentation to solve the problem of monocular 3D object detection. We make\nfollowing contributions: (i) rather than appealing to the complicated\npseudo-LiDAR based approach, we propose a depth-conditioned dynamic message\npropagation (DDMP) network to effectively integrate the multi-scale depth\ninformation with the image context;(ii) this is achieved by first adaptively\nsampling context-aware nodes in the image context and then dynamically\npredicting hybrid depth-dependent filter weights and affinity matrices for\npropagating information; (iii) by augmenting a center-aware depth encoding\n(CDE) task, our method successfully alleviates the inaccurate depth prior; (iv)\nwe thoroughly demonstrate the effectiveness of our proposed approach and show\nstate-of-the-art results among the monocular-based approaches on the KITTI\nbenchmark dataset. Particularly, we rank $1^{st}$ in the highly competitive\nKITTI monocular 3D object detection track on the submission day (November 16th,\n2020). Code and models are released at \\url{https://github.com/fudan-zvg/DDMP}\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 16:20:24 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Li", ""], ["Du", "Liang", ""], ["Ye", "Xiaoqing", ""], ["Fu", "Yanwei", ""], ["Guo", "Guodong", ""], ["Xue", "Xiangyang", ""], ["Feng", "Jianfeng", ""], ["Zhang", "Li", ""]]}, {"id": "2103.16481", "submitter": "G\\\"ul Varol", "authors": "G\\\"ul Varol, Liliane Momeni, Samuel Albanie, Triantafyllos Afouras,\n  Andrew Zisserman", "title": "Read and Attend: Temporal Localisation in Sign Language Videos", "comments": "Appears in: 2021 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2021). 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to annotate sign instances across a broad\nvocabulary in continuous sign language. We train a Transformer model to ingest\na continuous signing stream and output a sequence of written tokens on a\nlarge-scale collection of signing footage with weakly-aligned subtitles. We\nshow that through this training it acquires the ability to attend to a large\nvocabulary of sign instances in the input sequence, enabling their\nlocalisation. Our contributions are as follows: (1) we demonstrate the ability\nto leverage large quantities of continuous signing videos with weakly-aligned\nsubtitles to localise signs in continuous sign language; (2) we employ the\nlearned attention to automatically generate hundreds of thousands of\nannotations for a large sign vocabulary; (3) we collect a set of 37K manually\nverified sign instances across a vocabulary of 950 sign classes to support our\nstudy of sign language recognition; (4) by training on the newly annotated data\nfrom our method, we outperform the prior state of the art on the BSL-1K sign\nlanguage recognition benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 16:39:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Varol", "G\u00fcl", ""], ["Momeni", "Liliane", ""], ["Albanie", "Samuel", ""], ["Afouras", "Triantafyllos", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2103.16483", "submitter": "Oisin Mac Aodha", "authors": "Grant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge\n  Belongie, Oisin Mac Aodha", "title": "Benchmarking Representation Learning for Natural World Image Collections", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in self-supervised learning has resulted in models that are\ncapable of extracting rich representations from image collections without\nrequiring any explicit label supervision. However, to date the vast majority of\nthese approaches have restricted themselves to training on standard benchmark\ndatasets such as ImageNet. We argue that fine-grained visual categorization\nproblems, such as plant and animal species classification, provide an\ninformative testbed for self-supervised learning. In order to facilitate\nprogress in this area we present two new natural world visual classification\ndatasets, iNat2021 and NeWT. The former consists of 2.7M images from 10k\ndifferent species uploaded by users of the citizen science application\niNaturalist. We designed the latter, NeWT, in collaboration with domain experts\nwith the aim of benchmarking the performance of representation learning\nalgorithms on a suite of challenging natural world binary classification tasks\nthat go beyond standard species classification. These two new datasets allow us\nto explore questions related to large-scale representation and transfer\nlearning in the context of fine-grained categories. We provide a comprehensive\nanalysis of feature extractors trained with and without supervision on ImageNet\nand iNat2021, shedding light on the strengths and weaknesses of different\nlearned features across a diverse set of tasks. We find that features produced\nby standard supervised methods still outperform those produced by\nself-supervised approaches such as SimCLR. However, improved self-supervised\nlearning methods are constantly being released and the iNat2021 and NeWT\ndatasets are a valuable resource for tracking their progress.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 16:41:49 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 22:07:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Van Horn", "Grant", ""], ["Cole", "Elijah", ""], ["Beery", "Sara", ""], ["Wilber", "Kimberly", ""], ["Belongie", "Serge", ""], ["Mac Aodha", "Oisin", ""]]}, {"id": "2103.16492", "submitter": "Dominik M\\\"uller", "authors": "Dennis Hartmann, Dominik M\\\"uller, I\\~naki Soto-Rey and Frank Kramer", "title": "Assessing the Role of Random Forests in Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks represent a field of research that can quickly achieve very\ngood results in the field of medical image segmentation using a GPU. A possible\nway to achieve good results without GPUs are random forests. For this purpose,\ntwo random forest approaches were compared with a state-of-the-art deep\nconvolutional neural network. To make the comparison the PhC-C2DH-U373 and the\nretinal imaging datasets were used. The evaluation showed that the deep\nconvolutional neutral network achieved the best results. However, one of the\nrandom forest approaches also achieved a similar high performance. Our results\nindicate that random forest approaches are a good alternative to deep\nconvolutional neural networks and, thus, allow the usage of medical image\nsegmentation without a GPU.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 16:47:19 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hartmann", "Dennis", ""], ["M\u00fcller", "Dominik", ""], ["Soto-Rey", "I\u00f1aki", ""], ["Kramer", "Frank", ""]]}, {"id": "2103.16493", "submitter": "Yunhe Gao", "authors": "Yunhe Gao, Zhiqiang Tang, Mu Zhou, Dimitris Metaxas", "title": "Enabling Data Diversity: Efficient Automatic Augmentation via\n  Regularized Adversarial Training", "comments": "Accepted by IPMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Data augmentation has proved extremely useful by increasing training data\nvariance to alleviate overfitting and improve deep neural networks'\ngeneralization performance. In medical image analysis, a well-designed\naugmentation policy usually requires much expert knowledge and is difficult to\ngeneralize to multiple tasks due to the vast discrepancies among pixel\nintensities, image appearances, and object shapes in different medical tasks.\nTo automate medical data augmentation, we propose a regularized adversarial\ntraining framework via two min-max objectives and three differentiable\naugmentation models covering affine transformation, deformation, and appearance\nchanges. Our method is more automatic and efficient than previous automatic\naugmentation methods, which still rely on pre-defined operations with\nhuman-specified ranges and costly bi-level optimization. Extensive experiments\ndemonstrated that our approach, with less training overhead, achieves superior\nperformance over state-of-the-art auto-augmentation methods on both tasks of 2D\nskin cancer classification and 3D organs-at-risk segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 16:49:20 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Gao", "Yunhe", ""], ["Tang", "Zhiqiang", ""], ["Zhou", "Mu", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2103.16507", "submitter": "Yating Tian", "authors": "Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu,\n  Limin Wang, Zhenan Sun", "title": "3D Human Pose and Shape Regression with Pyramidal Mesh Alignment\n  Feedback Loop", "comments": "Technical report. Code and model available at\n  https://hongwenzhang.github.io/pymaf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression-based methods have recently shown promising results in\nreconstructing human meshes from monocular images. By directly mapping from raw\npixels to model parameters, these methods can produce parametric models in a\nfeed-forward manner via neural networks. However, minor deviation in parameters\nmay lead to noticeable misalignment between the estimated meshes and image\nevidences. To address this issue, we propose a Pyramidal Mesh Alignment\nFeedback (PyMAF) loop to leverage a feature pyramid and rectify the predicted\nparameters explicitly based on the mesh-image alignment status in our deep\nregressor. In PyMAF, given the currently predicted parameters, mesh-aligned\nevidences will be extracted from finer-resolution features accordingly and fed\nback for parameter rectification. To reduce noise and enhance the reliability\nof these evidences, an auxiliary pixel-wise supervision is imposed on the\nfeature encoder, which provides mesh-image correspondence guidance for our\nnetwork to preserve the most related information in spatial features. The\nefficacy of our approach is validated on several benchmarks, including\nHuman3.6M, 3DPW, LSP, and COCO, where experimental results show that our\napproach consistently improves the mesh-image alignment of the reconstruction.\nOur code is publicly available at https://hongwenzhang.github.io/pymaf .\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:07:49 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 11:46:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Hongwen", ""], ["Tian", "Yating", ""], ["Zhou", "Xinchi", ""], ["Ouyang", "Wanli", ""], ["Liu", "Yebin", ""], ["Wang", "Limin", ""], ["Sun", "Zhenan", ""]]}, {"id": "2103.16510", "submitter": "Cagatay Basdogan", "authors": "Senem Ezgi Emgin, Amirreza Aghakhani, T. Metin Sezgin, and Cagatay\n  Basdogan", "title": "HapTable: An Interactive Tabletop Providing Online Haptic Feedback for\n  Touch Gestures", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2019,\n  Vol. 25, No. 9, pp. 2749-2762", "doi": "10.1109/TVCG.2018.2855154", "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HapTable; a multimodal interactive tabletop that allows users to\ninteract with digital images and objects through natural touch gestures, and\nreceive visual and haptic feedback accordingly. In our system, hand pose is\nregistered by an infrared camera and hand gestures are classified using a\nSupport Vector Machine (SVM) classifier. To display a rich set of haptic\neffects for both static and dynamic gestures, we integrated electromechanical\nand electrostatic actuation techniques effectively on tabletop surface of\nHapTable, which is a surface capacitive touch screen. We attached four piezo\npatches to the edges of tabletop to display vibrotactile feedback for static\ngestures. For this purpose, the vibration response of the tabletop, in the form\nof frequency response functions (FRFs), was obtained by a laser Doppler\nvibrometer for 84 grid points on its surface. Using these FRFs, it is possible\nto display localized vibrotactile feedback on the surface for static gestures.\nFor dynamic gestures, we utilize the electrostatic actuation technique to\nmodulate the frictional forces between finger skin and tabletop surface by\napplying voltage to its conductive layer. Here, we present two examples of such\napplications, one for static and one for dynamic gestures, along with detailed\nuser studies. In the first one, user detects the direction of a virtual flow,\nsuch as that of wind or water, by putting their hand on the tabletop surface\nand feeling a vibrotactile stimulus traveling underneath it. In the second\nexample, user rotates a virtual knob on the tabletop surface to select an item\nfrom a menu while feeling the knob's detents and resistance to rotation in the\nform of frictional haptic feedback.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:12:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Emgin", "Senem Ezgi", ""], ["Aghakhani", "Amirreza", ""], ["Sezgin", "T. Metin", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2103.16515", "submitter": "Marc Aubreville", "authors": "Marc Aubreville, Christof Bertram, Mitko Veta, Robert Klopfleisch,\n  Nikolas Stathonikos, Katharina Breininger, Natalie ter Hoeve, Francesco\n  Ciompi, and Andreas Maier", "title": "Quantifying the Scanner-Induced Domain Gap in Mitosis Detection", "comments": "3 pages, 1 figure, 1 table, submitted as short paper to MIDL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated detection of mitotic figures in histopathology images has seen vast\nimprovements, thanks to modern deep learning-based pipelines. Application of\nthese methods, however, is in practice limited by strong variability of images\nbetween labs. This results in a domain shift of the images, which causes a\nperformance drop of the models. Hypothesizing that the scanner device plays a\ndecisive role in this effect, we evaluated the susceptibility of a standard\nmitosis detection approach to the domain shift introduced by using a different\nwhole slide scanner. Our work is based on the MICCAI-MIDOG challenge 2021 data\nset, which includes 200 tumor cases of human breast cancer and four scanners.\n  Our work indicates that the domain shift induced not by biochemical\nvariability but purely by the choice of acquisition device is underestimated so\nfar. Models trained on images of the same scanner yielded an average F1 score\nof 0.683, while models trained on a single other scanner only yielded an\naverage F1 score of 0.325. Training on another multi-domain mitosis dataset led\nto mean F1 scores of 0.52. We found this not to be reflected by domain-shifts\nmeasured as proxy A distance-derived metric.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:17:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Aubreville", "Marc", ""], ["Bertram", "Christof", ""], ["Veta", "Mitko", ""], ["Klopfleisch", "Robert", ""], ["Stathonikos", "Nikolas", ""], ["Breininger", "Katharina", ""], ["ter Hoeve", "Natalie", ""], ["Ciompi", "Francesco", ""], ["Maier", "Andreas", ""]]}, {"id": "2103.16516", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni and Michael S. Ryoo", "title": "Recognizing Actions in Videos from Unseen Viewpoints", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard methods for video recognition use large CNNs designed to capture\nspatio-temporal data. However, training these models requires a large amount of\nlabeled training data, containing a wide variety of actions, scenes, settings\nand camera viewpoints. In this paper, we show that current convolutional neural\nnetwork models are unable to recognize actions from camera viewpoints not\npresent in their training data (i.e., unseen view action recognition). To\naddress this, we develop approaches based on 3D representations and introduce a\nnew geometric convolutional layer that can learn viewpoint invariant\nrepresentations. Further, we introduce a new, challenging dataset for unseen\nview recognition and show the approaches ability to learn viewpoint invariant\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:17:54 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "2103.16525", "submitter": "David Recasens", "authors": "David Recasens, Jos\\'e Lamarca, Jos\\'e M. F\\'acil, J. M. M. Montiel,\n  Javier Civera", "title": "Endo-Depth-and-Motion: Reconstruction and Tracking in Endoscopic Videos\n  using Depth Networks and Photometric Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a scene reconstruction and the camera motion from in-body videos\nis challenging due to several factors, e.g. the deformation of in-body cavities\nor the lack of texture. In this paper we present Endo-Depth-and-Motion, a\npipeline that estimates the 6-degrees-of-freedom camera pose and dense 3D scene\nmodels from monocular endoscopic videos. Our approach leverages recent advances\nin self-supervised depth networks to generate pseudo-RGBD frames, then tracks\nthe camera pose using photometric residuals and fuses the registered depth maps\nin a volumetric representation. We present an extensive experimental evaluation\nin the public dataset Hamlyn, showing high-quality results and comparisons\nagainst relevant baselines. We also release all models and code for future\ncomparisons.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:29:31 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 19:44:31 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Recasens", "David", ""], ["Lamarca", "Jos\u00e9", ""], ["F\u00e1cil", "Jos\u00e9 M.", ""], ["Montiel", "J. M. M.", ""], ["Civera", "Javier", ""]]}, {"id": "2103.16528", "submitter": "Timo Hinzmann", "authors": "Timo Hinzmann, Roland Siegwart", "title": "SD-6DoF-ICLK: Sparse and Deep Inverse Compositional Lucas-Kanade\n  Algorithm on SE(3)", "comments": "Initial submission; 7 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SD-6DoF-ICLK, a learning-based Inverse Compositional\nLucas-Kanade (ICLK) pipeline that uses sparse depth information to optimize the\nrelative pose that best aligns two images on SE(3). To compute this six\nDegrees-of-Freedom (DoF) relative transformation, the proposed formulation\nrequires only sparse depth information in one of the images, which is often the\nonly available depth source in visual-inertial odometry or Simultaneous\nLocalization and Mapping (SLAM) pipelines. In an optional subsequent step, the\nframework further refines feature locations and the relative pose using\nindividual feature alignment and bundle adjustment for pose and structure\nre-alignment. The resulting sparse point correspondences with subpixel-accuracy\nand refined relative pose can be used for depth map generation, or the image\nalignment module can be embedded in an odometry or mapping framework.\nExperiments with rendered imagery show that the forward SD-6DoF-ICLK runs at\n145 ms per image pair with a resolution of 752 x 480 pixels each, and vastly\noutperforms the classical, sparse 6DoF-ICLK algorithm, making it the ideal\nframework for robust image alignment under severe conditions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:31:41 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hinzmann", "Timo", ""], ["Siegwart", "Roland", ""]]}, {"id": "2103.16544", "submitter": "Roozbeh Mottaghi", "authors": "Luca Weihs, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi", "title": "Visual Room Rearrangement", "comments": "CVPR 2021 - Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a significant recent progress in the field of Embodied AI with\nresearchers developing models and algorithms enabling embodied agents to\nnavigate and interact within completely unseen environments. In this paper, we\npropose a new dataset and baseline models for the task of Rearrangement. We\nparticularly focus on the task of Room Rearrangement: an agent begins by\nexploring a room and recording objects' initial configurations. We then remove\nthe agent and change the poses and states (e.g., open/closed) of some objects\nin the room. The agent must restore the initial configurations of all objects\nin the room. Our dataset, named RoomR, includes 6,000 distinct rearrangement\nsettings involving 72 different object types in 120 scenes. Our experiments\nshow that solving this challenging interactive task that involves navigation\nand object interaction is beyond the capabilities of the current\nstate-of-the-art techniques for embodied tasks and we are still very far from\nachieving perfect performance on these types of tasks. The code and the dataset\nare available at: https://ai2thor.allenai.org/rearrangement\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:51:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Weihs", "Luca", ""], ["Deitke", "Matt", ""], ["Kembhavi", "Aniruddha", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "2103.16547", "submitter": "Xiaohan Chen", "authors": "Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Jingjing Liu,\n  Zhangyang Wang", "title": "The Elastic Lottery Ticket Hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse\ntrainable subnetworks, or winning tickets, of training, which can be trained in\nisolation to achieve similar or even better performance compared to the full\nmodels. Despite many efforts being made, the most effective method to identify\nsuch winning tickets is still Iterative Magnitude-based Pruning (IMP), which is\ncomputationally expensive and has to be run thoroughly for every different\nnetwork. A natural question that comes in is: can we \"transform\" the winning\nticket found in one network to another with a different architecture, yielding\na winning ticket for the latter at the beginning, without re-doing the\nexpensive IMP? Answering this question is not only practically relevant for\nefficient \"once-for-all\" winning ticket finding, but also theoretically\nappealing for uncovering inherently scalable sparse patterns in networks. We\nconduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety\nof strategies to tweak the winning tickets found from different networks of the\nsame model family (e.g., ResNets). Based on these results, we articulate the\nElastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or\ndropping) and re-ordering layers for one network, its corresponding winning\nticket could be stretched (or squeezed) into a subnetwork for another deeper\n(or shallower) network from the same family, whose performance is nearly the\nsame competitive as the latter's winning ticket directly found by IMP. We have\nalso thoroughly compared E-LTH with pruning-at-initialization and dynamic\nsparse training methods, and discuss the generalizability of E-LTH to different\nmodel families, layer types, or across datasets. Code is available at\nhttps://github.com/VITA-Group/ElasticLTH.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:53:45 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:04:37 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chen", "Xiaohan", ""], ["Cheng", "Yu", ""], ["Wang", "Shuohang", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2103.16549", "submitter": "Joakim Johnander", "authors": "Joakim Johnander, Johan Edstedt, Martin Danelljan, Michael Felsberg,\n  Fahad Shahbaz Khan", "title": "Deep Gaussian Processes for Few-Shot Segmentation", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot segmentation is a challenging task, requiring the extraction of a\ngeneralizable representation from only a few annotated samples, in order to\nsegment novel query images. A common approach is to model each class with a\nsingle prototype. While conceptually simple, these methods suffer when the\ntarget appearance distribution is multi-modal or not linearly separable in\nfeature space. To tackle this issue, we propose a few-shot learner formulation\nbased on Gaussian process (GP) regression. Through the expressivity of the GP,\nour approach is capable of modeling complex appearance distributions in the\ndeep feature space. The GP provides a principled way of capturing uncertainty,\nwhich serves as another powerful cue for the final segmentation, obtained by a\nCNN decoder. We further exploit the end-to-end learning capabilities of our\napproach to learn the output space of the GP learner, ensuring a richer\nencoding of the segmentation mask. We perform comprehensive experimental\nanalysis of our few-shot learner formulation. Our approach sets a new\nstate-of-the-art for 5-shot segmentation, with mIoU scores of 68.1 and 49.8 on\nPASCAL-5i and COCO-20i, respectively\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:56:32 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Johnander", "Joakim", ""], ["Edstedt", "Johan", ""], ["Danelljan", "Martin", ""], ["Felsberg", "Michael", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "2103.16552", "submitter": "Philipp Henzler", "authors": "Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman\n  Shapovalov, Tobias Ritschel, Andrea Vedaldi, David Novotny", "title": "Unsupervised Learning of 3D Object Categories from Videos in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to learn a deep network that, given a small number of images of\nan object of a given category, reconstructs it in 3D. While several recent\nworks have obtained analogous results using synthetic data or assuming the\navailability of 2D primitives such as keypoints, we are interested in working\nwith challenging real data and with no manual annotations. We thus focus on\nlearning a model from multiple views of a large collection of object instances.\nWe contribute with a new large dataset of object centric videos suitable for\ntraining and benchmarking this class of models. We show that existing\ntechniques leveraging meshes, voxels, or implicit surfaces, which work well for\nreconstructing isolated objects, fail on this challenging data. Finally, we\npropose a new neural network design, called warp-conditioned ray embedding\n(WCR), which significantly improves reconstruction while obtaining a detailed\nimplicit representation of the object surface and texture, also compensating\nfor the noise in the initial SfM reconstruction that bootstrapped the learning\nprocess. Our evaluation demonstrates performance improvements over several deep\nmonocular reconstruction baselines on existing benchmarks and on our novel\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:57:01 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Henzler", "Philipp", ""], ["Reizenstein", "Jeremy", ""], ["Labatut", "Patrick", ""], ["Shapovalov", "Roman", ""], ["Ritschel", "Tobias", ""], ["Vedaldi", "Andrea", ""], ["Novotny", "David", ""]]}, {"id": "2103.16553", "submitter": "Antoine Miech", "authors": "Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, Andrew\n  Zisserman", "title": "Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with\n  Transformers", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is language-based search of large-scale image and video\ndatasets. For this task, the approach that consists of independently mapping\ntext and vision to a joint embedding space, a.k.a. dual encoders, is attractive\nas retrieval scales and is efficient for billions of images using approximate\nnearest neighbour search. An alternative approach of using vision-text\ntransformers with cross-attention gives considerable improvements in accuracy\nover the joint embeddings, but is often inapplicable in practice for\nlarge-scale retrieval given the cost of the cross-attention mechanisms required\nfor each sample at test time. This work combines the best of both worlds. We\nmake the following three contributions. First, we equip transformer-based\nmodels with a new fine-grained cross-attention architecture, providing\nsignificant improvements in retrieval accuracy whilst preserving scalability.\nSecond, we introduce a generic approach for combining a Fast dual encoder model\nwith our Slow but accurate transformer-based model via distillation and\nre-ranking. Finally, we validate our approach on the Flickr30K image dataset\nwhere we show an increase in inference speed by several orders of magnitude\nwhile having results competitive to the state of the art. We also extend our\nmethod to the video domain, improving the state of the art on the VATEX\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:57:08 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Miech", "Antoine", ""], ["Alayrac", "Jean-Baptiste", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2103.16554", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Shiyang Cheng and Jing Yang and Andrew Garbett and\n  Enrique Sanchez and Georgios Tzimiropoulos", "title": "Pre-training strategies and datasets for facial representation learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the best way to learn a universal face representation? Recent work on\nDeep Learning in the area of face analysis has focused on supervised learning\nfor specific tasks of interest (e.g. face recognition, facial landmark\nlocalization etc.) but has overlooked the overarching question of how to find a\nfacial representation that can be readily adapted to several facial analysis\ntasks and datasets. To this end, we make the following 4 contributions: (a) we\nintroduce, for the first time, a comprehensive evaluation benchmark for facial\nrepresentation learning consisting of 5 important face analysis tasks. (b) We\nsystematically investigate two ways of large-scale representation learning\napplied to faces: supervised and unsupervised pre-training. Importantly, we\nfocus our evaluations on the case of few-shot facial learning. (c) We\ninvestigate important properties of the training datasets including their size\nand quality (labelled, unlabelled or even uncurated). (d) To draw our\nconclusions, we conducted a very large number of experiments. Our main two\nfindings are: (1) Unsupervised pre-training on completely in-the-wild,\nuncurated data provides consistent and, in some cases, significant accuracy\nimprovements for all facial tasks considered. (2) Many existing facial video\ndatasets seem to have a large amount of redundancy. We will release code,\npre-trained models and data to facilitate future research.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:57:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bulat", "Adrian", ""], ["Cheng", "Shiyang", ""], ["Yang", "Jing", ""], ["Garbett", "Andrew", ""], ["Sanchez", "Enrique", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2103.16556", "submitter": "Christoph Mayer", "authors": "Christoph Mayer, Martin Danelljan, Danda Pani Paudel, Luc Van Gool", "title": "Learning Target Candidate Association to Keep Track of What Not to Track", "comments": "17 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of objects that are confusingly similar to the tracked target,\nposes a fundamental challenge in appearance-based visual tracking. Such\ndistractor objects are easily misclassified as the target itself, leading to\neventual tracking failure. While most methods strive to suppress distractors\nthrough more powerful appearance models, we take an alternative approach.\n  We propose to keep track of distractor objects in order to continue tracking\nthe target. To this end, we introduce a learned association network, allowing\nus to propagate the identities of all target candidates from frame-to-frame. To\ntackle the problem of lacking ground-truth correspondences between distractor\nobjects in visual tracking, we propose a training strategy that combines\npartial annotations with self-supervision. We conduct comprehensive\nexperimental validation and analysis of our approach on several challenging\ndatasets. Our tracker sets a new state-of-the-art on six benchmarks, achieving\nan AUC score of 67.2% on LaSOT and a +6.1% absolute gain on the OxUvA long-term\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:58:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Mayer", "Christoph", ""], ["Danelljan", "Martin", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""]]}, {"id": "2103.16559", "submitter": "Adri\\`a Recasens", "authors": "Adri\\`a Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang,\n  Florian Strub, Corentin Tallec, Mateusz Malinowski, Viorica Patraucean,\n  Florent Altch\\'e, Michal Valko, Jean-Bastien Grill, A\\\"aron van den Oord,\n  Andrew Zisserman", "title": "Broaden Your Views for Self-Supervised Video Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most successful self-supervised learning methods are trained to align the\nrepresentations of two independent views from the data. State-of-the-art\nmethods in video are inspired by image techniques, where these two views are\nsimilarly extracted by cropping and augmenting the resulting crop. However,\nthese methods miss a crucial element in the video domain: time. We introduce\nBraVe, a self-supervised learning framework for video. In BraVe, one of the\nviews has access to a narrow temporal window of the video while the other view\nhas a broad access to the video content. Our models learn to generalise from\nthe narrow view to the general content of the video. Furthermore, BraVe\nprocesses the views with different backbones, enabling the use of alternative\naugmentations or modalities into the broad view such as optical flow, randomly\nconvolved RGB frames, audio or their combinations. We demonstrate that BraVe\nachieves state-of-the-art results in self-supervised representation learning on\nstandard video and audio classification benchmarks including UCF101, HMDB51,\nKinetics, ESC-50 and AudioSet.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:58:46 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Recasens", "Adri\u00e0", ""], ["Luc", "Pauline", ""], ["Alayrac", "Jean-Baptiste", ""], ["Wang", "Luyu", ""], ["Strub", "Florian", ""], ["Tallec", "Corentin", ""], ["Malinowski", "Mateusz", ""], ["Patraucean", "Viorica", ""], ["Altch\u00e9", "Florent", ""], ["Valko", "Michal", ""], ["Grill", "Jean-Bastien", ""], ["Oord", "A\u00e4ron van den", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2103.16561", "submitter": "Wanrong Zhu", "authors": "Wanrong Zhu, Yuankai Qi, Pradyumna Narayana, Kazoo Sone, Sugato Basu,\n  Xin Eric Wang, Qi Wu, Miguel Eckstein, William Yang Wang", "title": "Diagnosing Vision-and-Language Navigation: What Really Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-language navigation (VLN) is a multimodal task where an agent\nfollows natural language instructions and navigates in visual environments.\nMultiple setups have been proposed, and researchers apply new model\narchitectures or training techniques to boost navigation performance. However,\nrecent studies witness a slow-down in the performance improvements in both\nindoor and outdoor VLN tasks, and the agents' inner mechanisms for making\nnavigation decisions remain unclear. To the best of our knowledge, the way the\nagents perceive the multimodal input is under-studied and clearly needs\ninvestigations. In this work, we conduct a series of diagnostic experiments to\nunveil agents' focus during navigation. Results show that indoor navigation\nagents refer to both object tokens and direction tokens in the instruction when\nmaking decisions. In contrast, outdoor navigation agents heavily rely on\ndirection tokens and have a poor understanding of the object tokens.\nFurthermore, instead of merely staring at surrounding objects, indoor\nnavigation agents can set their sights on objects further from the current\nviewpoint. When it comes to vision-and-language alignments, many models claim\nthat they are able to align object tokens with certain visual targets, but we\ncast doubt on the reliability of such alignments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:07 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhu", "Wanrong", ""], ["Qi", "Yuankai", ""], ["Narayana", "Pradyumna", ""], ["Sone", "Kazoo", ""], ["Basu", "Sugato", ""], ["Wang", "Xin Eric", ""], ["Wu", "Qi", ""], ["Eckstein", "Miguel", ""], ["Wang", "William Yang", ""]]}, {"id": "2103.16562", "submitter": "Bowen Cheng", "authors": "Bowen Cheng and Ross Girshick and Piotr Doll\\'ar and Alexander C. Berg\n  and Alexander Kirillov", "title": "Boundary IoU: Improving Object-Centric Image Segmentation Evaluation", "comments": "CVPR 2021, project page: https://bowenc0221.github.io/boundary-iou", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Boundary IoU (Intersection-over-Union), a new segmentation\nevaluation measure focused on boundary quality. We perform an extensive\nanalysis across different error types and object sizes and show that Boundary\nIoU is significantly more sensitive than the standard Mask IoU measure to\nboundary errors for large objects and does not over-penalize errors on smaller\nobjects. The new quality measure displays several desirable characteristics\nlike symmetry w.r.t. prediction/ground truth pairs and balanced responsiveness\nacross scales, which makes it more suitable for segmentation evaluation than\nother boundary-focused measures like Trimap IoU and F-measure. Based on\nBoundary IoU, we update the standard evaluation protocols for instance and\npanoptic segmentation tasks by proposing the Boundary AP (Average Precision)\nand Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments show\nthat the new evaluation metrics track boundary quality improvements that are\ngenerally overlooked by current Mask IoU-based evaluation metrics. We hope that\nthe adoption of the new boundary-sensitive evaluation metrics will lead to\nrapid progress in segmentation methods that improve boundary quality.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:20 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Cheng", "Bowen", ""], ["Girshick", "Ross", ""], ["Doll\u00e1r", "Piotr", ""], ["Berg", "Alexander C.", ""], ["Kirillov", "Alexander", ""]]}, {"id": "2103.16563", "submitter": "Benjamin Planche", "authors": "Benjamin Planche, Rajat Vikram Singh", "title": "Physics-based Differentiable Depth Sensor Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based algorithms are crucial to modern computer-vision and graphics\napplications, enabling learning-based optimization and inverse problems. For\nexample, photorealistic differentiable rendering pipelines for color images\nhave been proven highly valuable to applications aiming to map 2D and 3D\ndomains. However, to the best of our knowledge, no effort has been made so far\ntowards extending these gradient-based methods to the generation of depth\n(2.5D) images, as simulating structured-light depth sensors implies solving\ncomplex light transport and stereo-matching problems. In this paper, we\nintroduce a novel end-to-end differentiable simulation pipeline for the\ngeneration of realistic 2.5D scans, built on physics-based 3D rendering and\ncustom block-matching algorithms. Each module can be differentiated w.r.t\nsensor and scene parameters; e.g., to automatically tune the simulation for new\ndevices over some provided scans or to leverage the pipeline as a 3D-to-2.5D\ntransformer within larger computer-vision applications. Applied to the training\nof deep-learning methods for various depth-based recognition tasks\n(classification, pose estimation, semantic segmentation), our simulation\ngreatly improves the performance of the resulting models on real scans, thereby\ndemonstrating the fidelity and value of its synthetic depth data compared to\nprevious static simulations and learning-based domain adaptation schemes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Planche", "Benjamin", ""], ["Singh", "Rajat Vikram", ""]]}, {"id": "2103.16564", "submitter": "Chuang Gan", "authors": "Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua\n  B. Tenenbaum, Chuang Gan", "title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual\n  Reasoning", "comments": "ICLR 2021. Project page: http://dcl.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.SC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the problem of dynamic visual reasoning on raw videos. This is a\nchallenging problem; currently, state-of-the-art models often require dense\nsupervision on physical object properties and events from simulation, which are\nimpractical to obtain in real life. In this paper, we present the Dynamic\nConcept Learner (DCL), a unified framework that grounds physical objects and\nevents from video and language. DCL first adopts a trajectory extractor to\ntrack each object over time and to represent it as a latent, object-centric\nfeature vector. Building upon this object-centric representation, DCL learns to\napproximate the dynamic interaction among objects using graph networks. DCL\nfurther incorporates a semantic parser to parse questions into semantic\nprograms and, finally, a program executor to run the program to answer the\nquestion, levering the learned dynamics model. After training, DCL can detect\nand associate objects across the frames, ground visual properties, and physical\nevents, understand the causal relationship between events, make future and\ncounterfactual predictions, and leverage these extracted presentations for\nanswering queries. DCL achieves state-of-the-art performance on CLEVRER, a\nchallenging causal video reasoning dataset, even without using ground-truth\nattributes and collision labels from simulations for training. We further test\nDCL on a newly proposed video-retrieval and event localization dataset derived\nfrom CLEVRER, showing its strong generalization capacity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:48 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Zhenfang", ""], ["Mao", "Jiayuan", ""], ["Wu", "Jiajun", ""], ["Wong", "Kwan-Yee Kenneth", ""], ["Tenenbaum", "Joshua B.", ""], ["Gan", "Chuang", ""]]}, {"id": "2103.16565", "submitter": "Yuliang Zou", "authors": "Yuliang Zou, Jinwoo Choi, Qitong Wang, Jia-Bin Huang", "title": "Learning Representational Invariances for Data-Efficient Action\n  Recognition", "comments": "Project page: https://yuliang.vision/video-data-aug", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a ubiquitous technique for improving image\nclassification when labeled data is scarce. Constraining the model predictions\nto be invariant to diverse data augmentations effectively injects the desired\nrepresentational invariances to the model (e.g., invariance to photometric\nvariations), leading to improved accuracy. Compared to image data, the\nappearance variations in videos are far more complex due to the additional\ntemporal dimension. Yet, data augmentation methods for videos remain\nunder-explored. In this paper, we investigate various data augmentation\nstrategies that capture different video invariances, including photometric,\ngeometric, temporal, and actor/scene augmentations. When integrated with\nexisting consistency-based semi-supervised learning frameworks, we show that\nour data augmentation strategy leads to promising performance on the\nKinetics-100, UCF-101, and HMDB-51 datasets in the low-label regime. We also\nvalidate our data augmentation strategy in the fully supervised setting and\ndemonstrate improved performance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:49 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zou", "Yuliang", ""], ["Choi", "Jinwoo", ""], ["Wang", "Qitong", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2103.16597", "submitter": "Pratik Mazumder", "authors": "Pravendra Singh, Pratik Mazumder, Piyush Rai, Vinay P. Namboodiri", "title": "Rectification-based Knowledge Retention for Continual Learning", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models suffer from catastrophic forgetting when trained in an\nincremental learning setting. In this work, we propose a novel approach to\naddress the task incremental learning problem, which involves training a model\non new tasks that arrive in an incremental manner. The task incremental\nlearning problem becomes even more challenging when the test set contains\nclasses that are not part of the train set, i.e., a task incremental\ngeneralized zero-shot learning problem. Our approach can be used in both the\nzero-shot and non zero-shot task incremental learning settings. Our proposed\nmethod uses weight rectifications and affine transformations in order to adapt\nthe model to different tasks that arrive sequentially. Specifically, we adapt\nthe network weights to work for new tasks by \"rectifying\" the weights learned\nfrom the previous task. We learn these weight rectifications using very few\nparameters. We additionally learn affine transformations on the outputs\ngenerated by the network in order to better adapt them for the new task. We\nperform experiments on several datasets in both zero-shot and non zero-shot\ntask incremental learning settings and empirically show that our approach\nachieves state-of-the-art results. Specifically, our approach outperforms the\nstate-of-the-art non zero-shot task incremental learning method by over 5% on\nthe CIFAR-100 dataset. Our approach also significantly outperforms the\nstate-of-the-art task incremental generalized zero-shot learning method by\nabsolute margins of 6.91% and 6.33% for the AWA1 and CUB datasets,\nrespectively. We validate our approach using various ablation studies.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:11:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Singh", "Pravendra", ""], ["Mazumder", "Pratik", ""], ["Rai", "Piyush", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2103.16605", "submitter": "Yutong Zheng", "authors": "Yutong Zheng, Yu-Kai Huang, Ran Tao, Zhiqiang Shen and Marios Savvides", "title": "Unsupervised Disentanglement of Linear-Encoded Facial Semantics", "comments": "Accepted in IEEE Conference on Computer Vision and Pattern\n  Recognition 2021 (CVPR2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a method to disentangle linear-encoded facial semantics from\nStyleGAN without external supervision. The method derives from linear\nregression and sparse representation learning concepts to make the disentangled\nlatent representations easily interpreted as well. We start by coupling\nStyleGAN with a stabilized 3D deformable facial reconstruction method to\ndecompose single-view GAN generations into multiple semantics. Latent\nrepresentations are then extracted to capture interpretable facial semantics.\nIn this work, we make it possible to get rid of labels for disentangling\nmeaningful facial semantics. Also, we demonstrate that the guided extrapolation\nalong the disentangled representations can help with data augmentation, which\nsheds light on handling unbalanced data. Finally, we provide an analysis of our\nlearned localized facial representations and illustrate that the semantic\ninformation is encoded, which surprisingly complies with human intuition. The\noverall unsupervised design brings more flexibility to representation learning\nin the wild.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:23:48 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zheng", "Yutong", ""], ["Huang", "Yu-Kai", ""], ["Tao", "Ran", ""], ["Shen", "Zhiqiang", ""], ["Savvides", "Marios", ""]]}, {"id": "2103.16607", "submitter": "Oscar Ma\\~nas", "authors": "Oscar Ma\\~nas, Alexandre Lacoste, Xavier Giro-i-Nieto, David Vazquez,\n  Pau Rodriguez", "title": "Seasonal Contrast: Unsupervised Pre-Training from Uncurated Remote\n  Sensing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Remote sensing and automatic earth monitoring are key to solve global-scale\nchallenges such as disaster prevention, land use monitoring, or tackling\nclimate change. Although there exist vast amounts of remote sensing data, most\nof it remains unlabeled and thus inaccessible for supervised learning\nalgorithms. Transfer learning approaches can reduce the data requirements of\ndeep learning algorithms. However, most of these methods are pre-trained on\nImageNet and their generalization to remote sensing imagery is not guaranteed\ndue to the domain gap. In this work, we propose Seasonal Contrast (SeCo), an\neffective pipeline to leverage unlabeled data for in-domain pre-training of\nremote sensing representations. The SeCo pipeline is composed of two parts.\nFirst, a principled procedure to gather large-scale, unlabeled and uncurated\nremote sensing datasets containing images from multiple Earth locations at\ndifferent timestamps. Second, a self-supervised algorithm that takes advantage\nof time and position invariance to learn transferable representations for\nremote sensing applications. We empirically show that models trained with SeCo\nachieve better performance than their ImageNet pre-trained counterparts and\nstate-of-the-art self-supervised learning methods on multiple downstream tasks.\nThe datasets and models in SeCo will be made public to facilitate transfer\nlearning and enable rapid progress in remote sensing applications.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:26:39 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 16:27:08 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ma\u00f1as", "Oscar", ""], ["Lacoste", "Alexandre", ""], ["Giro-i-Nieto", "Xavier", ""], ["Vazquez", "David", ""], ["Rodriguez", "Pau", ""]]}, {"id": "2103.16617", "submitter": "Saverio Vadacchino", "authors": "Saverio Vadacchino, Raghav Mehta, Nazanin Mohammadi Sepahvand, Brennan\n  Nichyporuk, James J. Clark, and Tal Arbel", "title": "HAD-Net: A Hierarchical Adversarial Knowledge Distillation Network for\n  Improved Enhanced Tumour Segmentation Without Post-Contrast Images", "comments": "Accepted at Medical Imaging with Deep Learning (MIDL) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Segmentation of enhancing tumours or lesions from MRI is important for\ndetecting new disease activity in many clinical contexts. However, accurate\nsegmentation requires the inclusion of medical images (e.g., T1 post contrast\nMRI) acquired after injecting patients with a contrast agent (e.g.,\nGadolinium), a process no longer thought to be safe. Although a number of\nmodality-agnostic segmentation networks have been developed over the past few\nyears, they have been met with limited success in the context of enhancing\npathology segmentation. In this work, we present HAD-Net, a novel offline\nadversarial knowledge distillation (KD) technique, whereby a pre-trained\nteacher segmentation network, with access to all MRI sequences, teaches a\nstudent network, via hierarchical adversarial training, to better overcome the\nlarge domain shift presented when crucial images are absent during inference.\nIn particular, we apply HAD-Net to the challenging task of enhancing tumour\nsegmentation when access to post-contrast imaging is not available. The\nproposed network is trained and tested on the BraTS 2019 brain tumour\nsegmentation challenge dataset, where it achieves performance improvements in\nthe ranges of 16% - 26% over (a) recent modality-agnostic segmentation methods\n(U-HeMIS, U-HVED), (b) KD-Net adapted to this problem, (c) the pre-trained\nstudent network and (d) a non-hierarchical version of the network (AD-Net), in\nterms of Dice scores for enhancing tumour (ET). The network also shows\nimprovements in tumour core (TC) Dice scores. Finally, the network outperforms\nboth the baseline student network and AD-Net in terms of uncertainty\nquantification for enhancing tumour segmentation based on the BraTs 2019\nuncertainty challenge metrics. Our code is publicly available at:\nhttps://github.com/SaverioVad/HAD_Net\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:41:29 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 21:03:10 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 20:15:18 GMT"}, {"version": "v4", "created": "Wed, 12 May 2021 18:30:23 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Vadacchino", "Saverio", ""], ["Mehta", "Raghav", ""], ["Sepahvand", "Nazanin Mohammadi", ""], ["Nichyporuk", "Brennan", ""], ["Clark", "James J.", ""], ["Arbel", "Tal", ""]]}, {"id": "2103.16634", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Xiong Zhou, Tristan McKinney, Yanfeng Liu, Qinggang Zhou,\n  Fedor Zhdanov", "title": "Exploiting Invariance in Training Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by two basic mechanisms in animal visual systems, we introduce a\nfeature transform technique that imposes invariance properties in the training\nof deep neural networks. The resulting algorithm requires less parameter\ntuning, trains well with an initial learning rate 1.0, and easily generalizes\nto different tasks. We enforce scale invariance with local statistics in the\ndata to align similar samples generated in diverse situations. To accelerate\nconvergence, we enforce a GL(n)-invariance property with global statistics\nextracted from a batch that the gradient descent solution should remain\ninvariant under basis change. Tested on ImageNet, MS COCO, and Cityscapes\ndatasets, our proposed technique requires fewer iterations to train, surpasses\nall baselines by a large margin, seamlessly works on both small and large batch\nsize training, and applies to different computer vision tasks of image\nclassification, object detection, and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:18:31 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ye", "Chengxi", ""], ["Zhou", "Xiong", ""], ["McKinney", "Tristan", ""], ["Liu", "Yanfeng", ""], ["Zhou", "Qinggang", ""], ["Zhdanov", "Fedor", ""]]}, {"id": "2103.16651", "submitter": "Yuanyi Zhong", "authors": "Yuanyi Zhong, Jianfeng Wang, Lijuan Wang, Jian Peng, Yu-Xiong Wang,\n  Lei Zhang", "title": "DAP: Detection-Aware Pre-training with Weak Supervision", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a detection-aware pre-training (DAP) approach, which\nleverages only weakly-labeled classification-style datasets (e.g., ImageNet)\nfor pre-training, but is specifically tailored to benefit object detection\ntasks. In contrast to the widely used image classification-based pre-training\n(e.g., on ImageNet), which does not include any location-related training\ntasks, we transform a classification dataset into a detection dataset through a\nweakly supervised object localization method based on Class Activation Maps to\ndirectly pre-train a detector, making the pre-trained model location-aware and\ncapable of predicting bounding boxes. We show that DAP can outperform the\ntraditional classification pre-training in terms of both sample efficiency and\nconvergence speed in downstream detection tasks including VOC and COCO. In\nparticular, DAP boosts the detection accuracy by a large margin when the number\nof examples in the downstream task is small.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:48:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhong", "Yuanyi", ""], ["Wang", "Jianfeng", ""], ["Wang", "Lijuan", ""], ["Peng", "Jian", ""], ["Wang", "Yu-Xiong", ""], ["Zhang", "Lei", ""]]}, {"id": "2103.16652", "submitter": "Tobias Lorenz", "authors": "Tobias Lorenz, Anian Ruoss, Mislav Balunovi\\'c, Gagandeep Singh,\n  Martin Vechev", "title": "Robustness Certification for Point Cloud Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep 3D point cloud models in safety-critical applications, such\nas autonomous driving, dictates the need to certify the robustness of these\nmodels to semantic transformations. This is technically challenging as it\nrequires a scalable verifier tailored to point cloud models that handles a wide\nrange of semantic 3D transformations. In this work, we address this challenge\nand introduce 3DCertify, the first verifier able to certify robustness of point\ncloud models. 3DCertify is based on two key insights: (i) a generic relaxation\nbased on first-order Taylor approximations, applicable to any differentiable\ntransformation, and (ii) a precise relaxation for global feature pooling, which\nis more complex than pointwise activations (e.g., ReLU or sigmoid) but commonly\nemployed in point cloud models. We demonstrate the effectiveness of 3DCertify\nby performing an extensive evaluation on a wide range of 3D transformations\n(e.g., rotation, twisting) for both classification and part segmentation tasks.\nFor example, we can certify robustness against rotations by $\\pm60^\\circ$ for\n95.7% of point clouds, and our max pool relaxation increases certification by\nup to 15.6%.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:52:07 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Lorenz", "Tobias", ""], ["Ruoss", "Anian", ""], ["Balunovi\u0107", "Mislav", ""], ["Singh", "Gagandeep", ""], ["Vechev", "Martin", ""]]}, {"id": "2103.16670", "submitter": "Alexis Perakis", "authors": "Alexis Perakis, Ali Gorji, Samriddhi Jain, Krishna Chaitanya, Simone\n  Rizza, Ender Konukoglu", "title": "Contrastive Learning of Single-Cell Phenotypic Representations for\n  Treatment Classification", "comments": "12 pages, 2 figures, 7 tables. This article is a pre-print and is\n  currently under review at a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning robust representations to discriminate cell phenotypes based on\nmicroscopy images is important for drug discovery. Drug development efforts\ntypically analyse thousands of cell images to screen for potential treatments.\nEarly works focus on creating hand-engineered features from these images or\nlearn such features with deep neural networks in a fully or weakly-supervised\nframework. Both require prior knowledge or labelled datasets. Therefore,\nsubsequent works propose unsupervised approaches based on generative models to\nlearn these representations. Recently, representations learned with\nself-supervised contrastive loss-based methods have yielded state-of-the-art\nresults on various imaging tasks compared to earlier unsupervised approaches.\nIn this work, we leverage a contrastive learning framework to learn appropriate\nrepresentations from single-cell fluorescent microscopy images for the task of\nMechanism-of-Action classification. The proposed work is evaluated on the\nannotated BBBC021 dataset, and we obtain state-of-the-art results in NSC, NCSB\nand drop metrics for an unsupervised approach. We observe an improvement of 10%\nin NCSB accuracy and 11% in NSC-NSCB drop over the previously best unsupervised\nmethod. Moreover, the performance of our unsupervised approach ties with the\nbest supervised approach. Additionally, we observe that our framework performs\nwell even without post-processing, unlike earlier methods. With this, we\nconclude that one can learn robust cell representations with contrastive\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 20:29:04 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Perakis", "Alexis", ""], ["Gorji", "Ali", ""], ["Jain", "Samriddhi", ""], ["Chaitanya", "Krishna", ""], ["Rizza", "Simone", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2103.16671", "submitter": "Antonio Alliegro", "authors": "Antonio Alliegro, Diego Valsesia, Giulia Fracastoro, Enrico Magli,\n  Tatiana Tommasi", "title": "Denoise and Contrast for Category Agnostic Shape Completion", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep learning model that exploits the power of\nself-supervision to perform 3D point cloud completion, estimating the missing\npart and a context region around it. Local and global information are encoded\nin a combined embedding. A denoising pretext task provides the network with the\nneeded local cues, decoupled from the high-level semantics and naturally shared\nover multiple classes. On the other hand, contrastive learning maximizes the\nagreement between variants of the same shape with different missing portions,\nthus producing a representation which captures the global appearance of the\nshape. The combined embedding inherits category-agnostic properties from the\nchosen pretext tasks. Differently from existing approaches, this allows to\nbetter generalize the completion properties to new categories unseen at\ntraining time. Moreover, while decoding the obtained joint representation, we\nbetter blend the reconstructed missing part with the partial shape by paying\nattention to its known surrounding region and reconstructing this frame as\nauxiliary objective. Our extensive experiments and detailed ablation on the\nShapeNet dataset show the effectiveness of each part of the method with new\nstate of the art results. Our quantitative and qualitative analysis confirms\nhow our approach is able to work on novel categories without relying neither on\nclassification and shape symmetry priors, nor on adversarial training\nprocedures.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 20:33:24 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Alliegro", "Antonio", ""], ["Valsesia", "Diego", ""], ["Fracastoro", "Giulia", ""], ["Magli", "Enrico", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2103.16690", "submitter": "Vitor Guizilini", "authors": "Vitor Guizilini, Rares Ambrus, Wolfram Burgard, Adrien Gaidon", "title": "Sparse Auxiliary Networks for Unified Monocular Depth Prediction and\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating scene geometry from data obtained with cost-effective sensors is\nkey for robots and self-driving cars. In this paper, we study the problem of\npredicting dense depth from a single RGB image (monodepth) with optional sparse\nmeasurements from low-cost active depth sensors. We introduce Sparse Auxiliary\nNetworks (SANs), a new module enabling monodepth networks to perform both the\ntasks of depth prediction and completion, depending on whether only RGB images\nor also sparse point clouds are available at inference time. First, we decouple\nthe image and depth map encoding stages using sparse convolutions to process\nonly the valid depth map pixels. Second, we inject this information, when\navailable, into the skip connections of the depth prediction network,\naugmenting its features. Through extensive experimental analysis on one indoor\n(NYUv2) and two outdoor (KITTI and DDAD) benchmarks, we demonstrate that our\nproposed SAN architecture is able to simultaneously learn both tasks, while\nachieving a new state of the art in depth prediction by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:22:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Guizilini", "Vitor", ""], ["Ambrus", "Rares", ""], ["Burgard", "Wolfram", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2103.16693", "submitter": "Ilya Chugunov", "authors": "Ilya Chugunov, Seung-Hwan Baek, Qiang Fu, Wolfgang Heidrich, Felix\n  Heide", "title": "Mask-ToF: Learning Microlens Masks for Flying Pixel Correction in\n  Time-of-Flight Imaging", "comments": "CVPR 2021. Project page and code:\n  https://light.princeton.edu/publication/mask-tof", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Mask-ToF, a method to reduce flying pixels (FP) in\ntime-of-flight (ToF) depth captures. FPs are pervasive artifacts which occur\naround depth edges, where light paths from both an object and its background\nare integrated over the aperture. This light mixes at a sensor pixel to produce\nerroneous depth estimates, which can adversely affect downstream 3D vision\ntasks. Mask-ToF starts at the source of these FPs, learning a microlens-level\nocclusion mask which effectively creates a custom-shaped sub-aperture for each\nsensor pixel. This modulates the selection of foreground and background light\nmixtures on a per-pixel basis and thereby encodes scene geometric information\ndirectly into the ToF measurements. We develop a differentiable ToF simulator\nto jointly train a convolutional neural network to decode this information and\nproduce high-fidelity, low-FP depth reconstructions. We test the effectiveness\nof Mask-ToF on a simulated light field dataset and validate the method with an\nexperimental prototype. To this end, we manufacture the learned amplitude mask\nand design an optical relay system to virtually place it on a high-resolution\nToF sensor. We find that Mask-ToF generalizes well to real data without\nretraining, cutting FP counts in half.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:30:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chugunov", "Ilya", ""], ["Baek", "Seung-Hwan", ""], ["Fu", "Qiang", ""], ["Heidrich", "Wolfgang", ""], ["Heide", "Felix", ""]]}, {"id": "2103.16694", "submitter": "Vitor Guizilini", "authors": "Vitor Guizilini, Jie Li, Rares Ambrus, Adrien Gaidon", "title": "Geometric Unsupervised Domain Adaptation for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulators can efficiently generate large amounts of labeled synthetic data\nwith perfect supervision for hard-to-label tasks like semantic segmentation.\nHowever, they introduce a domain gap that severely hurts real-world\nperformance. We propose to use self-supervised monocular depth estimation as a\nproxy task to bridge this gap and improve sim-to-real unsupervised domain\nadaptation (UDA). Our Geometric Unsupervised Domain Adaptation method (GUDA)\nlearns a domain-invariant representation via a multi-task objective combining\nsynthetic semantic supervision with real-world geometric constraints on videos.\nGUDA establishes a new state of the art in UDA for semantic segmentation on\nthree benchmarks, outperforming methods that use domain adversarial learning,\nself-training, or other self-supervised proxy tasks. Furthermore, we show that\nour method scales well with the quality and quantity of synthetic data while\nalso improving depth prediction.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:33:00 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Guizilini", "Vitor", ""], ["Li", "Jie", ""], ["Ambrus", "Rares", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2103.16695", "submitter": "Roshan Reddy Upendra", "authors": "Roshan Reddy Upendra, Brian Jamison Wentz, Richard Simon, Suzanne M.\n  Shontz, Cristian A. Linte", "title": "CNN-based Cardiac Motion Extraction to Generate Deformable Geometric\n  Left Ventricle Myocardial Models from Cine MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patient-specific left ventricle (LV) myocardial models have the potential to\nbe used in a variety of clinical scenarios for improved diagnosis and treatment\nplans. Cine cardiac magnetic resonance (MR) imaging provides high resolution\nimages to reconstruct patient-specific geometric models of the LV myocardium.\nWith the advent of deep learning, accurate segmentation of cardiac chambers\nfrom cine cardiac MR images and unsupervised learning for image registration\nfor cardiac motion estimation on a large number of image datasets is\nattainable. Here, we propose a deep leaning-based framework for the development\nof patient-specific geometric models of LV myocardium from cine cardiac MR\nimages, using the Automated Cardiac Diagnosis Challenge (ACDC) dataset. We use\nthe deformation field estimated from the VoxelMorph-based convolutional neural\nnetwork (CNN) to propagate the isosurface mesh and volume mesh of the\nend-diastole (ED) frame to the subsequent frames of the cardiac cycle. We\nassess the CNN-based propagated models against segmented models at each cardiac\nphase, as well as models propagated using another traditional nonrigid image\nregistration technique.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:34:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Upendra", "Roshan Reddy", ""], ["Wentz", "Brian Jamison", ""], ["Simon", "Richard", ""], ["Shontz", "Suzanne M.", ""], ["Linte", "Cristian A.", ""]]}, {"id": "2103.16706", "submitter": "Yifan Wang", "authors": "Yifan Wang, Linjie Luo, Xiaohui Shen, Xing Mei", "title": "DynOcc: Learning Single-View Depth from Dynamic Occlusion Cues", "comments": "3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, significant progress has been made in single-view depth estimation\nthanks to increasingly large and diverse depth datasets. However, these\ndatasets are largely limited to specific application domains (e.g. indoor,\nautonomous driving) or static in-the-wild scenes due to hardware constraints or\ntechnical limitations of 3D reconstruction. In this paper, we introduce the\nfirst depth dataset DynOcc consisting of dynamic in-the-wild scenes. Our\napproach leverages the occlusion cues in these dynamic scenes to infer depth\nrelationships between points of selected video frames. To achieve accurate\nocclusion detection and depth order estimation, we employ a novel occlusion\nboundary detection, filtering and thinning scheme followed by a robust\nforeground/background classification method. In total our DynOcc dataset\ncontains 22M depth pairs out of 91K frames from a diverse set of videos. Using\nour dataset we achieved state-of-the-art results measured in weighted human\ndisagreement rate (WHDR). We also show that the inferred depth maps trained\nwith DynOcc can preserve sharper depth boundaries.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 22:17:36 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Yifan", ""], ["Luo", "Linjie", ""], ["Shen", "Xiaohui", ""], ["Mei", "Xing", ""]]}, {"id": "2103.16710", "submitter": "Albert Zeyer", "authors": "Albert Zeyer, Ralf Schl\\\"uter, Hermann Ney", "title": "A study of latent monotonic attention variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  End-to-end models reach state-of-the-art performance for speech recognition,\nbut global soft attention is not monotonic, which might lead to convergence\nproblems, to instability, to bad generalisation, cannot be used for online\nstreaming, and is also inefficient in calculation. Monotonicity can potentially\nfix all of this. There are several ad-hoc solutions or heuristics to introduce\nmonotonicity, but a principled introduction is rarely found in literature so\nfar. In this paper, we present a mathematically clean solution to introduce\nmonotonicity, by introducing a new latent variable which represents the audio\nposition or segment boundaries. We compare several monotonic latent models to\nour global soft attention baseline such as a hard attention model, a local\nwindowed soft attention model, and a segmental soft attention model. We can\nshow that our monotonic models perform as good as the global soft attention\nmodel. We perform our experiments on Switchboard 300h. We carefully outline the\ndetails of our training and release our code and configs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 22:35:56 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2103.16725", "submitter": "Zijian Hu", "authors": "Zijian Hu, Zhengyu Yang, Xuefeng Hu, Ram Nevatia", "title": "SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised\n  Classification", "comments": "Accepted to CVPR 2021. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common classification task situation is where one has a large amount of\ndata available for training, but only a small portion is annotated with class\nlabels. The goal of semi-supervised training, in this context, is to improve\nclassification accuracy by leverage information not only from labeled data but\nalso from a large amount of unlabeled data. Recent works have developed\nsignificant improvements by exploring the consistency constrain between\ndifferently augmented labeled and unlabeled data. Following this path, we\npropose a novel unsupervised objective that focuses on the less studied\nrelationship between the high confidence unlabeled data that are similar to\neach other. The new proposed Pair Loss minimizes the statistical distance\nbetween high confidence pseudo labels with similarity above a certain\nthreshold. Combining the Pair Loss with the techniques developed by the\nMixMatch family, our proposed SimPLE algorithm shows significant performance\ngains over previous algorithms on CIFAR-100 and Mini-ImageNet, and is on par\nwith the state-of-the-art methods on CIFAR-10 and SVHN. Furthermore, SimPLE\nalso outperforms the state-of-the-art methods in the transfer learning setting,\nwhere models are initialized by the weights pre-trained on ImageNet or\nDomainNet-Real. The code is available at github.com/zijian-hu/SimPLE.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 23:48:06 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hu", "Zijian", ""], ["Yang", "Zhengyu", ""], ["Hu", "Xuefeng", ""], ["Nevatia", "Ram", ""]]}, {"id": "2103.16744", "submitter": "Xinwen Liu", "authors": "Xinwen Liu, Jing Wang, Fangfang Tang, Shekhar S. Chandra, Feng Liu,\n  and Stuart Crozier", "title": "Deep Simultaneous Optimisation of Sampling and Reconstruction for\n  Multi-contrast MRI", "comments": "Presented at ISMRM 28th Annual Meeting & Exhibition (Poster #3619)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MRI images of the same subject in different contrasts contain shared\ninformation, such as the anatomical structure. Utilizing the redundant\ninformation amongst the contrasts to sub-sample and faithfully reconstruct\nmulti-contrast images could greatly accelerate the imaging speed, improve image\nquality and shorten scanning protocols. We propose an algorithm that generates\nthe optimised sampling pattern and reconstruction scheme of one contrast (e.g.\nT2-weighted image) when images with different contrast (e.g. T1-weighted image)\nhave been acquired. The proposed algorithm achieves increased PSNR and SSIM\nwith the resulting optimal sampling pattern compared to other acquisition\npatterns and single contrast methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 00:45:58 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Liu", "Xinwen", ""], ["Wang", "Jing", ""], ["Tang", "Fangfang", ""], ["Chandra", "Shekhar S.", ""], ["Liu", "Feng", ""], ["Crozier", "Stuart", ""]]}, {"id": "2103.16746", "submitter": "Xiao Wang", "authors": "Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong\n  Tian, Feng Wu", "title": "Towards More Flexible and Accurate Object Tracking with Natural\n  Language: Algorithms and Benchmark", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking by natural language specification is a new rising research topic\nthat aims at locating the target object in the video sequence based on its\nlanguage description. Compared with traditional bounding box (BBox) based\ntracking, this setting guides object tracking with high-level semantic\ninformation, addresses the ambiguity of BBox, and links local and global search\norganically together. Those benefits may bring more flexible, robust and\naccurate tracking performance in practical scenarios. However, existing natural\nlanguage initialized trackers are developed and compared on benchmark datasets\nproposed for tracking-by-BBox, which can't reflect the true power of\ntracking-by-language. In this work, we propose a new benchmark specifically\ndedicated to the tracking-by-language, including a large scale dataset, strong\nand diverse baseline methods. Specifically, we collect 2k video sequences\n(contains a total of 1,244,340 frames, 663 words) and split 1300/700 for the\ntrain/testing respectively. We densely annotate one sentence in English and\ncorresponding bounding boxes of the target object for each video. We also\nintroduce two new challenges into TNL2K for the object tracking task, i.e.,\nadversarial samples and modality switch. A strong baseline method based on an\nadaptive local-global-search scheme is proposed for future works to compare. We\nbelieve this benchmark will greatly boost related researches on natural\nlanguage guided tracking.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 00:57:32 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Xiao", ""], ["Shu", "Xiujun", ""], ["Zhang", "Zhipeng", ""], ["Jiang", "Bo", ""], ["Wang", "Yaowei", ""], ["Tian", "Yonghong", ""], ["Wu", "Feng", ""]]}, {"id": "2103.16748", "submitter": "Ning Yu", "authors": "Ning Yu, Guilin Liu, Aysegul Dundar, Andrew Tao, Bryan Catanzaro,\n  Larry Davis, Mario Fritz", "title": "Dual Contrastive Loss and Attention for GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) produce impressive results on\nunconditional image generation when powered with large-scale image datasets.\nYet generated images are still easy to spot especially on datasets with high\nvariance (e.g. bedroom, church). In this paper, we propose various improvements\nto further push the boundaries in image generation. Specifically, we propose a\nnovel dual contrastive loss and show that, with this loss, discriminator learns\nmore generalized and distinguishable representations to incentivize generation.\nIn addition, we revisit attention and extensively experiment with different\nattention blocks in the generator. We find attention to be still an important\nmodule for successful image generation even though it was not used in the\nrecent state-of-the-art models. Lastly, we study different attention\narchitectures in the discriminator, and propose a reference attention\nmechanism. By combining the strengths of these remedies, we improve the\ncompelling state-of-the-art Fr\\'{e}chet Inception Distance (FID) by at least\n17.5% on several benchmark datasets. We obtain even more significant\nimprovements on compositional synthetic scenes (up to 47.5% in FID).\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 01:10:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Ning", ""], ["Liu", "Guilin", ""], ["Dundar", "Aysegul", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""], ["Davis", "Larry", ""], ["Fritz", "Mario", ""]]}, {"id": "2103.16758", "submitter": "Jiesi Hu", "authors": "Jiesi Hu, Ganning Zhao, Suya You, C. C. Jay Kuo", "title": "Evaluation of Multimodal Semantic Segmentation using RGB-D Data", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to develop stable, accurate, and robust semantic scene\nunderstanding methods for wide-area scene perception and understanding,\nespecially in challenging outdoor environments. To achieve this, we are\nexploring and evaluating a range of related technology and solutions, including\nAI-driven multimodal scene perception, fusion, processing, and understanding.\nThis work reports our efforts on the evaluation of a state-of-the-art approach\nfor semantic segmentation with multiple RGB and depth sensing data. We employ\nfour large datasets composed of diverse urban and terrain scenes and design\nvarious experimental methods and metrics. In addition, we also develop new\nstrategies of multi-datasets learning to improve the detection and recognition\nof unseen objects. Extensive experiments, implementations, and results are\nreported in the paper.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 01:43:43 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hu", "Jiesi", ""], ["Zhao", "Ganning", ""], ["You", "Suya", ""], ["Kuo", "C. C. Jay", ""]]}, {"id": "2103.16760", "submitter": "Fernando Alonso-Fernandez", "authors": "Fernando Alonso-Fernandez, Kevin Hernandez Diaz, Silvia Ramis,\n  Francisco J. Perales, Josef Bigun", "title": "Facial Masks and Soft-Biometrics: Leveraging Face Recognition CNNs for\n  Age and Gender Prediction on Mobile Ocular Images", "comments": "Accepted for publication at IET Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the use of selfie ocular images captured with smartphones to\nestimate age and gender. Partial face occlusion has become an issue due to the\nmandatory use of face masks. Also, the use of mobile devices has exploded, with\nthe pandemic further accelerating the migration to digital services. However,\nstate-of-the-art solutions in related tasks such as identity or expression\nrecognition employ large Convolutional Neural Networks, whose use in mobile\ndevices is infeasible due to hardware limitations and size restrictions of\ndownloadable applications. To counteract this, we adapt two existing\nlightweight CNNs proposed in the context of the ImageNet Challenge, and two\nadditional architectures proposed for mobile face recognition. Since datasets\nfor soft-biometrics prediction using selfie images are limited, we counteract\nover-fitting by using networks pre-trained on ImageNet. Furthermore, some\nnetworks are further pre-trained for face recognition, for which very large\ntraining databases are available. Since both tasks employ similar input data,\nwe hypothesize that such strategy can be beneficial for soft-biometrics\nestimation. A comprehensive study of the effects of different pre-training over\nthe employed architectures is carried out, showing that, in most cases, a\nbetter accuracy is obtained after the networks have been fine-tuned for face\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 01:48:29 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 20:05:40 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Alonso-Fernandez", "Fernando", ""], ["Diaz", "Kevin Hernandez", ""], ["Ramis", "Silvia", ""], ["Perales", "Francisco J.", ""], ["Bigun", "Josef", ""]]}, {"id": "2103.16762", "submitter": "Shun Yi Pan", "authors": "Shun-Yi Pan, Cheng-You Lu, Shih-Po Lee, Wen-Hsiao Peng", "title": "Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional\n  Networks", "comments": "Accepted by ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work addresses weakly-supervised image semantic segmentation based on\nimage-level class labels. One common approach to this task is to propagate the\nactivation scores of Class Activation Maps (CAMs) using a random-walk mechanism\nin order to arrive at complete pseudo labels for training a semantic\nsegmentation network in a fully-supervised manner. However, the feed-forward\nnature of the random walk imposes no regularization on the quality of the\nresulting complete pseudo labels. To overcome this issue, we propose a Graph\nConvolutional Network (GCN)-based feature propagation framework. We formulate\nthe generation of complete pseudo labels as a semi-supervised learning task and\nlearn a 2-layer GCN separately for every training image by back-propagating a\nLaplacian and an entropy regularization loss. Experimental results on the\nPASCAL VOC 2012 dataset confirm the superiority of our scheme to several\nstate-of-the-art baselines. Our code is available at\nhttps://github.com/Xavier-Pan/WSGCN.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 02:05:01 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 11:42:30 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Pan", "Shun-Yi", ""], ["Lu", "Cheng-You", ""], ["Lee", "Shih-Po", ""], ["Peng", "Wen-Hsiao", ""]]}, {"id": "2103.16765", "submitter": "Xiangyu Yue", "authors": "Xiangyu Yue, Zangwei Zheng, Shanghang Zhang, Yang Gao, Trevor Darrell,\n  Kurt Keutzer, Alberto Sangiovanni Vincentelli", "title": "Prototypical Cross-domain Self-supervised Learning for Few-shot\n  Unsupervised Domain Adaptation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) transfers predictive models from a\nfully-labeled source domain to an unlabeled target domain. In some\napplications, however, it is expensive even to collect labels in the source\ndomain, making most previous works impractical. To cope with this problem,\nrecent work performed instance-wise cross-domain self-supervised learning,\nfollowed by an additional fine-tuning stage. However, the instance-wise\nself-supervised learning only learns and aligns low-level discriminative\nfeatures. In this paper, we propose an end-to-end Prototypical Cross-domain\nSelf-Supervised Learning (PCS) framework for Few-shot Unsupervised Domain\nAdaptation (FUDA). PCS not only performs cross-domain low-level feature\nalignment, but it also encodes and aligns semantic structures in the shared\nembedding space across domains. Our framework captures category-wise semantic\nstructures of the data by in-domain prototypical contrastive learning; and\nperforms feature alignment through cross-domain prototypical self-supervision.\nCompared with state-of-the-art methods, PCS improves the mean classification\naccuracy over different domain pairs on FUDA by 10.5%, 3.5%, 9.0%, and 13.2% on\nOffice, Office-Home, VisDA-2017, and DomainNet, respectively. Our project page\nis at http://xyue.io/pcs-fuda/index.html\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 02:07:42 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yue", "Xiangyu", ""], ["Zheng", "Zangwei", ""], ["Zhang", "Shanghang", ""], ["Gao", "Yang", ""], ["Darrell", "Trevor", ""], ["Keutzer", "Kurt", ""], ["Vincentelli", "Alberto Sangiovanni", ""]]}, {"id": "2103.16768", "submitter": "Daoping Zhang", "authors": "Daoping Zhang and Lok Ming Lui", "title": "Topology-Preserving 3D Image Segmentation Based On Hyperelastic\n  Regularization", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is to extract meaningful objects from a given image. For\ndegraded images due to occlusions, obscurities or noises, the accuracy of the\nsegmentation result can be severely affected. To alleviate this problem, prior\ninformation about the target object is usually introduced. In [10], a\ntopology-preserving registration-based segmentation model was proposed, which\nis restricted to segment 2D images only. In this paper, we propose a novel 3D\ntopology-preserving registration-based segmentation model with the hyperelastic\nregularization, which can handle both 2D and 3D images. The existence of the\nsolution of the proposed model is established. We also propose a converging\niterative scheme to solve the proposed model. Numerical experiments have been\ncarried out on the synthetic and real images, which demonstrate the\neffectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 02:20:46 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhang", "Daoping", ""], ["Lui", "Lok Ming", ""]]}, {"id": "2103.16773", "submitter": "Chaoyang Wang", "authors": "Chaoyang Wang and Simon Lucey", "title": "PAUL: Procrustean Autoencoder for Unsupervised Lifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent success in casting Non-rigid Structure from Motion (NRSfM) as an\nunsupervised deep learning problem has raised fundamental questions about what\nnovelty in NRSfM prior could the deep learning offer. In this paper we advocate\nfor a 3D deep auto-encoder framework to be used explicitly as the NRSfM prior.\nThe framework is unique as: (i) it learns the 3D auto-encoder weights solely\nfrom 2D projected measurements, and (ii) it is Procrustean in that it jointly\nresolves the unknown rigid pose for each shape instance. We refer to this\narchitecture as a Procustean Autoencoder for Unsupervised Lifting (PAUL), and\ndemonstrate state-of-the-art performance across a number of benchmarks in\ncomparison to recent innovations such as Deep NRSfM and C3PDO.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 02:31:01 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Chaoyang", ""], ["Lucey", "Simon", ""]]}, {"id": "2103.16775", "submitter": "Alana Santana Correia De", "authors": "Alana de Santana Correia, Esther Luna Colombini", "title": "Attention, please! A survey of Neural Attention Models in Deep Learning", "comments": "66 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In humans, Attention is a core property of all perceptual and cognitive\noperations. Given our limited ability to process competing sources, attention\nmechanisms select, modulate, and focus on the information most relevant to\nbehavior. For decades, concepts and functions of attention have been studied in\nphilosophy, psychology, neuroscience, and computing. For the last six years,\nthis property has been widely explored in deep neural networks. Currently, the\nstate-of-the-art in Deep Learning is represented by neural attention models in\nseveral application domains. This survey provides a comprehensive overview and\nanalysis of developments in neural attention models. We systematically reviewed\nhundreds of architectures in the area, identifying and discussing those in\nwhich attention has shown a significant impact. We also developed and made\npublic an automated methodology to facilitate the development of reviews in the\narea. By critically analyzing 650 works, we describe the primary uses of\nattention in convolutional, recurrent networks and generative models,\nidentifying common subgroups of uses and applications. Furthermore, we describe\nthe impact of attention in different application domains and their impact on\nneural networks' interpretability. Finally, we list possible trends and\nopportunities for further research, hoping that this review will provide a\nsuccinct overview of the main attentional models in the area and guide\nresearchers in developing future approaches that will drive further\nimprovements.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 02:42:28 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Correia", "Alana de Santana", ""], ["Colombini", "Esther Luna", ""]]}, {"id": "2103.16788", "submitter": "Jiangwei Xie", "authors": "Shipeng Yan, Jiangwei Xie, Xuming He", "title": "DER: Dynamically Expandable Representation for Class Incremental\n  Learning", "comments": "Accepted as an oral of CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of class incremental learning, which is a core step\ntowards achieving adaptive vision intelligence. In particular, we consider the\ntask setting of incremental learning with limited memory and aim to achieve\nbetter stability-plasticity trade-off. To this end, we propose a novel\ntwo-stage learning approach that utilizes a dynamically expandable\nrepresentation for more effective incremental concept modeling. Specifically,\nat each incremental step, we freeze the previously learned representation and\naugment it with additional feature dimensions from a new learnable feature\nextractor. This enables us to integrate new visual concepts with retaining\nlearned knowledge. We dynamically expand the representation according to the\ncomplexity of novel concepts by introducing a channel-level mask-based pruning\nstrategy. Moreover, we introduce an auxiliary loss to encourage the model to\nlearn diverse and discriminate features for novel concepts. We conduct\nextensive experiments on the three class incremental learning benchmarks and\nour method consistently outperforms other methods with a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 03:16:44 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yan", "Shipeng", ""], ["Xie", "Jiangwei", ""], ["He", "Xuming", ""]]}, {"id": "2103.16792", "submitter": "Shitao Tang", "authors": "Shitao Tang, Chengzhou Tang, Rui Huang, Siyu Zhu, Ping Tan", "title": "Learning Camera Localization via Dense Scene Matching", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera localization aims to estimate 6 DoF camera poses from RGB images.\nTraditional methods detect and match interest points between a query image and\na pre-built 3D model. Recent learning-based approaches encode scene structures\ninto a specific convolutional neural network (CNN) and thus are able to predict\ndense coordinates from RGB images. However, most of them require re-training or\nre-adaption for a new scene and have difficulties in handling large-scale\nscenes due to limited network capacity. We present a new method for scene\nagnostic camera localization using dense scene matching (DSM), where a cost\nvolume is constructed between a query image and a scene. The cost volume and\nthe corresponding coordinates are processed by a CNN to predict dense\ncoordinates. Camera poses can then be solved by PnP algorithms. In addition,\nour method can be extended to temporal domain, which leads to extra performance\nboost during testing time. Our scene-agnostic approach achieves comparable\naccuracy as the existing scene-specific approaches, such as KFNet, on the\n7scenes and Cambridge benchmark. This approach also remarkably outperforms\nstate-of-the-art scene-agnostic dense coordinate regression network SANet. The\nCode is available at https://github.com/Tangshitao/Dense-Scene-Matching.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 03:47:42 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tang", "Shitao", ""], ["Tang", "Chengzhou", ""], ["Huang", "Rui", ""], ["Zhu", "Siyu", ""], ["Tan", "Ping", ""]]}, {"id": "2103.16795", "submitter": "Amrutha Saseendran", "authors": "Amrutha Saseendran, Kathrin Skubch and Margret Keuper", "title": "Multi-Class Multi-Instance Count Conditioned Adversarial Image\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation has rapidly evolved in recent years. Modern architectures\nfor adversarial training allow to generate even high resolution images with\nremarkable quality. At the same time, more and more effort is dedicated towards\ncontrolling the content of generated images. In this paper, we take one further\nstep in this direction and propose a conditional generative adversarial network\n(GAN) that generates images with a defined number of objects from given\nclasses. This entails two fundamental abilities (1) being able to generate\nhigh-quality images given a complex constraint and (2) being able to count\nobject instances per class in a given image. Our proposed model modularly\nextends the successful StyleGAN2 architecture with a count-based conditioning\nas well as with a regression sub-network to count the number of generated\nobjects per class during training. In experiments on three different datasets,\nwe show that the proposed model learns to generate images according to the\ngiven multiple-class count condition even in the presence of complex\nbackgrounds. In particular, we propose a new dataset, CityCount, which is\nderived from the Cityscapes street scenes dataset, to evaluate our approach in\na challenging and practically relevant scenario.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 04:06:11 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Saseendran", "Amrutha", ""], ["Skubch", "Kathrin", ""], ["Keuper", "Margret", ""]]}, {"id": "2103.16806", "submitter": "Wang Wu", "authors": "Wu Wang, Yue Huang, Xinhao Ding", "title": "Self-Regression Learning for Blind Hyperspectral Image Fusion Without\n  Label", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hyperspectral image fusion (HIF) is critical to a wide range of applications\nin remote sensing and many computer vision applications. Most traditional HIF\nmethods assume that the observation model is predefined or known. However, in\nreal applications, the observation model involved are often complicated and\nunknown, which leads to the serious performance drop of many advanced HIF\nmethods. Also, deep learning methods can achieve outstanding performance, but\nthey generally require a large number of image pairs for model training, which\nare difficult to obtain in realistic scenarios. Towards these issues, we\nproposed a self-regression learning method that alternatively reconstructs\nhyperspectral image (HSI) and estimate the observation model. In particular, we\nadopt an invertible neural network (INN) for restoring the HSI, and two\nfully-connected network (FCN) for estimating the observation model. Moreover,\n\\emph{SoftMax} nonlinearity is applied to the FCN for satisfying the\nnon-negative, sparsity and equality constraints. Besides, we proposed a local\nconsistency loss function to constrain the observation model by exploring\ndomain specific knowledge. Finally, we proposed an angular loss function to\nimprove spectral reconstruction accuracy. Extensive experiments on both\nsynthetic and real-world dataset show that our model can outperform the\nstate-of-the-art methods\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 04:48:21 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Wu", ""], ["Huang", "Yue", ""], ["Ding", "Xinhao", ""]]}, {"id": "2103.16817", "submitter": "Suraj Nair", "authors": "Annie S. Chen, Suraj Nair, Chelsea Finn", "title": "Learning Generalizable Robotic Reward Functions from \"In-The-Wild\" Human\n  Videos", "comments": "https://sites.google.com/view/dvd-human-videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are motivated by the goal of generalist robots that can complete a wide\nrange of tasks across many environments. Critical to this is the robot's\nability to acquire some metric of task success or reward, which is necessary\nfor reinforcement learning, planning, or knowing when to ask for help. For a\ngeneral-purpose robot operating in the real world, this reward function must\nalso be able to generalize broadly across environments, tasks, and objects,\nwhile depending only on on-board sensor observations (e.g. RGB images). While\ndeep learning on large and diverse datasets has shown promise as a path towards\nsuch generalization in computer vision and natural language, collecting high\nquality datasets of robotic interaction at scale remains an open challenge. In\ncontrast, \"in-the-wild\" videos of humans (e.g. YouTube) contain an extensive\ncollection of people doing interesting tasks across a diverse range of\nsettings. In this work, we propose a simple approach, Domain-agnostic Video\nDiscriminator (DVD), that learns multitask reward functions by training a\ndiscriminator to classify whether two videos are performing the same task, and\ncan generalize by virtue of learning from a small amount of robot data with a\nbroad dataset of human videos. We find that by leveraging diverse human\ndatasets, this reward function (a) can generalize zero shot to unseen\nenvironments, (b) generalize zero shot to unseen tasks, and (c) can be combined\nwith visual model predictive control to solve robotic manipulation tasks on a\nreal WidowX200 robot in an unseen environment from a single human demo.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 05:25:05 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chen", "Annie S.", ""], ["Nair", "Suraj", ""], ["Finn", "Chelsea", ""]]}, {"id": "2103.16828", "submitter": "Wing Yin Yu", "authors": "Wing-Yin Yu, Lai-Man Po, Yuzhi Zhao, Jingjing Xiong, Kin-Wai Lau", "title": "Spatial Content Alignment For Pose Transfer", "comments": "IEEE International Conference on Multimedia and Expo (ICME) 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to unreliable geometric matching and content misalignment, most\nconventional pose transfer algorithms fail to generate fine-trained person\nimages. In this paper, we propose a novel framework Spatial Content Alignment\nGAN (SCAGAN) which aims to enhance the content consistency of garment textures\nand the details of human characteristics. We first alleviate the spatial\nmisalignment by transferring the edge content to the target pose in advance.\nSecondly, we introduce a new Content-Style DeBlk which can progressively\nsynthesize photo-realistic person images based on the appearance features of\nthe source image, the target pose heatmap and the prior transferred content in\nedge domain. We compare the proposed framework with several state-of-the-art\nmethods to show its superiority in quantitative and qualitative analysis.\nMoreover, detailed ablation study results demonstrate the efficacy of our\ncontributions. Codes are publicly available at\ngithub.com/rocketappslab/SCA-GAN.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:10:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Wing-Yin", ""], ["Po", "Lai-Man", ""], ["Zhao", "Yuzhi", ""], ["Xiong", "Jingjing", ""], ["Lau", "Kin-Wai", ""]]}, {"id": "2103.16831", "submitter": "Juhong Min", "authors": "Juhong Min, Minsu Cho", "title": "Convolutional Hough Matching Networks", "comments": "Accepted to CVPR 2021 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advances in feature representation, leveraging geometric relations is\ncrucial for establishing reliable visual correspondences under large variations\nof images. In this work we introduce a Hough transform perspective on\nconvolutional matching and propose an effective geometric matching algorithm,\ndubbed Convolutional Hough Matching (CHM). The method distributes similarities\nof candidate matches over a geometric transformation space and evaluate them in\na convolutional manner. We cast it into a trainable neural layer with a\nsemi-isotropic high-dimensional kernel, which learns non-rigid matching with a\nsmall number of interpretable parameters. To validate the effect, we develop\nthe neural network with CHM layers that perform convolutional matching in the\nspace of translation and scaling. Our method sets a new state of the art on\nstandard benchmarks for semantic visual correspondence, proving its strong\nrobustness to challenging intra-class variations.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:17:03 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Min", "Juhong", ""], ["Cho", "Minsu", ""]]}, {"id": "2103.16832", "submitter": "Zike Yan", "authors": "Zike Yan, Xin Wang, Hongbin Zha", "title": "Online Learning of a Probabilistic and Adaptive Scene Representation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Constructing and maintaining a consistent scene model on-the-fly is the core\ntask for online spatial perception, interpretation, and action. In this paper,\nwe represent the scene with a Bayesian nonparametric mixture model, seamlessly\ndescribing per-point occupancy status with a continuous probability density\nfunction. Instead of following the conventional data fusion paradigm, we\naddress the problem of online learning the process how sequential point cloud\ndata are generated from the scene geometry. An incremental and parallel\ninference is performed to update the parameter space in real-time. We\nexperimentally show that the proposed representation achieves state-of-the-art\naccuracy with promising efficiency. The consistent probabilistic formulation\nassures a generative model that is adaptive to different sensor\ncharacteristics, and the model complexity can be dynamically adjusted\non-the-fly according to different data scales.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:22:05 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yan", "Zike", ""], ["Wang", "Xin", ""], ["Zha", "Hongbin", ""]]}, {"id": "2103.16835", "submitter": "Jie Cao", "authors": "Jie Cao, Luanxuan Hou, Ming-Hsuan Yang, Ran He, Zhenan Sun", "title": "ReMix: Towards Image-to-Image Translation with Limited Data", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image (I2I) translation methods based on generative adversarial\nnetworks (GANs) typically suffer from overfitting when limited training data is\navailable. In this work, we propose a data augmentation method (ReMix) to\ntackle this issue. We interpolate training samples at the feature level and\npropose a novel content loss based on the perceptual relations among samples.\nThe generator learns to translate the in-between samples rather than memorizing\nthe training set, and thereby forces the discriminator to generalize. The\nproposed approach effectively reduces the ambiguity of generation and renders\ncontent-preserving results. The ReMix method can be easily incorporated into\nexisting GAN models with minor modifications. Experimental results on numerous\ntasks demonstrate that GAN models equipped with the ReMix method achieve\nsignificant improvements.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:24:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Cao", "Jie", ""], ["Hou", "Luanxuan", ""], ["Yang", "Ming-Hsuan", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "2103.16836", "submitter": "Hermann Courteille", "authors": "Hermann Courteille (LISTIC), A. Beno\\^it (LISTIC), N M\\'eger (LISTIC),\n  A Atto (LISTIC), D. Ienco (UMR TETIS)", "title": "Channel-Based Attention for LCC Using Sentinel-2 Time Series", "comments": null, "journal-ref": "International Geoscience and Remote Sensing Symposium (IGARSS),\n  Jul 2021, Brussels, Belgium", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are getting increasing attention to deal with\nLand Cover Classification (LCC) relying on Satellite Image Time Series (SITS).\nThough high performances can be achieved, the rationale of a prediction yielded\nby a DNN often remains unclear. An architecture expressing predictions with\nrespect to input channels is thus proposed in this paper. It relies on\nconvolutional layers and an attention mechanism weighting the importance of\neach channel in the final classification decision. The correlation between\nchannels is taken into account to set up shared kernels and lower model\ncomplexity. Experiments based on a Sentinel-2 SITS show promising results.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:24:15 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Courteille", "Hermann", "", "LISTIC"], ["Beno\u00eet", "A.", "", "LISTIC"], ["M\u00e9ger", "N", "", "LISTIC"], ["Atto", "A", "", "LISTIC"], ["Ienco", "D.", "", "UMR TETIS"]]}, {"id": "2103.16844", "submitter": "Jiangfan Han", "authors": "Jiangfan Han, Mengya Gao, Yujie Wang, Quanquan Li, Hongsheng Li,\n  Xiaogang Wang", "title": "Fixing the Teacher-Student Knowledge Discrepancy in Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training a small student network with the guidance of a larger teacher\nnetwork is an effective way to promote the performance of the student. Despite\nthe different types, the guided knowledge used to distill is always kept\nunchanged for different teacher and student pairs in previous knowledge\ndistillation methods. However, we find that teacher and student models with\ndifferent networks or trained from different initialization could have distinct\nfeature representations among different channels. (e.g. the high activated\nchannel for different categories). We name this incongruous representation of\nchannels as teacher-student knowledge discrepancy in the distillation process.\nIgnoring the knowledge discrepancy problem of teacher and student models will\nmake the learning of student from teacher more difficult. To solve this\nproblem, in this paper, we propose a novel student-dependent distillation\nmethod, knowledge consistent distillation, which makes teacher's knowledge more\nconsistent with the student and provides the best suitable knowledge to\ndifferent student networks for distillation. Extensive experiments on different\ndatasets (CIFAR100, ImageNet, COCO) and tasks (image classification, object\ndetection) reveal the widely existing knowledge discrepancy problem between\nteachers and students and demonstrate the effectiveness of our proposed method.\nOur method is very flexible that can be easily combined with other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:52:20 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Han", "Jiangfan", ""], ["Gao", "Mengya", ""], ["Wang", "Yujie", ""], ["Li", "Quanquan", ""], ["Li", "Hongsheng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2103.16847", "submitter": "Ella Lan", "authors": "Ella Selina Lan", "title": "A Novel Deep ML Architecture by Integrating Visual Simultaneous\n  Localization and Mapping (vSLAM) into Mask R-CNN for Real-time Surgical Video\n  Analysis", "comments": "18 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seven million people suffer complications after surgery each year. With\nsufficient surgical training and feedback, half of these complications could be\nprevented. Automatic surgical video analysis, especially for minimally invasive\nsurgery, plays a key role in training and review, with increasing interests\nfrom recent studies on tool and workflow detection. In this research, a novel\nmachine learning architecture, RPM-CNN, is created to perform real-time\nsurgical video analysis. This architecture, for the first time, integrates\nvisual simultaneous localization and mapping (vSLAM) into Mask R-CNN.\nSpatio-temporal information, in addition to the visual features, is utilized to\nincrease the accuracy to 96.8 mAP for tool detection and 97.5 mean Jaccard for\nworkflow detection, surpassing all previous works via the same benchmark\ndataset. As a real-time prediction, the RPM-CNN model reaches a 50 FPS runtime\nperformance speed, 10x faster than region based CNN, by modeling the\nspatio-temporal information directly from surgical videos during the vSLAM 3D\nmapping. Additionally, this novel Region Proposal Module (RPM) replaces the\nregion proposal network (RPN) in Mask R-CNN, accurately placing bounding-boxes\nand lessening the annotation requirement. In principle, this architecture\nintegrates the best of both worlds, inclusive of (1) vSLAM on object detection,\nthrough focusing on geometric information for region proposals and (2) CNN on\nobject recognition, through focusing on semantic information for image\nclassification; the integration of these two technologies into one joint\ntraining process opens a new door in computer vision. Furthermore, to apply\nRPM-CNN's real-time top performance to the real world, a Microsoft HoloLens 2\napplication is developed to provide an augmented reality (AR) based solution\nfor both surgical training and assistance.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:59:13 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 14:35:07 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Lan", "Ella Selina", ""]]}, {"id": "2103.16848", "submitter": "Hao Zhou", "authors": "Hao Zhou, Chongyang Zhang, Yan Luo, Yanjun Chen, Chuanping Hu", "title": "Embracing Uncertainty: Decoupling and De-bias for Robust Temporal\n  Grounding", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Temporal grounding aims to localize temporal boundaries within untrimmed\nvideos by language queries, but it faces the challenge of two types of\ninevitable human uncertainties: query uncertainty and label uncertainty. The\ntwo uncertainties stem from human subjectivity, leading to limited\ngeneralization ability of temporal grounding. In this work, we propose a novel\nDeNet (Decoupling and De-bias) to embrace human uncertainty: Decoupling - We\nexplicitly disentangle each query into a relation feature and a modified\nfeature. The relation feature, which is mainly based on skeleton-like words\n(including nouns and verbs), aims to extract basic and consistent information\nin the presence of query uncertainty. Meanwhile, modified feature assigned with\nstyle-like words (including adjectives, adverbs, etc) represents the subjective\ninformation, and thus brings personalized predictions; De-bias - We propose a\nde-bias mechanism to generate diverse predictions, aim to alleviate the bias\ncaused by single-style annotations in the presence of label uncertainty.\nMoreover, we put forward new multi-label metrics to diversify the performance\nevaluation. Extensive experiments show that our approach is more effective and\nrobust than state-of-the-arts on Charades-STA and ActivityNet Captions\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 07:00:56 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 07:29:25 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhou", "Hao", ""], ["Zhang", "Chongyang", ""], ["Luo", "Yan", ""], ["Chen", "Yanjun", ""], ["Hu", "Chuanping", ""]]}, {"id": "2103.16851", "submitter": "Jou Won Song", "authors": "Jou Won Song, Kyeongbo Kong, Ye In Park, Suk-Ju Kang", "title": "Attention Map-guided Two-stage Anomaly Detection using Hard Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection is a task that recognizes whether an input sample is\nincluded in the distribution of a target normal class or an anomaly class.\nConventional generative adversarial network (GAN)-based methods utilize an\nentire image including foreground and background as an input. However, in these\nmethods, a useless region unrelated to the normal class (e.g., unrelated\nbackground) is learned as normal class distribution, thereby leading to false\ndetection. To alleviate this problem, this paper proposes a novel two-stage\nnetwork consisting of an attention network and an anomaly detection GAN\n(ADGAN). The attention network generates an attention map that can indicate the\nregion representing the normal class distribution. To generate an accurate\nattention map, we propose the attention loss and the adversarial anomaly loss\nbased on synthetic anomaly samples generated from hard augmentation. By\napplying the attention map to an image feature map, ADGAN learns the normal\nclass distribution from which the useless region is removed, and it is possible\nto greatly reduce the problem difficulty of the anomaly detection task.\nAdditionally, the estimated attention map can be used for anomaly segmentation\nbecause it can distinguish between normal and anomaly regions. As a result, the\nproposed method outperforms the state-of-the-art anomaly detection and anomaly\nsegmentation methods for widely used datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 07:04:07 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Song", "Jou Won", ""], ["Kong", "Kyeongbo", ""], ["Park", "Ye In", ""], ["Kang", "Suk-Ju", ""]]}, {"id": "2103.16854", "submitter": "Fuyan Ma", "authors": "Fuyan Ma, Bin Sun and Shutao Li", "title": "Robust Facial Expression Recognition with Convolutional Visual\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Expression Recognition (FER) in the wild is extremely challenging due\nto occlusions, variant head poses, face deformation and motion blur under\nunconstrained conditions. Although substantial progresses have been made in\nautomatic FER in the past few decades, previous studies are mainly designed for\nlab-controlled FER. Real-world occlusions, variant head poses and other issues\ndefinitely increase the difficulty of FER on account of these\ninformation-deficient regions and complex backgrounds. Different from previous\npure CNNs based methods, we argue that it is feasible and practical to\ntranslate facial images into sequences of visual words and perform expression\nrecognition from a global perspective. Therefore, we propose Convolutional\nVisual Transformers to tackle FER in the wild by two main steps. First, we\npropose an attentional selective fusion (ASF) for leveraging the feature maps\ngenerated by two-branch CNNs. The ASF captures discriminative information by\nfusing multiple features with global-local attention. The fused feature maps\nare then flattened and projected into sequences of visual words. Second,\ninspired by the success of Transformers in natural language processing, we\npropose to model relationships between these visual words with global\nself-attention. The proposed method are evaluated on three public in-the-wild\nfacial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same\nsettings, extensive experiments demonstrate that our method shows superior\nperformance over other methods, setting new state of the art on RAF-DB with\n88.14%, FERPlus with 88.81% and AffectNet with 61.85%. We also conduct\ncross-dataset evaluation on CK+ show the generalization capability of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 07:07:56 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 03:41:03 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ma", "Fuyan", ""], ["Sun", "Bin", ""], ["Li", "Shutao", ""]]}, {"id": "2103.16871", "submitter": "Yuanxin Ye", "authors": "Yuanxin Ye, Jie Shan, Lorenzo Bruzzone, and Li Shen", "title": "Robust Registration of Multimodal Remote Sensing Images Based on\n  Structural Similarity", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2017.2656380", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic registration of multimodal remote sensing data (e.g., optical,\nLiDAR, SAR) is a challenging task due to the significant non-linear radiometric\ndifferences between these data. To address this problem, this paper proposes a\nnovel feature descriptor named the Histogram of Orientated Phase Congruency\n(HOPC), which is based on the structural properties of images. Furthermore, a\nsimilarity metric named HOPCncc is defined, which uses the normalized\ncorrelation coefficient (NCC) of the HOPC descriptors for multimodal\nregistration. In the definition of the proposed similarity metric, we first\nextend the phase congruency model to generate its orientation representation,\nand use the extended model to build HOPCncc. Then a fast template matching\nscheme for this metric is designed to detect the control points between images.\nThe proposed HOPCncc aims to capture the structural similarity between images,\nand has been tested with a variety of optical, LiDAR, SAR and map data. The\nresults show that HOPCncc is robust against complex non-linear radiometric\ndifferences and outperforms the state-of-the-art similarities metrics (i.e.,\nNCC and mutual information) in matching performance. Moreover, a robust\nregistration method is also proposed in this paper based on HOPCncc, which is\nevaluated using six pairs of multimodal remote sensing images. The experimental\nresults demonstrate the effectiveness of the proposed method for multimodal\nimage registration.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 07:51:21 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ye", "Yuanxin", ""], ["Shan", "Jie", ""], ["Bruzzone", "Lorenzo", ""], ["Shen", "Li", ""]]}, {"id": "2103.16874", "submitter": "Seunghwan Choi", "authors": "Seunghwan Choi, Sunghyun Park, Minsoo Lee, Jaegul Choo", "title": "VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware\n  Normalization", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of image-based virtual try-on aims to transfer a target clothing\nitem onto the corresponding region of a person, which is commonly tackled by\nfitting the item to the desired body part and fusing the warped item with the\nperson. While an increasing number of studies have been conducted, the\nresolution of synthesized images is still limited to low (e.g., 256x192), which\nacts as the critical limitation against satisfying online consumers. We argue\nthat the limitation stems from several challenges: as the resolution increases,\nthe artifacts in the misaligned areas between the warped clothes and the\ndesired clothing regions become noticeable in the final results; the\narchitectures used in existing methods have low performance in generating\nhigh-quality body parts and maintaining the texture sharpness of the clothes.\nTo address the challenges, we propose a novel virtual try-on method called\nVITON-HD that successfully synthesizes 1024x768 virtual try-on images.\nSpecifically, we first prepare the segmentation map to guide our virtual try-on\nsynthesis, and then roughly fit the target clothing item to a given person's\nbody. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS\ngenerator to handle the misaligned areas and preserve the details of 1024x768\ninputs. Through rigorous comparison with existing methods, we demonstrate that\nVITON-HD highly sur-passes the baselines in terms of synthesized image quality\nboth qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 07:52:41 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Choi", "Seunghwan", ""], ["Park", "Sunghyun", ""], ["Lee", "Minsoo", ""], ["Choo", "Jaegul", ""]]}, {"id": "2103.16877", "submitter": "Jie An", "authors": "Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu, Jiebo Luo", "title": "ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows", "comments": "CVPR 2021 Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Universal style transfer retains styles from reference images in content\nimages. While existing methods have achieved state-of-the-art style transfer\nperformance, they are not aware of the content leak phenomenon that the image\ncontent may corrupt after several rounds of stylization process. In this paper,\nwe propose ArtFlow to prevent content leak during universal style transfer.\nArtFlow consists of reversible neural flows and an unbiased feature transfer\nmodule. It supports both forward and backward inferences and operates in a\nprojection-transfer-reversion scheme. The forward inference projects input\nimages into deep features, while the backward inference remaps deep features\nback to input images in a lossless and unbiased way. Extensive experiments\ndemonstrate that ArtFlow achieves comparable performance to state-of-the-art\nstyle transfer methods while avoiding content leak.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 07:59:02 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 16:18:15 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["An", "Jie", ""], ["Huang", "Siyu", ""], ["Song", "Yibing", ""], ["Dou", "Dejing", ""], ["Liu", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "2103.16886", "submitter": "Ashkan Khakzar", "authors": "Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian\n  Rupprecht, Seong Tae Kim, Nassir Navab", "title": "Neural Response Interpretation through the Lens of Critical Pathways", "comments": "Accepted at CVPR 2021 (IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is critical input information encoded in specific sparse pathways within the\nneural network? In this work, we discuss the problem of identifying these\ncritical pathways and subsequently leverage them for interpreting the network's\nresponse to an input. The pruning objective -- selecting the smallest group of\nneurons for which the response remains equivalent to the original network --\nhas been previously proposed for identifying critical pathways. We demonstrate\nthat sparse pathways derived from pruning do not necessarily encode critical\ninput information. To ensure sparse pathways include critical fragments of the\nencoded input information, we propose pathway selection via neurons'\ncontribution to the response. We proceed to explain how critical pathways can\nreveal critical input features. We prove that pathways selected via neuron\ncontribution are locally linear (in an L2-ball), a property that we use for\nproposing a feature attribution method: \"pathway gradient\". We validate our\ninterpretation method using mainstream evaluation experiments. The validation\nof pathway gradient interpretation method further confirms that selected\npathways using neuron contributions correspond to critical input features. The\ncode is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:08:41 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Khakzar", "Ashkan", ""], ["Baselizadeh", "Soroosh", ""], ["Khanduja", "Saurabh", ""], ["Rupprecht", "Christian", ""], ["Kim", "Seong Tae", ""], ["Navab", "Nassir", ""]]}, {"id": "2103.16889", "submitter": "Liang Lin", "authors": "Guangrun Wang and Liang Lin and Rongcong Chen and Guangcong Wang and\n  Jiqi Zhang", "title": "Joint Learning of Neural Transfer and Architecture Adaptation for Image\n  Recognition", "comments": "To appear in IEEE Transactions ON Neural Networks and Learning\n  Systems. We prove that dynamically adapting network architectures tailored\n  for each domain task along with weight finetuning benefits in both efficiency\n  and effectiveness, compared to the existing image recognition pipeline that\n  only tunes the weights regardless of the architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art visual recognition systems usually rely on the\nfollowing pipeline: (a) pretraining a neural network on a large-scale dataset\n(e.g., ImageNet) and (b) finetuning the network weights on a smaller,\ntask-specific dataset. Such a pipeline assumes the sole weight adaptation is\nable to transfer the network capability from one domain to another domain,\nbased on a strong assumption that a fixed architecture is appropriate for all\ndomains. However, each domain with a distinct recognition target may need\ndifferent levels/paths of feature hierarchy, where some neurons may become\nredundant, and some others are re-activated to form new network structures. In\nthis work, we prove that dynamically adapting network architectures tailored\nfor each domain task along with weight finetuning benefits in both efficiency\nand effectiveness, compared to the existing image recognition pipeline that\nonly tunes the weights regardless of the architecture. Our method can be easily\ngeneralized to an unsupervised paradigm by replacing supernet training with\nself-supervised learning in the source domain tasks and performing linear\nevaluation in the downstream tasks. This further improves the search efficiency\nof our method. Moreover, we also provide principled and empirical analysis to\nexplain why our approach works by investigating the ineffectiveness of existing\nneural architecture search. We find that preserving the joint distribution of\nthe network architecture and weights is of importance. This analysis not only\nbenefits image recognition but also provides insights for crafting neural\nnetworks. Experiments on five representative image recognition tasks such as\nperson re-identification, age estimation, gender recognition, image\nclassification, and unsupervised domain adaptation demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:15:17 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Guangrun", ""], ["Lin", "Liang", ""], ["Chen", "Rongcong", ""], ["Wang", "Guangcong", ""], ["Zhang", "Jiqi", ""]]}, {"id": "2103.16909", "submitter": "Tian Xu", "authors": "Xu Chen, Bangguo Yin, Songqiang Chen, Haifeng Li and Tian Xu", "title": "Generating Multi-scale Maps from Remote Sensing Images via Series\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the success of generative adversarial networks (GANs) for\nimage-to-image translation, researchers have attempted to translate remote\nsensing images (RSIs) to maps (rs2map) through GAN for cartography. However,\nthese studies involved limited scales, which hinders multi-scale map creation.\nBy extending their method, multi-scale RSIs can be trivially translated to\nmulti-scale maps (multi-scale rs2map translation) through scale-wise rs2map\nmodels trained for certain scales (parallel strategy). However, this strategy\nhas two theoretical limitations. First, inconsistency between various spatial\nresolutions of multi-scale RSIs and object generalization on multi-scale maps\n(RS-m inconsistency) increasingly complicate the extraction of geographical\ninformation from RSIs for rs2map models with decreasing scale. Second, as\nrs2map translation is cross-domain, generators incur high computation costs to\ntransform the RSI pixel distribution to that on maps. Thus, we designed a\nseries strategy of generators for multi-scale rs2map translation to address\nthese limitations. In this strategy, high-resolution RSIs are inputted to an\nrs2map model to output large-scale maps, which are translated to multi-scale\nmaps through series multi-scale map translation models. The series strategy\navoids RS-m inconsistency as inputs are high-resolution large-scale RSIs, and\nreduces the distribution gap in multi-scale map generation through similar\npixel distributions among multi-scale maps. Our experimental results showed\nbetter quality multi-scale map generation with the series strategy, as shown by\naverage increases of 11.69%, 53.78%, 55.42%, and 72.34% in the structural\nsimilarity index, edge structural similarity index, intersection over union\n(road), and intersection over union (water) for data from Mexico City and Tokyo\nat zoom level 17-13.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:58:37 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chen", "Xu", ""], ["Yin", "Bangguo", ""], ["Chen", "Songqiang", ""], ["Li", "Haifeng", ""], ["Xu", "Tian", ""]]}, {"id": "2103.16923", "submitter": "David Reiser", "authors": "Nils L\\\"uling, David Reiser, Alexander Stana, H.W. Griepentrog", "title": "Using depth information and colour space variations for improving\n  outdoor robustness for instance segmentation of cabbage", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-based yield detection in agriculture could raiseharvest efficiency and\ncultivation performance of farms. Following this goal, this research focuses on\nimproving instance segmentation of field crops under varying environmental\nconditions. Five data sets of cabbage plants were recorded under varying\nlighting outdoor conditions. The images were acquired using a commercial mono\ncamera. Additionally, depth information was generated out of the image stream\nwith Structure-from-Motion (SfM). A Mask R-CNN was used to detect and segment\nthe cabbage heads. The influence of depth information and different colour\nspace representations were analysed. The results showed that depth combined\nwith colour information leads to a segmentation accuracy increase of 7.1%. By\ndescribing colour information by colour spaces using light and saturation\ninformation combined with depth information, additional segmentation\nimprovements of 16.5% could be reached. The CIELAB colour space combined with a\ndepth information layer showed the best results achieving a mean average\nprecision of 75.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:19:12 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["L\u00fcling", "Nils", ""], ["Reiser", "David", ""], ["Stana", "Alexander", ""], ["Griepentrog", "H. W.", ""]]}, {"id": "2103.16927", "submitter": "Yi Yu", "authors": "Yi Yu, Feipeng Da, Ziyu Zhang", "title": "Few-Data Guided Learning Upon End-to-End Point Cloud Network for 3D Face\n  Recognition", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face recognition has shown its potential in many application scenarios.\nAmong numerous 3D face recognition methods, deep-learning-based methods have\ndeveloped vigorously in recent years. In this paper, an end-to-end deep\nlearning network entitled Sur3dNet-Face for point-cloud-based 3D face\nrecognition is proposed. The network uses PointNet as the backbone, which is a\nsuccessful point cloud classification solution but does not work properly in\nface recognition. Supplemented with modifications in network architecture and a\nfew-data guided learning framework based on Gaussian process morphable model,\nthe backbone is successfully modified for 3D face recognition. Different from\nexisting methods training with a large amount of data in multiple datasets, our\nmethod uses Spring2003 subset of FRGC v2.0 for training which contains only 943\nfacial scans, and the network is well trained with the guidance of such a small\namount of real data. Without fine-tuning on the test set, the Rank-1\nRecognition Rate (RR1) is achieved as follows: 98.85% on FRGC v2.0 dataset and\n99.33% on Bosphorus dataset, which proves the effectiveness and the\npotentiality of our method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:26:14 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Yi", ""], ["Da", "Feipeng", ""], ["Zhang", "Ziyu", ""]]}, {"id": "2103.16938", "submitter": "Christoph Angermann", "authors": "Christoph Angermann and Ad\\'ela Moravov\\'a and Markus Haltmeier and\n  Steinbj\\\"orn J\\'onsson and Christian Laubichler", "title": "Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein\n  GANs", "comments": "submitted to the International Conference on Computer Vision (ICCV)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time estimation of actual environment depth is an essential module for\nvarious autonomous system tasks such as localization, obstacle detection and\npose estimation. During the last decade of machine learning, extensive\ndeployment of deep learning methods to computer vision tasks yielded successful\napproaches for realistic depth synthesis out of a simple RGB modality. While\nmost of these models rest on paired depth data or availability of video\nsequences and stereo images, there is a lack of methods facing single-image\ndepth synthesis in an unsupervised manner. Therefore, in this study, latest\nadvancements in the field of generative neural networks are leveraged to fully\nunsupervised single-image depth synthesis. To be more exact, two\ncycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are\nimplemented and simultaneously optimized using the Wasserstein-1 distance. To\nensure plausibility of the proposed method, we apply the models to a self\nacquised industrial data set as well as to the renown NYU Depth v2 data set,\nwhich allows comparison with existing approaches. The observed success in this\nstudy suggests high potential for unpaired single-image depth estimation in\nreal world applications.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:43:38 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Angermann", "Christoph", ""], ["Moravov\u00e1", "Ad\u00e9la", ""], ["Haltmeier", "Markus", ""], ["J\u00f3nsson", "Steinbj\u00f6rn", ""], ["Laubichler", "Christian", ""]]}, {"id": "2103.16940", "submitter": "ByungSoo Ko", "authors": "Byungsoo Ko, Geonmo Gu, Han-Gyu Kim", "title": "Learning with Memory-based Virtual Classes for Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core of deep metric learning (DML) involves learning visual similarities\nin high-dimensional embedding space. One of the main challenges is to\ngeneralize from seen classes of training data to unseen classes of test data.\nRecent works have focused on exploiting past embeddings to increase the number\nof instances for the seen classes. Such methods achieve performance improvement\nvia augmentation, while the strong focus on seen classes still remains. This\ncan be undesirable for DML, where training and test data exhibit entirely\ndifferent classes. In this work, we present a novel training strategy for DML\ncalled MemVir. Unlike previous works, MemVir memorizes both embedding features\nand class weights to utilize them as additional virtual classes. The\nexploitation of virtual classes not only utilizes augmented information for\ntraining but also alleviates a strong focus on seen classes for better\ngeneralization. Moreover, we embed the idea of curriculum learning by slowly\nadding virtual classes for a gradual increase in learning difficulty, which\nimproves the learning stability as well as the final performance. MemVir can be\neasily applied to many existing loss functions without any modification.\nExtensive experimental results on famous benchmarks demonstrate the superiority\nof MemVir over state-of-the-art competitors. Code of MemVir will be publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:44:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ko", "Byungsoo", ""], ["Gu", "Geonmo", ""], ["Kim", "Han-Gyu", ""]]}, {"id": "2103.16942", "submitter": "Luca Morreale", "authors": "Luca Morreale, Noam Aigerman, Vladimir Kim, Niloy J. Mitra", "title": "Neural Surface Maps", "comments": "project page: http://geometry.cs.ucl.ac.uk/projects/2021/neuralmaps/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maps are arguably one of the most fundamental concepts used to define and\noperate on manifold surfaces in differentiable geometry. Accordingly, in\ngeometry processing, maps are ubiquitous and are used in many core\napplications, such as paramterization, shape analysis, remeshing, and\ndeformation. Unfortunately, most computational representations of surface maps\ndo not lend themselves to manipulation and optimization, usually entailing\nhard, discrete problems. While algorithms exist to solve these problems, they\nare problem-specific, and a general framework for surface maps is still in\nneed. In this paper, we advocate considering neural networks as encoding\nsurface maps. Since neural networks can be composed on one another and are\ndifferentiable, we show it is easy to use them to define surfaces via atlases,\ncompose them for surface-to-surface mappings, and optimize differentiable\nobjectives relating to them, such as any notion of distortion, in a trivial\nmanner. In our experiments, we represent surfaces by generating a neural map\nthat approximates a UV parameterization of a 3D model. Then, we compose this\nmap with other neural maps which we optimize with respect to distortion\nmeasures. We show that our formulation enables trivial optimization of rather\nelusive mapping tasks, such as maps between a collection of surfaces.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:48:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Morreale", "Luca", ""], ["Aigerman", "Noam", ""], ["Kim", "Vladimir", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2103.17001", "submitter": "Senthil Yogamani", "authors": "Ciar\\'an Eising, Jonathan Horgan and Senthil Yogamani", "title": "Near-field Sensing Architecture for Low-Speed Vehicle Automation using a\n  Surround-view Fisheye Camera System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras are the primary sensor in automated driving systems. They provide\nhigh information density and are optimal for detecting road infrastructure cues\nlaid out for human vision. Surround view cameras typically comprise of four\nfisheye cameras with 190{\\deg} field-of-view covering the entire 360{\\deg}\naround the vehicle focused on near field sensing. They are the principal sensor\nfor low-speed, high accuracy and close-range sensing applications, such as\nautomated parking, traffic jam assistance and low-speed emergency braking. In\nthis work, we describe our visual perception architecture on surround view\ncameras designed for a system deployed in commercial vehicles, provide a\nfunctional review of the different stages of such a computer vision system, and\ndiscuss some of the current technological challenges. We have designed our\nsystem into four modular components namely Recognition, Reconstruction,\nRelocalization and Reorganization. We jointly call this the 4R Architecture. We\ndiscuss how each component accomplishes a specific aspect and how they are\nsynergized to form a complete system. Qualitative results are presented in the\nvideo at \\url{https://youtu.be/ae8bCOF77uY}.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:33:36 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Eising", "Ciar\u00e1n", ""], ["Horgan", "Jonathan", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2103.17012", "submitter": "Dat Thanh Tran", "authors": "Dat Thanh Tran, Moncef Gabbouj, Alexandros Iosifidis", "title": "Knowledge Distillation By Sparse Representation Matching", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation refers to a class of methods that transfers the\nknowledge from a teacher network to a student network. In this paper, we\npropose Sparse Representation Matching (SRM), a method to transfer intermediate\nknowledge obtained from one Convolutional Neural Network (CNN) to another by\nutilizing sparse representation learning. SRM first extracts sparse\nrepresentations of the hidden features of the teacher CNN, which are then used\nto generate both pixel-level and image-level labels for training intermediate\nfeature maps of the student network. We formulate SRM as a neural processing\nblock, which can be efficiently optimized using stochastic gradient descent and\nintegrated into any CNN in a plug-and-play manner. Our experiments demonstrate\nthat SRM is robust to architectural differences between the teacher and student\nnetworks, and outperforms other KD techniques across several datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:47:47 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tran", "Dat Thanh", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2103.17015", "submitter": "Yuanchao Bai", "authors": "Yuanchao Bai, Xianming Liu, Wangmeng Zuo, Yaowei Wang, Xiangyang Ji", "title": "Learning Scalable $\\ell_\\infty$-constrained Near-lossless Image\n  Compression via Joint Lossy Image and Residual Compression", "comments": "Accepted by CVPR 2021; Code:\n  https://github.com/BYchao100/Scalable-Near-lossless-Image-Compression", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel joint lossy image and residual compression framework for\nlearning $\\ell_\\infty$-constrained near-lossless image compression.\nSpecifically, we obtain a lossy reconstruction of the raw image through lossy\nimage compression and uniformly quantize the corresponding residual to satisfy\na given tight $\\ell_\\infty$ error bound. Suppose that the error bound is zero,\ni.e., lossless image compression, we formulate the joint optimization problem\nof compressing both the lossy image and the original residual in terms of\nvariational auto-encoders and solve it with end-to-end training. To achieve\nscalable compression with the error bound larger than zero, we derive the\nprobability model of the quantized residual by quantizing the learned\nprobability model of the original residual, instead of training multiple\nnetworks. We further correct the bias of the derived probability model caused\nby the context mismatch between training and inference. Finally, the quantized\nresidual is encoded according to the bias-corrected probability model and is\nconcatenated with the bitstream of the compressed lossy image. Experimental\nresults demonstrate that our near-lossless codec achieves the state-of-the-art\nperformance for lossless and near-lossless image compression, and achieves\ncompetitive PSNR while much smaller $\\ell_\\infty$ error compared with lossy\nimage codecs at high bit rates.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:53:36 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bai", "Yuanchao", ""], ["Liu", "Xianming", ""], ["Zuo", "Wangmeng", ""], ["Wang", "Yaowei", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2103.17020", "submitter": "Yuhongze Zhou", "authors": "Yuhongze Zhou, Liguang Zhou, Tin Lun Lam, Yangsheng Xu", "title": "Semantic-guided Automatic Natural Image Matting with Light-weight\n  Non-local Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural image matting aims to precisely separate foreground objects from\nbackground using alpha matte. Fully automatic natural image matting without\nexternal annotation is quite challenging. Well-performed matting methods\nusually require accurate labor-intensive handcrafted trimap as extra input,\nwhile the performance of automatic trimap generation method of dilating\nforeground segmentation fluctuates with segmentation quality. Therefore, we\nargue that how to handle trade-off of additional information input is a major\nissue in automatic matting. This paper presents a universal semantic-guided\nautomatic natural image matting pipeline with light-weight non-local attention\nwithout trimap and background image as input. Specifically, guided by semantic\ninformation of coarse foreground segmentation, Trimap Generation Network\nestimates accurate trimap. With estimated trimap and RGB image as input, our\nlight-weight Non-local Matting Network with Refinement produces final alpha\nmatte, whose trimap-guided global aggregation attention block is equipped with\nstride downsampling convolution, reducing computation complexity and promoting\nperformance. Experimental results show that our matting algorithm has\ncompetitive performance with current state-of-the-art methods in both\ntrimap-free and trimap-needed aspects.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:08:28 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 09:58:23 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhou", "Yuhongze", ""], ["Zhou", "Liguang", ""], ["Lam", "Tin Lun", ""], ["Xu", "Yangsheng", ""]]}, {"id": "2103.17022", "submitter": "Jia Zheng", "authors": "Jiale Xu and Jia Zheng and Yanyu Xu and Rui Tang and Shenghua Gao", "title": "Layout-Guided Novel View Synthesis from a Single Indoor Panorama", "comments": "To appear in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing view synthesis methods mainly focus on the perspective images and\nhave shown promising results. However, due to the limited field-of-view of the\npinhole camera, the performance quickly degrades when large camera movements\nare adopted. In this paper, we make the first attempt to generate novel views\nfrom a single indoor panorama and take the large camera translations into\nconsideration. To tackle this challenging problem, we first use Convolutional\nNeural Networks (CNNs) to extract the deep features and estimate the depth map\nfrom the source-view image. Then, we leverage the room layout prior, a strong\nstructural constraint of the indoor scene, to guide the generation of target\nviews. More concretely, we estimate the room layout in the source view and\ntransform it into the target viewpoint as guidance. Meanwhile, we also\nconstrain the room layout of the generated target-view images to enforce\ngeometric consistency. To validate the effectiveness of our method, we further\nbuild a large-scale photo-realistic dataset containing both small and large\ncamera translations. The experimental results on our challenging dataset\ndemonstrate that our method achieves state-of-the-art performance. The project\npage is at https://github.com/bluestyle97/PNVS.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:12:22 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Xu", "Jiale", ""], ["Zheng", "Jia", ""], ["Xu", "Yanyu", ""], ["Tang", "Rui", ""], ["Gao", "Shenghua", ""]]}, {"id": "2103.17045", "submitter": "Yusheng Peng", "authors": "Yusheng Peng, Gaofeng Zhang, Jun Shi, Benzhu Xu, Liping Zheng", "title": "SRA-LSTM: Social Relationship Attention LSTM for Human Trajectory\n  Prediction", "comments": "Submitted to Neural Computing and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pedestrian trajectory prediction for surveillance video is one of the\nimportant research topics in the field of computer vision and a key technology\nof intelligent surveillance systems. Social relationship among pedestrians is a\nkey factor influencing pedestrian walking patterns but was mostly ignored in\nthe literature. Pedestrians with different social relationships play different\nroles in the motion decision of target pedestrian. Motivated by this idea, we\npropose a Social Relationship Attention LSTM (SRA-LSTM) model to predict future\ntrajectories. We design a social relationship encoder to obtain the\nrepresentation of their social relationship through the relative position\nbetween each pair of pedestrians. Afterwards, the social relationship feature\nand latent movements are adopted to acquire the social relationship attention\nof this pair of pedestrians. Social interaction modeling is achieved by\nutilizing social relationship attention to aggregate movement information from\nneighbor pedestrians. Experimental results on two public walking pedestrian\nvideo datasets (ETH and UCY), our model achieves superior performance compared\nwith state-of-the-art methods. Contrast experiments with other attention\nmethods also demonstrate the effectiveness of social relationship attention.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:56:39 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Peng", "Yusheng", ""], ["Zhang", "Gaofeng", ""], ["Shi", "Jun", ""], ["Xu", "Benzhu", ""], ["Zheng", "Liping", ""]]}, {"id": "2103.17062", "submitter": "Xin Yang", "authors": "Xin Yang, Yu Qiao, Shaozhe Chen, Shengfeng He, Baocai Yin, Qiang\n  Zhang, Xiaopeng Wei, Rynson W.H.Lau", "title": "Smart Scribbles for Image Mating", "comments": "ACM Trans. Multimedia Comput. Commun. Appl", "journal-ref": null, "doi": "10.1145/3408323", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting is an ill-posed problem that usually requires additional user\ninput, such as trimaps or scribbles. Drawing a fne trimap requires a large\namount of user effort, while using scribbles can hardly obtain satisfactory\nalpha mattes for non-professional users. Some recent deep learning-based\nmatting networks rely on large-scale composite datasets for training to improve\nperformance, resulting in the occasional appearance of obvious artifacts when\nprocessing natural images. In this article, we explore the intrinsic\nrelationship between user input and alpha mattes and strike a balance between\nuser effort and the quality of alpha mattes. In particular, we propose an\ninteractive framework, referred to as smart scribbles, to guide users to draw\nfew scribbles on the input images to produce high-quality alpha mattes. It frst\ninfers the most informative regions of an image for drawing scribbles to\nindicate different categories (foreground, background, or unknown) and then\nspreads these scribbles (i.e., the category labels) to the rest of the image\nvia our well-designed two-phase propagation. Both neighboring low-level\nafnities and high-level semantic features are considered during the propagation\nprocess. Our method can be optimized without large-scale matting datasets and\nexhibits more universality in real situations. Extensive experiments\ndemonstrate that smart scribbles can produce more accurate alpha mattes with\nreduced additional input, compared to the state-of-the-art matting methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:30:49 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yang", "Xin", ""], ["Qiao", "Yu", ""], ["Chen", "Shaozhe", ""], ["He", "Shengfeng", ""], ["Yin", "Baocai", ""], ["Zhang", "Qiang", ""], ["Wei", "Xiaopeng", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "2103.17070", "submitter": "Jang Hyun Cho", "authors": "Jang Hyun Cho, Utkarsh Mall, Kavita Bala, Bharath Hariharan", "title": "PiCIE: Unsupervised Semantic Segmentation using Invariance and\n  Equivariance in Clustering", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new framework for semantic segmentation without annotations via\nclustering. Off-the-shelf clustering methods are limited to curated,\nsingle-label, and object-centric images yet real-world data are dominantly\nuncurated, multi-label, and scene-centric. We extend clustering from images to\npixels and assign separate cluster membership to different instances within\neach image. However, solely relying on pixel-wise feature similarity fails to\nlearn high-level semantic concepts and overfits to low-level visual cues. We\npropose a method to incorporate geometric consistency as an inductive bias to\nlearn invariance and equivariance for photometric and geometric variations.\nWith our novel learning objective, our framework can learn high-level semantic\nconcepts. Our method, PiCIE (Pixel-level feature Clustering using Invariance\nand Equivariance), is the first method capable of segmenting both things and\nstuff categories without any hyperparameter tuning or task-specific\npre-processing. Our method largely outperforms existing baselines on COCO and\nCityscapes with +17.5 Acc. and +4.5 mIoU. We show that PiCIE gives a better\ninitialization for standard supervised training. The code is available at\nhttps://github.com/janghyuncho/PiCIE.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:12:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Cho", "Jang Hyun", ""], ["Mall", "Utkarsh", ""], ["Bala", "Kavita", ""], ["Hariharan", "Bharath", ""]]}, {"id": "2103.17084", "submitter": "Jingyi Zhang", "authors": "Jingyi Zhang, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Shijian Lu", "title": "DA-DETR: Domain Adaptive Detection Transformer by Hybrid Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The prevalent approach in domain adaptive object detection adopts a two-stage\narchitecture (Faster R-CNN) that involves a number of hyper-parameters and\nhand-crafted designs such as anchors, region pooling, non-maximum suppression,\netc. Such architecture makes it very complicated while adopting certain\nexisting domain adaptation methods with different ways of feature alignment. In\nthis work, we adopt a one-stage detector and design DA-DETR, a simple yet\neffective domain adaptive object detection network that performs inter-domain\nalignment with a single discriminator. DA-DETR introduces a hybrid attention\nmodule that explicitly pinpoints the hard-aligned features for simple yet\neffective alignment across domains. It greatly simplifies traditional domain\nadaptation pipelines by eliminating sophisticated routines that involve\nmultiple adversarial learning frameworks with different types of features.\nDespite its simplicity, extensive experiments show that DA-DETR demonstrates\nsuperior accuracy as compared with highly-optimized state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:55:56 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhang", "Jingyi", ""], ["Huang", "Jiaxing", ""], ["Luo", "Zhipeng", ""], ["Zhang", "Gongjie", ""], ["Lu", "Shijian", ""]]}, {"id": "2103.17086", "submitter": "Peter Tan", "authors": "Dayu Tan, Zheng Huang, Xin Peng, Weimin Zhong, Vladimir Mahalec", "title": "Deep adaptive fuzzy clustering for evolutionary unsupervised\n  representation learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster assignment of large and complex images is a crucial but challenging\ntask in pattern recognition and computer vision. In this study, we explore the\npossibility of employing fuzzy clustering in a deep neural network framework.\nThus, we present a novel evolutionary unsupervised learning representation\nmodel with iterative optimization. It implements the deep adaptive fuzzy\nclustering (DAFC) strategy that learns a convolutional neural network\nclassifier from given only unlabeled data samples. DAFC consists of a deep\nfeature quality-verifying model and a fuzzy clustering model, where deep\nfeature representation learning loss function and embedded fuzzy clustering\nwith the weighted adaptive entropy is implemented. We joint fuzzy clustering to\nthe deep reconstruction model, in which fuzzy membership is utilized to\nrepresent a clear structure of deep cluster assignments and jointly optimize\nfor the deep representation learning and clustering. Also, the joint model\nevaluates current clustering performance by inspecting whether the re-sampled\ndata from estimated bottleneck space have consistent clustering properties to\nprogressively improve the deep clustering model. Comprehensive experiments on a\nvariety of datasets show that the proposed method obtains a substantially\nbetter performance for both reconstruction and clustering quality when compared\nto the other state-of-the-art deep clustering methods, as demonstrated with the\nin-depth analysis in the extensive experiments.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:58:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tan", "Dayu", ""], ["Huang", "Zheng", ""], ["Peng", "Xin", ""], ["Zhong", "Weimin", ""], ["Mahalec", "Vladimir", ""]]}, {"id": "2103.17104", "submitter": "Wenyan Cong", "authors": "Wenyan Cong, Junyan Cao, Li Niu, Jianfu Zhang, Xuesong Gao, Zhiwei\n  Tang, Liqing Zhang", "title": "Deep Image Harmonization by Bridging the Reality Gap", "comments": "17 pages with supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image harmonization has been significantly advanced with large-scale\nharmonization dataset. However, the current way to build dataset is still\nlabor-intensive, which adversely affects the extendability of dataset. To\naddress this problem, we propose to construct a large-scale rendered\nharmonization dataset RHHarmony with fewer human efforts to augment the\nexisting real-world dataset. To leverage both real-world images and rendered\nimages, we propose a cross-domain harmonization network CharmNet to bridge the\ndomain gap between two domains. Moreover, we also employ well-designed style\nclassifiers and losses to facilitate cross-domain knowledge transfer. Extensive\nexperiments demonstrate the potential of using rendered images for image\nharmonization and the effectiveness of our proposed network. Our dataset and\ncode are available at\nhttps://github.com/bcmi/Rendered_Image_Harmonization_Datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:19:56 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Cong", "Wenyan", ""], ["Cao", "Junyan", ""], ["Niu", "Li", ""], ["Zhang", "Jianfu", ""], ["Gao", "Xuesong", ""], ["Tang", "Zhiwei", ""], ["Zhang", "Liqing", ""]]}, {"id": "2103.17105", "submitter": "Eu Wern Teh", "authors": "Eu Wern Teh, Terrance DeVries, Brendan Duke, Ruowei Jiang, Parham\n  Aarabi, Graham W. Taylor", "title": "The GIST and RIST of Iterative Self-Training for Semi-Supervised\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of semi-supervised semantic segmentation, where we aim\nto produce pixel-wise semantic object masks given only a small number of\nhuman-labeled training examples. We focus on iterative self-training methods in\nwhich we explore the behavior of self-training over multiple refinement stages.\nWe show that iterative self-training leads to performance degradation if done\nna\\\"ively with a fixed ratio of human-labeled to pseudo-labeled training\nexamples. We propose Greedy Iterative Self-Training (GIST) and Random Iterative\nSelf-Training (RIST) strategies that alternate between training on either\nhuman-labeled data or pseudo-labeled data at each refinement stage, resulting\nin a performance boost rather than degradation. We further show that GIST and\nRIST can be combined with existing semi-supervised learning methods to boost\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:20:37 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 13:57:39 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Teh", "Eu Wern", ""], ["DeVries", "Terrance", ""], ["Duke", "Brendan", ""], ["Jiang", "Ruowei", ""], ["Aarabi", "Parham", ""], ["Taylor", "Graham W.", ""]]}, {"id": "2103.17107", "submitter": "Andrey Savchenko", "authors": "Andrey V. Savchenko", "title": "Facial expression and attributes recognition based on multi-task\n  learning of lightweight neural networks", "comments": "14 pages, 3 figures, accepted at IEEE SISY 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the multi-task learning of lightweight convolutional neural\nnetworks is studied for face identification and classification of facial\nattributes (age, gender, ethnicity) trained on cropped faces without margins.\nThe necessity to fine-tune these networks to predict facial expressions is\nhighlighted. Several models are presented based on MobileNet, EfficientNet and\nRexNet architectures. It was experimentally demonstrated that they lead to near\nstate-of-the-art results in age, gender and race recognition on the UTKFace\ndataset and emotion classification on the AffectNet dataset. Moreover, it is\nshown that the usage of the trained models as feature extractors of facial\nregions in video frames leads to 4.5% higher accuracy than the previously known\nstate-of-the-art single models for the AFEW and the VGAF datasets from the\nEmotiW challenges. The models and source code are publicly available at\nhttps://github.com/HSE-asavchenko/face-emotion-recognition.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:21:04 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 10:19:13 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Savchenko", "Andrey V.", ""]]}, {"id": "2103.17111", "submitter": "Ezequiel de la Rosa", "authors": "Ezequiel de la Rosa, David Robben, Diana M. Sima, Jan S. Kirschke,\n  Bjoern Menze", "title": "Differentiable Deconvolution for Improved Stroke Perfusion Analysis", "comments": "Accepted at MICCAI 2020", "journal-ref": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention 2020 Oct 4 (pp. 593-602)", "doi": "10.1007/978-3-030-59728-3_58", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Perfusion imaging is the current gold standard for acute ischemic stroke\nanalysis. It allows quantification of the salvageable and non-salvageable\ntissue regions (penumbra and core areas respectively). In clinical settings,\nthe singular value decomposition (SVD) deconvolution is one of the most\naccepted and used approaches for generating interpretable and physically\nmeaningful maps. Though this method has been widely validated in experimental\nand clinical settings, it might produce suboptimal results because the chosen\ninputs to the model cannot guarantee optimal performance. For the most critical\ninput, the arterial input function (AIF), it is still controversial how and\nwhere it should be chosen even though the method is very sensitive to this\ninput. In this work we propose an AIF selection approach that is optimized for\nmaximal core lesion segmentation performance. The AIF is regressed by a neural\nnetwork optimized through a differentiable SVD deconvolution, aiming to\nmaximize core lesion segmentation agreement with ground truth data. To our\nknowledge, this is the first work exploiting a differentiable deconvolution\nmodel with neural networks. We show that our approach is able to generate AIFs\nwithout any manual annotation, and hence avoiding manual rater's influences.\nThe method achieves manual expert performance in the ISLES18 dataset. We\nconclude that the methodology opens new possibilities for improving perfusion\nimaging quantification with deep neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:29:36 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["de la Rosa", "Ezequiel", ""], ["Robben", "David", ""], ["Sima", "Diana M.", ""], ["Kirschke", "Jan S.", ""], ["Menze", "Bjoern", ""]]}, {"id": "2103.17115", "submitter": "Hanzhe Hu", "authors": "Hanzhe Hu, Shuai Bai, Aoxue Li, Jinshi Cui, Liwei Wang", "title": "Dense Relation Distillation with Context-aware Aggregation for Few-Shot\n  Object Detection", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional deep learning based methods for object detection require a large\namount of bounding box annotations for training, which is expensive to obtain\nsuch high quality annotated data. Few-shot object detection, which learns to\nadapt to novel classes with only a few annotated examples, is very challenging\nsince the fine-grained feature of novel object can be easily overlooked with\nonly a few data available. In this work, aiming to fully exploit features of\nannotated novel object and capture fine-grained features of query object, we\npropose Dense Relation Distillation with Context-aware Aggregation (DCNet) to\ntackle the few-shot detection problem. Built on the meta-learning based\nframework, Dense Relation Distillation module targets at fully exploiting\nsupport features, where support features and query feature are densely matched,\ncovering all spatial locations in a feed-forward fashion. The abundant usage of\nthe guidance information endows model the capability to handle common\nchallenges such as appearance changes and occlusions. Moreover, to better\ncapture scale-aware features, Context-aware Aggregation module adaptively\nharnesses features from different scales for a more comprehensive feature\nrepresentation. Extensive experiments illustrate that our proposed approach\nachieves state-of-the-art results on PASCAL VOC and MS COCO datasets. Code will\nbe made available at https://github.com/hzhupku/DCNet.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:34:49 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hu", "Hanzhe", ""], ["Bai", "Shuai", ""], ["Li", "Aoxue", ""], ["Cui", "Jinshi", ""], ["Wang", "Liwei", ""]]}, {"id": "2103.17118", "submitter": "Zhenhua Xu", "authors": "Zhenhua Xu, Yuxiang Sun, Ming Liu", "title": "iCurb: Imitation Learning-based Detection of Road Curbs using Aerial\n  Images for Autonomous Driving", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters,6,(2021),1097-1104", "doi": "10.1109/LRA.2021.3056344", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of road curbs is an essential capability for autonomous driving. It\ncan be used for autonomous vehicles to determine drivable areas on roads.\nUsually, road curbs are detected on-line using vehicle-mounted sensors, such as\nvideo cameras and 3-D Lidars. However, on-line detection using video cameras\nmay suffer from challenging illumination conditions, and Lidar-based approaches\nmay be difficult to detect far-away road curbs due to the sparsity issue of\npoint clouds. In recent years, aerial images are becoming more and more\nworldwide available. We find that the visual appearances between road areas and\noff-road areas are usually different in aerial images, so we propose a novel\nsolution to detect road curbs off-line using aerial images. The input to our\nmethod is an aerial image, and the output is directly a graph (i.e., vertices\nand edges) representing road curbs. To this end, we formulate the problem as an\nimitation learning problem, and design a novel network and an innovative\ntraining strategy to train an agent to iteratively find the road-curb graph.\nThe experimental results on a public dataset confirm the effectiveness and\nsuperiority of our method. This work is accompanied with a demonstration video\nand a supplementary document at https://tonyxuqaq.github.io/iCurb/.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:40:31 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Xu", "Zhenhua", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""]]}, {"id": "2103.17119", "submitter": "Zhenhua Xu", "authors": "Zhenhua Xu, Yuxiang Sun, Ming Liu", "title": "Topo-boundary: A Benchmark Dataset on Topological Road-boundary\n  Detection Using Aerial Images for Autonomous Driving", "comments": "Accepted by IEEE Robotics and Automation Letters(RA-L) and The\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road-boundary detection is important for autonomous driving. It can be used\nto constrain autonomous vehicles running on road areas to ensure driving\nsafety. Compared with online road-boundary detection using on-vehicle\ncameras/Lidars, offline detection using aerial images could alleviate the\nsevere occlusion issue. Moreover, the offline detection results can be directly\nemployed to annotate high-definition (HD) maps. In recent years, deep-learning\ntechnologies have been used in offline detection. But there still lacks a\npublicly available dataset for this task, which hinders the research progress\nin this area. So in this paper, we propose a new benchmark dataset, named\n\\textit{Topo-boundary}, for offline topological road-boundary detection. The\ndataset contains 25,295 $1000\\times1000$-sized 4-channel aerial images. Each\nimage is provided with 8 training labels for different sub-tasks. We also\ndesign a new entropy-based metric for connectivity evaluation, which could\nbetter handle noises or outliers. We implement and evaluate 3\nsegmentation-based baselines and 5 graph-based baselines using the dataset. We\nalso propose a new imitation-learning-based baseline which is enhanced from our\nprevious work. The superiority of our enhancement is demonstrated from the\ncomparison. The dataset and our-implemented code for the baselines are\navailable at \\texttt{\\url{https://tonyxuqaq.github.io/Topo-boundary/}}.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:42:00 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 08:46:54 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Xu", "Zhenhua", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""]]}, {"id": "2103.17123", "submitter": "Trung-Nghia Le", "authors": "Trung-Nghia Le, Yubo Cao, Tan-Cong Nguyen, Minh-Quan Le, Khanh-Duy\n  Nguyen, Thanh-Toan Do, Minh-Triet Tran, Tam V. Nguyen", "title": "Camouflaged Instance Segmentation In-The-Wild: Dataset And Benchmark\n  Suite", "comments": "13 pages, 15 figures, 6 tables. Project page:\n  https://sites.google.com/view/ltnghia/research/camo_plus_plus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper pushes the envelope on camouflaged regions to decompose them into\nmeaningful components, namely, camouflaged instances. To promote the new task\nof camouflaged instance segmentation in-the-wild, we introduce a new dataset,\nnamely CAMO++, by extending our preliminary CAMO dataset (camouflaged object\nsegmentation) in terms of quantity and diversity. The new dataset substantially\nincreases the number of images with hierarchical pixel-wise ground-truths. We\nalso provide a benchmark suite for the task of camouflaged instance\nsegmentation. In particular, we conduct extensive evaluation of\nstate-of-the-art instance segmentation methods on our newly constructed CAMO++\ndataset in various scenarios. We also propose Camouflage Fusion Learning (CFL)\nframework for camouflaged instance segmentation to further improve the\nstate-of-the-art performance. The dataset, models, evaluation suite, and\nbenchmark will be publicly available at our project page:\nhttps://sites.google.com/view/ltnghia/research/camo_plus_plus\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:46:12 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 01:25:37 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 01:22:30 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Le", "Trung-Nghia", ""], ["Cao", "Yubo", ""], ["Nguyen", "Tan-Cong", ""], ["Le", "Minh-Quan", ""], ["Nguyen", "Khanh-Duy", ""], ["Do", "Thanh-Toan", ""], ["Tran", "Minh-Triet", ""], ["Nguyen", "Tam V.", ""]]}, {"id": "2103.17126", "submitter": "Jun Liu", "authors": "Jun Liu, Ryan Wen Liu, Jianing Sun, Tieyong Zeng", "title": "Rank-One Prior: Toward Real-Time Scene Recovery", "comments": "9 pages, 6 figures, 1 table, Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scene recovery is a fundamental imaging task for several practical\napplications, e.g., video surveillance and autonomous vehicles, etc. To improve\nvisual quality under different weather/imaging conditions, we propose a\nreal-time light correction method to recover the degraded scenes in the cases\nof sandstorms, underwater, and haze. The heart of our work is that we propose\nan intensity projection strategy to estimate the transmission. This strategy is\nmotivated by a straightforward rank-one transmission prior. The complexity of\ntransmission estimation is $O(N)$ where $N$ is the size of the single image.\nThen we can recover the scene in real-time. Comprehensive experiments on\ndifferent types of weather/imaging conditions illustrate that our method\noutperforms competitively several state-of-the-art imaging methods in terms of\nefficiency and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:47:55 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 02:19:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Liu", "Jun", ""], ["Liu", "Ryan Wen", ""], ["Sun", "Jianing", ""], ["Zeng", "Tieyong", ""]]}, {"id": "2103.17138", "submitter": "Fengda Zhu", "authors": "Fengda Zhu, Xiwen Liang, Yi Zhu, Xiaojun Chang, Xiaodan Liang", "title": "SOON: Scenario Oriented Object Navigation with Graph-based Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to navigate like a human towards a language-guided target from\nanywhere in a 3D embodied environment is one of the 'holy grail' goals of\nintelligent robots. Most visual navigation benchmarks, however, focus on\nnavigating toward a target from a fixed starting point, guided by an elaborate\nset of instructions that depicts step-by-step. This approach deviates from\nreal-world problems in which human-only describes what the object and its\nsurrounding look like and asks the robot to start navigation from anywhere.\nAccordingly, in this paper, we introduce a Scenario Oriented Object Navigation\n(SOON) task. In this task, an agent is required to navigate from an arbitrary\nposition in a 3D embodied environment to localize a target following a scene\ndescription. To give a promising direction to solve this task, we propose a\nnovel graph-based exploration (GBE) method, which models the navigation state\nas a graph and introduces a novel graph-based exploration approach to learn\nknowledge from the graph and stabilize training by learning sub-optimal\ntrajectories. We also propose a new large-scale benchmark named From Anywhere\nto Object (FAO) dataset. To avoid target ambiguity, the descriptions in FAO\nprovide rich semantic scene information includes: object attribute, object\nrelationship, region description, and nearby region description. Our\nexperiments reveal that the proposed GBE outperforms various state-of-the-arts\non both FAO and R2R datasets. And the ablation studies on FAO validates the\nquality of the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:01:04 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhu", "Fengda", ""], ["Liang", "Xiwen", ""], ["Zhu", "Yi", ""], ["Chang", "Xiaojun", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2103.17154", "submitter": "Bin Yan", "authors": "Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, Huchuan Lu", "title": "Learning Spatio-Temporal Transformer for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new tracking architecture with an encoder-decoder\ntransformer as the key component. The encoder models the global spatio-temporal\nfeature dependencies between target objects and search regions, while the\ndecoder learns a query embedding to predict the spatial positions of the target\nobjects. Our method casts object tracking as a direct bounding box prediction\nproblem, without using any proposals or predefined anchors. With the\nencoder-decoder transformer, the prediction of objects just uses a simple\nfully-convolutional network, which estimates the corners of objects directly.\nThe whole method is end-to-end, does not need any postprocessing steps such as\ncosine window and bounding box smoothing, thus largely simplifying existing\ntracking pipelines. The proposed tracker achieves state-of-the-art performance\non five challenging short-term and long-term benchmarks, while running at\nreal-time speed, being 6x faster than Siam R-CNN. Code and models are\nopen-sourced at https://github.com/researchmm/Stark.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:19:19 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yan", "Bin", ""], ["Peng", "Houwen", ""], ["Fu", "Jianlong", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""]]}, {"id": "2103.17171", "submitter": "Joona Pohjonen", "authors": "Joona Pohjonen, Carolin St\\\"urenberg, Antti Rannikko, Tuomas Mirtti,\n  Esa Pitk\\\"anen", "title": "Spectral decoupling allows training transferable neural networks in\n  medical imaging", "comments": "8 pages, 5 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many current neural networks for medical imaging generalise poorly to data\nunseen during training. Such behaviour can be caused by networks overfitting\neasy-to-learn, or statistically dominant, features while disregarding other\npotentially informative features. For example, indistinguishable differences in\nthe sharpness of the images from two different scanners can degrade the\nperformance of the network significantly. All neural networks intended for\nclinical practice need to be robust to variation in data caused by differences\nin imaging equipment, sample preparation and patient populations.\n  To address these challenges, we evaluate the utility of spectral decoupling\nas an implicit bias mitigation method. Spectral decoupling encourages the\nneural network to learn more features by simply regularising the networks'\nunnormalised prediction scores with an L2 penalty, thus having no added\ncomputational costs.\n  We show that spectral decoupling allows training neural networks on datasets\nwith strong spurious correlations. Networks trained without spectral decoupling\ndo not learn the original task and appear to make false predictions based on\nthe spurious correlations. Spectral decoupling also increases networks'\nrobustness for data distribution shifts. To validate our findings, we train\nnetworks with and without spectral decoupling to detect prostate cancer tissue\nslides and COVID-19 in chest radiographs. Networks trained with spectral\ndecoupling achieve substantially higher performance on all evaluation datasets.\n  Our results show that spectral decoupling helps with generalisation issues\nassociated with neural networks. We recommend using spectral decoupling as an\nimplicit bias mitigation method in any neural network intended for clinical\nuse.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:47:01 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 12:11:58 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 13:36:37 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Pohjonen", "Joona", ""], ["St\u00fcrenberg", "Carolin", ""], ["Rannikko", "Antti", ""], ["Mirtti", "Tuomas", ""], ["Pitk\u00e4nen", "Esa", ""]]}, {"id": "2103.17172", "submitter": "Tsuyoshi Okita", "authors": "Hokuto Hirano and Tsuyoshi Okita", "title": "Classification of Hematoma: Joint Learning of Semantic Segmentation and\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cerebral hematoma grows rapidly in 6-24 hours and misprediction of the growth\ncan be fatal if it is not operated by a brain surgeon. There are two types of\ncerebral hematomas: one that grows rapidly and the other that does not grow\nrapidly. We are developing the technique of artificial intelligence to\ndetermine whether the CT image includes the cerebral hematoma which leads to\nthe rapid growth. This problem has various difficulties: the few positive cases\nin this classification problem of cerebral hematoma and the targeted hematoma\nhas deformable object. Other difficulties include the imbalance classification,\nthe covariate shift, the small data, and the spurious correlation problems. It\nis difficult with the plain CNN classification such as VGG. This paper proposes\nthe joint learning of semantic segmentation and classification and evaluate the\nperformance of this.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:47:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hirano", "Hokuto", ""], ["Okita", "Tsuyoshi", ""]]}, {"id": "2103.17185", "submitter": "Dmytro Kotovenko", "authors": "Dmytro Kotovenko, Matthias Wright, Arthur Heimbrecht, Bj\\\"orn Ommer", "title": "Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes", "comments": "Accepted at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been many successful implementations of neural style transfer in\nrecent years. In most of these works, the stylization process is confined to\nthe pixel domain. However, we argue that this representation is unnatural\nbecause paintings usually consist of brushstrokes rather than pixels. We\npropose a method to stylize images by optimizing parameterized brushstrokes\ninstead of pixels and further introduce a simple differentiable rendering\nmechanism. Our approach significantly improves visual quality and enables\nadditional control over the stylization process such as controlling the flow of\nbrushstrokes through user input. We provide qualitative and quantitative\nevaluations that show the efficacy of the proposed parameterized\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:15:03 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kotovenko", "Dmytro", ""], ["Wright", "Matthias", ""], ["Heimbrecht", "Arthur", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2103.17195", "submitter": "Keshigeyan Chandrasegaran", "authors": "Keshigeyan Chandrasegaran, Ngoc-Trung Tran, Ngai-Man Cheung", "title": "A Closer Look at Fourier Spectrum Discrepancies for CNN-generated Images\n  Detection", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  CNN-based generative modelling has evolved to produce synthetic images\nindistinguishable from real images in the RGB pixel space. Recent works have\nobserved that CNN-generated images share a systematic shortcoming in\nreplicating high frequency Fourier spectrum decay attributes. Furthermore,\nthese works have successfully exploited this systematic shortcoming to detect\nCNN-generated images reporting up to 99% accuracy across multiple\nstate-of-the-art GAN models.\n  In this work, we investigate the validity of assertions claiming that\nCNN-generated images are unable to achieve high frequency spectral decay\nconsistency. We meticulously construct a counterexample space of high frequency\nspectral decay consistent CNN-generated images emerging from our handcrafted\nexperiments using DCGAN, LSGAN, WGAN-GP and StarGAN, where we empirically show\nthat this frequency discrepancy can be avoided by a minor architecture change\nin the last upsampling operation. We subsequently use images from this\ncounterexample space to successfully bypass the recently proposed forensics\ndetector which leverages on high frequency Fourier spectrum decay attributes\nfor CNN-generated image detection.\n  Through this study, we show that high frequency Fourier spectrum decay\ndiscrepancies are not inherent characteristics for existing CNN-based\ngenerative models--contrary to the belief of some existing work--, and such\nfeatures are not robust to perform synthetic image detection. Our results\nprompt re-thinking of using high frequency Fourier spectrum decay attributes\nfor CNN-generated image detection. Code and models are available at\nhttps://keshik6.github.io/Fourier-Discrepancies-CNN-Detection/\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:24:54 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chandrasegaran", "Keshigeyan", ""], ["Tran", "Ngoc-Trung", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "2103.17202", "submitter": "Abhinav Kumar", "authors": "Abhinav Kumar, Garrick Brazil and Xiaoming Liu", "title": "GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D\n  Object Detection", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern 3D object detectors have immensely benefited from the end-to-end\nlearning idea. However, most of them use a post-processing algorithm called\nNon-Maximal Suppression (NMS) only during inference. While there were attempts\nto include NMS in the training pipeline for tasks such as 2D object detection,\nthey have been less widely adopted due to a non-mathematical expression of the\nNMS. In this paper, we present and integrate GrooMeD-NMS -- a novel Grouped\nMathematically Differentiable NMS for monocular 3D object detection, such that\nthe network is trained end-to-end with a loss on the boxes after NMS. We first\nformulate NMS as a matrix operation and then group and mask the boxes in an\nunsupervised manner to obtain a simple closed-form expression of the NMS.\nGrooMeD-NMS addresses the mismatch between training and inference pipelines\nand, therefore, forces the network to select the best 3D box in a\ndifferentiable manner. As a result, GrooMeD-NMS achieves state-of-the-art\nmonocular 3D object detection results on the KITTI benchmark dataset performing\ncomparably to monocular video-based methods. Code and models at\nhttps://github.com/abhi1kumar/groomed_nms\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:29:50 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kumar", "Abhinav", ""], ["Brazil", "Garrick", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2103.17204", "submitter": "Dominik Rivoir", "authors": "Dominik Rivoir, Micha Pfeiffer, Reuben Docea, Fiona Kolbinger, Carina\n  Riediger, J\\\"urgen Weitz, Stefanie Speidel", "title": "Long-Term Temporally Consistent Unpaired Video Translation from\n  Simulated Surgical 3D Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research in unpaired video translation has mainly focused on short-term\ntemporal consistency by conditioning on neighboring frames. However for\ntransfer from simulated to photorealistic sequences, available information on\nthe underlying geometry offers potential for achieving global consistency\nacross views. We propose a novel approach which combines unpaired image\ntranslation with neural rendering to transfer simulated to photorealistic\nsurgical abdominal scenes. By introducing global learnable textures and a\nlighting-invariant view-consistency loss, our method produces consistent\ntranslations of arbitrary views and thus enables long-term consistent video\nsynthesis. We design and test our model to generate video sequences from\nminimally-invasive surgical abdominal scenes. Because labeled data is often\nlimited in this domain, photorealistic data where ground truth information from\nthe simulated domain is preserved is especially relevant. By extending existing\nimage-based methods to view-consistent videos, we aim to impact the\napplicability of simulated training and evaluation environments for surgical\napplications. Code and data will be made publicly available soon.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:31:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Rivoir", "Dominik", ""], ["Pfeiffer", "Micha", ""], ["Docea", "Reuben", ""], ["Kolbinger", "Fiona", ""], ["Riediger", "Carina", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}, {"id": "2103.17213", "submitter": "Andrea Loddo", "authors": "Andrea Loddo, Cecilia Di Ruberto, A.M.P.G. Vale, Mariano Ucchesu, J.M.\n  Soares, Gianluigi Bacchetta", "title": "An effective and friendly tool for seed image analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image analysis is an essential field for several topics in the life sciences,\nsuch as biology or botany. In particular, the analysis of seeds (e.g. fossil\nresearch) can provide significant information on their evolution, the history\nof agriculture, plant domestication and knowledge of diets in ancient times.\nThis work aims to present software that performs image analysis for feature\nextraction and classification from images containing seeds through a novel and\nunique framework. In detail, we propose two plugins \\emph{ImageJ}, one able to\nextract morphological, textual and colour features from seed images, and\nanother to classify seeds into categories using the extracted features. The\nexperimental results demonstrated the correctness and validity of both the\nextracted features and the classification predictions. The proposed tool is\neasily extendable to other fields of image analysis.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:56:22 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 10:34:12 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Loddo", "Andrea", ""], ["Di Ruberto", "Cecilia", ""], ["Vale", "A. M. P. G.", ""], ["Ucchesu", "Mariano", ""], ["Soares", "J. M.", ""], ["Bacchetta", "Gianluigi", ""]]}, {"id": "2103.17220", "submitter": "Chen Yukang", "authors": "Yukang Chen, Yanwei Li, Tao Kong, Lu Qi, Ruihang Chu, Lei Li, Jiaya\n  Jia", "title": "Scale-aware Automatic Augmentation for Object Detection", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Scale-aware AutoAug to learn data augmentation policies for object\ndetection. We define a new scale-aware search space, where both image- and\nbox-level augmentations are designed for maintaining scale invariance. Upon\nthis search space, we propose a new search metric, termed Pareto Scale Balance,\nto facilitate search with high efficiency. In experiments, Scale-aware AutoAug\nyields significant and consistent improvement on various object detectors\n(e.g., RetinaNet, Faster R-CNN, Mask R-CNN, and FCOS), even compared with\nstrong multi-scale training baselines. Our searched augmentation policies are\ntransferable to other datasets and box-level tasks beyond object detection\n(e.g., instance segmentation and keypoint estimation) to improve performance.\nThe search cost is much less than previous automated augmentation approaches\nfor object detection. It is notable that our searched policies have meaningful\npatterns, which intuitively provide valuable insight for human data\naugmentation design. Code and models will be available at\nhttps://github.com/Jia-Research-Lab/SA-AutoAug.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:11:14 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chen", "Yukang", ""], ["Li", "Yanwei", ""], ["Kong", "Tao", ""], ["Qi", "Lu", ""], ["Chu", "Ruihang", ""], ["Li", "Lei", ""], ["Jia", "Jiaya", ""]]}, {"id": "2103.17229", "submitter": "Zhenzhang Ye", "authors": "Zhenzhang Ye, Tarun Yenamandra, Florian Bernard, Daniel Cremers", "title": "Joint Deep Multi-Graph Matching and 3D Geometry Learning from\n  Inhomogeneous 2D Image Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph matching aims to establish correspondences between vertices of graphs\nsuch that both the node and edge attributes agree. Various learning-based\nmethods were recently proposed for finding correspondences between image key\npoints based on deep graph matching formulations. While these approaches mainly\nfocus on learning node and edge attributes, they completely ignore the 3D\ngeometry of the underlying 3D objects depicted in the 2D images. We fill this\ngap by proposing a trainable framework that takes advantage of graph neural\nnetworks for learning a deformable 3D geometry model from inhomogeneous image\ncollections, i.e. a set of images that depict different instances of objects\nfrom the same category. Experimentally we demonstrate that our method\noutperforms recent learning-based approaches for graph matching considering\nboth accuracy and cycle-consistency error, while we in addition obtain the\nunderlying 3D geometry of the objects depicted in the 2D images.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:25:36 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ye", "Zhenzhang", ""], ["Yenamandra", "Tarun", ""], ["Bernard", "Florian", ""], ["Cremers", "Daniel", ""]]}, {"id": "2103.17230", "submitter": "Jihwan Bang", "authors": "Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, Jonghyun Choi", "title": "Rainbow Memory: Continual Learning with a Memory of Diverse Samples", "comments": "Accepted paper at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning is a realistic learning scenario for AI models. Prevalent\nscenario of continual learning, however, assumes disjoint sets of classes as\ntasks and is less realistic rather artificial. Instead, we focus on 'blurry'\ntask boundary; where tasks shares classes and is more realistic and practical.\nTo address such task, we argue the importance of diversity of samples in an\nepisodic memory. To enhance the sample diversity in the memory, we propose a\nnovel memory management strategy based on per-sample classification uncertainty\nand data augmentation, named Rainbow Memory (RM). With extensive empirical\nvalidations on MNIST, CIFAR10, CIFAR100, and ImageNet datasets, we show that\nthe proposed method significantly improves the accuracy in blurry continual\nlearning setups, outperforming state of the arts by large margins despite its\nsimplicity. Code and data splits will be available in\nhttps://github.com/clovaai/rainbow-memory.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:28:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bang", "Jihwan", ""], ["Kim", "Heesu", ""], ["Yoo", "YoungJoon", ""], ["Ha", "Jung-Woo", ""], ["Choi", "Jonghyun", ""]]}, {"id": "2103.17235", "submitter": "Debesh Jha", "authors": "Nikhil Kumar Tomar, Debesh Jha, Michael A. Riegler, H{\\aa}vard D.\n  Johansen, Dag Johansen, Jens Rittscher, P{\\aa}l Halvorsen, and Sharib Ali", "title": "FANet: A Feedback Attention Network for Improved Biomedical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increase in available large clinical and experimental datasets,\nthere has been substantial amount of work being done on addressing the\nchallenges in the area of biomedical image analysis. Image segmentation, which\nis crucial for any quantitative analysis, has especially attracted attention.\nRecent hardware advancement has led to the success of deep learning approaches.\nHowever, although deep learning models are being trained on large datasets,\nexisting methods do not use the information from different learning epochs\neffectively. In this work, we leverage the information of each training epoch\nto prune the prediction maps of the subsequent epochs. We propose a novel\narchitecture called feedback attention network (FANet) that unifies the\nprevious epoch mask with the feature map of the current training epoch. The\nprevious epoch mask is then used to provide a hard attention to the learnt\nfeature maps at different convolutional layers. The network also allows to\nrectify the predictions in an iterative fashion during the test time. We show\nthat our proposed feedback attention model provides a substantial improvement\non most segmentation metrics tested on seven publicly available biomedical\nimaging datasets demonstrating the effectiveness of the proposed FANet.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:34:20 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tomar", "Nikhil Kumar", ""], ["Jha", "Debesh", ""], ["Riegler", "Michael A.", ""], ["Johansen", "H\u00e5vard D.", ""], ["Johansen", "Dag", ""], ["Rittscher", "Jens", ""], ["Halvorsen", "P\u00e5l", ""], ["Ali", "Sharib", ""]]}, {"id": "2103.17239", "submitter": "Hugo Touvron", "authors": "Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve,\n  Herv\\'e J\\'egou", "title": "Going deeper with Image Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have been recently adapted for large scale image classification,\nachieving high scores shaking up the long supremacy of convolutional neural\nnetworks. However the optimization of image transformers has been little\nstudied so far. In this work, we build and optimize deeper transformer networks\nfor image classification. In particular, we investigate the interplay of\narchitecture and optimization of such dedicated transformers. We make two\ntransformers architecture changes that significantly improve the accuracy of\ndeep transformers. This leads us to produce models whose performance does not\nsaturate early with more depth, for instance we obtain 86.5% top-1 accuracy on\nImagenet when training with no external data, we thus attain the current SOTA\nwith less FLOPs and parameters. Moreover, our best model establishes the new\nstate of the art on Imagenet with Reassessed labels and Imagenet-V2 / match\nfrequency, in the setting with no additional training data. We share our code\nand models.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:37:32 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 08:08:39 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Touvron", "Hugo", ""], ["Cord", "Matthieu", ""], ["Sablayrolles", "Alexandre", ""], ["Synnaeve", "Gabriel", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "2103.17242", "submitter": "Muhammad Waseem Ashraf", "authors": "Muhammad Waseem Ashraf, Waqas Sultani, Mubarak Shah", "title": "Dogfight: Detecting Drones from Drones Videos", "comments": "10 pages, 10 figures, Accepted for CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As airborne vehicles are becoming more autonomous and ubiquitous, it has\nbecome vital to develop the capability to detect the objects in their\nsurroundings. This paper attempts to address the problem of drones detection\nfrom other flying drones. The erratic movement of the source and target drones,\nsmall size, arbitrary shape, large intensity variations, and occlusion make\nthis problem quite challenging. In this scenario, region-proposal based methods\nare not able to capture sufficient discriminative foreground-background\ninformation. Also, due to the extremely small size and complex motion of the\nsource and target drones, feature aggregation based methods are unable to\nperform well. To handle this, instead of using region-proposal based methods,\nwe propose to use a two-stage segmentation-based approach employing\nspatio-temporal attention cues. During the first stage, given the overlapping\nframe regions, detailed contextual information is captured over convolution\nfeature maps using pyramid pooling. After that pixel and channel-wise attention\nis enforced on the feature maps to ensure accurate drone localization. In the\nsecond stage, first stage detections are verified and new probable drone\nlocations are explored. To discover new drone locations, motion boundaries are\nused. This is followed by tracking candidate drone detections for a few frames,\ncuboid formation, extraction of the 3D convolution feature map, and drones\ndetection within each cuboid. The proposed approach is evaluated on two\npublicly available drone detection datasets and outperforms several competitive\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:43:31 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 18:19:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ashraf", "Muhammad Waseem", ""], ["Sultani", "Waqas", ""], ["Shah", "Mubarak", ""]]}, {"id": "2103.17249", "submitter": "Dani Lischinski", "authors": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani\n  Lischinski", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "comments": "18 pages, 24 figures, code and video may be found here:\n  https://github.com/orpatashnik/StyleCLIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by the ability of StyleGAN to generate highly realistic images in a\nvariety of domains, much recent work has focused on understanding how to use\nthe latent spaces of StyleGAN to manipulate generated and real images. However,\ndiscovering semantically meaningful latent manipulations typically involves\npainstaking human examination of the many degrees of freedom, or an annotated\ncollection of images for each desired manipulation. In this work, we explore\nleveraging the power of recently introduced Contrastive Language-Image\nPre-training (CLIP) models in order to develop a text-based interface for\nStyleGAN image manipulation that does not require such manual effort. We first\nintroduce an optimization scheme that utilizes a CLIP-based loss to modify an\ninput latent vector in response to a user-provided text prompt. Next, we\ndescribe a latent mapper that infers a text-guided latent manipulation step for\na given input image, allowing faster and more stable text-based manipulation.\nFinally, we present a method for mapping a text prompts to input-agnostic\ndirections in StyleGAN's style space, enabling interactive text-driven image\nmanipulation. Extensive results and comparisons demonstrate the effectiveness\nof our approaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:51:25 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Patashnik", "Or", ""], ["Wu", "Zongze", ""], ["Shechtman", "Eli", ""], ["Cohen-Or", "Daniel", ""], ["Lischinski", "Dani", ""]]}, {"id": "2103.17260", "submitter": "Quoc-Huy Tran", "authors": "Sanjay Haresh and Sateesh Kumar and Huseyin Coskun and Shahram Najam\n  Syed and Andrey Konin and Muhammad Zeeshan Zia and Quoc-Huy Tran", "title": "Learning by Aligning Videos in Time", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised approach for learning video representations\nusing temporal video alignment as a pretext task, while exploiting both\nframe-level and video-level information. We leverage a novel combination of\ntemporal alignment loss and temporal regularization terms, which can be used as\nsupervision signals for training an encoder network. Specifically, the temporal\nalignment loss (i.e., Soft-DTW) aims for the minimum cost for temporally\naligning videos in the embedding space. However, optimizing solely for this\nterm leads to trivial solutions, particularly, one where all frames get mapped\nto a small cluster in the embedding space. To overcome this problem, we propose\na temporal regularization term (i.e., Contrastive-IDM) which encourages\ndifferent frames to be mapped to different points in the embedding space.\nExtensive evaluations on various tasks, including action phase classification,\naction phase progression, and fine-grained frame retrieval, on three datasets,\nnamely Pouring, Penn Action, and IKEA ASM, show superior performance of our\napproach over state-of-the-art methods for self-supervised representation\nlearning from videos. In addition, our method provides significant performance\ngain where labeled data is lacking.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:55:52 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Haresh", "Sanjay", ""], ["Kumar", "Sateesh", ""], ["Coskun", "Huseyin", ""], ["Syed", "Shahram Najam", ""], ["Konin", "Andrey", ""], ["Zia", "Muhammad Zeeshan", ""], ["Tran", "Quoc-Huy", ""]]}, {"id": "2103.17261", "submitter": "Aayush Bansal", "authors": "Kevin Wang and Deva Ramanan and Aayush Bansal", "title": "Video Exploration via Video-Specific Autoencoders", "comments": "Project Page: https://www.cs.cmu.edu/~aayushb/Video-ViSA/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present simple video-specific autoencoders that enables human-controllable\nvideo exploration. This includes a wide variety of analytic tasks such as (but\nnot limited to) spatial and temporal super-resolution, spatial and temporal\nediting, object removal, video textures, average video exploration, and\ncorrespondence estimation within and across videos. Prior work has\nindependently looked at each of these problems and proposed different\nformulations. In this work, we observe that a simple autoencoder trained (from\nscratch) on multiple frames of a specific video enables one to perform a large\nvariety of video processing and editing tasks. Our tasks are enabled by two key\nobservations: (1) latent codes learned by the autoencoder capture spatial and\ntemporal properties of that video and (2) autoencoders can project\nout-of-sample inputs onto the video-specific manifold. For e.g. (1)\ninterpolating latent codes enables temporal super-resolution and\nuser-controllable video textures; (2) manifold reprojection enables spatial\nsuper-resolution, object removal, and denoising without training for any of the\ntasks. Importantly, a two-dimensional visualization of latent codes via\nprincipal component analysis acts as a tool for users to both visualize and\nintuitively control video edits. Finally, we quantitatively contrast our\napproach with the prior art and found that without any supervision and\ntask-specific knowledge, our approach can perform comparably to supervised\napproaches specifically trained for a task.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:56:13 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Kevin", ""], ["Ramanan", "Deva", ""], ["Bansal", "Aayush", ""]]}, {"id": "2103.17263", "submitter": "Jiarui Xu", "authors": "Jiarui Xu, Xiaolong Wang", "title": "Rethinking Self-supervised Correspondence Learning: A Video Frame-level\n  Similarity Perspective", "comments": "ICCV 2021 (oral). Project Page: https://jerryxu.net/VFS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a good representation for space-time correspondence is the key for\nvarious computer vision tasks, including tracking object bounding boxes and\nperforming video object pixel segmentation. To learn generalizable\nrepresentation for correspondence in large-scale, a variety of self-supervised\npretext tasks are proposed to explicitly perform object-level or patch-level\nsimilarity learning. Instead of following the previous literature, we propose\nto learn correspondence using Video Frame-level Similarity (VFS) learning, i.e,\nsimply learning from comparing video frames. Our work is inspired by the recent\nsuccess in image-level contrastive learning and similarity learning for visual\nrecognition. Our hypothesis is that if the representation is good for\nrecognition, it requires the convolutional features to find correspondence\nbetween similar objects or parts. Our experiments show surprising results that\nVFS surpasses state-of-the-art self-supervised approaches for both OTB visual\nobject tracking and DAVIS video object segmentation. We perform detailed\nanalysis on what matters in VFS and reveals new properties on image and frame\nlevel similarity learning. Project page is available at https://jerryxu.net/VFS\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:56:35 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 18:15:49 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 17:30:52 GMT"}, {"version": "v4", "created": "Sat, 24 Jul 2021 14:52:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xu", "Jiarui", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2103.17265", "submitter": "Vladimir Guzov", "authors": "Vladimir Guzov, Aymen Mir, Torsten Sattler, Gerard Pons-Moll", "title": "Human POSEitioning System (HPS): 3D Human Pose Estimation and\n  Self-localization in Large Scenes from Body-Mounted Sensors", "comments": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce (HPS) Human POSEitioning System, a method to recover the full 3D\npose of a human registered with a 3D scan of the surrounding environment using\nwearable sensors. Using IMUs attached at the body limbs and a head mounted\ncamera looking outwards, HPS fuses camera based self-localization with\nIMU-based human body tracking. The former provides drift-free but noisy\nposition and orientation estimates while the latter is accurate in the\nshort-term but subject to drift over longer periods of time. We show that our\noptimization-based integration exploits the benefits of the two, resulting in\npose accuracy free of drift. Furthermore, we integrate 3D scene constraints\ninto our optimization, such as foot contact with the ground, resulting in\nphysically plausible motion. HPS complements more common third-person-based 3D\npose estimation methods. It allows capturing larger recording volumes and\nlonger periods of motion, and could be used for VR/AR applications where humans\ninteract with the scene without requiring direct line of sight with an external\ncamera, or to train agents that navigate and interact with the environment\nbased on first-person visual input, like real humans. With HPS, we recorded a\ndataset of humans interacting with large 3D scenes (300-1000 sq.m) consisting\nof 7 subjects and more than 3 hours of diverse motion. The dataset, code and\nvideo will be available on the project page:\nhttp://virtualhumans.mpi-inf.mpg.de/hps/ .\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:58:31 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Guzov", "Vladimir", ""], ["Mir", "Aymen", ""], ["Sattler", "Torsten", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2103.17266", "submitter": "Nikolaos Sarafianos", "authors": "Bindita Chaudhuri, Nikolaos Sarafianos, Linda Shapiro, Tony Tung", "title": "Semi-supervised Synthesis of High-Resolution Editable Textures for 3D\n  Humans", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel approach to generate diverse high fidelity texture maps\nfor 3D human meshes in a semi-supervised setup. Given a segmentation mask\ndefining the layout of the semantic regions in the texture map, our network\ngenerates high-resolution textures with a variety of styles, that are then used\nfor rendering purposes. To accomplish this task, we propose a Region-adaptive\nAdversarial Variational AutoEncoder (ReAVAE) that learns the probability\ndistribution of the style of each region individually so that the style of the\ngenerated texture can be controlled by sampling from the region-specific\ndistributions. In addition, we introduce a data generation technique to augment\nour training set with data lifted from single-view RGB inputs. Our training\nstrategy allows the mixing of reference image styles with arbitrary styles for\ndifferent regions, a property which can be valuable for virtual try-on AR/VR\napplications. Experimental results show that our method synthesizes better\ntexture maps compared to prior work while enabling independent layout and style\ncontrollability.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:58:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chaudhuri", "Bindita", ""], ["Sarafianos", "Nikolaos", ""], ["Shapiro", "Linda", ""], ["Tung", "Tony", ""]]}, {"id": "2103.17267", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "Bit-Mixer: Mixed-precision networks with runtime bit-width selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-precision networks allow for a variable bit-width quantization for\nevery layer in the network. A major limitation of existing work is that the\nbit-width for each layer must be predefined during training time. This allows\nlittle flexibility if the characteristics of the device on which the network is\ndeployed change during runtime. In this work, we propose Bit-Mixer, the very\nfirst method to train a meta-quantized network where during test time any layer\ncan change its bid-width without affecting at all the overall network's ability\nfor highly accurate inference. To this end, we make 2 key contributions: (a)\nTransitional Batch-Norms, and (b) a 3-stage optimization process which is shown\ncapable of training such a network. We show that our method can result in mixed\nprecision networks that exhibit the desirable flexibility properties for\non-device deployment without compromising accuracy. Code will be made\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:58:47 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2103.17269", "submitter": "Michael Niemeyer", "authors": "Michael Niemeyer, Andreas Geiger", "title": "CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous progress in deep generative models has led to photorealistic image\nsynthesis. While achieving compelling results, most approaches operate in the\ntwo-dimensional image domain, ignoring the three-dimensional nature of our\nworld. Several recent works therefore propose generative models which are\n3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to\nthe image plane. This leads to impressive 3D consistency, but incorporating\nsuch a bias comes at a price: the camera needs to be modeled as well. Current\napproaches assume fixed intrinsics and a predefined prior over camera pose\nranges. As a result, parameter tuning is typically required for real-world\ndata, and results degrade if the data distribution is not matched. Our key\nhypothesis is that learning a camera generator jointly with the image generator\nleads to a more principled approach to 3D-aware image synthesis. Further, we\npropose to decompose the scene into a background and foreground model, leading\nto more efficient and disentangled scene representations. While training from\nraw, unposed image collections, we learn a 3D- and camera-aware generative\nmodel which faithfully recovers not only the image but also the camera data\ndistribution. At test time, our model generates images with explicit control\nover the camera as well as the shape and appearance of the scene.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:59:24 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Niemeyer", "Michael", ""], ["Geiger", "Andreas", ""]]}, {"id": "2103.17271", "submitter": "Huaizu Jiang", "authors": "Huaizu Jiang, Erik Learned-Miller", "title": "DCVNet: Dilated Cost Volume Networks for Fast Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cost volume, capturing the similarity of possible correspondences across\ntwo input images, is a key ingredient in state-of-the-art optical flow\napproaches. When sampling for correspondences to build the cost volume, a large\nneighborhood radius is required to deal with large displacements, introducing a\nsignificant computational burden. To address this, a sequential strategy is\nusually adopted, where correspondence sampling in a local neighborhood with a\nsmall radius suffices. However, such sequential approaches, instantiated by\neither a pyramid structure over a deep neural network's feature hierarchy or by\na recurrent neural network, are slow due to the inherent need for sequential\nprocessing of cost volumes. In this paper, we propose dilated cost volumes to\ncapture small and large displacements simultaneously, allowing optical flow\nestimation without the need for the sequential estimation strategy. To process\nthe cost volume to get pixel-wise optical flow, existing approaches employ 2D\nor separable 4D convolutions, which we show either suffer from high GPU memory\nconsumption, inferior accuracy, or large model size. Therefore, we propose\nusing 3D convolutions for cost volume filtering to address these issues. By\ncombining the dilated cost volumes and 3D convolutions, our proposed model\nDCVNet not only exhibits real-time inference (71 fps on a mid-end 1080ti GPU)\nbut is also compact and obtains comparable accuracy to existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:59:31 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Jiang", "Huaizu", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "2103.17272", "submitter": "David Montero", "authors": "David Montero, Naiara Aginako, Basilio Sierra and Marcos Nieto", "title": "Efficient Large-Scale Face Clustering Using an Online Mixture of\n  Gaussians", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we address the problem of large-scale online face clustering:\ngiven a continuous stream of unknown faces, create a database grouping the\nincoming faces by their identity. The database must be updated every time a new\nface arrives. In addition, the solution must be efficient, accurate and\nscalable. For this purpose, we present an online gaussian mixture-based\nclustering method (OGMC). The key idea of this method is the proposal that an\nidentity can be represented by more than just one distribution or cluster.\nUsing feature vectors (f-vectors) extracted from the incoming faces, OGMC\ngenerates clusters that may be connected to others depending on their proximity\nand their robustness. Every time a cluster is updated with a new sample, its\nconnections are also updated. With this approach, we reduce the dependency of\nthe clustering process on the order and the size of the incoming data and we\nare able to deal with complex data distributions. Experimental results show\nthat the proposed approach outperforms state-of-the-art clustering methods on\nlarge-scale face clustering benchmarks not only in accuracy, but also in\nefficiency and scalability.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:59:38 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Montero", "David", ""], ["Aginako", "Naiara", ""], ["Sierra", "Basilio", ""], ["Nieto", "Marcos", ""]]}, {"id": "2103.17273", "submitter": "Richard Higgins", "authors": "Richard E.L. Higgins, David F. Fouhey, Dichang Zhang, Spiro K.\n  Antiochos, Graham Barnes, Todd Hoeksema, KD Leka, Yang Liu Peter W. Schuck,\n  Tamas I. Gombosi", "title": "Fast and Accurate Emulation of the SDO/HMI Stokes Inversion with\n  Uncertainty Quantification", "comments": null, "journal-ref": null, "doi": "10.3847/1538-4357/abd7fe", "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Helioseismic and Magnetic Imager (HMI) onboard NASA's Solar Dynamics\nObservatory (SDO) produces estimates of the photospheric magnetic field which\nare a critical input to many space weather modelling and forecasting systems.\nThe magnetogram products produced by HMI and its analysis pipeline are the\nresult of a per-pixel optimization that estimates solar atmospheric parameters\nand minimizes disagreement between a synthesized and observed Stokes vector. In\nthis paper, we introduce a deep learning-based approach that can emulate the\nexisting HMI pipeline results two orders of magnitude faster than the current\npipeline algorithms. Our system is a U-Net trained on input Stokes vectors and\ntheir accompanying optimization-based VFISV inversions. We demonstrate that our\nsystem, once trained, can produce high-fidelity estimates of the magnetic field\nand kinematic and thermodynamic parameters while also producing meaningful\nconfidence intervals. We additionally show that despite penalizing only\nper-pixel loss terms, our system is able to faithfully reproduce known\nsystematic oscillations in full-disk statistics produced by the pipeline. This\nemulation system could serve as an initialization for the full Stokes inversion\nor as an ultra-fast proxy inversion. This work is part of the NASA Heliophysics\nDRIVE Science Center (SOLSTICE) at the University of Michigan, under grant NASA\n80NSSC20K0600E, and has been open sourced.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:59:40 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Higgins", "Richard E. L.", ""], ["Fouhey", "David F.", ""], ["Zhang", "Dichang", ""], ["Antiochos", "Spiro K.", ""], ["Barnes", "Graham", ""], ["Hoeksema", "Todd", ""], ["Leka", "KD", ""], ["Schuck", "Yang Liu Peter W.", ""], ["Gombosi", "Tamas I.", ""]]}]