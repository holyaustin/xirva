[{"id": "1211.0071", "submitter": "Leonid A. Levin", "authors": "Leonid A. Levin", "title": "Randomness and Non-determinism", "comments": "1992 talk at ASL meeting", "journal-ref": "Journal of Symbolic Logic, 58/3:1102-1103, 1993", "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponentiation makes the difference between the bit-size of this line and the\nnumber (<< 2^{300}) of particles in the known Universe. The expulsion of\nexponential time algorithms from Computer Theory in the 60's broke its\numbilical cord from Mathematical Logic. It created a deep gap between\ndeterministic computation and -- formerly its unremarkable tools -- randomness\nand non-determinism. Little did we learn in the past decades about the power of\neither of these two basic \"freedoms\" of computation, but some vague pattern is\nemerging in relationships between them. The pattern of similar techniques\ninstrumental for quite different results in this area seems even more\ninteresting. Ideas like multilinear and low-degree multivariate polynomials,\nFourier transformation over low-periodic groups seem very illuminating. The\ntalk surveyed some recent results. One of them, given in a stronger form than\npreviously published, is described below.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 01:09:48 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Levin", "Leonid A.", ""]]}, {"id": "1211.0086", "submitter": "Yaser Sadra", "authors": "Sodeif Ahadpour, Mahdiyeh Majidpour, Yaser Sadra", "title": "Public key Steganography Using Discrete Cross-Coupled Chaotic Maps", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By cross-coupling two logistic maps a novel method is proposed for the public\nkey steganography in JPEG image. Chaotic maps entail high complexity in the\nused algorithm for embedding secret data in a medium. In this paper, discrete\ncross- coupled chaotic maps are used to specifying the location of the\ndifferent parts of the secret data in the image. Modifying JPEG format during\ncompressing and decompressing, and also using public key enhanced difficulty of\nthe algorithm. Simulation results show that in addition to excessive capacity,\nthis method has high robustness and resistance against hackers and can be\napplicable in secret communication. Also the PSNR value is high compared to the\nother works.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 04:09:10 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Ahadpour", "Sodeif", ""], ["Majidpour", "Mahdiyeh", ""], ["Sadra", "Yaser", ""]]}, {"id": "1211.0090", "submitter": "Yaser Sadra", "authors": "Sodeif Ahadpour, Yaser Sadra", "title": "A Chaos-based Image Encryption Scheme using Chaotic Coupled Map Lattices", "comments": "4 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1201.1449, arXiv:1112.3791", "journal-ref": "International Journal of Computer Applications 49(2):15-18, July\n  2012. Published by Foundation of Computer Science, New York, USA", "doi": "10.5120/7599-0311", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, information security essential in various arenas like\ninternet communication, multimedia systems, medical imaging, tele-medicine and\nmilitary communication. However, most of them faced with some problems such as\nthe lack of robustness and security. In this letter, after reviewing the main\npoints of the chaotic trigonometric maps and the coupled map lattices, we\nintroduce the scheme of chaos-based image encryption based on coupled map lat\ntices. The scheme decreases periodic effect of the ergodic dynamical systems in\nthe chaos-based image encryption. To evaluate the security of the encrypted\nimage of this scheme, the key space analysis, the correlation of two adjacent\npixels and differential attack were performed. This scheme tries to improve the\nproblem of failure of encryption such as small key space and level of security.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 04:30:15 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Ahadpour", "Sodeif", ""], ["Sadra", "Yaser", ""]]}, {"id": "1211.0377", "submitter": "Dr. Rajesh Kumar  Tiwari", "authors": "Rajesh Kumar Tiwari and Gadadhar Sahoo", "title": "Some New Methodologies for Image Hiding using Steganographic Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security and memory management are the major demands for electronics devices\nlike ipods, cell phones, pmps, iphones and digital cameras. In this paper, we\nhave suggested a high level of security mechanism by considering the concept of\nsteganography along with the principle of cryptography. Four different methods\nthat can save a considerable amount of memory space have been discussed. Based\non these methods, we have constructed secured stego image creator and secured\nmulti image viewer in Microsoft platform so as to provide high level of\nsecurity and using less memory space for storage of image files in the above\nsaid electronic devices\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 06:36:12 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Tiwari", "Rajesh Kumar", ""], ["Sahoo", "Gadadhar", ""]]}, {"id": "1211.0651", "submitter": "Xin Li", "authors": "Xin Li", "title": "Non-Malleable Condensers for Arbitrary Min-Entropy, and Almost Optimal\n  Protocols for Privacy Amplification", "comments": "arXiv admin note: substantial text overlap with arXiv:1112.1045", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the problem of privacy amplification with an active adversary has\nreceived a lot of attention. Given a shared n-bit weak random source X with\nmin-entropy k and a security parameter s, the main goal is to construct an\nexplicit 2-round privacy amplification protocol that achieves entropy loss\nO(s). Dodis and Wichs \\cite{DW09} showed that optimal protocols can be achieved\nby constructing explicit non-malleable extractors. However, the best known\nexplicit non-malleable extractor only achieves k=0.49n \\cite{Li12b} and\nevidence in \\cite{Li12b} suggests that constructing explicit non-malleable\nextractors for smaller min-entropy may be hard. In an alternative approach, Li\n\\cite{Li12} introduced the notion of a non-malleable condenser and showed that\nexplicit non-malleable condensers also give optimal privacy amplification\nprotocols.\n  In this paper, we give the first construction of non-malleable condensers for\narbitrary min-entropy. Using our construction, we obtain a 2-round privacy\namplification protocol with optimal entropy loss for security parameter up to\ns=\\Omega(\\sqrt{k}). This is the first protocol that simultaneously achieves\noptimal round complexity and optimal entropy loss for arbitrary min-entropy k.\nWe also generalize this result to obtain a protocol that runs in O(s/\\sqrt{k})\nrounds with optimal entropy loss, for security parameter up to s=\\Omega(k).\nThis significantly improves the protocol in \\cite{ckor}. Finally, we give a\nbetter non-malleable condenser for linear min-entropy, and in this case obtain\na 2-round protocol with optimal entropy loss for security parameter up to\ns=\\Omega(k), which improves the entropy loss and communication complexity of\nthe protocol in \\cite{Li12b}.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 23:50:09 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Li", "Xin", ""]]}, {"id": "1211.0737", "submitter": "Shihao Yan", "authors": "Shihao Yan, Robert Malaney, Ido Nevat, Gareth W. Peters", "title": "Optimal Information-Theoretic Wireless Location Verification", "comments": "Corrected typos and introduced new threat models", "journal-ref": null, "doi": "10.1109/TVT.2014.2302022", "report-no": null, "categories": "cs.IT cs.CR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new Location Verification System (LVS) focussed on network-based\nIntelligent Transport Systems and vehicular ad hoc networks. The algorithm we\ndevelop is based on an information-theoretic framework which uses the received\nsignal strength (RSS) from a network of base-stations and the claimed position.\nBased on this information we derive the optimal decision regarding the\nverification of the user's location. Our algorithm is optimal in the sense of\nmaximizing the mutual information between its input and output data. Our\napproach is based on the practical scenario in which a non-colluding malicious\nuser some distance from a highway optimally boosts his transmit power in an\nattempt to fool the LVS that he is on the highway. We develop a practical\nthreat model for this attack scenario, and investigate in detail the\nperformance of the LVS in terms of its input/output mutual information. We show\nhow our LVS decision rule can be implemented straightforwardly with a\nperformance that delivers near-optimality under realistic threat conditions,\nwith information-theoretic optimality approached as the malicious user moves\nfurther from the highway. The practical advantages our new\ninformation-theoretic scheme delivers relative to more traditional Bayesian\nverification frameworks are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 00:11:44 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 03:49:13 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Yan", "Shihao", ""], ["Malaney", "Robert", ""], ["Nevat", "Ido", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1211.0963", "submitter": "Mohammad Allahbakhsh", "authors": "Mohammad Allahbakhsh, Aleksandar Ignjatovic, Boualem Benatallah,\n  Seyed-Mehdi-Reza Beheshti, Norman Foo, Elisa Bertino", "title": "Detecting, Representing and Querying Collusion in Online Rating Systems", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "UNSW-CSE-TR-201220", "categories": "cs.CR cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online rating systems are subject to malicious behaviors mainly by posting\nunfair rating scores. Users may try to individually or collaboratively promote\nor demote a product. Collaborating unfair rating 'collusion' is more damaging\nthan individual unfair rating. Although collusion detection in general has been\nwidely studied, identifying collusion groups in online rating systems is less\nstudied and needs more investigation. In this paper, we study impact of\ncollusion in online rating systems and asses their susceptibility to collusion\nattacks. The proposed model uses a frequent itemset mining algorithm to detect\ncandidate collusion groups. Then, several indicators are used for identifying\ncollusion groups and for estimating how damaging such colluding groups might\nbe. Also, we propose an algorithm for finding possible collusive subgroup\ninside larger groups which are not identified as collusive. The model has been\nimplemented and we present results of experimental evaluation of our\nmethodology.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 07:50:05 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Allahbakhsh", "Mohammad", ""], ["Ignjatovic", "Aleksandar", ""], ["Benatallah", "Boualem", ""], ["Beheshti", "Seyed-Mehdi-Reza", ""], ["Foo", "Norman", ""], ["Bertino", "Elisa", ""]]}, {"id": "1211.1080", "submitter": "Douglas Stebila", "authors": "Anne Broadbent and Gus Gutoski and Douglas Stebila", "title": "Quantum one-time programs", "comments": "62 pages, 5 figures", "journal-ref": "Advances in Cryptology -- Proc. CRYPTO 2013, LNCS vol. 8043, pp.\n  344-360, Springer", "doi": "10.1007/978-3-642-40084-1_20", "report-no": null, "categories": "quant-ph cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-time programs are modelled after a black box that allows a single\nevaluation of a function, and then self-destructs. Because software can, in\nprinciple, be copied, general one-time programs exists only in the hardware\ntoken model: it has been shown that any function admits a one-time program as\nlong as we assume access to physical devices called one-time memories. Quantum\ninformation, with its well-known property of no-cloning, would, at first\nglance, prevent the basic copying attack for classical programs. We show that\nthis intuition is false: one-time programs for both classical and quantum maps,\nbased solely on quantum information, do not exist, even with computational\nassumptions. We complement this strong impossibility proof by an equally strong\npossibility result: assuming the same basic one-time memories as used for\nclassical one-time programs, we show that every quantum map has a quantum\none-time program that is secure in the universal composability framework. Our\nconstruction relies on a new, simpler quantum authentication scheme and\ncorresponding mechanism for computing on authenticated data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 00:01:27 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Broadbent", "Anne", ""], ["Gutoski", "Gus", ""], ["Stebila", "Douglas", ""]]}, {"id": "1211.1125", "submitter": "Rotem Arnon-Friedman", "authors": "Rotem Arnon-Friedman and Amnon Ta-Shma", "title": "Limits of privacy amplification against non-signalling memory attacks", "comments": "12 pages, 5 figures", "journal-ref": "Phys. Rev. A 86, 062333 (2012)", "doi": "10.1103/PhysRevA.86.062333", "report-no": null, "categories": "quant-ph cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of privacy amplification, in which Alice holds some partially secret\ninformation with respect to an adversary Eve and wishes to distill it until it\nis completely secret, is known to be solvable almost optimally both in the\nclassical and quantum world. Unfortunately, when considering an adversary who\nis only limited by non-signalling constraints such a statement cannot be made\nin general. We here prove that under the natural assumptions of time-ordered\nnon-signalling system, which allow past subsystems to signal future subsystems\n(using the device's memory for example), super-polynomial privacy amplification\nby any hashing is impossible. This is in great relevance when considering\npractical device independent key distribution protocols which assume a\nsuper-quantum adversary.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 07:11:58 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2012 15:57:02 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Arnon-Friedman", "Rotem", ""], ["Ta-Shma", "Amnon", ""]]}, {"id": "1211.1158", "submitter": "Ayman Bahaa-Eldin", "authors": "Hany N. Gabra, Ayman M. Bahaa-Eldin, Hoda K. Mohamed", "title": "Data Mining Based Technique for IDS Alerts Classification", "comments": null, "journal-ref": "International Journal of Electronic Commerce Studies Vol.5, No.1 ,\n  pp.1-6, 2014", "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrusion detection systems (IDSs) have become a widely used measure for\nsecurity systems. The main problem for those systems results is the irrelevant\nalerts on those results. We will propose a data mining based method for\nclassification to distinguish serious alerts and irrelevant one with a\nperformance of 99.9% which is better in comparison with the other recent data\nmining methods that have reached the performance of 97%. A ranked alerts list\nalso created according to alerts importance to minimize human interventions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 09:29:18 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2013 08:36:37 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Gabra", "Hany N.", ""], ["Bahaa-Eldin", "Ayman M.", ""], ["Mohamed", "Hoda K.", ""]]}, {"id": "1211.1457", "submitter": "Pallavali Radha Krishna Reddy", "authors": "G. Vidhisha, C. Surekha, S. Sanjeeva Rayudu, U. Seshadri", "title": "Preserving privacy for secure and outsourcing for Linear Programming in\n  cloud computing", "comments": "8, 2012 Vol 1 Issue 2 November 2278-9200, http://www.cschronicle.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Cloud computing is the long dreamed vision of computing as a utility, where\nusers can remotely store their data into the cloud so as to enjoy the on-demand\nhigh quality applications and services from a shared pool of configurable\ncomputing resources. By data outsourcing, users can be relieved from the burden\nof local data storage and maintenance. we utilize the public key based\nhomomorphism authenticator and uniquely integrate it with random mask technique\nto achieve a privacy-preserving public auditing system for cloud data storage\nsecurity while keeping all above requirements in mind. To support efficient\nhandling of multiple auditing tasks, we further explore the technique of\nbilinear aggregate signature to extend our main result into a multi-user\nsetting, where TPA can perform multiple auditing tasks simultaneously along\nwith investigates secure outsourcing of widely applicable linear programming\n(LP) computations. In order to achieve practical efficiency, our mechanism\ndesign explicitly decomposes the LP computation outsourcing into public LP\nsolvers running on the cloud and private LP parameters owned by the customer\nExtensive security and performance analysis shows the proposed schemes are\nprovably secure and highly efficient.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 05:39:46 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Vidhisha", "G.", ""], ["Surekha", "C.", ""], ["Rayudu", "S. Sanjeeva", ""], ["Seshadri", "U.", ""]]}, {"id": "1211.1572", "submitter": "Jarek Duda dr", "authors": "Jarek Duda", "title": "Embedding grayscale halftone pictures in QR Codes using Correction Trees", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Barcodes like QR Codes have made that encoded messages have entered our\neveryday life, what suggests to attach them a second layer of information:\ndirectly available to human receiver for informational or marketing purposes.\nWe will discuss a general problem of using codes with chosen statistical\nconstrains, for example reproducing given grayscale picture using halftone\ntechnique. If both sender and receiver know these constrains, the optimal\ncapacity can be easily approached by entropy coder. The problem is that this\ntime only the sender knows them - we will refer to these scenarios as\nconstrained coding. Kuznetsov and Tsybakov problem in which only the sender\nknows which bits are fixed can be seen as a special case, surprisingly\napproaching the same capacity as if both sides would know the constrains. We\nwill analyze Correction Trees to approach analogous capacity in the general\ncase - use weaker: statistical constrains, what allows to apply them to all\nbits. Finding satisfying coding is similar to finding the proper correction in\nerror correction problem, but instead of single ensured possibility, there are\nnow statistically expected some. While in standard steganography we hide\ninformation in the least important bits, this time we create codes resembling\ngiven picture - hide information in the freedom of realizing grayness by black\nand white pixels using halftone technique. We will also discuss combining with\nerror correction and application to rate distortion problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 15:19:23 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2012 08:59:33 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2012 08:44:38 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Duda", "Jarek", ""]]}, {"id": "1211.1654", "submitter": "Yue Wu", "authors": "Yue Wu, Sos Agaian, and Joseph P. Noonan", "title": "A New Randomness Evaluation Method with Applications to Image Shuffling\n  and Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter discusses the problem of testing the degree of randomness within\nan image, particularly for a shuffled or encrypted image. Its key contributions\nare: 1) a mathematical model of perfectly shuffled images; 2) the derivation of\nthe theoretical distribution of pixel differences; 3) a new $Z$-test based\napproach to differentiate whether or not a test image is perfectly shuffled;\nand 4) a randomized algorithm to unbiasedly evaluate the degree of randomness\nwithin a given image. Simulation results show that the proposed method is\nrobust and effective in evaluating the degree of randomness within an image,\nand may often be more suitable for image applications than commonly used\ntesting schemes designed for binary data like NIST 800-22. The developed method\nmay be also useful as a first step in determining whether or not a shuffling or\nencryption scheme is suitable for a particular cryptographic application.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 19:57:13 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Wu", "Yue", ""], ["Agaian", "Sos", ""], ["Noonan", "Joseph P.", ""]]}, {"id": "1211.1666", "submitter": "Abdoul Aziz  Ciss", "authors": "Abdoul Aziz Ciss and Djiby Sow", "title": "Pairings on Generalized Huff Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Tate pairing computation on generalized Huff curves\nproposed by Wu and Feng. In fact, we extend the results of the Tate pairing\ncomputation on the standard Huff elliptic curves done previously by Joye,\nTibouchi and Vergnaud. We show that the addition step of the Miller loop can be\nperformed in $1\\mathbf{M}+(k+15)\\mathbf{m}+2\\mathbf{c}$ and the doubling one in\n$1\\mathbf{M} + 1\\mathbf{S} + (k + 12) \\mathbf{m} + 5\\mathbf{s} + 2\\mathbf{c}$\non the generalized Huff curve.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 00:44:42 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Ciss", "Abdoul Aziz", ""], ["Sow", "Djiby", ""]]}, {"id": "1211.1736", "submitter": "Subrata Mitra", "authors": "Kanad Basu, Subrata Mitra, Srishti Mukherjee, Weixun Wang", "title": "A Novel Approach for Handling Misbehaving Nodes in Behavior-Aware Mobile\n  Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Profile-cast is a service paradigm within the communication framework of\ndelay tolerant networks (DTN). Instead of using destination addresses to\ndetermine the final destination it uses similarity-based forwarding protocol.\nWith the rise in popularity of various wireless networks, the need to make\nwireless technologies robust, resilient to attacks and failure becomes\nmandatory. One issue that remains to be addressed in behavioral networks is\nnode co-operation in forwarding packets. Nodes might behave selfishly (due to\nbandwidth preservation, energy /power constraints) or maliciously by dropping\npackets or not forwarding them to other nodes based on profile similarity. In\nboth cases the net result is degradation in the performance of the network. It\nis our goal to show that the performance of the behavioral network can be\nimproved by employing self-policing scheme that would detect node misbehavior\nand then decide how to tackle them in order to ensure node cooperation or so\nthat the overall performance does not fall below a certain threshold. For this\nvarious existing self-policing techniques which are in use in ad-hoc networks\nwill be first tried on this behavioral scenario.At various stages simulation\nwould be used to measure performances of the network under different\nconstraints, and after subjected to different techniques\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2012 00:39:23 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Basu", "Kanad", ""], ["Mitra", "Subrata", ""], ["Mukherjee", "Srishti", ""], ["Wang", "Weixun", ""]]}, {"id": "1211.1904", "submitter": "Dan Wallach", "authors": "Josh Benaloh and Mike Byrne and Philip Kortum and Neal McBurnett and\n  Olivier Pereira and Philip B. Stark and Dan S. Wallach", "title": "STAR-Vote: A Secure, Transparent, Auditable, and Reliable Voting System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In her 2011 EVT/WOTE keynote, Travis County, Texas County Clerk Dana\nDeBeauvoir described the qualities she wanted in her ideal election system to\nreplace their existing DREs. In response, in April of 2012, the authors,\nworking with DeBeauvoir and her staff, jointly architected STAR-Vote, a voting\nsystem with a DRE-style human interface and a \"belt and suspenders\" approach to\nverifiability. It provides both a paper trail and end-to-end cryptography using\nCOTS hardware. It is designed to support both ballot-level risk-limiting\naudits, and auditing by individual voters and observers. The human interface\nand process flow is based on modern usability research. This paper describes\nthe STAR-Vote architecture, which could well be the next-generation voting\nsystem for Travis County and perhaps elsewhere.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2012 17:06:33 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Benaloh", "Josh", ""], ["Byrne", "Mike", ""], ["Kortum", "Philip", ""], ["McBurnett", "Neal", ""], ["Pereira", "Olivier", ""], ["Stark", "Philip B.", ""], ["Wallach", "Dan S.", ""]]}, {"id": "1211.2087", "submitter": "Arindam Sarkar", "authors": "Arindam Sarkar and J. K. Mandal", "title": "Secured Wireless Communication using Fuzzy Logic based High Speed\n  Public-Key Cryptography (FLHSPKC)", "comments": "9 pages", "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 3, No. 10, 2012, 137-145", "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper secured wireless communication using fuzzy logic based high\nspeed public key cryptography (FLHSPKC) has been proposed by satisfying the\nmajor issues likes computational safety, power management and restricted usage\nof memory in wireless communication. Wireless Sensor Network (WSN) has several\nmajor constraints likes inadequate source of energy, restricted computational\npotentiality and limited memory. Though conventional Elliptic Curve\nCryptography (ECC) which is a sort of public key cryptography used in wireless\ncommunication provides equivalent level of security like other existing public\nkey algorithm using smaller parameters than other but this traditional ECC does\nnot take care of all these major limitations in WSN. In conventional ECC\nconsider Elliptic curve point p, an arbitrary integer k and modulus m, ECC\ncarry out scalar multiplication kP mod m, which takes about 80% of key\ncomputation time on WSN. In this paper proposed FLHSPKC scheme provides some\nnovel strategy including novel soft computing based strategy to speed up scalar\nmultiplication in conventional ECC and which in turn takes shorter\ncomputational time and also satisfies power consumption restraint, limited\nusage of memory without hampering the security level. Performance analysis of\nthe different strategies under FLHSPKC scheme and comparison study with\nexisting conventional ECC methods has been done.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 09:57:16 GMT"}], "update_date": "2012-11-12", "authors_parsed": [["Sarkar", "Arindam", ""], ["Mandal", "J. K.", ""]]}, {"id": "1211.2338", "submitter": "Roohallah Rastaghi", "authors": "Roohallah Rastaghi", "title": "An Efficient Encryption Algorithm for P2P Networks Robust Against\n  Man-in-the-Middle Adversary", "comments": null, "journal-ref": "IJCSI, International Journal of Computer Science Issues, Volume 9,\n  Issue 6, November 2012", "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer-to-peer (P2P) networks have become popular as a new paradigm for\ninformation exchange and are being used in many applications such as file\nsharing, distributed computing, video conference, VoIP, radio and TV\nbroadcasting. This popularity comes with security implications and\nvulnerabilities that need to be addressed. Especially duo to direct\ncommunication between two end nodes in P2P networks, these networks are\npotentially vulnerable to \"Man-in-the-Middle\" attacks. In this paper, we\npropose a new public-key cryptosystem for P2P networks that is robust against\nMan-in-the-Middle adversary. This cryptosystem is based on RSA and knapsack\nproblems. Our precoding-based algorithm uses knapsack problem for performing\npermutation and padding random data to the message. We show that comparing to\nother proposed cryptosystems, our algorithm is more efficient and it is fully\nsecure against an active adversary.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2012 17:51:55 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2012 22:23:13 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2013 14:40:52 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Rastaghi", "Roohallah", ""]]}, {"id": "1211.2354", "submitter": "Amin Milani Fard", "authors": "Amin Milani Fard", "title": "Privacy Preserving Web Query Log Publishing: A Survey on Anonymization\n  Techniques", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Releasing Web query logs which contain valuable information for research or\nmarketing, can breach the privacy of search engine users. Therefore rendering\nquery logs to limit linking a query to an individual while preserving the data\nusefulness for analysis, is an important research problem. This survey provides\nan overview and discussion on the recent studies on this direction.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2012 21:43:40 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Fard", "Amin Milani", ""]]}, {"id": "1211.2751", "submitter": "Monisha Prabhu", "authors": "Monisha Prabhu and Subhash Kak", "title": "Random Sequences from Primitive Pythagorean Triples", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that the six classes of PPTs can be put into two groups.\nAutocorrelation and cross-correlation functions of the six classes derived from\nthe gaps between each class type have been computed. It is shown that Classes A\nand D (in which the largest term is divisible by 5) are different from the\nother four classes in their randomness properties if they are ordered by the\nlargest term. In the other two orderings each of the six random Baudhayana\nsequences has excellent randomness properties.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 19:37:51 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Prabhu", "Monisha", ""], ["Kak", "Subhash", ""]]}, {"id": "1211.2875", "submitter": "Maged  Ibrahim", "authors": "Maged hamada Ibrahim", "title": "A Novel Approach to Fully Private and Secure Auction: A Sealed Bid\n  Knapsack Auction", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an electronic auction protocol, the main participants are the seller, a\nset of trusted auctioneer(s) and the set of bidders. In this paper we consider\nthe situation where there is a seller and a set of n bidders intending to come\nto an agreement on the selling price of a certain good. Full private or\nbidder-resolved auction means that this agreement is reached without the help\nof trusted parties or auctioneers. Therefore, only the seller and the set of\nbidders are involved, the role of the auctioneers becomes obsolete in this\ncase. property.We propose a new technique for the design of a full private\nsealed-bid auction protocol.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 02:12:49 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Ibrahim", "Maged hamada", ""]]}, {"id": "1211.2946", "submitter": "Ayman Bahaa-Eldin", "authors": "Ayman M. Bahaa-ElDin, Islam Tharwat A. Halim, Hossam M. A. Fahmy", "title": "ATDSR: Trusted On-Demand Routing Protocol based on Agents for Mobile\n  Ad-hoc Networks", "comments": "Submited by error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The routing performance in Mobile Ad-hoc Networks (MANETs) relies on the\nco-operation of the individual nodes that constitute the network. The existence\nof misbehaving nodes may paralyze the routing operation in MANETs. To overcome\nthis behavior, the trustworthiness of the network nodes should be considered in\nthe route selection process combined with the hop count. The trustworthiness is\nachieved by measuring the trust value for each node in the network. In this\npaper, a new protocol based on self monitoring (agent-based) and following the\ndynamic source routing (DSR) algorithm is presented. This protocol is called\nAgent-Based Trusted Dynamic Source Routing (ATDSR) Protocol for MANETs. The\nobjective of this protocol is to manage trust information locally with minimal\noverhead in terms of extra messages and time delay. This objective is achieved\nthrough installing in each participated node in the network a multi-agent\nsystem (MAS). MAS consists of two types of agents: monitoring agent (MOA) and\nrouting agent (ROA). A new mathematical and more realistic objective model for\nmeasuring the trust value is introduced. This model is weighted by both number\nand size of routed packets to reflect the selective forwarding behavior of a\nnode. The performance evaluation via simulation shows that our protocol is\nbetter than standard and trusted DSR. The simulation is done over a variety of\nenvironmental conditions such as number of malicious nodes, host density and\nmovement rates.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 10:40:02 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2013 16:41:12 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Bahaa-ElDin", "Ayman M.", ""], ["Halim", "Islam Tharwat A.", ""], ["Fahmy", "Hossam M. A.", ""]]}, {"id": "1211.3147", "submitter": "Keke Chen", "authors": "James Powers and Keke Chen", "title": "Secure Computation of Top-K Eigenvectors for Shared Matrices in the\n  Cloud", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of sensor network, mobile computing, and web\napplications, data are now collected from many distributed sources to form big\ndatasets. Such datasets can be hosted in the cloud to achieve economical\nprocessing. However, these data might be highly sensitive requiring secure\nstorage and processing. We envision a cloud-based data storage and processing\nframework that enables users to economically and securely share and handle big\ndatasets. Under this framework, we study the matrix-based data mining\nalgorithms with a focus on the secure top-k eigenvector algorithm. Our approach\nuses an iterative processing model in which the authorized user interacts with\nthe cloud to achieve the result. In this process, both the source matrix and\nthe intermediate results keep confidential and the client-side incurs low\ncosts. The security of this approach is guaranteed by using Paillier Encryption\nand a random perturbation technique. We carefully analyze its security under a\ncloud-specific threat model. Our experimental results show that the proposed\nmethod is scalable to big matrices while requiring low client-side costs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 21:59:18 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 05:35:22 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Powers", "James", ""], ["Chen", "Keke", ""]]}, {"id": "1211.3191", "submitter": "Amir Houmansadr", "authors": "Amir Houmansadr, Wenxuan Zhou, Matthew Caesar, Nikita Borisov", "title": "SWEET: Serving the Web by Exploiting Email Tunnels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open communication over the Internet poses a serious threat to countries with\nrepressive regimes, leading them to develop and deploy censorship mechanisms\nwithin their networks. Unfortunately, existing censorship circumvention systems\ndo not provide high availability guarantees to their users, as censors can\nidentify, hence disrupt, the traffic belonging to these systems using today's\nadvanced censorship technologies. In this paper we propose SWEET, a highly\navailable censorship-resistant infrastructure. SWEET works by encapsulating a\ncensored user's traffic to a proxy server inside email messages that are\ncarried over by public email service providers, like Gmail and Yahoo Mail. As\nthe operation of SWEET is not bound to specific email providers we argue that a\ncensor will need to block all email communications in order to disrupt SWEET,\nwhich is infeasible as email constitutes an important part of today's Internet.\nThrough experiments with a prototype of our system we find that SWEET's\nperformance is sufficient for web traffic. In particular, regular websites are\ndownloaded within couple of seconds.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 03:16:24 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 03:00:59 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Houmansadr", "Amir", ""], ["Zhou", "Wenxuan", ""], ["Caesar", "Matthew", ""], ["Borisov", "Nikita", ""]]}, {"id": "1211.3502", "submitter": "Muhammad Yasir Malik", "authors": "Muhammad Yasir Malik", "title": "Efficient Group Key Management Schemes for Multicast Dynamic\n  Communication Systems", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key management in multicast dynamic groups, where users can leave or join at\ntheir ease is one of the most crucial and essential part of secure\ncommunication. Various efficient management strategies have been proposed\nduring last decade that aim to decrease encryption costs and transmission\noverheads. In this report, two different types of key management schemes are\nproposed. First proposed scheme is based on One-way function tree (OFT). The\nproposed scheme fulfills the security gaps that have been pointed out in recent\nyears. Second proposed scheme is based on logical key hierarchy (LKH). This\nproposed scheme provides better performance for, rather inflexible and\nexpensive, LKH scheme.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 06:08:58 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Malik", "Muhammad Yasir", ""]]}, {"id": "1211.3553", "submitter": "Chengqing Li", "authors": "Chengqing Li, Yuansheng Liu, Tao Xie, Michael Z.Q. Chen", "title": "Breaking a novel image encryption scheme based on improved hyperchaotic\n  sequences", "comments": "6 pages", "journal-ref": "Nonlinear Dynamics (2013) 73:2083-2089", "doi": "10.1007/s11071-013-0924-6", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a novel image encryption scheme based on improved hyperchaotic\nsequences was proposed. A pseudo-random number sequence, generated by a\nhyper-chaos system, is used to determine two involved encryption functions,\nbitwise exclusive or (XOR) operation and modulo addition. It was reported that\nthe scheme can be broken with some pairs of chosen plain-images and the\ncorresponding cipherimages. This paper re-evaluates the security of the\nencryption scheme and finds that the encryption scheme can be broken with only\none known plain-image. The performance of the known-plaintext attack, in terms\nof success probability and computation load, become even much better when two\nknown plain-images are available. In addition, security defects on\ninsensitivity of the encryption result with respect to changes of secret key\nand plain-image are also reported.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 10:04:33 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 05:58:33 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Li", "Chengqing", ""], ["Liu", "Yuansheng", ""], ["Xie", "Tao", ""], ["Chen", "Michael Z. Q.", ""]]}, {"id": "1211.3682", "submitter": "Pallavali Radha Krishna Reddy", "authors": "P. Naga Aswani, K. Chandra Shekar", "title": "Fuzzy Keyword Search over Encrypted Data using Symbol-Based\n  Trie-traverse Search Scheme in Cloud Computing", "comments": "8 pages, 2012 CSC 2278-9200 published http://www.cschronicle.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We exploit edit distance to quantify keywords similarity and develop two\nadvanced techniques on constructing fuzzy keyword sets, which achieve optimized\nstorage and representation overheads. We further propose a brand new\nsymbol-based trie-traverse searching scheme, where a multi-way tree structure\nis built up using symbols transformed from the resulted fuzzy keyword sets.\nThrough rigorous security analysis, we show that our proposed solution is\nsecure and privacy-preserving, while correctly realizing the goal of fuzzy\nkeyword search. Extensive experimental results demonstrate the efficiency of\nthe proposed solution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 05:31:35 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Aswani", "P. Naga", ""], ["Shekar", "K. Chandra", ""]]}, {"id": "1211.3700", "submitter": "Michael Clarkson", "authors": "Andrew K. Hirsch and Michael R. Clarkson", "title": "Nexus Authorization Logic (NAL): Logical Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nexus Authorization Logic (NAL) [Schneider et al. 2011] is a logic for\nreasoning about authorization in distributed systems. A revised version of NAL\nis given here, including revised syntax, a revised proof theory using localized\nhypotheses, and a new Kripke semantics. The proof theory is proved sound with\nrespect to the semantics, and that proof is formalized in Coq.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 19:22:11 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2012 03:29:01 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Hirsch", "Andrew K.", ""], ["Clarkson", "Michael R.", ""]]}, {"id": "1211.3836", "submitter": "Jie Qian", "authors": "Jie Qian and Nafees Qamar", "title": "An experimental evaluation of de-identification tools for electronic\n  health records", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust development of Electronic Health Records (EHRs) causes a\nsignificant growth in sharing EHRs for clinical research. However, such a\nsharing makes it difficult to protect patient's privacy. A number of automated\nde-identification tools have been developed to reduce the re-identification\nrisk of published data, while preserving its statistical meaning. In this\npaper, we focus on the experimental evaluation of existing automated\nde-identification tools, as applied to our EHR database, to assess which tool\nperforms better with each quasi-identifiers defined in our paper. Performance\nof each tool is analyzed wrt. two aspects: individual disclosure risk and\ninformation loss. Through this experiment, the generalization method has better\nperformance on reducing risk and lower degree of information loss than\nsuppression, which validates it as more appropriate de-identification technique\nfor EHR databases.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 09:19:51 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Qian", "Jie", ""], ["Qamar", "Nafees", ""]]}, {"id": "1211.3908", "submitter": "Zhendong Ma", "authors": "Zhendong Ma, Paul Smith, Florian Skopik", "title": "Towards a Layered Architectural View for Security Analysis in SCADA\n  Systems", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Supervisory Control and Data Acquisition (SCADA) systems support and control\nthe operation of many critical infrastructures that our society depend on, such\nas power grids. Since SCADA systems become a target for cyber attacks and the\npotential impact of a successful attack could lead to disastrous consequences\nin the physical world, ensuring the security of these systems is of vital\nimportance. A fundamental prerequisite to securing a SCADA system is a clear\nunderstanding and a consistent view of its architecture. However, because of\nthe complexity and scale of SCADA systems, this is challenging to acquire. In\nthis paper, we propose a layered architectural view for SCADA systems, which\naims at building a common ground among stakeholders and supporting the\nimplementation of security analysis. In order to manage the complexity and\nscale, we define four interrelated architectural layers, and uses the concept\nof viewpoints to focus on a subset of the system. We indicate the applicability\nof our approach in the context of SCADA system security analysis.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 14:49:17 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Ma", "Zhendong", ""], ["Smith", "Paul", ""], ["Skopik", "Florian", ""]]}, {"id": "1211.3979", "submitter": "Mohamed Firdhous", "authors": "Mohamed Firdhous and Osman Ghazali and Suhaidi Hassan", "title": "Trust Management in Cloud Computing: A Critical Review", "comments": "13 pages, 1 figure, 1 table, 61 references", "journal-ref": "Publication in the International Journal on Advances in ICT for\n  Emerging Regions (ICTer), vol. 04, no. 02, 2011, pp. 24-36", "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has been attracting the attention of several researchers both\nin the academia and the industry as it provides many opportunities for\norganizations by offering a range of computing services. For cloud computing to\nbecome widely adopted by both the enterprises and individuals, several issues\nhave to be solved. A key issue that needs special attention is security of\nclouds, and trust management is an important component of cloud security. In\nthis paper, the authors look at what trust is and how trust has been applied in\ndistributed computing. Trust models proposed for various distributed system has\nthen been summarized. The trust management systems proposed for cloud computing\nhave been investigated with special emphasis on their capability, applicability\nin practical heterogonous cloud environment and implementabilty. Finally, the\nproposed models/systems have been compared with each other based on a selected\nset of cloud computing parameters in a table.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 02:22:54 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Firdhous", "Mohamed", ""], ["Ghazali", "Osman", ""], ["Hassan", "Suhaidi", ""]]}, {"id": "1211.4191", "submitter": "Feng Zhang feng", "authors": "Fengrong Zhang, Claude Carlet, Yupu Hu and Wenzheng Zhang", "title": "Secondary Constructions of Bent Functions and Highly Nonlinear Resilient\n  Functions", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we first present a new secondary construction of bent\nfunctions (building new bent functions from two already defined ones).\nFurthermore, we apply the construction using as initial functions some specific\nbent functions and then provide several concrete constructions of bent\nfunctions. The second part of the paper is devoted to the constructions of\nresilient functions. We give a generalization of the indirect sum construction\nfor constructing resilient functions with high nonlinearity. In addition, we\nmodify the generalized construction to ensure a high nonlinearity of the\nconstructed function.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2012 03:38:46 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Zhang", "Fengrong", ""], ["Carlet", "Claude", ""], ["Hu", "Yupu", ""], ["Zhang", "Wenzheng", ""]]}, {"id": "1211.4328", "submitter": "Ragib Hasan", "authors": "Shams Zawoad, Ragib Hasan", "title": "I Have the Proof: Providing Proofs of Past Data Possession in Cloud\n  Forensics", "comments": "To appear at the Proceedings of the 2012 ASE International Conference\n  on Cyber Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has emerged as a popular computing paradigm in recent years.\nHowever, today's cloud computing architectures often lack support for computer\nforensic investigations. A key task of digital forensics is to prove the\npresence of a particular file in a given storage system. Unfortunately, it is\nvery hard to do so in a cloud given the black-box nature of clouds and the\nmulti-tenant cloud models. In clouds, analyzing the data from a virtual machine\ninstance or data stored in a cloud storage only allows us to investigate the\ncurrent content of the cloud storage, but not the previous contents. In this\npaper, we introduce the idea of building proofs of past data possession in the\ncontext of a cloud storage service. We present a scheme for creating such\nproofs and evaluate its performance in a real cloud provider. We also discuss\nhow this proof of past data possession can be used effectively in cloud\nforensics.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 08:16:59 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Zawoad", "Shams", ""], ["Hasan", "Ragib", ""]]}, {"id": "1211.4493", "submitter": "Monowar  Bhuyan H", "authors": "Monowar H. Bhuyan and D. K. Bhattacharyya and J. K. Kalita", "title": "Survey on Incremental Approaches for Network Anomaly Detection", "comments": "14 pages, 1 figure, 11 tables referred journal publication", "journal-ref": "International Journal of Communication Networks and Information\n  Security (KUST), vol. 3, no. 3, pp. 226-239, 2011", "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the communication industry has connected distant corners of the globe\nusing advances in network technology, intruders or attackers have also\nincreased attacks on networking infrastructure commensurately. System\nadministrators can attempt to prevent such attacks using intrusion detection\ntools and systems. There are many commercially available signature-based\nIntrusion Detection Systems (IDSs). However, most IDSs lack the capability to\ndetect novel or previously unknown attacks. A special type of IDSs, called\nAnomaly Detection Systems, develop models based on normal system or network\nbehavior, with the goal of detecting both known and unknown attacks. Anomaly\ndetection systems face many problems including high rate of false alarm,\nability to work in online mode, and scalability. This paper presents a\nselective survey of incremental approaches for detecting anomaly in normal\nsystem or network traffic. The technological trends, open problems, and\nchallenges over anomaly detection using incremental approach are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 16:53:32 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2012 03:32:57 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Bhuyan", "Monowar H.", ""], ["Bhattacharyya", "D. K.", ""], ["Kalita", "J. K.", ""]]}, {"id": "1211.4503", "submitter": "Monowar  Bhuyan H", "authors": "Monowar H. Bhuyan and D. K. Bhattacharyya", "title": "An Effective Fingerprint Classification and Search Method", "comments": "10 pages, 8 figures, 6 tables, referred journal publication", "journal-ref": "International Journal of Computer Science and Network Security,\n  Vol. 9, No.11, pp. 39-48, 2009", "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an effective fingerprint classification method designed\nbased on a hierarchical agglomerative clustering technique. The performance of\nthe technique was evaluated in terms of several real-life datasets and a\nsignificant improvement in reducing the misclassification error has been\nnoticed. This paper also presents a query based faster fingerprint search\nmethod over the clustered fingerprint databases. The retrieval accuracy of the\nsearch method has been found effective in light of several real-life databases.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 17:13:26 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Bhuyan", "Monowar H.", ""], ["Bhattacharyya", "D. K.", ""]]}, {"id": "1211.4658", "submitter": "Monowar  Bhuyan H", "authors": "Monowar H. Bhuyan, Sarat Saharia, and Dhruba Kr Bhattacharyya", "title": "An Effective Method for Fingerprint Classification", "comments": "9 pages, 7 figures, 6 tables referred journal publication. arXiv\n  admin note: substantial text overlap with arXiv:1211.4503", "journal-ref": "International A. Journal of e-Technology, Vol. 1, No. 3, pp.\n  89-97, January, 2010", "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an effective method for fingerprint classification using\ndata mining approach. Initially, it generates a numeric code sequence for each\nfingerprint image based on the ridge flow patterns. Then for each class, a seed\nis selected by using a frequent itemsets generation technique. These seeds are\nsubsequently used for clustering the fingerprint images. The proposed method\nwas tested and evaluated in terms of several real-life datasets and a\nsignificant improvement in reducing the misclassification errors has been\nnoticed in comparison to its other counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 03:25:57 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Bhuyan", "Monowar H.", ""], ["Saharia", "Sarat", ""], ["Bhattacharyya", "Dhruba Kr", ""]]}, {"id": "1211.4704", "submitter": "Dominik Herrmann", "authors": "Dominik Herrmann and Christine Arndt and Hannes Federrath", "title": "IPv6 Prefix Alteration: An Opportunity to Improve Online Privacy", "comments": "This paper was peer-reviewed and presented at the 1st Workshop on\n  Privacy and Data Protection Technology (PDPT 2012), co-located with the\n  Amsterdam Privacy Conference (APC 2012), October 9, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused on privacy issues related to the prefix part of IPv6\naddresses. Long-lived prefixes may introduce additional tracking opportunities\nfor communication partners and third parties. We outline a number of prefix\nalteration schemes that may be deployed to maintain the unlinkability of users'\nactivities. While none of the schemes will solve all privacy problems on the\nInternet on their own, we argue that the development of practical prefix\nalteration techniques constitutes a worthwile avenue to pursue: They would\nallow Internet Service Providers to increase the attainable privacy level well\nabove the status quo in today's IPv4 networks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 10:34:58 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Herrmann", "Dominik", ""], ["Arndt", "Christine", ""], ["Federrath", "Hannes", ""]]}, {"id": "1211.4723", "submitter": "Arindam Sarkar", "authors": "Arindam Sarkar and J. K. Mandal", "title": "Key Generation and Certification using Multilayer Perceptron in Wireless\n  communication(KGCMLP)", "comments": "17 pages, International Journal of Security, Privacy and Trust\n  Management (IJSPTM), Vol. 1, No 5, October 2012. arXiv admin note:\n  substantial text overlap with arXiv:1208.2334; and text overlap with\n  arXiv:0711.2411 by other authors", "journal-ref": null, "doi": "10.5121/ijsptm.2012.1503", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a key generation and certification technique using multilayer\nperceptron (KGCMLP) has been proposed in wireless communication of\ndata/information. In this proposed KGCMLP technique both sender and receiver\nuses an identical multilayer perceptrons. Both perceptrons are start\nsynchronization by exchanging some control frames. During synchronization\nprocess message integrity test and synchronization test has been carried out.\nOnly the synchronization test does not guarantee the security for this reason\nkey certification phase also been introduced in KGCMLP technique. After Key\ngeneration and certification procedure synchronized identical weight vector\nforms the key for encryption/decryption. Parametric tests have been done and\nresults are compared with some existing classical techniques, which show\ncomparable results for the proposed technique.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 12:04:55 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Sarkar", "Arindam", ""], ["Mandal", "J. K.", ""]]}, {"id": "1211.4812", "submitter": "Martin Monperrus", "authors": "Erwan Abgrall (Uni.lu), Yves Le Traon (Uni.lu, S'nT), Martin Monperrus\n  (INRIA Lille - Nord Europe), Sylvain Gombault (RSM), Mario Heiderich, Alain\n  Ribault", "title": "XSS-FP: Browser Fingerprinting using HTML Parser Quirks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many scenarios in which inferring the type of a client browser is\ndesirable, for instance to fight against session stealing. This is known as\nbrowser fingerprinting. This paper presents and evaluates a novel\nfingerprinting technique to determine the exact nature (browser type and\nversion, eg Firefox 15) of a web-browser, exploiting HTML parser quirks\nexercised through XSS. Our experiments show that the exact version of a web\nbrowser can be determined with 71% of accuracy, and that only 6 tests are\nsufficient to quickly determine the exact family a web browser belongs to.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 17:44:57 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Abgrall", "Erwan", "", "Uni.lu"], ["Traon", "Yves Le", "", "Uni.lu, S'nT"], ["Monperrus", "Martin", "", "INRIA Lille - Nord Europe"], ["Gombault", "Sylvain", "", "RSM"], ["Heiderich", "Mario", ""], ["Ribault", "Alain", ""]]}, {"id": "1211.4976", "submitter": "Benjamin Varcoe", "authors": "Benjamin T. H. Varcoe", "title": "Channel Independent Cryptographic Key Distribution", "comments": "7 Pages, 5 Figures, Submitted to IEEE Transactions on Information\n  Theory, Corrected typo in eqn 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method of cryptographic key distribution using an\n`artificially' noisy channel. This is an important development because, while\nit is known that a noisy channel can be used to generate unconditional secrecy,\nthere are many circumstances in which it is not possible to have a noisy\ninformation exchange, such as in error corrected communication stacks. It is\nshown that two legitimate parties can simulate a noisy channel by adding local\nnoise onto the communication and that the simulated channel has a secrecy\ncapacity even if the underlying channel does not. A derivation of the secrecy\nconditions is presented along with numerical simulations of the channel\nfunction to show that key exchange is feasible.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 09:33:12 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2012 16:29:29 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Varcoe", "Benjamin T. H.", ""]]}, {"id": "1211.5080", "submitter": "Poonam Jindal Ms", "authors": "Poonam Jindal and Brahmjit Singh", "title": "Study And Performance Evaluation Of Security-Throughput Tradeoff With\n  Link Adaptive Encryption Scheme", "comments": "14 pages; IJSPTM, AIRCC, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever increasing volume of information over wireless medium, security\nhas assumed an important dimension. The security of transmitted data over a\nwireless channel aims at protecting the data from unauthorized intrusion.\nWireless network security is achieved using cryptographic primitives. Some\nproperties that give encryption mechanism their cryptographic strength also\nmake them very sensitive to channel error as well. Therefore, security for data\ntransmission over wireless channel results in throughput loss. Trade-off\nbetween security and throughput is always a major concern in wireless networks.\nIn this paper, a Link Adaptive Encryption scheme is evaluated that adapts to\nchannel variations and enhances the security level of WLANs without making any\ncompromise with the network performance. Numerical results obtained through\nsimulation are compared with the fixed block length encryption technique in two\ndifferent modes of operation- Electronic Code Book (ECB) & Cipher Block\nChaining (CBC). Optimal block length is also computed, which is assumed to be\nthe effective strength of the cipher. It has been observed that security\nattained with link adaptive scheme operating in ECB mode of cipher is a better\nsolution for security and throughput trade-off. However, it is found that if\ncomputational security is a major concern, link adaptive scheme in CBC mode\nshould be preferred.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 16:36:57 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Jindal", "Poonam", ""], ["Singh", "Brahmjit", ""]]}, {"id": "1211.5183", "submitter": "Emiliano De Cristofaro", "authors": "Abdelberi Chaabane and Emiliano De Cristofaro and Mohammed-Ali Kaafar\n  and Ersin Uzun", "title": "Privacy in Content-Oriented Networking: Threats and Countermeasures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the Internet struggles to cope with scalability, mobility, and security\nissues, new network architectures are being proposed to better accommodate the\nneeds of modern systems and applications. In particular, Content-Oriented\nNetworking (CON) has emerged as a promising next-generation Internet\narchitecture: it sets to decouple content from hosts, at the network layer, by\nnaming data rather than hosts. CON comes with a potential for a wide range of\nbenefits, including reduced congestion and improved delivery speed by means of\ncontent caching, simpler configuration of network devices, and security at the\ndata level. However, it remains an interesting open question whether or not,\nand to what extent, this emerging networking paradigm bears new privacy\nchallenges. In this paper, we provide a systematic privacy analysis of CON and\nthe common building blocks among its various architectural instances in order\nto highlight emerging privacy threats, and analyze a few potential\ncountermeasures. Finally, we present a comparison between CON and today's\nInternet in the context of a few privacy concepts, such as, anonymity,\ncensoring, traceability, and confidentiality.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 01:56:51 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2013 03:52:22 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2013 10:38:23 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2013 15:39:01 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Chaabane", "Abdelberi", ""], ["De Cristofaro", "Emiliano", ""], ["Kaafar", "Mohammed-Ali", ""], ["Uzun", "Ersin", ""]]}, {"id": "1211.5252", "submitter": "Shun Watanabe", "authors": "Shun Watanabe and Masahito Hayashi", "title": "Non-Asymptotic Analysis of Privacy Amplification via Renyi Entropy and\n  Inf-Spectral Entropy", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the privacy amplification problem, and compares the\nexisting two bounds: the exponential bound derived by one of the authors and\nthe min-entropy bound derived by Renner. It turns out that the exponential\nbound is better than the min-entropy bound when a security parameter is rather\nsmall for a block length, and that the min-entropy bound is better than the\nexponential bound when a security parameter is rather large for a block length.\nFurthermore, we present another bound that interpolates the exponential bound\nand the min-entropy bound by a hybrid use of the Renyi entropy and the\ninf-spectral entropy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 10:57:22 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Watanabe", "Shun", ""], ["Hayashi", "Masahito", ""]]}, {"id": "1211.5613", "submitter": "P\\'eter Pleva", "authors": "Peter Pleva", "title": "A Revised Classification of Anonymity", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper primarily addresses the issue of identifying all possible levels\nof digital anonymity, thereby allowing electronic services and mechanisms to be\ncategorised. For this purpose, we sophisticate the generic idea of anonymity\nand, filling a niche in the field, bring the scope of trust into the focus of\ncategorisation. One major concern of our work is to propose a novel and\nuniversal taxonomy which enables a dynamic, trust-based comparison between\nsystems at an abstract level. On the other hand, our contribution intentionally\ndoes not offer an alternative to anonymity metrics, but neither is it concerned\nwith methods of anonymous data retrieval (cf. data-mining techniques). However,\nfor ease of comprehension, it provides a systematic 'application manual' and\nalso presents a lucid overview of the correspondence between the current and\nrelated taxonomies. Additionally, as a generalisation of group signatures, we\nintroduce the notion of group schemes.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 21:30:21 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Pleva", "Peter", ""]]}, {"id": "1211.5614", "submitter": "Ankit Chaudhary", "authors": "Ankit Chaudhary, J. Vasavada, J.L. Raheja, S. Kumar, M. Sharma", "title": "A Hash based Approach for Secure Keyless Steganography in Lossless RGB\n  Images", "comments": "The paper is withdrawn due to license issue", "journal-ref": "The 22nd International Conference on Computer Graphics and Vision,\n  2012, pp.80-83", "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an improved steganography approach for hiding text\nmessages in lossless RGB images. The objective of this work is to increase the\nsecurity level and to improve the storage capacity with compression techniques.\nThe security level is increased by randomly distributing the text message over\nthe entire image instead of clustering within specific image portions. Storage\ncapacity is increased by utilizing all the color channels for storing\ninformation and providing the source text message compression. The degradation\nof the images can be minimized by changing only one least significant bit per\ncolor channel for hiding the message, incurring a very little change in the\noriginal image. Using steganography alone with simple LSB has a potential\nproblem that the secret message is easily detectable from the histogram\nanalysis method. To improve the security as well as the image embedding\ncapacity indirectly, a compression based scheme is introduced. Various tests\nhave been done to check the storage capacity and message distribution. These\ntestes show the superiority of the proposed approach with respect to other\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 21:34:53 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2013 16:41:40 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2013 01:27:39 GMT"}, {"version": "v4", "created": "Sun, 10 Mar 2013 08:21:45 GMT"}, {"version": "v5", "created": "Wed, 17 Apr 2013 00:20:00 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Chaudhary", "Ankit", ""], ["Vasavada", "J.", ""], ["Raheja", "J. L.", ""], ["Kumar", "S.", ""], ["Sharma", "M.", ""]]}, {"id": "1211.6166", "submitter": "Tao Zhu", "authors": "Tao Zhu, David Phipps, Adam Pridgen, Jedidiah R. Crandall, Dan S.\n  Wallach", "title": "Tracking and Quantifying Censorship on a Chinese Microblogging Site", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present measurements and analysis of censorship on Weibo, a popular\nmicroblogging site in China. Since we were limited in the rate at which we\ncould download posts, we identified users likely to participate in sensitive\ntopics and recursively followed their social contacts. We also leveraged new\nnatural language processing techniques to pick out trending topics despite the\nuse of neologisms, named entities, and informal language usage in Chinese\nsocial media. We found that Weibo dynamically adapts to the changing interests\nof its users through multiple layers of filtering. The filtering includes both\nretroactively searching posts by keyword or repost links to delete them, and\nrejecting posts as they are posted. The trend of sensitive topics is\nshort-lived, suggesting that the censorship is effective in stopping the\n\"viral\" spread of sensitive issues. We also give evidence that sensitive topics\nin Weibo only scarcely propagate beyond a core of sensitive posters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 23:54:27 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Zhu", "Tao", ""], ["Phipps", "David", ""], ["Pridgen", "Adam", ""], ["Crandall", "Jedidiah R.", ""], ["Wallach", "Dan S.", ""]]}, {"id": "1211.6409", "submitter": "Mohammed El-Dosuky", "authors": "Mohammed El-Dosuky, Ahmed EL-Bassiouny, Taher Hamza and Magdy Rashad", "title": "Obesity Heuristic, New Way On Artificial Immune Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a need for new metaphors from immunology to flourish the application\nareas of Artificial Immune Systems. A metaheuristic called Obesity Heuristic\nderived from advances in obesity treatment is proposed. The main forces of the\nalgorithm are the generation omega-6 and omega-3 fatty acids. The algorithm\nworks with Just-In-Time philosophy; by starting only when desired. A case study\nof data cleaning is provided. With experiments conducted on standard tables,\nresults show that Obesity Heuristic outperforms other algorithms, with 100%\nrecall. This is a great improvement over other algorithms\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2012 01:34:45 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["El-Dosuky", "Mohammed", ""], ["EL-Bassiouny", "Ahmed", ""], ["Hamza", "Taher", ""], ["Rashad", "Magdy", ""]]}, {"id": "1211.6610", "submitter": "Muhamed Halilovic", "authors": "Muhamed Halilovic, Abdulhamit Subasi", "title": "Intrusion Detection on Smartphones", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone technology is more and more becoming the predominant communication\ntool for people across the world. People use their smartphones to keep their\ncontact data, to browse the internet, to exchange messages, to keep notes,\ncarry their personal files and documents, etc. Users while browsing are also\ncapable of shopping online, thus provoking a need to type their credit card\nnumbers and security codes. As the smartphones are becoming widespread so do\nthe security threats and vulnerabilities facing this technology. Recent news\nand articles indicate huge increase in malware and viruses for operating\nsystems employed on smartphones (primarily Android and iOS). Major limitations\nof smartphone technology are its processing power and its scarce energy source\nsince smartphones rely on battery usage. Since smartphones are devices which\nchange their network location as the user moves between different places,\nintrusion detection systems for smartphone technology are most often classified\nas IDSs designed for mobile ad-hoc networks. The aim of this research is to\ngive a brief overview of IDS technology, give an overview of major machine\nlearning and pattern recognition algorithms used in IDS technologies, give an\noverview of security models of iOS and Android and propose a new host-based IDS\nmodel for smartphones and create proof-of-concept application for Android\nplatform for the newly proposed model. Keywords: IDS, SVM, Android, iOS;\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 14:34:18 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Halilovic", "Muhamed", ""], ["Subasi", "Abdulhamit", ""]]}, {"id": "1211.6847", "submitter": "Bernard Ycart", "authors": "Bernard Ycart (LJK)", "title": "Letter counting: a stem cell for Cryptology, Quantitative Linguistics,\n  and Statistics", "comments": null, "journal-ref": "Historiographia Linguistica 40, 3 (2013) 303-329", "doi": null, "report-no": null, "categories": "math.HO cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting letters in written texts is a very ancient practice. It has\naccompanied the development of Cryptology, Quantitative Linguistics, and\nStatistics. In Cryptology, counting frequencies of the different characters in\nan encrypted message is the basis of the so called frequency analysis method.\nIn Quantitative Linguistics, the proportion of vowels to consonants in\ndifferent languages was studied long before authorship attribution. In\nStatistics, the alternation vowel-consonants was the only example that Markov\never gave of his theory of chained events. A short history of letter counting\nis presented. The three domains, Cryptology, Quantitative Linguistics, and\nStatistics, are then examined, focusing on the interactions with the other two\nfields through letter counting. As a conclusion, the eclectism of past\ncenturies scholars, their background in humanities, and their familiarity with\ncryptograms, are identified as contributing factors to the mutual enrichment\nprocess which is described here.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 09:00:59 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Ycart", "Bernard", "", "LJK"]]}, {"id": "1211.6984", "submitter": "Roohallah Rastaghi", "authors": "Roohallah Rastaghi", "title": "New Approach for CCA2-Secure Post-Quantum Cryptosystem Using Knapsack\n  Problem", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chosen-ciphertext security, which guarantees confidentiality of encrypted\nmessages even in the presence of a decryption oracle, has become the defacto\nnotion of security for public-key encryption under active attack. In this\nmanuscript, for the first time, we propose a new approach for constructing\npost-quantum cryptosystems secure against adaptive chosen ciphertext attack\n(CCA2-secure) in the standard model using the knapsack problem. The\ncomputational version of the knapsack problem is NP-hard. Thus, this problem is\nexpected to be difficult to solve using quantum computers. Our construction is\na precoding-based encryption algorithm and uses the knapsack problem to perform\na permutation and pad some random fogged data to the message bits. Compared to\nother approaches were introduced today, our approach is very simple and more\nefficient.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 17:02:22 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2012 13:52:28 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2012 18:17:57 GMT"}, {"version": "v4", "created": "Sun, 13 Jan 2013 17:39:57 GMT"}, {"version": "v5", "created": "Sat, 5 Apr 2014 19:00:36 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Rastaghi", "Roohallah", ""]]}, {"id": "1211.7075", "submitter": "Yulong Shen", "authors": "Yulong Shen, Xiaohong Jiang, Jianfeng Ma and Weisong Shi", "title": "Secure and Reliable Transmission with Cooperative Relays in Two-Hop\n  Wireless Networks", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This work considers the secure and reliable information transmission in\ntwo-hop relay wireless networks without the information of both eavesdropper\nchannels and locations. While the previous work on this problem mainly studied\ninfinite networks and their asymptotic behavior and scaling law results, this\npapers focuses on a more practical network with finite number of system nodes\nand explores the corresponding exact results on the number of eavesdroppers the\nnetwork can tolerant to ensure a desired secrecy and reliability. For achieving\nsecure and reliable information transmission in a finite network, two\ntransmission protocols are considered in this paper, one adopts an optimal but\ncomplex relay selection process with less load balance capacity while the other\nadopts a random but simple relay selection process with good load balance\ncapacity. Theoretical analysis is further provided to determine the exact and\nmaximum number of independent and also uniformly distributed eavesdroppers one\nnetwork can tolerate to satisfy a specified requirement in terms of the maximum\nsecrecy outage probability and maximum transmission outage probability allowed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 21:01:22 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Shen", "Yulong", ""], ["Jiang", "Xiaohong", ""], ["Ma", "Jianfeng", ""], ["Shi", "Weisong", ""]]}, {"id": "1211.7302", "submitter": "Aaron Roth", "authors": "Zhiyi Huang and Aaron Roth", "title": "Exploiting Metric Structure for Efficient Private Query Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of privately answering queries defined on databases\nwhich are collections of points belonging to some metric space. We give simple,\ncomputationally efficient algorithms for answering distance queries defined\nover an arbitrary metric. Distance queries are specified by points in the\nmetric space, and ask for the average distance from the query point to the\npoints contained in the database, according to the specified metric. Our\nalgorithms run efficiently in the database size and the dimension of the space,\nand operate in both the online query release setting, and the offline setting\nin which they must in polynomial time generate a fixed data structure which can\nanswer all queries of interest. This represents one of the first subclasses of\nlinear queries for which efficient algorithms are known for the private query\nrelease problem, circumventing known hardness results for generic linear\nqueries.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 16:28:46 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Huang", "Zhiyi", ""], ["Roth", "Aaron", ""]]}]