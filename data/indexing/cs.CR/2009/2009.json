[{"id": "2009.00086", "submitter": "Douglas Stebila", "authors": "Moe Sabry and Reza Samavi and Douglas Stebila", "title": "ArchiveSafe: Mass-Leakage-Resistant Storage from Proof-of-Work", "comments": "21 pages, 8 figures. Published in Data Privacy Management (DPM)\n  International Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data breaches-mass leakage of stored information-are a major security\nconcern. Encryption can provide confidentiality, but encryption depends on a\nkey which, if compromised, allows the attacker to decrypt everything,\neffectively instantly. Security of encrypted data thus becomes a question of\nprotecting the encryption keys. In this paper, we propose using keyless\nencryption to construct a mass leakage resistant archiving system, where\ndecryption of a file is only possible after the requester, whether an\nauthorized user or an adversary, completes a proof of work in the form of\nsolving a cryptographic puzzle. This proposal is geared towards protection of\ninfrequently-accessed archival data, where any one file may not require too\nmuch work to decrypt, decryption of a large number of files-mass\nleakage-becomes increasingly expensive for an attacker. We present a prototype\nimplementation realized as a user-space file system driver for Linux. We report\nexperimental results of system behaviour under different file sizes and puzzle\ndifficulty levels. Our keyless encryption technique can be added as a layer on\ntop of traditional encryption: together they provide strong security against\nadversaries without the key and resistance against mass decryption by an\nattacker.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 20:01:05 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 15:48:07 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Sabry", "Moe", ""], ["Samavi", "Reza", ""], ["Stebila", "Douglas", ""]]}, {"id": "2009.00097", "submitter": "Linjun Zhou", "authors": "Linjun Zhou, Peng Cui, Yinan Jiang, Shiqiang Yang", "title": "Adversarial Eigen Attack on Black-Box Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box adversarial attack has attracted a lot of research interests for\nits practical use in AI safety. Compared with the white-box attack, a black-box\nsetting is more difficult for less available information related to the\nattacked model and the additional constraint on the query budget. A general way\nto improve the attack efficiency is to draw support from a pre-trained\ntransferable white-box model. In this paper, we propose a novel setting of\ntransferable black-box attack: attackers may use external information from a\npre-trained model with available network parameters, however, different from\nprevious studies, no additional training data is permitted to further change or\ntune the pre-trained model. To this end, we further propose a new algorithm,\nEigenBA to tackle this problem. Our method aims to explore more gradient\ninformation of the black-box model, and promote the attack efficiency, while\nkeeping the perturbation to the original attacked image small, by leveraging\nthe Jacobian matrix of the pre-trained white-box model. We show the optimal\nperturbations are closely related to the right singular vectors of the Jacobian\nmatrix. Further experiments on ImageNet and CIFAR-10 show that even the\nunlearnable pre-trained white-box model could also significantly boost the\nefficiency of the black-box attack and our proposed method could further\nimprove the attack efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 07:37:43 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Zhou", "Linjun", ""], ["Cui", "Peng", ""], ["Jiang", "Yinan", ""], ["Yang", "Shiqiang", ""]]}, {"id": "2009.00111", "submitter": "Daniel O'Malley", "authors": "Daniel O'Malley and John K. Golden", "title": "Homomorphic Encryption for Quantum Annealing with Spin Reversal\n  Transformations", "comments": "to be published at IEEE HPEC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homomorphic encryption has been an area of study in classical computing for\ndecades. The fundamental goal of homomorphic encryption is to enable\n(untrusted) Oscar to perform a computation for Alice without Oscar knowing the\ninput to the computation or the output from the computation. Alice encrypts the\ninput before sending it to Oscar, and Oscar performs the computation directly\non the encrypted data, producing an encrypted result. Oscar then sends the\nencrypted result of the computation back to Alice, who can decrypt it. We\ndescribe an approach to homomorphic encryption for quantum annealing based on\nspin reversal transformations and show that it comes with little or no\nperformance penalty. This is in contrast to approaches to homomorphic\nencryption for classical computing, which incur a significant additional\ncomputational cost. This implies that the performance gap between quantum\nannealing and classical computing is reduced when both paradigms use\nhomomorphic encryption. Further, homomorphic encryption is critical for quantum\nannealing because quantum annealers are native to the cloud -- a third party\n(such as untrusted Oscar) performs the computation. If sensitive information,\nsuch as health-related data subject to the Health Insurance Portability and\nAccountability Act, is to be processed with quantum annealers, such a technique\ncould be useful.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 21:23:42 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["O'Malley", "Daniel", ""], ["Golden", "John K.", ""]]}, {"id": "2009.00163", "submitter": "Binghui Wang", "authors": "Houxiang Fan, Binghui Wang, Pan Zhou, Ang Li, Meng Pang, Zichuan Xu,\n  Cai Fu, Hai Li, Yiran Chen", "title": "Reinforcement Learning-based Black-Box Evasion Attacks to Link\n  Prediction in Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction in dynamic graphs (LPDG) is an important research problem\nthat has diverse applications such as online recommendations, studies on\ndisease contagion, organizational studies, etc. Various LPDG methods based on\ngraph embedding and graph neural networks have been recently proposed and\nachieved state-of-the-art performance. In this paper, we study the\nvulnerability of LPDG methods and propose the first practical black-box evasion\nattack. Specifically, given a trained LPDG model, our attack aims to perturb\nthe graph structure, without knowing to model parameters, model architecture,\netc., such that the LPDG model makes as many wrong predicted links as possible.\nWe design our attack based on a stochastic policy-based RL algorithm. Moreover,\nwe evaluate our attack on three real-world graph datasets from different\napplication domains. Experimental results show that our attack is both\neffective and efficient.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 01:04:49 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 20:58:04 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Fan", "Houxiang", ""], ["Wang", "Binghui", ""], ["Zhou", "Pan", ""], ["Li", "Ang", ""], ["Pang", "Meng", ""], ["Xu", "Zichuan", ""], ["Fu", "Cai", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "2009.00203", "submitter": "Binghui Wang", "authors": "Binghui Wang, Tianxiang Zhou, Minhua Lin, Pan Zhou, Ang Li, Meng Pang,\n  Cai Fu, Hai Li, Yiran Chen", "title": "Evasion Attacks to Graph Neural Networks via Influence Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have achieved state-of-the-art performance in\nmany graph-related tasks, e.g., node classification. However, recent works show\nthat GNNs are vulnerable to evasion attacks, i.e., an attacker can slightly\nperturb the graph structure to fool GNN models. Existing evasion attacks to\nGNNs have several key drawbacks: 1) they are limited to attack two-layer GNNs;\n2) they are not efficient; or/and 3) they need to know GNN model parameters. We\naddress the above drawbacks in this paper and propose an influence-based\nevasion attack against GNNs. Specifically, we first introduce two influence\nfunctions, i.e., feature-label influence and label influence, that are defined\non GNNs and label propagation (LP), respectively. Then, we build a strong\nconnection between GNNs and LP in terms of influence. Next, we reformulate the\nevasion attack against GNNs to be related to calculating label influence on LP,\nwhich is applicable to multi-layer GNNs and does not need to know the GNN\nmodel. We also propose an efficient algorithm to calculate label influence.\nFinally, we evaluate our influence-based attack on three benchmark graph\ndatasets. Our experimental results show that, compared to state-of-the-art\nattack, our attack can achieve comparable attack performance, but has a 5-50x\nspeedup when attacking two-layer GNNs. Moreover, our attack is effective to\nattack multi-layer GNNs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 03:24:51 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 20:50:56 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Wang", "Binghui", ""], ["Zhou", "Tianxiang", ""], ["Lin", "Minhua", ""], ["Zhou", "Pan", ""], ["Li", "Ang", ""], ["Pang", "Meng", ""], ["Fu", "Cai", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "2009.00271", "submitter": "Waqas Aman", "authors": "Ahsan Mehmood, Waqas Aman, M. Mahboob Ur Rahman, M. A. Imran, Qammer\n  H. Abbasi", "title": "Preventing Identity Attacks in RFID Backscatter Communication Systems: A\n  Physical-Layer Approach", "comments": "Accepted for publication in the Proc. of 5th Int. Conf. on the\n  UK/China Emerging Technologies(UCET) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers identity attack on a radio-frequency identification\n(RFID)-based backscatter communication system. Specifically, we consider a\nsingle-reader, single-tag RFID system whereby the reader and the tag undergo\ntwo-way signaling which enables the reader to extract the tag ID in order to\nauthenticate the legitimate tag (L-tag). We then consider a scenario whereby a\nmalicious tag (M-tag)---having the same ID as the L-tag programmed in its\nmemory by a wizard---attempts to deceive the reader by pretending to be the\nL-tag. To this end, we counter the identity attack by exploiting the\nnon-reciprocity of the end-to-end channel (i.e., the residual channel) between\nthe reader and the tag as the fingerprint of the tag. The passive nature of the\ntag(s) (and thus, lack of any computational platform at the tag) implies that\nthe proposed light-weight physical-layer authentication method is implemented\nat the reader. To be concrete, in our proposed scheme, the reader acquires the\nraw data via two-way (challenge-response) message exchange mechanism, does\nleast-squares estimation to extract the fingerprint, and does binary hypothesis\ntesting to do authentication. We also provide closed-form expressions for the\ntwo error probabilities of interest (i.e., false alarm and missed detection).\nSimulation results attest to the efficacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 07:22:09 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Mehmood", "Ahsan", ""], ["Aman", "Waqas", ""], ["Rahman", "M. Mahboob Ur", ""], ["Imran", "M. A.", ""], ["Abbasi", "Qammer H.", ""]]}, {"id": "2009.00273", "submitter": "Martin Henze", "authors": "Benedikt Klaer, \\\"Omer Sen, Dennis van der Velde, Immanuel Hacker,\n  Michael Andres, Martin Henze", "title": "Graph-based Model of Smart Grid Architectures", "comments": "6 pages, 5 figures, to be published in Proceedings of the 3rd\n  International Conference on Smart Energy Systems and Technologies (SEST)", "journal-ref": null, "doi": "10.1109/SEST48500.2020.9203113", "report-no": null, "categories": "cs.SE cs.CR cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising use of information and communication technology in smart grids\nlikewise increases the risk of failures that endanger the security of power\nsupply, e.g., due to errors in the communication configuration, faulty control\nalgorithms, or cyber-attacks. Co-simulations can be used to investigate such\neffects, but require precise modeling of the energy, communication, and\ninformation domain within an integrated smart grid infrastructure model. Given\nthe complexity and lack of detailed publicly available communication network\nmodels for smart grid scenarios, there is a need for an automated and\nsystematic approach to creating such coupled models. In this paper, we present\nan approach to automatically generate smart grid infrastructure models based on\nan arbitrary electrical distribution grid model using a generic architectural\ntemplate. We demonstrate the applicability and unique features of our approach\nalongside examples concerning network planning, co-simulation setup, and\nspecification of domain-specific intrusion detection systems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 07:32:19 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Klaer", "Benedikt", ""], ["Sen", "\u00d6mer", ""], ["van der Velde", "Dennis", ""], ["Hacker", "Immanuel", ""], ["Andres", "Michael", ""], ["Henze", "Martin", ""]]}, {"id": "2009.00349", "submitter": "Sinem Sav", "authors": "Sinem Sav, Apostolos Pyrgelis, Juan R. Troncoso-Pastoriza, David\n  Froelicher, Jean-Philippe Bossuat, Joao Sa Sousa, and Jean-Pierre Hubaux", "title": "POSEIDON: Privacy-Preserving Federated Neural Network Learning", "comments": "Accepted for publication at Network and Distributed Systems Security\n  (NDSS) Symposium 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of privacy-preserving training and\nevaluation of neural networks in an $N$-party, federated learning setting. We\npropose a novel system, POSEIDON, the first of its kind in the regime of\nprivacy-preserving neural network training. It employs multiparty lattice-based\ncryptography to preserve the confidentiality of the training data, the model,\nand the evaluation data, under a passive-adversary model and collusions between\nup to $N-1$ parties. To efficiently execute the secure backpropagation\nalgorithm for training neural networks, we provide a generic packing approach\nthat enables Single Instruction, Multiple Data (SIMD) operations on encrypted\ndata. We also introduce arbitrary linear transformations within the\ncryptographic bootstrapping operation, optimizing the costly cryptographic\ncomputations over the parties, and we define a constrained optimization problem\nfor choosing the cryptographic parameters. Our experimental results show that\nPOSEIDON achieves accuracy similar to centralized or decentralized non-private\napproaches and that its computation and communication overhead scales linearly\nwith the number of parties. POSEIDON trains a 3-layer neural network on the\nMNIST dataset with 784 features and 60K samples distributed among 10 parties in\nless than 2 hours.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 11:06:31 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 09:34:46 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 13:27:01 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Sav", "Sinem", ""], ["Pyrgelis", "Apostolos", ""], ["Troncoso-Pastoriza", "Juan R.", ""], ["Froelicher", "David", ""], ["Bossuat", "Jean-Philippe", ""], ["Sousa", "Joao Sa", ""], ["Hubaux", "Jean-Pierre", ""]]}, {"id": "2009.00384", "submitter": "Kanav Gupta", "authors": "Kanav Gupta, Mithilesh Kumar and H{\\aa}vard Raddum", "title": "Obtuse Lattice Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lattice reduction is an algorithm that transforms the given basis of the\nlattice to another lattice basis such that problems like finding a shortest\nvector and closest vector become easier to solve. We define a class of bases\ncalled obtuse bases and show that any lattice basis can be transformed to an\nobtuse basis. A shortest vector $\\mathbf{s}$ can be written as\n$\\mathbf{s}=v_1\\mathbf{b}_1+\\dots+v_n\\mathbf{b}_n$ where\n$\\mathbf{b}_1,\\dots,\\mathbf{b}_n$ are the input basis vectors and\n$v_1,\\dots,v_n$ are integers. When the input basis is obtuse, all these\nintegers can be chosen to be positive for a shortest vector. This property of\nobtuse bases makes the lattice enumeration algorithm for finding a shortest\nvector exponentially faster. We have implemented the algorithm for making bases\nobtuse, and tested it some small bases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 12:30:14 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 10:07:56 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Gupta", "Kanav", ""], ["Kumar", "Mithilesh", ""], ["Raddum", "H\u00e5vard", ""]]}, {"id": "2009.00395", "submitter": "Shadi Rahimian", "authors": "Shadi Rahimian and Tribhuvanesh Orekondy and Mario Fritz", "title": "Sampling Attacks: Amplification of Membership Inference Attacks by\n  Repeated Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have been shown to leak information violating the\nprivacy of their training set. We focus on membership inference attacks on\nmachine learning models which aim to determine whether a data point was used to\ntrain the victim model. Our work consists of two sides: We introduce sampling\nattack, a novel membership inference technique that unlike other standard\nmembership adversaries is able to work under severe restriction of no access to\nscores of the victim model. We show that a victim model that only publishes the\nlabels is still susceptible to sampling attacks and the adversary can recover\nup to 100% of its performance compared to when posterior vectors are provided.\nThe other sides of our work includes experimental results on two recent\nmembership inference attack models and the defenses against them. For defense,\nwe choose differential privacy in the form of gradient perturbation during the\ntraining of the victim model as well as output perturbation at prediction time.\nWe carry out our experiments on a wide range of datasets which allows us to\nbetter analyze the interaction between adversaries, defense mechanism and\ndatasets. We find out that our proposed fast and easy-to-implement output\nperturbation technique offers good privacy protection for membership inference\nattacks at little impact on utility.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 12:54:54 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Rahimian", "Shadi", ""], ["Orekondy", "Tribhuvanesh", ""], ["Fritz", "Mario", ""]]}, {"id": "2009.00442", "submitter": "Xun Xian", "authors": "Xun Xian, Xinran Wang, Mingyi Hong, Jie Ding and Reza Ghanadan", "title": "Imitation Privacy", "comments": "8 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:2004.00566", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there have been many cloud-based machine learning services,\nwhere well-trained models are provided to users on a pay-per-query scheme\nthrough a prediction API. The emergence of these services motivates this work,\nwhere we will develop a general notion of model privacy named imitation\nprivacy. We show the broad applicability of imitation privacy in classical\nquery-response MLaaS scenarios and new multi-organizational learning scenarios.\nWe also exemplify the fundamental difference between imitation privacy and the\nusual data-level privacy.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 20:29:41 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Xian", "Xun", ""], ["Wang", "Xinran", ""], ["Hong", "Mingyi", ""], ["Ding", "Jie", ""], ["Ghanadan", "Reza", ""]]}, {"id": "2009.00530", "submitter": "Duy Phan Mr", "authors": "Phan The Duy, Do Thi Thu Hien, Van-Hau Pham", "title": "A survey on Blockchain-based applications for reforming data protection,\n  privacy and security", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The modern society, economy and industry have been changed remarkably by many\ncutting-edge technologies over the last years, and many more are in development\nand early implementation that will in turn led even wider spread of adoptions\nand greater alteration. Blockchain technology along with other rising ones is\nexpected to transform virtually every aspect of global business and\nindividuals' lifestyle in some areas. It has been spreading with multi-sector\napplications from financial services to healthcare, supply chain, and\ncybersecurity emerging every passing day. Simultaneously, in the digital world,\ndata protection and privacy are the most enormous issues which customers,\ncompanies and policymakers also take seriously into consideration due to the\nrecent increase of security breaches and surveillance in reported incidents. In\nthis case, blockchain has the capability and potential to revolutionize trust,\nsecurity and privacy of individual data in the online world. Hence, the purpose\nof this paper is to study the actual cases of Blockchain applied in the\nreformation of privacy and security field by discussing its impacts as well as\nthe opportunities and challenges.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 16:04:57 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Duy", "Phan The", ""], ["Hien", "Do Thi Thu", ""], ["Pham", "Van-Hau", ""]]}, {"id": "2009.00607", "submitter": "Xiaoqi Li", "authors": "Xiaoqi Li, Ting Chen, Xiapu Luo, Jiangshan Yu", "title": "Characterizing Erasable Accounts in Ethereum", "comments": "In Proceedings of the 23rd Information Security Conference (ISC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being the most popular permissionless blockchain that supports smart\ncontracts, Ethereum allows any user to create accounts on it. However, not all\naccounts matter. For example, the accounts due to attacks can be removed. In\nthis paper, we conduct the first investigation on erasable accounts that can be\nremoved to save system resources and even users' money (i.e., ETH or gas). In\nparticular, we propose and develop a novel tool named GLASER, which analyzes\nthe State DataBase of Ethereum to discover five kinds of erasable accounts. The\nexperimental results show that GLASER can accurately reveal 508,482 erasable\naccounts and these accounts lead to users wasting more than 106 million\ndollars. GLASER can help stop further economic loss caused by these detected\naccounts. Moreover, GLASER characterizes the attacks/behaviors related to\ndetected erasable accounts through graph analysis.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:57:47 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Li", "Xiaoqi", ""], ["Chen", "Ting", ""], ["Luo", "Xiapu", ""], ["Yu", "Jiangshan", ""]]}, {"id": "2009.00621", "submitter": "Sergi Ramos-Calderer", "authors": "Sergi Ramos-Calderer, Emanuele Bellini, Jos\\'e I. Latorre, Marc\n  Manzano, Victor Mateu", "title": "Quantum Search for Scaled Hash Function Preimages", "comments": "24 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the implementation of Grover's algorithm in a quantum simulator to\nperform a quantum search for preimages of two scaled hash functions, whose\ndesign only uses modular addition, word rotation, and bitwise exclusive or. Our\nimplementation provides the means to assess with precision the scaling of the\nnumber of gates and depth of a full-fledged quantum circuit designed to find\nthe preimages of a given hash digest. The detailed construction of the quantum\noracle shows that the presence of AND gates, OR gates, shifts of bits and the\nreuse of the initial state along the computation, require extra quantum\nresources as compared with other hash functions based on modular additions, XOR\ngates and rotations. We also track the entanglement entropy present in the\nquantum register at every step along the computation, showing that it becomes\nmaximal at the inner core of the first action of the quantum oracle, which\nimplies that no classical simulation based on Tensor Networks would be of\nrelevance. Finally, we show that strategies that suggest a shortcut based on\nsampling the quantum register after a few steps of Grover's algorithm can only\nprovide some marginal practical advantage in terms of error mitigation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 18:00:02 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Ramos-Calderer", "Sergi", ""], ["Bellini", "Emanuele", ""], ["Latorre", "Jos\u00e9 I.", ""], ["Manzano", "Marc", ""], ["Mateu", "Victor", ""]]}, {"id": "2009.00716", "submitter": "Vladimir Shpilrain", "authors": "Nael Rahman and Vladimir Shpilrain", "title": "MAKE: a Matrix Action Key Exchange", "comments": "8 pages, 4 figires", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a public key exchange protocol based on a semidirect product of two\ncyclic (semi)groups of matrices over Z_p. One of the (semi)groups is additive,\nthe other one multiplicative. This allows us to take advantage of both\noperations on matrices to diffuse information. We note that in our protocol, no\npower of any matrix or of any element of Z_p is ever exposed, so all standard\nattacks on Diffie-Hellman-like protocols (including Shor's quantum algorithm\nattack) are not applicable.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 21:19:00 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 21:42:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Rahman", "Nael", ""], ["Shpilrain", "Vladimir", ""]]}, {"id": "2009.00718", "submitter": "Shu Wang", "authors": "Shu Wang, Jiahao Cao, Xu He, Kun Sun and Qi Li", "title": "When the Differences in Frequency Domain are Compensated: Understanding\n  and Defeating Modulated Replay Attacks on Automatic Speech Recognition", "comments": "17 pages, 24 figures, In Proceedings of the 2020 ACM SIGSAC\n  Conference on Computer and Communications Security (CCS' 20)", "journal-ref": null, "doi": "10.1145/3372297.3417254", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) systems have been widely deployed in\nmodern smart devices to provide convenient and diverse voice-controlled\nservices. Since ASR systems are vulnerable to audio replay attacks that can\nspoof and mislead ASR systems, a number of defense systems have been proposed\nto identify replayed audio signals based on the speakers' unique acoustic\nfeatures in the frequency domain. In this paper, we uncover a new type of\nreplay attack called modulated replay attack, which can bypass the existing\nfrequency domain based defense systems. The basic idea is to compensate for the\nfrequency distortion of a given electronic speaker using an inverse filter that\nis customized to the speaker's transform characteristics. Our experiments on\nreal smart devices confirm the modulated replay attacks can successfully escape\nthe existing detection mechanisms that rely on identifying suspicious features\nin the frequency domain. To defeat modulated replay attacks, we design and\nimplement a countermeasure named DualGuard. We discover and formally prove that\nno matter how the replay audio signals could be modulated, the replay attacks\nwill either leave ringing artifacts in the time domain or cause spectrum\ndistortion in the frequency domain. Therefore, by jointly checking suspicious\nfeatures in both frequency and time domains, DualGuard can successfully detect\nvarious replay attacks including the modulated replay attacks. We implement a\nprototype of DualGuard on a popular voice interactive platform, ReSpeaker Core\nv2. The experimental results show DualGuard can achieve 98% accuracy on\ndetecting modulated replay attacks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 21:25:04 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wang", "Shu", ""], ["Cao", "Jiahao", ""], ["He", "Xu", ""], ["Sun", "Kun", ""], ["Li", "Qi", ""]]}, {"id": "2009.00774", "submitter": "Yanchao Sun", "authors": "Yanchao Sun, Da Huo and Furong Huang", "title": "Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown\n  Dynamics", "comments": null, "journal-ref": "The Ninth International Conference on Learning Representations\n  (ICLR 2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisoning attacks on Reinforcement Learning (RL) systems could take advantage\nof RL algorithm's vulnerabilities and cause failure of the learning. However,\nprior works on poisoning RL usually either unrealistically assume the attacker\nknows the underlying Markov Decision Process (MDP), or directly apply the\npoisoning methods in supervised learning to RL. In this work, we build a\ngeneric poisoning framework for online RL via a comprehensive investigation of\nheterogeneous poisoning models in RL. Without any prior knowledge of the MDP,\nwe propose a strategic poisoning algorithm called Vulnerability-Aware\nAdversarial Critic Poison (VA2C-P), which works for most policy-based deep RL\nagents, closing the gap that no poisoning method exists for policy-based RL\nagents. VA2C-P uses a novel metric, stability radius in RL, that measures the\nvulnerability of RL algorithms. Experiments on multiple deep RL agents and\nmultiple environments show that our poisoning algorithm successfully prevents\nagents from learning a good policy or teaches the agents to converge to a\ntarget policy, with a limited attacking budget.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 01:43:30 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 22:24:47 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 16:09:16 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Sun", "Yanchao", ""], ["Huo", "Da", ""], ["Huang", "Furong", ""]]}, {"id": "2009.00858", "submitter": "Gadekallu Thippa Reddy", "authors": "Natarajan Deepa, Quoc-Viet Pham, Dinh C. Nguyen, Sweta Bhattacharya, B\n  Prabadevi, Thippa Reddy Gadekallu, Praveen Kumar Reddy Maddikunta, Fang Fang,\n  Pubudu N. Pathirana", "title": "A Survey on Blockchain for Big Data: Approaches, Opportunities, and\n  Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data has generated strong interest in various scientific and engineering\ndomains over the last few years. Despite many advantages and applications,\nthere are many challenges in big data to be tackled for better quality of\nservice, e.g., big data analytics, big data management, and big data privacy\nand security. Blockchain with its decentralization and security nature has the\ngreat potential to improve big data services and applications. In this article,\nwe provide a comprehensive survey on blockchain for big data, focusing on\nup-to-date approaches, opportunities, and future directions. First, we present\na brief overview of blockchain and big data as well as the motivation behind\ntheir integration. Next, we survey various blockchain services for big data,\nincluding blockchain for secure big data acquisition, data storage, data\nanalytics, and data privacy preservation. Then, we review the state-of-the-art\nstudies on the use of blockchain for big data applications in different\nvertical domains such as smart city, smart healthcare, smart transportation,\nand smart grid. For a better understanding, some representative blockchain-big\ndata projects are also presented and analyzed. Finally, challenges and future\ndirections are discussed to further drive research in this promising area.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:14:45 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 06:14:13 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Deepa", "Natarajan", ""], ["Pham", "Quoc-Viet", ""], ["Nguyen", "Dinh C.", ""], ["Bhattacharya", "Sweta", ""], ["Prabadevi", "B", ""], ["Gadekallu", "Thippa Reddy", ""], ["Maddikunta", "Praveen Kumar Reddy", ""], ["Fang", "Fang", ""], ["Pathirana", "Pubudu N.", ""]]}, {"id": "2009.00951", "submitter": "Samuel Blake T", "authors": "Sam Blake", "title": "Embedded Blockchains: A Synthesis of Blockchains, Spread Spectrum\n  Watermarking, Perceptual Hashing & Digital Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a scheme for detecting manipulated audio and\nvideo. The scheme is a synthesis of blockchains, encrypted spread spectrum\nwatermarks, perceptual hashing and digital signatures, which we call an\nEmbedded Blockchain. Within this scheme, we use the blockchain for its data\nstructure of a cryptographically linked list, cryptographic hashing for\nabsolute comparisons, perceptual hashing for flexible comparisons, digital\nsignatures for proof of ownership, and encrypted spread spectrum watermarking\nto embed the blockchain into the background noise of the media. So each media\nrecording has its own unique blockchain, with each block holding information\ndescribing the media segment. The problem of verifying the integrity of the\nmedia is recast to traversing the blockchain, block-by-block, and\nsegment-by-segment of the media. If any chain is broken, the difference in the\ncomputed and extracted perceptual hash is used to estimate the level of\nmanipulation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 11:08:43 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 00:46:47 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 01:55:59 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Blake", "Sam", ""]]}, {"id": "2009.00960", "submitter": "Chen Ma", "authors": "Chen Ma, Li Chen, Jun-Hai Yong", "title": "Simulating Unknown Target Models for Query-Efficient Black-box Attacks", "comments": "Accepted at CVPR 2021. Code and models are available at\n  https://github.com/machanic/SimulatorAttack", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many adversarial attacks have been proposed to investigate the security\nissues of deep neural networks. In the black-box setting, current model\nstealing attacks train a substitute model to counterfeit the functionality of\nthe target model. However, the training requires querying the target model.\nConsequently, the query complexity remains high, and such attacks can be\ndefended easily. This study aims to train a generalized substitute model called\n\"Simulator\", which can mimic the functionality of any unknown target model. To\nthis end, we build the training data with the form of multiple tasks by\ncollecting query sequences generated during the attacks of various existing\nnetworks. The learning process uses a mean square error-based\nknowledge-distillation loss in the meta-learning to minimize the difference\nbetween the Simulator and the sampled networks. The meta-gradients of this loss\nare then computed and accumulated from multiple tasks to update the Simulator\nand subsequently improve generalization. When attacking a target model that is\nunseen in training, the trained Simulator can accurately simulate its\nfunctionality using its limited feedback. As a result, a large fraction of\nqueries can be transferred to the Simulator, thereby reducing query complexity.\nResults of the comprehensive experiments conducted using the CIFAR-10,\nCIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach\nreduces query complexity by several orders of magnitude compared to the\nbaseline method. The implementation source code is released at\nhttps://github.com/machanic/SimulatorAttack.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 11:30:40 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 14:01:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ma", "Chen", ""], ["Chen", "Li", ""], ["Yong", "Jun-Hai", ""]]}, {"id": "2009.01020", "submitter": "Samuel Steffen", "authors": "Nick Baumann, Samuel Steffen, Benjamin Bichsel, Petar Tsankov, Martin\n  Vechev", "title": "zkay v0.2: Practical Data Privacy for Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work introduces zkay, a system for specifying and enforcing data\nprivacy in smart contracts. While the original prototype implementation of zkay\n(v0.1) demonstrates the feasibility of the approach, its proof-of-concept\nimplementation suffers from severe limitations such as insecure encryption and\nlack of important language features.\n  In this report, we present zkay v0.2, which addresses its predecessor's\nlimitations. The new implementation significantly improves security, usability,\nmodularity, and performance of the system. In particular, zkay v0.2 supports\nstate-of-the-art asymmetric and hybrid encryption, introduces many new language\nfeatures (such as function calls, private control flow, and extended type\nsupport), allows for different zk-SNARKs backends, and reduces both compilation\ntime and on-chain costs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 12:51:21 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 06:42:12 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Baumann", "Nick", ""], ["Steffen", "Samuel", ""], ["Bichsel", "Benjamin", ""], ["Tsankov", "Petar", ""], ["Vechev", "Martin", ""]]}, {"id": "2009.01030", "submitter": "Haiwei Wu", "authors": "Haiwei Wu and Jiantao Zhou", "title": "Privacy Leakage of SIFT Features via Deep Generative Model based Image\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical applications, e.g., content based image retrieval and object\nrecognition, heavily rely on the local features extracted from the query image.\nAs these local features are usually exposed to untrustworthy parties, the\nprivacy leakage problem of image local features has received increasing\nattention in recent years. In this work, we thoroughly evaluate the privacy\nleakage of Scale Invariant Feature Transform (SIFT), which is one of the most\nwidely-used image local features. We first consider the case that the adversary\ncan fully access the SIFT features, i.e., both the SIFT descriptors and the\ncoordinates are available. We propose a novel end-to-end, coarse-to-fine deep\ngenerative model for reconstructing the latent image from its SIFT features.\nThe designed deep generative model consists of two networks, where the first\none attempts to learn the structural information of the latent image by\ntransforming from SIFT features to Local Binary Pattern (LBP) features, while\nthe second one aims to reconstruct the pixel values guided by the learned LBP.\nCompared with the state-of-the-art algorithms, the proposed deep generative\nmodel produces much improved reconstructed results over three public datasets.\nFurthermore, we address more challenging cases that only partial SIFT features\n(either SIFT descriptors or coordinates) are accessible to the adversary. It is\nshown that, if the adversary can only have access to the SIFT descriptors while\nnot their coordinates, then the modest success of reconstructing the latent\nimage can be achieved for highly-structured images (e.g., faces) and would fail\nin general settings. In addition, the latent image can be reconstructed with\nreasonably good quality solely from the SIFT coordinates. Our results would\nsuggest that the privacy leakage problem can be largely avoided if the SIFT\ncoordinates can be well protected.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 12:59:12 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wu", "Haiwei", ""], ["Zhou", "Jiantao", ""]]}, {"id": "2009.01098", "submitter": "Qiongxiu Li", "authors": "Qiongxiu Li, Jaron Skovsted Gundersen, Richard Heusdens and Mads\n  Gr{\\ae}sb{\\o}ll Christensen", "title": "Privacy-Preserving Distributed Processing: Metrics, Bounds, and\n  Algorithms", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preserving distributed processing has recently attracted considerable\nattention. It aims to design solutions for conducting signal processing tasks\nover networks in a decentralized fashion without violating privacy. Many\nalgorithms can be adopted to solve this problem such as differential privacy,\nsecure multiparty computation, and the recently proposed distributed\noptimization based subspace perturbation. However, how these algorithms relate\nto each other is not fully explored yet. In this paper, we therefore first\npropose information-theoretic metrics based on mutual information. Using the\nproposed metrics, we are able to compare and relate a number of existing\nwell-known algorithms. We then derive a lower bound on individual privacy that\ngives insights on the nature of the problem. To validate the above claims, we\ninvestigate a concrete example and compare a number of state-of-the-art\napproaches in terms of different aspects such as output utility, individual\nprivacy and algorithm robustness against the number of corrupted parties, using\nnot only theoretical analysis but also numerical validation. Finally, we\ndiscuss and provide principles for designing appropriate algorithms for\ndifferent applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:19:07 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Li", "Qiongxiu", ""], ["Gundersen", "Jaron Skovsted", ""], ["Heusdens", "Richard", ""], ["Christensen", "Mads Gr\u00e6sb\u00f8ll", ""]]}, {"id": "2009.01101", "submitter": "Mohammad Ghafari", "authors": "Mohammadreza Hazhirpasand, Mohammad Ghafari, Oscar Nierstrasz", "title": "Java Cryptography Uses in the Wild", "comments": "The ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement (ESEM) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [Background] Previous research has shown that developers commonly misuse\ncryptography APIs. [Aim] We have conducted an exploratory study to find out how\ncrypto APIs are used in open-source Java projects, what types of misuses exist,\nand why developers make such mistakes. [Method] We used a static analysis tool\nto analyze hundreds of open-source Java projects that rely on Java Cryptography\nArchitecture, and manually inspected half of the analysis results to assess the\ntool results. We also contacted the maintainers of these projects by creating\nan issue on the GitHub repository of each project, and discussed the misuses\nwith developers. [Results] We learned that 85% of Cryptography APIs are\nmisused, however, not every misuse has severe consequences. Developer feedback\nshowed that security caveats in the documentation of crypto APIs are rare,\ndevelopers may overlook misuses that originate in third-party code, and the\ncontext where a Crypto API is used should be taken into account. [Conclusion]\nWe conclude that using Crypto APIs is still problematic for developers but\nblindly blaming them for such misuses may lead to erroneous conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:22:54 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Hazhirpasand", "Mohammadreza", ""], ["Ghafari", "Mohammad", ""], ["Nierstrasz", "Oscar", ""]]}, {"id": "2009.01109", "submitter": "Radu Tudor Ionescu", "authors": "Cezara Benegui, Radu Tudor Ionescu", "title": "Adversarial Attacks on Deep Learning Systems for User Identification\n  based on Motion Sensors", "comments": "Extended version (12 pages, 1 figure) of our paper (9 pages, 1\n  figure) accepted at ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the time being, mobile devices employ implicit authentication mechanisms,\nnamely, unlock patterns, PINs or biometric-based systems such as fingerprint or\nface recognition. While these systems are prone to well-known attacks, the\nintroduction of an explicit and unobtrusive authentication layer can greatly\nenhance security. In this study, we focus on deep learning methods for explicit\nauthentication based on motion sensor signals. In this scenario, attackers\ncould craft adversarial examples with the aim of gaining unauthorized access\nand even restraining a legitimate user to access his mobile device. To our\nknowledge, this is the first study that aims at quantifying the impact of\nadversarial attacks on machine learning models used for user identification\nbased on motion sensors. To accomplish our goal, we study multiple methods for\ngenerating adversarial examples. We propose three research questions regarding\nthe impact and the universality of adversarial examples, conducting relevant\nexperiments in order to answer our research questions. Our empirical results\ndemonstrate that certain adversarial example generation methods are specific to\nthe attacked classification model, while others tend to be generic. We thus\nconclude that deep neural networks trained for user identification tasks based\non motion sensors are subject to a high percentage of misclassification when\ngiven adversarial input.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:35:05 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 20:11:32 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Benegui", "Cezara", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2009.01110", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas, Bingli Liao, Takahiro Kanzaki", "title": "Perceptual Deep Neural Networks: Adversarial Robustness through Input\n  Recreation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have shown that albeit highly accurate, models learned\nby machines, differently from humans, have many weaknesses. However, humans'\nperception is also fundamentally different from machines, because we do not see\nthe signals which arrive at the retina but a rather complex recreation of them.\nIn this paper, we explore how machines could recreate the input as well as\ninvestigate the benefits of such an augmented perception. In this regard, we\npropose Perceptual Deep Neural Networks ($\\varphi$DNN) which also recreate\ntheir own input before further processing. The concept is formalized\nmathematically and two variations of it are developed (one based on inpainting\nthe whole image and the other based on a noisy resized super resolution\nrecreation). Experiments reveal that $\\varphi$DNNs and their adversarial\ntraining variations can increase the robustness substantially, surpassing both\nstate-of-the-art defenses and pre-processing types of defenses in 100% of the\ntests. $\\varphi$DNNs are shown to scale well to bigger image sizes, keeping a\nsimilar high accuracy throughout; while the state-of-the-art worsen up to 35%.\nMoreover, the recreation process intentionally corrupts the input image.\nInterestingly, we show by ablation tests that corrupting the input is, although\ncounter-intuitive, beneficial. Thus, $\\varphi$DNNs reveal that input recreation\nhas strong benefits for artificial neural networks similar to biological ones,\nshedding light into the importance of purposely corrupting the input as well as\npioneering an area of perception models based on GANs and autoencoders for\nrobust recognition in artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:36:36 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 09:39:09 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 08:58:13 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 10:36:03 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Liao", "Bingli", ""], ["Kanzaki", "Takahiro", ""]]}, {"id": "2009.01120", "submitter": "Ahmad Hazimeh", "authors": "Ahmad Hazimeh, Adrian Herrera, Mathias Payer", "title": "Magma: A Ground-Truth Fuzzing Benchmark", "comments": "To appear in the Proceedings of the ACM on Measurement and Analysis\n  of Computing Systems (POMACS), Vol. 4, No. 3, Article 49", "journal-ref": null, "doi": "10.1145/3428334", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High scalability and low running costs have made fuzz testing the de facto\nstandard for discovering software bugs. Fuzzing techniques are constantly being\nimproved in a race to build the ultimate bug-finding tool. However, while\nfuzzing excels at finding bugs in the wild, evaluating and comparing fuzzer\nperformance is challenging due to the lack of metrics and benchmarks. For\nexample, crash count, perhaps the most commonly-used performance metric, is\ninaccurate due to imperfections in deduplication techniques. Additionally, the\nlack of a unified set of targets results in ad hoc evaluations that hinder fair\ncomparison.\n  We tackle these problems by developing Magma, a ground-truth fuzzing\nbenchmark that enables uniform fuzzer evaluation and comparison. By introducing\nreal bugs into real software, Magma allows for the realistic evaluation of\nfuzzers against a broad set of targets. By instrumenting these bugs, Magma also\nenables the collection of bug-centric performance metrics independent of the\nfuzzer. Magma is an open benchmark consisting of seven targets that perform a\nvariety of input manipulations and complex computations, presenting a challenge\nto state-of-the-art fuzzers.\n  We evaluate seven widely-used mutation-based fuzzers (AFL, AFLFast, AFL++,\nFairFuzz, MOpt-AFL, honggfuzz, and SymCC-AFL) against Magma over 200,000\nCPU-hours. Based on the number of bugs reached, triggered, and detected, we\ndraw conclusions about the fuzzers' exploration and detection capabilities.\nThis provides insight into fuzzer performance evaluation, highlighting the\nimportance of ground truth in performing more accurate and meaningful\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:52:51 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:57:32 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Hazimeh", "Ahmad", ""], ["Herrera", "Adrian", ""], ["Payer", "Mathias", ""]]}, {"id": "2009.01122", "submitter": "Carlos Novo", "authors": "Carlos Novo and Ricardo Morla (University of Porto and INESC TEC)", "title": "Flow-based detection and proxy-based evasion of encrypted malware C2\n  traffic", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": "10.1145/3411508.3421379", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art deep learning techniques are known to be vulnerable to\nevasion attacks where an adversarial sample is generated from a malign sample\nand misclassified as benign. Detection of encrypted malware command and control\ntraffic based on TCP/IP flow features can be framed as a learning task and is\nthus vulnerable to evasion attacks. However, unlike e.g. in image processing\nwhere generated adversarial samples can be directly mapped to images, going\nfrom flow features to actual TCP/IP packets requires crafting the sequence of\npackets, with no established approach for such crafting and a limitation on the\nset of modifiable features that such crafting allows. In this paper we discuss\nlearning and evasion consequences of the gap between generated and crafted\nadversarial samples. We exemplify with a deep neural network detector trained\non a public C2 traffic dataset, white-box adversarial learning, and a\nproxy-based approach for crafting longer flows. Our results show 1) the high\nevasion rate obtained by using generated adversarial samples on the detector\ncan be significantly reduced when using crafted adversarial samples; 2)\nrobustness against adversarial samples by model hardening varies according to\nthe crafting approach and corresponding set of modifiable features that the\nattack allows for; 3) incrementally training hardened models with adversarial\nsamples can produce a level playing field where no detector is best against all\nattacks and no attack is best against all detectors, in a given set of attacks\nand detectors. To the best of our knowledge this is the first time that level\nplaying field feature set- and iteration-hardening are analyzed in encrypted C2\nmalware traffic detection.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 15:00:41 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Novo", "Carlos", "", "University of Porto and INESC TEC"], ["Morla", "Ricardo", "", "University of Porto and INESC TEC"]]}, {"id": "2009.01144", "submitter": "Shweta Shinde", "authors": "Shweta Shinde and Jinhua Cui and Satyaki Sen and Pinghai Yuan and\n  Prateek Saxena", "title": "Binary Compatibility For SGX Enclaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enclaves, such as those enabled by Intel SGX, offer a powerful hardware\nisolation primitive for application partitioning. To become universally usable\non future commodity OSes, enclave designs should offer compatibility with\nexisting software. In this paper, we draw attention to 5 design decisions in\nSGX that create incompatibility with existing software. These represent\nconcrete starting points, we hope, for improvements in future TEEs. Further,\nwhile many prior works have offered partial forms of compatibility, we present\nthe first attempt to offer binary compatibility with existing software on SGX.\nWe present Ratel, a system that enables a dynamic binary translation engine\ninside SGX enclaves on Linux. Through the lens of Ratel, we expose the\nfundamental trade-offs between performance and complete mediation on the\nOS-enclave interface, which are rooted in the aforementioned 5 SGX design\nrestrictions. We report on an extensive evaluation of Ratel on over 200\nprograms, including micro-benchmarks and real applications such as Linux\nutilities.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 15:43:11 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Shinde", "Shweta", ""], ["Cui", "Jinhua", ""], ["Sen", "Satyaki", ""], ["Yuan", "Pinghai", ""], ["Saxena", "Prateek", ""]]}, {"id": "2009.01220", "submitter": "Anamay Chaturvedi", "authors": "Anamay Chaturvedi, Huy Nguyen, Eric Xu", "title": "Differentially private $k$-means clustering via exponential mechanism\n  and max cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new $(\\epsilon_p, \\delta_p)$-differentially private algorithm\nfor the $k$-means clustering problem. Given a dataset in Euclidean space, the\n$k$-means clustering problem requires one to find $k$ points in that space such\nthat the sum of squares of Euclidean distances between each data point and its\nclosest respective point among the $k$ returned is minimised. Although there\nexist privacy-preserving methods with good theoretical guarantees to solve this\nproblem [Balcan et al., 2017; Kaplan and Stemmer, 2018], in practice it is seen\nthat it is the additive error which dictates the practical performance of these\nmethods. By reducing the problem to a sequence of instances of maximum coverage\non a grid, we are able to derive a new method that achieves lower additive\nerror then previous works. For input datasets with cardinality $n$ and diameter\n$\\Delta$, our algorithm has an $O(\\Delta^2 (k \\log^2 n\n\\log(1/\\delta_p)/\\epsilon_p + k\\sqrt{d \\log(1/\\delta_p)}/\\epsilon_p))$ additive\nerror whilst maintaining constant multiplicative error. We conclude with some\nexperiments and find an improvement over previously implemented work for this\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:52:54 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Chaturvedi", "Anamay", ""], ["Nguyen", "Huy", ""], ["Xu", "Eric", ""]]}, {"id": "2009.01265", "submitter": "Irippuge Milinda Perera", "authors": "Shailesh Bavadekar, Andrew Dai, John Davis, Damien Desfontaines, Ilya\n  Eckstein, Katie Everett, Alex Fabrikant, Gerardo Flores, Evgeniy Gabrilovich,\n  Krishna Gadepalli, Shane Glass, Rayman Huang, Chaitanya Kamath, Dennis Kraft,\n  Akim Kumok, Hinali Marfatia, Yael Mayer, Benjamin Miller, Adam Pearce,\n  Irippuge Milinda Perera, Venky Ramachandran, Karthik Raman, Thomas Roessler,\n  Izhak Shafran, Tomer Shekel, Charlotte Stanton, Jacob Stimes, Mimi Sun,\n  Gregory Wellenius, Masrour Zoghi", "title": "Google COVID-19 Search Trends Symptoms Dataset: Anonymization Process\n  Description (version 1.0)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report describes the aggregation and anonymization process applied to\nthe initial version of COVID-19 Search Trends symptoms dataset (published at\nhttps://goo.gle/covid19symptomdataset on September 2, 2020), a publicly\navailable dataset that shows aggregated, anonymized trends in Google searches\nfor symptoms (and some related topics). The anonymization process is designed\nto protect the daily symptom search activity of every user with\n$\\varepsilon$-differential privacy for $\\varepsilon$ = 1.68.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:03:43 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Bavadekar", "Shailesh", ""], ["Dai", "Andrew", ""], ["Davis", "John", ""], ["Desfontaines", "Damien", ""], ["Eckstein", "Ilya", ""], ["Everett", "Katie", ""], ["Fabrikant", "Alex", ""], ["Flores", "Gerardo", ""], ["Gabrilovich", "Evgeniy", ""], ["Gadepalli", "Krishna", ""], ["Glass", "Shane", ""], ["Huang", "Rayman", ""], ["Kamath", "Chaitanya", ""], ["Kraft", "Dennis", ""], ["Kumok", "Akim", ""], ["Marfatia", "Hinali", ""], ["Mayer", "Yael", ""], ["Miller", "Benjamin", ""], ["Pearce", "Adam", ""], ["Perera", "Irippuge Milinda", ""], ["Ramachandran", "Venky", ""], ["Raman", "Karthik", ""], ["Roessler", "Thomas", ""], ["Shafran", "Izhak", ""], ["Shekel", "Tomer", ""], ["Stanton", "Charlotte", ""], ["Stimes", "Jacob", ""], ["Sun", "Mimi", ""], ["Wellenius", "Gregory", ""], ["Zoghi", "Masrour", ""]]}, {"id": "2009.01281", "submitter": "Couvreur Alain", "authors": "Alain Couvreur and Hugues Randriambololona", "title": "Algebraic geometry codes and some applications", "comments": "Survey chapter to appear in \"A Concise Encyclopedia of Coding\n  Theory\", W.C. Huffman, J.-L. Kim, and P. Sole' Eds., CRC Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.AG math.IT math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article surveys the development of the theory of algebraic geometry\ncodes since their discovery in the late 70's. We summarize the major results on\nvarious problems such as: asymptotic parameters, improved estimates on the\nminimum distance, and decoding algorithms. In addition, we present various\nmodern applications of these codes such as public-key cryptography, algebraic\ncomplexity theory, multiparty computation or distributed storage.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:25:30 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Couvreur", "Alain", ""], ["Randriambololona", "Hugues", ""]]}, {"id": "2009.01341", "submitter": "Jorge Pe\\~na Queralta", "authors": "Jorge Pe\\~na Queralta, Li Qingqing, Eduardo Castell\\'o Ferrer, Tomi\n  Westerlund", "title": "Secure Encoded Instruction Graphs for End-to-End Data Validation in\n  Autonomous Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous robots become increasingly ubiquitous, more attention is being\npaid to the security of robotic operation. Autonomous robots can be seen as\ncyber-physical systems that transverse the virtual realm and operate in the\nhuman dimension. As a consequence, securing the operation of autonomous robots\ngoes beyond securing data, from sensor input to mission instructions, towards\nsecuring the interaction with their environment. There is a lack of research\ntowards methods that would allow a robot to ensure that both its sensors and\nactuators are operating correctly without external feedback. This paper\nintroduces a robotic mission encoding method that serves as an end-to-end\nvalidation framework for autonomous robots. In particular, we put our framework\ninto practice with a proof of concept describing a novel map encoding method\nthat allows robots to navigate an objective environment with almost-zero a\npriori knowledge of it, and to validate operational instructions. We also\ndemonstrate the applicability of our framework through experiments with real\nrobots for two different map encoding methods. The encoded maps inherit all the\nadvantages of traditional landmark-based navigation, with the addition of\ncryptographic hashes that enable end-to-end information validation. This\nend-to-end validation can be applied to virtually any aspect of robotic\noperation where there is a predefined set of operations or instructions given\nto the robot.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 21:03:13 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Queralta", "Jorge Pe\u00f1a", ""], ["Qingqing", "Li", ""], ["Ferrer", "Eduardo Castell\u00f3", ""], ["Westerlund", "Tomi", ""]]}, {"id": "2009.01368", "submitter": "Biswadeep Chakraborty", "authors": "Biswadeep Chakraborty, Dinil Mon Divakaran, Ido Nevat, Gareth W.\n  Peters, Mohan Gurusamy", "title": "Cost-aware Feature Selection for IoT Device Classification", "comments": "33 pages, 9 figures", "journal-ref": "Internet of Things Journal 2021", "doi": "10.1109/JIOT.2021.3051480", "report-no": null, "categories": "cs.NI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification of IoT devices into different types is of paramount\nimportance, from multiple perspectives, including security and privacy aspects.\nRecent works have explored machine learning techniques for fingerprinting (or\nclassifying) IoT devices, with promising results. However, existing works have\nassumed that the features used for building the machine learning models are\nreadily available or can be easily extracted from the network traffic; in other\nwords, they do not consider the costs associated with feature extraction. In\nthis work, we take a more realistic approach, and argue that feature extraction\nhas a cost, and the costs are different for different features. We also take a\nstep forward from the current practice of considering the misclassification\nloss as a binary value, and make a case for different losses based on the\nmisclassification performance. Thereby, and more importantly, we introduce the\nnotion of risk for IoT device classification. We define and formulate the\nproblem of cost-aware IoT device classification. This being a combinatorial\noptimization problem, we develop a novel algorithm to solve it in a fast and\neffective way using the Cross-Entropy (CE) based stochastic optimization\ntechnique. Using traffic of real devices, we demonstrate the capability of the\nCE based algorithm in selecting features with minimal risk of misclassification\nwhile keeping the cost for feature extraction within a specified limit.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 22:16:11 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 17:45:18 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 18:48:57 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Chakraborty", "Biswadeep", ""], ["Divakaran", "Dinil Mon", ""], ["Nevat", "Ido", ""], ["Peters", "Gareth W.", ""], ["Gurusamy", "Mohan", ""]]}, {"id": "2009.01534", "submitter": "Yossi Adi", "authors": "Shahar Segal, Yossi Adi, Benny Pinkas, Carsten Baum, Chaya Ganesh,\n  Joseph Keshet", "title": "Fairness in the Eyes of the Data: Certifying Machine-Learning Models", "comments": "Accepted to AIES-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework that allows to certify the fairness degree of a model\nbased on an interactive and privacy-preserving test. The framework verifies any\ntrained model, regardless of its training process and architecture. Thus, it\nallows us to evaluate any deep learning model on multiple fairness definitions\nempirically. We tackle two scenarios, where either the test data is privately\navailable only to the tester or is publicly known in advance, even to the model\ncreator. We investigate the soundness of the proposed approach using\ntheoretical analysis and present statistical guarantees for the interactive\ntest. Finally, we provide a cryptographic technique to automate fairness\ntesting and certified inference with only black-box access to the model at hand\nwhile hiding the participants' sensitive data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:22:39 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 09:03:42 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 07:57:06 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Segal", "Shahar", ""], ["Adi", "Yossi", ""], ["Pinkas", "Benny", ""], ["Baum", "Carsten", ""], ["Ganesh", "Chaya", ""], ["Keshet", "Joseph", ""]]}, {"id": "2009.01604", "submitter": "Hooman Alavizadeh", "authors": "Hootan Alavizadeh, Hooman Alavizadeh, Julian Jang-Jaccard", "title": "Cyber Situation Awareness Monitoring and Proactive Response for\n  Enterprises on the Cloud", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cloud model allows many enterprises able to outsource computing resources\nat an affordable price without having to commit the expense upfront. Although\nthe cloud providers are responsible for the security of the cloud, there are\nstill many security concerns due to inherently complex model the cloud\nproviders operate on (e.g.,multi-tenancy). In addition, the enterprises whose\nservices have migrated into the cloud have a preference for their own\ncybersecurity situation awareness capability on top of the security mechanisms\nprovided by the cloud providers. In this way, the enterprises can monitor the\nperformance of the security offerings of the cloud and have a choice to decide\nand select potential response strategies more appropriate to the enterprise in\nthe presence of the attack where the defense provided by the cloud doesn't work\nfor them. However, some response strategies, such as Moving Target Defense\n(MTD) techniques shown to be effective to secure cloud, cannot be deployed by\nthe enterprise themselves. In this paper, we propose a framework that enables\nbetter collaboration between enterprises and cloud providers. Our proposed\nframework, which offers more in-depth security analysis based on the set of\nmost advanced security metrics, allows the security experts of the enterprise\nto obtain better situational awareness in the cloud. With better and more\neffective situation awareness of cloud security, our framework can support\nbetter decision making and further allows to deploy more appropriate threat\nresponses to protect the outsourced resources. We also propose a secure\nprotocol which can facilitate more secure communication between the enterprises\nand cloud provider. Using our proposed secure protocol, which is based on\nauthentication and key exchange mechanism, the enterprises can send a secure\nrequest to the cloud provider to perform a selected defensive strategy.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 12:21:18 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Alavizadeh", "Hootan", ""], ["Alavizadeh", "Hooman", ""], ["Jang-Jaccard", "Julian", ""]]}, {"id": "2009.01631", "submitter": "Riccardo Longo", "authors": "Michele Battagliola and Riccardo Longo and Alessio Meneghetti and\n  Massimiliano Sala", "title": "A Provably-Unforgeable Threshold EdDSA with an Offline Recovery Party", "comments": "25 pages. arXiv admin note: substantial text overlap with\n  arXiv:2007.04036", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $(t,n)$-threshold signature scheme enables distributed signing among $n$\nplayers such that any subset of size at least $t$ can sign, whereas any subset\nwith fewer players cannot. The goal is to produce threshold digital signatures\nthat are compatible with an existing centralized signature scheme. Starting\nfrom the threshold scheme for the ECDSA signature due to Battagliola et al., we\npresent the first protocol that supports EdDSA multi-party signatures with an\noffline participant during the key-generation phase, without relying on a\ntrusted third party. Under standard assumptions we prove our scheme secure\nagainst adaptive malicious adversaries. Furthermore we show how our security\nnotion can be strengthen when considering a rushing adversary. We discuss the\nresiliency of the recovery in the presence of a malicious party. Using a\nclassical game-based argument, we prove that if there is an adversary capable\nof forging the scheme with non-negligible probability, then we can build a\nforger for the centralized EdDSA scheme with non-negligible probability.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 15:36:21 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Battagliola", "Michele", ""], ["Longo", "Riccardo", ""], ["Meneghetti", "Alessio", ""], ["Sala", "Massimiliano", ""]]}, {"id": "2009.01698", "submitter": "Radek O\\v{s}lej\\v{s}ek", "authors": "Michal Beran and Frantisek Hrdina and Daniel Kouril and Radek Oslejsek\n  and Kristina Zakopcanova", "title": "Exploratory Analysis of File System Metadata for Rapid Investigation of\n  Security Incidents", "comments": null, "journal-ref": null, "doi": "10.1109/VizSec51108.2020.00008", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Investigating cybersecurity incidents requires in-depth knowledge from the\nanalyst. Moreover, the whole process is demanding due to the vast data volumes\nthat need to be analyzed. While various techniques exist nowadays to help with\nparticular tasks of the analysis, the process as a whole still requires a lot\nof manual activities and expert skills. We propose an approach that allows the\nanalysis of disk snapshots more efficiently and with lower demands on expert\nknowledge. Following a user-centered design methodology, we implemented an\nanalytical tool to guide analysts during security incident investigations. The\nviability of the solution was validated by an evaluation conducted with members\nof different security teams.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 14:23:22 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 08:11:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Beran", "Michal", ""], ["Hrdina", "Frantisek", ""], ["Kouril", "Daniel", ""], ["Oslejsek", "Radek", ""], ["Zakopcanova", "Kristina", ""]]}, {"id": "2009.01729", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja,\n  Naser Damer, Christoph Busch", "title": "MIPGAN -- Generating Strong and High Quality Morphing Attacks Using\n  Identity Prior Driven GAN", "comments": "Revised version. Submitted to IEEE T-BIOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face morphing attacks target to circumvent Face Recognition Systems (FRS) by\nemploying face images derived from multiple data subjects (e.g., accomplices\nand malicious actors). Morphed images can be verified against contributing data\nsubjects with a reasonable success rate, given they have a high degree of\nfacial resemblance. The success of morphing attacks is directly dependent on\nthe quality of the generated morph images. We present a new approach for\ngenerating strong attacks extending our earlier framework for generating face\nmorphs. We present a new approach using an Identity Prior Driven Generative\nAdversarial Network, which we refer to as MIPGAN (Morphing through Identity\nPrior driven GAN). The proposed MIPGAN is derived from the StyleGAN with a\nnewly formulated loss function exploiting perceptual quality and identity\nfactor to generate a high quality morphed facial image with minimal artefacts\nand with high resolution. We demonstrate the proposed approach's applicability\nto generate strong morphing attacks by evaluating its vulnerability against\nboth commercial and deep learning based Face Recognition System (FRS) and\ndemonstrate the success rate of attacks. Extensive experiments are carried out\nto assess the FRS's vulnerability against the proposed morphed face generation\ntechnique on three types of data such as digital images, re-digitized (printed\nand scanned) images, and compressed images after re-digitization from newly\ngenerated MIPGAN Face Morph Dataset. The obtained results demonstrate that the\nproposed approach of morph generation poses a high threat to FRS.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 15:08:38 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 08:47:41 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 11:02:03 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhang", "Haoyu", ""], ["Venkatesh", "Sushma", ""], ["Ramachandra", "Raghavendra", ""], ["Raja", "Kiran", ""], ["Damer", "Naser", ""], ["Busch", "Christoph", ""]]}, {"id": "2009.01864", "submitter": "Christopher Gorham", "authors": "Christopher L Gorham", "title": "Developing Enterprise Cyber Situational Awareness", "comments": null, "journal-ref": "IJMIT (2020) Volume 12, Number 3", "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic will focus on the U.S. Department of Defense strategy towards\nimproving their network security defenses for the department and the steps they\nhave taken at the agency level where components under DOD such as The Defense\nInformation Systems Agency are working towards adding tools that provides\nadditional capabilities in the cyber space. This approach will be analyzed to\ndetermine if DOD goals address any of their vulnerabilities towards protecting\ntheir networks. One of the agencies under the DOD umbrella called The Defense\nInformation Systems Agency provides DOD a template on how to build a network\nthat relies upon layers of security to help it combat cyber attacks against its\nnetwork. Whether that provides an effective solution to DOD remains a question\ndue to the many components that operate under its direction. Managing these\nnetworks is the principle responsibilities for the Department of Defense.\nNevertheless, it does demonstrates that there are tools available to help DOD\nbuild an strong enterprise cyber network of situational awareness that\nstrengthens the ability to protect their network infrastructure.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 18:16:06 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Gorham", "Christopher L", ""]]}, {"id": "2009.01867", "submitter": "Sheng Lin", "authors": "Sheng Lin, Chenghong Wang, Hongjia Li, Jieren Deng, Yanzhi Wang,\n  Caiwen Ding", "title": "ESMFL: Efficient and Secure Models for Federated Learning", "comments": "7 pages, 3 figures, accepted by NeurIPS Workshop 2020, SpicyFL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Deep Neural Networks are widely applied to various domains.\nHowever, massive data collection required for deep neural network reveals the\npotential privacy issues and also consumes large mounts of communication\nbandwidth. To address these problems, we propose a privacy-preserving method\nfor the federated learning distributed system, operated on Intel Software Guard\nExtensions, a set of instructions that increase the security of application\ncode and data. Meanwhile, the encrypted models make the transmission overhead\nlarger. Hence, we reduce the commutation cost by sparsification and it can\nachieve reasonable accuracy with different model architectures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 18:27:32 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 19:45:00 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Lin", "Sheng", ""], ["Wang", "Chenghong", ""], ["Li", "Hongjia", ""], ["Deng", "Jieren", ""], ["Wang", "Yanzhi", ""], ["Ding", "Caiwen", ""]]}, {"id": "2009.01869", "submitter": "Zahra Tarkhani", "authors": "Zahra Tarkhani, Anil Madhavapeddy", "title": "Enclave-Aware Compartmentalization and Secure Sharing with Sirius", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware-assisted trusted execution environments (TEEs) are critical building\nblocks of many modern applications. However, they have a one-way isolation\nmodel that introduces a semantic gap between a TEE and its outside world. This\nlack of information causes an ever-increasing set of attacks on TEE-enabled\napplications that exploit various insecure interactions with the host OSs,\napplications, or other enclaves. We introduce Sirius, the first\ncompartmentalization framework that achieves strong isolation and secure\nsharing in TEE-assisted applications by controlling the dataflows within\nprimary kernel objects (e.g. threads, processes, address spaces, files,\nsockets, pipes) in both the secure and normal worlds. Sirius replaces ad-hoc\ninteractions in current TEE systems with a principled approach that adds strong\ninter- and intra-address space isolation and effectively eliminates a wide\nrange of attacks. We evaluate Sirius on ARM platforms and find that it is\nlightweight ($\\approx 15K$ LoC) and only adds $\\approx 10.8\\%$ overhead to\nenable TEE support on applications such as httpd, and improves the performance\nof existing TEE-enabled applications such as the Darknet ML framework and ARM's\nLibDDSSec by $0.05\\%-5.6\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 18:30:02 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 11:52:52 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 14:19:30 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Tarkhani", "Zahra", ""], ["Madhavapeddy", "Anil", ""]]}, {"id": "2009.01884", "submitter": "Ulrich A\\\"ivodji", "authors": "Ulrich A\\\"ivodji, Alexandre Bolot, S\\'ebastien Gambs", "title": "Model extraction from counterfactual explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-hoc explanation techniques refer to a posteriori methods that can be\nused to explain how black-box machine learning models produce their outcomes.\nAmong post-hoc explanation techniques, counterfactual explanations are becoming\none of the most popular methods to achieve this objective. In particular, in\naddition to highlighting the most important features used by the black-box\nmodel, they provide users with actionable explanations in the form of data\ninstances that would have received a different outcome. Nonetheless, by doing\nso, they also leak non-trivial information about the model itself, which raises\nprivacy issues. In this work, we demonstrate how an adversary can leverage the\ninformation provided by counterfactual explanations to build high-fidelity and\nhigh-accuracy model extraction attacks. More precisely, our attack enables the\nadversary to build a faithful copy of a target model by accessing its\ncounterfactual explanations. The empirical evaluation of the proposed attack on\nblack-box models trained on real-world datasets demonstrates that they can\nachieve high-fidelity and high-accuracy extraction even under low query\nbudgets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 19:02:55 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["A\u00efvodji", "Ulrich", ""], ["Bolot", "Alexandre", ""], ["Gambs", "S\u00e9bastien", ""]]}, {"id": "2009.01887", "submitter": "Priyanka Singh", "authors": "Priyanka Singh", "title": "Robust Homomorphic Video Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has been weaponized to carry out cybercriminal activities at an\nunprecedented pace. The rising concerns for preserving the privacy of personal\ndata while availing modern tools and technologies is alarming. End-to-end\nencrypted solutions are in demand for almost all commercial platforms. On one\nside, it seems imperative to provide such solutions and give people trust to\nreliably use these platforms. On the other side, this creates a huge\nopportunity to carry out unchecked cybercrimes. This paper proposes a robust\nvideo hashing technique, scalable and efficient in chalking out matches from an\nenormous bulk of videos floating on these commercial platforms. The video hash\nis validated to be robust to common manipulations like scaling, corruptions by\nnoise, compression, and contrast changes that are most probable to happen\nduring transmission. It can also be transformed into the encrypted domain and\nwork on top of encrypted videos without deciphering. Thus, it can serve as a\npotential forensic tool that can trace the illegal sharing of videos without\nknowing the underlying content. Hence, it can help preserve privacy and combat\ncybercrimes such as revenge porn, hateful content, child abuse, or illegal\nmaterial propagated in a video.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 19:09:44 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Singh", "Priyanka", ""]]}, {"id": "2009.01931", "submitter": "Alejandro Cohen", "authors": "Alejandro Cohen, Rafael G. L. D'Oliveira, Salman Salamatian, Muriel\n  Medard", "title": "Network Coding-Based Post-Quantum Cryptography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hybrid universal network-coding cryptosystem (HUNCC) to\nobtain secure post-quantum cryptography at high communication rates. The secure\nnetwork-coding scheme we offer is hybrid in the sense that it combines\ninformation-theory security with public-key cryptography. In addition, the\nscheme is general and can be applied to any communication network, and to any\npublic-key cryptosystem. Our hybrid scheme is based on the information\ntheoretic notion of individual secrecy, which traditionally relies on the\nassumption that an eavesdropper can only observe a subset of the communication\nlinks between the trusted parties - an assumption that is often challenging to\nenforce. For this setting, several code constructions have been developed,\nwhere the messages are linearly mixed before transmission over each of the\npaths in a way that guarantees that an adversary which observes only a subset\nhas sufficient uncertainty about each individual message.\n  Instead, in this paper, we take a computational viewpoint, and construct a\ncoding scheme in which an arbitrary secure cryptosystem is utilized on a subset\nof the links, while a pre-processing similar to the one in individual security\nis utilized. Under this scheme, we demonstrate 1) a computational security\nguarantee for an adversary which observes the entirety of the links 2) an\ninformation theoretic security guarantee for an adversary which observes a\nsubset of the links, and 3) information rates which approach the capacity of\nthe network and greatly improve upon the current solutions.\n  A perhaps surprising consequence of our scheme is that, to guarantee a\ncomputational security level b, it is sufficient to encrypt a single link using\na computational post-quantum scheme. In addition, the information rate\napproaches 1 as the number of communication links increases.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 21:19:58 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Cohen", "Alejandro", ""], ["D'Oliveira", "Rafael G. L.", ""], ["Salamatian", "Salman", ""], ["Medard", "Muriel", ""]]}, {"id": "2009.01939", "submitter": "Blake Anderson", "authors": "Blake Anderson and David McGrew", "title": "Accurate TLS Fingerprinting using Destination Context and Knowledge\n  Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network fingerprinting is used to identify applications, provide insight into\nnetwork traffic, and detect malicious activity. With the broad adoption of TLS,\ntraditional fingerprinting techniques that rely on clear-text data are no\nlonger viable. TLS-specific techniques have been introduced that create a\nfingerprint string from carefully selected data features in the client_hello to\nfacilitate process identification before data is exchanged. Unfortunately, this\napproach fails in practice because hundreds of processes can map to the same\nfingerprint string. We solve this problem by presenting a TLS fingerprinting\nsystem that makes use of the destination address, port, and server name in\naddition to a carefully constructed fingerprint string. The destination context\nis used to disambiguate the set of processes that match a fingerprint string by\napplying a weighted naive Bayes classifier, resulting in far greater\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 22:00:42 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Anderson", "Blake", ""], ["McGrew", "David", ""]]}, {"id": "2009.02011", "submitter": "Chen Chen", "authors": "Chen Chen, Anrin Chakraborti and Radu Sion", "title": "PEARL: Plausibly Deniable Flash Translation Layer using WOM coding", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When adversaries are powerful enough to coerce users to reveal encryption\nkeys, encryption alone becomes insufficient for data protection. Plausible\ndeniability (PD) mechanisms resolve this by enabling users to hide the mere\nexistence of sensitive data, often by providing plausible \"cover texts\" or\n\"public data volumes\" hosted on the same device.\n  Unfortunately, with the increasing prevalence of (NAND) flash as a\nhigh-performance cost-effective storage medium, PD becomes even more\nchallenging in the presence of realistic adversaries who can usually access a\ndevice at multiple points in time (\"multi-snapshot\"). This is because\nread/write operations to flash do not result in intuitive corresponding changes\nto the underlying device state. The problem is further compounded by the fact\nthat this behavior is mostly proprietary. For example, in a majority of\ncommercially-available flash devices, an issued delete or overwrite operation\nfrom the upper layers almost certainly won't result in an actual immediate\nerase of the underlying flash cells.\n  To address these challenges, we designed a new class of write-once memory\n(WOM) codes to store hidden bits in the same physical locations as other public\nbits. This is made possible by the inherent nature of NAND flash and the\npossibility of issuing multiple writes to target cells that have not previous\nbeen written to in existing pages.\n  We designed PEARL, a general-purpose Flash Translation Layer (FTL) that\nallows users to plausibly deniably store hidden data in NAND flash devices. We\nimplemented and evaluated PEARL on a widely used simulator FlashSim (Kim et al.\n2019). PEARL performs well on real-world workloads, comparably to non-PD\nbaselines. PEARL is the first system that achieves strong plausible deniability\nfor NAND flash devices, secure against realistic multi-snapshot adversaries.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 05:01:24 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Chen", "Chen", ""], ["Chakraborti", "Anrin", ""], ["Sion", "Radu", ""]]}, {"id": "2009.02030", "submitter": "Hooman Alavizadeh", "authors": "Hooman Alavizadeh, Samin Aref, Dong Seong Kim, Julian Jang-Jaccard", "title": "Evaluating the Security and Economic Effects of Moving Target Defense\n  Techniques on the Cloud", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving Target Defense (MTD) is a proactive security mechanism which changes\nthe attack surface aiming to confuse attackers. Cloud computing leverages MTD\ntechniques to enhance cloud security posture against cyber threats. While many\nMTD techniques have been applied to cloud computing, there has not been a joint\nevaluation of the effectiveness of MTD techniques with respect to security and\neconomic metrics. In this paper, we first introduce mathematical definitions\nfor the combination of three MTD techniques: \\emph{Shuffle}, \\emph{Diversity},\nand \\emph{Redundancy}. Then, we utilize four security metrics including system\nrisk, attack cost, return on attack, and reliability to assess the\neffectiveness of the combined MTD techniques applied to large-scale cloud\nmodels. Secondly, we focus on a specific context based on a cloud model for\nE-health applications to evaluate the effectiveness of the MTD techniques using\nsecurity and economic metrics. We introduce (1) a strategy to effectively\ndeploy Shuffle MTD technique using a virtual machine placement technique and\n(2) two strategies to deploy Diversity MTD technique through operating system\ndiversification. As deploying Diversity incurs cost, we formulate the\n\\emph{Optimal Diversity Assignment Problem (O-DAP)} and solve it as a binary\nlinear programming model to obtain the assignment which maximizes the expected\nnet benefit.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 07:19:07 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 04:06:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Alavizadeh", "Hooman", ""], ["Aref", "Samin", ""], ["Kim", "Dong Seong", ""], ["Jang-Jaccard", "Julian", ""]]}, {"id": "2009.02114", "submitter": "Alexander Barabanov", "authors": "Alexander Barabanov, Denis Makrushin", "title": "Authentication and authorization in microservice-based systems: survey\n  of architecture patterns", "comments": "The work was done in Advanced Software Technology Laboratory, Huawei.\n  It is planned to be published in \"Voprosy kiberbezopasnosti\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context. Service-oriented architecture and its microservice-based approach\nincrease an attack surface of applications. Exposed microservices become a\npivot point for advanced persistent threats and completely change the threat\nlandscape. Correctly implemented authentication and authorization architecture\npatterns are basis of any software maturity program. Objective. The aim of this\nstudy is to provide a helpful resource to application security architect and\ndevelopers on existing architecture patterns to implement authentication and\nauthorization in microservices-based systems. Method. In this paper, we conduct\na review of major electronic databases and libraries as well as security\nstandards and presentations at the major security conferences. Results. In this\nwork based on research papers and major security conferences presentations\nanalysis we identified industry best practices in authentication and\nauthorization patterns and its applicability depending on environment\ncharacteristic. For each described patterns we reviewed its advantages and\ndisadvantages that could be used as decision-making criteria for application\nsecurity architects during architecture design phase.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:19:54 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Barabanov", "Alexander", ""], ["Makrushin", "Denis", ""]]}, {"id": "2009.02122", "submitter": "Sebastian Mazza", "authors": "Sebastian Mazza, Daniel Patel, Ivan Viola", "title": "Homomorphic-Encrypted Volume Rendering", "comments": "Paper accepted for presentation at IEEE VIS (SciVis) 2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030436", "report-no": null, "categories": "cs.CR cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computationally demanding tasks are typically calculated in dedicated data\ncenters, and real-time visualizations also follow this trend. Some rendering\ntasks, however, require the highest level of confidentiality so that no other\nparty, besides the owner, can read or see the sensitive data. Here we present a\ndirect volume rendering approach that performs volume rendering directly on\nencrypted volume data by using the homomorphic Paillier encryption algorithm.\nThis approach ensures that the volume data and rendered image are\nuninterpretable to the rendering server. Our volume rendering pipeline\nintroduces novel approaches for encrypted-data compositing, interpolation, and\nopacity modulation, as well as simple transfer function design, where each of\nthese routines maintains the highest level of privacy. We present performance\nand memory overhead analysis that is associated with our privacy-preserving\nscheme. Our approach is open and secure by design, as opposed to secure through\nobscurity. Owners of the data only have to keep their secure key confidential\nto guarantee the privacy of their volume data and the rendered images. Our work\nis, to our knowledge, the first privacy-preserving remote volume-rendering\napproach that does not require that any server involved be trustworthy; even in\ncases when the server is compromised, no sensitive data will be leaked to a\nforeign party.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:54:48 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 17:09:27 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Mazza", "Sebastian", ""], ["Patel", "Daniel", ""], ["Viola", "Ivan", ""]]}, {"id": "2009.02137", "submitter": "Stefan More", "authors": "Lukas Alber, Stefan More, Sebastian Ramacher", "title": "Short-Lived Forward-Secure Delegation for TLS", "comments": "This is the full version of a paper which appears in 2020 Cloud\n  Computing Security Workshop (CCSW '20), November 9, 2020, Virtual Event, USA,\n  ACM", "journal-ref": null, "doi": "10.1145/3411495.3421362", "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On today's Internet, combining the end-to-end security of TLS with Content\nDelivery Networks (CDNs) while ensuring the authenticity of connections results\nin a challenging delegation problem. When CDN servers provide content, they\nhave to authenticate themselves as the origin server to establish a valid\nend-to-end TLS connection with the client. In standard TLS, the latter requires\naccess to the secret key of the server. To curb this problem, multiple\nworkarounds exist to realize a delegation of the authentication.\n  In this paper, we present a solution that renders key sharing unnecessary and\nreduces the need for workarounds. By adapting identity-based signatures to this\nsetting, our solution offers short-lived delegations. Additionally, by enabling\nforward-security, existing delegations remain valid even if the server's secret\nkey leaks. We provide an implementation of the scheme and discuss integration\ninto a TLS stack. In our evaluation, we show that an efficient implementation\nincurs less overhead than a typical network round trip. Thereby, we propose an\nalternative approach to current delegation practices on the web.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 12:28:03 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 15:12:14 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Alber", "Lukas", ""], ["More", "Stefan", ""], ["Ramacher", "Sebastian", ""]]}, {"id": "2009.02206", "submitter": "Hadi Mardani Kamali", "authors": "Hadi Mardani Kamali, Kimia Zamiri Azar, Houman Homayoun, Avesta Sasan", "title": "InterLock: An Intercorrelated Logic and Routing Locking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a canonical prune-and-SAT (CP&SAT) attack for\nbreaking state-of-the-art routing-based obfuscation techniques. In the CP&SAT\nattack, we first encode the key-programmable routing blocks (keyRBs) based on\nan efficient SAT encoding mechanism suited for detailed routing constraints,\nand then efficiently re-encode and reduce the CNF corresponded to the keyRB\nusing a bounded variable addition (BVA) algorithm. In the CP&SAT attack, this\nis done before subjecting the circuit to the SAT attack. We illustrate that\nthis encoding and BVA-based pre-processing significantly reduces the size of\nthe CNF corresponded to the routing-based obfuscated circuit, in the result of\nwhich we observe 100% success rate for breaking prior art routing-based\nobfuscation techniques. Further, we propose a new intercorrelated logic and\nrouting locking technique, or in short InterLock, as a countermeasure to\nmitigate the CP&SAT attack. In Interlock, in addition to hiding the\nconnectivity, a part of the logic (gates) in the selected timing paths are also\nimplemented in the keyRB(s). We illustrate that when the logic gates are\ntwisted with keyRBs, the BVA could not provide any advantage as a\npre-processing step. Our experimental results show that, by using InterLock,\nwith only three 8$\\times$8 or only two 16x16 keyRBs (twisted with actual logic\ngates), the resilience against existing attacks as well as our new proposed\nCP&SAT attack would be guaranteed while, on average, the delay/area overhead is\nless than 10% for even medium-size benchmark circuits.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 14:02:33 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Kamali", "Hadi Mardani", ""], ["Azar", "Kimia Zamiri", ""], ["Homayoun", "Houman", ""], ["Sasan", "Avesta", ""]]}, {"id": "2009.02208", "submitter": "Hadi Mardani Kamali", "authors": "Kimia Zamiri Azar, Hadi Mardani Kamali, Houman Homayoun, Avesta Sasan", "title": "NNgSAT: Neural Network guided SAT Attack on Logic Locked Complex\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The globalization of the IC supply chain has raised many security threats,\nespecially when untrusted parties are involved. This has created a demand for a\ndependable logic obfuscation solution to combat these threats. Amongst a wide\nrange of threats and countermeasures on logic obfuscation in the 2010s decade,\nthe Boolean satisfiability (SAT) attack, or one of its derivatives, could break\nalmost all state-of-the-art logic obfuscation countermeasures. However, in some\ncases, particularly when the logic locked circuits contain complex structures,\nsuch as big multipliers, large routing networks, or big tree structures, the\nlogic locked circuit is hard-to-be-solved for the SAT attack. Usage of these\nstructures for obfuscation may lead a strong defense, as many SAT solvers fail\nto handle such complexity. However, in this paper, we propose a\nneural-network-guided SAT attack (NNgSAT), in which we examine the capability\nand effectiveness of a message-passing neural network (MPNN) for solving these\ncomplex structures (SAT-hard instances). In NNgSAT, after being trained as a\nclassifier to predict SAT/UNSAT on a SAT problem (NN serves as a SAT solver),\nthe neural network is used to guide/help the actual SAT solver for finding the\nSAT assignment(s). By training NN on conjunctive normal forms (CNFs)\ncorresponded to a dataset of logic locked circuits, as well as fine-tuning the\nconfidence rate of the NN prediction, our experiments show that NNgSAT could\nsolve 93.5% of the logic locked circuits containing complex structures within a\nreasonable time, while the existing SAT attack cannot proceed the attack flow\nin them.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 14:07:17 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Azar", "Kimia Zamiri", ""], ["Kamali", "Hadi Mardani", ""], ["Homayoun", "Houman", ""], ["Sasan", "Avesta", ""]]}, {"id": "2009.02286", "submitter": "Hadi Mansourifar", "authors": "Hadi Mansourifar, Weidong Shi", "title": "Vulnerability of Face Recognition Systems Against Composite Face\n  Reconstruction Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rounding confidence score is considered trivial but a simple and effective\ncountermeasure to stop gradient descent based image reconstruction attacks.\nHowever, its capability in the face of more sophisticated reconstruction\nattacks is an uninvestigated research area. In this paper, we prove that, the\nface reconstruction attacks based on composite faces can reveal the\ninefficiency of rounding policy as countermeasure. We assume that, the attacker\ntakes advantage of face composite parts which helps the attacker to get access\nto the most important features of the face or decompose it to the independent\nsegments. Afterwards, decomposed segments are exploited as search parameters to\ncreate a search path to reconstruct optimal face. Face composition parts enable\nthe attacker to violate the privacy of face recognition models even with a\nblind search. However, we assume that, the attacker may take advantage of\nrandom search to reconstruct the target face faster. The algorithm is started\nwith random composition of face parts as initial face and confidence score is\nconsidered as fitness value. Our experiments show that, since the rounding\npolicy as countermeasure can't stop the random search process, current face\nrecognition systems are extremely vulnerable against such sophisticated\nattacks. To address this problem, we successfully test Face Detection Score\nFiltering (FDSF) as a countermeasure to protect the privacy of training data\nagainst proposed attack.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 03:37:51 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Mansourifar", "Hadi", ""], ["Shi", "Weidong", ""]]}, {"id": "2009.02326", "submitter": "Mojan Javaheripi", "authors": "Mojan Javaheripi, Mohammad Samragh, Gregory Fields, Tara Javidi,\n  Farinaz Koushanfar", "title": "CLEANN: Accelerated Trojan Shield for Embedded Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3400302.3415671", "report-no": null, "categories": "cs.LG cs.AR cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose CLEANN, the first end-to-end framework that enables online\nmitigation of Trojans for embedded Deep Neural Network (DNN) applications. A\nTrojan attack works by injecting a backdoor in the DNN while training; during\ninference, the Trojan can be activated by the specific backdoor trigger. What\ndifferentiates CLEANN from the prior work is its lightweight methodology which\nrecovers the ground-truth class of Trojan samples without the need for labeled\ndata, model retraining, or prior assumptions on the trigger or the attack. We\nleverage dictionary learning and sparse approximation to characterize the\nstatistical behavior of benign data and identify Trojan triggers. CLEANN is\ndevised based on algorithm/hardware co-design and is equipped with specialized\nhardware to enable efficient real-time execution on resource-constrained\nembedded platforms. Proof of concept evaluations on CLEANN for the\nstate-of-the-art Neural Trojan attacks on visual benchmarks demonstrate its\ncompetitive advantage in terms of attack resiliency and execution overhead.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 05:29:38 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Javaheripi", "Mojan", ""], ["Samragh", "Mohammad", ""], ["Fields", "Gregory", ""], ["Javidi", "Tara", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "2009.02377", "submitter": "Yudi Huang", "authors": "Yudi Huang, Ting He, Nilanjan Ray Chaudhuri, Thomas La Porta", "title": "Power Grid State Estimation under General Cyber-Physical Attacks", "comments": "Assumptions clarified; proof revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective defense against cyber-physical attacks in power grid requires the\ncapability of accurate damage assessment within the attacked area. While some\nsolutions have been proposed to recover the phase angles and the link status\n(i.e., breaker status) within the attacked area, existing solutions made the\nlimiting assumption that the grid stays connected after the attack. To fill\nthis gap, we study the problem of recovering the phase angles and the link\nstatus under a general cyber-physical attack that may partition the grid into\nislands. To this end, we (i) show that the existing solutions and recovery\nconditions still hold if the post-attack power injections in the attacked area\nare known, and (ii) propose a linear programming-based algorithm that can\nperfectly recover the link status under certain conditions even if the\npost-attack power injections are unknown. Our numerical evaluations based on\nthe Polish power grid demonstrate that the proposed algorithm is highly\naccurate in localizing failed links once the phase angles are known.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 20:01:19 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 03:59:36 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Huang", "Yudi", ""], ["He", "Ting", ""], ["Chaudhuri", "Nilanjan Ray", ""], ["La Porta", "Thomas", ""]]}, {"id": "2009.02412", "submitter": "Johann Knechtel", "authors": "Mohammed Nabeel, Mohammed Ashraf, Satwik Patnaik, Vassos Soteriou,\n  Ozgur Sinanoglu, Johann Knechtel", "title": "2.5D Root of Trust: Secure System-Level Integration of Untrusted\n  Chiplets", "comments": "[v2] Dedicated, after acceptance and publication, in memory of the\n  late Vassos Soteriou. Besides, scaled down some figures for smaller overall\n  file size", "journal-ref": null, "doi": "10.1109/TC.2020.3020777", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dedicated, after acceptance and publication, in memory of the late Vassos\nSoteriou. For the first time, we leverage the 2.5D interposer technology to\nestablish system-level security in the face of hardware- and software-centric\nadversaries. More specifically, we integrate chiplets (i.e., third-party hard\nintellectual property of complex functionality, like microprocessors) using a\nsecurity-enforcing interposer. Such hardware organization provides a robust\n2.5D root of trust for trustworthy, yet powerful and flexible, computation\nsystems. The security paradigms for our scheme, employed firmly by design and\nconstruction, are: 1) stringent physical separation of trusted from untrusted\ncomponents, and 2) runtime monitoring. The system-level activities of all\nuntrusted commodity chiplets are checked continuously against security policies\nvia physically separated security features. Aside from the security promises,\nthe good economics of outsourced supply chains are still maintained; the system\nvendor is free to procure chiplets from the open market, while only producing\nthe interposer and assembling the 2.5D system oneself. We showcase our scheme\nusing the Cortex-M0 core and the AHB-Lite bus by ARM, building a secure 64-core\nsystem with shared memories. We evaluate our scheme through hardware\nsimulation, considering different threat scenarios. Finally, we devise a\nphysical-design flow for 2.5D systems, based on commercial-grade design tools,\nto demonstrate and evaluate our 2.5D root of trust.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 22:31:58 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 21:12:21 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Nabeel", "Mohammed", ""], ["Ashraf", "Mohammed", ""], ["Patnaik", "Satwik", ""], ["Soteriou", "Vassos", ""], ["Sinanoglu", "Ozgur", ""], ["Knechtel", "Johann", ""]]}, {"id": "2009.02561", "submitter": "Chao Li", "authors": "Chao Li, Balaji Palanisamy, Runhua Xu, Jian Wang, Jiqiang Liu", "title": "NF-Crowd: Nearly-free Blockchain-based Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in distributed ledger technologies are rapidly driving the rise\nof decentralized crowdsourcing systems on top of open smart contract platforms\nlike Ethereum. While decentralized blockchain-based crowdsourcing provides\nnumerous benefits compared to centralized solutions, current implementations of\ndecentralized crowdsourcing suffer from fundamental scalability limitations by\nrequiring all participants to pay a small transaction fee every time they\ninteract with the blockchain. This increases the cost of using decentralized\ncrowdsourcing solutions, resulting in a total payment that could be even higher\nthan the price charged by centralized crowdsourcing platforms. This paper\nproposes a novel suite of protocols called NF-Crowd that resolves the\nscalability issue by reducing the lower bound of the total cost of a\ndecentralized crowdsourcing project to O(1). NF-Crowd is a highly reliable\nsolution for scaling decentralized crowdsourcing. We prove that as long as\nparticipants of a project powered by NF-Crowd are rational, the O(1) lower\nbound of cost could be reached regardless of the scale of the crowd. We also\ndemonstrate that as long as at least one participant of a project powered by\nNF-Crowd is honest, the project cannot be aborted and the results are\nguaranteed to be correct. We design NF-Crowd protocols for a representative\ntype of project named crowdsourcing contest with open community review\n(CC-OCR). We implement the protocols over the Ethereum official test network.\nOur results demonstrate that NF-Crowd protocols can reduce the cost of running\na CC-OCR project to less than $2 regardless of the scale of the crowd,\nproviding a significant cost benefit in adopting decentralized crowdsourcing\nsolutions.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 16:25:47 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 02:56:44 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Li", "Chao", ""], ["Palanisamy", "Balaji", ""], ["Xu", "Runhua", ""], ["Wang", "Jian", ""], ["Liu", "Jiqiang", ""]]}, {"id": "2009.02608", "submitter": "Haekyu Park", "authors": "Nilaksh Das, Haekyu Park, Zijie J. Wang, Fred Hohman, Robert Firstman,\n  Emily Rogers, Duen Horng Chau", "title": "Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural\n  Networks", "comments": "This paper is accepted at IEEE VIS'20 Short Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are now commonly used in many domains. However,\nthey are vulnerable to adversarial attacks: carefully crafted perturbations on\ndata inputs that can fool a model into making incorrect predictions. Despite\nsignificant research on developing DNN attack and defense techniques, people\nstill lack an understanding of how such attacks penetrate a model's internals.\nWe present Bluff, an interactive system for visualizing, characterizing, and\ndeciphering adversarial attacks on vision-based neural networks. Bluff allows\npeople to flexibly visualize and compare the activation pathways for benign and\nattacked images, revealing mechanisms that adversarial attacks employ to\ninflict harm on a model. Bluff is open-sourced and runs in modern web browsers.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 22:08:35 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 02:38:11 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Das", "Nilaksh", ""], ["Park", "Haekyu", ""], ["Wang", "Zijie J.", ""], ["Hohman", "Fred", ""], ["Firstman", "Robert", ""], ["Rogers", "Emily", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2009.02626", "submitter": "Rong Su", "authors": "Rong Su", "title": "On Decidability of Existence of Nonblocking Supervisors Resilient to\n  Smart Sensor Attacks", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity of discrete event systems (DES) has been gaining more and more\nattention recently, due to its high relevance to the so-called 4th industrial\nrevolution that heavily relies on data communication among networked systems.\nOne key challenge is how to ensure system resilience to sensor and/or actuator\nattacks, which may tamper data integrity and service availability. In this\npaper we focus on some key decidability issues related to smart sensor attacks.\nWe first present a sufficient and necessary condition that ensures the\nexistence of a smart sensor attack, which reveals a novel demand-supply\nrelationship between an attacker and a controlled plant, represented as a set\nof risky pairs. Each risky pair consists of a damage string desired by the\nattacker and an observable sequence feasible in the supervisor such that the\nlatter induces a sequence of control patterns, which allows the damage string\nto happen. It turns out that each risky pair can induce a smart weak sensor\nattack. Next, we show that, when the plant, supervisor and damage language are\nregular, it is computationally feasible to remove all such risky pairs from the\nplant behaviour, via a genuine encoding scheme, upon which we are able to\nestablish our key result that the existence of a nonblocking supervisor\nresilient to smart sensor attacks is decidable. To the best of our knowledge,\nthis is the first result of its kind in the DES literature on cyber attacks.\nThe proposed decision process renders a specific synthesis procedure that\nguarantees to compute a resilient supervisor whenever it exists, which so far\nhas not been achieved in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 01:53:10 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Su", "Rong", ""]]}, {"id": "2009.02643", "submitter": "Qinghua Lu", "authors": "Weishan Zhang, Qinghua Lu, Qiuyu Yu, Zhaotong Li, Yue Liu, Sin Kit Lo,\n  Shiping Chen, Xiwei Xu, Liming Zhu", "title": "Blockchain-based Federated Learning for Device Failure Detection in\n  Industrial IoT", "comments": "accepted by IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Device failure detection is one of most essential problems in industrial\ninternet of things (IIoT). However, in conventional IIoT device failure\ndetection, client devices need to upload raw data to the central server for\nmodel training, which might lead to disclosure of sensitive business data.\nTherefore, in this paper, to ensure client data privacy, we propose a\nblockchain-based federated learning approach for device failure detection in\nIIoT. First, we present a platform architecture of blockchain-based federated\nlearning systems for failure detection in IIoT, which enables verifiable\nintegrity of client data. In the architecture, each client periodically creates\na Merkle tree in which each leaf node represents a client data record, and\nstores the tree root on a blockchain. Further, to address the data\nheterogeneity issue in IIoT failure detection, we propose a novel centroid\ndistance weighted federated averaging (CDW\\_FedAvg) algorithm taking into\naccount the distance between positive class and negative class of each client\ndataset. In addition, to motivate clients to participate in federated learning,\na smart contact based incentive mechanism is designed depending on the size and\nthe centroid distance of client data used in local model training. A prototype\nof the proposed architecture is implemented with our industry partner, and\nevaluated in terms of feasibility, accuracy and performance. The results show\nthat the approach is feasible, and has satisfactory accuracy and performance.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 04:03:13 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 11:26:20 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhang", "Weishan", ""], ["Lu", "Qinghua", ""], ["Yu", "Qiuyu", ""], ["Li", "Zhaotong", ""], ["Liu", "Yue", ""], ["Lo", "Sin Kit", ""], ["Chen", "Shiping", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""]]}, {"id": "2009.02651", "submitter": "Ying Zhao", "authors": "Zengsheng Zhong, Shuirun Wei, Yeting Xu, Ying Zhao, Fangfang Zhou,\n  Feng Luo, and Ronghua Shi", "title": "SilkViser:A Visual Explorer of Blockchain-based Cryptocurrency\n  Transaction Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many blockchain-based cryptocurrencies provide users with online blockchain\nexplorers for viewing online transaction data. However, traditional blockchain\nexplorers mostly present transaction information in textual and tabular forms.\nSuch forms make understanding cryptocurrency transaction mechanisms difficult\nfor novice users (NUsers). They are also insufficiently informative for\nexperienced users (EUsers) to recognize advanced transaction information. This\nstudy introduces a new online cryptocurrency transaction data viewing tool\ncalled SilkViser. Guided by detailed scenario and requirement analyses, we\ncreate a series of appreciating visualization designs, such as paper\nledger-inspired block and blockchain visualizations and ancient copper\ncoin-inspired transaction visualizations, to help users understand\ncryptocurrency transaction mechanisms and recognize advanced transaction\ninformation. We also provide a set of lightweight interactions to facilitate\neasy and free data exploration. Moreover, a controlled user study is conducted\nto quantitatively evaluate the usability and effectiveness of SilkViser.\nResults indicate that SilkViser can satisfy the requirements of NUsers and\nEUsers. Our visualization designs can compensate for the inexperience of NUsers\nin data viewing and attract potential users to participate in cryptocurrency\ntransactions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 05:54:11 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhong", "Zengsheng", ""], ["Wei", "Shuirun", ""], ["Xu", "Yeting", ""], ["Zhao", "Ying", ""], ["Zhou", "Fangfang", ""], ["Luo", "Feng", ""], ["Shi", "Ronghua", ""]]}, {"id": "2009.02667", "submitter": "Bertram Poettering", "authors": "Jeroen Pijnenburg, Bertram Poettering", "title": "Efficiency Improvements for Encrypt-to-Self", "comments": "this is the full version of content that appears at CYSARM'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Pijnenburg and Poettering (ESORICS'20) explores the novel\ncryptographic Encrypt-to-Self primitive that is dedicated to use cases of\nsymmetric encryption where encryptor and decryptor coincide. The primitive is\nenvisioned to be useful whenever a memory-bounded computing device is required\nto encrypt some data with the aim of temporarily depositing it on an untrusted\nstorage device. While the new primitive protects the confidentiality of\npayloads as much as classic authenticated encryption primitives would do, it\nprovides considerably better authenticity guarantees: Specifically, while\nclassic solutions would completely fail in a context involving user\ncorruptions, if an encrypt-to-self scheme is used to protect the data, all\nciphertexts and messages fully remain unforgeable.\n  To instantiate their encrypt-to-self primitive, Pijnenburg et al propose a\nmode of operation of the compression function of a hash function, with a\ncarefully designed encoding function playing the central role in the\nserialization of the processed message and associated data. In the present work\nwe revisit the design of this encoding function. Without questioning its\nadequacy for securely accomplishing the encrypt-to-self job, we improve on it\nfrom a technical/implementational perspective by proposing modifications that\nalleviate certain conditions that would inevitably require implementations to\ndisrespect memory alignment restrictions imposed by the word-wise operation of\nmodern CPUs, ultimately leading to performance penalties. Our main\ncontributions are thus to propose an improved encoding function, to explain why\nit offers better performance, and to prove that it provides as much security as\nits predecessor. We finally report on our open-source implementation of the\nencrypt-to-self primitive based on the new encoding function.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 08:00:41 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Pijnenburg", "Jeroen", ""], ["Poettering", "Bertram", ""]]}, {"id": "2009.02668", "submitter": "Jalaj Upadhyay", "authors": "Jalaj Upadhyay, Sarvagya Upadhyay", "title": "A Framework for Private Matrix Analysis", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study private matrix analysis in the sliding window model where only the\nlast $W$ updates to matrices are considered useful for analysis. We give first\nefficient $o(W)$ space differentially private algorithms for spectral\napproximation, principal component analysis, and linear regression. We also\ninitiate and show efficient differentially private algorithms for two important\nvariants of principal component analysis: sparse principal component analysis\nand non-negative principal component analysis. Prior to our work, no such\nresult was known for sparse and non-negative differentially private principal\ncomponent analysis even in the static data setting. These algorithms are\nobtained by identifying sufficient conditions on positive semidefinite matrices\nformed from streamed matrices. We also show a lower bound on space required to\ncompute low-rank approximation even if the algorithm gives multiplicative\napproximation and incurs additive error. This follows via reduction to a\ncertain communication complexity problem.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 08:01:59 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Upadhyay", "Jalaj", ""], ["Upadhyay", "Sarvagya", ""]]}, {"id": "2009.02738", "submitter": "Chuanxi Chen", "authors": "Dengpan Ye, Chuanxi Chen, Changrui Liu, Hao Wang, Shunzhi Jiang", "title": "Detection Defense Against Adversarial Attacks with Saliency Map", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well established that neural networks are vulnerable to adversarial\nexamples, which are almost imperceptible on human vision and can cause the deep\nmodels misbehave. Such phenomenon may lead to severely inestimable consequences\nin the safety and security critical applications. Existing defenses are trend\nto harden the robustness of models against adversarial attacks, e.g.,\nadversarial training technology. However, these are usually intractable to\nimplement due to the high cost of re-training and the cumbersome operations of\naltering the model architecture or parameters. In this paper, we discuss the\nsaliency map method from the view of enhancing model interpretability, it is\nsimilar to introducing the mechanism of the attention to the model, so as to\ncomprehend the progress of object identification by the deep networks. We then\npropose a novel method combined with additional noises and utilize the\ninconsistency strategy to detect adversarial examples. Our experimental results\nof some representative adversarial attacks on common datasets including\nImageNet and popular models show that our method can detect all the attacks\nwith high detection success rate effectively. We compare it with the existing\nstate-of-the-art technique, and the experiments indicate that our method is\nmore general.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 13:57:17 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Ye", "Dengpan", ""], ["Chen", "Chuanxi", ""], ["Liu", "Changrui", ""], ["Wang", "Hao", ""], ["Jiang", "Shunzhi", ""]]}, {"id": "2009.02845", "submitter": "Hui Li", "authors": "Yuqiu Qian, Conghui Tan, Danhao Ding, Hui Li, Nikos Mamoulis", "title": "Fast and Secure Distributed Nonnegative Matrix Factorization", "comments": "Published in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE). This arXiv version includes the appendices with additional proofs", "journal-ref": null, "doi": "10.1109/TKDE.2020.2985964", "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has been successfully applied in\nseveral data mining tasks. Recently, there is an increasing interest in the\nacceleration of NMF, due to its high cost on large matrices. On the other hand,\nthe privacy issue of NMF over federated data is worthy of attention, since NMF\nis prevalently applied in image and text analysis which may involve leveraging\nprivacy data (e.g, medical image and record) across several parties (e.g.,\nhospitals). In this paper, we study the acceleration and security problems of\ndistributed NMF. Firstly, we propose a distributed sketched alternating\nnonnegative least squares (DSANLS) framework for NMF, which utilizes a matrix\nsketching technique to reduce the size of nonnegative least squares subproblems\nwith a convergence guarantee. For the second problem, we show that DSANLS with\nmodification can be adapted to the security setting, but only for one or\nlimited iterations. Consequently, we propose four efficient distributed NMF\nmethods in both synchronous and asynchronous settings with a security\nguarantee. We conduct extensive experiments on several real datasets to show\nthe superiority of our proposed methods. The implementation of our methods is\navailable at https://github.com/qianyuqiu79/DSANLS.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 01:12:20 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Qian", "Yuqiu", ""], ["Tan", "Conghui", ""], ["Ding", "Danhao", ""], ["Li", "Hui", ""], ["Mamoulis", "Nikos", ""]]}, {"id": "2009.02930", "submitter": "Aneet Kumar Dutta Aneet", "authors": "Aneet K. Dutta, Bhaskar Mukhoty, Sandeep K. Shukla", "title": "Unsupervised Learning Based Robust Multivariate Intrusion Detection\n  System for Cyber-Physical Systems using Low Rank Matrix", "comments": "9pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular and uninterrupted operation of critical infrastructures such as\npower, transport, communication etc. are essential for proper functioning of a\ncountry. Cyber-attacks causing disruption in critical infrastructure service in\nthe past, are considered as a significant threat. With the advancement in\ntechnology and the progress of the critical infrastructures towards IP based\ncommunication, cyber-physical systems are lucrative targets of the attackers.\nIn this paper, we propose a robust multivariate intrusion detection system\ncalled RAD for detecting attacks in the cyber-physical systems in O(d) space\nand time complexity, where d is the number parameters in the system state\nvector. The proposed Intrusion Detection System(IDS) is developed in an\nunsupervised learning setting without using labelled data denoting attacks. It\nallows a fraction of the training data to be corrupted by outliers or under\nattack, by subscribing to robust training procedure. The proposed IDS\noutperforms existing anomaly detection techniques in several real-world\ndatasets and attack scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 08:01:57 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Dutta", "Aneet K.", ""], ["Mukhoty", "Bhaskar", ""], ["Shukla", "Sandeep K.", ""]]}, {"id": "2009.02933", "submitter": "Yuanyu Zhang", "authors": "Yuanyu Zhang, Mirei Yutaka, Masahiro Sasabe and Shoji Kasahara", "title": "Attribute-Based Access Control for Smart Cities: A Smart Contract-Driven\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and reliable access control in smart cities is critical for the\nprotection of various resources for decision making and task execution.\nExisting centralized access control schemes suffer from the limitations of\nsingle point of failure, low reliability and poor scalability. This paper\ntherefore proposes a distributed and reliable access control framework for\nsmart cities by combining the blockchain smart contract technology and the\nAttribute-Based Access Control (ABAC) model. The framework consists of one\nPolicy Management Contract (PMC) for managing the ABAC policies, one Subject\nAttribute Management Contract (SAMC) for managing the attributes of subjects\n(i.e., entities accessing resources), one Object Attribute Management Contract\n(OAMC) for managing the attributes of objects (i.e., resources being accessed),\nand one Access Control Contract (ACC) for performing the access control. To\nshow the feasibility of the proposed framework, we construct a local private\nEthereum blockchain system to implement the four smart contracts and also\nconduct experiments to evaluate the monetary cost as well as to compare the\nproposed framework with an existing Access Control List (ACL)-based scheme. The\nexperimental results show that although the proposed scheme consumes more money\nthan the ACL-based scheme at the deployment stage, it introduces less monetary\ncost during the system running especially for large-scale smart cities.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 08:05:34 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhang", "Yuanyu", ""], ["Yutaka", "Mirei", ""], ["Sasabe", "Masahiro", ""], ["Kasahara", "Shoji", ""]]}, {"id": "2009.02942", "submitter": "Divyasree Ir", "authors": "Divyasree I R, Selvamani K, Riasudheen H", "title": "Detection of Colluded Black-hole and Grey-hole attacks in Cloud\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of the high-capacity network, massive storage, hardware\nvirtualization, utility computing, service-oriented architecture leads to high\naccessibility of cloud computing. The extensive usage of cloud resources causes\noodles of security controversies. Black-hole & Gray-hole attacks are the\nnotable cloud network defenseless attacks while they launched easily but\ndifficult to detect. This research work focuses on proposing an efficient\nintegrated detection method for individual and collusion attacks in cloud\ncomputing. In the individual attack detection phase, the forwarding ratio\nmetric is used for differentiating the malicious node and normal nodes. In the\ncollusion attack detection phase, the malicious nodes are manipulated the\nencounter records for escaping the detection process. To overcome this user,\nfake encounters are examined along with appearance frequency, and the number of\nmessages exploits abnormal patterns. The simulation results shown in this\nproposed system detect with better accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 08:36:39 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["R", "Divyasree I", ""], ["K", "Selvamani", ""], ["H", "Riasudheen", ""]]}, {"id": "2009.03012", "submitter": "Qinghua Lu", "authors": "Yue Liu, Qinghua Lu, Chunsheng Zhu, Qiuyu Yu", "title": "A Blockchain-based Platform Architecture for Multimedia Data Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive amounts of multimedia data (i.e., text, audio, video, graphics and\nanimation) are being generated everyday. Conventionally, multimedia data are\nmanaged by the platforms maintained by multimedia service providers, which are\ngenerally designed using centralised architecture. However, such centralised\narchitecture may lead to a single point of failure and disputes over royalties\nor other rights. It is hard to ensure the data integrity and track fulfilment\nof obligations listed on the copyright agreement. To tackle these issues, in\nthis paper, we present a blockchain-based platform architecture for multimedia\ndata management. We adopt self-sovereign identity for identity management and\ndesign a multi-level capability-based mechanism for access control. We\nimplement a proof-of-concept prototype using the proposed approach and evaluate\nit using a use case. The results show that the proposed approach is feasible\nand has scalable performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:52:51 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Liu", "Yue", ""], ["Lu", "Qinghua", ""], ["Zhu", "Chunsheng", ""], ["Yu", "Qiuyu", ""]]}, {"id": "2009.03015", "submitter": "Sahar Abdelnabi", "authors": "Sahar Abdelnabi and Mario Fritz", "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance\n  with Data Hiding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in natural language generation have introduced powerful\nlanguage models with high-quality output text. However, this raises concerns\nabout the potential misuse of such models for malicious purposes. In this\npaper, we study natural language watermarking as a defense to help better mark\nand trace the provenance of text. We introduce the Adversarial Watermarking\nTransformer (AWT) with a jointly trained encoder-decoder and adversarial\ntraining that, given an input text and a binary message, generates an output\ntext that is unobtrusively encoded with the given message. We further study\ndifferent training and inference strategies to achieve minimal changes to the\nsemantics and correctness of the input text.\n  AWT is the first end-to-end model to hide data in text by automatically\nlearning -- without ground truth -- word substitutions along with their\nlocations in order to encode the message. We empirically show that our model is\neffective in largely preserving text utility and decoding the watermark while\nhiding its presence against adversaries. Additionally, we demonstrate that our\nmethod is robust against a range of attacks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 11:01:24 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 12:21:27 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Abdelnabi", "Sahar", ""], ["Fritz", "Mario", ""]]}, {"id": "2009.03062", "submitter": "Harshal Tupsamudre", "authors": "Harshal Tupsamudre and Sachin Lodha", "title": "Passwords: Divided they Stand, United they Fall", "comments": "We propose an offline password guessing model in which attacker uses\n  information from previous breaches and surveys to divide the search space\n  into partitions. We prove that the success rate of attacker is maximum if the\n  resulting partitions are explored in decreasing order of density. We\n  demonstrate that the partition attack model is generic. This is an extended\n  version of Passwords2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, offline attacks are one of the most severe threats to password\nsecurity. These attacks have claimed millions of passwords from prominent\nwebsites including Yahoo, LinkedIn, Twitter, Sony, Adobe and many more.\nTherefore, as a preventive measure, it is necessary to gauge the offline\nguessing resistance of a password database and to help users choose secure\npasswords. The rule-based mechanisms that rely on minimum password length and\ndifferent character classes are too naive to capture the intricate human\nbehavior whereas those based on probabilistic models require the knowledge of\nan entire password distribution which is not always easy to learn. In this\npaper, we propose a space partition attack model which uses information from\nprevious leaks, surveys, attacks and other sources to divide the password\nsearch space into non-overlapping partitions and learn partition densities. We\nprove that the expected success of a partition attacker is maximum if the\nresulting partitions are explored in decreasing order of density. We show that\nthe proposed attack model is more general and various popular attack techniques\nincluding probabilistic-based, dictionary-based, grammar-based and brute-force\nare just different instances of a partition attacker. Later, we introduce bin\nattacker, another instance of a partition attacker, and measure the guessing\nresistance of real-world password databases. We demonstrate that the utilized\nsearch space is very small and as a result even a weak attacker can cause\nsufficient damage to the system. We prove that partition attacks can be\ncountered only if partition densities are uniform. We use this result and\npropose a system that thwarts partition attacker by distributing users across\ndifferent partitions. Finally, we demonstrate how some of the well-known\npassword schemes can be adapted to help users in choosing passwords from the\nsystem assigned partitions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 12:39:02 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 02:36:07 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 03:13:25 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Tupsamudre", "Harshal", ""], ["Lodha", "Sachin", ""]]}, {"id": "2009.03253", "submitter": "Andreea Buterchi", "authors": "Andreea Buterchi and Andrei Arusoaie", "title": "DApp for Rating", "comments": "Technical report, 15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "tr20-0x, UAIC FII", "categories": "cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lots of existing web applications include a component for rating internet\nresources (e.g., social media platforms include mechanisms for rating videos or\nposts). Based on the obtained rating, the most popular internet resources can\ngenerate large amounts of money from advertising. One issue here is that the\nexisting rating systems resources are entirely controlled by a single entity\n(e.g., social media platforms).\n  In this paper we present a blockchain-based decentralized application for\nrating internet resources. The proposed solution provides a transparent rating\nmechanism, since no central authority is involved and the rating operations are\nhandled by a specialised smart contract. We provide an implementation of our\nidea, where we combine existing authentication methods with blockchain specific\nfeatures so that anonymity is preserved. We show that this approach is better\nthan existing rating components present in various web applications.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:42:59 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Buterchi", "Andreea", ""], ["Arusoaie", "Andrei", ""]]}, {"id": "2009.03488", "submitter": "Jintang Li", "authors": "Jintang Li, Tao Xie, Liang Chen, Fenfang Xie, Xiangnan He, Zibin Zheng", "title": "Adversarial Attack on Large Scale Graph", "comments": "Accepted by TKDE, the codes are availiable at\n  https://github.com/EdisonLeeeee/SGAttack", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that graph neural networks (GNNs) are vulnerable\nagainst perturbations due to lack of robustness and can therefore be easily\nfooled. Currently, most works on attacking GNNs are mainly using gradient\ninformation to guide the attack and achieve outstanding performance. However,\nthe high complexity of time and space makes them unmanageable for large scale\ngraphs and becomes the major bottleneck that prevents the practical usage. We\nargue that the main reason is that they have to use the whole graph for\nattacks, resulting in the increasing time and space complexity as the data\nscale grows. In this work, we propose an efficient Simplified Gradient-based\nAttack (SGA) method to bridge this gap. SGA can cause the GNNs to misclassify\nspecific target nodes through a multi-stage attack framework, which needs only\na much smaller subgraph. In addition, we present a practical metric named\nDegree Assortativity Change (DAC) to measure the impacts of adversarial attacks\non graph data. We evaluate our attack method on four real-world graph networks\nby attacking several commonly used GNNs. The experimental results demonstrate\nthat SGA can achieve significant time and memory efficiency improvements while\nmaintaining competitive attack performance compared to state-of-art attack\ntechniques. Codes are available via: https://github.com/EdisonLeeeee/SGAttack.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 02:17:55 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 14:15:27 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Jintang", ""], ["Xie", "Tao", ""], ["Chen", "Liang", ""], ["Xie", "Fenfang", ""], ["He", "Xiangnan", ""], ["Zheng", "Zibin", ""]]}, {"id": "2009.03510", "submitter": "Boyi Liu", "authors": "Boyi Liu, Bingjie Yan, Yize Zhou, Zhixuan Liang, Cheng-Zhong Xu", "title": "FedCM: A Real-time Contribution Measurement Method for Participants in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated Learning (FL) creates an ecosystem for multiple agents to\ncollaborate on building models with data privacy consideration. The method for\ncontribution measurement of each agent in the FL system is critical for fair\ncredits allocation but few are proposed. In this paper, we develop a real-time\ncontribution measurement method FedCM that is simple but powerful. The method\ndefines the impact of each agent, comprehensively considers the current round\nand the previous round to obtain the contribution rate of each agent with\nattention aggregation. Moreover, FedCM updates contribution every round, which\nenable it to perform in real-time. Real-time is not considered by the existing\napproaches, but it is critical for FL systems to allocate computing power,\ncommunication resources, etc. Compared to the state-of-the-art method, the\nexperimental results show that FedCM is more sensitive to data quantity and\ndata quality under the premise of real-time. Furthermore, we developed\nfederated learning open-source software based on FedCM. The software has been\napplied to identify COVID-19 based on medical images.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 04:05:10 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 14:03:54 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Liu", "Boyi", ""], ["Yan", "Bingjie", ""], ["Zhou", "Yize", ""], ["Liang", "Zhixuan", ""], ["Xu", "Cheng-Zhong", ""]]}, {"id": "2009.03518", "submitter": "AKM Mubashwir Alam", "authors": "A K M Mubashwir Alam, Sagar Sharma, Keke Chen", "title": "SGX-MR: Regulating Dataflows for Protecting Access Patterns of\n  Data-Intensive SGX Applications", "comments": "To appear in Privacy Enhancing Technologies Symposium, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intel SGX has been a popular trusted execution environment (TEE) for\nprotecting the integrity and confidentiality of applications running on\nuntrusted platforms such as cloud. However, the access patterns of SGX-based\nprograms can still be observed by adversaries, which may leak important\ninformation for successful attacks. Researchers have been experimenting with\nOblivious RAM (ORAM) to address the privacy of access patterns. ORAM is a\npowerful low-level primitive that provides application-agnostic protection for\nany I/O operations, however, at a high cost. We find that some\napplication-specific access patterns, such as sequential block I/O, do not\nprovide additional information to adversaries. Others, such as sorting, can be\nreplaced with specific oblivious algorithms that are more efficient than ORAM.\nThe challenge is that developers may need to look into all the details of\napplication-specific access patterns to design suitable solutions, which is\ntime-consuming and error-prone. In this paper, we present the lightweight SGX\nbased MapReduce (SGX-MR) approach that regulates the dataflow of data-intensive\nSGX applications for easier application-level access-pattern analysis and\nprotection. It uses the MapReduce framework to cover a large class of\ndata-intensive applications, and the entire framework can be implemented with a\nsmall memory footprint. With this framework, we have examined the stages of\ndata processing, identified the access patterns that need protection, and\ndesigned corresponding efficient protection methods. Our experiments show that\nSGX-MR based applications are much more efficient than ORAM-based\nimplementations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 04:53:10 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Alam", "A K M Mubashwir", ""], ["Sharma", "Sagar", ""], ["Chen", "Keke", ""]]}, {"id": "2009.03561", "submitter": "Emiliano De Cristofaro", "authors": "Mohammad Naseri, Jamie Hayes, and Emiliano De Cristofaro", "title": "Toward Robustness and Privacy in Federated Learning: Experimenting with\n  Local and Central Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) allows multiple participants to train machine\nlearning models collaboratively by keeping their datasets local and only\nexchanging model updates. Alas, recent work highlighted several privacy and\nrobustness weaknesses in FL, presenting, respectively, membership/property\ninference and backdoor attacks. In this paper, we investigate to what extent\nDifferential Privacy (DP) can be used to protect not only privacy but also\nrobustness in FL.\n  We present a first-of-its-kind empirical evaluation of Local and Central\nDifferential Privacy (LDP/CDP) techniques in FL, assessing their feasibility\nand effectiveness. We show that both DP variants do defend against backdoor\nattacks, with varying levels of protection and utility, and overall much more\neffectively than previously proposed defenses. They also mitigate white-box\nmembership inference attacks in FL, and our work is the first to show how\neffectively; neither, however, provides viable defenses against property\ninference. Our work also provides a re-usable measurement framework to quantify\nthe trade-offs between robustness/privacy and utility in differentially private\nFL.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 07:37:23 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 15:13:53 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 08:37:16 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Naseri", "Mohammad", ""], ["Hayes", "Jamie", ""], ["De Cristofaro", "Emiliano", ""]]}, {"id": "2009.03645", "submitter": "Mohammad Sayad Haghighi", "authors": "Mohammad Sadegh Sadeghi Garmaroodi, Faezeh Farivar, Mohammad Sayad\n  Haghighi, Mahdi Aliyari Shoorehdeli, Alireza Jolfaei", "title": "Detection of Anomalies and Faults in Industrial IoT Systems by Data\n  Mining: Study of CHRIST Osmotron Water Purification System", "comments": "this paper is under review in IEEE IoTJ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 will make manufacturing processes smarter but this smartness\nrequires more environmental awareness, which in case of Industrial Internet of\nThings, is realized by the help of sensors. This article is about industrial\npharmaceutical systems and more specifically, water purification systems.\nPurified water which has certain conductivity is an important ingredient in\nmany pharmaceutical products. Almost every pharmaceutical company has a water\npurifying unit as a part of its interdependent systems. Early detection of\nfaults right at the edge can significantly decrease maintenance costs and\nimprove safety and output quality, and as a result, lead to the production of\nbetter medicines. In this paper, with the help of a few sensors and data mining\napproaches, an anomaly detection system is built for CHRIST Osmotron water\npurifier. This is a practical research with real-world data collected from\nSinaDarou Labs Co. Data collection was done by using six sensors over two-week\nintervals before and after system overhaul. This gave us normal and faulty\noperation samples. Given the data, we propose two anomaly detection approaches\nto build up our edge fault detection system. The first approach is based on\nsupervised learning and data mining e.g. by support vector machines. However,\nsince we cannot collect all possible faults data, an anomaly detection approach\nis proposed based on normal system identification which models the system\ncomponents by artificial neural networks. Extensive experiments are conducted\nwith the dataset generated in this study to show the accuracy of the\ndata-driven and model-based anomaly detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 11:31:43 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Garmaroodi", "Mohammad Sadegh Sadeghi", ""], ["Farivar", "Faezeh", ""], ["Haghighi", "Mohammad Sayad", ""], ["Shoorehdeli", "Mahdi Aliyari", ""], ["Jolfaei", "Alireza", ""]]}, {"id": "2009.03698", "submitter": "Anisa Halimi", "authors": "Anisa Halimi and Erman Ayday", "title": "Efficient Quantification of Profile Matching Risk in Social Networks", "comments": "arXiv admin note: text overlap with arXiv:2008.09608", "journal-ref": "Proceedings of the 25th European Symposium on Research in Computer\n  Security (ESORICS 2020)", "doi": null, "report-no": null, "categories": "cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymous data sharing has been becoming more challenging in today's\ninterconnected digital world, especially for individuals that have both\nanonymous and identified online activities. The most prominent example of such\ndata sharing platforms today are online social networks (OSNs). Many\nindividuals have multiple profiles in different OSNs, including anonymous and\nidentified ones (depending on the nature of the OSN). Here, the privacy threat\nis profile matching: if an attacker links anonymous profiles of individuals to\ntheir real identities, it can obtain privacy-sensitive information which may\nhave serious consequences, such as discrimination or blackmailing. Therefore,\nit is very important to quantify and show to the OSN users the extent of this\nprivacy risk. Existing attempts to model profile matching in OSNs are\ninadequate and computationally inefficient for real-time risk quantification.\nThus, in this work, we develop algorithms to efficiently model and quantify\nprofile matching attacks in OSNs as a step towards real-time privacy risk\nquantification. For this, we model the profile matching problem using a graph\nand develop a belief propagation (BP)-based algorithm to solve this problem in\na significantly more efficient and accurate way compared to the\nstate-of-the-art. We evaluate the proposed framework on three real-life\ndatasets (including data from four different social networks) and show how\nusers' profiles in different OSNs can be matched efficiently and with high\nprobability. We show that the proposed model generation has linear complexity\nin terms of number of user pairs, which is significantly more efficient than\nthe state-of-the-art (which has cubic complexity). Furthermore, it provides\ncomparable accuracy, precision, and recall compared to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 16:42:54 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Halimi", "Anisa", ""], ["Ayday", "Erman", ""]]}, {"id": "2009.03727", "submitter": "Takumi Ishiyama", "authors": "Takumi Ishiyama, Takuya Suzuki, Hayato Yamana", "title": "Highly Accurate CNN Inference Using Approximate Activation Functions\n  over Homomorphic Encryption", "comments": "Accepted at 7th International Workshop on Privacy and Security of Big\n  Data in conjunction with 2020 IEEE International Conference on Big Data (IEEE\n  BigData 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the big data era, cloud-based machine learning as a service (MLaaS) has\nattracted considerable attention. However, when handling sensitive data, such\nas financial and medical data, a privacy issue emerges, because the cloud\nserver can access clients' raw data. A common method of handling sensitive data\nin the cloud uses homomorphic encryption, which allows computation over\nencrypted data without decryption. Previous research usually adopted a\nlow-degree polynomial mapping function, such as the square function, for data\nclassification. However, this technique results in low classification accuracy.\nIn this study, we seek to improve the classification accuracy for inference\nprocessing in a convolutional neural network (CNN) while using homomorphic\nencryption. We adopt an activation function that approximates Google's Swish\nactivation function while using a fourth-order polynomial. We also adopt batch\nnormalization to normalize the inputs for the Swish function to fit the input\nrange to minimize the error. We implemented CNN inference labeling over\nhomomorphic encryption using the Microsoft's Simple Encrypted Arithmetic\nLibrary for the Cheon-Kim-Kim-Song (CKKS) scheme. The experimental evaluations\nconfirmed classification accuracies of 99.22% and 80.48% for MNIST and\nCIFAR-10, respectively, which entails 0.04% and 4.11% improvements,\nrespectively, over previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 13:20:59 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 15:09:48 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Ishiyama", "Takumi", ""], ["Suzuki", "Takuya", ""], ["Yamana", "Hayato", ""]]}, {"id": "2009.03777", "submitter": "Simson Garfinkel", "authors": "Simson L. Garfinkel and Philip Leclerc", "title": "Randomness Concerns When Deploying Differential Privacy", "comments": "12 pages plus 2 pages bibliography", "journal-ref": "19th Workshop on Privacy in the Electronic Society (WPES'20),\n  November 9, 2020, Virtual Event, USA", "doi": "10.1145/3411497.3420211", "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The U.S. Census Bureau is using differential privacy (DP) to protect\nconfidential respondent data collected for the 2020 Decennial Census of\nPopulation & Housing. The Census Bureau's DP system is implemented in the\nDisclosure Avoidance System (DAS) and requires a source of random numbers. We\nestimate that the 2020 Census will require roughly 90TB of random bytes to\nprotect the person and household tables. Although there are critical\ndifferences between cryptography and DP, they have similar requirements for\nrandomness. We review the history of random number generation on deterministic\ncomputers, including von Neumann's \"middle-square\" method, Mersenne Twister\n(MT19937) (previously the default NumPy random number generator, which we\nconclude is unacceptable for use in production privacy-preserving systems), and\nthe Linux /dev/urandom device. We also review hardware random number generator\nschemes, including the use of so-called \"Lava Lamps\" and the Intel Secure Key\nRDRAND instruction. We finally present our plan for generating random bits in\nthe Amazon Web Services (AWS) environment using AES-CTR-DRBG seeded by mixing\nbits from /dev/urandom and the Intel Secure Key RDSEED instruction, a\ncompromise of our desire to rely on a trusted hardware implementation, the\nunease of our external reviewers in trusting a hardware-only implementation,\nand the need to generate so many random bits.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 15:28:40 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Garfinkel", "Simson L.", ""], ["Leclerc", "Philip", ""]]}, {"id": "2009.03779", "submitter": "Edward Raff", "authors": "Edward Raff, Richard Zak, Gary Lopez Munoz, William Fleming, Hyrum S.\n  Anderson, Bobby Filar, Charles Nicholas, James Holt", "title": "Automatic Yara Rule Generation Using Biclustering", "comments": "to be published in the 13th ACM Workshop on Artificial Intelligence\n  and Security (AISec)", "journal-ref": null, "doi": "10.1145/3411508.3421372", "report-no": null, "categories": "cs.CR cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yara rules are a ubiquitous tool among cybersecurity practitioners and\nanalysts. Developing high-quality Yara rules to detect a malware family of\ninterest can be labor- and time-intensive, even for expert users. Few tools\nexist and relatively little work has been done on how to automate the\ngeneration of Yara rules for specific families. In this paper, we leverage\nlarge n-grams ($n \\geq 8$) combined with a new biclustering algorithm to\nconstruct simple Yara rules more effectively than currently available software.\nOur method, AutoYara, is fast, allowing for deployment on low-resource\nequipment for teams that deploy to remote networks. Our results demonstrate\nthat AutoYara can help reduce analyst workload by producing rules with useful\ntrue-positive rates while maintaining low false-positive rates, sometimes\nmatching or even outperforming human analysts. In addition, real-world testing\nby malware analysts indicates AutoYara could reduce analyst time spent\nconstructing Yara rules by 44-86%, allowing them to spend their time on the\nmore advanced malware that current tools can't handle. Code will be made\navailable at https://github.com/NeuromorphicComputationResearchProgram .\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 02:02:43 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Raff", "Edward", ""], ["Zak", "Richard", ""], ["Munoz", "Gary Lopez", ""], ["Fleming", "William", ""], ["Anderson", "Hyrum S.", ""], ["Filar", "Bobby", ""], ["Nicholas", "Charles", ""], ["Holt", "James", ""]]}, {"id": "2009.03961", "submitter": "Joseph Gardiner", "authors": "Joseph Gardiner, Awais Rashid", "title": "Technical Report: Gone in 20 Seconds -- Overview of a Password\n  Vulnerability in Siemens HMIs", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siemens produce a range of industrial human machine interface (HMI) screens\nwhich allow operators to both view information about and control physical\nprocesses. For scenarios where an operator cannot physically access the screen,\nSiemens provide the SM@rtServer features on HMIs, which when activated provides\nremote access either through their own Sm@rtClient application, or through\nthird party VNC client software. Through analysing this server, we discovered a\nlack of protection against brute-force password attacks on basic devices. On\nadvanced devices which include a brute-force protection mechanism, we\ndiscovered an attacker strategy that is able to evade the mechanism allowing\nfor unlimited password guess attempts with minimal effect on the guess rate.\nThis vulnerability has been assigned two CVEs - CVE-2020-15786 and\nCVE-2020-157867. In this report, we provide an overview of this vulnerability,\ndiscuss the impact of a successful exploitation and propose mitigations to\nprovide protection against this vulnerability. This report accompanies a demo\npresented at CPSIoTSec 2020.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 19:20:02 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Gardiner", "Joseph", ""], ["Rashid", "Awais", ""]]}, {"id": "2009.04002", "submitter": "Harrison Williams", "authors": "Harrison Williams (1), Alexander Lind (1), Kishankumar Parikh (1),\n  Matthew Hicks (1) ((1) Virginia Tech)", "title": "Silicon Dating", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to service an ever-growing base of legacy electronics, both\ngovernment and industry customers must turn to third-party brokers for\ncomponents in short supply or discontinued by the original manufacturer.\nSourcing equipment from a third party creates an opportunity for unscrupulous\ngray market suppliers to insert counterfeit devices: failed, knock-off, or\notherwise inferior to the original product. This increases the supplier's\nprofits at the expense of reduced performance/reliability of the customer's\nsystem. The most challenging class of counterfeit devices to detect is recycled\ncounterfeits: recovered genuine devices which are re-sold as new. Such devices\nare difficult to detect because they typically pass performance and parametric\ntests but fail prematurely due to age-related wear.\n  To address the challenge of detecting recycled devices pre-deployment, we\ndevelop Silicon Dating: a low-overhead classifier for detecting recycled\nintegrated circuits using Static Random-Access Memory (SRAM) power-on states.\nSilicon Dating targets devices with no known-new record or purpose-built\nanti-recycling hardware. We observe that over time, software running on a\ndevice imprints its unique data patterns into SRAM through analog-domain\nchanges; we measure the level and direction of this change through SRAM\npower-on state statistics. In contrast to highly symmetric power-on states\nproduced by variation during SRAM fabrication, we show that embedded software\ndata is generally highly asymmetric and that the degree of power-on state\nasymmetry imprinted by software reveals device use. Using empirical results\nfrom embedded benchmarks running on several microcontrollers, we show that\nSilicon Dating identifies recycled devices with 84.1% accuracy with no\nsoftware-specific knowledge and with 92.0% accuracy by incorporating software\nknowledge---without prior device enrollment or modification.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 21:32:47 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Williams", "Harrison", "", "Virginia Tech"], ["Lind", "Alexander", "", "Virginia Tech"], ["Parikh", "Kishankumar", "", "Virginia Tech"], ["Hicks", "Matthew", "", "Virginia Tech"]]}, {"id": "2009.04013", "submitter": "Wanrong Zhang", "authors": "Wanrong Zhang, Olga Ohrimenko, Rachel Cummings", "title": "Attribute Privacy: Framework and Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring the privacy of training data is a growing concern since many machine\nlearning models are trained on confidential and potentially sensitive data.\nMuch attention has been devoted to methods for protecting individual privacy\nduring analyses of large datasets. However in many settings, global properties\nof the dataset may also be sensitive (e.g., mortality rate in a hospital rather\nthan presence of a particular patient in the dataset). In this work, we depart\nfrom individual privacy to initiate the study of attribute privacy, where a\ndata owner is concerned about revealing sensitive properties of a whole dataset\nduring analysis. We propose definitions to capture \\emph{attribute privacy} in\ntwo relevant cases where global attributes may need to be protected: (1)\nproperties of a specific dataset and (2) parameters of the underlying\ndistribution from which dataset is sampled. We also provide two efficient\nmechanisms and one inefficient mechanism that satisfy attribute privacy for\nthese settings. We base our results on a novel use of the Pufferfish framework\nto account for correlations across attributes in the data, thus addressing \"the\nchallenging problem of developing Pufferfish instantiations and algorithms for\ngeneral aggregate secrets\" that was left open by \\cite{kifer2014pufferfish}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 22:38:57 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 23:23:04 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhang", "Wanrong", ""], ["Ohrimenko", "Olga", ""], ["Cummings", "Rachel", ""]]}, {"id": "2009.04063", "submitter": "Mahmoud Elmohr", "authors": "Mahmoud Khalafalla, Mahmoud A. Elmohr, Catherine Gebotys", "title": "Going Deep: Using deep learning techniques with simplified mathematical\n  models against XOR BR and TBR PUFs (Attacks and Countermeasures)", "comments": "To appear in proceedings of 2020 IEEE International Symposium on\n  Hardware Oriented Security and Trust (HOST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the study of PUFs vulnerability against modeling\nattacks by evaluating the security of XOR BR PUFs, XOR TBR PUFs, and obfuscated\narchitectures of XOR BR PUF using a simplified mathematical model and deep\nlearning (DL) techniques. Obtained results show that DL modeling attacks could\neasily break the security of 4-input XOR BR PUFs and 4-input XOR TBR PUFs with\nmodeling accuracy $\\sim$ 99%. Similar attacks were executed using single-layer\nneural networks (NN) and support vector machines (SVM) with polynomial kernel\nand the obtained results showed that single NNs failed to break the PUF\nsecurity. Furthermore, SVM results confirmed the same modeling accuracy\nreported in previous research ($\\sim$ 50%). For the first time, this research\nempirically shows that DL networks can be used as powerful modeling techniques\nagainst these complex PUF architectures for which previous conventional machine\nlearning techniques had failed. Furthermore, a detailed scalability analysis is\nconducted on the DL networks with respect to PUFs' stage size and complexity.\nThe analysis shows that the number of layers and hidden neurons inside every\nlayer has a linear relationship with PUFs' stage size, which agrees with the\ntheoretical findings in deep learning. Consequently, A new obfuscated\narchitecture is introduced as a first step to counter DL modeling attacks and\nit showed significant resistance against such attacks (16% - 40% less\naccuracy). This research provides an important step towards prioritizing the\nefforts to introduce new PUF architectures that are more secure and\ninvulnerable to modeling attacks. Moreover, it triggers future discussions on\nthe removal of influential bits and the level of obfuscation needed to confirm\nthat a specific PUF architecture is resistant against powerful DL modeling\nattacks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 01:41:57 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Khalafalla", "Mahmoud", ""], ["Elmohr", "Mahmoud A.", ""], ["Gebotys", "Catherine", ""]]}, {"id": "2009.04131", "submitter": "Linyi Li", "authors": "Linyi Li, Xiangyu Qi, Tao Xie, Bo Li", "title": "SoK: Certified Robustness for Deep Neural Networks", "comments": "14 pages for the main text; code available at\n  https://github.com/AI-secure/VeriGauge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great advancement in deep neural networks (DNNs) has led to state-of-the-art\nperformance on a wide range of tasks. However, recent studies have shown that\nDNNs are vulnerable to adversarial attacks, which have brought great concerns\nwhen deploying these models to safety-critical applications such as autonomous\ndriving. Different defense approaches have been proposed against adversarial\nattacks, including: 1) empirical defenses, which can be adaptively attacked\nagain without providing robustness certification; and 2) certifiably robust\napproaches, which consist of robustness verification providing the lower bound\nof robust accuracy against any attacks under certain conditions and\ncorresponding robust training approaches. In this paper, we focus on these\ncertifiably robust approaches and provide the first work to perform large-scale\nsystematic analysis of different robustness verification and training\napproaches. In particular, we 1) provide a taxonomy for the robustness\nverification and training approaches, as well as discuss the detailed\nmethodologies for representative algorithms, 2) reveal the fundamental\nconnections among these approaches, 3) discuss current research progresses,\ntheoretical barriers, main challenges, and several promising future directions\nfor certified defenses for DNNs, and 4) provide an open-sourced unified\nplatform to evaluate 20+ representative verification and corresponding robust\ntraining approaches on a wide range of DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 07:00:55 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 05:04:49 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Li", "Linyi", ""], ["Qi", "Xiangyu", ""], ["Xie", "Tao", ""], ["Li", "Bo", ""]]}, {"id": "2009.04155", "submitter": "Saiful Islam Salim", "authors": "Saiful Islam Salim, Adnan Quaium, Sriram Chellappan, A. B. M. Alim Al\n  Islam", "title": "Enhancing Fidelity of Quantum Cryptography using Maximally Entangled\n  Qubits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Securing information transmission is critical today. However, with rapidly\ndeveloping powerful quantum technologies, conventional cryptography techniques\nare becoming more prone to attacks each day. New techniques in the realm of\nquantum cryptography to preserve security against powerful attacks are slowly\nemerging. What is important though now is the fidelity of the cryptography,\nbecause security with massive processing power is not worth much if it is not\ncorrect. Focusing on this issue, we propose a method to enhance the fidelity of\nquantum cryptography using maximally entangled qubit pairs. For doing so, we\ncreated a graph state along a path consisting of all the qubits of ibmqx4 and\nibmq_16_melbourne respectively and we measure the strength of the entanglement\nusing negativity measurement of the qubit pairs. Then, using the qubits with\nmaximal entanglement, we send the modified encryption key to the receiver. The\nkey is modified by permutation and superdense coding before transmission. The\nreceiver reverts the process and gets the actual key. We carried out the\ncomplete experiment in the IBM Quantum Experience project. Our result shows a\n15% to 20% higher fidelity of encryption and decryption than a random selection\nof qubits.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 08:12:18 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Salim", "Saiful Islam", ""], ["Quaium", "Adnan", ""], ["Chellappan", "Sriram", ""], ["Islam", "A. B. M. Alim Al", ""]]}, {"id": "2009.04157", "submitter": "Behrooz Razeghi", "authors": "Behrooz Razeghi, Flavio. P. Calmon, Deniz Gunduz, Slava Voloshynovskiy", "title": "On Perfect Obfuscation: Local Information Geometry Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of privacy-preserving data release for a specific\nutility task under perfect obfuscation constraint. We establish the necessary\nand sufficient condition to extract features of the original data that carry as\nmuch information about a utility attribute as possible, while not revealing any\ninformation about the sensitive attribute. This problem formulation generalizes\nboth the information bottleneck and privacy funnel problems. We adopt a local\ninformation geometry analysis that provides useful insight into information\ncoupling and trajectory construction of spherical perturbation of probability\nmass functions. This analysis allows us to construct the modal decomposition of\nthe joint distributions, divergence transfer matrices, and mutual information.\nBy decomposing the mutual information into orthogonal modes, we obtain the\nlocally sufficient statistics for inferences about the utility attribute, while\nsatisfying perfect obfuscation constraint. Furthermore, we develop the notion\nof perfect obfuscation based on $\\chi^2$-divergence and Kullback-Leibler\ndivergence in the Euclidean information geometry.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 08:22:39 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Razeghi", "Behrooz", ""], ["Calmon", "Flavio. P.", ""], ["Gunduz", "Deniz", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "2009.04207", "submitter": "Markus Heinrich", "authors": "Christian Schlehuber, Markus Heinrich, Tsvetoslava Vateva-Gurova,\n  Stefan Katzenbeisser, Neeraj Suri", "title": "A Security Architecture for Railway Signalling", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-66266-4_21", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the proposed security architecture Deutsche Bahn plans to deploy\nto protect its trackside safety-critical signalling system against\ncyber-attacks. We first present the existing reference interlocking system that\nis built using standard components. Next, we present a taxonomy to help model\nthe attack vectors relevant for the railway environment. Building upon this, we\npresent the proposed \"compartmentalized\" defence concept for securing the\nupcoming signalling systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 11:09:05 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Schlehuber", "Christian", ""], ["Heinrich", "Markus", ""], ["Vateva-Gurova", "Tsvetoslava", ""], ["Katzenbeisser", "Stefan", ""], ["Suri", "Neeraj", ""]]}, {"id": "2009.04263", "submitter": "Thilo Krachenfels", "authors": "Thilo Krachenfels, Fatemeh Ganji, Amir Moradi, Shahin Tajik,\n  Jean-Pierre Seifert", "title": "Real-World Snapshots vs. Theory: Questioning the t-Probing Security\n  Model", "comments": "This is the authors' version of the article accepted for publication\n  at IEEE Symposium on Security and Privacy 2021", "journal-ref": null, "doi": "10.1109/SP40001.2021.00029", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its sound theoretical basis and practical efficiency, masking has\nbecome the most prominent countermeasure to protect cryptographic\nimplementations against physical side-channel attacks (SCAs). The core idea of\nmasking is to randomly split every sensitive intermediate variable during\ncomputation into at least t+1 shares, where t denotes the maximum number of\nshares that are allowed to be observed by an adversary without learning any\nsensitive information. In other words, it is assumed that the adversary is\nbounded either by the possessed number of probes (e.g., microprobe needles) or\nby the order of statistical analyses while conducting higher-order SCA attacks\n(e.g., differential power analysis). Such bounded models are employed to prove\nthe SCA security of the corresponding implementations. Consequently, it is\nbelieved that given a sufficiently large number of shares, the vast majority of\nknown SCA attacks are mitigated. In this work, we present a novel\nlaser-assisted SCA technique, called Laser Logic State Imaging (LLSI), which\noffers an unlimited number of contactless probes, and therefore, violates the\nprobing security model assumption. This technique enables us to take snapshots\nof hardware implementations, i.e., extract the logical state of all registers\nat any arbitrary clock cycle with a single measurement. To validate this, we\nmount our attack on masked AES hardware implementations and practically\ndemonstrate the extraction of the full-length key in two different scenarios.\nFirst, we assume that the location of the registers (key and/or state) is\nknown, and hence, their content can be directly read by a single snapshot.\nSecond, we consider an implementation with unknown register locations, where we\nmake use of multiple snapshots and a SAT solver to reveal the secrets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 12:30:30 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Krachenfels", "Thilo", ""], ["Ganji", "Fatemeh", ""], ["Moradi", "Amir", ""], ["Tajik", "Shahin", ""], ["Seifert", "Jean-Pierre", ""]]}, {"id": "2009.04274", "submitter": "Carlo Meijer", "authors": "Carlo Meijer, Veelasha Moonsamy and Jos Wetzels", "title": "Where's Crypto?: Automated Identification and Classification of\n  Proprietary Cryptographic Primitives in Binary Code", "comments": "A proof-of-concept implementation can be found at\n  https://github.com/wheres-crypto/wheres-crypto", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuing use of proprietary cryptography in embedded systems across\nmany industry verticals, from physical access control systems and\ntelecommunications to machine-to-machine authentication, presents a significant\nobstacle to black-box security-evaluation efforts. In-depth security analysis\nrequires locating and classifying the algorithm in often very large binary\nimages, thus rendering manual inspection, even when aided by heuristics, time\nconsuming.\n  In this paper, we present a novel approach to automate the identification and\nclassification of (proprietary) cryptographic primitives within binary code.\nOur approach is based on Data Flow Graph (DFG) isomorphism, previously proposed\nby Lestringant et al. Unfortunately, their DFG isomorphism approach is limited\nto known primitives only, and relies on heuristics for selecting code fragments\nfor analysis. By combining the said approach with symbolic execution, we\novercome all limitations of their work, and are able to extend the analysis\ninto the domain of unknown, proprietary cryptographic primitives. To\ndemonstrate that our proposal is practical, we develop various signatures, each\ntargeted at a distinct class of cryptographic primitives, and present\nexperimental evaluations for each of them on a set of binaries, both publicly\navailable (and thus providing reproducible results), and proprietary ones.\nLastly, we provide a free and open-source implementation of our approach,\ncalled Where's Crypto?, in the form of a plug-in for the popular IDA\ndisassembler.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 12:55:04 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 15:59:33 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Meijer", "Carlo", ""], ["Moonsamy", "Veelasha", ""], ["Wetzels", "Jos", ""]]}, {"id": "2009.04344", "submitter": "Michele Campobasso", "authors": "Michele Campobasso and Luca Allodi (Eindhoven University of\n  Technology)", "title": "Impersonation-as-a-Service: Characterizing the Emerging Criminal\n  Infrastructure for User Impersonation at Scale", "comments": "Presented at ACM CCS 2020. Appendix on \"Deriving a Threat Model from\n  Observation\" available at\n  https://michelecampobasso.github.io/publication/2020-11-10-impaas", "journal-ref": "In Proceedings of the 2020 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS '20), Pages 1665-1680", "doi": "10.1145/3372297.3417892", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide evidence of an emerging criminal infrastructure\nenabling impersonation attacks at scale. Impersonation-as-a-Service (ImpaaS)\nallows attackers to systematically collect and enforce user profiles\n(consisting of user credentials, cookies, device and behavioural fingerprints,\nand other metadata) to circumvent risk-based authentication system and\neffectively bypass multi-factor authentication mechanisms. We present the\nImpaaS model and evaluate its implementation by analysing the operation of a\nlarge, invite-only, Russian ImpaaS platform providing user profiles for more\nthan $260'000$ Internet users worldwide. Our findings suggest that the ImpaaS\nmodel is growing, and provides the mechanisms needed to systematically evade\nauthentication controls across multiple platforms, while providing attackers\nwith a reliable, up-to-date, and semi-automated environment enabling target\nselection and user impersonation against Internet users as scale.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 15:08:51 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 15:45:39 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Campobasso", "Michele", "", "Eindhoven University of\n  Technology"], ["Allodi", "Luca", "", "Eindhoven University of\n  Technology"]]}, {"id": "2009.04390", "submitter": "Dayeol Lee", "authors": "Dayeol Lee, Dmitrii Kuvaiskii, Anjo Vahldiek-Oberwagner, Mona Vij", "title": "Privacy-Preserving Machine Learning in Untrusted Clouds Made Simple", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical framework to deploy privacy-preserving machine\nlearning (PPML) applications in untrusted clouds based on a trusted execution\nenvironment (TEE). Specifically, we shield unmodified PyTorch ML applications\nby running them in Intel SGX enclaves with encrypted model parameters and\nencrypted input data to protect the confidentiality and integrity of these\nsecrets at rest and during runtime. We use the open-source Graphene library OS\nwith transparent file encryption and SGX-based remote attestation to minimize\nporting effort and seamlessly provide file protection and attestation. Our\napproach is completely transparent to the machine learning application: the\ndeveloper and the end-user do not need to modify the ML application in any way.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 16:16:06 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Lee", "Dayeol", ""], ["Kuvaiskii", "Dmitrii", ""], ["Vahldiek-Oberwagner", "Anjo", ""], ["Vij", "Mona", ""]]}, {"id": "2009.04549", "submitter": "Daniel Dunlavy", "authors": "Scott Heidbrink, Kathryn N. Rodhouse, Daniel M. Dunlavy", "title": "Multimodal Deep Learning for Flaw Detection in Software Programs", "comments": "13 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": "SAND2020-9429R", "categories": "cs.LG cs.AI cs.CR cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of multiple deep learning models for detecting flaws in\nsoftware programs. Current, standard approaches for flaw detection rely on a\nsingle representation of a software program (e.g., source code or a program\nbinary). We illustrate that, by using techniques from multimodal deep learning,\nwe can simultaneously leverage multiple representations of software programs to\nimprove flaw detection over single representation analyses. Specifically, we\nadapt three deep learning models from the multimodal learning literature for\nuse in flaw detection and demonstrate how these models outperform traditional\ndeep learning models. We present results on detecting software flaws using the\nJuliet Test Suite and Linux Kernel.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 20:15:11 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Heidbrink", "Scott", ""], ["Rodhouse", "Kathryn N.", ""], ["Dunlavy", "Daniel M.", ""]]}, {"id": "2009.04587", "submitter": "Aadesh Neupane", "authors": "Aadesh Neupane", "title": "A brief history on Homomorphic learning: A privacy-focused approach to\n  machine learning", "comments": "A CS611 class project paper to trace the history of privacy focused\n  machine learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Cryptography and data science research grew exponential with the internet\nboom. Legacy encryption techniques force users to make a trade-off between\nusability, convenience, and security. Encryption makes valuable data\ninaccessible, as it needs to be decrypted each time to perform any operation.\nBillions of dollars could be saved, and millions of people could benefit from\ncryptography methods that don't compromise between usability, convenience, and\nsecurity. Homomorphic encryption is one such paradigm that allows running\narbitrary operations on encrypted data. It enables us to run any sophisticated\nmachine learning algorithm without access to the underlying raw data. Thus,\nhomomorphic learning provides the ability to gain insights from sensitive data\nthat has been neglected due to various governmental and organization privacy\nrules.\n  In this paper, we trace back the ideas of homomorphic learning formally posed\nby Ronald L. Rivest and Len Alderman as \"Can we compute upon encrypted data?\"\nin their 1978 paper. Then we gradually follow the ideas sprouting in the\nbrilliant minds of Shafi Goldwasser, Kristin Lauter, Dan Bonch, Tomas Sander,\nDonald Beaver, and Craig Gentry to address that vital question. It took more\nthan 30 years of collective effort to finally find the answer \"yes\" to that\nimportant question.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 21:57:47 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 02:14:16 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Neupane", "Aadesh", ""]]}, {"id": "2009.04682", "submitter": "Nagender Aneja", "authors": "Rajarshi Roy Chowdhury, Sandhya Aneja, Nagender Aneja, Emeroylariffion\n  Abas", "title": "Network Traffic Analysis based IoT Device Identification", "comments": null, "journal-ref": "International Conference on Big Data and Internet of Things\n  (BDIOT2020), August 22-24, 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Device identification is the process of identifying a device on Internet\nwithout using its assigned network or other credentials. The sharp rise of\nusage in Internet of Things (IoT) devices has imposed new challenges in device\nidentification due to a wide variety of devices, protocols and control\ninterfaces. In a network, conventional IoT devices identify each other by\nutilizing IP or MAC addresses, which are prone to spoofing. Moreover, IoT\ndevices are low power devices with minimal embedded security solution. To\nmitigate the issue in IoT devices, fingerprint (DFP) for device identification\ncan be used. DFP identifies a device by using implicit identifiers, such as\nnetwork traffic (or packets), radio signal, which a device used for its\ncommunication over the network. These identifiers are closely related to the\ndevice hardware and software features. In this paper, we exploit TCP/IP packet\nheader features to create a device fingerprint utilizing device originated\nnetwork packets. We present a set of three metrics which separate some features\nfrom a packet which contribute actively for device identification. To evaluate\nour approach, we used publicly accessible two datasets. We observed the\naccuracy of device genre classification 99.37% and 83.35% of accuracy in the\nidentification of an individual device from IoT Sentinel dataset. However,\nusing UNSW dataset device type identification accuracy reached up to 97.78%.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 06:28:11 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chowdhury", "Rajarshi Roy", ""], ["Aneja", "Sandhya", ""], ["Aneja", "Nagender", ""], ["Abas", "Emeroylariffion", ""]]}, {"id": "2009.04718", "submitter": "Alessio Merlo Prof.", "authors": "Alessio Merlo, Antonio Ruggia, Luigi Sciolla, Luca Verderame", "title": "You Shall not Repackage! Demystifying Anti-Repackaging on Android", "comments": null, "journal-ref": "Computers & Security, 2021", "doi": "10.1016/j.cose.2021.102181", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  App repackaging refers to the practice of customizing an existing mobile app\nand redistributing it in the wild. In this way, the attacker aims to force some\nmobile users to install the repackaged(likely malicious) app instead of the\noriginal one. This phenomenon strongly affects Android, where apps are\navailable on public stores, and the only requirement for an app to execute\nproperly is to be digitally signed. Anti-repackaging techniques try\ncounteracting this attack by adding logical controls in the app at\ncompile-time. Such controls activate in case of repackaging and lead the\nrepackaged app to fail at runtime. On the other side, the attacker must detect\nand bypass the controls to repackage safely. The high-availability of working\nrepackaged apps in the Android ecosystem suggests that the attacker's side is\nwinning. In this respect, this paper aims to bring out the main issues of the\ncurrent anti-repackaging approaches. The contribution of the paper is\nthree-fold: 1) analyze the weaknesses of the current state-of-the-art\nanti-repackaging schemes (i.e., Self-Protection through Dex Encryption, AppIS,\nSSN, SDC, BombDroid, and NRP), 2) summarize the main attack vectors to\nanti-repackaging techniques composing those schemes, and 3) show how such\nattack vectors allow circumventing the current proposals. The paper will also\nshow a full-fledged attack to NRP, the only publicly-available anti repackaging\ntool to date.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 08:14:31 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 09:14:38 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 18:58:58 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Merlo", "Alessio", ""], ["Ruggia", "Antonio", ""], ["Sciolla", "Luigi", ""], ["Verderame", "Luca", ""]]}, {"id": "2009.04748", "submitter": "Wei Zhang", "authors": "Wei Zhang, Yi Wu, Zhishuog Zhang, Hu Xiong and Zhiguang Qin", "title": "Multi-Authority Ciphertext-Policy Attribute Based Encryption With\n  Accountability", "comments": "9 pages,1figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute-based encryption (ABE) is a promising tool for implementing\nfine-grained access control.To solve the matters of security in single\nauthority, access policy public, not traceable of malicious user,we proposed a\nscheme of multi-authority. Moreover, multi-authority may bring about the\ncollusion of different authorities.In order to solve these problem,we proposed\na scheme of access tree structure with policy hidden and access complex.Once\nthe private key is leaked, our scheme can extract the user ID and find it.If\nthe authorities share their information with each other,the scheme avoid them\nto combine together to compute the key information and decrypt the\nciphertext.Finally,the scheme proved to be secure under selective-set of\nIND-CPA.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:38:46 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Zhang", "Wei", ""], ["Wu", "Yi", ""], ["Zhang", "Zhishuog", ""], ["Xiong", "Hu", ""], ["Qin", "Zhiguang", ""]]}, {"id": "2009.04872", "submitter": "Yang Zou", "authors": "Yang Zou, Zhikun Zhang, Michael Backes, Yang Zhang", "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference\n  Attacks against Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While being deployed in many critical applications as core components,\nmachine learning (ML) models are vulnerable to various security and privacy\nattacks. One major privacy attack in this domain is membership inference, where\nan adversary aims to determine whether a target data sample is part of the\ntraining set of a target ML model. So far, most of the current membership\ninference attacks are evaluated against ML models trained from scratch.\nHowever, real-world ML models are typically trained following the transfer\nlearning paradigm, where a model owner takes a pretrained model learned from a\ndifferent dataset, namely teacher model, and trains her own student model by\nfine-tuning the teacher model with her own data.\n  In this paper, we perform the first systematic evaluation of membership\ninference attacks against transfer learning models. We adopt the strategy of\nshadow model training to derive the data for training our membership inference\nclassifier. Extensive experiments on four real-world image datasets show that\nmembership inference can achieve effective performance. For instance, on the\nCIFAR100 classifier transferred from ResNet20 (pretrained with Caltech101), our\nmembership inference achieves $95\\%$ attack AUC. Moreover, we show that\nmembership inference is still effective when the architecture of target model\nis unknown. Our results shed light on the severity of membership risks stemming\nfrom machine learning models in practice.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 14:14:22 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Zou", "Yang", ""], ["Zhang", "Zhikun", ""], ["Backes", "Michael", ""], ["Zhang", "Yang", ""]]}, {"id": "2009.04987", "submitter": "Joachim Neu", "authors": "Joachim Neu, Ertem Nusret Tas, David Tse", "title": "Ebb-and-Flow Protocols: A Resolution of the Availability-Finality\n  Dilemma", "comments": "Forthcoming in IEEE Symposium on Security and Privacy 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CAP theorem says that no blockchain can be live under dynamic\nparticipation and safe under temporary network partitions. To resolve this\navailability-finality dilemma, we formulate a new class of flexible consensus\nprotocols, ebb-and-flow protocols, which support a full dynamically available\nledger in conjunction with a finalized prefix ledger. The finalized ledger\nfalls behind the full ledger when the network partitions but catches up when\nthe network heals. Gasper, the current candidate protocol for Ethereum 2.0's\nbeacon chain, combines the finality gadget Casper FFG with the LMD GHOST fork\nchoice rule and aims to achieve this property. However, we discovered an attack\nin the standard synchronous network model, highlighting a general difficulty\nwith existing finality-gadget-based designs. We present a construction of\nprovably secure ebb-and-flow protocols with optimal resilience. Nodes run an\noff-the-shelf dynamically available protocol, take snapshots of the growing\navailable ledger, and input them into a separate off-the-shelf BFT protocol to\nfinalize a prefix. We explore connections with flexible BFT and improve upon\nthe state-of-the-art for that problem.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 16:55:36 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 23:48:14 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 06:07:34 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Neu", "Joachim", ""], ["Tas", "Ertem Nusret", ""], ["Tse", "David", ""]]}, {"id": "2009.05107", "submitter": "Yuexin Xiang", "authors": "Yuexin Xiang, Wei Ren, Tiantian Li, Xianghan Zheng, Tianqing Zhu and\n  Kim-Kwang Raymond Choo", "title": "Efficiently Constructing Adversarial Examples by Feature Watermarking", "comments": "15 pages, 17figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing attentions of deep learning models, attacks are also\nupcoming for such models. For example, an attacker may carefully construct\nimages in specific ways (also referred to as adversarial examples) aiming to\nmislead the deep learning models to output incorrect classification results.\nSimilarly, many efforts are proposed to detect and mitigate adversarial\nexamples, usually for certain dedicated attacks. In this paper, we propose a\nnovel digital watermark based method to generate adversarial examples for deep\nlearning models. Specifically, partial main features of the watermark image are\nembedded into the host image invisibly, aiming to tamper and damage the\nrecognition capabilities of the deep learning models. We devise an efficient\nmechanism to select host images and watermark images, and utilize the improved\ndiscrete wavelet transform (DWT) based Patchwork watermarking algorithm and the\nmodified discrete cosine transform (DCT) based Patchwork watermarking\nalgorithm. The experimental results showed that our scheme is able to generate\na large number of adversarial examples efficiently. In addition, we find that\nusing the extracted features of the image as the watermark images, can increase\nthe success rate of an attack under certain conditions with minimal changes to\nthe host image. To ensure repeatability, reproducibility, and code sharing, the\nsource code is available on GitHub\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 09:03:26 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Xiang", "Yuexin", ""], ["Ren", "Wei", ""], ["Li", "Tiantian", ""], ["Zheng", "Xianghan", ""], ["Zhu", "Tianqing", ""], ["Choo", "Kim-Kwang Raymond", ""]]}, {"id": "2009.05126", "submitter": "William Buchanan Prof", "authors": "William J Buchanan, Muhammad Ali Imran, Masood Ur-Rehman, Lei Zhang,\n  Qammer H. Abbasi, Christos Chrysoulas, David Haynes, Nikolaos Pitropakis,\n  Pavlos Papadopoulos", "title": "Review and Critical Analysis of Privacy-preserving Infection Tracking\n  and Contact Tracing", "comments": null, "journal-ref": null, "doi": "10.3389/frcmn.2020.583376", "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The outbreak of viruses have necessitated contact tracing and infection\ntracking methods. Despite various efforts, there is currently no standard\nscheme for the tracing and tracking. Many nations of the world have therefore,\ndeveloped their own ways where carriers of disease could be tracked and their\ncontacts traced. These are generalized methods developed either in a\ndistributed manner giving citizens control of their identity or in a\ncentralised manner where a health authority gathers data on those who are\ncarriers. This paper outlines some of the most significant approaches that have\nbeen established for contact tracing around the world. A comprehensive review\non the key enabling methods used to realise the infrastructure around these\ninfection tracking and contact tracing methods is also presented and\nrecommendations are made for the most effective way to develop such a practice.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 19:54:49 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Buchanan", "William J", ""], ["Imran", "Muhammad Ali", ""], ["Ur-Rehman", "Masood", ""], ["Zhang", "Lei", ""], ["Abbasi", "Qammer H.", ""], ["Chrysoulas", "Christos", ""], ["Haynes", "David", ""], ["Pitropakis", "Nikolaos", ""], ["Papadopoulos", "Pavlos", ""]]}, {"id": "2009.05158", "submitter": "Hailey James", "authors": "Hailey James, Otkrist Gupta, Dan Raviv", "title": "OCR Graph Features for Manipulation Detection in Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting manipulations in digital documents is becoming increasingly\nimportant for information verification purposes. Due to the proliferation of\nimage editing software, altering key information in documents has become widely\naccessible. Nearly all approaches in this domain rely on a procedural approach,\nusing carefully generated features and a hand-tuned scoring system, rather than\na data-driven and generalizable approach. We frame this issue as a graph\ncomparison problem using the character bounding boxes, and propose a model that\nleverages graph features using OCR (Optical Character Recognition). Our model\nrelies on a data-driven approach to detect alterations by training a random\nforest classifier on the graph-based OCR features. We evaluate our algorithm's\nforgery detection performance on dataset constructed from real business\ndocuments with slight forgery imperfections. Our proposed model dramatically\noutperforms the most closely-related document manipulation detection model on\nthis task.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 21:50:45 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 15:52:09 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["James", "Hailey", ""], ["Gupta", "Otkrist", ""], ["Raviv", "Dan", ""]]}, {"id": "2009.05184", "submitter": "Mohammad Adiban", "authors": "Mohammad Adiban, Arash Safari, Giampiero Salvi", "title": "STEP-GAN: A Step-by-Step Training for Multi Generator GANs with\n  application to Cyber Security in Power Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce a novel unsupervised countermeasure for smart\ngrid power systems, based on generative adversarial networks (GANs). Given the\npivotal role of smart grid systems (SGSs) in urban life, their security is of\nparticular importance. In recent years, however, advances in the field of\nmachine learning, have raised concerns about cyber attacks on these systems.\nPower systems, among the most important components of urban infrastructure,\nhave, for example, been widely attacked by adversaries. Attackers disrupt power\nsystems using false data injection attacks (FDIA), resulting in a breach of\navailability, integrity, or confidential principles of the system. Our model\nsimulates possible attacks on power systems using multiple generators in a\nstep-by-step interaction with a discriminator in the training phase. As a\nconsequence, our system is robust to unseen attacks. Moreover, the proposed\nmodel considerably reduces the well-known mode collapse problem of GAN-based\nmodels. Our method is general and it can be potentially employed in a wide\nrange of one of one-class classification tasks. The proposed model has low\ncomputational complexity and outperforms baseline systems about 14% and 41% in\nterms of accuracy on the highly imbalanced publicly available industrial\ncontrol system (ICS) cyber attack power system dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 00:47:07 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Adiban", "Mohammad", ""], ["Safari", "Arash", ""], ["Salvi", "Giampiero", ""]]}, {"id": "2009.05241", "submitter": "Tianhao Wang", "authors": "Tianhao Wang, Yuheng Zhang, Ruoxi Jia", "title": "Improving Robustness to Model Inversion Attacks via Mutual Information\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies defense mechanisms against model inversion (MI) attacks --\na type of privacy attacks aimed at inferring information about the training\ndata distribution given the access to a target machine learning model. Existing\ndefense mechanisms rely on model-specific heuristics or noise injection. While\nbeing able to mitigate attacks, existing methods significantly hinder model\nperformance. There remains a question of how to design a defense mechanism that\nis applicable to a variety of models and achieves better utility-privacy\ntradeoff. In this paper, we propose the Mutual Information Regularization based\nDefense (MID) against MI attacks. The key idea is to limit the information\nabout the model input contained in the prediction, thereby limiting the ability\nof an adversary to infer the private training attributes from the model\nprediction. Our defense principle is model-agnostic and we present tractable\napproximations to the regularizer for linear regression, decision trees, and\nneural networks, which have been successfully attacked by prior work if not\nattached with any defenses. We present a formal study of MI attacks by devising\na rigorous game-based definition and quantifying the associated information\nleakage. Our theoretical analysis sheds light on the inefficacy of DP in\ndefending against MI attacks, which has been empirically observed in several\nprior works. Our experiments demonstrate that MID leads to state-of-the-art\nperformance for a variety of MI attacks, target models and datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 06:02:44 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 04:35:57 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Wang", "Tianhao", ""], ["Zhang", "Yuheng", ""], ["Jia", "Ruoxi", ""]]}, {"id": "2009.05262", "submitter": "Pascal Nasahl", "authors": "Pascal Nasahl, Robert Schilling, Mario Werner, Stefan Mangard", "title": "HECTOR-V: A Heterogeneous CPU Architecture for a Secure RISC-V Execution\n  Environment", "comments": null, "journal-ref": null, "doi": "10.1145/3433210.3453112", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure secure and trustworthy execution of applications, vendors\nfrequently embed trusted execution environments into their systems. Here,\napplications are protected from adversaries, including a malicious operating\nsystem. TEEs are usually built by integrating protection mechanisms directly\ninto the processor or by using dedicated external secure elements. However,\nboth of these approaches only cover a narrow threat model resulting in limited\nsecurity guarantees. Enclaves in the application processor typically provide\nweak isolation between the secure and non-secure domain, especially when\nconsidering side-channel attacks. Although secure elements do provide strong\nisolation, the slow communication interface to the application processor is\nexposed to adversaries and restricts the use cases. Independently of the used\nimplementation approach, TEEs often lack the possibility to establish secure\ncommunication to external peripherals, and most operating systems executed\ninside TEEs do not provide state-of-the-art defense strategies, making them\nvulnerable against various attacks. We argue that TEEs implemented on the main\napplication processor are insecure, especially when considering side-channel\nattacks. We demonstrate how a heterogeneous architecture can be utilized to\nrealize a secure TEE design. We directly embed a processor into our\narchitecture to provide strong isolation between the secure and non-secure\ndomain. The tight coupling of TEE and REE enables HECTOR-V to provide\nmechanisms for establishing secure communication channels. We further introduce\nRISC-V Secure Co-Processor, a security-hardened processor tailored for TEEs. To\nsecure applications executed inside the TEE, RVSCP provides control-flow\nintegrity, rigorously restricts I/O accesses to certain execution states, and\nprovides operating system services directly in hardware.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 07:37:52 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 09:03:45 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 10:59:13 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Nasahl", "Pascal", ""], ["Schilling", "Robert", ""], ["Werner", "Mario", ""], ["Mangard", "Stefan", ""]]}, {"id": "2009.05328", "submitter": "Rihab Almutawa", "authors": "Rihab Fahd Al-Mutawa, Fathy Albouraey Eassa", "title": "A Smart Home System based on Internet of Things", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2020.0110234", "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Internet of Things (IoT) describes a network infrastructure of\nidentifiable things that share data through the Internet. A smart home is one\nof the applications for the Internet of Things. In a smart home, household\nappliances could be monitored and controlled remotely. This raises a demand for\nreliable security solutions for IoT systems. Authorization and authentication\nare challenging IoT security operations that need to be considered. For\ninstance, unauthorized access, such as cyber-attacks, to a smart home system\ncould cause danger by controlling sensors and actuators, opening the doors for\na thief. This paper applies an extra layer of security of multi-factor\nauthentication to act as a prevention method for mitigating unauthorized\naccess. One of those factors is face recognition, as it has recently become\npopular due to its non-invasive biometric techniques, which is easy to use with\ncameras attached to most trending computers and smartphones. In this paper, the\ngaps in existing IoT smart home systems have been analyzed, and we have\nsuggested improvements for overcoming them by including necessary system\nmodules and enhancing user registration and log-in authentication. We propose\nsoftware architecture for implementing such a system. To the best of our\nknowledge, the existing IoT smart home management research does not support\nface recognition and liveness detection within the authentication operation of\ntheir suggested software architectures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 10:34:48 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Al-Mutawa", "Rihab Fahd", ""], ["Eassa", "Fathy Albouraey", ""]]}, {"id": "2009.05356", "submitter": "Lizhi Xiong", "authors": "Lizhi Xiong, Wenhao Zhou, Zhihua Xia, Qi Gu, Jian Weng", "title": "Efficient Privacy-Preserving Computation Based on Additive Secret\n  Sharing", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of cloud computing provides a new computing paradigm for users\n-- massive and complex computing tasks can be outsourced to cloud servers.\nHowever, the privacy issues also follow. Fully homomorphic encryption shows\ngreat potential in privacy-preserving computation, yet it is not ready for\npractice. At present, secure multiparty computation (MPC) remains mainly\napproach to deal with sensitive data. In this paper, following the secret\nsharing based MPC paradigm, we propose a secure 2-party computation scheme, in\nwhich cloud servers can securely evaluate functions with high efficiency. We\nfirst propose the multiplicative secret sharing (MSS) based on typical additive\nsecret sharing (ASS). Then, we design protocols to switch shared secret between\nMSS and ASS, based on which a series of protocols for comparison and nearly all\nof the elementary functions are proposed. We prove that all the proposed\nprotocols are Universally Composable secure in the honest-but-curious model.\nFinally, we will show the remarkable progress of our protocols on both\ncommunication efficiency and functionality completeness.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 11:50:37 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 12:02:25 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Xiong", "Lizhi", ""], ["Zhou", "Wenhao", ""], ["Xia", "Zhihua", ""], ["Gu", "Qi", ""], ["Weng", "Jian", ""]]}, {"id": "2009.05401", "submitter": "Thomas Steinke", "authors": "Thomas Steinke", "title": "Multi-Central Differential Privacy", "comments": "Short working paper (10 pages) - comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is typically studied in the central model where a\ntrusted \"aggregator\" holds the sensitive data of all the individuals and is\nresponsible for protecting their privacy. A popular alternative is the local\nmodel in which the aggregator is untrusted and instead each individual is\nresponsible for their own privacy. The decentralized privacy guarantee of the\nlocal model comes at a high price in statistical utility or computational\ncomplexity. Thus intermediate models such as the shuffled model and pan privacy\nhave been studied in an attempt to attain the best of both worlds.\n  In this note, we propose an intermediate trust model for differential\nprivacy, which we call the multi-central model. Here there are multiple\naggregators and we only assume that they do not collude nefariously. This model\nrelaxes the trust requirements of the central model while avoiding the price of\nthe local model. We motivate this model and provide some simple and efficient\nalgorithms for it. We argue that this model is a promising direction for\nfurther research.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 12:47:27 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Steinke", "Thomas", ""]]}, {"id": "2009.05413", "submitter": "Michael Neuder", "authors": "Michael Neuder, Daniel J. Moroz, Rithvik Rao, David C. Parkes", "title": "Defending Against Malicious Reorgs in Tezos Proof-of-Stake", "comments": "To appear in the second ACM conference on Advances in Financial\n  Technology (AFT'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains are intended to be immutable, so an attacker who is able to\ndelete transactions through a chain reorganization (a malicious reorg) can\nperform a profitable double-spend attack. We study the rate at which an\nattacker can execute reorgs in the Tezos Proof-of-Stake protocol. As an\nexample, an attacker with 40% of the staking power is able to execute a\n20-block malicious reorg at an average rate of once per day, and the attack\nprobability increases super-linearly as the staking power grows beyond 40%.\nMoreover, an attacker of the Tezos protocol knows in advance when an attack\nopportunity will arise, and can use this knowledge to arrange transactions to\ndouble-spend. We show that in particular cases, the Tezos protocol can be\nadjusted to protect against deep reorgs. For instance, we demonstrate protocol\nparameters that reduce the rate of length-20 reorg opportunities for a 40%\nattacker by two orders of magnitude. We also observe a trade-off between\noptimizing for robustness to deep reorgs (costly deviations that may be net\nprofitable because they enable double-spends) and robustness to selfish mining\n(mining deviations that result in typically short reorgs that are profitable\neven without double-spends). That is, the parameters that optimally protect\nagainst one make the other attack easy. Finally, we develop a method that\nmonitors the Tezos blockchain health with respect to malicious reorgs using\nonly publicly available information.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 12:58:51 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Neuder", "Michael", ""], ["Moroz", "Daniel J.", ""], ["Rao", "Rithvik", ""], ["Parkes", "David C.", ""]]}, {"id": "2009.05531", "submitter": "Youness Arjoune Mr.", "authors": "Youness Arjoune and Saleh Faruque", "title": "Smart Jamming Attacks in 5G New Radio: A Review", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CR cs.SY eess.SY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The fifth generation of wireless cellular networks (5G) is expected to be the\ninfrastructure for emergency services, natural disasters rescue, public safety,\nand military communications. 5G, as any previous wireless cellular network, is\nvulnerable to jamming attacks, which create deliberate interference to hinder\nthe communication of legitimate users. Therefore, jamming 5G networks can be a\nreal threat to public safety. Thus, there is a strong need to investigate to\nwhat extent these networks are vulnerable to jamming attacks. For this\ninvestigation, we consider the 3GPP standard released in 2017, which is widely\naccepted as the primary reference for the deployment of these networks. First,\nwe describe the key elements of 5G New Radio (NR) architecture, such as\ndifferent channels and signals exchanged between the base station and user\nequipment. Second, we conduct an in-depth review of the jamming attack models\nand we assess the 5G NR vulnerabilities to these jamming attacks. Then, we\npresent the state-of-the-art detection and mitigation techniques, and we\ndiscuss their suitability to defeat smart jammers in 5G wireless networks.\nFinally, we provide some recommendations and future research directions at the\nend of this paper.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 17:07:02 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Arjoune", "Youness", ""], ["Faruque", "Saleh", ""]]}, {"id": "2009.05537", "submitter": "Lichao Sun", "authors": "Lichao Sun, Lingjuan Lyu", "title": "Federated Model Distillation with Noise-Free Differential Privacy", "comments": "accepted by IJCAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional federated learning directly averages model weights, which is\nonly possible for collaboration between models with homogeneous architectures.\nSharing prediction instead of weight removes this obstacle and eliminates the\nrisk of white-box inference attacks in conventional federated learning.\nHowever, the predictions from local models are sensitive and would leak\ntraining data privacy to the public. To address this issue, one naive approach\nis adding the differentially private random noise to the predictions, which\nhowever brings a substantial trade-off between privacy budget and model\nperformance. In this paper, we propose a novel framework called FEDMD-NFDP,\nwhich applies a Noise-Free Differential Privacy (NFDP) mechanism into a\nfederated model distillation framework. Our extensive experimental results on\nvarious datasets validate that FEDMD-NFDP can deliver not only comparable\nutility and communication efficiency but also provide a noise-free differential\nprivacy guarantee. We also demonstrate the feasibility of our FEDMD-NFDP by\nconsidering both IID and non-IID setting, heterogeneous model architectures,\nand unlabelled public datasets from a different distribution.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 17:19:56 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 11:16:47 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sun", "Lichao", ""], ["Lyu", "Lingjuan", ""]]}, {"id": "2009.05540", "submitter": "Ilya Sapranidi", "authors": "Aleksei Pupyshev, Ilya Sapranidi, Elshan Dzhafarov, Shamil Khalilov,\n  Ilya Teterin", "title": "Graviton: interchain swaps and wrapped tokens liquidity incentivisation\n  solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the issues with liquidity that inhibit adoption of\nso-called wrapped tokens, i.e. digital assets issued in one blockchain\necosystem (origin) with representation in other blockchain networks\n(destination), and an incentive model and a governance mechanism for solving\nthese issues are suggested. The proposed liquidity model called Graviton can be\nimplemented both within the framework of a single destination chain, or as a\nblockchain-agnostic solution combining various blockchain platforms together\nand providing liquidity to wrapped tokens in each of them. This model does not\ndepend on how cross-chain transfer gateways are implemented, and can work with\nboth centralized gates and bridges, or decentralized trustless gateways, as\nwell as gateways based on oracle networks and threshold signatures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 17:23:52 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Pupyshev", "Aleksei", ""], ["Sapranidi", "Ilya", ""], ["Dzhafarov", "Elshan", ""], ["Khalilov", "Shamil", ""], ["Teterin", "Ilya", ""]]}, {"id": "2009.05566", "submitter": "Muhammad Muqsit Nawaz", "authors": "Muqsit Nawaz, Aditya Gulati, Kunlong Liu, Vishwajeet Agrawal,\n  Prabhanjan Ananth and Trinabh Gupta", "title": "Accelerating 2PC-based ML with Limited Trusted Hardware", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design, implementation, and evaluation of Otak, a\nsystem that allows two non-colluding cloud providers to run machine learning\n(ML) inference without knowing the inputs to inference. Prior work for this\nproblem mostly relies on advanced cryptography such as two-party secure\ncomputation (2PC) protocols that provide rigorous guarantees but suffer from\nhigh resource overhead. Otak improves efficiency via a new 2PC protocol that\n(i) tailors recent primitives such as function and homomorphic secret sharing\nto ML inference, and (ii) uses trusted hardware in a limited capacity to\nbootstrap the protocol. At the same time, Otak reduces trust assumptions on\ntrusted hardware by running a small code inside the hardware, restricting its\nuse to a preprocessing step, and distributing trust over heterogeneous trusted\nhardware platforms from different vendors. An implementation and evaluation of\nOtak demonstrates that its CPU and network overhead converted to a dollar\namount is 5.4$-$385$\\times$ lower than state-of-the-art 2PC-based works.\nBesides, Otak's trusted computing base (code inside trusted hardware) is only\n1,300 lines of code, which is 14.6$-$29.2$\\times$ lower than the code-size in\nprior trusted hardware-based works.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 17:53:13 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Nawaz", "Muqsit", ""], ["Gulati", "Aditya", ""], ["Liu", "Kunlong", ""], ["Agrawal", "Vishwajeet", ""], ["Ananth", "Prabhanjan", ""], ["Gupta", "Trinabh", ""]]}, {"id": "2009.05602", "submitter": "Lan Zhang", "authors": "Lan Zhang, Peng Liu, Yoon-Ho Choi", "title": "Semantic-preserving Reinforcement Learning Attack Against Graph Neural\n  Networks for Malware Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an increasing number of deep-learning-based malware scanners have been\nproposed, the existing evasion techniques, including code obfuscation and\npolymorphic malware, are found to be less effective. In this work, we propose a\nreinforcement learning-based semantics-preserving\n(i.e.functionality-preserving) attack against black-box GNNs (GraphNeural\nNetworks) for malware detection. The key factor of adversarial malware\ngeneration via semantic Nops insertion is to select the appropriate\nsemanticNopsand their corresponding basic blocks. The proposed attack uses\nreinforcement learning to automatically make these \"how to select\" decisions.\nTo evaluate the attack, we have trained two kinds of GNNs with five types(i.e.,\nBackdoor, Trojan-Downloader, Trojan-Ransom, Adware, and Worm) of Windows\nmalware samples and various benign Windows programs. The evaluation results\nhave shown that the proposed attack can achieve a significantly higher evasion\nrate than three baseline attacks, namely the semantics-preserving random\ninstruction insertion attack, the semantics-preserving accumulative instruction\ninsertion attack, and the semantics-preserving gradient-based instruction\ninsertion attack.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 18:30:35 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 15:38:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Lan", ""], ["Liu", "Peng", ""], ["Choi", "Yoon-Ho", ""]]}, {"id": "2009.05669", "submitter": "Daniel Gibney", "authors": "Jason W. Bentley, Daniel Gibney, Gary Hoppenworth, Sumit Kumar Jha", "title": "Quantifying Membership Inference Vulnerability via Generalization Gap\n  and Other Model Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how a target model's generalization gap leads directly to an\neffective deterministic black box membership inference attack (MIA). This\nprovides an upper bound on how secure a model can be to MIA based on a simple\nmetric. Moreover, this attack is shown to be optimal in the expected sense\ngiven access to only certain likely obtainable metrics regarding the network's\ntraining and performance. Experimentally, this attack is shown to be comparable\nin accuracy to state-of-art MIAs in many cases.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 21:53:50 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bentley", "Jason W.", ""], ["Gibney", "Daniel", ""], ["Hoppenworth", "Gary", ""], ["Jha", "Sumit Kumar", ""]]}, {"id": "2009.05679", "submitter": "Amrita Roy Chowdhury", "authors": "Amrita Roy Chowdhury, Bolin Ding, Somesh Jha, Weiran Liu, Jingren Zhou", "title": "Intertwining Order Preserving Encryption and Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ciphertexts of an order-preserving encryption (OPE) scheme preserve the order\nof their corresponding plaintexts. However, OPEs are vulnerable to inference\nattacks that exploit this preserved order. At another end, differential privacy\nhas become the de-facto standard for achieving data privacy. One of the most\nattractive properties of DP is that any post-processing (inferential)\ncomputation performed on the noisy output of a DP algorithm does not degrade\nits privacy guarantee. In this paper, we intertwine the two approaches and\npropose a novel differentially private order preserving encryption scheme,\nOP$\\epsilon$. Under OP$\\epsilon$, the leakage of order from the ciphertexts is\ndifferentially private. As a result, in the least, OP$\\epsilon$ ensures a\nformal guarantee (specifically, a relaxed DP guarantee) even in the face of\ninference attacks. To the best of our knowledge, this is the first work to\nintertwine DP with a property-preserving encryption scheme. We demonstrate\nOP$\\epsilon$'s practical utility in answering range queries via extensive\nempirical evaluation on four real-world datasets. For instance, OP$\\epsilon$\nmisses only around $4$ in every $10K$ correct records on average for a dataset\nof size $\\sim732K$ with an attribute of domain size $\\sim18K$ and $\\epsilon=\n1$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 22:38:43 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 00:53:00 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chowdhury", "Amrita Roy", ""], ["Ding", "Bolin", ""], ["Jha", "Somesh", ""], ["Liu", "Weiran", ""], ["Zhou", "Jingren", ""]]}, {"id": "2009.05683", "submitter": "Sumit Mukherjee", "authors": "Xiyang Liu, Yixi Xu, Shruti Tople, Sumit Mukherjee, Juan Lavista\n  Ferres", "title": "MACE: A Flexible Framework for Membership Privacy Estimation in\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we formally study the membership privacy risk of generative\nmodels and propose a membership privacy estimation framework. We formulate the\nmembership privacy risk as a statistical divergence between training samples\nand hold-out samples, and propose sample-based methods to estimate this\ndivergence. Unlike previous works, our proposed metric and estimators make\nrealistic and flexible assumptions. First, we offer a generalizable metric as\nan alternative to accuracy for imbalanced datasets. Second, our estimators are\ncapable of estimating the membership privacy risk given any scalar or vector\nvalued attributes from the learned model, while prior work require access to\nspecific attributes. This allows our framework to provide data-driven\ncertificates for trained generative models in terms of membership privacy risk.\nFinally, we show a connection to differential privacy, which allows our\nproposed estimators to be used to understand the privacy budget 'epsilon'\nneeded for differentially private generative models. We demonstrate the utility\nof our framework through experimental demonstrations on different generative\nmodels using various model attributes yielding some new insights about\nmembership leakage and vulnerabilities of models.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 23:15:05 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 00:26:30 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 17:03:58 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liu", "Xiyang", ""], ["Xu", "Yixi", ""], ["Tople", "Shruti", ""], ["Mukherjee", "Sumit", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "2009.05714", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Nalin Asanka Gamagedara Arachchilage and Mumtaz Abdul Hameed", "title": "Designing a Serious Game: Teaching Developers to Embed Privacy into\n  Software Systems", "comments": "6", "journal-ref": "35th IEEE/ACM International Conference on Automated Software\n  Engineering Workshops (ASEW '20), September 25, 2020, Virtual Event,\n  Australia. ACM, New York, NY, USA", "doi": "10.1145/3417113.3422149", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software applications continue to challenge user privacy when users interact\nwith them. Privacy practices (e.g. Data Minimisation (DM), Privacy by Design\n(PbD) or General Data Protection Regulation (GDPR)) and related \"privacy\nengineering\" methodologies exist and provide clear instructions for developers\nto implement privacy into software systems they develop that preserve user\nprivacy. However, those practices and methodologies are not yet a common\npractice in the software development community. There has been no previous\nresearch focused on developing \"educational\" interventions such as serious\ngames to enhance software developers' coding behaviour. Therefore, this\nresearch proposes a game design framework as an educational tool for software\ndevelopers to improve (secure) coding behaviour, so they can develop\nprivacy-preserving software applications that people can use. The elements of\nthe proposed framework were incorporated into a gaming application scenario\nthat enhances the software developers' coding behaviour through their\nmotivation. The proposed work not only enables the development of\nprivacy-preserving software systems but also helping the software development\ncommunity to put privacy guidelines and engineering methodologies into\npractice.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 03:18:11 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Arachchilage", "Nalin Asanka Gamagedara", ""], ["Hameed", "Mumtaz Abdul", ""]]}, {"id": "2009.05796", "submitter": "John Lim", "authors": "John Lim, True Price, Fabian Monrose, Jan-Michael Frahm", "title": "Revisiting the Threat Space for Vision-based Keystroke Inference Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vision-based keystroke inference attack is a side-channel attack in which\nan attacker uses an optical device to record users on their mobile devices and\ninfer their keystrokes. The threat space for these attacks has been studied in\nthe past, but we argue that the defining characteristics for this threat space,\nnamely the strength of the attacker, are outdated. Previous works do not study\nadversaries with vision systems that have been trained with deep neural\nnetworks because these models require large amounts of training data and\ncurating such a dataset is expensive. To address this, we create a large-scale\nsynthetic dataset to simulate the attack scenario for a keystroke inference\nattack. We show that first pre-training on synthetic data, followed by adopting\ntransfer learning techniques on real-life data, increases the performance of\nour deep learning models. This indicates that these models are able to learn\nrich, meaningful representations from our synthetic data and that training on\nthe synthetic data can help overcome the issue of having small, real-life\ndatasets for vision-based key stroke inference attacks. For this work, we focus\non single keypress classification where the input is a frame of a keypress and\nthe output is a predicted key. We are able to get an accuracy of 95.6% after\npre-training a CNN on our synthetic data and training on a small set of\nreal-life data in an adversarial domain adaptation framework. Source Code for\nSimulator:\nhttps://github.com/jlim13/keystroke-inference-attack-synthetic-dataset-generator-\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 14:03:55 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lim", "John", ""], ["Price", "True", ""], ["Monrose", "Fabian", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "2009.05826", "submitter": "Matthieu Lequesne", "authors": "Alain Couvreur, Matthieu Lequesne", "title": "On the security of subspace subcodes of Reed-Solomon codes for public\n  key encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses the security of McEliece-like encryption schemes using\nsubspace subcodes of Reed-Solomon codes, i.e. subcodes of Reed-Solomon codes\nover $\\mathbb{F}_{q^m}$ whose entries lie in a fixed collection of\n$\\mathbb{F}_q$-subspaces of $\\mathbb{F}_{q^m}$. These codes appear to be a\nnatural generalisation of Goppa and alternant codes and provide a broader\nflexibility in designing code based encryption schemes. For the security\nanalysis, we introduce a new operation on codes called the twisted product\nwhich yields a polynomial time distinguisher on such subspace subcodes as soon\nas the chosen $\\mathbb{F}_q$-subspaces have dimension larger than $m/2$. From\nthis distinguisher, we build an efficient attack which in particular breaks\nsome parameters of a recent proposal due to Khathuria, Rosenthal and Weger.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 17:02:23 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Couvreur", "Alain", ""], ["Lequesne", "Matthieu", ""]]}, {"id": "2009.05872", "submitter": "Yanmin Gong", "authors": "Zhidong Gao, Rui Hu, Yanmin Gong", "title": "Certified Robustness of Graph Classification against Topology Attack\n  with Randomized Smoothing", "comments": "Accepted to IEEE GLOBECOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph classification has practical applications in diverse fields. Recent\nstudies show that graph-based machine learning models are especially vulnerable\nto adversarial perturbations due to the non i.i.d nature of graph data. By\nadding or deleting a small number of edges in the graph, adversaries could\ngreatly change the graph label predicted by a graph classification model. In\nthis work, we propose to build a smoothed graph classification model with\ncertified robustness guarantee. We have proven that the resulting graph\nclassification model would output the same prediction for a graph under $l_0$\nbounded adversarial perturbation. We also evaluate the effectiveness of our\napproach under graph convolutional network (GCN) based multi-class graph\nclassification model.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 22:18:54 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Gao", "Zhidong", ""], ["Hu", "Rui", ""], ["Gong", "Yanmin", ""]]}, {"id": "2009.05886", "submitter": "Dylan Slack", "authors": "Gavin Kerrigan and Dylan Slack and Jens Tuyls", "title": "Differentially Private Language Models Benefit from Public Pre-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language modeling is a keystone task in natural language processing. When\ntraining a language model on sensitive information, differential privacy (DP)\nallows us to quantify the degree to which our private data is protected.\nHowever, training algorithms which enforce differential privacy often lead to\ndegradation in model quality. We study the feasibility of learning a language\nmodel which is simultaneously high-quality and privacy preserving by tuning a\npublic base model on a private corpus. We find that DP fine-tuning boosts the\nperformance of language models in the private domain, making the training of\nsuch models possible.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 00:50:44 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 16:04:43 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kerrigan", "Gavin", ""], ["Slack", "Dylan", ""], ["Tuyls", "Jens", ""]]}, {"id": "2009.05944", "submitter": "Guanyao Li", "authors": "Guanyao Li, Siyan Hu, Shuhan Zhong, Wai Lun Tsui, and S.-H. Gary Chan", "title": "vContact: Private WiFi-based Contact Tracing with Virus Lifespan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covid-19 is primarily spread through contact with the virus which may survive\non surfaces with lifespan of more than hours. To curb its spread, it is hence\nof vital importance to detect and quarantine those who have been in contact\nwith the virus for sustained period of time, the so-called close contacts. In\nthis work, we study, for the first time, automatic contact detection when the\nvirus has a lifespan. Leveraging upon the ubiquity of WiFi signals, we propose\na novel, private, and fully distributed WiFi-based approach called vContact.\nUsers installing an app continuously scan WiFi and store its hashed IDs. Given\na confirmed case, the signals of the major places he/she visited are then\nuploaded to a server and matched with the stored signals of users to detect\ncontact. vContact is not based on phone pairing, and no information of any\nother users is stored locally. The confirmed case does not need to have\ninstalled the app for it to work properly. As WiFi data are sampled\nsporadically, we propose efficient signal processing approaches and similarity\nmetric to align and match signals of any time. We conduct extensive indoor and\noutdoor experiments to evaluate the performance of vContact. Our results\ndemonstrate that vContact is efficient and robust for contact detection. The\nprecision and recall of contact detection are high (in the range of 50-90%) for\nclose contact proximity (2m). Its performance is robust with respect to signal\nlengths (AP numbers) and phone heterogeneity. By implementing vContact as an\napp, we present a case study to demonstrate the validity of our design in\nnotifying its users their exposure to virus with lifespan.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 07:40:44 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 04:37:43 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Guanyao", ""], ["Hu", "Siyan", ""], ["Zhong", "Shuhan", ""], ["Tsui", "Wai Lun", ""], ["Chan", "S. -H. Gary", ""]]}, {"id": "2009.05945", "submitter": "Hongyue Kang", "authors": "Hongyue Kang, Xiaolin Chang, Jelena Mi\\v{s}i\\'c, Vojislav B.\n  Mi\\v{s}i\\'c, Yingying Yao, Zhi Chen", "title": "Stochastic Modeling Approaches for Analyzing Blockchain: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology has been attracting much attention from both academia\nand industry. It brings many benefits to various applications like Internet of\nThings. However, there are critical issues to be addressed before its\nwidespread deployment, such as transaction efficiency, bandwidth bottleneck,\nand security. Techniques are being explored to tackle these issues. Stochastic\nmodeling, as one of these techniques, has been applied to analyze a variety of\nblockchain characteristics, but there is a lack of a comprehensive survey on\nit. In this survey, we aim to fill the gap and review the stochastic models\nproposed to address common issues in blockchain. Firstly, this paper provides\nthe basic knowledge of blockchain technology and stochastic models. Then,\naccording to different objects, the stochastic models for blockchain analysis\nare divided into network-oriented and application-oriented (mainly refer to\ncryptocurrency). The network-oriented stochastic models are further classified\ninto two categories, namely, performance and security. About the\napplication-oriented stochastic models, the widest adoption mainly concentrates\non the price prediction of cryptocurrency. Moreover, we provide analysis and\ncomparison in detail on every taxonomy and discuss the strengths and weaknesses\nof the related works to serve guides for further researches. Finally,\nchallenges and future research directions are given to apply stochastic\nmodeling approaches to study blockchain. By analyzing and classifying the\nexisting researches, we hope that our survey can provide suggestions for the\nresearchers who are interested in blockchain and good at using stochastic\nmodels as a tool to address problems.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 07:55:01 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Kang", "Hongyue", ""], ["Chang", "Xiaolin", ""], ["Mi\u0161i\u0107", "Jelena", ""], ["Mi\u0161i\u0107", "Vojislav B.", ""], ["Yao", "Yingying", ""], ["Chen", "Zhi", ""]]}, {"id": "2009.06005", "submitter": "Sudipta Paul Ms.", "authors": "Sudipta Paul, Poushali Sengupta and Subhankar Mishra", "title": "FLaPS: Federated Learning and Privately Scaling", "comments": "5 figures, 8 tables, Accepted to the SLICE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning (FL) is a distributed learning process where the model\n(weights and checkpoints) is transferred to the devices that posses data rather\nthan the classical way of transferring and aggregating the data centrally. In\nthis way, sensitive data does not leave the user devices. FL uses the FedAvg\nalgorithm, which is trained in the iterative model averaging way, on the\nnon-iid and unbalanced distributed data, without depending on the data\nquantity. Some issues with the FL are, 1) no scalability, as the model is\niteratively trained over all the devices, which amplifies with device drops; 2)\nsecurity and privacy trade-off of the learning process still not robust enough\nand 3) overall communication efficiency and the cost are higher. To mitigate\nthese challenges we present Federated Learning and Privately Scaling (FLaPS)\narchitecture, which improves scalability as well as the security and privacy of\nthe system. The devices are grouped into clusters which further gives better\nprivacy scaled turn around time to finish a round of training. Therefore, even\nif a device gets dropped in the middle of training, the whole process can be\nstarted again after a definite amount of time. The data and model both are\ncommunicated using differentially private reports with iterative shuffling\nwhich provides a better privacy-utility trade-off. We evaluated FLaPS on MNIST,\nCIFAR10, and TINY-IMAGENET-200 dataset using various CNN models. Experimental\nresults prove FLaPS to be an improved, time and privacy scaled environment\nhaving better and comparable after-learning-parameters with respect to the\ncentral and FL models.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 14:20:17 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Paul", "Sudipta", ""], ["Sengupta", "Poushali", ""], ["Mishra", "Subhankar", ""]]}, {"id": "2009.06077", "submitter": "Paul-Olivier Dehaye", "authors": "Paul-Olivier Dehaye and Joel Reardon", "title": "Proximity Tracing in an Ecosystem of Surveillance Capitalism", "comments": null, "journal-ref": "19th Workshop on Privacy in the Electronic Society, WPES 2020", "doi": "10.1145/3411497.3420219", "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proximity tracing apps have been proposed as an aide in dealing with the\nCOVID-19 crisis. Some of those apps leverage attenuation of Bluetooth beacons\nfrom mobile devices to build a record of proximate encounters between a pair of\ndevice owners. The underlying protocols are known to suffer from false positive\nand re-identification attacks. We present evidence that the attacker's\ndifficulty in mounting such attacks has been overestimated. Indeed, an attacker\nleveraging a moderately successful app or SDK with Bluetooth and location\naccess can eavesdrop and interfere with these proximity tracing systems at no\nhardware cost and perform these attacks against users who do not have this app\nor SDK installed. We describe concrete examples of actors who would be in a\ngood position to execute such attacks. We further present a novel attack, which\nwe call a biosurveillance attack, which allows the attacker to monitor the\nexposure risk of a smartphone user who installs their app or SDK but who does\nnot use any contact tracing system and may falsely believe that they have opted\nout of the system.\n  Through traffic auditing with an instrumented testbed, we characterize\nprecisely the behaviour of one such SDK that we found in a handful of\napps---but installed on more than one hundred million mobile devices. Its\nbehaviour is functionally indistinguishable from a re-identification or\nbiosurveillance attack and capable of executing a false positive attack with\nminimal effort. We also discuss how easily an attacker could acquire a position\nconducive to such attacks, by leveraging the lax logic for granting permissions\nto apps in the Android framework: any app with some geolocation permission\ncould acquire the necessary Bluetooth permission through an upgrade, without\nany additional user prompt. Finally we discuss motives for conducting such\nattacks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 20:06:30 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Dehaye", "Paul-Olivier", ""], ["Reardon", "Joel", ""]]}, {"id": "2009.06112", "submitter": "Xinran Wang", "authors": "Xinran Wang, Yu Xiang, Jun Gao, Jie Ding", "title": "Information Laundering for Model Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose information laundering, a novel framework for\nenhancing model privacy. Unlike data privacy that concerns the protection of\nraw data information, model privacy aims to protect an already-learned model\nthat is to be deployed for public use. The private model can be obtained from\ngeneral learning methods, and its deployment means that it will return a\ndeterministic or random response for a given input query. An\ninformation-laundered model consists of probabilistic components that\ndeliberately maneuver the intended input and output for queries to the model,\nso the model's adversarial acquisition is less likely. Under the proposed\nframework, we develop an information-theoretic principle to quantify the\nfundamental tradeoffs between model utility and privacy leakage and derive the\noptimal design.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 23:24:08 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Wang", "Xinran", ""], ["Xiang", "Yu", ""], ["Gao", "Jun", ""], ["Ding", "Jie", ""]]}, {"id": "2009.06124", "submitter": "Pengfei Wang", "authors": "Xu Zhou, Pengfei Wang, Chenyifan Liu, Tai Yue, Yingying Liu, Congxi\n  Song, Kai Lu, Qidi Yin", "title": "UniFuzz: Optimizing Distributed Fuzzing via Dynamic Centralized Task\n  Scheduling", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzing is one of the most efficient technology for vulnerability detection.\nSince the fuzzing process is computing-intensive and the performance improved\nby algorithm optimization is limited, recent research seeks to improve fuzzing\nperformance by utilizing parallel computing. However, parallel fuzzing has to\novercome challenges such as task conflicts, scalability in a distributed\nenvironment, synchronization overhead, and workload imbalance. In this paper,\nwe design and implement UniFuzz, a distributed fuzzing optimization based on a\ndynamic centralized task scheduling. UniFuzz evaluates and distributes seeds in\na centralized manner to avoid task conflicts. It uses a \"request-response\"\nscheme to dynamically distribute fuzzing tasks, which avoids workload\nimbalance. Besides, UniFuzz can adaptively switch the role of computing cores\nbetween evaluating, and fuzzing, which avoids the potential bottleneck of seed\nevaluation. To improve synchronization efficiency, UniFuzz shares different\nfuzzing information in a different way according to their characteristics, and\nthe average overhead of synchronization is only about 0.4\\%. We evaluated\nUniFuzz with real-world programs, and the results show that UniFuzz outperforms\nstate-of-the-art tools, such as AFL, PAFL and EnFuzz. Most importantly, the\nexperiment reveals a counter-intuitive result that parallel fuzzing can achieve\na super-linear acceleration to the single-core fuzzing. We made a detailed\nexplanation and proved it with additional experiments. UniFuzz also discovered\n16 real-world vulnerabilities.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 00:30:08 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhou", "Xu", ""], ["Wang", "Pengfei", ""], ["Liu", "Chenyifan", ""], ["Yue", "Tai", ""], ["Liu", "Yingying", ""], ["Song", "Congxi", ""], ["Lu", "Kai", ""], ["Yin", "Qidi", ""]]}, {"id": "2009.06228", "submitter": "Yijue Wang", "authors": "Yijue Wang, Jieren Deng, Dan Guo, Chenghong Wang, Xianrui Meng, Hang\n  Liu, Caiwen Ding, Sanguthevar Rajasekaran", "title": "SAPAG: A Self-Adaptive Privacy Attack From Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning such as federated learning or collaborative learning\nenables model training on decentralized data from users and only collects local\ngradients, where data is processed close to its sources for data privacy. The\nnature of not centralizing the training data addresses the privacy issue of\nprivacy-sensitive data. Recent studies show that a third party can reconstruct\nthe true training data in the distributed machine learning system through the\npublicly-shared gradients. However, existing reconstruction attack frameworks\nlack generalizability on different Deep Neural Network (DNN) architectures and\ndifferent weight distribution initialization, and can only succeed in the early\ntraining phase. To address these limitations, in this paper, we propose a more\ngeneral privacy attack from gradient, SAPAG, which uses a Gaussian kernel based\nof gradient difference as a distance measure. Our experiments demonstrate that\nSAPAG can construct the training data on different DNNs with different weight\ninitializations and on DNNs in any training phases.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 07:04:02 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Wang", "Yijue", ""], ["Deng", "Jieren", ""], ["Guo", "Dan", ""], ["Wang", "Chenghong", ""], ["Meng", "Xianrui", ""], ["Liu", "Hang", ""], ["Ding", "Caiwen", ""], ["Rajasekaran", "Sanguthevar", ""]]}, {"id": "2009.06299", "submitter": "Roberto Doriguzzi Corin", "authors": "Maged Abdelaty, Roberto Doriguzzi-Corin, Domenico Siracusa", "title": "DAICS: A Deep Learning Solution for Anomaly Detection in Industrial\n  Control Systems", "comments": null, "journal-ref": null, "doi": "10.1109/TETC.2021.3073017", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is emerging as an effective technique to detect sophisticated\ncyber-attacks targeting Industrial Control Systems (ICSs). The conventional\napproach to detection in literature is to learn the \"normal\" behaviour of the\nsystem, to be then able to label noteworthy deviations from it as anomalies.\nHowever, during operations, ICSs inevitably and continuously evolve their\nbehaviour, due to e.g., replacement of devices, workflow modifications, or\nother reasons. As a consequence, the accuracy of the anomaly detection process\nmay be dramatically affected with a considerable amount of false alarms being\ngenerated. This paper presents DAICS, a novel deep learning framework with a\nmodular design to fit in large ICSs. The key component of the framework is a\n2-branch neural network that learns the changes in the ICS behaviour with a\nsmall number of data samples and a few gradient updates. This is supported by\nan automatic tuning mechanism of the detection threshold that takes into\naccount the changes in the prediction error under normal operating conditions.\nIn this regard, no specialised human intervention is needed to update the other\nparameters of the system. DAICS has been evaluated using publicly available\ndatasets and shows an increased detection rate and accuracy compared to state\nof the art approaches, as well as higher robustness to additive noise.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 09:54:26 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Abdelaty", "Maged", ""], ["Doriguzzi-Corin", "Roberto", ""], ["Siracusa", "Domenico", ""]]}, {"id": "2009.06368", "submitter": "Yanjun  Qi Dr.", "authors": "Jin Yong Yoo, John X. Morris, Eli Lifland, Yanjun Qi", "title": "Searching for a Search Method: Benchmarking Search Algorithms for\n  Generating NLP Adversarial Examples", "comments": "14 pages, 5 figures, 4 tables; Accepted by EMNLP BlackBox NLP\n  Workshop 2020 @ https://blackboxnlp.github.io/cfp.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behavior of several black-box search algorithms used for\ngenerating adversarial examples for natural language processing (NLP) tasks. We\nperform a fine-grained analysis of three elements relevant to search: search\nalgorithm, search space, and search budget. When new search algorithms are\nproposed in past work, the attack search space is often modified alongside the\nsearch algorithm. Without ablation studies benchmarking the search algorithm\nchange with the search space held constant, one cannot tell if an increase in\nattack success rate is a result of an improved search algorithm or a less\nrestrictive search space. Additionally, many previous studies fail to properly\nconsider the search algorithms' run-time cost, which is essential for\ndownstream tasks like adversarial training. Our experiments provide a\nreproducible benchmark of search algorithms across a variety of search spaces\nand query budgets to guide future research in adversarial NLP. Based on our\nexperiments, we recommend greedy attacks with word importance ranking when\nunder a time constraint or attacking long inputs, and either beam search or\nparticle swarm optimization otherwise. Code implementation shared via\nhttps://github.com/QData/TextAttack-Search-Benchmark\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 17:04:42 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 19:46:36 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Yoo", "Jin Yong", ""], ["Morris", "John X.", ""], ["Lifland", "Eli", ""], ["Qi", "Yanjun", ""]]}, {"id": "2009.06389", "submitter": "Sahib Singh", "authors": "Tom Farrand, Fatemehsadat Mireshghallah, Sahib Singh, Andrew Trask", "title": "Neither Private Nor Fair: Impact of Data Imbalance on Utility and\n  Fairness in Differential Privacy", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deployment of deep learning in different fields and industries is growing day\nby day due to its performance, which relies on the availability of data and\ncompute. Data is often crowd-sourced and contains sensitive information about\nits contributors, which leaks into models that are trained on it. To achieve\nrigorous privacy guarantees, differentially private training mechanisms are\nused. However, it has recently been shown that differential privacy can\nexacerbate existing biases in the data and have disparate impacts on the\naccuracy of different subgroups of data. In this paper, we aim to study these\neffects within differentially private deep learning. Specifically, we aim to\nstudy how different levels of imbalance in the data affect the accuracy and the\nfairness of the decisions made by the model, given different levels of privacy.\nWe demonstrate that even small imbalances and loose privacy guarantees can\ncause disparate impacts.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 18:35:49 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 16:00:29 GMT"}, {"version": "v3", "created": "Sat, 3 Oct 2020 11:55:05 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Farrand", "Tom", ""], ["Mireshghallah", "Fatemehsadat", ""], ["Singh", "Sahib", ""], ["Trask", "Andrew", ""]]}, {"id": "2009.06468", "submitter": "Sai Sri Sathya", "authors": "Ramesh Raskar and Sai Sri Sathya", "title": "Bluetooth based Proximity, Multi-hop Analysis and Bi-directional Trust:\n  Epidemics and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a trust layer on top of Bluetooth and similar\nwireless communication technologies that can form mesh networks. This layer as\na protocol enables computing trust scores based on proximity and bi-directional\ntransfer of messages in multiple hops across a network of mobile devices. We\ndescribe factors and an approach for determining these trust scores and\nhighlight its applications during epidemics such as COVID-19 through improved\ncontact-tracing, better privacy and verification for sensitive data sharing in\nthe numerous Bluetooth and GPS based mobile applications that are being\ndeveloped to track the spread.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:23:00 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Raskar", "Ramesh", ""], ["Sathya", "Sai Sri", ""]]}, {"id": "2009.06490", "submitter": "Matthew Cole", "authors": "Matthew Cole (1) and Aravind Prakash (1) ((1) Binghamton University)", "title": "Simplex: Repurposing Intel Memory Protection Extensions for Information\n  Hiding", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in software exploits, the last few decades have seen\nseveral hardware-level features to enhance security (e.g., Intel MPX, ARM\nTrustZone, Intel SGX, Intel CET). Due to security, performance and/or usability\nissues these features have attracted steady criticism. One such feature is the\nIntel Memory Protection Extensions (MPX), an instruction set architecture\nextension promising spatial memory safety at a lower performance cost due to\nhardware-accelerated bounds checking. However, recent investigations into MPX\nhave found that is neither as performant, accurate, nor precise as cutting-edge\nsoftware-based spatial memory safety. As a direct consequence, compiler and\noperating system support for MPX is dying, and Intel has begun to manufacture\ndesktop CPUs without MPX. Nonetheless, given how ubiquitous MPX is, it provides\nan excellent yet under-utilized hardware resource that can be aptly salvaged\nfor security purposes. In this paper, we propose Simplex, a library framework\nthat re-purposes MPX registers as general purpose registers. Using Simplex, we\ndemonstrate how MPX registers can be used to store sensitive information (e.g.,\nencryption keys) directly on the hardware. We evaluate Simplex for performance\nand find that its overhead is small enough to permit its deployment in all but\nthe most performance-intensive code. We refactored the string.h buffer\nmanipulation functions and found a geometric mean 0.9% performance overhead. We\nalso modified the deepsjeng and lbm SPEC CPU2017 benchmarks to use Simplex and\nfound a 1% and 0.98% performance overhead respectively. Finally, we investigate\nthe behavior of the MPX context with regards to multi-process and multi-thread\nprograms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:49:12 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Cole", "Matthew", "", "Binghamton University"], ["Prakash", "Aravind", "", "Binghamton University"]]}, {"id": "2009.06505", "submitter": "M. Emre Gursoy", "authors": "Mehmet Emre Gursoy, Vivekanand Rajasekar, Ling Liu", "title": "Utility-Optimized Synthesis of Differentially Private Location Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially private location trace synthesis (DPLTS) has recently emerged\nas a solution to protect mobile users' privacy while enabling the analysis and\nsharing of their location traces. A key challenge in DPLTS is to best preserve\nthe utility in location trace datasets, which is non-trivial considering the\nhigh dimensionality, complexity and heterogeneity of datasets, as well as the\ndiverse types and notions of utility. In this paper, we present OptaTrace: a\nutility-optimized and targeted approach to DPLTS. Given a real trace dataset D,\nthe differential privacy parameter epsilon controlling the strength of privacy\nprotection, and the utility/error metric Err of interest; OptaTrace uses\nBayesian optimization to optimize DPLTS such that the output error (measured in\nterms of given metric Err) is minimized while epsilon-differential privacy is\nsatisfied. In addition, OptaTrace introduces a utility module that contains\nseveral built-in error metrics for utility benchmarking and for choosing Err,\nas well as a front-end web interface for accessible and interactive DPLTS\nservice. Experiments show that OptaTrace's optimized output can yield\nsubstantial utility improvement and error reduction compared to previous work.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:07:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Gursoy", "Mehmet Emre", ""], ["Rajasekar", "Vivekanand", ""], ["Liu", "Ling", ""]]}, {"id": "2009.06538", "submitter": "Jianyu Yang", "authors": "Jianyu Yang, Tianhao Wang, Ninghui Li, Xiang Cheng, Sen Su", "title": "Answering Multi-Dimensional Range Queries under Local Differential\n  Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of answering multi-dimensional range\nqueries under local differential privacy. There are three key technical\nchallenges: capturing the correlations among attributes, avoiding the curse of\ndimensionality, and dealing with the large domains of attributes. None of the\nexisting approaches satisfactorily deals with all three challenges. Overcoming\nthese three challenges, we first propose an approach called Two-Dimensional\nGrids (TDG). Its main idea is to carefully use binning to partition the\ntwo-dimensional (2-D) domains of all attribute pairs into 2-D grids that can\nanswer all 2-D range queries and then estimate the answer of a higher\ndimensional range query from the answers of the associated 2-D range queries.\nHowever, in order to reduce errors due to noises, coarse granularities are\nneeded for each attribute in 2-D grids, losing fine-grained distribution\ninformation for individual attributes. To correct this deficiency, we further\npropose Hybrid-Dimensional Grids (HDG), which also introduces 1-D grids to\ncapture finer-grained information on distribution of each individual attribute\nand combines information from 1-D and 2-D grids to answer range queries. To\nmake HDG consistently effective, we provide a guideline for properly choosing\ngranularities of grids based on an analysis of how different sources of errors\nare impacted by these choices. Extensive experiments conducted on real and\nsynthetic datasets show that HDG can give a significant improvement over the\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 16:08:53 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yang", "Jianyu", ""], ["Wang", "Tianhao", ""], ["Li", "Ninghui", ""], ["Cheng", "Xiang", ""], ["Su", "Sen", ""]]}, {"id": "2009.06630", "submitter": "Fauzi Adi Rafrastara", "authors": "Fauzi Adi Rafrastara, Faizal M. A", "title": "Advanced Virus Monitoring and Analysis System", "comments": "4 pages, 3 figures, International Journal of Computer Science and\n  Information Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposed an architecture and a system which able to monitor the\nvirus behavior and classify them as a traditional or polymorphic virus.\nPreliminary research was conducted to get the current virus behavior and to\nfind the certain parameters which usually used by virus to attack the computer\ntarget. Finally, test bed environment is used to test our system by releasing\nthe virus in a real environment, and try to capture their behavior, and\nfollowed by generating the conclusion that the tested or monitored virus is\nclassified as a traditional or polymorphic virus.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:57:28 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Rafrastara", "Fauzi Adi", ""], ["A", "Faizal M.", ""]]}, {"id": "2009.06700", "submitter": "Petr Svenda", "authors": "Adam Janovsky, Matus Nemec, Petr Svenda, Peter Sekan, Vashek Matyas", "title": "Biased RSA private keys: Origin attribution of GCD-factorable keys", "comments": null, "journal-ref": "ESORICS 2020, 978-3-030-59013-0, Springer", "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In 2016, Svenda et al. (USENIX 2016, The Million-key Question) reported that\nthe implementation choices in cryptographic libraries allow for qualified\nguessing about the origin of public RSA keys. We extend the technique to two\nnew scenarios when not only public but also private keys are available for the\norigin attribution - analysis of a source of GCD-factorable keys in IPv4-wide\nTLS scans and forensic investigation of an unknown source. We learn several\nrepresentatives of the bias from the private keys to train a model on more than\n150 million keys collected from 70 cryptographic libraries, hardware security\nmodules and cryptographic smartcards. Our model not only doubles the number of\ndistinguishable groups of libraries (compared to public keys from Svenda et\nal.) but also improves more than twice in accuracy w.r.t. random guessing when\na single key is classified. For a forensic scenario where at least 10 keys from\nthe same source are available, the correct origin library is correctly\nidentified with average accuracy of 89% compared to 4% accuracy of a random\nguess. The technique was also used to identify libraries producing\nGCD-factorable TLS keys, showing that only three groups are the probable\nsuspects.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 19:20:42 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Janovsky", "Adam", ""], ["Nemec", "Matus", ""], ["Svenda", "Petr", ""], ["Sekan", "Peter", ""], ["Matyas", "Vashek", ""]]}, {"id": "2009.06701", "submitter": "Takami Sato", "authors": "Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jack Jia, Xue Lin, Qi\n  Alfred Chen", "title": "Dirty Road Can Attack: Security of Deep Learning based Automated Lane\n  Centering under Physical-World Attack", "comments": "Accepted to Usenix Security '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Lane Centering (ALC) systems are convenient and widely deployed\ntoday, but also highly security and safety critical. In this work, we are the\nfirst to systematically study the security of state-of-the-art deep learning\nbased ALC systems in their designed operational domains under physical-world\nadversarial attacks. We formulate the problem with a safety-critical attack\ngoal, and a novel and domain-specific attack vector: dirty road patches. To\nsystematically generate the attack, we adopt an optimization-based approach and\novercome domain-specific design challenges such as camera frame\ninter-dependencies due to attack-influenced vehicle control, and the lack of\nobjective function design for lane detection models.\n  We evaluate our attack on a production ALC using 80 scenarios from real-world\ndriving traces. The results show that our attack is highly effective with over\n97.5% success rates and less than 0.903 sec average success time, which is\nsubstantially lower than the average driver reaction time. This attack is also\nfound (1) robust to various real-world factors such as lighting conditions and\nview angles, (2) general to different model designs, and (3) stealthy from the\ndriver's view. To understand the safety impacts, we conduct experiments using\nsoftware-in-the-loop simulation and attack trace injection in a real vehicle.\nThe results show that our attack can cause a 100% collision rate in different\nscenarios, including when tested with common safety features such as automatic\nemergency braking. We also evaluate and discuss defenses.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 19:22:39 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 22:38:38 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sato", "Takami", ""], ["Shen", "Junjie", ""], ["Wang", "Ningfei", ""], ["Jia", "Yunhan Jack", ""], ["Lin", "Xue", ""], ["Chen", "Qi Alfred", ""]]}, {"id": "2009.06709", "submitter": "Jaydip Sen", "authors": "Jaydip Sen and Sidra Mehtab", "title": "Machine Learning Applications in Misuse and Anomaly Detection", "comments": "21 Pages; 4 Figures; 3 Tables", "journal-ref": "Book Chapter Published in the Volume: \"Security and Privacy From a\n  Legal, Ethical, and Technical Perspective\", Editors: Christos Kalloniatis and\n  Carlos Travieso-Gonzalez, InTechOpen Publishers, London, UK, June 2020", "doi": "10.5772/intechopen.92653", "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and data mining algorithms play important roles in designing\nintrusion detection systems. Based on their approaches toward the detection of\nattacks in a network, intrusion detection systems can be broadly categorized\ninto two types. In the misuse detection systems, an attack in a system is\ndetected whenever the sequence of activities in the network matches with a\nknown attack signature. In the anomaly detection approach, on the other hand,\nanomalous states in a system are identified based on a significant difference\nin the state transitions of the system from its normal states. This chapter\npresents a comprehensive discussion on some of the existing schemes of\nintrusion detection based on misuse detection, anomaly detection and hybrid\ndetection approaches. Some future directions of research in the design of\nalgorithms for intrusion detection are also identified.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 19:52:00 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Sen", "Jaydip", ""], ["Mehtab", "Sidra", ""]]}, {"id": "2009.06764", "submitter": "Jean-Francois Rajotte", "authors": "Jean-Francois Rajotte, Raymond T Ng", "title": "Private data sharing between decentralized users through the privGAN\n  architecture", "comments": "6 pages, 9 figures, to be in the proceedings of International\n  Workshop on Privacy and Security in Enterprise Modeling (PriSEM'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More data is almost always beneficial for analysis and machine learning\ntasks. In many realistic situations however, an enterprise cannot share its\ndata, either to keep a competitive advantage or to protect the privacy of the\ndata sources, the enterprise's clients for example. We propose a method for\ndata owners to share synthetic or fake versions of their data without sharing\nthe actual data, nor the parameters of models that have direct access to the\ndata. The method proposed is based on the privGAN architecture where local GANs\nare trained on their respective data subsets with an extra penalty from a\ncentral discriminator aiming to discriminate the origin of a given fake sample.\nWe demonstrate that this approach, when applied to subsets of various sizes,\nleads to better utility for the owners than the utility from their real small\ndatasets. The only shared pieces of information are the parameter updates of\nthe central discriminator. The privacy is demonstrated with white-box attacks\non the most vulnerable elments of the architecture and the results are close to\nrandom guessing. This method would apply naturally in a federated learning\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 22:06:13 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Rajotte", "Jean-Francois", ""], ["Ng", "Raymond T", ""]]}, {"id": "2009.06847", "submitter": "Guansong Pang", "authors": "Guansong Pang, Anton van den Hengel, Chunhua Shen, Longbing Cao", "title": "Toward Deep Supervised Anomaly Detection: Reinforcement Learning from\n  Partially Labeled Anomaly Data", "comments": "Accepted to KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467417", "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of anomaly detection with a small set of partially\nlabeled anomaly examples and a large-scale unlabeled dataset. This is a common\nscenario in many important applications. Existing related methods either\nexclusively fit the limited anomaly examples that typically do not span the\nentire set of anomalies, or proceed with unsupervised learning from the\nunlabeled data. We propose here instead a deep reinforcement learning-based\napproach that enables an end-to-end optimization of the detection of both\nlabeled and unlabeled anomalies. This approach learns the known abnormality by\nautomatically interacting with an anomaly-biased simulation environment, while\ncontinuously extending the learned abnormality to novel classes of anomaly\n(i.e., unknown anomalies) by actively exploring possible anomalies in the\nunlabeled data. This is achieved by jointly optimizing the exploitation of the\nsmall labeled anomaly data and the exploration of the rare unlabeled anomalies.\nExtensive experiments on 48 real-world datasets show that our model\nsignificantly outperforms five state-of-the-art competing methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 03:05:39 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 13:40:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Pang", "Guansong", ""], ["Hengel", "Anton van den", ""], ["Shen", "Chunhua", ""], ["Cao", "Longbing", ""]]}, {"id": "2009.06861", "submitter": "Imdad Ullah", "authors": "Imdad Ullah, Roksana Boreli, and Salil S. Kanhere", "title": "Privacy in Targeted Advertising: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted advertising has transformed the marketing landscape for a wide\nvariety of businesses, by creating new opportunities for advertisers to reach\nprospective customers by delivering personalised ads, using an infrastructure\nof a number of intermediary entities and technologies. The advertising and\nanalytics companies collect, aggregate, process and trade a vast amount of\nuser's personal data, which has prompted serious privacy concerns among both\nindividuals and organisations. This article presents a detailed survey of the\nassociated privacy risks and proposed solutions in a mobile environment. We\noutline details of the information flow between the advertising platform and\nad/analytics networks, the profiling process, advertising sources and criteria,\nthe measurement analysis of targeted advertising based on user's interests and\nprofiling context and the ads delivery process, for both in-app and in-browser\ntargeted ads; we also include an overview of data sharing and tracking\ntechnologies. We discuss challenges in preserving user privacy that include\nthreats related to private information extraction and exchange among various\nadvertising entities, privacy threats from third-party tracking,\nre-identification of private information and associated privacy risks.\nSubsequently, we present various techniques for preserving user privacy and a\ncomprehensive analysis of the proposals based on such techniques; we compare\nthe proposals based on the underlying architectures, privacy mechanisms and\ndeployment scenarios. Finally, we discuss the potential research challenges and\nopen research issues.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 04:40:45 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 08:18:22 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 07:33:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ullah", "Imdad", ""], ["Boreli", "Roksana", ""], ["Kanhere", "Salil S.", ""]]}, {"id": "2009.06863", "submitter": "Meng Sun", "authors": "Linlin Zheng, Jiakang Li, Meng Sun, Xiongwei Zhang, Thomas Fang Zheng", "title": "When Automatic Voice Disguise Meets Automatic Speaker Verification", "comments": "accepted for publication", "journal-ref": "IEEE Transactions on Information Forensics and Security, 2020", "doi": null, "report-no": null, "categories": "eess.AS cs.CR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique of transforming voices in order to hide the real identity of a\nspeaker is called voice disguise, among which automatic voice disguise (AVD) by\nmodifying the spectral and temporal characteristics of voices with\nmiscellaneous algorithms are easily conducted with softwares accessible to the\npublic. AVD has posed great threat to both human listening and automatic\nspeaker verification (ASV). In this paper, we have found that ASV is not only a\nvictim of AVD but could be a tool to beat some simple types of AVD. Firstly,\nthree types of AVD, pitch scaling, vocal tract length normalization (VTLN) and\nvoice conversion (VC), are introduced as representative methods.\nState-of-the-art ASV methods are subsequently utilized to objectively evaluate\nthe impact of AVD on ASV by equal error rates (EER). Moreover, an approach to\nrestore disguised voice to its original version is proposed by minimizing a\nfunction of ASV scores w.r.t. restoration parameters. Experiments are then\nconducted on disguised voices from Voxceleb, a dataset recorded in real-world\nnoisy scenario. The results have shown that, for the voice disguise by pitch\nscaling, the proposed approach obtains an EER around 7% comparing to the 30%\nEER of a recently proposed baseline using the ratio of fundamental frequencies.\nThe proposed approach generalizes well to restore the disguise with nonlinear\nfrequency warping in VTLN by reducing its EER from 34.3% to 18.5%. However, it\nis difficult to restore the source speakers in VC by our approach, where more\ncomplex forms of restoration functions or other paralinguistic cues might be\nnecessary to restore the nonlinear transform in VC. Finally, contrastive\nvisualization on ASV features with and without restoration illustrate the role\nof the proposed approach in an intuitive way.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 04:41:52 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zheng", "Linlin", ""], ["Li", "Jiakang", ""], ["Sun", "Meng", ""], ["Zhang", "Xiongwei", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "2009.06893", "submitter": "Qi Gu", "authors": "Zhihua Xia, Qi Gu, Lizhi Xiong, Wenhao Zhou, Jian Weng", "title": "Privacy-Preserving Image Retrieval Based on Additive Secret Sharing", "comments": "This version repairs many mistakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of digital images motivates individuals and organizations to\nupload their images to the cloud server. To preserve privacy, image owners\nwould prefer to encrypt the images before uploading, but it would strongly\nlimit the efficient usage of images. Plenty of existing schemes on\nprivacy-preserving Content-Based Image Retrieval (PPCBIR) try to seek the\nbalance between security and retrieval ability. However, compared to the\nadvanced technologies in CBIR like Convolutional Neural Network (CNN), the\nexisting PPCBIR schemes are far deficient in both accuracy and efficiency. With\nmore cloud service providers, the collaborative secure image retrieval service\nprovided by multiple cloud servers becomes possible. In this paper, inspired by\nadditive secret sharing technology, we propose a series of additive secure\ncomputing protocols on numbers and matrices with better efficiency, and then\nshow their application in PPCBIR. Specifically, we extract CNN features,\ndecrease the dimension of features and build the index securely with the help\nof our protocols, which include the full process of image retrieval in the\nplaintext domain. The experiments and security analysis demonstrate the\nefficiency, accuracy, and security of our scheme.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 07:12:51 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 07:03:36 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Xia", "Zhihua", ""], ["Gu", "Qi", ""], ["Xiong", "Lizhi", ""], ["Zhou", "Wenhao", ""], ["Weng", "Jian", ""]]}, {"id": "2009.06896", "submitter": "Lilian Bossuet", "authors": "El Mehdi Benhani (LHC), Cuauhtemoc Mancillas Lopez (CINVESTAV-IPN),\n  Lilian Bossuet (LHC)", "title": "Secure Internal Communication of a Trustzone-Enabled Heterogeneous Soc\n  Lightweight Encryption", "comments": null, "journal-ref": "International Conference on Field-Programmable Technology (ICFPT),\n  Dec 2019, Tianjin, China. pp.239-242", "doi": "10.1109/ICFPT47387.2019.00037", "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security in TrustZone-enabled heterogeneous system-on-chip (SoC) is gaining\nincreasing attention for several years. Mainly because this type of SoC can be\nfound in more and more applications in servers or in the cloud. The inside-SoC\ncommunication layer is one of the main element of heterogeneous SoC; indeed all\nthe data goes through it. Monitoring and controlling inside-SoC communications\nenables to fend off attacks before system corruption. In this article, we study\nthe feasibility of encrypted data exchange between the secure software executed\nin a trusted execution environment (TEE) and the secure logic part of an\nheterogeneous SoC. Experiment are done with a Xilinx Zynq-7010 SoC and two\nlightweight stream ciphers. We show that using lightweight stream ciphers is an\nefficient solution without excessive overheads.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 07:17:38 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Benhani", "El Mehdi", "", "LHC"], ["Lopez", "Cuauhtemoc Mancillas", "", "CINVESTAV-IPN"], ["Bossuet", "Lilian", "", "LHC"]]}, {"id": "2009.06927", "submitter": "Joaquin Garcia-Alfaro", "authors": "Mariana Segovia, Jose Rubio-Hernan, Ana Rosa Cavalli and Joaquin\n  Garcia-Alfaro", "title": "Cyber-Resilience Evaluation of Cyber-Physical Systems", "comments": "11 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-Physical Systems (CPS) use computational resources to control physical\nprocess and provide critical services. For this reason, an attack in these\nsystems may have dangerous consequences in the physical world. Hence,\nresilience is a fundamental property to ensure the safety of the people, the\nenvironment and the controlled physical process. In this paper, we present\nmetrics to quantify the resilience level based on the design, structure,\nstability, and performance under the attack of a given CPS. The metrics provide\nreference points to evaluate whether the system is better prepared or not to\nface the adversaries. This way, it is possible to quantify the ability to\nrecover from an adversary using its mathematical model based on switched linear\nsystems and actuators saturation. Finally, we validate our approach using a\nnumeric simulation on the Tennesse Eastman control challenge problem.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 08:48:23 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Segovia", "Mariana", ""], ["Rubio-Hernan", "Jose", ""], ["Cavalli", "Ana Rosa", ""], ["Garcia-Alfaro", "Joaquin", ""]]}, {"id": "2009.06975", "submitter": "Ioannis Zografopoulos", "authors": "Ioannis Zografopoulos, Juan Ospina, Charalambos Konstantinou", "title": "Harness the Power of DERs for Secure Communications in Electric Energy\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric energy systems are undergoing significant changes to improve system\nreliability and accommodate increasing power demands. The penetration of\ndistributed energy resources (DERs) including roof-top solar panels, energy\nstorage, electric vehicles, etc., enables the on-site generation of\neconomically dispatchable power curtailing operational costs. The effective\ncontrol of DERs requires communication between utilities and DER system\noperators. The communication protocols employed for DER management and control\nlack sophisticated cybersecurity features and can compromise power systems\nsecure operation if malicious control commands are issued to DERs. To overcome\nauthentication-related protocol issues, we present a bolt-on security extension\nthat can be implemented on Distributed Network Protocol v3 (DNP3). We port an\nauthentication framework, DERauth, into DNP3, and utilize real-time\nmeasurements from a simulated DER battery energy storage system to enhance\ncommunication security. We evaluate our framework in a testbed setup using DNP3\nmaster and outstation devices performing secure authentication by leveraging\nthe entropy of DERs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 10:35:17 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zografopoulos", "Ioannis", ""], ["Ospina", "Juan", ""], ["Konstantinou", "Charalambos", ""]]}, {"id": "2009.06996", "submitter": "Yufei Wang", "authors": "Haoliang Li (1), Yufei Wang (1), Xiaofei Xie (1), Yang Liu (1), Shiqi\n  Wang (2), Renjie Wan (1), Lap-Pui Chau (1), and Alex C. Kot (1) ((1) Nanyang\n  Technological University, Singapore, (2) City University of Hong Kong)", "title": "Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition\n  Systems", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have shown great success in many computer vision\napplications. However, they are also known to be susceptible to backdoor\nattacks. When conducting backdoor attacks, most of the existing approaches\nassume that the targeted DNN is always available, and an attacker can always\ninject a specific pattern to the training data to further fine-tune the DNN\nmodel. However, in practice, such attack may not be feasible as the DNN model\nis encrypted and only available to the secure enclave.\n  In this paper, we propose a novel black-box backdoor attack technique on face\nrecognition systems, which can be conducted without the knowledge of the\ntargeted DNN model. To be specific, we propose a backdoor attack with a novel\ncolor stripe pattern trigger, which can be generated by modulating LED in a\nspecialized waveform. We also use an evolutionary computing strategy to\noptimize the waveform for backdoor attack. Our backdoor attack can be conducted\nin a very mild condition: 1) the adversary cannot manipulate the input in an\nunnatural way (e.g., injecting adversarial noise); 2) the adversary cannot\naccess the training database; 3) the adversary has no knowledge of the training\nmodel as well as the training set used by the victim party.\n  We show that the backdoor trigger can be quite effective, where the attack\nsuccess rate can be up to $88\\%$ based on our simulation study and up to $40\\%$\nbased on our physical-domain study by considering the task of face recognition\nand verification based on at most three-time attempts during authentication.\nFinally, we evaluate several state-of-the-art potential defenses towards\nbackdoor attacks, and find that our attack can still be effective. We highlight\nthat our study revealed a new physical backdoor attack, which calls for the\nattention of the security issue of the existing face recognition/verification\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 11:50:29 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Li", "Haoliang", ""], ["Wang", "Yufei", ""], ["Xie", "Xiaofei", ""], ["Liu", "Yang", ""], ["Wang", "Shiqi", ""], ["Wan", "Renjie", ""], ["Chau", "Lap-Pui", ""], ["Kot", "Alex C.", ""]]}, {"id": "2009.07091", "submitter": "Zain Ul Abideen", "authors": "Malik Imran and Zain Ul Abideen and Samuel Pagliarini", "title": "A Systematic Study of Lattice-based NIST PQC Algorithms: from Reference\n  Implementations to Hardware Accelerators", "comments": "38 pages and 8 figures", "journal-ref": "https://www.mdpi.com/2079-9292/9/11/1953", "doi": "10.3390/electronics9111953", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security of currently deployed public key cryptography algorithms is foreseen\nto be vulnerable against quantum computer attacks. Hence, a community effort\nexists to develop post-quantum cryptography (PQC) algorithms, i.e., algorithms\nthat are resistant to quantum attacks. In this work, we have investigated how\nlattice-based candidate algorithms from the NIST PQC standardization\ncompetition fare when conceived as hardware accelerators. To achieve this, we\nhave assessed the reference implementations of selected algorithms with the\ngoal of identifying what are their basic building blocks. We assume the\nhardware accelerators will be implemented in application specific integrated\ncircuit (ASIC) and the targeted technology in our experiments is a commercial\n65nm node. In order to estimate the characteristics of each algorithm, we have\nassessed their memory requirements, use of multipliers, and how each algorithm\nemploys hashing functions. Furthermore, for these building blocks, we have\ncollected area and power figures for 12 candidate algorithms. For memories, we\nmake use of a commercial memory compiler. For logic, we make use of a standard\ncell library. In order to compare the candidate algorithms fairly, we select a\nreference frequency of operation of 500MHz. Our results reveal that our area\nand power numbers are comparable to the state of the art, despite targeting a\nhigher frequency of operation and a higher security level in our experiments.\nThe comprehensive investigation of lattice-based NIST PQC algorithms performed\nin this paper can be used for guiding ASIC designers when selecting an\nappropriate algorithm while respecting requirements and design constraints.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:32:03 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 08:50:24 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 15:55:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Imran", "Malik", ""], ["Abideen", "Zain Ul", ""], ["Pagliarini", "Samuel", ""]]}, {"id": "2009.07099", "submitter": "Fauzi Adi Rafrastara", "authors": "Fauzi Adi Rafrastara, Qi DeYu", "title": "Revealing the Weaknesses of File Sharing System on Cloud Storages", "comments": "5 pages, 0 figure, International Journal of Advances in Computer\n  Science & Its Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud storage provides the simpler way to share the files privately and\npublicly. A good Cloud Storage Provider (SCP) is not only measured by the\naccess speed or file size that can be shared to others, but also regarding the\nsecurity issues in file sharing itself. In this paper, we analyze the security\nof file sharing in 3 Chinese CSPs, which are: Baidu, Weiyun and Kanbox. Those\nCSPs have their own vulnerabilities that successfully revealed. We also provide\nsome suggestions to countermeasure the weaknesses so that they can maintain the\nquality while improving the security.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:58:13 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Rafrastara", "Fauzi Adi", ""], ["DeYu", "Qi", ""]]}, {"id": "2009.07191", "submitter": "Chen Ma", "authors": "Chen Ma, Shuyu Cheng, Li Chen, Jun Zhu, Junhai Yong", "title": "Switching Transferable Gradient Directions for Query-Efficient Black-Box\n  Adversarial Attacks", "comments": "18 pages, including the supplementary material after the reference\n  section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and highly query-efficient black-box adversarial attack\nnamed SWITCH, which has a state-of-the-art performance in the score-based\nsetting. SWITCH features a highly efficient and effective utilization of the\ngradient of a surrogate model $\\hat{\\mathbf{g}}$ w.r.t. the input image, i.e.,\nthe transferable gradient. In each iteration, SWITCH first tries to update the\ncurrent sample along the direction of $\\hat{\\mathbf{g}}$, but considers\nswitching to its opposite direction $-\\hat{\\mathbf{g}}$ if our algorithm\ndetects that it does not increase the value of the attack objective function.\nWe justify the choice of switching to the opposite direction by a local\napproximate linearity assumption. In SWITCH, only one or two queries are needed\nper iteration, but it is still effective due to the rich information provided\nby the transferable gradient, thereby resulting in unprecedented query\nefficiency. To improve the robustness of SWITCH, we further propose\nSWITCH$_\\text{RGF}$ in which the update follows the direction of a random\ngradient-free (RGF) estimate when neither $\\hat{\\mathbf{g}}$ nor its opposite\ndirection can increase the objective, while maintaining the advantage of SWITCH\nin terms of query efficiency. Experimental results conducted on CIFAR-10,\nCIFAR-100 and TinyImageNet show that compared with other methods, SWITCH\nachieves a satisfactory attack success rate using much fewer queries, and\nSWITCH$_\\text{RGF}$ achieves the state-of-the-art attack success rate with\nfewer queries overall. Our approach can serve as a strong baseline for future\nblack-box attacks because of its simplicity. The PyTorch source code is\nreleased on https://github.com/machanic/SWITCH.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 15:55:08 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 12:47:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ma", "Chen", ""], ["Cheng", "Shuyu", ""], ["Chen", "Li", ""], ["Zhu", "Jun", ""], ["Yong", "Junhai", ""]]}, {"id": "2009.07351", "submitter": "Meng Jiang", "authors": "Meng Jiang and Taeho Jung and Ryan Karl and Tong Zhao", "title": "Federated Dynamic GNN with Secure Aggregation", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given video data from multiple personal devices or street cameras, can we\nexploit the structural and dynamic information to learn dynamic representation\nof objects for applications such as distributed surveillance, without storing\ndata at a central server that leads to a violation of user privacy? In this\nwork, we introduce Federated Dynamic Graph Neural Network (Feddy), a\ndistributed and secured framework to learn the object representations from\nmulti-user graph sequences: i) It aggregates structural information from nearby\nobjects in the current graph as well as dynamic information from those in the\nprevious graph. It uses a self-supervised loss of predicting the trajectories\nof objects. ii) It is trained in a federated learning manner. The centrally\nlocated server sends the model to user devices. Local models on the respective\nuser devices learn and periodically send their learning to the central server\nwithout ever exposing the user's data to server. iii) Studies showed that the\naggregated parameters could be inspected though decrypted when broadcast to\nclients for model synchronizing, after the server performed a weighted average.\nWe design an appropriate aggregation mechanism of secure aggregation primitives\nthat can protect the security and privacy in federated learning with\nscalability. Experiments on four video camera datasets (in four different\nscenes) as well as simulation demonstrate that Feddy achieves great\neffectiveness and security.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 21:04:45 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Jiang", "Meng", ""], ["Jung", "Taeho", ""], ["Karl", "Ryan", ""], ["Zhao", "Tong", ""]]}, {"id": "2009.07352", "submitter": "Meng Jiang", "authors": "Dylan Chou and Meng Jiang", "title": "Data-Driven Network Intrusion Detection: A Taxonomy of Challenges and\n  Methods", "comments": "38 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven methods have been widely used in network intrusion detection\n(NID) systems. However, there are currently a number of challenges derived from\nhow the datasets are being collected. Most attack classes in network intrusion\ndatasets are considered the minority compared to normal traffic and many\ndatasets are collected through virtual machines or other simulated environments\nrather than real-world networks. These challenges undermine the performance of\nintrusion detection machine learning models by fitting models such as random\nforests or support vector machines to unrepresentative \"sandbox\" datasets. This\nsurvey presents a carefully designed taxonomy highlighting eight main\nchallenges and solutions and explores common datasets from 1999 to 2020. Trends\nare analyzed on the distribution of challenges addressed for the past decade\nand future directions are proposed on expanding NID into cloud-based\nenvironments, devising scalable models for larger amount of network intrusion\ndata, and creating labeled datasets collected in real-world networks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 21:12:07 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Chou", "Dylan", ""], ["Jiang", "Meng", ""]]}, {"id": "2009.07403", "submitter": "Molla Rashied Hussein", "authors": "Molla Rashied Hussein, Md. Ashikur Rahman, Md. Jahidul Hassan\n  Mojumder, Shakib Ahmed, Samia Naz Isha, Shaila Akter, Abdullah Bin Shams,\n  Ehsanul Hoque Apu", "title": "Trust Concerns in Health Apps collecting Personally Identifiable\n  Information during COVID-19-like Zoonosis", "comments": "6 pages, 1 table, submitted to the 23rd International Conference on\n  Computer and Information Technology (ICCIT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019, or COVID-19 in short, is a zoonosis, i.e., a\ndisease that spreads from animals to humans. Due to its highly epizootic\nnature, it has compelled the public health experts to deploy smartphone\napplications to trace its rapid transmission pattern along with the infected\npersons as well by utilizing the persons' personally identifiable information.\nHowever, these information may summon several undesirable provocations towards\nthe technical experts in terms of privacy and cyber security, particularly the\ntrust concerns. If not resolved by now, the circumstances will affect the mass\nlevel population through inadequate usage of the health applications in the\nsmartphones and thus liberate the forgery of a catastrophe for another\nCOVID-19-like zoonosis to come. Therefore, an extensive study was required to\naddress this severe issue. This paper has fulfilled the study mentioned above\nneeded by not only discussing the recently designed and developed health\napplications all over the regions around the world but also investigating their\nusefulness and limitations. The trust defiance is identified as well as\nscrutinized from the viewpoint of an end-user. Several recommendations are\nsuggested in the later part of this paper to leverage awareness among the\nordinary individuals.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 00:34:05 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Hussein", "Molla Rashied", ""], ["Rahman", "Md. Ashikur", ""], ["Mojumder", "Md. Jahidul Hassan", ""], ["Ahmed", "Shakib", ""], ["Isha", "Samia Naz", ""], ["Akter", "Shaila", ""], ["Shams", "Abdullah Bin", ""], ["Apu", "Ehsanul Hoque", ""]]}, {"id": "2009.07413", "submitter": "Thomas Hardjono", "authors": "Thomas Hardjono, Alexander Lipton, Alex Pentland", "title": "Towards a Contract Service Provider Model for Virtual Assets and VASPs", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the contract service provider (CSP) model as an analog of the\nsuccessful Internet ISP model. Our exploration is motivated by the need to seek\nalternative blockchain service-fee models that departs from the\ntoken-for-operations (gas fee) model for smart contracts found on many popular\nblockchain platforms today. A given CSP community consisting of multiple CSP\nbusiness entities (VASPs) form a contract domain which implement well-defined\ncontract primitives, policies and contract-ledger. The nodes of the members of\nCSP community form the blockchain network. We discuss a number of design\nprinciples borrowed from the design principles of the Internet Architecture,\nand we discuss the interoperability of cross-domain (cross-chain) transfers of\nvirtual assets in the context of contract domains.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 01:41:38 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 13:54:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Hardjono", "Thomas", ""], ["Lipton", "Alexander", ""], ["Pentland", "Alex", ""]]}, {"id": "2009.07455", "submitter": "Jianzong Wang", "authors": "Anxun He, Jianzong Wang, Zhangcheng Huang and Jing Xiao", "title": "FedSmart: An Auto Updating Federated Learning Optimization Mechanism", "comments": "has been presented in APWeb-WAIM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has made an important contribution to data\nprivacy-preserving. Many previous works are based on the assumption that the\ndata are independently identically distributed (IID). As a result, the model\nperformance on non-identically independently distributed (non-IID) data is\nbeyond expectation, which is the concrete situation. Some existing methods of\nensuring the model robustness on non-IID data, like the data-sharing strategy\nor pretraining, may lead to privacy leaking. In addition, there exist some\nparticipants who try to poison the model with low-quality data. In this paper,\na performance-based parameter return method for optimization is introduced, we\nterm it FederatedSmart (FedSmart). It optimizes different model for each client\nthrough sharing global gradients, and it extracts the data from each client as\na local validation set, and the accuracy that model achieves in round t\ndetermines the weights of the next round. The experiment results show that\nFedSmart enables the participants to allocate a greater weight to the ones with\nsimilar data distribution.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 03:59:33 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["He", "Anxun", ""], ["Wang", "Jianzong", ""], ["Huang", "Zhangcheng", ""], ["Xiao", "Jing", ""]]}, {"id": "2009.07513", "submitter": "Kenji Yasunaga", "authors": "Maiki Fujita and Takeshi Koshiba and Kenji Yasunaga", "title": "Perfectly Secure Message Transmission against Rational Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure Message Transmission (SMT) is a two-party cryptographic protocol by\nwhich the sender can securely and reliably transmit messages to the receiver\nusing multiple channels. An adversary for SMT can corrupt a subset of the\nchannels and make eavesdropping and tampering over the channels. In this work,\nwe introduce a game-theoretic security model for SMT in which adversaries have\nsome preferences for the protocol execution. We define rational \"timid\"\nadversaries who prefer to violate the security requirements, but do not prefer\nthe tampering to be detected. Such adversaries could arise since they may fear\nlosing their corrupted channels for which they needed some cost or risks.\nFirst, we consider the basic setting in which a single adversary attacks the\nprotocol. We show that, even if all but one of the channels are corrupted, we\ncan construct perfect SMT protocols against rational adversaries. In the\ntraditional cryptographic setting, perfect SMT can be constructed only when the\nadversary corrupts a minority of the channels. Our results demonstrate a way of\ncircumventing the cryptographic impossibility results by a game-theoretic\napproach. Next, we study the setting in which all the channels can be corrupted\nby multiple adversaries who do not cooperate. Since we cannot hope for any\nsecurity if a single adversary corrupts all the channels or multiple\nadversaries cooperate maliciously, the scenario can arise from a game-theoretic\nmodel. We present several perfect SMT protocols, including a non-interactive\nprotocol based on the idea of cheater-identifiable secret sharing. We also\nstudy the scenario in which both malicious and rational adversaries exist.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:22:03 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 00:19:29 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Fujita", "Maiki", ""], ["Koshiba", "Takeshi", ""], ["Yasunaga", "Kenji", ""]]}, {"id": "2009.07578", "submitter": "Stephan Robert-Nicoud", "authors": "Giulia Moschini, R\\'egis Houssou, J\\'er\\^ome Bovay, Stephan\n  Robert-Nicoud", "title": "Anomaly and Fraud Detection in Credit Card Transactions Using the ARIMA\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of unsupervised approach of credit card\nfraud detection in unbalanced dataset using the ARIMA model. The ARIMA model is\nfitted on the regular spending behaviour of the customer and is used to detect\nfraud if some deviations or discrepancies appear. Our model is applied to\ncredit card datasets and is compared to 4 anomaly detection approaches such as\nK-Means, Box-Plot, Local Outlier Factor and Isolation Forest. The results show\nthat the ARIMA model presents a better detecting power than the benchmark\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 09:48:26 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Moschini", "Giulia", ""], ["Houssou", "R\u00e9gis", ""], ["Bovay", "J\u00e9r\u00f4me", ""], ["Robert-Nicoud", "Stephan", ""]]}, {"id": "2009.07672", "submitter": "Ahmed Mohamed Hussain", "authors": "Ahmed Mohamed Hussain, Gabriele Oligeri, and Thiemo Voigt", "title": "The Dark (and Bright) Side of IoT: Attacks and Countermeasures for\n  Identifying Smart Home Devices and Services", "comments": "15 pages, 7 figures. Accepted for the 9th International Symposium On\n  Security And Privacy On Internet of Things (SPIoT), 2020", "journal-ref": null, "doi": "10.1007/978-3-030-68884-4_10", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new machine learning-based attack that exploits network patterns\nto detect the presence of smart IoT devices and running services in the WiFi\nradio spectrum. We perform an extensive measurement campaign of data\ncollection, and we build up a model describing the traffic patterns\ncharacterizing three popular IoT smart home devices, i.e., Google Nest Mini,\nAmazon Echo, and Amazon Echo Dot. We prove that it is possible to detect and\nidentify with overwhelming probability their presence and the services running\nby the aforementioned devices in a crowded WiFi scenario. This work proves that\nstandard encryption techniques alone are not sufficient to protect the privacy\nof the end-user, since the network traffic itself exposes the presence of both\nthe device and the associated service. While more work is required to prevent\nnon-trusted third parties to detect and identify the user's devices, we\nintroduce Eclipse, a technique to mitigate these types of attacks, which\nreshapes the traffic making the identification of the devices and the\nassociated services similar to the random classification baseline.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 13:28:59 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 19:53:37 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 15:06:06 GMT"}, {"version": "v4", "created": "Sun, 25 Jul 2021 08:26:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Hussain", "Ahmed Mohamed", ""], ["Oligeri", "Gabriele", ""], ["Voigt", "Thiemo", ""]]}, {"id": "2009.07691", "submitter": "Ioannis Zografopoulos", "authors": "Abraham Peedikayil Kuruvila, Ioannis Zografopoulos, Kanad Basu,\n  Charalambos Konstantinou", "title": "Hardware-Assisted Detection of Firmware Attacks in Inverter-Based\n  Cyberphysical Microgrids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electric grid modernization effort relies on the extensive deployment of\nmicrogrid (MG) systems. MGs integrate renewable resources and energy storage\nsystems, allowing to generate economic and zero-carbon footprint electricity,\ndeliver sustainable energy to communities using local energy resources, and\nenhance grid resilience. MGs as cyberphysical systems include interconnected\ndevices that measure, control, and actuate energy resources and loads. For\noptimal operation, cyberphysical MGs regulate the onsite energy generation\nthrough support functions enabled by smart inverters. Smart inverters, being\nconsumer electronic firmware-based devices, are susceptible to increasing\nsecurity threats. If inverters are maliciously controlled, they can\nsignificantly disrupt MG operation and electricity delivery as well as impact\nthe grid stability. In this paper, we demonstrate the impact of\ndenial-of-service (DoS) as well as controller and setpoint modification attacks\non a simulated MG system. Furthermore, we employ custom-built hardware\nperformance counters (HPCs) as design-for-security (DfS) primitives to detect\nmalicious firmware modifications on MG inverters. The proposed HPCs measure\nperiodically the order of various instruction types within the MG inverter's\nfirmware code. Our experiments illustrate that the firmware modifications are\nsuccessfully identified by our custom-built HPCs utilizing various machine\nlearning-based classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 13:51:50 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 14:31:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kuruvila", "Abraham Peedikayil", ""], ["Zografopoulos", "Ioannis", ""], ["Basu", "Kanad", ""], ["Konstantinou", "Charalambos", ""]]}, {"id": "2009.07707", "submitter": "Zhi Wang", "authors": "Zhi Wang, Chaoge Liu, Xiang Cui, Di Wu, Jie Yin, Jiaxi Liu, Jialong\n  Zhang", "title": "DeepC2: AI-powered Covert Botnet Command and Control on OSNs", "comments": "13 pages, 15 figures, 7 tables. Discussion on possible\n  countermeasures updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Botnets are one of the major threats to computer security. In previous botnet\ncommand and control (C&C) scenarios using online social networks (OSNs),\nmethods for addressing (e.g., IDs, links, or DGAs) are hardcoded into bots.\nOnce a bot is reverse engineered, the botmaster and C&C infrastructure will be\nexposed. Additionally, abnormal content from explicit commands may expose\nbotmasters and raise anomalies on OSNs. To overcome these deficiencies, we\nproposed DeepC2, an AI-powered covert C&C method on OSNs. By leveraging neural\nnetworks, bots can find botmasters by avatars, which are converted into feature\nvectors and embedded into bots. Adversaries cannot infer botmasters' accounts\nfrom the vectors. Commands are embedded into normal contents (e.g., tweets and\ncomments) using text data augmentation and hash collision. Experiments on\nTwitter show that command-embedded contents can be generated efficiently, and\nbots can find botmasters and obtain commands accurately. Security analysis on\ndifferent scenarios show that DeepC2 is robust and hard to be shut down. By\ndemonstrating how AI may help promote covert communication on OSNs, this work\nprovides a new perspective on botnet detection and confrontation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 14:31:49 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 09:11:18 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 12:07:41 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 08:20:16 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Zhi", ""], ["Liu", "Chaoge", ""], ["Cui", "Xiang", ""], ["Wu", "Di", ""], ["Yin", "Jie", ""], ["Liu", "Jiaxi", ""], ["Zhang", "Jialong", ""]]}, {"id": "2009.07753", "submitter": "Erick Galinkin", "authors": "Erick Galinkin", "title": "Malicious Network Traffic Detection via Deep Learning: An Information\n  Theoretic View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The attention that deep learning has garnered from the academic community and\nindustry continues to grow year over year, and it has been said that we are in\na new golden age of artificial intelligence research. However, neural networks\nare still often seen as a \"black box\" where learning occurs but cannot be\nunderstood in a human-interpretable way. Since these machine learning systems\nare increasingly being adopted in security contexts, it is important to explore\nthese interpretations. We consider an Android malware traffic dataset for\napproaching this problem. Then, using the information plane, we explore how\nhomeomorphism affects learned representation of the data and the invariance of\nthe mutual information captured by the parameters on that data. We empirically\nvalidate these results, using accuracy as a second measure of similarity of\nlearned representations.\n  Our results suggest that although the details of learned representations and\nthe specific coordinate system defined over the manifold of all parameters\ndiffer slightly, the functional approximations are the same. Furthermore, our\nresults show that since mutual information remains invariant under\nhomeomorphism, only feature engineering methods that alter the entropy of the\ndataset will change the outcome of the neural network. This means that for some\ndatasets and tasks, neural networks require meaningful, human-driven feature\nengineering or changes in architecture to provide enough information for the\nneural network to generate a sufficient statistic. Applying our results can\nserve to guide analysis methods for machine learning engineers and suggests\nthat neural networks that can exploit the convolution theorem are equally\naccurate as standard convolutional neural networks, and can be more\ncomputationally efficient.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 15:37:44 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Galinkin", "Erick", ""]]}, {"id": "2009.07773", "submitter": "Joseph Gravellier", "authors": "Joseph Gravellier, Jean-Max Dutertre, Yannick Teglia, Philippe Loubet\n  Moundi", "title": "SideLine: How Delay-Lines (May) Leak Secrets from your SoC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the ever-growing need for performance in silicon devices, SoC\nproviders have been increasingly relying on software-hardware cooperation. By\ncontrolling hardware resources such as power or clock management from the\nsoftware, developers earn the possibility to build more flexible and power\nefficient applications. Despite the benefits, these hardware components are now\nexposed to software code and can potentially be misused as open-doors to\njeopardize trusted environments, perform privilege escalation or steal\ncryptographic secrets. In this work, we introduce SideLine, a novel\nside-channel vector based on delay-line components widely implemented in\nhigh-end SoCs. After providing a detailed method on how to access and convert\ndelay-line data into power consumption information, we demonstrate that these\nentities can be used to perform remote power side-channel attacks. We report\nexperiments carried out on two SoCs from distinct vendors and we recount\nseveral core-vs-core attack scenarios in which an adversary process located in\none processor core aims at eavesdropping the activity of a victim process\nlocated in another core. For each scenario, we demonstrate the adversary\nability to fully recover the secret key of an OpenSSL AES running in the victim\ncore. Even more detrimental, we show that these attacks are still practicable\nif the victim or the attacker program runs over an operating system.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:05:23 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Gravellier", "Joseph", ""], ["Dutertre", "Jean-Max", ""], ["Teglia", "Yannick", ""], ["Moundi", "Philippe Loubet", ""]]}, {"id": "2009.07834", "submitter": "Andriy Miranskyy", "authors": "William Pourmajidi and Lei Zhang and John Steinbacher and Tony Erwin\n  and Andriy Miranskyy", "title": "Immutable Log Storage as a Service on Private and Public Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the normal operation of a Cloud solution, no one pays attention to the\nlogs except the system reliability engineers, who may periodically check them\nto ensure that the Cloud platform's performance conforms to the Service Level\nAgreements (SLA). However, the moment a component fails, or a customer\ncomplains about a breach of SLA, the importance of logs increases\nsignificantly. All departments, including management, customer support, and\neven the actual customer, may turn to logs to determine the cause and timeline\nof the issue and to find the party responsible for the issue. The party at\nfault may be motivated to tamper with the logs to hide their role. Given the\nnumber and volume of logs generated by the Cloud platforms, many tampering\nopportunities exist. We argue that the critical nature of logs calls for\nimmutability and verification mechanism without the presence of a single\ntrusted party.\n  This paper proposes such a mechanism by describing a blockchain-based log\nsystem, called Logchain, which can be integrated with existing private and\npublic blockchain solutions. Logchain uses the immutability feature of\nblockchain to provide a tamper-resistance storage platform for log storage.\nAdditionally, we propose a hierarchical structure to combine the hash-binding\nof two blockchains to address blockchains' scalability issues. To validate the\nmechanism, we integrate Logchain into two different types of blockchains. We\nchoose Ethereum as a public, permission-less blockchain and IBM Blockchain as a\nprivate, permission-based one. We show that the solution is scalable on both\nthe private and public blockchains. Additionally, we perform the analysis of\nthe cost of ownership for private and public blockchains implementations to\nhelp a reader selecting an implementation that would be applicable to their\nneeds.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:51:48 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Pourmajidi", "William", ""], ["Zhang", "Lei", ""], ["Steinbacher", "John", ""], ["Erwin", "Tony", ""], ["Miranskyy", "Andriy", ""]]}, {"id": "2009.07884", "submitter": "Eleanor Birrell", "authors": "Sean O'Connor, Ryan Nurwono, Aden Siebel, Eleanor Birrell", "title": "(Un)clear and (In)conspicuous: The right to opt-out of sale under CCPA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The California Consumer Privacy Act (CCPA) -- which began enforcement on July\n1, 2020 -- grants California users the affirmative right to opt-out of the sale\nof their personal information. In this work, we perform a series of\nobservational studies to understand how websites implement this right. We\nperform two manual analyses of the top 500 U.S. websites (one conducted in July\n2020 and a second conducted in January 2021) and classify how each site\nimplements this new requirement. We also perform an automated analysis of the\nTop 5000 U.S. websites. We find that the vast majority of sites that implement\nopt-out mechanisms do so with a Do Not Sell link rather than with a privacy\nbanner, and that many of the linked opt-out controls exhibit features such as\nnudging and indirect mechanisms (e.g., fillable forms). We then perform a pair\nof user studies with 4357 unique users (recruited from Google Ads and Amazon\nMechanical Turk) in which we observe how users interact with different opt-out\nmechanisms and evaluate how the implementation choices we observed -- exclusive\nuse of links, prevalent nudging, and indirect mechanisms -- affect the rate at\nwhich users exercise their right to opt-out of sale. We find that these design\nelements significantly deter interactions with opt-out mechanisms -- including\nreducing the opt-out rate for users who are uncomfortable with the sale of\ntheir information -- and that they reduce users' awareness of their ability to\nopt-out. Our results demonstrate the importance of regulations that provide\nclear implementation requirements in order empower users to exercise their\nprivacy rights.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 18:31:23 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 15:59:28 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["O'Connor", "Sean", ""], ["Nurwono", "Ryan", ""], ["Siebel", "Aden", ""], ["Birrell", "Eleanor", ""]]}, {"id": "2009.07937", "submitter": "Richa Varma", "authors": "Richa Varma, Chris Melville, Claudio Pinello, Tuhin Sahai", "title": "Post Quantum Secure Command and Control of Mobile Agents : Inserting\n  quantum-resistant encryption schemes in the Secure Robot Operating System", "comments": "8 pages, 5 figures, 3 tables, Submitted to The Fourth IEEE\n  International Conference on Robotic Computing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The secure command and control (C&C) of mobile agents arises in various\nsettings including unmanned aerial vehicles, single pilot operations in\ncommercial settings, and mobile robots to name a few. As more and more of these\napplications get integrated into aerospace and defense use cases, the security\nof the communication channel between the ground station and the mobile agent is\nof increasing importance. The development of quantum computing devices poses a\nunique threat to secure communications due to the vulnerability of asymmetric\nciphers to Shor's algorithm. Given the active development of new quantum\nresistant encryption techniques, we report the first integration of\npost-quantum secure encryption schemes with the robot operating system (ROS)\nand C&C of mobile agents, in general. We integrate these schemes in the\napplication and network layers, and study the performance of these methods by\ncomparing them to present day security schemes such as the widely used RSA\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 21:13:56 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Varma", "Richa", ""], ["Melville", "Chris", ""], ["Pinello", "Claudio", ""], ["Sahai", "Tuhin", ""]]}, {"id": "2009.07998", "submitter": "Zecheng He", "authors": "Zecheng He, Guangyuan Hu, Ruby Lee", "title": "New Models for Understanding and Reasoning about Speculative Execution\n  Attacks", "comments": "Accepted to IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectre and Meltdown attacks and their variants exploit hardware performance\noptimization features to cause security breaches. Secret information is\naccessed and leaked through covert or side channels. New attack variants keep\nappearing and we do not have a systematic way to capture the critical\ncharacteristics of these attacks and evaluate why they succeed or fail.\n  In this paper, we provide a new attack-graph model for reasoning about\nspeculative execution attacks. We model attacks as ordered dependency graphs,\nand prove that a race condition between two nodes can occur if there is a\nmissing dependency edge between them. We define a new concept, \"security\ndependency\", between a resource access and its prior authorization operation.\nWe show that a missing security dependency is equivalent to a race condition\nbetween authorization and access, which is a root cause of speculative\nexecution attacks. We show detailed examples of how our attack graph models the\nSpectre and Meltdown attacks, and is generalizable to all the attack variants\npublished so far. This attack model is also very useful for identifying new\nattacks and for generalizing defense strategies. We identify several defense\nstrategies with different performance-security tradeoffs. We show that the\ndefenses proposed so far all fit under one of our defense strategies. We also\nexplain how attack graphs can be constructed and point to this as promising\nfuture work for tool designers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:09:23 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 21:32:46 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["He", "Zecheng", ""], ["Hu", "Guangyuan", ""], ["Lee", "Ruby", ""]]}, {"id": "2009.08000", "submitter": "Albert Cheu", "authors": "Albert Cheu and Jonathan Ullman", "title": "The Limits of Pan Privacy and Shuffle Privacy for Learning and\n  Estimation", "comments": "Corrected a proof in the Appendix. Added a reference to parallel and\n  concurrent work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent wave of interest in intermediate trust models for\ndifferential privacy that eliminate the need for a fully trusted central data\ncollector, but overcome the limitations of local differential privacy. This\ninterest has led to the introduction of the shuffle model (Cheu et al.,\nEUROCRYPT 2019; Erlingsson et al., SODA 2019) and revisiting the pan-private\nmodel (Dwork et al., ITCS 2010). The message of this line of work is that, for\na variety of low-dimensional problems -- such as counts, means, and histograms\n-- these intermediate models offer nearly as much power as central differential\nprivacy. However, there has been considerably less success using these models\nfor high-dimensional learning and estimation problems. In this work, we show\nthat, for a variety of high-dimensional learning and estimation problems, both\nthe shuffle model and the pan-private model inherently incur an exponential\nprice in sample complexity relative to the central model. For example, we show\nthat, private agnostic learning of parity functions over $d$ bits requires\n$\\Omega(2^{d/2})$ samples in these models, and privately selecting the most\ncommon attribute from a set of $d$ choices requires $\\Omega(d^{1/2})$ samples,\nboth of which are exponential separations from the central model. Our work\ngives the first non-trivial lower bounds for these problems for both the\npan-private model and the general multi-message shuffle model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:15:55 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 17:22:28 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 19:05:20 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Cheu", "Albert", ""], ["Ullman", "Jonathan", ""]]}, {"id": "2009.08006", "submitter": "Thao Tran Phuong", "authors": "Tran Phuong Thao", "title": "Improving Homograph Attack Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A visual homograph attack is a way that the attacker deceives the web users\nabout which domain they are visiting by exploiting forged domains that look\nsimilar to the genuine domains. T. Thao et al. (IFIP SEC'19) proposed a\nhomograph classification by applying conventional supervised learning\nalgorithms on the features extracted from a single-character-based Structural\nSimilarity Index (SSIM). This paper aims to improve the classification accuracy\nby combining their SSIM features with 199 features extracted from a N-gram\nmodel and applying advanced ensemble learning algorithms. The experimental\nresult showed that our proposed method could enhance even 1.81% of accuracy and\nreduce 2.15% of false-positive rate. Furthermore, existing work applied machine\nlearning on some features without being able to explain why applying it can\nimprove the accuracy. Even though the accuracy could be improved, understanding\nthe ground-truth is also crucial. Therefore, in this paper, we conducted an\nerror empirical analysis and could obtain several findings behind our proposed\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:37:21 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Thao", "Tran Phuong", ""]]}, {"id": "2009.08025", "submitter": "Thao Tran Phuong", "authors": "Tran Phuong Thao", "title": "Location-based Behavioral Authentication Using GPS Distance Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current user authentication systems are based on PIN code,\npassword, or biometrics traits which can have some limitations in usage and\nsecurity. Lifestyle authentication has become a new research approach. A\npromising idea for it is to use the location history since it is relatively\nunique. Even when people are living in the same area or have occasional travel,\nit does not vary from day to day. For Global Positioning System (GPS) data, the\nprevious work used the longitude, the latitude, and the timestamp as the\nfeatures for the classification. In this paper, we investigate a new approach\nutilizing the distance coherence which can be extracted from the GPS itself\nwithout the need to require other information. We applied three ensemble\nclassification RandomForest, ExtraTrees, and Bagging algorithms; and the\nexperimental result showed that the approach can achieve 99.42%, 99.12%, and\n99.25% of accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:26:20 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Thao", "Tran Phuong", ""]]}, {"id": "2009.08063", "submitter": "Ruixuan Liu", "authors": "Ruixuan Liu, Yang Cao, Hong Chen, Ruoyang Guo, Masatoshi Yoshikawa", "title": "FLAME: Differentially Private Federated Learning in the Shuffle Model", "comments": "accepted by AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a promising machine learning paradigm that enables\nthe analyzer to train a model without collecting users' raw data. To ensure\nusers' privacy, differentially private federated learning has been intensively\nstudied. The existing works are mainly based on the \\textit{curator model} or\n\\textit{local model} of differential privacy. However, both of them have pros\nand cons. The curator model allows greater accuracy but requires a trusted\nanalyzer. In the local model where users randomize local data before sending\nthem to the analyzer, a trusted analyzer is not required but the accuracy is\nlimited. In this work, by leveraging the \\textit{privacy amplification} effect\nin the recently proposed shuffle model of differential privacy, we achieve the\nbest of two worlds, i.e., accuracy in the curator model and strong privacy\nwithout relying on any trusted party. We first propose an FL framework in the\nshuffle model and a simple protocol (SS-Simple) extended from existing work. We\nfind that SS-Simple only provides an insufficient privacy amplification effect\nin FL since the dimension of the model parameter is quite large. To solve this\nchallenge, we propose an enhanced protocol (SS-Double) to increase the privacy\namplification effect by subsampling. Furthermore, for boosting the utility when\nthe model size is greater than the user population, we propose an advanced\nprotocol (SS-Topk) with gradient sparsification techniques. We also provide\ntheoretical analysis and numerical evaluations of the privacy amplification of\nthe proposed protocols. Experiments on real-world dataset validate that SS-Topk\nimproves the testing accuracy by 60.7\\% than the local model based FL.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 04:44:27 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 02:40:56 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 05:55:16 GMT"}, {"version": "v4", "created": "Sat, 20 Mar 2021 09:05:39 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Ruixuan", ""], ["Cao", "Yang", ""], ["Chen", "Hong", ""], ["Guo", "Ruoyang", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "2009.08120", "submitter": "Kim Hammar", "authors": "Kim Hammar and Rolf Stadler", "title": "Finding Effective Security Strategies through Reinforcement Learning and\n  Self-Play", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.14128.38405", "report-no": null, "categories": "cs.LG cs.CR cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to automatically find security strategies for the use\ncase of intrusion prevention. Following this method, we model the interaction\nbetween an attacker and a defender as a Markov game and let attack and defense\nstrategies evolve through reinforcement learning and self-play without human\nintervention. Using a simple infrastructure configuration, we demonstrate that\neffective security strategies can emerge from self-play. This shows that\nself-play, which has been applied in other domains with great success, can be\neffective in the context of network security. Inspection of the converged\npolicies show that the emerged policies reflect common-sense knowledge and are\nsimilar to strategies of humans. Moreover, we address known challenges of\nreinforcement learning in this domain and present an approach that uses\nfunction approximation, an opponent pool, and an autoregressive policy\nrepresentation. Through evaluations we show that our method is superior to two\nbaseline methods but that policy convergence in self-play remains a challenge.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 07:41:27 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 16:22:54 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hammar", "Kim", ""], ["Stadler", "Rolf", ""]]}, {"id": "2009.08148", "submitter": "Michele Campobasso", "authors": "Michele Campobasso, Pavlo Burda and Luca Allodi", "title": "CARONTE: Crawling Adversarial Resources Over Non-Trusted, High-Profile\n  Environments", "comments": null, "journal-ref": "2019 IEEE European Symposium on Security and Privacy Workshops\n  (EuroS&PW), Stockholm, Sweden, 2019, pp. 433-442", "doi": "10.1109/EuroSPW.2019.00055", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monitoring of underground criminal activities is often automated to\nmaximize the data collection and to train ML models to automatically adapt data\ncollection tools to different communities. On the other hand, sophisticated\nadversaries may adopt crawling-detection capabilities that may significantly\njeopardize researchers' opportunities to perform the data collection, for\nexample by putting their accounts under the spotlight and being expelled from\nthe community. This is particularly undesirable in prominent and high-profile\ncriminal communities where entry costs are significant (either monetarily or\nfor example for background checking or other trust-building mechanisms). This\npaper presents CARONTE, a tool to semi-automatically learn virtually any forum\nstructure for parsing and data-extraction, while maintaining a low profile for\nthe data collection and avoiding the requirement of collecting massive datasets\nto maintain tool scalability. We showcase the tool against four underground\nforums, and compare the network traffic it generates (as seen from the\nadversary's position, i.e. the underground community's server) against\nstate-of-the-art tools for web-crawling as well as human users.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 08:32:44 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Campobasso", "Michele", ""], ["Burda", "Pavlo", ""], ["Allodi", "Luca", ""]]}, {"id": "2009.08294", "submitter": "Amir Alansary", "authors": "Matei Grama, Maria Musat, Luis Mu\\~noz-Gonz\\'alez, Jonathan\n  Passerat-Palmbach, Daniel Rueckert, Amir Alansary", "title": "Robust Aggregation for Adaptive Privacy Preserving Federated Learning in\n  Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has enabled training models collaboratively from\nmultiple data owning parties without sharing their data. Given the privacy\nregulations of patient's healthcare data, learning-based systems in healthcare\ncan greatly benefit from privacy-preserving FL approaches. However, typical\nmodel aggregation methods in FL are sensitive to local model updates, which may\nlead to failure in learning a robust and accurate global model. In this work,\nwe implement and evaluate different robust aggregation methods in FL applied to\nhealthcare data. Furthermore, we show that such methods can detect and discard\nfaulty or malicious local clients during training. We run two sets of\nexperiments using two real-world healthcare datasets for training medical\ndiagnosis classification tasks. Each dataset is used to simulate the\nperformance of three different robust FL aggregation strategies when facing\ndifferent poisoning attacks. The results show that privacy preserving methods\ncan be successfully applied alongside Byzantine-robust aggregation techniques.\nWe observed in particular how using differential privacy (DP) did not\nsignificantly impact the final learning convergence of the different\naggregation strategies.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 13:41:26 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Grama", "Matei", ""], ["Musat", "Maria", ""], ["Mu\u00f1oz-Gonz\u00e1lez", "Luis", ""], ["Passerat-Palmbach", "Jonathan", ""], ["Rueckert", "Daniel", ""], ["Alansary", "Amir", ""]]}, {"id": "2009.08313", "submitter": "Mar\\'ia \\'Oskarsd\\'ottir", "authors": "Mar\\'ia \\'Oskarsd\\'ottir, Waqas Ahmed, Katrien Antonio, Bart Baesens,\n  R\\'emi Dendievel, Tom Donas, Tom Reynkens", "title": "Social network analytics for supervised fraud detection in insurance", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insurance fraud occurs when policyholders file claims that are exaggerated or\nbased on intentional damages. This contribution develops a fraud detection\nstrategy by extracting insightful information from the social network of a\nclaim. First, we construct a network by linking claims with all their involved\nparties, including the policyholders, brokers, experts, and garages. Next, we\nestablish fraud as a social phenomenon in the network and use the BiRank\nalgorithm with a fraud specific query vector to compute a fraud score for each\nclaim. From the network, we extract features related to the fraud scores as\nwell as the claims' neighborhood structure. Finally, we combine these network\nfeatures with the claim-specific features and build a supervised model with\nfraud in motor insurance as the target variable. Although we build a model for\nonly motor insurance, the network includes claims from all available lines of\nbusiness. Our results show that models with features derived from the network\nperform well when detecting fraud and even outperform the models using only the\nclassical claim-specific features. Combining network and claim-specific\nfeatures further improves the performance of supervised learning models to\ndetect fraud. The resulting model flags highly suspicions claims that need to\nbe further investigated. Our approach provides a guided and intelligent\nselection of claims and contributes to a more effective fraud investigation\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 21:40:15 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["\u00d3skarsd\u00f3ttir", "Mar\u00eda", ""], ["Ahmed", "Waqas", ""], ["Antonio", "Katrien", ""], ["Baesens", "Bart", ""], ["Dendievel", "R\u00e9mi", ""], ["Donas", "Tom", ""], ["Reynkens", "Tom", ""]]}, {"id": "2009.08401", "submitter": "Davide Berardi", "authors": "Davide Berardi, Franco Callegati, Andrea Melis, Marco Prandini", "title": "Password similarity using probabilistic data structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Passwords should be easy to remember, yet expiration policies mandate their\nfrequent change. Caught in the crossfire between these conflicting\nrequirements, users often adopt creative methods to perform slight variations\nover time. While easily fooling the most basic checks for similarity, these\nschemes lead to a substantial decrease in actual security, because leaked\npasswords, albeit expired, can be effectively exploited as seeds for crackers.\nThis work describes an approach based on Bloom filters to detect password\nsimilarity, which can be used to discourage password reuse habits. The proposed\nscheme intrinsically obfuscates the stored passwords to protect them in case of\ndatabase leaks, and can be tuned to be resistant to common cryptanalytic\ntechniques, making it suitable for usage on exposed systems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 16:21:37 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Berardi", "Davide", ""], ["Callegati", "Franco", ""], ["Melis", "Andrea", ""], ["Prandini", "Marco", ""]]}, {"id": "2009.08435", "submitter": "Youwei Liang", "authors": "Youwei Liang, Dong Huang", "title": "Large Norms of CNN Layers Do Not Hurt Adversarial Robustness", "comments": "15 pages, 4 figures; v5: corrected typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the Lipschitz properties of CNN are widely considered to be related to\nadversarial robustness, we theoretically characterize the $\\ell_1$ norm and\n$\\ell_\\infty$ norm of 2D multi-channel convolutional layers and provide\nefficient methods to compute the exact $\\ell_1$ norm and $\\ell_\\infty$ norm.\nBased on our theorem, we propose a novel regularization method termed norm\ndecay, which can effectively reduce the norms of convolutional layers and\nfully-connected layers. Experiments show that norm-regularization methods,\nincluding norm decay, weight decay, and singular value clipping, can improve\ngeneralization of CNNs. However, they can slightly hurt adversarial robustness.\nObserving this unexpected phenomenon, we compute the norms of layers in the\nCNNs trained with three different adversarial training frameworks and\nsurprisingly find that adversarially robust CNNs have comparable or even larger\nlayer norms than their non-adversarially robust counterparts. Furthermore, we\nprove that under a mild assumption, adversarially robust classifiers can be\nachieved, and can have an arbitrarily large Lipschitz constant. For this\nreason, enforcing small norms on CNN layers may be neither necessary nor\neffective in achieving adversarial robustness. The code is available at\nhttps://github.com/youweiliang/norm_robustness.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:33:50 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 06:28:59 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 07:34:27 GMT"}, {"version": "v4", "created": "Tue, 29 Dec 2020 02:58:32 GMT"}, {"version": "v5", "created": "Thu, 10 Jun 2021 06:21:01 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liang", "Youwei", ""], ["Huang", "Dong", ""]]}, {"id": "2009.08559", "submitter": "Abhinav Aggarwal", "authors": "Abhinav Aggarwal, Zekun Xu, Oluwaseyi Feyisetan, Nathanael Teissier", "title": "On Primes, Log-Loss Scores and (No) Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Membership Inference Attacks exploit the vulnerabilities of exposing models\ntrained on customer data to queries by an adversary. In a recently proposed\nimplementation of an auditing tool for measuring privacy leakage from sensitive\ndatasets, more refined aggregates like the Log-Loss scores are exposed for\nsimulating inference attacks as well as to assess the total privacy leakage\nbased on the adversary's predictions. In this paper, we prove that this\nadditional information enables the adversary to infer the membership of any\nnumber of datapoints with full accuracy in a single query, causing complete\nmembership privacy breach. Our approach obviates any attack model training or\naccess to side knowledge with the adversary. Moreover, our algorithms are\nagnostic to the model under attack and hence, enable perfect membership\ninference even for models that do not memorize or overfit. In particular, our\nobservations provide insight into the extent of information leakage from\nstatistical aggregates and how they can be exploited.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 23:35:12 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Aggarwal", "Abhinav", ""], ["Xu", "Zekun", ""], ["Feyisetan", "Oluwaseyi", ""], ["Teissier", "Nathanael", ""]]}, {"id": "2009.08681", "submitter": "Georg Maringer", "authors": "Georg Maringer and Sven Puchinger and Antonia Wachter-Zeh", "title": "Information- and Coding-Theoretic Analysis of the RLWE Channel", "comments": "13 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several cryptosystems based on the \\emph{Ring Learning with Errors} (RLWE)\nproblem have been proposed within the NIST post-quantum cryptography\nstandardization process, e.g. NewHope. Furthermore, there are systems like\nKyber which are based on the closely related MLWE assumption. Both previously\nmentioned schemes feature a non-zero decryption failure rate (DFR). The\ncombination of encryption and decryption for these kinds of algorithms can be\ninterpreted as data transmission over noisy channels. To the best of our\nknowledge this paper is the first work that analyzes the capacity of this\nchannel. We show how to modify the encryption schemes such that the input\nalphabets of the corresponding channels are increased. In particular, we\npresent lower bounds on their capacities which show that the transmission rate\ncan be significantly increased compared to standard proposals in the\nliterature. Furthermore, under the common assumption of stochastically\nindependent coefficient failures, we give lower bounds on achievable rates\nbased on both the Gilbert-Varshamov bound and concrete code constructions using\nBCH codes. By means of our constructions, we can either increase the total\nbitrate (by a factor of $1.84$ for Kyber and by factor of $7$ for NewHope)\nwhile guaranteeing the same \\emph{decryption failure rate} (DFR). Moreover, for\nthe same bitrate, we can significantly reduce the DFR for all schemes\nconsidered in this work (e.g., for NewHope from $2^{-216}$ to $2^{-12769}$).\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 08:19:56 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 17:01:46 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Maringer", "Georg", ""], ["Puchinger", "Sven", ""], ["Wachter-Zeh", "Antonia", ""]]}, {"id": "2009.08697", "submitter": "Shangwei Guo", "authors": "Shangwei Guo, Tianwei Zhang, Han Qiu, Yi Zeng, Tao Xiang, and Yang Liu", "title": "Fine-tuning Is Not Enough: A Simple yet Effective Watermark Removal\n  Attack for DNN Models", "comments": "7 pages, 4 figures, accpeted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watermarking has become the tendency in protecting the intellectual property\nof DNN models. Recent works, from the adversary's perspective, attempted to\nsubvert watermarking mechanisms by designing watermark removal attacks.\nHowever, these attacks mainly adopted sophisticated fine-tuning techniques,\nwhich have certain fatal drawbacks or unrealistic assumptions. In this paper,\nwe propose a novel watermark removal attack from a different perspective.\nInstead of just fine-tuning the watermarked models, we design a simple yet\npowerful transformation algorithm by combining imperceptible pattern embedding\nand spatial-level transformations, which can effectively and blindly destroy\nthe memorization of watermarked models to the watermark samples. We also\nintroduce a lightweight fine-tuning strategy to preserve the model performance.\nOur solution requires much less resource or knowledge about the watermarking\nscheme than prior works. Extensive experimental results indicate that our\nattack can bypass state-of-the-art watermarking solutions with very high\nsuccess rates. Based on our attack, we propose watermark augmentation\ntechniques to enhance the robustness of existing watermarks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:14:54 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 06:23:29 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Guo", "Shangwei", ""], ["Zhang", "Tianwei", ""], ["Qiu", "Han", ""], ["Zeng", "Yi", ""], ["Xiang", "Tao", ""], ["Liu", "Yang", ""]]}, {"id": "2009.08739", "submitter": "Ruoxin Chen", "authors": "Ruoxin Chen, Jie Li, Chentao Wu, Bin Sheng, Ping Li", "title": "A Framework of Randomized Selection Based Certified Defenses Against\n  Data Poisoning Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network classifiers are vulnerable to data poisoning attacks, as\nattackers can degrade or even manipulate their predictions thorough poisoning\nonly a few training samples. However, the robustness of heuristic defenses is\nhard to measure. Random selection based defenses can achieve certified\nrobustness by averaging the classifiers' predictions on the sub-datasets\nsampled from the training set. This paper proposes a framework of random\nselection based certified defenses against data poisoning attacks.\nSpecifically, we prove that the random selection schemes that satisfy certain\nconditions are robust against data poisoning attacks. We also derive the\nanalytical form of the certified radius for the qualified random selection\nschemes. The certified radius of bagging derived by our framework is tighter\nthan the previous work. Our framework allows users to improve robustness by\nleveraging prior knowledge about the training set and the poisoning model.\nGiven higher level of prior knowledge, we can achieve higher certified accuracy\nboth theoretically and practically. According to the experiments on three\nbenchmark datasets: MNIST 1/7, MNIST, and CIFAR-10, our method outperforms the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 10:38:12 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 09:33:36 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Chen", "Ruoxin", ""], ["Li", "Jie", ""], ["Wu", "Chentao", ""], ["Sheng", "Bin", ""], ["Li", "Ping", ""]]}, {"id": "2009.08772", "submitter": "Jens Hiller", "authors": "Jens Hiller, Johanna Amann, and Oliver Hohlfeld", "title": "The Boon and Bane of Cross-Signing: Shedding Light on a Common Practice\n  in Public Key Infrastructures", "comments": "Extended version of paper accepted at 2020 ACM SIGSAC Conference on\n  Computer and Communications Security (CCS '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public Key Infrastructures (PKIs) with their trusted Certificate Authorities\n(CAs) provide the trust backbone for the Internet: CAs sign certificates which\nprove the identity of servers, applications, or users. To be trusted by\noperating systems and browsers, a CA has to undergo lengthy and costly\nvalidation processes. Alternatively, trusted CAs can cross-sign other CAs to\nextend their trust to them. In this paper, we systematically analyze the\npresent and past state of cross-signing in the Web PKI. Our dataset (derived\nfrom passive TLS monitors and public CT logs) encompasses more than 7 years and\n225 million certificates with 9.3 billion trust paths. We show benefits and\nrisks of cross-signing. We discuss the difficulty of revoking trusted CA\ncertificates where, worrisome, cross-signing can result in valid trust paths to\nremain after revocation; a problem for non-browser software that often blindly\ntrusts all CA certificates and ignores revocations. However, cross-signing also\nenables fast bootstrapping of new CAs, e.g., Let's Encrypt, and achieves a\nnon-disruptive user experience by providing backward compatibility. In this\npaper, we propose new rules and guidance for cross-signing to preserve its\npositive potential while mitigating its risks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 12:16:09 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Hiller", "Jens", ""], ["Amann", "Johanna", ""], ["Hohlfeld", "Oliver", ""]]}, {"id": "2009.08842", "submitter": "Solon Falas", "authors": "Solon Falas, Charalambos Konstantinou, Maria K. Michael", "title": "Physics-Informed Neural Networks for Securing Water Distribution Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics-informed neural networks (PINNs) is an emerging category of neural\nnetworks which can be trained to solve supervised learning tasks while taking\ninto consideration given laws of physics described by general nonlinear partial\ndifferential equations. PINNs demonstrate promising characteristics such as\nperformance and accuracy using minimal amount of data for training, utilized to\naccurately represent the physical properties of a system's dynamic environment.\nIn this work, we employ the emerging paradigm of PINNs to demonstrate their\npotential in enhancing the security of intelligent cyberphysical systems. In\nparticular, we present a proof-of-concept scenario using the use case of water\ndistribution networks, which involves an attack on a controller in charge of\nregulating a liquid pump through liquid flow sensor measurements. PINNs are\nused to mitigate the effects of the attack while demonstrating the\napplicability and challenges of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 13:55:24 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Falas", "Solon", ""], ["Konstantinou", "Charalambos", ""], ["Michael", "Maria K.", ""]]}, {"id": "2009.08898", "submitter": "Mengce Zheng", "authors": "Minhui Jin, Mengce Zheng, Honggang Hu, Nenghai Yu", "title": "An Enhanced Convolutional Neural Network in Side-Channel Attacks and Its\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the convolutional neural networks (CNNs) have received a lot\nof interest in the side-channel community. The previous work has shown that\nCNNs have the potential of breaking the cryptographic algorithm protected with\nmasking or desynchronization. Before, several CNN models have been exploited,\nreaching the same or even better level of performance compared to the\ntraditional side-channel attack (SCA). In this paper, we investigate the\narchitecture of Residual Network and build a new CNN model called attention\nnetwork. To enhance the power of the attention network, we introduce an\nattention mechanism - Convolutional Block Attention Module (CBAM) and\nincorporate CBAM into the CNN architecture. CBAM points out the informative\npoints of the input traces and makes the attention network focus on the\nrelevant leakages of the measurements. It is able to improve the performance of\nthe CNNs. Because the irrelevant points will introduce the extra noises and\ncause a worse performance of attacks. We compare our attention network with the\none designed for the masking AES implementation called ASCAD network in this\npaper. We show that the attention network has a better performance than the\nASCAD network. Finally, a new visualization method, named Class Gradient\nVisualization (CGV) is proposed to recognize which points of the input traces\nhave a positive influence on the predicted result of the neural networks. In\nanother aspect, it can explain why the attention network is superior to the\nASCAD network. We validate the attention network through extensive experiments\non four public datasets and demonstrate that the attention network is efficient\nin different AES implementations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:44:25 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Jin", "Minhui", ""], ["Zheng", "Mengce", ""], ["Hu", "Honggang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2009.08907", "submitter": "Borzoo Bonakdarpour", "authors": "Tzu-Han Hsu, Cesar Sanchez, and Borzoo Bonakdarpour", "title": "Bounded Model Checking for Hyperproperties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperproperties are properties of systems that relate multiple computation\ntraces, including security and concurrency properties. This paper introduces a\nbounded model checking (BMC) algorithm for hyperproperties expressed in\nHyperLTL, which - to the best of our knowledge - is the first such algorithm.\nJust as the classic BMC technique for LTL primarily aims at finding bugs, our\napproach also targets identifying counterexamples. BMC for LTL is reduced to\nSAT solving, because LTL describes a property via inspecting individual traces.\nHyperLTL allows explicit and simultaneous quantification over traces and\ndescribes properties that involves multiple traces and, hence, our BMC approach\nnaturally reduces to QBF solving. We report on successful and efficient model\nchecking, implemented in a tool called HyperQube, of a rich set of experiments\non a variety of case studies, including security, concurrent data structures,\npath planning for robots, and testing.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 16:07:33 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 23:43:49 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hsu", "Tzu-Han", ""], ["Sanchez", "Cesar", ""], ["Bonakdarpour", "Borzoo", ""]]}, {"id": "2009.09052", "submitter": "Giuseppe Vietri", "authors": "Giuseppe Vietri, Borja Balle, Akshay Krishnamurthy, Zhiwei Steven Wu", "title": "Private Reinforcement Learning with PAC and Regret Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by high-stakes decision-making domains like personalized medicine\nwhere user information is inherently sensitive, we design privacy preserving\nexploration policies for episodic reinforcement learning (RL). We first provide\na meaningful privacy formulation using the notion of joint differential privacy\n(JDP)--a strong variant of differential privacy for settings where each user\nreceives their own sets of output (e.g., policy recommendations). We then\ndevelop a private optimism-based learning algorithm that simultaneously\nachieves strong PAC and regret bounds, and enjoys a JDP guarantee. Our\nalgorithm only pays for a moderate privacy cost on exploration: in comparison\nto the non-private bounds, the privacy parameter only appears in lower-order\nterms. Finally, we present lower bounds on sample complexity and regret for\nreinforcement learning subject to JDP.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 20:18:35 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Vietri", "Giuseppe", ""], ["Balle", "Borja", ""], ["Krishnamurthy", "Akshay", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "2009.09075", "submitter": "Juan Carlos Olivares Rojas", "authors": "Juan C. Olivares-Rojas, Enrique Reyes-Archundia, Jos\\'e A.\n  Guti\\'errez-Gnecchi, Ismael Molina-Moreno", "title": "A Survey on Smart Metering Systems using Blockchain for E-Mobility", "comments": "2020 IEEE 4th Electric Vehicle International Symposium", "journal-ref": null, "doi": null, "report-no": "Tecnol\\'ogico Nacional de M\\'exico grants 7948.20-P and 8000.20-P", "categories": "cs.CY cs.CR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electricity is an essential comfort to support our daily activities. With the\ncompetitive increase and energy costs by the industry, new values and\nopportunities for delivering electricity to customers are produced. One of\nthese new opportunities is electric vehicles. With the arrival of electric\nvehicles, various challenges and opportunities are being presented in the\nelectric power system worldwide. For example, under the traditional electric\npower billing scheme, electric power has to be consumed where it is needed so\nthat end-users could not charge their electric vehicles at different points\n(e.g. a relative's house) if this the correct user is not billed (this due to\nthe high consumption of electrical energy that makes it expensive). To achieve\nelectric mobility, they must solve new challenges, such as the smart metering\nof energy consumption and the cybersecurity of these measurements. The present\nwork shows a study of the different smart metering technologies that use\nblockchain and other security mechanisms to achieve e-mobility.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 22:55:25 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Olivares-Rojas", "Juan C.", ""], ["Reyes-Archundia", "Enrique", ""], ["Guti\u00e9rrez-Gnecchi", "Jos\u00e9 A.", ""], ["Molina-Moreno", "Ismael", ""]]}, {"id": "2009.09090", "submitter": "Gururaj Saileshwar", "authors": "Gururaj Saileshwar and Moinuddin Qureshi", "title": "MIRAGE: Mitigating Conflict-Based Cache Attacks with a Practical\n  Fully-Associative Design", "comments": "Accepted to appear in USENIX Security 2021. This camera-ready version\n  has an updated Security discussion (Sec-5, Sec-6) and Appendix (new Gem5\n  results) compared to previous Arxiv version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared processor caches are vulnerable to conflict-based side-channel\nattacks, where an attacker can monitor access patterns of a victim by evicting\nvictim cache lines using cache-set conflicts. Recent mitigations propose\nrandomized mapping of addresses to cache lines to obfuscate the locations of\nset-conflicts. However, these are vulnerable to new attacks that discover\nconflicting sets of addresses despite such mitigations, because these designs\nselect eviction-candidates from a small set of conflicting lines.\n  This paper presents Mirage, a practical design for a fully associative cache,\nwherein eviction candidates are selected randomly from all lines resident in\nthe cache, to be immune to set-conflicts. A key challenge for enabling such\ndesigns in large shared caches (containing tens of thousands of cache lines) is\nthe complexity of cache-lookup, as a naive design can require searching through\nall the resident lines. Mirage achieves full-associativity while retaining\npractical set-associative lookups by decoupling placement and replacement,\nusing pointer-based indirection from tag-store to data-store to allow a newly\ninstalled address to globally evict the data of any random resident line. To\neliminate set-conflicts, Mirage provisions extra invalid tags in a\nskewed-associative tag-store design where lines can be installed without\nset-conflict, along with a load-aware skew-selection policy that guarantees the\navailability of sets with invalid tags. Our analysis shows Mirage provides the\nglobal eviction property of a fully-associative cache throughout system\nlifetime (violations of full-associativity, i.e. set-conflicts, occur less than\nonce in 10^4 to 10^17 years), thus offering a principled defense against any\neviction-set discovery and any potential conflict based attacks. Mirage incurs\nlimited slowdown (2%) and 17-20% extra storage compared to a non-secure cache.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 21:12:18 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 22:39:15 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 01:15:17 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Saileshwar", "Gururaj", ""], ["Qureshi", "Moinuddin", ""]]}, {"id": "2009.09170", "submitter": "Adrian Herrera", "authors": "Adrian Herrera", "title": "Optimizing Away JavaScript Obfuscation", "comments": "6 pages, 1 figures, to be published at the IEEE International Working\n  Conference on Source Code Analysis and Manipulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JavaScript is a popular attack vector for releasing malicious payloads on\nunsuspecting Internet users. Authors of this malicious JavaScript often employ\nnumerous obfuscation techniques in order to prevent the automatic detection by\nantivirus and hinder manual analysis by professional malware analysts.\nConsequently, this paper presents SAFE-Deobs, a JavaScript deobfuscation tool\nthat we have built.\n  The aim of SAFE-Deobs is to automatically deobfuscate JavaScript malware such\nthat an analyst can more rapidly determine the malicious script's intent. This\nis achieved through a number of static analyses, inspired by techniques from\ncompiler theory. We demonstrate the utility of SAFE-Deobs through a case study\non real-world JavaScript malware, and show that it is a useful addition to a\nmalware analyst's toolset.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 05:24:29 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Herrera", "Adrian", ""]]}, {"id": "2009.09191", "submitter": "Fanchao Qi", "authors": "Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Bairu Hou, Yuan\n  Zang, Zhiyuan Liu, Maosong Sun", "title": "OpenAttack: An Open-source Textual Adversarial Attack Toolkit", "comments": "Work in progress, 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Textual adversarial attacking has received wide and increasing attention in\nrecent years. Various attack models have been proposed, which are enormously\ndistinct and implemented with different programming frameworks and settings.\nThese facts hinder quick utilization and apt comparison of attack models. In\nthis paper, we present an open-source textual adversarial attack toolkit named\nOpenAttack. It currently builds in 12 typical attack models that cover all the\nattack types. Its highly inclusive modular design not only supports quick\nutilization of existing attack models, but also enables great flexibility and\nextensibility. OpenAttack has broad uses including comparing and evaluating\nattack models, measuring robustness of a victim model, assisting in developing\nnew attack models, and adversarial training. Source code, built-in models and\ndocumentation can be obtained at https://github.com/thunlp/OpenAttack.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 09:02:56 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zeng", "Guoyang", ""], ["Qi", "Fanchao", ""], ["Zhou", "Qianrui", ""], ["Zhang", "Tingji", ""], ["Hou", "Bairu", ""], ["Zang", "Yuan", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2009.09192", "submitter": "Fanchao Qi", "authors": "Yuan Zang, Bairu Hou, Fanchao Qi, Zhiyuan Liu, Xiaojun Meng, Maosong\n  Sun", "title": "Learning to Attack: Towards Textual Adversarial Attacking in Real-world\n  Situations", "comments": "work in progress, 10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacking aims to fool deep neural networks with adversarial\nexamples. In the field of natural language processing, various textual\nadversarial attack models have been proposed, varying in the accessibility to\nthe victim model. Among them, the attack models that only require the output of\nthe victim model are more fit for real-world situations of adversarial\nattacking. However, to achieve high attack performance, these models usually\nneed to query the victim model too many times, which is neither efficient nor\nviable in practice. To tackle this problem, we propose a reinforcement learning\nbased attack model, which can learn from attack history and launch attacks more\nefficiently. In experiments, we evaluate our model by attacking several\nstate-of-the-art models on the benchmark datasets of multiple tasks including\nsentiment analysis, text classification and natural language inference.\nExperimental results demonstrate that our model consistently achieves both\nbetter attack performance and higher efficiency than recently proposed baseline\nmethods. We also find our attack model can bring more robustness improvement to\nthe victim model by adversarial training. All the code and data of this paper\nwill be made public.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 09:12:24 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zang", "Yuan", ""], ["Hou", "Bairu", ""], ["Qi", "Fanchao", ""], ["Liu", "Zhiyuan", ""], ["Meng", "Xiaojun", ""], ["Sun", "Maosong", ""]]}, {"id": "2009.09205", "submitter": "Liming Zhai", "authors": "Liming Zhai, Felix Juefei-Xu, Qing Guo, Xiaofei Xie, Lei Ma, Wei Feng,\n  Shengchao Qin, Yang Liu", "title": "It's Raining Cats or Dogs? Adversarial Rain Attack on DNN Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain is a common phenomenon in nature and an essential factor for many deep\nneural network (DNN) based perception systems. Rain can often post inevitable\nthreats that must be carefully addressed especially in the context of safety\nand security-sensitive scenarios (e.g., autonomous driving). Therefore, a\ncomprehensive investigation of the potential risks of the rain to a DNN is of\ngreat importance. Unfortunately, in practice, it is often rather difficult to\ncollect or synthesize rainy images that can represent all raining situations\nthat possibly occur in the real world. To this end, in this paper, we start\nfrom a new perspective and propose to combine two totally different studies,\ni.e., rainy image synthesis and adversarial attack. We present an adversarial\nrain attack, with which we could simulate various rainy situations with the\nguidance of deployed DNNs and reveal the potential threat factors that can be\nbrought by rain, helping to develop more rain-robust DNNs. In particular, we\npropose a factor-aware rain generation that simulates rain steaks according to\nthe camera exposure process and models the learnable rain factors for\nadversarial attack. With this generator, we further propose the adversarial\nrain attack against the image classification and object detection, where the\nrain factors are guided by the various DNNs. As a result, it enables to\ncomprehensively study the impacts of the rain factors to DNNs. Our largescale\nevaluation on three datasets, i.e., NeurIPS'17 DEV, MS COCO and KITTI,\ndemonstrates that our synthesized rainy images can not only present visually\nrealistic appearances, but also exhibit strong adversarial capability, which\nbuilds the foundation for further rain-robust perception studies.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 10:12:08 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhai", "Liming", ""], ["Juefei-Xu", "Felix", ""], ["Guo", "Qing", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Feng", "Wei", ""], ["Qin", "Shengchao", ""], ["Liu", "Yang", ""]]}, {"id": "2009.09210", "submitter": "Jason R.C. Nurse Dr", "authors": "Richard Knight and Jason R. C. Nurse", "title": "A framework for effective corporate communication after cyber security\n  incidents", "comments": null, "journal-ref": "Computers & Security, Volume 99, December 2020", "doi": "10.1016/j.cose.2020.102036", "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major cyber security incident can represent a cyber crisis for an\norganisation, in particular because of the associated risk of substantial\nreputational damage. As the likelihood of falling victim to a cyberattack has\nincreased over time, so too has the need to understand exactly what is\neffective corporate communication after an attack, and how best to engage the\nconcerns of customers, partners and other stakeholders. This research seeks to\ntackle this problem through a critical, multi-faceted investigation into the\nefficacy of crisis communication and public relations following a data breach.\nIt does so by drawing on academic literature, obtained through a systematic\nliterature review, and real-world case studies. Qualitative data analysis is\nused to interpret and structure the results, allowing for the development of a\nnew, comprehensive framework for corporate communication to support companies\nin their preparation and response to such events. The validity of this\nframework is demonstrated by its evaluation through interviews with senior\nindustry professionals, as well as a critical assessment against relevant\npractice and research. The framework is further refined based on these\nevaluations, and an updated version defined. This research represents the first\ngrounded, comprehensive and evaluated proposal for characterising effective\ncorporate communication after cyber security incidents.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 11:08:53 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Knight", "Richard", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "2009.09213", "submitter": "Yihao Huang", "authors": "Yihao Huang, Felix Juefei-Xu, Qing Guo, Xiaofei Xie, Lei Ma, Weikai\n  Miao, Yang Liu, Geguang Pu", "title": "FakeRetouch: Evading DeepFakes Detection via the Guidance of Deliberate\n  Noise", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novelty and creativity of DeepFake generation techniques have attracted\nworldwide media attention. Many researchers focus on detecting fake images\nproduced by these GAN-based image generation methods with fruitful results,\nindicating that the GAN-based image generation methods are not yet perfect.\nMany studies show that the upsampling procedure used in the decoder of\nGAN-based image generation methods inevitably introduce artifact patterns into\nfake images. In order to further improve the fidelity of DeepFake images, in\nthis work, we propose a simple yet powerful framework to reduce the artifact\npatterns of fake images without hurting image quality. The method is based on\nan important observation that adding noise to a fake image can successfully\nreduce the artifact patterns in both spatial and frequency domains. Thus we use\na combination of additive noise and deep image filtering to reconstruct the\nfake images, and we name our method FakeRetouch. The deep image filtering\nprovides a specialized filter for each pixel in the noisy image, taking full\nadvantages of deep learning. The deeply filtered images retain very high\nfidelity to their DeepFake counterparts. Moreover, we use the semantic\ninformation of the image to generate an adversarial guidance map to add noise\nintelligently. Our method aims at improving the fidelity of DeepFake images and\nexposing the problems of existing DeepFake detection methods, and we hope that\nthe found vulnerabilities can help improve the future generation DeepFake\ndetection methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 11:26:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Huang", "Yihao", ""], ["Juefei-Xu", "Felix", ""], ["Guo", "Qing", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Miao", "Weikai", ""], ["Liu", "Yang", ""], ["Pu", "Geguang", ""]]}, {"id": "2009.09224", "submitter": "Jamil Ispahany Mr", "authors": "Jamil Ispahany and Rafiqul Islam", "title": "Detecting Malicious URLs of COVID-19 Pandemic using ML technologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the COVID-19 outbreak, malicious attacks have become more\npervasive and damaging than ever. Malicious intruders have been responsible for\nmost cybercrimes committed recently and are the cause for a growing number of\ncyber threats, including identity and IP thefts, financial crimes, and\ncyber-attacks to critical infrastructures. Machine learning (ML) has proven\nitself as a prominent field of study over the past decade by solving many\nhighly complex and sophisticated real-world problems. This paper proposes an\nML-based classification technique to detect the growing number of malicious\nURLs, due to the COVID-19 pandemic, which is currently considered a threat to\nIT users. We have used a large volume of Open Source data and preprocessed it\nusing our developed tool to generate feature vectors and we trained the ML\nmodel using the apprehensive malicious threat weight. Our ML model has been\ntested, with and without entropy to forecast the threatening factors of\nCOVID-19 URLs. The empirical evidence proves our methods to be a promising\nmechanism to mitigate COVID-19 related threats early in the attack lifecycle.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 12:59:17 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ispahany", "Jamil", ""], ["Islam", "Rafiqul", ""]]}, {"id": "2009.09278", "submitter": "Chris Mitchell", "authors": "Chris J Mitchell", "title": "Two closely related insecure noninteractive group key establishment\n  schemes", "comments": "Paper updated to describe an attack on a closely related scheme", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serious weaknesses in two very closely related group authentication and group\nkey establishment schemes are described. Simple attacks against the group key\nestablishment part of the schemes are described, which strongly suggest that\nthe schemes should not be used.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 18:10:45 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 16:13:15 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Mitchell", "Chris J", ""]]}, {"id": "2009.09283", "submitter": "Kang Liu", "authors": "Kang Liu, Benjamin Tan, Siddharth Garg", "title": "Subverting Privacy-Preserving GANs: Hiding Secrets in Sanitized Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unprecedented data collection and sharing have exacerbated privacy concerns\nand led to increasing interest in privacy-preserving tools that remove\nsensitive attributes from images while maintaining useful information for other\ntasks. Currently, state-of-the-art approaches use privacy-preserving generative\nadversarial networks (PP-GANs) for this purpose, for instance, to enable\nreliable facial expression recognition without leaking users' identity.\nHowever, PP-GANs do not offer formal proofs of privacy and instead rely on\nexperimentally measuring information leakage using classification accuracy on\nthe sensitive attributes of deep learning (DL)-based discriminators. In this\nwork, we question the rigor of such checks by subverting existing\nprivacy-preserving GANs for facial expression recognition. We show that it is\npossible to hide the sensitive identification data in the sanitized output\nimages of such PP-GANs for later extraction, which can even allow for\nreconstruction of the entire input images, while satisfying privacy checks. We\ndemonstrate our approach via a PP-GAN-based architecture and provide\nqualitative and quantitative evaluations using two public datasets. Our\nexperimental results raise fundamental questions about the need for more\nrigorous privacy checks of PP-GANs, and we provide insights into the social\nimpact of these.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 19:02:17 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Liu", "Kang", ""], ["Tan", "Benjamin", ""], ["Garg", "Siddharth", ""]]}, {"id": "2009.09339", "submitter": "Zhiyi Zhang", "authors": "Zhiyi Zhang, Su Yong Wong, Junxiao Shi, Davide Pesavento, Alexander\n  Afanasyev, Lixia Zhang", "title": "On Certificate Management in Named Data Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Data Networking (NDN) secures network communications by requiring all\ndata packets to be signed when produced. This requirement necessitates\nefficient and usable mechanisms to handle NDN certificate issuance and\nrevocation, making these supporting mechanisms essential for NDN operations. In\nthis paper, we first investigate and clarify core concepts related to NDN\ncertificates and security design in general, and then present the model of NDN\ncertificate management and its desired properties. We proceed with the design\nof a specific realization of NDN's certificate management, NDNCERT, evaluate it\nusing a formal security analysis, and discuss the challenges in designing,\nimplementing, and deploying the system, to share our experiences with other NDN\nsecurity protocol development efforts.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 03:14:40 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhang", "Zhiyi", ""], ["Wong", "Su Yong", ""], ["Shi", "Junxiao", ""], ["Pesavento", "Davide", ""], ["Afanasyev", "Alexander", ""], ["Zhang", "Lixia", ""]]}, {"id": "2009.09451", "submitter": "Shangyu Xie", "authors": "Meisam Mohammady, Shangyu Xie, Yuan Hong, Mengyuan Zhang, Lingyu Wang,\n  Makan Pourzandi and Mourad Debbabi", "title": "R$^2$DP: A Universal and Automated Approach to Optimizing the\n  Randomization Mechanisms of Differential Privacy for Utility Metrics with No\n  Known Optimal Distributions", "comments": "To Appear in Proceedings of the 27th ACM Conference on Computer and\n  Communications Security, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy (DP) has emerged as a de facto standard privacy notion\nfor a wide range of applications. Since the meaning of data utility in\ndifferent applications may vastly differ, a key challenge is to find the\noptimal randomization mechanism, i.e., the distribution and its parameters, for\na given utility metric. Existing works have identified the optimal\ndistributions in some special cases, while leaving all other utility metrics\n(e.g., usefulness and graph distance) as open problems. Since existing works\nmostly rely on manual analysis to examine the search space of all\ndistributions, it would be an expensive process to repeat such efforts for each\nutility metric. To address such deficiency, we propose a novel approach that\ncan automatically optimize different utility metrics found in diverse\napplications under a common framework. Our key idea that, by regarding the\nvariance of the injected noise itself as a random variable, a two-fold\ndistribution may approximately cover the search space of all distributions.\nTherefore, we can automatically find distributions in this search space to\noptimize different utility metrics in a similar manner, simply by optimizing\nthe parameters of the two-fold distribution. Specifically, we define a\nuniversal framework, namely, randomizing the randomization mechanism of\ndifferential privacy (R$^2$DP), and we formally analyze its privacy and\nutility. Our experiments show that R$^2$DP can provide better results than the\nbaseline distribution (Laplace) for several utility metrics with no known\noptimal distributions, whereas our results asymptotically approach to the\noptimality for utility metrics having known optimal distributions. As a side\nbenefit, the added degree of freedom introduced by the two-fold distribution\nallows R$^2$DP to accommodate the preferences of both data owners and\nrecipients.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 15:06:42 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 04:30:06 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Mohammady", "Meisam", ""], ["Xie", "Shangyu", ""], ["Hong", "Yuan", ""], ["Zhang", "Mengyuan", ""], ["Wang", "Lingyu", ""], ["Pourzandi", "Makan", ""], ["Debbabi", "Mourad", ""]]}, {"id": "2009.09480", "submitter": "Andrew Lewis-Pye", "authors": "Andrew Lewis-Pye and Tim Roughgarden", "title": "A General Framework for the Security Analysis of Blockchain Protocols", "comments": "This paper has been merged with arXiv:2101.07095", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain protocols differ in fundamental ways, including the mechanics of\nselecting users to produce blocks (e.g., proof-of-work vs. proof-of-stake) and\nthe method to establish consensus (e.g., longest chain rules vs. Byzantine\nfault-tolerant (BFT) inspired protocols). These fundamental differences have\nhindered \"apples-to-apples\" comparisons between different categories of\nblockchain protocols and, in turn, the development of theory to formally\ndiscuss their relative merits.\n  This paper presents a parsimonious abstraction sufficient for capturing and\ncomparing properties of many well-known permissionless blockchain protocols,\nsimultaneously capturing essential properties of both proof-of-work (PoW) and\nproof-of-stake (PoS) protocols, and of both longest-chain-type and BFT-type\nprotocols. Our framework blackboxes the precise mechanics of the user selection\nprocess, allowing us to isolate the properties of the selection process that\nare significant for protocol design.\n  We demonstrate the utility of our general framework with several concrete\nresults:\n  1. We prove a CAP-type impossibility theorem asserting that liveness with an\nunknown level of participation rules out security in a partially synchronous\nsetting.\n  2. Delving deeper into the partially synchronous setting, we prove that a\nnecessary and sufficient condition for security is the production of\n\"certificates,\" meaning stand-alone proofs of block confirmation.\n  3. Restricting to synchronous settings, we prove that typical protocols with\na known level of participation (including longest chain-type PoS protocols) can\nbe adapted to provide certificates, but those with an unknown level of\nparticipation cannot.\n  4. Finally, we use our framework to articulate a modular two-step approach to\nblockchain security analysis that effectively reduces the permissionless case\nto the permissioned case.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 17:30:22 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:22:16 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 12:53:18 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Lewis-Pye", "Andrew", ""], ["Roughgarden", "Tim", ""]]}, {"id": "2009.09511", "submitter": "Bhaskar Ramasubramanian", "authors": "Bhaskar Ramasubramanian, Baicen Xiao, Linda Bushnell, Radha Poovendran", "title": "Safety-Critical Online Control with Adversarial Disturbances", "comments": "Paper accepted to the Conference on Decision and Control (CDC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.CR cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the control of safety-critical dynamical systems in the\npresence of adversarial disturbances. We seek to synthesize state-feedback\ncontrollers to minimize a cost incurred due to the disturbance, while\nrespecting a safety constraint. The safety constraint is given by a bound on an\nH-inf norm, while the cost is specified as an upper bound on the H-2 norm of\nthe system. We consider an online setting where costs at each time are revealed\nonly after the controller at that time is chosen. We propose an iterative\napproach to the synthesis of the controller by solving a modified discrete-time\nRiccati equation. Solutions of this equation enforce the safety constraint. We\ncompare the cost of this controller with that of the optimal controller when\none has complete knowledge of disturbances and costs in hindsight. We show that\nthe regret function, which is defined as the difference between these costs,\nvaries logarithmically with the time horizon. We validate our approach on a\nprocess control setup that is subject to two kinds of adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 19:59:15 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ramasubramanian", "Bhaskar", ""], ["Xiao", "Baicen", ""], ["Bushnell", "Linda", ""], ["Poovendran", "Radha", ""]]}, {"id": "2009.09524", "submitter": "Patrick Ah-Fat", "authors": "Patrick Ah-Fat and Michael Huth", "title": "Two and Three-Party Digital Goods Auctions: Scalable Privacy Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A digital goods auction is a type of auction where potential buyers bid the\nmaximal price that they are willing to pay for a certain item, which a seller\ncan produce at a negligible cost and in unlimited quantity. To maximise her\nbenefits, the aim for the seller is to find the optimal sales price, which\nevery buyer whose bid is not lower will pay. For fairness and privacy purposes,\nbuyers may be concerned about protecting the confidentiality of their bids.\nSecure Multi-Party Computation is a domain of Cryptography that would allow the\nseller to compute the optimal sales price while guaranteeing that the bids\nremain secret. Paradoxically, as a function of the buyers' bids, the sales\nprice inevitably reveals some private information. Generic frameworks and\nentropy-based techniques based on Quantitative Information Flow have been\ndeveloped in order to quantify and restrict those leakages. Due to their\ncombinatorial nature, these techniques do not scale to large input spaces. In\nthis work, we aim at scaling those privacy analyses to large input spaces in\nthe particular case of digital goods auctions. We derive closed-form formulas\nfor the posterior min-entropy of private inputs in two and three-party\nauctions, which enables us to effectively quantify the information leaks for\narbitrarily large input spaces. We also provide supportive experimental\nevidence that enables us to formulate a conjecture that would allow us to\nextend our results to any number of parties.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 20:59:54 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ah-Fat", "Patrick", ""], ["Huth", "Michael", ""]]}, {"id": "2009.09560", "submitter": "Xiaoyong Yuan", "authors": "Xiaoyong Yuan, Lei Ding, Lan Zhang, Xiaolin Li, Dapeng Wu", "title": "ES Attack: Model Stealing against Deep Neural Networks without Data\n  Hurdles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have become the essential components for various\ncommercialized machine learning services, such as Machine Learning as a Service\n(MLaaS). Recent studies show that machine learning services face severe privacy\nthreats - well-trained DNNs owned by MLaaS providers can be stolen through\npublic APIs, namely model stealing attacks. However, most existing works\nundervalued the impact of such attacks, where a successful attack has to\nacquire confidential training data or auxiliary data regarding the victim DNN.\nIn this paper, we propose ES Attack, a novel model stealing attack without any\ndata hurdles. By using heuristically generated synthetic data, ES\nAttackiteratively trains a substitute model and eventually achieves a\nfunctionally equivalent copy of the victim DNN. The experimental results reveal\nthe severity of ES Attack: i) ES Attack successfully steals the victim model\nwithout data hurdles, and ES Attack even outperforms most existing model\nstealing attacks using auxiliary data in terms of model accuracy; ii) most\ncountermeasures are ineffective in defending ES Attack; iii) ES Attack\nfacilitates further attacks relying on the stolen model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 01:26:06 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Yuan", "Xiaoyong", ""], ["Ding", "Lei", ""], ["Zhang", "Lan", ""], ["Li", "Xiaolin", ""], ["Wu", "Dapeng", ""]]}, {"id": "2009.09570", "submitter": "Yongjune Kim", "authors": "Yongjune Kim, Cyril Guyot, Young-Sik Kim", "title": "On the Efficient Estimation of Min-Entropy", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2021.3070424", "report-no": null, "categories": "cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The min-entropy is a widely used metric to quantify the randomness of\ngenerated random numbers in cryptographic applications; it measures the\ndifficulty of guessing the most likely output. An important min-entropy\nestimator is the compression estimator of NIST Special Publication (SP)\n800-90B, which relies on Maurer's universal test. In this paper, we propose two\nkinds of min-entropy estimators to improve computational complexity and\nestimation accuracy by leveraging two variations of Maurer's test: Coron's test\n(for Shannon entropy) and Kim's test (for Renyi entropy). First, we propose a\nmin-entropy estimator based on Coron's test. It is computationally more\nefficient than the compression estimator while maintaining the estimation\naccuracy. The secondly proposed estimator relies on Kim's test that computes\nthe Renyi entropy. This estimator improves estimation accuracy as well as\ncomputational complexity. We analytically characterize the bias-variance\ntradeoff, which depends on the order of Renyi entropy. By taking into account\nthis tradeoff, we observe that the order of two is a proper assignment and\nfocus on the min-entropy estimation based on the collision entropy (i.e., Renyi\nentropy of order two). The min-entropy estimation from the collision entropy\ncan be described by a closed-form solution, whereas both the compression\nestimator and the proposed estimator based on Coron's test do not have\nclosed-form solutions. By leveraging the closed-form solution, we also propose\na lightweight estimator that processes data samples in an online manner.\nNumerical evaluations demonstrate that the first proposed estimator achieves\nthe same accuracy as the compression estimator with much less computation. The\nproposed estimator based on the collision entropy can even improve the accuracy\nand reduce the computational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 01:53:14 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 02:35:26 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Kim", "Yongjune", ""], ["Guyot", "Cyril", ""], ["Kim", "Young-Sik", ""]]}, {"id": "2009.09604", "submitter": "Badih Ghazi", "authors": "Lijie Chen, Badih Ghazi, Ravi Kumar, Pasin Manurangsi", "title": "On Distributed Differential Privacy and Counting Distinct Elements", "comments": "68 pages, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the setup where each of $n$ users holds an element from a discrete\nset, and the goal is to count the number of distinct elements across all users,\nunder the constraint of $(\\epsilon, \\delta)$-differentially privacy:\n  - In the non-interactive local setting, we prove that the additive error of\nany protocol is $\\Omega(n)$ for any constant $\\epsilon$ and for any $\\delta$\ninverse polynomial in $n$.\n  - In the single-message shuffle setting, we prove a lower bound of\n$\\Omega(n)$ on the error for any constant $\\epsilon$ and for some $\\delta$\ninverse quasi-polynomial in $n$. We do so by building on the moment-matching\nmethod from the literature on distribution estimation.\n  - In the multi-message shuffle setting, we give a protocol with at most one\nmessage per user in expectation and with an error of $\\tilde{O}(\\sqrt(n))$ for\nany constant $\\epsilon$ and for any $\\delta$ inverse polynomial in $n$. Our\nprotocol is also robustly shuffle private, and our error of $\\sqrt(n)$ matches\na known lower bound for such protocols.\n  Our proof technique relies on a new notion, that we call dominated protocols,\nand which can also be used to obtain the first non-trivial lower bounds against\nmulti-message shuffle protocols for the well-studied problems of selection and\nlearning parity.\n  Our first lower bound for estimating the number of distinct elements provides\nthe first $\\omega(\\sqrt(n))$ separation between global sensitivity and error in\nlocal differential privacy, thus answering an open question of Vadhan (2017).\nWe also provide a simple construction that gives $\\tilde{\\Omega}(n)$ separation\nbetween global sensitivity and error in two-party differential privacy, thereby\nanswering an open question of McGregor et al. (2011).\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 04:13:34 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chen", "Lijie", ""], ["Ghazi", "Badih", ""], ["Kumar", "Ravi", ""], ["Manurangsi", "Pasin", ""]]}, {"id": "2009.09663", "submitter": "Yu Li", "authors": "Yu Li, Min Li, Bo Luo, Ye Tian, and Qiang Xu", "title": "DeepDyve: Dynamic Verification for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3372297.3423338", "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have become one of the enabling technologies in\nmany safety-critical applications, e.g., autonomous driving and medical image\nanalysis. DNN systems, however, suffer from various kinds of threats, such as\nadversarial example attacks and fault injection attacks. While there are many\ndefense methods proposed against maliciously crafted inputs, solutions against\nfaults presented in the DNN system itself (e.g., parameters and calculations)\nare far less explored. In this paper, we develop a novel lightweight\nfault-tolerant solution for DNN-based systems, namely DeepDyve, which employs\npre-trained neural networks that are far simpler and smaller than the original\nDNN for dynamic verification. The key to enabling such lightweight checking is\nthat the smaller neural network only needs to produce approximate results for\nthe initial task without sacrificing fault coverage much. We develop efficient\nand effective architecture and task exploration techniques to achieve optimized\nrisk/overhead trade-off in DeepDyve. Experimental results show that DeepDyve\ncan reduce 90% of the risks at around 10% overhead.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 07:58:18 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 03:00:09 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Li", "Yu", ""], ["Li", "Min", ""], ["Luo", "Bo", ""], ["Tian", "Ye", ""], ["Xu", "Qiang", ""]]}, {"id": "2009.09668", "submitter": "Julian Renner", "authors": "Johannes Kunz, Julian Renner, Georg Maringer, Thomas Schamberger,\n  Antonia Wachter-Zeh", "title": "On Software Implementation of Gabidulin Decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work compares the performance of software implementations of different\nGabidulin decoders. The parameter sets used within the comparison stem from\ntheir applications in recently proposed cryptographic schemes. The complexity\nanalysis of the decoders is recalled, counting the occurrence of each operation\nwithin the respective decoders. It is shown that knowing the number of\noperations may be misleading when comparing different algorithms as the\nrun-time of the implementation depends on the instruction set of the device on\nwhich the algorithm is executed.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:11:13 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kunz", "Johannes", ""], ["Renner", "Julian", ""], ["Maringer", "Georg", ""], ["Schamberger", "Thomas", ""], ["Wachter-Zeh", "Antonia", ""]]}, {"id": "2009.09691", "submitter": "Meng Shen", "authors": "Liehuang Zhu, Xiangyun Tang, Meng Shen, Jie Zhang, Xiaojiang Du", "title": "Privacy-Preserving Machine Learning Training in Aggregation Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To develop Smart City, the growing popularity of Machine Learning (ML) that\nappreciates high-quality training datasets generated from diverse IoT devices\nraises natural questions about the privacy guarantees that can be provided in\nsuch settings. Privacy-preserving ML training in an aggregation scenario\nenables a model demander to securely train ML models with the sensitive IoT\ndata gathered from personal IoT devices. Existing solutions are generally\nserver-aided, cannot deal with the collusion threat between the servers or\nbetween the servers and data owners, and do not match the delicate environments\nof IoT. We propose a privacy-preserving ML training framework named Heda that\nconsists of a library of building blocks based on partial homomorphic\nencryption (PHE) enabling constructing multiple privacy-preserving ML training\nprotocols for the aggregation scenario without the assistance of untrusted\nservers and defending the security under collusion situations. Rigorous\nsecurity analysis demonstrates the proposed protocols can protect the privacy\nof each participant in the honest-but-curious model and defend the security\nunder most collusion situations. Extensive experiments validate the efficiency\nof Heda which achieves the privacy-preserving ML training without losing the\nmodel accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 09:00:44 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhu", "Liehuang", ""], ["Tang", "Xiangyun", ""], ["Shen", "Meng", ""], ["Zhang", "Jie", ""], ["Du", "Xiaojiang", ""]]}, {"id": "2009.09800", "submitter": "Ji Liu", "authors": "Ji Liu (1), Hang Zhao (1), Jiyuan Yang (1), Yu Shi (1), Ruichang Liu\n  (1), Dong Yuan (1) and Shiping Chen (1 and 2) ((1) School of Electrical &\n  Information Engineering, University of Sydney, Australia, (2) Commonwealth\n  Scientific & Industrial Research Organisation (CSIRO), DATA61, Australia)", "title": "ServiceNet: A P2P Service Network", "comments": "15 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large number of online services on the Internet, from time to time,\npeople are still struggling to find out the services that they need. On the\nother hand, when there are considerable research and development on service\ndiscovery and service recommendation, most of the related work are centralized\nand thus suffers inherent shortages of the centralized systems, e.g.,\nadv-driven, lack at trust, transparence and fairness. In this paper, we propose\na ServiceNet - a peer-to-peer (P2P) service network for service discovery and\nservice recommendation. ServiceNet is inspired by blockchain technology and\naims at providing an open, transparent and self-growth, and self-management\nservice ecosystem. The paper will present the basic idea, an architecture\ndesign of the prototype, and an initial implementation and performance\nevaluation the prototype design.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 08:58:41 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Liu", "Ji", "", "1 and 2"], ["Zhao", "Hang", "", "1 and 2"], ["Yang", "Jiyuan", "", "1 and 2"], ["Shi", "Yu", "", "1 and 2"], ["Liu", "Ruichang", "", "1 and 2"], ["Yuan", "Dong", "", "1 and 2"], ["Chen", "Shiping", "", "1 and 2"]]}, {"id": "2009.09803", "submitter": "Usman Roshan", "authors": "Yunzhe Xue, Meiyan Xie, Usman Roshan", "title": "Defending against substitute model black box adversarial attacks with\n  the 01 loss", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.07800;\n  text overlap with arXiv:2008.09148", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Substitute model black box attacks can create adversarial examples for a\ntarget model just by accessing its output labels. This poses a major challenge\nto machine learning models in practice, particularly in security sensitive\napplications. The 01 loss model is known to be more robust to outliers and\nnoise than convex models that are typically used in practice. Motivated by\nthese properties we present 01 loss linear and 01 loss dual layer neural\nnetwork models as a defense against transfer based substitute model black box\nattacks. We compare the accuracy of adversarial examples from substitute model\nblack box attacks targeting our 01 loss models and their convex counterparts\nfor binary classification on popular image benchmarks. Our 01 loss dual layer\nneural network has an adversarial accuracy of 66.2%, 58%, 60.5%, and 57% on\nMNIST, CIFAR10, STL10, and ImageNet respectively whereas the sigmoid activated\nlogistic loss counterpart has accuracies of 63.5%, 19.3%, 14.9%, and 27.6%.\nExcept for MNIST the convex counterparts have substantially lower adversarial\naccuracies. We show practical applications of our models to deter traffic sign\nand facial recognition adversarial attacks. On GTSRB street sign and CelebA\nfacial detection our 01 loss network has 34.6% and 37.1% adversarial accuracy\nrespectively whereas the convex logistic counterpart has accuracy 24% and 1.9%.\nFinally we show that our 01 loss network can attain robustness on par with\nsimple convolutional neural networks and much higher than its convex\ncounterpart even when attacked with a convolutional network substitute model.\nOur work shows that 01 loss models offer a powerful defense against substitute\nmodel black box attacks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 22:32:51 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Xue", "Yunzhe", ""], ["Xie", "Meiyan", ""], ["Roshan", "Usman", ""]]}, {"id": "2009.09869", "submitter": "Run Wang", "authors": "Run Wang, Felix Juefei-Xu, Meng Luo, Yang Liu, Lina Wang", "title": "FakeTagger: Robust Safeguards against DeepFake Dissemination via\n  Provenance Tracking", "comments": "Accepted to ACM Multimedia 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, DeepFake is becoming a common threat to our society, due to\nthe remarkable progress of generative adversarial networks (GAN) in image\nsynthesis. Unfortunately, existing studies that propose various approaches, in\nfighting against DeepFake and determining if the facial image is real or fake,\nis still at an early stage. Obviously, the current DeepFake detection method\nstruggles to catch the rapid progress of GANs, especially in the adversarial\nscenarios where attackers can evade the detection intentionally, such as adding\nperturbations to fool the DNN-based detectors. While passive detection simply\ntells whether the image is fake or real, DeepFake provenance, on the other\nhand, provides clues for tracking the sources in DeepFake forensics. Thus, the\ntracked fake images could be blocked immediately by administrators and avoid\nfurther spread in social networks.In this paper, we investigate the potentials\nof image tagging in serving the DeepFake provenance tracking. Specifically, we\ndevise a deep learning-based approach, named FakeTagger, with a simple yet\neffective encoder and decoder design along with channel coding to embed message\nto the facial image, which is to recover the embedded message after various\ndrastic GAN-based DeepFake transformation with high confidence. Experimental\nresults demonstrate that our proposed approach could recover the embedded\nmessage with an average accuracy of more than 95% over the four common types of\nDeepFakes. Our research finding confirms effective privacy-preserving\ntechniques for protecting personal photos from being DeepFaked. Thus, effective\nproactive defense mechanisms should be developed for fighting against\nDeepFakes, instead of simply devising DeepFake detection methods that can be\nmostly ineffective in practice.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 13:41:24 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 12:13:29 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wang", "Run", ""], ["Juefei-Xu", "Felix", ""], ["Luo", "Meng", ""], ["Liu", "Yang", ""], ["Wang", "Lina", ""]]}, {"id": "2009.09876", "submitter": "Jean-Fran\\c{c}ois Determe", "authors": "Jean-Fran\\c{c}ois Determe and Sophia Azzagnuni and Utkarsh Singh and\n  Fran\\c{c}ois Horlin and Philippe De Doncker", "title": "Collisions of uniformly distributed identifiers with an application to\n  MAC address anonymization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main contribution of this paper consists in theoretical approximations of\nthe collision rate of $n$ random identifiers uniformly distributed in $m (> n)$\nbuckets---along with bounds on the approximation errors. A secondary\ncontribution is a decentralized anonymization system of media access control\n(MAC) addresses with a low collision rate. The main contribution supports the\nsecondary one in that it quantifies its collision rate, thereby allowing\ndesigners to minimize $m$ while attaining specific collision rates. Recent\nworks in crowd monitoring based on WiFi probe requests, for which collected MAC\naddresses should be anonymized, have inspired this research.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 13:54:40 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Determe", "Jean-Fran\u00e7ois", ""], ["Azzagnuni", "Sophia", ""], ["Singh", "Utkarsh", ""], ["Horlin", "Fran\u00e7ois", ""], ["De Doncker", "Philippe", ""]]}, {"id": "2009.09957", "submitter": "Renpeng Zou", "authors": "Renpeng Zou (1), Xixiang Lv (1), Jingsong Zhao (1) ((1) School of\n  Cyber Engineering, Xidian University, Xian, China)", "title": "SPChain: Blockchain-based Medical Data Sharing and Privacy-preserving\n  eHealth System", "comments": null, "journal-ref": "Information Processing & Management, 2021, 58(4): 102604", "doi": "10.1016/j.ipm.2021.102604", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of eHealth systems has brought great convenience to people's\nlife. Researchers have been combining new technologies to make eHealth systems\nwork better for patients. The Blockchain-based eHealth system becomes popular\nbecause of its unique distributed tamper-resistant and privacy-preserving\nfeatures. However, due to the security issues of the blockchain system, there\nare many security risks in eHealth systems utilizing the blockchain technology.\ni.e. 51% attacks can destroy blockchain-based systems. Besides, trivial\ntransactions and frequent calls of smart contracts in the blockchain system\nbring additional costs and security risks to blockchain-based eHealth systems.\nWorse still, electronic medical records (EMRs) are controlled by medical\ninstitutions rather than patients, which causes privacy leakage issues. In this\npaper, we propose a medical data Sharing and Privacy-preserving eHealth system\nbased on blockChain technology (SPChain). We combine RepuCoin with the\nSNARKs-based chameleon hash function to resist underlying blockchain attacks,\nand design a new chain structure to make microblocks contribute to the weight\nof blockchain. The system allows patients to share their EMRs among different\nmedical institutions in a privacy-preserving way. Besides, authorized medical\ninstitutions can label wrong EMRs with the patients' permissions in the case of\nmisdiagnosis. Security analysis and performance evaluation demonstrate that the\nproposed system can provide a strong security guarantee with a high efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:28:45 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 04:51:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zou", "Renpeng", ""], ["Lv", "Xixiang", ""], ["Zhao", "Jingsong", ""]]}, {"id": "2009.09959", "submitter": "Xiaoqing Sun", "authors": "Xin Fang and Xiaoqing Sun and Jiahai Yang and Xinran Liu", "title": "Domain-Embeddings Based DGA Detection with Incremental Training Method", "comments": "6 pages, 3 figures, ISCC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DGA-based botnet, which uses Domain Generation Algorithms (DGAs) to evade\nsupervision, has become a part of the most destructive threats to network\nsecurity. Over the past decades, a wealth of defense mechanisms focusing on\ndomain features have emerged to address the problem. Nonetheless, DGA detection\nremains a daunting and challenging task due to the big data nature of Internet\ntraffic and the potential fact that the linguistic features extracted only from\nthe domain names are insufficient and the enemies could easily forge them to\ndisturb detection. In this paper, we propose a novel DGA detection system which\nemploys an incremental word-embeddings method to capture the interactions\nbetween end hosts and domains, characterize time-series patterns of DNS queries\nfor each IP address and therefore explore temporal similarities between\ndomains. We carefully modify the Word2Vec algorithm and leverage it to\nautomatically learn dynamic and discriminative feature representations for over\n1.9 million domains, and develop an simple classifier for distinguishing\nmalicious domains from the benign. Given the ability to identify temporal\npatterns of domains and update models incrementally, the proposed scheme makes\nthe progress towards adapting to the changing and evolving strategies of DGA\ndomains. Our system is evaluated and compared with the state-of-art system\nFANCI and two deep-learning methods CNN and LSTM, with data from a large\nuniversity's network named TUNET. The results suggest that our system\noutperforms the strong competitors by a large margin on multiple metrics and\nmeanwhile achieves a remarkable speed-up on model updating.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:34:07 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Fang", "Xin", ""], ["Sun", "Xiaoqing", ""], ["Yang", "Jiahai", ""], ["Liu", "Xinran", ""]]}, {"id": "2009.09983", "submitter": "Suthee Ruangwises", "authors": "Suthee Ruangwises, Toshiya Itoh", "title": "Physical Zero-Knowledge Proof for Ripple Effect", "comments": "This paper has appeared at WALCOM 2021", "journal-ref": null, "doi": "10.1007/978-3-030-68211-8_24", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ripple Effect is a logic puzzle with an objective to fill numbers into a\nrectangular grid divided into rooms. Each room must contain consecutive\nintegers starting from 1 to its size. Also, if two cells in the same row or\ncolumn have the same number $x$, the space separating the two cells must be at\nleast $x$ cells. In this paper, we propose a physical protocol of\nzero-knowledge proof for Ripple Effect puzzle using a deck of cards, which\nallows a prover to physically show that he/she knows a solution without\nrevealing it. In particular, we develop a physical protocol that, given a\nsecret number $x$ and a list of numbers, verifies that $x$ does not appear\namong the first $x$ numbers in the list without revealing $x$ or any number in\nthe list.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 16:08:08 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 17:55:43 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ruangwises", "Suthee", ""], ["Itoh", "Toshiya", ""]]}, {"id": "2009.10031", "submitter": "Om Thakkar", "authors": "Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H. Brendan\n  McMahan, Fran\\c{c}oise Beaufays", "title": "Training Production Language Models without Memorizing User Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the first consumer-scale next-word prediction (NWP) model\ntrained with Federated Learning (FL) while leveraging the Differentially\nPrivate Federated Averaging (DP-FedAvg) technique. There has been prior work on\nbuilding practical FL infrastructure, including work demonstrating the\nfeasibility of training language models on mobile devices using such\ninfrastructure. It has also been shown (in simulations on a public corpus) that\nit is possible to train NWP models with user-level differential privacy using\nthe DP-FedAvg algorithm. Nevertheless, training production-quality NWP models\nwith DP-FedAvg in a real-world production environment on a heterogeneous fleet\nof mobile phones requires addressing numerous challenges. For instance, the\ncoordinating central server has to keep track of the devices available at the\nstart of each round and sample devices uniformly at random from them, while\nensuring \\emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL\nwork of which we are aware, for the first time we demonstrate the deployment of\na differentially private mechanism for the training of a production neural\nnetwork in FL, as well as the instrumentation of the production training\ninfrastructure to perform an end-to-end empirical measurement of unintended\nmemorization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:12:33 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ramaswamy", "Swaroop", ""], ["Thakkar", "Om", ""], ["Mathews", "Rajiv", ""], ["Andrew", "Galen", ""], ["McMahan", "H. Brendan", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "2009.10060", "submitter": "Benjamin Harsha", "authors": "Wenjie Bai, Jeremiah Blocki, Ben Harsha", "title": "Information Signaling: A Counter-Intuitive Defense Against Password\n  Cracking", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce password strength information signaling as a novel, yet\ncounter-intuitive, defense mechanism against password cracking attacks. Recent\nbreaches have exposed billions of user passwords to the dangerous threat of\noffline password cracking attacks. An offline attacker can quickly check\nmillions (or sometimes billions/trillions) of password guesses by comparing\ntheir hash value with the stolen hash from a breached authentication server.\nThe attacker is limited only by the resources he is willing to invest. Our key\nidea is to have the authentication server store a (noisy) signal about the\nstrength of each user password for an offline attacker to find. Surprisingly,\nwe show that the noise distribution for the signal can often be tuned so that a\nrational (profit-maximizing) attacker will crack fewer passwords. The signaling\nscheme exploits the fact that password cracking is not a zero-sum game i.e.,\nthe attacker's profit is given by the value of the cracked passwords minus the\ntotal guessing cost. Thus, a well-defined signaling strategy will encourage the\nattacker to reduce his guessing costs by cracking fewer passwords. We use an\nevolutionary algorithm to compute the optimal signaling scheme for the\ndefender. As a proof-of-concept, we evaluate our mechanism on several password\ndatasets and show that it can reduce the total number of cracked passwords by\nup to $12\\%$ (resp. $5\\%$) of all users in defending against offline (resp.\nonline) attacks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:51:46 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 02:50:14 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 20:43:41 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Bai", "Wenjie", ""], ["Blocki", "Jeremiah", ""], ["Harsha", "Ben", ""]]}, {"id": "2009.10064", "submitter": "Maurice Weber", "authors": "Maurice Weber, Nana Liu, Bo Li, Ce Zhang, Zhikuan Zhao", "title": "Optimal Provable Robustness of Quantum Classification via Quantum\n  Hypothesis Testing", "comments": "28 pages, 5 figures", "journal-ref": "npj Quantum Information 7, 76 (2021)", "doi": "10.1038/s41534-021-00410-5", "report-no": null, "categories": "quant-ph cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum machine learning models have the potential to offer speedups and\nbetter predictive accuracy compared to their classical counterparts. However,\nthese quantum algorithms, like their classical counterparts, have been shown to\nalso be vulnerable to input perturbations, in particular for classification\nproblems. These can arise either from noisy implementations or, as a worst-case\ntype of noise, adversarial attacks. In order to develop defence mechanisms and\nto better understand the reliability of these algorithms, it is crucial to\nunderstand their robustness properties in presence of natural noise sources or\nadversarial manipulation. From the observation that measurements involved in\nquantum classification algorithms are naturally probabilistic, we uncover and\nformalize a fundamental link between binary quantum hypothesis testing and\nprovably robust quantum classification. This link leads to a tight robustness\ncondition which puts constraints on the amount of noise a classifier can\ntolerate, independent of whether the noise source is natural or adversarial.\nBased on this result, we develop practical protocols to optimally certify\nrobustness. Finally, since this is a robustness condition against worst-case\ntypes of noise, our result naturally extends to scenarios where the noise\nsource is known. Thus, we also provide a framework to study the reliability of\nquantum classification protocols beyond the adversarial, worst-case noise\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:55:28 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 09:07:35 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Weber", "Maurice", ""], ["Liu", "Nana", ""], ["Li", "Bo", ""], ["Zhang", "Ce", ""], ["Zhao", "Zhikuan", ""]]}, {"id": "2009.10131", "submitter": "Joseph Sweeney", "authors": "Joseph Sweeney, Marijn J.H. Heule, Lawrence Pileggi", "title": "Modeling Techniques for Logic Locking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic locking is a method to prevent intellectual property (IP) piracy.\nHowever, under a reasonable attack model, SAT-based methods have proven to be\npowerful in obtaining the secret key. In response, many locking techniques have\nbeen developed to specifically resist this form of attack. In this paper, we\ndemonstrate two SAT modeling techniques that can provide many orders of\nmagnitude speed up in discovering the correct key. Specifically, we consider\nrelaxed encodings and symmetry breaking. To demonstrate their impact, we model\nand attack a state-of-the-art logic locking technique, Full-Lock. We show that\ncircuits previously unbreakable within 15 days of run time can be solved in\nseconds. Consequently, in assessing the strength of any given locking, it is\nimperative that these modeling techniques be considered. To remedy this\nvulnerability in the considered locking technique, we demonstrate an extended\nversion, logic-enhanced Banyan locking, that is resistant to our proposed\nmodeling techniques.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 18:51:49 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Sweeney", "Joseph", ""], ["Heule", "Marijn J. H.", ""], ["Pileggi", "Lawrence", ""]]}, {"id": "2009.10149", "submitter": "Khaza Anuarul Hoque", "authors": "Gautam Raj Mode, Khaza Anuarul Hoque", "title": "Crafting Adversarial Examples for Deep Learning Based Prognostics\n  (Extended Version)", "comments": "This is the extended version of the paper \"Crafting Adversarial\n  Examples for Deep Learning Based Prognostics\" accepted for publication in the\n  IEEE International Conference on Machine Learning and Applications (ICMLA\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In manufacturing, unexpected failures are considered a primary operational\nrisk, as they can hinder productivity and can incur huge losses.\nState-of-the-art Prognostics and Health Management (PHM) systems incorporate\nDeep Learning (DL) algorithms and Internet of Things (IoT) devices to ascertain\nthe health status of equipment, and thus reduce the downtime, maintenance cost\nand increase the productivity. Unfortunately, IoT sensors and DL algorithms,\nboth are vulnerable to cyber attacks, and hence pose a significant threat to\nPHM systems. In this paper, we adopt the adversarial example crafting\ntechniques from the computer vision domain and apply them to the PHM domain.\nSpecifically, we craft adversarial examples using the Fast Gradient Sign Method\n(FGSM) and Basic Iterative Method (BIM) and apply them on the Long Short-Term\nMemory (LSTM), Gated Recurrent Unit (GRU), and Convolutional Neural Network\n(CNN) based PHM models. We evaluate the impact of adversarial attacks using\nNASA's turbofan engine dataset. The obtained results show that all the\nevaluated PHM models are vulnerable to adversarial attacks and can cause a\nserious defect in the remaining useful life estimation. The obtained results\nalso show that the crafted adversarial examples are highly transferable and may\ncause significant damages to PHM systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 19:43:38 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 15:26:35 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Mode", "Gautam Raj", ""], ["Hoque", "Khaza Anuarul", ""]]}, {"id": "2009.10150", "submitter": "Ren\\'e Mayrhofer", "authors": "Ren\\'e Mayrhofer, Vishwath Mohan, Stephan Sigg", "title": "Adversary Models for Mobile Device Authentication", "comments": "All authors had equal contribution and are listed in alphabetic order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile device authentication has been a highly active research topic for over\n10 years, with a vast range of methods having been proposed and analyzed. In\nrelated areas such as secure channel protocols, remote authentication, or\ndesktop user authentication, strong, systematic, and increasingly formal threat\nmodels have already been established and are used to qualitatively and\nquantitatively compare different methods. Unfortunately, the analysis of mobile\ndevice authentication is often based on weak adversary models, suggesting\noverly optimistic results on their respective security. In this article, we\nfirst introduce a new classification of adversaries to better analyze and\ncompare mobile device authentication methods. We then apply this classification\nto a systematic literature survey. The survey shows that security is still an\nafterthought and that most proposed protocols lack a comprehensive security\nanalysis. Our proposed classification of adversaries provides a strong uniform\nadversary model that can offer a comparable and transparent classification of\nsecurity properties in mobile device authentication methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 19:44:01 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Mayrhofer", "Ren\u00e9", ""], ["Mohan", "Vishwath", ""], ["Sigg", "Stephan", ""]]}, {"id": "2009.10158", "submitter": "Lynsay Shepherd", "authors": "Jamie O'Hare and Lynsay A. Shepherd", "title": "Proposal of a Novel Bug Bounty Implementation Using Gamification", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant popularity, the bug bounty process has remained broadly\nunchanged since its inception, with limited implementation of gamification\naspects. Existing literature recognises that current methods generate intensive\nresource demands, and can encounter issues impacting program effectiveness.\nThis paper proposes a novel bug bounty process aiming to alleviate resource\ndemands and mitigate inherent issues. Through the additional crowdsourcing of\nreport verification where fellow hackers perform vulnerability verification and\nreproduction, the client organisation can reduce overheads at the cost of\nrewarding more participants. The incorporation of gamification elements\nprovides a substitute for monetary rewards, as well as presenting possible\nmitigation of bug bounty program effectiveness issues. Collectively, traits of\nthe proposed process appear appropriate for resource and budget-constrained\norganisations - such Higher Education institutions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 20:11:53 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["O'Hare", "Jamie", ""], ["Shepherd", "Lynsay A.", ""]]}, {"id": "2009.10171", "submitter": "Jose Rugeles", "authors": "Jose de Jesus Rugeles, Edward Paul Guillen, Leonardo S Cardoso", "title": "A Technical Review of Wireless security for the Internet of things:\n  Software Defined Radio perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase of cyberattacks using IoT devices has exposed the\nvulnerabilities in the infrastructures that make up the IoT and have shown how\nsmall devices can affect networks and services functioning. This paper presents\na review of the vulnerabilities of the wireless technologies that bear the IoT\nand assessing the experiences in implementing wireless attacks targeting the\nInternet of Things using Software-Defined Radio (SDR) technologies. A\nsystematic literature review was conducted. The types of vulnerabilities and\nattacks that can affect the wireless technologies that stand the IoT ecosystem\nand SDR radio platforms were compared. On the IoT system model layer,\nperception layer was identified as the most vulnerable. Most attacks at this\nlevel occur due to limitations in hardware, physical exposure of devices, and\nheterogeneity of technologies. Future cybersecurity systems based on SDR radios\nhave notable advantages due to their flexibility to adapt to new communication\ntechnologies and their potential for the development of advanced tools.\nHowever, cybersecurity challenges for the Internet of Things are so complex\nthat it is needed to merge SDR hardware with cognitive techniques and\nintelligent techniques such as deep learning to adapt to rapid technological\nchanges.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 20:48:41 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Rugeles", "Jose de Jesus", ""], ["Guillen", "Edward Paul", ""], ["Cardoso", "Leonardo S", ""]]}, {"id": "2009.10187", "submitter": "Suat Mercan", "authors": "Suat Mercan, Kemal Akkaya, Lisa Cain, John Thomas", "title": "Security, Privacy and Ethical Concerns of IoT Implementations in\n  Hospitality Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) has been on the rise in the last decade as it\nfinds applications in various domains. Hospitality is one of the pioneer\nsectors that has adopted this technology to create novel services such as smart\nhotel rooms, personalized services etc. Hotels, restaurants, theme parks, and\ncruise ships are some specific application areas to improve customer\nsatisfaction by creating an intense interactive environment and data collection\nwith the use of appropriate sensors and actuators. However, applying IoT\nsolutions in the hospitality environment has some unique challenges such as\neasy physical access to devices. In addition, due to the very nature of these\ndomains, the customers are at the epicenter of these IoT technologies that\nresult in a massive amount of data collection from them. Such data and its\nmanagement along with business purposes also raises new concerns regarding\nprivacy and ethical considerations. Therefore, this paper surveys and analyzes\nsecurity, privacy and ethical issues regarding the utilization of IoT devices\nby focusing on the hospitality industry specifically. We explore some exemplary\nuses, cases, potential problems and solutions in order to contribute to better\nunderstanding and guiding the business operators in this sector.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 21:46:21 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Mercan", "Suat", ""], ["Akkaya", "Kemal", ""], ["Cain", "Lisa", ""], ["Thomas", "John", ""]]}, {"id": "2009.10200", "submitter": "Zhengxian He", "authors": "Zhengxian He, Mohit Narayan Rajput, Mustaque Ahamad", "title": "Using Inaudible Audio and Voice Assistants to Transmit Sensitive Data\n  over Telephony", "comments": "14 pages, 16 figures, arXiv:1808.05665, arXiv:1908.01551", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New security and privacy concerns arise due to the growing popularity of\nvoice assistant (VA) deployments in home and enterprise networks. A number of\npast research results have demonstrated how malicious actors can use hidden\ncommands to get VAs to perform certain operations even when a person may be in\ntheir vicinity. However, such work has not explored how compromised computers\nthat are close to VAs can leverage the phone channel to exfiltrate data with\nthe help of VAs. After characterizing the communication channel that is set up\nby commanding a VA to make a call to a phone number, we demonstrate how malware\ncan encode data into audio and send it via the phone channel. Such an attack,\nwhich can be crafted remotely, at scale and at low cost, can be used to bypass\nnetwork defenses that may be deployed against leakage of sensitive data. We use\nDual-Tone Multi-Frequency tones to encode arbitrary binary data into audio that\ncan be played over computer speakers and sent through a VA mediated phone\nchannel to a remote system. We show that modest amounts of data can be\ntransmitted with high accuracy with a short phone call lasting a few minutes.\nThis can be done while making the audio nearly inaudible for most people by\nmodulating it with a carrier with frequencies that are near the higher end of\nthe human hearing range. Several factors influence the data transfer rate,\nincluding the distance between the computer and the VA, the ambient noise that\nmay be present and the frequency of modulating carrier. With the help of a\nprototype built by us, we experimentally assess the impact of these factors on\ndata transfer rates and transmission accuracy. Our results show that voice\nassistants in the vicinity of computers can pose new threats to data stored on\nsuch computers. These threats are not addressed by traditional host and network\ndefenses. We briefly discuss possible mitigation ways.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 22:16:31 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["He", "Zhengxian", ""], ["Rajput", "Mohit Narayan", ""], ["Ahamad", "Mustaque", ""]]}, {"id": "2009.10251", "submitter": "EPTCS", "authors": "Yuri Gil Dantas (fortiss GmbH), Antoaneta Kondeva (fortiss GmbH),\n  Vivek Nigam (fortiss GmbH)", "title": "Less Manual Work for Safety Engineers: Towards an Automated Safety\n  Reasoning with Safety Patterns", "comments": "In Proceedings ICLP 2020, arXiv:2009.09158", "journal-ref": "EPTCS 325, 2020, pp. 244-257", "doi": "10.4204/EPTCS.325.29", "report-no": null, "categories": "eess.SY cs.CR cs.FL cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of safety-critical systems requires the control of hazards\nthat can potentially cause harm. To this end, safety engineers rely during the\ndevelopment phase on architectural solutions, called safety patterns, such as\nsafety monitors, voters, and watchdogs. The goal of these patterns is to\ncontrol (identified) faults that can trigger hazards. Safety patterns can\ncontrol such faults by e.g., increasing the redundancy of the system.\nCurrently, the reasoning of which pattern to use at which part of the target\nsystem to control which hazard is documented mostly in textual form or by means\nof models, such as GSN-models, with limited support for automation. This paper\nproposes the use of logic programming engines for the automated reasoning about\nsystem safety. We propose a domain-specific language for embedded system safety\nand specify as disjunctive logic programs reasoning principles used by safety\nengineers to deploy safety patterns, e.g., when to use safety monitors, or\nwatchdogs. Our machinery enables two types of automated safety reasoning: (1)\nidentification of which hazards can be controlled and which ones cannot be\ncontrolled by the existing safety patterns; and (2) automated recommendation of\nwhich patterns could be used at which place of the system to control potential\nhazards. Finally, we apply our machinery to two examples taken from the\nautomotive domain: an adaptive cruise control system and a battery management\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 00:50:58 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Dantas", "Yuri Gil", "", "fortiss GmbH"], ["Kondeva", "Antoaneta", "", "fortiss GmbH"], ["Nigam", "Vivek", "", "fortiss GmbH"]]}, {"id": "2009.10278", "submitter": "Kovila  P.L. Coopamootoo", "authors": "Kovila P.L. Coopamootoo", "title": "Usage Patterns of Privacy-Enhancing Technologies", "comments": "To be published in the Proceedings of the 2020 ACM SIGSAC Conference\n  on Computer and Communications Security (CCS '20)", "journal-ref": null, "doi": "10.1145/3372297.3423347", "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The steady reports of privacy invasions online paints a picture of the\nInternet growing into a more dangerous place. This is supported by reports of\nthe potential scale for online harms facilitated by the mass deployment of\nonline technology and the data-intensive web. While Internet users often\nexpress concern about privacy, some report taking actions to protect their\nprivacy online. We investigate the methods and technologies that individuals\nemploy to protect their privacy online. We conduct two studies, of N=180 and\nN=907, to elicit individuals' use of privacy methods online, within the US, the\nUK and Germany. We find that non-technology methods are among the most used\nmethods in the three countries. We identify distinct groupings of privacy\nmethods usage in a cluster map. The map shows that together with non-technology\nmethods of privacy protection, simple PETs that are integrated in services,\nform the most used cluster, whereas more advanced PETs form a different, least\nused cluster. We further investigate user perception and reasoning for mostly\nusing one set of PETs in a third study with N=183 participants. We do not find\na difference in perceived competency in protecting privacy online between\nadvanced and simpler PETs users. We compare use perceptions between advanced\nand simpler PETs and report on user reasoning for not using advanced PETs, as\nwell as support needed for potential use. This paper contributes to privacy\nresearch by eliciting use and perception of use across $43$ privacy methods,\nincluding $26$ PETs across three countries and provides a map of PETs usage.\nThe cluster map provides a systematic and reliable point of reference for\nfuture user-centric investigations across PETs. Overall, this research provides\na broad understanding of use and perceptions across a collection of PETs, and\ncan lead to future research for scaling use of PETs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 02:17:37 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Coopamootoo", "Kovila P. L.", ""]]}, {"id": "2009.10453", "submitter": "Daniel Hurtado", "authors": "Daniel Hurtado Ram\\'irez, J. M. Au\\~n\\'on", "title": "Privacy Preserving K-Means Clustering: A Secure Multi-Party Computation\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge discovery is one of the main goals of Artificial Intelligence. This\nKnowledge is usually stored in databases spread in different environments,\nbeing a tedious (or impossible) task to access and extract data from them. To\nthis difficulty we must add that these datasources may contain private data,\ntherefore the information can never leave the source. Privacy Preserving\nMachine Learning (PPML) helps to overcome this difficulty, employing\ncryptographic techniques, allowing knowledge discovery while ensuring data\nprivacy. K-means is one of the data mining techniques used in order to discover\nknowledge, grouping data points in clusters that contain similar features. This\npaper focuses in Privacy Preserving Machine Learning applied to K-means using\nrecent protocols from the field of criptography. The algorithm is applied to\ndifferent scenarios where data may be distributed either horizontally or\nvertically.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 11:19:24 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Ram\u00edrez", "Daniel Hurtado", ""], ["Au\u00f1\u00f3n", "J. M.", ""]]}, {"id": "2009.10524", "submitter": "Javad Hassannataj Joloudari", "authors": "Javad Hassannataj Joloudari, Mojtaba Haderbadi, Amir Mashmool,\n  Mohammad GhasemiGol, Shahab S., Amir Mosavi", "title": "Early detection of the advanced persistent threat attack using\n  performance analysis of deep learning", "comments": "38 pages, 15 figures, 5 tables", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3029202", "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common and important destructive attacks on the victim system\nis Advanced Persistent Threat (APT)-attack. The APT attacker can achieve his\nhostile goals by obtaining information and gaining financial benefits regarding\nthe infrastructure of a network. One of the solutions to detect a secret APT\nattack is using network traffic. Due to the nature of the APT attack in terms\nof being on the network for a long time and the fact that the network may crash\nbecause of high traffic, it is difficult to detect this type of attack. Hence,\nin this study, machine learning methods such as C5.0 decision tree, Bayesian\nnetwork and deep neural network are used for timely detection and\nclassification of APT-attacks on the NSL-KDD dataset. Moreover, 10-fold cross\nvalidation method is used to experiment these models. As a result, the accuracy\n(ACC) of the C5.0 decision tree, Bayesian network and 6-layer deep learning\nmodels is obtained as 95.64%, 88.37% and 98.85%, respectively, and also, in\nterms of the important criterion of the false positive rate (FPR), the FPR\nvalue for the C5.0 decision tree, Bayesian network and 6-layer deep learning\nmodels is obtained as 2.56, 10.47 and 1.13, respectively. Other criterions such\nas sensitivity, specificity, accuracy, false negative rate and F-measure are\nalso investigated for the models, and the experimental results show that the\ndeep learning model with automatic multi-layered extraction of features has the\nbest performance for timely detection of an APT-attack comparing to other\nclassification models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 19:27:35 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Joloudari", "Javad Hassannataj", ""], ["Haderbadi", "Mojtaba", ""], ["Mashmool", "Amir", ""], ["GhasemiGol", "Mohammad", ""], ["S.", "Shahab", ""], ["Mosavi", "Amir", ""]]}, {"id": "2009.10537", "submitter": "Yaguan Qian", "authors": "Yaguan Qian, Qiqi Shao, Jiamin Wang, Xiang Lin, Yankai Guo, Zhaoquan\n  Gu, Bin Wang, Chunming Wu", "title": "EI-MTD:Moving Target Defense for Edge Intelligence against Adversarial\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the boom of edge intelligence, its vulnerability to adversarial attacks\nbecomes an urgent problem. The so-called adversarial example can fool a deep\nlearning model on the edge node to misclassify. Due to the property of\ntransferability, the adversary can easily make a black-box attack using a local\nsubstitute model. Nevertheless, the limitation of resource of edge nodes cannot\nafford a complicated defense mechanism as doing on the cloud data center. To\novercome the challenge, we propose a dynamic defense mechanism, namely EI-MTD.\nIt first obtains robust member models with small size through differential\nknowledge distillation from a complicated teacher model on the cloud data\ncenter. Then, a dynamic scheduling policy based on a Bayesian Stackelberg game\nis applied to the choice of a target model for service. This dynamic defense\ncan prohibit the adversary from selecting an optimal substitute model for\nblack-box attacks. Our experimental result shows that this dynamic scheduling\ncan effectively protect edge intelligence against adversarial attacks under the\nblack-box setting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 09:04:18 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 02:44:15 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 01:13:39 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Qian", "Yaguan", ""], ["Shao", "Qiqi", ""], ["Wang", "Jiamin", ""], ["Lin", "Xiang", ""], ["Guo", "Yankai", ""], ["Gu", "Zhaoquan", ""], ["Wang", "Bin", ""], ["Wu", "Chunming", ""]]}, {"id": "2009.10568", "submitter": "Mengce Zheng", "authors": "Ruizhe Gu, Ping Wang, Mengce Zheng, Honggang Hu, Nenghai Yu", "title": "Adversarial Attack Based Countermeasures against Deep Learning\n  Side-Channel Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous previous works have studied deep learning algorithms applied in the\ncontext of side-channel attacks, which demonstrated the ability to perform\nsuccessful key recoveries. These studies show that modern cryptographic devices\nare increasingly threatened by side-channel attacks with the help of deep\nlearning. However, the existing countermeasures are designed to resist\nclassical side-channel attacks, and cannot protect cryptographic devices from\ndeep learning based side-channel attacks. Thus, there arises a strong need for\ncountermeasures against deep learning based side-channel attacks. Although deep\nlearning has the high potential in solving complex problems, it is vulnerable\nto adversarial attacks in the form of subtle perturbations to inputs that lead\na model to predict incorrectly.\n  In this paper, we propose a kind of novel countermeasures based on\nadversarial attacks that is specifically designed against deep learning based\nside-channel attacks. We estimate several models commonly used in deep learning\nbased side-channel attacks to evaluate the proposed countermeasures. It shows\nthat our approach can effectively protect cryptographic devices from deep\nlearning based side-channel attacks in practice. In addition, our experiments\nshow that the new countermeasures can also resist classical side-channel\nattacks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 14:17:18 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Gu", "Ruizhe", ""], ["Wang", "Ping", ""], ["Zheng", "Mengce", ""], ["Hu", "Honggang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2009.10644", "submitter": "Daniel Dunlavy", "authors": "Alexis Cooper and Xin Zhou and Scott Heidbrink and Daniel M. Dunlavy", "title": "Using Neural Architecture Search for Improving Software Flaw Detection\n  in Multimodal Deep Learning Models", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": "SAND2020-10141R", "categories": "stat.ML cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software flaw detection using multimodal deep learning models has been\ndemonstrated as a very competitive approach on benchmark problems. In this\nwork, we demonstrate that even better performance can be achieved using neural\narchitecture search (NAS) combined with multimodal learning models. We adapt a\nNAS framework aimed at investigating image classification to the problem of\nsoftware flaw detection and demonstrate improved results on the Juliet Test\nSuite, a popular benchmarking data set for measuring performance of machine\nlearning models in this problem domain.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:59:21 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Cooper", "Alexis", ""], ["Zhou", "Xin", ""], ["Heidbrink", "Scott", ""], ["Dunlavy", "Daniel M.", ""]]}, {"id": "2009.10664", "submitter": "Laurent Chuat", "authors": "Joel Wanner and Laurent Chuat and Adrian Perrig", "title": "A Formally Verified Protocol for Log Replication with Byzantine Fault\n  Tolerance", "comments": "International Symposium on Reliable Distributed Systems (SRDS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine fault tolerant protocols enable state replication in the presence\nof crashed, malfunctioning, or actively malicious processes. Designing such\nprotocols without the assistance of verification tools, however, is remarkably\nerror-prone. In an adversarial environment, performance and flexibility come at\nthe cost of complexity, making the verification of existing protocols extremely\ndifficult. We take a different approach and propose a formally verified\nconsensus protocol designed for a specific use case: secure logging. Our\nprotocol allows each node to propose entries in a parallel subroutine, and\nguarantees that correct nodes agree on the set of all proposed entries, without\nleader election. It is simple yet practical, as it can accommodate the workload\nof a logging system such as Certificate Transparency. We show that it is\noptimal in terms of both required rounds and tolerable faults. Using\nIsabelle/HOL, we provide a fully machine-checked security proof based upon the\nHeard-Of model, which we extend to support signatures. We also present and\nevaluate a prototype implementation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:35:31 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Wanner", "Joel", ""], ["Chuat", "Laurent", ""], ["Perrig", "Adrian", ""]]}, {"id": "2009.10861", "submitter": "Samira Pouyanfar", "authors": "Ankit Srivastava, Samira Pouyanfar, Joshua Allen, Ken Johnston, Qida\n  Ma", "title": "Distributed Differentially Private Mutual Information Ranking and Its\n  Applications", "comments": null, "journal-ref": "2020 IEEE 21st International Conference on Information Reuse and\n  Integration for Data Science (IRI) (pp. 90-96)", "doi": "10.1109/IRI49571.2020.00021", "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of Mutual Information (MI) helps understand the amount of\ninformation shared between a pair of random variables. Automated feature\nselection techniques based on MI ranking are regularly used to extract\ninformation from sensitive datasets exceeding petabytes in size, over millions\nof features and classes. Series of one-vs-all MI computations can be cascaded\nto produce n-fold MI results, rapidly pinpointing informative relationships.\nThis ability to quickly pinpoint the most informative relationships from\ndatasets of billions of users creates privacy concerns. In this paper, we\npresent Distributed Differentially Private Mutual Information (DDP-MI), a\nprivacy-safe fast batch MI, across various scenarios such as feature selection,\nsegmentation, ranking, and query expansion. This distributed implementation is\nprotected with global model differential privacy to provide strong assurances\nagainst a wide range of privacy attacks. We also show that our DDP-MI can\nsubstantially improve the efficiency of MI calculations compared to standard\nimplementations on a large-scale public dataset.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 23:55:08 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Srivastava", "Ankit", ""], ["Pouyanfar", "Samira", ""], ["Allen", "Joshua", ""], ["Johnston", "Ken", ""], ["Ma", "Qida", ""]]}, {"id": "2009.10918", "submitter": "Zhuoran Ma", "authors": "Zhuoran Ma, Jianfeng Ma, Yinbin Miao, Ximeng Liu, Kim-Kwang Raymond\n  Choo and Robert H. Deng", "title": "Pocket Diagnosis: Secure Federated Learning against Poisoning Attack in\n  the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has become prevalent in medical diagnosis due to its\neffectiveness in training a federated model among multiple health institutions\n(i.e. Data Islands (DIs)). However, increasingly massive DI-level poisoning\nattacks have shed light on a vulnerability in federated learning, which inject\npoisoned data into certain DIs to corrupt the availability of the federated\nmodel. Previous works on federated learning have been inadequate in ensuring\nthe privacy of DIs and the availability of the final federated model. In this\npaper, we design a secure federated learning mechanism with multiple keys to\nprevent DI-level poisoning attacks for medical diagnosis, called SFPA.\nConcretely, SFPA provides privacy-preserving random forest-based federated\nlearning by using the multi-key secure computation, which guarantees the\nconfidentiality of DI-related information. Meanwhile, a secure defense strategy\nover encrypted locally-submitted models is proposed to defense DI-level\npoisoning attacks. Finally, our formal security analysis and empirical tests on\na public cloud platform demonstrate the security and efficiency of SFPA as well\nas its capability of resisting DI-level poisoning attacks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 03:15:20 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ma", "Zhuoran", ""], ["Ma", "Jianfeng", ""], ["Miao", "Yinbin", ""], ["Liu", "Ximeng", ""], ["Choo", "Kim-Kwang Raymond", ""], ["Deng", "Robert H.", ""]]}, {"id": "2009.10940", "submitter": "Dr. Vinita Jindal", "authors": "Punam Bedi, Neha Gupta, Vinita Jindal", "title": "I-SiamIDS: an improved Siam-IDS for handling class imbalance in\n  network-based intrusion detection systems", "comments": "21 pages", "journal-ref": "(2020) 1-21 Applied Intelligence", "doi": "10.1007/s10489-020-01886-y", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NIDSs identify malicious activities by analyzing network traffic. NIDSs are\ntrained with the samples of benign and intrusive network traffic. Training\nsamples belong to either majority or minority classes depending upon the number\nof available instances. Majority classes consist of abundant samples for the\nnormal traffic as well as for recurrent intrusions. Whereas, minority classes\ninclude fewer samples for unknown events or infrequent intrusions. NIDSs\ntrained on such imbalanced data tend to give biased predictions against\nminority attack classes, causing undetected or misclassified intrusions. Past\nresearch works handled this class imbalance problem using data-level approaches\nthat either increase minority class samples or decrease majority class samples\nin the training data set. Although these data-level balancing approaches\nindirectly improve the performance of NIDSs, they do not address the underlying\nissue in NIDSs i.e. they are unable to identify attacks having limited training\ndata only. This paper proposes an algorithm-level approach called I-SiamIDS,\nwhich is a two-layer ensemble for handling class imbalance problem. I-SiamIDS\nidentifies both majority and minority classes at the algorithm-level without\nusing any data-level balancing techniques. The first layer of I-SiamIDS uses an\nensemble of b-XGBoost, Siamese-NN and DNN for hierarchical filtration of input\nsamples to identify attacks. These attacks are then sent to the second layer of\nI-SiamIDS for classification into different attack classes using m-XGBoost. As\ncompared to its counterparts, I-SiamIDS showed significant improvement in terms\nof Accuracy, Recall, Precision, F1-score and values of AUC for both NSL-KDD and\nCIDDS-001 datasets. To further strengthen the results, computational cost\nanalysis was also performed to study the acceptability of the proposed\nI-SiamIDS.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 06:27:59 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Bedi", "Punam", ""], ["Gupta", "Neha", ""], ["Jindal", "Vinita", ""]]}, {"id": "2009.10965", "submitter": "Jinyuan Chen", "authors": "Jinyuan Chen", "title": "Fundamental Limits of Byzantine Agreement", "comments": "The proposed protocol is simplified; the communication complexity is\n  slightly improved; and the optimal communication complexity exponent is\n  characterized", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine agreement (BA) is a distributed consensus problem where $n$\nprocessors want to reach agreement on an $\\ell$-bit message or value, but up to\n$t$ of the processors are dishonest or faulty. The challenge of this BA problem\nlies in achieving agreement despite the presence of dishonest processors who\nmay arbitrarily deviate from the designed protocol. The quality of a BA\nprotocol is measured primarily by using the following three parameters: the\nnumber of processors $n$ as a function of $t$ allowed (resilience); the number\nof rounds (round complexity, denoted by $r$); and the total number of\ncommunication bits (communication complexity, denoted by $b$). For any\nerror-free BA protocol, the known lower bounds on those three parameters are\n$n\\geq 3t+1$, $r\\geq t+1$ and $b\\geq\\Omega(\\max\\{n\\ell, nt\\})$, respectively,\nwhere a protocol that is guaranteed to be correct in all executions is said to\nbe error free. In this work by using coding theory, together with graph theory\nand linear algebra, we design a coded BA protocol (termed as COOL) that\nachieves consensus on an $\\ell$-bit message with optimal resilience,\nasymptotically optimal round complexity, and asymptotically optimal\ncommunication complexity when $\\ell \\geq t\\log t$, simultaneously. The proposed\nCOOL is an error-free and deterministic BA protocol that does not rely on\ncryptographic technique. It is secure against computationally unbounded\nadversary. With the achievable performance by the proposed COOL and the known\nlower bounds, we characterize the optimal communication complexity exponent as\n\\[\\beta^*(\\alpha,\\delta)=\\max\\{1+\\alpha,1+\\delta\\}\\] for $\\beta=\n\\lim_{n\\to\\infty}\\log b/\\log n$, $\\alpha=\\lim_{n \\to \\infty} \\log \\ell/\\log n$\nand $\\delta=\\lim_{n\\to\\infty} \\log t/\\log n$. This work reveals that coding is\nan effective approach for achieving the fundamental limits of Byzantine\nagreement and its variants.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:11:42 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 17:37:25 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "Jinyuan", ""]]}, {"id": "2009.10975", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini", "title": "A Partial Break of the Honeypots Defense to Catch Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent defense proposes to inject \"honeypots\" into neural networks in order\nto detect adversarial attacks. We break the baseline version of this defense by\nreducing the detection true positive rate to 0\\% and the detection AUC to 0.02,\nmaintaining the original distortion bounds. The authors of the original paper\nhave amended the defense in their CCS'20 paper to mitigate this attacks. To aid\nfurther research, we release the complete 2.5 hour keystroke-by-keystroke\nscreen recording of our attack process at\nhttps://nicholas.carlini.com/code/ccs_honeypot_break.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:36:37 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Carlini", "Nicholas", ""]]}, {"id": "2009.11033", "submitter": "Chander Govindarajan Mr", "authors": "Prabal Banerjee, Chander Govindarajan, Praveen Jayachandran, Sushmita\n  Ruj", "title": "Reliable, Fair and Decentralized Marketplace for Content Sharing Using\n  Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content sharing platforms such as Youtube and Vimeo have promoted pay per\nview models for artists to monetize their content. Yet, artists remain at the\nmercy of centralized platforms that control content listing and advertisement,\nwith little transparency and fairness in terms of number of views or revenue.\nOn the other hand, consumers are distanced from the publishers and cannot\nauthenticate originality of the content. In this paper, we develop a reliable\nand fair platform for content sharing without a central facilitator. The\nplatform is built as a decentralized data storage layer to store and share\ncontent in a fault-tolerant manner, where the peers also participate in a\nblockchain network. The blockchain is used to manage content listings and as an\nauditable and fair marketplace transaction processor that automatically pays\nout the content creators and the storage facilitators using smart contracts. We\ndemonstrate the system with the blockchain layer built on Hyperledger Fabric\nand the data layer built on Tahoe-LAFS,and show that our design is practical\nand scalable with low overheads.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 10:04:15 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Banerjee", "Prabal", ""], ["Govindarajan", "Chander", ""], ["Jayachandran", "Praveen", ""], ["Ruj", "Sushmita", ""]]}, {"id": "2009.11086", "submitter": "Malte Breuer", "authors": "Malte Breuer and Ulrike Meyer and Susanne Wetzel and Anja M\\\"uhlfeld", "title": "A Privacy-Preserving Protocol for the Kidney Exchange Problem", "comments": null, "journal-ref": "Proceedings of the 19th Workshop on Privacy in the Electronic\n  Society (WPES 2020)", "doi": "10.1145/3411497.3420213", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kidney donations from living donors form an attractive alternative to long\nwaiting times on a list for a post-mortem donation. However, even if a living\ndonor for a given patient is found, the donor's kidney might not meet the\npatient's medical requirements. If several patients are in this position, they\nmay be able to exchange donors in a cyclic fashion. Current algorithmic\napproaches for determining such exchange cycles neglect the privacy\nrequirements of donors and patients as they require their medical data to be\ncentrally collected and evaluated. In this paper, we present the first\ndistributed privacy-preserving protocol for kidney exchange that ensures the\ncorrect computing of the exchange cycles while at the same time protecting the\nprivacy of the patients' sensitive medical data. We prove correctness and\nsecurity of the new protocol and evaluate its practical performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:12:58 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Breuer", "Malte", ""], ["Meyer", "Ulrike", ""], ["Wetzel", "Susanne", ""], ["M\u00fchlfeld", "Anja", ""]]}, {"id": "2009.11101", "submitter": "Sudip Mittal", "authors": "Maanak Gupta, Sudip Mittal, Mahmoud Abdelsalam", "title": "AI assisted Malware Analysis: A Course for Next Generation Cybersecurity\n  Workforce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Artificial Intelligence (AI) and Machine Learning (ML) to solve\ncybersecurity problems has been gaining traction within industry and academia,\nin part as a response to widespread malware attacks on critical systems, such\nas cloud infrastructures, government offices or hospitals, and the vast amounts\nof data they generate. AI- and ML-assisted cybersecurity offers data-driven\nautomation that could enable security systems to identify and respond to cyber\nthreats in real time. However, there is currently a shortfall of professionals\ntrained in AI and ML for cybersecurity. Here we address the shortfall by\ndeveloping lab-intensive modules that enable undergraduate and graduate\nstudents to gain fundamental and advanced knowledge in applying AI and ML\ntechniques to real-world datasets to learn about Cyber Threat Intelligence\n(CTI), malware analysis, and classification, among other important topics in\ncybersecurity.\n  Here we describe six self-contained and adaptive modules in \"AI-assisted\nMalware Analysis.\" Topics include: (1) CTI and malware attack stages, (2)\nmalware knowledge representation and CTI sharing, (3) malware data collection\nand feature identification, (4) AI-assisted malware detection, (5) malware\nclassification and attribution, and (6) advanced malware research topics and\ncase studies such as adversarial learning and Advanced Persistent Threat (APT)\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 22:03:02 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Gupta", "Maanak", ""], ["Mittal", "Sudip", ""], ["Abdelsalam", "Mahmoud", ""]]}, {"id": "2009.11116", "submitter": "Vahid Shahrivari", "authors": "Vahid Shahrivari, Mohammad Mahdi Darabi, Mohammad Izadi", "title": "Phishing Detection Using Machine Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has become an indispensable part of our life, However, It also\nhas provided opportunities to anonymously perform malicious activities like\nPhishing. Phishers try to deceive their victims by social engineering or\ncreating mock-up websites to steal information such as account ID, username,\npassword from individuals and organizations. Although many methods have been\nproposed to detect phishing websites, Phishers have evolved their methods to\nescape from these detection methods. One of the most successful methods for\ndetecting these malicious activities is Machine Learning. This is because most\nPhishing attacks have some common characteristics which can be identified by\nmachine learning methods. In this paper, we compared the results of multiple\nmachine learning methods for predicting phishing websites.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 11:52:52 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Shahrivari", "Vahid", ""], ["Darabi", "Mohammad Mahdi", ""], ["Izadi", "Mohammad", ""]]}, {"id": "2009.11248", "submitter": "Swanand Kadhe", "authors": "Swanand Kadhe, Nived Rajaraman, O. Ozan Koyluoglu, Kannan Ramchandran", "title": "FastSecAgg: Scalable Secure Aggregation for Privacy-Preserving Federated\n  Learning", "comments": "Shorter version accepted in ICML Workshop on Federated Learning, July\n  2020, and CCS Workshop on Privacy-Preserving Machine Learning in Practice,\n  November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent attacks on federated learning demonstrate that keeping the training\ndata on clients' devices does not provide sufficient privacy, as the model\nparameters shared by clients can leak information about their training data. A\n'secure aggregation' protocol enables the server to aggregate clients' models\nin a privacy-preserving manner. However, existing secure aggregation protocols\nincur high computation/communication costs, especially when the number of model\nparameters is larger than the number of clients participating in an iteration\n-- a typical scenario in federated learning.\n  In this paper, we propose a secure aggregation protocol, FastSecAgg, that is\nefficient in terms of computation and communication, and robust to client\ndropouts. The main building block of FastSecAgg is a novel multi-secret sharing\nscheme, FastShare, based on the Fast Fourier Transform (FFT), which may be of\nindependent interest. FastShare is information-theoretically secure, and\nachieves a trade-off between the number of secrets, privacy threshold, and\ndropout tolerance. Riding on the capabilities of FastShare, we prove that\nFastSecAgg is (i) secure against the server colluding with 'any' subset of some\nconstant fraction (e.g. $\\sim10\\%$) of the clients in the honest-but-curious\nsetting; and (ii) tolerates dropouts of a 'random' subset of some constant\nfraction (e.g. $\\sim10\\%$) of the clients. FastSecAgg achieves significantly\nsmaller computation cost than existing schemes while achieving the same\n(orderwise) communication cost. In addition, it guarantees security against\nadaptive adversaries, which can perform client corruptions dynamically during\nthe execution of the protocol.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:49:02 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Kadhe", "Swanand", ""], ["Rajaraman", "Nived", ""], ["Koyluoglu", "O. Ozan", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "2009.11274", "submitter": "Laszlo Erdodi", "authors": "Laszlo Erdodi and Fabio Massimo Zennaro", "title": "The Agent Web Model -- Modelling web hacking for reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Website hacking is a frequent attack type used by malicious actors to obtain\nconfidential information, modify the integrity of web pages or make websites\nunavailable. The tools used by attackers are becoming more and more automated\nand sophisticated, and malicious machine learning agents seems to be the next\ndevelopment in this line. In order to provide ethical hackers with similar\ntools, and to understand the impact and the limitations of artificial agents,\nwe present in this paper a model that formalizes web hacking tasks for\nreinforcement learning agents. Our model, named Agent Web Model, considers web\nhacking as a capture-the-flag style challenge, and it defines reinforcement\nlearning problems at seven different levels of abstraction. We discuss the\ncomplexity of these problems in terms of actions and states an agent has to\ndeal with, and we show that such a model allows to represent most of the\nrelevant web vulnerabilities. Aware that the driver of advances in\nreinforcement learning is the availability of standardized challenges, we\nprovide an implementation for the first three abstraction layers, in the hope\nthat the community would consider these challenges in order to develop\nintelligent web hacking agents.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 17:35:34 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Erdodi", "Laszlo", ""], ["Zennaro", "Fabio Massimo", ""]]}, {"id": "2009.11397", "submitter": "Matthias Rottmann", "authors": "Matthias Rottmann, Kira Maag, Mathis Peyron, Natasa Krejic and Hanno\n  Gottschalk", "title": "Detection of Iterative Adversarial Attacks via Counter Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have proven to be powerful tools for processing\nunstructured data. However for high-dimensional data, like images, they are\ninherently vulnerable to adversarial attacks. Small almost invisible\nperturbations added to the input can be used to fool DNNs. Various attacks,\nhardening methods and detection methods have been introduced in recent years.\nNotoriously, Carlini-Wagner (CW) type attacks computed by iterative\nminimization belong to those that are most difficult to detect. In this work we\noutline a mathematical proof that the CW attack can be used as a detector\nitself. That is, under certain assumptions and in the limit of attack\niterations this detector provides asymptotically optimal separation of original\nand attacked images. In numerical experiments, we experimentally validate this\nstatement and furthermore obtain AUROC values up to 99.73% on CIFAR10 and\nImageNet. This is in the upper part of the spectrum of current state-of-the-art\ndetection rates for CW attacks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 21:54:36 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 14:21:02 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Rottmann", "Matthias", ""], ["Maag", "Kira", ""], ["Peyron", "Mathis", ""], ["Krejic", "Natasa", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "2009.11440", "submitter": "Riadul Islam", "authors": "Riadul Islam, Rafi Ud Daula Refat, Sai Manikanta Yerram, Hafiz Malik", "title": "Graph-Based Intrusion Detection System for Controller Area Networks", "comments": "This paper is accepted to IEEE Transactions on Intelligent\n  Transportation Systems for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The controller area network (CAN) is the most widely used intra-vehicular\ncommunication network in the automotive industry. Because of its simplicity in\ndesign, it lacks most of the requirements needed for a security-proven\ncommunication protocol. However, a safe and secured environment is imperative\nfor autonomous as well as connected vehicles. Therefore CAN security is\nconsidered one of the important topics in the automotive research community. In\nthis paper, we propose a four-stage intrusion detection system that uses the\nchi-squared method and can detect any kind of strong and weak cyber attacks in\na CAN. This work is the first-ever graph-based defense system proposed for the\nCAN. Our experimental results show that we have a very low 5.26%\nmisclassification for denial of service (DoS) attack, 10% misclassification for\nfuzzy attack, 4.76% misclassification for replay attack, and no\nmisclassification for spoofing attack. In addition, the proposed methodology\nexhibits up to 13.73% better accuracy compared to existing ID sequence-based\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 01:33:58 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 16:59:25 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Islam", "Riadul", ""], ["Refat", "Rafi Ud Daula", ""], ["Yerram", "Sai Manikanta", ""], ["Malik", "Hafiz", ""]]}, {"id": "2009.11484", "submitter": "Ryan K. L. Ko", "authors": "Hetong Jiang, Taejun Choi, Ryan K. L. Ko", "title": "Pandora: A Cyber Range Environment for the Safe Testing and Deployment\n  of Autonomous Cyber Attack Tools", "comments": "20 pages, 10 figures, to be published in SSCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity tools are increasingly automated with artificial intelligent\n(AI) capabilities to match the exponential scale of attacks, compensate for the\nrelatively slower rate of training new cybersecurity talents, and improve of\nthe accuracy and performance of both tools and users. However, the safe and\nappropriate usage of autonomous cyber attack tools - especially at the\ndevelopment stages for these tools - is still largely an unaddressed gap. Our\nsurvey of current literature and tools showed that most of the existing cyber\nrange designs are mostly using manual tools and have not considered augmenting\nautomated tools or the potential security issues caused by the tools. In other\nwords, there is still room for a novel cyber range design which allow security\nresearchers to safely deploy autonomous tools and perform automated tool\ntesting if needed. In this paper, we introduce Pandora, a safe testing\nenvironment which allows security researchers and cyber range users to perform\nexperiments on automated cyber attack tools that may have strong potential of\nusage and at the same time, a strong potential for risks. Unlike existing\ntestbeds and cyber ranges which have direct compatibility with enterprise\ncomputer systems and the potential for risk propagation across the enterprise\nnetwork, our test system is intentionally designed to be incompatible with\nenterprise real-world computing systems to reduce the risk of attack\npropagation into actual infrastructure. Our design also provides a tool to\nconvert in-development automated cyber attack tools into to executable test\nbinaries for validation and usage realistic enterprise system environments if\nrequired. Our experiments tested automated attack tools on our proposed system\nto validate the usability of our proposed environment. Our experiments also\nproved the safety of our environment by compatibility testing using simple\nmalicious code.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 04:38:47 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Jiang", "Hetong", ""], ["Choi", "Taejun", ""], ["Ko", "Ryan K. L.", ""]]}, {"id": "2009.11501", "submitter": "Ehsan Aghaei", "authors": "Ehsan Aghaei, Waseem Shadid, Ehab Al-Shaer", "title": "ThreatZoom: CVE2CWE using Hierarchical Neural Network", "comments": "This is accepted paper in EAI SecureComm 2020, 16th EAI International\n  Conference on Security and Privacy in Communication Networks", "journal-ref": "EAI SecureComm 2020, 16th EAI International Conference on Security\n  and Privacy in Communication Networks", "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Common Vulnerabilities and Exposures (CVE) represent standard means for\nsharing publicly known information security vulnerabilities. One or more CVEs\nare grouped into the Common Weakness Enumeration (CWE) classes for the purpose\nof understanding the software or configuration flaws and potential impacts\nenabled by these vulnerabilities and identifying means to detect or prevent\nexploitation. As the CVE-to-CWE classification is mostly performed manually by\ndomain experts, thousands of critical and new CVEs remain unclassified, yet\nthey are unpatchable. This significantly limits the utility of CVEs and slows\ndown proactive threat mitigation. This paper presents the first automatic tool\nto classify CVEs to CWEs. ThreatZoom uses a novel learning algorithm that\nemploys an adaptive hierarchical neural network which adjusts its weights based\non text analytic scores and classification errors. It automatically estimates\nthe CWE classes corresponding to a CVE instance using both statistical and\nsemantic features extracted from the description of a CVE. This tool is\nrigorously tested by various datasets provided by MITRE and the National\nVulnerability Database (NVD). The accuracy of classifying CVE instances to\ntheir correct CWE classes are 92% (fine-grain) and 94% (coarse-grain) for NVD\ndataset, and 75% (fine-grain) and 90% (coarse-grain) for MITRE dataset, despite\nthe small corpus.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 06:04:56 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Aghaei", "Ehsan", ""], ["Shadid", "Waseem", ""], ["Al-Shaer", "Ehab", ""]]}, {"id": "2009.11542", "submitter": "Majid Rafiei", "authors": "Majid Rafiei and Wil M. P. van der Aalst", "title": "Practical Aspect of Privacy-Preserving Data Publishing in Process Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process mining techniques such as process discovery and conformance checking\nprovide insights into actual processes by analyzing event data that are widely\navailable in information systems. These data are very valuable, but often\ncontain sensitive information, and process analysts need to balance\nconfidentiality and utility. Privacy issues in process mining are recently\nreceiving more attention from researchers which should be complemented by a\ntool to integrate the solutions and make them available in the real world. In\nthis paper, we introduce a Python-based infrastructure implementing\nstate-of-the-art privacy preservation techniques in process mining. The\ninfrastructure provides a hierarchy of usages from single techniques to the\ncollection of techniques, integrated as web-based tools. Our infrastructure\nmanages both standard and non-standard event data resulting from privacy\npreservation techniques. It also stores explicit privacy metadata to track the\nmodifications applied to protect sensitive data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 08:14:46 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Rafiei", "Majid", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2009.11546", "submitter": "Renpeng Zou", "authors": "Renpeng Zou (1), Xixiang Lv (1) ((1) School of Cyber Engineering,\n  Xidian University, Xian, China)", "title": "BCMIX: A Dynamic Self-organizing Blockchain-based Mix Anonymous System", "comments": "14 pages, 8 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing awareness of privacy-preserving has led to a strong focus on\nanonymous systems protecting anonymity. By studying early schemes, we summarize\nsome intractable problems of anonymous systems. Centralization setting is a\nuniversal problem since most anonymous system rely on central proxies or\npresetting nodes to forward and mix messages, which compromises users' privacy\nin some way. Besides, availability becomes another important factor limiting\nthe development of anonymous system due to the large requirement of additional\nadditional resources (i.e. bandwidth and storage) and high latency. Moreover,\nexisting anonymous systems may suffer from different attacks including\nabominable Man-in-the-Middle (MitM) attacks, Distributed Denial-of-service\n(DDoS) attacks and so on. In this context, we first come up with a\nBlockChain-based Mix-Net (BCMN) protocol and theoretically demonstrate its\nsecurity and anonymity. Then we construct a concrete dynamic self-organizing\nBlockChain-based MIX anonymous system (BCMIX). In the system, users and mix\nnodes utilize the blockchain transactions and their addresses to negotiate keys\nwith each other, which can resist the MitM attacks. In addition, we design an\nIP sharding algorithm to mitigate Sybil attacks. To evaluate the BCMIX system,\nwe leverage the distribution of mining pools in the real world to test the\nsystem's performance and ability to resistant attacks. Compared with other\nsystems, BCMIX provides better resilience to known attacks, while achieving low\nlatency anonymous communication without significant bandwidth or storage\nresources.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 08:25:41 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Zou", "Renpeng", ""], ["Lv", "Xixiang", ""]]}, {"id": "2009.11572", "submitter": "Hui Zhu", "authors": "Hui Zhu and Christian Gehrmann", "title": "Lic-Sec: an enhanced AppArmor Docker security profile generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the rapid development of cloud computing technology,\ncontainerization technology has drawn much attention from both industry and\nacademia. In this paper, we perform a comparative measurement analysis of\nDocker-sec, which is a Linux Security Module proposed in 2018, and a new\nAppArmor profile generator called Lic-Sec, which combines Docker-sec with a\nmodified version of LiCShield, which is also a Linux Security Module proposed\nin 2015. Docker-sec and LiCShield can be used to enhance Docker container\nsecurity based on mandatory access control and allows protection of the\ncontainer without manually configurations. Lic-Sec brings together their\nstrengths and provides stronger protection. We evaluate the effectiveness and\nperformance of Docker-sec and Lic-Sec by testing them with real-world attacks.\nWe generate an exploit database with 42 exploits effective on Docker containers\nselected from the latest 400 exploits on Exploit-db. We launch these exploits\non containers spawned with Docker-sec and Lic-Sec separately. Our evaluations\nshow that for demanding images, Lic-Sec gives protection for all privilege\nescalation attacks for which Docker-sec failed to give protection.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 09:42:56 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Zhu", "Hui", ""], ["Gehrmann", "Christian", ""]]}, {"id": "2009.11751", "submitter": "Miguel Araujo", "authors": "Miguel Araujo, Miguel Almeida, Jaime Ferreira, Luis Silva, Pedro\n  Bizarro", "title": "BreachRadar: Automatic Detection of Points-of-Compromise", "comments": "9 pages, 10 figures, published in SIAM's 2017 International\n  Conference on Data Mining (SDM17)", "journal-ref": null, "doi": "10.1137/1.9781611974973.63", "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bank transaction fraud results in over $13B annual losses for banks,\nmerchants, and card holders worldwide. Much of this fraud starts with a\nPoint-of-Compromise (a data breach or a skimming operation) where credit and\ndebit card digital information is stolen, resold, and later used to perform\nfraud. We introduce this problem and present an automatic Points-of-Compromise\n(POC) detection procedure. BreachRadar is a distributed alternating algorithm\nthat assigns a probability of being compromised to the different possible\nlocations. We implement this method using Apache Spark and show its linear\nscalability in the number of machines and transactions. BreachRadar is applied\nto two datasets with billions of real transaction records and fraud labels\nwhere we provide multiple examples of real Points-of-Compromise we are able to\ndetect. We further show the effectiveness of our method when injecting\nPoints-of-Compromise in one of these datasets, simultaneously achieving over\n90% precision and recall when only 10% of the cards have been victims of fraud.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 15:25:14 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Araujo", "Miguel", ""], ["Almeida", "Miguel", ""], ["Ferreira", "Jaime", ""], ["Silva", "Luis", ""], ["Bizarro", "Pedro", ""]]}, {"id": "2009.11762", "submitter": "Chenzhuang Du", "authors": "Chenwei Wu, Chenzhuang Du, Yang Yuan", "title": "Secure Data Sharing With Flow Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical multi-party computation setting, multiple parties jointly\ncompute a function without revealing their own input data. We consider a\nvariant of this problem, where the input data can be shared for machine\nlearning training purposes, but the data are also encrypted so that they cannot\nbe recovered by other parties. We present a rotation based method using flow\nmodel, and theoretically justified its security. We demonstrate the\neffectiveness of our method in different scenarios, including supervised secure\nmodel training, and unsupervised generative model training. Our code is\navailable at https://github.com/ duchenzhuang/flowencrypt.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 15:40:14 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Wu", "Chenwei", ""], ["Du", "Chenzhuang", ""], ["Yuan", "Yang", ""]]}, {"id": "2009.11776", "submitter": "Daniele Antonioli", "authors": "Daniele Antonioli, Nils Ole Tippenhauer, Kasper Rasmussen, Mathias\n  Payer", "title": "BLURtooth: Exploiting Cross-Transport Key Derivation in Bluetooth\n  Classic and Bluetooth Low Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bluetooth standard specifies two incompatible wireless transports:\nBluetooth Classic (BT) for high-throughput services and Bluetooth Low Energy\n(BLE) for very low-power services. BT and BLE have different security\narchitectures and threat models, but they use similar security mechanisms. In\nparticular, pairing enables two devices to establish a long term key to secure\nthe communication. Two devices have to pair over BT and BLE to use both\ntransports securely. Since pairing the same devices two times is considered\nuser-unfriendly, Bluetooth v4.2 introduced Cross-Transport Key Derivation\n(CTKD). CTKD allows two devices to pair once, either over BT or BLE, and\ngenerate both BT and BLE long term keys. Despite CTKD allowing traversal of the\nsecurity boundary between BT and BLE, the security implications of CTKD have\nnot yet been investigated. We present the first security analysis of CTKD and\nidentify five cross-transport issues for BT and BLE. These issues enable, for\nthe first time, exploitation of both BT and BLE by attacking either transport.\nBased on the identified issues, we demonstrate four novel cross-transport\nattacks resulting in device impersonation, traffic manipulation, and malicious\nsession establishment. We refer to them as BLUR attacks, as they blur the\nsecurity boundary between BT and BLE. The BLUR attacks are standard-compliant\nand therefore apply to all devices supporting CTKD, regardless of\nimplementation details. We successfully demonstrate the BLUR attacks on 13\ndevices with 10 unique Bluetooth chips, and discuss effective countermeasures.\nWe disclosed our findings and countermeasures to the Bluetooth SIG in May 2020.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:04:52 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Antonioli", "Daniele", ""], ["Tippenhauer", "Nils Ole", ""], ["Rasmussen", "Kasper", ""], ["Payer", "Mathias", ""]]}, {"id": "2009.12115", "submitter": "Mario Kahlhofer", "authors": "Mario Kahlhofer, Michael H\\\"olzl, Andreas Berger", "title": "Towards Reconstructing Multi-Step Cyber Attacks in Modern Cloud\n  Environments with Tripwires", "comments": "To be published in European Interdisciplinary Cybersecurity\n  Conference (EICC 2020)", "journal-ref": null, "doi": "10.1145/3424954.3424968", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapidly-changing cloud environments that consist of heavily interconnected\ncomponents are difficult to secure. Existing solutions often try to correlate\nmany weak indicators to identify and reconstruct multi-step cyber attacks. The\nlack of a true, causal link between most of these indicators still leaves\nadministrators with a lot of false-positives to browse through. We argue that\ncyber deception can improve the precision of attack detection systems, if used\nin a structured, and automatic way, i.e., in the form of so-called tripwires\nthat ultimately span an attack graph, which assists attack reconstruction\nalgorithms. This paper proposes an idea for a framework that combines cyber\ndeception, automatic tripwire injection and attack graphs, which eventually\nenables us to reconstruct multi-step cyber attacks in modern cloud\nenvironments.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 10:33:02 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Kahlhofer", "Mario", ""], ["H\u00f6lzl", "Michael", ""], ["Berger", "Andreas", ""]]}, {"id": "2009.12116", "submitter": "Daegeon Kim", "authors": "Daegeon Kim and Do Hyung Gu and Huy Kang Kim", "title": "Beyond PS-LTE: Security Model Design Framework for PPDR Operational\n  Environment", "comments": null, "journal-ref": "Security and Communication Networks, vol. 2020, Article ID 8869418", "doi": "10.1155/2020/8869418", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  National disasters can threaten national security and require several\norganizations to integrate the functionalities to correspond to the event. Many\ncountries are constructing a nationwide mobile communication network\ninfrastructure to share information and promptly communicate with corresponding\norganizations. Public Safety Long-Term Evolution (PS-LTE) is a communication\nmechanism adopted in many countries to achieve such a purpose. Organizations\ncan increase the efficiency of public protection and disaster relief (PPDR)\noperations by securely connecting the services run on their legacy networks to\nthe PS-LTE infrastructure. This environment allows the organizations to\ncontinue facilitating the information and system functionalities provided by\nthe legacy network. The vulnerabilities in the environment, which differ from\ncommercial LTE, need to be resolved to connect the network securely. In this\nstudy, we propose a security model design framework to derive the system\narchitecture and the security requirements targeting the restricted environment\napplied by certain technologies for a particular purpose. After analyzing the\nPPDR operation environment's characteristics under the PS-LTE infrastructure,\nwe applied the framework to derive the security model for organizations using\nPPDR services operated in their legacy networks through this infrastructure.\nAlthough the proposed security model design framework is applied to the\nspecific circumstance in this research, it can be generally adopted for the\napplication environment.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 10:41:41 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 09:54:23 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 02:51:56 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Kim", "Daegeon", ""], ["Gu", "Do Hyung", ""], ["Kim", "Huy Kang", ""]]}, {"id": "2009.12118", "submitter": "Robert Young", "authors": "K. Longmate, E.M. Ball, E. Dable-Heath, and R.J. Young", "title": "Signing Information in the Quantum Era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signatures are primarily used as a mark of authenticity, to demonstrate that\nthe sender of a message is who they claim to be. In the current digital age,\nsignatures underpin trust in the vast majority of information that we exchange,\nparticularly on public networks such as the internet. However, schemes for\nsigning digital information which are based on assumptions of computational\ncomplexity are facing challenges from advances in mathematics, the capability\nof computers, and the advent of the quantum era. Here we present a review of\ndigital signature schemes, looking at their origins and where they are under\nthreat. Next, we introduce post-quantum digital schemes, which are being\ndeveloped with the specific intent of mitigating against threats from quantum\nalgorithms whilst still relying on digital processes and infrastructure.\nFinally, we review schemes for signing information carried on quantum channels,\nwhich promise provable security metrics. Signatures were invented as a\npractical means of authenticating communications and it is important that the\npracticality of novel signature schemes is considered carefully, which is kept\nas a common theme of interest throughout this review.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 10:44:36 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Longmate", "K.", ""], ["Ball", "E. M.", ""], ["Dable-Heath", "E.", ""], ["Young", "R. J.", ""]]}, {"id": "2009.12140", "submitter": "Massimo Bartoletti", "authors": "Massimo Bartoletti and Andrea Bracciali and Cristian Lepore and\n  Alceste Scalas and Roberto Zunino", "title": "A formal model of Algorand smart contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a formal model of Algorand stateless smart contracts (stateless\nASC1.) We exploit our model to prove fundamental properties of the Algorand\nblockchain, and to establish the security of some archetypal smart contracts.\nWhile doing this, we highlight various design patterns supported by Algorand.\nWe perform experiments to validate the coherence of our formal model w.r.t. the\nactual implementation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 11:47:30 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 05:00:30 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 11:41:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Bartoletti", "Massimo", ""], ["Bracciali", "Andrea", ""], ["Lepore", "Cristian", ""], ["Scalas", "Alceste", ""], ["Zunino", "Roberto", ""]]}, {"id": "2009.12153", "submitter": "Franziska Boenisch", "authors": "Franziska Boenisch", "title": "A Survey on Model Watermarking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are applied in an increasing variety of domains.\nThe availability of large amounts of data and computational resources\nencourages the development of ever more complex and valuable models. These\nmodels are considered intellectual property of the legitimate parties who have\ntrained them, which makes their protection against stealing, illegitimate\nredistribution, and unauthorized application an urgent need. Digital\nwatermarking presents a strong mechanism for marking model ownership and,\nthereby, offers protection against those threats. The emergence of numerous\nwatermarking schemes and attacks against them is pushed forward by both\nacademia and industry, which motivates a comprehensive survey on this field.\nThis document at hand provides the first extensive literature review on ML\nmodel watermarking schemes and attacks against them. It offers a taxonomy of\nexisting approaches and systemizes general knowledge around them. Furthermore,\nit assembles the security requirements to watermarking approaches and evaluates\nschemes published by the scientific community according to them in order to\npresent systematic shortcomings and vulnerabilities. Thus, it can not only\nserve as valuable guidance in choosing the appropriate scheme for specific\nscenarios, but also act as an entry point into developing new mechanisms that\novercome presented shortcomings, and thereby contribute in advancing the field.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:03:02 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Boenisch", "Franziska", ""]]}, {"id": "2009.12204", "submitter": "Cedric Herzog", "authors": "C\\'edric Herzog (Inria, CIDRE), Val\\'erie Viet Triem Tong (MSR -\n  INRIA, CIDRE), Pierre Wilke (CIDRE), Arnaud van Straaten (MSR - INRIA),\n  Jean-Louis Lanet (LHS - Inria, CIDRE)", "title": "Evasive Windows Malware: Impact on Antiviruses and Possible\n  Countermeasures", "comments": null, "journal-ref": "17th International Conference on Security and Cryptography, Jul\n  2020, Lieusaint - Paris, France. pp.302-309", "doi": "10.5220/0009816703020309", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perpetual opposition between antiviruses and malware leads both parties\nto evolve continuously. On the one hand, antiviruses put in place solutions\nthat are more and more sophisticated and propose more complex detection\ntechniques in addition to the classic signature analysis. This sophistication\nleads antiviruses to leave more traces of their presence on the machine they\nprotect. To remain undetected as long as possible, malware can avoid executing\nwithin such environments by hunting down the modifications left by the\nantiviruses. This paper aims at determining the possibilities for malware to\ndetect the antiviruses and then evaluating the efficiency of these techniques\non a panel of antiviruses that are the most used nowadays. We then collect\nsamples showing this kind of behavior and propose to evaluate a countermeasure\nthat creates false artifacts, thus forcing malware to evade.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:59:43 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Herzog", "C\u00e9dric", "", "Inria, CIDRE"], ["Tong", "Val\u00e9rie Viet Triem", "", "MSR -\n  INRIA, CIDRE"], ["Wilke", "Pierre", "", "CIDRE"], ["van Straaten", "Arnaud", "", "MSR - INRIA"], ["Lanet", "Jean-Louis", "", "LHS - Inria, CIDRE"]]}, {"id": "2009.12360", "submitter": "Dan Li", "authors": "Dan Li, Paritosh Ramanan, Nagi Gebraeel, and Kamran Paynabar", "title": "Deep Learning based Covert Attack Identification for Industrial Control\n  Systems", "comments": "Accepted in IEEE ICMLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity of Industrial Control Systems (ICS) is drawing significant\nconcerns as data communication increasingly leverages wireless networks. A lot\nof data-driven methods were developed for detecting cyberattacks, but few are\nfocused on distinguishing them from equipment faults. In this paper, we develop\na data-driven framework that can be used to detect, diagnose, and localize a\ntype of cyberattack called covert attacks on smart grids. The framework has a\nhybrid design that combines an autoencoder, a recurrent neural network (RNN)\nwith a Long-Short-Term-Memory (LSTM) layer, and a Deep Neural Network (DNN).\nThis data-driven framework considers the temporal behavior of a generic\nphysical system that extracts features from the time series of the sensor\nmeasurements that can be used for detecting covert attacks, distinguishing them\nfrom equipment faults, as well as localize the attack/fault. We evaluate the\nperformance of the proposed method through a realistic simulation study on the\nIEEE 14-bus model as a typical example of ICS. We compare the performance of\nthe proposed method with the traditional model-based method to show its\napplicability and efficacy.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 17:48:43 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Li", "Dan", ""], ["Ramanan", "Paritosh", ""], ["Gebraeel", "Nagi", ""], ["Paynabar", "Kamran", ""]]}, {"id": "2009.12390", "submitter": "Thomas Gross", "authors": "Mohammed Aamir Ali, Thomas Gro{\\ss}, Aad van Moorsel", "title": "Investigation of 3-D Secure's Model for Fraud Detection", "comments": "Open Science Framework: https://osf.io/x6yfh. 17 pages. Author's copy\n  of the work. The work was supported by the ERC Starting Grant CASCAde, GA no.\n  716980", "journal-ref": "Proceedings of the 8th Workshop on Socio-Technical Aspects in\n  Security and Trust (STAST'18), ACM Press, December 2018, pp. 1-11", "doi": "10.1145/3361331.3361334", "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background. 3-D Secure 2.0 (3DS 2.0) is an identity federation protocol\nauthenticating the payment initiator for credit card transactions on the Web.\nAim. We aim to quantify the impact of factors used by 3DS 2.0 in its\nfraud-detection decision making process. Method. We ran credit card\ntransactions with two Web sites systematically manipulating the nominal IVs\n\\textsf{machine\\_data}, \\textsf{value}, \\textsf{region}, and \\textsf{website}.\nWe measured whether the user was \\textsf{challenged} with an authentication,\nwhether the transaction was \\textsf{declined}, and whether the card was\n\\textsf{blocked} as nominal DVs. Results. While \\textsf{website} and\n\\textsf{card} largely did not show a significant impact on any outcome,\n\\textsf{machine\\_data}, \\textsf{value} and \\textsf{region} did. A change in\n\\textsf{machine\\_data}, \\textsf{region} or \\textsf{value} made it 5-7 times as\nlikely to be challenged with password authentication. However, even in a\nforeign region with another factor being changed, the overall likelihood of\nbeing challenged only reached $60\\%$. When in the card's home region, a\ntransaction will be rarely declined ($< 5\\%$ in control, $40\\%$ with one factor\nchanged). However, in a region foreign to the card the system will more likely\ndecline transactions anyway (about $60\\%$) and any change in\n\\textsf{machine\\_data} or \\textsf{value} will lead to a near-certain declined\ntransaction. The \\textsf{region} was the only significant predictor for a card\nbeing blocked ($\\mathsf{OR}=3$). Conclusions. We found that the decisions to\nchallenge the user with a password authentication, to decline a transaction and\nto block a card are governed by different weightings. 3DS 2.0 is most likely to\ndecline transactions, especially in a foreign region. It is less likely to\nchallenge users with password authentication, even if \\textsf{machine\\_data} or\n\\textsf{value} are changed.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 18:19:26 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Ali", "Mohammed Aamir", ""], ["Gro\u00df", "Thomas", ""], ["van Moorsel", "Aad", ""]]}, {"id": "2009.12447", "submitter": "Rakshith Gopalakrishna", "authors": "Sandy Schoettler, Andrew Thompson, Rakshith Gopalakrishna, and Trinabh\n  Gupta", "title": "Walnut: A low-trust trigger-action platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trigger-action platforms are a new type of system that connect IoT devices\nwith web services. For example, the popular IFTTT platform can connect Fitbit\nwith Google Calendar to add a bedtime reminder based on sleep history. However,\nthese platforms present confidentiality and integrity risks as they run on\npublic cloud infrastructure and compute over sensitive user data. This paper\ndescribes the design, implementation, and evaluation of Walnut, a low-trust\ntrigger-action platform that mimics the functionality of IFTTT, while ensuring\nconfidentiality of data and correctness of computation, at a low resource cost.\nThe key enabler for Walnut is a new two-party secure computation protocol that\n(i) efficiently performs strings substitutions, which is a common computation\nin trigger-action platform workloads, and (ii) replicates computation over\nheterogeneous trusted-hardware machines from different vendors to ensure\ncorrectness of computation output as long as one of the machines is not\ncompromised. An evaluation of Walnut demonstrates its plausible deployability\nand low overhead relative to a non-secure baseline--3.6x in CPU and 4.3x in\nnetwork for all but a small percentage of programs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 21:58:27 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Schoettler", "Sandy", ""], ["Thompson", "Andrew", ""], ["Gopalakrishna", "Rakshith", ""], ["Gupta", "Trinabh", ""]]}, {"id": "2009.12542", "submitter": "Ashish Rajendra Kumar Sai", "authors": "Ashish Rajendra Sai, Jim Buckley, Brian Fitzgerald and Andrew Le Gear", "title": "Taxonomy of Centralization in Public Blockchain Systems: A Systematic\n  Literature Review", "comments": "Currently under review at ELS Information Processing and Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin introduced delegation of control over a monetary system from a select\nfew to all who participate in that system. This delegation is known as the\ndecentralization of controlling power and is a powerful security mechanism for\nthe ecosystem. After the introduction of Bitcoin, the field of cryptocurrency\nhas seen widespread attention from industry and academia, so much so that the\noriginal novel contribution of Bitcoin i.e. decentralization, may be\noverlooked, due to decentralizations assumed fundamental existence for the\nfunctioning of such cryptoassets. However recent studies have observed a trend\nof increased centralization in cryptocurrencies such as Bitcoin and Ethereum.\nAs this increased centralization has an impact the security of the blockchain,\nit is crucial that it is measured, towards adequate control. This research\nderives an initial taxonomy of centralization present in decentralized\nblockchains through rigorous synthesis using a systematic literature review.\nThis is followed by iterative refinement through expert interviews. We\nsystematically analyzed 89 research papers published between 2009 and 2019. Our\nstudy contributes to the existing body of knowledge by highlighting the\nmultiple definitions and measurements of centralization in the literature. We\nidentify different aspects of centralization and propose an encompassing\ntaxonomy of centralization concerns. This taxonomy is based on empirically\nobservable and measurable characteristics. It consists of 13 aspects of\ncentralization classified over six architectural layers Governance Network\nConsensus Incentive Operational and Application. We also discuss how the\nimplications of centralization can vary depending on the aspects studied. We\nbelieve that this review and taxonomy provides a comprehensive overview of\ncentralization in decentralized blockchains involving various\nconceptualizations and measures.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 08:58:48 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Sai", "Ashish Rajendra", ""], ["Buckley", "Jim", ""], ["Fitzgerald", "Brian", ""], ["Gear", "Andrew Le", ""]]}, {"id": "2009.12562", "submitter": "Ferdinando Fioretto", "authors": "Cuong Tran, Ferdinando Fioretto, Pascal Van Hentenryck", "title": "Differentially Private and Fair Deep Learning: A Lagrangian Dual\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical concern in data-driven decision making is to build models whose\noutcomes do not discriminate against some demographic groups, including gender,\nethnicity, or age. To ensure non-discrimination in learning tasks, knowledge of\nthe sensitive attributes is essential, while, in practice, these attributes may\nnot be available due to legal and ethical requirements. To address this\nchallenge, this paper studies a model that protects the privacy of the\nindividuals sensitive information while also allowing it to learn\nnon-discriminatory predictors. The method relies on the notion of differential\nprivacy and the use of Lagrangian duality to design neural networks that can\naccommodate fairness constraints while guaranteeing the privacy of sensitive\nattributes. The paper analyses the tension between accuracy, privacy, and\nfairness and the experimental evaluation illustrates the benefits of the\nproposed model on several prediction tasks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 10:50:33 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tran", "Cuong", ""], ["Fioretto", "Ferdinando", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "2009.12574", "submitter": "Laszlo Csirmaz", "authors": "Laszlo Csirmaz", "title": "An optimization problem for continuous submodular functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real continuous submodular functions, as a generalization of the\ncorresponding discrete notion to the continuous domain, gained considerable\nattention recently. The analog notion for entropy functions requires additional\nproperties: a real function defined on the non-negative orthant of $\\mathbb\nR^n$ is entropy-like (EL) if it is submodular, takes zero at zero,\nnon-decreasing, and has the Diminishing Returns property.\n  Motivated by problems concerning the Shannon complexity of multipartite\nsecret sharing, a special case of the following general optimization problem is\nconsidered: find the minimal cost of those EL functions which satisfy certain\nconstraints.\n  In our special case the cost of an EL function is the maximal value of the\n$n$ partial derivatives at zero. Another possibility could be the supremum of\nthe function range. The constraints are specified by a smooth bounded surface\n$S$ cutting off a downward closed subset. An EL function is feasible if at the\ninternal points of $S$ the left and right partial derivatives of the function\ndiffer by at least one.\n  A general lower bound for the minimal cost is given in terms of the normals\nof the surface $S$. The bound is tight when $S$ is linear. In the\ntwo-dimensional case the same bound is tight for convex or concave $S$. It is\nshown that the optimal EL function is not necessarily unique. The paper\nconcludes with several open problems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 11:44:38 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 11:36:47 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Csirmaz", "Laszlo", ""]]}, {"id": "2009.12718", "submitter": "Abhinav Aggarwal", "authors": "Nan Xu, Oluwaseyi Feyisetan, Abhinav Aggarwal, Zekun Xu, Nathanael\n  Teissier", "title": "Differentially Private Adversarial Robustness Through Randomized\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks, despite their great success in diverse domains, are\nprovably sensitive to small perturbations on correctly classified examples and\nlead to erroneous predictions. Recently, it was proposed that this behavior can\nbe combatted by optimizing the worst case loss function over all possible\nsubstitutions of training examples. However, this can be prone to weighing\nunlikely substitutions higher, limiting the accuracy gain. In this paper, we\nstudy adversarial robustness through randomized perturbations, which has two\nimmediate advantages: (1) by ensuring that substitution likelihood is weighted\nby the proximity to the original word, we circumvent optimizing the worst case\nguarantees and achieve performance gains; and (2) the calibrated randomness\nimparts differentially-private model training, which additionally improves\nrobustness against adversarial attacks on the model outputs. Our approach uses\na novel density-based mechanism based on truncated Gumbel noise, which ensures\ntraining on substitutions of both rare and dense words in the vocabulary while\nmaintaining semantic similarity for model robustness.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 00:58:32 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Xu", "Nan", ""], ["Feyisetan", "Oluwaseyi", ""], ["Aggarwal", "Abhinav", ""], ["Xu", "Zekun", ""], ["Teissier", "Nathanael", ""]]}, {"id": "2009.12724", "submitter": "Shixian Wen", "authors": "Shixian Wen, Amanda Rios, Laurent Itti", "title": "Beneficial Perturbations Network for Defending Adversarial Examples", "comments": "submitted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks can be fooled by adversarial attacks: adding carefully\ncomputed small adversarial perturbations to clean inputs can cause\nmisclassification on state-of-the-art machine learning models. The reason is\nthat neural networks fail to accommodate the distribution drift of the input\ndata caused by adversarial perturbations. Here, we present a new solution -\nBeneficial Perturbation Network (BPN) - to defend against adversarial attacks\nby fixing the distribution drift. During training, BPN generates and leverages\nbeneficial perturbations (somewhat opposite to well-known adversarial\nperturbations) by adding new, out-of-network biasing units. Biasing units\ninfluence the parameter space of the network, to preempt and neutralize future\nadversarial perturbations on input data samples. To achieve this, BPN creates\nreverse adversarial attacks during training, with very little cost, by\nrecycling the training gradients already computed. Reverse attacks are captured\nby the biasing units, and the biases can in turn effectively defend against\nfuture adversarial examples. Reverse attacks are a shortcut, i.e., they affect\nthe network's parameters without requiring instantiation of adversarial\nexamples that could assist training. We provide comprehensive empirical\nevidence showing that 1) BPN is robust to adversarial examples and is much more\nrunning memory and computationally efficient compared to classical adversarial\ntraining. 2) BPN can defend against adversarial examples with negligible\nadditional computation and parameter costs compared to training only on clean\nexamples; 3) BPN hurts the accuracy on clean examples much less than classic\nadversarial training; 4) BPN can improve the generalization of the network 5)\nBPN trained only with Fast Gradient Sign Attack can generalize to defend PGD\nattacks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 02:05:26 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 07:25:51 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wen", "Shixian", ""], ["Rios", "Amanda", ""], ["Itti", "Laurent", ""]]}, {"id": "2009.12740", "submitter": "Shengzhe Xu", "authors": "Shengzhe Xu, Manish Marwah, Naren Ramakrishnan", "title": "STAN: Synthetic Network Traffic Generation using Autoregressive Neural\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have achieved great success in recent years. However,\nlarge amounts of data are typically required to train such models. While some\ntypes of data, such as images, videos, and text, are easier to find, data in\ncertain domains is difficult to obtain. For instance, cybersecurity\napplications routinely use network traffic data which organizations are\nreluctant to share, even internally, due to privacy reasons. An alternative is\nto use synthetically generated data; however, most existing data generating\nmethods lack the ability to capture complex dependency structures that are\nusually prevalent in real data by assuming independence either temporally or\nbetween attributes. This paper presents our approach called STAN, Synthetic\nNetwork Traffic Generation using Autoregressive Neural models, to generate\nrealistic synthetic network traffic data. Our novel autoregressive neural\narchitecture captures both temporal dependence and dependence between\nattributes at any given time. It integrates convolutional neural layers (CNN)\nwith mixture density layers (MDN) and softmax layers to model both continuous\nand discrete variables. We evaluate performance of STAN by training it on both\na simulated dataset and a real network traffic data set. Multiple metrics are\nused to compare the generated data with real data and with data generated via\nseveral baseline methods. Finally, to answer the question -- can real network\ntraffic data be substituted with synthetic data to train models of comparable\naccuracy -- we consider two commonly used models for anomaly detection in such\ndata, and compare F1/MSE measures of models trained on real data and those on\nincreasing proportions of generated data. The results show only a small decline\nin accuracy of models trained solely on synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 04:20:02 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Xu", "Shengzhe", ""], ["Marwah", "Manish", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "2009.12749", "submitter": "Livat Tyapaev", "authors": "L. B. Tyapaev", "title": "Non-Archimedean dynamics of the complex shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CR cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An (asynchronous) automaton transformation of one-sided infinite words over\np-letter alphabet Fp = Z/pZ, where p is a prime, is a continuous transformation\n(w.r.t. the p-adic metric) of the ring of p-adic integers Zp. Moreover, an\nautomaton mapping generates a non-Archimedean dynamical system on Zp.\nMeasure-preservation and ergodicity (w.r.t. the Haar measure) of such dynamical\nsystems play an important role in cryptography (e.g., in stream cyphers). The\naim of this paper is to present a novel way of realizing a complex shift in\np-adics. In particular, we introduce conditions on the Mahler expansion of a\ntransformation on the p-adics which are sufficient for it to be complex shift.\nMoreover, we have a sufficient condition of ergodicity of such mappings in\nterms of Mahler expansion.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 05:44:10 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tyapaev", "L. B.", ""]]}, {"id": "2009.12913", "submitter": "Katina Kralevska", "authors": "Anton Hasselgren, Paul Kengfai Wan, Margareth Horn, Katina Kralevska,\n  Danilo Gligoroski, Arild Faxvaag", "title": "GDPR Compliance for Blockchain Applications in Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transparent and decentralized characteristics associated with blockchain\ncan be both appealing and problematic when applied to a healthcare use-case. As\nhealth data is highly sensitive, it is also highly regulated to ensure the\nprivacy of patients. At the same time, access to health data and\ninteroperability is in high demand. Regulatory frameworks such as GDPR and\nHIPAA are, amongst other objectives, meant to contribute to mitigating the risk\nof privacy violations in health data. Blockchain features can likely improve\ninteroperability and access control to health data, and at the same time,\npreserve or even increase, the privacy of patients. Blockchain applications\nshould address compliance with the current regulatory framework to increase\nreal-world feasibility. This exploratory work indicates that published\nproof-of-concepts in the health domain comply with GDRP, to an extent.\nBlockchain developers need to make design choices to be compliant with GDPR\nsince currently, none available blockchain platform can show compliance out of\nthe box.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 18:05:25 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Hasselgren", "Anton", ""], ["Wan", "Paul Kengfai", ""], ["Horn", "Margareth", ""], ["Kralevska", "Katina", ""], ["Gligoroski", "Danilo", ""], ["Faxvaag", "Arild", ""]]}, {"id": "2009.12920", "submitter": "Yining Wang", "authors": "Xi Chen and David Simchi-Levi and Yining Wang", "title": "Privacy-Preserving Dynamic Personalized Pricing with Demand Learning", "comments": "Final version. Accepted to Management Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of e-commerce has made detailed customers' personal\ninformation readily accessible to retailers, and this information has been\nwidely used in pricing decisions. When involving personalized information, how\nto protect the privacy of such information becomes a critical issue in\npractice. In this paper, we consider a dynamic pricing problem over $T$ time\nperiods with an \\emph{unknown} demand function of posted price and personalized\ninformation. At each time $t$, the retailer observes an arriving customer's\npersonal information and offers a price. The customer then makes the purchase\ndecision, which will be utilized by the retailer to learn the underlying demand\nfunction. There is potentially a serious privacy concern during this process: a\nthird party agent might infer the personalized information and purchase\ndecisions from price changes from the pricing system. Using the fundamental\nframework of differential privacy from computer science, we develop a\nprivacy-preserving dynamic pricing policy, which tries to maximize the retailer\nrevenue while avoiding information leakage of individual customer's information\nand purchasing decisions. To this end, we first introduce a notion of\n\\emph{anticipating} $(\\varepsilon, \\delta)$-differential privacy that is\ntailored to dynamic pricing problem. Our policy achieves both the privacy\nguarantee and the performance guarantee in terms of regret. Roughly speaking,\nfor $d$-dimensional personalized information, our algorithm achieves the\nexpected regret at the order of $\\tilde{O}(\\varepsilon^{-1} \\sqrt{d^3 T})$,\nwhen the customers' information is adversarially chosen. For stochastic\npersonalized information, the regret bound can be further improved to\n$\\tilde{O}(\\sqrt{d^2T} + \\varepsilon^{-2} d^2)$\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 18:32:34 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 18:53:42 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Xi", ""], ["Simchi-Levi", "David", ""], ["Wang", "Yining", ""]]}, {"id": "2009.13018", "submitter": "Ishan Karunanayake", "authors": "Ishan Karunanayake, Nadeem Ahmed, Robert Malaney, Rafiqul Islam,\n  Sanjay Jha", "title": "Anonymity with Tor: A Survey on Tor Attacks", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": "10.1109/COMST.2021.3093615", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymity networks are becoming increasingly popular in today's online world\nas more users attempt to safeguard their online privacy. Tor is currently the\nmost popular anonymity network in use and provides anonymity to both users and\nservices (hidden services). However, the anonymity provided by Tor is also\nbeing misused in various ways. Hosting illegal sites for selling drugs, hosting\ncommand and control servers for botnets, and distributing censored content are\nbut a few such examples. As a result, various parties, including governments\nand law enforcement agencies, are interested in attacks that assist in\nde-anonymising the Tor network, disrupting its operations, and bypassing its\ncensorship circumvention mechanisms. In this paper, we survey known Tor attacks\nand identify currently available techniques that lead to improved\nde-anonymisation of users and hidden services.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 02:16:12 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 23:50:16 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Karunanayake", "Ishan", ""], ["Ahmed", "Nadeem", ""], ["Malaney", "Robert", ""], ["Islam", "Rafiqul", ""], ["Jha", "Sanjay", ""]]}, {"id": "2009.13033", "submitter": "Chang Liao", "authors": "Chang Liao, Yao Cheng, Chengfang Fang, Jie Shi", "title": "Where Does the Robustness Come from? A Study of the Transformation-based\n  Ensemble Defence", "comments": "The 27th ACM Conference on Computer and Communications Security (CCS)\n  Workshop, AISec 2020", "journal-ref": "the 13th ACM Workshop on Artificial Intelligence and Security 2020", "doi": "10.1145/3411508.3421380", "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to provide a thorough study on the effectiveness of the\ntransformation-based ensemble defence for image classification and its reasons.\nIt has been empirically shown that they can enhance the robustness against\nevasion attacks, while there is little analysis on the reasons. In particular,\nit is not clear whether the robustness improvement is a result of\ntransformation or ensemble. In this paper, we design two adaptive attacks to\nbetter evaluate the transformation-based ensemble defence. We conduct\nexperiments to show that 1) the transferability of adversarial examples exists\namong the models trained on data records after different reversible\ntransformations; 2) the robustness gained through transformation-based ensemble\nis limited; 3) this limited robustness is mainly from the irreversible\ntransformations rather than the ensemble of a number of models; and 4) blindly\nincreasing the number of sub-models in a transformation-based ensemble does not\nbring extra robustness gain.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 02:55:56 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 09:16:18 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Liao", "Chang", ""], ["Cheng", "Yao", ""], ["Fang", "Chengfang", ""], ["Shi", "Jie", ""]]}, {"id": "2009.13067", "submitter": "Mubarak Albarka Umar", "authors": "Mubarak Albarka Umar, Chen Zhanfang, Yan Liu", "title": "A Hybrid Intrusion Detection with Decision Tree for Feature Selection", "comments": "16 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:2008.07405", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the size and nature of intrusion detection datasets, intrusion\ndetection systems (IDS) typically take high computational complexity to examine\nfeatures of data and identify intrusive patterns. Data preprocessing techniques\nsuch as feature selection can be used to reduce such complexity by eliminating\nirrelevant and redundant features in the dataset. The objective of this study\nis to analyze the efficiency and effectiveness of some feature selection\napproaches namely, wrapper-based and filter-based modeling approaches. To\nachieve that, a hybrid of feature selection algorithm in combination with\nwrapper and filter selection processes is designed. We propose a wrapper-based\nhybrid intrusion detection modeling with a decision tree algorithm to guide the\nselection process. Five machine learning algorithms are used on the wrapper and\nfilter-based feature selection methods to build IDS models using the UNSW-NB15\ndataset. The three filter-based methods namely, information gain, gain ratio,\nand relief are used for comparison to determine the efficiency and\neffectiveness of the proposed approach. Furthermore, a fair comparison with\nother state-of-the-art intrusion detection approaches is also performed. The\nexperimental results show that our approach is quite effective in comparison to\nstate-of-the-art works, however, it takes high computational time in comparison\nto the filter-based methods whilst achieves similar results. Our work also\nrevealed unobserved issues about the conformity of the UNSW-NB15 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 13:04:59 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Umar", "Mubarak Albarka", ""], ["Zhanfang", "Chen", ""], ["Liu", "Yan", ""]]}, {"id": "2009.13153", "submitter": "Qi Gu", "authors": "Zhihua Xia, Qi Gu, Wenhao Zhou, Lizhi Xiong, Jian Weng, Neal N. Xiong", "title": "STR: Secure Computation on Additive Shares Using the\n  Share-Transform-Reveal Strategy", "comments": "This paper has been accepted by IEEE Transactions on computers", "journal-ref": null, "doi": "10.1109/TC.2021.3073171", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of cloud computing has probably benefited each of us.\nHowever, the privacy risks brought by untrustworthy cloud servers arise the\nattention of more and more people and legislatures. In the last two decades,\nplenty of works seek to outsource various specific tasks while ensuring the\nsecurity of private data. The tasks to be outsourced are countless; however,\nthe computations involved are similar. In this paper, we construct a series of\nnovel protocols that support the secure computation of various functions on\nnumbers (e.g., the basic elementary functions) and matrices (e.g., the\ncalculation of eigenvectors and eigenvalues) in arbitrary $n\\geq 2$ servers.\nAll protocols only require constant rounds of interactions and achieve the low\ncomputation complexity. Moreover, the proposed $n$-party protocols ensure the\nsecurity of private data even though $n-1$ servers collude. The convolutional\nneural network models are utilized as the case studies to verify the protocols.\nThe theoretical analysis and experimental results demonstrate the correctness,\nefficiency, and security of the proposed protocols.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 09:08:02 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 07:05:38 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 00:49:29 GMT"}, {"version": "v4", "created": "Sun, 25 Apr 2021 01:56:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Xia", "Zhihua", ""], ["Gu", "Qi", ""], ["Zhou", "Wenhao", ""], ["Xiong", "Lizhi", ""], ["Weng", "Jian", ""], ["Xiong", "Neal N.", ""]]}, {"id": "2009.13243", "submitter": "Ishai Rosenberg", "authors": "Ishai Rosenberg and Shai Meir and Jonathan Berrebi and Ilay Gordon and\n  Guillaume Sicard and Eli (Omid) David", "title": "Generating End-to-End Adversarial Examples for Malware Classifiers Using\n  Explainability", "comments": "Accepted as a conference paper at IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the topic of explainable machine learning (ML) has been\nextensively researched. Up until now, this research focused on regular ML users\nuse-cases such as debugging a ML model. This paper takes a different posture\nand show that adversaries can leverage explainable ML to bypass multi-feature\ntypes malware classifiers. Previous adversarial attacks against such\nclassifiers only add new features and not modify existing ones to avoid harming\nthe modified malware executable's functionality. Current attacks use a single\nalgorithm that both selects which features to modify and modifies them blindly,\ntreating all features the same. In this paper, we present a different approach.\nWe split the adversarial example generation task into two parts: First we find\nthe importance of all features for a specific sample using explainability\nalgorithms, and then we conduct a feature-specific modification,\nfeature-by-feature. In order to apply our attack in black-box scenarios, we\nintroduce the concept of transferability of explainability, that is, applying\nexplainability algorithms to different classifiers using different features\nsubsets and trained on different datasets still result in a similar subset of\nimportant features. We conclude that explainability algorithms can be leveraged\nby adversaries and thus the advocates of training more interpretable\nclassifiers should consider the trade-off of higher vulnerability of those\nclassifiers to adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 12:17:32 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Rosenberg", "Ishai", "", "Omid"], ["Meir", "Shai", "", "Omid"], ["Berrebi", "Jonathan", "", "Omid"], ["Gordon", "Ilay", "", "Omid"], ["Sicard", "Guillaume", "", "Omid"], ["Eli", "", "", "Omid"], ["David", "", ""]]}, {"id": "2009.13250", "submitter": "Nathaniel Bastian PhD", "authors": "Tyler J. Shipp, Daniel J. Clouse, Michael J. De Lucia, Metin B.\n  Ahiskali, Kai Steverson, Jonathan M. Mullin, Nathaniel D. Bastian", "title": "Advancing the Research and Development of Assured Artificial\n  Intelligence and Machine Learning Capabilities", "comments": "Presented at AAAI FSS-20: Artificial Intelligence in Government and\n  Public Sector, Washington, DC, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) and machine learning (ML) have become\nincreasingly vital in the development of novel defense and intelligence\ncapabilities across all domains of warfare. An adversarial AI (A2I) and\nadversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is\nimperative that AI/ML models can defend against these attacks. A2I/AML defenses\nwill help provide the necessary assurance of these advanced capabilities that\nuse AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research\nand development of assured AI/ML capabilities via new A2I/AML defenses by\nfostering a collaborative environment across the U.S. Department of Defense and\nU.S. Intelligence Community. The A2IWG aims to identify specific challenges\nthat it can help solve or address more directly, with initial focus on three\ntopics: AI Trusted Robustness, AI System Security, and AI/ML Architecture\nVulnerabilities.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 20:12:14 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Shipp", "Tyler J.", ""], ["Clouse", "Daniel J.", ""], ["De Lucia", "Michael J.", ""], ["Ahiskali", "Metin B.", ""], ["Steverson", "Kai", ""], ["Mullin", "Jonathan M.", ""], ["Bastian", "Nathaniel D.", ""]]}, {"id": "2009.13300", "submitter": "Ananya Gangavarapu", "authors": "Ananya Gangavarapu, Ellie Daw, Abhishek Singh, Rohan Iyer, Gabriel\n  Harp, Sam Zimmerman, and Ramesh Raskar", "title": "Target Privacy Threat Modeling for COVID-19 Exposure Notification\n  Systems", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of digital contact tracing (DCT) technology during the\nCOVID-19pandemic has shown multiple benefits, including helping to slow the\nspread of infectious disease and to improve the dissemination of accurate\ninformation. However, to support both ethical technology deployment and user\nadoption, privacy must be at the forefront. With the loss of privacy being a\ncritical threat, thorough threat modeling will help us to strategize and\nprotect privacy as digital contact tracing technologies advance. Various threat\nmodeling frameworks exist today, such as LINDDUN, STRIDE, PASTA, and NIST,\nwhich focus on software system privacy, system security, application security,\nand data-centric risk, respectively. When applied to the exposure notification\nsystem (ENS) context, these models provide a thorough view of the software side\nbut fall short in addressing the integrated nature of hardware, humans,\nregulations, and software involved in such systems. Our approach addresses\nENSsas a whole and provides a model that addresses the privacy complexities of\na multi-faceted solution. We define privacy principles, privacy threats,\nattacker capabilities, and a comprehensive threat model. Finally, we outline\nthreat mitigation strategies that address the various threats defined in our\nmodel\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 02:09:51 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gangavarapu", "Ananya", ""], ["Daw", "Ellie", ""], ["Singh", "Abhishek", ""], ["Iyer", "Rohan", ""], ["Harp", "Gabriel", ""], ["Zimmerman", "Sam", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2009.13352", "submitter": "Subhash Lakshminarayana", "authors": "Subhash Lakshminarayana, Sondipon Adhikari, and Carsten Maple", "title": "Analysis of IoT-Based Load Altering Attacks Against Power Grids Using\n  the Theory of Second-Order Dynamical Systems", "comments": null, "journal-ref": "IEEE Trans. on Smart Grids, 2021", "doi": null, "report-no": null, "categories": "cs.CR cs.IT cs.SY eess.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that large-scale Internet of Things (IoT)-based\nload altering attacks can have a serious impact on power grid operations such\nas causing unsafe frequency excursions and destabilizing the grid's control\nloops. In this work, we present an analytical framework to investigate the\nimpact of IoT-based static/dynamic load altering attacks (S/DLAAs) on the power\ngrid's dynamic response. Existing work on this topic has mainly relied on\nnumerical simulations and, to date, there is no analytical framework to\nidentify the victim nodes from which that attacker can launch the most\nimpactful attacks. To address these shortcomings, we use results from\nsecond-order dynamical systems to analyze the power grid frequency control loop\nunder S/DLAAs. We use parametric sensitivity of the system's eigensolutions to\nidentify victim nodes that correspond to the least-effort destabilizing DLAAs.\nFurther, to analyze the SLAAs, we present closed-form expression for the\nsystem's frequency response in terms of the attacker's inputs, helping us\ncharacterize the minimum load change required to cause unsafe frequency\nexcursions. Using these results, we formulate the defense against S/DLAAs as a\nlinear programming problem in which we determine the minimum amount of load\nthat needs to be secured at the victim nodes to ensure system safety/stability.\nExtensive simulations conducted using benchmark IEEE-bus systems validate the\naccuracy and efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:17:16 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 18:22:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lakshminarayana", "Subhash", ""], ["Adhikari", "Sondipon", ""], ["Maple", "Carsten", ""]]}, {"id": "2009.13380", "submitter": "Federico Tavella", "authors": "Federico Tavella, Alberto Giaretta, Mauro Conti, Sasitharan\n  Balasubramaniam", "title": "A Machine Learning-based Approach to Detect Threats in Bio-Cyber DNA\n  Storage Systems", "comments": "12 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data storage is one of the main computing issues of this century. Not only\nstorage devices are converging to strict physical limits, but also the amount\nof data generated by users is growing at an unbelievable rate. To face these\nchallenges, data centres grew constantly over the past decades. However, this\ngrowth comes with a price, particularly from the environmental point of view.\nAmong various promising media, DNA is one of the most fascinating candidate. In\nour previous work, we have proposed an automated archival architecture which\nuses bioengineered bacteria to store and retrieve data, previously encoded into\nDNA. This storage technique is one example of how biological media can deliver\npower-efficient storing solutions. The similarities between these biological\nmedia and classical ones can also be a drawback, as malicious parties might\nreplicate traditional attacks on the former archival system, using biological\ninstruments and techniques. In this paper, first we analyse the main\ncharacteristics of our storage system and the different types of attacks that\ncould be executed on it. Then, aiming at identifying on-going attacks, we\npropose and evaluate detection techniques, which rely on traditional metrics\nand machine learning algorithms. We identify and adapt two suitable metrics for\nthis purpose, namely generalized entropy and information distance. Moreover,\nour trained models achieve an AUROC over 0.99 and AUPRC over 0.91.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:55:20 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Tavella", "Federico", ""], ["Giaretta", "Alberto", ""], ["Conti", "Mauro", ""], ["Balasubramaniam", "Sasitharan", ""]]}, {"id": "2009.13510", "submitter": "Uri Stemmer", "authors": "Amos Beimel, Iftach Haitner, Kobbi Nissim, Uri Stemmer", "title": "On the Round Complexity of the Shuffle Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shuffle model of differential privacy was proposed as a viable model for\nperforming distributed differentially private computations. Informally, the\nmodel consists of an untrusted analyzer that receives messages sent by\nparticipating parties via a shuffle functionality, the latter potentially\ndisassociates messages from their senders. Prior work focused on one-round\ndifferentially private shuffle model protocols, demonstrating that\nfunctionalities such as addition and histograms can be performed in this model\nwith accuracy levels similar to that of the curator model of differential\nprivacy, where the computation is performed by a fully trusted party.\n  Focusing on the round complexity of the shuffle model, we ask in this work\nwhat can be computed in the shuffle model of differential privacy with two\nrounds. Ishai et al. [FOCS 2006] showed how to use one round of the shuffle to\nestablish secret keys between every two parties. Using this primitive to\nsimulate a general secure multi-party protocol increases its round complexity\nby one. We show how two parties can use one round of the shuffle to send secret\nmessages without having to first establish a secret key, hence retaining round\ncomplexity. Combining this primitive with the two-round semi-honest protocol of\nApplebaun et al. [TCC 2018], we obtain that every randomized functionality can\nbe computed in the shuffle model with an honest majority, in merely two rounds.\nThis includes any differentially private computation. We then move to examine\ndifferentially private computations in the shuffle model that (i) do not\nrequire the assumption of an honest majority, or (ii) do not admit one-round\nprotocols, even with an honest majority. For that, we introduce two\ncomputational tasks: the common-element problem and the nested-common-element\nproblem, for which we show separations between one-round and two-round\nprotocols.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:57:42 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Beimel", "Amos", ""], ["Haitner", "Iftach", ""], ["Nissim", "Kobbi", ""], ["Stemmer", "Uri", ""]]}, {"id": "2009.13562", "submitter": "Jacob Springer", "authors": "Jacob M. Springer, Bryn Marie Reinstadler, Una-May O'Reilly", "title": "STRATA: Building Robustness with a Simple Method for Generating\n  Black-box Adversarial Attacks for Models of Code", "comments": "13 pages, 3 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are imperceptible perturbations in the input to a neural\nmodel that result in misclassification. Generating adversarial examples for\nsource code poses an additional challenge compared to the domains of images and\nnatural language, because source code perturbations must adhere to strict\nsemantic guidelines so the resulting programs retain the functional meaning of\nthe code. We propose a simple and efficient black-box method for generating\nstate-of-the-art adversarial examples on models of code. Our method generates\nuntargeted and targeted attacks, and empirically outperforms competing\ngradient-based methods with less information and less computational effort. We\nalso use adversarial training to construct a model robust to these attacks; our\nattack reduces the F1 score of code2seq by 42%. Adversarial training brings the\nF1 score on adversarial examples up to 99% of baseline.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 18:21:19 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Springer", "Jacob M.", ""], ["Reinstadler", "Bryn Marie", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "2009.13644", "submitter": "Sergio Rajsbaum", "authors": "Sergio Rajsbaum", "title": "A Distributed Computing Perspective of Unconditionally Secure\n  Information Transmission in Russian Cards Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of $A$ privately transmitting information to $B$ by a public\nannouncement overheard by an eavesdropper $C$ is considered. To do so by a\ndeterministic protocol, their inputs must be correlated. Dependent inputs are\nrepresented using a deck of cards. There is a publicly known signature\n$(a,b,c)$, where $n = a + b + c + r$, and $A$ gets $a$ cards, $B$ gets $b$\ncards, and $C$ gets $c$ cards, out of the deck of $n$ cards.\n  Using a deterministic protocol, $A$ decides its announcement based on her\nhand. Using techniques from coding theory, Johnson graphs, and additive number\ntheory, a novel perspective inspired by distributed computing theory is\nprovided, to analyze the amount of information that $A$ needs to send, while\npreventing $C$ from learning a single card of her hand. In one extreme, the\ngeneralized Russian cards problem, $B$ wants to learn all of $A$'s cards, and\nin the other, $B$ wishes to learn something about $A$'s hand.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:44:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Rajsbaum", "Sergio", ""]]}, {"id": "2009.13689", "submitter": "Olga Ohrimenko", "authors": "Sajin Sasy and Olga Ohrimenko", "title": "Oblivious Sampling Algorithms for Private Data Analysis", "comments": "Appeared in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study secure and privacy-preserving data analysis based on queries\nexecuted on samples from a dataset. Trusted execution environments (TEEs) can\nbe used to protect the content of the data during query computation, while\nsupporting differential-private (DP) queries in TEEs provides record privacy\nwhen query output is revealed. Support for sample-based queries is attractive\ndue to \\emph{privacy amplification} since not all dataset is used to answer a\nquery but only a small subset. However, extracting data samples with TEEs while\nproving strong DP guarantees is not trivial as secrecy of sample indices has to\nbe preserved. To this end, we design efficient secure variants of common\nsampling algorithms. Experimentally we show that accuracy of models trained\nwith shuffling and sampling is the same for differentially private models for\nMNIST and CIFAR-10, while sampling provides stronger privacy guarantees than\nshuffling.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 23:45:30 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Sasy", "Sajin", ""], ["Ohrimenko", "Olga", ""]]}, {"id": "2009.13720", "submitter": "Sharan Raja", "authors": "Sharan Raja, Rudraksh Tuwani", "title": "Adversarial Attacks Against Deep Learning Systems for ICD-9 Code\n  Assignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual annotation of ICD-9 codes is a time consuming and error-prone process.\nDeep learning based systems tackling the problem of automated ICD-9 coding have\nachieved competitive performance. Given the increased proliferation of\nelectronic medical records, such automated systems are expected to eventually\nreplace human coders. In this work, we investigate how a simple typo-based\nadversarial attack strategy can impact the performance of state-of-the-art\nmodels for the task of predicting the top 50 most frequent ICD-9 codes from\ndischarge summaries. Preliminary results indicate that a malicious adversary,\nusing gradient information, can craft specific perturbations, that appear as\nregular human typos, for less than 3% of words in the discharge summary to\nsignificantly affect the performance of the baseline model.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 01:45:11 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Raja", "Sharan", ""], ["Tuwani", "Rudraksh", ""]]}, {"id": "2009.13738", "submitter": "Xiaochen Li", "authors": "Xiaochen Li, Weiran Liu, Ziyi Chen, Kunzhe Huang, Zhan Qin, Lei Zhang,\n  Kui Ren", "title": "DUMP: A Dummy-Point-Based Framework for Histogram Estimation in Shuffle\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Central Differential Privacy (CDP), there is a trusted analyst who\ncollects the data from users and publishes the datasets/statistics after the\nprocedure of random perturbation. However, in this paradigm, the submitted\nusers' raw data is completely revealed to the analyst. In Local Differential\nPrivacy (LDP), each user locally perturbs the data before submitting it to the\ndata collector. Users no longer need to fully trust the analyst. Nevertheless,\nthe LDP paradigm suffers from a strong constraint on the utility of statistical\nfunctions. To tackle the conflicts, recent works propose the shuffle model to\nenhance the utility of LDP mechanism. A new participant, i.e., shuffler, is\nintroduced between users and the analyst to produce the privacy amplification.\nIn this paper, we propose DUMP (\\underline{DUM}my-\\underline{P}oint-based), a\nframework for privacy-preserving histogram estimation in shuffle model. DUMP\ncan summarize all existing histogram estimation protocols in shuffle model. We\nintroduce $dummy\\ blanket$ intuition to analyze the advantage of dummy points\nin improving utility. Then we design two protocols: pureDUMP and mixDUMP under\nDUMP framework, which achieve better trade-offs between privacy, accuracy, and\ncommunication than existing protocols. We also prove that dummy points have\nprivacy amplification locally, which can achieve enhanced privacy protection on\nthe shuffler. Besides, existing related studies lacks experimental evaluation\nthat results are still in the theoretical stage. We conduct a comprehensive\nexperimental evaluation to evaluate our proposed protocols and existing other\nprotocols. Experimental results on both synthetic and real-world datasets show\nthat our proposed protocols achieve better utility for both accuracy and\ncommunication under the same privacy guarantee than existing protocols.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 02:59:32 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Li", "Xiaochen", ""], ["Liu", "Weiran", ""], ["Chen", "Ziyi", ""], ["Huang", "Kunzhe", ""], ["Qin", "Zhan", ""], ["Zhang", "Lei", ""], ["Ren", "Kui", ""]]}, {"id": "2009.13839", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Arun Balaji Buduru, Ponnurangam Kumaraguru", "title": "imdpGAN: Generating Private and Specific Data with Generative\n  Adversarial Networks", "comments": "9 pages, 7 figures, Accepted at IEEE TPS'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Network (GAN) and its variants have shown promising\nresults in generating synthetic data. However, the issues with GANs are: (i)\nthe learning happens around the training samples and the model often ends up\nremembering them, consequently, compromising the privacy of individual samples\n- this becomes a major concern when GANs are applied to training data including\npersonally identifiable information, (ii) the randomness in generated data -\nthere is no control over the specificity of generated samples. To address these\nissues, we propose imdpGAN - an information maximizing differentially private\nGenerative Adversarial Network. It is an end-to-end framework that\nsimultaneously achieves privacy protection and learns latent representations.\nWith experiments on MNIST dataset, we show that imdpGAN preserves the privacy\nof the individual data point, and learns latent codes to control the\nspecificity of the generated samples. We perform binary classification on digit\npairs to show the utility versus privacy trade-off. The classification accuracy\ndecreases as we increase privacy levels in the framework. We also\nexperimentally show that the training process of imdpGAN is stable but\nexperience a 10-fold time increase as compared with other GAN frameworks.\nFinally, we extend imdpGAN framework to CelebA dataset to show how the privacy\nand learned representations can be used to control the specificity of the\noutput.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:03:32 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Gupta", "Saurabh", ""], ["Buduru", "Arun Balaji", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2009.13865", "submitter": "Andrea Coladangelo", "authors": "Andrea Coladangelo, Christian Majenz, Alexander Poremba", "title": "Quantum copy-protection of compute-and-compare programs in the quantum\n  random oracle model", "comments": "48 pages. Minor fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy-protection allows a software distributor to encode a program in such a\nway that it can be evaluated on any input, yet it cannot be \"pirated\" - a\nnotion that is impossible to achieve in a classical setting. Aaronson (CCC\n2009) initiated the formal study of quantum copy-protection schemes, and\nspeculated that quantum cryptography could offer a solution to the problem\nthanks to the quantum no-cloning theorem. In this work, we introduce a quantum\ncopy-protection scheme for a large class of evasive functions known as\n\"compute-and-compare programs\" - a more expressive generalization of point\nfunctions. A compute-and-compare program $\\mathsf{CC}[f,y]$ is specified by a\nfunction $f$ and a string $y$ within its range: on input $x$,\n$\\mathsf{CC}[f,y]$ outputs $1$, if $f(x) = y$, and $0$ otherwise. We prove that\nour scheme achieves non-trivial security against fully malicious adversaries in\nthe quantum random oracle model (QROM), which makes it the first\ncopy-protection scheme to enjoy any level of provable security in a standard\ncryptographic model. As a complementary result, we show that the same scheme\nfulfils a weaker notion of software protection, called \"secure software\nleasing\", introduced very recently by Ananth and La Placa (eprint 2020), with a\nstandard security bound in the QROM, i.e. guaranteeing negligible adversarial\nadvantage.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:41:53 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:22:44 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Coladangelo", "Andrea", ""], ["Majenz", "Christian", ""], ["Poremba", "Alexander", ""]]}, {"id": "2009.13868", "submitter": "Israr Ali", "authors": "Israr Ali, Syed Hasan Adil, Mansoor Ebrahim", "title": "Intrusion Detection Framework for SQL Injection", "comments": "7 pages, 5 Figures,ASIAN JOURNAL OF ENGINEERING, SCIENCES &\n  TECHNOLOGY,VOL.6, ISSUE 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of internet, E-Business and e-commerce applications are using\nDatabases as their integral part. These Databases irrespective of the\ntechnology used are vulnerable to SQL injection attacks. These Attacks are\nconsidered very dangerous as well as very easy to use for attackers and\nintruders. In this paper, we are proposing a new approach to detect intrusion\nfrom attackers by using SQL injection. The main idea of our proposed solution\nis to create trusted user profiles fetched from the Queries submitted by\nauthorized users by using association rules. After that we will use a hybrid\n(anomaly + misuse) detection model which will depend on data mining techniques\nto detect queries that deviates from our normal behavior profile. The normal\nbehavior profile will be created in XML format. In this way we can minimize\nfalse positive alarms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:46:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Ali", "Israr", ""], ["Adil", "Syed Hasan", ""], ["Ebrahim", "Mansoor", ""]]}, {"id": "2009.13914", "submitter": "Shaza Zeitouni", "authors": "Shaza Zeitouni, Ghada Dessouky and Ahmad-Reza Sadeghi", "title": "SoK: On the Security Challenges and Risks of Multi-Tenant FPGAs in the\n  Cloud", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their continuous growth and penetration into new markets, Field\nProgrammable Gate Arrays (FPGAs) have recently made their way into hardware\nacceleration of machine learning among other specialized compute-intensive\nservices in cloud data centers, such as Amazon and Microsoft. To further\nmaximize their utilization in the cloud, several academic works propose the\nspatial multi-tenant deployment model, where the FPGA fabric is simultaneously\nshared among mutually mistrusting clients. This is enabled by leveraging the\npartial reconfiguration property of FPGAs, which allows to split the FPGA\nfabric into several logically isolated regions and reconfigure the\nfunctionality of each region independently at runtime. In this paper, we survey\nindustrial and academic deployment models of multi-tenant FPGAs in the cloud\ncomputing settings, and highlight their different adversary models and security\nguarantees, while shedding light on their fundamental shortcomings from a\nsecurity standpoint. We further survey and classify existing academic works\nthat demonstrate a new class of remotely exploitable physical attacks on\nmulti-tenant FPGA devices, where these attacks are launched remotely by\nmalicious clients sharing physical resources with victim users. Through\ninvestigating the problem of end-to-end multi-tenant FPGA deployment more\ncomprehensively, we reveal how these attacks actually represent only one\ndimension of the problem, while various open security and privacy challenges\nremain unaddressed. We conclude with our insights and a call for future\nresearch to tackle these challenges.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 10:12:53 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 11:53:05 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zeitouni", "Shaza", ""], ["Dessouky", "Ghada", ""], ["Sadeghi", "Ahmad-Reza", ""]]}, {"id": "2009.13978", "submitter": "Joaquin Garcia-Alfaro", "authors": "Neetu Sharma, Rajeev Anand Sahu, Vishal Saraswat and Joaquin\n  Garcia-Alfaro", "title": "Anonymous proof-of-asset transactions using designated blind signatures", "comments": "17 pages, extended conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scheme to preserve the anonymity of users in proof-of-asset\ntransactions. We assume bitcoin-like cryptocurrency systems in which a user\nmust prove the strength of its assets (i.e., solvency), prior conducting\nfurther transactions. The traditional way of addressing such a problem is the\nuse of blind signatures, i.e., a kind of digital signature whose properties\nsatisfy the anonymity of the signer. Our work focuses on the use of a\ndesignated verifier signature scheme that limits to only a single authorized\nparty (within a group of signature requesters) to verify the correctness of the\ntransaction.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 12:59:27 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 17:54:28 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Sharma", "Neetu", ""], ["Sahu", "Rajeev Anand", ""], ["Saraswat", "Vishal", ""], ["Garcia-Alfaro", "Joaquin", ""]]}, {"id": "2009.14007", "submitter": "Tin Tironsakkul", "authors": "Tin Tironsakkul, Manuel Maarek, Andrea Eross, Mike Just", "title": "Tracking Mixed Bitcoins", "comments": "17 pages, 3 figures, CBT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixer services purportedly remove all connections between the input\n(deposited) Bitcoins and the output (withdrawn) mixed Bitcoins, seemingly\nrendering taint analysis tracking ineffectual. In this paper, we introduce and\nexplore a novel tracking strategy, called \\emph{Address Taint Analysis}, that\nadapts from existing transaction-based taint analysis techniques for tracking\nBitcoins that have passed through a mixer service. We also investigate the\npotential of combining address taint analysis with address clustering and\nbackward tainting. We further introduce a set of filtering criteria that reduce\nthe number of false-positive results based on the characteristics of withdrawn\ntransactions and evaluate our solution with verifiable mixing transactions of\nnine mixer services from previous reverse-engineering studies. Our finding\nshows that it is possible to track the mixed Bitcoins from the deposited\nBitcoins using address taint analysis and the number of potential transaction\noutputs can be significantly reduced with the filtering criteria.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 13:43:38 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 10:44:03 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Tironsakkul", "Tin", ""], ["Maarek", "Manuel", ""], ["Eross", "Andrea", ""], ["Just", "Mike", ""]]}, {"id": "2009.14021", "submitter": "Liyi Zhou", "authors": "Liyi Zhou, Kaihua Qin, Christof Ferreira Torres, Duc V Le, Arthur\n  Gervais", "title": "High-Frequency Trading on Decentralized On-Chain Exchanges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized exchanges (DEXs) allow parties to participate in financial\nmarkets while retaining full custody of their funds. However, the transparency\nof blockchain-based DEX in combination with the latency for transactions to be\nprocessed, makes market-manipulation feasible. For instance, adversaries could\nperform front-running -- the practice of exploiting (typically non-public)\ninformation that may change the price of an asset for financial gain. In this\nwork we formalize, analytically exposit and empirically evaluate an augmented\nvariant of front-running: sandwich attacks, which involve front- and\nback-running victim transactions on a blockchain-based DEX. We quantify the\nprobability of an adversarial trader being able to undertake the attack, based\non the relative positioning of a transaction within a blockchain block. We find\nthat a single adversarial trader can earn a daily revenue of over several\nthousand USD when performing sandwich attacks on one particular DEX -- Uniswap,\nan exchange with over 5M USD daily trading volume by June 2020. In addition to\na single-adversary game, we simulate the outcome of sandwich attacks under\nmultiple competing adversaries, to account for the real-world trading\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 13:50:35 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Zhou", "Liyi", ""], ["Qin", "Kaihua", ""], ["Torres", "Christof Ferreira", ""], ["Le", "Duc V", ""], ["Gervais", "Arthur", ""]]}, {"id": "2009.14285", "submitter": "Manasi Mohandas", "authors": "Gaganjeet Reen, Manasi Mohandas and S Venkatesan", "title": "Decentralized Patient Centric e-Health Record Management System using\n  Blockchain and IPFS", "comments": null, "journal-ref": "2019 IEEE Conference on Information and Communication Technology,\n  Allahabad, India, 2019, pp. 1-7", "doi": "10.1109/CICT48419.2019.9066212", "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Health Records(EHR) are gaining a lot of popularity all over the\nworld. The current EHR systems however have their fair share of problems\nrelated to privacy and security. We have proposed a mechanism which provides a\nsolution to most of these problems. Using a permissioned Ethereum blockchain\nallows the hospitals and patients across the world to be connected to each\nother. Our mechanism uses a combination of symmetric and asymmetric key\ncryptography to ensure the secure storage and selective access of records. It\ngives patients full control over their health records and also allows them to\ngrant or revoke a hospital's access to his/her records. We have used IPFS(inter\nplanetary file system) to store records which has the advantage of being\ndistributed and ensures immutability of records. The proposed model also\nmaintains the statistics of diseases without violating the privacy of any\npatient.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 19:54:58 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Reen", "Gaganjeet", ""], ["Mohandas", "Manasi", ""], ["Venkatesan", "S", ""]]}, {"id": "2009.14313", "submitter": "Jonatas Santos De Souza J. S. de Souza", "authors": "Jonatas S. de Souza, Jair M. Abe, Luiz A. de Lima, Nilson A. de Souza", "title": "The General Law Principles for Protection the Personal Data and their\n  Importance", "comments": "12 pages, 5 figures, 7th International Conference on Computer\n  Science, Engineering and Information Technology (CSEIT 2020)", "journal-ref": "Computer Science & Information Technology (CS & IT), 2020", "doi": "10.5121/csit.2020.101110", "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid technological change and globalization have created new challenges when\nit comes to the protection and processing of personal data. In 2018, Brazil\npresented a new law that has the proposal to inform how personal data should be\ncollected and treated, to guarantee the security and integrity of the data\nholder. The purpose of this paper is to emphasize the principles of the General\nLaw on Personal Data Protection, informing real cases of leakage of personal\ndata and thus obtaining an understanding of the importance of gains that meet\nthe interests of Internet users on the subject and its benefits to the entire\nBrazilian society.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 21:28:14 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["de Souza", "Jonatas S.", ""], ["Abe", "Jair M.", ""], ["de Lima", "Luiz A.", ""], ["de Souza", "Nilson A.", ""]]}, {"id": "2009.14330", "submitter": "Ha Dao", "authors": "Ha Dao, Kensuke Fukuda", "title": "A machine learning approach for detecting CNAME cloaking-based tracking\n  on the Web", "comments": "This paper is going to be published in IEEE Globecom 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various in-browser privacy protection techniques have been designed to\nprotect end-users from third-party tracking. In an arms race against these\ncounter-measures, the tracking providers developed a new technique called CNAME\ncloaking based tracking to avoid issues with browsers that block third-party\ncookies and requests. To detect this tracking technique, browser extensions\nrequire on-demand DNS lookup APIs. This feature is however only supported by\nthe Firefox browser.\n  In this paper, we propose a supervised machine learning-based method to\ndetect CNAME cloaking-based tracking without the on-demand DNS lookup. Our goal\nis to detect both sites and requests linked to CNAME cloaking-related tracking.\nWe crawl a list of target sites and store all HTTP/HTTPS requests with their\nattributes. Then we label all instances automatically by looking up CNAME\nrecord of subdomain, and applying wildcard matching based on well-known\ntracking filter lists. After extracting features, we build a supervised\nclassification model to distinguish site and request related to CNAME\ncloaking-based tracking. Our evaluation shows that the proposed approach\noutperforms well-known tracking filter lists: F1 scores of 0.790 for sites and\n0.885 for requests. By analyzing the feature permutation importance, we\ndemonstrate that the number of scripts and the proportion of XMLHttpRequests\nare discriminative for detecting sites, and the length of URL request is\nhelpful in detecting requests. Finally, we analyze concept drift by using the\n2018 dataset to train a model and obtain a reasonable performance on the 2020\ndataset for detecting both sites and requests using CNAME cloaking-based\ntracking.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 22:33:19 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Dao", "Ha", ""], ["Fukuda", "Kensuke", ""]]}, {"id": "2009.14376", "submitter": "Ahmadreza Mosallanezhad", "authors": "Ahmadreza Mosallanezhad and Yasin N. Silva and Michelle V. Mancenido\n  and Huan Liu", "title": "Toward Privacy and Utility Preserving Image Representation", "comments": "Accepted as a working paper in SBP-BRiMS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face images are rich data items that are useful and can easily be collected\nin many applications, such as in 1-to-1 face verification tasks in the domain\nof security and surveillance systems. Multiple methods have been proposed to\nprotect an individual's privacy by perturbing the images to remove traces of\nidentifiable information, such as gender or race. However, significantly less\nattention has been given to the problem of protecting images while maintaining\noptimal task utility. In this paper, we study the novel problem of creating\nprivacy-preserving image representations with respect to a given utility task\nby proposing a principled framework called the Adversarial Image Anonymizer\n(AIA). AIA first creates an image representation using a generative model, then\nenhances the learned image representations using adversarial learning to\npreserve privacy and utility for a given task. Experiments were conducted on a\npublicly available data set to demonstrate the effectiveness of AIA as a\nprivacy-preserving mechanism for face images.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 01:25:00 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 16:27:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mosallanezhad", "Ahmadreza", ""], ["Silva", "Yasin N.", ""], ["Mancenido", "Michelle V.", ""], ["Liu", "Huan", ""]]}, {"id": "2009.14455", "submitter": "Jayaraman J. Thiagarajan", "authors": "Uday Shankar Shanthamallu, Jayaraman J. Thiagarajan and Andreas\n  Spanias", "title": "Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs), a generalization of neural networks to\ngraph-structured data, are often implemented using message passes between\nentities of a graph. While GNNs are effective for node classification, link\nprediction and graph classification, they are vulnerable to adversarial\nattacks, i.e., a small perturbation to the structure can lead to a non-trivial\nperformance degradation. In this work, we propose Uncertainty Matching GNN\n(UM-GNN), that is aimed at improving the robustness of GNN models, particularly\nagainst poisoning attacks to the graph structure, by leveraging epistemic\nuncertainties from the message passing framework. More specifically, we propose\nto build a surrogate predictor that does not directly access the graph\nstructure, but systematically extracts reliable knowledge from a standard GNN\nthrough a novel uncertainty-matching strategy. Interestingly, this uncoupling\nmakes UM-GNN immune to evasion attacks by design, and achieves significantly\nimproved robustness against poisoning attacks. Using empirical studies with\nstandard benchmarks and a suite of global and target attacks, we demonstrate\nthe effectiveness of UM-GNN, when compared to existing baselines including the\nstate-of-the-art robust GCN.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 05:29:42 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Shanthamallu", "Uday Shankar", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Spanias", "Andreas", ""]]}, {"id": "2009.14685", "submitter": "\\\"Omer Faruk Irmak", "authors": "\\\"Omer Faruk Irmak, Arda Yurdakul", "title": "An Embedded RISC-V Core with Fast Modular Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest concerns in IoT is privacy and security. Encryption and\nauthentication need big power budgets, which battery-operated IoT end-nodes do\nnot have. Hardware accelerators designed for specific cryptographic operations\nprovide little to no flexibility for future updates. Custom instruction\nsolutions are smaller in area and provide more flexibility for new methods to\nbe implemented. One drawback of custom instructions is that the processor has\nto wait for the operation to finish. Eventually, the response time of the\ndevice to real-time events gets longer. In this work, we propose a processor\nwith an extended custom instruction for modular multiplication, which blocks\nthe processor, typically, two cycles for any size of modular multiplication\nwhen used in Partial Execution mode. We adopted embedded and compressed\nextensions of RISC-V for our proof-of-concept CPU. Our design is benchmarked on\nrecent cryptographic algorithms in the field of elliptic-curve cryptography.\nOur CPU with 128-bit modular multiplication operates at 136MHz on ASIC and\n81MHz on FPGA. It achieves up to 13x speed up on software implementations while\nreducing overall power consumption by up to 95\\% with 41\\% average area\noverhead over our base architecture.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:02:36 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Irmak", "\u00d6mer Faruk", ""], ["Yurdakul", "Arda", ""]]}, {"id": "2009.14720", "submitter": "Huanrui Yang", "authors": "Huanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew\n  Gardner, Andrew Touchet, Wesley Wilkes, Heath Berry, Hai Li", "title": "DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of\n  Ensembles", "comments": "To be appeared in NeurIPS 2020 conference (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research finds CNN models for image classification demonstrate\noverlapped adversarial vulnerabilities: adversarial attacks can mislead CNN\nmodels with small perturbations, which can effectively transfer between\ndifferent models trained on the same dataset. Adversarial training, as a\ngeneral robustness improvement technique, eliminates the vulnerability in a\nsingle model by forcing it to learn robust features. The process is hard, often\nrequires models with large capacity, and suffers from significant loss on clean\ndata accuracy. Alternatively, ensemble methods are proposed to induce\nsub-models with diverse outputs against a transfer adversarial example, making\nthe ensemble robust against transfer attacks even if each sub-model is\nindividually non-robust. Only small clean accuracy drop is observed in the\nprocess. However, previous ensemble training methods are not efficacious in\ninducing such diversity and thus ineffective on reaching robust ensemble. We\npropose DVERGE, which isolates the adversarial vulnerability in each sub-model\nby distilling non-robust features, and diversifies the adversarial\nvulnerability to induce diverse outputs against a transfer attack. The novel\ndiversity metric and training procedure enables DVERGE to achieve higher\nrobustness against transfer attacks comparing to previous ensemble methods, and\nenables the improved robustness when more sub-models are added to the ensemble.\nThe code of this work is available at https://github.com/zjysteven/DVERGE\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:57:35 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 16:35:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yang", "Huanrui", ""], ["Zhang", "Jingyang", ""], ["Dong", "Hongliang", ""], ["Inkawhich", "Nathan", ""], ["Gardner", "Andrew", ""], ["Touchet", "Andrew", ""], ["Wilkes", "Wesley", ""], ["Berry", "Heath", ""], ["Li", "Hai", ""]]}, {"id": "2009.14732", "submitter": "Divya Ojha", "authors": "Divya Ojha and Sandhya Dwarkadas (University of Rochester)", "title": "Timing Cache Accesses to Eliminate Side Channels in Shared Software", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report: UR CSD / 1009", "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timing side channels have been used to extract cryptographic keys and\nsensitive documents, even from trusted enclaves. In this paper, we focus on\ncache side channels created by access to shared code or data in the memory\nhierarchy. This vulnerability is exploited by several known attacks, e.g,\nevict+reload for recovering an RSA key and Spectre variants for data leaked due\nto speculative accesses.\n  The key insight in this paper is the importance of the first access to the\nshared data after a victim brings the data into the cache. To eliminate the\ntiming side channel, we ensure that the first access by a process to any cache\nline loaded by another process results in a miss. We accomplish this goal by\nusing a combination of timestamps and a novel hardware design to allow\nefficient parallel comparisons of the timestamps. The solution works at all the\ncache levels and defends against an attacker process running on another core,\nsame core, or another hyperthread. Our design retains the benefits of a shared\ncache: allowing processes to utilize the entire cache for their execution and\nretaining a single copy of shared code and data (data deduplication).\n  Our implementation in the GEM5 simulator demonstrates that the system is able\nto defend against RSA key extraction. We evaluate performance using SPECCPU2006\nand observe overhead due to first access delay to be 2.17%. The overhead due to\nthe security context bookkeeping is of the order of 0.3%.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:15:00 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ojha", "Divya", "", "University of Rochester"], ["Dwarkadas", "Sandhya", "", "University of Rochester"]]}]