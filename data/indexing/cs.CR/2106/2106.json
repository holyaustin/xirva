[{"id": "2106.00001", "submitter": "Vikrant Singhal", "authors": "Vikrant Singhal, Thomas Steinke", "title": "Privately Learning Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private data analysis suffers a costly curse of dimensionality. However, the\ndata often has an underlying low-dimensional structure. For example, when\noptimizing via gradient descent, the gradients often lie in or near a\nlow-dimensional subspace. If that low-dimensional structure can be identified,\nthen we can avoid paying (in terms of privacy or accuracy) for the high ambient\ndimension.\n  We present differentially private algorithms that take input data sampled\nfrom a low-dimensional linear subspace (possibly with a small amount of error)\nand output that subspace (or an approximation to it). These algorithms can\nserve as a pre-processing step for other procedures.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:09:23 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Singhal", "Vikrant", ""], ["Steinke", "Thomas", ""]]}, {"id": "2106.00038", "submitter": "Lei Jiang", "authors": "Qian Lou and Lei Jiang", "title": "HEMET: A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile\n  Neural Network Architecture", "comments": null, "journal-ref": "The Thirty-eighth International Conference on Machine Learning\n  2021", "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently Homomorphic Encryption (HE) is used to implement Privacy-Preserving\nNeural Networks (PPNNs) that perform inferences directly on encrypted data\nwithout decryption. Prior PPNNs adopt mobile network architectures such as\nSqueezeNet for smaller computing overhead, but we find na\\\"ively using mobile\nnetwork architectures for a PPNN does not necessarily achieve shorter inference\nlatency. Despite having less parameters, a mobile network architecture\ntypically introduces more layers and increases the HE multiplicative depth of a\nPPNN, thereby prolonging its inference latency. In this paper, we propose a\n\\textbf{HE}-friendly privacy-preserving \\textbf{M}obile neural n\\textbf{ET}work\narchitecture, \\textbf{HEMET}. Experimental results show that, compared to\nstate-of-the-art (SOTA) PPNNs, HEMET reduces the inference latency by\n$59.3\\%\\sim 61.2\\%$, and improves the inference accuracy by $0.4 \\% \\sim\n0.5\\%$.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 18:05:53 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lou", "Qian", ""], ["Jiang", "Lei", ""]]}, {"id": "2106.00073", "submitter": "Tanujay Saha", "authors": "Jacob Brown, Tanujay Saha, Niraj K. Jha", "title": "GRAVITAS: Graphical Reticulated Attack Vectors for Internet-of-Things\n  Aggregate Security", "comments": "This article has been published in IEEE Transactions on Emerging\n  Topics in Computing, 2021", "journal-ref": null, "doi": "10.1109/TETC.2021.3082525", "report-no": null, "categories": "cs.CR cs.AI cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet-of-Things (IoT) and cyber-physical systems (CPSs) may consist of\nthousands of devices connected in a complex network topology. The diversity and\ncomplexity of these components present an enormous attack surface, allowing an\nadversary to exploit security vulnerabilities of different devices to execute a\npotent attack. Though significant efforts have been made to improve the\nsecurity of individual devices in these systems, little attention has been paid\nto security at the aggregate level. In this article, we describe a\ncomprehensive risk management system, called GRAVITAS, for IoT/CPS that can\nidentify undiscovered attack vectors and optimize the placement of defenses\nwithin the system for optimal performance and cost. While existing risk\nmanagement systems consider only known attacks, our model employs a machine\nlearning approach to extrapolate undiscovered exploits, enabling us to identify\nattacks overlooked by manual penetration testing (pen-testing). The model is\nflexible enough to analyze practically any IoT/CPS and provide the system\nadministrator with a concrete list of suggested defenses that can reduce system\nvulnerability at optimal cost. GRAVITAS can be employed by governments,\ncompanies, and system administrators to design secure IoT/CPS at scale,\nproviding a quantitative measure of security and efficiency in a world where\nIoT/CPS devices will soon be ubiquitous.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 19:35:23 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Brown", "Jacob", ""], ["Saha", "Tanujay", ""], ["Jha", "Niraj K.", ""]]}, {"id": "2106.00141", "submitter": "Tyler Kaczmarek", "authors": "Shamaria Engram, Tyler Kaczmarek, Alice Lee, and David Bigelow", "title": "Proactive Provenance Policies for Automatic Cryptographic Data Centric\n  Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data provenance analysis has been used as an assistive measure for ensuring\nsystem integrity. However, such techniques are typically reactive approaches to\nidentify the root cause of an attack in its aftermath. This is in part due to\nfact that the collection of provenance metadata often results in a deluge of\ninformation that cannot easily be queried and analyzed in real time. This paper\npresents an approach for proactively reasoning about provenance metadata within\nthe Automatic Cryptographic Data Centric (ACDC) security architecture, a new\nsecurity infrastructure in which all data interactions are considered at a\ncoarse granularity, similar to the Function as a Service model. At this scale,\nwe have found that data interactions are manageable for the proactive\nspecification and evaluation of provenance policies -- constraints placed on\nprovenance metadata to prevent the consumption of untrusted data. This paper\nprovides a model for proactively evaluating provenance metadata in the ACDC\nparadigm as well as a case study of an electronic voting scheme to demonstrate\nthe applicability of ACDC and the provenance policies needed to ensure data\nintegrity.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 23:20:54 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Engram", "Shamaria", ""], ["Kaczmarek", "Tyler", ""], ["Lee", "Alice", ""], ["Bigelow", "David", ""]]}, {"id": "2106.00217", "submitter": "Chinmay Garg", "authors": "Chinmay Garg, Aravind Machiry, Andrea Continella, Christopher Kruegel,\n  Giovanni Vigna", "title": "Toward a Secure Crowdsourced Location Tracking System", "comments": "10 pages - ACM WiSec 2021 - Preprint", "journal-ref": null, "doi": "10.1145/3448300.3467821", "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-energy Bluetooth devices have become ubiquitous and widely used for\ndifferent applications. Among these, Bluetooth trackers are becoming popular as\nthey allow users to track the location of their physical objects. To do so,\nBluetooth trackers are often built-in within other commercial products\nconnected to a larger crowdsourced tracking system. Such a system, however, can\npose a threat to the security and privacy of the users, for instance, by\nrevealing the location of a user's valuable object. In this paper, we introduce\na set of security properties and investigate the state of commercial\ncrowdsourced tracking systems, which present common design flaws that make them\ninsecure. Leveraging the results of our investigation, we propose a new design\nfor a secure crowdsourced tracking system (SECrow), which allows devices to\nleverage the benefits of the crowdsourced model without sacrificing security\nand privacy. Our preliminary evaluation shows that SECrow is a practical,\nsecure, and effective crowdsourced tracking solution\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 04:10:46 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Garg", "Chinmay", ""], ["Machiry", "Aravind", ""], ["Continella", "Andrea", ""], ["Kruegel", "Christopher", ""], ["Vigna", "Giovanni", ""]]}, {"id": "2106.00275", "submitter": "He Yang", "authors": "He Yang", "title": "H-FL: A Hierarchical Communication-Efficient and Privacy-Protected\n  Architecture for Federated Learning", "comments": "Accepted by IJCAI 2021, 7pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longstanding goals of federated learning (FL) require rigorous privacy\nguarantees and low communication overhead while holding a relatively high model\naccuracy. However, simultaneously achieving all the goals is extremely\nchallenging. In this paper, we propose a novel framework called hierarchical\nfederated learning (H-FL) to tackle this challenge. Considering the degradation\nof the model performance due to the statistic heterogeneity of the training\ndata, we devise a runtime distribution reconstruction strategy, which\nreallocates the clients appropriately and utilizes mediators to rearrange the\nlocal training of the clients. In addition, we design a compression-correction\nmechanism incorporated into H-FL to reduce the communication overhead while not\nsacrificing the model performance. To further provide privacy guarantees, we\nintroduce differential privacy while performing local training, which injects\nmoderate amount of noise into only part of the complete model. Experimental\nresults show that our H-FL framework achieves the state-of-art performance on\ndifferent datasets for the real-world image recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 07:15:31 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yang", "He", ""]]}, {"id": "2106.00388", "submitter": "Felix Mannhardt", "authors": "Gamal Elkoumy, Stephan A. Fahrenkrog-Petersen, Mohammadreza Fani Sani,\n  Agnes Koschmider, Felix Mannhardt, Saskia Nu\\~nez von Voigt, Majid Rafiei,\n  Leopold von Waldthausen", "title": "Privacy and Confidentiality in Process Mining -- Threats and Research\n  Challenges", "comments": "Accepted for publication in ACM Transactions on Management\n  Information Systems", "journal-ref": null, "doi": "10.1145/3468877", "report-no": null, "categories": "cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy and confidentiality are very important prerequisites for applying\nprocess mining in order to comply with regulations and keep company secrets.\nThis paper provides a foundation for future research on privacy-preserving and\nconfidential process mining techniques. Main threats are identified and related\nto an motivation application scenario in a hospital context as well as to the\ncurrent body of work on privacy and confidentiality in process mining. A newly\ndeveloped conceptual model structures the discussion that existing techniques\nleave room for improvement. This results in a number of important research\nchallenges that should be addressed by future process mining research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:51:11 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Elkoumy", "Gamal", ""], ["Fahrenkrog-Petersen", "Stephan A.", ""], ["Sani", "Mohammadreza Fani", ""], ["Koschmider", "Agnes", ""], ["Mannhardt", "Felix", ""], ["von Voigt", "Saskia Nu\u00f1ez", ""], ["Rafiei", "Majid", ""], ["von Waldthausen", "Leopold", ""]]}, {"id": "2106.00456", "submitter": "Thanh Vinh Vo", "authors": "Thanh Vinh Vo, Trong Nghia Hoang, Young Lee, Tze-Yun Leong", "title": "Federated Estimation of Causal Effects from Observational Data", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications collect data that comes in federated spirit, with\ndata kept locally and undisclosed. Till date, most insight into the causal\ninference requires data to be stored in a central repository. We present a\nnovel framework for causal inference with federated data sources. We assess and\nintegrate local causal effects from different private data sources without\ncentralizing them. Then, the treatment effects on subjects from observational\ndata using a non-parametric reformulation of the classical potential outcomes\nframework is estimated. We model the potential outcomes as a random function\ndistributed by Gaussian processes, whose defining parameters can be efficiently\nlearned from multiple data sources, respecting privacy constraints. We\ndemonstrate the promise and efficiency of the proposed approach through a set\nof simulated and real-world benchmark examples.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:06:00 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Vo", "Thanh Vinh", ""], ["Hoang", "Trong Nghia", ""], ["Lee", "Young", ""], ["Leong", "Tze-Yun", ""]]}, {"id": "2106.00463", "submitter": "Ziyue Huang", "authors": "Ziyue Huang, Yuting Liang, Ke Yi", "title": "Instance-optimal Mean Estimation Under Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean estimation under differential privacy is a fundamental problem, but\nworst-case optimal mechanisms do not offer meaningful utility guarantees in\npractice when the global sensitivity is very large. Instead, various heuristics\nhave been proposed to reduce the error on real-world data that do not resemble\nthe worst-case instance. This paper takes a principled approach, yielding a\nmechanism that is instance-optimal in a strong sense. In addition to its\ntheoretical optimality, the mechanism is also simple and practical, and adapts\nto a variety of data characteristics without the need of parameter tuning. It\neasily extends to the local and shuffle model as well.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:15:50 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Huang", "Ziyue", ""], ["Liang", "Yuting", ""], ["Yi", "Ke", ""]]}, {"id": "2106.00474", "submitter": "Antti Honkela", "authors": "Antti Honkela", "title": "Gaussian Processes with Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are non-parametric Bayesian models that are widely\nused for diverse prediction tasks. Previous work in adding strong privacy\nprotection to GPs via differential privacy (DP) has been limited to protecting\nonly the privacy of the prediction targets (model outputs) but not inputs. We\nbreak this limitation by introducing GPs with DP protection for both model\ninputs and outputs. We achieve this by using sparse GP methodology and\npublishing a private variational approximation on known inducing points. The\napproximation covariance is adjusted to approximately account for the added\nuncertainty from DP noise. The approximation can be used to compute arbitrary\npredictions using standard sparse GP techniques. We propose a method for\nhyperparameter learning using a private selection protocol applied to\nvalidation set log-likelihood. Our experiments demonstrate that given\nsufficient amount of data, the method can produce accurate models under strong\nprivacy protection.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:23:16 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Honkela", "Antti", ""]]}, {"id": "2106.00477", "submitter": "Antti Koskela", "authors": "Antti Koskela, Mikko A. Heikkil\\\"a, Antti Honkela", "title": "Tight Accounting in the Shuffle Model of Differential Privacy", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shuffle model of differential privacy is a novel distributed privacy model\nbased on a combination of local privacy mechanisms and a trusted shuffler. It\nhas been shown that the additional randomisation provided by the shuffler\nimproves privacy bounds compared to the purely local mechanisms. Accounting\ntight bounds, especially for multi-message protocols, is complicated by the\ncomplexity brought by the shuffler. The recently proposed Fourier Accountant\nfor evaluating $(\\varepsilon,\\delta)$-differential privacy guarantees has been\nshown to give tighter bounds than commonly used methods for non-adaptive\ncompositions of various complex mechanisms. In this paper we show how to\ncompute tight privacy bounds using the Fourier Accountant for multi-message\nversions of several ubiquitous mechanisms in the shuffle model and demonstrate\nlooseness of the existing bounds in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:30:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Koskela", "Antti", ""], ["Heikkil\u00e4", "Mikko A.", ""], ["Honkela", "Antti", ""]]}, {"id": "2106.00508", "submitter": "Alireza Farhadi", "authors": "Alireza Farhadi, MohammadTaghi Hajiaghayi and Elaine Shi", "title": "Differentially Private Densest Subgraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a graph, the densest subgraph problem asks for a set of vertices such\nthat the average degree among these vertices is maximized. Densest subgraph has\nnumerous applications in learning, e.g., community detection in social\nnetworks, link spam detection, correlation mining, bioinformatics, and so on.\nAlthough there are efficient algorithms that output either exact or approximate\nsolutions to the densest subgraph problem, existing algorithms may violate the\nprivacy of the individuals in the network, e.g., leaking the\nexistence/non-existence of edges.\n  In this paper, we study the densest subgraph problem in the framework of the\ndifferential privacy, and we derive the first upper and lower bounds for this\nproblem. We show that there exists a linear-time $\\epsilon$-differentially\nprivate algorithm that finds a $2$-approximation of the densest subgraph with\nan extra poly-logarithmic additive error. Our algorithm not only reports the\napproximate density of the densest subgraph, but also reports the vertices that\nform the dense subgraph.\n  Our upper bound almost matches the famous $2$-approximation by Charikar both\nin performance and in approximation ratio, but we additionally achieve\ndifferential privacy. In comparison with Charikar's algorithm, our algorithm\nhas an extra poly-logarithmic additive error. We partly justify the additive\nerror with a new lower bound, showing that for any differentially private\nalgorithm that provides a constant-factor approximation, a sub-logarithmic\nadditive error is inherent.\n  We also practically study our differentially private algorithm on real-world\ngraphs, and we show that in practice the algorithm finds a solution which is\nvery close to the optimal\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 14:11:18 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Farhadi", "Alireza", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Shi", "Elaine", ""]]}, {"id": "2106.00541", "submitter": "Fabio De Gaspari", "authors": "Michal Piskozub, Fabio De Gaspari, Frederick Barr-Smith, Luigi V.\n  Mancini, Ivan Martinovic", "title": "MalPhase: Fine-Grained Malware Detection Using Network Flow Data", "comments": "Paper accepted for publication at ACM AsiaCCS 2021", "journal-ref": null, "doi": "10.1145/3433210.3453101", "report-no": null, "categories": "cs.CR cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Economic incentives encourage malware authors to constantly develop new,\nincreasingly complex malware to steal sensitive data or blackmail individuals\nand companies into paying large ransoms. In 2017, the worldwide economic impact\nof cyberattacks is estimated to be between 445 and 600 billion USD, or 0.8% of\nglobal GDP. Traditionally, one of the approaches used to defend against malware\nis network traffic analysis, which relies on network data to detect the\npresence of potentially malicious software. However, to keep up with increasing\nnetwork speeds and amount of traffic, network analysis is generally limited to\nwork on aggregated network data, which is traditionally challenging and yields\nmixed results. In this paper we present MalPhase, a system that was designed to\ncope with the limitations of aggregated flows. MalPhase features a multi-phase\npipeline for malware detection, type and family classification. The use of an\nextended set of network flow features and a simultaneous multi-tier\narchitecture facilitates a performance improvement for deep learning models,\nmaking them able to detect malicious flows (>98% F1) and categorize them to a\nrespective malware type (>93% F1) and family (>91% F1). Furthermore, the use of\nrobust features and denoising autoencoders allows MalPhase to perform well on\nsamples with varying amounts of benign traffic mixed in. Finally, MalPhase\ndetects unseen malware samples with performance comparable to that of known\nsamples, even when interlaced with benign flows to reflect realistic network\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 14:53:38 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Piskozub", "Michal", ""], ["De Gaspari", "Fabio", ""], ["Barr-Smith", "Frederick", ""], ["Mancini", "Luigi V.", ""], ["Martinovic", "Ivan", ""]]}, {"id": "2106.00660", "submitter": "Ilia Shumailov", "authors": "David Khachaturov, Ilia Shumailov, Yiren Zhao, Nicolas Papernot, Ross\n  Anderson", "title": "Markpainting: Adversarial Machine Learning meets Inpainting", "comments": "Proceedings of the 38th International Conference on Machine Learning\n  (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inpainting is a learned interpolation technique that is based on generative\nmodeling and used to populate masked or missing pieces in an image; it has wide\napplications in picture editing and retouching. Recently, inpainting started\nbeing used for watermark removal, raising concerns. In this paper we study how\nto manipulate it using our markpainting technique. First, we show how an image\nowner with access to an inpainting model can augment their image in such a way\nthat any attempt to edit it using that model will add arbitrary visible\ninformation. We find that we can target multiple different models\nsimultaneously with our technique. This can be designed to reconstitute a\nwatermark if the editor had been trying to remove it. Second, we show that our\nmarkpainting technique is transferable to models that have different\narchitectures or were trained on different datasets, so watermarks created\nusing it are difficult for adversaries to remove. Markpainting is novel and can\nbe used as a manipulation alarm that becomes visible in the event of\ninpainting.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:45:52 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Khachaturov", "David", ""], ["Shumailov", "Ilia", ""], ["Zhao", "Yiren", ""], ["Papernot", "Nicolas", ""], ["Anderson", "Ross", ""]]}, {"id": "2106.00667", "submitter": "Shayan Eskandari", "authors": "Shayan Eskandari, Mehdi Salehi, Wanyun Catherine Gu, Jeremy Clark", "title": "SoK: Oracles from the Ground Truth to Market Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One fundamental limitation of blockchain-based smart contracts is that they\nexecute in a closed environment and only have access to the data and\nfunctionality that is either already on the blockchain or fed into the\nblockchain. Thus any interactions with the real world need to be mediated by a\nbridge service, which is called an oracle. As decentralized applications\nmature, oracles are playing an increasingly prominent role. With their\nevolution comes more attacks, necessitating a greater attention to the trust\nmodel of using oracles. In this SoK, we systemize the design alternatives for\noracles, showcase attacks, and discuss attack mitigation strategies.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:55:36 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Eskandari", "Shayan", ""], ["Salehi", "Mehdi", ""], ["Gu", "Wanyun Catherine", ""], ["Clark", "Jeremy", ""]]}, {"id": "2106.00834", "submitter": "Nelly Elsayed", "authors": "Zaghloul Saad Zaghloul, Nelly Elsayed, Chengcheng Li, Magdy Bayoumi", "title": "Green IoT System Architecture for Applied Autonomous Network\n  Cybersecurity Monitoring", "comments": "Cybersecurity, IoT, NSM, packet capture, sensor, green systems, oil\n  and gas, Network Security Monitoring. Accepted in IEEE WF-IoT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network security morning (NSM) is essential for any cybersecurity system,\nwhere the average cost of a cyberattack is $1.1 million. No matter how much a\nsystem is secure, it will eventually fail without proper and continuous\nmonitoring. No wonder that the cybersecurity market is expected to grow up to\n$170.4 billion in 2022. However, the majority of legacy industries do not\ninvest in NSM implementation until it is too late due to the initial and\noperation cost and static unutilized resources. Thus, this paper proposes a\nnovel dynamic Internet of things (IoT) architecture for an industrial NSM that\nfeatures a low installation and operation cost, low power consumption,\nintelligent organization behavior, and environmentally friendly operation. As a\ncase study, the system is implemented in a midrange oil a gas manufacture\nfacility in the southern states with more than 300 machines and servers over\nthree remote locations and a production plant that features a challenging\natmosphere condition. The proposed system successfully shows a significant\nsaving (>65%) in power consumption, acquires one-tenth the installation cost,\ndevelops an intelligent operation expert system tools as well as saves the\nenvironment from more than 500 mg of CO2 pollution per hour, promoting green\nIoT systems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 22:27:41 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zaghloul", "Zaghloul Saad", ""], ["Elsayed", "Nelly", ""], ["Li", "Chengcheng", ""], ["Bayoumi", "Magdy", ""]]}, {"id": "2106.00859", "submitter": "Jie Yang", "authors": "Linghan Zhang, Jie Yang", "title": "A Continuous Liveness Detection for Voice Authentication on Smart\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice biometrics is drawing increasing attention as it is a promising\nalternative to legacy passwords for user authentication. Recently, a growing\nbody of work shows that voice biometrics is vulnerable to spoofing through\nreplay attacks, where an adversary tries to spoof voice authentication systems\nby using a pre-recorded voice sample collected from a genuine user. To this\nend, we propose VoiceGesture, a liveness detection solution for voice\nauthentication on smart devices such as smartphones and smart speakers.\nVoiceGesture detects a live user by leveraging both the unique articulatory\ngesture of the user when speaking a passphrase and the audio hardware advances\non these smart devices. Specifically, our system re-uses a pair of built-in\nspeaker and microphone on a smart device as a Doppler radar, which transmits a\nhigh-frequency acoustic sound from the speaker and listens to the reflections\nat the microphone when a user speaks a passphrase. Then we extract Doppler\nshifts resulted from the user's articulatory gestures for liveness detection.\nVoiceGesture is practical as it requires neither cumbersome operations nor\nadditional hardware but a speaker and a microphone commonly available on smart\ndevices that support voice input. Our experimental evaluation with 21\nparticipants and different smart devices shows that VoiceGesture achieves over\n99% and around 98% detection accuracy for text-dependent and text-independent\nliveness detection, respectively. Results also show that VoiceGesture is robust\nto different device placements, low audio sampling frequency, and supports\nmedium range liveness detection on smart speakers in various use scenarios like\nsmart homes and smart vehicles.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 23:53:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zhang", "Linghan", ""], ["Yang", "Jie", ""]]}, {"id": "2106.00873", "submitter": "Zhenyu Zhong", "authors": "Zhisheng Hu, Shengjian Guo, Zhenyu Zhong, Kang Li", "title": "Coverage-based Scene Fuzzing for Virtual Autonomous Driving Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based virtual testing has become an essential step to ensure the\nsafety of autonomous driving systems. Testers need to handcraft the virtual\ndriving scenes and configure various environmental settings like surrounding\ntraffic, weather conditions, etc. Due to the huge amount of configuration\npossibilities, the human efforts are subject to the inefficiency in detecting\nflaws in industry-class autonomous driving system. This paper proposes a\ncoverage-driven fuzzing technique to automatically generate diverse\nconfiguration parameters to form new driving scenes. Experimental results show\nthat our fuzzing method can significantly reduce the cost in deriving new risky\nscenes from the initial setup designed by testers. We expect automated fuzzing\nwill become a common practice in virtual testing for autonomous driving\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 00:49:59 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hu", "Zhisheng", ""], ["Guo", "Shengjian", ""], ["Zhong", "Zhenyu", ""], ["Li", "Kang", ""]]}, {"id": "2106.01053", "submitter": "Anastasiya Gorodilova", "authors": "A. Gorodilova, N. Tokareva, S. Agievich, C. Carlet, V. Idrisova, K.\n  Kalgin, D. Kolegov, A. Kutsenko, N. Mouha, M. Pudovkina, A. Udovenko", "title": "The Seventh International Olympiad in Cryptography: problems and\n  solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The International Olympiad in Cryptography NSUCRYPTO is the unique Olympiad\ncontaining scientific mathematical problems for professionals, school and\nuniversity students from any country. Its aim is to involve young researchers\nin solving curious and tough scientific problems of modern cryptography. In\n2020, it was held for the seventh time. Prizes and diplomas were awarded to 84\nparticipants in the first round and 49 teams in the second round from 32\ncountries. In this paper, problems and their solutions of NSUCRYPTO'2020 are\npresented. We consider problems related to attacks on ciphers and hash\nfunctions, protocols, permutations, primality tests, etc. We discuss several\nopen problems on JPEG encoding, Miller -- Rabin primality test, special bases\nin the vector space, AES-GCM. The problem of a modified Miller -- Rabin\nprimality test was solved during the Olympiad. The problem for finding special\nbases was partially solved.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:00:45 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 06:57:14 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Gorodilova", "A.", ""], ["Tokareva", "N.", ""], ["Agievich", "S.", ""], ["Carlet", "C.", ""], ["Idrisova", "V.", ""], ["Kalgin", "K.", ""], ["Kolegov", "D.", ""], ["Kutsenko", "A.", ""], ["Mouha", "N.", ""], ["Pudovkina", "M.", ""], ["Udovenko", "A.", ""]]}, {"id": "2106.01116", "submitter": "Vladimir Shpilrain", "authors": "Nael Rahman and Vladimir Shpilrain", "title": "MOBS (Matrices Over Bit Strings) public key exchange", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use matrices over bit strings as platforms for Diffie-Hellman-like public\nkey exchange protocols. When multiplying matrices like that, we use Boolean OR\noperation on bit strings in place of addition and Boolean AND operation in\nplace of multiplication. As a result, (1) computations with these matrices are\nvery efficient; (2) standard methods of attacking Diffie-Hellman-like protocols\nare not applicable.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:27:33 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Rahman", "Nael", ""], ["Shpilrain", "Vladimir", ""]]}, {"id": "2106.01131", "submitter": "Lee Livsey", "authors": "Lee Livsey, Helen Petrie, Siamak F. Shahandashti, Aidan Fray", "title": "Performance and Usability of Visual and Verbal Verification of\n  Word-based Key Fingerprints", "comments": "This is an accepted manuscript to appear in the proceedings of the\n  15th International Symposium on Human Aspects of Information Security &\n  Assurance, HAISA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The security of messaging applications against person-in-the-middle attacks\nrelies on the authenticity of the exchanged keys. For users unable to meet in\nperson, a manual key fingerprint verification is necessary to ascertain key\nauthenticity. Such fingerprints can be exchanged visually or verbally, and it\nis not clear in which condition users perform best. This paper reports the\nresults of a 62-participant study that investigated differences in performance\nand perceived usability of visual and verbal comparisons of word-based key\nfingerprints, and the influence of the individual's cognitive learning style.\nThe results show visual comparisons to be more effective against non-security\ncritical errors and are perceived to provide increased confidence, yet\nparticipants perceive verbal comparisons to be easier and require less mental\neffort. Besides, limited evidence was found on the influence of the\nindividual's learning style on their performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:57:31 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Livsey", "Lee", ""], ["Petrie", "Helen", ""], ["Shahandashti", "Siamak F.", ""], ["Fray", "Aidan", ""]]}, {"id": "2106.01154", "submitter": "Stjepan Gro\\v{s}", "authors": "Stjepan Gro\\v{s}, Ivan Kova\\v{c}evi\\'c, Ivan Dujmi\\'c, Matej\n  Petrinovi\\'c", "title": "Controlled Update of Software Components using Concurrent Exection of\n  Patched and Unpatched Versions", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Software patching is a common method of removing vulnerabilities in software\ncomponents to make IT systems more secure. However, there are many cases where\nsoftware patching is not possible due to the critical nature of the\napplication, especially when the vendor providing the application guarantees\ncorrect operation only in a specific configuration. In this paper, we propose a\nmethod to solve this problem. The idea is to run unpatched and patched\napplication instances concurrently, with the unpatched one having complete\ncontrol and the output of the patched one being used only for comparison, to\nwatch for differences that are consequences of introduced bugs. To test this\nidea, we developed a system that allows us to run web applications in parallel\nand tested three web applications. The experiments have shown that the idea is\npromising for web applications from the technical side. Furthermore, we discuss\nthe potential limitations of this system and the idea in general, how long two\ninstances should run in order to be able to claim with some probability that\nthe patched version has not introduced any new bugs, other potential use cases\nof the proposed system where two application instances run concurrently, and\nfinally the potential uses of this system with different types of applications,\nsuch as SCADA systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 13:44:59 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Gro\u0161", "Stjepan", ""], ["Kova\u010devi\u0107", "Ivan", ""], ["Dujmi\u0107", "Ivan", ""], ["Petrinovi\u0107", "Matej", ""]]}, {"id": "2106.01157", "submitter": "Zuoguang Wang", "authors": "Zuoguang Wang, Hongsong Zhu, Peipei Liu and Limin Sun", "title": "Social Engineering in Cybersecurity: A Domain Ontology and Knowledge\n  Graph Application Examples", "comments": "Z. Wang, H. Zhu, P. Liu and L. Sun. \"Social Engineering in\n  Cybersecurity: A Domain Ontology and Knowledge Graph Application Examples.\"\n  Cybersecurity 4(1), 2021. doi: 10.1186/s42400-021-00094-6", "journal-ref": null, "doi": "10.1186/s42400-021-00094-6", "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social engineering has posed a serious threat to cyberspace security. To\nprotect against social engineering attacks, a fundamental work is to know what\nconstitutes social engineering. This paper first develops a domain ontology of\nsocial engineering in cybersecurity and conducts ontology evaluation by its\nknowledge graph application. The domain ontology defines 11 concepts of core\nentities that significantly constitute or affect social engineering domain,\ntogether with 22 kinds of relations describing how these entities related to\neach other. It provides a formal and explicit knowledge schema to understand,\nanalyze, reuse and share domain knowledge of social engineering. Furthermore,\nthis paper builds a knowledge graph based on 15 social engineering attack\nincidents and scenarios. 7 knowledge graph application examples (in 6 analysis\npatterns) demonstrate that the ontology together with knowledge graph is useful\nto 1) understand and analyze social engineering attack scenario and incident,\n2) find the top ranked social engineering threat elements (e.g. the most\nexploited human vulnerabilities and most used attack mediums), 3) find\npotential social engineering threats to victims, 4) find potential targets for\nsocial engineering attackers, 5) find potential attack paths from specific\nattacker to specific target, and 6) analyze the same origin attacks.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:58:24 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Zuoguang", ""], ["Zhu", "Hongsong", ""], ["Liu", "Peipei", ""], ["Sun", "Limin", ""]]}, {"id": "2106.01161", "submitter": "Manuel Chakravarty", "authors": "Manuel M. T. Chakravarty and Nikos Karayannidis and Aggelos Kiayias\n  and Michael Peyton Jones and Polina Vinogradova", "title": "Babel Fees via Limited Liabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Custom currencies (ERC-20) on Ethereum are wildly popular, but they are\nsecond class to the primary currency Ether. Custom currencies are more complex\nand more expensive to handle than the primary currency as their accounting is\nnot natively performed by the underlying ledger, but instead in user-defined\ncontract code. Furthermore, and quite importantly, transaction fees can only be\npaid in Ether.\n  In this paper, we focus on being able to pay transaction fees in custom\ncurrencies. We achieve this by way of a mechanism permitting short term\nliabilities to pay transaction fees in conjunction with offers of custom\ncurrencies to compensate for those liabilities. This enables block producers to\naccept custom currencies in exchange for settling liabilities of transactions\nthat they process.\n  We present formal ledger rules to handle liabilities together with the\nconcept of babel fees to pay transaction fees in custom currencies. We also\ndiscuss how clients can determine what fees they have to pay, and we present a\nsolution to the knapsack problem variant that block producers have to solve in\nthe presence of babel fees to optimise their profits.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 13:55:05 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Chakravarty", "Manuel M. T.", ""], ["Karayannidis", "Nikos", ""], ["Kiayias", "Aggelos", ""], ["Jones", "Michael Peyton", ""], ["Vinogradova", "Polina", ""]]}, {"id": "2106.01221", "submitter": "Xiang Yue", "authors": "Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li, Huan Sun and Sherman\n  S. M. Chow", "title": "Differential Privacy for Text Analytics via Natural Text Sanitization", "comments": "ACL-ICJNLP'21 Findings; The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texts convey sophisticated knowledge. However, texts also convey sensitive\ninformation. Despite the success of general-purpose language models and\ndomain-specific mechanisms with differential privacy (DP), existing text\nsanitization mechanisms still provide low utility, as cursed by the\nhigh-dimensional text representation. The companion issue of utilizing\nsanitized texts for downstream analytics is also under-explored. This paper\ntakes a direct approach to text sanitization. Our insight is to consider both\nsensitivity and similarity via our new local DP notion. The sanitized texts\nalso contribute to our sanitization-aware pretraining and fine-tuning, enabling\nprivacy-preserving natural language processing over the BERT language model\nwith promising utility. Surprisingly, the high utility does not boost up the\nsuccess rate of inference attacks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:15:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yue", "Xiang", ""], ["Du", "Minxin", ""], ["Wang", "Tianhao", ""], ["Li", "Yaliang", ""], ["Sun", "Huan", ""], ["Chow", "Sherman S. M.", ""]]}, {"id": "2106.01240", "submitter": "Uri Kirstein Mr.", "authors": "Uri Kirstein, Shelly Grossman, Michael Mirkin, James Wilcox, Ittay\n  Eyal, Mooly Sagiv", "title": "Phoenix: A Formally Verified Regenerating Vault", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An attacker that gains access to a cryptocurrency user's private keys can\nperform any operation in her stead. Due to the decentralized nature of most\ncryptocurrencies, no entity can revert those operations. This is a central\nchallenge for decentralized systems, illustrated by numerous high-profile\nheists. Vault contracts reduce this risk by introducing artificial delay on\noperations, allowing abortion by the contract owner during the delay. However,\nthe theft of a key still renders the vault unusable and puts funds at risk.\n  We introduce Phoenix, a novel contract architecture that allows the user to\nrestore its security properties after key loss. Phoenix takes advantage of\nusers' ability to store keys in easily-available but less secure storage\n(tier-two) as well as more secure storage that is harder to access (tier-one).\nUnlike previous solutions, the user can restore Phoenix security after the\ntheft of tier-two keys and does not lose funds despite losing keys in either\ntier. Phoenix also introduces a mechanism to reduce the damage an attacker can\ncause in case of a tier-one compromise.\n  We formally specify Phoenix's required behavior and provide a prototype\nimplementation of Phoenix as an Ethereum contract. Since such an implementation\nis highly sensitive and vulnerable to subtle bugs, we apply a formal\nverification tool to prove specific code properties and identify faults. We\nhighlight a bug identified by the tool that could be exploited by an attacker\nto compromise Phoenix. After fixing the bug, the tool proved the low-level\nexecutable code's correctness.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:43:12 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kirstein", "Uri", ""], ["Grossman", "Shelly", ""], ["Mirkin", "Michael", ""], ["Wilcox", "James", ""], ["Eyal", "Ittay", ""], ["Sagiv", "Mooly", ""]]}, {"id": "2106.01242", "submitter": "Ferdinando Fioretto", "authors": "Anudit Nagar, Cuong Tran, Ferdinando Fioretto", "title": "A Privacy-Preserving and Trustable Multi-agent Learning Framework", "comments": "This paper is an extended version of Reference [32]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed multi-agent learning enables agents to cooperatively train a\nmodel without requiring to share their datasets. While this setting ensures\nsome level of privacy, it has been shown that, even when data is not directly\nshared, the training process is vulnerable to privacy attacks including data\nreconstruction and model inversion attacks. Additionally, malicious agents that\ntrain on inverted labels or random data, may arbitrarily weaken the accuracy of\nthe global model. This paper addresses these challenges and presents\nPrivacy-preserving and trustable Distributed Learning (PT-DL), a fully\ndecentralized framework that relies on Differential Privacy to guarantee strong\nprivacy protections of the agents' data, and Ethereum smart contracts to ensure\ntrustability. The paper shows that PT-DL is resilient up to a 50% collusion\nattack, with high probability, in a malicious trust model and the experimental\nevaluation illustrates the benefits of the proposed model as a\nprivacy-preserving and trustable distributed multi-agent learning system on\nseveral classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:46:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Nagar", "Anudit", ""], ["Tran", "Cuong", ""], ["Fioretto", "Ferdinando", ""]]}, {"id": "2106.01336", "submitter": "Huanyu Zhang", "authors": "Gautam Kamath, Xingtu Liu, Huanyu Zhang", "title": "Improved Rates for Differentially Private Stochastic Convex Optimization\n  with Heavy-Tailed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study stochastic convex optimization with heavy-tailed data under the\nconstraint of differential privacy. Most prior work on this problem is\nrestricted to the case where the loss function is Lipschitz. Instead, as\nintroduced by Wang, Xiao, Devadas, and Xu, we study general convex loss\nfunctions with the assumption that the distribution of gradients has bounded\n$k$-th moments. We provide improved upper bounds on the excess population risk\nunder approximate differential privacy of\n$\\tilde{O}\\left(\\sqrt{\\frac{d}{n}}+\\left(\\frac{d}{\\epsilon\nn}\\right)^{\\frac{k-1}{k}}\\right)$ and\n$\\tilde{O}\\left(\\frac{d}{n}+\\left(\\frac{d}{\\epsilon\nn}\\right)^{\\frac{2k-2}{k}}\\right)$ for convex and strongly convex loss\nfunctions, respectively. We also prove nearly-matching lower bounds under the\nconstraint of pure differential privacy, giving strong evidence that our bounds\nare tight.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:45:47 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 04:40:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kamath", "Gautam", ""], ["Liu", "Xingtu", ""], ["Zhang", "Huanyu", ""]]}, {"id": "2106.01340", "submitter": "Tim Roughgarden", "authors": "Tim Roughgarden", "title": "Transaction Fee Mechanism Design", "comments": "Appears in the 22nd ACM Conference on Economics and Computation (EC\n  '21). This conference paper is derived from Sections 2, 4, 5, 6, and 8 of the\n  longer general-audience report published as arXiv:2012.00854", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DS cs.GT econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand for blockchains such as Bitcoin and Ethereum is far larger than\nsupply, necessitating a mechanism that selects a subset of transactions to\ninclude \"on-chain\" from the pool of all pending transactions. EIP-1559 is a\nproposal to make several tightly coupled changes to the Ethereum blockchain's\ntransaction fee mechanism, including the introduction of variable-size blocks\nand a burned base fee that rises and falls with demand. These changes are\nslated for deployment in Ethereum's \"London fork,\" scheduled for late\nsummer~2021, at which point it will be the biggest economic change made to a\nmajor blockchain to date.\n  The first goal of this paper is to formalize the problem of designing a\ntransaction fee mechanism, taking into account the many idiosyncrasies of the\nblockchain setting (ranging from off-chain collusion between miners and users\nto the ease of money-burning). The second goal is to situate the specific\nmechanism proposed in EIP-1559 in this framework and rigorously interrogate its\ngame-theoretic properties. The third goal is to suggest competing designs that\noffer alternative sets of trade-offs. The final goal is to highlight research\nopportunities for the EC community that could help shape the future of\nblockchain transaction fee mechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:48:32 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Roughgarden", "Tim", ""]]}, {"id": "2106.01367", "submitter": "David Coimbra", "authors": "David Coimbra, Sofia Reis, Rui Abreu, Corina P\\u{a}s\\u{a}reanu, Hakan\n  Erdogmus", "title": "On using distributed representations of source code for the detection of\n  C security vulnerabilities", "comments": "Submitted to DX 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an evaluation of the code representation model Code2vec\nwhen trained on the task of detecting security vulnerabilities in C source\ncode. We leverage the open-source library astminer to extract path-contexts\nfrom the abstract syntax trees of a corpus of labeled C functions. Code2vec is\ntrained on the resulting path-contexts with the task of classifying a function\nas vulnerable or non-vulnerable. Using the CodeXGLUE benchmark, we show that\nthe accuracy of Code2vec for this task is comparable to simple\ntransformer-based methods such as pre-trained RoBERTa, and outperforms more\nnaive NLP-based methods. We achieved an accuracy of 61.43% while maintaining\nlow computational requirements relative to larger models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:18:23 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Coimbra", "David", ""], ["Reis", "Sofia", ""], ["Abreu", "Rui", ""], ["P\u0103s\u0103reanu", "Corina", ""], ["Erdogmus", "Hakan", ""]]}, {"id": "2106.01538", "submitter": "Alexander Matyasko", "authors": "Alexander Matyasko, Lap-Pui Chau", "title": "PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural networks are sensitive to small input\nperturbations. Since the discovery of this intriguing vulnerability, many\ndefence methods have been proposed that attempt to improve robustness to\nadversarial noise. Fast and accurate attacks are required to compare various\ndefence methods. However, evaluating adversarial robustness has proven to be\nextremely challenging. Existing norm minimisation adversarial attacks require\nthousands of iterations (e.g. Carlini & Wagner attack), are limited to the\nspecific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results\n(e.g. Brendel & Bethge attack). On the other hand, PGD attack, which is fast,\ngeneral and accurate, ignores the norm minimisation penalty and solves a\nsimpler perturbation-constrained problem. In this work, we introduce a fast,\ngeneral and accurate adversarial attack that optimises the original non-convex\nconstrained minimisation problem. We interpret optimising the Lagrangian of the\nadversarial attack optimisation problem as a two-player game: the first player\nminimises the Lagrangian wrt the adversarial noise; the second player maximises\nthe Lagrangian wrt the regularisation penalty. Our attack algorithm\nsimultaneously optimises primal and dual variables to find the minimal\nadversarial perturbation. In addition, for non-smooth $l_p$-norm minimisation,\nsuch as $l_{\\infty}$-, $l_1$-, and $l_0$-norms, we introduce primal-dual\nproximal gradient descent attack. We show in the experiments that our attack\noutperforms current state-of-the-art $l_{\\infty}$-, $l_2$-, $l_1$-, and\n$l_0$-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against\nunregularised and adversarially trained models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:45:48 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Matyasko", "Alexander", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2106.01632", "submitter": "Farhan Sadique", "authors": "Farhan Sadique, Ignacio Astaburuaga, Raghav Kaul, Shamik Sengupta,\n  Shahriar Badsha, James Schnebly, Adam Cassell, Jeff Springer, Nancy\n  Latourrette and Sergiu M. Dascalu", "title": "Cybersecurity Information Exchange with Privacy (CYBEX-P) and TAHOE -- A\n  Cyberthreat Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cybersecurity information sharing (CIS) is envisioned to protect\norganizations more effectively from advanced cyber attacks. However, a\ncompletely automated CIS platform is not widely adopted. The major challenges\nare: (1) the absence of a robust cyber threat language (CTL) and (2) the\nconcerns over data privacy. This work introduces Cybersecurity Information\nExchangewith Privacy (CYBEX-P), as a CIS framework, to tackle these challenges.\nCYBEX-P allows organizations to share heterogeneous data with granular,\nattribute based privacy control. It correlates the data to automatically\ngenerate intuitive reports and defensive rules. To achieve such versatility, we\nhave developed TAHOE - a graph based CTL. TAHOE is a structure for\nstoring,sharing and analyzing threat data. It also intrinsically correlates the\ndata. We have further developed a universal Threat Data Query Language (TDQL).\nIn this paper, we propose the system architecture for CYBEX-P. We then discuss\nits scalability and privacy features along with a use case of CYBEX-P providing\nInfrastructure as a Service (IaaS). We further introduce TAHOE& TDQL as better\nalternatives to existing CTLs and formulate ThreatRank - an algorithm to detect\nnew malicious even\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 07:10:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Sadique", "Farhan", ""], ["Astaburuaga", "Ignacio", ""], ["Kaul", "Raghav", ""], ["Sengupta", "Shamik", ""], ["Badsha", "Shahriar", ""], ["Schnebly", "James", ""], ["Cassell", "Adam", ""], ["Springer", "Jeff", ""], ["Latourrette", "Nancy", ""], ["Dascalu", "Sergiu M.", ""]]}, {"id": "2106.01806", "submitter": "Dorcas Ofori-Boateng", "authors": "Dorcas Ofori-Boateng, Ignacio Segovia Dominguez, Murat Kantarcioglu,\n  Cuneyt G. Akcora, Yulia R. Gel", "title": "Topological Anomaly Detection in Dynamic Multilayer Blockchain Networks", "comments": "26 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.AT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the recent surge of criminal activities with\ncross-cryptocurrency trades, we introduce a new topological perspective to\nstructural anomaly detection in dynamic multilayer networks. We postulate that\nanomalies in the underlying blockchain transaction graph that are composed of\nmultiple layers are likely to also be manifested in anomalous patterns of the\nnetwork shape properties. As such, we invoke the machinery of clique persistent\nhomology on graphs to systematically and efficiently track evolution of the\nnetwork shape and, as a result, to detect changes in the underlying network\ntopology and geometry. We develop a new persistence summary for multilayer\nnetworks, called stacked persistence diagram, and prove its stability under\ninput data perturbations. We validate our new topological anomaly detection\nframework in application to dynamic multilayer networks from the Ethereum\nBlockchain and the Ripple Credit Network, and demonstrate that our stacked PD\napproach substantially outperforms state-of-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 12:58:04 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 18:05:13 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Ofori-Boateng", "Dorcas", ""], ["Dominguez", "Ignacio Segovia", ""], ["Kantarcioglu", "Murat", ""], ["Akcora", "Cuneyt G.", ""], ["Gel", "Yulia R.", ""]]}, {"id": "2106.01840", "submitter": "Jie Yang", "authors": "Linghan Zhang, Jie Yang", "title": "A Continuous Liveness Detection System for Text-independent Speaker\n  Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice authentication is drawing increasing attention and becomes an\nattractive alternative to passwords for mobile authentication. Recent advances\nin mobile technology further accelerate the adoption of voice biometrics in an\narray of diverse mobile applications. However, recent studies show that voice\nauthentication is vulnerable to replay attacks, where an adversary can spoof a\nvoice authentication system using a pre-recorded voice sample collected from\nthe victim. In this paper, we propose VoiceLive, a liveness detection system\nfor both text-dependent and text-independent voice authentication on\nsmartphones. VoiceLive detects a live user by leveraging the user's unique\nvocal system and the stereo recording of smartphones. In particular, utilizing\nthe built-in gyroscope, loudspeaker, and microphone, VoiceLive first measures\nthe smartphone's distance and angle from the user, then it captures the\nposition-specific time-difference-of-arrival (TDoA) changes in a sequence of\nphoneme sounds to the two microphones of the phone, and uses such unique TDoA\ndynamic which doesn't exist under replay attacks for liveness detection.\nVoiceLive is practical as it doesn't require additional hardware but\ntwo-channel stereo recording that is supported by virtually all smartphones.\nOur experimental evaluation with 12 participants and different types of phones\nshows that VoiceLive achieves over 99% detection accuracy at around 1% Equal\nError Rate (EER) on the text-dependent system and around 99% accuracy and 2%\nEER on the text-independent one. Results also show that VoiceLive is robust to\ndifferent phone positions, i.e. the user is free to hold the smartphone with\ndistinct distances and angles.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 00:32:40 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Linghan", ""], ["Yang", "Jie", ""]]}, {"id": "2106.01850", "submitter": "Jian Xiang", "authors": "Jian Xiang, Nathan Fulton, Stephen Chong", "title": "Relational Analysis of Sensor Attacks on Cyber-Physical Systems", "comments": "This is an extended version of the paper with the same title that\n  appeared in the 2021 Computer Security Foundations Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cyber-physical systems, such as self-driving cars or autonomous aircraft,\nmust defend against attacks that target sensor hardware. Analyzing system\ndesign can help engineers understand how a compromised sensor could impact the\nsystem's behavior; however, designing security analyses for cyber-physical\nsystems is difficult due to their combination of discrete dynamics, continuous\ndynamics, and nondeterminism.\n  This paper contributes a framework for modeling and analyzing sensor attacks\non cyber-physical systems, using the formalism of hybrid programs. We formalize\nand analyze two relational properties of a system's robustness. These\nrelational properties respectively express (1) whether a system's safety\nproperty can be influenced by sensor attacks, and (2) whether a system's\nhigh-integrity state can be affected by sensor attacks. We characterize these\nrelational properties by defining an equivalence relation between a system\nunder attack and the original unattacked system. That is, the system satisfies\nthe robustness properties if executions of the attacked system are\nappropriately related to executions of the unattacked system.\n  We present two techniques for reasoning about the equivalence relation and\nthus proving the relational properties for a system. One proof technique\ndecomposes large proof obligations to smaller proof obligations. The other\nproof technique adapts the self-composition technique from the literature on\nsecure information-flow, allowing us to reduce reasoning about the equivalence\nof two systems to reasoning about properties of a single system. This technique\nallows us to reuse existing tools for reasoning about properties of hybrid\nprograms, but is challenging due to the combination of discrete dynamics,\ncontinuous dynamics, and nondeterminism.\n  To evaluate, we present three case studies motivated by real design flaws in\nexisting cyber-physical systems.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 13:54:59 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Xiang", "Jian", ""], ["Fulton", "Nathan", ""], ["Chong", "Stephen", ""]]}, {"id": "2106.01870", "submitter": "James Hsin-Yu Chiang", "authors": "Massimo Bartoletti, James Hsin-yu Chiang, Alberto Lluch-Lafuente", "title": "Maximizing Extractable Value from Automated Market Makers", "comments": "12 pages. Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CE cs.FL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Market Makers (AMMs) are decentralized applications that allow\nusers to exchange crypto-tokens without the need to find a matching exchange\norder. AMMs are one of the most successful DeFi use case so far, as the main\nAMM platforms UniSwap and Balancer process a daily volume of transactions worth\nbillions of dollars. Despite this success story, AMMs are well-known to suffer\nfrom transaction-ordering issues: indeed, adversaries can frontrun user\ntransactions to increase their gain to the detriment of honest users. Being\nspecifically designated to arrange user transactions into blocks, miners can\neasily play the role of adversary, by suitably selecting and ordering\ntransactions - and possibly inserting their own - to increase their gain. In\nthis paper we formally characterize rational miners as players which follow an\noptimal strategy in the mining game. We identify relevant variants of the game,\ncorresponding to specific real-world constraints that a miner might have. We\ndevise effective procedures to construct solutions to mining game, both in its\nmost general form and in some relevant variants. Most notably, miners can\nexploit these solutions to maximize the value extracted from user transactions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:32:05 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bartoletti", "Massimo", ""], ["Chiang", "James Hsin-yu", ""], ["Lluch-Lafuente", "Alberto", ""]]}, {"id": "2106.01940", "submitter": "Panagiotis Papadopoulos", "authors": "Gon\\c{c}alo Pestana, I\\~nigo Querejeta-Azurmendi, Panagiotis\n  Papadopoulos, Benjamin Livshits", "title": "THEMIS: A Decentralized Privacy-Preserving Ad Platform with Reporting\n  Integrity", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.05556", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Online advertising fuels the (seemingly) free internet. However, although\nusers can access most of the web services free of charge, they pay a heavy\ncoston their privacy. They are forced to trust third parties and\nintermediaries, who not only collect behavioral data but also absorb great\namounts of ad revenues. Consequently, more and more users opt out from\nadvertising by resorting to ad blockers, thus costing publishers millions of\ndollars in lost ad revenues. Albeit there are various privacy-preserving\nadvertising proposals (e.g.,Adnostic, Privad, Brave Ads) from both academia and\nindustry, they all rely on centralized management that users have to blindly\ntrust without being able to audit, while they also fail to guarantee the\nintegrity of the per-formance analytics they provide to advertisers.\n  In this paper, we design and deploy THEMIS, a novel, decentralized and\nprivacy-by-design ad platform that requires zero trust by users. THEMIS (i)\nprovides auditability to its participants, (ii) rewards users for viewing ads,\nand (iii) allows advertisers to verify the performance and billing reports of\ntheir ad campaigns. By leveraging smart contracts and zero-knowledge schemes,\nwe implement a prototype of THEMIS and early performance evaluation results\nshow that it can scale linearly on a multi sidechain setup while it supports\nmore than 51M users on a single-sidechain.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:44:31 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Pestana", "Gon\u00e7alo", ""], ["Querejeta-Azurmendi", "I\u00f1igo", ""], ["Papadopoulos", "Panagiotis", ""], ["Livshits", "Benjamin", ""]]}, {"id": "2106.01998", "submitter": "Akbar Siami Namin", "authors": "Faranak Abri, Luis Felipe Gutierrez, Chaitra T. Kulkarni, Akbar Siami\n  Namin, Keith S. Jones", "title": "Toward Explainable Users: Using NLP to Enable AI to Understand Users'\n  Perceptions of Cyber Attacks", "comments": "20 pages, 3 figures, COMPSAC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To understand how end-users conceptualize consequences of cyber security\nattacks, we performed a card sorting study, a well-known technique in Cognitive\nSciences, where participants were free to group the given consequences of\nchosen cyber attacks into as many categories as they wished using rationales\nthey see fit. The results of the open card sorting study showed a large amount\nof inter-participant variation making the research team wonder how the\nconsequences of security attacks were comprehended by the participants. As an\nexploration of whether it is possible to explain user's mental model and\nbehavior through Artificial Intelligence (AI) techniques, the research team\ncompared the card sorting data with the outputs of a number of Natural Language\nProcessing (NLP) techniques with the goal of understanding how participants\nperceived and interpreted the consequences of cyber attacks written in natural\nlanguages. The results of the NLP-based exploration methods revealed an\ninteresting observation implying that participants had mostly employed checking\nindividual keywords in each sentence to group cyber attack consequences\ntogether and less considered the semantics behind the description of\nconsequences of cyber attacks. The results reported in this paper are seemingly\nuseful and important for cyber attacks comprehension from user's perspectives.\nTo the best of our knowledge, this paper is the first introducing the use of AI\ntechniques in explaining and modeling users' behavior and their perceptions\nabout a context. The novel idea introduced here is about explaining users using\nAI.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:17:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Abri", "Faranak", ""], ["Gutierrez", "Luis Felipe", ""], ["Kulkarni", "Chaitra T.", ""], ["Namin", "Akbar Siami", ""], ["Jones", "Keith S.", ""]]}, {"id": "2106.02011", "submitter": "Siyu Zhang", "authors": "Siyu Zhang, Zhongliang Yang, Jinshuai Yang, Yongfeng Huang", "title": "Provably Secure Generative Linguistic Steganography", "comments": "Accepted by ACL-IJCNLP 2021: findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative linguistic steganography mainly utilized language models and\napplied steganographic sampling (stegosampling) to generate high-security\nsteganographic text (stegotext). However, previous methods generally lead to\nstatistical differences between the conditional probability distributions of\nstegotext and natural text, which brings about security risks. In this paper,\nto further ensure security, we present a novel provably secure generative\nlinguistic steganographic method ADG, which recursively embeds secret\ninformation by Adaptive Dynamic Grouping of tokens according to their\nprobability given by an off-the-shelf language model. We not only prove the\nsecurity of ADG mathematically, but also conduct extensive experiments on three\npublic corpora to further verify its imperceptibility. The experimental results\nreveal that the proposed method is able to generate stegotext with nearly\nperfect security.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:27:10 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Siyu", ""], ["Yang", "Zhongliang", ""], ["Yang", "Jinshuai", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2106.02012", "submitter": "Akbar Siami Namin", "authors": "Shuvalaxmi Dass, Prerit Datta, Akbar Siami Namin", "title": "Attack Prediction using Hidden Markov Model", "comments": "20 pages, 4 figures, COMPSAC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is important to predict any adversarial attacks and their types to enable\neffective defense systems. Often it is hard to label such activities as\nmalicious ones without adequate analytical reasoning. We propose the use of\nHidden Markov Model (HMM) to predict the family of related attacks. Our\nproposed model is based on the observations often agglomerated in the form of\nlog files and from the target or the victim's perspective. We have built an\nHMM-based prediction model and implemented our proposed approach using Viterbi\nalgorithm, which generates a sequence of states corresponding to stages of a\nparticular attack. As a proof of concept and also to demonstrate the\nperformance of the model, we have conducted a case study on predicting a family\nof attacks called Action Spoofing.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:32:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Dass", "Shuvalaxmi", ""], ["Datta", "Prerit", ""], ["Namin", "Akbar Siami", ""]]}, {"id": "2106.02105", "submitter": "Jacob Springer", "authors": "Jacob M. Springer, Melanie Mitchell, Garrett T. Kenyon", "title": "A Little Robustness Goes a Long Way: Leveraging Universal Features for\n  Targeted Transfer Attacks", "comments": "25 pages, 13 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adversarial examples for neural network image classifiers are known to be\ntransferable: examples optimized to be misclassified by a source classifier are\noften misclassified as well by classifiers with different architectures.\nHowever, targeted adversarial examples -- optimized to be classified as a\nchosen target class -- tend to be less transferable between architectures.\nWhile prior research on constructing transferable targeted attacks has focused\non improving the optimization procedure, in this work we examine the role of\nthe source classifier. Here, we show that training the source classifier to be\n\"slightly robust\" -- that is, robust to small-magnitude adversarial examples --\nsubstantially improves the transferability of targeted attacks, even between\narchitectures as different as convolutional neural networks and transformers.\nWe argue that this result supports a non-intuitive hypothesis: on the spectrum\nfrom non-robust (standard) to highly robust classifiers, those that are only\nslightly robust exhibit the most universal features -- ones that tend to\noverlap with the features learned by other classifiers trained on the same\ndataset. The results we present provide insight into the nature of adversarial\nexamples as well as the mechanisms underlying so-called \"robust\" classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 19:53:46 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Springer", "Jacob M.", ""], ["Mitchell", "Melanie", ""], ["Kenyon", "Garrett T.", ""]]}, {"id": "2106.02167", "submitter": "Nguyen Phong Hoang", "authors": "Nguyen Phong Hoang, Arian Akhavan Niaki, Jakub Dalek, Jeffrey Knockel,\n  Pellaeon Lin, Bill Marczak, Masashi Crete-Nishihata, Phillipa Gill, Michalis\n  Polychronakis", "title": "How Great is the Great Firewall? Measuring China's DNS Censorship", "comments": "To appear at the 30th USENIX Security Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.NI cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The DNS filtering apparatus of China's Great Firewall (GFW) has evolved\nconsiderably over the past two decades. However, most prior studies of China's\nDNS filtering were performed over short time periods, leading to unnoticed\nchanges in the GFW's behavior. In this study, we introduce GFWatch, a\nlarge-scale, longitudinal measurement platform capable of testing hundreds of\nmillions of domains daily, enabling continuous monitoring of the GFW's DNS\nfiltering behavior.\n  We present the results of running GFWatch over a nine-month period, during\nwhich we tested an average of 411M domains per day and detected a total of 311K\ndomains censored by GFW's DNS filter. To the best of our knowledge, this is the\nlargest number of domains tested and censored domains discovered in the\nliterature. We further reverse engineer regular expressions used by the GFW and\nfind 41K innocuous domains that match these filters, resulting in overblocking\nof their content. We also observe bogus IPv6 and globally routable IPv4\naddresses injected by the GFW, including addresses owned by US companies, such\nas Facebook, Dropbox, and Twitter.\n  Using data from GFWatch, we studied the impact of GFW blocking on the global\nDNS system. We found 77K censored domains with DNS resource records polluted in\npopular public DNS resolvers, such as Google and Cloudflare. Finally, we\npropose strategies to detect poisoned responses that can (1) sanitize poisoned\nDNS records from the cache of public DNS resolvers, and (2) assist in the\ndevelopment of circumvention tools to bypass the GFW's DNS censorship.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 22:59:27 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Hoang", "Nguyen Phong", ""], ["Niaki", "Arian Akhavan", ""], ["Dalek", "Jakub", ""], ["Knockel", "Jeffrey", ""], ["Lin", "Pellaeon", ""], ["Marczak", "Bill", ""], ["Crete-Nishihata", "Masashi", ""], ["Gill", "Phillipa", ""], ["Polychronakis", "Michalis", ""]]}, {"id": "2106.02203", "submitter": "Ryo Kikuchi", "authors": "Nuttapong Attrapadung and Koki Hamada and Dai Ikarashi and Ryo Kikuchi\n  and Takahiro Matsuda and Ibuki Mishina and Hiraku Morita and Jacob C. N.\n  Schuldt", "title": "Adam in Private: Secure and Fast Training of Deep Neural Networks with\n  Adaptive Moment Estimation", "comments": "24 pages, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Privacy-preserving machine learning (PPML) aims at enabling machine learning\n(ML) algorithms to be used on sensitive data. We contribute to this line of\nresearch by proposing a framework that allows efficient and secure evaluation\nof full-fledged state-of-the-art ML algorithms via secure multi-party\ncomputation (MPC). This is in contrast to most prior works, which substitute ML\nalgorithms with approximated \"MPC-friendly\" variants. A drawback of the latter\napproach is that fine-tuning of the combined ML and MPC algorithms is required,\nwhich might lead to less efficient algorithms or inferior quality ML. This is\nan issue for secure deep neural networks (DNN) training in particular, as this\ninvolves arithmetic algorithms thought to be \"MPC-unfriendly\", namely, integer\ndivision, exponentiation, inversion, and square root. In this work, we propose\nsecure and efficient protocols for the above seemingly MPC-unfriendly\ncomputations. Our protocols are three-party protocols in the honest-majority\nsetting, and we propose both passively secure and actively secure with abort\nvariants. A notable feature of our protocols is that they simultaneously\nprovide high accuracy and efficiency. This framework enables us to efficiently\nand securely compute modern ML algorithms such as Adam and the softmax function\n\"as is\", without resorting to approximations. As a result, we obtain secure DNN\ntraining that outperforms state-of-the-art three-party systems; our full\ntraining is up to 6.7 times faster than just the online phase of the recently\nproposed FALCON@PETS'21 on a standard benchmark network. We further perform\nmeasurements on real-world DNNs, AlexNet and VGG16. The performance of our\nframework is up to a factor of about 12-14 faster for AlexNet and 46-48 faster\nfor VGG16 to achieve an accuracy of 70% and 75%, respectively, when compared to\nFALCON.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 01:40:09 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Attrapadung", "Nuttapong", ""], ["Hamada", "Koki", ""], ["Ikarashi", "Dai", ""], ["Kikuchi", "Ryo", ""], ["Matsuda", "Takahiro", ""], ["Mishina", "Ibuki", ""], ["Morita", "Hiraku", ""], ["Schuldt", "Jacob C. N.", ""]]}, {"id": "2106.02378", "submitter": "Mazen Azzam", "authors": "Mazen Azzam, Liliana Pasquale, Gregory Provan, Bashar Nuseibeh", "title": "Efficient Predictive Monitoring of Linear Time-Invariant Systems Under\n  Stealthy Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attacks on Industrial Control Systems (ICS) can lead to significant physical\ndamage. While offline safety and security assessments can provide insight into\nvulnerable system components, they may not account for stealthy attacks\ndesigned to evade anomaly detectors during long operational transients. In this\npaper, we propose a predictive online monitoring approach to check the safety\nof the system under potential stealthy attacks. Specifically, we adapt previous\nresults in reachability analysis for attack impact assessment to provide an\nefficient algorithm for online safety monitoring for Linear Time-Invariant\n(LTI) systems. The proposed approach relies on an offline computation of\nsymbolic reachable sets in terms of the estimated physical state of the system.\nThese sets are then instantiated online, and safety checks are performed by\nleveraging ideas from ellipsoidal calculus. We illustrate and evaluate our\napproach using the Tennessee-Eastman process. We also compare our approach with\nthe baseline monitoring approaches proposed in previous work and assess its\nefficiency and scalability. Our evaluation results demonstrate that our\napproach can predict in a timely manner if a false data injection attack will\nbe able to cause damage, while remaining undetected. Thus, our approach can be\nused to provide operators with real-time early warnings about stealthy attacks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:44:06 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Azzam", "Mazen", ""], ["Pasquale", "Liliana", ""], ["Provan", "Gregory", ""], ["Nuseibeh", "Bashar", ""]]}, {"id": "2106.02383", "submitter": "Jingyu Feng", "authors": "Jingyu Feng, Tao Wang, Naixue Xiong, Teng Wang", "title": "PoDT: A Secure Multi-chains Consensus Scheme Against Diverse Miners\n  Behaviors Attacks in Blockchain Networks", "comments": "14 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cross-chain technologies make the interactions among different blockchains\n(hereinafter \"chains\") possible, multi-chains consensus is becoming more and\nmore important in blockchain networks. However, more attention has been paid to\nthe single-chain consensus schemes. The multi-chains consensus with trusted\nminers participation has been not considered, thus offering opportunities for\nmalicious users to launch Diverse Miners Behaviors (DMB) attacks on different\nchains. DMB attackers can be friendly in the consensus process of some chains\ncalled mask-chains to enhance trust value, while on other chains called\nkill-chains they engage in destructive behaviors of network. In this paper, we\npropose a multi-chains consensus scheme named as Proof-of-DiscTrust (PoDT) to\ndefend against DMB attacks. Distinctive trust idea (DiscTrust) is introduced to\nevaluate the trust value of each user concerning different chains. A dynamic\nbehaviors prediction scheme is designed to strengthen DiscTrust to prevent\nintensive DMB attackers who maintain high trust by alternately creating true or\nfalse blocks on kill-chains. On this basis, a trusted miners selection\nalgorithm for multi-chains can be achieved at a round of block creation.\nExperimental results show that PoDT is secure against DMB attacks and more\neffective than traditional consensus schemes in multi-chains environments.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:49:38 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Feng", "Jingyu", ""], ["Wang", "Tao", ""], ["Xiong", "Naixue", ""], ["Wang", "Teng", ""]]}, {"id": "2106.02470", "submitter": "Tao Wang", "authors": "Tao Wang, Tongjiang Yan, Vladimir Sidorenko, Xueting Wang", "title": "Quantum Synchronizable Codes From Cyclotomic Classes of Order Two over\n  $\\mathbb{Z}_{2q}$", "comments": "arXiv admin note: substantial text overlap with arXiv:2105.03619", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum synchronizable codes are kinds of quantum error-correcting codes that\ncan not only correct the effects of quantum noise on qubits but also the\nmisalignment in block synchronization. This paper contributes to constructing\ntwo classes of quantum synchronizable codes by the cyclotomic classes of order\ntwo over $\\mathbb{Z}_{2q}$, whose synchronization capabilities can reach the\nupper bound under certain conditions. Moreover, the quantum synchronizable\ncodes possess good error-correcting capability towards bit errors and phase\nerrors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:14:51 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 07:04:18 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wang", "Tao", ""], ["Yan", "Tongjiang", ""], ["Sidorenko", "Vladimir", ""], ["Wang", "Xueting", ""]]}, {"id": "2106.02483", "submitter": "Davide Caputo", "authors": "Davide Caputo, Francesco Pagano, Giovanni Bottino, Luca Verderame,\n  Alessio Merlo", "title": "You can't always get what you want: towards user-controlled privacy on\n  Android", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile applications (hereafter, apps) collect a plethora of information\nregarding the user behavior and his device through third-party analytics\nlibraries. However, the collection and usage of such data raised several\nprivacy concerns, mainly because the end-user - i.e., the actual owner of the\ndata - is out of the loop in this collection process. Also, the existing\nprivacy-enhanced solutions that emerged in the last years follow an \"all or\nnothing\" approach, leaving the user the sole option to accept or completely\ndeny the access to privacy-related data.\n  This work has the two-fold objective of assessing the privacy implications on\nthe usage of analytics libraries in mobile apps and proposing a data\nanonymization methodology that enables a trade-off between the utility and\nprivacy of the collected data and gives the user complete control over the\nsharing process. To achieve that, we present an empirical privacy assessment on\nthe analytics libraries contained in the 4500 most-used Android apps of the\nGoogle Play Store between November 2020 and January 2021. Then, we propose an\nempowered anonymization methodology, based on MobHide, that gives the end-user\ncomplete control over the collection and anonymization process. Finally, we\nempirically demonstrate the applicability and effectiveness of such\nanonymization methodology thanks to HideDroid, a fully-fledged anonymization\napp for the Android ecosystem.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:41:39 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Caputo", "Davide", ""], ["Pagano", "Francesco", ""], ["Bottino", "Giovanni", ""], ["Verderame", "Luca", ""], ["Merlo", "Alessio", ""]]}, {"id": "2106.02484", "submitter": "Adam Yala", "authors": "Adam Yala, Homa Esfahanizadeh, Rafael G. L. D' Oliveira, Ken R. Duffy,\n  Manya Ghobadi, Tommi S. Jaakkola, Vinod Vaikuntanathan, Regina Barzilay,\n  Muriel Medard", "title": "NeuraCrypt: Hiding Private Health Data via Random Neural Networks for\n  Public Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Balancing the needs of data privacy and predictive utility is a central\nchallenge for machine learning in healthcare. In particular, privacy concerns\nhave led to a dearth of public datasets, complicated the construction of\nmulti-hospital cohorts and limited the utilization of external machine learning\nresources. To remedy this, new methods are required to enable data owners, such\nas hospitals, to share their datasets publicly, while preserving both patient\nprivacy and modeling utility. We propose NeuraCrypt, a private encoding scheme\nbased on random deep neural networks. NeuraCrypt encodes raw patient data using\na randomly constructed neural network known only to the data-owner, and\npublishes both the encoded data and associated labels publicly. From a\ntheoretical perspective, we demonstrate that sampling from a sufficiently rich\nfamily of encoding functions offers a well-defined and meaningful notion of\nprivacy against a computationally unbounded adversary with full knowledge of\nthe underlying data-distribution. We propose to approximate this family of\nencoding functions through random deep neural networks. Empirically, we\ndemonstrate the robustness of our encoding to a suite of adversarial attacks\nand show that NeuraCrypt achieves competitive accuracy to non-private baselines\non a variety of x-ray tasks. Moreover, we demonstrate that multiple hospitals,\nusing independent private encoders, can collaborate to train improved x-ray\nmodels. Finally, we release a challenge dataset to encourage the development of\nnew attacks on NeuraCrypt.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:42:21 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yala", "Adam", ""], ["Esfahanizadeh", "Homa", ""], ["Oliveira", "Rafael G. L. D'", ""], ["Duffy", "Ken R.", ""], ["Ghobadi", "Manya", ""], ["Jaakkola", "Tommi S.", ""], ["Vaikuntanathan", "Vinod", ""], ["Barzilay", "Regina", ""], ["Medard", "Muriel", ""]]}, {"id": "2106.02494", "submitter": "Aly Sabri Abdalla", "authors": "Talha F. Rahman, Aly S. Abdalla, Keith Powell, Walaa AlQwider, and Vuk\n  Marojevic", "title": "Network and Physical Layer Attacks and countermeasures to AI-Enabled 6G\n  O-RAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial intelligence (AI) will play an increasing role in cellular network\ndeployment, configuration and management. This paper examines the security\nimplications of AI-driven 6G radio access networks (RANs). While the expected\ntimeline for 6G standardization is still several years out, pre-standardization\nefforts related to 6G security are already ongoing and will benefit from\nfundamental and experimental research. The Open RAN (O-RAN) describes an\nindustry-driven open architecture and interfaces for building next generation\nRANs with AI control. Considering this architecture, we identify the critical\nthreats to data driven network and physical layer elements, the corresponding\ncountermeasures, and the research directions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 16:36:37 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Rahman", "Talha F.", ""], ["Abdalla", "Aly S.", ""], ["Powell", "Keith", ""], ["AlQwider", "Walaa", ""], ["Marojevic", "Vuk", ""]]}, {"id": "2106.02623", "submitter": "Sam L. Thomas", "authors": "Chris McMahon Stone and Sam L. Thomas and Mathy Vanhoef and James\n  Henderson and Nicolas Bailluet and Tom Chothia", "title": "The Closer You Look, The More You Learn: A Grey-box Approach to Protocol\n  State Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new approach to infer state machine models from\nprotocol implementations. Our method, STATEINSPECTOR, learns protocol states by\nusing novel program analyses to combine observations of run-time memory and\nI/O. It requires no access to source code and only lightweight execution\nmonitoring of the implementation under test. We demonstrate and evaluate\nSTATEINSPECTOR's effectiveness on numerous TLS and WPA/2 implementations. In\nthe process, we show STATEINSPECTOR enables deeper state discovery, increased\nlearning efficiency, and more insightful post-mortem analyses than existing\napproaches. Further to improved learning, our method led us to discover several\nconcerning deviations from the standards and a high impact vulnerability in a\nprominent Wi-Fi implementation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:36:34 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 10:31:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Stone", "Chris McMahon", ""], ["Thomas", "Sam L.", ""], ["Vanhoef", "Mathy", ""], ["Henderson", "James", ""], ["Bailluet", "Nicolas", ""], ["Chothia", "Tom", ""]]}, {"id": "2106.02674", "submitter": "Ferdinando Fioretto", "authors": "Cuong Tran, My H. Dinh, Ferdinando Fioretto", "title": "Differentially Private Deep Learning under the Fairness Lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential Privacy (DP) is an important privacy-enhancing technology for\nprivate machine learning systems. It allows to measure and bound the risk\nassociated with an individual participation in a computation. However, it was\nrecently observed that DP learning systems may exacerbate bias and unfairness\nfor different groups of individuals. This paper builds on these important\nobservations and sheds light on the causes of the disparate impacts arising in\nthe problem of differentially private empirical risk minimization. It focuses\non the accuracy disparity arising among groups of individuals in two\nwell-studied DP learning methods: output perturbation and differentially\nprivate stochastic gradient descent. The paper analyzes which data and model\nproperties are responsible for the disproportionate impacts, why these aspects\nare affecting different groups disproportionately and proposes guidelines to\nmitigate these effects. The proposed approach is evaluated on several datasets\nand settings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 19:10:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tran", "Cuong", ""], ["Dinh", "My H.", ""], ["Fioretto", "Ferdinando", ""]]}, {"id": "2106.02731", "submitter": "Yanjun Pan", "authors": "Yanjun Pan, Ziqi Xu, Ming Li, Loukas Lazos", "title": "Man-in-the-Middle Attack Resistant Secret Key Generation via Channel\n  Randomization", "comments": "13 pages, 8 figures, 4 tables", "journal-ref": null, "doi": "10.1145/3466772.3467052", "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Physical-layer based key generation schemes exploit the channel reciprocity\nfor secret key extraction, which can achieve information-theoretic secrecy\nagainst eavesdroppers. Such methods, although practical, have been shown to be\nvulnerable against man-in-the-middle (MitM) attacks, where an active adversary,\nMallory, can influence and infer part of the secret key generated between Alice\nand Bob by injecting her own packet upon observing highly correlated\nchannel/RSS measurements from Alice and Bob. As all the channels remain stable\nwithin the channel coherence time, Mallory's injected packets cause Alice and\nBob to measure similar RSS, which allows Mallory to successfully predict the\nderived key bits. To defend against such a MitM attack, we propose to utilize a\nreconfigurable antenna at one of the legitimate transceivers to proactively\nrandomize the channel state across different channel probing rounds. The\nrandomization of the antenna mode at every probing round breaks the temporal\ncorrelation of the channels from the adversary to the legitimate devices, while\npreserving the reciprocity of the channel between the latter. This prevents key\ninjection from the adversary without affecting Alice and Bob's ability to\nmeasure common randomness. We theoretically analyze the security of the\nprotocol and conduct extensive simulations and real-world experiments to\nevaluate its performance. Our results show that our approach eliminates the\nadvantage of an active MitM attack by driving down the probability of\nsuccessfully guessing bits of the secret key to a random guess.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:33:52 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 21:50:19 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Pan", "Yanjun", ""], ["Xu", "Ziqi", ""], ["Li", "Ming", ""], ["Lazos", "Loukas", ""]]}, {"id": "2106.02732", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Shucheng Yu", "title": "BO-DBA: Query-Efficient Decision-Based Adversarial Attacks via Bayesian\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-based attacks (DBA), wherein attackers perturb inputs to spoof\nlearning algorithms by observing solely the output labels, are a type of severe\nadversarial attacks against Deep Neural Networks (DNNs) requiring minimal\nknowledge of attackers. State-of-the-art DBA attacks relying on zeroth-order\ngradient estimation require an excessive number of queries. Recently, Bayesian\noptimization (BO) has shown promising in reducing the number of queries in\nscore-based attacks (SBA), in which attackers need to observe real-valued\nprobability scores as outputs. However, extending BO to the setting of DBA is\nnontrivial because in DBA only output labels instead of real-valued scores, as\nneeded by BO, are available to attackers. In this paper, we close this gap by\nproposing an efficient DBA attack, namely BO-DBA. Different from existing\napproaches, BO-DBA generates adversarial examples by searching so-called\n\\emph{directions of perturbations}. It then formulates the problem as a BO\nproblem that minimizes the real-valued distortion of perturbations. With the\noptimized perturbation generation process, BO-DBA converges much faster than\nthe state-of-the-art DBA techniques. Experimental results on pre-trained\nImageNet classifiers show that BO-DBA converges within 200 queries while the\nstate-of-the-art DBA techniques need over 15,000 queries to achieve the same\nlevel of perturbation distortion. BO-DBA also shows similar attack success\nrates even as compared to BO-based SBA attacks but with less distortion.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:46:37 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Yu", "Shucheng", ""]]}, {"id": "2106.02747", "submitter": "Thomas Debris-Alazard", "authors": "Thomas Debris-Alazard, Maxime Remaud and Jean-Pierre Tillich", "title": "Quantum Reduction of Finding Short Code Vectors to the Decoding Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a quantum reduction from finding short codewords in a random linear\ncode to decoding for the Hamming metric. This is the first time such a\nreduction (classical or quantum) has been obtained. Our reduction adapts to\nlinear codes Stehl\\'{e}-Steinfield-Tanaka-Xagawa' re-interpretation of Regev's\nquantum reduction from finding short lattice vectors to solving the Closest\nVector Problem. The Hamming metric is a much coarser metric than the Euclidean\nmetric and this adaptation has needed several new ingredients to make it work.\nFor instance, in order to have a meaningful reduction it is necessary in the\nHamming metric to choose a very large decoding radius and this needs in many\ncases to go beyond the radius where decoding is unique. Another crucial step\nfor the analysis of the reduction is the choice of the errors that are being\nfed to the decoding algorithm. For lattices, errors are usually sampled\naccording to a Gaussian distribution. However, it turns out that the Bernoulli\ndistribution (the analogue for codes of the Gaussian) is too much spread out\nand can not be used for the reduction with codes. Instead we choose here the\nuniform distribution over errors of a fixed weight and bring in orthogonal\npolynomials tools to perform the analysis and an additional amplitude\namplification step to obtain the aforementioned result.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:42:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Debris-Alazard", "Thomas", ""], ["Remaud", "Maxime", ""], ["Tillich", "Jean-Pierre", ""]]}, {"id": "2106.02766", "submitter": "Naresh Goud Boddu", "authors": "Divesh Aggarwal, Naresh Goud Boddu, Rahul Jain, Maciej Obremski", "title": "Quantum Measurement Adversary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-source-extractors are functions that extract uniform randomness from\nmultiple (weak) sources of randomness. Quantum multi-source-extractors were\nconsidered by Kasher and Kempe (for the quantum-independent-adversary and the\nquantum-bounded-storage-adversary), Chung, Li and Wu (for the\ngeneral-entangled-adversary) and Arnon-Friedman, Portmann and Scholz (for the\nquantum-Markov-adversary). One of the main objectives of this work is to unify\nall the existing quantum multi-source adversary models. We propose two new\nmodels of adversaries: 1) the quantum-measurement-adversary (qm-adv), which\ngenerates side-information using entanglement and on post-measurement and 2)\nthe quantum-communication-adversary (qc-adv), which generates side-information\nusing entanglement and communication between multiple sources. We show that, 1.\nqm-adv is the strongest adversary among all the known adversaries, in the sense\nthat the side-information of all other adversaries can be generated by qm-adv.\n2. The (generalized) inner-product function (in fact a general class of\ntwo-wise independent functions) continues to work as a good extractor against\nqm-adv with matching parameters as that of Chor and Goldreich. 3. A\nnon-malleable-extractor proposed by Li (against classical-adversaries)\ncontinues to be secure against quantum side-information. This result implies a\nnon-malleable-extractor result of Aggarwal, Chung, Lin and Vidick with uniform\nseed. We strengthen their result via a completely different proof to make the\nnon-malleable-extractor of Li secure against quantum side-information even when\nthe seed is not uniform. 4. A modification (working with weak sources instead\nof uniform sources) of the Dodis and Wichs protocol for privacy-amplification\nis secure against active quantum adversaries. This strengthens on a recent\nresult due to Aggarwal, Chung, Lin and Vidick which uses uniform sources.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 01:14:21 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Boddu", "Naresh Goud", ""], ["Jain", "Rahul", ""], ["Obremski", "Maciej", ""]]}, {"id": "2106.02769", "submitter": "Rafael Dowsley", "authors": "Samuel Adams, Chaitali Choudhary, Martine De Cock, Rafael Dowsley,\n  David Melanson, Anderson C. A. Nascimento, Davis Railsback, Jianwei Shen", "title": "Privacy-Preserving Training of Tree Ensembles over Continuous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Secure Multi-Party Computation (MPC) protocols for\nprivacy-preserving training of decision trees over distributed data assume that\nthe features are categorical. In real-life applications, features are often\nnumerical. The standard ``in the clear'' algorithm to grow decision trees on\ndata with continuous values requires sorting of training examples for each\nfeature in the quest for an optimal cut-point in the range of feature values in\neach node. Sorting is an expensive operation in MPC, hence finding secure\nprotocols that avoid such an expensive step is a relevant problem in\nprivacy-preserving machine learning. In this paper we propose three more\nefficient alternatives for secure training of decision tree based models on\ndata with continuous features, namely: (1) secure discretization of the data,\nfollowed by secure training of a decision tree over the discretized data; (2)\nsecure discretization of the data, followed by secure training of a random\nforest over the discretized data; and (3) secure training of extremely\nrandomized trees (``extra-trees'') on the original data. Approaches (2) and (3)\nboth involve randomizing feature choices. In addition, in approach (3)\ncut-points are chosen randomly as well, thereby alleviating the need to sort or\nto discretize the data up front. We implemented all proposed solutions in the\nsemi-honest setting with additive secret sharing based MPC. In addition to\nmathematically proving that all proposed approaches are correct and secure, we\nexperimentally evaluated and compared them in terms of classification accuracy\nand runtime. We privately train tree ensembles over data sets with 1000s of\ninstances or features in a few minutes, with accuracies that are at par with\nthose obtained in the clear. This makes our solution orders of magnitude more\nefficient than the existing approaches, which are based on oblivious sorting.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 01:28:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Adams", "Samuel", ""], ["Choudhary", "Chaitali", ""], ["De Cock", "Martine", ""], ["Dowsley", "Rafael", ""], ["Melanson", "David", ""], ["Nascimento", "Anderson C. A.", ""], ["Railsback", "Davis", ""], ["Shen", "Jianwei", ""]]}, {"id": "2106.02779", "submitter": "Tao Xiang", "authors": "Tao Xiang, Hangcheng Liu, Shangwei Guo, Tianwei Zhang", "title": "PEEL: A Provable Removal Attack on Deep Hiding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep hiding, embedding images into another using deep neural networks, has\nshown its great power in increasing the message capacity and robustness. In\nthis paper, we conduct an in-depth study of state-of-the-art deep hiding\nschemes and analyze their hidden vulnerabilities. Then, according to our\nobservations and analysis, we propose a novel ProvablE rEmovaL attack (PEEL)\nusing image inpainting to remove secret images from containers without any\nprior knowledge about the deep hiding scheme. We also propose a systemic\nmethodology to improve the efficiency and image quality of PEEL by carefully\ndesigning a removal strategy and fully utilizing the visual information of\ncontainers. Extensive evaluations show our attacks can completely remove secret\nimages and has negligible impact on the quality of containers.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 02:32:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xiang", "Tao", ""], ["Liu", "Hangcheng", ""], ["Guo", "Shangwei", ""], ["Zhang", "Tianwei", ""]]}, {"id": "2106.02818", "submitter": "Amir Ahooye Atashin", "authors": "Amir Ahooye Atashin, Behrooz Razeghi, Deniz G\\\"und\\\"uz, Slava\n  Voloshynovskiy", "title": "Variational Leakage: The Role of Information Complexity in Privacy\n  Leakage", "comments": null, "journal-ref": null, "doi": "10.1145/3468218.3469040", "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of information complexity in privacy leakage about an\nattribute of an adversary's interest, which is not known a priori to the system\ndesigner. Considering the supervised representation learning setup and using\nneural networks to parameterize the variational bounds of information\nquantities, we study the impact of the following factors on the amount of\ninformation leakage: information complexity regularizer weight, latent space\ndimension, the cardinalities of the known utility and unknown sensitive\nattribute sets, the correlation between utility and sensitive attributes, and a\npotential bias in a sensitive attribute of adversary's interest. We conduct\nextensive experiments on Colored-MNIST and CelebA datasets to evaluate the\neffect of information complexity on the amount of intrinsic leakage.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 06:56:39 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:17:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Atashin", "Amir Ahooye", ""], ["Razeghi", "Behrooz", ""], ["G\u00fcnd\u00fcz", "Deniz", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "2106.02848", "submitter": "Sivakanth Gopi", "authors": "Sivakanth Gopi, Yin Tat Lee, Lukas Wutschitz", "title": "Numerical Composition of Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a fast algorithm to optimally compose privacy guarantees of\ndifferentially private (DP) algorithms to arbitrary accuracy. Our method is\nbased on the notion of privacy loss random variables to quantify the privacy\nloss of DP algorithms. The running time and memory needed for our algorithm to\napproximate the privacy curve of a DP algorithm composed with itself $k$ times\nis $\\tilde{O}(\\sqrt{k})$. This improves over the best prior method by Koskela\net al. (2020) which requires $\\tilde{\\Omega}(k^{1.5})$ running time. We\ndemonstrate the utility of our algorithm by accurately computing the privacy\nloss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm\nspeeds up the privacy computations by a few orders of magnitude compared to\nprior work, while maintaining similar accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 09:20:15 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 22:30:06 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gopi", "Sivakanth", ""], ["Lee", "Yin Tat", ""], ["Wutschitz", "Lukas", ""]]}, {"id": "2106.02850", "submitter": "Ajith Suresh", "authors": "Nishat Koti, Arpita Patra, Rahul Rachuri, Ajith Suresh", "title": "Tetrad: Actively Secure 4PC for Secure Training and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we design an efficient mixed-protocol framework, Tetrad, with\napplications to privacy-preserving machine learning. It is designed for the\nfour-party setting with at most one active corruption and supports rings.\n  Our fair multiplication protocol requires communicating only 5 ring elements\nimproving over the state-of-the-art protocol of Trident (Chaudhari et al.\nNDSS'20). The technical highlights of Tetrad include efficient (a) truncation\nwithout any overhead, (b) multi-input multiplication protocols for arithmetic\nand boolean worlds, (c) garbled-world, tailor-made for the mixed-protocol\nframework, and (d) conversion mechanisms to switch between the computation\nstyles. The fair framework is also extended to provide robustness without\ninflating the costs.\n  The competence of Tetrad is tested with benchmarks for deep neural networks\nsuch as LeNet and VGG16 and support vector machines. One variant of our\nframework aims at minimizing the execution time, while the other focuses on the\nmonetary cost. We observe improvements up to 6x over Trident across these\nparameters.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 09:34:43 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 07:48:39 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Koti", "Nishat", ""], ["Patra", "Arpita", ""], ["Rachuri", "Rahul", ""], ["Suresh", "Ajith", ""]]}, {"id": "2106.02900", "submitter": "Jay Tenenbaum", "authors": "Jay Tenenbaum, Haim Kaplan, Yishay Mansour, Uri Stemmer", "title": "Differentially Private Multi-Armed Bandits in the Shuffle Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an $(\\varepsilon,\\delta)$-differentially private algorithm for the\nmulti-armed bandit (MAB) problem in the shuffle model with a\ndistribution-dependent regret of $O\\left(\\left(\\sum_{a\\in\n[k]:\\Delta_a>0}\\frac{\\log\nT}{\\Delta_a}\\right)+\\frac{k\\sqrt{\\log\\frac{1}{\\delta}}\\log\nT}{\\varepsilon}\\right)$, and a distribution-independent regret of\n$O\\left(\\sqrt{kT\\log T}+\\frac{k\\sqrt{\\log\\frac{1}{\\delta}}\\log\nT}{\\varepsilon}\\right)$, where $T$ is the number of rounds, $\\Delta_a$ is the\nsuboptimality gap of the arm $a$, and $k$ is the total number of arms. Our\nupper bound almost matches the regret of the best known algorithms for the\ncentralized model, and significantly outperforms the best known algorithm in\nthe local model.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 14:11:01 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 03:41:22 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Tenenbaum", "Jay", ""], ["Kaplan", "Haim", ""], ["Mansour", "Yishay", ""], ["Stemmer", "Uri", ""]]}, {"id": "2106.02970", "submitter": "Mohamed Alzayat", "authors": "Mohamed Alzayat (1), Johnnatan Messias (1), Balakrishnan\n  Chandrasekaran (2 and 3), Krishna P. Gummadi (1), and Patrick Loiseau (4)\n  ((1) MPI-SWS (2) MPI-INF (3) VU Amsterdam (4) Univ. Grenoble Alpes, Inria,\n  CNRS, Grenoble INP, LIG)", "title": "Modeling Coordinated vs. P2P Mining: An Analysis of Inefficiency and\n  Inequality in Proof-of-Work Blockchains", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study efficiency in a proof-of-work blockchain with non-zero latencies,\nfocusing in particular on the (inequality in) individual miners' efficiencies.\nPrior work attributed differences in miners' efficiencies mostly to attacks,\nbut we pursue a different question: Can inequality in miners' efficiencies be\nexplained by delays, even when all miners are honest? Traditionally, such\nefficiency-related questions were tackled only at the level of the overall\nsystem, and in a peer-to-peer (P2P) setting where miners directly connect to\none another. Despite it being common today for miners to pool compute\ncapacities in a mining pool managed by a centralized coordinator, efficiency in\nsuch a coordinated setting has barely been studied.\n  In this paper, we propose a simple model of a proof-of-work blockchain with\nlatencies for both the P2P and the coordinated settings. We derive a\nclosed-form expression for the efficiency in the coordinated setting with an\narbitrary number of miners and arbitrary latencies, both for the overall system\nand for each individual miner. We leverage this result to show that\ninequalities arise from variability in the delays, but that if all miners are\nequidistant from the coordinator, they have equal efficiency irrespective of\ntheir compute capacities. We then prove that, under a natural consistency\ncondition, the overall system efficiency in the P2P setting is higher than that\nin the coordinated setting. Finally, we perform a simulation-based study to\ndemonstrate that even in the P2P setting delays between miners introduce\ninequalities, and that there is a more complex interplay between delays and\ncompute capacities.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 21:34:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Alzayat", "Mohamed", "", "2 and 3"], ["Messias", "Johnnatan", "", "2 and 3"], ["Chandrasekaran", "Balakrishnan", "", "2 and 3"], ["Gummadi", "Krishna P.", ""], ["Loiseau", "Patrick", ""]]}, {"id": "2106.02976", "submitter": "Himanshu Thapliyal", "authors": "Carson Labrado, Himanshu Thapliyal, Saraju P. Mohanty", "title": "Fortifying Vehicular Security Through Low Overhead Physically Unclonable\n  Functions", "comments": "19 Pages", "journal-ref": "ACM Journal on Emerging Technologies in Computing Systems, 2021", "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within vehicles, the Controller Area Network (CAN) allows efficient\ncommunication between the electronic control units (ECUs) responsible for\ncontrolling the various subsystems. The CAN protocol was not designed to\ninclude much support for secure communication. The fact that so many critical\nsystems can be accessed through an insecure communication network presents a\nmajor security concern. Adding security features to CAN is difficult due to the\nlimited resources available to the individual ECUs and the costs that would be\nassociated with adding the necessary hardware to support any additional\nsecurity operations without overly degrading the performance of standard\ncommunication. Replacing the protocol is another option, but it is subject to\nmany of the same problems. The lack of security becomes even more concerning as\nvehicles continue to adopt smart features. Smart vehicles have a multitude of\ncommunication interfaces would an attacker could exploit to gain access to the\nnetworks. In this work we propose a security framework that is based on\nphysically unclonable functions (PUFs) and lightweight cryptography (LWC). The\nframework does not require any modification to the standard CAN protocol while\nalso minimizing the amount of additional message overhead required for its\noperation. The improvements in our proposed framework results in major\nreduction in the number of CAN frames that must be sent during operation. For a\nsystem with 20 ECUs for example, our proposed framework only requires 6.5% of\nthe number of CAN frames that is required by the existing approach to\nsuccessfully authenticate every ECU.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 22:07:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Labrado", "Carson", ""], ["Thapliyal", "Himanshu", ""], ["Mohanty", "Saraju P.", ""]]}, {"id": "2106.02982", "submitter": "Sagar Dasgupta", "authors": "Sagar Dasgupta, Mizanur Rahman, Mhafuzul Islam, Mashrur Chowdhury", "title": "Sensor Fusion-based GNSS Spoofing Attack Detection Framework for\n  Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this study, a sensor fusion based GNSS spoofing attack detection framework\nis presented that consists of three concurrent strategies for an autonomous\nvehicle (AV): (i) prediction of location shift, (ii) detection of turns (left\nor right), and (iii) recognition of motion state (including standstill state).\nData from multiple low-cost in-vehicle sensors (i.e., accelerometer, steering\nangle sensor, speed sensor, and GNSS) are fused and fed into a recurrent neural\nnetwork model, which is a long short-term memory (LSTM) network for predicting\nthe location shift, i.e., the distance that an AV travels between two\nconsecutive timestamps. We have then combined k-Nearest Neighbors (k-NN) and\nDynamic Time Warping (DTW) algorithms to detect turns using data from the\nsteering angle sensor. In addition, data from an AV's speed sensor is used to\nrecognize the AV's motion state including the standstill state. To prove the\nefficacy of the sensor fusion-based attack detection framework, attack datasets\nare created for three unique and sophisticated spoofing attacks turn by turn,\novershoot, and stop using the publicly available real-world Honda Research\nInstitute Driving Dataset (HDD). Our analysis reveals that the sensor\nfusion-based detection framework successfully detects all three types of\nspoofing attacks within the required computational latency threshold.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 23:02:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dasgupta", "Sagar", ""], ["Rahman", "Mizanur", ""], ["Islam", "Mhafuzul", ""], ["Chowdhury", "Mashrur", ""]]}, {"id": "2106.03099", "submitter": "Kevin Roth", "authors": "Kevin Roth", "title": "A Primer on Multi-Neuron Relaxation-based Adversarial Robustness\n  Certification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The existence of adversarial examples poses a real danger when deep neural\nnetworks are deployed in the real world. The go-to strategy to quantify this\nvulnerability is to evaluate the model against specific attack algorithms. This\napproach is however inherently limited, as it says little about the robustness\nof the model against more powerful attacks not included in the evaluation. We\ndevelop a unified mathematical framework to describe relaxation-based\nrobustness certification methods, which go beyond adversary-specific robustness\nevaluation and instead provide provable robustness guarantees against attacks\nby any adversary. We discuss the fundamental limitations posed by single-neuron\nrelaxations and show how the recent ``k-ReLU'' multi-neuron relaxation\nframework of Singh et al. (2019) obtains tighter correlation-aware activation\nbounds by leveraging additional relational constraints among groups of neurons.\nSpecifically, we show how additional pre-activation bounds can be mapped to\ncorresponding post-activation bounds and how they can in turn be used to obtain\ntighter robustness certificates. We also present an intuitive way to visualize\ndifferent relaxation-based certification methods. By approximating multiple\nnon-linearities jointly instead of separately, the k-ReLU method is able to\nbypass the convex barrier imposed by single neuron relaxations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 11:59:27 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Roth", "Kevin", ""]]}, {"id": "2106.03271", "submitter": "Sofia Reis M.D.", "authors": "Sofia Reis, Rui Abreu, Luis Cruz", "title": "Fixing Vulnerabilities Potentially Hinders Maintainability", "comments": "Accepted at the Empirical Software Engineering Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Security is a requirement of utmost importance to produce high-quality\nsoftware. However, there is still a considerable amount of vulnerabilities\nbeing discovered and fixed almost weekly. We hypothesize that developers affect\nthe maintainability of their codebases when patching vulnerabilities. This\npaper evaluates the impact of patches to improve security on the\nmaintainability of open-source software. Maintainability is measured based on\nthe Better Code Hub's model of 10 guidelines on a dataset, including 1300\nsecurity-related commits. Results show evidence of a trade-off between security\nand maintainability for 41.90% of the cases, i.e., developers may hinder\nsoftware maintainability. Our analysis shows that 38.29% of patches increased\nsoftware complexity and 37.87% of patches increased the percentage of LOCs per\nunit. The implications of our study are that changes to codebases while\npatching vulnerabilities need to be performed with extra care; tools for patch\nrisk assessment should be integrated into the CI/CD pipeline; computer science\ncurricula needs to be updated; and, more secure programming languages are\nnecessary.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 23:08:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Reis", "Sofia", ""], ["Abreu", "Rui", ""], ["Cruz", "Luis", ""]]}, {"id": "2106.03328", "submitter": "Ramy E. Ali", "authors": "Jinhyun So, Ramy E. Ali, Basak Guler, Jiantao Jiao, Salman Avestimehr", "title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure aggregation is a critical component in federated learning, which\nenables the server to learn the aggregate model of the users without observing\ntheir local models. Conventionally, secure aggregation algorithms focus only on\nensuring the privacy of individual users in a single training round. We contend\nthat such designs can lead to significant privacy leakages over multiple\ntraining rounds, due to partial user selection/participation at each round of\nfederated learning. In fact, we empirically show that the conventional random\nuser selection strategies for federated learning lead to leaking users'\nindividual models within number of rounds linear in the number of users. To\naddress this challenge, we introduce a secure aggregation framework with\nmulti-round privacy guarantees. In particular, we introduce a new metric to\nquantify the privacy guarantees of federated learning over multiple training\nrounds, and develop a structured user selection strategy that guarantees the\nlong-term privacy of each user (over any number of training rounds). Our\nframework also carefully accounts for the fairness and the average number of\nparticipating users at each round. We perform several experiments on MNIST and\nCIFAR-10 datasets in the IID and the non-IID settings to demonstrate the\nperformance improvement over the baseline algorithms, both in terms of privacy\nprotection and test accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 04:14:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["So", "Jinhyun", ""], ["Ali", "Ramy E.", ""], ["Guler", "Basak", ""], ["Jiao", "Jiantao", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2106.03346", "submitter": "Yannic Noller", "authors": "Yannic Noller, Saeid Tizpaz-Niari", "title": "QFuzz: Quantitative Fuzzing for Side Channels", "comments": null, "journal-ref": "ACM SIGSOFT International Symposium on Software Testing and\n  Analysis (ISSTA'21), July 11-17, 2021, Virtual, Denmark", "doi": "10.1145/3460319.3464817", "report-no": null, "categories": "cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Side channels pose a significant threat to the confidentiality of software\nsystems. Such vulnerabilities are challenging to detect and evaluate because\nthey arise from non-functional properties of software such as execution times\nand require reasoning on multiple execution traces. Recently, noninterference\nnotions have been adapted in static analysis, symbolic execution, and greybox\nfuzzing techniques. However, noninterference is a strict notion and may reject\nsecurity even if the strength of information leaks are weak. A quantitative\nnotion of security allows for the relaxation of noninterference and tolerates\nsmall (unavoidable) leaks. Despite progress in recent years, the existing\nquantitative approaches have scalability limitations in practice. In this work,\nwe present QFuzz, a greybox fuzzing technique to quantitatively evaluate the\nstrength of side channels with a focus on min entropy. Min entropy is a measure\nbased on the number of distinguishable observations (partitions) to assess the\nresulting threat from an attacker who tries to compromise secrets in one try.\nWe develop a novel greybox fuzzing equipped with two partitioning algorithms\nthat try to maximize the number of distinguishable observations and the cost\ndifferences between them. We evaluate QFuzz on a large set of benchmarks from\nexisting work and real-world libraries (with a total of 70 subjects). QFuzz\ncompares favorably to three state-of-the-art detection techniques. QFuzz\nprovides quantitative information about leaks beyond the capabilities of all\nthree techniques. Crucially, we compare QFuzz to a state-of-the-art\nquantification tool and find that QFuzz significantly outperforms the tool in\nscalability while maintaining similar precision. Overall, we find that our\napproach scales well for real-world applications and provides useful\ninformation to evaluate resulting threats. Additionally, QFuzz identifies a\nzero-d...\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 05:25:45 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 05:35:52 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 02:46:14 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Noller", "Yannic", ""], ["Tizpaz-Niari", "Saeid", ""]]}, {"id": "2106.03408", "submitter": "Ilya Mironov", "authors": "Mani Malek, Ilya Mironov, Karthik Prasad, Igor Shilov, Florian\n  Tram\\`er", "title": "Antipodes of Label Differential Privacy: PATE and ALIBI", "comments": "Code for implementation of algorithms and memorization attacks is\n  available from https://github.com/facebookresearch/label_dp_antipodes under\n  MIT license", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the privacy-preserving machine learning (ML) setting where the\ntrained model must satisfy differential privacy (DP) with respect to the labels\nof the training examples. We propose two novel approaches based on,\nrespectively, the Laplace mechanism and the PATE framework, and demonstrate\ntheir effectiveness on standard benchmarks.\n  While recent work by Ghazi et al. proposed Label DP schemes based on a\nrandomized response mechanism, we argue that additive Laplace noise coupled\nwith Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover,\nwe show how to achieve very strong privacy levels in some regimes, with our\nadaptation of the PATE framework that builds on recent advances in\nsemi-supervised learning.\n  We complement theoretical analysis of our algorithms' privacy guarantees with\nempirical evaluation of their memorization properties. Our evaluation suggests\nthat comparing different algorithms according to their provable DP guarantees\ncan be misleading and favor a less private algorithm with a tighter analysis.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:14:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Malek", "Mani", ""], ["Mironov", "Ilya", ""], ["Prasad", "Karthik", ""], ["Shilov", "Igor", ""], ["Tram\u00e8r", "Florian", ""]]}, {"id": "2106.03470", "submitter": "Daniel Weber", "authors": "Daniel Weber, Ahmad Ibrahim, Hamed Nemati, Michael Schwarz, Christian\n  Rossow (CISPA Helmholtz Center for Information Security)", "title": "Osiris: Automated Discovery of Microarchitectural Side Channels", "comments": "Will be published at USENIX Security'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years, a series of side channels have been discovered on CPUs.\nThese side channels have been used in powerful attacks, e.g., on cryptographic\nimplementations, or as building blocks in transient-execution attacks such as\nSpectre or Meltdown. However, in many cases, discovering side channels is still\na tedious manual process.\n  In this paper, we present Osiris, a fuzzing-based framework to automatically\ndiscover microarchitectural side channels. Based on a machine-readable\nspecification of a CPU's ISA, Osiris generates instruction-sequence triples and\nautomatically tests whether they form a timing-based side channel. Furthermore,\nOsiris evaluates their usability as a side channel in transient-execution\nattacks, i.e., as the microarchitectural encoding for attacks like Spectre. In\ntotal, we discover four novel timing-based side channels on Intel and AMD CPUs.\nBased on these side channels, we demonstrate exploitation in three case\nstudies. We show that our microarchitectural KASLR break using non-temporal\nloads, FlushConflict, even works on the new Intel Ice Lake and Comet Lake\nmicroarchitectures. We present a cross-core cross-VM covert channel that is not\nrelying on the memory subsystem and transmits up to 1 kbit/s. We demonstrate\nthis channel on the AWS cloud, showing that it is stealthy and noise resistant.\nFinally, we demonstrate Stream+Reload, a covert channel for transient-execution\nattacks that, on average, allows leaking 7.83 bytes within a transient window,\nimproving state-of-the-art attacks that only leak up to 3 bytes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:54:20 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Weber", "Daniel", "", "CISPA Helmholtz Center for Information Security"], ["Ibrahim", "Ahmad", "", "CISPA Helmholtz Center for Information Security"], ["Nemati", "Hamed", "", "CISPA Helmholtz Center for Information Security"], ["Schwarz", "Michael", "", "CISPA Helmholtz Center for Information Security"], ["Rossow", "Christian", "", "CISPA Helmholtz Center for Information Security"]]}, {"id": "2106.03626", "submitter": "Miguel Grilo", "authors": "Miguel Grilo, Jo\\~ao F. Ferreira and Jos\\'e Bacelar Almeida", "title": "Towards Formal Verification of Password Generation Algorithms used in\n  Password Managers", "comments": "shortpaper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Password managers are important tools that enable us to use stronger\npasswords, freeing us from the cognitive burden of remembering them. Despite\nthis, there are still many users who do not fully trust password managers. In\nthis paper, we focus on a feature that most password managers offer that might\nimpact the user's trust, which is the process of generating a random password.\nWe survey which algorithms are most commonly used and we propose a solution for\na formally verified reference implementation of a password generation\nalgorithm. We use EasyCrypt as our framework to both specify the reference\nimplementation and to prove its functional correctness and security.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:57:07 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 14:43:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Grilo", "Miguel", ""], ["Ferreira", "Jo\u00e3o F.", ""], ["Almeida", "Jos\u00e9 Bacelar", ""]]}, {"id": "2106.03645", "submitter": "Ruben Ohana", "authors": "Ruben Ohana, Hamlet J. Medina Ruiz, Julien Launay, Alessandro\n  Cappelli, Iacopo Poli, Liva Ralaivola, Alain Rakotomamonjy", "title": "Photonic Differential Privacy with Direct Feedback Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Processing Units (OPUs) -- low-power photonic chips dedicated to\nlarge scale random projections -- have been used in previous work to train deep\nneural networks using Direct Feedback Alignment (DFA), an effective alternative\nto backpropagation. Here, we demonstrate how to leverage the intrinsic noise of\noptical random projections to build a differentially private DFA mechanism,\nmaking OPUs a solution of choice to provide a private-by-design training. We\nprovide a theoretical analysis of our adaptive privacy mechanism, carefully\nmeasuring how the noise of optical random projections propagates in the process\nand gives rise to provable Differential Privacy. Finally, we conduct\nexperiments demonstrating the ability of our learning procedure to achieve\nsolid end-task performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:18:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ohana", "Ruben", ""], ["Ruiz", "Hamlet J. Medina", ""], ["Launay", "Julien", ""], ["Cappelli", "Alessandro", ""], ["Poli", "Iacopo", ""], ["Ralaivola", "Liva", ""], ["Rakotomamonjy", "Alain", ""]]}, {"id": "2106.03699", "submitter": "Anshuman Suri", "authors": "Anshuman Suri and David Evans", "title": "Formalizing Distribution Inference Risks", "comments": "ICML 2021 Workshop on Theory and Practice of Differential Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property inference attacks reveal statistical properties about a training set\nbut are difficult to distinguish from the primary purposes of statistical\nmachine learning, which is to produce models that capture statistical\nproperties about a distribution. Motivated by Yeom et al.'s membership\ninference framework, we propose a formal and generic definition of property\ninference attacks. The proposed notion describes attacks that can distinguish\nbetween possible training distributions, extending beyond previous property\ninference attacks that infer the ratio of a particular type of data in the\ntraining data set. In this paper, we show how our definition captures previous\nproperty inference attacks as well as a new attack that reveals the average\ndegree of nodes of a training graph and report on experiments giving insight\ninto the potential risks of property inference attacks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:10:06 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 21:07:57 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Suri", "Anshuman", ""], ["Evans", "David", ""]]}, {"id": "2106.03785", "submitter": "Zuobin Xiong", "authors": "Zhipeng Cai, Zuobin Xiong, Honghui Xu, Peng Wang, Wei Li, Yi Pan", "title": "Generative Adversarial Networks: A Survey Towards Private and Secure\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GAN) have promoted a variety of applications\nin computer vision, natural language processing, etc. due to its generative\nmodel's compelling ability to generate realistic examples plausibly drawn from\nan existing distribution of samples. GAN not only provides impressive\nperformance on data generation-based tasks but also stimulates fertilization\nfor privacy and security oriented research because of its game theoretic\noptimization strategy. Unfortunately, there are no comprehensive surveys on GAN\nin privacy and security, which motivates this survey paper to summarize those\nstate-of-the-art works systematically. The existing works are classified into\nproper categories based on privacy and security functions, and this survey\npaper conducts a comprehensive analysis of their advantages and drawbacks.\nConsidering that GAN in privacy and security is still at a very initial stage\nand has imposed unique challenges that are yet to be well addressed, this paper\nalso sheds light on some potential privacy and security applications with GAN\nand elaborates on some future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:47:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cai", "Zhipeng", ""], ["Xiong", "Zuobin", ""], ["Xu", "Honghui", ""], ["Wang", "Peng", ""], ["Li", "Wei", ""], ["Pan", "Yi", ""]]}, {"id": "2106.03815", "submitter": "Michael Ampatzis", "authors": "Michael Ampatzis and Theodore Andronikos", "title": "QKD based on symmetric entangled Bernstein-Vazirani", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": "10.3390/e23070870", "report-no": null, "categories": "quant-ph cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a novel entanglement-based QKD protocol, that makes use\nof a modified symmetric version of the Bernstein-Vazirani algorithm, in order\nto achieve a secure and efficient key distribution. Two variants of the\nprotocol, one fully symmetric and one semi-symmetric, are presented. In both\ncases, the spatially separated Alice and Bob share multiple EPR pairs, one\nqubit of the pair each. The fully symmetric version allows both parties to\ninput a secret key from the irrespective location and, finally, acquire in the\nend a totally new and original key, an idea which was inspired by the\nDiffie-Hellman key exchange protocol. In the semi-symmetric version, Alice\nsends her chosen secret key to Bob (or vice versa). Furthermore, their\nperformance against an eavesdropper's attack is analyzed. Finally, in order to\nillustrate the operation of the protocols in practice, two small scale but\ndetailed examples are given.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:24:45 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ampatzis", "Michael", ""], ["Andronikos", "Theodore", ""]]}, {"id": "2106.04030", "submitter": "Zhiyi Zhang", "authors": "Zhiyi Zhang, Siqi Liu, Randy King, Lixia Zhang", "title": "Supporting Multiparty Signing over Named Data Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern digitally controlled systems require multiparty authentication and\nauthorization to meet the desired security requirement. This paper describes\nthe design and development of NDN-MPS, an automated solution to support\nmultiparty signature signing and verification for NDN-enabled applications.\nNDN-MPS suggests several changes and extensions to the existing NDN security\nsolutions. First, it introduces a new type of trust schema to support signing\nand verification for multiple signers under complex policies such as threshold\nschemes. Second, it extends the NDN signature format to accommodate\nmultisignature schemes such as BLS signature. Third, it introduces a signature\ncollection protocol to solicit signatures securely from multiple signers. We\nfurther evaluate NDN-MPS by assessing its security properties and measuring its\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 00:49:34 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhang", "Zhiyi", ""], ["Liu", "Siqi", ""], ["King", "Randy", ""], ["Zhang", "Lixia", ""]]}, {"id": "2106.04119", "submitter": "Christian Wressnegger", "authors": "Niclas K\\\"uhnapfel, Stefan Preu{\\ss}ler, Maximilian Noppel, Thomas\n  Schneider, Konrad Rieck, and Christian Wressnegger", "title": "LaserShark: Establishing Fast, Bidirectional Communication into\n  Air-Gapped Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical isolation, so called air-gapping, is an effective method for\nprotecting security-critical computers and networks. While it might be possible\nto introduce malicious code through the supply chain, insider attacks, or\nsocial engineering, communicating with the outside world is prevented.\nDifferent approaches to breach this essential line of defense have been\ndeveloped based on electromagnetic, acoustic, and optical communication\nchannels. However, all of these approaches are limited in either data rate or\ndistance, and frequently offer only exfiltration of data. We present a novel\napproach to infiltrate data to and exfiltrate data from air-gapped systems\nwithout any additional hardware on-site. By aiming lasers at already built-in\nLEDs and recording their response, we are the first to enable a long-distance\n(25m), bidirectional, and fast (18.2kbps in & 100kbps out) covert communication\nchannel. The approach can be used against any office device that operates LEDs\nat the CPU's GPIO interface.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:10:23 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 14:11:09 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["K\u00fchnapfel", "Niclas", ""], ["Preu\u00dfler", "Stefan", ""], ["Noppel", "Maximilian", ""], ["Schneider", "Thomas", ""], ["Rieck", "Konrad", ""], ["Wressnegger", "Christian", ""]]}, {"id": "2106.04247", "submitter": "Pasin Manurangsi", "authors": "Badih Ghazi, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh", "title": "Private Counting from Anonymous Messages: Near-Optimal Accuracy with\n  Vanishing Communication Overhead", "comments": "Originally appeared in ICML'20. This version contains a correction of\n  calculation errors in Theorem 13 of the ICML'20 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy (DP) is a formal notion for quantifying the privacy loss\nof algorithms. Algorithms in the central model of DP achieve high accuracy but\nmake the strongest trust assumptions whereas those in the local DP model make\nthe weakest trust assumptions but incur substantial accuracy loss. The shuffled\nDP model (Bittau et al., 2017; Erlingsson et al., 2019; Cheu et al., 2019) has\nrecently emerged as a feasible middle ground between the central and local\nmodels, providing stronger trust assumptions than the former while promising\nhigher accuracies than the latter. In this paper, we obtain practical\ncommunication-efficient algorithms in the shuffled DP model for two basic\naggregation primitives used in machine learning: 1) binary summation, and 2)\nhistograms over a moderate number of buckets. Our algorithms achieve accuracy\nthat is arbitrarily close to that of central DP algorithms with an expected\ncommunication per user essentially matching what is needed without any privacy\nconstraints! We demonstrate the practicality of our algorithms by\nexperimentally comparing their performance to several widely-used protocols\nsuch as Randomized Response (Warner, 1965) and RAPPOR (Erlingsson et al.,\n2014).\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 10:51:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ghazi", "Badih", ""], ["Kumar", "Ravi", ""], ["Manurangsi", "Pasin", ""], ["Pagh", "Rasmus", ""]]}, {"id": "2106.04267", "submitter": "Stefan Rass", "authors": "Stefan Rass, Sandra K\\\"onig, Jasmin Wachter, Manuel Egger, Manuel\n  Hobisch", "title": "Supervised Machine Learning with Plausible Deniability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the question of how well machine learning (ML) models trained on a\ncertain data set provide privacy for the training data, or equivalently,\nwhether it is possible to reverse-engineer the training data from a given ML\nmodel. While this is easy to answer negatively in the most general case, it is\ninteresting to note that the protection extends over non-recoverability towards\nplausible deniability: Given an ML model $f$, we show that one can take a set\nof purely random training data, and from this define a suitable ``learning\nrule'' that will produce a ML model that is exactly $f$. Thus, any speculation\nabout which data has been used to train $f$ is deniable upon the claim that any\nother data could have led to the same results. We corroborate our theoretical\nfinding with practical examples, and open source implementations of how to find\nthe learning rules for a chosen set of raining data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:54:51 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Rass", "Stefan", ""], ["K\u00f6nig", "Sandra", ""], ["Wachter", "Jasmin", ""], ["Egger", "Manuel", ""], ["Hobisch", "Manuel", ""]]}, {"id": "2106.04405", "submitter": "Vasileios Perifanis", "authors": "Vasileios Perifanis and Pavlos S. Efraimidis", "title": "Federated Neural Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a federated version of the state-of-the-art Neural\nCollaborative Filtering (NCF) approach for item recommendations. The system,\nnamed FedNCF, allows learning without requiring users to expose or transmit\ntheir raw data. Experimental validation shows that FedNCF achieves comparable\nrecommendation quality to the original NCF system. Although federated learning\n(FL) enables learning without raw data transmission, recent attacks showed that\nFL alone does not eliminate privacy concerns. To overcome this challenge, we\nintegrate a privacy-preserving enhancement with a secure aggregation scheme\nthat satisfies the security requirements against an honest-but-curious (HBC)\nentity, without affecting the quality of the original model. Finally, we\ndiscuss the peculiarities observed in the application of FL in a collaborative\nfiltering (CF) task as well as we evaluate the privacy-preserving mechanism in\nterms of computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 21:05:41 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Perifanis", "Vasileios", ""], ["Efraimidis", "Pavlos S.", ""]]}, {"id": "2106.04435", "submitter": "Netanel Raviv", "authors": "Netanel Raviv, Aidan Kelley, Michael Guo, Yevgeny Vorobeychik", "title": "Enhancing Robustness of Neural Networks through Fourier Stabilization", "comments": "Full version of an ICML 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the considerable success of neural networks in security settings such\nas malware detection, such models have proved vulnerable to evasion attacks, in\nwhich attackers make slight changes to inputs (e.g., malware) to bypass\ndetection. We propose a novel approach, \\emph{Fourier stabilization}, for\ndesigning evasion-robust neural networks with binary inputs. This approach,\nwhich is complementary to other forms of defense, replaces the weights of\nindividual neurons with robust analogs derived using Fourier analytic tools.\nThe choice of which neurons to stabilize in a neural network is then a\ncombinatorial optimization problem, and we propose several methods for\napproximately solving it. We provide a formal bound on the per-neuron drop in\naccuracy due to Fourier stabilization, and experimentally demonstrate the\neffectiveness of the proposed approach in boosting robustness of neural\nnetworks in several detection settings. Moreover, we show that our approach\neffectively composes with adversarial training.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:12:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Raviv", "Netanel", ""], ["Kelley", "Aidan", ""], ["Guo", "Michael", ""], ["Vorobeychik", "Yevgeny", ""]]}, {"id": "2106.04468", "submitter": "Caner Goztepe", "authors": "Caner Goztepe, Saliha Buyukcorak, Gunes Karabulut Kurt, Halim\n  Yanikomeroglu", "title": "Localization Threats in Next-Generation Wireless Networks", "comments": "7 pages, 5 figures, 1 table, 15 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of localization systems in our daily lives is increasing. As\nnext-generation networks will introduce hyperconnectivity with the emerging\napplications, this impact will undoubtedly further increase, proliferating the\nimportance of the location information's reliability. As society becomes more\ndependent on this information in terms of the products and services, security\nsolutions will have to be enriched to provide countermeasures sufficiently\nadvanced to ever-evolving threats, forcing the joint design of communication\nand localization systems. This paper envisions integrated communication and\nlocalization systems by focusing on localization security. Also, conventional\nand next-generation attacks on localization are discussed along with an\nefficient attack detection method and test-bed-based demonstration,\nhighlighting the need for effective countermeasures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:53:10 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Goztepe", "Caner", ""], ["Buyukcorak", "Saliha", ""], ["Kurt", "Gunes Karabulut", ""], ["Yanikomeroglu", "Halim", ""]]}, {"id": "2106.04511", "submitter": "Deepak Kumar", "authors": "Deepak Kumar, Patrick Gage Kelley, Sunny Consolvo, Joshua Mason, Elie\n  Bursztein, Zakir Durumeric, Kurt Thomas, Michael Bailey", "title": "Designing Toxic Content Classification for a Diversity of Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we demonstrate how existing classifiers for identifying toxic\ncomments online fail to generalize to the diverse concerns of Internet users.\nWe survey 17,280 participants to understand how user expectations for what\nconstitutes toxic content differ across demographics, beliefs, and personal\nexperiences. We find that groups historically at-risk of harassment - such as\npeople who identify as LGBTQ+ or young adults - are more likely to to flag a\nrandom comment drawn from Reddit, Twitter, or 4chan as toxic, as are people who\nhave personally experienced harassment in the past. Based on our findings, we\nshow how current one-size-fits-all toxicity classification algorithms, like the\nPerspective API from Jigsaw, can improve in accuracy by 86% on average through\npersonalized model tuning. Ultimately, we highlight current pitfalls and new\ndesign directions that can improve the equity and efficacy of toxic content\nclassifiers for all users.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:45:15 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kumar", "Deepak", ""], ["Kelley", "Patrick Gage", ""], ["Consolvo", "Sunny", ""], ["Mason", "Joshua", ""], ["Bursztein", "Elie", ""], ["Durumeric", "Zakir", ""], ["Thomas", "Kurt", ""], ["Bailey", "Michael", ""]]}, {"id": "2106.04575", "submitter": "Md Amiruzzaman", "authors": "Hassnain ul hassan, Rizal Mohd Nor, Md Amiruzzaman, Sharyar Wani", "title": "DNS attack mitigation Using OpenStack Isolation", "comments": "6 pages, 3 figures, and 2 tables", "journal-ref": "2020 IEEE 7th International Conference on Engineering Technologies\n  and Applied Sciences (ICETAS)", "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Domain Name System (DNS) is essential for the Internet, giving a\nmechanism to resolve hostnames into Internet Protocol (IP) addresses. DNS is\nknown as the world's largest distributed database that manages hostnames and\nInternet Protocol. By having the DNS, only simple names that can be easily\nmemorized will be used and then the domain name system will map it into the\nnumeric Internet Protocol addresses that are used by computers to communicate.\nThis research aims to propose a model for the development of a private cloud\ninfrastructure to host DNS. The cloud infrastructure will be created using the\nOpenStack software platform where each server will be hosted separately in a\ndifferent virtual machine. Virtual network architecture will be created using\nthe Software Defined Networking (SDN) approach and it will be secured using\nFirewall as a Service (FWaaS). By hosting DNS in private cloud infrastructure,\nthe DNS servers will be out of reach by attackers which will prevent DNS\nattacks. Besides, available research had proven that the cloud is the best\nchoice for DNS. A prototype had been implemented and evaluated for its\nefficiencies. The findings from the evaluation carried out shown a positive\nresult.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 20:17:24 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 02:18:05 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["hassan", "Hassnain ul", ""], ["Nor", "Rizal Mohd", ""], ["Amiruzzaman", "Md", ""], ["Wani", "Sharyar", ""]]}, {"id": "2106.04590", "submitter": "Seng Pei Liew", "authors": "Seng Pei Liew, Tsubasa Takahashi, Michihiko Ueno", "title": "PEARL: Data Synthesis via Private Embeddings and Adversarial\n  Reconstruction Learning", "comments": "23 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new framework of synthesizing data using deep generative models\nin a differentially private manner. Within our framework, sensitive data are\nsanitized with rigorous privacy guarantees in a one-shot fashion, such that\ntraining deep generative models is possible without re-using the original data.\nHence, no extra privacy costs or model constraints are incurred, in contrast to\npopular approaches such as Differentially Private Stochastic Gradient Descent\n(DP-SGD), which, among other issues, causes degradation in privacy guarantees\nas the training iteration increases. We demonstrate a realization of our\nframework by making use of the characteristic function and an adversarial\nre-weighting objective, which are of independent interest as well. Our proposal\nhas theoretical guarantees of performance, and empirical evaluations on\nmultiple datasets show that our approach outperforms other methods at\nreasonable levels of privacy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:00:01 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liew", "Seng Pei", ""], ["Takahashi", "Tsubasa", ""], ["Ueno", "Michihiko", ""]]}, {"id": "2106.04690", "submitter": "Sanghyun Hong", "authors": "Sanghyun Hong, Nicholas Carlini, Alexey Kurakin", "title": "Handcrafted Backdoors in Deep Neural Networks", "comments": "16 pages, 13 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs), while accurate, are expensive to train. Many\npractitioners, therefore, outsource the training process to third parties or\nuse pre-trained DNNs. This practice makes DNNs vulnerable to $backdoor$\n$attacks$: the third party who trains the model may act maliciously to inject\nhidden behaviors into the otherwise accurate model. Until now, the mechanism to\ninject backdoors has been limited to $poisoning$.\n  We argue that such a supply-chain attacker has more attack techniques\navailable. To study this hypothesis, we introduce a handcrafted attack that\ndirectly manipulates the parameters of a pre-trained model to inject backdoors.\nOur handcrafted attacker has more degrees of freedom in manipulating model\nparameters than poisoning. This makes it difficult for a defender to identify\nor remove the manipulations with straightforward methods, such as statistical\nanalysis, adding random noises to model parameters, or clipping their values\nwithin a certain range. Further, our attacker can combine the handcrafting\nprocess with additional techniques, $e.g.$, jointly optimizing a trigger\npattern, to inject backdoors into complex networks effectively$-$the\nmeet-in-the-middle attack.\n  In evaluations, our handcrafted backdoors remain effective across four\ndatasets and four network architectures with a success rate above 96%. Our\nbackdoored models are resilient to both parameter-level backdoor removal\ntechniques and can evade existing defenses by slightly changing the backdoor\nattack configurations. Moreover, we demonstrate the feasibility of suppressing\nunwanted behaviors otherwise caused by poisoning. Our results suggest that\nfurther research is needed for understanding the complete space of supply-chain\nbackdoor attacks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 20:58:23 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Hong", "Sanghyun", ""], ["Carlini", "Nicholas", ""], ["Kurakin", "Alexey", ""]]}, {"id": "2106.04720", "submitter": "Farhan Sadique", "authors": "Farhan Sadique, Shamik Sengupta", "title": "Analysis of Attacker Behavior in Compromised Hosts During Command and\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Traditional reactive approach of blacklisting botnets fails to adapt to the\nrapidly evolving landscape of cyberattacks. An automated and proactive approach\nto detect and block botnet hosts will immensely benefit the industry.\nBehavioral analysis of botnet is shown to be effective against a wide variety\nof attack types. Current works, however, focus solely on analyzing network\ntraffic from and to the bots. In this work we take a different approach of\nanalyzing the chain of commands input by attackers in a compromised host. We\nhave deployed several honeypots to simulate Linux shells and allowed attackers\naccess to the shells to collect a large dataset of commands. We have further\ndeveloped an automated mechanism to analyze these data. For the automation we\nhave developed a system called CYbersecurity information Exchange with Privacy\n(CYBEX-P). Finally, we have done a sequential analysis on the dataset to show\nthat we can successfully predict attacker behavior from the shell commands\nwithout analyzing network traffic like previous works.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 22:31:12 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sadique", "Farhan", ""], ["Sengupta", "Shamik", ""]]}, {"id": "2106.04777", "submitter": "Everton Lira", "authors": "Everton R. Lira, Heverton B. de Mac\\^edo, Danielli A. Lima, Leonardo\n  Alt, Gina M. B. Oliveira", "title": "A reversible system based on hybrid toggle radius-4 cellular automata\n  and its application as a block cipher", "comments": "34 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The dynamical system described herein uses a hybrid cellular automata (CA)\nmechanism to attain reversibility, and this approach is adapted to create a\nnovel block cipher algorithm called HCA. CA are widely used for modeling\ncomplex systems and employ an inherently parallel model. Therefore,\napplications derived from CA have a tendency to fit very well in the current\ncomputational paradigm where scalability and multi-threading potential are\nquite desirable characteristics. HCA model has recently received a patent by\nthe Brazilian agency INPI. Several evaluations and analyses performed on the\nmodel are presented here, such as theoretical discussions related to its\nreversibility and an analysis based on graph theory, which reduces HCA security\nto the well-known Hamiltonian cycle problem that belongs to the NP-complete\nclass. Finally, the cryptographic robustness of HCA is empirically evaluated\nthrough several tests, including avalanche property compliance and the NIST\nrandomness suite.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 02:52:05 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lira", "Everton R.", ""], ["de Mac\u00eado", "Heverton B.", ""], ["Lima", "Danielli A.", ""], ["Alt", "Leonardo", ""], ["Oliveira", "Gina M. B.", ""]]}, {"id": "2106.04808", "submitter": "Shantanu Pal", "authors": "Shantanu Pal, Ali Dorri, Raja Jurdak", "title": "Blockchain for IoT Access Control: Recent Trends and Future Research\n  Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of wireless sensor networks, smart devices, and\ntraditional information and communication technologies, there is tremendous\ngrowth in the use of Internet of Things (IoT) applications and services in our\neveryday life. IoT systems deal with high volumes of data. This data can be\nparticularly sensitive, as it may include health, financial, location, and\nother highly personal information. Fine-grained security management in IoT\ndemands effective access control. Several proposals discuss access control for\nthe IoT, however, a limited focus is given to the emerging blockchain-based\nsolutions for IoT access control. In this paper, we review the recent trends\nand critical needs for blockchain-based solutions for IoT access control. We\nidentify several important aspects of blockchain, including decentralised\ncontrol, secure storage and sharing information in a trustless manner, for IoT\naccess control including their benefits and limitations. Finally, we note some\nfuture research directions on how to converge blockchain in IoT access control\nefficiently and effectively.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 04:53:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pal", "Shantanu", ""], ["Dorri", "Ali", ""], ["Jurdak", "Raja", ""]]}, {"id": "2106.04811", "submitter": "Jem Guhit", "authors": "Jem Guhit, Edward Colone, Shawn McKee, Kris Steinhoff, and Katarina\n  Thomas", "title": "Benchmarking NetBASILISK: a Network Security Project for Science", "comments": "12 pages, 4 figures, presented at vCHEP '21 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infrastructures supporting distributed scientific collaborations must address\ncompeting goals in both providing high-performance access to resources while\nsimultaneously securing the infrastructure against security threats. The\nNetBASILISK project is attempting to improve the security of such\ninfrastructures while not adversely impacting their performance. This paper\nwill present our work to create a benchmark and monitoring infrastructure that\nallows us to test for any degradation in transferring data into a NetBASILISK\nprotected site.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 05:08:26 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Guhit", "Jem", ""], ["Colone", "Edward", ""], ["McKee", "Shawn", ""], ["Steinhoff", "Kris", ""], ["Thomas", "Katarina", ""]]}, {"id": "2106.04826", "submitter": "Kohei Suenaga", "authors": "Sota Sato and Ryotaro Banno and Jun Furuse and Kohei Suenaga and\n  Atsushi Igarashi", "title": "Verification of a Merkle Patricia Tree Library Using F*", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Merkle tree is a data structure for representing a key-value store as a\ntree. Each node of a Merkle tree is equipped with a hash value computed from\nthose of their descendants. A Merkle tree is often used for representing a\nstate of a blockchain system since it can be used for efficiently auditing the\nstate in a trustless manner. Due to the safety-critical nature of blockchains,\nensuring the correctness of their implementation is paramount.\n  We show our formally verified implementation of the core part of Plebeia\nusing F*. Plebeia is a library to manipulate an extension of Merkle trees\n(called Plebeia trees). It is being implemented as a part of the storage system\nof the Tezos blockchain system. To this end, we gradually ported Plebeia to F*;\nthe OCaml code extracted from the modules ported to F* is linked with the\nunverified part of Plebeia. By this gradual porting process, we can obtain a\nworking code from our partially verified implementation of Plebeia; we\nconfirmed that the binary passes all the unit tests of Plebeia.\n  More specifically, we verified the following properties on the implementation\nof Plebeia: (1) Each tree-manipulating function preserves the invariants on the\ndata structure of a Plebeia tree and satisfies the functional requirements as a\nnested key-value store; (2) Each function for serializing/deserializing a\nPlebeia tree to/from the low-level storage is implemented correctly; and (3)\nThe hash function for a Plebeia tree is relatively collision-resistant with\nrespect to the cryptographic safety of the blake2b hash function. During\nporting Plebeia to F*, we found a bug in an old version of Plebeia, which was\noverlooked by the tests bundled with the original implementation. To the best\nof our knowledge, this is the first work that verifies a production-level\nimplementation of a Merkle-tree library by F*.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:18:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sato", "Sota", ""], ["Banno", "Ryotaro", ""], ["Furuse", "Jun", ""], ["Suenaga", "Kohei", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "2106.04876", "submitter": "Eliya Nachmani", "authors": "Itamar Zimerman, Eliya Nachmani, Lior Wolf", "title": "Recovering AES Keys with a Deep Cold Boot Attack", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cold boot attacks inspect the corrupted random access memory soon after the\npower has been shut down. While most of the bits have been corrupted, many\nbits, at random locations, have not. Since the keys in many encryption schemes\nare being expanded in memory into longer keys with fixed redundancies, the keys\ncan often be restored. In this work, we combine a novel cryptographic variant\nof a deep error correcting code technique with a modified SAT solver scheme to\napply the attack on AES keys. Even though AES consists of Rijndael S-box\nelements, that are specifically designed to be resistant to linear and\ndifferential cryptanalysis, our method provides a novel formalization of the\nAES key scheduling as a computational graph, which is implemented by a neural\nmessage passing network. Our results show that our methods outperform the state\nof the art attack methods by a very large margin.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 07:57:01 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zimerman", "Itamar", ""], ["Nachmani", "Eliya", ""], ["Wolf", "Lior", ""]]}, {"id": "2106.04885", "submitter": "Shantanu Pal", "authors": "Shantanu Pal, Ambrose Hill, Tahiry Rabehaja, Michael Hitchens", "title": "A Blockchain-Based Trust Management Framework with Verifiable\n  Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been tremendous interest in the development of formal trust models\nand metrics through the use of analytics (e.g., Belief Theory and Bayesian\nmodels), logics (e.g., Epistemic and Subjective Logic) and other mathematical\nmodels. The choice of trust metric will depend on context, circumstance and\nuser requirements and there is no single best metric for use in all\ncircumstances. Where different users require different trust metrics to be\nemployed the trust score calculations should still be based on all available\ntrust evidence. Trust is normally computed using past experiences but, in\npractice (especially in centralised systems), the validity and accuracy of\nthese experiences are taken for granted. In this paper, we provide a formal\nframework and practical blockchain-based implementation that allows independent\ntrust providers to implement different trust metrics in a distributed manner\nwhile still allowing all trust providers to base their calculations on a common\nset of trust evidence. Further, our design allows experiences to be provably\nlinked to interactions without the need for a central authority. This leads to\nthe notion of evidence-based trust with provable interactions. Leveraging\nblockchain allows the trust providers to offer their services in a competitive\nmanner, charging fees while users are provided with payments for recording\nexperiences. Performance details of the blockchain implementation are provided.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:10:03 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pal", "Shantanu", ""], ["Hill", "Ambrose", ""], ["Rabehaja", "Tahiry", ""], ["Hitchens", "Michael", ""]]}, {"id": "2106.04907", "submitter": "Lars Almon", "authors": "Mikhail Fomichev, Julia Hesse, Lars Almon, Timm Lippert, Jun Han,\n  Matthias Hollick", "title": "FastZIP: Faster and More Secure Zero-Interaction Pairing", "comments": "ACM MobiSys '21 - Code and data at:\n  https://github.com/seemoo-lab/fastzip", "journal-ref": null, "doi": "10.1145/3458864.3467883", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the Internet of Things (IoT), establishing a secure\nchannel between smart devices becomes crucial. Recent research proposes\nzero-interaction pairing (ZIP), which enables pairing without user assistance\nby utilizing devices' physical context (e.g., ambient audio) to obtain a shared\nsecret key. The state-of-the-art ZIP schemes suffer from three limitations: (1)\nprolonged pairing time (i.e., minutes or hours), (2) vulnerability to\nbrute-force offline attacks on a shared key, and (3) susceptibility to attacks\ncaused by predictable context (e.g., replay attack) because they rely on\nlimited entropy of physical context to protect a shared key. We address these\nlimitations, proposing FastZIP, a novel ZIP scheme that significantly reduces\npairing time while preventing offline and predictable context attacks. In\nparticular, we adapt a recently introduced Fuzzy Password-Authenticated Key\nExchange (fPAKE) protocol and utilize sensor fusion, maximizing their\nadvantages. We instantiate FastZIP for intra-car device pairing to demonstrate\nits feasibility and show how the design of FastZIP can be adapted to other ZIP\nuse cases. We implement FastZIP and evaluate it by driving four cars for a\ntotal of 800 km. We achieve up to three times shorter pairing time compared to\nthe state-of-the-art ZIP schemes while assuring robust security with\nadversarial error rates below 0.5%.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:44:46 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Fomichev", "Mikhail", ""], ["Hesse", "Julia", ""], ["Almon", "Lars", ""], ["Lippert", "Timm", ""], ["Han", "Jun", ""], ["Hollick", "Matthias", ""]]}, {"id": "2106.04938", "submitter": "Boxi Wu", "authors": "Boxi Wu, Heng Pan, Li Shen, Jindong Gu, Shuai Zhao, Zhifeng Li, Deng\n  Cai, Xiaofei He, Wei Liu", "title": "Attacking Adversarial Attacks as A Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that adversarial attacks can fool deep neural networks with\nimperceptible perturbations. Although adversarial training significantly\nimproves model robustness, failure cases of defense still broadly exist. In\nthis work, we find that the adversarial attacks can also be vulnerable to small\nperturbations. Namely, on adversarially-trained models, perturbing adversarial\nexamples with a small random noise may invalidate their misled predictions.\nAfter carefully examining state-of-the-art attacks of various kinds, we find\nthat all these attacks have this deficiency to different extents. Enlightened\nby this finding, we propose to counter attacks by crafting more effective\ndefensive perturbations. Our defensive perturbations leverage the advantage\nthat adversarial training endows the ground-truth class with smaller local\nLipschitzness. By simultaneously attacking all the classes, the misled\npredictions with larger Lipschitzness can be flipped into correct ones. We\nverify our defensive perturbation with both empirical experiments and\ntheoretical analyses on a linear model. On CIFAR10, it boosts the\nstate-of-the-art model from 66.16% to 72.66% against the four attacks of\nAutoAttack, including 71.76% to 83.30% against the Square attack. On ImageNet,\nthe top-1 robust accuracy of FastAT is improved from 33.18% to 38.54% under the\n100-step PGD attack.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 09:31:10 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wu", "Boxi", ""], ["Pan", "Heng", ""], ["Shen", "Li", ""], ["Gu", "Jindong", ""], ["Zhao", "Shuai", ""], ["Li", "Zhifeng", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""], ["Liu", "Wei", ""]]}, {"id": "2106.04951", "submitter": "Ning Xi", "authors": "Ning Xi, Chao Chen, Jun Zhang, Cong Sun, Shigang Liu, Pengbin Feng and\n  Jianfeng Ma", "title": "Information flow based defensive chain for data leakage detection and\n  prevention: a survey", "comments": "36 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile and IoT applications have greatly enriched our daily life by providing\nconvenient and intelligent services. However, these smart applications have\nbeen a prime target of adversaries for stealing sensitive data. It poses a\ncrucial threat to users' identity security, financial security, or even life\nsecurity. Research communities and industries have proposed many Information\nFlow Control (IFC) techniques for data leakage detection and prevention,\nincluding secure modeling, type system, static analysis, dynamic analysis,\n\\textit{etc}. According to the application's development life cycle, although\nmost attacks are conducted during the application's execution phase, data\nleakage vulnerabilities have been introduced since the design phase. With a\nfocus on lifecycle protection, this survey reviews the recent representative\nworks adopted in different phases. We propose an information flow based\ndefensive chain, which provides a new framework to systematically understand\nvarious IFC techniques for data leakage detection and prevention in Mobile and\nIoT applications. In line with the phases of the application life cycle, each\nreviewed work is comprehensively studied in terms of technique, performance,\nand limitation. Research challenges and future directions are also pointed out\nby consideration of the integrity of the defensive chain.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 09:51:02 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Xi", "Ning", ""], ["Chen", "Chao", ""], ["Zhang", "Jun", ""], ["Sun", "Cong", ""], ["Liu", "Shigang", ""], ["Feng", "Pengbin", ""], ["Ma", "Jianfeng", ""]]}, {"id": "2106.04968", "submitter": "Sergio L\\'opez Bernal", "authors": "Sergio L\\'opez Bernal, Alberto Huertas Celdr\\'an, Gregorio Mart\\'inez\n  P\\'erez", "title": "Eight Reasons Why Cybersecurity on Novel Generations of Brain-Computer\n  Interfaces Must Be Prioritized", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This article presents eight neural cyberattacks affecting spontaneous neural\nactivity, inspired by well-known cyberattacks from the computer science domain:\nNeural Flooding, Neural Jamming, Neural Scanning, Neural Selective Forwarding,\nNeural Spoofing, Neural Sybil, Neural Sinkhole and Neural Nonce. These\ncyberattacks are based on the exploitation of vulnerabilities existing in the\nnew generation of Brain-Computer Interfaces. After presenting their formal\ndefinitions, the cyberattacks have been implemented over a neuronal simulation.\nTo evaluate the impact of each cyberattack, they have been implemented in a\nConvolutional Neural Network (CNN) simulating a portion of a mouse's visual\ncortex. This implementation is based on existing literature indicating the\nsimilarities that CNNs have with neuronal structures from the visual cortex.\nSome conclusions are also provided, indicating that Neural Nonce and Neural\nJamming are the most impactful cyberattacks for short-term effects, while\nNeural Scanning and Neural Nonce are the most damaging for long-term effects.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 10:26:46 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bernal", "Sergio L\u00f3pez", ""], ["Celdr\u00e1n", "Alberto Huertas", ""], ["P\u00e9rez", "Gregorio Mart\u00ednez", ""]]}, {"id": "2106.04974", "submitter": "Fabian Ising", "authors": "Simon Ebbers (M\\\"unster University of Applied Sciences), Fabian Ising\n  (M\\\"unster University of Applied Sciences), Christoph Saatjohann (M\\\"unster\n  University of Applied Sciences) and Sebastian Schinzel (M\\\"unster University\n  of Applied Sciences)", "title": "Grand Theft App: Digital Forensics of Vehicle Assistant Apps", "comments": "This is the extended version of the Short Paper Grand Theft App:\n  Digital Forensics of Vehicle Assistant Apps published at the 16th\n  International Conference on Availability, Reliability and Security (ARES\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing connectivity of modern vehicles, collected data is no\nlonger only stored in the vehicle itself but also transmitted to car\nmanufacturers and vehicle assistant apps. This development opens up new\npossibilities for digital forensics in criminal investigations involving modern\nvehicles. This paper deals with the digital forensic analysis of vehicle\nassistant apps of eight car manufacturers. We reconstruct the driver's\nactivities based on the data stored on the smartphones and in the\nmanufacturer's backend.\n  For this purpose, data of the Android and iOS apps of the car manufacturers\nAudi, BMW, Ford, Mercedes, Opel, Seat, Tesla, and Volkswagen were extracted\nfrom the smartphone and examined using digital forensic methods in accordance\nwith lawful government-approved forensics guidelines. Additionally,\nmanufacturer data was retrieved using Subject Access Requests. Using the\nextensive data gathered, we successfully reconstruct trips and refueling\nprocesses, determine parking positions and duration, and track the locking and\nunlocking of the vehicle.\n  These findings show that the digital forensic investigation of smartphone\napplications is a useful addition to vehicle forensics and should therefore be\ntaken into account in the strategic preparation of future digital forensic\ninvestigations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 10:40:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ebbers", "Simon", "", "M\u00fcnster University of Applied Sciences"], ["Ising", "Fabian", "", "M\u00fcnster University of Applied Sciences"], ["Saatjohann", "Christoph", "", "M\u00fcnster\n  University of Applied Sciences"], ["Schinzel", "Sebastian", "", "M\u00fcnster University\n  of Applied Sciences"]]}, {"id": "2106.05007", "submitter": "Martin Kotuliak", "authors": "Martin Kotuliak, Simon Erni, Patrick Leu, Marc Roeschlin, Srdjan\n  Capkun", "title": "LTrack: Stealthy Tracking of Mobile Phones in LTE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LTrack, a new tracking attack on LTE that allows an attacker to\nstealthily extract user devices' (UEs) permanent identifiers (IMSI) and\nlocations. To remain stealthy, the localization of UEs in LTrack is fully\npassive. It relies on our new uplink/downlink sniffer implementation, which\nrecords both times of arrivals of LTE messages and contents of Timing Advance\ncommands, based on which LTrack calculates UE locations. LTrack is the first to\nshow the feasibility of passive UE's localization through an implementation on\na software-defined radio.\n  Passive localization attacks reveal information about a UE's locations but\ncan at best link these locations to a UE's pseudonymous temporary identifier\n(TMSI), making tracking in dense areas challenging. LTrack overcomes this\nchallenge by introducing and implementing a new type of IMSI Catcher named IMSI\nExtractor. It extracts a UE's permanent identifier (IMSI) and binds it to its\ncurrent TMSI. Instead of relying on fake base stations like existing IMSI\nCatchers (which are detectable due to their output power), IMSI Extractor\nrelies on our uplink/downlink sniffer enhanced with surgical message\novershadowing. This makes our IMSI Extractor the stealthiest IMSI Catcher to\ndate.\n  We evaluate LTrack through a series of experiments and show that in\nline-of-sight conditions, the attacker can estimate the location of a phone\nwith less than 6m error in 90 of the cases. In addition, we successfully test\nour IMSI Extractor against a set of 17 modern smartphones connected to an\nindustry-grade LTE testbed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:10:13 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Kotuliak", "Martin", ""], ["Erni", "Simon", ""], ["Leu", "Patrick", ""], ["Roeschlin", "Marc", ""], ["Capkun", "Srdjan", ""]]}, {"id": "2106.05039", "submitter": "Simon Erni", "authors": "Simon Erni, Patrick Leu, Martin Kotuliak, Marc R\\\"oschlin, Srdjan\n  \\v{C}apkun", "title": "AdaptOver : Adaptive Overshadowing of LTE signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce AdaptOver, a new LTE signal overshadowing attack that allows an\nadversary to reactively and adaptively overshadow any downlink message between\nthe network and the user equipment (UE). We demonstrate the impact of AdaptOver\nby using it to launch targeted Denial-of-Service (DoS) attacks on UEs. We\nimplement AdaptOver using a commercially available software-defined radio. Our\nexperiments demonstrate that our DoS attacks cause persistent connection loss\nlasting more than 12 hours for a wide range of smartphones. DoS attacks based\non AdaptOver are stealthier than attacks that relied on the use of fake base\nstations, and more persistent than existing overshadowing attacks, which caused\nconnection loss of only up to 9 minutes. Given that AdaptOver can reactively\novershadow any downlink message, its use is not limited to DoS attacks - it can\nbe used for a wide range of other attacks, e.g., to extract the IMSI from a UE\nin a stealthier manner than traditional IMSI catchers. We consider AdaptOver to\nbe an essential building block for many attacks against real-world LTE\nnetworks. In particular, any fake base station attack that makes use of spoofed\ndownlink messages can be ported to the presented attack method, causing a much\nmore reliable, persistent, and stealthy effect.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:54:05 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Erni", "Simon", ""], ["Leu", "Patrick", ""], ["Kotuliak", "Martin", ""], ["R\u00f6schlin", "Marc", ""], ["\u010capkun", "Srdjan", ""]]}, {"id": "2106.05042", "submitter": "Margarita Vinaroz", "authors": "Mijung Park, Margarita Vinaroz, Mohammad-Amin Charusaie, Frederik\n  Harder", "title": "Polynomial magic! Hermite polynomials for private data generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel mean embedding is a useful tool to compare probability measures.\nDespite its usefulness, kernel mean embedding considers infinite-dimensional\nfeatures, which are challenging to handle in the context of differentially\nprivate data generation. A recent work proposes to approximate the kernel mean\nembedding of data distribution using finite-dimensional random features, where\nthe sensitivity of the features becomes analytically tractable. More\nimportantly, this approach significantly reduces the privacy cost, compared to\nother known privatization methods (e.g., DP-SGD), as the approximate kernel\nmean embedding of the data distribution is privatized only once and can then be\nrepeatedly used during training of a generator without incurring any further\nprivacy cost. However, the required number of random features is excessively\nhigh, often ten thousand to a hundred thousand, which worsens the sensitivity\nof the approximate kernel mean embedding. To improve the sensitivity, we\npropose to replace random features with Hermite polynomial features. Unlike the\nrandom features, the Hermite polynomial features are ordered, where the\nfeatures at the low orders contain more information on the distribution than\nthose at the high orders. Hence, a relatively low order of Hermite polynomial\nfeatures can more accurately approximate the mean embedding of the data\ndistribution compared to a significantly higher number of random features. As a\nresult, using the Hermite polynomial features, we significantly improve the\nprivacy-accuracy trade-off, reflected in the high quality and diversity of the\ngenerated data, when tested on several heterogeneous tabular datasets, as well\nas several image benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:56:41 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Park", "Mijung", ""], ["Vinaroz", "Margarita", ""], ["Charusaie", "Mohammad-Amin", ""], ["Harder", "Frederik", ""]]}, {"id": "2106.05050", "submitter": "Jawad Haj-Yahya", "authors": "Jawad Haj-Yahya, Jeremie S. Kim, A. Giray Yaglikci, Ivan Puddu, Lois\n  Orosa, Juan G\\'omez Luna, Mohammed Alser, Onur Mutlu", "title": "IChannels: Exploiting Current Management Mechanisms to Create Covert\n  Channels in Modern Processors", "comments": "To appear in ISCA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To operate efficiently across a wide range of workloads with varying power\nrequirements, a modern processor applies different current management\nmechanisms, which briefly throttle instruction execution while they adjust\nvoltage and frequency to accommodate for power-hungry instructions (PHIs) in\nthe instruction stream. Doing so 1) reduces the power consumption of non-PHI\ninstructions in typical workloads and 2) optimizes system voltage regulators'\ncost and area for the common use case while limiting current consumption when\nexecuting PHIs.\n  However, these mechanisms may compromise a system's confidentiality\nguarantees. In particular, we observe that multilevel side-effects of\nthrottling mechanisms, due to PHI-related current management mechanisms, can be\ndetected by two different software contexts (i.e., sender and receiver) running\non 1) the same hardware thread, 2) co-located Simultaneous Multi-Threading\n(SMT) threads, and 3) different physical cores.\n  Based on these new observations on current management mechanisms, we develop\na new set of covert channels, IChannels, and demonstrate them in real modern\nIntel processors (which span more than 70% of the entire client and server\nprocessor market). Our analysis shows that IChannels provides more than 24x the\nchannel capacity of state-of-the-art power management covert channels. We\npropose practical and effective mitigations to each covert channel in IChannels\nby leveraging the insights we gain through a rigorous characterization of real\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 13:03:08 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:24:57 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Haj-Yahya", "Jawad", ""], ["Kim", "Jeremie S.", ""], ["Yaglikci", "A. Giray", ""], ["Puddu", "Ivan", ""], ["Orosa", "Lois", ""], ["Luna", "Juan G\u00f3mez", ""], ["Alser", "Mohammed", ""], ["Mutlu", "Onur", ""]]}, {"id": "2106.05087", "submitter": "Yanchao Sun", "authors": "Yanchao Sun, Ruijie Zheng, Yongyuan Liang, Furong Huang", "title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion\n  Attacks in Deep RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the worst-case performance of a reinforcement learning (RL) agent\nunder the strongest/optimal adversarial perturbations on state observations\n(within some constraints) is crucial for understanding the robustness of RL\nagents. However, finding the optimal adversary is challenging, in terms of both\nwhether we can find the optimal attack and how efficiently we can find it.\nExisting works on adversarial RL either use heuristics-based methods that may\nnot find the strongest adversary, or directly train an RL-based adversary by\ntreating the agent as a part of the environment, which can find the optimal\nadversary but may become intractable in a large state space. In this paper, we\npropose a novel attacking algorithm which has an RL-based \"director\" searching\nfor the optimal policy perturbation, and an \"actor\" crafting state\nperturbations following the directions from the director (i.e. the actor\nexecutes targeted attacks). Our proposed algorithm, PA-AD, is theoretically\noptimal against an RL agent and significantly improves the efficiency compared\nwith prior RL-based works in environments with large or pixel state spaces.\nEmpirical results show that our proposed PA-AD universally outperforms\nstate-of-the-art attacking methods in a wide range of environments. Our method\ncan be easily applied to any RL algorithms to evaluate and improve their\nrobustness.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:06:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sun", "Yanchao", ""], ["Zheng", "Ruijie", ""], ["Liang", "Yongyuan", ""], ["Huang", "Furong", ""]]}, {"id": "2106.05131", "submitter": "Yuchao Tao", "authors": "Yuchao Tao, Johes Bater, Ashwin Machanavajjhala", "title": "Prior-Aware Distribution Estimation for Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint distribution estimation of a dataset under differential privacy is a\nfundamental problem for many privacy-focused applications, such as query\nanswering, machine learning tasks and synthetic data generation. In this work,\nwe examine the joint distribution estimation problem given two data points: 1)\ndifferentially private answers of a workload computed over private data and 2)\na prior empirical distribution from a public dataset. Our goal is to find a new\ndistribution such that estimating the workload using this distribution is as\naccurate as the differentially private answer, and the relative entropy, or KL\ndivergence, of this distribution is minimized with respect to the prior\ndistribution. We propose an approach based on iterative optimization for\nsolving this problem. An application of our solution won second place in the\nNIST 2020 Differential Privacy Temporal Map Challenge, Sprint 2.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:15:00 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Tao", "Yuchao", ""], ["Bater", "Johes", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "2106.05184", "submitter": "Pushkal Agarwal", "authors": "Pushkal Agarwal, Aravindh Raman, Kiran Garimella, Damilola Ibosiola,\n  Gareth Tyson, Nishanth Sastry", "title": "Tackling spam in the era of end-to-end encryption: A case study of\n  WhatsApp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  WhatsApp is a popular messaging app used by over a billion users around the\nglobe. Due to this popularity, spam on WhatsApp is an important issue. Despite\nthis, the distribution of spam via WhatsApp remains understudied by\nresearchers, in part because of the end-to-end encryption offered by the\nplatform. This paper addresses this gap by studying spam on a dataset of 2.6\nmillion messages sent to 5,051 public WhatsApp groups in India over 300 days.\nFirst, we characterise spam content shared within public groups and find that\nnearly 1 in 10 messages is spam. We observe a wide selection of topics ranging\nfrom job ads to adult content, and find that spammers post both URLs and phone\nnumbers to promote material. Second, we inspect the nature of spammers\nthemselves. We find that spam is often disseminated by groups of phone numbers,\nand that spam messages are generally shared for longer duration than non-spam\nmessages. Finally, we devise content and activity based detection algorithms\nthat can counter spam.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:52:46 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 07:05:17 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Agarwal", "Pushkal", ""], ["Raman", "Aravindh", ""], ["Garimella", "Kiran", ""], ["Ibosiola", "Damilola", ""], ["Tyson", "Gareth", ""], ["Sastry", "Nishanth", ""]]}, {"id": "2106.05211", "submitter": "Nour Almadhoun Alserr", "authors": "Nour Almadhoun Alserr, Gulce Kale, Onur Mutlu, Oznur Tastan, Erman\n  Ayday", "title": "Near-Optimal Privacy-Utility Tradeoff in Genomic Studies Using Selective\n  SNP Hiding", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Researchers need a rich trove of genomic datasets that they can\nleverage to gain a better understanding of the genetic basis of the human\ngenome and identify associations between phenotypes and specific parts of DNA.\nHowever, sharing genomic datasets that include sensitive genetic or medical\ninformation of individuals can lead to serious privacy-related consequences if\ndata lands in the wrong hands. Restricting access to genomic datasets is one\nsolution, but this greatly reduces their usefulness for research purposes. To\nallow sharing of genomic datasets while addressing these privacy concerns,\nseveral studies propose privacy-preserving mechanisms for data sharing.\nDifferential privacy (DP) is one of such mechanisms that formalize rigorous\nmathematical foundations to provide privacy guarantees while sharing aggregated\nstatistical information about a dataset. However, it has been shown that the\noriginal privacy guarantees of DP-based solutions degrade when there are\ndependent tuples in the dataset, which is a common scenario for genomic\ndatasets (due to the existence of family members). Results: In this work, we\nintroduce a near-optimal mechanism to mitigate the vulnerabilities of the\ninference attacks on differentially private query results from genomic datasets\nincluding dependent tuples. We propose a utility-maximizing and\nprivacy-preserving approach for sharing statistics by hiding selective SNPs of\nthe family members as they participate in a genomic dataset. By evaluating our\nmechanism on a real-world genomic dataset, we empirically demonstrate that our\nproposed mechanism can achieve up to 40% better privacy than state-of-the-art\nDP-based solutions, while near-optimally minimizing the utility loss.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:55:18 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Alserr", "Nour Almadhoun", ""], ["Kale", "Gulce", ""], ["Mutlu", "Onur", ""], ["Tastan", "Oznur", ""], ["Ayday", "Erman", ""]]}, {"id": "2106.05227", "submitter": "Pardis Emami-Naeini", "authors": "Pardis Emami-Naeini, Tiona Francisco, Tadayoshi Kohno, Franziska\n  Roesner", "title": "Understanding Privacy Attitudes and Concerns Towards Remote\n  Communications During the COVID-19 Pandemic", "comments": "To appear at the 17th Symposium on Usable Privacy and Security\n  (SOUPS'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since December 2019, the COVID-19 pandemic has caused people around the world\nto exercise social distancing, which has led to an abrupt rise in the adoption\nof remote communications for working, socializing, and learning from home. As\nremote communications will outlast the pandemic, it is crucial to protect\nusers' security and respect their privacy in this unprecedented setting, and\nthat requires a thorough understanding of their behaviors, attitudes, and\nconcerns toward various aspects of remote communications. To this end, we\nconducted an online study with 220 worldwide Prolific participants. We found\nthat privacy and security are among the most frequently mentioned factors\nimpacting participants' attitude and comfort level with conferencing tools and\nmeeting locations. Open-ended responses revealed that most participants lacked\nautonomy when choosing conferencing tools or using microphone/webcam in their\nremote meetings, which in several cases contradicted their personal privacy and\nsecurity preferences. Based on our findings, we distill several recommendations\non how employers, educators, and tool developers can inform and empower users\nto make privacy-protective decisions when engaging in remote communications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:17:06 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Emami-Naeini", "Pardis", ""], ["Francisco", "Tiona", ""], ["Kohno", "Tadayoshi", ""], ["Roesner", "Franziska", ""]]}, {"id": "2106.05256", "submitter": "Pranav Ravindra Maneriker", "authors": "Pranav Maneriker, Jack W. Stokes, Edir Garcia Lazo, Diana Carutasu,\n  Farid Tajaddodianfar, Arun Gururajan", "title": "URLTran: Improving Phishing URL Detection Using Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Browsers often include security features to detect phishing web pages. In the\npast, some browsers evaluated an unknown URL for inclusion in a list of known\nphishing pages. However, as the number of URLs and known phishing pages\ncontinued to increase at a rapid pace, browsers started to include one or more\nmachine learning classifiers as part of their security services that aim to\nbetter protect end users from harm. While additional information could be used,\nbrowsers typically evaluate every unknown URL using some classifier in order to\nquickly detect these phishing pages. Early phishing detection used standard\nmachine learning classifiers, but recent research has instead proposed the use\nof deep learning models for the phishing URL detection task. Concurrently, text\nembedding research using transformers has led to state-of-the-art results in\nmany natural language processing tasks. In this work, we perform a\ncomprehensive analysis of transformer models on the phishing URL detection\ntask. We consider standard masked language model and additional domain-specific\npre-training tasks, and compare these models to fine-tuned BERT and RoBERTa\nmodels. Combining the insights from these experiments, we propose URLTran which\nuses transformers to significantly improve the performance of phishing URL\ndetection over a wide range of very low false positive rates (FPRs) compared to\nother deep learning-based methods. For example, URLTran yields a true positive\nrate (TPR) of 86.80% compared to 71.20% for the next best baseline at an FPR of\n0.01%, resulting in a relative improvement of over 21.9%. Further, we consider\nsome classical adversarial black-box phishing attacks such as those based on\nhomoglyphs and compound word splits to improve the robustness of URLTran. We\nconsider additional fine tuning with these adversarial samples and demonstrate\nthat URLTran can maintain low FPRs under these scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:54:27 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Maneriker", "Pranav", ""], ["Stokes", "Jack W.", ""], ["Lazo", "Edir Garcia", ""], ["Carutasu", "Diana", ""], ["Tajaddodianfar", "Farid", ""], ["Gururajan", "Arun", ""]]}, {"id": "2106.05261", "submitter": "Jiachun Li", "authors": "Bin Liang and Jiachun Li and Jianjun Huang", "title": "We Can Always Catch You: Detecting Adversarial Patched Objects WITH or\n  WITHOUT Signature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the object detection based on deep learning has proven to be\nvulnerable to adversarial patch attacks. The attackers holding a specially\ncrafted patch can hide themselves from the state-of-the-art person detectors,\ne.g., YOLO, even in the physical world. This kind of attack can bring serious\nsecurity threats, such as escaping from surveillance cameras. In this paper, we\ndeeply explore the detection problems about the adversarial patch attacks to\nthe object detection. First, we identify a leverageable signature of existing\nadversarial patches from the point of the visualization explanation. A fast\nsignature-based defense method is proposed and demonstrated to be effective.\nSecond, we design an improved patch generation algorithm to reveal the risk\nthat the signature-based way may be bypassed by the techniques emerging in the\nfuture. The newly generated adversarial patches can successfully evade the\nproposed signature-based defense. Finally, we present a novel\nsignature-independent detection method based on the internal content semantics\nconsistency rather than any attack-specific prior knowledge. The fundamental\nintuition is that the adversarial object can appear locally but disappear\nglobally in an input image. The experiments demonstrate that the\nsignature-independent method can effectively detect the existing and improved\nattacks. It has also proven to be a general method by detecting unforeseen and\neven other types of attacks without any attack-specific prior knowledge. The\ntwo proposed detection methods can be adopted in different scenarios, and we\nbelieve that combining them can offer a comprehensive protection.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:58:08 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 07:38:23 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liang", "Bin", ""], ["Li", "Jiachun", ""], ["Huang", "Jianjun", ""]]}, {"id": "2106.05332", "submitter": "John Mern", "authors": "John Mern, Kyle Hatch, Ryan Silva, Jeff Brush, Mykel J. Kochenderfer", "title": "Reinforcement Learning for Industrial Control Network Cyber Security\n  Orchestration", "comments": "12 pages, submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Defending computer networks from cyber attack requires coordinating actions\nacross multiple nodes based on imperfect indicators of compromise while\nminimizing disruptions to network operations. Advanced attacks can progress\nwith few observable signals over several months before execution. The resulting\nsequential decision problem has large observation and action spaces and a long\ntime-horizon, making it difficult to solve with existing methods. In this work,\nwe present techniques to scale deep reinforcement learning to solve the cyber\nsecurity orchestration problem for large industrial control networks. We\npropose a novel attention-based neural architecture with size complexity that\nis invariant to the size of the network under protection. A pre-training\ncurriculum is presented to overcome early exploration difficulty. Experiments\nshow in that the proposed approaches greatly improve both the learning sample\ncomplexity and converged policy performance over baseline methods in\nsimulation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 18:44:17 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Mern", "John", ""], ["Hatch", "Kyle", ""], ["Silva", "Ryan", ""], ["Brush", "Jeff", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2106.05395", "submitter": "Vahid Rasouli Disfani", "authors": "Sakineh Khalili, Vahid Disfani, Mo Ahmadi", "title": "Impact of Blockchain Technology on Electric Power Grids -- A case study\n  in LO3 Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing amount of distributed energy resources including renewable\nenergy systems and electric vehicles is expected to change electric power grids\nsignificantly, where conventional consumers are transformed to prosumers since\nthey can produce electricity as well. In such an ecosystem, prosumers can start\noffering their excess energy to supply demands of the other customers on the\ngrids behind the meter without interference of distribution system operators\n(DSO). Besides, DSOs require more accurate and more frequent data form\nprosumers' net demand to be able to operate their network efficiently. The main\nchallenge in these new distribution grids is the amount of data that needs to\nbe collected in this platform is unbelievably high, and more immortally,\nprosumers will likely refuse to share their information with DSOs due to their\npotential privacy and economic concerns. Blockchain technology as an efficient\ndistributed solution for management of data and financial transactions, has\nbeen considered to solve this trust issue. With blockchain-based solutions,\ndata and financial transactions between all parties will take placed through\ndistributed ledgers without any interference from an intermediary. In this\npaper, impacts of blockchain technologies on electric power industry is\nstudied. The paper specifically focuses on LO3 Energy -- one of startups\napplying blockchain to electric power grids -- their blockchain-based solution\ncalled Exergy, and their use cases to implement such solutions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:18:10 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Khalili", "Sakineh", ""], ["Disfani", "Vahid", ""], ["Ahmadi", "Mo", ""]]}, {"id": "2106.05407", "submitter": "Rahmadi Trimananda", "authors": "Rahmadi Trimananda, Hieu Le, Hao Cui, Janice Tran Ho, Anastasia Shuba,\n  Athina Markopoulou", "title": "Auditing Network Traffic and Privacy Policies in Oculus VR", "comments": "13 pages (and 4 pages of references), 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual reality (VR) is an emerging technology that enables new applications\nbut also introduces privacy risks. In this paper, we focus on Oculus VR (OVR),\nthe leading platform in the VR space, and we provide the first comprehensive\nanalysis of personal data exposed by OVR apps and the platform itself, from a\ncombined networking and privacy policy perspective. We experimented with the\nQuest 2 headset, and we tested the most popular VR apps available on the\nofficial Oculus and the SideQuest app stores. We developed OVRseen, a\nmethodology and system for collecting, analyzing, and comparing network traffic\nand privacy policies on OVR. On the networking side, we captured and decrypted\nnetwork traffic of VR apps, which was previously not possible on OVR, and we\nextracted data flows (defined as <app, data type, destination>). We found that\nthe OVR ecosystem (compared to the mobile and other app ecosystems) is more\ncentralized, and driven by tracking and analytics, rather than by third-party\nadvertising. We show that the data types exposed by VR apps include personally\nidentifiable information (PII), device information that can be used for\nfingerprinting, and VR-specific data types. By comparing the data flows found\nin the network traffic with statements made in the apps' privacy policies, we\ndiscovered that approximately 70% of OVR data flows were not properly\ndisclosed. Furthermore, we provided additional context for these data flows,\nincluding the purpose, which we extracted from the privacy policies, and\nobserved that 69% were sent for purposes unrelated to the core functionality of\napps.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:52:13 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 04:36:50 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Trimananda", "Rahmadi", ""], ["Le", "Hieu", ""], ["Cui", "Hao", ""], ["Ho", "Janice Tran", ""], ["Shuba", "Anastasia", ""], ["Markopoulou", "Athina", ""]]}, {"id": "2106.05434", "submitter": "Chandra Thapa", "authors": "Chandra Thapa and Kallol Krishna Karmakar and Alberto Huertas Celdran\n  and Seyit Camtepe and Vijay Varadharajan and Surya Nepal", "title": "FedDICE: A ransomware spread detection in a distributed integrated\n  clinical environment using federated learning and SDN based mitigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An integrated clinical environment (ICE) enables the connection and\ncoordination of the internet of medical things around the care of patients in\nhospitals. However, ransomware attacks and their spread on hospital\ninfrastructures, including ICE, are rising. Often the adversaries are targeting\nmultiple hospitals with the same ransomware attacks. These attacks are detected\nby using machine learning algorithms. But the challenge is devising the\nanti-ransomware learning mechanisms and services under the following\nconditions: (1) provide immunity to other hospitals if one of them got the\nattack, (2) hospitals are usually distributed over geographical locations, and\n(3) direct data sharing is avoided due to privacy concerns. In this regard,\nthis paper presents a federated distributed integrated clinical environment,\naka. FedDICE. FedDICE integrates federated learning (FL), which is\nprivacy-preserving learning, to SDN-oriented security architecture to enable\ncollaborative learning, detection, and mitigation of ransomware attacks. We\ndemonstrate the importance of FedDICE in a collaborative environment with up to\nfour hospitals and four popular ransomware families, namely WannaCry, Petya,\nBadRabbit, and PowerGhost. Our results find that in both IID and non-IID data\nsetups, FedDICE achieves the centralized baseline performance that needs direct\ndata sharing for detection. However, as a trade-off to data privacy, FedDICE\nobserves overhead in the anti-ransomware model training, e.g., 28x for the\nlogistic regression model. Besides, FedDICE utilizes SDN's dynamic network\nprogrammability feature to remove the infected devices in ICE.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 23:59:18 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Thapa", "Chandra", ""], ["Karmakar", "Kallol Krishna", ""], ["Celdran", "Alberto Huertas", ""], ["Camtepe", "Seyit", ""], ["Varadharajan", "Vijay", ""], ["Nepal", "Surya", ""]]}, {"id": "2106.05463", "submitter": "Hong Su Dr.", "authors": "Hong Su", "title": "Cross-chain Interaction Model In a Fully Verified Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are different kinds of blockchains, which have been applied in various\nareas. Blockchains are relatively independent systems that are apt to form\nisolated data islands. Then cross-chain interaction is proposed to connect\ndifferent blockchains. However, the current cross-chain methods do not maintain\nthe security of the original blockchain. They either depend on a less secure\nthird-party system or a less secure method. This makes the cross-chain\ninteraction less secure than the original blockchains (the security downgrade\nissues), or the cross-chain interaction can be done even if the paired\nblockchain does not exist (the blockchain invisible issue). In this paper, we\nfirst propose a system interaction model and use it to analyze the possible\nsecurity issues. Based on conclusions got from the proposed model, we propose\nthe cross-chain method that verifies the data of the paired blockchain by the\nconsensus algorithm of the paired blockchain (the CIFuV method). With this\nmethod, the cross-chain interaction can be as the same security as in the\npaired blockchain. At last, we evaluate the security issues during the system\ninteraction process, and the possibility to have the CIFuV model on the public\nblockchains.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 02:47:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Su", "Hong", ""]]}, {"id": "2106.05478", "submitter": "Hyungjoon Koo", "authors": "Hyungjoon Koo, Soyeon Park, Daejin Choi, Taesoo Kim", "title": "Semantic-aware Binary Code Representation with BERT", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of binary analysis applications, such as bug discovery, malware\nanalysis and code clone detection, require recovery of contextual meanings on a\nbinary code. Recently, binary analysis techniques based on machine learning\nhave been proposed to automatically reconstruct the code representation of a\nbinary instead of manually crafting specifics of the analysis algorithm.\nHowever, the existing approaches utilizing machine learning are still\nspecialized to solve one domain of problems, rendering recreation of models for\ndifferent types of binary analysis. In this paper, we propose DeepSemantic\nutilizing BERT in producing the semantic-aware code representation of a binary\ncode.\n  To this end, we introduce well-balanced instruction normalization that holds\nrich information for each of instructions yet minimizing an out-of-vocabulary\n(OOV) problem. DeepSemantic has been carefully designed based on our study with\nlarge swaths of binaries. Besides, DeepSemantic leverages the essence of the\nBERT architecture into re-purposing a pre-trained generic model that is readily\navailable as a one-time processing, followed by quickly applying specific\ndownstream tasks with a fine-tuning process. We demonstrate DeepSemantic with\ntwo downstream tasks, namely, binary similarity comparison and compiler\nprovenance (i.e., compiler and optimization level) prediction. Our experimental\nresults show that the binary similarity model outperforms two state-of-the-art\nbinary similarity tools, DeepBinDiff and SAFE, 49.84% and 15.83% on average,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 03:31:29 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Koo", "Hyungjoon", ""], ["Park", "Soyeon", ""], ["Choi", "Daejin", ""], ["Kim", "Taesoo", ""]]}, {"id": "2106.05483", "submitter": "Vladimir Edemskiy", "authors": "Vladimir Edemskiy, Zhixiong Chen", "title": "On the 4-adic complexity of the two-prime quaternary generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  R. Hofer and A. Winterhof proved that the 2-adic complexity of the two-prime\n(binary) generator of period $pq$ with two odd primes $p\\neq q$ is close to its\nperiod and it can attain the maximum in many cases.\n  When the two-prime generator is applied to producing quaternary sequences, we\nneed to determine the 4-adic complexity. We present the formulae of possible\nvalues of the 4-adic complexity, which is larger than $pq-\\log_4(pq^2)-1$ if\n$p<q$. So it is good enough to resist the attack of the rational approximation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 03:55:59 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Edemskiy", "Vladimir", ""], ["Chen", "Zhixiong", ""]]}, {"id": "2106.05537", "submitter": "Yuichi Sano", "authors": "Yuichi Sano", "title": "Multi-server Blind Quantum Computation Protocol With Limited Classical\n  Communication Among Servers", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A user who does not have a quantum computer but wants to perform quantum\ncomputations may delegate his computation to a quantum cloud server. In order\nthat the delegation works, it must be assured that no evil server can obtain\nany important information on the computation. The blind protocol was proposed\nas a way for the user to protect his information from the unauthorized actions\nof the server. Among the blind protocols proposed thus far, a protocol with two\nservers sharing entanglement, while it does not require to a user any quantum\nresource, does not allow the servers to communicate even after the computation.\nIn this paper, we propose a protocol, by extend this two-server protocol to\nmultiple servers, which remains secure even if some servers communicate with\neach other after the computation. Dummy gates and a circuit modeled after\nbrickwork states play a crucial role in the new protocol.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:58:22 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Sano", "Yuichi", ""]]}, {"id": "2106.05577", "submitter": "Benjamin Smith", "authors": "Gustavo Banegas (GRACE), Koen Zandberg (TRiBE), Adrian Herrmann (Freie\n  Universit\\\"at Berlin), Emmanuel Baccelli (TRiBE), Benjamin Smith (GRACE)", "title": "Quantum-Resistant Security for Software Updates on Low-power Networked\n  Embedded Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the Internet of Things (IoT) rolls out today to devices whose lifetime may\nwell exceed a decade, conservative threat models should consider attackers with\naccess to quantum computing power. The SUIT standard (specified by the IETF)\ndefines a security architecture for IoT software updates, standardizing the\nmetadata and the cryptographic tools-namely, digital signatures and hash\nfunctions-that guarantee the legitimacy of software updates. While the\nperformance of SUIT has previously been evaluated in the pre-quantum context,\nit has not yet been studied in a post-quantum context. Taking the open-source\nimplementation of SUIT available in RIOT as a case study, we overview\npost-quantum considerations, and quantum-resistant digital signatures in\nparticular, focusing on lowpower, microcontroller-based IoT devices which have\nstringent resource constraints in terms of memory, CPU, and energy consumption.\nWe benchmark a selection of proposed post-quantum signature schemes (LMS,\nFalcon, and Dilithium) and compare them with current pre-quantum signature\nschemes (Ed25519 and ECDSA). Our benchmarks are carried out on a variety of IoT\nhardware including ARM Cortex-M, RISC-V, and Espressif (ESP32), which form the\nbulk of modern 32-bit microcontroller architectures. We interpret our benchmark\nresults in the context of SUIT, and estimate the real-world impact of\npost-quantum alternatives for a range of typical software update categories.\nCCS CONCEPTS $\\bullet$ Computer systems organization $\\rightarrow$ Embedded\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:56:57 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 13:15:58 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Banegas", "Gustavo", "", "GRACE"], ["Zandberg", "Koen", "", "TRiBE"], ["Herrmann", "Adrian", "", "Freie\n  Universit\u00e4t Berlin"], ["Baccelli", "Emmanuel", "", "TRiBE"], ["Smith", "Benjamin", "", "GRACE"]]}, {"id": "2106.05625", "submitter": "Daniele Ucci", "authors": "Nicola Loi, Claudio Borile, Daniele Ucci", "title": "Towards an Automated Pipeline for Detecting and Classifying Malware\n  through Machine Learning", "comments": "12 pages and 6 figures. Presented at ITASEC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constant growth in the number of malware - software or code fragment\npotentially harmful for computers and information networks - and the use of\nsophisticated evasion and obfuscation techniques have seriously hindered\nclassic signature-based approaches. On the other hand, malware detection\nsystems based on machine learning techniques started offering a promising\nalternative to standard approaches, drastically reducing analysis time and\nturning out to be more robust against evasion and obfuscation techniques. In\nthis paper, we propose a malware taxonomic classification pipeline able to\nclassify Windows Portable Executable files (PEs). Given an input PE sample, it\nis first classified as either malicious or benign. If malicious, the pipeline\nfurther analyzes it in order to establish its threat type, family, and\nbehavior(s). We tested the proposed pipeline on the open source dataset EMBER,\ncontaining approximately 1 million PE samples, analyzed through static\nanalysis. Obtained malware detection results are comparable to other academic\nworks in the current state of art and, in addition, we provide an in-depth\nclassification of malicious samples. Models used in the pipeline provides\ninterpretable results which can help security analysts in better understanding\ndecisions taken by the automated pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:07:50 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Loi", "Nicola", ""], ["Borile", "Claudio", ""], ["Ucci", "Daniele", ""]]}, {"id": "2106.05632", "submitter": "Lois Orosa", "authors": "Lois Orosa, Yaohua Wang, Mohammad Sadrosadati, Jeremie S. Kim, Minesh\n  Patel, Ivan Puddu, Haocong Luo, Kaveh Razavi, Juan G\\'omez-Luna, Hasan\n  Hassan, Nika Mansouri-Ghiasi, Saugata Ghose, Onur Mutlu", "title": "CODIC: A Low-Cost Substrate for Enabling Custom In-DRAM Functionalities\n  and Optimizations", "comments": "Extended version of an ISCA 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DRAM is the dominant main memory technology used in modern computing systems.\nComputing systems implement a memory controller that interfaces with DRAM via\nDRAM commands. DRAM executes the given commands using internal components\n(e.g., access transistors, sense amplifiers) that are orchestrated by DRAM\ninternal timings, which are fixed foreach DRAM command. Unfortunately, the use\nof fixed internal timings limits the types of operations that DRAM can perform\nand hinders the implementation of new functionalities and custom mechanisms\nthat improve DRAM reliability, performance and energy. To overcome these\nlimitations, we propose enabling programmable DRAM internal timings for\ncontrolling in-DRAM components. To this end, we design CODIC, a new low-cost\nDRAM substrate that enables fine-grained control over four previously fixed\ninternal DRAM timings that are key to many DRAM operations. We implement CODIC\nwith only minimal changes to the DRAM chip and the DDRx interface. To\ndemonstrate the potential of CODIC, we propose two new CODIC-based security\nmechanisms that outperform state-of-the-art mechanisms in several ways: (1) a\nnew DRAM Physical Unclonable Function (PUF) that is more robust and has\nsignificantly higher throughput than state-of-the-art DRAM PUFs, and (2) the\nfirst cold boot attack prevention mechanism that does not introduce any\nperformance or energy overheads at runtime.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:15:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Orosa", "Lois", ""], ["Wang", "Yaohua", ""], ["Sadrosadati", "Mohammad", ""], ["Kim", "Jeremie S.", ""], ["Patel", "Minesh", ""], ["Puddu", "Ivan", ""], ["Luo", "Haocong", ""], ["Razavi", "Kaveh", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Hassan", "Hasan", ""], ["Mansouri-Ghiasi", "Nika", ""], ["Ghose", "Saugata", ""], ["Mutlu", "Onur", ""]]}, {"id": "2106.05673", "submitter": "Ali Hussain Khan", "authors": "Ali Hussain Khan, Naveed UL Hassan, Chau Yuen, Jun Zhao, Dusit Niyato,\n  Yan Zhang and H. Vincent Poor", "title": "Blockchain and 6G: The Future of Secure and Ubiquitous Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The future communication will be characterized by ubiquitous connectivity and\nsecurity. These features will be essential requirements for the efficient\nfunctioning of the futuristic applications. In this paper, in order to\nhighlight the impact of blockchain and 6G on the future communication systems,\nwe categorize these application requirements into two broad groups. In the\nfirst category, called Requirement Group I \\mbox{(RG-I)}, we include the\nperformance-related needs on data rates, latency, reliability and massive\nconnectivity, while in the second category, called Requirement Group II\n\\mbox{(RG-II)}, we include the security-related needs on data integrity,\nnon-repudiability, and auditability. With blockchain and 6G, the network\ndecentralization and resource sharing would minimize resource under-utilization\nthereby facilitating RG-I targets. Furthermore, through appropriate selection\nof blockchain type and consensus algorithms, RG-II needs of 6G applications can\nalso be readily addressed. Through this study, the combination of blockchain\nand 6G emerges as an elegant solution for secure and ubiquitous future\ncommunication.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:43:27 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Khan", "Ali Hussain", ""], ["Hassan", "Naveed UL", ""], ["Yuen", "Chau", ""], ["Zhao", "Jun", ""], ["Niyato", "Dusit", ""], ["Zhang", "Yan", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2106.05688", "submitter": "Sallam Abualhaija", "authors": "Orlando Amaral, Sallam Abualhaija, Damiano Torre, Mehrdad Sabetzadeh,\n  Lionel C. Briand", "title": "AI-enabled Automation for Completeness Checking of Privacy Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advances in information sharing have raised concerns about data\nprotection. Privacy policies contain privacy-related requirements about how the\npersonal data of individuals will be handled by an organization or a software\nsystem (e.g., a web service or an app). In Europe, privacy policies are subject\nto compliance with the General Data Protection Regulation (GDPR). A\nprerequisite for GDPR compliance checking is to verify whether the content of a\nprivacy policy is complete according to the provisions of GDPR. Incomplete\nprivacy policies might result in large fines on violating organization as well\nas incomplete privacy-related software specifications. Manual completeness\nchecking is both time-consuming and error-prone. In this paper, we propose\nAI-based automation for the completeness checking of privacy policies. Through\nsystematic qualitative methods, we first build two artifacts to characterize\nthe privacy-related provisions of GDPR, namely a conceptual model and a set of\ncompleteness criteria. Then, we develop an automated solution on top of these\nartifacts by leveraging a combination of natural language processing and\nsupervised machine learning. Specifically, we identify the GDPR-relevant\ninformation content in privacy policies and subsequently check them against the\ncompleteness criteria. To evaluate our approach, we collected 234 real privacy\npolicies from the fund industry. Over a set of 48 unseen privacy policies, our\napproach detected 300 of the total of 334 violations of some completeness\ncriteria correctly, while producing 23 false positives. The approach thus has a\nprecision of 92.9% and recall of 89.8%. Compared to a baseline that applies\nkeyword search only, our approach results in an improvement of 24.5% in\nprecision and 38% in recall.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 12:10:51 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Amaral", "Orlando", ""], ["Abualhaija", "Sallam", ""], ["Torre", "Damiano", ""], ["Sabetzadeh", "Mehrdad", ""], ["Briand", "Lionel C.", ""]]}, {"id": "2106.05702", "submitter": "Stjepan Gro\\v{s}", "authors": "Stjepan Gro\\v{s}", "title": "Myths and Misconceptions about Attackers and Attacks", "comments": "8 pages, 27 reference. This paper is work in progress and as such may\n  contain inaccuracies, missing or unfinished sentences and paragraphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper is based on a three year project during which we studied\nattackers' behavior, reading military planning literature, and thinking on how\nwould we do the same things they do, and what problems would we, as attackers,\nface. This research is still ongoing, but while participating in applications\nfor other projects and talking to cyber security experts we constantly face the\nsame issues, namely attackers' behavior is not well understood, and\nconsequently, there are a number of misconceptions floating around that are\nsimply not true, or are only partially true. This is actually expected as\nsomeone who casually follows news about incidents easily gets impression that\nattackers and attacks are everywhere and every one is under attack. Our goal in\nthis paper is to debunk these myths, to show what attackers really can and can\nnot, what dilemmas they face, what we don't know about attackers and attacks,\netc. The conclusion is that, while attackers do have upper hand, they don't\nhave absolute advantage, i.e. they also operate in an uncertain environment.\nKnowing this, means that defenses could be well established.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 12:40:29 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Gro\u0161", "Stjepan", ""]]}, {"id": "2106.05756", "submitter": "Zhuo Chen", "authors": "Zhuo Chen, Lei Wu, Jing Cheng, Yubo Hu, Yajin Zhou, Zhushou Tang,\n  Yexuan Chen, Jinku Li and Kui Ren", "title": "Lifting The Grey Curtain: A First Look at the Ecosystem of CULPRITWARE", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile apps are extensively involved in cyber-crimes. Some apps are malware\nwhich compromise users' devices, while some others may lead to privacy leakage.\nApart from them, there also exist apps which directly make profit from victims\nthrough deceiving, threatening or other criminal actions. We name these apps as\nCULPRITWARE. They have become emerging threats in recent years. However, the\ncharacteristics and the ecosystem of CULPRITWARE remain mysterious. This paper\ntakes the first step towards systematically studying CULPRITWARE and its\necosystem. Specifically, we first characterize CULPRITWARE by categorizing and\ncomparing them with benign apps and malware. The result shows that CULPRITWARE\nhave unique features, e.g., the usage of app generators (25.27%) deviates from\nthat of benign apps (5.08%) and malware (0.43%). Such a discrepancy can be used\nto distinguish CULPRITWARE from benign apps and malware. Then we understand the\nstructure of the ecosystem by revealing the four participating entities (i.e.,\ndeveloper, agent, operator and reaper) and the workflow. After that, we further\nreveal the characteristics of the ecosystem by studying the participating\nentities. Our investigation shows that the majority of CULPRITWARE (at least\n52.08%) are propagated through social media rather than the official app\nmarkets, and most CULPRITWARE (96%) indirectly rely on the covert fourth-party\npayment services to transfer the profits. Our findings shed light on the\necosystem, and can facilitate the community and law enforcement authorities to\nmitigate the threats. We will release the source code of our tools to engage\nthe community.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:03:07 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 02:40:49 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chen", "Zhuo", ""], ["Wu", "Lei", ""], ["Cheng", "Jing", ""], ["Hu", "Yubo", ""], ["Zhou", "Yajin", ""], ["Tang", "Zhushou", ""], ["Chen", "Yexuan", ""], ["Li", "Jinku", ""], ["Ren", "Kui", ""]]}, {"id": "2106.05761", "submitter": "Diptapriyo Majumdar", "authors": "Jason Crampton, Eduard Eiben, Gregory Gutin, Daniel Karapetyan,\n  Diptapriyo Majumdar", "title": "Valued Authorization Policy Existence Problem: Theory and Experiments", "comments": "32 pages, 5 figures. Preliminary version appeared in SACMAT 2021\n  (https://doi.org/10.1145/3450569.3463571). Some of the theoretical results\n  (algorithms) have been improved. Computational experiments have been added to\n  this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that many problems of satisfiability and resiliency in\nworkflows may be viewed as special cases of the authorization policy existence\nproblem (APEP), which returns an authorization policy if one exists and 'No'\notherwise. However, in many practical settings it would be more useful to\nobtain a 'least bad' policy than just a 'No', where 'least bad' is\ncharacterized by some numerical value indicating the extent to which the policy\nviolates the base authorization relation and constraints. Accordingly, we\nintroduce the Valued APEP, which returns an authorization policy of minimum\nweight, where the (non-negative) weight is determined by the constraints\nviolated by the returned solution. We then establish a number of results\nconcerning the parameterized complexity of Valued APEP. We prove that the\nproblem is fixed-parameter tractable (FPT) if the set of constraints satisfies\ntwo restrictions, but is intractable if only one of these restrictions holds.\n(Most constraints known to be of practical use satisfy both restrictions.) We\nalso introduce a new type of resiliency for workflow satisfiability problem,\nshow how it can be addressed using Valued APEP and use this to build a set of\nbenchmark instances for Valued APEP. Following a set of computational\nexperiments with two mixed integer programming (MIP) formulations, we\ndemonstrate that the Valued APEP formulation based on the user profile concept\nhas FPT-like running time and usually significantly outperforms a naive\nformulation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:06:18 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 13:58:35 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Crampton", "Jason", ""], ["Eiben", "Eduard", ""], ["Gutin", "Gregory", ""], ["Karapetyan", "Daniel", ""], ["Majumdar", "Diptapriyo", ""]]}, {"id": "2106.05825", "submitter": "Mohammad Samavatian", "authors": "Mohammad Hossein Samavatian, Saikat Majumdar, Kristin Barber, Radu\n  Teodorescu", "title": "HASI: Hardware-Accelerated Stochastic Inference, A Defense Against\n  Adversarial Machine Learning Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) are employed in an increasing number of\napplications, some of which are safety critical. Unfortunately, DNNs are known\nto be vulnerable to so-called adversarial attacks that manipulate inputs to\ncause incorrect results that can be beneficial to an attacker or damaging to\nthe victim. Multiple defenses have been proposed to increase the robustness of\nDNNs. In general, these defenses have high overhead, some require\nattack-specific re-training of the model or careful tuning to adapt to\ndifferent attacks.\n  This paper presents HASI, a hardware-accelerated defense that uses a process\nwe call stochastic inference to detect adversarial inputs. We show that by\ncarefully injecting noise into the model at inference time, we can\ndifferentiate adversarial inputs from benign ones. HASI uses the output\ndistribution characteristics of noisy inference compared to a non-noisy\nreference to detect adversarial inputs. We show an adversarial detection rate\nof 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds\nthe detection rate of the state of the art approaches, with a much lower\noverhead. We demonstrate two software/hardware-accelerated co-designs, which\nreduces the performance impact of stochastic inference to 1.58X-2X relative to\nthe unprotected baseline, compared to 15X-20X overhead for a software-only GPU\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:31:28 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 14:01:49 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Samavatian", "Mohammad Hossein", ""], ["Majumdar", "Saikat", ""], ["Barber", "Kristin", ""], ["Teodorescu", "Radu", ""]]}, {"id": "2106.05917", "submitter": "Adam Aviv", "authors": "David G. Balash and Dongkun Kim and Darikia Shaibekova and Rahel A.\n  Fainchtein and Micah Sherr and Adam J. Aviv", "title": "Examining the Examiners: Students' Privacy and Security Perceptions of\n  Online Proctoring Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response to the Covid-19 pandemic, educational institutions quickly\ntransitioned to remote learning. The problem of how to perform student\nassessment in an online environment has become increasingly relevant, leading\nmany institutions and educators to turn to online proctoring services to\nadminister remote exams. These services employ various student monitoring\nmethods to curb cheating, including restricted (\"lockdown\") browser modes,\nvideo/screen monitoring, local network traffic analysis, and eye tracking. In\nthis paper, we explore the security and privacy perceptions of the student\ntest-takers being proctored. We analyze user reviews of proctoring services'\nbrowser extensions and subsequently perform an online survey (n=102). Our\nfindings indicate that participants are concerned about both the amount and the\npersonal nature of the information shared with the exam proctoring companies.\nHowever, many participants also recognize a trade-off between pandemic safety\nconcerns and the arguably invasive means by which proctoring services ensure\nexam integrity. Our findings also suggest that institutional power dynamics and\nstudents' trust in their institutions may dissuade students' opposition to\nremote proctoring.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:03:13 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Balash", "David G.", ""], ["Kim", "Dongkun", ""], ["Shaibekova", "Darikia", ""], ["Fainchtein", "Rahel A.", ""], ["Sherr", "Micah", ""], ["Aviv", "Adam J.", ""]]}, {"id": "2106.05997", "submitter": "Edoardo Manino", "authors": "Luiz Sena, Xidan Song, Erickson Alves, Iury Bessa, Edoardo Manino,\n  Lucas Cordeiro", "title": "Verifying Quantized Neural Networks using SMT-Based Model Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.LO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) are being deployed on an increasing number\nof safety-critical applications, including autonomous cars and medical\ndiagnosis. However, concerns about their reliability have been raised due to\ntheir black-box nature and apparent fragility to adversarial attacks. Here, we\ndevelop and evaluate a symbolic verification framework using incremental model\nchecking (IMC) and satisfiability modulo theories (SMT) to check for\nvulnerabilities in ANNs. More specifically, we propose several ANN-related\noptimizations for IMC, including invariant inference via interval analysis and\nthe discretization of non-linear activation functions. With this, we can\nprovide guarantees on the safe behavior of ANNs implemented both in\nfloating-point and fixed-point (quantized) arithmetic. In this regard, our\nverification approach was able to verify and produce adversarial examples for\n52 test cases spanning image classification and general machine learning\napplications. For small- to medium-sized ANN, our approach completes most of\nits verification runs in minutes. Moreover, in contrast to most\nstate-of-the-art methods, our approach is not restricted to specific choices of\nactivation functions or non-quantized representations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 18:27:45 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Sena", "Luiz", ""], ["Song", "Xidan", ""], ["Alves", "Erickson", ""], ["Bessa", "Iury", ""], ["Manino", "Edoardo", ""], ["Cordeiro", "Lucas", ""]]}, {"id": "2106.06000", "submitter": "Stjepan Gro\\v{s}", "authors": "Dalibor Gernhardt, Stjepan Gro\\v{s}", "title": "Use of a non-peer reviewed sources in cyber-security scientific research", "comments": "9 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most publicly available data on cyber incidents comes from private companies\nand non-academic sources. Common sources of information include various\nsecurity bulletins, white papers, reports, court cases, and blog posts\ndescribing specific events, often from a single point of view, followed by\noccasional academic sources, usually conference proceedings. The main\ncharacteristics of the available data sources are: lack of peer review and\nunavailability of confidential data. In this paper, we use an indirect approach\nto identify trusted sources used in scientific work. We analyze how top-rated\npeer reviewed literature relies on the use of non-peer reviewed sources on\ncybersecurity incidents. To identify current non-peer reviewed sources on\ncybersecurity we analyze references in top rated peer reviewed computer\nsecurity conferences. We also analyze how non-peer reviewed sources are used,\nto motivate or support research. We examined 808 articles from top conferences\nin field of computer security. The result of this work are list of the most\ncommonly used non-peer reviewed data sources and information about the context\nin which this data is used. Since these sources are accepted in top\nconferences, other researchers can consider them in their future research. To\nthe best of our knowledge, analysis on how non-peer reviewed sources are used\nin cyber-security scientific research has not been done before.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 18:39:59 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Gernhardt", "Dalibor", ""], ["Gro\u0161", "Stjepan", ""]]}, {"id": "2106.06041", "submitter": "Jongmin Yoon", "authors": "Jongmin Yoon, Sung Ju Hwang, Juho Lee", "title": "Adversarial purification with Score-based generative models", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While adversarial training is considered as a standard defense method against\nadversarial attacks for image classifiers, adversarial purification, which\npurifies attacked images into clean images with a standalone purification\nmodel, has shown promises as an alternative defense method. Recently, an\nEnergy-Based Model (EBM) trained with Markov-Chain Monte-Carlo (MCMC) has been\nhighlighted as a purification model, where an attacked image is purified by\nrunning a long Markov-chain using the gradients of the EBM. Yet, the\npracticality of the adversarial purification using an EBM remains questionable\nbecause the number of MCMC steps required for such purification is too large.\nIn this paper, we propose a novel adversarial purification method based on an\nEBM trained with Denoising Score-Matching (DSM). We show that an EBM trained\nwith DSM can quickly purify attacked images within a few steps. We further\nintroduce a simple yet effective randomized purification scheme that injects\nrandom noises into images before purification. This process screens the\nadversarial perturbations imposed on images by the random noises and brings the\nimages to the regime where the EBM can denoise well. We show that our\npurification method is robust against various attacks and demonstrate its\nstate-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 04:35:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Yoon", "Jongmin", ""], ["Hwang", "Sung Ju", ""], ["Lee", "Juho", ""]]}, {"id": "2106.06046", "submitter": "Mohit Kumar", "authors": "Mohit Kumar, Bernhard A. Moser, Lukas Fischer, Bernhard Freudenthaler", "title": "Information Theoretic Evaluation of Privacy-Leakage, Interpretability,\n  and Transferability for a Novel Trustworthy AI Framework", "comments": "arXiv admin note: text overlap with arXiv:2105.04615,\n  arXiv:2104.07060", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Guidelines and principles of trustworthy AI should be adhered to in practice\nduring the development of AI systems. This work suggests a novel information\ntheoretic trustworthy AI framework based on the hypothesis that information\ntheory enables taking into account the ethical AI principles during the\ndevelopment of machine learning and deep learning models via providing a way to\nstudy and optimize the inherent tradeoffs between trustworthy AI principles.\nUnder the proposed framework, a unified approach to ``privacy-preserving\ninterpretable and transferable learning'' is considered to introduce the\ninformation theoretic measures for privacy-leakage, interpretability, and\ntransferability. A technique based on variational optimization, employing\n\\emph{conditionally deep autoencoders}, is developed for practically\ncalculating the defined information theoretic measures for privacy-leakage,\ninterpretability, and transferability.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 09:47:06 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:11:58 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 10:42:00 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Kumar", "Mohit", ""], ["Moser", "Bernhard A.", ""], ["Fischer", "Lukas", ""], ["Freudenthaler", "Bernhard", ""]]}, {"id": "2106.06053", "submitter": "Tasos Spiliotopoulos", "authors": "Tasos Spiliotopoulos, Dave Horsfall, Magdalene Ng, Kovila Coopamootoo,\n  Aad van Moorsel, Karen Elliott", "title": "Identifying and Supporting Financially Vulnerable Consumers in a\n  Privacy-Preserving Manner: A Use Case Using Decentralised Identifiers and\n  Verifiable Credentials", "comments": "Published in the ACM CHI 2021 workshop on Designing for New Forms of\n  Vulnerability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vulnerable individuals have a limited ability to make reasonable financial\ndecisions and choices and, thus, the level of care that is appropriate to be\nprovided to them by financial institutions may be different from that required\nfor other consumers. Therefore, identifying vulnerability is of central\nimportance for the design and effective provision of financial services and\nproducts. However, validating the information that customers share and\nrespecting their privacy are both particularly important in finance and this\nposes a challenge for identifying and caring for vulnerable populations. This\nposition paper examines the potential of the combination of two emerging\ntechnologies, Decentralized Identifiers (DIDs) and Verifiable Credentials\n(VCs), for the identification of vulnerable consumers in finance in an\nefficient and privacy-preserving manner.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 21:05:34 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Spiliotopoulos", "Tasos", ""], ["Horsfall", "Dave", ""], ["Ng", "Magdalene", ""], ["Coopamootoo", "Kovila", ""], ["van Moorsel", "Aad", ""], ["Elliott", "Karen", ""]]}, {"id": "2106.06056", "submitter": "Linyi Li", "authors": "Jiawei Zhang and Linyi Li and Huichen Li and Xiaolu Zhang and Shuang\n  Yang and Bo Li", "title": "Progressive-Scale Boundary Blackbox Attack via Projective Gradient\n  Estimation", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Boundary based blackbox attack has been recognized as practical and\neffective, given that an attacker only needs to access the final model\nprediction. However, the query efficiency of it is in general high especially\nfor high dimensional image data. In this paper, we show that such efficiency\nhighly depends on the scale at which the attack is applied, and attacking at\nthe optimal scale significantly improves the efficiency. In particular, we\npropose a theoretical framework to analyze and show three key characteristics\nto improve the query efficiency. We prove that there exists an optimal scale\nfor projective gradient estimation. Our framework also explains the\nsatisfactory performance achieved by existing boundary black-box attacks. Based\non our theoretical framework, we propose Progressive-Scale enabled projective\nBoundary Attack (PSBA) to improve the query efficiency via progressive scaling\ntechniques. In particular, we employ Progressive-GAN to optimize the scale of\nprojections, which we call PSBA-PGAN. We evaluate our approach on both spatial\nand frequency scales. Extensive experiments on MNIST, CIFAR-10, CelebA, and\nImageNet against different models including a real-world face recognition API\nshow that PSBA-PGAN significantly outperforms existing baseline attacks in\nterms of query efficiency and attack success rate. We also observe relatively\nstable optimal scales for different models and datasets. The code is publicly\navailable at https://github.com/AI-secure/PSBA.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 21:13:41 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhang", "Jiawei", ""], ["Li", "Linyi", ""], ["Li", "Huichen", ""], ["Zhang", "Xiaolu", ""], ["Yang", "Shuang", ""], ["Li", "Bo", ""]]}, {"id": "2106.06065", "submitter": "Igor Korkin", "authors": "Igor Korkin", "title": "Windows Kernel Hijacking Is Not an Option: MemoryRanger Comes to the\n  Rescue Again", "comments": "29 pages, 7 figures. Korkin, I. (2021, June 10). Windows Kernel\n  Hijacking Is Not an Option: MemoryRanger Comes to the Rescue Again. Journal\n  of Digital Forensics, Security and Law, Vol 16, No.1, Article 4. Available\n  at: https://commons.erau.edu/jdfsl/vol16/iss1/4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The security of a computer system depends on OS kernel protection. It is\ncrucial to reveal and inspect new attacks on kernel data, as these are used by\nhackers. The purpose of this paper is to continue research into attacks on\ndynamically allocated data in the Windows OS kernel and demonstrate the\ncapacity of MemoryRanger to prevent these attacks. This paper discusses three\nnew hijacking attacks on kernel data, which are based on bypassing OS security\nmechanisms. The first two hijacking attacks result in illegal access to files\nopen in exclusive access. The third attack escalates process privileges,\nwithout applying token swapping. Although Windows security experts have issued\nnew protection features, access attempts to the dynamically allocated data in\nthe kernel are not fully controlled. MemoryRanger hypervisor is designed to\nfill this security gap. The updated MemoryRanger prevents these new attacks as\nwell as supporting the Windows 10 1903 x64.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 21:56:49 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Korkin", "Igor", ""]]}, {"id": "2106.06089", "submitter": "Maximilian Lam", "authors": "Maximilian Lam, Gu-Yeon Wei, David Brooks, Vijay Janapa Reddi, Michael\n  Mitzenmacher", "title": "Gradient Disaggregation: Breaking Privacy in Federated Learning by\n  Reconstructing the User Participant Matrix", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that aggregated model updates in federated learning may be insecure.\nAn untrusted central server may disaggregate user updates from sums of updates\nacross participants given repeated observations, enabling the server to recover\nprivileged information about individual users' private training data via\ntraditional gradient inference attacks. Our method revolves around\nreconstructing participant information (e.g: which rounds of training users\nparticipated in) from aggregated model updates by leveraging summary\ninformation from device analytics commonly used to monitor, debug, and manage\nfederated learning systems. Our attack is parallelizable and we successfully\ndisaggregate user updates on settings with up to thousands of participants. We\nquantitatively and qualitatively demonstrate significant improvements in the\ncapability of various inference attacks on the disaggregated updates. Our\nattack enables the attribution of learned properties to individual users,\nviolating anonymity, and shows that a determined central server may undermine\nthe secure aggregation protocol to break individual users' data privacy in\nfederated learning.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 23:55:28 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lam", "Maximilian", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""], ["Reddi", "Vijay Janapa", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "2106.06094", "submitter": "James Bartusek", "authors": "James Bartusek and Giulio Malavolta", "title": "Indistinguishability Obfuscation of Null Quantum Circuits and\n  Applications", "comments": "arXiv admin note: text overlap with arXiv:1912.04769 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the notion of indistinguishability obfuscation for null quantum\ncircuits (quantum null-iO). We present a construction assuming: - The quantum\nhardness of learning with errors (LWE). - Post-quantum indistinguishability\nobfuscation for classical circuits. - A notion of ''dual-mode'' classical\nverification of quantum computation (CVQC).\n  We give evidence that our notion of dual-mode CVQC exists by proposing a\nscheme that is secure assuming LWE in the quantum random oracle model (QROM).\n  Then we show how quantum null-iO enables a series of new cryptographic\nprimitives that, prior to our work, were unknown to exist even making heuristic\nassumptions. Among others, we obtain the first witness encryption scheme for\nQMA, the first publicly verifiable non-interactive zero-knowledge (NIZK) scheme\nfor QMA, and the first attribute-based encryption (ABE) scheme for BQP.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 00:08:14 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Bartusek", "James", ""], ["Malavolta", "Giulio", ""]]}, {"id": "2106.06136", "submitter": "Rui Zhang", "authors": "Rui Zhang, Rui Xue, Ling Liu", "title": "Security and Privacy for Healthcare Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healthcare blockchains provide an innovative way to store healthcare\ninformation, execute healthcare transactions, and build trust for healthcare\ndata sharing and data integration in a decentralized open healthcare network\nenvironment. Although the healthcare blockchain technology has attracted broad\ninterests and attention in industry, government and academia, the security and\nprivacy concerns remain the focus of debate when deploying blockchains for\ninformation sharing in the healthcare sector from business operation to\nresearch collaboration. This paper focuses on the security and privacy\nrequirements for medical data sharing using blockchain, and provides a\ncomprehensive analysis of the security and privacy risks and requirements,\naccompanied by technical solution techniques and strategies. First, we discuss\nthe security and privacy requirements and attributes required for electronic\nmedical data sharing by deploying the healthcare blockchain. Second, we\ncategorize existing efforts into three reference blockchain usage scenarios for\nelectronic medical data sharing, and discuss the technologies for implementing\nthese security and privacy properties in the three categories of usage\nscenarios for healthcare blockchain, such as anonymous signatures,\nattribute-based encryption, zero-knowledge proofs, verification techniques for\nsmart contract security. Finally, we discuss other potential blockchain\napplication scenarios in healthcare sector. We conjecture that this survey will\nhelp healthcare professionals, decision makers, and healthcare service\ndevelopers to gain technical and intuitive insights into the security and\nprivacy of healthcare blockchains in terms of concepts, risks, requirements,\ndevelopment and deployment technologies and systems.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:47:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhang", "Rui", ""], ["Xue", "Rui", ""], ["Liu", "Ling", ""]]}, {"id": "2106.06272", "submitter": "Stefano Maria Nicoletti", "authors": "Christina Kolb, Stefano M. Nicoletti, Marijn Peppelman, Mari\\\"elle\n  Stoelinga", "title": "Model-based Safety and Security Co-analysis: a Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey the state-of-the-art on model-based formalisms for safety and\nsecurity analysis, where safety refers to the absence of unintended failures,\nand security absence of malicious attacks. We consider ten model-based\nformalisms, comparing their modeling principles, the interaction between safety\nand security, and analysis methods. In each formalism, we model the classical\nLocked Door Example where possible. Our key finding is that the exact nature of\nsafety-security interaction is still ill-understood. Existing formalisms merge\nprevious safety and security formalisms, without introducing specific\nconstructs to model safety-security interactions, or metrics to analyze trade\noffs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:38:23 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Kolb", "Christina", ""], ["Nicoletti", "Stefano M.", ""], ["Peppelman", "Marijn", ""], ["Stoelinga", "Mari\u00eblle", ""]]}, {"id": "2106.06306", "submitter": "Irene Villa", "authors": "Carla Mascia, Massimiliano Sala, Irene Villa", "title": "A survey on Functional Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Encryption (FE) expands traditional public-key encryption in two\ndifferent ways: it supports fine-grained access control and it allows to learn\na function of the encrypted data. In this paper, we review all FE classes,\ndescribing their functionalities and main characteristics. In particular, for\neach class we mention several schemes, providing their security assumptions and\ncomparing their properties. To our knowledge, this is the first survey that\nencompasses the entire FE family.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 10:54:49 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Mascia", "Carla", ""], ["Sala", "Massimiliano", ""], ["Villa", "Irene", ""]]}, {"id": "2106.06312", "submitter": "Zhaomin Wu", "authors": "Zhaomin Wu, Qinbin Li, Bingsheng He", "title": "Exploiting Record Similarity for Practical Vertical Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the privacy of machine learning has drawn increasing attention, federated\nlearning is introduced to enable collaborative learning without revealing raw\ndata. Notably, \\textit{vertical federated learning} (VFL), where parties share\nthe same set of samples but only hold partial features, has a wide range of\nreal-world applications. However, existing studies in VFL rarely study the\n``record linkage'' process. They either design algorithms assuming the data\nfrom different parties have been linked or use simple linkage methods like\nexact-linkage or top1-linkage. These approaches are unsuitable for many\napplications, such as the GPS location and noisy titles requiring fuzzy\nmatching. In this paper, we design a novel similarity-based VFL framework,\nFedSim, which is suitable for more real-world applications and achieves higher\nperformance on traditional VFL tasks. Moreover, we theoretically analyze the\nprivacy risk caused by sharing similarities. Our experiments on three synthetic\ndatasets and five real-world datasets with various similarity metrics show that\nFedSim consistently outperforms other state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 11:09:53 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wu", "Zhaomin", ""], ["Li", "Qinbin", ""], ["He", "Bingsheng", ""]]}, {"id": "2106.06361", "submitter": "Fanchao Qi", "authors": "Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun", "title": "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word\n  Substitution", "comments": "Accepted by the main conference of ACL-IJCNLP as a long paper.\n  Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent studies show that neural natural language processing (NLP) models are\nvulnerable to backdoor attacks. Injected with backdoors, models perform\nnormally on benign examples but produce attacker-specified predictions when the\nbackdoor is activated, presenting serious security threats to real-world\napplications. Since existing textual backdoor attacks pay little attention to\nthe invisibility of backdoors, they can be easily detected and blocked. In this\nwork, we present invisible backdoors that are activated by a learnable\ncombination of word substitution. We show that NLP models can be injected with\nbackdoors that lead to a nearly 100% attack success rate, whereas being highly\ninvisible to existing defense strategies and even human inspections. The\nresults raise a serious alarm to the security of NLP models, which requires\nfurther research to be resolved. All the data and code of this paper are\nreleased at https://github.com/thunlp/BkdAtk-LWS.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:03:17 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Qi", "Fanchao", ""], ["Yao", "Yuan", ""], ["Xu", "Sophia", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2106.06389", "submitter": "Kaihua Qin", "authors": "Kaihua Qin, Liyi Zhou, Pablo Gamito, Philipp Jovanovic and Arthur\n  Gervais", "title": "An Empirical Study of DeFi Liquidations: Incentives, Risks, and\n  Instabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial speculators often seek to increase their potential gains with\nleverage. Debt is a popular form of leverage, and with over 39.88B USD of total\nvalue locked (TVL), the Decentralized Finance (DeFi) lending markets are\nthriving. Debts, however, entail the risks of liquidation, the process of\nselling the debt collateral at a discount to liquidators. Nevertheless, few\nquantitative insights are known about the existing liquidation mechanisms.\n  In this paper, to the best of our knowledge, we are the first to study the\nbreadth of the borrowing and lending markets of the Ethereum DeFi ecosystem. We\nfocus on Aave, Compound, MakerDAO, and dYdX, which collectively represent over\n85% of the lending market on Ethereum. Given extensive liquidation data\nmeasurements and insights, we systematize the prevalent liquidation mechanisms\nand are the first to provide a methodology to compare them objectively. We find\nthat the existing liquidation designs well incentivize liquidators but sell\nexcessive amounts of discounted collateral at the borrowers' expenses. We\nmeasure various risks that liquidation participants are exposed to and quantify\nthe instabilities of existing lending protocols. Moreover, we propose an\noptimal strategy that allows liquidators to increase their liquidation profit,\nwhich may aggravate the loss of borrowers.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:49:33 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Qin", "Kaihua", ""], ["Zhou", "Liyi", ""], ["Gamito", "Pablo", ""], ["Jovanovic", "Philipp", ""], ["Gervais", "Arthur", ""]]}, {"id": "2106.06439", "submitter": "Divakar Singh Mr", "authors": "Divakar Singh", "title": "An Image Forensic Technique Based on JPEG Ghosts", "comments": "8 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented growth in the easy availability of photo-editing tools has\nendangered the power of digital images.An image was supposed to be worth more\nthan a thousand words,but now this can be said only if it can be authenticated\northe integrity of the image can be proved to be intact. In thispaper, we\npropose a digital image forensic technique for JPEG images. It can detect any\nforgery in the image if the forged portion called a ghost image is having a\ncompression quality different from that of the cover image. It is based on\nresaving the JPEG image at different JPEG qualities, and the detection of the\nforged portion is maximum when it is saved at the same JPEG quality as the\ncover image. Also, we can precisely predictthe JPEG quality of the cover image\nby analyzing the similarity using Structural Similarity Index Measure (SSIM) or\nthe energyof the images. The first maxima in SSIM or the first minima inenergy\ncorrespond to the cover image JPEG quality. We created adataset for varying\nJPEG compression qualities of the ghost and the cover images and validated the\nscalability of the experimental results.We also, experimented with varied\nattack scenarios, e.g. high-quality ghost image embedded in low quality of\ncover image,low-quality ghost image embedded in high-quality of cover image,and\nghost image and cover image both at the same quality.The proposed method is\nable to localize the tampered portions accurately even for forgeries as small\nas 10x10 sized pixel blocks.Our technique is also robust against other attack\nscenarios like copy-move forgery, inserting text into image, rescaling\n(zoom-out/zoom-in) ghost image and then pasting on cover image.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:52:43 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 02:42:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Singh", "Divakar", ""]]}, {"id": "2106.06453", "submitter": "Mark Simkin", "authors": "Nils Fleischhacker and Kasper Green Larsen and and Mark Simkin", "title": "Property-Preserving Hash Functions from Standard Assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Property-preserving hash functions allow for compressing long inputs $x_0$\nand $x_1$ into short hashes $h(x_0)$ and $h(x_1)$ in a manner that allows for\ncomputing a predicate $P(x_0, x_1)$ given only the two hash values without\nhaving access to the original data. Such hash functions are said to be\nadversarially robust if an adversary that gets to pick $x_0$ and $x_1$ after\nthe hash function has been sampled, cannot find inputs for which the predicate\nevaluated on the hash values outputs the incorrect result.\n  In this work we construct robust property-preserving hash functions for the\nhamming-distance predicate which distinguishes inputs with a hamming distance\nat least some threshold $t$ from those with distance less than $t$. The\nsecurity of the construction is based on standard lattice hardness assumptions.\n  Our construction has several advantages over the best known previous\nconstruction by Fleischhacker and Simkin. Our construction relies on a single\nwell-studied hardness assumption from lattice cryptography whereas the previous\nwork relied on a newly introduced family of computational hardness assumptions.\nIn terms of computational effort, our construction only requires a small number\nof modular additions per input bit, whereas previously several exponentiations\nper bit as well as the interpolation and evaluation of high-degree polynomials\nover large fields were required. An additional benefit of our construction is\nthat the description of the hash function can be compressed to $\\lambda$ bits\nassuming a random oracle. Previous work has descriptions of length\n$\\mathcal{O}(\\ell \\lambda)$ bits for input bit-length $\\ell$, which has a\nsecret structure and thus cannot be compressed.\n  We prove a lower bound on the output size of any property-preserving hash\nfunction for the hamming distance predicate. The bound shows that the size of\nour hash value is not far from optimal.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 15:21:38 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Fleischhacker", "Nils", ""], ["Larsen", "Kasper Green", ""], ["Simkin", "and Mark", ""]]}, {"id": "2106.06603", "submitter": "Casey Meehan", "authors": "Casey Meehan, Amrita Roy Chowdhury, Kamalika Chaudhuri, Somesh Jha", "title": "A Shuffling Framework for Local Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ldp deployments are vulnerable to inference attacks as an adversary can link\nthe noisy responses to their identity and subsequently, auxiliary information\nusing the order of the data. An alternative model, shuffle DP, prevents this by\nshuffling the noisy responses uniformly at random. However, this limits the\ndata learnability -- only symmetric functions (input order agnostic) can be\nlearned. In this paper, we strike a balance and propose a generalized shuffling\nframework that interpolates between the two deployment models. We show that\nsystematic shuffling of the noisy responses can thwart specific inference\nattacks while retaining some meaningful data learnability. To this end, we\npropose a novel privacy guarantee, d-sigma privacy, that captures the privacy\nof the order of a data sequence. d-sigma privacy allows tuning the granularity\nat which the ordinal information is maintained, which formalizes the degree the\nresistance to inference attacks trading it off with data learnability.\nAdditionally, we propose a novel shuffling mechanism that can achieve d-sigma\nprivacy and demonstrate the practicality of our mechanism via evaluation on\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:36:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Meehan", "Casey", ""], ["Chowdhury", "Amrita Roy", ""], ["Chaudhuri", "Kamalika", ""], ["Jha", "Somesh", ""]]}, {"id": "2106.06640", "submitter": "Salvador El\\'ias Venegas-Andraca", "authors": "Marcos Allende, Diego L\\'opez Le\\'on, Sergio Cer\\'on, Antonio Leal,\n  Adri\\'an Pareja, Marcelo Da Silva, Alejandro Pardo, Duncan Jones, David\n  Worrall, Ben Merriman, Jonathan Gilmore, Nick Kitchener, Salvador E.\n  Venegas-Andraca", "title": "Quantum-resistance in blockchain networks", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes the work carried out by the Inter-American Development\nBank, the IDB Lab, LACChain, Cambridge Quantum Computing (CQC), and Tecnologico\nde Monterrey to identify and eliminate quantum threats in blockchain networks.\n  The advent of quantum computing threatens internet protocols and blockchain\nnetworks because they utilize non-quantum resistant cryptographic algorithms.\nWhen quantum computers become robust enough to run Shor's algorithm on a large\nscale, the most used asymmetric algorithms, utilized for digital signatures and\nmessage encryption, such as RSA, (EC)DSA, and (EC)DH, will be no longer secure.\nQuantum computers will be able to break them within a short period of time.\nSimilarly, Grover's algorithm concedes a quadratic advantage for mining blocks\nin certain consensus protocols such as proof of work.\n  Today, there are hundreds of billions of dollars denominated in\ncryptocurrencies that rely on blockchain ledgers as well as the thousands of\nblockchain-based applications storing value in blockchain networks.\nCryptocurrencies and blockchain-based applications require solutions that\nguarantee quantum resistance in order to preserve the integrity of data and\nassets in their public and immutable ledgers. We have designed and developed a\nlayer-two solution to secure the exchange of information between blockchain\nnodes over the internet and introduced a second signature in transactions using\npost-quantum keys. Our versatile solution can be applied to any blockchain\nnetwork. In our implementation, quantum entropy was provided via the IronBridge\nPlatform from CQC and we used LACChain Besu as the blockchain network.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 23:39:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Allende", "Marcos", ""], ["Le\u00f3n", "Diego L\u00f3pez", ""], ["Cer\u00f3n", "Sergio", ""], ["Leal", "Antonio", ""], ["Pareja", "Adri\u00e1n", ""], ["Da Silva", "Marcelo", ""], ["Pardo", "Alejandro", ""], ["Jones", "Duncan", ""], ["Worrall", "David", ""], ["Merriman", "Ben", ""], ["Gilmore", "Jonathan", ""], ["Kitchener", "Nick", ""], ["Venegas-Andraca", "Salvador E.", ""]]}, {"id": "2106.06654", "submitter": "Ivan Evtimov", "authors": "Ivan Evtimov and Ian Covert and Aditya Kusupati and Tadayoshi Kohno", "title": "Disrupting Model Training with Adversarial Shortcuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data is publicly released for human consumption, it is unclear how to\nprevent its unauthorized usage for machine learning purposes. Successful model\ntraining may be preventable with carefully designed dataset modifications, and\nwe present a proof-of-concept approach for the image classification setting. We\npropose methods based on the notion of adversarial shortcuts, which encourage\nmodels to rely on non-robust signals rather than semantic features, and our\nexperiments demonstrate that these measures successfully prevent deep learning\nmodels from achieving high accuracy on real, unmodified data examples.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 01:04:41 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 21:48:44 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Evtimov", "Ivan", ""], ["Covert", "Ian", ""], ["Kusupati", "Aditya", ""], ["Kohno", "Tadayoshi", ""]]}, {"id": "2106.06663", "submitter": "Xu Zou", "authors": "Xu Zou, Qinkai Zheng, Yuxiao Dong, Xinyu Guan, Evgeny Kharlamov,\n  Jialiang Lu, Jie Tang", "title": "TDGIA:Effective Injection Attacks on Graph Neural Networks", "comments": "KDD 2021 research track paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) have achieved promising performance in various\nreal-world applications. However, recent studies have shown that GNNs are\nvulnerable to adversarial attacks. In this paper, we study a\nrecently-introduced realistic attack scenario on graphs -- graph injection\nattack (GIA). In the GIA scenario, the adversary is not able to modify the\nexisting link structure and node attributes of the input graph, instead the\nattack is performed by injecting adversarial nodes into it. We present an\nanalysis on the topological vulnerability of GNNs under GIA setting, based on\nwhich we propose the Topological Defective Graph Injection Attack (TDGIA) for\neffective injection attacks. TDGIA first introduces the topological defective\nedge selection strategy to choose the original nodes for connecting with the\ninjected ones. It then designs the smooth feature optimization objective to\ngenerate the features for the injected nodes. Extensive experiments on\nlarge-scale datasets show that TDGIA can consistently and significantly\noutperform various attack baselines in attacking dozens of defense GNN models.\nNotably, the performance drop on target GNNs resultant from TDGIA is more than\ndouble the damage brought by the best attack solution among hundreds of\nsubmissions on KDD-CUP 2020.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 01:53:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zou", "Xu", ""], ["Zheng", "Qinkai", ""], ["Dong", "Yuxiao", ""], ["Guan", "Xinyu", ""], ["Kharlamov", "Evgeny", ""], ["Lu", "Jialiang", ""], ["Tang", "Jie", ""]]}, {"id": "2106.06765", "submitter": "Stanislav Abaimov", "authors": "Stanislav Abaimov", "title": "Towards a Privacy-preserving Deep Learning-based Network Intrusion\n  Detection in Data Distribution Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data Distribution Service (DDS) is an innovative approach towards\ncommunication in ICS/IoT infrastructure and robotics. Being based on the\ncross-platform and cross-language API to be applicable in any computerised\ndevice, it offers the benefits of modern programming languages and the\nopportunities to develop more complex and advanced systems. However, the DDS\ncomplexity equally increases its vulnerability, while the existing security\nmeasures are limited to plug-ins and static rules, with the rest of the\nsecurity provided by third-party applications and operating system.\nSpecifically, traditional intrusion detection systems (IDS) do not detect any\nanomalies in the publish/subscribe method. With the exponentially growing\nglobal communication exchange, securing DDS is of the utmost importance to\nfutureproofing industrial, public, and even personal devices and systems. This\nreport presents an experimental work on the simulation of several specific\nattacks against DDS, and the application of Deep Learning for their detection.\nThe findings show that even though Deep Learning allows to detect all simulated\nattacks using only metadata analysis, their detection level varies, with some\nof the advanced attacks being harder to detect. The limitations imposed by the\nattempts to preserve privacy significantly decrease the detection rate. The\nreport also reviews the drawbacks and limitations of the Deep Learning approach\nand proposes a set of selected solutions and configurations, that can further\nimprove the DDS security.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 12:53:38 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Abaimov", "Stanislav", ""]]}, {"id": "2106.06895", "submitter": "Tolulope Odetola", "authors": "Tolulope Odetola, Faiq Khalid, Travis Sandefur, Hawzhin Mohammed and\n  Syed Rafay Hasan", "title": "FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have shown impressive performance in\ncomputer vision, natural language processing, and many other applications, but\nthey exhibit high computations and substantial memory requirements. To address\nthese limitations, especially in resource-constrained devices, the use of cloud\ncomputing for CNNs is becoming more popular. This comes with privacy and\nlatency concerns that have motivated the designers to develop embedded hardware\naccelerators for CNNs. However, designing a specialized accelerator increases\nthe time-to-market and cost of production. Therefore, to reduce the\ntime-to-market and access to state-of-the-art techniques, CNN hardware mapping\nand deployment on embedded accelerators are often outsourced to untrusted third\nparties, which is going to be more prevalent in futuristic artificial\nintelligence of things (AIoT) systems. These AIoT systems anticipate horizontal\ncollaboration among different resource-constrained AIoT node devices, where CNN\nlayers are partitioned and these devices collaboratively compute complex CNN\ntasks Therefore, there is a dire need to explore this attack surface for\ndesigning secure embedded hardware accelerators for CNNs. Towards this goal, in\nthis paper, we exploited this attack surface to propose an HT-based attack\ncalled FeSHI. This attack exploits the statistical distribution i.e., Gaussian\ndistribution, of the layer-by-layer feature maps of the CNN to design two\ntriggers for stealthy HT with a very low probability of triggering. To\nillustrate the effectiveness of the proposed attack, we deployed the LeNet and\nLeNet-3D on PYNQ to classify the MNIST and CIFAR-10 datasets, respectively, and\ntested FeSHI. The experimental results show that FeSHI utilizes up to 2% extra\nLUTs, and the overall resource overhead is less than 1% compared to the\noriginal designs\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 01:50:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Odetola", "Tolulope", ""], ["Khalid", "Faiq", ""], ["Sandefur", "Travis", ""], ["Mohammed", "Hawzhin", ""], ["Hasan", "Syed Rafay", ""]]}, {"id": "2106.07029", "submitter": "Riyanka Jena", "authors": "Riyanka Jena, Priyanka Singh, Manoranjan Mohanty", "title": "SSS-PRNU: Privacy-Preserving PRNU Based Camera Attribution using Shamir\n  Secret Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo Response Non-Uniformity(PRNU) noise has proven to be very effective\ntool in camera based forensics. It helps to match a photo to the device that\nclicked it. In today's scenario, where millions and millions of images are\nuploaded every hour, it is very easy to compute this unique PRNU pattern from a\ncouple of shared images on social profiles. This endangers the privacy of the\ncamera owner and becomes a cause of major concern for the privacy-aware\nsociety. We propose SSS-PRNU scheme that facilitates the forensic investigators\nto carry out their crime investigation without breaching the privacy of the\npeople. Thus, maintaining a balance between the two. To preserve privacy,\nextraction of camera fingerprint and PRNU noise for a suspicious image is\ncomputed in a trusted execution environment such as ARM TrustZone. After\nextraction, the sensitive information of camera fingerprint and PRNU noise is\ndistributed into multiple obfuscated shares using Shamir secret sharing(SSS)\nscheme. These shares are information-theoretically secure and leak no\ninformation of underlying content. The encrypted information is distributed to\nmultiple third-part servers where correlation is computed on a share basis\nbetween the camera fingerprint and the PRNU noise. These partial correlation\nvalues are combined together to obtain the final correlation value that becomes\nthe basis for a match decision. Transforming the computation of the correlation\nvalue in the encrypted domain and making it well suited for a distributed\nenvironment is the main contribution of the paper. Experiment results validate\nthe feasibility of the proposed scheme that provides a secure framework for\nPRNU based source camera attribution. The security analysis and evaluation of\ncomputational and storage overheads are performed to analysis the practical\nfeasibility of the scheme.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 15:49:25 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Jena", "Riyanka", ""], ["Singh", "Priyanka", ""], ["Mohanty", "Manoranjan", ""]]}, {"id": "2106.07033", "submitter": "Yaowei Han", "authors": "Yaowei Han, Yang Cao, Masatoshi Yoshikawa", "title": "Understanding the Interplay between Privacy and Robustness in Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is emerging as a promising paradigm of\nprivacy-preserving machine learning, which trains an algorithm across multiple\nclients without exchanging their data samples. Recent works highlighted several\nprivacy and robustness weaknesses in FL and addressed these concerns using\nlocal differential privacy (LDP) and some well-studied methods used in\nconventional ML, separately. However, it is still not clear how LDP affects\nadversarial robustness in FL. To fill this gap, this work attempts to develop a\ncomprehensive understanding of the effects of LDP on adversarial robustness in\nFL. Clarifying the interplay is significant since this is the first step\ntowards a principled design of private and robust FL systems. We certify that\nlocal differential privacy has both positive and negative effects on\nadversarial robustness using theoretical analysis and empirical verification.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 16:01:35 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Han", "Yaowei", ""], ["Cao", "Yang", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "2106.07047", "submitter": "Jatin Chauhan", "authors": "Jatin Chauhan, Karan Bhukar, Manohar Kaul", "title": "Target Model Agnostic Adversarial Attacks with Query Budgets on Language\n  Understanding Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite significant improvements in natural language understanding models\nwith the advent of models like BERT and XLNet, these neural-network based\nclassifiers are vulnerable to blackbox adversarial attacks, where the attacker\nis only allowed to query the target model outputs. We add two more realistic\nrestrictions on the attack methods, namely limiting the number of queries\nallowed (query budget) and crafting attacks that easily transfer across\ndifferent pre-trained models (transferability), which render previous attack\nmodels impractical and ineffective. Here, we propose a target model agnostic\nadversarial attack method with a high degree of attack transferability across\nthe attacked models. Our empirical studies show that in comparison to baseline\nmethods, our method generates highly transferable adversarial sentences under\nthe restriction of limited query budgets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:18:19 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chauhan", "Jatin", ""], ["Bhukar", "Karan", ""], ["Kaul", "Manohar", ""]]}, {"id": "2106.07074", "submitter": "Asaf Shabtai", "authors": "Shai Cohen and Efrat Levy and Avi Shaked and Tair Cohen and Yuval\n  Elovici and Asaf Shabtai", "title": "RadArnomaly: Protecting Radar Systems from Data Manipulation Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar systems are mainly used for tracking aircraft, missiles, satellites,\nand watercraft. In many cases, information regarding the objects detected by\nthe radar system is sent to, and used by, a peripheral consuming system, such\nas a missile system or a graphical user interface used by an operator. Those\nsystems process the data stream and make real-time, operational decisions based\non the data received. Given this, the reliability and availability of\ninformation provided by radar systems has grown in importance. Although the\nfield of cyber security has been continuously evolving, no prior research has\nfocused on anomaly detection in radar systems. In this paper, we present a deep\nlearning-based method for detecting anomalies in radar system data streams. We\npropose a novel technique which learns the correlation between numerical\nfeatures and an embedding representation of categorical features in an\nunsupervised manner. The proposed technique, which allows the detection of\nmalicious manipulation of critical fields in the data stream, is complemented\nby a timing-interval anomaly detection mechanism proposed for the detection of\nmessage dropping attempts. Real radar system data is used to evaluate the\nproposed method. Our experiments demonstrate the method's high detection\naccuracy on a variety of data stream manipulation attacks (average detection\nrate of 88% with 1.59% false alarms) and message dropping attacks (average\ndetection rate of 92% with 2.2% false alarms).\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 19:16:37 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cohen", "Shai", ""], ["Levy", "Efrat", ""], ["Shaked", "Avi", ""], ["Cohen", "Tair", ""], ["Elovici", "Yuval", ""], ["Shabtai", "Asaf", ""]]}, {"id": "2106.07084", "submitter": "Abdullah Giray Ya\\u{g}l{\\i}k\\c{c}{\\i}", "authors": "Abdullah Giray Ya\\u{g}l{\\i}k\\c{c}{\\i}, Jeremie S. Kim, Fabrice Devaux,\n  and Onur Mutlu", "title": "Security Analysis of the Silver Bullet Technique for RowHammer\n  Prevention", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of this document is to study the security properties of the\nSilver Bullet algorithm against worst-case RowHammer attacks. We mathematically\ndemonstrate that Silver Bullet, when properly configured and implemented in a\nDRAM chip, can securely prevent RowHammer attacks. The demonstration focuses on\nthe most representative implementation of Silver Bullet, the patent claiming\nmany implementation possibilities not covered in this demonstration. Our study\nconcludes that Silver Bullet is a promising RowHammer prevention mechanism that\ncan be configured to operate securely against RowHammer attacks at various\nefficiency-area tradeoff points, supporting relatively small hammer count\nvalues (e.g., 1000) and Silver Bullet table sizes (e.g., 1.06KB).\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 20:31:06 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 23:34:56 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ya\u011fl\u0131k\u00e7\u0131", "Abdullah Giray", ""], ["Kim", "Jeremie S.", ""], ["Devaux", "Fabrice", ""], ["Mutlu", "Onur", ""]]}, {"id": "2106.07098", "submitter": "Robert Hallyburton", "authors": "R. Spencer Hallyburton, Yupei Liu, Miroslav Pajic", "title": "Security Analysis of Camera-LiDAR Semantic-Level Fusion Against\n  Black-Box Attacks on Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To enable safe and reliable decision-making, autonomous vehicles (AVs) feed\nsensor data to perception algorithms to understand the environment. Sensor\nfusion, and particularly semantic fusion, with multi-frame tracking is becoming\nincreasingly popular for detecting 3D objects. Recently, it was shown that\nLiDAR-based perception built on deep neural networks is vulnerable to LiDAR\nspoofing attacks. Thus, in this work, we perform the first analysis of\ncamera-LiDAR fusion under spoofing attacks and the first security analysis of\nsemantic fusion in any AV context. We find first that fusion is more successful\nthan existing defenses at guarding against naive spoofing. However, we then\ndefine the frustum attack as a new class of attacks on AVs and find that\nsemantic camera-LiDAR fusion exhibits widespread vulnerability to frustum\nattacks with between 70% and 90% success against target models. Importantly,\nthe attacker needs less than 20 random spoof points on average for successful\nattacks - an order of magnitude less than established maximum capability.\nFinally, we are the first to analyze the longitudinal impact of perception\nattacks by showing the impact of multi-frame attacks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 21:59:19 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 14:58:51 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hallyburton", "R. Spencer", ""], ["Liu", "Yupei", ""], ["Pajic", "Miroslav", ""]]}, {"id": "2106.07105", "submitter": "Ayoub Mars", "authors": "Ayoub Mars, Hussam Ghandour, and Wael Adi", "title": "SRAM-SUC: Ultra-Low Latency Robust Digital PUF", "comments": "13 pages, 15 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Secret Unknown Ciphers (SUC) have been proposed recently as digital\nclone-resistant functions overcoming some of Physical(ly) Unclonable Functions\n(PUF) downsides, mainly their inconsistency because of PUFs analog nature. In\nthis paper, we propose a new practical mechanism for creating internally random\nciphers in modern volatile and non-volatile SoC FPGAs, coined as SRAM-SUC. Each\ncreated random cipher inside a SoC FPGA constitutes a robust digital PUF. This\nwork also presents a class of involutive SUCs, optimized for the targeted SoC\nFPGA architecture, as sample realization of the concept; it deploys a generated\nclass of involutive 8-bit S-Boxes, that are selected randomly from a defined\nlarge set through an internal process inside the SoC FPGA. Hardware and\nsoftware implementations show that the resulting SRAM-SUC has ultra-low latency\ncompared to well-known PUF-based authentication mechanisms. SRAM-SUC requires\nonly $2.88/0.72 \\mu s$ to generate a response for a challenge at 50/200 MHz\nrespectively. This makes SRAM-SUC a promising and appealing solution for\nUltra-Reliable Low Latency Communication (URLLC).\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 22:51:38 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mars", "Ayoub", ""], ["Ghandour", "Hussam", ""], ["Adi", "Wael", ""]]}, {"id": "2106.07141", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Esla Timothy Anzaku, Wesley De Neve, Arnout Van Messem", "title": "Selection of Source Images Heavily Influences the Effectiveness of\n  Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the adoption rate of deep neural networks (DNNs) has tremendously\nincreased in recent years, a solution for their vulnerability against\nadversarial examples has not yet been found. As a result, substantial research\nefforts are dedicated to fix this weakness, with many studies typically using a\nsubset of source images to generate adversarial examples, treating every image\nin this subset as equal. We demonstrate that, in fact, not every source image\nis equally suited for this kind of assessment. To do so, we devise a\nlarge-scale model-to-model transferability scenario for which we meticulously\nanalyze the properties of adversarial examples, generated from every suitable\nsource image in ImageNet by making use of two of the most frequently deployed\nattacks. In this transferability scenario, which involves seven distinct DNN\nmodels, including the recently proposed vision transformers, we reveal that it\nis possible to have a difference of up to $12.5\\%$ in model-to-model\ntransferability success, $1.01$ in average $L_2$ perturbation, and $0.03$\n($8/225$) in average $L_{\\infty}$ perturbation when $1,000$ source images are\nsampled randomly among all suitable candidates. We then take one of the first\nsteps in evaluating the robustness of images used to create adversarial\nexamples, proposing a number of simple but effective methods to identify\nunsuitable source images, thus making it possible to mitigate extreme cases in\nexperimentation and support high-quality benchmarking.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 02:45:45 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 02:32:16 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ozbulak", "Utku", ""], ["Anzaku", "Esla Timothy", ""], ["De Neve", "Wesley", ""], ["Van Messem", "Arnout", ""]]}, {"id": "2106.07153", "submitter": "Terrance Liu", "authors": "Terrance Liu, Giuseppe Vietri, Zhiwei Steven Wu", "title": "Iterative Methods for Private Synthetic Data: Unifying Framework and New\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study private synthetic data generation for query release, where the goal\nis to construct a sanitized version of a sensitive dataset, subject to\ndifferential privacy, that approximately preserves the answers to a large\ncollection of statistical queries. We first present an algorithmic framework\nthat unifies a long line of iterative algorithms in the literature. Under this\nframework, we propose two new methods. The first method, private entropy\nprojection (PEP), can be viewed as an advanced variant of MWEM that adaptively\nreuses past query measurements to boost accuracy. Our second method, generative\nnetworks with the exponential mechanism (GEM), circumvents computational\nbottlenecks in algorithms such as MWEM and PEP by optimizing over generative\nmodels parameterized by neural networks, which capture a rich family of\ndistributions while enabling fast gradient-based optimization. We demonstrate\nthat PEP and GEM empirically outperform existing algorithms. Furthermore, we\nshow that GEM nicely incorporates prior information from public data while\novercoming limitations of PMW^Pub, the existing state-of-the-art method that\nalso leverages public data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 04:19:35 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Terrance", ""], ["Vietri", "Giuseppe", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "2106.07158", "submitter": "Renpeng Zou", "authors": "Dong Ma, Xixiang Lyu, Renpeng Zou", "title": "A Novel Variable K-Pseudonym Scheme Applied to 5G Anonymous Access\n  Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymous access authentication schemes provide users with massive\napplication services while protecting the privacy of users' identities. The\nidentity protection schemes in 3G and 4G are not suitable for 5G anonymous\naccess authentication due to complex computation and pseudonym asynchrony. In\nthis paper, we consider mobile devices with limited resources in the 5G network\nand propose an anonymous access authentication scheme without the Public Key\nInfrastructure. The anonymous access authentication scheme provides users with\nvariable shard pseudonyms to protect users' identities asynchronously. With the\nvariable shared pseudonym, our scheme can ensure user anonymity and resist the\nmark attack, a novel attack aimed at the basic k-pseudonym scheme. Finally, we\nanalyze the scheme with BAN logic analysis and verify the user anonymity.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 04:33:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ma", "Dong", ""], ["Lyu", "Xixiang", ""], ["Zou", "Renpeng", ""]]}, {"id": "2106.07160", "submitter": "Kim Hammar", "authors": "Kim Hammar and Rolf Stadler", "title": "Learning Intrusion Prevention Policies through Optimal Stopping", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study automated intrusion prevention using reinforcement learning. In a\nnovel approach, we formulate the problem of intrusion prevention as an optimal\nstopping problem. This formulation allows us insight into the structure of the\noptimal policies, which turn out to be threshold based. Since the computation\nof the optimal defender policy using dynamic programming is not feasible for\npractical cases, we approximate the optimal policy through reinforcement\nlearning in a simulation environment. To define the dynamics of the simulation,\nwe emulate the target infrastructure and collect measurements. Our evaluations\nshow that the learned policies are close to optimal and that they indeed can be\nexpressed using thresholds.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 04:45:37 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hammar", "Kim", ""], ["Stadler", "Rolf", ""]]}, {"id": "2106.07214", "submitter": "Antonio Emanuele Cin\\`a", "authors": "Antonio Emanuele Cin\\`a, Kathrin Grosse, Sebastiano Vascon, Ambra\n  Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo", "title": "Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence\n  Functions", "comments": "21 pages, submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor attacks inject poisoning samples during training, with the goal of\nenforcing a machine-learning model to output an attacker-chosen class when\npresented a specific trigger at test time. Although backdoor attacks have been\ndemonstrated in a variety of settings and against different models, the factors\naffecting their success are not yet well understood. In this work, we provide a\nunifying framework to study the process of backdoor learning under the lens of\nincremental learning and influence functions. We show that the success of\nbackdoor attacks inherently depends on (i) the complexity of the learning\nalgorithm, controlled by its hyperparameters, and (ii) the fraction of backdoor\nsamples injected into the training set. These factors affect how fast a\nmachine-learning model learns to correlate the presence of a backdoor trigger\nwith the target class. Interestingly, our analysis shows that there exists a\nregion in the hyperparameter space in which the accuracy on clean test samples\nis still high while backdoor attacks become ineffective, thereby suggesting\nnovel criteria to improve existing defenses.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:00:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cin\u00e0", "Antonio Emanuele", ""], ["Grosse", "Kathrin", ""], ["Vascon", "Sebastiano", ""], ["Demontis", "Ambra", ""], ["Biggio", "Battista", ""], ["Roli", "Fabio", ""], ["Pelillo", "Marcello", ""]]}, {"id": "2106.07229", "submitter": "HyungChul Kang", "authors": "Joon-Woo Lee, HyungChul Kang, Yongwoo Lee, Woosuk Choi, Jieun Eom,\n  Maxim Deryabin, Eunsang Lee, Junghyun Lee, Donghoon Yoo, Young-Sik Kim,\n  Jong-Seon No", "title": "Privacy-Preserving Machine Learning with Fully Homomorphic Encryption\n  for Deep Neural Network", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fully homomorphic encryption (FHE) is one of the prospective tools for\nprivacypreserving machine learning (PPML), and several PPML models have been\nproposed based on various FHE schemes and approaches. Although the FHE schemes\nare known as suitable tools to implement PPML models, previous PPML models on\nFHE encrypted data are limited to only simple and non-standard types of machine\nlearning models. These non-standard machine learning models are not proven\nefficient and accurate with more practical and advanced datasets. Previous PPML\nschemes replace non-arithmetic activation functions with simple arithmetic\nfunctions instead of adopting approximation methods and do not use\nbootstrapping, which enables continuous homomorphic evaluations. Thus, they\ncould not use standard activation functions and could not employ a large number\nof layers. The maximum classification accuracy of the existing PPML model with\nthe FHE for the CIFAR-10 dataset was only 77% until now. In this work, we\nfirstly implement the standard ResNet-20 model with the RNS-CKKS FHE with\nbootstrapping and verify the implemented model with the CIFAR-10 dataset and\nthe plaintext model parameters. Instead of replacing the non-arithmetic\nfunctions with the simple arithmetic function, we use state-of-the-art\napproximation methods to evaluate these non-arithmetic functions, such as the\nReLU, with sufficient precision [1]. Further, for the first time, we use the\nbootstrapping technique of the RNS-CKKS scheme in the proposed model, which\nenables us to evaluate a deep learning model on the encrypted data. We\nnumerically verify that the proposed model with the CIFAR-10 dataset shows\n98.67% identical results to the original ResNet-20 model with non-encrypted\ndata. The classification accuracy of the proposed model is 90.67%, which is\npretty close to that of the original ResNet-20 CNN model...\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:30:45 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Lee", "Joon-Woo", ""], ["Kang", "HyungChul", ""], ["Lee", "Yongwoo", ""], ["Choi", "Woosuk", ""], ["Eom", "Jieun", ""], ["Deryabin", "Maxim", ""], ["Lee", "Eunsang", ""], ["Lee", "Junghyun", ""], ["Yoo", "Donghoon", ""], ["Kim", "Young-Sik", ""], ["No", "Jong-Seon", ""]]}, {"id": "2106.07271", "submitter": "Dmytro Petryk", "authors": "Dmytro Petryk, Zoya Dyka, Roland Sorge, Jan Schaeffner and Peter\n  Langendoerfer", "title": "Optical Fault Injection Attacks against Radiation-Hard Registers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  If devices are physically accessible optical fault injection attacks pose a\ngreat threat since the data processed as well as the operation flow can be\nmanipulated. Successful physical attacks may lead not only to leakage of secret\ninformation such as cryptographic private keys, but can also cause economic\ndamage especially if as a result of such a manipulation a critical\ninfrastructure is successfully attacked. Laser based attacks exploit the\nsensitivity of CMOS technologies to electromagnetic radiation in the visible or\nthe infrared spectrum. It can be expected that radiation-hard designs,\nspecially crafted for space applications, are more robust not only against\nhigh-energy particles and short electromagnetic waves but also against optical\nfault injection attacks. In this work we investigated the sensitivity of\nradiation-hard JICG shift registers to optical fault injection attacks. In our\nexperiments, we were able to trigger bit-set and bit-reset repeatedly changing\nthe data stored in single JICG flip-flops despite their high-radiation fault\ntolerance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 09:46:30 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 14:20:29 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Petryk", "Dmytro", ""], ["Dyka", "Zoya", ""], ["Sorge", "Roland", ""], ["Schaeffner", "Jan", ""], ["Langendoerfer", "Peter", ""]]}, {"id": "2106.07303", "submitter": "Alexander Schl\\\"ogl", "authors": "Alexander Schl\\\"ogl, Tobias Kupek, Rainer B\\\"ohme", "title": "iNNformant: Boundary Samples as Telltale Watermarks", "comments": "Will be presented at IH&MMSEC '21", "journal-ref": null, "doi": "10.1145/3437880.3460411", "report-no": null, "categories": "cs.LG cs.CR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary samples are special inputs to artificial neural networks crafted to\nidentify the execution environment used for inference by the resulting output\nlabel. The paper presents and evaluates algorithms to generate transparent\nboundary samples. Transparency refers to a small perceptual distortion of the\nhost signal (i.e., a natural input sample). For two established image\nclassifiers, ResNet on FMNIST and CIFAR10, we show that it is possible to\ngenerate sets of boundary samples which can identify any of four tested\nmicroarchitectures. These sets can be built to not contain any sample with a\nworse peak signal-to-noise ratio than 70dB. We analyze the relationship between\nsearch complexity and resulting transparency.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 11:18:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Schl\u00f6gl", "Alexander", ""], ["Kupek", "Tobias", ""], ["B\u00f6hme", "Rainer", ""]]}, {"id": "2106.07371", "submitter": "Liyi Zhou", "authors": "Liyi Zhou and Kaihua Qin and Arthur Gervais", "title": "A2MM: Mitigating Frontrunning, Transaction Reordering and Consensus\n  Instability in Decentralized Exchanges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asset trading volume on blockchain-based exchanges (DEX) increased\nsubstantially since the advent of Automated Market Makers (AMM). Yet, AMMs and\ntheir forks compete on the same blockchain, incurring unnecessary network and\nblock-space overhead, by attracting sandwich attackers and arbitrage\ncompetitions. Moreover, conceptually speaking, a blockchain is one database,\nand we find little reason to partition this database into multiple competing\nexchanges, which then necessarily require price synchronization through\narbitrage.\n  This paper shows that DEX arbitrage and trade routing among similar AMMs can\nbe performed efficiently and atomically on-chain within smart contracts. These\ninsights lead us to create a new AMM design, an Automated Arbitrage Market\nMaker, short A2MM DEX. A2MM aims to unite multiple AMMs to reduce overheads,\ncosts and increase blockchain security. With respect to Miner Extractable Value\n(MEV), A2MM serves as a decentralized design for users to atomically collect\nMEV, mitigating the dangers of centralized MEV relay services.\n  We show that A2MM offers essential security benefits. First, A2MM strengthens\nthe blockchain consensus security by mitigating the competitive exploitation of\nMEV, therefore reducing the risks of consensus forks. A2MM reduces the network\nlayer overhead of competitive transactions, improves network propagation,\nleading to less stale blocks and better blockchain security. Through trade\nrouting, A2MM reduces the predatory risks of sandwich attacks by taking\nadvantage of the minimum profitable victim input. A2MM also offers financial\nbenefits to traders. Failed swap transactions from competitive trading occupy\nvaluable block space, implying an upward pressure on transaction fees. Our\nevaluations shows that A2MM frees up 32.8% block-space of AMM-related\ntransactions. In expectation, A2MM's revenue allows to reduce swap fees by 90%.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 12:41:36 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 03:41:19 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zhou", "Liyi", ""], ["Qin", "Kaihua", ""], ["Gervais", "Arthur", ""]]}, {"id": "2106.07445", "submitter": "Carl-Johann Simon-Gabriel", "authors": "Carl-Johann Simon-Gabriel and Noman Ahmed Sheikh and Andreas Krause", "title": "PopSkipJump: Decision-Based Attack for Probabilistic Classifiers", "comments": "ICML'21. Code available at https://github.com/cjsg/PopSkipJump . 9\n  pages & 7 figures in main part, 14 pages & 10 figures in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current classifiers are vulnerable to adversarial examples, small input\nperturbations that change the classification output. Many existing attack\nalgorithms cover various settings, from white-box to black-box classifiers, but\ntypically assume that the answers are deterministic and often fail when they\nare not. We therefore propose a new adversarial decision-based attack\nspecifically designed for classifiers with probabilistic outputs. It is based\non the HopSkipJump attack by Chen et al. (2019, arXiv:1904.02144v5 ), a strong\nand query efficient decision-based attack originally designed for deterministic\nclassifiers. Our P(robabilisticH)opSkipJump attack adapts its amount of queries\nto maintain HopSkipJump's original output quality across various noise levels,\nwhile converging to its query efficiency as the noise level decreases. We test\nour attack on various noise models, including state-of-the-art off-the-shelf\nrandomized defenses, and show that they offer almost no extra robustness to\ndecision-based attacks. Code is available at\nhttps://github.com/cjsg/PopSkipJump .\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:13:12 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Simon-Gabriel", "Carl-Johann", ""], ["Sheikh", "Noman Ahmed", ""], ["Krause", "Andreas", ""]]}, {"id": "2106.07449", "submitter": "Calvin Deutschbein", "authors": "Calvin Deutschbein, Andres Meza, Francesco Restuccia, Ryan Kastner and\n  Cynthia Sturton", "title": "A Methodology For Creating Information Flow Specifications of Hardware\n  Designs", "comments": "9 pages, 4 figures, submitted to ICCAD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a methodology for creating information flow specifications of\nhardware designs. Such specifications can help designers better understand\ntheir design and are necessary for security validation processes. By combining\ninformation flow tracking and specification mining, we are able to produce\ninformation flow properties of a design without prior knowledge of security\nagreements or specifications. We develop a tool, Isadora, to evaluate our\nmethodology. We demonstrate Isadora may define the information flows within an\naccess control module in isolation and within an SoC and over a RISC-V design.\nOver the access control module, Isadora mined output completely covers an\nassertion based security specification of the design provided by the designers.\nFor both the access control module and RISC-V, we sample Isadora output\nproperties and find 10 out of 10 and 8 out of 10 properties, respectively,\ndefine the design behavior to relevant to a Common Weakness Enumeration (CWE).\nWe find our methodology may independently mine security properties manually\ndeveloped by hardware designers, automatically generate properties describing\nCWEs over a design, and scale to SoC and CPU designs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:20:33 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Deutschbein", "Calvin", ""], ["Meza", "Andres", ""], ["Restuccia", "Francesco", ""], ["Kastner", "Ryan", ""], ["Sturton", "Cynthia", ""]]}, {"id": "2106.07528", "submitter": "Xin Kang", "authors": "Hannah Lim Jing Ting, Xin Kang, Tieyan Li, Haiguang Wang, Cheng-Kang\n  Chu", "title": "On the Trust and Trust Modelling for the Future Fully-Connected Digital\n  World: A Comprehensive Study", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast development of digital technologies, we are running into a\ndigital world. The relationship among people and the connections among things\nbecome more and more complex, and new challenges arise. To tackle these\nchallenges, trust-a soft security mechanism-is considered as a promising\ntechnology. Thus, in this survey, we do a comprehensive study on the trust and\ntrust modelling for the future digital world. We revisit the definitions and\nproperties of trust, and analysis the trust theories and discuss their impact\non digital trust modelling. We analyze the digital world and its corresponding\nenvironment where people, things, and infrastructure connect with each other.\nWe detail the challenges that require trust in these digital scenarios. Under\nour analysis of trust and the digital world, we define different types of trust\nrelationships and find out the factors that are needed to ensure a fully\nrepresentative model. Next, to meet the challenges of digital trust modelling,\ncomprehensive trust model evaluation criteria are proposed, and potential\nsecurities and privacy issues of trust modelling are analyzed. Finally, we\nprovide a wide-ranging analysis of different methodologies, mathematical\ntheories, and how they can be applied to trust modelling.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:55:19 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ting", "Hannah Lim Jing", ""], ["Kang", "Xin", ""], ["Li", "Tieyan", ""], ["Wang", "Haiguang", ""], ["Chu", "Cheng-Kang", ""]]}, {"id": "2106.07534", "submitter": "Martino Trevisan Dr", "authors": "Nikhil Jha, Thomas Favale, Luca Vassio, Martino Trevisan, Marco Mellia", "title": "z-anonymity: Zero-Delay Anonymization for Data Streams", "comments": null, "journal-ref": "In 2020 IEEE International Conference on Big Data (Big Data), pp.\n  3996-4005. IEEE, 2020", "doi": "10.1109/BigData50022.2020.9378422", "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of big data and the birth of the data markets that sell\npersonal information, individuals' privacy is of utmost importance. The\nclassical response is anonymization, i.e., sanitizing the information that can\ndirectly or indirectly allow users' re-identification. The most popular\nsolution in the literature is the k-anonymity. However, it is hard to achieve\nk-anonymity on a continuous stream of data, as well as when the number of\ndimensions becomes high.In this paper, we propose a novel anonymization\nproperty called z-anonymity. Differently from k-anonymity, it can be achieved\nwith zero-delay on data streams and it is well suited for high dimensional\ndata. The idea at the base of z-anonymity is to release an attribute (an atomic\ninformation) about a user only if at least z - 1 other users have presented the\nsame attribute in a past time window. z-anonymity is weaker than k-anonymity\nsince it does not work on the combinations of attributes, but treats them\nindividually. In this paper, we present a probabilistic framework to map the\nz-anonymity into the k-anonymity property. Our results show that a proper\nchoice of the z-anonymity parameters allows the data curator to likely obtain a\nk-anonymized dataset, with a precisely measurable probability. We also evaluate\na real use case, in which we consider the website visits of a population of\nusers and show that z-anonymity can work in practice for obtaining the\nk-anonymity too.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:00:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Jha", "Nikhil", ""], ["Favale", "Thomas", ""], ["Vassio", "Luca", ""], ["Trevisan", "Martino", ""], ["Mellia", "Marco", ""]]}, {"id": "2106.07715", "submitter": "Zhe Qu", "authors": "Zhe Qu, Shangqing Zhao, Jie Xu, Zhuo Lu, and Yao Liu", "title": "How to Test the Randomness from the Wireless Channel for Security?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the traditional framework of wireless secret key generation, where\ntwo parties leverage the wireless channel randomness to establish a secret key.\nThe essence in the framework is to quantify channel randomness into bit\nsequences for key generation. Conducting randomness tests on such bit sequences\nhas been a common practice to provide the confidence to validate whether they\nare random. Interestingly, despite different settings in the tests, existing\nstudies interpret the results the same: passing tests means that the bit\nsequences are indeed random.\n  In this paper, we investigate how to properly test the wireless channel\nrandomness to ensure enough security strength and key generation efficiency. In\nparticular, we define an adversary model that leverages the imperfect\nrandomness of the wireless channel to search the generated key, and create a\nguideline to set up randomness testing and privacy amplification to eliminate\nsecurity loss and achieve efficient key generation rate. We use theoretical\nanalysis and comprehensive experiments to reveal that common practice misuses\nrandomness testing and privacy amplification: (i) no security insurance of key\nstrength, (ii) low efficiency of key generation rate. After revision by our\nguideline, security loss can be eliminated and key generation rate can be\nincreased significantly.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:23:21 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Qu", "Zhe", ""], ["Zhao", "Shangqing", ""], ["Xu", "Jie", ""], ["Lu", "Zhuo", ""], ["Liu", "Yao", ""]]}, {"id": "2106.07731", "submitter": "Burak Hasircioglu", "authors": "Burak Hasircioglu, Jesus Gomez-Vilardebo, Deniz Gunduz", "title": "Bivariate Polynomial Codes for Secure Distributed Matrix Multiplication", "comments": "arXiv admin note: text overlap with arXiv:2102.08304", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of secure distributed matrix multiplication. Coded\ncomputation has been shown to be an effective solution in distributed matrix\nmultiplication, both providing privacy against workers and boosting the\ncomputation speed by efficiently mitigating stragglers. In this work, we\npresent a non-direct secure extension of the recently introduced bivariate\npolynomial codes. Bivariate polynomial codes have been shown to be able to\nfurther speed up distributed matrix multiplication by exploiting the partial\nwork done by the stragglers rather than completely ignoring them while reducing\nthe upload communication cost and/or the workers' storage's capacity needs. We\nshow that, especially for upload communication or storage constrained settings,\nthe proposed approach reduces the average computation time of secure\ndistributed matrix multiplication compared to its competitors in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:57:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hasircioglu", "Burak", ""], ["Gomez-Vilardebo", "Jesus", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2106.07750", "submitter": "Luca Mariot", "authors": "Luca Mariot", "title": "Hip to Be (Latin) Square: Maximal Period Sequences from Orthogonal\n  Cellular Automata", "comments": "16 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DM cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal Cellular Automata (OCA) have been recently investigated in the\nliterature as a new approach to construct orthogonal Latin squares for\ncryptographic applications such as secret sharing schemes. In this paper, we\nconsider OCA for a different cryptographic task, namely the generation of\npseudorandom sequences. The idea is to iterate a dynamical system where the\noutput of an OCA pair is fed back as a new set of coordinates on the superposed\nsquares. The main advantage is that OCA ensure a certain amount of diffusion in\nthe generated sequences, a property which is usually missing from traditional\nCA-based pseudorandom number generators. We study the problem of finding OCA\npairs with maximal period by first performing an exhaustive search up to local\nrules of diameter $d=5$, and then focusing on the subclass of linear\nbipermutive rules. In this case, we characterize the periods of the sequences\nin terms of the order of the subgroup generated by an invertible Sylvester\nmatrix. We finally devise an algorithm based on Lagrange's theorem to\nefficiently enumerate all linear OCA pairs of maximal period up to diameter\n$d=11$.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 20:42:10 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Mariot", "Luca", ""]]}, {"id": "2106.07785", "submitter": "Itzhak Tamo", "authors": "Netanel Raviv, Ben Langton, and Itzhak Tamo", "title": "Multivariate Public Key Cryptosystem from Sidon Spaces", "comments": "Appeared in Public-Key Cryptography - PKC 2021, 24th IACR\n  International Conference on Practice and Theory of Public Key Cryptography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Sidon space is a subspace of an extension field over a base field in which\nthe product of any two elements can be factored uniquely, up to constants. This\npaper proposes a new public-key cryptosystem of the multivariate type which is\nbased on Sidon spaces, and has the potential to remain secure even if quantum\nsupremacy is attained. This system, whose security relies on the hardness of\nthe well-known MinRank problem, is shown to be resilient to several\nstraightforward algebraic attacks. In particular, it is proved that the two\npopular attacks on the MinRank problem, the kernel attack, and the minor\nattack, succeed only with exponentially small probability. The system is\nimplemented in software, and its hardness is demonstrated experimentally.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 22:33:47 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 15:05:31 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Raviv", "Netanel", ""], ["Langton", "Ben", ""], ["Tamo", "Itzhak", ""]]}, {"id": "2106.07798", "submitter": "Kiran Karra", "authors": "Chace Ashcraft, Kiran Karra", "title": "Poisoning Deep Reinforcement Learning Agents with In-Distribution\n  Triggers", "comments": "4 pages, 1 figure, Published at ICLR 2021 Workshop on Security and\n  Safety in Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new data poisoning attack and apply it to deep\nreinforcement learning agents. Our attack centers on what we call\nin-distribution triggers, which are triggers native to the data distributions\nthe model will be trained on and deployed in. We outline a simple procedure for\nembedding these, and other, triggers in deep reinforcement learning agents\nfollowing a multi-task learning paradigm, and demonstrate in three common\nreinforcement learning environments. We believe that this work has important\nimplications for the security of deep learning models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 23:16:41 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ashcraft", "Chace", ""], ["Karra", "Kiran", ""]]}, {"id": "2106.07815", "submitter": "Hao Wu", "authors": "Hao Wu, Anthony Wirth", "title": "Locally Differentially Private Frequency Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present two new local differentially private algorithms for frequency\nestimation. One solves the fundamental frequency oracle problem; the other\nsolves the well-known heavy hitters identification problem. Consistent with\nprior art, these are randomized algorithms. As a function of failure\nprobability~$\\beta$, the former achieves optimal worst-case estimation error\nfor every~$\\beta$, while the latter is optimal when~$\\beta$ is at least inverse\npolynomial in~$n$, the number of users. In both algorithms, server running time\nis~$\\tilde{O}(n)$ while user running time is~$\\tilde{O}(1)$. Our\nfrequency-oracle algorithm achieves lower estimation error than the prior works\nof Bassily et al. (NeurIPS 2017). On the other hand, our heavy hitters\nidentification method is as easily implementable as as TreeHist (Bassily et\nal., 2017) and has superior worst-case error, by a factor of $\\Omega(\\sqrt{\\log\nn})$.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 00:22:58 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wu", "Hao", ""], ["Wirth", "Anthony", ""]]}, {"id": "2106.07831", "submitter": "Yuan Lu", "authors": "Yingzi Gao, Yuan Lu, Zhenliang Lu, Qiang Tang, Jing Xu, Zhenfeng Zhang", "title": "Efficient Asynchronous Byzantine Agreement without Private Setups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For asynchronous binary agreement (ABA) with optimal resilience, prior\nprivate-setup free protocols (Cachin et al., CCS' 2002; Kokoris-Kogias et al.,\nCCS' 2020) incur $O({\\lambda}n^4)$ bits and $O(n^3)$ messages; for asynchronous\nmulti-valued agreement with external validity (VBA), Abraham et al. [2] very\nrecently gave the first elegant construction with $O(n^3)$ messages, relying on\npublic key infrastructure (PKI), but still costs $O({\\lambda} n^3 \\log n)$\nbits. We for the first time close the remaining efficiency gap, i.e., reducing\ntheir communication to $O({\\lambda} n^3)$ bits on average. At the core of our\ndesign, we give a systematic treatment of reasonably fair common randomness:\n  - We construct a reasonably fair common coin (Canetti and Rabin, STOC' 1993)\nin the asynchronous setting with PKI instead of private setup, using only\n$O({\\lambda} n^3)$ bit and constant asynchronous rounds. The common coin\nprotocol ensures that with at least 1/3 probability, all honest parties can\noutput a common bit that is as if uniformly sampled, rendering a more efficient\nprivate-setup free ABA with expected $O({\\lambda} n^3)$ bit communication and\nconstant running time.\n  - More interestingly, we lift our reasonably fair common coin protocol to\nattain perfect agreement without incurring any extra factor in the asymptotic\ncomplexities, resulting in an efficient reasonably fair leader election\nprimitive pluggable in all existing VBA protocols, thus reducing the\ncommunication of private-setup free VBA to expected $O({\\lambda} n^3)$ bits\nwhile preserving expected constant running time.\n  - Along the way, we improve an important building block, asynchronous\nverifiable secret sharing by presenting a private-setup free implementation\ncosting only $O({\\lambda} n^2)$ bits in the PKI setting. By contrast, prior art\nhaving the same complexity (Backes et al., CT-RSA' 2013) has to rely on a\nprivate setup.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 01:34:11 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gao", "Yingzi", ""], ["Lu", "Yuan", ""], ["Lu", "Zhenliang", ""], ["Tang", "Qiang", ""], ["Xu", "Jing", ""], ["Zhang", "Zhenfeng", ""]]}, {"id": "2106.07833", "submitter": "Zhongyuan Hau", "authors": "Chengzeng You, Zhongyuan Hau, Soteris Demetriou", "title": "Temporal Consistency Checks to Detect LiDAR Spoofing Attacks on\n  Autonomous Vehicle Perception", "comments": "Accepted in 1st Workshop on Security and Privacy for Mobile AI (MAISP\n  2021)", "journal-ref": null, "doi": "10.1145/3469261.3469406", "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR sensors are used widely in Autonomous Vehicles for better perceiving\nthe environment which enables safer driving decisions. Recent work has\ndemonstrated serious LiDAR spoofing attacks with alarming consequences. In\nparticular, model-level LiDAR spoofing attacks aim to inject fake depth\nmeasurements to elicit ghost objects that are erroneously detected by 3D Object\nDetectors, resulting in hazardous driving decisions. In this work, we explore\nthe use of motion as a physical invariant of genuine objects for detecting such\nattacks. Based on this, we propose a general methodology, 3D Temporal\nConsistency Check (3D-TC2), which leverages spatio-temporal information from\nmotion prediction to verify objects detected by 3D Object Detectors. Our\npreliminary design and implementation of a 3D-TC2 prototype demonstrates very\npromising performance, providing more than 98% attack detection rate with a\nrecall of 91% for detecting spoofed Vehicle (Car) objects, and is able to\nachieve real-time detection at 41Hz\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 01:36:40 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["You", "Chengzeng", ""], ["Hau", "Zhongyuan", ""], ["Demetriou", "Soteris", ""]]}, {"id": "2106.07851", "submitter": "Christopher M. Poskitt", "authors": "Yuqi Chen, Christopher M. Poskitt, Jun Sun", "title": "Code Integrity Attestation for PLCs using Black Box Neural Network\n  Predictions", "comments": "Accepted by the 29th ACM Joint European Software Engineering\n  Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems (CPSs) are widespread in critical domains, and\nsignificant damage can be caused if an attacker is able to modify the code of\ntheir programmable logic controllers (PLCs). Unfortunately, traditional\ntechniques for attesting code integrity (i.e. verifying that it has not been\nmodified) rely on firmware access or roots-of-trust, neither of which\nproprietary or legacy PLCs are likely to provide. In this paper, we propose a\npractical code integrity checking solution based on privacy-preserving black\nbox models that instead attest the input/output behaviour of PLC programs.\nUsing faithful offline copies of the PLC programs, we identify their most\nimportant inputs through an information flow analysis, execute them on multiple\ncombinations to collect data, then train neural networks able to predict PLC\noutputs (i.e. actuator commands) from their inputs. By exploiting the black box\nnature of the model, our solution maintains the privacy of the original PLC\ncode and does not assume that attackers are unaware of its presence. The trust\ninstead comes from the fact that it is extremely hard to attack the PLC code\nand neural networks at the same time and with consistent outcomes. We evaluated\nour approach on a modern six-stage water treatment plant testbed, finding that\nit could predict actuator states from PLC inputs with near-100% accuracy, and\nthus could detect all 120 effective code mutations that we subjected the PLCs\nto. Finally, we found that it is not practically possible to simultaneously\nmodify the PLC code and apply discreet adversarial noise to our attesters in a\nway that leads to consistent (mis-)predictions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:09:29 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 07:05:39 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Yuqi", ""], ["Poskitt", "Christopher M.", ""], ["Sun", "Jun", ""]]}, {"id": "2106.07855", "submitter": "Himanshu Thapliyal", "authors": "Zachary Kahleifeh and Himanshu Thapliyal", "title": "Low-Energy and CPA-Resistant Adiabatic CMOS/MTJ Logic for IoT Devices", "comments": "6 pages, 2021 IEEE Computer Society Annual Symposium on VLSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The tremendous growth in the number of Internet of Things (IoT) devices has\nincreased focus on the energy efficiency and security of an IoT device. In this\npaper, we will present a design level, non-volatile adiabatic architecture for\nlow-energy and Correlation Power Analysis (CPA) resistant IoT devices. IoT\ndevices constructed with CMOS integrated circuits suffer from high dynamic\nenergy and leakage power. To solve this, we look at both adiabatic logic and\nSTT-MTJs (Spin Transfer Torque Magnetic Tunnel Junctions) to reduce both\ndynamic energy and leakage power. Furthermore, CMOS integrated circuits suffer\nfrom side-channel leakage making them insecure against power analysis attacks.\nWe again look to adiabatic logic to design secure circuits with uniform power\nconsumption, thus, defending against power analysis attacks. We have developed\na hybrid adiabatic- MTJ architecture using two-phase adiabatic logic. We show\nthat hybrid adiabatic-MTJ circuits are both low energy and secure when compared\nwith CMOS circuits. As a case study, we have constructed one round of PRESENT\nand have shown energy savings of 64.29% at a frequency of 25 MHz. Furthermore,\nwe have performed a correlation power analysis attack on our proposed design\nand determined that the key was kept hidden.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:19:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kahleifeh", "Zachary", ""], ["Thapliyal", "Himanshu", ""]]}, {"id": "2106.07860", "submitter": "Ioannis Boutsikas", "authors": "John Boutsikas, Maksim E. Eren, Charles Varga, Edward Raff, Cynthia\n  Matuszek, Charles Nicholas", "title": "Evading Malware Classifiers via Monte Carlo Mutant Feature Discovery", "comments": "Presented at the Malware Technical Exchange Meeting, Online,2021.\n  Copyright 2021 by the author(s)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Machine Learning has become a significant part of malware\ndetection efforts due to the influx of new malware, an ever changing threat\nlandscape, and the ability of Machine Learning methods to discover meaningful\ndistinctions between malicious and benign software. Antivirus vendors have also\nbegun to widely utilize malware classifiers based on dynamic and static malware\nanalysis features. Therefore, a malware author might make evasive binary\nmodifications against Machine Learning models as part of the malware\ndevelopment life cycle to execute an attack successfully. This makes the\nstudying of possible classifier evasion strategies an essential part of cyber\ndefense against malice. To this extent, we stage a grey box setup to analyze a\nscenario where the malware author does not know the target classifier\nalgorithm, and does not have access to decisions made by the classifier, but\nknows the features used in training. In this experiment, a malicious actor\ntrains a surrogate model using the EMBER-2018 dataset to discover binary\nmutations that cause an instance to be misclassified via a Monte Carlo tree\nsearch. Then, mutated malware is sent to the victim model that takes the place\nof an antivirus API to test whether it can evade detection.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:31:02 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Boutsikas", "John", ""], ["Eren", "Maksim E.", ""], ["Varga", "Charles", ""], ["Raff", "Edward", ""], ["Matuszek", "Cynthia", ""], ["Nicholas", "Charles", ""]]}, {"id": "2106.07867", "submitter": "Rajesh Kumar", "authors": "Mohit Agrawal and Pragyan Mehrotra and Rajesh Kumar and Rajiv Ratn\n  Shah", "title": "Defending Touch-based Continuous Authentication Systems from Active\n  Adversaries Using Generative Adversarial Networks", "comments": "2021 IEEE International Joint Conference on Biometrics (IJCB), 8\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previous studies have demonstrated that commonly studied (vanilla)\ntouch-based continuous authentication systems (V-TCAS) are susceptible to\npopulation attack. This paper proposes a novel Generative Adversarial Network\nassisted TCAS (G-TCAS) framework, which showed more resilience to the\npopulation attack. G-TCAS framework was tested on a dataset of 117 users who\ninteracted with a smartphone and tablet pair. On average, the increase in the\nfalse accept rates (FARs) for V-TCAS was much higher (22%) than G-TCAS (13%)\nfor the smartphone. Likewise, the increase in the FARs for V-TCAS was 25%\ncompared to G-TCAS (6%) for the tablet.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 04:04:58 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Agrawal", "Mohit", ""], ["Mehrotra", "Pragyan", ""], ["Kumar", "Rajesh", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2106.07868", "submitter": "Haibin Wu", "authors": "Haibin Wu, Yang Zhang, Zhiyong Wu, Dong Wang, Hung-yi Lee", "title": "Voting for the right answer: Adversarial defense for speaker\n  verification", "comments": "Accepted by Interspeech 2021. Code is available at\n  https://github.com/thuhcsi/adsv_voting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic speaker verification (ASV) is a well developed technology for\nbiometric identification, and has been ubiquitous implemented in\nsecurity-critic applications, such as banking and access control. However,\nprevious works have shown that ASV is under the radar of adversarial attacks,\nwhich are very similar to their original counterparts from human's perception,\nyet will manipulate the ASV render wrong prediction. Due to the very late\nemergence of adversarial attacks for ASV, effective countermeasures against\nthem are limited. Given that the security of ASV is of high priority, in this\nwork, we propose the idea of \"voting for the right answer\" to prevent risky\ndecisions of ASV in blind spot areas, by employing random sampling and voting.\nExperimental results show that our proposed method improves the robustness\nagainst both the limited-knowledge attackers by pulling the adversarial samples\nout of the blind spots, and the perfect-knowledge attackers by introducing\nrandomness and increasing the attackers' budgets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 04:05:28 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 07:35:50 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wu", "Haibin", ""], ["Zhang", "Yang", ""], ["Wu", "Zhiyong", ""], ["Wang", "Dong", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2106.07893", "submitter": "Irippuge Milinda Perera", "authors": "Shruthi Gorantala, Rob Springer, Sean Purser-Haskell, William Lam,\n  Royce Wilson, Asra Ali, Eric P. Astor, Itai Zukerman, Sam Ruth, Christoph\n  Dibak, Phillipp Schoppmann, Sasha Kulankhina, Alain Forget, David Marn,\n  Cameron Tew, Rafael Misoczki, Bernat Guillen, Xinyu Ye, Dennis Kraft, Damien\n  Desfontaines, Aishe Krishnamurthy, Miguel Guevara, Irippuge Milinda Perera,\n  Yurii Sushko, Bryant Gipson", "title": "A General Purpose Transpiler for Fully Homomorphic Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fully homomorphic encryption (FHE) is an encryption scheme which enables\ncomputation on encrypted data without revealing the underlying data. While\nthere have been many advances in the field of FHE, developing programs using\nFHE still requires expertise in cryptography. In this white paper, we present a\nfully homomorphic encryption transpiler that allows developers to convert\nhigh-level code (e.g., C++) that works on unencrypted data into high-level code\nthat operates on encrypted data. Thus, our transpiler makes transformations\npossible on encrypted data.\n  Our transpiler builds on Google's open-source XLS SDK\n(https://github.com/google/xls) and uses an off-the-shelf FHE library, TFHE\n(https://tfhe.github.io/tfhe/), to perform low-level FHE operations. The\ntranspiler design is modular, which means the underlying FHE library as well as\nthe high-level input and output languages can vary. This modularity will help\naccelerate FHE research by providing an easy way to compare arbitrary programs\nin different FHE schemes side-by-side. We hope this lays the groundwork for\neventual easy adoption of FHE by software developers. As a proof-of-concept, we\nare releasing an experimental transpiler\n(https://github.com/google/fully-homomorphic-encryption/tree/main/transpiler)\nas open-source software.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:03:58 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gorantala", "Shruthi", ""], ["Springer", "Rob", ""], ["Purser-Haskell", "Sean", ""], ["Lam", "William", ""], ["Wilson", "Royce", ""], ["Ali", "Asra", ""], ["Astor", "Eric P.", ""], ["Zukerman", "Itai", ""], ["Ruth", "Sam", ""], ["Dibak", "Christoph", ""], ["Schoppmann", "Phillipp", ""], ["Kulankhina", "Sasha", ""], ["Forget", "Alain", ""], ["Marn", "David", ""], ["Tew", "Cameron", ""], ["Misoczki", "Rafael", ""], ["Guillen", "Bernat", ""], ["Ye", "Xinyu", ""], ["Kraft", "Dennis", ""], ["Desfontaines", "Damien", ""], ["Krishnamurthy", "Aishe", ""], ["Guevara", "Miguel", ""], ["Perera", "Irippuge Milinda", ""], ["Sushko", "Yurii", ""], ["Gipson", "Bryant", ""]]}, {"id": "2106.07895", "submitter": "Asaf Shabtai", "authors": "Efrat Levy and Asaf Shabtai and Bogdan Groza and Pal-Stefan Murvay and\n  Yuval Elovici", "title": "CAN-LOC: Spoofing Detection and Physical Intrusion Localization on an\n  In-Vehicle CAN Bus Based on Deep Features of Voltage Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Controller Area Network (CAN) is used for communication between\nin-vehicle devices. The CAN bus has been shown to be vulnerable to remote\nattacks. To harden vehicles against such attacks, vehicle manufacturers have\ndivided in-vehicle networks into sub-networks, logically isolating critical\ndevices. However, attackers may still have physical access to various\nsub-networks where they can connect a malicious device. This threat has not\nbeen adequately addressed, as methods proposed to determine physical intrusion\npoints have shown weak results, emphasizing the need to develop more advanced\ntechniques. To address this type of threat, we propose a security hardening\nsystem for in-vehicle networks. The proposed system includes two mechanisms\nthat process deep features extracted from voltage signals measured on the CAN\nbus. The first mechanism uses data augmentation and deep learning to detect and\nlocate physical intrusions when the vehicle starts; this mechanism can detect\nand locate intrusions, even when the connected malicious devices are silent.\nThis mechanism's effectiveness (100% accuracy) is demonstrated in a wide\nvariety of insertion scenarios on a CAN bus prototype. The second mechanism is\na continuous device authentication mechanism, which is also based on deep\nlearning; this mechanism's robustness (99.8% accuracy) is demonstrated on a\nreal moving vehicle.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:12:33 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Levy", "Efrat", ""], ["Shabtai", "Asaf", ""], ["Groza", "Bogdan", ""], ["Murvay", "Pal-Stefan", ""], ["Elovici", "Yuval", ""]]}, {"id": "2106.07943", "submitter": "Cheng Yukun", "authors": "Yukun Cheng, Mengce Zheng, Fan Huang, Jiajia Zhang, Honggang Hu and\n  Nenghai Yu", "title": "A Fast-Detection and Fault-Correction Algorithm against Persistent Fault\n  Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Persistent Fault Attack (PFA) is a recently proposed Fault Attack (FA) method\nin CHES 2018. It is able to recover full AES secret key in the\nSingle-Byte-Fault scenario. It is demonstrated that classical FA\ncountermeasures, such as Dual Modular Redundancy (DMR) and mask protection, are\nunable to thwart PFA. In this paper, we propose a fast-detection and\nfaultcorrection algorithm to prevent PFA. We construct a fixed input and output\npair to detect faults rapidly. Then we build two extra redundant tables to\nstore the relationship between the adjacent elements in the S-box, by which the\nalgorithm can correct the faulty elements in the S-box. Our experimental\nresults show that our algorithm can effectively prevent PFA in both\nSingle-ByteFault and Multiple-Bytes-Faults scenarios. Compared with the\nclassical FA countermeasures, our algorithm has a much better effect against\nPFA. Further, the time cost of our algorithm is 40% lower than the classical FA\ncountermeasures.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:57:32 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Cheng", "Yukun", ""], ["Zheng", "Mengce", ""], ["Huang", "Fan", ""], ["Zhang", "Jiajia", ""], ["Hu", "Honggang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2106.07961", "submitter": "Giovanni Apruzzese", "authors": "Andrea Corsini, Shanchieh Jay Yang, Giovanni Apruzzese", "title": "On the Evaluation of Sequential Machine Learning for Network Intrusion\n  Detection", "comments": null, "journal-ref": null, "doi": "10.1145/3465481.3470065", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep learning renewed the research interests in machine\nlearning for Network Intrusion Detection Systems (NIDS). Specifically,\nattention has been given to sequential learning models, due to their ability to\nextract the temporal characteristics of Network traffic Flows (NetFlows), and\nuse them for NIDS tasks. However, the applications of these sequential models\noften consist of transferring and adapting methodologies directly from other\nfields, without an in-depth investigation on how to leverage the specific\ncircumstances of cybersecurity scenarios; moreover, there is a lack of\ncomprehensive studies on sequential models that rely on NetFlow data, which\npresents significant advantages over traditional full packet captures. We\ntackle this problem in this paper. We propose a detailed methodology to extract\ntemporal sequences of NetFlows that denote patterns of malicious activities.\nThen, we apply this methodology to compare the efficacy of sequential learning\nmodels against traditional static learning models. In particular, we perform a\nfair comparison of a `sequential' Long Short-Term Memory (LSTM) against a\n`static' Feedforward Neural Networks (FNN) in distinct environments represented\nby two well-known datasets for NIDS: the CICIDS2017 and the CTU13. Our results\nhighlight that LSTM achieves comparable performance to FNN in the CICIDS2017\nwith over 99.5\\% F1-score; while obtaining superior performance in the CTU13,\nwith 95.7\\% F1-score against 91.5\\%. This paper thus paves the way to future\napplications of sequential learning models for NIDS.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 08:29:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Corsini", "Andrea", ""], ["Yang", "Shanchieh Jay", ""], ["Apruzzese", "Giovanni", ""]]}, {"id": "2106.07980", "submitter": "Mazen Azzam", "authors": "Mazen Azzam, Liliana Pasquale, Gregory Provan, Bashar Nuseibeh", "title": "Grounds for Suspicion: Physics-based Early Warnings for Stealthy Attacks\n  on Industrial Control Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stealthy attacks on Industrial Control Systems can cause significant damage\nwhile evading detection. In this paper, instead of focusing on the detection of\nstealthy attacks, we aim to provide early warnings to operators, in order to\navoid physical damage and preserve in advance data that may serve as an\nevidence during an investigation. We propose a framework to provide grounds for\nsuspicion, i.e. preliminary indicators reflecting the likelihood of success of\na stealthy attack. We propose two grounds for suspicion based on the behaviour\nof the physical process: (i) feasibility of a stealthy attack, and (ii)\nproximity to unsafe operating regions. We propose a metric to measure grounds\nfor suspicion in real-time and provide soundness principles to ensure that such\na metric is consistent with the grounds for suspicion. We apply our framework\nto Linear Time-Invariant (LTI) systems and formulate the suspicion metric\ncomputation as a real-time reachability problem. We validate our framework on a\ncase study involving the benchmark Tennessee-Eastman process. We show through\nnumerical simulation that we can provide early warnings well before a potential\nstealthy attack can cause damage, while incurring minimal load on the network.\nFinally, we apply our framework on a use case to illustrate its usefulness in\nsupporting early evidence collection.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 08:56:47 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Azzam", "Mazen", ""], ["Pasquale", "Liliana", ""], ["Provan", "Gregory", ""], ["Nuseibeh", "Bashar", ""]]}, {"id": "2106.07992", "submitter": "Cheng Feng", "authors": "Cheng Feng, Pengwei Tian", "title": "Time Series Anomaly Detection for Cyber-physical Systems via Neural\n  System Identification and Bayesian Filtering", "comments": "Accepted to appear in KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467137", "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in AIoT technologies have led to an increasing popularity of\nutilizing machine learning algorithms to detect operational failures for\ncyber-physical systems (CPS). In its basic form, an anomaly detection module\nmonitors the sensor measurements and actuator states from the physical plant,\nand detects anomalies in these measurements to identify abnormal operation\nstatus. Nevertheless, building effective anomaly detection models for CPS is\nrather challenging as the model has to accurately detect anomalies in presence\nof highly complicated system dynamics and unknown amount of sensor noise. In\nthis work, we propose a novel time series anomaly detection method called\nNeural System Identification and Bayesian Filtering (NSIBF) in which a\nspecially crafted neural network architecture is posed for system\nidentification, i.e., capturing the dynamics of CPS in a dynamical state-space\nmodel; then a Bayesian filtering algorithm is naturally applied on top of the\n\"identified\" state-space model for robust anomaly detection by tracking the\nuncertainty of the hidden state of the system recursively over time. We provide\nqualitative as well as quantitative experiments with the proposed method on a\nsynthetic and three real-world CPS datasets, showing that NSIBF compares\nfavorably to the state-of-the-art methods with considerable improvements on\nanomaly detection in CPS.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:11:35 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Feng", "Cheng", ""], ["Tian", "Pengwei", ""]]}, {"id": "2106.08013", "submitter": "Man Zhou", "authors": "Man Zhou, Qian Wang (Senior Member, IEEE), Qi Li (Senior Member,\n  IEEE), Peipei Jiang, Jingxiao Yang, Chao Shen (Senior Member, IEEE), Cong\n  Wang (Fellow, IEEE), and Shouhong Ding", "title": "Securing Face Liveness Detection Using Unforgeable Lip Motion Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face authentication usually utilizes deep learning models to verify users\nwith high recognition accuracy. However, face authentication systems are\nvulnerable to various attacks that cheat the models by manipulating the digital\ncounterparts of human faces. So far, lots of liveness detection schemes have\nbeen developed to prevent such attacks. Unfortunately, the attacker can still\nbypass these schemes by constructing wide-ranging sophisticated attacks. We\nstudy the security of existing face authentication services (e.g., Microsoft,\nAmazon, and Face++) and typical liveness detection approaches. Particularly, we\ndevelop a new type of attack, i.e., the low-cost 3D projection attack that\nprojects manipulated face videos on a 3D face model, which can easily evade\nthese face authentication services and liveness detection approaches. To this\nend, we propose FaceLip, a novel liveness detection scheme for face\nauthentication, which utilizes unforgeable lip motion patterns built upon\nwell-designed acoustic signals to enable a strong security guarantee. The\nunique lip motion patterns for each user are unforgeable because FaceLip\nverifies the patterns by capturing and analyzing the acoustic signals that are\ndynamically generated according to random challenges, which ensures that our\nsignals for liveness detection cannot be manipulated. Specially, we develop\nrobust algorithms for FaceLip to eliminate the impact of noisy signals in the\nenvironment and thus can accurately infer the lip motions at larger distances.\nWe prototype FaceLip on off-the-shelf smartphones and conduct extensive\nexperiments under different settings. Our evaluation with 44 participants\nvalidates the effectiveness and robustness of FaceLip.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:46:46 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhou", "Man", "", "Senior Member, IEEE"], ["Wang", "Qian", "", "Senior Member, IEEE"], ["Li", "Qi", "", "Senior Member,\n  IEEE"], ["Jiang", "Peipei", "", "Senior Member, IEEE"], ["Yang", "Jingxiao", "", "Senior Member, IEEE"], ["Shen", "Chao", "", "Senior Member, IEEE"], ["Wang", "Cong", "", "Fellow, IEEE"], ["Ding", "Shouhong", ""]]}, {"id": "2106.08024", "submitter": "Max Maass", "authors": "Max Maass and Marc-Pascal Clement and Matthias Hollick", "title": "Snail Mail Beats Email Any Day: On Effective Operator Security\n  Notifications in the Internet", "comments": "Accepted at The 16th International Conference on Availability,\n  Reliability and Security (ARES '21). Code and data:\n  https://doi.org/10.5281/zenodo.4817463", "journal-ref": null, "doi": "10.1145/3465481.3465743", "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of large-scale internet scanning, misconfigured websites are a\nfrequent cause of data leaks and security incidents. Previous research has\ninvestigated sending automated email notifications to operators of insecure or\ncompromised websites, but has often met with limited success due to challenges\nin address data quality, spam filtering, and operator distrust and disinterest.\nWhile several studies have investigated the design and phrasing of notification\nemails in a bid to increase their effectiveness, the use of other contact\nchannels has remained almost completely unexplored due to the required effort\nand cost. In this paper, we investigate two methods to increase notification\nsuccess: the use of letters as an alternative delivery medium, and the\ndescription of attack scenarios to incentivize remediation. We evaluate these\nfactors as part of a notification campaign utilizing manually-collected address\ninformation from 1359 German website operators and focusing on unintentional\ninformation leaks from web servers. We find that manually collected addresses\nlead to large increases in delivery rates compared to previous work, and\nletters were markedly more effective than emails, increasing remediation rates\nby up to 25 percentage points. Counterintuitively, providing detailed\ndescriptions of possible attacks can actually *decrease* remediation rates,\nhighlighting the need for more research into how notifications are perceived by\nrecipients.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:17:59 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Maass", "Max", ""], ["Clement", "Marc-Pascal", ""], ["Hollick", "Matthias", ""]]}, {"id": "2106.08029", "submitter": "Max Maass", "authors": "Max Maass and Henning Prid\\\"ohl and Dominik Herrmann and Matthias\n  Hollick", "title": "Best Practices for Notification Studies for Security and Privacy Issues\n  on the Internet", "comments": "Accepted to the 3rd International Workshop on Information Security\n  Methodology and Replication Studies (IWSMR '21), colocated with ARES '21", "journal-ref": null, "doi": "10.1145/3465481.3470081", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers help operators of vulnerable and non-compliant internet services\nby individually notifying them about security and privacy issues uncovered in\ntheir research. To improve efficiency and effectiveness of such efforts,\ndedicated notification studies are imperative. As of today, there is no\ncomprehensive documentation of pitfalls and best practices for conducting such\nnotification studies, which limits validity of results and impedes\nreproducibility. Drawing on our experience with such studies and guidance from\nrelated work, we present a set of guidelines and practical recommendations,\nincluding initial data collection, sending of notifications, interacting with\nthe recipients, and publishing the results. We note that future studies can\nespecially benefit from extensive planning and automation of crucial processes,\ni.e., activities that take place well before the first notifications are sent.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:25:58 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Maass", "Max", ""], ["Prid\u00f6hl", "Henning", ""], ["Herrmann", "Dominik", ""], ["Hollick", "Matthias", ""]]}, {"id": "2106.08060", "submitter": "Antoine Boutet", "authors": "Th\\'eo Jourdan, Antoine Boutet, Carole Frindel", "title": "Privacy Assessment of Federated Learning using Private Personalized\n  Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a collaborative scheme to train a learning model\nacross multiple participants without sharing data. While FL is a clear step\nforward towards enforcing users' privacy, different inference attacks have been\ndeveloped. In this paper, we quantify the utility and privacy trade-off of a FL\nscheme using private personalized layers. While this scheme has been proposed\nas local adaptation to improve the accuracy of the model through local\npersonalization, it has also the advantage to minimize the information about\nthe model exchanged with the server. However, the privacy of such a scheme has\nnever been quantified. Our evaluations using motion sensor dataset show that\npersonalized layers speed up the convergence of the model and slightly improve\nthe accuracy for all users compared to a standard FL scheme while better\npreventing both attribute and membership inferences compared to a FL scheme\nusing local differential privacy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:40:16 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Jourdan", "Th\u00e9o", ""], ["Boutet", "Antoine", ""], ["Frindel", "Carole", ""]]}, {"id": "2106.08072", "submitter": "Matthieu Latapy", "authors": "Jules Azad Emery and Matthieu Latapy", "title": "Full Bitcoin Blockchain Data Made Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the fact that it is publicly available, collecting and processing the\nfull bitcoin blockchain data is not trivial. Its mere size, history, and other\nfeatures indeed raise quite specific challenges, that we address in this paper.\nThe strengths of our approach are the following: it relies on very basic and\nstandard tools, which makes the procedure reliable and easily reproducible; it\nis a purely lossless procedure ensuring that we catch and preserve all existing\ndata; it provides additional indexing that makes it easy to further process the\nwhole data and select appropriate subsets of it. We present our procedure in\ndetails and illustrate its added value on large-scale use cases, like address\nclustering. We provide an implementation online, as well as the obtained\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:02:49 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Emery", "Jules Azad", ""], ["Latapy", "Matthieu", ""]]}, {"id": "2106.08114", "submitter": "Kexin Hu", "authors": "Kexin Hu, Kaiwen Guo, Qiang Tang, Zhenfeng Zhang, Hao Cheng, Zhiyang\n  Zhao", "title": "Leopard: Scaling BFT without Sacrificing Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of large-scale decentralized applications, a scalable and\nefficient Byzantine Fault Tolerant (BFT) protocol of hundreds of replicas is\ndesirable. Although the throughput of existing leader-based BFT protocols has\nreached a high level of $10^5$ requests per second for a small scale of\nreplicas, it drops significantly when the number of replicas increases, which\nleads to a lack of practicality. This paper focuses on the scalability of BFT\nprotocols and identifies a major bottleneck to leader-based BFT protocols due\nto the excessive workload of the leader at large scales. A new metric of\nscaling factor is defined to capture whether a BFT protocol will get stuck when\nit scales out, which can be used to measure the performance of efficiency and\nscalability of BFT protocols. We propose \"Leopard\", the first leader-based BFT\nprotocol that scales to multiple hundreds of replicas, and more importantly,\npreserves a high efficiency. We remove the bottleneck by introducing a\ntechnique of achieving a constant scaling factor, which takes full advantage of\nthe idle resource and adaptively balances the workload of the leader among all\nreplicas. We implement Leopard and evaluate its performance compared to\nHotStuff, the state-of-the-art BFT protocol. We run extensive experiments on\nthe two systems with up to 600 replicas. The results show that Leopard achieves\nsignificant performance improvements both on throughput and scalability. In\nparticular, the throughput of Leopard remains at a high level of $10^5$ when\nthe system scales out to 600 replicas. It achieves a $5\\times$ throughput over\nHotStuff when the scale is 300 (which is already the largest scale we can see\nthe progress of the latter in our experiments), and the gap becomes wider as\nthe number of replicas further increases.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:16:48 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 03:46:43 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 07:42:55 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Hu", "Kexin", ""], ["Guo", "Kaiwen", ""], ["Tang", "Qiang", ""], ["Zhang", "Zhenfeng", ""], ["Cheng", "Hao", ""], ["Zhao", "Zhiyang", ""]]}, {"id": "2106.08157", "submitter": "Kaihua Qin", "authors": "Kaihua Qin, Liyi Zhou, Yaroslav Afonin, Ludovico Lazzaretti, Arthur\n  Gervais", "title": "CeFi vs. DeFi -- Comparing Centralized to Decentralized Finance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To non-experts, the traditional Centralized Finance (CeFi) ecosystem may seem\nobscure, because users are typically not aware of the underlying rules or\nagreements of financial assets and products. Decentralized Finance (DeFi),\nhowever, is making its debut as an ecosystem claiming to offer transparency and\ncontrol, which are partially attributable to the underlying integrity-protected\nblockchain, as well as currently higher financial asset yields than CeFi. Yet,\nthe boundaries between CeFi and DeFi may not be always so clear cut.\n  In this work, we systematically analyze the differences between CeFi and\nDeFi, covering legal, economic, security, privacy and market manipulation. We\nprovide a structured methodology to differentiate between a CeFi and a DeFi\nservice. Our findings show that certain DeFi assets (such as USDC or USDT\nstablecoins) do not necessarily classify as DeFi assets, and may endanger the\neconomic security of intertwined DeFi protocols. We conclude this work with the\nexploration of possible synergies between CeFi and DeFi.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:53:43 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 13:03:35 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Qin", "Kaihua", ""], ["Zhou", "Liyi", ""], ["Afonin", "Yaroslav", ""], ["Lazzaretti", "Ludovico", ""], ["Gervais", "Arthur", ""]]}, {"id": "2106.08177", "submitter": "Shaykh Siddique", "authors": "Shaykh Siddique, Monica Yasmin, Tasnova Bintee Taher, Mushfiqul Alam", "title": "The Reliability and Acceptance of Biometric System in Bangladesh: Users\n  Perspective", "comments": "7 pages, 4 figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology, 69(6),\n  15-21, June 2021", "doi": "10.14445/22312803/IJCTT-V69I6P103", "report-no": null, "categories": "cs.CR cs.CY cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Biometric systems are the latest technologies of unique identification.\nPeople all over the world prefer to use this unique identification technology\nfor their authentication security. The goal of this research is to evaluate the\nbiometric systems based on system reliability and user satisfaction. As\ntechnology fully depends on personal data, so in terms of the quality and\nreliability of biometric systems, user satisfaction is a principal factor. To\nwalk with the digital era, it is extremely important to assess users' concerns\nabout data security as the systems are conducted the authentication by\nanalyzing users' personal data. The study shows that users are satisfied by\nusing biometric systems rather than other security systems. Besides, hardware\nfailure is a big issue faced by biometric systems users. Finally, a matrix is\ngenerated to compare the performance of popular biometric systems from the\nusers' opinions. As system reliability and user satisfaction are the focused\nissue of this research, biometric service providers can use these phenomena to\nfind what aspect of improvement they need for their services. Also, this study\ncan be a great visualizer for Bangladeshi users, so that they can easily\nrealize which biometric system they have to choose.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:46:00 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Siddique", "Shaykh", ""], ["Yasmin", "Monica", ""], ["Taher", "Tasnova Bintee", ""], ["Alam", "Mushfiqul", ""]]}, {"id": "2106.08209", "submitter": "Pedro Miguel Sanchez Sanchez", "authors": "Pedro Miguel S\\'anchez S\\'anchez, Jos\\'e Mar\\'ia Jorquera Valero,\n  Alberto Huertas Celdr\\'an, G\\'er\\^ome Bovet, Manuel Gil P\\'erez, Gregorio\n  Mart\\'inez P\\'erez", "title": "Can Evil IoT Twins Be Identified? Now Yes, a Hardware Behavioral\n  Fingerprinting Methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The connectivity and resource-constrained nature of IoT, and in particular\nsingle-board devices, opens up to cybersecurity concerns affecting the\nIndustrial Internet of Things (IIoT). One of the most important is the presence\nof evil IoT twins. Evil IoT twins are malicious devices, with identical\nhardware and software configurations to authorized ones, that can provoke\nsensitive information leakages, data poisoning, or privilege escalation in\nindustrial scenarios. Combining behavioral fingerprinting and Machine/Deep\nLearning (ML/DL) techniques is a promising solution to identify evil IoT twins\nby detecting minor performance differences generated by imperfections in\nmanufacturing. However, existing solutions are not suitable for single-board\ndevices because they do not consider their hardware and software limitations,\nunderestimate critical aspects during the identification performance\nevaluation, and do not explore the potential of ML/DL techniques. Moreover,\nthere is a dramatic lack of work explaining essential aspects to considering\nduring the identification of identical devices. This work proposes an\nML/DL-oriented methodology that uses behavioral fingerprinting to identify\nidentical single-board devices. The methodology leverages the different\nbuilt-in components of the system, comparing their internal behavior with each\nother to detect variations that occurred in manufacturing processes. The\nvalidation has been performed in a real environment composed of identical\nRaspberry Pi 4 Model B devices, achieving the identification for all devices by\nsetting a 50% threshold in the evaluation process. Finally, a discussion\ncompares the proposed solution with related work and provides important lessons\nlearned and limitations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 15:16:54 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["S\u00e1nchez", "Pedro Miguel S\u00e1nchez", ""], ["Valero", "Jos\u00e9 Mar\u00eda Jorquera", ""], ["Celdr\u00e1n", "Alberto Huertas", ""], ["Bovet", "G\u00e9r\u00f4me", ""], ["P\u00e9rez", "Manuel Gil", ""], ["P\u00e9rez", "Gregorio Mart\u00ednez", ""]]}, {"id": "2106.08361", "submitter": "Alexey Zaytsev", "authors": "Ivan Fursov, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun,\n  Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey\n  Zaytsev, Evgeny Burnaev", "title": "Adversarial Attacks on Deep Models for Financial Transaction Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR q-fin.ST", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine learning models using transaction records as inputs are popular among\nfinancial institutions. The most efficient models use deep-learning\narchitectures similar to those in the NLP community, posing a challenge due to\ntheir tremendous number of parameters and limited robustness. In particular,\ndeep-learning models are vulnerable to adversarial attacks: a little change in\nthe input harms the model's output.\n  In this work, we examine adversarial attacks on transaction records data and\ndefences from these attacks. The transaction records data have a different\nstructure than the canonical NLP or time series data, as neighbouring records\nare less connected than words in sentences, and each record consists of both\ndiscrete merchant code and continuous transaction amount. We consider a\nblack-box attack scenario, where the attack doesn't know the true decision\nmodel, and pay special attention to adding transaction tokens to the end of a\nsequence. These limitations provide more realistic scenario, previously\nunexplored in NLP world.\n  The proposed adversarial attacks and the respective defences demonstrate\nremarkable performance using relevant datasets from the financial industry. Our\nresults show that a couple of generated transactions are sufficient to fool a\ndeep-learning model. Further, we improve model robustness via adversarial\ntraining or separate adversarial examples detection. This work shows that\nembedding protection from adversarial attacks improves model robustness,\nallowing a wider adoption of deep models for transaction records in banking and\nfinance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 18:15:26 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Fursov", "Ivan", ""], ["Morozov", "Matvey", ""], ["Kaploukhaya", "Nina", ""], ["Kovtun", "Elizaveta", ""], ["Rivera-Castro", "Rodrigo", ""], ["Gusev", "Gleb", ""], ["Babaev", "Dmitry", ""], ["Kireev", "Ivan", ""], ["Zaytsev", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2106.08387", "submitter": "Jiefeng Chen", "authors": "Jiefeng Chen, Yang Guo, Xi Wu, Tianqi Li, Qicheng Lao, Yingyu Liang,\n  Somesh Jha", "title": "Towards Adversarial Robustness via Transductive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been emerging interest to use transductive learning for adversarial\nrobustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020). Compared to\ntraditional \"test-time\" defenses, these defense mechanisms \"dynamically\nretrain\" the model based on test time input via transductive learning; and\ntheoretically, attacking these defenses boils down to bilevel optimization,\nwhich seems to raise the difficulty for adaptive attacks. In this paper, we\nfirst formalize and analyze modeling aspects of transductive robustness. Then,\nwe propose the principle of attacking model space for solving bilevel attack\nobjectives, and present an instantiation of the principle which breaks previous\ntransductive defenses. These attacks thus point to significant difficulties in\nthe use of transductive learning to improve adversarial robustness. To this\nend, we present new theoretical and empirical evidence in support of the\nutility of transductive learning.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 19:32:12 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chen", "Jiefeng", ""], ["Guo", "Yang", ""], ["Wu", "Xi", ""], ["Li", "Tianqi", ""], ["Lao", "Qicheng", ""], ["Liang", "Yingyu", ""], ["Jha", "Somesh", ""]]}, {"id": "2106.08393", "submitter": "Elchanan Mossel", "authors": "Ankur Moitra and Elchanan Mossel and Colin Sandon", "title": "Spoofing Generalization: When Can't You Trust Proprietary Models?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study the computational complexity of determining whether a\nmachine learning model that perfectly fits the training data will generalizes\nto unseen data. In particular, we study the power of a malicious agent whose\ngoal is to construct a model g that fits its training data and nothing else,\nbut is indistinguishable from an accurate model f. We say that g strongly\nspoofs f if no polynomial-time algorithm can tell them apart. If instead we\nrestrict to algorithms that run in $n^c$ time for some fixed $c$, we say that g\nc-weakly spoofs f. Our main results are\n  1. Under cryptographic assumptions, strong spoofing is possible and 2. For\nany c> 0, c-weak spoofing is possible unconditionally\n  While the assumption of a malicious agent is an extreme scenario (hopefully\ncompanies training large models are not malicious), we believe that it sheds\nlight on the inherent difficulties of blindly trusting large proprietary models\nor data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 19:46:53 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Moitra", "Ankur", ""], ["Mossel", "Elchanan", ""], ["Sandon", "Colin", ""]]}, {"id": "2106.08475", "submitter": "Zahra Ghodsi", "authors": "Zahra Ghodsi, Nandan Kumar Jha, Brandon Reagen, Siddharth Garg", "title": "Circa: Stochastic ReLUs for Private Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simultaneous rise of machine learning as a service and concerns over user\nprivacy have increasingly motivated the need for private inference (PI). While\nrecent work demonstrates PI is possible using cryptographic primitives, the\ncomputational overheads render it impractical. The community is largely\nunprepared to address these overheads, as the source of slowdown in PI stems\nfrom the ReLU operator whereas optimizations for plaintext inference focus on\noptimizing FLOPs. In this paper we re-think the ReLU computation and propose\noptimizations for PI tailored to properties of neural networks. Specifically,\nwe reformulate ReLU as an approximate sign test and introduce a novel\ntruncation method for the sign test that significantly reduces the cost per\nReLU. These optimizations result in a specific type of stochastic ReLU. The key\nobservation is that the stochastic fault behavior is well suited for the\nfault-tolerant properties of neural network inference. Thus, we provide\nsignificant savings without impacting accuracy. We collectively call the\noptimizations Circa and demonstrate improvements of up to 4.7x storage and 3x\nruntime over baseline implementations; we further show that Circa can be used\non top of recent PI optimizations to obtain 1.8x additional speedup.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:52:45 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ghodsi", "Zahra", ""], ["Jha", "Nandan Kumar", ""], ["Reagen", "Brandon", ""], ["Garg", "Siddharth", ""]]}, {"id": "2106.08554", "submitter": "Yuzhe (Richard) Tang", "authors": "Yibo Wang and Qi Zhang and Kai Li and Yuzhe Tang and Xiapu Luo and\n  Ting Chen", "title": "iBatch: Saving Ethereum Fees via Secure and Cost-Effective Batching of\n  Smart-Contract Invocations", "comments": "ESEC/FSE 2021", "journal-ref": null, "doi": "10.1145/3468264.3468568", "report-no": null, "categories": "cs.CR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents iBatch, a middleware system running on top of an\noperational Ethereum network to enable secure batching of smart-contract\ninvocations against an untrusted relay server off-chain. iBatch does so at a\nlow overhead by validating the server's batched invocations in smart contracts\nwithout additional states. The iBatch mechanism supports a variety of policies,\nranging from conservative to aggressive batching, and can be configured\nadaptively to the current workloads. iBatch automatically rewrites smart\ncontracts to integrate with legacy applications and support large-scale\ndeployment.\n  We built an evaluation platform for fast and cost-accurate transaction\nreplaying and constructed real transaction benchmarks on popular Ethereum\napplications. With a functional prototype of iBatch, we conduct extensive cost\nevaluations, which shows iBatch saves $14.6\\%\\sim{}59.1\\%$ Gas cost per\ninvocation with a moderate 2-minute delay and $19.06\\%\\sim{}31.52\\%$ Ether cost\nper invocation with a delay of $0.26\\sim{}1.66$ blocks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 05:13:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Yibo", ""], ["Zhang", "Qi", ""], ["Li", "Kai", ""], ["Tang", "Yuzhe", ""], ["Luo", "Xiapu", ""], ["Chen", "Ting", ""]]}, {"id": "2106.08567", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu, Jinshuo Dong and Yu-Xiang Wang", "title": "Optimal Accounting of Differential Privacy via Characteristic Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Characterizing the privacy degradation over compositions, i.e., privacy\naccounting, is a fundamental topic in differential privacy (DP) with many\napplications to differentially private machine learning and federated learning.\n  We propose a unification of recent advances (Renyi DP, privacy profiles,\n$f$-DP and the PLD formalism) via the characteristic function ($\\phi$-function)\nof a certain ``worst-case'' privacy loss random variable.\n  We show that our approach allows natural adaptive composition like Renyi DP,\n  provides exactly tight privacy accounting like PLD, and can be (often\nlosslessly) converted to privacy profile and $f$-DP, thus providing\n$(\\epsilon,\\delta)$-DP guarantees and interpretable tradeoff functions.\nAlgorithmically, we propose an analytical Fourier accountant that represents\nthe complex logarithm of $\\phi$-functions symbolically and uses Gaussian\nquadrature for numerical computation. On several popular DP mechanisms and\ntheir subsampled counterparts, we demonstrate the flexibility and tightness of\nour approach in theory and experiments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 06:13:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhu", "Yuqing", ""], ["Dong", "Jinshuo", ""], ["Wang", "Yu-Xiang", ""]]}, {"id": "2106.08568", "submitter": "Simon Hacks", "authors": "Simon Hacks, Robert Lagerstr\\\"om, Daniel Ritter", "title": "Towards Automated Attack Simulations of BPMN-based Processes", "comments": "Submitted for review to EDOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process digitization and integration is an increasing need for enterprises,\nwhile cyber-attacks denote a growing threat. Using the Business Process\nManagement Notation (BPMN) is common to handle the digital and integration\nfocus within and across organizations. In other parts of the same companies,\nthreat modeling and attack graphs are used for analyzing the security posture\nand resilience.\n  In this paper, we propose a novel approach to use attack graph simulations on\nprocesses represented in BPMN. Our contributions are the identification of\nBPMN's attack surface, a mapping of BPMN elements to concepts in a Meta Attack\nLanguage (MAL)-based Domain-Specific Language (DSL), called coreLang, and a\nprototype to demonstrate our approach in a case study using a real-world\ninvoice integration process. The study shows that non-invasively enriching BPMN\ninstances with cybersecurity analysis through attack graphs is possible without\nmuch human expert input. The resulting insights into potential vulnerabilities\ncould be beneficial for the process modelers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 06:19:50 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Hacks", "Simon", ""], ["Lagerstr\u00f6m", "Robert", ""], ["Ritter", "Daniel", ""]]}, {"id": "2106.08654", "submitter": "Steffen Wendzel", "authors": "Steffen Wendzel, Luca Caviglione, Wojciech Mazurczyk, Aleksandra\n  Mileva, Jana Dittmann, Christian Kr\\\"atzer, Kevin Lamsh\\\"oft, Claus\n  Vielhauer, Laura Hartmann, J\\\"org Keller, Tom Neubert", "title": "A Revised Taxonomy of Steganography Embedding Patterns", "comments": null, "journal-ref": "In Proc. of the 16th International Conference on Availability,\n  Reliability and Security (ARES'21), August 17--20, 2021, Vienna, Austria", "doi": "10.1145/3465481.347006", "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steganography embraces several hiding techniques which spawn across multiple\ndomains. However, the related terminology is not unified among the different\ndomains, such as digital media steganography, text steganography,\ncyber-physical systems steganography, network steganography (network covert\nchannels), local covert channels, and out-of-band covert channels. To cope with\nthis, a prime attempt has been done in 2015, with the introduction of the\nso-called hiding patterns, which allow to describe hiding techniques in a more\nabstract manner. Despite significant enhancements, the main limitation of such\na taxonomy is that it only considers the case of network steganography.\n  Therefore, this paper reviews both the terminology and the taxonomy of hiding\npatterns as to make them more general. Specifically, hiding patterns are split\ninto those that describe the embedding and the representation of hidden data\nwithin the cover object.\n  As a first research action, we focus on embedding hiding patterns and we show\nhow they can be applied to multiple domains of steganography instead of being\nlimited to the network scenario. Additionally, we exemplify representation\npatterns using network steganography. Our pattern collection is available under\nhttps://patterns.ztt.hs-worms.de.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 09:34:40 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wendzel", "Steffen", ""], ["Caviglione", "Luca", ""], ["Mazurczyk", "Wojciech", ""], ["Mileva", "Aleksandra", ""], ["Dittmann", "Jana", ""], ["Kr\u00e4tzer", "Christian", ""], ["Lamsh\u00f6ft", "Kevin", ""], ["Vielhauer", "Claus", ""], ["Hartmann", "Laura", ""], ["Keller", "J\u00f6rg", ""], ["Neubert", "Tom", ""]]}, {"id": "2106.08692", "submitter": "Andras Gazdag", "authors": "Irina Chiscop, Andr\\'as Gazdag, Joost Bosman, Gergely Bicz\\'ok", "title": "Detecting message modification attacks on the CAN bus with Temporal\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": "10.5220/0010445504880496", "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple attacks have shown that in-vehicle networks have vulnerabilities\nwhich can be exploited. Securing the Controller Area Network (CAN) for modern\nvehicles has become a necessary task for car manufacturers. Some attacks inject\npotentially large amount of fake messages into the CAN network; however, such\nattacks are relatively easy to detect. In more sophisticated attacks, the\noriginal messages are modified, making the detection a more complex problem. In\nthis paper, we present a novel machine learning based intrusion detection\nmethod for CAN networks. We focus on detecting message modification attacks,\nwhich do not change the timing patterns of communications. Our proposed\ntemporal convolutional network-based solution can learn the normal behavior of\nCAN signals and differentiate them from malicious ones. The method is evaluated\non multiple CAN-bus message IDs from two public datasets including different\ntypes of attacks. Performance results show that our lightweight approach\ncompares favorably to the state-of-the-art unsupervised learning approach,\nachieving similar or better accuracy for a wide range of scenarios with a\nsignificantly lower false positive rate.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:51:58 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chiscop", "Irina", ""], ["Gazdag", "Andr\u00e1s", ""], ["Bosman", "Joost", ""], ["Bicz\u00f3k", "Gergely", ""]]}, {"id": "2106.08746", "submitter": "Buse Gul Atli", "authors": "Buse G.A. Tekgul, Shelly Wang, Samuel Marchal, N. Asokan", "title": "Real-time Attacks Against Deep Reinforcement Learning Policies", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has discovered that deep reinforcement learning (DRL) policies\nare vulnerable to adversarial examples. These attacks mislead the policy of DRL\nagents by perturbing the state of the environment observed by agents. They are\nfeasible in principle but too slow to fool DRL policies in real time. We\npropose a new attack to fool DRL policies that is both effective and efficient\nenough to be mounted in real time. We utilize the Universal Adversarial\nPerturbation (UAP) method to compute effective perturbations independent of the\nindividual inputs to which they are applied. Via an extensive evaluation using\nAtari 2600 games, we show that our technique is effective, as it fully degrades\nthe performance of both deterministic and stochastic policies (up to 100%, even\nwhen the $l_\\infty$ bound on the perturbation is as small as 0.005). We also\nshow that our attack is efficient, incurring an online computational cost of\n0.027ms on average. It is faster compared to the response time (0.6ms on\naverage) of agents with different DRL policies, and considerably faster than\nprior attacks (2.7ms on average). Furthermore, we demonstrate that known\ndefenses are ineffective against universal perturbations. We propose an\neffective detection technique which can form the basis for robust defenses\nagainst attacks based on universal perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 12:44:59 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tekgul", "Buse G. A.", ""], ["Wang", "Shelly", ""], ["Marchal", "Samuel", ""], ["Asokan", "N.", ""]]}, {"id": "2106.08759", "submitter": "Nicola Tuveri", "authors": "Daniel J. Bernstein (1 and 2), Billy Bob Brumley (3), Ming-Shing Chen\n  (2), Nicola Tuveri (3) ((1) Department of Computer Science, University of\n  Illinois at Chicago, Chicago, IL, USA, (2) Ruhr University Bochum, Bochum,\n  Germany, (3) Tampere University, Tampere, Finland)", "title": "OpenSSLNTRU: Faster post-quantum TLS key exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Google's CECPQ1 experiment in 2016 integrated a post-quantum key-exchange\nalgorithm, newhope1024, into TLS 1.2. The Google-Cloudflare CECPQ2 experiment\nin 2019 integrated a more efficient key-exchange algorithm, ntruhrss701, into\nTLS 1.3.\n  This paper revisits the choices made in CECPQ2, and shows how to achieve\nhigher performance for post-quantum key exchange in TLS 1.3 using a\nhigher-security algorithm, sntrup761. Previous work had indicated that\nntruhrss701 key generation was much faster than sntrup761 key generation, but\nthis paper makes sntrup761 key generation much faster by generating a batch of\nkeys at once.\n  Batch key generation is invisible at the TLS protocol layer, but raises\nsoftware-engineering questions regarding the difficulty of integrating batch\nkey exchange into existing TLS libraries and applications. This paper shows\nthat careful choices of software layers make it easy to integrate fast\npost-quantum software, including batch key exchange, into TLS with minor\nchanges to TLS libraries and no changes to applications.\n  As a demonstration of feasibility, this paper reports successful integration\nof its fast sntrup761 library, via a lightly patched OpenSSL, into an\nunmodified web browser and an unmodified TLS terminator. This paper also\nreports TLS 1.3 handshake benchmarks, achieving more TLS 1.3 handshakes per\nsecond than any software included in OpenSSL.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:12:46 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Bernstein", "Daniel J.", "", "1 and 2"], ["Brumley", "Billy Bob", ""], ["Chen", "Ming-Shing", ""], ["Tuveri", "Nicola", ""]]}, {"id": "2106.08877", "submitter": "Mahya Morid Ahmadi", "authors": "Mahya Morid Ahmadi, Faiq Khalid, Muhammad Shafique", "title": "Side-Channel Attacks on RISC-V Processors: Current Progress, Challenges,\n  and Opportunities", "comments": "CYBER 2020, The Fifth International Conference on Cyber-Technologies\n  and Cyber-Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Side-channel attacks on microprocessors, like the RISC-V, exhibit security\nvulnerabilities that lead to several design challenges. Hence, it is imperative\nto study and analyze these security vulnerabilities comprehensively. In this\npaper, we present a brief yet comprehensive study of the security\nvulnerabilities in modern microprocessors with respect to side-channel attacks\nand their respective mitigation techniques. The focus of this paper is to\nanalyze the hardware-exploitable side-channel attack using power consumption\nand software-exploitable side-channel attacks to manipulate cache. Towards\nthis, we perform an in-depth analysis of the applicability and practical\nimplications of cache attacks on RISC-V microprocessors and their associated\nchallenges. Finally, based on the comparative study and our analysis, we\nhighlight some key research directions to develop robust RISC-V microprocessors\nthat are resilient to side-channel attacks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:51:00 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ahmadi", "Mahya Morid", ""], ["Khalid", "Faiq", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2106.08913", "submitter": "Moritz Schloegel", "authors": "Moritz Schloegel, Tim Blazytko, Moritz Contag, Cornelius Aschermann,\n  Julius Basler, Thorsten Holz, Ali Abbasi", "title": "Loki: Hardening Code Obfuscation Against Automated Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Software obfuscation is a crucial technology to protect intellectual\nproperty. Despite its importance, commercial and academic state-of-the-art\nobfuscation approaches are vulnerable to a plethora of automated deobfuscation\nattacks, such as symbolic execution, taint analysis, or program synthesis.\nWhile several enhanced techniques were proposed to thwart taint analysis or\nsymbolic execution, they either impose a prohibitive runtime overhead or can be\nremoved by compiler optimizations. In general, they suffer from focusing on a\nsingle attack vector, allowing an attacker to switch to other more effective\ntechniques, such as program synthesis. In this work, we present Loki, an\napproach for code obfuscation that is resilient against all known automated\ndeobfuscation attacks. To this end, we deploy multiple techniques, including a\ngeneric approach to synthesize formally verified expressions of arbitrary\ncomplexity. Contrary to state-of-the-art approaches that rely on a few\nhardcoded generation rules, our expressions are more diverse and harder to\npattern match against. Moreover, Loki protects against previously unaccounted\nattack vectors such as program synthesis, for which it reduces the success rate\nto merely 19%. Overall, our design incurs significantly less overhead while\nproviding a much stronger protection level.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:05:27 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 10:17:24 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Schloegel", "Moritz", ""], ["Blazytko", "Tim", ""], ["Contag", "Moritz", ""], ["Aschermann", "Cornelius", ""], ["Basler", "Julius", ""], ["Holz", "Thorsten", ""], ["Abbasi", "Ali", ""]]}, {"id": "2106.08970", "submitter": "Hossein Souri", "authors": "Hossein Souri, Micah Goldblum, Liam Fowl, Rama Chellappa, Tom\n  Goldstein", "title": "Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks\n  Trained from Scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the curation of data for machine learning becomes increasingly automated,\ndataset tampering is a mounting threat. Backdoor attackers tamper with training\ndata to embed a vulnerability in models that are trained on that data. This\nvulnerability is then activated at inference time by placing a \"trigger\" into\nthe model's input. Typical backdoor attacks insert the trigger directly into\nthe training data, although the presence of such an attack may be visible upon\ninspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning\nwithout placing a trigger into the training data at all. However, this hidden\ntrigger attack is ineffective at poisoning neural networks trained from\nscratch. We develop a new hidden trigger attack, Sleeper Agent, which employs\ngradient matching, data selection, and target model re-training during the\ncrafting process. Sleeper Agent is the first hidden trigger backdoor attack to\nbe effective against neural networks trained from scratch. We demonstrate its\neffectiveness on ImageNet and in black-box settings. Our implementation code\ncan be found at https://github.com/hsouri/Sleeper-Agent.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:09:55 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Souri", "Hossein", ""], ["Goldblum", "Micah", ""], ["Fowl", "Liam", ""], ["Chellappa", "Rama", ""], ["Goldstein", "Tom", ""]]}, {"id": "2106.09006", "submitter": "Daniel Bailey", "authors": "Daniel V. Bailey and Philipp Markert and Adam J. Aviv", "title": "\"I have no idea what they're trying to accomplish:\" Enthusiastic and\n  Casual Signal Users' Understanding of Signal PINs", "comments": "To appear at Symposium on Usable Privacy and Security (SOUPS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We conducted an online study with $n = 235$ Signal users on their\nunderstanding and usage of PINs in Signal. In our study, we observe a split in\nPIN management and composition strategies between users who can explain the\npurpose of the Signal PINs (56%; enthusiasts) and users who cannot (44%; casual\nusers). Encouraging adoption of PINs by Signal appears quite successful: only\n14% opted-out of setting a PIN entirely. Among those who did set a PIN, most\nenthusiasts had long, complex alphanumeric PINs generated by and saved in a\npassword manager. Meanwhile more casual Signal users mostly relied on short\nnumeric-only PINs. Our results suggest that better communication about the\npurpose of the Signal PIN could help more casual users understand the features\nPINs enable (such as that it is not simply a personal identification number).\nThis communication could encourage a stronger security posture.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:55:46 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Bailey", "Daniel V.", ""], ["Markert", "Philipp", ""], ["Aviv", "Adam J.", ""]]}, {"id": "2106.09218", "submitter": "Shahid Alam", "authors": "Shahid Alam, M. Zain ul Abideen, Shahzad Saleem", "title": "DroidMorph: Are We Ready to Stop the Attack of Android Malware Clones?", "comments": null, "journal-ref": null, "doi": "10.1109/ISMSIT.2018.8567059", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of Android malware variants (clones) are on the rise and, to stop\nthis attack of clones we need to develop new methods and techniques for\nanalysing and detecting them. As a first step, we need to study how these\nmalware clones are generated. This will help us better anticipate and recognize\nthese clones. In this paper we present a new tool named DroidMorph, that\nprovides morphing of Android applications (APKs) at different level of\nabstractions, and can be used to create Android application (malware/benign)\nclones. As a case study we perform testing and evaluating resilience of current\ncommercial anti-malware products against attack of the Android malware clones\ngenerated by DroidMorph. We found that 8 out of 17 leading commercial\nanti-malware programs were not able to detect any of the morphed APKs. We hope\nthat DroidMorph will be used in future research, to improve Android malware\nclones analysis and detection, and help stop them.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 02:42:01 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Alam", "Shahid", ""], ["Abideen", "M. Zain ul", ""], ["Saleem", "Shahzad", ""]]}, {"id": "2106.09222", "submitter": "Ousmane Dia", "authors": "Ousmane Amadou Dia, Theofanis Karaletsos, Caner Hazirbas, Cristian\n  Canton Ferrer, Ilknur Kaynar Kabul, Erik Meijer", "title": "Localized Uncertainty Attacks", "comments": "CVPR 2021 Workshop on Adversarial Machine Learning in Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The susceptibility of deep learning models to adversarial perturbations has\nstirred renewed attention in adversarial examples resulting in a number of\nattacks. However, most of these attacks fail to encompass a large spectrum of\nadversarial perturbations that are imperceptible to humans. In this paper, we\npresent localized uncertainty attacks, a novel class of threat models against\ndeterministic and stochastic classifiers. Under this threat model, we create\nadversarial examples by perturbing only regions in the inputs where a\nclassifier is uncertain. To find such regions, we utilize the predictive\nuncertainty of the classifier when the classifier is stochastic or, we learn a\nsurrogate model to amortize the uncertainty when it is deterministic. Unlike\n$\\ell_p$ ball or functional attacks which perturb inputs indiscriminately, our\ntargeted changes can be less perceptible. When considered under our threat\nmodel, these attacks still produce strong adversarial examples; with the\nexamples retaining a greater degree of similarity with the inputs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 03:07:22 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Dia", "Ousmane Amadou", ""], ["Karaletsos", "Theofanis", ""], ["Hazirbas", "Caner", ""], ["Ferrer", "Cristian Canton", ""], ["Kabul", "Ilknur Kaynar", ""], ["Meijer", "Erik", ""]]}, {"id": "2106.09249", "submitter": "Ningfei Wang", "authors": "Yulong Cao*, Ningfei Wang*, Chaowei Xiao*, Dawei Yang*, Jin Fang,\n  Ruigang Yang, Qi Alfred Chen, Mingyan Liu, Bo Li (*co-first authors)", "title": "Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion\n  based Perception in Autonomous Driving Under Physical-World Attacks", "comments": "Accepted by IEEE S&P 2021", "journal-ref": null, "doi": "10.1109/SP40001.2021.00076", "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Autonomous Driving (AD) systems, perception is both security and safety\ncritical. Despite various prior studies on its security issues, all of them\nonly consider attacks on camera- or LiDAR-based AD perception alone. However,\nproduction AD systems today predominantly adopt a Multi-Sensor Fusion (MSF)\nbased design, which in principle can be more robust against these attacks under\nthe assumption that not all fusion sources are (or can be) attacked at the same\ntime. In this paper, we present the first study of security issues of MSF-based\nperception in AD systems. We directly challenge the basic MSF design assumption\nabove by exploring the possibility of attacking all fusion sources\nsimultaneously. This allows us for the first time to understand how much\nsecurity guarantee MSF can fundamentally provide as a general defense strategy\nfor AD perception.\n  We formulate the attack as an optimization problem to generate a\nphysically-realizable, adversarial 3D-printed object that misleads an AD system\nto fail in detecting it and thus crash into it. We propose a novel attack\npipeline that addresses two main design challenges: (1) non-differentiable\ntarget camera and LiDAR sensing systems, and (2) non-differentiable cell-level\naggregated features popularly used in LiDAR-based AD perception. We evaluate\nour attack on MSF included in representative open-source industry-grade AD\nsystems in real-world driving scenarios. Our results show that the attack\nachieves over 90% success rate across different object types and MSF. Our\nattack is also found stealthy, robust to victim positions, transferable across\nMSF algorithms, and physical-world realizable after being 3D-printed and\ncaptured by LiDAR and camera devices. To concretely assess the end-to-end\nsafety impact, we further perform simulation evaluation and show that it can\ncause a 100% vehicle collision rate for an industry-grade AD system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 05:11:07 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Cao*", "Yulong", "", "*co-first authors"], ["Wang*", "Ningfei", "", "*co-first authors"], ["Xiao*", "Chaowei", "", "*co-first authors"], ["Yang*", "Dawei", "", "*co-first authors"], ["Fang", "Jin", "", "*co-first authors"], ["Yang", "Ruigang", "", "*co-first authors"], ["Chen", "Qi Alfred", "", "*co-first authors"], ["Liu", "Mingyan", "", "*co-first authors"], ["Li", "Bo", "", "*co-first authors"]]}, {"id": "2106.09349", "submitter": "Amirmohammad Pasdar", "authors": "Amirmohammad Pasdar, Zhongli Dong and Young Choon Lee", "title": "Blockchain Oracle Design Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain is a form of distributed ledger technology (DLT) where data is\nshared among users connected over the internet. Transactions are data state\nchanges on the blockchain that are permanently recorded in a secure and\ntransparent way without the need of a third party. Besides, the introduction of\nsmart contracts to the blockchain has added programmability to the blockchain\nand revolutionized the software ecosystem leading toward decentralized\napplications (DApps) attracting businesses and organizations to employ this\ntechnology. Although promising, blockchains and smart contracts have no access\nto the external systems (i.e., off-chain) where real-world data and events\nresides; consequently, the usability of smart contracts in terms of performance\nand programmability would be limited to the on-chain data. Hence,\n\\emph{blockchain oracles} are introduced to mitigate the issue and are defined\nas trusted third-party services that send and verify the external information\n(i.e., feedback) and submit it to smart contracts for triggering state changes\nin the blockchain. In this paper, we will study and analyze blockchain oracles\nwith regard to how they provide feedback to the blockchain and smart contracts.\nWe classify the blockchain oracle techniques into two major groups such as\nvoting-based strategies and reputation-based ones. The former mainly relies on\nparticipants' stakes for outcome finalization while the latter considers\nreputation in conjunction with authenticity proof mechanisms for data\ncorrectness and integrity. We then provide a structured description of patterns\nin detail for each classification and discuss research directions in the end.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:08:49 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Pasdar", "Amirmohammad", ""], ["Dong", "Zhongli", ""], ["Lee", "Young Choon", ""]]}, {"id": "2106.09352", "submitter": "Da Yu", "authors": "Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu", "title": "Large Scale Private Learning via Low-rank Reparametrization", "comments": "Published as a conference paper in International Conference on\n  Machine Learning (ICML 2021). Source code available at\n  https://github.com/dayu11/Differentially-Private-Deep-Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reparametrization scheme to address the challenges of applying\ndifferentially private SGD on large neural networks, which are 1) the huge\nmemory cost of storing individual gradients, 2) the added noise suffering\nnotorious dimensional dependence. Specifically, we reparametrize each weight\nmatrix with two \\emph{gradient-carrier} matrices of small dimension and a\n\\emph{residual weight} matrix. We argue that such reparametrization keeps the\nforward/backward process unchanged while enabling us to compute the projected\ngradient without computing the gradient itself. To learn with differential\nprivacy, we design \\emph{reparametrized gradient perturbation (RGP)} that\nperturbs the gradients on gradient-carrier matrices and reconstructs an update\nfor the original weight from the noisy gradients. Importantly, we use\nhistorical updates to find the gradient-carrier matrices, whose optimality is\nrigorously justified under linear regression and empirically verified with deep\nlearning tasks. RGP significantly reduces the memory cost and improves the\nutility. For example, we are the first able to apply differential privacy on\nthe BERT model and achieve an average accuracy of $83.9\\%$ on four downstream\ntasks with $\\epsilon=8$, which is within $5\\%$ loss compared to the non-private\nbaseline but enjoys much lower privacy leakage risk.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:14:43 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 12:00:41 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 06:01:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yu", "Da", ""], ["Zhang", "Huishuai", ""], ["Chen", "Wei", ""], ["Yin", "Jian", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2106.09376", "submitter": "Ossi R\\\"ais\\\"a", "authors": "Ossi R\\\"ais\\\"a, Antti Koskela, Antti Honkela", "title": "Differentially Private Hamiltonian Monte Carlo", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms have long been the main workhorses\nof Bayesian inference. Among them, Hamiltonian Monte Carlo (HMC) has recently\nbecome very popular due to its efficiency resulting from effective use of the\ngradients of the target distribution. In privacy-preserving machine learning,\ndifferential privacy (DP) has become the gold standard in ensuring that the\nprivacy of data subjects is not violated. Existing DP MCMC algorithms either\nuse random-walk proposals, or do not use the Metropolis--Hastings (MH)\nacceptance test to ensure convergence without decreasing their step size to\nzero. We present a DP variant of HMC using the MH acceptance test that builds\non a recently proposed DP MCMC algorithm called the penalty algorithm, and adds\nnoise to the gradient evaluations of HMC. We prove that the resulting algorithm\nconverges to the correct distribution, and is ergodic. We compare DP-HMC with\nthe existing penalty, DP-SGLD and DP-SGNHT algorithms, and find that DP-HMC has\nbetter or equal performance than the penalty algorithm, and performs more\nconsistently than DP-SGLD or DP-SGNHT.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:50:28 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["R\u00e4is\u00e4", "Ossi", ""], ["Koskela", "Antti", ""], ["Honkela", "Antti", ""]]}, {"id": "2106.09380", "submitter": "Giovanni Apruzzese", "authors": "Giovanni Apruzzese, Mauro Andreolini, Luca Ferretti, Mirco Marchetti,\n  Michele Colajanni", "title": "Modeling Realistic Adversarial Attacks against Network Intrusion\n  Detection Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3469659", "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The incremental diffusion of machine learning algorithms in supporting\ncybersecurity is creating novel defensive opportunities but also new types of\nrisks. Multiple researches have shown that machine learning methods are\nvulnerable to adversarial attacks that create tiny perturbations aimed at\ndecreasing the effectiveness of detecting threats. We observe that existing\nliterature assumes threat models that are inappropriate for realistic\ncybersecurity scenarios because they consider opponents with complete knowledge\nabout the cyber detector or that can freely interact with the target systems.\nBy focusing on Network Intrusion Detection Systems based on machine learning,\nwe identify and model the real capabilities and circumstances required by\nattackers to carry out feasible and successful adversarial attacks. We then\napply our model to several adversarial attacks proposed in literature and\nhighlight the limits and merits that can result in actual adversarial attacks.\nThe contributions of this paper can help hardening defensive systems by letting\ncyber defenders address the most critical and real issues, and can benefit\nresearchers by allowing them to devise novel forms of adversarial attacks based\non realistic threat models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:52:42 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Apruzzese", "Giovanni", ""], ["Andreolini", "Mauro", ""], ["Ferretti", "Luca", ""], ["Marchetti", "Mirco", ""], ["Colajanni", "Michele", ""]]}, {"id": "2106.09485", "submitter": "Onur G\\\"unl\\\"u Dr.-Ing.", "authors": "Onur G\\\"unl\\\"u, Matthieu Bloch, and Rafael F. Schaefer", "title": "Secure Multi-Function Computation with Private Remote Sources", "comments": "Shorter version to appear in the IEEE International Symposium on\n  Information Theory 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC cs.LG eess.SP math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a distributed function computation problem in which parties\nobserving noisy versions of a remote source facilitate the computation of a\nfunction of their observations at a fusion center through public communication.\nThe distributed function computation is subject to constraints, including not\nonly reliability and storage but also privacy and secrecy. Specifically, 1) the\nremote source should remain private from an eavesdropper and the fusion center,\nmeasured in terms of the information leaked about the remote source; 2) the\nfunction computed should remain secret from the eavesdropper, measured in terms\nof the information leaked about the arguments of the function, to ensure\nsecrecy regardless of the exact function used. We derive the exact rate regions\nfor lossless and lossy single-function computation and illustrate the lossy\nsingle-function computation rate region for an information bottleneck example,\nin which the optimal auxiliary random variables are characterized for\nbinary-input symmetric-output channels. We extend the approach to lossless and\nlossy asynchronous multiple-function computations with joint secrecy and\nprivacy constraints, in which case inner and outer bounds for the rate regions\ndiffering only in the Markov chain conditions imposed are characterized.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:34:40 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["G\u00fcnl\u00fc", "Onur", ""], ["Bloch", "Matthieu", ""], ["Schaefer", "Rafael F.", ""]]}, {"id": "2106.09518", "submitter": "Song-Kyoo Amang Kim Ph.D.", "authors": "Song-Kyoo Kim", "title": "Multi-Layered Blockchain Governance Game", "comments": "This working paper is targeted to submit at an international journal\n  in the Applied Mathematics area", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper deals with design of an integrated secure Blockchain network\nframework to prevent damages from attackers. The multi-layer concept which\ncould handle multiple number of networks is adapted on the top of Blockchain\nGovernance Game frameworks. This new integrated theoretical model is designed\nto find the best strategies toward preparation for preventing whole network\nsystems malfunction from attackers and it is developed based on the combination\nof the Blockchain Governance Game and the Strategic Alliance for Blockchain\nGovernance Game. Analytically tractable results for executing a safety mode are\nfully obtained and simulated results are demonstrated to obtain the optimal\nvalues of hyper parameters of a Blockchain based security network. This\nresearch helps those whom are constructing a multiple layer network by\nenhancing security features through multi-layer framework in a decentralized\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:47:15 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kim", "Song-Kyoo", ""]]}, {"id": "2106.09527", "submitter": "Gadekallu Thippa Reddy", "authors": "Shaashwat Agrawal, Sagnik Sarkar, Ons Aouedi, Gokul Yenduri, Kandaraj\n  Piamrat, Sweta Bhattacharya, Praveen Kumar Reddy Maddikunta, Thippa Reddy\n  Gadekallu", "title": "Federated Learning for Intrusion Detection System: Concepts, Challenges\n  and Future Directions", "comments": "Submitted to JNCA, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid development of the Internet and smart devices trigger surge in\nnetwork traffic making its infrastructure more complex and heterogeneous. The\npredominated usage of mobile phones, wearable devices and autonomous vehicles\nare examples of distributed networks which generate huge amount of data each\nand every day. The computational power of these devices have also seen steady\nprogression which has created the need to transmit information, store data\nlocally and drive network computations towards edge devices. Intrusion\ndetection systems play a significant role in ensuring security and privacy of\nsuch devices. Machine Learning and Deep Learning with Intrusion Detection\nSystems have gained great momentum due to their achievement of high\nclassification accuracy. However the privacy and security aspects potentially\ngets jeopardised due to the need of storing and communicating data to\ncentralized server. On the contrary, federated learning (FL) fits in\nappropriately as a privacy-preserving decentralized learning technique that\ndoes not transfer data but trains models locally and transfers the parameters\nto the centralized server. The present paper aims to present an extensive and\nexhaustive review on the use of FL in intrusion detection system. In order to\nestablish the need for FL, various types of IDS, relevant ML approaches and its\nassociated issues are discussed. The paper presents detailed overview of the\nimplementation of FL in various aspects of anomaly detection. The allied\nchallenges of FL implementations are also identified which provides idea on the\nscope of future direction of research. The paper finally presents the plausible\nsolutions associated with the identified challenges in FL based intrusion\ndetection system implementation acting as a baseline for prospective research.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:13:04 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Agrawal", "Shaashwat", ""], ["Sarkar", "Sagnik", ""], ["Aouedi", "Ons", ""], ["Yenduri", "Gokul", ""], ["Piamrat", "Kandaraj", ""], ["Bhattacharya", "Sweta", ""], ["Maddikunta", "Praveen Kumar Reddy", ""], ["Gadekallu", "Thippa Reddy", ""]]}, {"id": "2106.09536", "submitter": "Priyanka Joshi", "authors": "Priyanka Joshi and Bodhistwa Mazumdar", "title": "Single Event Transient Fault Analysis of ELEPHANT cipher", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel fault attack termed as Single Event\nTransient Fault Analysis (SETFA) attack, which is well suited for hardware\nimplementations. The proposed approach pinpoints hotspots in the cypher's Sbox\ncombinational logic circuit that significantly reduce the key entropy when\nsubjected to faults. ELEPHANT is a parallel authenticated encryption and\nassociated data (AEAD) scheme targeted to hardware implementations, a finalist\nin the Lightweight cryptography (LWC) competition launched by NIST. In this\nwork, we investigate vulnerabilities of ELEPHANT against fault analysis. We\nobserve that the use of 128-bit random nonce makes it resistant against many\ncryptanalysis techniques like differential, linear, etc., and their variants.\nHowever, the relaxed nature of Statistical Fault Analysis (SFA) methods makes\nthem widely applicable in restrictive environments. We propose a SETFA-based\nkey recovery attack on Elephant. We performed Single experiments with random\nplaintexts and keys, on Dumbo, a Sponge-based instance of the Elephant-AEAD\nscheme. Our proposed approach could recover the secret key in 85-250\nciphertexts. In essence, this work investigates new vulnerabilities towards\nfault analysis that may require to be addressed to ensure secure computations\nand communications in IoT scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 16:00:26 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Joshi", "Priyanka", ""], ["Mazumdar", "Bodhistwa", ""]]}, {"id": "2106.09565", "submitter": "Jie Ding", "authors": "Jie Ding and Bangjun Ding", "title": "Interval Privacy: A Framework for Data Collection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging public awareness and government regulations of data privacy\nmotivate new paradigms of collecting and analyzing data transparent and\nacceptable to data owners. We present a new concept of privacy and\ncorresponding data formats, mechanisms, and tradeoffs for privatizing data\nduring data collection. The privacy, named Interval Privacy, enforces the raw\ndata conditional distribution on the privatized data to be the same as its\nunconditional distribution over a nontrivial support set. Correspondingly, the\nproposed privacy mechanism will record each data value as a random interval\ncontaining it. The proposed interval privacy mechanisms can be easily deployed\nthrough most existing survey-based data collection paradigms, e.g., by asking a\nrespondent whether its data value is within a randomly generated range. Another\nunique feature of interval mechanisms is that they obfuscate the truth but not\ndistort it. The way of using narrowed range to convey information is\ncomplementary to the popular paradigm of perturbing data. Also, the interval\nmechanisms can generate progressively refined information at the discretion of\nindividual respondents. We study different theoretical aspects of the proposed\nprivacy. In the context of supervised learning, we also offer a method such\nthat existing supervised learning algorithms designed for point-valued data\ncould be directly applied to learning from interval-valued data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:47:21 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ding", "Jie", ""], ["Ding", "Bangjun", ""]]}, {"id": "2106.09599", "submitter": "Alex Wiesmaier", "authors": "Alexander Wiesmaier (1), Nouri Alnahawi (1), Tobias Grasmeyer (1),\n  Julian Gei{\\ss}ler (1), Alexander Zeier (2), Pia Bauspie{\\ss} (1), Andreas\n  Heinemann (1) ((1) Darmstadt University of Applied Sciences - Germany, (2)\n  MTG AG - Germany)", "title": "On PQC Migration and Crypto-Agility", "comments": "12 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the development of PQC algorithms, the actual migration of IT systems\nto such new schemes has to be considered, best by utilizing or establishing\ncrypto-agility. Much work in this respect is currently conducted all over the\nworld, making it hard to keep track of the many individual challenges and\nrespective solutions that have been identified. In consequence, it is difficult\nto judge for both individual application scenarios and on a global scale,\nwhether all (known) challenges have been addressed respectively or what their\ncurrent state is. We provide a literature survey and a snapshot of the\ndiscovered challenges and solutions categorized in different areas. We use this\nas starting point for a community project to keep track of the ongoing efforts\nand the state of the art in this field. Thereby we offer a single entry-point\ninto the subject reflecting the current state in a timely manner.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:30:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wiesmaier", "Alexander", ""], ["Alnahawi", "Nouri", ""], ["Grasmeyer", "Tobias", ""], ["Gei\u00dfler", "Julian", ""], ["Zeier", "Alexander", ""], ["Bauspie\u00df", "Pia", ""], ["Heinemann", "Andreas", ""]]}, {"id": "2106.09680", "submitter": "Harsha Nori", "authors": "Harsha Nori, Rich Caruana, Zhiqi Bu, Judy Hanwen Shen, Janardhan\n  Kulkarni", "title": "Accuracy, Interpretability, and Differential Privacy via Explainable\n  Boosting", "comments": "To be published in ICML 2021. 12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that adding differential privacy to Explainable Boosting Machines\n(EBMs), a recent method for training interpretable ML models, yields\nstate-of-the-art accuracy while protecting privacy. Our experiments on multiple\nclassification and regression datasets show that DP-EBM models suffer\nsurprisingly little accuracy loss even with strong differential privacy\nguarantees. In addition to high accuracy, two other benefits of applying DP to\nEBMs are: a) trained models provide exact global and local interpretability,\nwhich is often important in settings where differential privacy is needed; and\nb) the models can be edited after training without loss of privacy to correct\nerrors which DP noise may have introduced.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:33:00 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Nori", "Harsha", ""], ["Caruana", "Rich", ""], ["Bu", "Zhiqi", ""], ["Shen", "Judy Hanwen", ""], ["Kulkarni", "Janardhan", ""]]}, {"id": "2106.09779", "submitter": "Andrew Lowy", "authors": "Andrew Lowy and Meisam Razaviyayn", "title": "Locally Differentially Private Federated Learning: Efficient Algorithms\n  with Tight Risk Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning (FL) is a distributed learning paradigm in which many\nclients with heterogeneous, unbalanced, and often sensitive local data,\ncollaborate to learn a model. Local Differential Privacy (LDP) provides a\nstrong guarantee that each client's data cannot be leaked during and after\ntraining, without relying on a trusted third party. While LDP is often believed\nto be too stringent to allow for satisfactory utility, our paper challenges\nthis belief. We consider a general setup with unbalanced, heterogeneous data,\ndisparate privacy needs across clients, and unreliable communication, where a\nrandom number/subset of clients is available each round. We propose three LDP\nalgorithms for smooth (strongly) convex FL; each are noisy variations of\ndistributed minibatch SGD. One is accelerated and one involves novel\ntime-varying noise, which we use to obtain the first non-trivial LDP excess\nrisk bound for the fully general non-i.i.d. FL problem. Specializing to i.i.d.\nclients, our risk bounds interpolate between the best known and/or optimal\nbounds in the centralized setting and the cross-device setting, where each\nclient represents just one person's data. Furthermore, we show that in certain\nregimes, our convergence rate (nearly) matches the corresponding non-private\nlower bound or outperforms state of the art non-private algorithms (``privacy\nfor free''). Finally, we validate our theoretical results and illustrate the\npractical utility of our algorithm with numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 19:41:23 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lowy", "Andrew", ""], ["Razaviyayn", "Meisam", ""]]}, {"id": "2106.09802", "submitter": "Deborah Shands", "authors": "Deborah Shands and Carolyn Talcott", "title": "Intentional Forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many damaging cybersecurity attacks are enabled when an attacker can access\nresidual sensitive information (e.g. cryptographic keys, personal identifiers)\nleft behind from earlier computation. Attackers can sometimes use residual\ninformation to take control of a system, impersonate a user, or manipulate\ndata. Current approaches to addressing access to residual sensitive information\naim to patch individual software or hardware vulnerabilities. While such\npatching approaches are necessary to mitigate sometimes serious security\nvulnerabilities in the near term, they cannot address the underlying issue:\nexplicit requirements for adequately eliminating residual information and\nexplicit representations of the erasure capabilities of systems are necessary\nto ensure that sensitive information is handled as expected.\n  This position paper introduces the concept of intentional forgetting and the\ncapabilities that are needed to achieve it. Intentional forgetting enables\nsoftware and hardware system designers at every level of abstraction to clearly\nspecify and rigorously reason about the forgetting capabilities required of and\nprovided by a system. We identify related work that may help to illuminate\nchallenges or contribute to solutions and consider conceptual and engineering\ntradeoffs in implementations of forgetting capabilities. We discuss approaches\nto modeling intentional forgetting and then modeling the strength of a system's\nforgetting capability by its resistance to disclosing information to different\ntypes of detectors. Research is needed in a variety of domains to advance the\ntheory, specification techniques, system foundations, implementation tools, and\nmethodologies for effective, practical forgetting. We highlight research\nchallenges in several domains and encourage cross-disciplinary collaboration to\none day create a robust theory and practice of intentional forgetting.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:33:56 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Shands", "Deborah", ""], ["Talcott", "Carolyn", ""]]}, {"id": "2106.09805", "submitter": "Albert Cheu", "authors": "Albert Cheu and Matthew Joseph and Jieming Mao and Binghui Peng", "title": "Shuffle Private Stochastic Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In shuffle privacy, each user sends a collection of randomized messages to a\ntrusted shuffler, the shuffler randomly permutes these messages, and the\nresulting shuffled collection of messages must satisfy differential privacy.\nPrior work in this model has largely focused on protocols that use a single\nround of communication to compute algorithmic primitives like means,\nhistograms, and counts. In this work, we present interactive shuffle protocols\nfor stochastic convex optimization. Our optimization protocols rely on a new\nnoninteractive protocol for summing vectors of bounded $\\ell_2$ norm. By\ncombining this sum subroutine with techniques including mini-batch stochastic\ngradient descent, accelerated gradient descent, and Nesterov's smoothing\nmethod, we obtain loss guarantees for a variety of convex loss functions that\nsignificantly improve on those of the local model and sometimes match those of\nthe central model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:44:00 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Cheu", "Albert", ""], ["Joseph", "Matthew", ""], ["Mao", "Jieming", ""], ["Peng", "Binghui", ""]]}, {"id": "2106.09820", "submitter": "Ana-Maria Cretu", "authors": "Shubham Jain, Ana-Maria Cretu, Yves-Alexandre de Montjoye", "title": "Adversarial Detection Avoidance Attacks: Evaluating the robustness of\n  perceptual hashing-based client-side scanning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end encryption (E2EE) by messaging platforms enable people to securely\nand privately communicate with one another. Its widespread adoption however\nraised concerns that illegal content might now be shared undetected. Following\nthe global pushback against key escrow systems, client-side scanning based on\nperceptual hashing has been recently proposed by governments and researchers to\ndetect illegal content in E2EE communications. We here propose the first\nframework to evaluate the robustness of perceptual hashing-based client-side\nscanning to detection avoidance attacks and show current systems to not be\nrobust. More specifically, we propose three adversarial attacks -- a general\nblack-box attack and two white-box attacks for discrete cosine-based-based\nalgorithms -- against perceptual hashing algorithms. In a large-scale\nevaluation, we show perceptual hashing-based client-side scanning mechanisms to\nbe highly vulnerable to detection avoidance attacks in a black-box setting,\nwith more than 99.9% of images successfully attacked while preserving the\ncontent of the image. We furthermore show our attack to generate diverse\nperturbations, strongly suggesting that straightforward mitigation strategies\nwould be ineffective. Finally, we show that the larger thresholds necessary to\nmake the attack harder would probably require more than one billion images to\nbe flagged and decrypted daily, raising strong privacy concerns.Taken together,\nour results shed serious doubts on the robustness of perceptual hashing-based\nclient-side scanning mechanisms currently proposed by governments,\norganizations, and researchers around the world.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 21:16:56 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Jain", "Shubham", ""], ["Cretu", "Ana-Maria", ""], ["de Montjoye", "Yves-Alexandre", ""]]}, {"id": "2106.09841", "submitter": "Marcela Melara", "authors": "Marcela S. Melara, Mic Bowman", "title": "Enabling Security-Oriented Orchestration of Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As cloud providers push multi-tenancy to new levels to meet growing\nscalability demands, ensuring that externally developed untrusted microservices\nwill preserve tenant isolation has become a high priority. Developers, in turn,\nlack a means for expressing and automatically enforcing high-level application\nsecurity requirements at deployment time. In this paper, we observe that\norchestration systems are ideally situated between developers and the cloud\nprovider to address these issues. We propose a security policy framework that\nenables security-oriented orchestration of microservices by capturing and\nauditing code properties that are incorporated into microservice code\nthroughout the software supply chain. Orchestrators can leverage these\nproperties to deploy microservices on a node that matches both the developer's\nand cloud provider's security policy and their resource requirements. We\ndemonstrate our approach with a proof-of-concept based on the Private Data\nObjects [1] confidential smart contract framework, deploying code only after\nchecking its provenance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 22:39:58 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Melara", "Marcela S.", ""], ["Bowman", "Mic", ""]]}, {"id": "2106.09843", "submitter": "Marcela Melara", "authors": "Marcela S. Melara, Mic Bowman", "title": "Hardware-Enforced Integrity and Provenance for Distributed Code\n  Deployments", "comments": "2 pages, NIST Workshop on Enhancing Software Supply Chain Security\n  (2021), see\n  https://www.nist.gov/itl/executive-order-improving-nations-cybersecurity/enhancing-software-supply-chain-security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deployed microservices must adhere to a multitude of application-level\nsecurity requirements and regulatory constraints imposed by mutually\ndistrusting application principals--software developers, cloud providers, and\neven data owners. Although these principals wish to enforce their individual\nsecurity requirements, they do not currently have a common way of easily\nidentifying, expressing and automatically enforcing these requirements at\ndeployment time. CDI (Code Deployment Integrity) is a security policy framework\nthat enables distributed application principals to establish trust in deployed\ncode through high-integrity provenance information. We observe that principals\nexpect the software supply chain to preserve certain code security properties\nthroughout the creation of an executable bundle, even if the code is\ntransformed or inspected through various tools (e.g., compilation inserts stack\ncanaries for memory safety). Our key insight in designing CDI is that even if\napplication principals do not trust each other directly, they can trust a\nmicroservice bundle to meet their security policies if they can trust the tools\ninvolved in creating the bundle.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 23:07:58 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Melara", "Marcela S.", ""], ["Bowman", "Mic", ""]]}, {"id": "2106.09898", "submitter": "Nicholas Boucher", "authors": "Nicholas Boucher, Ilia Shumailov, Ross Anderson, Nicolas Papernot", "title": "Bad Characters: Imperceptible NLP Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Several years of research have shown that machine-learning systems are\nvulnerable to adversarial examples, both in theory and in practice. Until now,\nsuch attacks have primarily targeted visual models, exploiting the gap between\nhuman and machine perception. Although text-based models have also been\nattacked with adversarial examples, such attacks struggled to preserve semantic\nmeaning and indistinguishability. In this paper, we explore a large class of\nadversarial examples that can be used to attack text-based models in a\nblack-box setting without making any human-perceptible visual modification to\ninputs. We use encoding-specific perturbations that are imperceptible to the\nhuman eye to manipulate the outputs of a wide range of Natural Language\nProcessing (NLP) systems from neural machine-translation pipelines to web\nsearch engines. We find that with a single imperceptible encoding injection --\nrepresenting one invisible character, homoglyph, reordering, or deletion -- an\nattacker can significantly reduce the performance of vulnerable models, and\nwith three injections most models can be functionally broken. Our attacks work\nagainst currently-deployed commercial systems, including those produced by\nMicrosoft and Google, in addition to open source models published by Facebook\nand IBM. This novel series of attacks presents a significant threat to many\nlanguage processing systems: an attacker can affect systems in a targeted\nmanner without any assumptions about the underlying model. We conclude that\ntext-based NLP systems require careful input sanitization, just like\nconventional applications, and that given such systems are now being deployed\nrapidly at scale, the urgent attention of architects and operators is required.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:42:56 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Boucher", "Nicholas", ""], ["Shumailov", "Ilia", ""], ["Anderson", "Ross", ""], ["Papernot", "Nicolas", ""]]}, {"id": "2106.09904", "submitter": "Hassan Jameel Asghar", "authors": "Tham Nguyen, Hassan Jameel Asghar, Raghav Bhakar, Dali Kaafar, Farhad\n  Farokhi", "title": "Sharing in a Trustless World: Privacy-Preserving Data Analytics with\n  Potentially Cheating Participants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lack of trust between organisations and privacy concerns about their data are\nimpediments to an otherwise potentially symbiotic joint data analysis. We\npropose DataRing, a data sharing system that allows mutually mistrusting\nparticipants to query each others' datasets in a privacy-preserving manner\nwhile ensuring the correctness of input datasets and query answers even in the\npresence of (cheating) participants deviating from their true datasets. By\nrelying on the assumption that if only a small subset of rows of the true\ndataset are known, participants cannot submit answers to queries deviating\nsignificantly from their true datasets. We employ differential privacy and a\nsuite of cryptographic tools to ensure individual privacy for each\nparticipant's dataset and data confidentiality from the system. Our results\nshow that the evaluation of 10 queries on a dataset with 10 attributes and\n500,000 records is achieved in 90.63 seconds. DataRing could detect cheating\nparticipant that deviates from its true dataset in few queries with high\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 04:00:42 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Nguyen", "Tham", ""], ["Asghar", "Hassan Jameel", ""], ["Bhakar", "Raghav", ""], ["Kaafar", "Dali", ""], ["Farokhi", "Farhad", ""]]}, {"id": "2106.09947", "submitter": "Maura Pintor", "authors": "Maura Pintor, Luca Demetrio, Angelo Sotgiu, Giovanni Manca, Ambra\n  Demontis, Nicholas Carlini, Battista Biggio, Fabio Roli", "title": "Indicators of Attack Failure: Debugging and Improving Optimization of\n  Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating robustness of machine-learning models to adversarial examples is a\nchallenging problem. Many defenses have been shown to provide a false sense of\nsecurity by causing gradient-based attacks to fail, and they have been broken\nunder more rigorous evaluations. Although guidelines and best practices have\nbeen suggested to improve current adversarial robustness evaluations, the lack\nof automatic testing and debugging tools makes it difficult to apply these\nrecommendations in a systematic manner. In this work, we overcome these\nlimitations by (i) defining a set of quantitative indicators which unveil\ncommon failures in the optimization of gradient-based attacks, and (ii)\nproposing specific mitigation strategies within a systematic evaluation\nprotocol. Our extensive experimental analysis shows that the proposed\nindicators of failure can be used to visualize, debug and improve current\nadversarial robustness evaluations, providing a first concrete step towards\nautomatizing and systematizing current adversarial robustness evaluations. Our\nopen-source code is available at:\nhttps://github.com/pralab/IndicatorsOfAttackFailure.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:57:58 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Pintor", "Maura", ""], ["Demetrio", "Luca", ""], ["Sotgiu", "Angelo", ""], ["Manca", "Giovanni", ""], ["Demontis", "Ambra", ""], ["Carlini", "Nicholas", ""], ["Biggio", "Battista", ""], ["Roli", "Fabio", ""]]}, {"id": "2106.09966", "submitter": "Mriganka Chakravarty Mr.", "authors": "Mriganka Shekhar Chakravarty and Biswabandan Panda", "title": "Introducing Fast and Secure Deterministic Stash Free Write Only\n  Oblivious RAMs for Demand Paging in Keystone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keystone is a trusted execution environment, based on RISC-V architecture. It\ndivides the memory into a secure Keystone private memory and an unsecure\nnon-Keystone memory, and allows code that lies inside the Keystone private\nmemory to execute securely. Simple demand paging in Keystone ends up leaking\nsensitive access patterns of Keystone application to the Operating System(OS),\nthat is assumed to be malicious. This is because, to access the unsecure\nnon-Keystone memory, Keystone needs support of the OS. To mitigate this,\nKeystone needs to implement oblivious demand paging while obfuscating its page\naccess patterns by using Oblivious RAM(ORAM) techniques. This causes\nsubstantial slowdown in the application execution.\n  In this paper, we bridge the performance gap between application execution\ntime with unsecure and secure demand paging in Keystone by using Deterministic,\nstash free, Write only ORAM (DetWoORAM) for oblivious demand paging. We also\nshow why DetWoORAM, that is a write-only ORAM, is sufficient for oblivious\ndemand paging. DetWoORAM logically partitions the memory into a main area and a\nholding area. The actual pages are stored in main area. We propose two\nenhancements over DetWoORAM that improves the application execution slowdown.\nThe first enhancement, which we call the Eager DetWoORAM, involves page\npreloading that exploits the deterministic access pattern of DetWoORAM, and\ntries to hide the ORAM latency. The second enhancement, which we call the\nParallel DetWoORAM, involves spawning multiple threads and each thread performs\na part of the DetWoORAM memory access algorithm. Compared to DetWoORAM that\nshows slowdown of [1.4x, 2x, and 3.24x], Eager DetWoORAM and Parallel DetWoORAM\nprovide slowdown of [1.2x, 1.8x, and 3.2x] and [1.1x, 1.1x, and 1.4x], for k=\n3, 7, and 15, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 07:42:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chakravarty", "Mriganka Shekhar", ""], ["Panda", "Biswabandan", ""]]}, {"id": "2106.09967", "submitter": "Benjamin Smith", "authors": "Jes\\'us-Javier Chi-Dom\\'inguez (CINVESTAV-IPN), Francisco\n  Rodr\\'iguez-Henr\\'iquez (CINVESTAV-IPN), Benjamin Smith (GRACE, LIX)", "title": "Extending the GLS endomorphism to speed up GHS Weil descent using Magma", "comments": "Finite Fields and Their Applications, Elsevier, In press, 75", "journal-ref": null, "doi": "10.1016/j.ffa.2021.101891", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $q = 2^n$, and let $E / \\mathbb{F}_{q^{\\ell}}$ be a generalized\nGalbraith--Lin--Scott (GLS) binary curve, with $\\ell \\ge 2$ and $(\\ell, n) =\n1$.We show that the GLS endomorphism on $E / \\mathbb{F}_{q^{\\ell}}$ induces an\nefficient endomorphism on the Jacobian $J_H(\\mathbb{F}_q)$ of the genus-$g$\nhyperelliptic curve $H$ corresponding to the image of the GHS Weil-descent\nattack applied to $E/\\mathbb{F}_{q^\\ell}$, and that this endomorphism yields a\nfactor-$n$ speedup when using standard index-calculus procedures for solving\nthe Discrete Logarithm Problem (DLP) on $J_H(\\mathbb{F}_q)$. Our analysis is\nbacked up by the explicit computation of a discrete logarithm defined on a\nprime-order subgroup of a GLS elliptic curve over the field\n$\\mathbb{F}_{2^{5\\cdot 31}}$. A Magma implementation of our algorithm finds the\naforementioned discrete logarithm in about $1,035$ CPU-days.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 07:44:26 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chi-Dom\u00ednguez", "Jes\u00fas-Javier", "", "CINVESTAV-IPN"], ["Rodr\u00edguez-Henr\u00edquez", "Francisco", "", "CINVESTAV-IPN"], ["Smith", "Benjamin", "", "GRACE, LIX"]]}, {"id": "2106.09986", "submitter": "Dimitra Papatsaroucha", "authors": "Dimitra Papatsaroucha, Yannis Nikoloudakis, Ioannis Kefaloukos,\n  Evangelos Pallis, and Evangelos K. Markakis (Department of Electrical and\n  Computer Engineering, Hellenic Mediterranean University)", "title": "A Survey on Human and Personality Vulnerability Assessment in\n  Cyber-security: Challenges, Approaches, and Open Issues", "comments": "39 pages, 4 figures, Submitted for Review to Association for\n  Computing Machinery (ACM) - Computing Surveys, fixed author's name\n  misspelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  These days, cyber-criminals target humans rather than machines since they try\nto accomplish their malicious intentions by exploiting the weaknesses of end\nusers. Thus, human vulnerabilities pose a serious threat to the security and\nintegrity of computer systems and data. The human tendency to trust and help\nothers, as well as personal, social, and cultural characteristics, are\nindicative of the level of susceptibility that one may exhibit towards certain\nattack types and deception strategies. This work aims to investigate the\nfactors that affect human susceptibility by studying the existing literature\nrelated to this subject. The objective is also to explore and describe state of\nthe art human vulnerability assessment models, current prevention, and\nmitigation approaches regarding user susceptibility, as well as educational and\nawareness raising training strategies. Following the review of the literature,\nseveral conclusions are reached. Among them, Human Vulnerability Assessment has\nbeen included in various frameworks aiming to assess the cyber security\ncapacity of organizations, but it concerns a one time assessment rather than a\ncontinuous practice. Moreover, human maliciousness is still neglected from\ncurrent Human Vulnerability Assessment frameworks; thus, insider threat actors\nevade identification, which may lead to an increased cyber security risk.\nFinally, this work proposes a user susceptibility profile according to the\nfactors stemming from our research.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:17:38 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 13:07:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Papatsaroucha", "Dimitra", "", "Department of Electrical and\n  Computer Engineering, Hellenic Mediterranean University"], ["Nikoloudakis", "Yannis", "", "Department of Electrical and\n  Computer Engineering, Hellenic Mediterranean University"], ["Kefaloukos", "Ioannis", "", "Department of Electrical and\n  Computer Engineering, Hellenic Mediterranean University"], ["Pallis", "Evangelos", "", "Department of Electrical and\n  Computer Engineering, Hellenic Mediterranean University"], ["Markakis", "Evangelos K.", "", "Department of Electrical and\n  Computer Engineering, Hellenic Mediterranean University"]]}, {"id": "2106.09989", "submitter": "Yulin Zhu", "authors": "Yulin Zhu, Yuni Lai, Kaifa Zhao, Xiapu Luo, Mingquan Yuan, Jian Ren,\n  Kai Zhou", "title": "BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful\nrepresentation abilities of graphs as well as recent advances in graph mining\ntechniques. These GAD tools, however, expose a new attacking surface,\nironically due to their unique advantage of being able to exploit the relations\namong data. That is, attackers now can manipulate those relations (i.e., the\nstructure of the graph) to allow some target nodes to evade detection. In this\npaper, we exploit this vulnerability by designing a new type of targeted\nstructural poisoning attacks to a representative regression-based GAD system\ntermed OddBall. Specially, we formulate the attack against OddBall as a\nbi-level optimization problem, where the key technical challenge is to\nefficiently solve the problem in a discrete domain. We propose a novel attack\nmethod termed BinarizedAttack based on gradient descent. Comparing to prior\narts, BinarizedAttack can better use the gradient information, making it\nparticularly suitable for solving combinatorial optimization problems.\nFurthermore, we investigate the attack transferability of BinarizedAttack by\nemploying it to attack other representation-learning-based GAD systems. Our\ncomprehensive experiments demonstrate that BinarizedAttack is very effective in\nenabling target nodes to evade graph-based anomaly detection tools with limited\nattackers' budget, and in the black-box transfer attack setting,\nBinarizedAttack is also tested effective and in particular, can significantly\nchange the node embeddings learned by the GAD systems. Our research thus opens\nthe door to studying a new type of attack against security analytic tools that\nrely on graph data.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:20:23 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:14:07 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 15:20:08 GMT"}, {"version": "v4", "created": "Sat, 3 Jul 2021 12:18:01 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhu", "Yulin", ""], ["Lai", "Yuni", ""], ["Zhao", "Kaifa", ""], ["Luo", "Xiapu", ""], ["Yuan", "Mingquan", ""], ["Ren", "Jian", ""], ["Zhou", "Kai", ""]]}, {"id": "2106.09993", "submitter": "Tianyu Pang", "authors": "Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu", "title": "Accumulative Poisoning Attacks on Real-time Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting training data from untrusted sources exposes machine learning\nservices to poisoning adversaries, who maliciously manipulate training data to\ndegrade the model accuracy. When trained on offline datasets, poisoning\nadversaries have to inject the poisoned data in advance before training, and\nthe order of feeding these poisoned batches into the model is stochastic. In\ncontrast, practical systems are more usually trained/fine-tuned on sequentially\ncaptured real-time data, in which case poisoning adversaries could dynamically\npoison each data batch according to the current model state. In this paper, we\nfocus on the real-time settings and propose a new attacking strategy, which\naffiliates an accumulative phase with poisoning attacks to secretly (i.e.,\nwithout affecting accuracy) magnify the destructive effect of a (poisoned)\ntrigger batch. By mimicking online learning and federated learning on CIFAR-10,\nwe show that the model accuracy will significantly drop by a single update step\non the trigger batch after the accumulative phase. Our work validates that a\nwell-designed but straightforward attacking strategy can dramatically amplify\nthe poisoning effects, with no need to explore complex techniques.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:29:53 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Pang", "Tianyu", ""], ["Yang", "Xiao", ""], ["Dong", "Yinpeng", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2106.10035", "submitter": "Saad Sajid Hashmi", "authors": "Saad Sajid Hashmi and Nazar Waheed and Gioacchino Tangari and Muhammad\n  Ikram and Stephen Smith", "title": "Longitudinal Compliance Analysis of Android Applications with Privacy\n  Policies", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contemporary mobile applications (apps) are designed to track, use, and share\nusers' data, often without their consent, which results in potential privacy\nand transparency issues. To investigate whether mobile apps have always been\n(non-)transparent regarding how they collect information about users, we\nperform a longitudinal analysis of the historical versions of 268 Android apps.\nThese apps comprise 5,240 app releases or versions between 2008 and 2016. We\ndetect inconsistencies between apps' behaviors and the stated use of data\ncollection in privacy policies to reveal compliance issues. We utilize machine\nlearning techniques for the classification of the privacy policy text to\nidentify the purported practices that collect and/or share users' personal\ninformation, such as phone numbers and email addresses. We then uncover the\ndata leaks of an app through static and dynamic analysis. Over time, our\nresults show a steady increase in the number of apps' data collection practices\nthat are undisclosed in the privacy policies. This behavior is particularly\ntroubling since privacy policy is the primary tool for describing the app's\nprivacy protection practices. We find that newer versions of the apps are\nlikely to be more non-compliant than their preceding versions. The\ndiscrepancies between the purported and the actual data practices show that\nprivacy policies are often incoherent with the apps' behaviors, thus defying\nthe 'notice and choice' principle when users install apps.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:17:41 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 08:48:05 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Hashmi", "Saad Sajid", ""], ["Waheed", "Nazar", ""], ["Tangari", "Gioacchino", ""], ["Ikram", "Muhammad", ""], ["Smith", "Stephen", ""]]}, {"id": "2106.10068", "submitter": "Christian Janos Lebeda", "authors": "Martin Aum\\\"uller, Christian Janos Lebeda, Rasmus Pagh", "title": "Differentially private sparse vectors with low error, optimal space, and\n  fast access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a sparse histogram, or more generally a sparse vector, is a\nfundamental task in differential privacy. An ideal solution would use space\nclose to information-theoretical lower bounds, have an error distribution that\ndepends optimally on the desired privacy level, and allow fast random access to\nentries in the vector. However, existing approaches have only achieved two of\nthese three goals.\n  In this paper we introduce the Approximate Laplace Projection (ALP) mechanism\nfor approximating k-sparse vectors. This mechanism is shown to simultaneously\nhave information-theoretically optimal space (up to constant factors), fast\naccess to vector entries, and error of the same magnitude as the\nLaplace-mechanism applied to dense vectors. A key new technique is a unary\nrepresentation of small integers, which is shown to be robust against\n``randomized response'' noise. This representation is combined with hashing, in\nthe spirit of Bloom filters, to obtain a space-efficient, differentially\nprivate representation. Our theoretical performance bounds are complemented by\nsimulations which show that the constant factors on the main performance\nparameters are quite small, suggesting practicality of the technique.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:29:25 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Lebeda", "Christian Janos", ""], ["Pagh", "Rasmus", ""]]}, {"id": "2106.10083", "submitter": "Befekadu Gebraselase", "authors": "Befekadu G. Gebraselase, Bjarne E. Helvik, Yuming Jiang", "title": "An Analysis of Transaction Handling in Bitcoin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bitcoin has become the leading cryptocurrency system, but the limit on its\ntransaction processing capacity has resulted in increased transaction fees and\ndelayed transaction confirmation. As such, it is pertinent to understand and\nprobably predict how transactions are handled by Bitcoin such that a user may\nadapt the transaction requests and a miner may adjust the block generation\nstrategy and/or the mining pool to join. To this aim, the present paper\nintroduces results from an analysis of transaction handling in Bitcoin.\n  Specifically, the analysis consists of two-part. The first part is an\nexploratory data analysis revealing key characteristics in Bitcoin transaction\nhandling. The second part is a predictability analysis intended to provide\ninsights on transaction handling such as (i) transaction confirmation time,\n(ii) block attributes, and (iii) who has created the block. The result shows\nthat some models do reasonably well for (ii), but surprisingly not for (i) or\n(iii).\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 12:09:55 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Gebraselase", "Befekadu G.", ""], ["Helvik", "Bjarne E.", ""], ["Jiang", "Yuming", ""]]}, {"id": "2106.10147", "submitter": "Suyoung Lee", "authors": "Suyoung Lee, Wonho Song, Suman Jana, Meeyoung Cha, Sooel Son", "title": "Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trigger set-based watermarking schemes have gained emerging attention as they\nprovide a means to prove ownership for deep neural network model owners. In\nthis paper, we argue that state-of-the-art trigger set-based watermarking\nalgorithms do not achieve their designed goal of proving ownership. We posit\nthat this impaired capability stems from two common experimental flaws that the\nexisting research practice has committed when evaluating the robustness of\nwatermarking algorithms: (1) incomplete adversarial evaluation and (2)\noverlooked adaptive attacks.\n  We conduct a comprehensive adversarial evaluation of 10 representative\nwatermarking schemes against six of the existing attacks and demonstrate that\neach of these watermarking schemes lacks robustness against at least two\nattacks. We also propose novel adaptive attacks that harness the adversary's\nknowledge of the underlying watermarking algorithm of a target model. We\ndemonstrate that the proposed attacks effectively break all of the 10\nwatermarking schemes, consequently allowing adversaries to obscure the\nownership of any watermarked model. We encourage follow-up studies to consider\nour guidelines when evaluating the robustness of their watermarking schemes via\nconducting comprehensive adversarial evaluation that include our adaptive\nattacks to demonstrate a meaningful upper bound of watermark robustness.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:23:55 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lee", "Suyoung", ""], ["Song", "Wonho", ""], ["Jana", "Suman", ""], ["Cha", "Meeyoung", ""], ["Son", "Sooel", ""]]}, {"id": "2106.10151", "submitter": "Odelia Melamed", "authors": "Adi Shamir, Odelia Melamed, Oriel BenShmuel", "title": "The Dimpled Manifold Model of Adversarial Examples in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extreme fragility of deep neural networks when presented with tiny\nperturbations in their inputs was independently discovered by several research\ngroups in 2013, but in spite of enormous effort these adversarial examples\nremained a baffling phenomenon with no clear explanation. In this paper we\nintroduce a new conceptual framework (which we call the Dimpled Manifold Model)\nwhich provides a simple explanation for why adversarial examples exist, why\ntheir perturbations have such tiny norms, why these perturbations look like\nrandom noise, and why a network which was adversarially trained with\nincorrectly labeled images can still correctly classify test images. In the\nlast part of the paper we describe the results of numerous experiments which\nstrongly support this new model, and in particular our assertion that\nadversarial perturbations are roughly perpendicular to the low dimensional\nmanifold which contains all the training examples.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:32:55 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Shamir", "Adi", ""], ["Melamed", "Odelia", ""], ["BenShmuel", "Oriel", ""]]}, {"id": "2106.10154", "submitter": "Paula Delgado-Santos", "authors": "Paula Delgado-Santos, Giuseppe Stragapede, Ruben Tolosana, Richard\n  Guest, Farzin Deravi and Ruben Vera-Rodriguez", "title": "A Survey of Privacy Vulnerabilities of Mobile Device Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The number of mobile devices, such as smartphones and smartwatches, is\nrelentlessly increasing to almost 6.8 billion by 2022, and along with it, the\namount of personal and sensitive data captured by them. This survey overviews\nthe state of the art of what personal and sensitive user attributes can be\nextracted from mobile device sensors, emphasising critical aspects such as\ndemographics, health and body features, activity and behaviour recognition,\netc. In addition, we review popular metrics in the literature to quantify the\ndegree of privacy, and discuss powerful privacy methods to protect the\nsensitive data while preserving data utility for analysis. Finally, open\nresearch questions a represented for further advancements in the field.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:42:09 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Delgado-Santos", "Paula", ""], ["Stragapede", "Giuseppe", ""], ["Tolosana", "Ruben", ""], ["Guest", "Richard", ""], ["Deravi", "Farzin", ""], ["Vera-Rodriguez", "Ruben", ""]]}, {"id": "2106.10333", "submitter": "Audra McMillan", "authors": "Joerg Drechsler, Ira Globus-Harris, Audra McMillan, Jayshree Sarathy,\n  and Adam Smith", "title": "Non-parametric Differentially Private Confidence Intervals for the\n  Median", "comments": "44 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a restriction on data processing algorithms that\nprovides strong confidentiality guarantees for individual records in the data.\nHowever, research on proper statistical inference, that is, research on\nproperly quantifying the uncertainty of the (noisy) sample estimate regarding\nthe true value in the population, is currently still limited. This paper\nproposes and evaluates several strategies to compute valid differentially\nprivate confidence intervals for the median. Instead of computing a\ndifferentially private point estimate and deriving its uncertainty, we directly\nestimate the interval bounds and discuss why this approach is superior if\nensuring privacy is important. We also illustrate that addressing both sources\nof uncertainty--the error from sampling and the error from protecting the\noutput--simultaneously should be preferred over simpler approaches that\nincorporate the uncertainty in a sequential fashion. We evaluate the\nperformance of the different algorithms under various parameter settings in\nextensive simulation studies and demonstrate how the findings could be applied\nin practical settings using data from the 1940 Decennial Census.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:45:37 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 18:05:54 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Drechsler", "Joerg", ""], ["Globus-Harris", "Ira", ""], ["McMillan", "Audra", ""], ["Sarathy", "Jayshree", ""], ["Smith", "Adam", ""]]}, {"id": "2106.10336", "submitter": "Lukas Daubner", "authors": "Lukas Daubner, Raimundas Matulevi\\v{c}ius", "title": "Risk-Oriented Design Approach For Forensic-Ready Software Systems", "comments": "The 16th International Conference on Availability, Reliability and\n  Security (ARES 2021), August 17--20, 2021, Vienna, Austria", "journal-ref": null, "doi": "10.1145/3465481.3470052", "report-no": null, "categories": "cs.CR cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital forensic investigation is a complex and time-consuming activity in\nresponse to a cybersecurity incident or cybercrime to answer questions related\nto it. These typically are what happened, when, where, how, and who is\nresponsible. However, answering them is often very laborious and sometimes\noutright impossible due to a lack of useable data. The forensic-ready software\nsystems are designed to produce valuable on-point data for use in the\ninvestigation with potentially high evidence value. Still, the particular ways\nto develop these systems are currently not explored.\n  This paper proposes consideration of forensic readiness within security risk\nmanagement to refine specific requirements on forensic-ready software systems.\nThe idea is to re-evaluate the taken security risk decisions with the aim to\nprovide trustable data when the security measures fail. Additionally, it also\nconsiders possible disputes, which the digital evidence can solve. Our proposed\napproach, risk-oriented forensic-ready design, composes of two parts: (1)\nprocess guiding the identification of the requirements in the form of potential\nevidence sources, and (2) supporting BPMN notation capturing the potential\nevidence sources and their relationship. Together they are aimed to provide a\nhigh-level overview of the forensic-ready requirements within the system.\nFinally, the approach is demonstrated on an automated valet parking scenario,\nfollowed by a discussion regarding its impact and usefulness within the\nforensic readiness effort.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:51:05 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Daubner", "Lukas", ""], ["Matulevi\u010dius", "Raimundas", ""]]}, {"id": "2106.10339", "submitter": "Fang Liu", "authors": "Dong Wang, Fang Liu", "title": "Privacy-preserving Publication and Sharing of COVID-19 Pandemic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A huge amount of data of various types are collected during the COVID-19\npandemic, the analysis and interpretation of which has been indispensable for\ncurbing the spread of the coronavirus. As the pandemic slows down, the\ncollected data during the pandemic will continue to be rich sources for further\nstudying the pandemic and understanding its impacts on public health,\neconomics, and societies. On the other hand, na\\\"{i}ve release and sharing of\nthe information can be associated with serious privacy concerns. In this paper,\naiming at shedding light on privacy-preserving sharing of pandemic data and\nthus promoting and encouraging more data sharing for research and public use,\nwe examine three common data types -- case surveillance, patient location\nhistories and hot spot maps, and contact tracing networks -- collected during\nthe pandemic and develop and apply privacy-preserving approaches for publishing\nor sharing each data type. We illustrate the applications and examine the\nutility of released privacy-preserving data in examples and experiments at\nvarious levels of privacy guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 20:07:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Dong", ""], ["Liu", "Fang", ""]]}, {"id": "2106.10362", "submitter": "Zhuolun Xiang", "authors": "Rati Gelashvili, Lefteris Kokoris-Kogias, Alberto Sonnino, Alexander\n  Spiegelman, Zhuolun Xiang", "title": "Jolteon and Ditto: Network-Adaptive Efficient Consensus with\n  Asynchronous Fallback", "comments": "arXiv admin note: text overlap with arXiv:2103.03181", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing committee-based Byzantine state machine replication (SMR) protocols,\ntypically deployed in production blockchains, face a clear trade-off: (1) they\neither achieve linear communication cost in the happy path, but sacrifice\nliveness during periods of asynchrony, or (2) they are robust (progress with\nprobability one) but pay quadratic communication cost. We believe this\ntrade-off is unwarranted since existing linear protocols still have asymptotic\nquadratic cost in the worst case. We design Ditto, a Byzantine SMR protocol\nthat enjoys the best of both worlds: optimal communication on and off the happy\npath (linear and quadratic, respectively) and progress guarantee under\nasynchrony and DDoS attacks. We achieve this by replacing the\nview-synchronization of partially synchronous protocols with an asynchronous\nfallback mechanism at no extra asymptotic cost. Specifically, we start from\nHotStuff, a state-of-the-art linear protocol, and gradually build Ditto. As a\nseparate contribution and an intermediate step, we design a 2-chain version of\nHotStuff, Jolteon, which leverages a quadratic view-change mechanism to reduce\nthe latency of the standard 3-chain HotStuff. We implement and experimentally\nevaluate all our systems. Notably, Jolteon's commit latency outperforms\nHotStuff by 200-300ms with varying system size. Additionally, Ditto adapts to\nthe network and provides better performance than Jolteon under faulty\nconditions and better performance than VABA (a state-of-the-art asynchronous\nprotocol) under faultless conditions. This proves our case that breaking the\nrobustness-efficiency trade-off is in the realm of practicality.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 21:34:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gelashvili", "Rati", ""], ["Kokoris-Kogias", "Lefteris", ""], ["Sonnino", "Alberto", ""], ["Spiegelman", "Alexander", ""], ["Xiang", "Zhuolun", ""]]}, {"id": "2106.10460", "submitter": "Paul H\\\"oller", "authors": "Paul H\\\"oller, Alexander Krumeich and Luigi Lo Iacono", "title": "XML Signature Wrapping Still Considered Harmful: A Case Study on the\n  Personal Health Record in Germany", "comments": "Accepted for IFIP SEC 2021", "journal-ref": "ICT Systems Security and Privacy Protection, 2021, p. 3-18", "doi": "10.1007/978-3-030-78120-01", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML Signature Wrapping (XSW) has been a relevant threat to web services for\n15 years until today. Using the Personal Health Record (PHR), which is\ncurrently under development in Germany, we investigate a current SOAP-based web\nservices system as a case study. In doing so, we highlight several deficiencies\nin defending against XSW. Using this real-world contemporary example as\nmotivation, we introduce a guideline for more secure XML signature processing\nthat provides practitioners with easier access to the effective countermeasures\nidentified in the current state of research.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 09:36:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["H\u00f6ller", "Paul", ""], ["Krumeich", "Alexander", ""], ["Iacono", "Luigi Lo", ""]]}, {"id": "2106.10478", "submitter": "Yi Li", "authors": "Yi Li, Shaohua Wang, Tien N. Nguyen", "title": "Vulnerability Detection with Fine-grained Interpretations", "comments": null, "journal-ref": null, "doi": "10.1145/3468264.3468597", "report-no": null, "categories": "cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the successes of machine learning (ML) and deep learning (DL) based\nvulnerability detectors (VD), they are limited to providing only the decision\non whether a given code is vulnerable or not, without details on what part of\nthe code is relevant to the detected vulnerability. We present IVDetect an\ninterpretable vulnerability detector with the philosophy of using Artificial\nIntelligence (AI) to detect vulnerabilities, while using Intelligence Assistant\n(IA) via providing VD interpretations in terms of vulnerable statements.\n  For vulnerability detection, we separately consider the vulnerable statements\nand their surrounding contexts via data and control dependencies. This allows\nour model better discriminate vulnerable statements than using the mixture of\nvulnerable code and~contextual code as in existing approaches. In addition to\nthe coarse-grained vulnerability detection result, we leverage interpretable AI\nto provide users with fine-grained interpretations that include the sub-graph\nin the Program Dependency Graph (PDG) with the crucial statements that are\nrelevant to the detected vulnerability. Our empirical evaluation on\nvulnerability databases shows that IVDetect outperforms the existing DL-based\napproaches by 43%--84% and 105%--255% in top-10 nDCG and MAP ranking scores.\nIVDetect correctly points out the vulnerable statements relevant to the\nvulnerability via its interpretation~in 67% of the cases with a top-5 ranked\nlist. It improves over baseline interpretation models by 12.3%--400% and\n9%--400% in accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 11:53:49 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Yi", ""], ["Wang", "Shaohua", ""], ["Nguyen", "Tien N.", ""]]}, {"id": "2106.10606", "submitter": "Naveed Akhtar Dr.", "authors": "Naveed Akhtar, Muhammad A. A. K. Jalwana, Mohammed Bennamoun, Ajmal\n  Mian", "title": "Attack to Fool and Explain Deep Networks", "comments": "To appear in IEEE TPAMI. arXiv admin note: text overlap with\n  arXiv:1905.11544", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep visual models are susceptible to adversarial perturbations to inputs.\nAlthough these signals are carefully crafted, they still appear noise-like\npatterns to humans. This observation has led to the argument that deep visual\nrepresentation is misaligned with human perception. We counter-argue by\nproviding evidence of human-meaningful patterns in adversarial perturbations.\nWe first propose an attack that fools a network to confuse a whole category of\nobjects (source class) with a target label. Our attack also limits the\nunintended fooling by samples from non-sources classes, thereby circumscribing\nhuman-defined semantic notions for network fooling. We show that the proposed\nattack not only leads to the emergence of regular geometric patterns in the\nperturbations, but also reveals insightful information about the decision\nboundaries of deep models. Exploring this phenomenon further, we alter the\n`adversarial' objective of our attack to use it as a tool to `explain' deep\nvisual representation. We show that by careful channeling and projection of the\nperturbations computed by our method, we can visualize a model's understanding\nof human-defined semantic notions. Finally, we exploit the explanability\nproperties of our perturbations to perform image generation, inpainting and\ninteractive image manipulation by attacking adversarialy robust\n`classifiers'.In all, our major contribution is a novel pragmatic adversarial\nattack that is subsequently transformed into a tool to interpret the visual\nmodels. The article also makes secondary contributions in terms of establishing\nthe utility of our attack beyond the adversarial objective with multiple\ninteresting applications.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 03:07:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Akhtar", "Naveed", ""], ["Jalwana", "Muhammad A. A. K.", ""], ["Bennamoun", "Mohammed", ""], ["Mian", "Ajmal", ""]]}, {"id": "2106.10662", "submitter": "Qingchen Liu", "authors": "Nhan Khanh Le and Yang Liu and Quang Minh Nguyen and Qingchen Liu and\n  Fangzhou Liu and Quanwei Cai and Sandra Hirche", "title": "FedXGBoost: Privacy-Preserving XGBoost for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is the distributed machine learning framework that enables\ncollaborative training across multiple parties while ensuring data privacy.\nPractical adaptation of XGBoost, the state-of-the-art tree boosting framework,\nto federated learning remains limited due to high cost incurred by conventional\nprivacy-preserving methods. To address the problem, we propose two variants of\nfederated XGBoost with privacy guarantee: FedXGBoost-SMM and FedXGBoost-LDP.\nOur first protocol FedXGBoost-SMM deploys enhanced secure matrix multiplication\nmethod to preserve privacy with lossless accuracy and lower overhead than\nencryption-based techniques. Developed independently, the second protocol\nFedXGBoost-LDP is heuristically designed with noise perturbation for local\ndifferential privacy, and empirically evaluated on real-world and synthetic\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 09:17:45 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 14:50:42 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Le", "Nhan Khanh", ""], ["Liu", "Yang", ""], ["Nguyen", "Quang Minh", ""], ["Liu", "Qingchen", ""], ["Liu", "Fangzhou", ""], ["Cai", "Quanwei", ""], ["Hirche", "Sandra", ""]]}, {"id": "2106.10671", "submitter": "Thee Chanyaswad", "authors": "Thee Chanyaswad, J. Morris Chang, S.Y. Kung", "title": "A compressive multi-kernel method for privacy-preserving machine\n  learning", "comments": "Published in 2017 International Joint Conference on Neural Networks\n  (IJCNN). IEEE, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the analytic tools become more powerful, and more data are generated on a\ndaily basis, the issue of data privacy arises. This leads to the study of the\ndesign of privacy-preserving machine learning algorithms. Given two objectives,\nnamely, utility maximization and privacy-loss minimization, this work is based\non two previously non-intersecting regimes -- Compressive Privacy and\nmulti-kernel method. Compressive Privacy is a privacy framework that employs\nutility-preserving lossy-encoding scheme to protect the privacy of the data,\nwhile multi-kernel method is a kernel based machine learning regime that\nexplores the idea of using multiple kernels for building better predictors. The\ncompressive multi-kernel method proposed consists of two stages -- the\ncompression stage and the multi-kernel stage. The compression stage follows the\nCompressive Privacy paradigm to provide the desired privacy protection. Each\nkernel matrix is compressed with a lossy projection matrix derived from the\nDiscriminant Component Analysis (DCA). The multi-kernel stage uses the\nsignal-to-noise ratio (SNR) score of each kernel to non-uniformly combine\nmultiple compressive kernels. The proposed method is evaluated on two\nmobile-sensing datasets -- MHEALTH and HAR -- where activity recognition is\ndefined as utility and person identification is defined as privacy. The results\nshow that the compression regime is successful in privacy preservation as the\nprivacy classification accuracies are almost at the random-guess level in all\nexperiments. On the other hand, the novel SNR-based multi-kernel shows utility\nclassification accuracy improvement upon the state-of-the-art in both datasets.\nThese results indicate a promising direction for research in privacy-preserving\nmachine learning.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 10:27:13 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chanyaswad", "Thee", ""], ["Chang", "J. Morris", ""], ["Kung", "S. Y.", ""]]}, {"id": "2106.10740", "submitter": "Kris Oosthoek", "authors": "Kris Oosthoek", "title": "Flash Crash for Cash: Cyber Threats in Decentralized Finance", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized Finance (DeFi) took shape in 2020. An unprecedented amount of\nover 14 billion USD moved into DeFi projects offering trading, loans and\ninsurance. But its growth has also drawn the attention of malicious actors.\nMany projects were exploited as quickly as they launched and millions of USD\nwere lost. While many developers understand integer overflows and reentrancy\nattacks, security threats to the DeFi ecosystem are more complex and still\npoorly understood. In this paper we provide the first overview of in-the-wild\nDeFi security incidents. We observe that many of these exploits are market\nattacks, weaponizing weakly implemented business logic in one protocol with\ncredit provided by another to inflate appropriations. Rather than misusing\nindividual protocols, attackers increasingly use DeFi's strength of\npermissionless composability against itself. By providing the first holistic\nanalysis of real-world security incidents within the nascent financial\necosystem DeFi is, we hope to inform threat modeling in decentralized\ncryptoeconomic initiatives in the years ahead.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 19:02:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Oosthoek", "Kris", ""]]}, {"id": "2106.10807", "submitter": "Liam Fowl", "authors": "Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojtek\n  Czaja, Tom Goldstein", "title": "Adversarial Examples Make Strong Poisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adversarial machine learning literature is largely partitioned into\nevasion attacks on testing data and poisoning attacks on training data. In this\nwork, we show that adversarial examples, originally intended for attacking\npre-trained models, are even more effective for data poisoning than recent\nmethods designed specifically for poisoning. Our findings indicate that\nadversarial examples, when assigned the original label of their natural base\nimage, cannot be used to train a classifier for natural images. Furthermore,\nwhen adversarial examples are assigned their adversarial class label, they are\nuseful for training. This suggests that adversarial examples contain useful\nsemantic content, just with the ``wrong'' labels (according to a network, but\nnot a human). Our method, adversarial poisoning, is substantially more\neffective than existing poisoning methods for secure dataset release, and we\nrelease a poisoned version of ImageNet, ImageNet-P, to encourage research into\nthe strength of this form of data obfuscation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 01:57:14 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fowl", "Liam", ""], ["Goldblum", "Micah", ""], ["Chiang", "Ping-yeh", ""], ["Geiping", "Jonas", ""], ["Czaja", "Wojtek", ""], ["Goldstein", "Tom", ""]]}, {"id": "2106.10972", "submitter": "Robert Annessi", "authors": "Robert Annessi and Ethan Fast", "title": "Improving security for users of decentralized exchanges through\n  multiparty computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized cryptocurrency exchanges offer compelling security benefits\nover centralized exchanges: users control their funds and avoid the risk of an\nexchange hack or malicious operator. However, because user assets are fully\naccessible by a secret key, decentralized exchanges pose significant internal\nsecurity risks for trading firms and automated trading systems, where a\ncompromised system can result in total loss of funds. Centralized exchanges\nmitigate this risk through API key based security policies that allow\nprofessional users to give individual traders or automated systems specific and\ncustomizable access rights such as trading or withdrawal limits. Such policies,\nhowever, are not compatible with decentralized exchanges, where all exchange\noperations require a signature generated by the owner's secret key. This paper\nintroduces a protocol based upon multiparty computation that allows for the\ncreation of API keys and security policies that can be applied to any existing\ndecentralized exchange. Our protocol works with both ECDSA and EdDSA signature\nschemes and prioritizes efficient computation and communication. We have\ndeployed this protocol on Nash exchange, as well as around several\nEthereum-based automated market maker smart contracts, where it secures the\ntrading accounts and wallets of thousands of users.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 10:46:24 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Annessi", "Robert", ""], ["Fast", "Ethan", ""]]}, {"id": "2106.11200", "submitter": "L\\'idia del Rio", "authors": "Lorenzo Laneve and Lidia del Rio", "title": "Impossibility of composable Oblivious Transfer in relativistic quantum\n  cryptography", "comments": "13+11 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the cryptographic primitive Oblivious Transfer; a composable\nconstruction of this resource would allow arbitrary multi-party computation to\nbe carried out in a secure way, i.e. to compute functions in a distributed way\nwhile keeping inputs from different parties private. First we review a\nframework that allows us to analyze composability of classical and quantum\ncryptographic protocols in special relativity: Abstract Cryptography\nimplemented with Causal Boxes. We then (1) explore and formalize different\nversions of oblivious transfer found in the literature, (2) prove that their\nequivalence holds also in relativistic quantum settings, (3) show that it is\nimpossible to composably construct any of these versions of oblivious transfer\nfrom only classical or quantum communication among distrusting agents in\nrelativistic settings, (4) prove that the impossibility also extends to\nmulti-party computation, and (5) provide a mutual construction between\noblivious transfer and bit commitment.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:37:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Laneve", "Lorenzo", ""], ["del Rio", "Lidia", ""]]}, {"id": "2106.11210", "submitter": "Mengjie Ding", "authors": "Mengjie Ding, Peiru Li, Shanshan Li, He Zhang", "title": "HFContractFuzzer: Fuzzing Hyperledger Fabric Smart Contracts for\n  Vulnerability Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With its unique advantages such as decentralization and immutability,\nblockchain technology has been widely used in various fields in recent years.\nThe smart contract running on the blockchain is also playing an increasingly\nimportant role in decentralized application scenarios. Therefore, the automatic\ndetection of security vulnerabilities in smart contracts has become an urgent\nproblem in the application of blockchain technology. Hyperledger Fabric is a\nsmart contract platform based on enterprise-level licensed distributed ledger\ntechnology. However, the research on the vulnerability detection technology of\nHyperledger Fabric smart contracts is still in its infancy. In this paper, we\npropose HFContractFuzzer, a method based on Fuzzing technology to detect\nHyperledger Fabric smart contracts, which combines a Fuzzing tool for golang\nnamed go-fuzz and smart contracts written by golang. We use HFContractFuzzer to\ndetect vulnerabilities in five contracts from typical sources and discover that\nfour of them have security vulnerabilities, proving the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:52:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ding", "Mengjie", ""], ["Li", "Peiru", ""], ["Li", "Shanshan", ""], ["Zhang", "He", ""]]}, {"id": "2106.11384", "submitter": "Huseyin Inan", "authors": "Saeed Mahloujifar, Huseyin A. Inan, Melissa Chase, Esha Ghosh,\n  Marcello Hasegawa", "title": "Membership Inference on Word Embedding and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the text processing context, most ML models are built on word embeddings.\nThese embeddings are themselves trained on some datasets, potentially\ncontaining sensitive data. In some cases this training is done independently,\nin other cases, it occurs as part of training a larger, task-specific model. In\neither case, it is of interest to consider membership inference attacks based\non the embedding layer as a way of understanding sensitive information leakage.\nBut, somewhat surprisingly, membership inference attacks on word embeddings and\ntheir effect in other natural language processing (NLP) tasks that use these\nembeddings, have remained relatively unexplored.\n  In this work, we show that word embeddings are vulnerable to black-box\nmembership inference attacks under realistic assumptions. Furthermore, we show\nthat this leakage persists through two other major NLP applications:\nclassification and text-generation, even when the embedding layer is not\nexposed to the attacker. We show that our MI attack achieves high attack\naccuracy against a classifier model and an LSTM-based language model. Indeed,\nour attack is a cheaper membership inference attack on text-generative models,\nwhich does not require the knowledge of the target model or any expensive\ntraining of text-generative models as shadow models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:37:06 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mahloujifar", "Saeed", ""], ["Inan", "Huseyin A.", ""], ["Chase", "Melissa", ""], ["Ghosh", "Esha", ""], ["Hasegawa", "Marcello", ""]]}, {"id": "2106.11390", "submitter": "Bhagyashri Tushir", "authors": "Holden Gordon, Conrad Park, Bhagyashri Tushir, Yuhong Liu, Behnam\n  Dezfouli", "title": "An Efficient SDN Architecture for Smart Home Security Accelerated by\n  FPGA", "comments": "published in LANMAN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise in Internet of Things (IoT) devices, home network management\nand security are becoming complex. There is an urgent requirement to make smart\nhome network management efficient. This work proposes an SDN-based architecture\nto secure smart home networks through K-Nearest Neighbor (KNN) based device\nclassifications and malicious traffic detection. The efficiency is further\nenhanced by offloading the computation-intensive KNN model to Field\nProgrammable Gate Arrays (FPGA), which offers parallel processing power of GPU\nplatforms at lower costs and higher efficiencies, and can be used to accelerate\ntime-sensitive tasks. The proposed parallelization and implementation of KNN on\nFPGA are achieved by using the Vivado Design Suite from Xilinx and High-Level\nSynthesis (HLS). When optimized with 10-fold cross-validation, the proposed\nsolution for KNN consistently exhibits the best performances on FPGA when\ncompared with four alternative KNN instances (i.e., 78% faster than the\nparallel bubble sort-based implementation and 99\\% faster than the other three\nsorting algorithms). Moreover, with 36,225 training samples, the proposed KNN\nsolution classifies a test query with 95% accuracy in approximately 4\nmilliseconds on FPGA compared to 57 seconds on a CPU platform.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:59:14 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 01:08:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gordon", "Holden", ""], ["Park", "Conrad", ""], ["Tushir", "Bhagyashri", ""], ["Liu", "Yuhong", ""], ["Dezfouli", "Behnam", ""]]}, {"id": "2106.11424", "submitter": "AmirMahdi Sadeghzadeh", "authors": "Amir Mahdi Sadeghzadeh, Faezeh Dehghan, Amir Mohammad Sobhanian, and\n  Rasool Jalili", "title": "Hardness of Samples Is All You Need: Protecting Deep Learning Models\n  Using Hardness of Samples", "comments": "19 pages, 9 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent studies have shown that Deep Neural Network (DNN)-based\nclassifiers are vulnerable against model extraction attacks. In model\nextraction attacks, an adversary exploits the target classifier to create a\nsurrogate classifier imitating the target classifier with respect to some\ncriteria. In this paper, we investigate the hardness degree of samples and\ndemonstrate that the hardness degree histogram of model extraction attacks\nsamples is distinguishable from the hardness degree histogram of normal\nsamples. Normal samples come from the target classifier's training data\ndistribution. As the training process of DNN-based classifiers is done in\nseveral epochs, we can consider this process as a sequence of subclassifiers so\nthat each subclassifier is created at the end of an epoch. We use the sequence\nof subclassifiers to calculate the hardness degree of samples. We investigate\nthe relation between hardness degree of samples and the trust in the classifier\noutputs. We propose Hardness-Oriented Detection Approach (HODA) to detect the\nsample sequences of model extraction attacks. The results demonstrate that HODA\ncan detect the sample sequences of model extraction attacks with a high success\nrate by only watching 100 attack samples. We also investigate the hardness\ndegree of adversarial examples and indicate that the hardness degree histogram\nof adversarial examples is distinct from the hardness degree histogram of\nnormal samples.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 22:03:31 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sadeghzadeh", "Amir Mahdi", ""], ["Dehghan", "Faezeh", ""], ["Sobhanian", "Amir Mohammad", ""], ["Jalili", "Rasool", ""]]}, {"id": "2106.11446", "submitter": "Yoshi Fujiwara", "authors": "Yoshi Fujiwara, Rubaiyat Islam", "title": "Bitcoin's Crypto Flow Network", "comments": "39 pages, 18 Figures; \"Blockchain in Kyoto 2021\" conference;\n  forthcoming in JPS Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How crypto flows among Bitcoin users is an important question for\nunderstanding the structure and dynamics of the cryptoasset at a global scale.\nWe compiled all the blockchain data of Bitcoin from its genesis to the year\n2020, identified users from anonymous addresses of wallets, and constructed\nmonthly snapshots of networks by focusing on regular users as big players. We\napply the methods of bow-tie structure and Hodge decomposition in order to\nlocate the users in the upstream, downstream, and core of the entire crypto\nflow. Additionally, we reveal principal components hidden in the flow by using\nnon-negative matrix factorization, which we interpret as a probabilistic model.\nWe show that the model is equivalent to a probabilistic latent semantic\nanalysis in natural language processing, enabling us to estimate the number of\nsuch hidden components. Moreover, we find that the bow-tie structure and the\nprincipal components are quite stable among those big players. This study can\nbe a solid basis on which one can further investigate the temporal change of\ncrypto flow, entry and exit of big players, and so forth.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 23:15:49 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 23:35:08 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Fujiwara", "Yoshi", ""], ["Islam", "Rubaiyat", ""]]}, {"id": "2106.11710", "submitter": "Hannes Hattenbach", "authors": "Hannes Hattenbach", "title": "Quantum-resistant digital signatures schemes for low-power IoT", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum computers are on the horizon to get to a sufficient size. These will\nthen be able to break most of the encryption and signature schemes currently in\nuse. This is the case for human interface devices as well as for IoT nodes. In\nthis paper i am comparing some signature schemes currently in the process of\nstandardization by the NIST. After explaining the underlying basis on why some\nschemes are different in some aspects compared to others i will evaluate which\ncurrently available implementations are better suited for usage in IoT\nuse-cases. We will come to further focus on the most promising schemes FALCON\nand Dilithium, which differ in one signifiant aspect that makes FALCON worse\nfor signing but very good for verification purposes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 12:29:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Hattenbach", "Hannes", ""]]}, {"id": "2106.11755", "submitter": "Chinmay Hegde", "authors": "Minsu Cho, Zahra Ghodsi, Brandon Reagen, Siddharth Garg, Chinmay Hegde", "title": "Sphynx: ReLU-Efficient Network Design for Private Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The emergence of deep learning has been accompanied by privacy concerns\nsurrounding users' data and service providers' models. We focus on private\ninference (PI), where the goal is to perform inference on a user's data sample\nusing a service provider's model. Existing PI methods for deep networks enable\ncryptographically secure inference with little drop in functionality; however,\nthey incur severe latency costs, primarily caused by non-linear network\noperations (such as ReLUs). This paper presents Sphynx, a ReLU-efficient\nnetwork design method based on micro-search strategies for convolutional cell\ndesign. Sphynx achieves Pareto dominance over all existing private inference\nmethods on CIFAR-100. We also design large-scale networks that support\ncryptographically private inference on Tiny-ImageNet and ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:11:10 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Cho", "Minsu", ""], ["Ghodsi", "Zahra", ""], ["Reagen", "Brandon", ""], ["Garg", "Siddharth", ""], ["Hegde", "Chinmay", ""]]}, {"id": "2106.11760", "submitter": "GuanLin Li", "authors": "Li Guanlin, Guo Shangwei, Wang Run, Xu Guowen, Zhang Tianwei", "title": "A Stealthy and Robust Fingerprinting Scheme for Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel fingerprinting methodology for the Intellectual\nProperty protection of generative models. Prior solutions for discriminative\nmodels usually adopt adversarial examples as the fingerprints, which give\nanomalous inference behaviors and prediction results. Hence, these methods are\nnot stealthy and can be easily recognized by the adversary. Our approach\nleverages the invisible backdoor technique to overcome the above limitation.\nSpecifically, we design verification samples, whose model outputs look normal\nbut can trigger a backdoor classifier to make abnormal predictions. We propose\na new backdoor embedding approach with Unique-Triplet Loss and fine-grained\ncategorization to enhance the effectiveness of our fingerprints. Extensive\nevaluations show that this solution can outperform other strategies with higher\nrobustness, uniqueness and stealthiness for various GAN models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 06:25:10 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Guanlin", "Li", ""], ["Shangwei", "Guo", ""], ["Run", "Wang", ""], ["Guowen", "Xu", ""], ["Tianwei", "Zhang", ""]]}, {"id": "2106.11762", "submitter": "A K M Nuhil Mehdy", "authors": "A K M Nuhil Mehdy, Hoda Mehrpouyan", "title": "Modeling of Personalized Privacy Disclosure Behavior: A Formal Method\n  Approach", "comments": null, "journal-ref": null, "doi": "10.1145/3465481.3470102", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to create user-centric and personalized privacy management tools,\nthe underlying models must account for individual users' privacy expectations,\npreferences, and their ability to control their information sharing activities.\nExisting studies of users' privacy behavior modeling attempt to frame the\nproblem from a request's perspective, which lack the crucial involvement of the\ninformation owner, resulting in limited or no control of policy management.\nMoreover, very few of them take into the consideration the aspect of\ncorrectness, explainability, usability, and acceptance of the methodologies for\neach user of the system. In this paper, we present a methodology to formally\nmodel, validate, and verify personalized privacy disclosure behavior based on\nthe analysis of the user's situational decision-making process. We use a model\nchecking tool named UPPAAL to represent users' self-reported privacy disclosure\nbehavior by an extended form of finite state automata (FSA), and perform\nreachability analysis for the verification of privacy properties through\ncomputation tree logic (CTL) formulas. We also describe the practical use cases\nof the methodology depicting the potential of formal technique towards the\ndesign and development of user-centric behavioral modeling. This paper, through\nextensive amounts of experimental outcomes, contributes several insights to the\narea of formal methods and user-tailored privacy behavior modeling.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 09:56:48 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mehdy", "A K M Nuhil", ""], ["Mehrpouyan", "Hoda", ""]]}, {"id": "2106.11767", "submitter": "Zhiqi Bu", "authors": "Matteo Sordello, Zhiqi Bu, Jinshuo Dong", "title": "Privacy Amplification via Iteration for Shuffled and Online PNSGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the framework of privacy amplification via\niteration, which is originally proposed by Feldman et al. and subsequently\nsimplified by Asoodeh et al. in their analysis via the contraction coefficient.\nThis line of work focuses on the study of the privacy guarantees obtained by\nthe projected noisy stochastic gradient descent (PNSGD) algorithm with hidden\nintermediate updates. A limitation in the existing literature is that only the\nearly stopped PNSGD has been studied, while no result has been proved on the\nmore widely-used PNSGD applied on a shuffled dataset. Moreover, no scheme has\nbeen yet proposed regarding how to decrease the injected noise when new data\nare received in an online fashion. In this work, we first prove a privacy\nguarantee for shuffled PNSGD, which is investigated asymptotically when the\nnoise is fixed for each sample size $n$ but reduced at a predetermined rate\nwhen $n$ increases, in order to achieve the convergence of privacy loss. We\nthen analyze the online setting and provide a faster decaying scheme for the\nmagnitude of the injected noise that also guarantees the convergence of privacy\nloss.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 17:48:06 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sordello", "Matteo", ""], ["Bu", "Zhiqi", ""], ["Dong", "Jinshuo", ""]]}, {"id": "2106.11770", "submitter": "Raviraj Joshi", "authors": "Prutha Gaherwar, Shraddha Joshi, Raviraj Joshi, Rahul Khengare", "title": "SISA: Securing Images by Selective Alteration", "comments": "Accepted at ICTCS 2020", "journal-ref": null, "doi": "10.1007/978-981-16-0739-4_69", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an increase in mobile and camera devices' popularity, digital content in\nthe form of images has increased drastically. As personal life is being\ncontinuously documented in pictures, the risk of losing it to eavesdroppers is\na matter of grave concern. Secondary storage is the most preferred medium for\nthe storage of personal and other images. Our work is concerned with the\nsecurity of such images. While encryption is the best way to ensure image\nsecurity, full encryption and decryption is a computationally-intensive\nprocess. Moreover, as cameras are getting better every day, image quality, and\nthus, the pixel density has increased considerably. The increased pixel density\nmakes encryption and decryption more expensive. We, therefore, delve into\nselective encryption and selective blurring based on the region of interest.\nInstead of encrypting or blurring the entire photograph, we only encode\nselected regions of the image. We present a comparative analysis of the partial\nand full encryption of the photos. This kind of encoding will help us lower the\nencryption overhead without compromising security. The applications utilizing\nthis technique will become more usable due to the reduction in the decryption\ntime. Additionally, blurred images being more readable than encrypted ones,\nallowed us to define the level of security. We leverage the machine learning\nalgorithms like Mask-RCNN (Region-based convolutional neural network) and YOLO\n(You Only Look Once) to select the region of interest. These algorithms have\nset new benchmarks for object recognition. We develop an end to end system to\ndemonstrate our idea of selective encryption.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 05:31:47 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gaherwar", "Prutha", ""], ["Joshi", "Shraddha", ""], ["Joshi", "Raviraj", ""], ["Khengare", "Rahul", ""]]}, {"id": "2106.11821", "submitter": "Niall McLaughlin", "authors": "Niall McLaughlin, Jesus Martinez del Rincon", "title": "Data Augmentation for Opcode Sequence Based Malware Detection", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data augmentation has been successfully used in many areas of deep-learning\nto significantly improve model performance. Typically data augmentation\nsimulates realistic variations in data in order to increase the apparent\ndiversity of the training-set. However, for opcode-based malware analysis,\nwhere deep learning methods are already achieving state of the art performance,\nit is not immediately clear how to apply data augmentation. In this paper we\nstudy different methods of data augmentation starting with basic methods using\nfixed transformations and moving to methods that adapt to the data. We propose\na novel data augmentation method based on using an opcode embedding layer\nwithin the network and its corresponding opcode embedding matrix to perform\nadaptive data augmentation during training. To the best of our knowledge this\nis the first paper to carry out a systematic study of different augmentation\nmethods applied to opcode sequence based malware classification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:36:35 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["McLaughlin", "Niall", ""], ["del Rincon", "Jesus Martinez", ""]]}, {"id": "2106.11865", "submitter": "Cheng-Te Li", "authors": "I-Chung Hsieh, Cheng-Te Li", "title": "NetFense: Adversarial Defenses against Privacy Attacks on Neural\n  Networks for Graph Data", "comments": "Accepted to IEEE TKDE 2021. Code is available at\n  https://github.com/ICHproject/NetFense/", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering (TKDE) 2021", "doi": "10.1109/TKDE.2021.3087515", "report-no": null, "categories": "cs.LG cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in protecting node privacy on graph data and attacking graph\nneural networks (GNNs) gain much attention. The eye does not bring these two\nessential tasks together yet. Imagine an adversary can utilize the powerful\nGNNs to infer users' private labels in a social network. How can we\nadversarially defend against such privacy attacks while maintaining the utility\nof perturbed graphs? In this work, we propose a novel research task,\nadversarial defenses against GNN-based privacy attacks, and present a graph\nperturbation-based approach, NetFense, to achieve the goal. NetFense can\nsimultaneously keep graph data unnoticeability (i.e., having limited changes on\nthe graph structure), maintain the prediction confidence of targeted label\nclassification (i.e., preserving data utility), and reduce the prediction\nconfidence of private label classification (i.e., protecting the privacy of\nnodes). Experiments conducted on single- and multiple-target perturbations\nusing three real graph data exhibit that the perturbed graphs by NetFense can\neffectively maintain data utility (i.e., model unnoticeability) on targeted\nlabel classification and significantly decrease the prediction confidence of\nprivate label classification (i.e., privacy protection). Extensive studies also\nbring several insights, such as the flexibility of NetFense, preserving local\nneighborhoods in data unnoticeability, and better privacy protection for\nhigh-degree nodes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 15:32:50 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Hsieh", "I-Chung", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2106.11900", "submitter": "Mohammad Arif Ul Alam", "authors": "Mohammad Arif Ul Alam", "title": "Person Re-identification Attack on Wearable Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a critical privacy attack in publicly shared\nhealthcare data as per Health Insurance Portability and Accountability Act\n(HIPAA) privacy rule. In this paper, we investigate the possibility of a new\ntype of privacy attack, Person Re-identification Attack (PRI-attack) on\npublicly shared privacy insensitive wearable data. We investigate user's\nspecific biometric signature in terms of two contextual biometric traits,\nphysiological (photoplethysmography and electrodermal activity) and physical\n(accelerometer) contexts. In this regard, we develop a Multi-Modal Siamese\nConvolutional Neural Network (mmSNN) model. The framework learns the spatial\nand temporal information individually and combines them together in a modified\nweighted cost with an objective of predicting a person's identity. We evaluated\nour proposed model using real-time collected data from 3 collected datasets and\none publicly available dataset. Our proposed framework shows that PPG-based\nbreathing rate and heart rate in conjunction with hand gesture contexts can be\nutilized by attackers to re-identify user's identity (max. 71%) from HIPAA\ncompliant wearable data. Given publicly placed camera can estimate heart rate\nand breathing rate along with hand gestures remotely, person re-identification\nusing them imposes a significant threat to future HIPAA compliant server which\nrequires a better encryption method to store wearable healthcare data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:10:17 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Alam", "Mohammad Arif Ul", ""]]}, {"id": "2106.12021", "submitter": "Abhishek Moitra", "authors": "Abhishek Moitra and Priyadarshini Panda", "title": "DetectX -- Adversarial Input Detection using Current Signatures in\n  Memristive XBar Arrays", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial input detection has emerged as a prominent technique to harden\nDeep Neural Networks(DNNs) against adversarial attacks. Most prior works use\nneural network-based detectors or complex statistical analysis for adversarial\ndetection. These approaches are computationally intensive and vulnerable to\nadversarial attacks. To this end, we propose DetectX - a hardware friendly\nadversarial detection mechanism using hardware signatures like Sum of column\nCurrents (SoI) in memristive crossbars (XBar). We show that adversarial inputs\nhave higher SoI compared to clean inputs. However, the difference is too small\nfor reliable adversarial detection. Hence, we propose a dual-phase training\nmethodology: Phase1 training is geared towards increasing the separation\nbetween clean and adversarial SoIs; Phase2 training improves the overall\nrobustness against different strengths of adversarial attacks. For\nhardware-based adversarial detection, we implement the DetectX module using\n32nm CMOS circuits and integrate it with a Neurosim-like analog crossbar\narchitecture. We perform hardware evaluation of the Neurosim+DetectX system on\nthe Neurosim platform using datasets-CIFAR10(VGG8), CIFAR100(VGG16) and\nTinyImagenet(ResNet18). Our experiments show that DetectX is 10x-25x more\nenergy efficient and immune to dynamic adversarial attacks compared to previous\nstate-of-the-art works. Moreover, we achieve high detection performance\n(ROC-AUC > 0.95) for strong white-box and black-box attacks. The code has been\nreleased at https://github.com/Intelligent-Computing-Lab-Yale/DetectX\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 19:09:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Moitra", "Abhishek", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2106.12033", "submitter": "Michael Neuder", "authors": "Michael Neuder, Rithvik Rao, Daniel J. Moroz, David C. Parkes", "title": "Strategic Liquidity Provision in Uniswap v3", "comments": "Submitted to ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniswap is the largest decentralized exchange for digital currencies. The\nnewest version, called Uniswap v3, allows liquidity providers to allocate\nliquidity to one or more closed intervals of the price of an asset, instead of\nover the total range of prices. While the price of the asset remains in that\ninterval, the liquidity provider earns rewards proportionally to the amount of\nliquidity allocated. This induces the problem of strategic liquidity provision:\nsmaller intervals result in higher concentration of liquidity and\ncorrespondingly larger rewards when the price remains in the interval, but with\nhigher risk. We formalize this problem and study three classes of strategies\nfor liquidity providers: uniform, proportional, and optimal (via a constrained\noptimization problem). We present experimental results based on the historical\nprice data of Ethereum, which show that simple liquidity provision strategies\ncan yield near-optimal utility and earn over 200x more than Uniswap v2\nliquidity provision.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 19:48:02 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Neuder", "Michael", ""], ["Rao", "Rithvik", ""], ["Moroz", "Daniel J.", ""], ["Parkes", "David C.", ""]]}, {"id": "2106.12083", "submitter": "Frank Cangialosi", "authors": "Frank Cangialosi, Neil Agarwal, Venkat Arun, Junchen Jiang, Srinivas\n  Narayana, Anand Sarwate and Ravi Netravali", "title": "Privid: Practical, Privacy-Preserving Video Analytics Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytics on video recorded by cameras in public areas have the potential to\nfuel many exciting applications, but also pose the risk of intruding on\nindividuals' privacy. Unfortunately, existing solutions fail to practically\nresolve this tension between utility and privacy, relying on perfect detection\nof all private information in each video frame--an elusive requirement. This\npaper presents: (1) a new notion of differential privacy (DP) for video\nanalytics, $(\\rho,K,\\epsilon)$-event-duration privacy, which protects all\nprivate information visible for less than a particular duration, rather than\nrelying on perfect detections of that information, and (2) a practical system\ncalled Privid that enforces duration-based privacy even with the (untrusted)\nanalyst-provided deep neural networks that are commonplace for video analytics\ntoday. Across a variety of videos and queries, we show that Privid achieves\naccuracies within 79-99% of a non-private system.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 22:25:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Cangialosi", "Frank", ""], ["Agarwal", "Neil", ""], ["Arun", "Venkat", ""], ["Jiang", "Junchen", ""], ["Narayana", "Srinivas", ""], ["Sarwate", "Anand", ""], ["Netravali", "Ravi", ""]]}, {"id": "2106.12118", "submitter": "Ryan McKenna", "authors": "Ryan McKenna, Gerome Miklau, Michael Hay, Ashwin Machanavajjhala", "title": "HDMM: Optimizing error of high-dimensional statistical queries under\n  differential privacy", "comments": "arXiv admin note: text overlap with arXiv:1808.03537", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we describe the High-Dimensional Matrix Mechanism (HDMM), a\ndifferentially private algorithm for answering a workload of predicate counting\nqueries. HDMM represents query workloads using a compact implicit matrix\nrepresentation and exploits this representation to efficiently optimize over (a\nsubset of) the space of differentially private algorithms for one that is\nunbiased and answers the input query workload with low expected error. HDMM can\nbe deployed for both $\\epsilon$-differential privacy (with Laplace noise) and\n$(\\epsilon, \\delta)$-differential privacy (with Gaussian noise), although the\ncore techniques are slightly different for each. We demonstrate empirically\nthat HDMM can efficiently answer queries with lower expected error than\nstate-of-the-art techniques, and in some cases, it nearly matches existing\nlower bounds for the particular class of mechanisms we consider.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 01:19:18 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["McKenna", "Ryan", ""], ["Miklau", "Gerome", ""], ["Hay", "Michael", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "2106.12288", "submitter": "Chen Liu", "authors": "Chen Liu, Bo Li, Jun Zhao, Ming Su, Xu-Dong Liu", "title": "MG-DVD: A Real-time Framework for Malware Variant Detection Based on\n  Dynamic Heterogeneous Graph Learning", "comments": "8 pages, 7 figures, Accepted at the 30th International Joint\n  Conference on Artificial Intelligence(IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the newly emerging malware variants in real time is crucial for\nmitigating cyber risks and proactively blocking intrusions. In this paper, we\npropose MG-DVD, a novel detection framework based on dynamic heterogeneous\ngraph learning, to detect malware variants in real time. Particularly, MG-DVD\nfirst models the fine-grained execution event streams of malware variants into\ndynamic heterogeneous graphs and investigates real-world meta-graphs between\nmalware objects, which can effectively characterize more discriminative\nmalicious evolutionary patterns between malware and their variants. Then,\nMG-DVD presents two dynamic walk-based heterogeneous graph learning methods to\nlearn more comprehensive representations of malware variants, which\nsignificantly reduces the cost of the entire graph retraining. As a result,\nMG-DVD is equipped with the ability to detect malware variants in real time,\nand it presents better interpretability by introducing meaningful meta-graphs.\nComprehensive experiments on large-scale samples prove that our proposed MG-DVD\noutperforms state-of-the-art methods in detecting malware variants in terms of\neffectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:17:38 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 06:56:56 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Liu", "Chen", ""], ["Li", "Bo", ""], ["Zhao", "Jun", ""], ["Su", "Ming", ""], ["Liu", "Xu-Dong", ""]]}, {"id": "2106.12321", "submitter": "Ievgen Kabin", "authors": "Ievgen Kabin, Zoya Dyka, Dan Klann, Peter Langendoerfer", "title": "EC Scalar Multiplication: Successful Simple Address Bit SCA Attack\n  against Atomic Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work we discuss the resistance of atomic pattern algorithms for\nelliptic curve point multiplication against simple side channel analysis\nattacks using our own implementation as an example. The idea of the atomicity\nprinciple is to make kP implementations resistant against simple side channel\nanalysis attacks. One of the assumptions, on which the atomicity principle is\nbased, is the indistinguishability of register operations, i.e. two\nwrite-to-register operations cannot be distinguished if their old and new data\nvalues are the same. But before the data can be stored to a register/block,\nthis register/block has to be addressed for storing the data. Different\nregisters/blocks have different addresses. In praxis, this different and key\ndependent addressing can be used to reveal the key, even by running simple SCA\nattacks. The key dependent addressing of registers/blocks allows to reveal the\nkey and is an inherent feature of the binary kP algorithms. This means that the\nassumption, that addressing of different registers/blocks is an\nindistinguishable operation, may no longer be applied when realizing kP\nimplementations, at least not for hardware implementations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:35:37 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kabin", "Ievgen", ""], ["Dyka", "Zoya", ""], ["Klann", "Dan", ""], ["Langendoerfer", "Peter", ""]]}, {"id": "2106.12328", "submitter": "Paul Prasse", "authors": "Paul Prasse, Jan Brabec, Jan Kohout, Martin Kopp, Lukas Bajer, Tobias\n  Scheffer", "title": "Learning Explainable Representations of Malware Behavior", "comments": "This is a pre-print of an article to appear in Machine Learning and\n  Knowledge Discovery in Databases. ECML PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address the problems of identifying malware in network telemetry logs and\nproviding \\emph{indicators of compromise} -- comprehensible explanations of\nbehavioral patterns that identify the threat. In our system, an array of\nspecialized detectors abstracts network-flow data into comprehensible\n\\emph{network events} in a first step. We develop a neural network that\nprocesses this sequence of events and identifies specific threats, malware\nfamilies and broad categories of malware. We then use the\n\\emph{integrated-gradients} method to highlight events that jointly constitute\nthe characteristic behavioral pattern of the threat. We compare network\narchitectures based on CNNs, LSTMs, and transformers, and explore the efficacy\nof unsupervised pre-training experimentally on large-scale telemetry data. We\ndemonstrate how this system detects njRAT and other malware based on behavioral\npatterns.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:50:57 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Prasse", "Paul", ""], ["Brabec", "Jan", ""], ["Kohout", "Jan", ""], ["Kopp", "Martin", ""], ["Bajer", "Lukas", ""], ["Scheffer", "Tobias", ""]]}, {"id": "2106.12336", "submitter": "Arthur Drichel", "authors": "Arthur Drichel, Nils Faerber, Ulrike Meyer", "title": "First Step Towards EXPLAINable DGA Multiclass Classification", "comments": "Accepted at The 16th International Conference on Availability,\n  Reliability and Security (ARES 2021)", "journal-ref": null, "doi": "10.1145/3465481.3465749", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous malware families rely on domain generation algorithms (DGAs) to\nestablish a connection to their command and control (C2) server. Counteracting\nDGAs, several machine learning classifiers have been proposed enabling the\nidentification of the DGA that generated a specific domain name and thus\ntriggering targeted remediation measures. However, the proposed\nstate-of-the-art classifiers are based on deep learning models. The black box\nnature of these makes it difficult to evaluate their reasoning. The resulting\nlack of confidence makes the utilization of such models impracticable. In this\npaper, we propose EXPLAIN, a feature-based and contextless DGA multiclass\nclassifier. We comparatively evaluate several combinations of feature sets and\nhyperparameters for our approach against several state-of-the-art classifiers\nin a unified setting on the same real-world data. Our classifier achieves\ncompetitive results, is real-time capable, and its predictions are easier to\ntrace back to features than the predictions made by the DGA multiclass\nclassifiers proposed in related work.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 12:05:13 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Drichel", "Arthur", ""], ["Faerber", "Nils", ""], ["Meyer", "Ulrike", ""]]}, {"id": "2106.12343", "submitter": "Arthur Drichel", "authors": "Arthur Drichel, Vincent Drury, Justus von Brandt, Ulrike Meyer", "title": "Finding Phish in a Haystack: A Pipeline for Phishing Classification on\n  Certificate Transparency Logs", "comments": "Accepted at The 16th International Conference on Availability,\n  Reliability and Security (ARES 2021)", "journal-ref": null, "doi": "10.1145/3465481.3470111", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current popular phishing prevention techniques mainly utilize reactive\nblocklists, which leave a ``window of opportunity'' for attackers during which\nvictims are unprotected. One possible approach to shorten this window aims to\ndetect phishing attacks earlier, during website preparation, by monitoring\nCertificate Transparency (CT) logs. Previous attempts to work with CT log data\nfor phishing classification exist, however they lack evaluations on actual CT\nlog data. In this paper, we present a pipeline that facilitates such\nevaluations by addressing a number of problems when working with CT log data.\nThe pipeline includes dataset creation, training, and past or live\nclassification of CT logs. Its modular structure makes it possible to easily\nexchange classifiers or verification sources to support ground truth labeling\nefforts and classifier comparisons. We test the pipeline on a number of new and\nexisting classifiers, and find a general potential to improve classifiers for\nthis scenario in the future. We publish the source code of the pipeline and the\nused datasets along with this paper\n(https://gitlab.com/rwth-itsec/ctl-pipeline), thus making future research in\nthis direction more accessible.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 12:24:19 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Drichel", "Arthur", ""], ["Drury", "Vincent", ""], ["von Brandt", "Justus", ""], ["Meyer", "Ulrike", ""]]}, {"id": "2106.12458", "submitter": "Sarah E Carter", "authors": "Sarah E. Carter", "title": "Is Downloading this App Consistent with my Values? Conceptualizing a\n  Value-Centered Privacy Assistant", "comments": "This is an Author Accepted Manuscript for I3E2021, to be held in\n  September 2021. The final version will be available at Springer. A link and\n  the DOI will be added in due course", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital privacy notices aim to provide users with information to make\ninformed decisions. They are, however, fraught with difficulties. Instead, I\npropose that data privacy decisions can be understood as an expression of user\nvalues. To optimize this value expression, I further propose the creation of a\nvalue-centered privacy assistant (VcPA). Here, I preliminary explore how a VcPA\ncould enhance user value expression by utilizing three user scenarios in the\ncontext of considering whether or not to download an environmental application,\nthe OpenLitterMap app. These scenarios are conceptually constructed from\nestablished privacy user groups - the privacy fundamentalists; the privacy\npragmatists; and the privacy unconcerned. I conclude that the VcPA best\nfacilitates user value expression of the privacy fundamentalists. In contrast,\nthe value expression of the privacy pragmatists and the privacy unconcerned\ncould be enhanced or hindered depending on the context and their internal\nstates. Possible implications for optimal VcPA design are also discussed.\nFollowing this initial conceptual exploration of VcPAs, further empirical\nresearch will be required to demonstrate the effectiveness of the VcPA system\nin real-world settings.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 15:08:58 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Carter", "Sarah E.", ""]]}, {"id": "2106.12478", "submitter": "Yufei Chen", "authors": "Yufei Chen, Chao Shen, Cong Wang, Yang Zhang", "title": "Teacher Model Fingerprinting Attacks Against Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning has become a common solution to address training data\nscarcity in practice. It trains a specified student model by reusing or\nfine-tuning early layers of a well-trained teacher model that is usually\npublicly available. However, besides utility improvement, the transferred\npublic knowledge also brings potential threats to model confidentiality, and\neven further raises other security and privacy issues.\n  In this paper, we present the first comprehensive investigation of the\nteacher model exposure threat in the transfer learning context, aiming to gain\na deeper insight into the tension between public knowledge and model\nconfidentiality. To this end, we propose a teacher model fingerprinting attack\nto infer the origin of a student model, i.e., the teacher model it transfers\nfrom. Specifically, we propose a novel optimization-based method to carefully\ngenerate queries to probe the student model to realize our attack. Unlike\nexisting model reverse engineering approaches, our proposed fingerprinting\nmethod neither relies on fine-grained model outputs, e.g., posteriors, nor\nauxiliary information of the model architecture or training dataset. We\nsystematically evaluate the effectiveness of our proposed attack. The empirical\nresults demonstrate that our attack can accurately identify the model origin\nwith few probing queries. Moreover, we show that the proposed attack can serve\nas a stepping stone to facilitating other attacks against machine learning\nmodels, such as model stealing.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 15:52:35 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chen", "Yufei", ""], ["Shen", "Chao", ""], ["Wang", "Cong", ""], ["Zhang", "Yang", ""]]}, {"id": "2106.12563", "submitter": "Dylan Slack", "authors": "Dylan Slack, Sophie Hilgard, Sameer Singh, Himabindu Lakkaraju", "title": "Feature Attributions and Counterfactual Explanations Can Be Manipulated", "comments": "arXiv admin note: text overlap with arXiv:2106.02666", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As machine learning models are increasingly used in critical decision-making\nsettings (e.g., healthcare, finance), there has been a growing emphasis on\ndeveloping methods to explain model predictions. Such \\textit{explanations} are\nused to understand and establish trust in models and are vital components in\nmachine learning pipelines. Though explanations are a critical piece in these\nsystems, there is little understanding about how they are vulnerable to\nmanipulation by adversaries. In this paper, we discuss how two broad classes of\nexplanations are vulnerable to manipulation. We demonstrate how adversaries can\ndesign biased models that manipulate model agnostic feature attribution methods\n(e.g., LIME \\& SHAP) and counterfactual explanations that hill-climb during the\ncounterfactual search (e.g., Wachter's Algorithm \\& DiCE) into\n\\textit{concealing} the model's biases. These vulnerabilities allow an\nadversary to deploy a biased model, yet explanations will not reveal this bias,\nthereby deceiving stakeholders into trusting the model. We evaluate the\nmanipulations on real world data sets, including COMPAS and Communities \\&\nCrime, and find explanations can be manipulated in practice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 17:43:31 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 18:08:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Slack", "Dylan", ""], ["Hilgard", "Sophie", ""], ["Singh", "Sameer", ""], ["Lakkaraju", "Himabindu", ""]]}, {"id": "2106.12576", "submitter": "Sahib Singh", "authors": "Archit Uniyal, Rakshit Naidu, Sasikanth Kotti, Sahib Singh, Patrik\n  Joslin Kenfack, Fatemehsadat Mireshghallah, Andrew Trask", "title": "DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?", "comments": "4 pages, 3 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in differentially private deep learning have demonstrated\nthat application of differential privacy, specifically the DP-SGD algorithm,\nhas a disparate impact on different sub-groups in the population, which leads\nto a significantly high drop-in model utility for sub-populations that are\nunder-represented (minorities), compared to well-represented ones. In this\nwork, we aim to compare PATE, another mechanism for training deep learning\nmodels using differential privacy, with DP-SGD in terms of fairness. We show\nthat PATE does have a disparate impact too, however, it is much less severe\nthan DP-SGD. We draw insights from this observation on what might be promising\ndirections in achieving better fairness-privacy trade-offs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 20:37:12 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Uniyal", "Archit", ""], ["Naidu", "Rakshit", ""], ["Kotti", "Sasikanth", ""], ["Singh", "Sahib", ""], ["Kenfack", "Patrik Joslin", ""], ["Mireshghallah", "Fatemehsadat", ""], ["Trask", "Andrew", ""]]}, {"id": "2106.12638", "submitter": "Arsalan Vahi", "authors": "Arsalan Vahi and Mirkamal Mirnia", "title": "On the Differential Cryptanalysis of SEPAR Cipher", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  SEPAR is a lightweight cryptographic algorithm, designed to implement on\nresource-constrained devices especially those employed in IoT environments.\nMeanwhile, the mixed structure design of cipher leads to speed improvement\nwhile guaranteeing its resistance against common cryptographic attacks,\nespecially differential and linear attacks. In order to confirm the resistance\nof the cipher against differential attack, an extensive investigation was\npresented in our previous work. In his study, we conduct new research\ncontinuing the previously presented research. We prove that there are enough\nactive S-boxes so as to resist cipher against differential cryptanalysis.\nMoreover, this can provide a tight bound of resisting cipher against this\nattack.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 19:30:29 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Vahi", "Arsalan", ""], ["Mirnia", "Mirkamal", ""]]}, {"id": "2106.12693", "submitter": "Niloofar Bayat", "authors": "Niloofar Bayat and Weston Jackson and Derrick Liu", "title": "Deep Learning for Network Traffic Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Monitoring network traffic to identify content, services, and applications is\nan active research topic in network traffic control systems. While modern\nfirewalls provide the capability to decrypt packets, this is not appealing for\nprivacy advocates. Hence, identifying any information from encrypted traffic is\na challenging task. Nonetheless, previous work has identified machine learning\nmethods that may enable application and service identification. The process\ninvolves high level feature extraction from network packet data then training a\nrobust machine learning classifier for traffic identification. We propose a\nclassification technique using an ensemble of deep learning architectures on\npacket, payload, and inter-arrival time sequences. To our knowledge, this is\nthe first time such deep learning architectures have been applied to the Server\nName Indication (SNI) classification problem. Our ensemble model beats the\nstate of the art machine learning methods and our up-to-date model can be found\non github: \\url{https://github.com/niloofarbayat/NetworkClassification}\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 04:11:32 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Bayat", "Niloofar", ""], ["Jackson", "Weston", ""], ["Liu", "Derrick", ""]]}, {"id": "2106.12714", "submitter": "Fatemeh Ganji", "authors": "Ana Covic, Fatemeh Ganji, Domenic Forte", "title": "Circuit Masking: From Theory to Standardization, A Comprehensive Survey\n  for Hardware Security Researchers and Practitioners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Side-channel attacks extracting sensitive data from implementations have been\nconsidered a major threat to the security of cryptographic schemes. This has\nelevated the need for improved designs by embodying countermeasures, with\nmasking being the most prominent example. To formally verify the security of a\nmasking scheme, numerous attack models have been developed to capture the\nphysical properties of the information leakage as well as the capabilities of\nthe adversary. With regard to these models, extensive research has been\nperformed to realize masking schemes. These research efforts have led to\nsignificant progress in the development of security assessment methodologies\nand further initiated standardization activities. However, since the majority\nof this work is theoretical, it is challenging for the more practice-oriented\nhardware security community to fully grasp and contribute to. To bridge the\ngap, these advancements are reviewed and discussed in this survey, mainly from\nthe perspective of hardware security. In doing so, a clear taxonomy is provided\nthat is helpful for a systematic treatment of the masking-related topics. By\ngiving an extensive overview of the existing methods, this survey (1) provides\na research landscape of circuit masking for newcomers to the field, (2) offers\nguidelines on which attack model and verification tool to choose when designing\nmasking schemes, and (3) identifies interesting new research directions where\nmasking models and assessment tools can be applied. Thus, this survey serves as\nan essential reference for hardware security practitioners interested in the\ntheory behind masking techniques, the tools useful to verify the security of\nmasked circuits, and their potential applications.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 01:21:16 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 01:29:25 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Covic", "Ana", ""], ["Ganji", "Fatemeh", ""], ["Forte", "Domenic", ""]]}, {"id": "2106.12749", "submitter": "Kaito Ito", "authors": "Genki Sugiura, Kaito Ito, Kenji Kashima", "title": "Bayesian Differential Privacy for Linear Dynamical Systems", "comments": "7 pages, 6 figures", "journal-ref": "IEEE Control Systems Letters, 2021", "doi": "10.1109/LCSYS.2021.3087096", "report-no": null, "categories": "math.OC cs.CR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a privacy measure based on the difficulty of\ndiscriminating between similar input data. In differential privacy analysis,\nsimilar data usually implies that their distance does not exceed a\npredetermined threshold. It, consequently, does not take into account the\ndifficulty of distinguishing data sets that are far apart, which often contain\nhighly private information. This problem has been pointed out in the research\non differential privacy for static data, and Bayesian differential privacy has\nbeen proposed, which provides a privacy protection level even for outlier data\nby utilizing the prior distribution of the data. In this study, we introduce\nthis Bayesian differential privacy to dynamical systems, and provide privacy\nguarantees for distant input data pairs and reveal its fundamental property.\nFor example, we design a mechanism that satisfies the desired level of privacy\nprotection, which characterizes the trade-off between privacy and information\nutility.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 03:19:02 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sugiura", "Genki", ""], ["Ito", "Kaito", ""], ["Kashima", "Kenji", ""]]}, {"id": "2106.12753", "submitter": "Woosub Jung", "authors": "Woosub Jung (1), Yizhou Feng (2), Sabbir Ahmed Khan (2), Chunsheng Xin\n  (2), Danella Zhao (2), and Gang Zhou (1) ((1) William & Mary, (2) Old\n  Dominion University)", "title": "DeepAuditor: Distributed Online Intrusion Detection System for IoT\n  devices via Power Side-channel Auditing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the number of IoT devices has increased rapidly, IoT botnets have\nexploited the vulnerabilities of IoT devices. However, it is still challenging\nto detect the initial intrusion on IoT devices prior to massive attacks. Recent\nstudies have utilized power side-channel information to characterize this\nintrusion behavior on IoT devices but still lack real-time detection\napproaches. This study aimed to design an online intrusion detection system\ncalled DeepAuditor for IoT devices via power auditing. To realize the real-time\nsystem, we first proposed a lightweight power auditing device called Power\nAuditor. With the Power Auditor, we developed a Distributed CNN classifier for\nonline inference in our laboratory setting. In order to protect data leakage\nand reduce networking redundancy, we also proposed a privacy-preserved\ninference protocol via Packed Homomorphic Encryption and a sliding window\nprotocol in our system. The classification accuracy and processing time were\nmeasured in our laboratory settings. We also demonstrated that the distributed\nCNN design is secure against any distributed components. Overall, the\nmeasurements were shown to the feasibility of our real-time distributed system\nfor intrusion detection on IoT devices.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 03:32:23 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Jung", "Woosub", ""], ["Feng", "Yizhou", ""], ["Khan", "Sabbir Ahmed", ""], ["Xin", "Chunsheng", ""], ["Zhao", "Danella", ""], ["Zhou", "Gang", ""]]}, {"id": "2106.12894", "submitter": "Nishant Kumar", "authors": "Nishant Kumar, Pia Hanfeld, Michael Hecht, Michael Bussmann, Stefan\n  Gumhold and Nico Hoffmannn", "title": "InFlow: Robust outlier detection utilizing Normalizing Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normalizing flows are prominent deep generative models that provide tractable\nprobability distributions and efficient density estimation. However, they are\nwell known to fail while detecting Out-of-Distribution (OOD) inputs as they\ndirectly encode the local features of the input representations in their latent\nspace. In this paper, we solve this overconfidence issue of normalizing flows\nby demonstrating that flows, if extended by an attention mechanism, can\nreliably detect outliers including adversarial attacks. Our approach does not\nrequire outlier data for training and we showcase the efficiency of our method\nfor OOD detection by reporting state-of-the-art performance in diverse\nexperimental settings. Code available at\nhttps://github.com/ComputationalRadiationPhysics/InFlow .\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:42:50 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kumar", "Nishant", ""], ["Hanfeld", "Pia", ""], ["Hecht", "Michael", ""], ["Bussmann", "Michael", ""], ["Gumhold", "Stefan", ""], ["Hoffmannn", "Nico", ""]]}, {"id": "2106.12900", "submitter": "Shuyu Zhao", "authors": "Fan Liu, Shuyu Zhao, Xuelong Dai, Bin Xiao", "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for\n  Few-shot Classification Tasks", "comments": "Accepted by the ICML 2021 Workshop on A Blessing in Disguise: The\n  Prospects and Perils of Adversarial Machine\n  Learning(https://openreview.net/group?id=ICML.cc/2021/Workshop/AML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:31:16 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 12:13:14 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 09:41:16 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liu", "Fan", ""], ["Zhao", "Shuyu", ""], ["Dai", "Xuelong", ""], ["Xiao", "Bin", ""]]}, {"id": "2106.12949", "submitter": "Zhikun Zhang", "authors": "Ninghui Li and Zhikun Zhang and Tianhao Wang", "title": "DPSyn: Experiences in the NIST Differential Privacy Data Synthesis\n  Challenges", "comments": "To appear in Journal of Privacy and Confidentiality (JPC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We summarize the experience of participating in two differential privacy\ncompetitions organized by the National Institute of Standards and Technology\n(NIST). In this paper, we document our experiences in the competition, the\napproaches we have used, the lessons we have learned, and our call to the\nresearch community to further bridge the gap between theory and practice in DP\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:18:48 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Li", "Ninghui", ""], ["Zhang", "Zhikun", ""], ["Wang", "Tianhao", ""]]}, {"id": "2106.13076", "submitter": "Yuchen Li", "authors": "Yuchen Li, Yifan Bao, Liyao Xiang, Junhan Liu, Cen Chen, Li Wang,\n  Xinbing Wang", "title": "Privacy Threats Analysis to Secure Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning is emerging as a machine learning technique that trains a\nmodel across multiple decentralized parties. It is renowned for preserving\nprivacy as the data never leaves the computational devices, and recent\napproaches further enhance its privacy by hiding messages transferred in\nencryption. However, we found that despite the efforts, federated learning\nremains privacy-threatening, due to its interactive nature across different\nparties. In this paper, we analyze the privacy threats in industrial-level\nfederated learning frameworks with secure computation, and reveal such threats\nwidely exist in typical machine learning models such as linear regression,\nlogistic regression and decision tree. For the linear and logistic regression,\nwe show through theoretical analysis that it is possible for the attacker to\ninvert the entire private input of the victim, given very few information. For\nthe decision tree model, we launch an attack to infer the range of victim's\nprivate inputs. All attacks are evaluated on popular federated learning\nframeworks and real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 15:02:54 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Li", "Yuchen", ""], ["Bao", "Yifan", ""], ["Xiang", "Liyao", ""], ["Liu", "Junhan", ""], ["Chen", "Cen", ""], ["Wang", "Li", ""], ["Wang", "Xinbing", ""]]}, {"id": "2106.13123", "submitter": "Fahad Shaon", "authors": "Fahad Shaon (1), Sazzadur Rahaman (2), Murat Kantarcioglu (1) ((1)\n  Data Security Technologies, (2) University of Arizona)", "title": "SecureDL: Securing Code Execution and Access Control for Distributed\n  Data Analytics Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data analytics platforms such as Apache Spark enable\ncost-effective processing and storage. These platforms allow users to\ndistribute data to multiple nodes and enable arbitrary code execution over this\ndistributed data. However, such capabilities create new security and privacy\nchallenges. First, the user-submitted code may potentially contain malicious\ncode to circumvent existing security checks. In addition, providing\nfine-grained access control for different types of data (e.g., text, images,\netc.) may not be feasible for different data storage options. To address these\nchallenges, we provide a fine-grained access control framework tailored for\ndistributed data analytics platforms, which is protected against evasion\nattacks with two distinct layers of defense. Access control is implemented with\nruntime injection of access control logic on a submitted data analysis job. The\nproactive security layer utilizes state-of-the-art program analysis to detect\npotentially malicious user code. The reactive security layer consists of binary\nintegrity checking, instrumentation-based runtime checks, and sandboxed\nexecution. To the best of our knowledge, this is the first work that provides\nfine-grained attribute-based access control for distributed data analytics\nplatforms using code rewriting and static program analysis. Furthermore, we\nevaluated the performance of our security system under different settings and\nshow that the performance overhead due to added security is low.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 15:57:12 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 03:27:41 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Shaon", "Fahad", ""], ["Rahaman", "Sazzadur", ""], ["Kantarcioglu", "Murat", ""]]}, {"id": "2106.13203", "submitter": "Rakshit Naidu", "authors": "Rakshit Naidu, Aman Priyanshu, Aadith Kumar, Sasikanth Kotti, Haofan\n  Wang, Fatemehsadat Mireshghallah", "title": "When Differential Privacy Meets Interpretability: A Case Study", "comments": "4 pages, 7 figures; Extended abstract presented at RCV-CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the increase in the use of personal data for training Deep Neural\nNetworks (DNNs) in tasks such as medical imaging and diagnosis, differentially\nprivate training of DNNs is surging in importance and there is a large body of\nwork focusing on providing better privacy-utility trade-off. However, little\nattention is given to the interpretability of these models, and how the\napplication of DP affects the quality of interpretations. We propose an\nextensive study into the effects of DP training on DNNs, especially on medical\nimaging applications, on the APTOS dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:32:45 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 06:34:06 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Naidu", "Rakshit", ""], ["Priyanshu", "Aman", ""], ["Kumar", "Aadith", ""], ["Kotti", "Sasikanth", ""], ["Wang", "Haofan", ""], ["Mireshghallah", "Fatemehsadat", ""]]}, {"id": "2106.13263", "submitter": "Francesco Restuccia", "authors": "Francesco Restuccia, Andres Meza, and Ryan Kastner", "title": "AKER: A Design and Verification Framework for Safe andSecure SoC Access\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern systems on a chip (SoCs) utilize heterogeneous architectures where\nmultiple IP cores have concurrent access to on-chip shared resources. In\nsecurity-critical applications, IP cores have different privilege levels for\naccessing shared resources, which must be regulated by an access control\nsystem. AKER is a design and verification framework for SoC access control.\nAKER builds upon the Access Control Wrapper (ACW) -- a high performance and\neasy-to-integrate hardware module that dynamically manages access to shared\nresources. To build an SoC access control system, AKER distributes the ACWs\nthroughout the SoC, wrapping controller IP cores, and configuring the ACWs to\nperform local access control. To ensure the access control system is\nfunctioning correctly and securely, AKER provides a property-driven security\nverification using MITRE common weakness enumerations. AKER verifies the SoC\naccess control at the IP level to ensure the absence of bugs in the\nfunctionalities of the ACW module, at the firmware level to confirm the secure\noperation of the ACW when integrated with a hardware root-of-trust (HRoT), and\nat the system level to evaluate security threats due to the interactions among\nshared resources. The performance, resource usage, and security of access\ncontrol systems implemented through AKER is experimentally evaluated on a\nXilinx UltraScale+ programmable SoC, it is integrated with the OpenTitan\nhardware root-of-trust, and it is used to design an access control system for\nthe OpenPULP multicore architecture.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 18:09:28 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Restuccia", "Francesco", ""], ["Meza", "Andres", ""], ["Kastner", "Ryan", ""]]}, {"id": "2106.13339", "submitter": "Ziaur Rahman", "authors": "Ziaur Rahman, Ibrahim Khalil, Xun Yi and Mohammed Atiquzzaman", "title": "Blockchain-based Security Framework for Critical Industry 4.0\n  Cyber-physical System", "comments": "07 Pages, 4 Figures, IEEE Communication Magazine", "journal-ref": "in IEEE Communications Magazine, vol. 59, no. 5, pp. 128-134, May\n  2021", "doi": "10.1109/MCOM.001.2000679.", "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There has been an intense concern for security alternatives because of the\nrecent rise of cyber attacks, mainly targeting critical systems such as\nindustry, medical, or energy ecosystem. Though the latest industry\ninfrastructures largely depend on AI-driven maintenance, the prediction based\non corrupted data undoubtedly results in loss of life and capital. Admittedly,\nan inadequate data-protection mechanism can readily challenge the security and\nreliability of the network. The shortcomings of the conventional cloud or\ntrusted certificate-driven techniques have motivated us to exhibit a unique\nBlockchain-based framework for a secure and efficient industry 4.0 system. The\ndemonstrated framework obviates the long-established certificate authority\nafter enhancing the consortium Blockchain that reduces the data processing\ndelay, and increases cost-effective throughput. Nonetheless, the distributed\nindustry 4.0 security model entails cooperative trust than depending on a\nsingle party, which in essence indulges the costs and threat of the single\npoint of failure. Therefore, multi-signature technique of the proposed\nframework accomplishes the multi-party authentication, which confirms its\napplicability for the real-time and collaborative cyber-physical system.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 22:17:37 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Rahman", "Ziaur", ""], ["Khalil", "Ibrahim", ""], ["Yi", "Xun", ""], ["Atiquzzaman", "Mohammed", ""]]}, {"id": "2106.13420", "submitter": "Rachit Agarwal", "authors": "Rohit Kumar Sachan, Rachit Agarwal, Sandeep Kumar Shukla", "title": "Identifying malicious accounts in Blockchains using Domain Names and\n  associated temporal properties", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise in the adoption of blockchain technology has led to increased\nillegal activities by cyber-criminals costing billions of dollars. Many machine\nlearning algorithms are applied to detect such illegal behavior. These\nalgorithms are often trained on the transaction behavior and, in some cases,\ntrained on the vulnerabilities that exist in the system. In our approach, we\nstudy the feasibility of using metadata such as Domain Name (DN) associated\nwith the account in the blockchain and identify whether an account should be\ntagged malicious or not. Here, we leverage the temporal aspects attached to the\nDNs. Our results identify 144930 DNs that show malicious behavior, and out of\nthese, 54114 DNs show persistent malicious behavior over time. Nonetheless,\nnone of these identified malicious DNs were reported in new officially tagged\nmalicious blockchain DNs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 04:19:14 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Sachan", "Rohit Kumar", ""], ["Agarwal", "Rachit", ""], ["Shukla", "Sandeep Kumar", ""]]}, {"id": "2106.13422", "submitter": "Rachit Agarwal", "authors": "Rachit Agarwal, Tanmay Thapliyal, Sandeep Kumar Shukla", "title": "Vulnerability and Transaction behavior based detection of Malicious\n  Smart Contracts", "comments": "Submitted to a conf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart Contracts (SCs) in Ethereum can automate tasks and provide different\nfunctionalities to a user. Such automation is enabled by the `Turing-complete'\nnature of the programming language (Solidity) in which SCs are written. This\nalso opens up different vulnerabilities and bugs in SCs that malicious actors\nexploit to carry out malicious or illegal activities on the cryptocurrency\nplatform. In this work, we study the correlation between malicious activities\nand the vulnerabilities present in SCs and find that some malicious activities\nare correlated with certain types of vulnerabilities. We then develop and study\nthe feasibility of a scoring mechanism that corresponds to the severity of the\nvulnerabilities present in SCs to determine if it is a relevant feature to\nidentify suspicious SCs. We analyze the utility of severity score towards\ndetection of suspicious SCs using unsupervised machine learning (ML) algorithms\nacross different temporal granularities and identify behavioral changes. In our\nexperiments with on-chain SCs, we were able to find a total of 1094 benign SCs\nacross different granularities which behave similar to malicious SCs, with the\ninclusion of the smart contract vulnerability scores in the feature set.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 04:25:23 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Agarwal", "Rachit", ""], ["Thapliyal", "Tanmay", ""], ["Shukla", "Sandeep Kumar", ""]]}, {"id": "2106.13460", "submitter": "Qian Ren", "authors": "Qian Ren, Han Liu, Yue Li, Hong Lei", "title": "CLOAK: A Framework For Development of Confidential Blockchain Smart\n  Contracts", "comments": "accepted by ICDCS'21 Demo & Poster Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, as blockchain adoption has been expanding across a wide\nrange of domains, e.g., digital asset, supply chain finance, etc., the\nconfidentiality of smart contracts is now a fundamental demand for practical\napplications. However, while new privacy protection techniques keep coming out,\nhow existing ones can best fit development settings is little studied.\nSuffering from limited architectural support in terms of programming\ninterfaces, state-of-the-art solutions can hardly reach general developers. In\nthis paper, we proposed the CLOAK framework for developing confidential smart\ncontracts. The key capability of CLOAK is allowing developers to implement and\ndeploy practical solutions to multi-party transaction (MPT) problems, i.e.,\ntransact with secret inputs and states owned by different parties by simply\nspecifying it. To this end, CLOAK introduced a domain-specific annotation\nlanguage for declaring privacy specifications and further automatically\ngenerating confidential smart contracts to be deployed with trusted execution\nenvironment (TEE) on blockchain. In our evaluation on both simple and\nreal-world applications, developers managed to deploy business services on\nblockchain in a concise manner by only developing CLOAK smart contracts whose\nsize is less than 30% of the deployed ones.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 07:05:22 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Ren", "Qian", ""], ["Liu", "Han", ""], ["Li", "Yue", ""], ["Lei", "Hong", ""]]}, {"id": "2106.13478", "submitter": "Jukka Ruohonen", "authors": "Jukka Ruohonen, Joonas Salovaara, Ville Lepp\\\"anen", "title": "Crossing Cross-Domain Paths in the Current Web", "comments": "Proceedings of the 16th Annual Conference on Privacy, Security and\n  Trust (PST 2018), Belfast, IEEE, pp. 1-5", "journal-ref": null, "doi": "10.1109/PST.2018.8514163", "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The loading of resources from third-parties has evoked new security and\nprivacy concerns about the current world wide web. Building on the concepts of\nforced and implicit trust, this paper examines cross-domain transmission\ncontrol protocol (TCP) connections that are initiated to domains other than the\ndomain queried with a web browser. The dataset covers nearly ten thousand\ndomains and over three hundred thousand TCP connections initiated by querying\npopular Finnish websites and globally popular sites. According to the results,\n(i) cross-domain connections are extremely common in the current Web. (ii) Most\nof these transmit encrypted content, although mixed content delivery is\nrelatively common; many of the cross-domain connections deliver unencrypted\ncontent at the same time. (iii) Many of the cross-domain connections are\ninitiated to known web advertisement domains, but a much larger share traces to\nsocial media platforms and cloud infrastructures. Finally, (iv) the results\ndiffer slightly between the Finnish web sites sampled and the globally popular\nsites. With these results, the paper contributes to the ongoing work for better\nunderstanding cross-domain connections and dependencies in the world wide web.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 07:51:01 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Ruohonen", "Jukka", ""], ["Salovaara", "Joonas", ""], ["Lepp\u00e4nen", "Ville", ""]]}, {"id": "2106.13499", "submitter": "Stefan Marksteiner", "authors": "Christian Wolschke, Behrooz Sangchoolie, Jacob Simon, Stefan\n  Marksteiner, Tobias Braun, Hayk Hamazaryan", "title": "SaSeVAL: A Safety/Security-Aware Approach for Validation of\n  Safety-Critical Systems", "comments": "8 pages, 2 figures Presented at the 7th International Workshop on\n  Safety and Security of Intelligent Vehicles (SSIV+ 2021, held in conjunction\n  with DSN2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing communication and self-driving capabilities for road vehicles lead\nto threats imposed by attackers. Especially attacks leading to safety\nviolations have to be identified to address them by appropriate measures. The\nimpact of an attack depends on the threat exploited, potential countermeasures\nand the traffic situation. In order to identify such attacks and to use them\nfor testing, we propose the systematic approach SaSeVAL for deriving attacks of\nautonomous vehicles. SaSeVAL is based on threats identification and\nsafety-security analysis. The impact of automotive use cases to attacks is\nconsidered. The threat identification considers the attack interface of\nvehicles and classifies threat scenarios according to threat types, which are\nthen mapped to attack types. The safety-security analysis identifies the\nnecessary requirements which have to be tested based on the architecture of the\nsystem under test. lt determines which safety impact a security violation may\nhave, and in which traffic situations the highest impact is expected. Finally,\nthe results of threat identification and safety-security analysis are used to\ndescribe attacks. The goal of SaSeVAL is to achieve safety validation of the\nvehicle w.r.t. security concerns. lt traces safety goals to threats and to\nattacks explicitly. Hence, the coverage of safety concerns by security testing\nis assured. Two use cases of vehicle communication and autonomous driving are\ninvestigated to prove the applicability of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 08:36:07 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wolschke", "Christian", ""], ["Sangchoolie", "Behrooz", ""], ["Simon", "Jacob", ""], ["Marksteiner", "Stefan", ""], ["Braun", "Tobias", ""], ["Hamazaryan", "Hayk", ""]]}, {"id": "2106.13513", "submitter": "Roi Livni", "authors": "Noah Golowich and Roi Livni", "title": "Littlestone Classes are Privately Online Learnable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online classification under a privacy constraint.\nIn this setting a learner observes sequentially a stream of labelled examples\n$(x_t, y_t)$, for $1 \\leq t \\leq T$, and returns at each iteration $t$ a\nhypothesis $h_t$ which is used to predict the label of each new example $x_t$.\nThe learner's performance is measured by her regret against a known hypothesis\nclass $\\mathcal{H}$. We require that the algorithm satisfies the following\nprivacy constraint: the sequence $h_1, \\ldots, h_T$ of hypotheses output by the\nalgorithm needs to be an $(\\epsilon, \\delta)$-differentially private function\nof the whole input sequence $(x_1, y_1), \\ldots, (x_T, y_T)$. We provide the\nfirst non-trivial regret bound for the realizable setting. Specifically, we\nshow that if the class $\\mathcal{H}$ has constant Littlestone dimension then,\ngiven an oblivious sequence of labelled examples, there is a private learner\nthat makes in expectation at most $O(\\log T)$ mistakes -- comparable to the\noptimal mistake bound in the non-private case, up to a logarithmic factor.\nMoreover, for general values of the Littlestone dimension $d$, the same mistake\nbound holds but with a doubly-exponential in $d$ factor. A recent line of work\nhas demonstrated a strong connection between classes that are online learnable\nand those that are differentially-private learnable. Our results strengthen\nthis connection and show that an online learning algorithm can in fact be\ndirectly privatized (in the realizable setting). We also discuss an adaptive\nsetting and provide a sublinear regret bound of $O(\\sqrt{T})$.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:08:33 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Golowich", "Noah", ""], ["Livni", "Roi", ""]]}, {"id": "2106.13560", "submitter": "Yang Li", "authors": "Yang Li", "title": "Checking chordality on homomorphically encrypted graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The breakthrough of achieving fully homomorphic encryption sparked enormous\nstudies on where and how to apply homomorphic encryption schemes so that\noperations can be performed on encrypted data without the secret key while\nstill obtaining the correct outputs. Due to the computational cost, inflated\nciphertext size and limited arithmetic operations that are allowed in most\nencryption schemes, feasible applications of homomorphic encryption are few.\nWhile theorists are working on the mathematical and cryptographical foundations\nof homomorphic encryption in order to overcome the current limitations,\npractitioners are also re-designing queries and algorithms to adapt the\nfunctionalities of the current encryption schemes. As an initial study on\nworking with homomorphically encrypted graphs, this paper provides an\neasy-to-implement interactive algorithm to check whether or not a\nhomomorphically encrypted graph is chordal. This algorithm is simply a\nrefactoring of a current method to run on the encrypted adjacency matrices.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 11:06:51 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Li", "Yang", ""]]}, {"id": "2106.13646", "submitter": "Suthee Ruangwises", "authors": "Suthee Ruangwises", "title": "Two Standard Decks of Playing Cards are Sufficient for a ZKP for Sudoku", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sudoku is a logic puzzle with an objective to fill a number between 1 and 9\nin each empty cell of a $9 \\times 9$ grid such that every number appears\nexactly once in each row, each column, and each $3 \\times 3$ block. In 2020,\nSasaki et al. proposed a physical zero-knowledge proof (ZKP) protocol for\nSudoku using 90 cards, which allows a prover to physically show that he/she\nknows a solution without revealing it. However, their protocol requires nine\nidentical copies of some cards, which cannot be found in a standard deck of\nplaying cards. Therefore, nine decks of cards are actually required in order to\nperform that protocol. In this paper, we propose a new ZKP protocol for Sudoku\nthat can be performed using only two standard decks of playing cards. In\ngeneral, we develop the first ZKP protocol for an $n \\times n$ Sudoku that can\nbe performed using a deck of all different cards.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 14:03:36 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Ruangwises", "Suthee", ""]]}, {"id": "2106.13673", "submitter": "Xinwei Zhang", "authors": "Xinwei Zhang, Xiangyi Chen, Mingyi Hong, Zhiwei Steven Wu and Jinfeng\n  Yi", "title": "Understanding Clipping for Federated Learning: Convergence and\n  Client-Level Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing privacy protection has been one of the primary motivations of\nFederated Learning (FL). Recently, there has been a line of work on\nincorporating the formal privacy notion of differential privacy with FL. To\nguarantee the client-level differential privacy in FL algorithms, the clients'\ntransmitted model updates have to be clipped before adding privacy noise. Such\nclipping operation is substantially different from its counterpart of gradient\nclipping in the centralized differentially private SGD and has not been\nwell-understood. In this paper, we first empirically demonstrate that the\nclipped FedAvg can perform surprisingly well even with substantial data\nheterogeneity when training neural networks, which is partly because the\nclients' updates become similar for several popular deep architectures. Based\non this key observation, we provide the convergence analysis of a differential\nprivate (DP) FedAvg algorithm and highlight the relationship between clipping\nbias and the distribution of the clients' updates. To the best of our\nknowledge, this is the first work that rigorously investigates theoretical and\nempirical issues regarding the clipping operation in FL algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 14:47:19 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhang", "Xinwei", ""], ["Chen", "Xiangyi", ""], ["Hong", "Mingyi", ""], ["Wu", "Zhiwei Steven", ""], ["Yi", "Jinfeng", ""]]}, {"id": "2106.13693", "submitter": "Ziqing Wang", "authors": "Ziqing Wang, Robert Malaney, Ryan Aguinaldo", "title": "Temporal Modes of Light in Satellite-to-Earth Quantum Communications", "comments": "7 pages, 3 figures. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR cs.IT math.IT physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The photonic Temporal Mode (TM) represents a possible candidate for the\ndelivery of viable multidimensional quantum communications. However, relative\nto other multidimensional quantum information carriers such as the Orbital\nAngular Momentum (OAM), the TM has received less attention. Moreover, in the\ncontext of the emerging quantum internet and satellite-based quantum\ncommunications, the TM has received no attention. In this work, we remedy this\nsituation by considering the traversal through the satellite-to-Earth channel\nof single photons encoded in TM space. Our results indicate that for\nanticipated atmospheric conditions the photonic TM offers a promising avenue\nfor the delivery of high-throughput quantum communications from a satellite to\na terrestrial receiver. In particular, we show how these modes can provide for\nimproved multiplexing performance and superior quantum key distribution in the\nsatellite-to-Earth channel, relative to OAM single-photon states. The levels of\nTM discrimination that guarantee this outcome are outlined and implications of\nour results for the emerging satellite-based quantum internet are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:28:09 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Ziqing", ""], ["Malaney", "Robert", ""], ["Aguinaldo", "Ryan", ""]]}, {"id": "2106.13756", "submitter": "Alireza Fallah", "authors": "Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, Kunal Talwar", "title": "Private Adaptive Gradient Methods for Convex Optimization", "comments": "To appear in 38th International Conference on Machine Learning (ICML\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study adaptive methods for differentially private convex optimization,\nproposing and analyzing differentially private variants of a Stochastic\nGradient Descent (SGD) algorithm with adaptive stepsizes, as well as the\nAdaGrad algorithm. We provide upper bounds on the regret of both algorithms and\nshow that the bounds are (worst-case) optimal. As a consequence of our\ndevelopment, we show that our private versions of AdaGrad outperform adaptive\nSGD, which in turn outperforms traditional SGD in scenarios with non-isotropic\ngradients where (non-private) Adagrad provably outperforms SGD. The major\nchallenge is that the isotropic noise typically added for privacy dominates the\nsignal in gradient geometry for high-dimensional problems; approaches to this\nthat effectively optimize over lower-dimensional subspaces simply ignore the\nactual problems that varying gradient geometries introduce. In contrast, we\nstudy non-isotropic clipping and noise addition, developing a principled\ntheoretical approach; the consequent procedures also enjoy significantly\nstronger empirical performance than prior approaches.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:46:45 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Asi", "Hilal", ""], ["Duchi", "John", ""], ["Fallah", "Alireza", ""], ["Javidbakht", "Omid", ""], ["Talwar", "Kunal", ""]]}, {"id": "2106.13784", "submitter": "Yuan Yao", "authors": "Yuan Yao, Pantea Kiaei, Richa Singh, Shahin Tajik, Patrick Schaumont", "title": "Programmable RO (PRO): A Multipurpose Countermeasure against\n  Side-channel and Fault Injection Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Side-channel and fault injection attacks reveal secret information by\nmonitoring or manipulating the physical effects of computations involving\nsecret variables. Circuit-level countermeasures help to deter these attacks,\nand traditionally such countermeasures have been developed for each attack\nvector separately. We demonstrate a multipurpose ring oscillator design -\nProgrammable Ring Oscillator (PRO) to address both fault attacks and\nside-channel attacks in a generic, application-independent manner. PRO, as an\nintegrated primitive, can provide on-chip side-channel resistance, power\nmonitoring, and fault detection capabilities to a secure design. We present a\ngrid of PROs monitoring the on-chip power network to detect anomalies. Such\npower anomalies may be caused by external factors such as electromagnetic fault\ninjection and power glitches, as well as by internal factors such as hardware\nTrojans. By monitoring the frequency of the ring oscillators, we are able to\ndetect the on-chip power anomaly in time as well as in location. Moreover, we\nshow that the PROs can also inject a random noise pattern into a design's power\nconsumption. By randomly switching the frequency of a ring oscillator, the\nresulting power-noise pattern significantly reduces the power-based\nside-channel leakage of a cipher. We discuss the design of PRO and present\nmeasurement results on a Xilinx Spartan-6 FPGA prototype, and we show that\nside-channel and fault vulnerabilities can be addressed at a low cost by\nintroducing PRO to the design. We conclude that PRO can serve as an\napplication-independent, multipurpose countermeasure.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 17:38:22 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Yao", "Yuan", ""], ["Kiaei", "Pantea", ""], ["Singh", "Richa", ""], ["Tajik", "Shahin", ""], ["Schaumont", "Patrick", ""]]}, {"id": "2106.13926", "submitter": "Qian Ren", "authors": "Qian Ren, Han Liu, Yue Li, Hong Lei, Lei Wang, Bangdao Chen", "title": "CLOAK: Towards Practical Development and Deployment of Confidential\n  Smart Contracts", "comments": "submited. arXiv admin note: text overlap with arXiv:2106.13460", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, as blockchain adoption has been expanding across a wide\nrange of domains, e.g., supply chain finance, digital asset, etc., the\nconfidentiality of smart contracts is now a fundamental demand for practical\napplications. However, while new privacy protection techniques keep coming out,\nhow existing ones can best fit development settings is little studied.\nState-of-the-art solutions suffer from limited architectural support in terms\nof programming interfaces, thus can hardly reach general developers. This paper\nproposes Cloak, a pluggable and configurable framework for developing and\ndeploying confidential smart contracts. The key capability of Cloakis allowing\ndevelopers to implement and deploy practical solutions to multi-party\ntransaction (MPT) problems, i.e., transact with secret inputs and states owned\nby different parties, by simply specifying it. To this end, Cloak allows users\nto specify privacy invariants in a declarative way, automatically generate\nruntime with verifiably enforced privacy and deploy it to the existing\nplatforms with TEE-Blockchain architecture to enable the MPT. In our evaluation\non both examples and real-world applications, developers manage to deploy\nbusiness services on blockchain in a concise manner by only developing Cloak\nsmart contracts whose size is less than 30% of the deployed ones and the gas\ncost of deployed MPTs reduces 19%.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 02:22:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ren", "Qian", ""], ["Liu", "Han", ""], ["Li", "Yue", ""], ["Lei", "Hong", ""], ["Wang", "Lei", ""], ["Chen", "Bangdao", ""]]}, {"id": "2106.13958", "submitter": "Jingwei Ye", "authors": "Jingwei Ye, Xin Kang, Ying-Chang Liang, Sumei Sun", "title": "A Trust-Centric Privacy-Preserving Blockchain for Dynamic Spectrum\n  Management in IoT Networks", "comments": "15pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a trust-centric privacy-preserving blockchain for\ndynamic spectrum access in IoT networks. To be specific, we propose a trust\nevaluation mechanism to evaluate the trustworthiness of sensing nodes and\ndesign a Proof-of-Trust (PoT) consensus mechanism to build a scalable\nblockchain with high transaction-per-second (TPS). Moreover, a privacy\nprotection scheme is proposed to protect sensors' real-time geolocatioin\ninformation when they upload sensing data to the blockchain. Two smart\ncontracts are designed to make the whole procedure (spectrum sensing, spectrum\nauction, and spectrum allocation) run automatically. Simulation results\ndemonstrate the expected computation cost of the PoT consensus algorithm for\nreliable sensing nodes is low, and the cooperative sensing performance is\nimproved with the help of trust value evaluation mechanism.In addition,\nincentivization and security are also analyzed, which show that our design not\nonly can encourage nodes' participation, but also resist to many kinds of\nattacks which are frequently encountered in trust-based blockchain systems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 07:09:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ye", "Jingwei", ""], ["Kang", "Xin", ""], ["Liang", "Ying-Chang", ""], ["Sun", "Sumei", ""]]}, {"id": "2106.13973", "submitter": "Rakshit Naidu", "authors": "Priyam Basu, Tiasa Singha Roy, Rakshit Naidu, Zumrut Muftuoglu, Sahib\n  Singh, Fatemehsadat Mireshghallah", "title": "Benchmarking Differential Privacy and Federated Learning for BERT Models", "comments": "4 pages, 3 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Processing (NLP) techniques can be applied to help with the\ndiagnosis of medical conditions such as depression, using a collection of a\nperson's utterances. Depression is a serious medical illness that can have\nadverse effects on how one feels, thinks, and acts, which can lead to emotional\nand physical problems. Due to the sensitive nature of such data, privacy\nmeasures need to be taken for handling and training models with such data. In\nthis work, we study the effects that the application of Differential Privacy\n(DP) has, in both a centralized and a Federated Learning (FL) setup, on\ntraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).\nWe offer insights on how to privately train NLP models and what architectures\nand setups provide more desirable privacy utility trade-offs. We envisage this\nwork to be used in future healthcare and mental health studies to keep medical\nhistory private. Therefore, we provide an open-source implementation of this\nwork.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 08:52:02 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Basu", "Priyam", ""], ["Roy", "Tiasa Singha", ""], ["Naidu", "Rakshit", ""], ["Muftuoglu", "Zumrut", ""], ["Singh", "Sahib", ""], ["Mireshghallah", "Fatemehsadat", ""]]}, {"id": "2106.13997", "submitter": "Ivan Y. Tyukin", "authors": "Ivan Y. Tyukin, Desmond J. Higham, Eliyas Woldegeorgis, Alexander N.\n  Gorban", "title": "The Feasibility and Inevitability of Stealth Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop and study new adversarial perturbations that enable an attacker to\ngain control over decisions in generic Artificial Intelligence (AI) systems\nincluding deep learning neural networks. In contrast to adversarial data\nmodification, the attack mechanism we consider here involves alterations to the\nAI system itself. Such a stealth attack could be conducted by a mischievous,\ncorrupt or disgruntled member of a software development team. It could also be\nmade by those wishing to exploit a \"democratization of AI\" agenda, where\nnetwork architectures and trained parameter sets are shared publicly. Building\non work by [Tyukin et al., International Joint Conference on Neural Networks,\n2020], we develop a range of new implementable attack strategies with\naccompanying analysis, showing that with high probability a stealth attack can\nbe made transparent, in the sense that system performance is unchanged on a\nfixed validation set which is unknown to the attacker, while evoking any\ndesired output on a trigger input of interest. The attacker only needs to have\nestimates of the size of the validation set and the spread of the AI's relevant\nlatent space. In the case of deep learning neural networks, we show that a one\nneuron attack is possible - a modification to the weights and bias associated\nwith a single neuron - revealing a vulnerability arising from\nover-parameterization. We illustrate these concepts in a realistic setting.\nGuided by the theory and computational results, we also propose strategies to\nguard against stealth attacks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 10:50:07 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Tyukin", "Ivan Y.", ""], ["Higham", "Desmond J.", ""], ["Woldegeorgis", "Eliyas", ""], ["Gorban", "Alexander N.", ""]]}, {"id": "2106.14020", "submitter": "Suthee Ruangwises", "authors": "Suthee Ruangwises", "title": "An Improved Physical ZKP for Nonogram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonogram is a logic puzzle consisting of a rectangular grid with an objective\nto color every cell black or white such that the lengths of blocks of\nconsecutive black cells in each row and column are equal to the given numbers.\nIn 2010, Chien and Hon developed the first physical zero-knowledge proof for\nNonogram, which allows a prover to physically show that he/she knows a solution\nof the puzzle without revealing it. However, their protocol requires special\ntools such as scratch-off cards and a machine to seal the cards, which are\ndifficult to find in everyday life. Their protocol also has a nonzero soundness\nerror. In this paper, we propose a more practical physical zero-knowledge proof\nfor Nonogram that uses only a deck of regular paper cards and also has perfect\nsoundness.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 13:32:25 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ruangwises", "Suthee", ""]]}, {"id": "2106.14054", "submitter": "Shuwen Deng", "authors": "Shuwen Deng, Nikolay Matyunin, Wenjie Xiong, Stefan Katzenbeisser,\n  Jakub Szefer", "title": "Evaluation of Cache Attacks on Arm Processors and Secure Caches", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timing-based side and covert channels in processor caches continue to be a\nthreat to modern computers. This work shows for the first time a systematic,\nlarge-scale analysis of Arm devices and the detailed results of attacks the\nprocessors are vulnerable to. Compared to x86, Arm uses different\narchitectures, microarchitectural implementations, cache replacement policies,\netc., which affects how attacks can be launched, and how security testing for\nthe vulnerabilities should be done. To evaluate security, this paper presents\nsecurity benchmarks specifically developed for testing Arm processors and their\ncaches. The benchmarks are themselves evaluated with sensitivity tests, which\nexamine how sensitive the benchmarks are to having a correct configuration in\nthe testing phase. Further, to evaluate a large number of devices, this work\nleverages a novel approach of using a cloud-based Arm device testbed for\narchitectural and security research on timing channels and runs the benchmarks\non 34 different physical devices. In parallel, there has been much interest in\nsecure caches to defend the various attacks. Consequently, this paper also\ninvestigates secure cache architectures using the proposed benchmarks.\nEspecially, this paper implements and evaluates the secure PL and RF caches,\nshowing the security of PL and RF caches, but also uncovers new weaknesses.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 16:09:47 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Deng", "Shuwen", ""], ["Matyunin", "Nikolay", ""], ["Xiong", "Wenjie", ""], ["Katzenbeisser", "Stefan", ""], ["Szefer", "Jakub", ""]]}, {"id": "2106.14058", "submitter": "Michael M\\\"uhlhauser", "authors": "Michael M\\\"uhlhauser, Henning Prid\\\"ohl, Dominik Herrmann", "title": "How Private is Android's Private DNS Setting? Identifying Apps by\n  Encrypted DNS Traffic", "comments": "Accepted at The 16th International Conference on Availability,\n  Reliability and Security (ARES 2021)", "journal-ref": null, "doi": "10.1145/3465481.3465764", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNS over TLS (DoT) and DNS over HTTPS (DoH) promise to improve privacy and\nsecurity of DNS by encrypting DNS messages, especially when messages are padded\nto a uniform size. Firstly, to demonstrate the limitations of recommended\npadding approaches, we present Segram, a novel app fingerprinting attack that\nallows adversaries to infer which mobile apps are executed on a device.\nSecondly, we record traffic traces of 118 Android apps using 10 different\nDoT/DoH resolvers to study the effectiveness of Segram under different\nconditions. According to our results, Segram identifies apps with accuracies of\nup to 72% with padding in a controlled closed world setting. The effectiveness\nof Segram is comparable with state-of-the-art techniques but Segram requires\nless computational effort. We release our datasets and code. Thirdly, we study\nthe prevalence of padding among privacy-focused DoT/DoH resolvers, finding that\nup to 81% of our sample fail to enable padding. Our results suggest that\nrecommended padding approaches are less effective than expected and that\nresolver operators are not sufficiently aware about this feature.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 16:34:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["M\u00fchlhauser", "Michael", ""], ["Prid\u00f6hl", "Henning", ""], ["Herrmann", "Dominik", ""]]}, {"id": "2106.14139", "submitter": "Zhongyun Hua", "authors": "Zhongyun Hua and Yanxiang Wang and Shuang Yi and Yicong Zhou and\n  Xiaohua Jia", "title": "Secure Reversible Data Hiding in Encrypted Images Using Cipher-Feedback\n  Secret Sharing", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible data hiding in encrypted images (RDH-EI) has attracted increasing\nattention, since it can protect the privacy of original images while the\nembedded data can be exactly extracted. Recently, some RDH-EI schemes with\nmultiple data hiders have been proposed using secret sharing technique.\nHowever, these schemes protect the contents of the original images with\nlightweight security level. In this paper, we propose a high-security RDH-EI\nscheme with multiple data hiders. First, we introduce a cipher-feedback secret\nsharing (CFSS) technique. It follows the cryptography standards by introducing\nthe cipher-feedback strategy of AES. Then, using the CFSS technique, we devise\na new (r,n)-threshold (r<=n) RDH-EI scheme with multiple data hiders called\nCFSS-RDHEI. It can encrypt an original image into n encrypted images with\nreduced size using an encryption key and sends each encrypted image to one data\nhider. Each data hider can independently embed secret data into the encrypted\nimage to obtain the corresponding marked encrypted image. The original image\ncan be completely recovered from r marked encrypted images and the encryption\nkey. Performance evaluations show that our CFSS-RDHEI scheme has high embedding\nrate and its generated encrypted images are much smaller, compared to existing\nsecret sharing-based RDH-EI schemes. Security analysis demonstrates that it can\nachieve high security to defense some commonly used security attacks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 04:03:56 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hua", "Zhongyun", ""], ["Wang", "Yanxiang", ""], ["Yi", "Shuang", ""], ["Zhou", "Yicong", ""], ["Jia", "Xiaohua", ""]]}, {"id": "2106.14149", "submitter": "Xu Wang Dr", "authors": "Xu Wang (1), Wei Ni (2), Xuan Zha (3), Guangsheng Yu (1), Ren Ping Liu\n  (1), Nektarios Georgalas (4), Andrew Reeves (4) ((1) Global Big Data\n  Technologies Centre, University of Technology Sydney, Australia, (2) Data61,\n  CSIRO, Australia, (3) China Academy of Information and Communications\n  Technology, Beijing, China, (4) Applied Research, British Telecom,\n  Martlesham, UK)", "title": "Capacity Analysis of Public Blockchain", "comments": null, "journal-ref": null, "doi": "10.1016/j.comcom.2021.06.019", "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As distributed ledgers, blockchains run consensus protocols which trade\ncapacity for consistency, especially in non-ideal networks with incomplete\nconnectivity and erroneous links. Existing studies on the tradeoff between\ncapacity and consistency are only qualitative or rely on specific assumptions.\nThis paper presents discrete-time Markov chain models to quantify the capacity\nof Proof-of-Work based public blockchains in non-ideal networks. The\ncomprehensive model is collapsed to be ergodic under the eventual consistency\nof blockchains, achieving tractability and efficient evaluations of blockchain\ncapacity. A closed-form expression for the capacity is derived in the case of\ntwo miners. Another important aspect is that we extend the ergodic model to\nanalyze the capacity under strong consistency, evaluating the robustness of\nblockchains against double-spending attacks. Validated by simulations, the\nproposed models are accurate and reveal the effect of link quality and the\ndistribution of mining rates on blockchain capacity and the ratio of stale\nblocks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 05:38:13 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Xu", ""], ["Ni", "Wei", ""], ["Zha", "Xuan", ""], ["Yu", "Guangsheng", ""], ["Liu", "Ren Ping", ""], ["Georgalas", "Nektarios", ""], ["Reeves", "Andrew", ""]]}, {"id": "2106.14152", "submitter": "Kishor Datta Gupta", "authors": "Kishor Datta Gupta, Dipankar Dasgupta", "title": "Who is Responsible for Adversarial Defense?", "comments": "Accepted for poster presentation in ICML 2021 workshop \"Challenges in\n  Deploying and monitoring Machine Learning Systems\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have seen a surge in research aims toward adversarial attacks and defenses\nin AI/ML systems. While it is crucial to formulate new attack methods and\ndevise novel defense strategies for robustness, it is also imperative to\nrecognize who is responsible for implementing, validating, and justifying the\nnecessity of these defenses. In particular, which components of the system are\nvulnerable to what type of adversarial attacks, and the expertise needed to\nrealize the severity of adversarial attacks. Also how to evaluate and address\nthe adversarial challenges in order to recommend defense strategies for\ndifferent applications. This paper opened a discussion on who should examine\nand implement the adversarial defenses and the reason behind such efforts.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 06:09:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gupta", "Kishor Datta", ""], ["Dasgupta", "Dipankar", ""]]}, {"id": "2106.14180", "submitter": "Mohsen Rahnamaei", "authors": "Mohsen Rahnamaei, Saeid Tousi Saeidi, Siavash Khorsandi and Mehdi\n  Shajari", "title": "A Fair Model of Identity Information Exchange Leveraging Zero-Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many companies use identity information for different goals. There are a lot\nof marketplaces for identity information. These markets have some practical\nissues such as privacy, mutual trust and fairing exchange. The management of\nidentity information is one of the most important applications for blockchain,\nfor which researchers have proposed a large number of models. In the present\npaper, an attempt has been made to solve the problems that mentioned earlier to\nexchange identity information on the blockchain. By using the game theory we\npropose a fair model of selling authorized identity information in an\nenvironment that include untrusted parties. Moreover, we employ ZK-SNARK to\nprotect users' privacy. Also, we use proxy re-encryption to record these\ninformations in IPFS.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 09:54:02 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Rahnamaei", "Mohsen", ""], ["Saeidi", "Saeid Tousi", ""], ["Khorsandi", "Siavash", ""], ["Shajari", "Mehdi", ""]]}, {"id": "2106.14191", "submitter": "Dominika Woszczyk", "authors": "Dominika Woszczyk and Alvin Lee and Soteris Demetriou", "title": "Open, Sesame! Introducing Access Control to Voice Services", "comments": "6 pages, to appear in the 1st Workshop on Security and Privacy for\n  Mobile AI (MAISP'21) - co-located with ACM MobiSys 2021", "journal-ref": null, "doi": "10.1145/3469261.3469405", "report-no": null, "categories": "cs.CR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personal voice assistants (VAs) are shown to be vulnerable against\nrecord-and-replay, and other acoustic attacks which allow an adversary to gain\nunauthorized control of connected devices within a smart home. Existing\ndefenses either lack detection and management capabilities or are too\ncoarse-grained to enable flexible policies on par with other computing\ninterfaces. In this work, we present Sesame, a lightweight framework for edge\ndevices which is the first to enable fine-grained access control of smart-home\nvoice commands. Sesame combines three components: Automatic Speech Recognition,\nNatural Language Understanding (NLU) and a Policy module. We implemented Sesame\non Android devices and demonstrate that our system can enforce security\npolicies for both Alexa and Google Home in real-time (362ms end-to-end\ninference time), with a lightweight (<25MB) NLU model which exhibits minimal\naccuracy loss compared to its non-compact equivalent.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 10:34:48 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Woszczyk", "Dominika", ""], ["Lee", "Alvin", ""], ["Demetriou", "Soteris", ""]]}, {"id": "2106.14253", "submitter": "Wei Sun", "authors": "Wenxiu Ding, Wei Sun, Zheng Yan, Robert H. Deng", "title": "An efficient and secure scheme of verifiable computation for Intel SGX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing offers resource-constrained users big-volume data storage and\nenergy-consuming complicated computation. However, owing to the lack of full\ntrust in the cloud, the cloud users prefer privacy-preserving outsourced data\ncomputation with correctness verification. However, cryptography-based schemes\nintroduce high computational costs to both the cloud and its users for\nverifiable computation with privacy preservation, which makes it difficult to\nsupport complicated computations in practice.\n  Intel Software Guard Extensions (SGX) as a trusted execution environment is\nwidely researched in various fields (such as secure data analytics and\ncomputation), and is regarded as a promising way to achieve efficient\noutsourced data computation with privacy preservation over the cloud. But we\nfind two types of threats towards the computation with SGX: Disarranging\nData-Related Code threat and Output Tampering and Misrouting threat. In this\npaper, we depict these threats using formal methods and successfully conduct\nthe two threats on the enclave program constructed by Rust SGX SDK to\ndemonstrate their impacts on the correctness of computations over SGX enclaves.\nIn order to provide countermeasures, we propose an efficient and secure scheme\nto resist the threats and realize verifiable computation for Intel SGX. We\nprove the security and show the efficiency and correctness of our proposed\nscheme through theoretic analysis and extensive experiments. Furthermore, we\ncompare the performance of our scheme with that of some cryptography-based\nschemes to show its high efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:16:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ding", "Wenxiu", ""], ["Sun", "Wei", ""], ["Yan", "Zheng", ""], ["Deng", "Robert H.", ""]]}, {"id": "2106.14265", "submitter": "Leon Witt", "authors": "Leon Witt, Usama Zafar, KuoYeh Shen, Felix Sattler, Dan Li, Wojciech\n  Samek", "title": "Reward-Based 1-bit Compressed Federated Distillation on Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advent of various forms of Federated Knowledge Distillation (FD)\npaves the way for a new generation of robust and communication-efficient\nFederated Learning (FL), where mere soft-labels are aggregated, rather than\nwhole gradients of Deep Neural Networks (DNN) as done in previous FL schemes.\nThis security-per-design approach in combination with increasingly performant\nInternet of Things (IoT) and mobile devices opens up a new realm of\npossibilities to utilize private data from industries as well as from\nindividuals as input for artificial intelligence model training. Yet in\nprevious FL systems, lack of trust due to the imbalance of power between\nworkers and a central authority, the assumption of altruistic worker\nparticipation and the inability to correctly measure and compare contributions\nof workers hinder this technology from scaling beyond small groups of already\nentrusted entities towards mass adoption. This work aims to mitigate the\naforementioned issues by introducing a novel decentralized federated learning\nframework where heavily compressed 1-bit soft-labels, resembling 1-hot label\npredictions, are aggregated on a smart contract. In a context where workers'\ncontributions are now easily comparable, we modify the Peer Truth Serum for\nCrowdsourcing mechanism (PTSC) for FD to reward honest participation based on\npeer consistency in an incentive compatible fashion. Due to heavy reductions of\nboth computational complexity and storage, our framework is a fully\non-blockchain FL system that is feasible on simple smart contracts and\ntherefore blockchain agnostic. We experimentally test our new framework and\nvalidate its theoretical properties.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:51:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Witt", "Leon", ""], ["Zafar", "Usama", ""], ["Shen", "KuoYeh", ""], ["Sattler", "Felix", ""], ["Li", "Dan", ""], ["Samek", "Wojciech", ""]]}, {"id": "2106.14273", "submitter": "Hasan Ali Khattak", "authors": "Sidra Zafar, Mohsin Nazir, Taimur Bakhshi, Hasan Ali Khattak,\n  Sarmadullah Khan, Muhammad Bilal, Kim-Kwang Raymond Choo, Kyung-Sup Kwak7,\n  Aneeqa Sabah", "title": "A Systematic Review of Bio-Cyber Interface Technologies and Security\n  Issues for Internet of Bio-Nano Things", "comments": "41 pages, 9 tables, 6 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3093442", "report-no": null, "categories": "cs.NI cs.CR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in synthetic biology and nanotechnology have contributed to the\ndesign of tools that can be used to control, reuse, modify, and re-engineer\ncells' structure, as well as enabling engineers to effectively use biological\ncells as programmable substrates to realize Bio-Nano Things (biological\nembedded computing devices). Bio-NanoThings are generally tiny, non-intrusive,\nand concealable devices that can be used for in-vivo applications such as\nintra-body sensing and actuation networks, where the use of artificial devices\ncan be detrimental. Such (nano-scale) devices can be used in various healthcare\nsettings such as continuous health monitoring, targeted drug delivery, and\nnano-surgeries. These services can also be grouped to form a collaborative\nnetwork (i.e., nanonetwork), whose performance can potentially be improved when\nconnected to higher bandwidth external networks such as the Internet, say via\n5G. However, to realize the IoBNT paradigm, it is also important to seamlessly\nconnect the biological environment with the technological landscape by having a\ndynamic interface design to convert biochemical signals from the human body\ninto an equivalent electromagnetic signal (and vice versa). This,\nunfortunately, risks the exposure of internal biological mechanisms to\ncyber-based sensing and medical actuation, with potential security and privacy\nimplications. This paper comprehensively reviews bio-cyber interface for IoBNT\narchitecture, focusing on bio-cyber interfacing options for IoBNT like\nbiologically inspired bio-electronic devices, RFID enabled implantable chips,\nand electronic tattoos. This study also identifies known and potential security\nand privacy vulnerabilities and mitigation strategies for consideration in\nfuture IoBNT designs and implementations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 16:37:11 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zafar", "Sidra", ""], ["Nazir", "Mohsin", ""], ["Bakhshi", "Taimur", ""], ["Khattak", "Hasan Ali", ""], ["Khan", "Sarmadullah", ""], ["Bilal", "Muhammad", ""], ["Choo", "Kim-Kwang Raymond", ""], ["Kwak7", "Kyung-Sup", ""], ["Sabah", "Aneeqa", ""]]}, {"id": "2106.14300", "submitter": "Ren Wang", "authors": "Ren Wang, Tianqi Chen, Philip Yao, Sijia Liu, Indika Rajapakse, Alfred\n  Hero", "title": "ASK: Adversarial Soft k-Nearest Neighbor Attack and Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Nearest Neighbor (kNN)-based deep learning methods have been applied to\nmany applications due to their simplicity and geometric interpretability.\nHowever, the robustness of kNN-based classification models has not been\nthoroughly explored and kNN attack strategies are underdeveloped. In this\npaper, we propose an Adversarial Soft kNN (ASK) loss to both design more\neffective kNN attack strategies and to develop better defenses against them.\nOur ASK loss approach has two advantages. First, ASK loss can better\napproximate the kNN's probability of classification error than objectives\nproposed in previous works. Second, the ASK loss is interpretable: it preserves\nthe mutual information between the perturbed input and the kNN of the\nunperturbed input. We use the ASK loss to generate a novel attack method called\nthe ASK-Attack (ASK-Atk), which shows superior attack efficiency and accuracy\ndegradation relative to previous kNN attacks. Based on the ASK-Atk, we then\nderive an ASK-Defense (ASK-Def) method that optimizes the worst-case training\nloss induced by ASK-Atk.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 17:58:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Ren", ""], ["Chen", "Tianqi", ""], ["Yao", "Philip", ""], ["Liu", "Sijia", ""], ["Rajapakse", "Indika", ""], ["Hero", "Alfred", ""]]}, {"id": "2106.14406", "submitter": "Nayan Saxena", "authors": "Robert Wu, Nayan Saxena, Rohan Jain", "title": "Poisoning the Search Space in Neural Architecture Search", "comments": "All authors contributed equally. Appears in AdvML Workshop @\n  ICML2021: A Blessing in Disguise: The Prospects and Perils of Adversarial\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has proven to be a highly effective problem-solving tool for\nobject detection and image segmentation across various domains such as\nhealthcare and autonomous driving. At the heart of this performance lies neural\narchitecture design which relies heavily on domain knowledge and prior\nexperience on the researchers' behalf. More recently, this process of finding\nthe most optimal architectures, given an initial search space of possible\noperations, was automated by Neural Architecture Search (NAS). In this paper,\nwe evaluate the robustness of one such algorithm known as Efficient NAS (ENAS)\nagainst data agnostic poisoning attacks on the original search space with\ncarefully designed ineffective operations. By evaluating algorithm performance\non the CIFAR-10 dataset, we empirically demonstrate how our novel search space\npoisoning (SSP) approach and multiple-instance poisoning attacks exploit design\nflaws in the ENAS controller to result in inflated prediction error rates for\nchild networks. Our results provide insights into the challenges to surmount in\nusing NAS for more adversarially robust architecture search.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 05:45:57 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wu", "Robert", ""], ["Saxena", "Nayan", ""], ["Jain", "Rohan", ""]]}, {"id": "2106.14418", "submitter": "William Buchanan Prof", "authors": "Simon R Davies, Richard Macfarlane, William J Buchanan", "title": "Differential Area Analysis for Ransomware Attack Detection within Mixed\n  File Datasets", "comments": null, "journal-ref": "Computers & Security, 102377, 2021", "doi": "10.1016/j.cose.2021.102377", "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The threat from ransomware continues to grow both in the number of affected\nvictims as well as the cost incurred by the people and organisations impacted\nin a successful attack. In the majority of cases, once a victim has been\nattacked there remain only two courses of action open to them; either pay the\nransom or lose their data. One common behaviour shared between all crypto\nransomware strains is that at some point during their execution they will\nattempt to encrypt the users' files. Previous research Penrose et al. (2013);\nZhao et al. (2011) has highlighted the difficulty in differentiating between\ncompressed and encrypted files using Shannon entropy as both file types exhibit\nsimilar values. One of the experiments described in this paper shows a unique\ncharacteristic for the Shannon entropy of encrypted file header fragments. This\ncharacteristic was used to differentiate between encrypted files and other high\nentropy files such as archives. This discovery was leveraged in the development\nof a file classification model that used the differential area between the\nentropy curve of a file under analysis and one generated from random data. When\ncomparing the entropy plot values of a file under analysis against one\ngenerated by a file containing purely random numbers, the greater the\ncorrelation of the plots is, the higher the confidence that the file under\nanalysis contains encrypted data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 06:46:53 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Davies", "Simon R", ""], ["Macfarlane", "Richard", ""], ["Buchanan", "William J", ""]]}, {"id": "2106.14484", "submitter": "Stefan Marksteiner", "authors": "Stefan Marksteiner, Bernhard Jandl-Scherf, and Harald Lernbei{\\ss}", "title": "Automatically Determining a Network Reconnaissance Scope Using Passive\n  Scanning Techniques", "comments": "11 pages, 4 figures, presented at Fourth International Congress on\n  Information and Communication Technology (ICICT 2019), published by Springer", "journal-ref": "In: ICICT 2019. Adv. in Intelligent Systems and Computing, vol\n  1027. Springer (2020)", "doi": "10.1007/978-981-32-9343-4_11", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The starting point of securing a network is having a concise overview of it.\nAs networks are becoming more and more complex both in general and with the\nintroduction of IoT technology and their topological peculiarities in\nparticular, this is increasingly difficult to achieve. Especially in\ncyber-physical environments, such as smart factories, gaining a reliable\npicture of the network can be, due to intertwining of a vast amount of devices\nand different protocols, a tedious task. Nevertheless, this work is necessary\nto conduct security audits, compare documentation with actual conditions or\nfound vulnerabilities using an attacker's view, for all of which a reliable\ntopology overview is pivotal. For security auditors, however, there might not\nmuch information, such as asset management access, be available beforehand,\nwhich is why this paper assumes network to audit as a complete black box. The\ngoal is therefore to set security auditors in a condition of, without having\nany a priori knowledge at all, automatically gaining a topology oversight. This\npaper describes, in the context of a bigger system that uses active scanning to\ndetermine the network topology, an approach to automate the first steps of this\nprocedure: passively scanning the network and determining the network's scope,\nas well as gaining a valid address to perform the active scanning. This allows\nfor bootstrapping an automatic network discovery process without prior\nknowledge.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:53:24 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Marksteiner", "Stefan", ""], ["Jandl-Scherf", "Bernhard", ""], ["Lernbei\u00df", "Harald", ""]]}, {"id": "2106.14508", "submitter": "Nicola Nostro", "authors": "Massimiliano Leone Itria, Enrico Schiavone, Nicola Nostro", "title": "Towards anomaly detection in smart grids by combining Complex Events\n  Processing and SNMP objects", "comments": "Version submitted for review at the 2021 IEEE International\n  Conference on Cyber Security and Resilience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes the architecture and the fundamental methodology of an\nanomaly detector, which by continuously monitoring Simple Network Management\nProtocol data and by processing it as complex-events, is able to timely\nrecognize patterns of faults and relevant cyber-attacks. This solution has been\napplied in the context of smart grids, and in particular as part of a security\nand resilience component of the Information and Communication Technologies\n(ICT) Gateway, a middleware-based architecture that correlates and fuses\nmeasurement data from different sources (e.g., Inverters, Smart Meters) to\nprovide control coordination and to enable grid observability applications. The\ndetector has been evaluated through experiments, where we selected some\nrepresentative anomalies that can occur on the ICT side of the energy\ndistribution infrastructure: non-malicious faults (indicated by patterns in the\nsystem resources usage), as well as effects of typical cyber-attacks directed\nto the smart grid infrastructure. The results show that the detection is\npromisingly fast and efficient.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 09:46:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Itria", "Massimiliano Leone", ""], ["Schiavone", "Enrico", ""], ["Nostro", "Nicola", ""]]}, {"id": "2106.14577", "submitter": "Yamin Sepehri", "authors": "Yamin Sepehri, Pedram Pad, Pascal Frossard, L. Andrea Dunbar", "title": "Privacy-Preserving Image Acquisition Using Trainable Optical Kernel", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving privacy is a growing concern in our society where sensors and\ncameras are ubiquitous. In this work, for the first time, we propose a\ntrainable image acquisition method that removes the sensitive identity\nrevealing information in the optical domain before it reaches the image sensor.\nThe method benefits from a trainable optical convolution kernel which transmits\nthe desired information while filters out the sensitive content. As the\nsensitive content is suppressed before it reaches the image sensor, it does not\nenter the digital domain therefore is unretrievable by any sort of privacy\nattack. This is in contrast with the current digital privacy-preserving methods\nthat are all vulnerable to direct access attack. Also, in contrast with the\nprevious optical privacy-preserving methods that cannot be trained, our method\nis data-driven and optimized for the specific application at hand. Moreover,\nthere is no additional computation, memory, or power burden on the acquisition\nsystem since this processing happens passively in the optical domain and can\neven be used together and on top of the fully digital privacy-preserving\nsystems. The proposed approach is adaptable to different digital neural\nnetworks and content. We demonstrate it for several scenarios such as smile\ndetection as the desired attribute while the gender is filtered out as the\nsensitive content. We trained the optical kernel in conjunction with two\nadversarial neural networks where the analysis network tries to detect the\ndesired attribute and the adversarial network tries to detect the sensitive\ncontent. We show that this method can reduce 65.1% of sensitive content when it\nis selected to be the gender and it only loses 7.3% of the desired content.\nMoreover, we reconstruct the original faces using the deep reconstruction\nmethod that confirms the ineffectiveness of reconstruction attacks to obtain\nthe sensitive content.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 11:08:14 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sepehri", "Yamin", ""], ["Pad", "Pedram", ""], ["Frossard", "Pascal", ""], ["Dunbar", "L. Andrea", ""]]}, {"id": "2106.14631", "submitter": "Dinh Nguyen", "authors": "Dinh C. Nguyen, Ming Ding, Pubudu N. Pathirana, Aruna Seneviratne", "title": "Blockchain and AI-based Solutions to Combat Coronavirus (COVID-19)-like\n  Epidemics: A Survey", "comments": "Accepted at IEEE Access Journal, 24 pages", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3093633", "report-no": null, "categories": "cs.CR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The beginning of 2020 has seen the emergence of coronavirus outbreak caused\nby a novel virus called SARS-CoV-2. The sudden explosion and uncontrolled\nworldwide spread of COVID-19 show the limitations of existing healthcare\nsystems in timely handling public health emergencies. In such contexts,\ninnovative technologies such as blockchain and Artificial Intelligence (AI)\nhave emerged as promising solutions for fighting coronavirus epidemic. In\nparticular, blockchain can combat pandemics by enabling early detection of\noutbreaks, ensuring the ordering of medical data, and ensuring reliable medical\nsupply chain during the outbreak tracing. Moreover, AI provides intelligent\nsolutions for identifying symptoms caused by coronavirus for treatments and\nsupporting drug manufacturing. Therefore, we present an extensive survey on the\nuse of blockchain and AI for combating COVID-19 epidemics. First, we introduce\na new conceptual architecture which integrates blockchain and AI for fighting\nCOVID-19. Then, we survey the latest research efforts on the use of blockchain\nand AI for fighting COVID-19 in various applications. The newly emerging\nprojects and use cases enabled by these technologies to deal with coronavirus\npandemic are also presented. A case study is also provided using federated AI\nfor COVID-19 detection. Finally, we point out challenges and future directions\nthat motivate more research efforts to deal with future coronavirus-like\nepidemics.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:34:16 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nguyen", "Dinh C.", ""], ["Ding", "Ming", ""], ["Pathirana", "Pubudu N.", ""], ["Seneviratne", "Aruna", ""]]}, {"id": "2106.14647", "submitter": "Dattaraj Rao", "authors": "Dattaraj Rao, Shraddha Mane", "title": "Zero-shot learning approach to adaptive Cybersecurity using Explainable\n  AI", "comments": "arXiv admin note: substantial text overlap with arXiv:2103.07110", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cybersecurity is a domain where there is constant change in patterns of\nattack, and we need ways to make our Cybersecurity systems more adaptive to\nhandle new attacks and categorize for appropriate action. We present a novel\napproach to handle the alarm flooding problem faced by Cybersecurity systems\nlike security information and event management (SIEM) and intrusion detection\n(IDS). We apply a zero-shot learning method to machine learning (ML) by\nleveraging explanations for predictions of anomalies generated by a ML model.\nThis approach has huge potential to auto detect alarm labels generated in SIEM\nand associate them with specific attack types. In this approach, without any\nprior knowledge of attack, we try to identify it, decipher the features that\ncontribute to classification and try to bucketize the attack in a specific\ncategory - using explainable AI. Explanations give us measurable factors as to\nwhat features influence the prediction of a cyber-attack and to what degree.\nThese explanations generated based on game-theory are used to allocate credit\nto specific features based on their influence on a specific prediction. Using\nthis allocation of credit, we propose a novel zero-shot approach to categorize\nnovel attacks into specific new classes based on feature influence. The\nresulting system demonstrated will get good at separating attack traffic from\nnormal flow and auto-generate a label for attacks based on features that\ncontribute to the attack. These auto-generated labels can be presented to SIEM\nanalyst and are intuitive enough to figure out the nature of attack. We apply\nthis approach to a network flow dataset and demonstrate results for specific\nattack types like ip sweep, denial of service, remote to local, etc.\n  Paper was presented at the first Conference on Deployable AI at IIT-Madras in\nJune 2021.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:29:13 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Rao", "Dattaraj", ""], ["Mane", "Shraddha", ""]]}, {"id": "2106.14649", "submitter": "James Thomas Brown", "authors": "J. Thomas Brown, Chao Yan, Weiyi Xia, Zhijun Yin, Zhiyu Wan, Aris\n  Gkoulalas-Divanis, Murat Kantarcioglu, Bradley A. Malin", "title": "Dynamically Adjusting Case-Reporting Policy to Maximize Privacy and\n  Utility in the Face of a Pandemic", "comments": "68 pages, 18 figures/tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Managing a pandemic requires continuous dissemination of infectious disease\nsurveillance data. Legislation permits sharing de-identified patient data;\nhowever, current de-identification approaches are time-consuming and do not\nflex with changes in infection rates or population demographics over time. In\nthis paper, we introduce a framework to dynamically adapt de-identification for\nnear-real time sharing of patient-level surveillance data. The framework\nleverages a simulation mechanism, capable of being applied to any geographic\nlevel, to forecast and manage disclosure risks. We use data from Johns Hopkins\nUniversity and the Centers for Disease Control and Prevention to demonstrate\nthe framework's effectiveness in maintaining the privacy risk below a threshold\nbased on public health standards for COVID-19 county-level case data from\nAugust 2020 to April 2021. Across all US counties, the framework's approach\nmeets the threshold for 95.2% of daily data releases, while a policy based on\ncurrent de-identification techniques meets the threshold for only 24.6%.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:49:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Brown", "J. Thomas", ""], ["Yan", "Chao", ""], ["Xia", "Weiyi", ""], ["Yin", "Zhijun", ""], ["Wan", "Zhiyu", ""], ["Gkoulalas-Divanis", "Aris", ""], ["Kantarcioglu", "Murat", ""], ["Malin", "Bradley A.", ""]]}, {"id": "2106.14651", "submitter": "Sam Kumar", "authors": "Sam Kumar, David E. Culler, Raluca Ada Popa", "title": "MAGE: Nearly Zero-Cost Virtual Memory for Secure Computation", "comments": "19 pages; Accepted to OSDI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure Computation (SC) is a family of cryptographic primitives for computing\non encrypted data in single-party and multi-party settings. SC is being\nincreasingly adopted by industry for a variety of applications. A significant\nobstacle to using SC for practical applications is the memory overhead of the\nunderlying cryptography. We develop MAGE, an execution engine for SC that\nefficiently runs SC computations that do not fit in memory. We observe that,\ndue to their intended security guarantees, SC schemes are inherently oblivious\n-- their memory access patterns are independent of the input data. Using this\nproperty, MAGE calculates the memory access pattern ahead of time and uses it\nto produce a memory management plan. This formulation of memory management,\nwhich we call memory programming, is a generalization of paging that allows\nMAGE to provide a highly efficient virtual memory abstraction for SC. MAGE\noutperforms the OS virtual memory system by up to an order of magnitude, and in\nmany cases, runs SC computations that do not fit in memory at nearly the same\nspeed as if the underlying machines had unbounded physical memory to fit the\nentire computation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 23:44:27 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kumar", "Sam", ""], ["Culler", "David E.", ""], ["Popa", "Raluca Ada", ""]]}, {"id": "2106.14701", "submitter": "Jason R.C. Nurse Dr", "authors": "Betsy Uchendu and Jason R. C. Nurse and Maria Bada and Steven Furnell", "title": "Developing a cyber security culture: Current practices and future needs", "comments": null, "journal-ref": "Computers & Security, 2021", "doi": "10.1016/j.cose.2021.102387", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the creation of a strong security culture has been researched and\ndiscussed for decades, it continues to elude many businesses. Part of the\nchallenge faced is distilling pertinent, recent academic findings and research\ninto useful guidance. In this article, we aim to tackle this issue by\nconducting a state-of-the-art study into organisational cyber security culture\nresearch. This work investigates four questions, including how cyber security\nculture is defined, what factors are essential to building and maintaining such\na culture, the frameworks proposed to cultivate a security culture and the\nmetrics suggested to assess it. Through the application of the PRISMA\nsystematic literature review technique, we identify and analyse 58 research\narticles from the last 10 years (2010-2020). Our findings demonstrate that\nwhile there have been notable changes in the use of terms (e.g., information\nsecurity culture and cyber security culture), many of the most influential\nfactors across papers are similar. Top management support, policy and\nprocedures, and awareness for instance, are critical in engendering cyber\nsecurity culture. Many of the frameworks reviewed revealed common foundations,\nwith organisational culture playing a substantial role in crafting appropriate\ncyber security culture models. Questionnaires and surveys are the most used\ntool to measure cyber security culture, but there are also concerns as to\nwhether more dynamic measures are needed. For practitioners, this article\nhighlights factors and models essential to the creation and management of a\nrobust security culture. For research, we produce an up-to-date\ncharacterisation of the field and also define open issues deserving of further\nattention such as the role of change management processes and national culture\nin an enterprise's cyber security culture.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:31:33 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Uchendu", "Betsy", ""], ["Nurse", "Jason R. C.", ""], ["Bada", "Maria", ""], ["Furnell", "Steven", ""]]}, {"id": "2106.14707", "submitter": "Chuanpu Fu", "authors": "Chuanpu Fu, Qi Li, Meng Shen, Ke Xu", "title": "Realtime Robust Malicious Traffic Detection via Frequency Domain\n  Analysis", "comments": "To Appear in ACM CCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) based malicious traffic detection is an emerging\nsecurity paradigm, particularly for zero-day attack detection, which is\ncomplementary to existing rule based detection. However, the existing ML based\ndetection has low detection accuracy and low throughput incurred by inefficient\ntraffic features extraction. Thus, they cannot detect attacks in realtime\nespecially in high throughput networks. Particularly, these detection systems\nsimilar to the existing rule based detection can be easily evaded by\nsophisticated attacks. To this end, we propose Whisper, a realtime ML based\nmalicious traffic detection system that achieves both high accuracy and high\nthroughput by utilizing frequency domain features. It utilizes sequential\nfeatures represented by the frequency domain features to achieve bounded\ninformation loss, which ensures high detection accuracy, and meanwhile\nconstrains the scale of features to achieve high detection throughput.\nParticularly, attackers cannot easily interfere with the frequency domain\nfeatures and thus Whisper is robust against various evasion attacks. Our\nexperiments with 42 types of attacks demonstrate that, compared with the\nstate-of-theart systems, Whisper can accurately detect various sophisticated\nand stealthy attacks, achieving at most 18.36% improvement, while achieving two\norders of magnitude throughput. Even under various evasion attacks, Whisper is\nstill able to maintain around 90% detection accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:38:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fu", "Chuanpu", ""], ["Li", "Qi", ""], ["Shen", "Meng", ""], ["Xu", "Ke", ""]]}, {"id": "2106.14756", "submitter": "Wolfgang Ost", "authors": "Hendrik Fichtenberger, Monika Henzinger, Wolfgang Ost", "title": "Differentially Private Algorithms for Graphs Under Continual Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentially private algorithms protect individuals in data analysis\nscenarios by ensuring that there is only a weak correlation between the\nexistence of the user in the data and the result of the analysis. Dynamic graph\nalgorithms maintain the solution to a problem (e.g., a matching) on an evolving\ninput, i.e., a graph where nodes or edges are inserted or deleted over time.\nThey output the value of the solution after each update operation, i.e.,\ncontinuously. We study (event-level and user-level) differentially private\nalgorithms for graph problems under continual observation, i.e., differentially\nprivate dynamic graph algorithms. We present event-level private algorithms for\npartially dynamic counting-based problems such as triangle count that improve\nthe additive error by a polynomial factor (in the length $T$ of the update\nsequence) on the state of the art, resulting in the first algorithms with\nadditive error polylogarithmic in $T$.\n  We also give $\\varepsilon$-differentially private and partially dynamic\nalgorithms for minimum spanning tree, minimum cut, densest subgraph, and\nmaximum matching. The additive error of our improved MST algorithm is $O(W\n\\log^{3/2}T / \\varepsilon)$, where $W$ is the maximum weight of any edge,\nwhich, as we show, is tight up to a $(\\sqrt{\\log T} / \\varepsilon)$-factor. For\nthe other problems, we present a partially-dynamic algorithm with\nmultiplicative error $(1+\\beta)$ for any constant $\\beta > 0$ and additive\nerror $O(W \\log(nW) \\log(T) / (\\varepsilon \\beta) )$. Finally, we show that the\nadditive error for a broad class of dynamic graph algorithms with user-level\nprivacy must be linear in the value of the output solution's range.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:31:37 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fichtenberger", "Hendrik", ""], ["Henzinger", "Monika", ""], ["Ost", "Wolfgang", ""]]}, {"id": "2106.14760", "submitter": "Fabian Stiehle", "authors": "Fabian Stiehle, Erik Daniel, Florian Tschorsch", "title": "Modeling the Block Verification Time of Zcash", "comments": "Preprint, Submitted to IEEE S&B 2021 Security & Privacy on the\n  Blockchain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of the propagation delay in blockchain networks is the\nblock verification time, which is also responsible for the so-called verifier's\ndilemma. Models for the block verification time can help to understand and\nimprove the verification process. Moreover, modeling the verification time is\nnecessary for blockchain network simulations. In this paper, we present JOIST,\na new model for the block verification time of Zcash. We identify\ncomputationally complex operations in the verification process of Zcash, and\nderive our model based on characteristic transaction features. We evaluate\nJOIST and show that the model is consistently more accurate than existing\nmodels, which consider the block size only.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:35:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Stiehle", "Fabian", ""], ["Daniel", "Erik", ""], ["Tschorsch", "Florian", ""]]}, {"id": "2106.14763", "submitter": "Yin Yang", "authors": "Yin Yang", "title": "Training Massive Deep Neural Networks in a Smart Contract: A New Hope", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep neural networks (DNNs) could be very useful in blockchain applications\nsuch as DeFi and NFT trading. However, training / running large-scale DNNs as\npart of a smart contract is infeasible on today's blockchain platforms, due to\ntwo fundamental design issues of these platforms. First, blockchains nowadays\ntypically require that each node maintain the complete world state at any time,\nmeaning that the node must execute all transactions in every block. This is\nprohibitively expensive for computationally intensive smart contracts involving\nDNNs. Second, existing blockchain platforms expect smart contract transactions\nto have deterministic, reproducible results and effects. In contrast, DNNs are\nusually trained / run lock-free on massively parallel computing devices such as\nGPUs, TPUs and / or computing clusters, which often do not yield deterministic\nresults.\n  This paper proposes novel platform designs, collectively called A New Hope\n(ANH), that address the above issues. The main ideas are (i)\ncomputing-intensive smart contract transactions are only executed by nodes who\nneed their results, or by specialized serviced providers, and (ii) a\nnon-deterministic smart contract transaction leads to uncertain results, which\ncan still be validated, though at a relatively high cost; specifically for\nDNNs, the validation cost can often be reduced by verifying properties of the\nresults instead of their exact values. In addition, we discuss various\nimplications of ANH, including its effects on token fungibility, sharding,\nprivate transactions, and the fundamental meaning of a smart contract.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:38:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yang", "Yin", ""]]}, {"id": "2106.14815", "submitter": "Gilad Gressel", "authors": "Gilad Gressel, Niranjan Hegde, Archana Sreekumar, and Michael Darling", "title": "Feature Importance Guided Attack: A Model Agnostic Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models are susceptible to adversarial attacks which\ndramatically reduce their performance. Reliable defenses to these attacks are\nan unsolved challenge. In this work, we present a novel evasion attack: the\n'Feature Importance Guided Attack' (FIGA) which generates adversarial evasion\nsamples. FIGA is model agnostic, it assumes no prior knowledge of the defending\nmodel's learning algorithm, but does assume knowledge of the feature\nrepresentation. FIGA leverages feature importance rankings; it perturbs the\nmost important features of the input in the direction of the target class we\nwish to mimic. We demonstrate FIGA against eight phishing detection models. We\nkeep the attack realistic by perturbing phishing website features that an\nadversary would have control over. Using FIGA we are able to cause a reduction\nin the F1-score of a phishing detection model from 0.96 to 0.41 on average.\nFinally, we implement adversarial training as a defense against FIGA and show\nthat while it is sometimes effective, it can be evaded by changing the\nparameters of FIGA.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 15:46:22 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gressel", "Gilad", ""], ["Hegde", "Niranjan", ""], ["Sreekumar", "Archana", ""], ["Darling", "Michael", ""]]}, {"id": "2106.14851", "submitter": "Florian Tram\\`er", "authors": "Evani Radiya-Dixit, Florian Tram\\`er", "title": "Data Poisoning Won't Save You From Facial Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data poisoning has been proposed as a compelling defense against facial\nrecognition models trained on Web-scraped pictures. By perturbing the images\nthey post online, users can fool models into misclassifying future\n(unperturbed) pictures. We demonstrate that this strategy provides a false\nsense of security, as it ignores an inherent asymmetry between the parties:\nusers' pictures are perturbed once and for all before being published (at which\npoint they are scraped) and must thereafter fool all future models -- including\nmodels trained adaptively against the users' past attacks, or models that use\ntechnologies discovered after the attack. We evaluate two systems for poisoning\nattacks against large-scale facial recognition, Fawkes (500,000+ downloads) and\nLowKey. We demonstrate how an \"oblivious\" model trainer can simply wait for\nfuture developments in computer vision to nullify the protection of pictures\ncollected in the past. We further show that an adversary with black-box access\nto the attack can (i) train a robust model that resists the perturbations of\ncollected pictures and (ii) detect poisoned pictures uploaded online. We\ncaution that facial recognition poisoning will not admit an \"arms race\" between\nattackers and defenders. Once perturbed pictures are scraped, the attack cannot\nbe changed so any future successful defense irrevocably undermines users'\nprivacy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:06:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Radiya-Dixit", "Evani", ""], ["Tram\u00e8r", "Florian", ""]]}, {"id": "2106.14861", "submitter": "Zainul Abi Din", "authors": "Zainul Abi Din (1), Hari Venugopalan (1), Henry Lin (2), Adam\n  Wushensky (2), Steven Liu (2), Samuel T. King (1 and 2) ((1) University of\n  California, Davis, (2) Bouncer Technologies)", "title": "Doing good by fighting fraud: Ethical anti-fraud systems for mobile\n  payments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  App builders commonly use security challenges, a form of step-up\nauthentication, to add security to their apps. However, the ethical\nimplications of this type of architecture has not been studied previously. In\nthis paper, we present a large-scale measurement study of running an existing\nanti-fraud security challenge, Boxer, in real apps running on mobile devices.\nWe find that although Boxer does work well overall, it is unable to scan\neffectively on devices that run its machine learning models at less than one\nframe per second (FPS), blocking users who use inexpensive devices. With the\ninsights from our study, we design Daredevil, anew anti-fraud system for\nscanning payment cards that work swell across the broad range of performance\ncharacteristics and hardware configurations found on modern mobile devices.\nDaredevil reduces the number of devices that run at less than one FPS by an\norder of magnitude compared to Boxer, providing a more equitable system for\nfighting fraud. In total, we collect data from 5,085,444 real devices spread\nacross 496 real apps running production software and interacting with real\nusers.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:28:28 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 01:46:13 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Din", "Zainul Abi", "", "1 and 2"], ["Venugopalan", "Hari", "", "1 and 2"], ["Lin", "Henry", "", "1 and 2"], ["Wushensky", "Adam", "", "1 and 2"], ["Liu", "Steven", "", "1 and 2"], ["King", "Samuel T.", "", "1 and 2"]]}, {"id": "2106.14941", "submitter": "Firuz Kamalov", "authors": "Firuz Kamalov, Sherif Moussa, Rita Zgheib, Omar Mashaal", "title": "Feature selection for intrusion detection systems", "comments": "Accepted version of conference paper presented at ISCID 2020", "journal-ref": null, "doi": "10.1109/ISCID51228.2020.00065", "report-no": null, "categories": "cs.CR cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze existing feature selection methods to identify the\nkey elements of network traffic data that allow intrusion detection. In\naddition, we propose a new feature selection method that addresses the\nchallenge of considering continuous input features and discrete target values.\nWe show that the proposed method performs well against the benchmark selection\nmethods. We use our findings to develop a highly effective machine\nlearning-based detection systems that achieves 99.9% accuracy in distinguishing\nbetween DDoS and benign signals. We believe that our results can be useful to\nexperts who are interested in designing and building automated intrusion\ndetection systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 18:53:21 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kamalov", "Firuz", ""], ["Moussa", "Sherif", ""], ["Zgheib", "Rita", ""], ["Mashaal", "Omar", ""]]}, {"id": "2106.14962", "submitter": "Charalambos Konstantinou", "authors": "Charalambos Konstantinou, George Stergiopoulos, Masood Parvania, Paulo\n  Esteves-Verissimo", "title": "Chaos Engineering for Enhanced Resilience of Cyber-Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems (CPS) incorporate the complex and large-scale\nengineered systems behind critical infrastructure operations, such as water\ndistribution networks, energy delivery systems, healthcare services,\nmanufacturing systems, and transportation networks. Industrial CPS in\nparticular need to simultaneously satisfy requirements of available, secure,\nsafe and reliable system operation against diverse threats, in an adaptive and\nsustainable way. These adverse events can be of accidental or malicious nature\nand may include natural disasters, hardware or software faults, cyberattacks,\nor even infrastructure design and implementation faults. They may drastically\naffect the results of CPS algorithms and mechanisms, and subsequently the\noperations of industrial control systems (ICS) deployed in those critical\ninfrastructures. Such a demanding combination of properties and threats calls\nfor resilience-enhancement methodologies and techniques, working in real-time\noperation. However, the analysis of CPS resilience is a difficult task as it\ninvolves evaluation of various interdependent layers with heterogeneous\ncomputing equipment, physical components, network technologies, and data\nanalytics. In this paper, we apply the principles of chaos engineering (CE) to\nindustrial CPS, in order to demonstrate the benefits of such practices on\nsystem resilience. The systemic uncertainty of adverse events can be tamed by\napplying runtime CE-based analyses to CPS in production, in order to predict\nenvironment changes and thus apply mitigation measures limiting the range and\nseverity of the event, and minimizing its blast radius.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 20:02:10 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Konstantinou", "Charalambos", ""], ["Stergiopoulos", "George", ""], ["Parvania", "Masood", ""], ["Esteves-Verissimo", "Paulo", ""]]}, {"id": "2106.15015", "submitter": "Vincenzo DiLuoffo", "authors": "Vincenzo DiLuoffo, William R.Michalson", "title": "A Survey on Trust Metrics for Autonomous Robotic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper surveys the area of Trust Metrics related to security for\nautonomous robotic systems. As the robotics industry undergoes a transformation\nfrom programmed, task oriented, systems to Artificial Intelligence-enabled\nlearning, these autonomous systems become vulnerable to several security risks,\nmaking a security assessment of these systems of critical importance.\nTherefore, our focus is on a holistic approach for assessing system trust which\nrequires incorporating system, hardware, software, cognitive robustness, and\nsupplier level trust metrics into a unified model of trust. We set out to\ndetermine if there were already trust metrics that defined such a holistic\nsystem approach. While there are extensive writings related to various aspects\nof robotic systems such as, risk management, safety, security assurance and so\non, each source only covered subsets of an overall system and did not\nconsistently incorporate the relevant costs in their metrics. This paper\nattempts to put this prior work into perspective, and to show how it might be\nextended to develop useful system-level trust metrics for evaluating complex\nrobotic (and other) systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:56:54 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 23:44:27 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["DiLuoffo", "Vincenzo", ""], ["Michalson", "William R.", ""]]}, {"id": "2106.15023", "submitter": "Pedro Pachuca", "authors": "Oliver Bryniarski, Nabeel Hingun, Pedro Pachuca, Vincent Wang,\n  Nicholas Carlini", "title": "Evading Adversarial Example Detection Defenses with Orthogonal Projected\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evading adversarial example detection defenses requires finding adversarial\nexamples that must simultaneously (a) be misclassified by the model and (b) be\ndetected as non-adversarial. We find that existing attacks that attempt to\nsatisfy multiple simultaneous constraints often over-optimize against one\nconstraint at the cost of satisfying another. We introduce Orthogonal Projected\nGradient Descent, an improved attack technique to generate adversarial examples\nthat avoids this problem by orthogonalizing the gradients when running standard\ngradient-based attacks. We use our technique to evade four state-of-the-art\ndetection defenses, reducing their accuracy to 0% while maintaining a 0%\ndetection rate.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 23:19:02 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Bryniarski", "Oliver", ""], ["Hingun", "Nabeel", ""], ["Pachuca", "Pedro", ""], ["Wang", "Vincent", ""], ["Carlini", "Nicholas", ""]]}, {"id": "2106.15050", "submitter": "Mabrook Al-Rakhami Mr.", "authors": "Mabrook S. Al-Rakhami, Abdu Gumaei, Sk. Md. Mizanur Rahman and Atif\n  Al-Amri", "title": "Decentralized Blockchain-based model for Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchain technology is among the fastest-growing technologies in the world\ntoday. It has been adopted in diverse areas but mostly in financial systems,\nsuch as Bitcoin cryptocurrency. Therefore, it is a niche that has attracted\ninterest from researchers from various fields, including computer science.\nOther areas where Blockchain is being embraced are the Smart Grid and Internet\nof Things (IoT) technologies, among others. While it is all good and improving\nmany areas of applications, Blockchain still has some shortcomings. For\nexample, it is not designed for high scalability when accommodating normal\ntransactions. On the other hand, a parallel technology that has diverse\napplications in distributed networks better known as edge computing has\nemerged. Its main advantage is that it increases the speed of pf processes\nwithin those networks. However, like Blockchain, edge computing has its\nshortcomings. Its security systems and management systems have been found to be\nwanting. Hence the idea to integrate the two technologies and take advantage of\ntheir strengths. A blend of the two would lead to advanced network servers,\nhuge data storage, and heightened security in transactions. However, this\nintegration will best happen when some measures are taken. For example, there\nis a need to address scalability, resource management satisfactorily, and the\nsecurity of the systems. To solve the integration problem, a decentralized\nBlockchain-based model of Edge computing is proposed in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 01:35:37 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Al-Rakhami", "Mabrook S.", ""], ["Gumaei", "Abdu", ""], ["Rahman", "Sk. Md. Mizanur", ""], ["Al-Amri", "Atif", ""]]}, {"id": "2106.15130", "submitter": "Ehsan Nowroozi", "authors": "Mauro Conti, Simone Milani, Ehsan Nowroozi, Gabriele Orazi", "title": "Do Not Deceive Your Employer with a Virtual Background: A Video\n  Conferencing Manipulation-Detection System", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The last-generation video conferencing software allows users to utilize a\nvirtual background to conceal their personal environment due to privacy\nconcerns, especially in official meetings with other employers. On the other\nhand, users maybe want to fool people in the meeting by considering the virtual\nbackground to conceal where they are. In this case, developing tools to\nunderstand the virtual background utilize for fooling people in meeting plays\nan important role. Besides, such detectors must prove robust against different\nkinds of attacks since a malicious user can fool the detector by applying a set\nof adversarial editing steps on the video to conceal any revealing footprint.\nIn this paper, we study the feasibility of an efficient tool to detect whether\na videoconferencing user background is real. In particular, we provide the\nfirst tool which computes pixel co-occurrences matrices and uses them to search\nfor inconsistencies among spectral and spatial bands. Our experiments confirm\nthat cross co-occurrences matrices improve the robustness of the detector\nagainst different kinds of attacks. This work's performance is especially\nnoteworthy with regard to color SPAM features. Moreover, the performance\nespecially is significant with regard to robustness versus post-processing,\nlike geometric transformations, filtering, contrast enhancement, and JPEG\ncompression with different quality factors.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 07:31:21 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Conti", "Mauro", ""], ["Milani", "Simone", ""], ["Nowroozi", "Ehsan", ""], ["Orazi", "Gabriele", ""]]}, {"id": "2106.15225", "submitter": "Nilupulee Gunathilake", "authors": "Nilupulee A. Gunathilake, Ahmed Al-Dubai, William J. Buchanan, Owen Lo", "title": "Electromagnetic Analysis of an Ultra-Lightweight Cipher: PRESENT", "comments": "20 pages", "journal-ref": "10th international conference on Cryptography and Information\n  Security 2021", "doi": "10.1521/csit.2021.110915", "report-no": "Volume 11 Number 09", "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Side-channel attacks are an unpredictable risk factor in cryptography.\nTherefore, continuous observations of physical leakages are essential to\nminimise vulnerabilities associated with cryptographic functions. Lightweight\ncryptography is a novel approach in progress towards internet-of-things (IoT)\nsecurity. Thus, it would provide sufficient data and privacy protection in such\na constrained ecosystem. IoT devices are resource-limited in terms of data\nrates (in kbps), power maintainability (battery) as well as hardware and\nsoftware footprints (physical size, internal memory, RAM/ROM). Due to the\ndifficulty in handling conventional cryptographic algorithms, lightweight\nciphers consist of small key sizes, block sizes and few operational rounds.\nUnlike in the past, affordability to perform side-channel attacks using\ninexpensive electronic circuitries is becoming a reality. Hence, cryptanalysis\nof physical leakage in these emerging ciphers is crucial. Among existing\nstudies, power analysis seems to have enough attention in research, whereas\nother aspects such as electromagnetic, timing, cache and optical attacks\ncontinue to be appropriately evaluated to play a role in forensic analysis.\n  As a result, we started analysing electromagnetic emission leakage of an\nultra-lightweight block cipher, PRESENT. According to the literature, PRESENT\npromises to be adequate for IoT devices, and there still seems not to exist any\nwork regarding correlation electromagnetic analysis (CEMA) of it. Firstly, we\nconducted simple electromagnetic analysis in both time and frequency domains\nand then proceeded towards CEMA attack modelling. This paper provides a summary\nof the related literature (IoT, lightweight cryptography, side-channel attacks\nand EMA), our methodology, current outcomes and future plans for the optimised\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 10:20:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gunathilake", "Nilupulee A.", ""], ["Al-Dubai", "Ahmed", ""], ["Buchanan", "William J.", ""], ["Lo", "Owen", ""]]}, {"id": "2106.15285", "submitter": "Sam Royston", "authors": "Sam Royston, Ben Greenberg, Omeed Tavasoli, Courtenay Cotton", "title": "Anomaly Detection and Automated Labeling for Voter Registration File\n  Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Voter eligibility in United States elections is determined by a patchwork of\nstate databases containing information about which citizens are eligible to\nvote. Administrators at the state and local level are faced with the\nexceedingly difficult task of ensuring that each of their jurisdictions is\nproperly managed, while also monitoring for improper modifications to the\ndatabase. Monitoring changes to Voter Registration Files (VRFs) is crucial,\ngiven that a malicious actor wishing to disrupt the democratic process in the\nUS would be well-advised to manipulate the contents of these files in order to\nachieve their goals. In 2020, we saw election officials perform admirably when\nfaced with administering one of the most contentious elections in US history,\nbut much work remains to secure and monitor the election systems Americans rely\non. Using data created by comparing snapshots taken of VRFs over time, we\npresent a set of methods that make use of machine learning to ease the burden\non analysts and administrators in protecting voter rolls. We first evaluate the\neffectiveness of multiple unsupervised anomaly detection methods in detecting\nVRF modifications by modeling anomalous changes as sparse additive noise. In\nthis setting we determine that statistical models comparing administrative\ndistricts within a short time span and non-negative matrix factorization are\nmost effective for surfacing anomalous events for review. These methods were\ndeployed during 2019-2020 in our organization's monitoring system and were used\nin collaboration with the office of the Iowa Secretary of State. Additionally,\nwe propose a newly deployed model which uses historical and demographic\nmetadata to label the likely root cause of database modifications. We hope to\nuse this model to predict which modifications have known causes and therefore\nbetter identify potentially anomalous modifications.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 21:48:31 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Royston", "Sam", ""], ["Greenberg", "Ben", ""], ["Tavasoli", "Omeed", ""], ["Cotton", "Courtenay", ""]]}, {"id": "2106.15335", "submitter": "Pierre Tholoniat", "authors": "Tao Luo, Mingen Pan, Pierre Tholoniat, Asaf Cidon, Roxana Geambasu,\n  Mathias L\\'ecuyer", "title": "Privacy Budget Scheduling", "comments": "Extended version of a paper presented at the 15th USENIX Symposium on\n  Operating Systems Design and Implementation (OSDI '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models trained on personal data have been shown to leak\ninformation about users. Differential privacy (DP) enables model training with\na guaranteed bound on this leakage. Each new model trained with DP increases\nthe bound on data leakage and can be seen as consuming part of a global privacy\nbudget that should not be exceeded. This budget is a scarce resource that must\nbe carefully managed to maximize the number of successfully trained models.\n  We describe PrivateKube, an extension to the popular Kubernetes datacenter\norchestrator that adds privacy as a new type of resource to be managed\nalongside other traditional compute resources, such as CPU, GPU, and memory.\nThe abstractions we design for the privacy resource mirror those defined by\nKubernetes for traditional resources, but there are also major differences. For\nexample, traditional compute resources are replenishable while privacy is not:\na CPU can be regained after a model finishes execution while privacy budget\ncannot. This distinction forces a re-design of the scheduler. We present DPF\n(Dominant Private Block Fairness) -- a variant of the popular Dominant Resource\nFairness (DRF) algorithm -- that is geared toward the non-replenishable privacy\nresource but enjoys similar theoretical properties as DRF.\n  We evaluate PrivateKube and DPF on microbenchmarks and an ML workload on\nAmazon Reviews data. Compared to existing baselines, DPF allows training more\nmodels under the same global privacy guarantee. This is especially true for DPF\nover R\\'enyi DP, a highly composable form of DP.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:43:47 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Luo", "Tao", ""], ["Pan", "Mingen", ""], ["Tholoniat", "Pierre", ""], ["Cidon", "Asaf", ""], ["Geambasu", "Roxana", ""], ["L\u00e9cuyer", "Mathias", ""]]}, {"id": "2106.15343", "submitter": "Alekhya Akkinepally", "authors": "Tabish Maniar, Alekhya Akkinepally, Anantha Sharma", "title": "Differential Privacy for Credit Risk Model", "comments": "7 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of machine learning algorithms to model user behavior and drive\nbusiness decisions has become increasingly commonplace, specifically providing\nintelligent recommendations to automated decision making. This has led to an\nincrease in the use of customers personal data to analyze customer behavior and\npredict their interests in a companys products. Increased use of this customer\npersonal data can lead to better models but also to the potential of customer\ndata being leaked, reverse engineered, and mishandled. In this paper, we assess\ndifferential privacy as a solution to address these privacy problems by\nbuilding privacy protections into the data engineering and model training\nstages of predictive model development. Our interest is a pragmatic\nimplementation in an operational environment, which necessitates a general\npurpose differentially private modeling framework, and we evaluate one such\ntool from LeapYear as applied to the Credit Risk modeling domain. Credit Risk\nModel is a major modeling methodology in banking and finance where user data is\nanalyzed to determine the total Expected Loss to the bank. We examine the\napplication of differential privacy on the credit risk model and evaluate the\nperformance of a Differentially Private Model with a Non Differentially Private\nModel. Credit Risk Model is a major modeling methodology in banking and finance\nwhere users data is analyzed to determine the total Expected Loss to the bank.\nIn this paper, we explore the application of differential privacy on the credit\nrisk model and evaluate the performance of a Non Differentially Private Model\nwith Differentially Private Model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:58:49 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Maniar", "Tabish", ""], ["Akkinepally", "Alekhya", ""], ["Sharma", "Anantha", ""]]}, {"id": "2106.15349", "submitter": "Manjesh Kumar Hanawal", "authors": "Sayan Chatterjee and Manjesh K. Hanawal", "title": "Federated Learning for Intrusion Detection in IoT Security: A Hybrid\n  Ensemble Approach", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Critical role of Internet of Things (IoT) in various domains like smart city,\nhealthcare, supply chain and transportation has made them the target of\nmalicious attacks. Past works in this area focused on centralized Intrusion\nDetection System (IDS), assuming the existence of a central entity to perform\ndata analysis and identify threats. However, such IDS may not always be\nfeasible, mainly due to spread of data across multiple sources and gathering at\ncentral node can be costly. Also, the earlier works primarily focused on\nimproving True Positive Rate (TPR) and ignored the False Positive Rate (FPR),\nwhich is also essential to avoid unnecessary downtime of the systems. In this\npaper, we first present an architecture for IDS based on hybrid ensemble model,\nnamed PHEC, which gives improved performance compared to state-of-the-art\narchitectures. We then adapt this model to a federated learning framework that\nperforms local training and aggregates only the model parameters. Next, we\npropose Noise-Tolerant PHEC in centralized and federated settings to address\nthe label-noise problem. The proposed idea uses classifiers using weighted\nconvex surrogate loss functions. Natural robustness of KNN classifier towards\nnoisy data is also used in the proposed architecture. Experimental results on\nfour benchmark datasets drawn from various security attacks show that our model\nachieves high TPR while keeping FPR low on noisy and clean data. Further, they\nalso demonstrate that the hybrid ensemble models achieve performance in\nfederated settings close to that of the centralized settings.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 06:33:35 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chatterjee", "Sayan", ""], ["Hanawal", "Manjesh K.", ""]]}, {"id": "2106.15360", "submitter": "Zhuo Yang", "authors": "Zhuo Yang, Yufei Han, Xiangliang Zhang", "title": "Attack Transferability Characterization for Adversarially Robust\n  Multi-label Classification", "comments": "Accepted in ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite of the pervasive existence of multi-label evasion attack, it is an\nopen yet essential problem to characterize the origin of the adversarial\nvulnerability of a multi-label learning system and assess its attackability. In\nthis study, we focus on non-targeted evasion attack against multi-label\nclassifiers. The goal of the threat is to cause miss-classification with\nrespect to as many labels as possible, with the same input perturbation. Our\nwork gains in-depth understanding about the multi-label adversarial attack by\nfirst characterizing the transferability of the attack based on the functional\nproperties of the multi-label classifier. We unveil how the transferability\nlevel of the attack determines the attackability of the classifier via\nestablishing an information-theoretic analysis of the adversarial risk.\nFurthermore, we propose a transferability-centered attackability assessment,\nnamed Soft Attackability Estimator (SAE), to evaluate the intrinsic\nvulnerability level of the targeted multi-label classifier. This estimator is\nthen integrated as a transferability-tuning regularization term into the\nmulti-label learning paradigm to achieve adversarially robust classification.\nThe experimental study on real-world data echos the theoretical analysis and\nverify the validity of the transferability-regularized multi-label learning\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:50:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yang", "Zhuo", ""], ["Han", "Yufei", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2106.15387", "submitter": "Luca Wilke", "authors": "Luca Wilke (1), Jan Wichelmann (1), Florian Sieck (1), Thomas\n  Eisenbarth (1) ((1) University of L\\\"ubeck)", "title": "undeSErVed trust: Exploiting Permutation-Agnostic Remote Attestation", "comments": "11 pages, 5 figures, accepted at WOOT 2021, proceedings not yet\n  published", "journal-ref": "2021 IEEE Security and Privacy Workshops (SPW), Year 2021, Pages\n  456-466", "doi": "10.1109/SPW53761.2021.00064", "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing trend of moving data and computation to the cloud is met with\nconcerns regarding privacy and protection of intellectual property. Cloud\nService Providers (CSP) must be fully trusted to not tamper with or disclose\nprocessed data, hampering adoption of cloud services for many sensitive or\ncritical applications. As a result, CSPs and CPU manufacturers are rushing to\nfind solutions for secure outsourced computation in the Cloud. While enclaves,\nlike Intel SGX, are strongly limited in terms of throughput and size, AMD's\nSecure Encrypted Virtualization (SEV) offers hardware support for transparently\nprotecting code and data of entire VMs, thus removing the performance, memory\nand software adaption barriers of enclaves. Through attestation of boot code\nintegrity and means for securely transferring secrets into an encrypted VM,\nCSPs are effectively removed from the list of trusted entities. There have been\nseveral attacks on the security of SEV, by abusing I/O channels to encrypt and\ndecrypt data, or by moving encrypted code blocks at runtime. Yet, none of these\nattacks have targeted the attestation protocol, the core of the secure\ncomputing environment created by SEV. We show that the current attestation\nmechanism of Zen 1 and Zen 2 architectures has a significant flaw, allowing us\nto manipulate the loaded code without affecting the attestation outcome. An\nattacker may abuse this weakness to inject arbitrary code at startup -- and\nthus take control over the entire VM execution, without any indication to the\nVM's owner. Our attack primitives allow the attacker to do extensive\nmodifications to the bootloader and the operating system, like injecting spy\ncode or extracting secret data. We present a full end-to-end attack, from the\ninitial exploit to leaking the key of the encrypted disk image during boot,\ngiving the attacker unthrottled access to all of the VM's persistent data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:17:27 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wilke", "Luca", "", "University of L\u00fcbeck"], ["Wichelmann", "Jan", "", "University of L\u00fcbeck"], ["Sieck", "Florian", "", "University of L\u00fcbeck"], ["Eisenbarth", "Thomas", "", "University of L\u00fcbeck"]]}, {"id": "2106.15497", "submitter": "Chaochen Shi", "authors": "Chaochen Shi, Yong Xiang, Robin Ram Mohan Doss, Jiangshan Yu, Keshav\n  Sood, Longxiang Gao", "title": "A Bytecode-based Approach for Smart Contract Classification", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of blockchain technologies, the number of smart\ncontracts deployed on blockchain platforms is growing exponentially, which\nmakes it difficult for users to find desired services by manual screening. The\nautomatic classification of smart contracts can provide blockchain users with\nkeyword-based contract searching and helps to manage smart contracts\neffectively. Current research on smart contract classification focuses on\nNatural Language Processing (NLP) solutions which are based on contract source\ncode. However, more than 94% of smart contracts are not open-source, so the\napplication scenarios of NLP methods are very limited. Meanwhile, NLP models\nare vulnerable to adversarial attacks. This paper proposes a classification\nmodel based on features from contract bytecode instead of source code to solve\nthese problems. We also use feature selection and ensemble learning to optimize\nthe model. Our experimental studies on over 3,300 real-world Ethereum smart\ncontracts show that our model can classify smart contracts without source code\nand has better performance than baseline models. Our model also has good\nresistance to adversarial attacks compared with NLP-based models. In addition,\nour analysis reveals that account features used in many smart contract\nclassification models have little effect on classification and can be excluded.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 03:00:29 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Shi", "Chaochen", ""], ["Xiang", "Yong", ""], ["Doss", "Robin Ram Mohan", ""], ["Yu", "Jiangshan", ""], ["Sood", "Keshav", ""], ["Gao", "Longxiang", ""]]}, {"id": "2106.15526", "submitter": "Kelechi Emerole", "authors": "Kelechi Chuwkunonyerem Emerole", "title": "Generalizing Syndrome Decoding problem to the totally Non-negative\n  Grassmannian", "comments": "10 pages double column, 8 figures, feedback on the paper are\n  encouraged and welcomed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The syndrome decoding problem has been proposed as a computational hardness\nassumption for code based cryptosystem that are safe against quantum computing.\nThe problem has been reduced to finding the codeword with the smallest non-zero\ncolumns that would satisfy a linear check equation. Variants of Information set\ndecoding algorithms has been developed as cryptanalytic tools to solve the\nproblem. In this paper, we study and generalize the solution to codes\nassociated with the totally non-negative Grassmannian in the Grassmann metric.\nThis is achieved by reducing it to an instance of finding a subset of the\nplucker coordinates with the smallest number of columns. Subsequently, the\ntheory of the totally non negative Grassmann is extended to connect the concept\nof boundary measurement map to Tanner graph like code construction while\nderiving new analytical bounds on its parameters. The derived bounds shows that\nthe complexity scales up on the size of the plucker coordinates. Finally,\nexperimental results on decoding failure probability and complexity based on\nrow operations are presented and compared to Low Density parity check codes in\nthe Hamming metric.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:55:11 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Emerole", "Kelechi Chuwkunonyerem", ""]]}, {"id": "2106.15601", "submitter": "Adam Morrison", "authors": "Ofek Kirzner and Adam Morrison", "title": "An Analysis of Speculative Type Confusion Vulnerabilities in the Wild", "comments": "To appear in USENIX Security 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectre v1 attacks, which exploit conditional branch misprediction, are often\nidentified with attacks that bypass array bounds checking to leak data from a\nvictim's memory. Generally, however, Spectre v1 attacks can exploit any\nconditional branch misprediction that makes the victim execute code\nincorrectly. In this paper, we investigate speculative type confusion, a\nSpectre v1 attack vector in which branch mispredictions make the victim execute\nwith variables holding values of the wrong type and thereby leak memory\ncontent.\n  We observe that speculative type confusion can be inadvertently introduced by\na compiler, making it extremely hard for programmers to reason about security\nand manually apply Spectre mitigations. We thus set out to determine the extent\nto which speculative type confusion affects the Linux kernel. Our analysis\nfinds exploitable and potentially-exploitable arbitrary memory disclosure\nvulnerabilities. We also find many latent vulnerabilities, which could become\nexploitable due to innocuous system changes, such as coding style changes.\n  Our results suggest that Spectre mitigations which rely on\nstatically/manually identifying \"bad\" code patterns need to be rethought, and\nmore comprehensive mitigations are needed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:41:34 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 06:49:44 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kirzner", "Ofek", ""], ["Morrison", "Adam", ""]]}, {"id": "2106.15764", "submitter": "Yisroel Mirsky Dr.", "authors": "Yisroel Mirsky, Ambra Demontis, Jaidip Kotak, Ram Shankar, Deng Gelei,\n  Liu Yang, Xiangyu Zhang, Wenke Lee, Yuval Elovici, Battista Biggio", "title": "The Threat of Offensive AI to Organizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  AI has provided us with the ability to automate tasks, extract information\nfrom vast amounts of data, and synthesize media that is nearly\nindistinguishable from the real thing. However, positive tools can also be used\nfor negative purposes. In particular, cyber adversaries can use AI (such as\nmachine learning) to enhance their attacks and expand their campaigns.\n  Although offensive AI has been discussed in the past, there is a need to\nanalyze and understand the threat in the context of organizations. For example,\nhow does an AI-capable adversary impact the cyber kill chain? Does AI benefit\nthe attacker more than the defender? What are the most significant AI threats\nfacing organizations today and what will be their impact on the future?\n  In this survey, we explore the threat of offensive AI on organizations.\nFirst, we present the background and discuss how AI changes the adversary's\nmethods, strategies, goals, and overall attack model. Then, through a\nliterature review, we identify 33 offensive AI capabilities which adversaries\ncan use to enhance their attacks. Finally, through a user study spanning\nindustry and academia, we rank the AI threats and provide insights on the\nadversaries.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 01:03:28 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Mirsky", "Yisroel", ""], ["Demontis", "Ambra", ""], ["Kotak", "Jaidip", ""], ["Shankar", "Ram", ""], ["Gelei", "Deng", ""], ["Yang", "Liu", ""], ["Zhang", "Xiangyu", ""], ["Lee", "Wenke", ""], ["Elovici", "Yuval", ""], ["Biggio", "Battista", ""]]}, {"id": "2106.15820", "submitter": "Birhanu Eshete", "authors": "Abderrahmen Amich, Birhanu Eshete", "title": "Explanation-Guided Diagnosis of Machine Learning Evasion Attacks", "comments": "To appear in the proceedings of the 17th EAI International Conference\n  on Security and Privacy in Communication Networks (SecureComm 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) models are susceptible to evasion attacks. Evasion\naccuracy is typically assessed using aggregate evasion rate, and it is an open\nquestion whether aggregate evasion rate enables feature-level diagnosis on the\neffect of adversarial perturbations on evasive predictions. In this paper, we\nintroduce a novel framework that harnesses explainable ML methods to guide\nhigh-fidelity assessment of ML evasion attacks. Our framework enables\nexplanation-guided correlation analysis between pre-evasion perturbations and\npost-evasion explanations. Towards systematic assessment of ML evasion attacks,\nwe propose and evaluate a novel suite of model-agnostic metrics for\nsample-level and dataset-level correlation analysis. Using malware and image\nclassifiers, we conduct comprehensive evaluations across diverse model\narchitectures and complementary feature representations. Our explanation-guided\ncorrelation analysis reveals correlation gaps between adversarial samples and\nthe corresponding perturbations performed on them. Using a case study on\nexplanation-guided evasion, we show the broader usage of our methodology for\nassessing robustness of ML models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 05:47:12 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Amich", "Abderrahmen", ""], ["Eshete", "Birhanu", ""]]}, {"id": "2106.15890", "submitter": "Khizar Hameed", "authors": "Khizar Hameed, Saurabh Garg, Muhammad Bilal Amin, Byeong Kang, Abid\n  Khan", "title": "A Context-Aware Information-Based Clone Node Attack Detection Scheme in\n  Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapidly expanding nature of the Internet of Things (IoT) networks is\nbeginning to attract interest across a range of applications, including smart\nhomes, smart transportation, smart health, and industrial contexts. This\ncutting-edge technology enables individuals to track and control their\nintegrated environment in real-time and remotely via a thousand IoT devices\ncomprised of sensors and actuators that actively participate in sensing,\nprocessing, storing, and sharing information. Nonetheless, IoT devices are\nfrequently deployed in hostile environments, wherein adversaries attempt to\ncapture and breach them in order to seize control of the entire network. One\nsuch example of potentially malicious behaviour is the cloning of IoT devices,\nin which an attacker can physically capture the devices, obtain some sensitive\ninformation, duplicate the devices, and intelligently deploy them in desired\nlocations to conduct various insider attacks. A device cloning attack on IoT\nnetworks is a significant security concern since it allows for selective\nforwarding, sink-hole, and black-hole attacks. To address this issue, this\npaper provides an efficient scheme for detecting clone node attacks on IoT\nnetworks that makes use of semantic information about IoT devices known as\ncontext information sensed from the deployed environment to locate them\nsecurely. We design a location proof mechanism by combining location proofs and\nbatch verification of the extended elliptic curve digital signature technique\nto accelerate the verification process at selected trusted nodes. We\ndemonstrate the security of our scheme and its resilience to secure clone node\nattack detection by conducting a comprehensive security analysis. The\nperformance of our proposed scheme provides a high degree of detection accuracy\nwith minimal detection time and significantly reduces the computation,\ncommunication and storage overhead.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:32:33 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Hameed", "Khizar", ""], ["Garg", "Saurabh", ""], ["Amin", "Muhammad Bilal", ""], ["Kang", "Byeong", ""], ["Khan", "Abid", ""]]}, {"id": "2106.15934", "submitter": "Hechuan Guo", "authors": "Chunchi Liu and Hechuan Guo and Minghui Xu and Shengling Wang and\n  Dongxiao Yu and Jiguo Yu and Xiuzhen Cheng", "title": "Extending On-chain Trust to Off-chain - A Trustworthy Vaccine Shipping\n  Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain creates a secure environment on top of strict cryptographic\nassumptions and rigorous security proofs. It permits on-chain interactions to\nachieve trustworthy properties such as traceability, transparency, and\naccountability. However, current blockchain trustworthiness is only confined to\non-chain, creating a \"trust gap\" to the physical, off-chain environment. This\nis due to the lack of a scheme that can truthfully reflect the physical world\nin a real-time and consistent manner. Such an absence hinders further\nreal-world blockchain applications, especially for security-sensitive ones.\n  In this paper, we propose a scheme to extend blockchain trust from on-chain\nto off-chain, and take trustworthy vaccine transportation as an example. Our\nscheme consists of 1) a Trusted Execution Environment (TEE)-enabled trusted\nenvironment monitoring system built with the Arm Cortex-M33 microcontroller\nthat continuously senses the inside of a vaccine box through trusted sensors\nand generates anti-forgery data; and 2) a consistency protocol to upload the\nenvironment status data from the TEE system to blockchain in a truthful,\nreal-time consistent, continuous and fault-tolerant fashion. Our security\nanalysis indicates that no adversary can tamper with the vaccine in any way\nwithout being captured. We carry out an experiment to record the internal\nstatus of a vaccine shipping box during transportation, and the results\nindicate that the proposed system incurs an average latency of 84 ms in local\nsensing and processing followed by an average latency of 130 ms to have the\nsensed data transmitted to and available in the blockchain.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:34:09 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Liu", "Chunchi", ""], ["Guo", "Hechuan", ""], ["Xu", "Minghui", ""], ["Wang", "Shengling", ""], ["Yu", "Dongxiao", ""], ["Yu", "Jiguo", ""], ["Cheng", "Xiuzhen", ""]]}, {"id": "2106.15935", "submitter": "Erik Daniel", "authors": "Erik Daniel and Florian Tschorsch", "title": "Towards Verifiable Mutability for Blockchains", "comments": "Extended Abstract, IEEE Euro S&P 2021 Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their immutable log of information, blockchains can be considered as a\ntransparency-enhancing technology. The immutability, however, also introduces\nthreats and challenges with respect to privacy laws and illegal content.\nIntroducing a certain degree of mutability, which enables the possibility to\nstore and remove information, can therefore increase the opportunities for\nblockchains. In this paper, we present a concept for a mutable blockchain\nstructure. Our approach enables the removal of certain blocks, while\nmaintaining the blockchain's verifiability property. Since our concept is\nagnostic to any consensus algorithms, it can be implemented with permissioned\nand permissionless blockchains.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:37:11 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Daniel", "Erik", ""], ["Tschorsch", "Florian", ""]]}, {"id": "2106.16016", "submitter": "Alessandro Brighente", "authors": "Alessandro Brighente, Mauro Conti, Denis Donadel, Federico Turrin", "title": "EVScout2.0: Electric Vehicle Profiling Through Charging Profile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  EVs (Electric Vehicles) represent a green alternative to traditional\nfuel-powered vehicles. To enforce their widespread use, both the technical\ndevelopment and the security of users shall be guaranteed. Privacy of users\nrepresents one of the possible threats impairing EVs adoption. In particular,\nrecent works showed the feasibility of identifying EVs based on the current\nexchanged during the charging phase. In fact, while the resource negotiation\nphase runs over secure communication protocols, the signal exchanged during the\nactual charging contains features peculiar to each EV. A suitable feature\nextractor can hence associate such features to each EV, in what is commonly\nknown as profiling. In this paper, we propose EVScout2.0, an extended and\nimproved version of our previously proposed framework to profile EVs based on\ntheir charging behavior. By exploiting the current and pilot signals exchanged\nduring the charging phase, our scheme is able to extract features peculiar for\neach EV, allowing hence for their profiling. We implemented and tested\nEVScout2.0 over a set of real-world measurements considering over 7500 charging\nsessions from a total of 137 EVs. In particular, numerical results show the\nsuperiority of EVScout2.0 with respect to the previous version. EVScout2.0 can\nprofile EVs, attaining a maximum of 0.88 recall and 0.88 precision. To the best\nof the authors' knowledge, these results set a new benchmark for upcoming\nprivacy research for large datasets of EVs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 12:32:39 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Brighente", "Alessandro", ""], ["Conti", "Mauro", ""], ["Donadel", "Denis", ""], ["Turrin", "Federico", ""]]}, {"id": "2106.16085", "submitter": "Jinwook Lee Ph.D.", "authors": "Matthew J. Schneider and Jinwook Lee", "title": "Protecting Time Series Data with Minimal Forecast Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting could be negatively impacted due to anonymization requirements in\ndata protection legislation. To measure the potential severity of this problem,\nwe derive theoretical bounds for the loss to forecasts from additive\nexponential smoothing models using protected data. Following the guidelines of\nanonymization from the General Data Protection Regulation (GDPR) and California\nConsumer Privacy Act (CCPA), we develop the $k$-nearest Time Series ($k$-nTS)\nSwapping and $k$-means Time Series ($k$-mTS) Shuffling methods to create\nprotected time series data that minimizes the loss to forecasts while\npreventing a data intruder from detecting privacy issues. For efficient and\neffective decision making, we formally model an integer programming problem for\na perfect matching for simultaneous data swapping in each cluster. We call it a\ntwo-party data privacy framework since our optimization model includes the\nutilities of a data provider and data intruder. We apply our data protection\nmethods to thousands of time series and find that it maintains the forecasts\nand patterns (level, trend, and seasonality) of time series well compared to\nstandard data protection methods suggested in legislation. Substantively, our\npaper addresses the challenge of protecting time series data when used for\nforecasting. Our findings suggest the managerial importance of incorporating\nthe concerns of forecasters into the data protection itself.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 14:20:02 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Schneider", "Matthew J.", ""], ["Lee", "Jinwook", ""]]}, {"id": "2106.16158", "submitter": "Jiahua Xu", "authors": "Gaspard Peduzzi, Jason James, Jiahua Xu", "title": "Jack The Rippler: Arbitrage on the Decentralized Exchange of the XRP\n  Ledger", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The XRP Ledger (XRPL) is a peer-to-peer cryptographic ledger. It features a\ndecentralized exchange (DEX) where network participants can issue and trade\nuser-defined digital assets and currencies. We present Jack the Rippler, a bot\nthat identifies and exploits arbitrage opportunities on the XRPL DEX. We\ndescribe the Jack's arbitrage process and discuss risks associated with using\narbitrage bots.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:05:43 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Peduzzi", "Gaspard", ""], ["James", "Jason", ""], ["Xu", "Jiahua", ""]]}]