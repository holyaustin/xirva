[{"id": "2107.00214", "submitter": "Kiran Sharma Dr.", "authors": "Parul Khurana, Geetha Ganesan, Gulshan Kumar, and Kiran Sharma", "title": "Proof of Reference(PoR): A unified informetrics based consensus\n  mechanism", "comments": "6 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bibliometrics is useful to analyze the research impact for measuring the\nresearch quality. Different bibliographic databases like Scopus, Web of\nScience, Google Scholar etc. are accessed for evaluating the trend of\npublications and citations from time to time. Some of these databases are free\nand some are subscription based. Its always debatable that which bibliographic\ndatabase is better and in what terms. To provide an optimal solution to\navailability of multiple bibliographic databases, we have implemented a single\nauthentic database named as ``conflate'' which can be used for fetching\npublication and citation trend of an author. To further strengthen the\ngenerated database and to provide the transparent system to the stakeholders, a\nconsensus mechanism ``proof of reference (PoR)'' is proposed. Due to three\nconsent based checks implemented in PoR, we feel that it could be considered as\na authentic and honest citation data source for the calculation of unified\ninformetrics for an author.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 04:51:01 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Khurana", "Parul", ""], ["Ganesan", "Geetha", ""], ["Kumar", "Gulshan", ""], ["Sharma", "Kiran", ""]]}, {"id": "2107.00396", "submitter": "Konstantin Bulatov", "authors": "Konstantin Bulatov, Ekaterina Emelianova, Daniil Tropin, Natalya\n  Skoryukina, Yulia Chernyshova, Alexander Sheshkus, Sergey Usilin, Zuheng\n  Ming, Jean-Christophe Burie, Muhammad Muzzamil Luqman, Vladimir V. Arlazarov", "title": "MIDV-2020: A Comprehensive Benchmark Dataset for Identity Document\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity documents recognition is an important sub-field of document\nanalysis, which deals with tasks of robust document detection, type\nidentification, text fields recognition, as well as identity fraud prevention\nand document authenticity validation given photos, scans, or video frames of an\nidentity document capture. Significant amount of research has been published on\nthis topic in recent years, however a chief difficulty for such research is\nscarcity of datasets, due to the subject matter being protected by security\nrequirements. A few datasets of identity documents which are available lack\ndiversity of document types, capturing conditions, or variability of document\nfield values. In addition, the published datasets were typically designed only\nfor a subset of document recognition problems, not for a complex identity\ndocument analysis. In this paper, we present a dataset MIDV-2020 which consists\nof 1000 video clips, 2000 scanned images, and 1000 photos of 1000 unique mock\nidentity documents, each with unique text field values and unique artificially\ngenerated faces, with rich annotation. For the presented benchmark dataset\nbaselines are provided for such tasks as document location and identification,\ntext fields recognition, and face detection. With 72409 annotated images in\ntotal, to the date of publication the proposed dataset is the largest publicly\navailable identity documents dataset with variable artificially generated data,\nand we believe that it will prove invaluable for advancement of the field of\ndocument analysis and recognition. The dataset is available for download at\nftp://smartengines.com/midv-2020 and http://l3i-share.univ-lr.fr .\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:14:17 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bulatov", "Konstantin", ""], ["Emelianova", "Ekaterina", ""], ["Tropin", "Daniil", ""], ["Skoryukina", "Natalya", ""], ["Chernyshova", "Yulia", ""], ["Sheshkus", "Alexander", ""], ["Usilin", "Sergey", ""], ["Ming", "Zuheng", ""], ["Burie", "Jean-Christophe", ""], ["Luqman", "Muhammad Muzzamil", ""], ["Arlazarov", "Vladimir V.", ""]]}, {"id": "2107.00516", "submitter": "Jian Wu", "authors": "Muntabir Hasan Choudhury, Himarsha R. Jayanetti, Jian Wu, William A.\n  Ingram, Edward A. Fox", "title": "Automatic Metadata Extraction Incorporating Visual Features from Scanned\n  Electronic Theses and Dissertations", "comments": "7 pages, 4 figures, 1 table. Accepted by JCDL '21 as a short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electronic Theses and Dissertations (ETDs) contain domain knowledge that can\nbe used for many digital library tasks, such as analyzing citation networks and\npredicting research trends. Automatic metadata extraction is important to build\nscalable digital library search engines. Most existing methods are designed for\nborn-digital documents, so they often fail to extract metadata from scanned\ndocuments such as for ETDs. Traditional sequence tagging methods mainly rely on\ntext-based features. In this paper, we propose a conditional random field (CRF)\nmodel that combines text-based and visual features. To verify the robustness of\nour model, we extended an existing corpus and created a new ground truth corpus\nconsisting of 500 ETD cover pages with human validated metadata. Our\nexperiments show that CRF with visual features outperformed both a heuristic\nand a CRF model with only text-based features. The proposed model achieved\n81.3%-96% F1 measure on seven metadata fields. The data and source code are\npublicly available on Google Drive (https://tinyurl.com/y8kxzwrp) and a GitHub\nrepository (https://github.com/lamps-lab/ETDMiner/tree/master/etd_crf),\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 14:59:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Choudhury", "Muntabir Hasan", ""], ["Jayanetti", "Himarsha R.", ""], ["Wu", "Jian", ""], ["Ingram", "William A.", ""], ["Fox", "Edward A.", ""]]}, {"id": "2107.00893", "submitter": "Michael V\\\"olske", "authors": "Michael V\\\"olske, Janek Bevendorff, Johannes Kiesel, Benno Stein, Maik\n  Fr\\\"obe, Matthias Hagen, Martin Potthast", "title": "Web Archive Analytics", "comments": "12 pages, 5 figures. Published in the proceedings of INFORMATIK 2020", "journal-ref": "INFORMATIK 2020. Gesellschaft f\\\"ur Informatik, Bonn. (pp. 61-72)", "doi": "10.18420/inf2020_05", "report-no": null, "categories": "cs.DL cs.NI cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Web archive analytics is the exploitation of publicly accessible web pages\nand their evolution for research purposes -- to the extent organizationally\npossible for researchers. In order to better understand the complexity of this\ntask, the first part of this paper puts the entirety of the world's captured,\ncreated, and replicated data (the \"Global Datasphere\") in relation to other\nimportant data sets such as the public internet and its web pages, or what is\npreserved thereof by the Internet Archive.\n  Recently, the Webis research group, a network of university chairs to which\nthe authors belong, concluded an agreement with the Internet Archive to\ndownload a substantial part of its web archive for research purposes. The\nsecond part of the paper in hand describes our infrastructure for processing\nthis data treasure: We will eventually host around 8 PB of web archive data\nfrom the Internet Archive and Common Crawl, with the goal of supplementing\nexisting large scale web corpora and forming a non-biased subset of the 30 PB\nweb archive at the Internet Archive.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 08:12:50 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["V\u00f6lske", "Michael", ""], ["Bevendorff", "Janek", ""], ["Kiesel", "Johannes", ""], ["Stein", "Benno", ""], ["Fr\u00f6be", "Maik", ""], ["Hagen", "Matthias", ""], ["Potthast", "Martin", ""]]}, {"id": "2107.01076", "submitter": "Marya Bazzi", "authors": "Adam Tsakalidis, Pierpaolo Basile, Marya Bazzi, Mihai Cucuringu and\n  Barbara McGillivray", "title": "DUKweb: Diachronic word representations from the UK Web Archive corpus", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n  In this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:32:33 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Tsakalidis", "Adam", ""], ["Basile", "Pierpaolo", ""], ["Bazzi", "Marya", ""], ["Cucuringu", "Mihai", ""], ["McGillivray", "Barbara", ""]]}, {"id": "2107.01232", "submitter": "Xiaoyao Yu", "authors": "Xiaoyao Yu, Boleslaw K. Szymanski, Tao Jia", "title": "Become a better you: correlation between the change of research\n  direction and the change of scientific performance", "comments": "22 pages, 4 figures, and SI, to be published in Journal of\n  Informetrics", "journal-ref": "Journal of Infometrics vol. 15 (3):101193, August, 2021", "doi": "10.1016/j.joi.2021.101193", "report-no": null, "categories": "physics.soc-ph cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to explore how scientists decide their research agenda and\nthe corresponding consequences, as their decisions collectively shape\ncontemporary science. There are studies focusing on the overall performance of\nindividuals with different problem choosing strategies. Here we ask a slightly\ndifferent but relatively unexplored question: how is a scientist's change of\nresearch agenda associated with her change of scientific performance. Using\npublication records of over 14,000 authors in physics, we quantitatively\nmeasure the extent of research direction change and the performance change of\nindividuals. We identify a strong positive correlation between the direction\nchange and impact change. Scientists with a larger direction change not only\nare more likely to produce works with increased scientific impact compared to\ntheir past ones, but also have a higher growth rate of scientific impact. On\nthe other hand, the direction change is not associated with productivity\nchange. Those who stay in familiar topics do not publish faster than those who\nventure out and establish themselves in a new field. The gauge of research\ndirection in this work is uncorrelated with the diversity of research agenda\nand the switching probability among topics, capturing the evolution of\nindividual careers from a new point of view. Though the finding is inevitably\naffected by the survival bias, it sheds light on a range of problems in the\ncareer development of individual scientists.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 18:29:07 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Yu", "Xiaoyao", ""], ["Szymanski", "Boleslaw K.", ""], ["Jia", "Tao", ""]]}, {"id": "2107.01910", "submitter": "Maria-Esther Vidal", "authors": "Irlan Grangel-Gonzalez and Maria-Esther Vidal", "title": "Analyzing a Knowledge Graph of Industry4.0 Standards", "comments": "Based on the paper Irlan Grangel-Gonzalez, Maria-Esther Vidal:\n  Analyzing a Knowledge Graph of Industry 4.0 Standards. WWW (Companion Volume)\n  2021: 16-25", "journal-ref": null, "doi": "10.1145/3442442.3453541", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we tackle the problem of standard interoperability across\ndifferent standardization frameworks, and devise a knowledge-driven approach\nthat allows for the description of standards and standardization frameworks\ninto an Industry 4.0 knowledge graph (I40KG). The STO ontology represents\nproperties of standards and standardization frameworks, as well as\nrelationships among them. The I40KG integrates more than 200 standards and four\nstandardization frameworks. To populate the I40KG, the landscape of standards\nhas been analyzed from a semantic perspective and the resulting I40KG\nrepresents knowledge expressed in more than 200 industrial related documents\nincluding technical reports, research articles, and white papers. Additionally,\nthe I40KG has been linked to existing knowledge graphs and an automated\nreasoning has been implemented to reveal implicit relations between standards\nas well as mappings across standardization frameworks. We analyze both the\nnumber of discovered relations between standards and the accuracy of these\nrelations. Observed results indicate that both reasoning and linking processes\nenable for increasing the connectivity in the knowledge graph by up to 80%,\nwhilst up to 96% of the relations can be validated. These outcomes suggest that\nintegrating standards and standardization frameworks into the I40KG enables the\nresolution of semantic interoperability conflicts, empowering the communication\nin smart factories.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 10:03:06 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Grangel-Gonzalez", "Irlan", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "2107.02680", "submitter": "Alexander Nwala", "authors": "Alexander C. Nwala, Michele C. Weigle, Michael L. Nelson", "title": "Garbage, Glitter, or Gold: Assigning Multi-dimensional Quality Scores to\n  Social Media Seeds for Web Archive Collections", "comments": "This is an extended version of the ACM/IEEE Joint Conference on\n  Digital Libraries (JCDL2021) paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  From popular uprisings to pandemics, the Web is an essential source consulted\nby scientists and historians for reconstructing and studying past events.\nUnfortunately, the Web is plagued by reference rot which causes important Web\nresources to disappear. Web archive collections help reduce the costly effects\nof reference rot by saving Web resources that chronicle important\nstories/events before they disappear. These collections often begin with URLs\ncalled seeds, hand-selected by experts or scraped from social media. The\nquality of social media content varies widely, therefore, we propose a\nframework for assigning multi-dimensional quality scores to social media seeds\nfor Web archive collections about stories and events. We leveraged\ncontributions from social media research for attributing quality to social\nmedia content and users based on credibility, reputation, and influence. We\ncombined these with additional contributions from the Web archive research that\nemphasizes the importance of considering geographical and temporal constraints\nwhen selecting seeds. Next, we developed the Quality Proxies (QP) framework\nwhich assigns seeds extracted from social media a quality score across 10 major\ndimensions: popularity, geographical, temporal, subject expert, retrievability,\nrelevance, reputation, and scarcity. We instantiated the framework and showed\nthat seeds can be scored across multiple QP classes that map to different\npolicies for ranking seeds such as prioritizing seeds from local news,\nreputable and/or popular sources, etc. The QP framework is extensible and\nrobust. Our results showed that Quality Proxies resulted in the selection of\nquality seeds with increased precision (by ~0.13) when novelty is and is not\nprioritized. These contributions provide an explainable score applicable to\nrank and select quality seeds for Web archive collections and other domains.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:40:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Nwala", "Alexander C.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "2107.02887", "submitter": "Macy Huston", "authors": "Julia LaFond, Jason T. Wright, Macy J. Huston", "title": "Furthering a Comprehensive SETI Bibliography", "comments": "7 pages, 3 figures, accepted to JBIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL astro-ph.IM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In 2019, Reyes & Wright used the NASA Astrophysics Data System (ADS) to\ninitiate a comprehensive bibliography for SETI accessible to the public. Since\nthen, updates to the library have been incomplete, partly due to the difficulty\nin managing the large number of false positive publications generated by\nsearching ADS using simple search terms. In preparation for a recent update,\nthe scope of the library was revised and reexamined. The scope now includes\nsocial sciences and commensal SETI. Results were curated based on five SETI\nkeyword searches: \"SETI\", \"technosignature\", \"Fermi Paradox,\" \"Drake Equation\",\nand \"extraterrestrial intelligence.\" These keywords returned 553 publications\nthat merited inclusion in the bibliography that were not previously present. A\ncurated library of false positive results is now concurrently maintained to\nfacilitate their exclusion from future searches. A search query and workflow\nwas developed to capture nearly all SETI-related papers indexed by ADS while\nminimizing false positives. These tools will enable efficient, consistent\nupdates of the SETI library by future curators, and could be adopted for other\nbibliography projects as well.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 20:55:03 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["LaFond", "Julia", ""], ["Wright", "Jason T.", ""], ["Huston", "Macy J.", ""]]}, {"id": "2107.03297", "submitter": "Angelo Salatino Dr", "authors": "Mojtaba Nayyeri, Gokce Muge Cil, Sahar Vahdati, Francesco Osborne,\n  Mahfuzur Rahman, Simone Angioni, Angelo Salatino, Diego Reforgiato Recupero,\n  Nadezhda Vassilyeva, Enrico Motta, Jens Lehmann", "title": "Trans4E: Link Prediction on Scholarly Knowledge Graphs", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2021.02.100", "report-no": null, "categories": "cs.AI cs.CL cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the\nquality of AI-based services. In the scholarly domain, KGs describing research\npublications typically lack important information, hindering our ability to\nanalyse and predict research dynamics. In recent years, link prediction\napproaches based on Knowledge Graph Embedding models became the first aid for\nthis issue. In this work, we present Trans4E, a novel embedding model that is\nparticularly fit for KGs which include N to M relations with N$\\gg$M. This is\ntypical for KGs that categorize a large number of entities (e.g., research\narticles, patents, persons) according to a relatively small set of categories.\nTrans4E was applied on two large-scale knowledge graphs, the Academia/Industry\nDynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the\ninformation about Fields of Study (e.g., 'neural networks', 'machine learning',\n'artificial intelligence'), and affiliation types (e.g., 'education',\n'company', 'government'), improving the scope and accuracy of the resulting\ndata. We evaluated our approach against alternative solutions on AIDA, MAG, and\nfour other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms\nthe other models when using low embedding dimensions and obtains competitive\nresults in high dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 09:34:44 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Nayyeri", "Mojtaba", ""], ["Cil", "Gokce Muge", ""], ["Vahdati", "Sahar", ""], ["Osborne", "Francesco", ""], ["Rahman", "Mahfuzur", ""], ["Angioni", "Simone", ""], ["Salatino", "Angelo", ""], ["Recupero", "Diego Reforgiato", ""], ["Vassilyeva", "Nadezhda", ""], ["Motta", "Enrico", ""], ["Lehmann", "Jens", ""]]}, {"id": "2107.03816", "submitter": "Allard Oelen", "authors": "Allard Oelen, Markus Stocker, S\\\"oren Auer", "title": "SmartReviews: Towards Human- and Machine-actionable Reviews", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Review articles summarize state-of-the-art work and provide a means to\norganize the growing number of scholarly publications. However, the current\nreview method and publication mechanisms hinder the impact review articles can\npotentially have. Among other limitations, reviews only provide a snapshot of\nthe current literature and are generally not readable by machines. In this\nwork, we identify the weaknesses of the current review method. Afterwards, we\npresent the SmartReview approach addressing those weaknesses. The approach\npushes towards semantic community-maintained review articles. At the core of\nour approach, knowledge graphs are employed to make articles more\nmachine-actionable and maintainable.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 12:49:00 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Oelen", "Allard", ""], ["Stocker", "Markus", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2107.04070", "submitter": "Justin F Brunelle", "authors": "Justin F. Brunelle, Ryan Farley, Grant Atkins, Trevor Bostic, Marites\n  Hendrix, Zak Zebrowski", "title": "Introducing A Dark Web Archival Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a framework for web-scale archiving of the dark web. While\ncommonly associated with illicit and illegal activity, the dark web provides a\nway to privately access web information. This is a valuable and socially\nbeneficial tool to global citizens, such as those wishing to access information\nwhile under oppressive political regimes that work to limit information\navailability. However, little institutional archiving is performed on the dark\nweb (limited to the Archive.is dark web presence, a page-at-a-time archiver).\nWe use surface web tools, techniques, and procedures (TTPs) and adapt them for\narchiving the dark web. We demonstrate the viability of our framework in a\nproof-of-concept and narrowly scoped prototype, implemented with the following\nlightly adapted open source tools: the Brozzler crawler for capture, WARC file\nfor storage, and pywb for replay. Using these tools, we demonstrate the\nviability of modified surface web archiving TTPs for archiving the dark web.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 19:03:50 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Brunelle", "Justin F.", ""], ["Farley", "Ryan", ""], ["Atkins", "Grant", ""], ["Bostic", "Trevor", ""], ["Hendrix", "Marites", ""], ["Zebrowski", "Zak", ""]]}, {"id": "2107.04165", "submitter": "Eamon Duede", "authors": "Eamon Duede, Misha Teplistkiy, Karim Lakhani, James Evans", "title": "Being Together in Place as a Catalyst for Scientific Advance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has necessitated social distancing at every level of\nsociety, including universities and research institutes, raising essential\nquestions concerning the continuing importance of physical proximity for\nscientific and scholarly advance. Using customized author surveys about the\nintellectual influence of referenced work on scientist's own papers, combined\nwith precise measures of geographic and semantic distances between focal and\nreferenced works, we find that being at the same institution is strongly\nassociated with intellectual influence on scientists' and scholars' published\nwork. Yet, this influence increases with intellectual distance: the more\ndifferent the referenced work done by colleagues at one's institution, the more\ninfluential it is on one's own. Universities worldwide constitute places where\npeople doing very different work engage in sustained interactions through\ndepartments, committees, seminars and communities. These interactions come to\nuniquely influence their published research, suggesting the need to replace,\nrather than displace diverse engagements for sustainable advance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 01:09:33 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Duede", "Eamon", ""], ["Teplistkiy", "Misha", ""], ["Lakhani", "Karim", ""], ["Evans", "James", ""]]}, {"id": "2107.04347", "submitter": "Gilles Falquet", "authors": "Vincenzo Daponte and Gilles Falquet", "title": "An ontology for the formalization and visualization of scientific\n  knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The construction of an ontology of scientific knowledge objects, presented\nhere, is part of the development of an approach oriented towards the\nvisualization of scientific knowledge. It is motivated by the fact that the\nconcepts that are used to organize scientific knowledge (theorem, law,\nexperience, proof, etc.) appear in existing ontologies but that none of these\nontologies is centered on this topic and presents them in a simple and easily\nunderstandable organization. This ontology has been constructed by 1) selecting\nconcepts that appear in high level ontologies or in ontologies of knowledge\nobjects of specific fields and 2) by interviewing scientists in different\nfields. We have aligned this ontology with some of the sources used, which has\nallowed us to verify its consistency with respect to them. The validation of\nthe ontology consists in using it to formalize knowledge from various sources,\nwhich we have begun to do in the field of physics.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 10:33:45 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 18:00:17 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Daponte", "Vincenzo", ""], ["Falquet", "Gilles", ""]]}, {"id": "2107.04382", "submitter": "Zeyd Boukhers", "authors": "Zeyd Boukhers, Nagaraj Bahubali, Abinaya Thulsi Chandrasekaran, Adarsh\n  Anand, Soniya Manchenahalli Gnanendra Prasadand, Sriram Aralappa", "title": "Bib2Auth: Deep Learning Approach for Author Disambiguation using\n  Bibliographic Data", "comments": "Accepted and presented at the workshop BiblioDAP@KDD2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Author name ambiguity remains a critical open problem in digital libraries\ndue to synonymy and homonymy of names. In this paper, we propose a novel\napproach to link author names to their real-world entities by relying on their\nco-authorship pattern and area of research. Our supervised deep learning model\nidentifies an author by capturing his/her relationship with his/her co-authors\nand area of research, which is represented by the titles and sources of the\ntarget author's publications. These attributes are encoded by their semantic\nand symbolic representations. To this end, Bib2Auth uses ~ 22K bibliographic\nrecords from the DBLP repository and is trained with each pair of co-authors.\nThe extensive experiments have proved the capability of the approach to\ndistinguish between authors sharing the same name and recognize authors with\ndifferent name variations. Bib2Auth has shown good performance on a relatively\nlarge dataset, which qualifies it to be directly integrated into bibliographic\nindices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 12:25:11 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Boukhers", "Zeyd", ""], ["Bahubali", "Nagaraj", ""], ["Chandrasekaran", "Abinaya Thulsi", ""], ["Anand", "Adarsh", ""], ["Prasadand", "Soniya Manchenahalli Gnanendra", ""], ["Aralappa", "Sriram", ""]]}, {"id": "2107.04887", "submitter": "Philip Purnell", "authors": "Philip J. Purnell", "title": "The prevalence and impact of university affiliation discrepancies\n  between four well-known bibliometric databases", "comments": "40 pages, 4 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research managers benchmarking universities against international peers face\nthe problem of affiliation disambiguation. Different databases have taken\nseparate approaches to this problem and discrepancies exist between them.\nBibliometric data sources typically conduct a disambiguation process that\nunifies variant institutional names and those of its sub-units so that\nresearchers can then search all records from that institution using a single\nunified name. This study examined affiliation discrepancies between Scopus, Web\nof Science, Dimensions, and Microsoft Academic for 18 Arab universities over a\nfive-year period. We confirmed that digital object identifiers (DOIs) are\nsuitable for extracting comparable scholarly material across databases and\nquantified the affiliation discrepancies between them. A substantial share of\nrecords assigned to the selected universities in any one database were not\nassigned to the same university in another. The share of discrepancy was higher\nin the larger databases, Dimensions and Microsoft Academic. The smaller, more\nselective databases, Scopus and especially Web of Science tended to agree to a\ngreater degree with affiliations in the other databases. Manual examination of\naffiliation discrepancies showed they were caused by a mixture of missing\naffiliations, unification differences, and assignation of records to the wrong\ninstitution.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 18:20:18 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Purnell", "Philip J.", ""]]}, {"id": "2107.05376", "submitter": "Milo\\v{s} Nikoli\\'c", "authors": "Dusan Teodorovic, Milos Nikolic", "title": "Measuring scientific output of researchers by t-index and Data\n  Envelopment Analysis", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is a growing need for ranking universities, departments, research\ngroups, and individual scholars. Usually, the scientific community measures the\nscientific merits of the researchers by using a variety of indicators that take\ninto account both the productivity of scholars and the impact of their\npublications. We propose the t-index, the new indicator to measure the\nscientific merits of the individual researchers. The proposed t-index takes\ninto account the number of citations, number of coauthors on every published\npaper, and career duration. The t-index makes the possible comparison of\nresearchers at various stages of their careers. We also use in this paper the\nData Envelopment Analysis (DEA) to measure the scientific merits of the\nindividual researchers within the observed group of researchers. We chose 15\nscholars in the scientific area of transportation engineering and measured\ntheir t-index values, as well as DEA scores.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 12:40:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Teodorovic", "Dusan", ""], ["Nikolic", "Milos", ""]]}, {"id": "2107.05383", "submitter": "Jesse David Dinneen", "authors": "Jesse David Dinneen and Helen Bubinger", "title": "Not Quite 'Ask a Librarian': AI on the Nature, Value, and Future of LIS", "comments": "Final version to appear in ASIS&T '21: Proceedings of the 84th Annual\n  Meeting of the Association for Information Science & Technology, 58", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CY cs.HC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  AI language models trained on Web data generate prose that reflects human\nknowledge and public sentiments, but can also contain novel insights and\npredictions. We asked the world's best language model, GPT-3, fifteen difficult\nquestions about the nature, value, and future of library and information\nscience (LIS), topics that receive perennial attention from LIS scholars. We\npresent highlights from its 45 different responses, which range from platitudes\nand caricatures to interesting perspectives and worrisome visions of the\nfuture, thus providing an LIS-tailored demonstration of the current performance\nof AI language models. We also reflect on the viability of using AI to forecast\nor generate research ideas in this way today. Finally, we have shared the full\nresponse log online for readers to consider and evaluate for themselves.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:20:17 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Dinneen", "Jesse David", ""], ["Bubinger", "Helen", ""]]}, {"id": "2107.05447", "submitter": "Golsa Heidaei", "authors": "Golsa Heidari, Ahmad Ramadan, Markus Stocker, S\\\"oren Auer", "title": "Leveraging a Federation of Knowledge Graphs to Improve Faceted Search in\n  Digital Libraries", "comments": "12 pages, 4 figures, TPDL 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientists always look for the most accurate and relevant answers to their\nqueries in the literature. Traditional scholarly digital libraries list\ndocuments in search results, and therefore are unable to provide precise\nanswers to search queries. In other words, search in digital libraries is\nmetadata search and, if available, full-text search. We present a methodology\nfor improving a faceted search system on structured content by leveraging a\nfederation of scholarly knowledge graphs. We implemented the methodology on top\nof a scholarly knowledge graph. This search system can leverage content from\nthird-party knowledge graphs to improve the exploration of scholarly content. A\nnovelty of our approach is that we use dynamic facets on diverse data types,\nmeaning that facets can change according to the user query. The user can also\nadjust the granularity of dynamic facets. An additional novelty is that we\nleverage third-party knowledge graphs to improve exploring scholarly knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:39:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Heidari", "Golsa", ""], ["Ramadan", "Ahmad", ""], ["Stocker", "Markus", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2107.05497", "submitter": "Vincent Guichard", "authors": "S\\'ebastien Durost, Guillaume Reich (MSHE), Jean-Pierre Girard\n  (Arch\\'eorient)", "title": "Terminologies, mod{\\`e}les de donn{\\'e}es arch{\\'e}ologiques et\n  th{\\'e}saurus documentaires", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HyperTh{\\'e}sau and Bibracte num{\\'e}rique projects have given rise to a\ncollective effort centred on the use of vocabulary as a means of ensuring the\ninteroperability of archaeological data throughout its life cycle. To this end,\nthe use of the standardised form of the thesaurus -- via the Opentheso platform\n-- provides a tool that is already adapted to the Linked Data. Nevertheless,\nits use quickly raised the question of the different paradigms presiding over\nthe elaboration of a specific vocabulary by each (group of) scientist(s). The\nISO 25964 standard -- designed for the management and interoperability of\nindexing languages -- is flexible enough to permit the comparison and linking\nof different scientific or documentary ``points of view''. Their coherence\nthrough interoperability alignments nevertheless requires to interface\ndifferent semantic granularities: search reporting, the description of raw\ndata, a gateway or ''pivot'' between the two, by using a regulated cooperation\nmethodology. The challenges that remain to be met on this path do not prevent\nthe thesaurus tool from already being a suitable support for a complete\n''human-to-machine-to-human'' interoperability, developed within the framework\nof the Bibracte Ville Ouverte project and exemplified through a research on the\nceramics of that archaeological site.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 14:49:33 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Durost", "S\u00e9bastien", "", "MSHE"], ["Reich", "Guillaume", "", "MSHE"], ["Girard", "Jean-Pierre", "", "Arch\u00e9orient"]]}, {"id": "2107.05522", "submitter": "Hasan Abu-Rasheed", "authors": "Eleni Ilkou, Hasan Abu-Rasheed, Mohammadreza Tavakoli, Sherzod\n  Hakimov, G\\'abor Kismih\\'ok, S\\\"oren Auer, Wolfgang Nejdl", "title": "EduCOR: An Educational and Career-Oriented Recommendation Ontology", "comments": "Accepted in the The 20th International Semantic Web Conference\n  (ISWC2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increased dependence on online learning platforms and educational\nresource repositories, a unified representation of digital learning resources\nbecomes essential to support a dynamic and multi-source learning experience. We\nintroduce the EduCOR ontology, an educational, career-oriented ontology that\nprovides a foundation for representing online learning resources for\npersonalised learning systems. The ontology is designed to enable learning\nmaterial repositories to offer learning path recommendations, which correspond\nto the user's learning goals, academic and psychological parameters, and the\nlabour-market skills. We present the multiple patterns that compose the EduCOR\nontology, highlighting its cross-domain applicability and integrability with\nother ontologies. A demonstration of the proposed ontology on the real-life\nlearning platform eDoer is discussed as a use-case. We evaluate the EduCOR\nontology using both gold standard and task-based approaches. The comparison of\nEduCOR to three gold schemata, and its application in two use-cases, shows its\ncoverage and adaptability to multiple OER repositories, which allows generating\nuser-centric and labour-market oriented recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 15:50:46 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 07:23:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ilkou", "Eleni", ""], ["Abu-Rasheed", "Hasan", ""], ["Tavakoli", "Mohammadreza", ""], ["Hakimov", "Sherzod", ""], ["Kismih\u00f3k", "G\u00e1bor", ""], ["Auer", "S\u00f6ren", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "2107.05738", "submitter": "Golsa Heidaei", "authors": "Golsa Heidari, Ahmad Ramadan, Markus Stocker, S\\\"oren Auer", "title": "Demonstration of Faceted Search on Scholarly Knowledge Graphs", "comments": "2 pages, 1 figure, WWW 2021 Demo. arXiv admin note: substantial text\n  overlap with arXiv:2107.05447", "journal-ref": null, "doi": "10.1145/3442442.3458605", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientists always look for the most accurate and relevant answer to their\nqueries on the scholarly literature. Traditional scholarly search systems list\ndocuments instead of providing direct answers to the search queries. As data in\nknowledge graphs are not acquainted semantically, they are not\nmachine-readable. Therefore, a search on scholarly knowledge graphs ends up in\na full-text search, not a search in the content of scholarly literature. In\nthis demo, we present a faceted search system that retrieves data from a\nscholarly knowledge graph, which can be compared and filtered to better satisfy\nuser information needs. Our practice's novelty is that we use dynamic facets,\nwhich means facets are not fixed and will change according to the content of a\ncomparison.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:53:02 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Heidari", "Golsa", ""], ["Ramadan", "Ahmad", ""], ["Stocker", "Markus", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2107.06472", "submitter": "Bei Yu", "authors": "Jun Wang, Bei Yu", "title": "Linking Health News to Research Literature", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately linking news articles to scientific research works is a critical\ncomponent in a number of applications, such as measuring the social impact of a\nresearch work and detecting inaccuracies or distortions in science news.\nAlthough the lack of links between news and literature has been a challenge in\nthese applications, it is a relatively unexplored research problem. In this\npaper we designed and evaluated a new approach that consists of (1) augmenting\nlatest named-entity recognition techniques to extract various metadata, and (2)\ndesigning a new elastic search engine that can facilitate the use of enriched\nmetadata queries. To evaluate our approach, we constructed two datasets of\npaired news articles and research papers: one is used for training models to\nextract metadata, and the other for evaluation. Our experiments showed that the\nnew approach performed significantly better than a baseline approach used by\naltmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate\nthe effectiveness of the approach, we also conducted a study on 37,600\nhealth-related press releases published on EurekAlert!, which showed that our\napproach was able to identify the corresponding research papers with a top-1\naccuracy of at least 0.97.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 03:50:51 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wang", "Jun", ""], ["Yu", "Bei", ""]]}, {"id": "2107.06476", "submitter": "Yian Yin", "authors": "Ryan Hill, Yian Yin, Carolyn Stein, Dashun Wang, Benjamin F. Jones", "title": "Adaptability and the Pivot Penalty in Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The ability to confront new questions, opportunities, and challenges is of\nfundamental importance to human progress and the resilience of human societies,\nyet the capacity of science to meet new demands remains poorly understood. Here\nwe deploy a new measurement framework to investigate the scientific response to\nthe COVID-19 pandemic and the adaptability of science as a whole. We find that\nscience rapidly shifted to engage COVID-19 following the advent of the virus,\nwith scientists across all fields making large jumps from their prior research\nstreams. However, this adaptive response reveals a pervasive \"pivot penalty\",\nwhere the impact of the new research steeply declines the further the\nscientists move from their prior work. The pivot penalty is severe amidst\nCOVID-19 research, but it is not unique to COVID-19. Rather it applies nearly\nuniversally across the sciences, and has been growing in magnitude over the\npast five decades. While further features condition pivoting, including a\nscientist's career stage, prior expertise and impact, collaborative scale, the\nuse of new coauthors, and funding, we find that the pivot penalty persists and\nremains substantial regardless of these features, suggesting the pivot penalty\nacts as a fundamental friction that governs science's ability to adapt. The\npivot penalty not only holds key implications for the design of the scientific\nsystem and human capacity to confront emergent challenges through scientific\nadvance, but may also be relevant to other social and economic systems, where\nshifting to meet new demands is central to survival and success.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 03:58:22 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hill", "Ryan", ""], ["Yin", "Yian", ""], ["Stein", "Carolyn", ""], ["Wang", "Dashun", ""], ["Jones", "Benjamin F.", ""]]}, {"id": "2107.06751", "submitter": "Guillaume Cabanac", "authors": "Guillaume Cabanac and Cyril Labb\\'e and Alexander Magazinov", "title": "Tortured phrases: A dubious writing style emerging in science. Evidence\n  of critical issues affecting established journals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic text generators have been used to produce fake scientific\npapers for more than a decade. Such nonsensical papers are easily detected by\nboth human and machine. Now more complex AI-powered generation techniques\nproduce texts indistinguishable from that of humans and the generation of\nscientific texts from a few keywords has been documented. Our study introduces\nthe concept of tortured phrases: unexpected weird phrases in lieu of\nestablished ones, such as 'counterfeit consciousness' instead of 'artificial\nintelligence.' We combed the literature for tortured phrases and study one\nreputable journal where these concentrated en masse. Hypothesising the use of\nadvanced language models we ran a detector on the abstracts of recent articles\nof this journal and on several control sets. The pairwise comparisons reveal a\nconcentration of abstracts flagged as 'synthetic' in the journal. We also\nhighlight irregularities in its operation, such as abrupt changes in editorial\ntimelines. We substantiate our call for investigation by analysing several\nindividual dubious articles, stressing questionable features: tortured writing\nstyle, citation of non-existent literature, and unacknowledged image reuse.\nSurprisingly, some websites offer to rewrite texts for free, generating\ngobbledegook full of tortured phrases. We believe some authors used rewritten\ntexts to pad their manuscripts. We wish to raise the awareness on publications\ncontaining such questionable AI-generated or rewritten texts that passed (poor)\npeer review. Deception with synthetic texts threatens the integrity of the\nscientific literature.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 20:47:08 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Cabanac", "Guillaume", ""], ["Labb\u00e9", "Cyril", ""], ["Magazinov", "Alexander", ""]]}, {"id": "2107.07348", "submitter": "Abdelghani Maddi", "authors": "Abdelghani Maddi (HCERES), David Sapinho", "title": "Article Processing Charges based publications: to which extent the price\n  explains scientific impact?", "comments": "ISSI 2021 - 18th International Conference on Scientometrics &\n  Informetrics, Jul 2021, Leuven, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.DL physics.soc-ph q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study aims to analyze relationship between Citations Normalized\nScore (NCS) of scientific publications and Article Processing Charges (APCs)\namounts of Gold Open access publications. To do so, we use APCs information\nprovided by OpenAPC database and citations scores of publications in the Web of\nScience database (WoS). Database covers the period from 2006 to 2019 with\n83,752 articles published in 4751 journals belonging to 267 distinct\npublishers. Results show that contrary to this belief, paying dearly does not\nnecessarily increase the impact of publications. First, large publishers with\nhigh impact are not the most expensive. Second, publishers with the highest\nAPCs are not necessarily the best in terms of impact. Correlation between APCs\nand impact is moderate. Otherwise, in the econometric analysis we have shown\nthat publication quality is strongly determined by journal quality in which it\nis published. International collaboration also plays an important role in\ncitations score.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:52:20 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Maddi", "Abdelghani", "", "HCERES"], ["Sapinho", "David", ""]]}, {"id": "2107.08190", "submitter": "Maksim Eren", "authors": "Maksim E. Eren, Nick Solovyev, Chris Hamer, Renee McDonald, Boian S.\n  Alexandrov, Charles Nicholas", "title": "COVID-19 Multidimensional Kaggle Literature Organization", "comments": "Maksim E. Eren, Nick Solovyev, Chris Hamer, Renee McDonald, Boian\n  S.Alexandrov, and Charles Nicholas. 2021. COVID-19 Multidimensional Kaggle\n  Literature Organization. In ACM Symposium on Document Engineering 2021", "journal-ref": null, "doi": "10.1145/3469096.3474927", "report-no": null, "categories": "cs.LG cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented outbreak of Severe Acute Respiratory Syndrome Coronavirus-2\n(SARS-CoV-2), or COVID-19, continues to be a significant worldwide problem. As\na result, a surge of new COVID-19 related research has followed suit. The\ngrowing number of publications requires document organization methods to\nidentify relevant information. In this paper, we expand upon our previous work\nwith clustering the CORD-19 dataset by applying multi-dimensional analysis\nmethods. Tensor factorization is a powerful unsupervised learning method\ncapable of discovering hidden patterns in a document corpus. We show that a\nhigher-order representation of the corpus allows for the simultaneous grouping\nof similar articles, relevant journals, authors with similar research\ninterests, and topic keywords. These groupings are identified within and among\nthe latent components extracted via tensor decomposition. We further\ndemonstrate the application of this method with a publicly available\ninteractive visualization of the dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 06:16:36 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 01:59:41 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Eren", "Maksim E.", ""], ["Solovyev", "Nick", ""], ["Hamer", "Chris", ""], ["McDonald", "Renee", ""], ["Alexandrov", "Boian S.", ""], ["Nicholas", "Charles", ""]]}, {"id": "2107.08214", "submitter": "Hui-Zhen Fu", "authors": "Hui-Zhen Fu (1), Ludo Waltman (2) ((1) Department of Information\n  Resources Management, School of Public Affairs, Zhejiang University, China\n  (2) Centre for Science and Technology Studies, Leiden University, Leiden, The\n  Netherlands)", "title": "A large-scale bibliometric analysis of global climate change research\n  between 2001 and 2018", "comments": "This manuscript has been submitted to Science of the Total\n  Environment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Global climate change is attracting widespread scientific, political, and\npublic attention owing to the involvement of international initiatives such as\nthe Paris Agreement and the Intergovernmental Panel on Climate Change. We\npresent a large-scale bibliometric analysis based on approximately 120,000\nclimate change publications between 2001 and 2018 to examine how climate change\nis studied in scientific research. Our analysis provides an overview of\nscientific knowledge, shifts of research hotspots, global geographical\ndistribution of research, and focus of individual countries. In our analysis,\nwe identify five key fields in climate change research: physical sciences,\npaleoclimatology, climate-change ecology, climate technology, and climate\npolicy. We draw the following key conclusions: (1) Over the investigated time\nperiod, the focus of climate change research has shifted from understanding the\nclimate system toward climate technologies and policies, such as efficient\nenergy use and legislation. (2) There is an imbalance in scientific production\nbetween developed and developing countries. (3) Geography, national demands,\nand national strategies have been important drivers that influence the research\ninterests and concerns of researchers in different countries. Our study can be\nused by researchers and policy makers to reflect on the directions in which\nclimate change research is developing and discuss priorities for future\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 10:28:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Fu", "Hui-Zhen", ""], ["Waltman", "Ludo", ""]]}, {"id": "2107.08351", "submitter": "Satoshi Takahashi", "authors": "Satoshi Takahashi and Keiko Yamaguchi and Asuka Watanabe", "title": "A Novel Approach to Analyze Fashion Digital Archive from Humanities", "comments": "18 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2009.13395", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fashion styles adopted every day are an important aspect of culture, and\nstyle trend analysis helps provide a deeper understanding of our societies and\ncultures. To analyze everyday fashion trends from the humanities perspective,\nwe need a digital archive that includes images of what people wore in their\ndaily lives over an extended period. In fashion research, building digital\nfashion image archives has attracted significant attention. However, the\nexisting archives are not suitable for retrieving everyday fashion trends. In\naddition, to interpret how the trends emerge, we need non-fashion data sources\nrelevant to why and how people choose fashion. In this study, we created a new\nfashion image archive called Chronicle Archive of Tokyo Street Fashion (CAT\nSTREET) based on a review of the limitations in the existing digital fashion\narchives. CAT STREET includes images showing the clothing people wore in their\ndaily lives during the period 1970--2017, which contain timestamps and street\nlocation annotations. We applied machine learning to CAT STREET and found two\ntypes of fashion trend patterns. Then, we demonstrated how magazine archives\nhelp us interpret how trend patterns emerge. These empirical analyses show our\napproach's potential to discover new perspectives to promote an understanding\nof our societies and cultures through fashion embedded in consumers' daily\nlives.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 03:09:56 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Takahashi", "Satoshi", ""], ["Yamaguchi", "Keiko", ""], ["Watanabe", "Asuka", ""]]}, {"id": "2107.08481", "submitter": "Raoul Wadhwa", "authors": "James Yu, Hayley Beltz, Milind Y. Desai, P\\'eter \\'Erdi, Jacob G.\n  Scott, Raoul R. Wadhwa", "title": "Accessing United States Bulk Patent Data with patentpy and patentr", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The United States Patent and Trademark Office (USPTO) provides publicly\naccessible bulk data files containing information for all patents from 1976\nonward. However, the format of these files changes over time and is\nmemory-inefficient, which can pose issues for individual researchers. Here, we\nintroduce the patentpy and patentr packages for the Python and R programming\nlanguages. They allow users to programmatically fetch bulk data from the USPTO\nwebsite and access it locally in a cleaned, rectangular format. Research\ndepending on United States patent data would benefit from the use of patentpy\nand patentr. We describe package implementation, quality control mechanisms,\nand present use cases highlighting simple, yet effective, applications of this\nsoftware.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 16:10:41 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Yu", "James", ""], ["Beltz", "Hayley", ""], ["Desai", "Milind Y.", ""], ["\u00c9rdi", "P\u00e9ter", ""], ["Scott", "Jacob G.", ""], ["Wadhwa", "Raoul R.", ""]]}, {"id": "2107.09176", "submitter": "Qing Ke", "authors": "Chao Min, Qing Ke", "title": "Temporal search in the scientific space predicts breakthrough inventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of inventions is theorized as a process of searching and\nrecombining existing knowledge components. Previous studies under this theory\nhave examined myriad characteristics of recombined knowledge and their\nperformance implications. One feature that has received much attention is\ntechnological knowledge age. Yet, little is known about how the age of\nscientific knowledge influences the impact of inventions, despite the widely\nknown catalyzing role of science in the creation of new technologies. Here we\nuse a large corpus of patents and derive features characterizing how patents\ntemporally search in the scientific space. We find that patents that cite\nscientific papers have more citations and substantially more likely to become\nbreakthroughs. Conditional on searching in the scientific space, referencing\nmore recent papers increases the impact of patents and the likelihood of being\nbreakthroughs. However, this positive effect can be offset if patents cite\npapers whose ages exhibit a low variance. These effects are consistent across\ntechnological fields.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 22:08:33 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Min", "Chao", ""], ["Ke", "Qing", ""]]}, {"id": "2107.10434", "submitter": "Chengzhi Zhang", "authors": "Qingqing Zhou, Chengzhi Zhang", "title": "Impacts Towards a comprehensive assessment of the book impact by\n  integrating multiple evaluation sources", "comments": null, "journal-ref": "Journal of Informetrics, 2021. 15(3): 101162", "doi": "10.1016/j.joi.2021.101195", "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The surge in the number of books published makes the manual evaluation\nmethods difficult to efficiently evaluate books. The use of books' citations\nand alternative evaluation metrics can assist manual evaluation and reduce the\ncost of evaluation. However, most existing evaluation research was based on a\nsingle evaluation source with coarse-grained analysis, which may obtain\nincomprehensive or one-sided evaluation results of book impact. Meanwhile,\nrelying on a single resource for book assessment may lead to the risk that the\nevaluation results cannot be obtained due to the lack of the evaluation data,\nespecially for newly published books. Hence, this paper measured book impact\nbased on an evaluation system constructed by integrating multiple evaluation\nsources. Specifically, we conducted finer-grained mining on the multiple\nevaluation sources, including books' internal evaluation resources and external\nevaluation resources. Various technologies (e.g. topic extraction, sentiment\nanalysis, text classification) were used to extract corresponding evaluation\nmetrics from the internal and external evaluation resources. Then, Expert\nevaluation combined with analytic hierarchy process was used to integrate the\nevaluation metrics and construct a book impact evaluation system. Finally, the\nreliability of the evaluation system was verified by comparing with the results\nof expert evaluation, detailed and diversified evaluation results were then\nobtained. The experimental results reveal that differential evaluation\nresources can measure the books' impacts from different dimensions, and the\nintegration of multiple evaluation data can assess books more comprehensively.\nMeanwhile, the book impact evaluation system can provide personalized\nevaluation results according to the users' evaluation purposes. In addition,\nthe disciplinary differences should be considered for assessing books' impacts.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 03:11:10 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhou", "Qingqing", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "2107.10724", "submitter": "Rohan Alexander", "authors": "Annie Collins, Rohan Alexander", "title": "Reproducibility of COVID-19 pre-prints", "comments": "14 pages, 6 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To examine the reproducibility of COVID-19 research, we create a dataset of\npre-prints posted to arXiv, bioRxiv, medRxiv, and SocArXiv between 28 January\n2020 and 30 June 2021 that are related to COVID-19. We extract the text from\nthese pre-prints and parse them looking for keyword markers signalling the\navailability of the data and code underpinning the pre-print. For the\npre-prints that are in our sample, we are unable to find markers of either open\ndata or open code for 75 per cent of those on arXiv, 67 per cent of those on\nbioRxiv, 79 per cent of those on medRxiv, and 85 per cent of those on SocArXiv.\nWe conclude that there may be value in having authors categorize the degree of\nopenness of their pre-print as part of the pre-print submissions process, and\nmore broadly, there is a need to better integrate open science training into a\nwide range of fields.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 15:02:06 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Collins", "Annie", ""], ["Alexander", "Rohan", ""]]}, {"id": "2107.11521", "submitter": "Weishu Liu", "authors": "Weishu Liu", "title": "Caveats for the use of Web of Science Core Collection in old literature\n  retrieval and historical bibliometric analysis", "comments": null, "journal-ref": "Technological Forecasting and Social Change(2021)", "doi": "10.1016/j.techfore.2021.121023", "report-no": null, "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By using publications from Web of Science Core Collection (WoSCC), Fosso\nWamba and his colleagues published an interesting and comprehensive paper in\nTechnological Forecasting and Social Change to explore the structure and\ndynamics of artificial intelligence (AI) scholarship. Data demonstrated in\nFosso Wamba's study implied that the year 1991 seemed to be a \"watershed\" of AI\nresearch. This research note tried to uncover the 1991 phenomenon from the\nperspective of database limitation by probing the limitations of search in\nabstract/author keywords/keywords plus fields of WoSCC empirically. The low\navailability rates of abstract/author keywords/keywords plus information in\nWoSCC found in this study can explain the \"watershed\" phenomenon of AI\nscholarship in 1991 to a large extent. Some other caveats for the use of WoSCC\nin old literature retrieval and historical bibliometric analysis were also\nmentioned in the discussion section. This research note complements Fosso Wamba\nand his colleagues' study and also helps avoid improper interpretation in the\nuse of WoSCC in old literature retrieval and historical bibliometric analysis.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 03:39:19 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Weishu", ""]]}, {"id": "2107.12222", "submitter": "Shir Aviv-Reuven", "authors": "Shir Aviv-Reuven and Ariel Rosenfeld (Department of Information\n  Sciences, Bar-Ilan University, Israel)", "title": "Journal subject classification: intra- and inter-system discrepancies in\n  Web Of Science and Scopus", "comments": "25 pages, 20 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Journal classification into subject categories is an important aspect in\nscholarly research evaluation as well as in bibliometric analysis. Journal\nclassification systems use a variety of (partially) overlapping and\nnon-exhaustive subject categories which results in many journals being\nclassified into more than a single subject category. As such, discrepancies are\nlikely to be encountered within any given system and between different systems.\nIn this study, we set to examine both types of discrepancies in the two most\nwidely used indexing systems - Web Of Science and Scopus. We use known distance\nmeasures, as well as logical set theory to examine and compare the category\nschemes defined by these systems. Our results demonstrate significant\ndiscrepancies within each system where a higher number of classified categories\ncorrelates with increased range and variance of rankings within them, and where\nredundant categories are found. Our results also show significant discrepancies\nbetween the two system. Specifically, very few categories in one system are\n\"similar\" to categories in the second system, where \"similarity\" is measured by\nsubset & interesting categories and minimally covering categories. Taken\njointly, our findings suggest that both types of discrepancies are systematic\nand cannot be easily disregarded when relying on these subject classification\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 14:02:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Aviv-Reuven", "Shir", "", "Department of Information\n  Sciences, Bar-Ilan University, Israel"], ["Rosenfeld", "Ariel", "", "Department of Information\n  Sciences, Bar-Ilan University, Israel"]]}, {"id": "2107.12639", "submitter": "Milad Haghani", "authors": "Milad Haghani, Michiel C. J. Bliemer", "title": "Structure and temporal evolution of transportation literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fifty years of evolution of the transportation field is revisited at a macro\nscale using scientometric analysis of all publications in all 39 journals\nindexed in the category of Transportation by the Web of Science. The size of\nthe literature is estimated to have reached 50,000 documents. At the highest\nlevel of aggregation, four major divisions of the literature are differentiated\nthrough these analyses, namely (i) network analysis and traffic flow, (ii)\neconomics of transportation and logistics, (iii) travel behaviour, and (iv)\nroad safety. Influential and emerging authors of each division are identified.\nTemporal trends in transportation research are also investigated via document\nco-citation analysis. This analysis identifies various major streams of\ntransportation research while determining their approximate time of emergence\nand duration of activity. It documents topics that have been most trendy at any\nperiod of time during the last fifty years. Three clusters associated with the\ntravel behaviour division (collectively embodying topics of land-use, active\ntransportation, residential self-selection, traveller experience/satisfaction,\nsocial exclusion and transport/spatial equity), one cluster of statistical\nmodelling of road accidents, and a cluster of network modelling linked\npredominantly to the notion of macroscopic fundamental diagram demonstrate\ncharacteristics of being current hot topics of the field. Three smaller\nclusters linked predominantly to electric mobility and autonomous/automated\nvehicles show characteristics of being emerging hot topics. A cluster labelled\nshared mobility is the youngest emerging cluster. Influential articles within\neach cluster of references are identified. Additional outcomes are the\ndetermination the influential outsiders of the transportation field.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 07:27:01 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Haghani", "Milad", ""], ["Bliemer", "Michiel C. J.", ""]]}, {"id": "2107.12751", "submitter": "Peter Kokol PhD", "authors": "Helena Blazun Vosner, Peter Kokol, Danica Zeleznik, Jernej Zavrsnik", "title": "Bibliometric Profile of Nursing Research in Ex Yugoslavian Countries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The development of modern nursing and consequently nursing research in Ex-\nYugoslavia is about a century old. To profile the development, volume, and\ncontent of nursing research we completed a performance and spatial bibliometric\nanalysis combined with synthetic content analysis to identify the most\nproductive countries and institutions, most prolific source titles, country\ncooperation, publication production trends, the content of research and hot\ntopics. The corpus was harvested from the Web of Science All databases and\ncontained 1380 papers. Slovenia was the most productive country, followed by\nCroatia and Serbia. The synthetic content analysis demonstrated that nursing\nresearch in ex-Yugoslavian countries is growing both in scope and number of\npublications, notwithstanding the fact that research content differs between\ncountries and it seems that each country is focused on their local health\nproblems. A substantial part of the research is published in national journals\nin national languages however, it is noteworthy to note that some\nex-Yugoslavian authors have succeeded in publishing their research in top\nnursing journals. The study also revealed substantial international cooperation\nespecially among ex-Yugoslavian countries and European Union.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 11:46:09 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Vosner", "Helena Blazun", ""], ["Kokol", "Peter", ""], ["Zeleznik", "Danica", ""], ["Zavrsnik", "Jernej", ""]]}, {"id": "2107.13073", "submitter": "Jian Gao", "authors": "Jian Gao, Yian Yin, Kyle R. Myers, Karim R. Lakhani, Dashun Wang", "title": "Loss of New Ideas: Potentially Long-lasting Effects of the Pandemic on\n  Scientists", "comments": "14 pages, 2 figures, with supplementary information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extensive research has documented the immediate impacts of the COVID-19\npandemic on scientists, yet it remains unclear if and how such impacts have\nshifted over time. Here we compare results from two surveys of principal\ninvestigators, conducted between April 2020 and January 2021, along with\nanalyses of large-scale publication data. We find that there has been a clear\nsign of recovery in some regards, as scientists' time spent on their work has\nalmost returned to pre-pandemic levels. However, the latest data also reveals a\nnew dimension in which the pandemic is affecting the scientific workforce: the\nrate of initiating new research projects. Except for the small fraction of\nscientists who directly engaged in COVID-related research, most scientists\nstarted significantly fewer new research projects in 2020. This decline is most\npronounced amongst the same demographic groups of scientists who reported the\nlargest initial disruptions: female scientists and those with young children.\nYet in sharp contrast to the earlier phase of the pandemic, when there were\nlarge disparities across scientific fields, this loss of new projects appears\nremarkably homogeneous across fields. Analyses of large-scale publication data\nreveal a global decline in the rate of new collaborations, especially in\nnon-COVID-related preprints, which is consistent with the reported decline in\nnew projects. Overall, these findings highlight that, while the end of the\npandemic may appear in sight in some countries, its large and unequal impact on\nthe scientific workforce may be enduring, which may have broad implications for\ninequality and the long-term vitality of science.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:42:03 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Gao", "Jian", ""], ["Yin", "Yian", ""], ["Myers", "Kyle R.", ""], ["Lakhani", "Karim R.", ""], ["Wang", "Dashun", ""]]}, {"id": "2107.13238", "submitter": "Swapan Kumar Patra", "authors": "Swapan Kumar Patra", "title": "Library and Information Science Research in Indian Universities: Growth,\n  Core Journals, Keywords and Collaboration Patterns", "comments": "4 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article maps Library and Information Science (LIS) research in Indian\nuniversities. As the two prominent citation databases, Web of Science and\nScopus have very limited coverage of Indian LIS journals, the publications\ngenerated by the library and science departments of about 114 selected Indian\nuniversities and the two national institutions of importance in LIS research\nwere extracted from Library, Information Science & Technology Abstracts\n(LISTA). The relevant publication records were analyzed using scientometrics\nand Social Network Analysis (SNA) tools. The study traces the growth of\npublications, prominent keywords, leading journals where the articles are\npublished and the institutional collaboration patterns of Indian university\npublications. The results show that there is a growth in scholarly publications\nfrom Indian universities in LIS. However, the numbers of publications are\nlimited to only a few universities and national institutes of importance. The\nmaximum LIS research outputs are published in Indian journals. Bibliometrics\nrelated investigations are the most important research areas. Located in major\ncities of India, the productive institutes show healthy collaboration. The\nstudy concludes with some observations which may be useful for formulating\npolicies in LIS research in India.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 09:42:04 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Patra", "Swapan Kumar", ""]]}, {"id": "2107.13877", "submitter": "Mila Runnwerth", "authors": "Susanne Arndt, Patrick Ion, Mila Runnwerth, Moritz Schubotz, Olaf\n  Teschke", "title": "10 Years Later: The Mathematics Subject Classification and Linked Open\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL math.HO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ten years ago, the Mathematics Subject Classification MSC 2010 was released,\nand a corresponding machine-readable Linked Open Data collection was published\nusing the Simple Knowledge Organization System (SKOS). Now, the new MSC 2020 is\nout.\n  This paper recaps the last ten years of working on machine-readable MSC data\nand presents the new machine-readable MSC 2020. We describe the processing\nrequired to convert the version of record, as agreed by the editors of zbMATH\nand Mathematical Reviews, into the Linked Open Data form we call MSC2020-SKOS.\nThe new form includes explicit marking of the changes from 2010 to 2020, some\ntranslations of English code descriptions into Chinese, Italian, and Russian,\nand extra material relating MSC to other mathematics classification efforts. We\nalso outline future potential uses for MSC2020-SKOS in semantic indexing and\nsketch its embedding in a larger vision of scientific research data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 10:23:11 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Arndt", "Susanne", ""], ["Ion", "Patrick", ""], ["Runnwerth", "Mila", ""], ["Schubotz", "Moritz", ""], ["Teschke", "Olaf", ""]]}, {"id": "2107.13957", "submitter": "Pavlos Fafalios", "authors": "Pavlos Fafalios, Konstantina Konsolaki, Lida Charami, Kostas Petrakis,\n  Manos Paterakis, Dimitris Angelakis, Yannis Tzitzikas, Chrysoula Bekiari,\n  Martin Doerr", "title": "Towards Semantic Interoperability in Historical Research: Documenting\n  Research Data and Knowledge with Synthesis", "comments": "This is a preprint of an article accepted for publication at the 20th\n  International Semantic Web Conference (ISWC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A vast area of research in historical science concerns the documentation and\nstudy of artefacts and related evidence. Current practice mostly uses\nspreadsheets or simple relational databases to organise the information as rows\nwith multiple columns of related attributes. This form offers itself for data\nanalysis and scholarly interpretation, however it also poses problems including\ni) the difficulty for collaborative but controlled documentation by a large\nnumber of users, ii) the lack of representation of the details from which the\ndocumented relations are inferred, iii) the difficulty to extend the underlying\ndata structures as well as to combine and integrate data from multiple and\ndiverse information sources, and iv) the limitation to reuse the data beyond\nthe context of a particular research activity. To support historians to cope\nwith these problems, in this paper we describe the Synthesis documentation\nsystem and its use by a large number of historians in the context of an ongoing\nresearch project in the field of History of Art. The system is Web-based and\ncollaborative, and makes use of existing standards for information\ndocumentation and publication (CIDOC-CRM, RDF), focusing on semantic\ninteroperability and the production of data of high value and long-term\nvalidity.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 13:37:39 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fafalios", "Pavlos", ""], ["Konsolaki", "Konstantina", ""], ["Charami", "Lida", ""], ["Petrakis", "Kostas", ""], ["Paterakis", "Manos", ""], ["Angelakis", "Dimitris", ""], ["Tzitzikas", "Yannis", ""], ["Bekiari", "Chrysoula", ""], ["Doerr", "Martin", ""]]}, {"id": "2107.13983", "submitter": "Etienne-Victor Depasquale", "authors": "Etienne-Victor Depasquale, Humaira Abdul Salam, Franco Davoli", "title": "PAD: a graphical and numerical enhancement of structural coding to\n  facilitate thematic analysis of a literature corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We suggest an enhancement to structural coding through the use of (a)\ncausally bound codes, (b) basic constructs of graph theory and (c) statistics.\nAs is the norm with structural coding, the codes are collected into categories.\nThe categories are represented by nodes (graph theory). The causality is\nillustrated through links (graph theory) between the nodes and the entire set\nof linked nodes is collected into a single directed acyclic graph. The number\nof occurrences of the nodes and the links provide the input required to analyze\nrelative frequency of occurrence, as well as opening a scope for further\nstatistical analysis. While our raw data was a corpus of literature from a\nspecific discipline, this enhancement is accessible to any qualitative analysis\nthat recognizes causality in its structural codes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 07:59:34 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Depasquale", "Etienne-Victor", ""], ["Salam", "Humaira Abdul", ""], ["Davoli", "Franco", ""]]}, {"id": "2107.14019", "submitter": "Stephen L. France", "authors": "Stephen L. France, Mahyar Sharif Vaghefi, Brett Kazandjian", "title": "Who Owns the Data? A Systematic Review at the Boundary of Information\n  Systems and Marketing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a systematic research review at the boundary of the\ninformation systems (IS) and marketing disciplines. First, a historical\noverview of these disciplines is given to put the review into context. This is\nfollowed by a bibliographic analysis to select articles at the boundary of IS\nand marketing. Text analysis is then performed on the selected articles to\ngroup them into homogeneous research clusters, which are refined by selecting\n\"distinct\" articles that best represent the clusters. The citation asymmetries\nbetween IS and marketing are noted and an overall conceptual model is created\nthat describes the \"areas of collaboration\" between IS and marketing. Forward\nlooking suggestions are made on how academic researchers can better interface\nwith industry and how academic research at the boundary of IS and marketing can\nbe further developed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:31:44 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["France", "Stephen L.", ""], ["Vaghefi", "Mahyar Sharif", ""], ["Kazandjian", "Brett", ""]]}]