[{"id": "1702.00198", "submitter": "Zeon Trevor Fernando", "authors": "Zeon Trevor Fernando, Ivana Marenzi, Wolfgang Nejdl, Rishita Kalyani", "title": "ArchiveWeb: Collaboratively Extending and Exploring Web Archive\n  Collections", "comments": "Published via Springer in International Conference on Theory and\n  Practice of Digital Libraries (TPDL 2016)", "journal-ref": null, "doi": "10.1007/978-3-319-43997-6_9", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curated web archive collections contain focused digital contents which are\ncollected by archiving organizations to provide a representative sample\ncovering specific topics and events to preserve them for future exploration and\nanalysis. In this paper, we discuss how to best support collaborative\nconstruction and exploration of these collections through the ArchiveWeb\nsystem. ArchiveWeb has been developed using an iterative evaluation-driven\ndesign-based research approach, with considerable user feedback at all stages.\nThis paper describes the functionalities of our current prototype for\nsearching, constructing, exploring and discussing web archive collections, as\nwell as feedback on this prototype from seven archiving organizations, and our\nplans for improving the next release of the system.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 10:39:07 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Fernando", "Zeon Trevor", ""], ["Marenzi", "Ivana", ""], ["Nejdl", "Wolfgang", ""], ["Kalyani", "Rishita", ""]]}, {"id": "1702.00436", "submitter": "Zeon Trevor Fernando", "authors": "Zeon Trevor Fernando, Ivana Marenzi, Wolfgang Nejdl", "title": "ArchiveWeb: collaboratively extending and exploring web archive\n  collections - How would you like to work with your collections?", "comments": "Published via Springer in International Journal on Digital Libraries", "journal-ref": null, "doi": "10.1007/s00799-016-0206-2", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curated web archive collections contain focused digital content which is\ncollected by archiving organizations, groups, and individuals to provide a\nrepresentative sample covering specific topics and events to preserve them for\nfuture exploration and analysis. In this paper, we discuss how to best support\ncollaborative construction and exploration of these collections through the\nArchiveWeb system. ArchiveWeb has been developed using an iterative\nevaluation-driven design-based research approach, with considerable user\nfeedback at all stages. The first part of this paper describes the important\ninsights we gained from our initial requirements engineering phase during the\nfirst year of the project and the main functionalities of the current\nArchiveWeb system for searching, constructing, exploring, and discussing web\narchive collections. The second part summarizes the feedback we received on\nthis version from archiving organizations and libraries, as well as our\ncorresponding plans for improving and extending the system for the next\nrelease.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 20:05:08 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Fernando", "Zeon Trevor", ""], ["Marenzi", "Ivana", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1702.00502", "submitter": "Andrew Tomkins", "authors": "Andrew Tomkins, Min Zhang, William D. Heavlin", "title": "Single versus Double Blind Reviewing at WSDM 2017", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the implications for conference program committees of\nusing single-blind reviewing, in which committee members are aware of the names\nand affiliations of paper authors, versus double-blind reviewing, in which this\ninformation is not visible to committee members. WSDM 2017, the 10th ACM\nInternational ACM Conference on Web Search and Data Mining, performed a\ncontrolled experiment in which each paper was reviewed by four committee\nmembers. Two of these four reviewers were chosen from a pool of committee\nmembers who had access to author information; the other two were chosen from a\ndisjoint pool who did not have access to this information. This information\nasymmetry persisted through the process of bidding for papers, reviewing\npapers, and entering scores. Reviewers in the single-blind condition typically\nbid for 22% fewer papers, and preferentially bid for papers from top\ninstitutions. Once papers were allocated to reviewers, single-blind reviewers\nwere significantly more likely than their double-blind counterparts to\nrecommend for acceptance papers from famous authors and top institutions. The\nestimated odds multipliers are 1.63 for famous authors and 1.58 and 2.10 for\ntop universities and companies respectively, so the result is tangible. For\nfemale authors, the associated odds multiplier of 0.78 is not statistically\nsignificant in our study. However, a meta-analysis places this value in line\nwith that of other experiments, and in the context of this larger aggregate the\ngender effect is also statistically significant.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 23:44:44 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 16:11:46 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 02:42:28 GMT"}, {"version": "v4", "created": "Wed, 19 Apr 2017 01:17:18 GMT"}, {"version": "v5", "created": "Fri, 26 May 2017 02:21:13 GMT"}, {"version": "v6", "created": "Sun, 8 Oct 2017 00:09:29 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Tomkins", "Andrew", ""], ["Zhang", "Min", ""], ["Heavlin", "William D.", ""]]}, {"id": "1702.00554", "submitter": "Tariq Ahmad Mir", "authors": "Tariq Ahmad Mir, Marcel Ausloos", "title": "Benford's law: a 'sleeping beauty' sleeping in the dirty pages of\n  logarithmic tables", "comments": "18 pages, 4 figures, 3 tables, 79 references, Accepted for\n  publication in Journal of the Association for Information Science and\n  Technology", "journal-ref": "Journal of the Association for Information Science and Technology\n  69(3) (2018) 349-358", "doi": "10.1002/asi.23845", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benford's law is an empirical observation, first reported by Simon Newcomb in\n1881 and then independently by Frank Benford in 1938: the first significant\ndigits of numbers in large data are often distributed according to a\nlogarithmically decreasing function. Being contrary to intuition, the law was\nforgotten as a mere curious observation. However, in the last two decades,\nrelevant literature has grown exponentially, - an evolution typical of\n\"Sleeping Beauties\" (SBs) publications that go unnoticed (sleep) for a long\ntime and then suddenly become center of attention (are awakened). Thus, in the\npresent study, we show that Newcomb (1881) and Benford (1938) papers are\nclearly SBs. The former was in deep sleep for 110 years whereas the latter was\nin deep sleep for a comparatively lesser period of 31 years up to 1968, and in\na state of less deep sleep for another 27 years up to 1995. Both SBs were\nawakened in the year 1995 by Hill (1995a). In so doing, we show that the waking\nprince (Hill, 1995a) is more often quoted than the SB whom he kissed, - in this\nBenford's law case, wondering whether this is a general effect, - to be\nusefully studied.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 07:08:27 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Mir", "Tariq Ahmad", ""], ["Ausloos", "Marcel", ""]]}, {"id": "1702.00860", "submitter": "Jaimie Murdock", "authors": "Colin Allen and Hongliang Luo and Jaimie Murdock and Jianghuai Pu and\n  Xiaohong Wang and Yanjie Zhai and Kun Zhao", "title": "Topic Modeling the H\\`an di\\u{a}n Ancient Classics", "comments": "24 pages; 14 pages supplemental", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ancient Chinese texts present an area of enormous challenge and opportunity\nfor humanities scholars interested in exploiting computational methods to\nassist in the development of new insights and interpretations of culturally\nsignificant materials. In this paper we describe a collaborative effort between\nIndiana University and Xi'an Jiaotong University to support exploration and\ninterpretation of a digital corpus of over 18,000 ancient Chinese documents,\nwhich we refer to as the \"Handian\" ancient classics corpus (H\\`an di\\u{a}n\ng\\u{u} j\\'i, i.e, the \"Han canon\" or \"Chinese classics\"). It contains classics\nof ancient Chinese philosophy, documents of historical and biographical\nsignificance, and literary works. We begin by describing the Digital Humanities\ncontext of this joint project, and the advances in humanities computing that\nmade this project feasible. We describe the corpus and introduce our\napplication of probabilistic topic modeling to this corpus, with attention to\nthe particular challenges posed by modeling ancient Chinese documents. We give\na specific example of how the software we have developed can be used to aid\ndiscovery and interpretation of themes in the corpus. We outline more advanced\nforms of computer-aided interpretation that are also made possible by the\nprogramming interface provided by our system, and the general implications of\nthese methods for understanding the nature of meaning in these texts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 22:51:04 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Allen", "Colin", ""], ["Luo", "Hongliang", ""], ["Murdock", "Jaimie", ""], ["Pu", "Jianghuai", ""], ["Wang", "Xiaohong", ""], ["Zhai", "Yanjie", ""], ["Zhao", "Kun", ""]]}, {"id": "1702.01015", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Vinay Goel, Avishek Anand", "title": "ArchiveSpark: Efficient Web Archive Access, Extraction and Derivation", "comments": "JCDL 2016, Newark, NJ, USA", "journal-ref": null, "doi": "10.1145/2910896.2910902", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archives are a valuable resource for researchers of various disciplines.\nHowever, to use them as a scholarly source, researchers require a tool that\nprovides efficient access to Web archive data for extraction and derivation of\nsmaller datasets. Besides efficient access we identify five other objectives\nbased on practical researcher needs such as ease of use, extensibility and\nreusability.\n  Towards these objectives we propose ArchiveSpark, a framework for efficient,\ndistributed Web archive processing that builds a research corpus by working on\nexisting and standardized data formats commonly held by Web archiving\ninstitutions. Performance optimizations in ArchiveSpark, facilitated by the use\nof a widely available metadata index, result in significant speed-ups of data\nprocessing. Our benchmarks show that ArchiveSpark is faster than alternative\napproaches without depending on any additional data stores while improving\nusability by seamlessly integrating queries and derivations with external\ntools.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 14:17:02 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Holzmann", "Helge", ""], ["Goel", "Vinay", ""], ["Anand", "Avishek", ""]]}, {"id": "1702.01090", "submitter": "Jaimie Murdock", "authors": "Jaimie Murdock and Colin Allen and Katy B\\\"orner and Robert Light and\n  Simon McAlister and Andrew Ravenscroft and Robert Rose and Doori Rose and Jun\n  Otsuka and David Bourget and John Lawrence and Chris Reed", "title": "Multi-level computational methods for interdisciplinary research in the\n  HathiTrust Digital Library", "comments": "revised, 29 pages, 3 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0184188", "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show how faceted search using a combination of traditional classification\nsystems and mixed-membership topic models can go beyond keyword search to\ninform resource discovery, hypothesis formulation, and argument extraction for\ninterdisciplinary research. Our test domain is the history and philosophy of\nscientific work on animal mind and cognition. The methods can be generalized to\nother research areas and ultimately support a system for semi-automatic\nidentification of argument structures. We provide a case study for the\napplication of the methods to the problem of identifying and extracting\narguments about anthropomorphism during a critical period in the development of\ncomparative psychology. We show how a combination of classification systems and\nmixed-membership models trained over large digital libraries can inform\nresource discovery in this domain. Through a novel approach of \"drill-down\"\ntopic modeling---simultaneously reducing both the size of the corpus and the\nunit of analysis---we are able to reduce a large collection of fulltext volumes\nto a much smaller set of pages within six focal volumes containing arguments of\ninterest to historians and philosophers of comparative psychology. The volumes\nidentified in this way did not appear among the first ten results of the\nkeyword search in the HathiTrust digital library and the pages bear the kind of\n\"close reading\" needed to generate original interpretations that is the heart\nof scholarly work in the humanities. Zooming back out, we provide a way to\nplace the books onto a map of science originally constructed from very\ndifferent data and for different purposes. The multilevel approach advances\nunderstanding of the intellectual and societal contexts in which writings are\ninterpreted.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 17:36:19 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 00:22:59 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Murdock", "Jaimie", ""], ["Allen", "Colin", ""], ["B\u00f6rner", "Katy", ""], ["Light", "Robert", ""], ["McAlister", "Simon", ""], ["Ravenscroft", "Andrew", ""], ["Rose", "Robert", ""], ["Rose", "Doori", ""], ["Otsuka", "Jun", ""], ["Bourget", "David", ""], ["Lawrence", "John", ""], ["Reed", "Chris", ""]]}, {"id": "1702.01151", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Wolfgang Nejdl, Avishek Anand", "title": "The Dawn of Today's Popular Domains: A Study of the Archived German Web\n  over 18 Years", "comments": "JCDL 2016, Newark, NJ, USA", "journal-ref": null, "doi": "10.1145/2910896.2910901", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web has been around and maturing for 25 years. The popular websites of\ntoday have undergone vast changes during this period, with a few being there\nalmost since the beginning and many new ones becoming popular over the years.\nThis makes it worthwhile to take a look at how these sites have evolved and\nwhat they might tell us about the future of the Web. We therefore embarked on a\nlongitudinal study spanning almost the whole period of the Web, based on data\ncollected by the Internet Archive starting in 1996, to retrospectively analyze\nhow the popular Web as of now has evolved over the past 18 years.\n  For our study we focused on the German Web, specifically on the top 100 most\npopular websites in 17 categories. This paper presents a selection of the most\ninteresting findings in terms of volume, size as well as age of the Web. While\nrelated work in the field of Web Dynamics has mainly focused on change rates\nand analyzed datasets spanning less than a year, we looked at the evolution of\nwebsites over 18 years. We found that around 70% of the pages we investigated\nare younger than a year, with an observed exponential growth in age as well as\nin size up to now. If this growth rate continues, the number of pages from the\npopular domains will almost double in the next two years. In addition, we give\ninsights into our data set, provided by the Internet Archive, which hosts the\nlargest and most complete Web archive as of today.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 20:45:56 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Nejdl", "Wolfgang", ""], ["Anand", "Avishek", ""]]}, {"id": "1702.01163", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Wolfram Sperber, Mila Runnwerth", "title": "Archiving Software Surrogates on the Web for Future Reference", "comments": "TPDL 2016, Hannover, Germany", "journal-ref": null, "doi": "10.1007/978-3-319-43997-6_17", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software has long been established as an essential aspect of the scientific\nprocess in mathematics and other disciplines. However, reliably referencing\nsoftware in scientific publications is still challenging for various reasons. A\ncrucial factor is that software dynamics with temporal versions or states are\ndifficult to capture over time. We propose to archive and reference surrogates\ninstead, which can be found on the Web and reflect the actual software to a\nremarkable extent. Our study shows that about a half of the webpages of\nsoftware are already archived with almost all of them including some kind of\ndocumentation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:17:25 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Sperber", "Wolfram", ""], ["Runnwerth", "Mila", ""]]}, {"id": "1702.01165", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Mila Runnwerth, Wolfram Sperber", "title": "Linking Mathematical Software in Web Archives", "comments": "ICMS 2016, Berlin, Germany", "journal-ref": null, "doi": "10.1007/978-3-319-42432-3_52", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web is our primary source of all kinds of information today. This\nincludes information about software as well as associated materials, like\nsource code, documentation, related publications and change logs. Such data is\nof particular importance in research in order to conduct, comprehend and\nreconstruct scientific experiments that involve software. swMATH, a\nmathematical software directory, attempts to identify software mentions in\nscientific articles and provides additional information as well as links to the\nWeb. However, just like software itself, the Web is dynamic and most likely the\ninformation on the Web has changed since it was referenced in a scientific\npublication. Therefore, it is crucial to preserve the resources of a software\non the Web to capture its states over time.\n  We found that around 40% of the websites in swMATH are already included in an\nexisting Web archive. Out of these, 60% of contain some kind of documentation\nand around 45% even provide downloads of software artifacts. Hence, already\ntoday links can be established based on the publication dates of corresponding\narticles. The contained data enable enriching existing information with a\ntemporal dimension. In the future, specialized infrastructure will improve the\ncoverage of software resources and allow explicit references in scientific\npublications.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:23:21 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Runnwerth", "Mila", ""], ["Sperber", "Wolfram", ""]]}, {"id": "1702.01172", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Thomas Risse", "title": "Insights into Entity Name Evolution on Wikipedia", "comments": "WISE 2014, Thessaloniki, Greece", "journal-ref": null, "doi": "10.1007/978-3-319-11746-1_4", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working with Web archives raises a number of issues caused by their temporal\ncharacteristics. Depending on the age of the content, additional knowledge\nmight be needed to find and understand older texts. Especially facts about\nentities are subject to change. Most severe in terms of information retrieval\nare name changes. In order to find entities that have changed their name over\ntime, search engines need to be aware of this evolution. We tackle this problem\nby analyzing Wikipedia in terms of entity evolutions mentioned in articles\nregardless the structural elements. We gathered statistics and automatically\nextracted minimum excerpts covering name changes by incorporating lists\ndedicated to that subject. In future work, these excerpts are going to be used\nto discover patterns and detect changes in other sources. In this work we\ninvestigate whether or not Wikipedia is a suitable source for extracting the\nrequired knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:36:46 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Risse", "Thomas", ""]]}, {"id": "1702.01176", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Thomas Risse", "title": "Named Entity Evolution Analysis on Wikipedia", "comments": "WebSci 2014, Bloomington, IN, USA. arXiv admin note: substantial text\n  overlap with arXiv:1702.01172", "journal-ref": null, "doi": "10.1145/2615569.2615639", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessing Web archives raises a number of issues caused by their temporal\ncharacteristics. Additional knowledge is needed to find and understand older\ntexts. Especially entities mentioned in texts are subject to change. Most\nsevere in terms of information retrieval are name changes. In order to find\nentities that have changed their name over time, search engines need to be\naware of this evolution. We tackle this problem by analyzing Wikipedia in terms\nof entity evolutions mentioned in articles. We present statistical data on\nexcerpts covering name changes, which will be used to discover similar text\npassages and extract evolution knowledge in future work.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:44:26 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Holzmann", "Helge", ""], ["Risse", "Thomas", ""]]}, {"id": "1702.01179", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Thomas Risse", "title": "Extraction of Evolution Descriptions from the Web", "comments": "Digital Libraries (JCDL) 2014, London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of named entities affects exploration and retrieval tasks in\ndigital libraries. An information retrieval system that is aware of name\nchanges can actively support users in finding former occurrences of evolved\nentities. However, current structured knowledge bases, such as DBpedia or\nFreebase, do not provide enough information about evolutions, even though the\ndata is available on their resources, like Wikipedia. Our \\emph{Evolution Base}\nprototype will demonstrate how excerpts describing name evolutions can be\nidentified on these websites with a promising precision. The descriptions are\nclassified by means of models that we trained based on a recent analysis of\nnamed entity evolutions on Wikipedia.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:48:18 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Risse", "Thomas", ""]]}, {"id": "1702.01187", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Nina Tahmasebi, Thomas Risse", "title": "Named Entity Evolution Recognition on the Blogosphere", "comments": "IJDL 2015", "journal-ref": "International Journal on Digital Libraries 2015, Volume 15, Issue\n  2, pp 209-235", "doi": "10.1007/s00799-014-0135-x", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in technology and culture lead to changes in our language. These\nchanges create a gap between the language known by users and the language\nstored in digital archives. It affects user's possibility to firstly find\ncontent and secondly interpret that content. In previous work we introduced our\napproach for Named Entity Evolution Recognition~(NEER) in newspaper\ncollections. Lately, increasing efforts in Web preservation lead to increased\navailability of Web archives covering longer time spans. However, language on\nthe Web is more dynamic than in traditional media and many of the basic\nassumptions from the newspaper domain do not hold for Web data. In this paper\nwe discuss the limitations of existing methodology for NEER. We approach these\nby adapting an existing NEER method to work on noisy data like the Web and the\nBlogosphere in particular. We develop novel filters that reduce the noise and\nmake use of Semantic Web resources to obtain more information about terms. Our\nevaluation shows the potentials of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 22:07:57 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Tahmasebi", "Nina", ""], ["Risse", "Thomas", ""]]}, {"id": "1702.02381", "submitter": "Camelia Mu\\~noz-Caro", "authors": "C. Mu\\~noz-Caro (1), A. Ni\\~no (1) and S. Reyes (1) ((1) SciCom\n  Research Group, Universidad de Castilla-La Mancha, Spain)", "title": "A bibliometric approach to Systematic Mapping Studies: The case of the\n  evolution and perspectives of community detection in complex networks", "comments": "27 pages; 5 figures; 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical analysis of the state of the art is a necessary task when\nidentifying new research lines worthwhile to pursue. To such an end, all the\navailable work related to the field of interest must be taken into account. The\nkey point is how to organize, analyze, and make sense of the huge amount of\nscientific literature available today on any topic. To tackle this problem, we\npresent here a bibliometric approach to Systematic Mapping Studies (SMS). Thus,\na modify SMS protocol is used relying on the scientific references metadata to\nextract, process and interpret the wealth of information contained in nowadays\nresearch literature. As a test case, the procedure is applied to determine the\ncurrent state and perspectives of community detection in complex networks. Our\nresults show that community detection is a still active, far from exhausted, in\ndevelopment, field. In addition, we find that, by far, the most exploited\nmethods are those related to determining hierarchical community structures. On\nthe other hand, the results show that fuzzy clustering techniques, despite\ntheir interest, are underdeveloped as well as the adaptation of existing\nalgorithms to parallel or, more specifically, distributed, computational\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 11:36:35 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Mu\u00f1oz-Caro", "C.", ""], ["Ni\u00f1o", "A.", ""], ["Reyes", "S.", ""]]}, {"id": "1702.02808", "submitter": "Frank Havemann", "authors": "Frank Havemann, Jochen Gl\\\"aser, and Michael Heinz", "title": "Memetic search for overlapping topics based on a local evaluation of\n  link communities", "comments": "Accepted for a special issue of journal Scientometrics edited by J.\n  Gl\\\"{a}ser, A. Scharnhorst, & W. Gl\\\"{a}nzel: Same data -- different results?\n  Towards a comparative approach to the identification of thematic structures\n  in science. 32 pages, 13 figures, 6 tables. We here used some sentences from\n  our unpublished arXiv paper 1501.05139", "journal-ref": "Scientometrics, 2017, volume 111(2), pages 1089-1118", "doi": "10.1007/s11192-017-2302-5", "report-no": null, "categories": "cs.SI cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In spite of recent advances in field delineation methods, bibliometricians\nstill don't know the extent to which their topic detection algorithms\nreconstruct `ground truths', i.e. thematic structures in the scientific\nliterature. In this paper, we demonstrate a new approach to the delineation of\nthematic structures that attempts to match the algorithm to theoretically\nderived and empirically observed properties all thematic structures have in\ncommon. We cluster citation links rather than publication nodes, use\npredominantly local information and search for communities of links starting\nfrom seed subgraphs in order to allow for pervasive overlaps of topics. We\nevaluate sets of links with a new cost function and assume that local minima in\nthe cost landscape correspond to link communities. Because this cost landscape\nhas many local minima we define a valid community as the community with the\nlowest minimum within a certain range. Since finding all valid communities is\nimpossible for large networks, we designed a memetic algorithm that combines\nprobabilistic evolutionary strategies with deterministic local searches. We\napply our approach to a network of about 15,000 Astronomy & Astrophysics papers\npublished 2010 and their cited sources, and to a network of about 100,000\nAstronomy & Astrophysics papers (published 2003--2010) which are linked through\ndirect citations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 12:39:19 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Havemann", "Frank", ""], ["Gl\u00e4ser", "Jochen", ""], ["Heinz", "Michael", ""]]}, {"id": "1702.03411", "submitter": "Nees Jan van Eck", "authors": "Nees Jan van Eck and Ludo Waltman", "title": "Citation-based clustering of publications using CitNetExplorer and\n  VOSviewer", "comments": "25 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering scientific publications in an important problem in bibliometric\nresearch. We demonstrate how two software tools, CitNetExplorer and VOSviewer,\ncan be used to cluster publications and to analyze the resulting clustering\nsolutions. CitNetExplorer is used to cluster a large set of publications in the\nfield of astronomy and astrophysics. The publications are clustered based on\ndirect citation relations. CitNetExplorer and VOSviewer are used together to\nanalyze the resulting clustering solutions. Both tools use visualizations to\nsupport the analysis of the clustering solutions, with CitNetExplorer focusing\non the analysis at the level of individual publications and VOSviewer focusing\non the analysis at an aggregate level. The demonstration provided in this paper\nshows how a clustering of publications can be created and analyzed using freely\navailable software tools. Using the approach presented in this paper,\nbibliometricians are able to carry out sophisticated cluster analyses without\nthe need to have a deep knowledge of clustering techniques and without\nrequiring advanced computer skills.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 11:25:35 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["van Eck", "Nees Jan", ""], ["Waltman", "Ludo", ""]]}, {"id": "1702.03790", "submitter": "Ralph Ewerth", "authors": "Markus M\\\"uhling, Manja Meister, Nikolaus Korfhage, J\\\"org Wehling,\n  Angelika H\\\"orth, Ralph Ewerth, Bernd Freisleben", "title": "Content-Based Video Retrieval in Historical Collections of the German\n  Broadcasting Archive", "comments": "TPDL 2016, Hannover, Germany. Final version is available at Springer\n  via DOI", "journal-ref": null, "doi": "10.1007/978-3-319-43997-6_6", "report-no": null, "categories": "cs.DL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The German Broadcasting Archive (DRA) maintains the cultural heritage of\nradio and television broadcasts of the former German Democratic Republic (GDR).\nThe uniqueness and importance of the video material stimulates a large\nscientific interest in the video content. In this paper, we present an\nautomatic video analysis and retrieval system for searching in historical\ncollections of GDR television recordings. It consists of video analysis\nalgorithms for shot boundary detection, concept classification, person\nrecognition, text recognition and similarity search. The performance of the\nsystem is evaluated from a technical and an archival perspective on 2,500 hours\nof GDR television recordings.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 14:42:31 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["M\u00fchling", "Markus", ""], ["Meister", "Manja", ""], ["Korfhage", "Nikolaus", ""], ["Wehling", "J\u00f6rg", ""], ["H\u00f6rth", "Angelika", ""], ["Ewerth", "Ralph", ""], ["Freisleben", "Bernd", ""]]}, {"id": "1702.03991", "submitter": "Enrique Ordu\\~na-Malea", "authors": "Enrique Orduna-Malea, Alberto Martin-Martin and Emilio Delgado\n  Lopez-Cozar", "title": "Google Scholar and the gray literature: A reply to Bonato's review", "comments": "17 pages, 14 figures, 1 table", "journal-ref": null, "doi": null, "report-no": "EC3 Working Papers, 23", "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a review concluded that Google Scholar (GS) is not a suitable\nsource of information \"for identifying recent conference papers or other gray\nliterature publications\". The goal of this letter is to demonstrate that GS can\nbe an effective tool to search and find gray literature, as long as appropriate\nsearch strategies are used. To do this, we took as examples the same two case\nstudies used by the original review, describing first how GS processes\noriginal's search strategies, then proposing alternative search strategies, and\nfinally generalizing each case study to compose a general search procedure\naimed at finding gray literature in Google Scholar for two wide selected case\nstudies: a) all contributions belonging to a congress (the ASCO Annual\nMeeting); and b) indexed guidelines as well as gray literature within medical\ninstitutions (National Institutes of Health) and governmental agencies (U.S.\nDepartment of Health & Human Services). The results confirm that original\nsearch strategies were undertrained offering misleading results and erroneous\nconclusions. Google Scholar lacks many of the advanced search features\navailable in other bibliographic databases (such as Pubmed), however, it is one\nthing to have a friendly search experience, and quite another to find gray\nliterature. We finally conclude that Google Scholar is a powerful tool for\nsearching gray literature, as long as the users are familiar with all the\npossibilities it offers as a search engine. Poorly formulated searches will\nundoubtedly return misleading results.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 21:27:05 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Orduna-Malea", "Enrique", ""], ["Martin-Martin", "Alberto", ""], ["Lopez-Cozar", "Emilio Delgado", ""]]}, {"id": "1702.04584", "submitter": "Lina Antonietta Coppola", "authors": "Lina Antonietta Coppola", "title": "Developing an ontology for the access to the contents of an archival\n  fonds: the case of the Catasto Gregoriano", "comments": "in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research was proposed to exploit and extend the relational and contextual\nnature of the information assets of the Catasto Gregoriano, kept at the\nArchivio di Stato in Rome. Developed within the MODEUS project (Making Open\nData Effectively Usable), this study originates from the following key ideas of\nMODEUS: to require Open Data to be expressed in terms of an ontology, and to\ninclude such an ontology as a documentation of the data themselves. Thus, Open\nData are naturally linked by means of the ontology, which meets the\nrequirements of the Linked Open Data vision.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 12:58:39 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Coppola", "Lina Antonietta", ""]]}, {"id": "1702.04614", "submitter": "Dmitry Lande", "authors": "D.V. Lande, V.B. Andrushchenko, I.V. Balagura", "title": "Wiki-index of authors popularity", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new index of the author's popularity estimation is represented in the\npaper. The index is calculated on the basis of Wikipedia encyclopedia analysis\n(Wikipedia Index - WI). Unlike the conventional existed citation indices, the\nsuggested mark allows to evaluate not only the popularity of the author, as it\ncan be done by means of calculating the general citation number or by the\nHirsch index, which is often used to measure the author's research rate. The\nindex gives an opportunity to estimate the author's popularity, his/her\ninfluence within the sought-after area \"knowledge area\" in the Internet - in\nthe Wikipedia. The suggested index is supposed to be calculated in frames of\nthe subject domain, and it, on the one hand, avoids the mistaken computation of\nthe homonyms, and on the other hand - provides the entirety of the subject\narea. There are proposed algorithms and the technique of the Wikipedia Index\ncalculation through the network encyclopedia sounding, the exemplified\ncalculations of the index for the prominent researchers, and also the methods\nof the information networks formation - models of the subject domains by the\nautomatic monitoring and networks information reference resources analysis. The\nconsidered in the paper notion network corresponds the terms-heads of the\nWikipedia articles.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 14:28:43 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Lande", "D. V.", ""], ["Andrushchenko", "V. B.", ""], ["Balagura", "I. V.", ""]]}, {"id": "1702.04946", "submitter": "Shenghui Wang", "authors": "Shenghui Wang, Rob Koopman", "title": "Clustering articles based on semantic similarity", "comments": "Special Issue of Scientometrics: Same data - different results?\n  Towards a comparative approach to the identification of thematic structures\n  in science", "journal-ref": null, "doi": "10.1007/s11192-017-2298-x", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document clustering is generally the first step for topic identification.\nSince many clustering methods operate on the similarities between documents, it\nis important to build representations of these documents which keep their\nsemantics as much as possible and are also suitable for efficient similarity\ncalculation. The metadata of articles in the Astro dataset contribute to a\nsemantic matrix, which uses a vector space to capture the semantics of entities\nderived from these articles and consequently supports the contextual\nexploration of these entities in LittleAriadne. However, this semantic matrix\ndoes not allow to calculate similarities between articles directly. In this\npaper, we will describe in detail how we build a semantic representation for an\narticle from the entities that are associated with it. Base on such semantic\nrepresentations of articles, we apply two standard clustering methods, K-Means\nand the Louvain community detection algorithm, which leads to our two\nclustering solutions labelled as OCLC-31 (standing for K-Means) and\nOCLC-Louvain (standing for Louvain). In this paper, we will give the\nimplementation details and a basic comparison with other clustering solutions\nthat are reported in this special issue.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 12:48:54 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Wang", "Shenghui", ""], ["Koopman", "Rob", ""]]}, {"id": "1702.05112", "submitter": "Evgeny Lipachev K", "authors": "Alexander Elizarov, Alexander Kirillovich, Evgeny Lipachev, and Olga\n  Nevzorova", "title": "OntoMath Digital Ecosystem: Ontologies, Mathematical Knowledge Analytics\n  and Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the basic ideas, approaches and results of\ndeveloping of mathematical knowledge management technologies based on\nontologies. These solutions form the basis of a specialized digital ecosystem\nOntoMath which consists of the ontology of the logical structure of\nmathematical documents Mocassin and ontology of mathematical knowledge\nOntoMathPRO, tools of text analysis, recommender system and other applications\nto manage mathematical knowledge. The studies are in according to the ideas of\ncreating a distributed system of interconnected repositories of digitized\nversions of mathematical documents and project to create a World Digital\nMathematical Library.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 19:14:00 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Elizarov", "Alexander", ""], ["Kirillovich", "Alexander", ""], ["Lipachev", "Evgeny", ""], ["Nevzorova", "Olga", ""]]}, {"id": "1702.05379", "submitter": "Philipp Singer", "authors": "Philipp Singer, Florian Lemmerich, Robert West, Leila Zia, Ellery\n  Wulczyn, Markus Strohmaier, Jure Leskovec", "title": "Why We Read Wikipedia", "comments": "Published in WWW'17; v2 fixes caption of Table 3", "journal-ref": null, "doi": "10.1145/3038912.3052716", "report-no": null, "categories": "cs.SI cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wikipedia is one of the most popular sites on the Web, with millions of users\nrelying on it to satisfy a broad range of information needs every day. Although\nit is crucial to understand what exactly these needs are in order to be able to\nmeet them, little is currently known about why users visit Wikipedia. The goal\nof this paper is to fill this gap by combining a survey of Wikipedia readers\nwith a log-based analysis of user activity. Based on an initial series of user\nsurveys, we build a taxonomy of Wikipedia use cases along several dimensions,\ncapturing users' motivations to visit Wikipedia, the depth of knowledge they\nare seeking, and their knowledge of the topic of interest prior to visiting\nWikipedia. Then, we quantify the prevalence of these use cases via a\nlarge-scale user survey conducted on live Wikipedia with almost 30,000\nresponses. Our analyses highlight the variety of factors driving users to\nWikipedia, such as current events, media coverage of a topic, personal\ncuriosity, work or school assignments, or boredom. Finally, we match survey\nresponses to the respondents' digital traces in Wikipedia's server logs,\nenabling the discovery of behavioral patterns associated with specific use\ncases. For instance, we observe long and fast-paced page sequences across\ntopics for users who are bored or exploring randomly, whereas those using\nWikipedia for work or school spend more time on individual articles focused on\ntopics such as science. Our findings advance our understanding of reader\nmotivations and behavior on Wikipedia and can have implications for developers\naiming to improve Wikipedia's user experience, editors striving to cater to\ntheir readers' needs, third-party services (such as search engines) providing\naccess to Wikipedia content, and researchers aiming to build tools such as\nrecommendation engines.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:16:04 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 10:08:20 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Singer", "Philipp", ""], ["Lemmerich", "Florian", ""], ["West", "Robert", ""], ["Zia", "Leila", ""], ["Wulczyn", "Ellery", ""], ["Strohmaier", "Markus", ""], ["Leskovec", "Jure", ""]]}, {"id": "1702.05671", "submitter": "Li Menghui", "authors": "Hongguang Dong, Menghui Li, Ru Liu, Chensheng Wu, Jinshan Wu", "title": "Allometric Scaling in Scientific Fields", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": "10.1007/s11192-017-2333-y", "report-no": null, "categories": "physics.soc-ph cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allometric scaling can reflect underlying mechanisms, dynamics and structures\nin complex systems; examples include typical scaling laws in biology, ecology\nand urban development. In this work, we study allometric scaling in scientific\nfields. By performing an analysis of the outputs/inputs of various scientific\nfields, including the numbers of publications, citations, and references, with\nrespect to the number of authors, we find that in all fields that we have\nstudied thus far, including physics, mathematics and economics, there are\nallometric scaling laws relating the outputs/inputs and the sizes of scientific\nfields. Furthermore, the exponents of the scaling relations have remained quite\nstable over the years. We also find that the deviations of individual subfields\nfrom the overall scaling laws are good indicators for ranking subfields\nindependently of their sizes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 06:49:39 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 06:47:15 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Dong", "Hongguang", ""], ["Li", "Menghui", ""], ["Liu", "Ru", ""], ["Wu", "Chensheng", ""], ["Wu", "Jinshan", ""]]}, {"id": "1702.06176", "submitter": "Ilya Safro", "authors": "Justin Sybrandt, Michael Shtutman, Ilya Safro", "title": "MOLIERE: Automatic Biomedical Hypothesis Generation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI q-bio.QM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis generation is becoming a crucial time-saving technique which\nallows biomedical researchers to quickly discover implicit connections between\nimportant concepts. Typically, these systems operate on domain-specific\nfractions of public medical data. MOLIERE, in contrast, utilizes information\nfrom over 24.5 million documents. At the heart of our approach lies a\nmulti-modal and multi-relational network of biomedical objects extracted from\nseveral heterogeneous datasets from the National Center for Biotechnology\nInformation (NCBI). These objects include but are not limited to scientific\npapers, keywords, genes, proteins, diseases, and diagnoses. We model hypotheses\nusing Latent Dirichlet Allocation applied on abstracts found near shortest\npaths discovered within this network, and demonstrate the effectiveness of\nMOLIERE by performing hypothesis generation on historical data. Our network,\nimplementation, and resulting data are all publicly available for the broad\nscientific community.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 21:09:44 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 22:29:28 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 19:34:47 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Sybrandt", "Justin", ""], ["Shtutman", "Michael", ""], ["Safro", "Ilya", ""]]}, {"id": "1702.07481", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff, Dieter Franz Kogler and Bowen Yan", "title": "Mapping Patent Classifications: Portfolio and Statistical Analysis, and\n  the Comparison of Strengths and Weaknesses", "comments": "Scientometrics 112(3) (2017) 1573-1591;\n  http://link.springer.com/article/10.1007/s11192-017-2449-0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cooperative Patent Classifications (CPC) jointly developed by the\nEuropean and US Patent Offices provide a new basis for mapping and portfolio\nanalysis. This update provides an occasion for rethinking the parameter\nchoices. The new maps are significantly different from previous ones, although\nthis may not always be obvious on visual inspection. Since these maps are\nstatistical constructs based on index terms, their quality--as different from\nutility--can only be controlled discursively. We provide nested maps online and\na routine for portfolio overlays and further statistical analysis. We add a new\ntool for \"difference maps\" which is illustrated by comparing the portfolios of\npatents granted to Novartis and MSD in 2016.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 07:29:46 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 13:01:52 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Kogler", "Dieter Franz", ""], ["Yan", "Bowen", ""]]}, {"id": "1702.07960", "submitter": "Qiang Wu", "authors": "Xingchen Li, Qiang Wu, Nan Zhang", "title": "Citation personal display: A case study of personal websites by\n  physicists in 11 well-known universities", "comments": "25 pages, 4 figures, Journal of Documentation, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to investigate the extent to which researchers display\ncitation, and wants to examine whether there are researcher differences in\ncitation personal display at the level of university, country, and academic\nrank. Physicists in 11 well-known universities in USA, Britain, and China were\nchosen as the object of study. It was manually identified if physicists had\nmentioned citation counts, citation-based indices, or a link to Google Scholar\nCitations (GSC) on the personal websites. A chi-square test is constructed to\ntest researcher differences in citation personal display. Results showed that\nthe overall proportion of citation personal display is not high (14.8%), with\n129 of 870 physicists displaying citation. And physicists from different\nwell-known universities indeed had a significant difference in citation\npersonal display. Moreover, at the national level, it was noticed that\nphysicists in well-known Chinese universities had the highest level of citation\npersonal display, followed by Britain and the USA. Further, this study also\nfound that researchers who had the academic rank of professor had the highest\ncitation personal display. In addition, the differences in h-index personal\ndisplay by university, country or academic rank were analyzed, and the results\nshowed that they were not statistically significant.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 00:46:11 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Li", "Xingchen", ""], ["Wu", "Qiang", ""], ["Zhang", "Nan", ""]]}, {"id": "1702.08070", "submitter": "William Rowe", "authors": "William Rowe, Paul D. Dobson, Bede Constantinides, and Mark Platt", "title": "PubTree: A Hierarchical Search Tool for the MEDLINE Database", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keeping track of the ever-increasing body of scientific literature is an\nescalating challenge. We present PubTree a hierarchical search tool that\nefficiently searches the PubMed/MEDLINE dataset based upon a decision tree\nconstructed using >26 million abstracts. The tool is implemented as a webpage,\nwhere users are asked a series of eighteen questions to locate pertinent\narticles. The implementation of this hierarchical search tool highlights issues\nendemic with document retrieval. However, the construction of this tree\nindicates that with future developments hierarchical search could become an\neffective tool (or adjunct) in the mining of biological literature.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 19:09:59 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Rowe", "William", ""], ["Dobson", "Paul D.", ""], ["Constantinides", "Bede", ""], ["Platt", "Mark", ""]]}, {"id": "1702.08199", "submitter": "Shenghui Wang", "authors": "Rob Koopman, Shenghui Wang", "title": "Mutual Information based labelling and comparing clusters", "comments": "Special Issue of Scientometrics: Same data - different results?\n  Towards a comparative approach to the identification of thematic structures\n  in science", "journal-ref": null, "doi": "10.1007/s11192-017-2305-2", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After a clustering solution is generated automatically, labelling these\nclusters becomes important to help understanding the results. In this paper, we\npropose to use a Mutual Information based method to label clusters of journal\narticles. Topical terms which have the highest Normalised Mutual Information\n(NMI) with a certain cluster are selected to be the labels of the cluster.\nDiscussion of the labelling technique with a domain expert was used as a check\nthat the labels are discriminating not only lexical-wise but also semantically.\nBased on a common set of topical terms, we also propose to generate lexical\nfingerprints as a representation of individual clusters. Eventually, we\nvisualise and compare these fingerprints of different clusters from either one\nclustering solution or different ones.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 09:23:46 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Koopman", "Rob", ""], ["Wang", "Shenghui", ""]]}, {"id": "1702.08210", "submitter": "Shenghui Wang", "authors": "Rob Koopman, Shenghui Wang, Andrea Scharnhorst", "title": "Contextualization of topics: Browsing through the universe of\n  bibliographic information", "comments": "Special Issue of Scientometrics: Same data - different results?\n  Towards a comparative approach to the identification of thematic structures\n  in science", "journal-ref": null, "doi": "10.1007/s11192-017-2303-4", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how semantic indexing can help to generate a contextual\noverview of topics and visually compare clusters of articles. The method was\noriginally developed for an innovative information exploration tool, called\nAriadne, which operates on bibliographic databases with tens of millions of\nrecords. In this paper, the method behind Ariadne is further developed and\napplied to the research question of the special issue \"Same data, different\nresults\" - the better understanding of topic (re-)construction by different\nbibliometric approaches. For the case of the Astro dataset of 111,616 articles\nin astronomy and astrophysics, a new instantiation of the interactive exploring\ntool, LittleAriadne, has been created. This paper contributes to the overall\nchallenge to delineate and define topics in two different ways. First, we\nproduce two clustering solutions based on vector representations of articles in\na lexical space. These vectors are built on semantic indexing of entities\nassociated with those articles. Second, we discuss how LittleAriadne can be\nused to browse through the network of topical terms, authors, journals,\ncitations and various cluster solutions of the Astro dataset. More\nspecifically, we treat the assignment of an article to the different clustering\nsolutions as an additional element of its bibliographic record. Keeping the\nprinciple of semantic indexing on the level of such an extended list of\nentities of the bibliographic record, LittleAriadne in turn provides a\nvisualization of the context of a specific clustering solution. It also conveys\nthe similarity of article clusters produced by different algorithms, hence\nrepresenting a complementary approach to other possible means of comparison.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 10:01:08 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Koopman", "Rob", ""], ["Wang", "Shenghui", ""], ["Scharnhorst", "Andrea", ""]]}]