[{"id": "1804.00026", "submitter": "Jinhyuk Yun", "authors": "Jisung Yoon, Jinhyuk Yun, Woo-Sung Jung", "title": "Build up of a subject classification system from collective intelligence", "comments": "14 pages, 8 figures", "journal-ref": "New Phys.: Sae Mulli 2018; 68: 647~654", "doi": "10.3938/NPSM.68.647", "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematized subject classification is essential for funding and assessing\nscientific projects. Conventionally, classification schemes are founded on the\nempirical knowledge of the group of experts; thus, the experts' perspectives\nhave influenced the current systems of scientific classification. Those systems\narchived the current state-of-art in practice, yet the global effect of the\naccelerating scientific change over time has made the updating of the\nclassifications system on a timely basis vertually impossible. To overcome the\naforementioned limitations, we propose an unbiased classification scheme that\ntakes advantage of collective knowledge; Wikipedia, an Internet encyclopedia\nedited by millions of users, sets a prompt classification in a collective\nfashion. We construct a Wikipedia network for scientific disciplines and\nextract the backbone of the network. This structure displays a landscape of\nscience and technology that is based on a collective intelligence and that is\nmore unbiased and adaptable than conventional classifications.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 18:36:17 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 02:56:41 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Yoon", "Jisung", ""], ["Yun", "Jinhyuk", ""], ["Jung", "Woo-Sung", ""]]}, {"id": "1804.01062", "submitter": "Andrej Kastrin", "authors": "Andrej Kastrin, Jelena Klisara, Borut Lu\\v{z}ar, Janez Povh", "title": "Is science driven by principal investigators?", "comments": "26 pages, 18 figures", "journal-ref": null, "doi": "10.1007/s11192-018-2900-x", "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the scientific and career performance of principal\ninvestigators (PI's) of publicly funded research projects compared to\nscientific performance of all researchers. Our study is based on high quality\ndata about (i) research projects awarded in Slovenia in the period 1994-2016\n(7508 projects with 2725 PI's in total) and (ii) about scientific productivity\nof all researchers in Slovenia that were active in the period 1970-2016 - there\nare 19598 such researchers in total, including the PI's. We compare average\nproductivity, collaboration, internationality and interdisciplinarity of PI's\nand of all active researchers. Our analysis shows that for all four indicators\nthe average performance of PI's is much higher compared to average performance\nof all active researchers. Additionally, we analyze careers of both groups of\nresearchers. The results show that the PI's have on average longer and more\nfruitful career compared to all active researchers, with regards to all career\nindicators. The PI's that have received a postdoc grant have at the beginning\noutstanding scientific performance, but later deviate towards average. On long\nrun, the PI's leading the research programs (the most prestigious grants) on\naverage demonstrate the best scientific performance. In the last part of the\npaper we study 23 co-authorship networks, spanned by all active researchers in\nthe periods 1970-1994, ..., 1970-2016. We find out that they are well connected\nand that PI's are well distributed across these networks forming their\nbackbones. Even more, PI's generate new PI's, since more than 90% of new PI's\nare connected (have at least one joint scientific publication) with existing\nPI's. We believe that our study sheds new light to the relations between the\npublic funding of the science and the scientific output and can be considered\nas an affirmative answer to the question posed in the title.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 16:51:26 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2018 08:49:14 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Kastrin", "Andrej", ""], ["Klisara", "Jelena", ""], ["Lu\u017ear", "Borut", ""], ["Povh", "Janez", ""]]}, {"id": "1804.01603", "submitter": "Martin Klein", "authors": "Martin Klein and Lyudmila Balakireva and Herbert Van de Sompel", "title": "Focused Crawl of Web Archives to Build Event Collections", "comments": "accepted for publication at WebSci 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event collections are frequently built by crawling the live web on the basis\nof seed URIs nominated by human experts. Focused web crawling is a technique\nwhere the crawler is guided by reference content pertaining to the event. Given\nthe dynamic nature of the web and the pace with which topics evolve, the timing\nof the crawl is a concern for both approaches. We investigate the feasibility\nof performing focused crawls on the archived web. By utilizing the Memento\ninfrastructure, we obtain resources from 22 web archives that contribute to\nbuilding event collections. We create collections on four events and compare\nthe relevance of their resources to collections built from crawling the live\nweb as well as from a manually curated collection. Our results show that\nfocused crawling on the archived web can be done and indeed results in highly\nrelevant collections, especially for events that happened further in the past.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 20:40:47 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Klein", "Martin", ""], ["Balakireva", "Lyudmila", ""], ["Van de Sompel", "Herbert", ""]]}, {"id": "1804.02445", "submitter": "Noah Siegel", "authors": "Noah Siegel, Nicholas Lourie, Russell Power, Waleed Ammar", "title": "Extracting Scientific Figures with Distantly Supervised Neural Networks", "comments": "10 pages, 5 figures, paper accepted at JCDL 2018", "journal-ref": null, "doi": "10.1145/3197026.3197040", "report-no": null, "categories": "cs.DL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-textual components such as charts, diagrams and tables provide key\ninformation in many scientific documents, but the lack of large labeled\ndatasets has impeded the development of data-driven methods for scientific\nfigure extraction. In this paper, we induce high-quality training labels for\nthe task of figure extraction in a large number of scientific documents, with\nno human intervention. To accomplish this we leverage the auxiliary data\nprovided in two large web collections of scientific documents (arXiv and\nPubMed) to locate figures and their associated captions in the rasterized PDF.\nWe share the resulting dataset of over 5.5 million induced labels---4,000 times\nlarger than the previous largest figure extraction dataset---with an average\nprecision of 96.8%, to enable the development of modern data-driven methods for\nthis task. We use this dataset to train a deep neural network for end-to-end\nfigure detection, yielding a model that can be more easily extended to new\ndomains compared to previous work. The model was successfully deployed in\nSemantic Scholar, a large-scale academic search engine, and used to extract\nfigures in 13 million scientific documents.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 20:22:47 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 19:13:53 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Siegel", "Noah", ""], ["Lourie", "Nicholas", ""], ["Power", "Russell", ""], ["Ammar", "Waleed", ""]]}, {"id": "1804.02751", "submitter": "Christoph Carl Kling", "authors": "Maryam Mehrazar, Christoph Carl Kling, Steffen Lemke, Athanasios\n  Mazarakis, Isabella Peters", "title": "Can We Count on Social Media Metrics? First Insights into the Active\n  Scholarly Use of Social Media", "comments": "Accepted at 10th ACM Conference on Web Science, Amsterdam", "journal-ref": null, "doi": "10.1145/3201064.3201101", "report-no": null, "categories": "cs.SI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring research impact is important for ranking publications in academic\nsearch engines and for research evaluation. Social media metrics or altmetrics\nmeasure the impact of scientific work based on social media activity.\nAltmetrics are complementary to traditional, citation-based metrics, e.g.\nallowing the assessment of new publications for which citations are not yet\navailable. Despite the increasing importance of altmetrics, their\ncharacteristics are not well understood: Until now it has not been researched\nwhat kind of researchers are actively using which social media services and why\n- important questions for scientific impact prediction. Based on a survey among\n3,430 scientists, we uncover previously unknown and significant differences\nbetween social media services: We identify services which attract young and\nexperienced researchers, respectively, and detect differences in usage\nmotivations. Our findings have direct implications for the future design of\naltmetrics for scientific impact prediction.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 20:13:59 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Mehrazar", "Maryam", ""], ["Kling", "Christoph Carl", ""], ["Lemke", "Steffen", ""], ["Mazarakis", "Athanasios", ""], ["Peters", "Isabella", ""]]}, {"id": "1804.02760", "submitter": "Allison Morgan", "authors": "Allison C. Morgan, Samuel F. Way, Aaron Clauset", "title": "Automatically assembling a full census of an academic field", "comments": "11 pages, 6 figures, 2 tables", "journal-ref": "PLoS ONE 13(8), e0202223 (2018)", "doi": "10.1371/journal.pone.0202223", "report-no": null, "categories": "cs.DL cs.CY cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The composition of the scientific workforce shapes the direction of\nscientific research, directly through the selection of questions to\ninvestigate, and indirectly through its influence on the training of future\nscientists. In most fields, however, complete census information is difficult\nto obtain, complicating efforts to study workforce dynamics and the effects of\npolicy. This is particularly true in computer science, which lacks a single,\nall-encompassing directory or professional organization. A full census of\ncomputer science would serve many purposes, not the least of which is a better\nunderstanding of the trends and causes of unequal representation in computing.\nPrevious academic census efforts have relied on narrow or biased samples, or on\nprofessional society membership rolls. A full census can be constructed\ndirectly from online departmental faculty directories, but doing so by hand is\nprohibitively expensive and time-consuming. Here, we introduce a topical web\ncrawler for automating the collection of faculty information from web-based\ndepartment rosters, and demonstrate the resulting system on the 205\nPhD-granting computer science departments in the U.S. and Canada. This method\nconstructs a complete census of the field within a few minutes, and achieves\nover 99% precision and recall. We conclude by comparing the resulting 2017\ncensus to a hand-curated 2011 census to quantify turnover and retention in\ncomputer science, in general and for female faculty in particular,\ndemonstrating the types of analysis made possible by automated census\nconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 21:34:57 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 05:19:20 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Morgan", "Allison C.", ""], ["Way", "Samuel F.", ""], ["Clauset", "Aaron", ""]]}, {"id": "1804.02773", "submitter": "Attila Varga", "authors": "Attila Varga", "title": "Novelty and Foreseeing Research Trends; The Case of Astrophysics and\n  Astronomy", "comments": null, "journal-ref": null, "doi": "10.3847/1538-4365/aab765", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metrics based on reference lists of research articles or on keywords have\nbeen used to predict citation impact. The concept behind such metrics is that\noriginal ideas stem from the reconfiguration of the structure of past\nknowledge, and therefore atypical combinations in the reference lists,\nkeywords, or classification codes indicate future high impact research. The\ncurrent paper serves as an introduction to this line of research for\nastronomers and also addresses some methodological questions of this field of\ninnovation studies. It is still not clear if the choice of particular indexes,\nsuch as references to journals, articles, or specific bibliometric\nclassification codes would affect the relationship between atypical\ncombinations and citation impact. To understand more aspects of the innovation\nprocess, a new metric has been devised to measure to what extent researchers\nare able to anticipate the changing combinatorial trends of the future. Results\nshow that the variant of the latter anticipation scores that is based on paper\ncombinations is a good predictor of future citation impact of scholarly works.\nThe study also shows that the effect of tested indexes vary with the\naggregation level that was used to construct them. A detailed analysis of\ncombinatorial novelty in the field reveals that certain sub-fields of astronomy\nand astrophysics have different roles in the reconfiguration in past knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 23:03:01 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Varga", "Attila", ""]]}, {"id": "1804.03446", "submitter": "Nicolas Robinson-Garcia", "authors": "Daniel Torres-Salinas, Nicolas Robinson-Garcia, Enrique Herrera-Viedma\n  and Evaristo Jim\\'enez-Contreras", "title": "Methodological considerations on the use of the normalized impact in the\n  Severo Ochoa and Mar\\'ia de Maetzu programmes", "comments": "Paper published in Spanish language in El profesional de la\n  informaci\\'on", "journal-ref": "Torres-Salinas, D., et al. (2018) Consideraciones metodologicas\n  sobre el uso del impacto normalizado en las convocatorias Severo Ochoa y\n  Maria de Maetzu. El profesional de la informacion, 27(2), 367-374", "doi": "10.3145/epi.2018.mar.15", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2011, the programme for Severo Ochoa Centers of Excellence and Mar\\'ia de\nMaeztu Units of Excellence was launched for the first time. Since this\nprogramme has become one of the axes of the Spanish scientific policy. 186\nmillion euros have been distributed and 26 centers and 16 units have been\naccredited. One of the most relevant criteria for submission is the need for\nguarantor researchers to have a Normalized Impact of 1.5. In this work, we\ncritically analyze the origin of this bibliometric indicator rooted in the\n1980s, the different variants that have been proposed and the limitations its\nuse in this programme have. Finally, we offer a series of practical\nrecommendations for a more accurate use of normalized impact indicators for\nevaluative purposes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 10:50:37 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Torres-Salinas", "Daniel", ""], ["Robinson-Garcia", "Nicolas", ""], ["Herrera-Viedma", "Enrique", ""], ["Jim\u00e9nez-Contreras", "Evaristo", ""]]}, {"id": "1804.03713", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Ingo Frommholz, Guillaume Cabanac", "title": "Report on the 7th International Workshop on Bibliometric-enhanced\n  Information Retrieval (BIR 2018)", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bibliometric-enhanced Information Retrieval (BIR) workshop series has\nstarted at ECIR in 2014 and serves as the annual gathering of IR researchers\nwho address various information-related tasks on scientific corpora and\nbibliometrics. We welcome contributions elaborating on dedicated IR systems, as\nwell as studies revealing original characteristics on how scientific knowledge\nis created, communicated, and used. This report presents all accepted papers at\nthe 7th BIR workshop at ECIR 2018 in Grenoble, France.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 20:47:22 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Mayr", "Philipp", ""], ["Frommholz", "Ingo", ""], ["Cabanac", "Guillaume", ""]]}, {"id": "1804.03869", "submitter": "Ludo Waltman", "authors": "Nees Jan van Eck and Ludo Waltman", "title": "Analyzing the activities of visitors of the Leiden Ranking website", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide a better understanding of the way in which university rankings are\nused, we present a detailed analysis of the activities of visitors of a\nuniversity ranking website. We use the website of the CWTS Leiden Ranking for\nthis purpose. We for instance study the countries from which visitors\noriginate, the specific pages on the Leiden Ranking website that they visit,\nthe countries or the universities that they find of special interest, and the\nindicators that they focus on. In addition, we also discuss two experiments\nthat were carried out on the Leiden Ranking website. Our analysis does not only\nprovide new insights into the use of university rankings, but it also suggests\npossible ways in which these rankings can be improved.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 08:33:36 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 20:59:50 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["van Eck", "Nees Jan", ""], ["Waltman", "Ludo", ""]]}, {"id": "1804.04105", "submitter": "Qing Ke", "authors": "Qing Ke", "title": "Comparing scientific and technological impact of biomedical research", "comments": null, "journal-ref": "Journal of Informetrics 12, 706-717 (2018)", "doi": "10.1016/j.joi.2018.06.010", "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the number of citations that a scholarly paper receives from\nother papers is used as the proxy of its scientific impact. Yet citations can\ncome from domains outside the scientific community, and one such example is\nthrough patented technologies---paper can be cited by patents, achieving\ntechnological impact. While the scientific impact of papers has been\nextensively studied, the technological aspect remains less known in the\nliterature. Here we aim to fill this gap by presenting a comparative study on\nhow 919 thousand biomedical papers are cited by U.S. patents and by other\npapers over time. We observe a positive correlation between citations from\npatents and from papers, but there is little overlap between the two domains in\neither the most cited papers, or papers with the most delayed recognition. We\nalso find that the two types of citations exhibit distinct temporal variations,\nwith patent citations lagging behind paper citations for a median of 6 years\nfor the majority of papers. Our work contributes to the understanding of the\ntechnological impact of papers.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:22:19 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 21:31:10 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Ke", "Qing", ""]]}, {"id": "1804.04956", "submitter": "Moritz Schubotz", "authors": "Moritz Schubotz, Andre Greiner-Petter, Philipp Scharpf, Norman\n  Meuschke, Howard Cohl, Bela Gipp", "title": "Improving the Representation and Conversion of Mathematical Formulae by\n  Considering their Textual Context", "comments": "10 pages, 4 figures", "journal-ref": "Proceedings of the ACM/IEEE-CS Joint Conference on Digital\n  Libraries (JCDL), Jun. 2018, Fort Worth, USA", "doi": "10.1145/3197026.3197058", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical formulae represent complex semantic information in a concise\nform. Especially in Science, Technology, Engineering, and Mathematics,\nmathematical formulae are crucial to communicate information, e.g., in\nscientific papers, and to perform computations using computer algebra systems.\nEnabling computers to access the information encoded in mathematical formulae\nrequires machine-readable formats that can represent both the presentation and\ncontent, i.e., the semantics, of formulae. Exchanging such information between\nsystems additionally requires conversion methods for mathematical\nrepresentation formats. We analyze how the semantic enrichment of formulae\nimproves the format conversion process and show that considering the textual\ncontext of formulae reduces the error rate of such conversions. Our main\ncontributions are: (1) providing an openly available benchmark dataset for the\nmathematical format conversion task consisting of a newly created test\ncollection, an extensive, manually curated gold standard and task-specific\nevaluation metrics; (2) performing a quantitative evaluation of\nstate-of-the-art tools for mathematical format conversions; (3) presenting a\nnew approach that considers the textual context of formulae to reduce the error\nrate for mathematical format conversions. Our benchmark dataset facilitates\nfuture research on mathematical format conversions as well as research on many\nproblems in mathematical information retrieval. Because we annotated and linked\nall components of formulae, e.g., identifiers, operators and other entities, to\nWikidata entries, the gold standard can, for instance, be used to train methods\nfor formula concept discovery and recognition. Such methods can then be applied\nto improve mathematical information retrieval systems, e.g., for semantic\nformula search, recommendation of mathematical content, or detection of\nmathematical plagiarism.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 14:08:16 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Schubotz", "Moritz", ""], ["Greiner-Petter", "Andre", ""], ["Scharpf", "Philipp", ""], ["Meuschke", "Norman", ""], ["Cohl", "Howard", ""], ["Gipp", "Bela", ""]]}, {"id": "1804.05024", "submitter": "Andrea Scharnhorst", "authors": "Ludo Waltman, Sybille Hinze, Andrea Scharnhorst, Jesper Wiborg\n  Schneider, Theresa Velden", "title": "Exploration of reproducibility issues in scientometric research Part 1:\n  Direct reproducibility", "comments": "STI2018 Leiden, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the first part of a small-scale explorative study in an effort to\nstart assessing reproducibility issues specific to scientometrics research.\nThis effort is motivated by the desire to generate empirical data to inform\ndebates about reproducibility in scientometrics. Rather than attempt to\nreproduce studies, we explore how we might assess \"in principle\"\nreproducibility based on a critical review of the content of published papers.\nThe first part of the study focuses on direct reproducibility - that is the\nability to reproduce the specific evidence produced by an original study using\nthe same data, methods, and procedures. The second part (Velden et al. 2018) is\ndedicated to conceptual reproducibility - that is the robustness of knowledge\nclaims towards verification by an alternative approach using different data,\nmethods and procedures. The study is exploratory: it investigates only a very\nlimited number of publications and serves us to develop instruments for\nidentifying potential reproducibility issues of published studies: These are a\ncategorization of study types and a taxonomy of threats to reproducibility. We\nwork with a select sample of five publications in scientometrics covering a\nvariation of study types of theoretical, methodological, and empirical nature.\nBased on observations made during our exploratory review, we conclude this\npaper with open questions on how to approach and assess the status of direct\nreproducibility in scientometrics, intended for discussion at the special track\non \"Reproducibility in Scientometrics\" at STI2018 in Leiden.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 16:49:38 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Waltman", "Ludo", ""], ["Hinze", "Sybille", ""], ["Scharnhorst", "Andrea", ""], ["Schneider", "Jesper Wiborg", ""], ["Velden", "Theresa", ""]]}, {"id": "1804.05026", "submitter": "Andrea Scharnhorst", "authors": "Theresa Velden, Sybille Hinze, Andrea Scharnhorst, Jesper Wiborg\n  Schneider, Ludo Waltman", "title": "Exploration of Reproducibility Issues in Scientometric Research Part 2:\n  Conceptual Reproducibility", "comments": "submitted STI2018 Leiden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the second part of a small-scale explorative study in an effort to\nassess reproducibility issues specific to scientometrics research. This effort\nis motivated by the desire to generate empirical data to inform debates about\nreproducibility in scientometrics. Rather than attempt to reproduce studies, we\nexplore how we might assess \"in principle\" reproducibility based on a critical\nreview of the content of published papers. While the first part of the study\n(Waltman et al. 2018) focuses on direct reproducibility - that is the ability\nto reproduce the specific evidence produced by an original study using the same\ndata, methods, and procedures, this second part is dedicated to conceptual\nreproducibility - that is the robustness of knowledge claims towards\nverification by an alternative approach using different data, methods and\nprocedures. The study is exploratory: it investigates only a very limited\nnumber of publications and serves us to develop instruments for identifying\npotential reproducibility issues of published studies: These are a\ncategorization of study types and a taxonomy of threats to reproducibility. We\nwork with a select sample of five publications in scientometrics covering a\nvariation of study types of theoretical, methodological, and empirical nature.\nBased on observations made during our exploratory review, we conclude with open\nquestions on how to approach and assess the status of conceptual\nreproducibility in scientometrics intended for discussion at the special track\non \"Reproducibility in Scientometrics\" at STI2018 in Leiden.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 16:54:41 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Velden", "Theresa", ""], ["Hinze", "Sybille", ""], ["Scharnhorst", "Andrea", ""], ["Schneider", "Jesper Wiborg", ""], ["Waltman", "Ludo", ""]]}, {"id": "1804.05365", "submitter": "Enrique Ordu\\~na-Malea", "authors": "Enrique Orduna-Malea and Emilio Delgado Lopez-Cozar", "title": "Dimensions: re-discovering the ecosystem of scientific information", "comments": "17 pages, 7 figures, 5 tables", "journal-ref": "El Profesional de la Informacion, 27(2), 420-431 (2018)", "doi": "10.3145/epi.2018.mar.21", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overarching aim of this work is to provide a detailed description of the\nfree version of Dimensions (new bibliographic database produced by Digital\nScience and launched in January 2018). To do this, the work is divided into two\ndifferentiated blocks. First, its characteristics, operation and features are\ndescribed, focusing on its main strengths and weaknesses. Secondly, an analysis\nof its coverage is carried out (comparing it Scopus and Google Scholar) in\norder to determine whether the bibliometric indicators offered by Dimensions\nhave an order of magnitude significant enough to be used. To this end, an\nanalysis is carried out at three levels: journals (sample of 20 publications in\n'Library & Information Science'), documents (276 articles published by the\nJournal of informetrics between 2013 and 2015) and authors (28 people awarded\nwith the Derek de Solla Price prize). Preliminary results indicate that\nDimensions has coverage of the recent literature superior to Scopus although\ninferior to Google Scholar. With regard to the number of citations received,\nDimensions offers slightly lower figures than Scopus. Despite this, the number\nof citations in Dimensions exhibits a strong correlation with Scopus and\nsomewhat less (although still significant) with Google Scholar. For this\nreason, it is concluded that Dimensions is an alternative for carrying out\ncitation studies, being able to rival Scopus (greater coverage and free of\ncharge) and with Google Scholar (greater functionalities for the treatment and\ndata export).\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 15:10:45 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Orduna-Malea", "Enrique", ""], ["Lopez-Cozar", "Emilio Delgado", ""]]}, {"id": "1804.05492", "submitter": "Bernadette Boscoe", "authors": "Bernadette M. Boscoe (Randles), Irene V. Pasquetto, Milena S. Golshan,\n  Christine L. Borgman", "title": "Using the Jupyter Notebook as a Tool for Open Science: An Empirical\n  Study", "comments": null, "journal-ref": "2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL) (2017).\n  Toronto, ON, Canada. June 19, 2017 to June 23, 2017, ISBN: 978-1-5386-3862-0\n  pp: 1-2", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As scientific work becomes more computational and data intensive, research\nprocesses and results become more difficult to interpret and reproduce. In this\nposter, we show how the Jupyter notebook, a tool originally designed as a free\nversion of Mathematica notebooks, has evolved to become a robust tool for\nscientists to share code, associated computation, and documentation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 03:40:10 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Boscoe", "Bernadette M.", "", "Randles"], ["Pasquetto", "Irene V.", ""], ["Golshan", "Milena S.", ""], ["Borgman", "Christine L.", ""]]}, {"id": "1804.05514", "submitter": "Mayank Singh", "authors": "Mayank Singh, Pradeep Dogga, Sohan Patro, Dhiraj Barnwal, Ritam Dutt,\n  Rajarshi Haldar, Pawan Goyal and Animesh Mukherjee", "title": "CL Scholar: The ACL Anthology Knowledge Graph Miner", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CL Scholar, the ACL Anthology knowledge graph miner to facilitate\nhigh-quality search and exploration of current research progress in the\ncomputational linguistics community. In contrast to previous works,\nperiodically crawling, indexing and processing of new incoming articles is\ncompletely automated in the current system. CL Scholar utilizes both textual\nand network information for knowledge graph construction. As an additional\nnovel initiative, CL Scholar supports more than 1200 scholarly natural language\nqueries along with standard keyword-based search on constructed knowledge\ngraph. It answers binary, statistical and list based natural language queries.\nThe current system is deployed at http://cnerg.iitkgp.ac.in/aclakg. We also\nprovide REST API support along with bulk download facility. Our code and data\nare available at https://github.com/CLScholar.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 06:15:06 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Singh", "Mayank", ""], ["Dogga", "Pradeep", ""], ["Patro", "Sohan", ""], ["Barnwal", "Dhiraj", ""], ["Dutt", "Ritam", ""], ["Haldar", "Rajarshi", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1804.05599", "submitter": "Alberto Mart\\'in-Mart\\'in", "authors": "Alberto Mart\\'in-Mart\\'in, Enrique Orduna-Malea, Emilio Delgado\n  L\\'opez-C\\'ozar", "title": "Author-level metrics in the new academic profile platforms: The online\n  behaviour of the Bibliometrics community", "comments": "26 pages, 6 figures, 7 tables", "journal-ref": "2018, Journal of Informetrics, 12(2), 494-509", "doi": "10.1016/j.joi.2018.04.001", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new web-based academic communication platforms do not only enable\nresearchers to better advertise their academic outputs, making them more\nvisible than ever before, but they also provide a wide supply of metrics to\nhelp authors better understand the impact their work is making. This study has\nthree objectives: a) to analyse the uptake of some of the most popular\nplatforms (Google Scholar Citations, ResearcherID, ResearchGate, Mendeley and\nTwitter) by a specific scientific community (bibliometrics, scientometrics,\ninformetrics, webometrics, and altmetrics); b) to compare the metrics available\nfrom each platform; and c) to determine the meaning of all these new metrics.\nTo do this, the data available in these platforms about a sample of 811 authors\n(researchers in bibliometrics for whom a public profile Google Scholar\nCitations was found) were extracted. A total of 31 metrics were analysed. The\nresults show that a high number of the analysed researchers only had a profile\nin Google Scholar Citations (159), or only in Google Scholar Citations and\nResearchGate (142). Lastly, we find two kinds of metrics of online impact.\nFirst, metrics related to connectivity (followers), and second, all metrics\nassociated to academic impact. This second group can further be divided into\nusage metrics (reads, views), and citation metrics. The results suggest that\nGoogle Scholar Citations is the source that provides more comprehensive\ncitation-related data, whereas Twitter stands out in connectivity-related\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 10:29:08 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Mart\u00edn-Mart\u00edn", "Alberto", ""], ["Orduna-Malea", "Enrique", ""], ["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""]]}, {"id": "1804.05942", "submitter": "Justin Sybrandt", "authors": "Justin Sybrandt, Angelo Carrabba, Alexander Herzog, Ilya Safro", "title": "Are Abstracts Enough for Hypothesis Generation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential for automatic hypothesis generation (HG) systems to improve\nresearch productivity keeps pace with the growing set of publicly available\nscientific information. But as data becomes easier to acquire, we must\nunderstand the effect different textual data sources have on our resulting\nhypotheses. Are abstracts enough for HG, or does it need full-text papers? How\nmany papers does an HG system need to make valuable predictions? How sensitive\nis a general-purpose HG system to hyperparameter values or input quality? What\neffect does corpus size and document length have on HG results? To answer these\nquestions we train multiple versions of knowledge network-based HG system,\nMoliere, on varying corpora in order to compare challenges and trade offs in\nterms of result quality and computational requirements. Moliere generalizes\nmain principles of similar knowledge network-based HG systems and reinforces\nthem with topic modeling components. The corpora include the abstract and\nfull-text versions of PubMed Central, as well as iterative halves of MEDLINE,\nwhich allows us to compare the effect document length and count has on the\nresults. We find that, quantitatively, corpora with a higher median document\nlength result in marginally higher quality results, yet require substantially\nlonger to process. However, qualitatively, full-length papers introduce a\nsignificant number of intruder terms to the resulting topics, which decreases\nhuman interpretability. Additionally, we find that the effect of document\nlength is greater than that of document count, even if both sets contain only\npaper abstracts. Reproducibility: Our code and data are available at\ngithub.com/jsybran/moliere, and bit.ly/2GxghpM respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 17:08:45 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 17:36:53 GMT"}, {"version": "v3", "created": "Sat, 20 Oct 2018 19:57:05 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Sybrandt", "Justin", ""], ["Carrabba", "Angelo", ""], ["Herzog", "Alexander", ""], ["Safro", "Ilya", ""]]}, {"id": "1804.05957", "submitter": "Donald Comeau", "authors": "Donald C. Comeau, Chih-Hsuan Wei, Rezarta Islamaj Do\\u{g}an and\n  Zhiyong Lu", "title": "PMC text mining subset in BioC: 2.3 million full text articles and\n  growing", "comments": "8 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in full text mining biomedical research articles is growing. NCBI\nprovides the PMC Open Access and Author Manuscript sets of articles which are\navailable for text mining. We have made all of these articles available in\nBioC, an XML and JSON format which is convenient for sharing text, annotations,\nand relations. These articles are available both via ftp for bulk download and\nvia a Web API for updates or more focused collection. Availability:\nhttps://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 21:55:28 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Comeau", "Donald C.", ""], ["Wei", "Chih-Hsuan", ""], ["Do\u011fan", "Rezarta Islamaj", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1804.06169", "submitter": "Mandy Neumann", "authors": "Mandy Neumann, Christopher Michels, Philipp Schaer, Ralf Schenkel", "title": "Prioritizing and Scheduling Conferences for Metadata Harvesting in dblp", "comments": "submitted to JCDL 2018", "journal-ref": null, "doi": "10.1145/3197026.3197069", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining literature databases and online bibliographies is a core\nresponsibility of metadata aggregators such as digital libraries. In the\nprocess of monitoring all the available data sources the question arises which\ndata source should be prioritized. Based on a broad definition of information\nquality we are looking for different ways to find the best fitting and most\npromising conference candidates to harvest next. We evaluate different\nconference ranking features by using a pseudo-relevance assessment and a\ncomponent-based evaluation of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 11:28:01 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Neumann", "Mandy", ""], ["Michels", "Christopher", ""], ["Schaer", "Philipp", ""], ["Schenkel", "Ralf", ""]]}, {"id": "1804.06426", "submitter": "Philipp Mayr", "authors": "Zeljko Carevic, Sascha Sch\\\"uller, Philipp Mayr, Norbert Fuhr", "title": "Contextualised Browsing in a Digital Library's Living Lab", "comments": "10 pages, 2 figures, paper accepted at JCDL 2018", "journal-ref": null, "doi": "10.1145/3197026.3197054", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualisation has proven to be effective in tailoring \\linebreak search\nresults towards the users' information need. While this is true for a basic\nquery search, the usage of contextual session information during exploratory\nsearch especially on the level of browsing has so far been underexposed in\nresearch. In this paper, we present two approaches that contextualise browsing\non the level of structured metadata in a Digital Library (DL), (1) one variant\nbases on document similarity and (2) one variant utilises implicit session\ninformation, such as queries and different document metadata encountered during\nthe session of a users. We evaluate our approaches in a living lab environment\nusing a DL in the social sciences and compare our contextualisation approaches\nagainst a non-contextualised approach. For a period of more than three months\nwe analysed 47,444 unique retrieval sessions that contain search activities on\nthe level of browsing. Our results show that a contextualisation of browsing\nsignificantly outperforms our baseline in terms of the position of the first\nclicked item in the result set. The mean rank of the first clicked document\n(measured as mean first relevant - MFR) was 4.52 using a non-contextualised\nranking compared to 3.04 when re-ranking the result lists based on similarity\nto the previously viewed document. Furthermore, we observed that both\ncontextual approaches show a noticeably higher click-through rate. A\ncontextualisation based on document similarity leads to almost twice as many\ndocument views compared to the non-contextualised ranking.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 18:30:29 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Carevic", "Zeljko", ""], ["Sch\u00fcller", "Sascha", ""], ["Mayr", "Philipp", ""], ["Fuhr", "Norbert", ""]]}, {"id": "1804.06648", "submitter": "Danny Kingsley Dr", "authors": "Katie Shamash and Dr Danny Kingsley", "title": "ArXiv and the REF open access policy", "comments": "21 pages, 13 figures, 2 appendices. Version 2 has an amended abstract\n  to align with the abstract in the record", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  HEFCE's Policy for open access in the post-2014 Research Excellence Framework\nstates \"authors' outputs must have been deposited in an institutional or\nsubject repository\". There is no definition of a subject repository in the\npolicy: however, there is a footnote stating: \"Individuals depositing their\noutputs in a subject repository are advised to ensure that their chosen\nrepository meets the requirements set out in this policy.\" The longest standing\nsubject repository (or repository of any kind) is arXiv.org, established in\n1991. arXiv is an open access repository of scientific research available to\nauthors and researchers worldwide and acts as a scholarly communications forum\ninformed and guided by scientists. Content held on arXiv is free to the end\nuser and researchers can deposit their content freely. As of April 2018, arXiv\nheld over 1,377,000 eprints. In some disciplines arXiv is considered essential\nto the sharing and publication of research. The HEFCE requirements on\nrepositories are defined in the Information and Audit Requirements which lists\nthe \"Accepted date\", the \"Version of deposited file\" and \"available open access\nimmediately after the publisher embargo\" are expected as part of the REF\nsubmission. However, while many records in arXiv have multiple versions of\nwork, the Author's Accepted Manuscript is not identified and there is no field\nto record the acceptance date of the work. Because arXiv does not capture these\ntwo specific information points it does not meet the technical requirements to\nbe a compliant subject repository for the purposes of REF. This paper is\npresenting the case that articles deposited to arXiv are, in general, compliant\nwith the requirements of the HEFCE policy. The paper summarises some work\nundertaken by Jisc to establish if there are other factors that can indicate\nthe likelihood of formal compliance to the HEFCE policy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 10:51:19 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 14:38:07 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Shamash", "Katie", ""], ["Kingsley", "Dr Danny", ""]]}, {"id": "1804.07185", "submitter": "Thomas Gutberlet", "authors": "T. Gutberlet, D. Tunger, P. Zeitler, T. Br\\\"uckel", "title": "Do neutrons publish? A neutron publication survey 2005-2015", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publication in scientific journals is the main product of scientific\nresearch. The amount of papers published, their placement in high impact\njournals, and their citations are used as a measure of the productivity of\nindividual scientists, institutes or fields of science. To give a profound\nbasis on the publication record and the quality of the publication efforts in\nneutron scattering, a survey has been done following the approach to use\nbibliographic databases. Questions to be addressed by this survey are: Is the\nproductivity of research with neutrons changing over the years? Which is the\ngeographic distribution in this field of research? Which ones are leading\nfacilities? Is the quality of publications changing? The main results found are\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 14:17:57 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Gutberlet", "T.", ""], ["Tunger", "D.", ""], ["Zeitler", "P.", ""], ["Br\u00fcckel", "T.", ""]]}, {"id": "1804.07268", "submitter": "Alberto Mart\\'in-Mart\\'in", "authors": "Enrique Ordu\\~na-Malea, Juan M. Ayll\\'on, Alberto Mart\\'in-Mart\\'in,\n  Emilio Delgado L\\'opez-C\\'ozar", "title": "The lost academic home: institutional affiliation links in Google\n  Scholar Citations", "comments": null, "journal-ref": "2017, Online Information Review, 41(6), 762-781", "doi": "10.1108/OIR-10-2016-0302", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the new affiliation feature available in Google-Scholar\nCitations revealing that the affiliation-tool works well for most-institutions,\nit is unable to detect all existing institutions in database, and it is not\nalways able to create unique-standardized entry for each-institution.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 16:46:26 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Ordu\u00f1a-Malea", "Enrique", ""], ["Ayll\u00f3n", "Juan M.", ""], ["Mart\u00edn-Mart\u00edn", "Alberto", ""], ["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""]]}, {"id": "1804.07708", "submitter": "Afshin Sadeghi", "authors": "Afshin Sadeghi, Mahdi Jaberzadeh Ansari, Johannes Wilm, Christoph\n  Lange", "title": "A Survey of User Expectations and Tool Limitations in Collaborative\n  Scientific Authoring and Reviewing", "comments": "This is the version of the article before review and it is currently\n  under review. This version is subject to updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative scientific authoring is increasingly being supported by\nsoftware tools. Traditionally, desktop-based authoring tools had the most\nadvanced editing features, allowed for more formatting options, and included\nmore import/export filters. Web-based tools have excelled in their\ncollaboration support. Recently, developers on both sides have been trying to\nclose this gap by extending desktop-based tools to better support collaboration\nand by making web-based tools richer in functionality. To verify to what extent\nthese developments actually meet the needs of researchers, we gathered precise\nrequirements towards better tool support for scientific authoring and reviewing\nworkflows by interviewing 213 users and studying a corpus of 27 documents. We\npresent the design of the survey and interpret its results. The conclusion is\nthat WYSIWYG and offline desktop authoring tools continue to be more popular\namong academics than text-based and online editors.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 16:24:24 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Sadeghi", "Afshin", ""], ["Ansari", "Mahdi Jaberzadeh", ""], ["Wilm", "Johannes", ""], ["Lange", "Christoph", ""]]}, {"id": "1804.08342", "submitter": "John Collomosse", "authors": "John Collomosse, Tu Bui, Alan Brown, John Sheridan, Alex Green, Mark\n  Bell, Jamie Fawcett, Jez Higgins, Olivier Thereaux", "title": "ARCHANGEL: Trusted Archives of Digital Public Documents", "comments": "Submitted to ACM Document Engineering 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ARCHANGEL; a de-centralised platform for ensuring the long-term\nintegrity of digital documents stored within public archives. Document\nintegrity is fundamental to public trust in archives. Yet currently that trust\nis built upon institutional reputation --- trust at face value in a centralised\nauthority, like a national government archive or University. ARCHANGEL proposes\na shift to a technological underscoring of that trust, using distributed ledger\ntechnology (DLT) to cryptographically guarantee the provenance, immutability\nand so the integrity of archived documents. We describe the ARCHANGEL\narchitecture, and report on a prototype of that architecture build over the\nEthereum infrastructure. We report early evaluation and feedback of ARCHANGEL\nfrom stakeholders in the research data archives space.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 11:24:34 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Collomosse", "John", ""], ["Bui", "Tu", ""], ["Brown", "Alan", ""], ["Sheridan", "John", ""], ["Green", "Alex", ""], ["Bell", "Mark", ""], ["Fawcett", "Jamie", ""], ["Higgins", "Jez", ""], ["Thereaux", "Olivier", ""]]}, {"id": "1804.08435", "submitter": "Eva Isaksson", "authors": "Eva Isaksson, Henrik Vesterinen", "title": "Evaluation of research publications and publication channels in\n  astronomy and astrophysics", "comments": "9 pages, 9 figures. Library and Information Services in Astronomy\n  (LISA) 8 conference, Strasbourg June 6-9, 2017. To appear in EPJ Web of\n  Conferences", "journal-ref": null, "doi": "10.1051/epjconf/201818606002", "report-no": null, "categories": "cs.DL astro-ph.IM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The astronomy community usually turns to the Astrophysics Data System for\nbibliometrics. When the context is cross-disciplinary, commercial products like\nWeb of Science and Scopus are used along with related analytics tools instead.\nThe results are often tainted by inherent problems in the chosen classification\nsystem. A review of the most common challenges and pitfalls is given.\n  Commercial altmetrics products could be added to the evaluation toolbox in\nthe near future despite the fact that they are best suited for promotion\ninstead of evaluation.\n  Norway, Denmark, and Finland have created journal and publisher ranking\nsystems that are used in national funding models. Differences in how astronomy\njournals are weighed in these systems night be related to the volume of papers\npublished on a national level.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 13:53:33 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Isaksson", "Eva", ""], ["Vesterinen", "Henrik", ""]]}, {"id": "1804.08479", "submitter": "Serhii Nazarovets", "authors": "Serhii Nazarovets", "title": "Black Open Access in Ukraine: Analysis of Downloading Sci-Hub\n  Publications by Ukrainian Internet Users", "comments": null, "journal-ref": "Nauka innov. 2018, 14(2):19-26", "doi": "10.15407/scin14.02.019", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction. High subscription fees to scholarly research journals provoke\nresearchers to use illegal channels of access to scientific information.\nAnalysis of statistical data on downloads of scholarly research papers by\nUkrainian Internet users from illegal web resource can help to define gaps in\ninformation provision at the institutional or the state level for each\nscientific field. Problem Statement. To conduct an analysis of behavior and\ngeography of downloads of scholarly research publications from illegal web\nresource Sci-Hub by Ukrainian Internet users within the period from September\n1, 2015 to February 29, 2016. Purpose. To assess the information needs of\nUkrainian researchers who download scientific papers from Sci-Hub. Materials\nand Methods. The used file is available at public domain and contains complete\ndata of downloads of scholarly research articles from Sci-Hub for the period\nfrom September 1, 2015 to February 29, 2016. Inquiries of users with Ukrainian\nIP-addresses have been selected. Using DOI of downloaded articles enables\nfinding the publishers and journal brands with the help of CrossRef API,\nwhereas using the All Science Journal Classification (ASJC) codes makes it\npossible to identify the subject. Results. The study has shown that the most\ndocuments downloaded related to natural sciences (primarily, chemistry,\nphysics, and astronomy), with Elsevier publications being the most frequently\ninquired by Ukrainian users of Sci-Hub and Internet users from Kyiv downloading\nthe papers most actively. Conclusion. The obtained data are important for\nunderstanding the information needs of Ukrainian researchers and can be used to\nformulate an optimal subscription policy for providing access to information\nresources at Ukrainian R&D institutions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 14:44:55 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Nazarovets", "Serhii", ""]]}, {"id": "1804.08559", "submitter": "Srijan Kumar", "authors": "Srijan Kumar, Neil Shah", "title": "False Information on Web and Social Media: A Survey", "comments": "To appear in the book titled Social Media Analytics: Advances and\n  Applications, by CRC press, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False information can be created and spread easily through the web and social\nmedia platforms, resulting in widespread real-world impact. Characterizing how\nfalse information proliferates on social platforms and why it succeeds in\ndeceiving readers are critical to develop efficient detection algorithms and\ntools for early detection. A recent surge of research in this area has aimed to\naddress the key issues using methods based on feature engineering, graph\nmining, and information modeling. Majority of the research has primarily\nfocused on two broad categories of false information: opinion-based (e.g., fake\nreviews), and fact-based (e.g., false news and hoaxes). Therefore, in this\nwork, we present a comprehensive survey spanning diverse aspects of false\ninformation, namely (i) the actors involved in spreading false information,\n(ii) rationale behind successfully deceiving readers, (iii) quantifying the\nimpact of false information, (iv) measuring its characteristics across\ndifferent dimensions, and finally, (iv) algorithms developed to detect false\ninformation. In doing so, we create a unified framework to describe these\nrecent methods and highlight a number of important directions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 16:52:49 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Kumar", "Srijan", ""], ["Shah", "Neil", ""]]}, {"id": "1804.09479", "submitter": "Alberto Mart\\'in-Mart\\'in", "authors": "Alberto Mart\\'in-Mart\\'in, Enrique Orduna-Malea, Emilio Delgado\n  L\\'opez-C\\'ozar", "title": "Coverage of highly-cited documents in Google Scholar, Web of Science,\n  and Scopus: a multidisciplinary comparison", "comments": "11 pages, 3 tables, 1 figure. Accepted for publication in\n  Scientometrics", "journal-ref": "Scientometrics, 116(3), 2175-2188, 2018", "doi": "10.1007/s11192-018-2820-9", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study explores the extent to which bibliometric indicators based on\ncounts of highly-cited documents could be affected by the choice of data\nsource. The initial hypothesis is that databases that rely on journal selection\ncriteria for their document coverage may not necessarily provide an accurate\nrepresentation of highly-cited documents across all subject areas, while\ninclusive databases, which give each document the chance to stand on its own\nmerits, might be better suited to identify highly-cited documents. To test this\nhypothesis, an analysis of 2,515 highly-cited documents published in 2006 that\nGoogle Scholar displays in its Classic Papers product is carried out at the\nlevel of broad subject categories, checking whether these documents are also\ncovered in Web of Science and Scopus, and whether the citation counts offered\nby the different sources are similar. The results show that a large fraction of\nhighly-cited documents in the Social Sciences and Humanities (8.6%-28.2%) are\ninvisible to Web of Science and Scopus. In the Natural, Life, and Health\nSciences the proportion of missing highly-cited documents in Web of Science and\nScopus is much lower. Furthermore, in all areas, Spearman correlation\ncoefficients of citation counts in Google Scholar, as compared to Web of\nScience and Scopus citation counts, are remarkably strong (.83-.99). The main\nconclusion is that the data about highly-cited documents available in the\ninclusive database Google Scholar does indeed reveal significant coverage\ndeficiencies in Web of Science and Scopus in several areas of research.\nTherefore, using these selective databases to compute bibliometric indicators\nbased on counts of highly-cited documents might produce biased assessments in\npoorly covered areas.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 11:04:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 12:45:50 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 16:10:41 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Mart\u00edn-Mart\u00edn", "Alberto", ""], ["Orduna-Malea", "Enrique", ""], ["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""]]}, {"id": "1804.10436", "submitter": "Pablo Dorta-Gonzalez", "authors": "Pablo Dorta-Gonz\\'alez, Yolanda Santana-Jim\\'enez", "title": "Characterizing the highly cited articles: a large-scale bibliometric\n  analysis of the top 1% most cited research", "comments": "23 pages, 6 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We conducted a large-scale analysis of around 10,000 scientific articles,\nfrom the period 2007-2016, to study the bibliometric or formal aspects\ninfluencing citations. A transversal analysis was conducted disaggregating the\narticles into more than one hundred scientific areas and two groups, one\nexperimental and one control, each with a random sample of around five thousand\ndocuments. The experimental group comprised a random sample of the top 1% most\ncited articles in each field and year of publication (highly cited articles),\nand the control group a random sample of the remaining articles in the Journal\nCitation Reports (science and social science citation indexes in the Web of\nScience database). As the main result, highly cited articles differ from\nnon-highly cited articles in most of the bibliometric aspects considered. There\nare significant differences, below the 0.01 level, between the groups of\narticles in many variables and areas. The highly cited articles are published\nin journals of higher impact factor (33 percentile points above) and have 25%\nhigher co-authorship. The highly cited articles are also longer in terms of\nnumber of pages (10% higher) and bibliographical references (35% more).\nFinally, highly cited articles have slightly shorter titles (3% lower) but,\ncontrastingly, longer abstracts (10% higher).\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 10:57:45 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Dorta-Gonz\u00e1lez", "Pablo", ""], ["Santana-Jim\u00e9nez", "Yolanda", ""]]}, {"id": "1804.10439", "submitter": "Alberto Mart\\'in-Mart\\'in", "authors": "Alberto Mart\\'in-Mart\\'in, Enrique Orduna-Malea, Anne-Wil Harzing,\n  Emilio Delgado L\\'opez-C\\'ozar", "title": "Can we use Google Scholar to identify highly-cited documents?", "comments": null, "journal-ref": "Mart\\'in-Mart\\'in, A., Orduna-Malea, E., Harzing, A.-W., & Delgado\n  L\\'opez-C\\'ozar, E. (2017). Can we use Google Scholar to identify\n  highly-cited documents? Journal of Informetrics, 11(1), 152-163", "doi": "10.1016/j.joi.2016.11.008", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of this paper is to empirically test whether the\nidentification of highly-cited documents through Google Scholar is feasible and\nreliable. To this end, we carried out a longitudinal analysis (1950 to 2013),\nrunning a generic query (filtered only by year of publication) to minimise the\neffects of academic search engine optimisation. This gave us a final sample of\n64,000 documents (1,000 per year). The strong correlation between a document's\ncitations and its position in the search results (r= -0.67) led us to conclude\nthat Google Scholar is able to identify highly-cited papers effectively. This,\ncombined with Google Scholar's unique coverage (no restrictions on document\ntype and source), makes the academic search engine an invaluable tool for\nbibliometric research relating to the identification of the most influential\nscientific documents. We find evidence, however, that Google Scholar ranks\nthose documents whose language (or geographical web domain) matches with the\nuser's interface language higher than could be expected based on citations.\nNonetheless, this language effect and other factors related to the Google\nScholar's operation, i.e. the proper identification of versions and the date of\npublication, only have an incidental impact. They do not compromise the ability\nof Google Scholar to identify the highly-cited papers.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 11:02:21 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Mart\u00edn-Mart\u00edn", "Alberto", ""], ["Orduna-Malea", "Enrique", ""], ["Harzing", "Anne-Wil", ""], ["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""]]}, {"id": "1804.10530", "submitter": "Ellie Small", "authors": "Ellie Small, Javier Cabrera, John B. Kostis, William Kostis", "title": "Abstract Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed an application that will take a \"MEDLINE\" output from the\nPubMed database and allows the user to cluster all non-trivial words of the\nabstracts of the PubMed output. The number of clusters to use can be selected\nby the user.\n  A specific cluster may be selected, and the PMIDs and dates for all\npublications in the selected cluster are displayed underneath. See figure 2,\nwhere cluster 12 is selected.\n  The application also has an \"Abstracts\" tab, where the abstracts for the\nselected cluster can be perused. Here, it is also possible to download a HTML\nfile containing the PMID, date, title, and abstract for each publication in the\nselected cluster.\n  A third tab is called \"Titles\", where all the titles for the selected cluster\nare displayed.\n  Via a \"Use Cluster\" button, the selected Cluster can itself be clustered. A\n\"Back\" button allows the user to return to any previous state.\n  Finally, it is also possible to exclude documents whose abstracts contain\ncertain words (see figure 3).\n  The application will allow researchers to enter general search terms in the\nPubMed search engine, then use the application to search for publications of\nspecial interest within those search terms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 15:21:42 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Small", "Ellie", ""], ["Cabrera", "Javier", ""], ["Kostis", "John B.", ""], ["Kostis", "William", ""]]}, {"id": "1804.11209", "submitter": "Alberto Mart\\'in-Mart\\'in", "authors": "Alberto Mart\\'in-Mart\\'in, Enrique Orduna-Malea, Emilio Delgado\n  L\\'opez-C\\'ozar", "title": "A novel method for depicting academic disciplines through Google Scholar\n  Citations: The case of Bibliometrics", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.02412", "journal-ref": "Mart\\'in-Mart\\'in, A., Orduna-Malea, E., & Delgado\n  L\\'opez-C\\'ozar, E. (2018). A novel method for depicting academic disciplines\n  through Google Scholar Citations: The case of Bibliometrics. Scientometrics,\n  114(3), 1251-1273", "doi": "10.1007/s11192-017-2587-4", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a procedure to generate a snapshot of the structure of\na specific scientific community and their outputs based on the information\navailable in Google Scholar Citations (GSC). We call this method MADAP\n(Multifaceted Analysis of Disciplines through Academic Profiles). The\ninternational community of researchers working in Bibliometrics,\nScientometrics, Informetrics, Webometrics, and Altmetrics was selected as a\ncase study. The records of the top 1,000 most cited documents by these authors\naccording to GSC were manually processed to fill any missing information and\ndeduplicate fields like the journal titles and book publishers. The results\nsuggest that it is feasible to use GSC and the MADAP method to produce an\naccurate depiction of the community of researchers working in Bibliometrics\n(both specialists and occasional researchers) and their publication habits\n(main publication venues such as journals and book publishers). Additionally,\nthe wide document coverage of Google Scholar (specially books and book\nchapters) enables more comprehensive analyses of the documents published in a\nspecific discipline than were previously possible with other citation indexes,\nfinally shedding light on what until now had been a blind spot in most citation\nanalyses.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 11:17:26 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Mart\u00edn-Mart\u00edn", "Alberto", ""], ["Orduna-Malea", "Enrique", ""], ["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""]]}, {"id": "1804.11210", "submitter": "Lutz Bornmann Dr.", "authors": "Julian N. Marewski, Lutz Bornmann", "title": "Opium in science and society: Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In science and beyond, numbers are omnipresent when it comes to justifying\ndifferent kinds of judgments. Which scientific author, hiring committee-member,\nor advisory board panelist has not been confronted with page-long \"publication\nmanuals\", \"assessment reports\", \"evaluation guidelines\", calling for p-values,\ncitation rates, h-indices, or other statistics in order to motivate judgments\nabout the \"quality\" of findings, applicants, or institutions? Yet, many of\nthose relying on and calling for statistics do not even seem to understand what\ninformation those numbers can actually convey, and what not. Focusing on the\nuninformed usage of bibliometrics as worrysome outgrowth of the increasing\nquantification of science and society, we place the abuse of numbers into\nlarger historical contexts and trends. These are characterized by a\ntechnology-driven bureaucratization of science, obsessions with control and\naccountability, and mistrust in human intuitive judgment. The ongoing digital\nrevolution increases those trends. We call for bringing sanity back into\nscientific judgment exercises. Despite all number crunching, many judgments -\nbe it about scientific output, scientists, or research institutions - will\nneither be unambiguous, uncontroversial, or testable by external standards, nor\ncan they be otherwise validated or objectified. Under uncertainty, good human\njudgment remains, for the better, indispensable, but it can be aided, so we\nconclude, by a toolbox of simple judgment tools, called heuristics. In the best\nposition to use those heuristics are research evaluators (1) who have expertise\nin the to-be-evaluated area of research, (2) who have profound knowledge in\nbibliometrics, and (3) who are statistically literate.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 11:32:12 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 06:43:39 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Marewski", "Julian N.", ""], ["Bornmann", "Lutz", ""]]}]