[{"id": "1106.0114", "submitter": "Vincenzo Carbone", "authors": "Vincenzo Carbone", "title": "Fractional counting of authorship to quantify scientific research output", "comments": "Submitted to Europhysics Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of counting co-authorhip in order to quantify the\nimpact and relevance of scientific research output through normalized\n\\textit{h-index} and \\textit{g-index}. We use the papers whose authors belong\nto a subset of full professors of the Italian Settore Scientifico Disciplinare\n(SSD) FIS01 - Experimental Physics. In this SSD two populations, characterized\nby the number of co-authors of each paper, are roughly present. The total\nnumber of citations for each individuals, as well as their h-index and g-index,\nstrongly depends on the average number of co-authors. We show that, in order to\nremove the dependence of the various indices on the two populations, the best\nway to define a fractional counting of autorship is to divide the number of\ncitations received by each paper by the square root of the number of\nco-authors. This allows us to obtain some information which can be used for a\nbetter understanding of the scientific knowledge made through the process of\nwriting and publishing papers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 08:09:37 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Carbone", "Vincenzo", ""]]}, {"id": "1106.0217", "submitter": "Philipp Schaer", "authors": "Philipp Schaer", "title": "Using Lotkaian Informetrics for Ranking in Digital Libraries", "comments": "4 pages; Proceedings of the ASIS&T European Workshop 2011 (AEW 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to propose the use of models, theories and laws\nin bibliometrics and scientometrics to enhance information retrieval processes,\nespecially ranking. A common pattern in many man-made data sets is Lotka's Law\nwhich follows the well-known power-law distributions. These informetric\ndistributions can be used to give an alternative order to large and scattered\nresult sets and can be applied as a new ranking mechanism. The\npolyrepresentation of information in Digital Library systems is used to enhance\nthe retrieval quality, to overcome the drawbacks of the typical term-based\nranking approaches and to enable users to explore retrieved document sets from\na different perspective.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:13:18 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Schaer", "Philipp", ""]]}, {"id": "1106.0718", "submitter": "Arun Kumar", "authors": "Arun Kumar, Christopher R\\'e", "title": "Probabilistic Management of OCR Data using an RDBMS", "comments": "41 pages including the appendix. Shorter version (without appendix)\n  to appear as a full research paper in VLDB 2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  322-333 (2011)", "doi": null, "report-no": "vol5no4/p322_arunkumar_vldb2012", "categories": "cs.DB cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digitization of scanned forms and documents is changing the data sources\nthat enterprises manage. To integrate these new data sources with enterprise\ndata, the current state-of-the-art approach is to convert the images to ASCII\ntext using optical character recognition (OCR) software and then to store the\nresulting ASCII text in a relational database. The OCR problem is challenging,\nand so the output of OCR often contains errors. In turn, queries on the output\nof OCR may fail to retrieve relevant answers. State-of-the-art OCR programs,\ne.g., the OCR powering Google Books, use a probabilistic model that captures\nmany alternatives during the OCR process. Only when the results of OCR are\nstored in the database, do these approaches discard the uncertainty. In this\nwork, we propose to retain the probabilistic models produced by OCR process in\na relational database management system. A key technical challenge is that the\nprobabilistic data produced by OCR software is very large (a single book blows\nup to 2GB from 400kB as ASCII). As a result, a baseline solution that\nintegrates these models with an RDBMS is over 1000x slower versus standard text\nprocessing for single table select-project queries. However, many applications\nmay have quality-performance needs that are in between these two extremes of\nASCII and the complete model output by the OCR software. Thus, we propose a\nnovel approximation scheme called Staccato that allows a user to trade recall\nfor query performance. Additionally, we provide a formal analysis of our\nscheme's properties, and describe how we integrate our scheme with\nstandard-RDBMS text indexing.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 18:01:59 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2011 00:33:41 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2011 07:34:44 GMT"}, {"version": "v4", "created": "Fri, 6 Jan 2012 04:59:24 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Kumar", "Arun", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1106.1523", "submitter": "Daniel Hienert", "authors": "Daniel Hienert, Philipp Schaer, Johann Schaible, Philipp Mayr", "title": "A Novel Combined Term Suggestion Service for Domain-Specific Digital\n  Libraries", "comments": "To be published in Proceedings of Theories and Practice in Digital\n  Libraries (TPDL), 2011", "journal-ref": null, "doi": "10.1007/978-3-642-24469-8_21", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive query expansion can assist users during their query formulation\nprocess. We conducted a user study with over 4,000 unique visitors and four\ndifferent design approaches for a search term suggestion service. As a basis\nfor our evaluation we have implemented services which use three different\nvocabularies: (1) user search terms, (2) terms from a terminology service and\n(3) thesaurus terms. Additionally, we have created a new combined service which\nutilizes thesaurus term and terms from a domain-specific search term\nre-commender. Our results show that the thesaurus-based method clearly is used\nmore often compared to the other single-method implementations. We interpret\nthis as a strong indicator that term suggestion mechanisms should be\ndomain-specific to be close to the user terminology. Our novel combined\napproach which interconnects a thesaurus service with additional statistical\nrelations out-performed all other implementations. All our observations show\nthat domain-specific vocabulary can support the user in finding alternative\nconcepts and formulating queries.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2011 09:04:58 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Hienert", "Daniel", ""], ["Schaer", "Philipp", ""], ["Schaible", "Johann", ""], ["Mayr", "Philipp", ""]]}, {"id": "1106.2473", "submitter": "Theresa Velden", "authors": "Theresa Velden, Asif-ul Haque and Carl Lagoze", "title": "Resolving Author Name Homonymy to Improve Resolution of Structures in\n  Co-author Networks", "comments": "Accepted for JCDL 2011. Groundtruth data set attached and described\n  in README file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how author name homonymy distorts clustered large-scale\nco-author networks, and present a simple, effective, scalable and generalizable\nalgorithm to ameliorate such distortions. We evaluate the performance of the\nalgorithm to improve the resolution of mesoscopic network structures. To this\nend, we establish the ground truth for a sample of author names that is\nstatistically representative of different types of nodes in the co-author\nnetwork, distinguished by their role for the connectivity of the network. We\nfinally observe that this distinction of node roles based on the mesoscopic\nstructure of the network, in combination with a quantification of author name\ncommonality, suggests a new approach to assess network distortion by homonymy\nand to analyze the reduction of distortion in the network after disambiguation,\nwithout requiring ground truth sampling.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2011 15:34:18 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Velden", "Theresa", ""], ["Haque", "Asif-ul", ""], ["Lagoze", "Carl", ""]]}, {"id": "1106.2649", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern and David C. Parkes", "title": "Viewpoint: Journals for Certification, Conferences for Rapid\n  Dissemination", "comments": "To appear, Communications of the ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The publication culture in Computer Science is different from that of all\nother disciplines. Whereas other disciplines focus on journal publication, the\nstandard practice in CS has been to publish in a conference and then\n(sometimes) publish a journal version of the conference paper. We discuss the\nrole of journal publication in CS.\n  Indeed, it is through publication in selective, leading conferences that the\nquality of CS research is typically assessed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 09:37:39 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2011 13:45:46 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2012 22:17:24 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Parkes", "David C.", ""]]}, {"id": "1106.3305", "submitter": "Matthew Graham", "authors": "Matthew J. Graham", "title": "The Art of Data Science", "comments": "12 pages, invited talk at Astrostatistics and Data Mining in Large\n  Astronomical Databases workshop, La Palma, Spain, 30 May - 3 June 2011, to\n  appear in Springer Series on Astrostatistics", "journal-ref": null, "doi": "10.1007/978-1-4614-3323-1_4", "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To flourish in the new data-intensive environment of 21st century science, we\nneed to evolve new skills. These can be expressed in terms of the systemized\nframework that formed the basis of mediaeval education - the trivium (logic,\ngrammar, and rhetoric) and quadrivium (arithmetic, geometry, music, and\nastronomy). However, rather than focusing on number, data is the new keystone.\nWe need to understand what rules it obeys, how it is symbolized and\ncommunicated and what its relationship to physical space and time is. In this\npaper, we will review this understanding in terms of the technologies and\nprocesses that it requires. We contend that, at least, an appreciation of all\nthese aspects is crucial to enable us to extract scientific information and\nknowledge from the data sets which threaten to engulf and overwhelm us.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2011 18:45:32 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Graham", "Matthew J.", ""]]}, {"id": "1106.4176", "submitter": "Yannis Tzitzikas", "authors": "Michael Hickson, Yannis Kargakis and Yannis Tzitzikas", "title": "Similarity-based Browsing over Linked Open Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of data is published on the Web according to the Linked\nOpen Data (LOD) principles. End users would like to browse these data in a\nflexible manner. In this paper we focus on similarity-based browsing and we\nintroduce a novel method for computing the similarity between two entities of a\ngiven RDF/S graph. The distinctive characteristics of the proposed metric is\nthat it is generic (it can be used to compare nodes of any kind), it takes into\naccount the neighborhoods of the nodes, and it is configurable (with respect to\nthe accuracy vs computational complexity tradeoff). We demonstrate the behavior\nof the metric using examples from an application over LOD. Finally, we\ngeneralize and elaborate on implementation approaches harmonized with the\ndistributed nature of LOD which can be used for computing the most similar\nentities using neighborhood-based similarity metrics.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 11:39:11 GMT"}], "update_date": "2011-06-22", "authors_parsed": [["Hickson", "Michael", ""], ["Kargakis", "Yannis", ""], ["Tzitzikas", "Yannis", ""]]}, {"id": "1106.4880", "submitter": "Ying Ding", "authors": "Qian Zhu, Yuyin Sun, Sashikiran Challa, Ying Ding, Michael S.\n  Lajiness, David J. Wild", "title": "Semantic Inference using Chemogenomics Data for Drug Discovery", "comments": "23 pages, 9 figures, 4 tables", "journal-ref": null, "doi": "10.1186/1471-2105-12-256", "report-no": null, "categories": "q-bio.QM cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Semantic Web Technology (SWT) makes it possible to integrate and\nsearch the large volume of life science datasets in the public domain, as\ndemonstrated by well-known linked data projects such as LODD, Bio2RDF, and\nChem2Bio2RDF. Integration of these sets creates large networks of information.\nWe have previously described a tool called WENDI for aggregating information\npertaining to new chemical compounds, effectively creating evidence paths\nrelating the compounds to genes, diseases and so on. In this paper we examine\nthe utility of automatically inferring new compound-disease associations (and\nthus new links in the network) based on semantically marked-up versions of\nthese evidence paths, rule-sets and inference engines.\n  Results Through the implementation of a semantic inference algorithm, rule\nset, Semantic Web methods (RDF, OWL and SPARQL) and new interfaces, we have\ncreated a new tool called Chemogenomic Explorer that uses networks of\nontologically annotated RDF statements along with deductive reasoning tools to\ninfer new associations between the query structure and genes and diseases from\nWENDI results. The tool then permits interactive clustering and filtering of\nthese evidence paths.\n  Conclusions We present a new aggregate approach to inferring links between\nchemical compounds and diseases using semantic inference. This approach allows\nmultiple evidence paths between compounds and diseases to be identified using a\nrule-set and semantically annotated data, and for these evidence paths to be\nclustered to show overall evidence linking the compound to a disease. We\nbelieve this is a powerful approach, because it allows compound-disease\nrelationships to be ranked by the amount of evidence supporting them.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 03:21:56 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Zhu", "Qian", ""], ["Sun", "Yuyin", ""], ["Challa", "Sashikiran", ""], ["Ding", "Ying", ""], ["Lajiness", "Michael S.", ""], ["Wild", "David J.", ""]]}, {"id": "1106.5178", "submitter": "Bernhard Haslhofer", "authors": "Bernhard Haslhofer, Rainer Simon, Robert Sanderson, Herbert van de\n  Sompel", "title": "The Open Annotation Collaboration (OAC) Model", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Annotations allow users to associate additional information with existing\nresources. Using proprietary and closed systems on the Web, users are already\nable to annotate multimedia resources such as images, audio and video. So far,\nhowever, this information is almost always kept locked up and inaccessible to\nthe Web of Data. We believe that an important step to take is the integration\nof multimedia annotations and the Linked Data principles. This should allow\nclients to easily publish and consume, thus exchange annotations about\nresources via common Web standards. We first present the current status of the\nOpen Annotation Collaboration, an international initiative that is currently\nworking on annotation interoperability specifications based on best practices\nfrom the Linked Data effort. Then we present two use cases and early prototypes\nthat make use of the proposed annotation model and present lessons learned and\ndiscuss yet open technical issues.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2011 22:55:17 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Haslhofer", "Bernhard", ""], ["Simon", "Rainer", ""], ["Sanderson", "Robert", ""], ["van de Sompel", "Herbert", ""]]}, {"id": "1106.5304", "submitter": "Florin Pop Mr.", "authors": "George Milescu, Gabriel Noaje, Florin Pop", "title": "OpenPh - Numerical Physics Library", "comments": "(ISSN 1223-7027)", "journal-ref": "UPB Scientific Bulletin, Series A: Applied Mathematics and\n  Physics, volume 68, number 1, pp: 73-78, 2006", "doi": null, "report-no": null, "categories": "cs.DL physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical physics has gained a lot of importance in the last decade, its\nefficiency being motivated and sustained by the growth of computational power.\nThis paper presents a concept that is to be developed in the next few years:\nOpenPh. OpenPh is a numerical physics library that makes use of the advantages\nof both open source software and MATLAB programming. Its aim is to deliver the\ninstruments for providing numerical and graphical solutions for various physics\nproblems. It has a modular structure, allowing the user to add new modules to\nthe existing ones and to create its own modules according to its needs, being\nvirtually unlimited extendable. The modules of OpenPh are implemented using\nMATLAB engine because it is the best solution used in engineering and science,\nproviding a wide range of optimized methods to accomplish even the toughest\njobs. Current version of OpenPh includes two modules, the first one providing\ntools for quantum physics and the second one for mechanics. The quantum physics\nmodule deals with the photoelectric effect, the radioactive decay of carbon-11,\nand the Schr\\\"odinger equation - particle in a box. The classical mechanics\nmodule includes the study of the uniform circular motion, the forced damped\nharmonic oscillations and the vibration of a fixed-fixed string.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 06:03:50 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Milescu", "George", ""], ["Noaje", "Gabriel", ""], ["Pop", "Florin", ""]]}, {"id": "1106.5644", "submitter": "Edwin Henneken", "authors": "Edwin A. Henneken, Michael J. Kurtz, Alberto Accomazzi", "title": "The ADS in the Information Age - Impact on Discovery", "comments": "10 pages, 5 figures, to appear in \"Organizations, People and\n  Strategies in Astronomy (OPSA)\", volume 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The SAO/NASA Astrophysics Data System (ADS) grew up with and has been riding\nthe waves of the Information Age, closely monitoring and anticipating the needs\nof its end-users. By now, all professional astronomers are using the ADS on a\ndaily basis, and a substantial fraction have been using it for their entire\nprofessional career. In addition to being an indispensable tool for\nprofessional scientists, the ADS also moved into the public domain, as a tool\nfor science education. In this paper we will highlight and discuss some aspects\nindicative of the impact the ADS has had on research and the access to\nscholarly publications.\n  The ADS is funded by NASA Grant NNX09AB39G\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 12:21:55 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Henneken", "Edwin A.", ""], ["Kurtz", "Michael J.", ""], ["Accomazzi", "Alberto", ""]]}, {"id": "1106.5651", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff", "title": "Hyperincursive Cogitata and Incursive Cogitantes: Scholarly Discourse as\n  a Strongly Anticipatory System", "comments": "arXiv admin note: substantial text overlap with arXiv:1011.3244", "journal-ref": "International Journal of Computing Anticipatory Systems, 28,\n  173-186", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strongly anticipatory systems-that is, systems which use models of themselves\nfor their further development-and which additionally may be able to run\nhyperincursive routines-that is, develop only with reference to their future\nstates-cannot exist in res extensa, but can only be envisaged in res cogitans.\nOne needs incursive routines in cogitantes to instantiate these systems. Unlike\nhistorical systems (with recursion), these hyper-incursive routines generate\nredundancies by opening horizons of other possible states. Thus, intentional\nsystems can enrich our perceptions of the cases that have happened to occur.\nThe perspective of hindsight codified at the above-individual level enables us\nfurthermore to intervene technologically. The theory and computation of\nanticipatory systems have made these loops between supra-individual\nhyper-incursion, individual incursion (in instantiation), and historical\nrecursion accessible for modeling and empirical investigation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 12:56:19 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2015 06:01:18 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Leydesdorff", "Loet", ""]]}]