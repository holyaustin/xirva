[{"id": "2009.00091", "submitter": "Jon Saad-Falcon", "authors": "Jon Saad-Falcon, Omar Shaikh, Zijie J. Wang, Austin P. Wright, Sasha\n  Richardson, and Duen Horng Chau", "title": "Mapping Researchers with PeopleMap", "comments": "2020 IEEE Visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering research expertise at universities can be a difficult task.\nDirectories routinely become outdated, and few help in visually summarizing\nresearchers' work or supporting the exploration of shared interests among\nresearchers. This results in lost opportunities for both internal and external\nentities to discover new connections, nurture research collaboration, and\nexplore the diversity of research. To address this problem, at Georgia Tech, we\nhave been developing PeopleMap, an open-source interactive web-based tool that\nuses natural language processing (NLP) to create visual maps for researchers\nbased on their research interests and publications. Requiring only the\nresearchers' Google Scholar profiles as input, PeopleMap generates and\nvisualizes embeddings for the researchers, significantly reducing the need for\nmanual curation of publication information. To encourage and facilitate easy\nadoption and extension of PeopleMap, we have open-sourced it under the\npermissive MIT license at https://github.com/poloclub/people-map. PeopleMap has\nreceived positive feedback and enthusiasm for expanding its adoption across\nGeorgia Tech.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 20:46:27 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Saad-Falcon", "Jon", ""], ["Shaikh", "Omar", ""], ["Wang", "Zijie J.", ""], ["Wright", "Austin P.", ""], ["Richardson", "Sasha", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2009.00513", "submitter": "Allen Riddell", "authors": "Allen Riddell and Troy J. Bassett", "title": "What Library Digitization Leaves Out: Predicting the Availability of\n  Digital Surrogates of English Novels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Library digitization has made more than a hundred thousand 19th-century\nEnglish-language books available to the public. Do the books which have been\ndigitized reflect the population of published books? An affirmative answer\nwould allow book and literary historians to use holdings of major digital\nlibraries as proxies for the population of published works, sparing them the\nlabor of collecting a representative sample. We address this question by taking\nadvantage of exhaustive bibliographies of novels published for the first time\nin the British Isles in 1836 and 1838, identifying which of these novels have\nat least one digital surrogate in the Internet Archive, HathiTrust, Google\nBooks, and the British Library. We find that digital surrogate availability is\nnot random. Certain kinds of novels, notably novels written by men and novels\npublished in multivolume format, have digital surrogates available at\ndistinctly higher rates than other kinds of novels. As the processes leading to\nthis outcome are unlikely to be isolated to the novel and the late 1830s, these\nfindings suggest that similar patterns will likely be observed during adjacent\ndecades and in other genres of publishing (e.g., non-fiction).\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 15:20:15 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Riddell", "Allen", ""], ["Bassett", "Troy J.", ""]]}, {"id": "2009.00611", "submitter": "Krutarth Patel", "authors": "Krutarth Patel, Cornelia Caragea, Mark Phillips, Nathaniel Fox", "title": "Identifying Documents In-Scope of a Collection from Web Archives", "comments": "10 pages", "journal-ref": "In Proceedings of the ACM/IEEE Joint Conference on Digital\n  Libraries in 2020 (JCDL 2020)", "doi": "10.1145/3383583.3398540", "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archive data usually contains high-quality documents that are very useful\nfor creating specialized collections of documents, e.g., scientific digital\nlibraries and repositories of technical reports. In doing so, there is a\nsubstantial need for automatic approaches that can distinguish the documents of\ninterest for a collection out of the huge number of documents collected by web\narchiving institutions. In this paper, we explore different learning models and\nfeature representations to determine the best performing ones for identifying\nthe documents of interest from the web archived data. Specifically, we study\nboth machine learning and deep learning models and \"bag of words\" (BoW)\nfeatures extracted from the entire document or from specific portions of the\ndocument, as well as structural features that capture the structure of\ndocuments. We focus our evaluation on three datasets that we created from three\ndifferent Web archives. Our experimental results show that the BoW classifiers\nthat focus only on specific portions of the documents (rather than the full\ntext) outperform all compared methods on all three datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 16:22:23 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Patel", "Krutarth", ""], ["Caragea", "Cornelia", ""], ["Phillips", "Mark", ""], ["Fox", "Nathaniel", ""]]}, {"id": "2009.01812", "submitter": "Xin Li", "authors": "Xuli Tang, Xin Li, Ying Ding, Min Song, Yi Bu", "title": "The Pace of Artificial Intelligence Innovations: Speed, Talent, and\n  Trial-and-Error", "comments": "Journal of Informetrics 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovations in artificial intelligence (AI) are occurring at speeds faster\nthan ever witnessed before. However, few studies have managed to measure or\ndepict this increasing velocity of innovations in the field of AI. In this\npaper, we combine data on AI from arXiv and Semantic Scholar to explore the\npace of AI innovations from three perspectives: AI publications, AI players,\nand AI updates (trial and error). A research framework and three novel\nindicators, Average Time Interval (ATI), Innovation Speed (IS) and Update Speed\n(US), are proposed to measure the pace of innovations in the field of AI. The\nresults show that: (1) in 2019, more than 3 AI preprints were submitted to\narXiv per hour, over 148 times faster than in 1994. Furthermore, there was one\ndeep learning-related preprint submitted to arXiv every 0.87 hours in 2019,\nover 1,064 times faster than in 1994. (2) For AI players, 5.26 new researchers\nentered into the field of AI each hour in 2019, more than 175 times faster than\nin the 1990s. (3) As for AI updates (trial and error), one updated AI preprint\nwas submitted to arXiv every 41 days, with around 33% of AI preprints having\nbeen updated at least twice in 2019. In addition, as reported in 2019, it took,\non average, only around 0.2 year for AI preprints to receive their first\ncitations, which is 5 times faster than 2000-2007. This swift pace in AI\nillustrates the increase in popularity of AI innovation. The systematic and\nfine-grained analysis of the AI field enabled to portrait the pace of AI\ninnovation and demonstrated that the proposed approach can be adopted to\nunderstand other fast-growing fields such as cancer research and nano science.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 17:26:04 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Tang", "Xuli", ""], ["Li", "Xin", ""], ["Ding", "Ying", ""], ["Song", "Min", ""], ["Bu", "Yi", ""]]}, {"id": "2009.02678", "submitter": "Raja Appuswamy", "authors": "Raja Appuswamy and Vincent Joguin", "title": "Universal Layout Emulation for Long-Term Database Archival", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on alternate media technologies, like film, synthetic DNA, and\nglass, for long-term data archival has received a lot of attention recently due\nto the media obsolescence issues faced by contemporary storage media like tape,\nHard Disk Drives (HDD), and Solid State Disks (SSD). While researchers have\ndeveloped novel layout and encoding techniques for archiving databases on these\nnew media types, one key question remains unaddressed: How do we ensure that\nthe decoders developed today will be available and executable by a user who is\nrestoring an archived database several decades later in the future, on a\ncomputing platform that potentially does not even exist today?\n  In this paper, we make the case for Universal Layout Emulation (ULE), a new\napproach for future-proof, long-term database archival that advocates archiving\ndecoders together with the data to ensure successful recovery. In order to do\nso, ULE brings together concepts from Data Management and Digital Preservation\ncommunities by using emulation for archiving decoders. In order to show that\nULE can be implemented in practice, we present the design and evaluation of\nMicr'Olonys, an end-to-end long-term database archival system that can be used\nto archive databases using visual analog media like film, microform, and\narchival paper.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 09:06:13 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 10:09:25 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Appuswamy", "Raja", ""], ["Joguin", "Vincent", ""]]}, {"id": "2009.03390", "submitter": "Laura Tateosian", "authors": "Alexander Yoshizumi, Megan M. Coffer, Elyssa L. Collins, Mollie D.\n  Gaines, Xiaojie Gao, Kate Jones, Ian R. McGregor, Katie A. McQuillan,\n  Vinicius Perin, Laura M. Tomkins, Thom Worm, Laura Tateosian", "title": "A Review of Geospatial Content in IEEE Visualization Publications", "comments": "5 pages, 4 figures, IEEE VIS Short Paper Proceedings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geospatial analysis is crucial for addressing many of the world's most\npressing challenges. Given this, there is immense value in improving and\nexpanding the visualization techniques used to communicate geospatial data. In\nthis work, we explore this important intersection -- between geospatial\nanalytics and visualization -- by examining a set of recent IEEE VIS Conference\npapers (a selection from 2017-2019) to assess the inclusion of geospatial data\nand geospatial analyses within these papers. After removing the papers with no\ngeospatial data, we organize the remaining literature into geospatial data\ndomain categories and provide insight into how these categories relate to VIS\nConference paper types. We also contextualize our results by investigating the\nuse of geospatial terms in IEEE Visualization publications over the last 30\nyears. Our work provides an understanding of the quantity and role of\ngeospatial subject matter in recent IEEE VIS publications and supplies a\nfoundation for future meta-analytical work around geospatial analytics and\ngeovisualization that may shed light on opportunities for innovation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 19:42:56 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Yoshizumi", "Alexander", ""], ["Coffer", "Megan M.", ""], ["Collins", "Elyssa L.", ""], ["Gaines", "Mollie D.", ""], ["Gao", "Xiaojie", ""], ["Jones", "Kate", ""], ["McGregor", "Ian R.", ""], ["McQuillan", "Katie A.", ""], ["Perin", "Vinicius", ""], ["Tomkins", "Laura M.", ""], ["Worm", "Thom", ""], ["Tateosian", "Laura", ""]]}, {"id": "2009.04281", "submitter": "Marc-Andr\\'e Simard", "authors": "Marc-Andre Simard, Jason Priem, Heather Piwowar", "title": "How much does an interlibrary loan request cost? A review of the\n  literature", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interlibrary loan (ILL) services are used to fill the gap between academic\nlibraries' collections and the information needs of their users. Today's trend\ntoward the cancellation of serials \"Big Deals\" has increased the importance of\nclear information around ILL to support decision-making. In order to plan the\ncancellation of a journal package, academic libraries need to be able to\nforecast their total spendings on ILL, which requires to have an appropriate\nestimate of what it costs to fulfill an individual ILL request. This paper aims\nto help librarians answer this question by reviewing the most recent academic\nliterature related to these costs. There are several factors that may affect\nthe cost of an ILL service, including the cost of labour, the geographic\nlocation of the library, the use of a ILL software, and membership to a library\nconsortium. We find that there is a wide range of estimates for ILL cost, from\n$3.75 (USD) to $100.00 (USD). However, Jackson's (2004) figure of $17.50 (USD)\nper transaction remains the guideline for most researchers and librarians.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 13:02:05 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Simard", "Marc-Andre", ""], ["Priem", "Jason", ""], ["Piwowar", "Heather", ""]]}, {"id": "2009.04287", "submitter": "Marc-Andre Simard", "authors": "Marc-Andre Simard, Jason Priem, Heather Piwowar", "title": "The aftermath of Big Deal cancellations and their impact on interlibrary\n  loans", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \"Big Deal\" is a bundle of journals that is offered to libraries by\npublishers as a \"one-price, one size fits all package\" (Frazier, 2001). There\nhave been several accounts of Big Deals cancellations by academic libraries in\nthe scientific literature. This paper presents the finding of a literature\nreview aimed at documenting the aftermath of Big Deal cancellation in\nUniversity Libraries, particularly their impacts on interlibrary loan services.\nWe find that many academic libraries have successfully cancelled their Big\nDeals, realizing budget savings while limiting negative effects on library\nusers. In particular, existing literature reveals that cancellations have a\nsurprisingly small effect on interlibrary loan requests. The reviewed studies\nfurther highlight the importance of access to proper usage data and inclusion\nof community members of the community (staff, faculty members, students, etc.)\nin the decision-making process.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 13:18:06 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Simard", "Marc-Andre", ""], ["Priem", "Jason", ""], ["Piwowar", "Heather", ""]]}, {"id": "2009.04586", "submitter": "Jatin Sharma", "authors": "Jatin Sharma, Nikhilesh Behera, Priya Venkatraman, Boon Thau Loo", "title": "RapidLearn: A General Purpose Toolkit for Autonomic Networking", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Defined Networking has unfolded a new area of opportunity in\ndistributed networking and intelligent networks. There has been a great\ninterest in performing machine learning in distributed setting, exploiting the\nabstraction of SDN which makes it easier to write complex ML queries on\nstandard control plane. However, most of the research has been made towards\nspecialized problems (security, performance improvement, middlebox management\netc) and not towards a generic framework. Also, existing tools and software\nrequire specialized knowledge of the algorithm/network to operate or monitor\nthese systems. We built a generic toolkit which abstracts out the underlying\nstructure, algorithms and other intricacies and gives an intuitive way for a\ncommon user to create and deploy distributed machine learning network\napplications. Decisions are made at local level by the switches and\ncommunicated to other switches to improve upon these decisions. Finally, a\nglobal decision is taken by controller based on another algorithm (in our case\nvoting). We demonstrate efficacy of the framework through a simple DDoS\ndetection algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 21:56:26 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Sharma", "Jatin", ""], ["Behera", "Nikhilesh", ""], ["Venkatraman", "Priya", ""], ["Loo", "Boon Thau", ""]]}, {"id": "2009.05392", "submitter": "Fran\\c{c}ois Renaville", "authors": "Christophe Dony, Maurane Raskinet, Fran\\c{c}ois Renaville, St\\'ephanie\n  Simon, Paul Thirion", "title": "How reliable and useful is Cabell's Blacklist ? A data-driven analysis", "comments": "38 pages", "journal-ref": "LIBER Quarterly 30 (2020) 1", "doi": "10.18352/lq.10339", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scholarly publishing, blacklists aim to register fraudulent or deceptive\njournals and publishers, also known as \"predatory\", to minimise the spread of\nunreliable research and the growing of fake publishing outlets. However,\nblacklisting remains a very controversial activity for several reasons: there\nis no consensus regarding the criteria used to determine fraudulent journals,\nthe criteria used may not always be transparent or relevant, and blacklists are\nrarely updated regularly. Cabell's paywalled blacklist service attempts to\novercome some of these issues in reviewing fraudulent journals on the basis of\ntransparent criteria and in providing allegedly up-to-date information at the\njournal entry level. We tested Cabell's blacklist to analyse whether or not it\ncould be adopted as a reliable tool by stakeholders in scholarly communication,\nincluding our own academic library. To do so, we used a copy of Walt Crawford's\nGray Open Access dataset (2012-2016) to assess the coverage of Cabell's\nblacklist and get insights on their methodology. Out of the 10,123 journals\nthat we tested, 4,681 are included in Cabell's blacklist. Out of this number of\njournals included in the blacklist, 3,229 are empty journals, i.e. journals in\nwhich no single article has ever been published. Other collected data points to\nquestionable weighing and reviewing methods and shows a lack of rigour in how\nCabell applies its own procedures: some journals are blacklisted on the basis\nof 1 to 3 criteria, identical criteria are recorded multiple times in\nindividual journal entries, discrepancies exist between reviewing dates and the\ncriteria version used and recorded by Cabell, reviewing dates are missing, and\nwe observed two journals blacklisted twice with a different number of\nviolations. Based on these observations, we conclude with recommendations and\nsuggestions that could help improve Cabell's blacklist service.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 12:28:58 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Dony", "Christophe", ""], ["Raskinet", "Maurane", ""], ["Renaville", "Fran\u00e7ois", ""], ["Simon", "St\u00e9phanie", ""], ["Thirion", "Paul", ""]]}, {"id": "2009.05588", "submitter": "Silvio Peroni", "authors": "Erika Alves dos Santos, Silvio Peroni, Marcos Luiz Mucheroni", "title": "Citing and referencing habits in Medicine and Social Sciences journals\n  in 2019", "comments": "Accepted for publication on 18 January 2021 in Journal of\n  Documentation", "journal-ref": null, "doi": "10.1108/JD-08-2020-0144", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article explores citing and referencing systems in Social Sciences and\nMedicine articles from different theoretical and practical perspectives,\nconsidering bibliographic references as a facet of descriptive representation.\nThe analysis of citing and referencing elements (i.e. bibliographic references,\nmentions, quotations, and respective in-text reference pointers) identified\nciting and referencing habits within disciplines under consideration and errors\noccurring over the long term as stated by previous studies now expanded. Future\nexpected trends of information retrieval from bibliographic metadata was\ngathered by approaching these referencing elements from the FRBR Entities\nconcepts. Reference styles do not fully accomplish with their role of guiding\nauthors and publishers on providing concise and well-structured bibliographic\nmetadata within bibliographic references. Trends on representative description\nrevision suggest a predicted distancing on the ways information is approached\nby bibliographic references and bibliographic catalogs adopting FRBR concepts,\nincluding the description levels adopted by each of them under the perspective\nof the FRBR Entities concept. This study was based on a subset of Medicine and\nSocial Sciences articles published in 2019 and, therefore, it may not be taken\nas a final and broad coverage. Future studies expanding these approaches to\nother disciplines and chronological periods are encouraged. By approaching\nciting and referencing issues as descriptive representation's facets, findings\non this study may encourage further studies that will support Information\nScience and Computer Science on providing tools to become bibliographic\nmetadata description simpler, better structured and more efficient facing the\nrevision of descriptive representation actually in progress.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 18:06:21 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 07:37:39 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Santos", "Erika Alves dos", ""], ["Peroni", "Silvio", ""], ["Mucheroni", "Marcos Luiz", ""]]}, {"id": "2009.05793", "submitter": "Petar Radanliev", "authors": "Petar Radanliev, David De Roure, Rob Walton", "title": "Data mining and analysis of scientific research data records on Covid 19\n  mortality, immunity, and vaccine development in the first wave of the Covid\n  19 pandemic", "comments": null, "journal-ref": "Volume 14, Issue 5, September October 2020,", "doi": "10.1016/j.dsx.2020.06.063", "report-no": null, "categories": "cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate the scientific research response from the early\nstages of the pandemic, and we review key findings on how the early warning\nsystems developed in previous epidemics responded to contain the virus. The\ndata records are analysed with commutable statistical methods, including R\nStudio, Bibliometrix package, and the Web of Science data mining tool. We\nidentified few different clusters, containing references to exercise,\ninflammation, smoking, obesity and many additional factors. From the analysis\non Covid-19 and vaccine, we discovered that although the USA is leading in\nvolume of scientific research on Covid 19 vaccine, the leading 3 research\ninstitutions (Fudan, Melbourne, Oxford) are not based in the USA. Hence, it is\ndifficult to predict which country would be first to produce a Covid 19\nvaccine.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 13:34:05 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Radanliev", "Petar", ""], ["De Roure", "David", ""], ["Walton", "Rob", ""]]}, {"id": "2009.06116", "submitter": "Jannis Born", "authors": "Jannis Born, Nina Wiedemann, Gabriel Br\\\"andle, Charlotte Buhre,\n  Bastian Rieck, Karsten Borgwardt", "title": "Accelerating COVID-19 Differential Diagnosis with Explainable Ultrasound\n  Image Analysis", "comments": "8 pages, 4 figures", "journal-ref": "Applied Sciences 2021 (special issue on: \"Fighting COVID-19:\n  Emerging Techniques and Aid Systems for Prevention, Forecasting and\n  Diagnosis\")", "doi": "10.3390/app11020672", "report-no": null, "categories": "cs.CV cs.DB cs.DL cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controlling the COVID-19 pandemic largely hinges upon the existence of fast,\nsafe, and highly-available diagnostic tools. Ultrasound, in contrast to CT or\nX-Ray, has many practical advantages and can serve as a globally-applicable\nfirst-line examination technique. We provide the largest publicly available\nlung ultrasound (US) dataset for COVID-19 consisting of 106 videos from three\nclasses (COVID-19, bacterial pneumonia, and healthy controls); curated and\napproved by medical experts. On this dataset, we perform an in-depth study of\nthe value of deep learning methods for differential diagnosis of COVID-19. We\npropose a frame-based convolutional neural network that correctly classifies\nCOVID-19 US videos with a sensitivity of 0.98+-0.04 and a specificity of\n0.91+-08 (frame-based sensitivity 0.93+-0.05, specificity 0.87+-0.07). We\nfurther employ class activation maps for the spatio-temporal localization of\npulmonary biomarkers, which we subsequently validate for human-in-the-loop\nscenarios in a blindfolded study with medical experts. Aiming for scalability\nand robustness, we perform ablation studies comparing mobile-friendly, frame-\nand video-based architectures and show reliability of the best model by\naleatoric and epistemic uncertainty estimates. We hope to pave the road for a\ncommunity effort toward an accessible, efficient and interpretable screening\nmethod and we have started to work on a clinical validation of the proposed\nmethod. Data and code are publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 23:52:03 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Born", "Jannis", ""], ["Wiedemann", "Nina", ""], ["Br\u00e4ndle", "Gabriel", ""], ["Buhre", "Charlotte", ""], ["Rieck", "Bastian", ""], ["Borgwardt", "Karsten", ""]]}, {"id": "2009.06139", "submitter": "Weishu Liu", "authors": "Weishu Liu", "title": "A matter of time: publication dates in Web of Science Core Collection", "comments": "forthcoming in Scientometrics", "journal-ref": null, "doi": "10.1007/s11192-020-03697-x", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web of Science Core Collection, one of the most authoritative bibliographic\ndatabases, is widely used in academia to track high-quality research. This\ndatabase has begun to index online-first articles since December 2017. This new\npractice has introduced two different publication dates (online and final\npublication dates) into the database for more and more early access\npublications. It may confuse many users who want to search or analyze\nliterature by using the publication-year related tools provided by Web of\nScience Core Collection. By developing custom retrieval strategies and checking\nmanually, this study finds that the \"year published\" field in search page\nsearches in both online and final publication date fields of indexed records.\nEach indexed record is allocated to only one \"publication year\" on the left of\nthe search results page which will inherit first from online publication date\nfield even when the online publication date is later than the final publication\ndate. The \"publication year\" field in the results analysis page and the\ntimespan \"custom year range\" field in the search page have the same function as\nthat of the filter \"publication year\" in search results page. The potential\nimpact of the availability of two different publication dates in calculating\nbibliometric indicators is also discussed at the end of the article.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 01:43:24 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Liu", "Weishu", ""]]}, {"id": "2009.06337", "submitter": "Jennifer Blanke", "authors": "Jennifer Blanke and Thomas Riechert", "title": "Towards an RDF Knowledge Graph of Scholars from Early Modern History", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of Semantic Web Technologies supports research in the field of\ndigital humanities. In this paper we focus on the creation of semantic\nindependent online databases such as those of historical prosopography. These\ndatabases contain biographical information of historical persons. We focus on\nthis information with an interest in German professorial career patterns from\nthe 16th to the 18th century. In that respect, we describe the process of\nbuilding an Early Modern Scholarly Career RDF Knowledge Graph from two existing\nprosopography online databases: the Catalogus Professorum Lipsiensium and the\nCatalogus Professorum Helmstadiensium. Further, we provide an insight in how to\nquery the information using KBox to answer research questions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 11:55:58 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Blanke", "Jennifer", ""], ["Riechert", "Thomas", ""]]}, {"id": "2009.06888", "submitter": "Guoqiang Liang", "authors": "Guoqiang Liang, Yi Jiang, Haiyan Hou", "title": "Same data may bring conflict results: a caution to use the disruptive\n  index", "comments": "Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, scholars have designed various types of\nbibliographic related indicators to identify breakthrough-class academic\nachievements. In this study, we take a further step to look at properties of\nthe promising disruptive index, thus deepening our understanding of this index\nand further facilitating its wise use in bibliometrics. Using publication\nrecords for Nobel laureates between 1900 and 2016, we calculate the DI of Nobel\nPrize-winning articles and its benchmark articles in each year and use the\nmedian DI to denote the central tendency in each year, and compare results\nbetween Medicine, Chemistry, and Physics. We find that conclusions based on DI\ndepend on the length of their citation time window, and different citation time\nwindows may cause different, even controversial, results. Also, discipline and\ntime play a role on the length of citation window when using DI to measure the\ninnovativeness of a scientific work. Finally, not all articles with DI equals\nto 1 were the breakthrough-class achievements. In other words, the DI stands up\ntheoretically, but we should not neglect that the DI was only shaped by the\nnumber of citing articles and times the references have been cited, these data\nmay vary from database to database.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 07:00:04 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Liang", "Guoqiang", ""], ["Jiang", "Yi", ""], ["Hou", "Haiyan", ""]]}, {"id": "2009.07561", "submitter": "Matan Shelomi", "authors": "Matan Shelomi", "title": "Comment on \"Open is not forever: a study of vanished open access\n  journals\"", "comments": "3 pages excluding title and abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We comment on a recent article by Laakso et al. (arXiv:2008.11933 [cs.DL]),\nin which the disappearance of 176 open access journals from the Internet is\nnoted. We argue that one reason these journals may have vanished is that they\nwere predatory journals. The de-listing of predators from the Directory of Open\nAccess Journals in 2014 and the abundance of predatory journals and awareness\nthereof in North America parsimoniously explain the temporal and geographic\npatterns Laakso et al. observed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 09:09:03 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Shelomi", "Matan", ""]]}, {"id": "2009.07568", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Alona Zharova, Janine Tellinger-Rice, Wolfgang Karl H\\\"ardle", "title": "How to Measure the Performance of a Collaborative Research Center", "comments": null, "journal-ref": "Scientometrics 2018, volume 117", "doi": "10.1007/s11192-018-2910-8", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New Public Management helps universities and research institutions to perform\nin a highly competitive research environment. Evaluating publicly financed\nresearch improves transparency, helps in reflection and self-assessment, and\nprovides information for strategic decision making. In this paper we provide\nempirical evidence using data from a Collaborative Research Center (CRC) on\nfinancial inputs and research output from 2005 to 2016. After selecting\nperformance indicators suitable for a CRC, we describe main properties of the\ndata using visualization techniques. To study the relationship between the\ndimensions of research performance, we use a time fixed effects panel data\nmodel and fixed effects Poisson model. With the help of year dummy variables,\nwe show how the pattern of research productivity changes over time after\ncontrolling for staff and travel costs. The joint depiction of the time fixed\neffects and the research project's life cycle allows a better understanding of\nthe development of the number of discussion papers over time.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 09:23:11 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Zharova", "Alona", ""], ["Tellinger-Rice", "Janine", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2009.07642", "submitter": "Marco Anteghini", "authors": "Marco Anteghini, Jennifer D'Souza, Vitor A.P. Martins dos Santos,\n  S\\\"oren Auer", "title": "Representing Semantified Biological Assays in the Open Research\n  Knowledge Graph", "comments": "In Proceedings of 'The 22nd International Conference on Asia-Pacific\n  Digital Libraries'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the biotechnology and biomedical domains, recent text mining efforts\nadvocate for machine-interpretable, and preferably, semantified, documentation\nformats of laboratory processes. This includes wet-lab protocols, (in)organic\nmaterials synthesis reactions, genetic manipulations and procedures for faster\ncomputer-mediated analysis and predictions. Herein, we present our work on the\nrepresentation of semantified bioassays in the Open Research Knowledge Graph\n(ORKG). In particular, we describe a semantification system work-in-progress to\ngenerate, automatically and quickly, the critical semantified bioassay data\nmass needed to foster a consistent user audience to adopt the ORKG for\nrecording their bioassays and facilitate the organisation of research,\naccording to FAIR principles.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 12:42:37 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Anteghini", "Marco", ""], ["D'Souza", "Jennifer", ""], ["Santos", "Vitor A. P. Martins dos", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2009.07815", "submitter": "Jamal El-Ouahi", "authors": "Jamal El-Ouahi, Nicolas Robinson-Garcia, Rodrigo Costas", "title": "Analysing Scientific Mobility and Collaboration in the Middle East and\n  North Africa", "comments": "37 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the scientific mobility and international\ncollaboration networks in the Middle East and North Africa (MENA) region\nbetween 2008 and 2017. By using affiliation metadata available in scientific\npublications, we analyse international scientific mobility flows and\ncollaboration linkages. Three complementary approaches allow us to obtain a\ndetailed characterization of scientific mobility. First, we uncover the main\ndestinations and origins of mobile scholars for each country. Results reveal\ngeographical, cultural and historical proximities. Cooperation programs also\ncontribute to explain some of the observed flows. Second, we use the academic\nage. The average academic age of migrant scholars in MENA was about 12.4 years.\nThe academic age group 6-to-10 years is the most common for both emigrant and\nimmigrant scholars. Immigrants are relatively younger than emigrants, except\nfor Iran, Palestine, Lebanon, and Turkey. Scholars who migrated to Gulf\nCooperation Council countries, Jordan and Morocco were in average younger than\nemigrants by 1.5 year from the same countries. Third, we analyse gender\ndifferences. We observe a clear gender gap: Male scholars represent the largest\ngroup of migrants in MENA. We conclude discussing the policy relevance of the\nscientific mobility and collaboration aspects.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:17:00 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 12:24:20 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["El-Ouahi", "Jamal", ""], ["Robinson-Garcia", "Nicolas", ""], ["Costas", "Rodrigo", ""]]}, {"id": "2009.08114", "submitter": "Mariona Coll Ardanuy", "authors": "Mariona Coll Ardanuy, Kasra Hosseini, Katherine McDonough, Amrey\n  Krause, Daniel van Strien and Federico Nanni", "title": "A Deep Learning Approach to Geographical Candidate Selection through\n  Toponym Matching", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing toponyms and resolving them to their real-world referents is\nrequired for providing advanced semantic access to textual data. This process\nis often hindered by the high degree of variation in toponyms. Candidate\nselection is the task of identifying the potential entities that can be\nreferred to by a toponym previously recognized. While it has traditionally\nreceived little attention in the research community, it has been shown that\ncandidate selection has a significant impact on downstream tasks (i.e. entity\nresolution), especially in noisy or non-standard text. In this paper, we\nintroduce a flexible deep learning method for candidate selection through\ntoponym matching, using state-of-the-art neural network architectures. We\nperform an intrinsic toponym matching evaluation based on several new realistic\ndatasets, which cover various challenging scenarios (cross-lingual and regional\nvariations, as well as OCR errors). We report its performance on candidate\nselection in the context of the downstream task of toponym resolution, both on\nexisting datasets and on a new manually-annotated resource of\nnineteenth-century English OCR'd text.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 07:24:56 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 14:24:12 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Ardanuy", "Mariona Coll", ""], ["Hosseini", "Kasra", ""], ["McDonough", "Katherine", ""], ["Krause", "Amrey", ""], ["van Strien", "Daniel", ""], ["Nanni", "Federico", ""]]}, {"id": "2009.08374", "submitter": "Chaomei Chen", "authors": "Chaomei Chen", "title": "A Glimpse of the First Eight Months of the COVID-19 Literature on\n  Microsoft Academic Graph: Themes, Citation Contexts, and Uncertainties", "comments": "9 figures, 5 tables", "journal-ref": "Front. Res. Metr. Anal. 5:607286 (2020)", "doi": "10.3389/frma.2020.607286", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As scientists worldwide search for answers to the overwhelmingly unknown\nbehind the deadly pandemic, the literature concerning COVID-19 has been growing\nexponentially. Keeping abreast of the body of literature at such a rapidly\nadvancing pace poses significant challenges not only to active researchers but\nalso to the society as a whole. Although numerous data resources have been made\nopenly available, the analytic and synthetic process that is essential in\neffectively navigating through the vast amount of information with heightened\nlevels of uncertainty remains a significant bottleneck. We introduce a generic\nmethod that facilitates the data collection and sense-making process when\ndealing with a rapidly growing landscape of a research domain such as COVID-19\nat multiple levels of granularity. The method integrates the analysis of\nstructural and temporal patterns in scholarly publications with the delineation\nof thematic concentrations and the types of uncertainties that may offer\nadditional insights into the complexity of the unknown. We demonstrate the\napplication of the method in a study of the COVID-19 literature.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:39:16 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Chen", "Chaomei", ""]]}, {"id": "2009.09049", "submitter": "Simon Razniewski", "authors": "Jesse Josua Benjamin, Claudia M\\\"uller-Birn, Simon Razniewski", "title": "Examining the Impact of Algorithm Awareness on Wikidata's Recommender\n  System Recoin", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global infrastructure of the Web, designed as an open and transparent\nsystem, has a significant impact on our society. However, algorithmic systems\nof corporate entities that neglect those principles increasingly populated the\nWeb. Typical representatives of these algorithmic systems are recommender\nsystems that influence our society both on a scale of global politics and\nduring mundane shopping decisions. Recently, such recommender systems have come\nunder critique for how they may strengthen existing or even generate new kinds\nof biases. To this end, designers and engineers are increasingly urged to make\nthe functioning and purpose of recommender systems more transparent. Our\nresearch relates to the discourse of algorithm awareness, that reconsiders the\nrole of algorithm visibility in interface design. We conducted online\nexperiments with 105 participants using MTurk for the recommender system\nRecoin, a gadget for Wikidata. In these experiments, we presented users with\none of a set of three different designs of Recoin's user interface, each of\nthem exhibiting a varying degree of explainability and interactivity. Our\nfindings include a positive correlation between comprehension of and trust in\nan algorithmic system in our interactive redesign. However, our results are not\nconclusive yet, and suggest that the measures of comprehension, fairness,\naccuracy and trust are not yet exhaustive for the empirical study of algorithm\nawareness. Our qualitative insights provide a first indication for further\nmeasures. Our study participants, for example, were less concerned with the\ndetails of understanding an algorithmic calculation than with who or what is\njudging the result of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 20:06:53 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Benjamin", "Jesse Josua", ""], ["M\u00fcller-Birn", "Claudia", ""], ["Razniewski", "Simon", ""]]}, {"id": "2009.09074", "submitter": "Xia Li", "authors": "Rachel Grotheer, Yihuan Huang, Pengyu Li, Elizaveta Rebrova, Deanna\n  Needell, Longxiu Huang, Alona Kryshchenko, Xia Li, Kyung Ha, Oleksandr\n  Kryshchenko", "title": "COVID-19 Literature Topic-Based Search via Hierarchical NMF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dataset of COVID-19-related scientific literature is compiled, combining\nthe articles from several online libraries and selecting those with open access\nand full text available. Then, hierarchical nonnegative matrix factorization is\nused to organize literature related to the novel coronavirus into a tree\nstructure that allows researchers to search for relevant literature based on\ndetected topics. We discover eight major latent topics and 52 granular\nsubtopics in the body of literature, related to vaccines, genetic structure and\nmodeling of the disease and patient studies, as well as related diseases and\nvirology. In order that our tool may help current researchers, an interactive\nwebsite is created that organizes available literature using this hierarchical\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 05:45:03 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Grotheer", "Rachel", ""], ["Huang", "Yihuan", ""], ["Li", "Pengyu", ""], ["Rebrova", "Elizaveta", ""], ["Needell", "Deanna", ""], ["Huang", "Longxiu", ""], ["Kryshchenko", "Alona", ""], ["Li", "Xia", ""], ["Ha", "Kyung", ""], ["Kryshchenko", "Oleksandr", ""]]}, {"id": "2009.10442", "submitter": "Michael Taylor", "authors": "Michael Taylor", "title": "Open Access Books in the Humanities and Social Sciences: an Open Access\n  Altmetric Advantage", "comments": null, "journal-ref": null, "doi": "10.1007/s11192-020-03735-8", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last decade has seen two significant phenomena emerge in research\ncommunication: the rise of open access (OA) publishing, and evidence of online\nsharing in the form of altmetrics. There has been limited examination of the\neffect of OA on online sharing for journal articles, and little for books. This\npaper examines the altmetrics of a set of 32,222 books (of which 5% are OA) and\na set of 220,527 chapters (of which 7% are OA) indexed by the scholarly\ndatabase Dimensions in the Social Sciences and Humanities. Both OA books and\nchapters have significantly higher use on social networks, higher coverage in\nthe mass media and blogs, and evidence of higher rates of social impact in\npolicy documents. OA chapters have higher rates of coverage on Wikipedia than\ntheir non-OA equivalents, and are more likely to be shared on Mendeley. Even\nwithin the Humanities and Social Sciences, disciplinary differences in\naltmetric activity are evident. The effect is confirmed for chapters, although\nsampling issues prevent the strong conclusion that OA facilitates extra\nattention at whole book level, the apparent OA altmetrics advantage suggests\nthat the move towards OA is increasing social sharing and broader impact.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 10:41:19 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Taylor", "Michael", ""]]}, {"id": "2009.11207", "submitter": "Tiziano Piccardi", "authors": "Tiziano Piccardi, Robert West", "title": "Crosslingual Topic Modeling with WikiPDA", "comments": "10 pages, WWW - The Web Conference, 2021", "journal-ref": null, "doi": "10.1145/3442381.3449805", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Wikipedia-based Polyglot Dirichlet Allocation (WikiPDA), a\ncrosslingual topic model that learns to represent Wikipedia articles written in\nany language as distributions over a common set of language-independent topics.\nIt leverages the fact that Wikipedia articles link to each other and are mapped\nto concepts in the Wikidata knowledge base, such that, when represented as bags\nof links, articles are inherently language-independent. WikiPDA works in two\nsteps, by first densifying bags of links using matrix completion and then\ntraining a standard monolingual topic model. A human evaluation shows that\nWikiPDA produces more coherent topics than monolingual text-based LDA, thus\noffering crosslinguality at no cost. We demonstrate WikiPDA's utility in two\napplications: a study of topical biases in 28 Wikipedia editions, and\ncrosslingual supervised classification. Finally, we highlight WikiPDA's\ncapacity for zero-shot language transfer, where a model is reused for new\nlanguages without any fine-tuning. Researchers can benefit from WikiPDA as a\npractical tool for studying Wikipedia's content across its 299 language\neditions in interpretable ways, via an easy-to-use library publicly available\nat https://github.com/epfl-dlab/WikiPDA.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:19:27 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 13:28:18 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Piccardi", "Tiziano", ""], ["West", "Robert", ""]]}, {"id": "2009.11920", "submitter": "David Guillermo Fajardo Ortiz Dr.", "authors": "David Fajardo-Ortiz, Stefan Hornbostel, Maywa Montenegro-de-Wit, Annie\n  Shattuck", "title": "Funding CRISPR: Understanding the role of government and philanthropic\n  institutions in supporting academic research within the CRISPR innovation\n  system", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CRISPR/Cas has the potential to revolutionize medicine, agriculture, and\nbiology. Understanding the trajectory of innovation, how it is influenced and\nwho pays for it, is an essential research policy question, especially as US\ngovernment support for research experiences a relative decline. We use a new\nmethod -- based on funding sources identified in publications' funding\nacknowledgements -- to map the networks involved in supporting key stages of\nhighly influential research, namely basic biological research and technology\ndevelopment. We present a model of co-funding networks at the two most\nprominent institutions for CRISPR/Cas research, the University of California\nand the Harvard/MIT/Broad Institute, to illuminate how philanthropic and\ncharitable organizations have articulated with US government agencies to\nco-finance the discovery and development of CRISPR/Cas. We mapped foundational\nUS government support to both stages of CRISPR/Cas research at both\ninstitutions, while philanthropic organizations have concentrated in co-funding\nCRISPR/Cas technology development as opposed to basic biological research. This\nis particularly true for the Broad/Harvard/MIT system, where philanthropic\ninvestment clustered around particular technological development themes. These\nnetwork models raise fundamental questions about the role of the state and the\ninfluence of philanthropy over the trajectory of transformative technologies.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 19:29:34 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 19:13:03 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 21:13:32 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 15:25:12 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Fajardo-Ortiz", "David", ""], ["Hornbostel", "Stefan", ""], ["Montenegro-de-Wit", "Maywa", ""], ["Shattuck", "Annie", ""]]}, {"id": "2009.12417", "submitter": "Mohammad Javad Shayegan", "authors": "Mohammad Javad Shayegan and Maasoumeh Kouhzadi", "title": "An Analysis of the Impact of SEO on University Website Ranking", "comments": "A Persian version of this paper has been published", "journal-ref": "Seyfabad, M. K., & Fard, M. J. S. (2019). An Analysis of the\n  Impact of SEO on University Website Ranking. Iranian Journal of Information\n  Processing and Management, 34(4), 1787-1810", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, ranking systems in universities have been considered by the academic\ncommunity, and there is a tight competition between world universities to\nachieve higher ranks. In the meantime, the ranking of university websites is\nalso in the spotlight, and the Webometric research center announces the ranks\nof university websites twice a year. Examining university rankings indicators\nand the Webometric ranks of the university indicates that some of these\nindicators, directly and indirectly, affect each other. On the other hand, a\npreliminary study of Webometric indicators shows that some Search Engine\nOptimization (SEO) indicators can affect Webometric ranks. The purpose of this\nresearch is to show how far the SEO metrics can affect the website rank of the\nuniversity. To do this, after extracting 38 points of the significant SEO\nmetrics of the extracted universities using various tools, data analysis was\nconducted along with applying association rules on the data. The results of the\nresearch show that some of the SEO metrics, such as the number of backlinks,\nAlexa Rank, and Page Rank have a direct and significant impact on the website\nrank of universities, and in this regard, interesting rules have been\nextracted.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 20:03:00 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 15:46:39 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Shayegan", "Mohammad Javad", ""], ["Kouhzadi", "Maasoumeh", ""]]}, {"id": "2009.12500", "submitter": "Meijun Liu", "authors": "Meijun Liu, Yi Bu, Chongyan Chen, Jian Xu, Daifeng Li, Yan Leng,\n  Richard Barry Freeman, Eric Meyer, Wonjin Yoon, Mujeen Sung, Minbyul Jeong,\n  Jinhyuk Lee, Jaewoo Kang, Chao Min, Min Song, Yujia Zhai, Ying Ding", "title": "Can pandemics transform scientific novelty? Evidence from COVID-19", "comments": "38 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific novelty is important during the pandemic due to its critical role\nin generating new vaccines. Parachuting collaboration and international\ncollaboration are two crucial channels to expand teams' search activities for a\nbroader scope of resources required to address the global challenge. Our\nanalysis of 58,728 coronavirus papers suggests that scientific novelty measured\nby the BioBERT model that is pre-trained on 29 million PubMed articles, and\nparachuting collaboration dramatically increased after the outbreak of\nCOVID-19, while international collaboration witnessed a sudden decrease. During\nthe COVID-19, papers with more parachuting collaboration and internationally\ncollaborative papers are predicted to be more novel. The findings suggest the\nnecessity of reaching out for distant resources, and the importance of\nmaintaining a collaborative scientific community beyond established networks\nand nationalism during a pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 01:31:09 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 14:53:22 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Liu", "Meijun", ""], ["Bu", "Yi", ""], ["Chen", "Chongyan", ""], ["Xu", "Jian", ""], ["Li", "Daifeng", ""], ["Leng", "Yan", ""], ["Freeman", "Richard Barry", ""], ["Meyer", "Eric", ""], ["Yoon", "Wonjin", ""], ["Sung", "Mujeen", ""], ["Jeong", "Minbyul", ""], ["Lee", "Jinhyuk", ""], ["Kang", "Jaewoo", ""], ["Min", "Chao", ""], ["Song", "Min", ""], ["Zhai", "Yujia", ""], ["Ding", "Ying", ""]]}, {"id": "2009.13059", "submitter": "Shashwat Aggarwal", "authors": "Shashwat Aggarwal, Ramesh Singh", "title": "Visual Exploration and Knowledge Discovery from Biomedical Dark Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization techniques proffer efficient means to organize and present\ndata in graphically appealing formats, which not only speeds up the process of\ndecision making and pattern recognition but also enables decision-makers to\nfully understand data insights and make informed decisions. Over time, with the\nrise in technological and computational resources, there has been an\nexponential increase in the world's scientific knowledge. However, most of it\nlacks structure and cannot be easily categorized and imported into regular\ndatabases. This type of data is often termed as Dark Data. Data visualization\ntechniques provide a promising solution to explore such data by allowing quick\ncomprehension of information, the discovery of emerging trends, identification\nof relationships and patterns, etc. In this empirical research study, we use\nthe rich corpus of PubMed comprising of more than 30 million citations from\nbiomedical literature to visually explore and understand the underlying\nkey-insights using various information visualization techniques. We employ a\nnatural language processing based pipeline to discover knowledge out of the\nbiomedical dark data. The pipeline comprises of different lexical analysis\ntechniques like Topic Modeling to extract inherent topics and major focus\nareas, Network Graphs to study the relationships between various entities like\nscientific documents and journals, researchers, and, keywords and terms, etc.\nWith this analytical research, we aim to proffer a potential solution to\novercome the problem of analyzing overwhelming amounts of information and\ndiminish the limitation of human cognition and perception in handling and\nexamining such large volumes of data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 04:27:05 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Aggarwal", "Shashwat", ""], ["Singh", "Ramesh", ""]]}, {"id": "2009.13294", "submitter": "Rohit Rawat", "authors": "Rohit Rawat", "title": "Virtual Proximity Citation (VCP): A Supervised Deep Learning Method to\n  Relate Uncited Papers On Grounds of Citation Proximity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Citation based approaches have seen good progress for recommending research\npapers using citations in the paper. Citation proximity analysis which uses the\nin-text citation proximity to find relatedness between two research papers is\nbetter than co-citation analysis and bibliographic analysis. However, one\ncommon problem which exists in each approach is that paper should be well\ncited. If documents are not cited properly or not cited at all, then using\nthese approaches will not be helpful. To overcome the problem, this paper\ndiscusses the approach Virtual Citation Proximity (VCP) which uses Siamese\nNeural Network along with the notion of citation proximity analysis and\ncontent-based filtering. To train this model, the actual distance between the\ntwo citations in a document is used as ground truth, this distance is the word\ncount between the two citations. VCP is trained on Wikipedia articles for which\nthe actual word count is available which is used to calculate the similarity\nbetween the documents. This can be used to calculate relatedness between two\ndocuments in a way they would have been cited in the proximity even if the\ndocuments are uncited. This approach has shown a great improvement in\npredicting proximity with basic neural networks over the approach which uses\nthe Average Citation Proximity index value as the ground truth. This can be\nimproved by using a complex neural network and proper hyper tuning of\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:24:00 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Rawat", "Rohit", ""]]}, {"id": "2009.13395", "submitter": "Satoshi Takahashi", "authors": "Satoshi Takahashi, Keiko Yamaguchi, and Asuka Watanabe", "title": "CAT STREET: Chronicle Archive of Tokyo Street-fashion", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of daily-life fashion trends can provide us a profound\nunderstanding of our societies and cultures. However, no appropriate digital\narchive exists that includes images illustrating what people wore in their\ndaily lives over an extended period. In this study, we propose a new fashion\nimage archive, Chronicle Archive of Tokyo Street-fashion (CAT STREET), to shed\nlight on daily-life fashion trends. CAT STREET includes images showing what\npeople wore in their daily lives during 1970--2017, and these images contain\ntimestamps and street location annotations. This novel database combined with\nmachine learning enables us to observe daily-life fashion trends over a long\nterm and analyze them quantitatively. To evaluate the potential of our proposed\napproach with the novel database, we corroborated the rules-of-thumb of two\nfashion trend phenomena that have been observed and discussed qualitatively in\nprevious studies. Through these empirical analyses, we verified that our\napproach to quantify fashion trends can help in exploring unsolved research\nquestions. We also demonstrate CAT STREET's potential to find new standpoints\nto promote the understanding of societies and cultures through fashion embedded\nin consumers' daily lives.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:16:45 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 13:54:35 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Takahashi", "Satoshi", ""], ["Yamaguchi", "Keiko", ""], ["Watanabe", "Asuka", ""]]}, {"id": "2009.14323", "submitter": "Alberto Accomazzi", "authors": "Michael J. Kurtz, Alberto Accomazzi, Edwin A. Henneken", "title": "Enabling Synergy: Improving the Information Infrastructure for Planetary\n  Science", "comments": "8 pages, submitted to the Planetary Science and Astrobiology Decadal\n  Survey 2023-2032", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this whitepaper we advocate that the Planetary Science (PS) community\nbuild a discipline-specific digital library, in collaboration with the existing\nastronomy digital library, ADS. We suggest that the PS data archives increase\ntheir level of curation to allow for direct linking between the archival data\nand the derived journal articles. And we suggest that a new component of the PS\ninformation infrastructure be created to collate and curate information on\nfeatures and objects in our solar system, beginning with the USGS/IAU Gazetteer\nof Planetary Nomenclature.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 22:06:36 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Kurtz", "Michael J.", ""], ["Accomazzi", "Alberto", ""], ["Henneken", "Edwin A.", ""]]}]