[{"id": "1805.00390", "submitter": "Snehanshu Saha", "authors": "Gouri Ginde, Snehanshu Saha, Archana Mathur, Harsha Vamsi, Sudeepa Roy\n  Dey, Swati Sampatrao Gambhire", "title": "Use of NoSQL database and visualization techniques to analyze massive\n  scholarly article data from journals", "comments": "Data acquisition methods, Web scraping, Graph database, Neo4j, data\n  visualization, Cobb Douglas model. arXiv admin note: substantial text overlap\n  with arXiv:1611.01152", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization of the massive data is a challenging endeavor. Extracting data\nand providing graphical representations can aid in its effective utilization in\nterms of interpretation and knowledge discovery. Publishing research articles\nhas become a way of life for academicians. The scholarly publications can\nshape-up the professional growth of authors and also expand the research and\ntechnological growth of a country, continent and other demographic regions.\nScholarly articles have grown in gigantic numbers that are published in\ndifferent domains by various journals. Information related to articles,\nauthors, their affiliations, number of citations, country, publisher,\nreferences and other information is like a gold mine for statisticians and data\nanalysts. This data when used skillfully, via visual analysis tool, can provide\nvaluable understanding and can aid in deeper exposition for researchers working\nin domains like scientometrics and bibliometrics. Since the data is not readily\navailable, we used Google scholar, a comprehensive and free repository of\nscholarly articles, as data source for our study. Data was scraped from Google\nscholar and stored as a graph and later visualized in the form of nodes and its\nrelationships, which offered discerning and concealed information of growing\nimpact of articles, journals and authors in their domains. Not only this,\nevident domain shift of an author, various research domains spread for an\nauthor, predicting emerging domain and subdomains, detecting cartel behavior at\nJournal and author-level was also depicted by graphical analysis. Neo4j graph\ndatabase was used in the background to help store the data in structured\nmanner.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 06:30:39 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Ginde", "Gouri", ""], ["Saha", "Snehanshu", ""], ["Mathur", "Archana", ""], ["Vamsi", "Harsha", ""], ["Dey", "Sudeepa Roy", ""], ["Gambhire", "Swati Sampatrao", ""]]}, {"id": "1805.02434", "submitter": "Henk Moed", "authors": "Henk F. Moed, Valentina Markusova and Mark Akoev", "title": "Trends in Russian research output indexed in Scopus and Web of Science", "comments": "Author copy of a manuscript accepted for publication in the journal\n  Scientometrics, May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trends are analysed in the annual number of documents published by Russian\ninstitutions and indexed in Scopus and Web of Science, giving special attention\nto the time period starting in the year 2013 in which the Project 5-100 was\nlaunched by the Russian Government. Numbers are broken down by document type,\npublication language, type of source, research discipline, country and source.\nIt is concluded that Russian publication counts strongly depend upon the\ndatabase used, and upon changes in database coverage, and that one should be\ncautious when using indicators derived from WoS, and especially from Scopus, as\ntools in the measurement of research performance and international orientation\nof the Russian science system.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 10:46:21 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Moed", "Henk F.", ""], ["Markusova", "Valentina", ""], ["Akoev", "Mark", ""]]}, {"id": "1805.03348", "submitter": "Roy Ka-Wei Lee", "authors": "Roy Ka-Wei Lee and David Lo", "title": "Wisdom in Sum of Parts: Multi-Platform Activity Prediction in Social\n  Collaborative Sites", "comments": "Pre-print for 10th ACM Conference on Web Science 2018 (WebSci'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a novel framework which uses user interests\ninferred from activities (a.k.a., activity interests) in multiple social\ncollaborative platforms to predict users' platform activities. Included in the\nframework are two prediction approaches: (i) direct platform activity\nprediction, which predicts a user's activities in a platform using his or her\nactivity interests from the same platform (e.g., predict if a user answers a\ngiven Stack Overflow question using the user's interests inferred from his or\nher prior answer and favorite activities in Stack Overflow), and (ii)\ncross-platform activity prediction, which predicts a user's activities in a\nplatform using his or her activity interests from another platform (e.g.,\npredict if a user answers a given Stack Overflow question using the user's\ninterests inferred from his or her fork and watch activities in GitHub). To\nevaluate our proposed method, we conduct prediction experiments on two widely\nused social collaborative platforms in the software development community:\nGitHub and Stack Overflow. Our experiments show that combining both direct and\ncross-platform activity prediction approaches yield the best accuracies for\npredicting user activities in GitHub (AUC=0.75) and Stack Overflow (AUC=0.89).\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 02:01:19 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Lee", "Roy Ka-Wei", ""], ["Lo", "David", ""]]}, {"id": "1805.03423", "submitter": "Gy\\\"orgy Csom\\'os", "authors": "Gyorgy Csomos", "title": "Factors Influencing Cities' Publishing Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a vast number of scientific publications have been produced in\ncities in emerging countries. It has long been observed that the publication\noutput of Beijing has exceeded that of any other city in the world, including\nsuch leading centres of science as Boston, New York, London, Paris, and Tokyo.\nResearchers have suggested that, instead of focusing on cities' total\npublication output, the quality of the output in terms of the number of highly\ncited papers should be examined. However, in the period from 2014 to 2016,\nBeijing produced as many highly cited papers as Boston, London, or New York. In\nthis paper, I propose another method to measure cities' publishing performance;\nI focus on cities' publishing efficiency (i.e., the ratio of highly cited\narticles to all articles produced in that city). First, I rank 554 cities based\non their publishing efficiency, then I reveal some general factors influencing\ncities' publishing efficiency. The general factors examined in this paper are\nas follows: the linguistic environment, cities' economic development level, the\nlocation of excellent organisations, cities' international collaboration\npatterns, and the productivity of scientific disciplines.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 09:07:56 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Csomos", "Gyorgy", ""]]}, {"id": "1805.03558", "submitter": "Snehanshu Saha", "authors": "Poulami Sarkar, Snehanshu Saha, Archana Mathur, Rahul Aedula, Saibal\n  Kar, Surbhi Agrawal and Kakoli Bora", "title": "Time Reversed Delay Differential Equation Based Modeling Of Journal\n  Influence In An Emerging Area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent independent study resulted in a ranking system which ranked\nAstronomy and Computing (ASCOM) much higher than most of the older journals\nhighlighting its niche prominence. We investigate the notable ascendancy in\nreputation of ASCOM by proposing a novel differential equation based modeling.\nThe modeling is a consequence of knowledge discovery from big data methods,\nnamely L1-SVD. We propose a growth model by accounting for the behavior of\nparameters that contribute to the growth of a field. It is worthwhile to spend\nsome time in analyzing the cause and control variables behind rapid rise in the\nreputation of a journal in a niche area. We intend to identify and probe the\nparameters responsible for its growing influence. Delay differential equations\nare used to model the change of influence on a journal's status by exploiting\nthe effects of historical data. The manuscript justifies the use of implicit\ncontrol variables and models those accordingly that demonstrate certain\nbehavior in the journal influence.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 14:42:11 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Sarkar", "Poulami", ""], ["Saha", "Snehanshu", ""], ["Mathur", "Archana", ""], ["Aedula", "Rahul", ""], ["Kar", "Saibal", ""], ["Agrawal", "Surbhi", ""], ["Bora", "Kakoli", ""]]}, {"id": "1805.03885", "submitter": "Niel Chah", "authors": "Niel Chah", "title": "OK Google, What Is Your Ontology? Or: Exploring Freebase Classification\n  to Understand Google's Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reconstructs the Freebase data dumps to understand the underlying\nontology behind Google's semantic search feature. The Freebase knowledge base\nwas a major Semantic Web and linked data technology that was acquired by Google\nin 2010 to support the Google Knowledge Graph, the backend for Google search\nresults that include structured answers to queries instead of a series of links\nto external resources. After its shutdown in 2016, Freebase is contained in a\ndata dump of 1.9 billion Resource Description Format (RDF) triples. A\nrecomposition of the Freebase ontology will be analyzed in relation to concepts\nand insights from the literature on classification by Bowker and Star. This\npaper will explore how the Freebase ontology is shaped by many of the forces\nthat also shape classification systems through a deep dive into the ontology\nand a small correlational study. These findings will provide a glimpse into the\nproprietary blackbox Knowledge Graph and what is meant by Google's mission to\n\"organize the world's information and make it universally accessible and\nuseful\".\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 08:39:40 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 11:54:16 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Chah", "Niel", ""]]}, {"id": "1805.04022", "submitter": "Dimitar Dimitrov", "authors": "Dimitar Dimitrov, Florian Lemmerich, Fabian Fl\\\"ock, and Markus\n  Strohmaier", "title": "Query for Architecture, Click through Military: Comparing the Roles of\n  Search and Navigation on Wikipedia", "comments": null, "journal-ref": null, "doi": "10.1145/3201064.3201092", "report-no": null, "categories": "cs.SI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the richest sources of encyclopedic information on the Web,\nWikipedia generates an enormous amount of traffic. In this paper, we study\nlarge-scale article access data of the English Wikipedia in order to compare\narticles with respect to the two main paradigms of information seeking, i.e.,\nsearch by formulating a query, and navigation by following hyperlinks. To this\nend, we propose and employ two main metrics, namely (i) searchshare -- the\nrelative amount of views an article received by search --, and (ii) resistance\n-- the ability of an article to relay traffic to other Wikipedia articles -- to\ncharacterize articles. We demonstrate how articles in distinct topical\ncategories differ substantially in terms of these properties. For example,\narchitecture-related articles are often accessed through search and are\nsimultaneously a \"dead end\" for traffic, whereas historical articles about\nmilitary events are mainly navigated. We further link traffic differences to\nvarying network, content, and editing activity features. Lastly, we measure the\nimpact of the article properties by modeling access behavior on articles with a\ngradient boosting approach. The results of this paper constitute a step towards\nunderstanding human information seeking behavior on the Web.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 15:25:46 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Dimitrov", "Dimitar", ""], ["Lemmerich", "Florian", ""], ["Fl\u00f6ck", "Fabian", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1805.04515", "submitter": "Harris Kyriakou", "authors": "Peng Huang, Atreyi Kankanhalli, Harris Kyriakou, Rajiv Sabherwal", "title": "Research Curation on Knowledge Management", "comments": null, "journal-ref": "MIS Quarterly, 2018", "doi": null, "report-no": null, "categories": "cs.DL cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The notions of knowledge and its management have been at the core of the\ninformation systems (IS) field almost since its inception. Knowledge has been\nviewed in several ways in the prior literature, including as a state of mind,\nan object, a process, access to information, and a capability. A commonly-used\ndefinition characterizes knowledge as a justified belief that increases an\nentity's capacity for effective action (Alavi and Leidner 2001, p. 109).\nRelatedly, knowledge management (KM) has been defined as a systemic process to\nacquire, organize, and communicate individual knowledge so that others may make\nuse of it (Beck et al. 2014). Knowledge-management systems (KMSs) support these\nprocesses for creating, exchanging, and storing knowledge (Beck et al. 2014),\nand have been viewed as being either repository- based or network-based\n(Kankanhalli et al. 2005). In an attempt to provide a useful resource for\nscholars interested in KM, we take stock of the pertinent research published in\nMISQ. More specifically, the goal of this curation is to serve as a living\ndocument that will offer a starting point for future KM research. This curation\nhighlights the 44 articles with a primary focus on KM (Table 1). The articles\naddress theoretical and conceptual issues, provide methodological guidance, and\nuse a wide range of quantitative and qualitative research methods. To define\nthe scope of this curation, we excluded: (1) articles in which KM is used as\npart of another construct; (2) some early articles that were practice- oriented\nwith limited scholarly orientation; and (3) articles that focus on knowledge\n(such as the knowledge requirements of IS professionals) but not on KM.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 10:31:48 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Huang", "Peng", ""], ["Kankanhalli", "Atreyi", ""], ["Kyriakou", "Harris", ""], ["Sabherwal", "Rajiv", ""]]}, {"id": "1805.04647", "submitter": "Thomas Price", "authors": "Tom Price, Sabine Hossenfelder", "title": "Measuring Scientific Broadness", "comments": "18 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Who has not read letters of recommendations that comment on a student's\n`broadness' and wondered what to make of it? We here propose a way to quantify\nscientific broadness by a semantic analysis of researchers' publications. We\napply our methods to papers on the open-access server arXiv.org and report our\nfindings.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 04:14:48 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 05:32:09 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Price", "Tom", ""], ["Hossenfelder", "Sabine", ""]]}, {"id": "1805.04798", "submitter": "Niall Martin Ryan", "authors": "Niall Martin Ryan", "title": "Citation Data-set for Machine Learning Citation Styles and Entity\n  Extraction from Citation Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Citation parsing is fundamental for search engines within academia and the\nprotection of intellectual property. Meticulous extraction is further needed\nwhen evaluating the similarity of documents and calculating their citation\nimpact. Citation parsing involves the identification and dissection of citation\nstrings into their bibliography components such as \"Author\", \"Volume\",\"Title\",\netc. This meta-data can be difficult to acquire accurately due to the thousands\nof different styles and noise that can be applied to a bibliography to create\nthe citation string. Many approaches exist already to accomplish accurate\nparsing of citation strings. This dissertation describes the creation of a\nlarge data-set which can be used to aid in the training of these approaches\nwhich have limited data.It also describes the investigation into if the\ndownfall of these approaches to citation parsing and in particular the machine\nlearning based approaches is because of the lack of size associated to the data\nused to train them.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 23:45:24 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 22:42:15 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ryan", "Niall Martin", ""]]}, {"id": "1805.04882", "submitter": "Georgios Tritsaris", "authors": "Georgios A. Tritsaris, Afreen Siddiqi", "title": "Interdisciplinary collaboration in research networks: Empirical analysis\n  of energy-related research in Greece", "comments": "21 pages, 6 figures, 2 tables, 1 SI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological innovation is intimately related to knowledge creation and\nrecombination. In this work we introduce a combined statistical and\nnetwork-based approach to study collaboration in scientific authorship. We\napply it to characterize recent research efforts in renewable energy technology\nand its intersections with the domains of nanoscience and nanotechnology with\nfocus on materials, and electrical engineering and computer science in Greece\nand its broader European and international environment as a case study. Using\nour methods we attempt to illuminate the processes which underlie knowledge\ncreation and diversification in these research networks: a (positive)\nrelationship between expenditure on research and development and the extent and\ndiversity of team-based research at the intersections of the three domains is\nestablished. Our specific findings collectively provide insights into the\ncollaboration structure and evolution of energy-related research activity in\nGreece, while our methodology can be used for evidence-based design,\nmonitoring, and evaluation of interdisciplinary research programs.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 13:24:27 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 14:26:27 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Tritsaris", "Georgios A.", ""], ["Siddiqi", "Afreen", ""]]}, {"id": "1805.05238", "submitter": "Waleed Ammar", "authors": "Sergey Feldman, Kyle Lo, Waleed Ammar", "title": "Citation Count Analysis for Papers with Preprints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the degree to which papers prepublished on arXiv garner more\ncitations, in an attempt to paint a sharper picture of fairness issues related\nto prepublishing. A paper's citation count is estimated using a\nnegative-binomial generalized linear model (GLM) while observing a binary\nvariable which indicates whether the paper has been prepublished. We control\nfor author influence (via the authors' h-index at the time of paper writing),\npublication venue, and overall time that paper has been available on arXiv. Our\nanalysis only includes papers that were eventually accepted for publication at\ntop-tier CS conferences, and were posted on arXiv either before or after the\nacceptance notification. We observe that papers submitted to arXiv before\nacceptance have, on average, 65\\% more citations in the following year compared\nto papers submitted after. We note that this finding is not causal, and discuss\npossible next steps.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 15:42:07 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Feldman", "Sergey", ""], ["Lo", "Kyle", ""], ["Ammar", "Waleed", ""]]}, {"id": "1805.06153", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Werner Marx", "title": "Critical rationalism and the search for standard (field-normalized)\n  indicators in bibliometrics", "comments": "Accepted for publication in Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliometrics plays an increasingly important role in research evaluation.\nHowever, no gold standard exists for a set of reliable and valid\n(field-normalized) impact indicators in research evaluation. This opinion paper\nrecommends that bibliometricians develop and analyze these impact indicators\nagainst the backdrop of Popper's critical rationalism. The studies critically\ninvestigating the indicators should publish the results in such a way that they\ncan be included in meta-analyses. The results of meta-analyses give guidance on\nwhich indicators can then be part of a set of indicators used as standard in\nbibliometrics. The generation and continuous revision of the standard set could\nbe handled by the International Society for Informetrics and Scientometrics\n(ISSI).\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 06:45:15 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 08:58:48 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Bornmann", "Lutz", ""], ["Marx", "Werner", ""]]}, {"id": "1805.06981", "submitter": "Ferdinando Patat", "authors": "Ferdinando Patat", "title": "Peer-review under review - A statistical study on proposal ranking at\n  ESO. Part I: the pre-meeting phase", "comments": "22 pages, 18 figures. Accepted for publication in the Publications of\n  the Astronomical Society of Pacific", "journal-ref": null, "doi": "10.1088/1538-3873/aac463", "report-no": null, "categories": "physics.soc-ph astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer review is the most common mechanism in place for assessing requests for\nresources in a large variety of scientific disciplines. One of the strongest\ncriticisms to this paradigm is the limited reproducibility of the process,\nespecially at largely oversubscribed facilities. In this and in a subsequent\npaper we address this specific aspect in a quantitative way, through a\nstatistical study on proposal ranking at the European Southern Observatory. For\nthis purpose we analysed a sample of about 15000 proposals, submitted by more\nthan 3000 Principal Investigators over 8 years. The proposals were reviewed by\nmore than 500 referees, who assigned over 140000 grades in about 200 panel\nsessions. After providing a detailed analysis of the statistical properties of\nthe sample, the paper presents an heuristic model based on these findings,\nwhich is then used to provide quantitative estimates of the reproducibility of\nthe pre-meeting process. On average, about one third of the proposals ranked in\nthe top quartile by one referee are ranked in the same quartile by any other\nreferee of the panel. A similar value is observed for the bottom quartile. In\nthe central quartiles, the agreement fractions are very marginally above the\nvalue expected for a fully aleatory process (25%). The agreement fraction\nbetween two panels composed by 6 referees is 55+/-5% (50% confidence level) for\nthe top and bottom quartiles. The corresponding fraction for the central\nquartiles is 33+/-5%. The model predictions are confirmed by the results\nobtained from boot-strapping the data for sub-panels composed by 3 referees,\nand fully consistent with the NIPS experiment. The post-meeting phase will be\npresented and discussed in a forthcoming paper.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 12:36:44 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Patat", "Ferdinando", ""]]}, {"id": "1805.08040", "submitter": "Marco Molinaro", "authors": "Marco Molinaro, Nicola F. Calabria, Robert Butora, Sonia Zorba and\n  Riccardo Smareglia", "title": "Italian center for Astronomical Archives publishing solution: modular\n  and distributed", "comments": "SPIE Astronomical Telescopes + Instrumentation 2018, Software and\n  Cyberinfrastructure for Astronomy V, pre-publishing draft proceeding (reduced\n  abstract)", "journal-ref": "Proc. SPIE 10707, Software and Cyberinfrastructure for Astronomy\n  V, 1070722 (6 July 2018)", "doi": "10.1117/12.2311967", "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Italian center for Astronomical Archives tries to provide astronomical\ndata resources as interoperable services based on IVOA standards. Its VO\nexpertise and knowledge comes from active participation within IVOA and VO at\nEuropean and international level, with a double-fold goal: learn from the\ncollaboration and provide inputs to the community. The first solution to build\nan easy to configure and maintain resource publisher conformant to VO standards\nproved to be too optimistic. For this reason it has been necessary to re-think\nthe architecture with a modular system built around the messaging concept,\nwhere each modular component speaks to the other interested parties through a\nsystem of broker-managed queues. The first implemented protocol, the Simple\nCone Search, shows the messaging task architecture connecting the parametric\nHTTP interface to the database backend access module, the logging module, and\nallows multiple cone search resources to be managed together through a\nconfiguration manager module. Even if relatively young, it already proved the\nflexibility required by the overall system when the database backend changed\nfrom MySQL to PostgreSQL+PgSphere. Another implementation test has been made to\nleverage task distribution over multiple servers to serve simultaneously: FITS\ncubes direct linking, cubes cutout and cubes positional merging. Currently the\nimplementation of the SIA-2.0 standard protocol is ongoing while for TAP we\nwill be adapting the TAPlib library. Alongside these tools a first\nadministration tool (TASMAN) has been developed to ease the build up and\nmaintenance of TAP_SCHEMA-ta including also ObsCore maintenance capability.\nFuture work will be devoted at widening the range of VO protocols covered by\nthe set of available modules, improve the configuration management and develop\nspecific purpose modules common to all the service components.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 13:27:31 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Molinaro", "Marco", ""], ["Calabria", "Nicola F.", ""], ["Butora", "Robert", ""], ["Zorba", "Sonia", ""], ["Smareglia", "Riccardo", ""]]}, {"id": "1805.08529", "submitter": "David Pride Mr", "authors": "David Pride, Petr Knoth", "title": "Peer review and citation data in predicting university rankings, a\n  large-scale analysis", "comments": "12 pages, 7 tables, 2 figures. Submitted to TPDL2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Performance-based Research Funding Systems (PRFS) draw on peer review\nand bibliometric indicators, two different methodologies which are sometimes\ncombined. A common argument against the use of indicators in such research\nevaluation exercises is their low correlation at the article level with peer\nreview judgments. In this study, we analyse 191,000 papers from 154 higher\neducation institutes which were peer reviewed in a national research evaluation\nexercise. We combine these data with 6.95 million citations to the original\npapers. We show that when citation-based indicators are applied at the\ninstitutional or departmental level, rather than at the level of individual\npapers, surprisingly large correlations with peer review judgments can be\nobserved, up to r <= 0.802, n = 37, p < 0.001 for some disciplines. In our\nevaluation of ranking prediction performance based on citation data, we show we\ncan reduce the mean rank prediction error by 25% compared to previous work.\nThis suggests that citation-based indicators are sufficiently aligned with peer\nreview results at the institutional level to be used to lessen the overall\nburden of peer review on national evaluation exercises leading to considerable\ncost savings.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 12:01:59 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Pride", "David", ""], ["Knoth", "Petr", ""]]}, {"id": "1805.09687", "submitter": "Michele Dolfi", "authors": "Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas", "title": "Corpus Conversion Service: A machine learning platform to ingest\n  documents at scale [Poster abstract]", "comments": "Accepted in SysML 2018 (www.sysml.cc)", "journal-ref": null, "doi": "10.13140/RG.2.2.10858.82888", "report-no": null, "categories": "cs.DL cs.CL cs.CV cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make their\ncontent discoverable. Unfortunately, both the format of these documents (e.g.\nthe PDF format or bitmap images) as well as the presentation of the data (e.g.\ncomplex tables) make the extraction of qualitative and quantitive data\nextremely challenging. We present a platform to ingest documents at scale which\nis powered by Machine Learning techniques and allows the user to train custom\nmodels on document collections. We show precision/recall results greater than\n97% with regard to conversion to structured formats, as well as scaling\nevidence for each of the microservices constituting the platform.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 07:05:52 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Staar", "Peter W J", ""], ["Dolfi", "Michele", ""], ["Auer", "Christoph", ""], ["Bekas", "Costas", ""]]}, {"id": "1805.10260", "submitter": "Alexander Nwala", "authors": "Alexander C. Nwala and Michele C. Weigle and Michael L. Nelson", "title": "Scraping SERPs for Archival Seeds: It Matters When You Start", "comments": "This is an extended version of the ACM/IEEE Joint Conference on\n  Digital Libraries (JCDL 2018) full paper:\n  https://doi.org/10.1145/3197026.3197056. Some of the figure numbers have\n  changed", "journal-ref": null, "doi": "10.1145/3197026.3197056", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Event-based collections are often started with a web search, but the search\nresults you find on Day 1 may not be the same as those you find on Day 7. In\nthis paper, we consider collections that originate from extracting URIs\n(Uniform Resource Identifiers) from Search Engine Result Pages (SERPs).\nSpecifically, we seek to provide insight about the retrievability of URIs of\nnews stories found on Google, and to answer two main questions: first, can one\n\"refind\" the same URI of a news story (for the same query) from Google after a\ngiven time? Second, what is the probability of finding a story on Google over a\ngiven period of time? To answer these questions, we issued seven queries to\nGoogle every day for over seven months (2017-05-25 to 2018-01-12) and collected\nlinks from the first five SERPs to generate seven collections for each query.\nThe queries represent public interest stories: \"healthcare bill,\" \"manchester\nbombing,\" \"london terrorism,\" \"trump russia,\" \"travel ban,\" \"hurricane harvey,\"\nand \"hurricane irma.\" We tracked each URI in all collections over time to\nestimate the discoverability of URIs from the first five SERPs. Our results\nshowed that the daily average rate at which stories were replaced on the\ndefault Google SERP ranged from 0.21 -0.54, and a weekly rate of 0.39 - 0.79,\nsuggesting the fast replacement of older stories by newer stories. The\nprobability of finding the same URI of a news story after one day from the\ninitial appearance on the SERP ranged from 0.34 - 0.44. After a week, the\nprobability of finding the same news stories diminishes rapidly to 0.01 - 0.11.\nOur findings suggest that due to the difficulty in retrieving the URIs of news\nstories from Google, collection building that originates from search engines\nshould begin as soon as possible in order to capture the first stages of\nevents, and should persist in order to capture the evolution of the events...\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 17:30:35 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Nwala", "Alexander C.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1805.10359", "submitter": "Orlando Fonseca Guilarte", "authors": "Orlando Fonseca Guilarte, Simone Diniz Junqueira Barbosa, Sinesio\n  Pesco", "title": "Simplified Graph-based Visualization for Scientific Publication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding citations to scientific publications is a task of vital\nimportance in the academic world. This task can be supported by appropriate\ndata structures and visualization mechanisms. One challenge is the amount of\nexisting relationships and the difficulty of determining which of the\nreferences of a document are considered the most potentially relevant to it. In\nthis paper, we propose a simplified visualization of the relationships between\nscientific publications, in the form of a directed acyclic graph. From a given\ndocument, it is possible to visualize a path of references in which each step\ncorresponds to the main citation of the previous one. A methodology is proposed\nin order to build this graph based in the opinion of the authors of scientific\narticles and an editorial board.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 20:44:10 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Guilarte", "Orlando Fonseca", ""], ["Barbosa", "Simone Diniz Junqueira", ""], ["Pesco", "Sinesio", ""]]}, {"id": "1805.10521", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Antonio Osorio", "title": "The value and credits of n-authors publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaboration among researchers is becoming increasingly common, which raises\na large number of scientometrics questions for which there is not a clear and\ngenerally accepted answer. For instance, what value should be given to a\ntwo-author or three-author publication with respect to a single-author\npublication? This paper uses axiomatic analysis and proposes a practical method\nto compute the expected value of an n-authors publication that takes into\nconsideration the added value induced by collaboration in contexts in which\nthere is no prior or ex-ante information about the publication's potential\nmerits or scientific impact. The only information required is the number of\nauthors. We compared the obtained theoretical values with the empirical values\nbased on a large dataset from the Web of Science database. We found that the\ntheoretical values are very close to the empirical values for some disciplines,\nbut not for all. This observation provides support in favor of the method\nproposed in this paper. We expect that our findings can help researchers and\ndecision-makers to choose more effective and fair counting methods that take\ninto account the benefits of collaboration.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 18:57:01 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 06:39:17 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Bornmann", "Lutz", ""], ["Osorio", "Antonio", ""]]}, {"id": "1805.11050", "submitter": "Robert B. Allen", "authors": "Robert B. Allen, Teryn K. Jones", "title": "XFO: Toward Programming Rich Semantic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have proposed that ontologies and programming languages should be more\nclosely aligned. Specifically, we have argued that the Basic Formal Ontology\n(BFO2) has many features that are consistent with object-oriented analysis,\ndesign, and modeling. Here, we describe the eXtended Formal Ontology (XFO), a\nprogramming environment we developed to support semantic modeling. We then use\nXFO to implement a Traffic Light Microworld and discuss more complex\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 14:19:07 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Allen", "Robert B.", ""], ["Jones", "Teryn K.", ""]]}, {"id": "1805.11821", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff and Ivan Cucco", "title": "Regions, Innovation Systems, and the North-South Divide in Italy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using firm-level data collected by Statistics Italy for 2008, 2011, and 2015,\nwe examine the Triple-Helix synergy among geographical and size distributions\nof firms, and the NACE codes attributed to these firms, at the different levels\nof regional and national government. At which levels is innovation-systemness\nindicated? The contributions of regions to the Italian innovation system have\nincreased, but synergy generation between regions and supra-regionally has\nremained at almost 45%. As against the statistical classification of Italy into\ntwenty regions or into Northern, Central, and Southern Italy, the greatest\nsynergy is retrieved by considering the country in terms of Northern and\nSouthern Italy as two sub-systems, with Tuscany included as part of Northern\nItaly. We suggest that separate innovation strategies should be developed for\nthese two parts of the country. The current focus on regions for innovation\npolicies may to some extent be an artifact of the statistics and EU policies.\nIn terms of sectors, both medium- and high-tech manufacturing (MHTM) and\nknowledge-intensive services (KIS) are proportionally integrated in the various\nregions.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 06:33:30 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Cucco", "Ivan", ""]]}, {"id": "1805.12124", "submitter": "George Mathew", "authors": "George Mathew, Tim Menzies", "title": "Better Metrics for Ranking SE Researchers", "comments": "5 pages, 2 figures, 4 tables. Submitted to IEEE TSE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how SE researchers are ranked using a variety of metrics\nand data from 35,406 authors of 35,391 papers from 34 top SE venues in the\nperiod 1992-2016. Based on that analysis, we: deprecate the widely used\n\"h-index\", favoring instead an alternate Weighted Page Rank(PR_W) metric that\nis somewhat analogous to the PageRank(PR) metric developed at Google. Unlike\nthe h-index, PR_W rewards not just citation counts but also how often authors\ncollaborate. Using PR_W, we offer a ranking of the top 20 SE authors in the\nlast decade.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 23:00:37 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Mathew", "George", ""], ["Menzies", "Tim", ""]]}, {"id": "1805.12216", "submitter": "Zhihong (Iris) Shen", "authors": "Zhihong Shen, Hao Ma, Kuansan Wang", "title": "A Web-scale system for scientific knowledge exploration", "comments": "6 pages, accepted for ACL 2018 demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable efficient exploration of Web-scale scientific knowledge, it is\nnecessary to organize scientific publications into a hierarchical concept\nstructure. In this work, we present a large-scale system to (1) identify\nhundreds of thousands of scientific concepts, (2) tag these identified concepts\nto hundreds of millions of scientific publications by leveraging both text and\ngraph structure, and (3) build a six-level concept hierarchy with a\nsubsumption-based model. The system builds the most comprehensive cross-domain\nscientific concept ontology published to date, with more than 200 thousand\nconcepts and over one million relationships.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 20:28:36 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Shen", "Zhihong", ""], ["Ma", "Hao", ""], ["Wang", "Kuansan", ""]]}, {"id": "1805.12376", "submitter": "Marcos Baez", "authors": "Jorge Ramirez, Evgeny Krivosheev, Marcos Baez, Fabio Casati, Boualem\n  Benatallah", "title": "CrowdRev: A platform for Crowd-based Screening of Literature Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper and demo we present a crowd and crowd+AI based system, called\nCrowdRev, supporting the screening phase of literature reviews and achieving\nthe same quality as author classification at a fraction of the cost, and\nnear-instantly. CrowdRev makes it easy for authors to leverage the crowd, and\nensures that no money is wasted even in the face of difficult papers or\ncriteria: if the system detects that the task is too hard for the crowd, it\njust gives up trying (for that paper, or for that criteria, or altogether),\nwithout wasting money and never compromising on quality.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 08:32:37 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Ramirez", "Jorge", ""], ["Krivosheev", "Evgeny", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""], ["Benatallah", "Boualem", ""]]}]