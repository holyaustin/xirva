[{"id": "1905.00011", "submitter": "Philipp Mayr", "authors": "Neda Abediyarandi and Philipp Mayr", "title": "The State of Open Access in Germany: An Analysis of the Publication\n  Output of German Universities", "comments": "2 pages, 2 figures, revised poster at 17th International Conference\n  on Scientometrics & Informetrics (ISSI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with the Berlin declaration in 2003, Open Access (OA) publishing has\nestablished a new era of scholarly communication due to the unrestricted\nelectronic access to peer reviewed publications. OA offers a number of benefits\nlike e.g. increased citation counts (Gargouri et al., 2010) and enhanced\nvisibility and accessibility of research output (Tennant et al., 2016). The OA\nmovement with its powerful mandating and policymaking has been very successful\nin recent years. Relatively little is known about the real effects of these\nactivities in terms of OA publication output of institutions on a larger scale\n(Piwowar et al., 2018). The aim of this article is to investigate to what\nextent the OA fraction of the publication output of German universities has\nincreased in the last years. To answer this question, we analysed and compared\ntotal number of publications which have been published by researchers of the\nlargest German universities. We compared the numbers of OA versus closed\npublications for 66 large German universities in the time span of 2000-2017.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 17:17:56 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 10:22:37 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 15:14:20 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Abediyarandi", "Neda", ""], ["Mayr", "Philipp", ""]]}, {"id": "1905.00092", "submitter": "Max Schr\\\"oder", "authors": "Max Schr\\\"oder, Frank Kr\\\"uger, Sascha Spors", "title": "Reproducible Research is more than Publishing Research Artefacts: A\n  Systematic Analysis of Jupyter Notebooks from Research Articles", "comments": "4 pages, 5 figures, 1 table, In Proceedings of the DAGA 2019 (the\n  annual conference of the German Acoustical Society, DEGA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of Open Science, researchers have started to publish their\nresearch artefacts (i. e., data, software, and other products of the\ninvestigations) in order to allow others to reproduce their investigations.\nWhile this publication is beneficial for science in general, it often lacks a\ncomprehensive documentation and completeness with respect to the artefacts.\nThis, in turn, prevents the successful reproduction of the analyses. Typical\nexamples are missing scripts, incomplete datasets or specification of used\nsoftware. Moreover, issues about licences often create legal concerns. This is\ntrue for the use of commercial software but also for the publication of\nresearch artefacts without proper sharing licence. As a result, the sole\npublication of research artefacts does not automatically result in reproducible\nresearch.\n  To empirically confirm this, we have been systematically analysing research\npublications that also published their investigations as Jupyter notebooks. In\nthis paper, we present preliminary results of this analysis for five\npublications. The results show, that the quality of the published research\nartefacts must be improved in order to assure reproducibility.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 08:54:48 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Schr\u00f6der", "Max", ""], ["Kr\u00fcger", "Frank", ""], ["Spors", "Sascha", ""]]}, {"id": "1905.00422", "submitter": "Byungsoo Jeon", "authors": "Byungsoo Jeon, Eyal Shafran, Luke Breitfeller, Jason Levin, Carolyn P.\n  Rose", "title": "Time-series Insights into the Process of Passing or Failing Online\n  University Courses using Neural-Induced Interpretable Student States", "comments": "11 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a key challenge in Educational Data Mining, namely to\nmodel student behavioral trajectories in order to provide a means for\nidentifying students most at-risk, with the goal of providing supportive\ninterventions. While many forms of data including clickstream data or data from\nsensors have been used extensively in time series models for such purposes, in\nthis paper we explore the use of textual data, which is sometimes available in\nthe records of students at large, online universities. We propose a time series\nmodel that constructs an evolving student state representation using both\nclickstream data and a signal extracted from the textual notes recorded by\nhuman mentors assigned to each student. We explore how the addition of this\ntextual data improves both the predictive power of student states for the\npurpose of identifying students at risk for course failure as well as for\nproviding interpretable insights about student course engagement processes.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 16:04:12 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Jeon", "Byungsoo", ""], ["Shafran", "Eyal", ""], ["Breitfeller", "Luke", ""], ["Levin", "Jason", ""], ["Rose", "Carolyn P.", ""]]}, {"id": "1905.00522", "submitter": "Paul Sheridan", "authors": "Paul Sheridan, Mikael Onsj\\\"o, Janna Hastings", "title": "The Literary Theme Ontology for Media Annotation and Information\n  Retrieval", "comments": "12 pages, 2 figures, 1 tables, minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literary theme identification and interpretation is a focal point of literary\nstudies scholarship. Classical forms of literary scholarship, such as close\nreading, have flourished with scarcely any need for commonly defined literary\nthemes. However, the rise in popularity of collaborative and algorithmic\nanalyses of literary themes in works of fiction, together with a requirement\nfor computational searching and indexing facilities for large corpora, creates\nthe need for a collection of shared literary themes to ensure common\nterminology and definitions. To address this need, we here introduce a first\ndraft of the Literary Theme Ontology. Inspired by a traditional framing from\nliterary theory, the ontology comprises literary themes drawn from the authors\nown analyses, reference books, and online sources. The ontology is available at\nhttps://github.com/theme-ontology/lto under a Creative Commons Attribution 4.0\nInternational license (CC BY 4.0).\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 22:33:16 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 03:25:25 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Sheridan", "Paul", ""], ["Onsj\u00f6", "Mikael", ""], ["Hastings", "Janna", ""]]}, {"id": "1905.00880", "submitter": "Daniel Hook", "authors": "Daniel W. Hook, Mark Hahnel and Christian Herzog", "title": "The Price of Gold: Curiosity?", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gold open access as characterised by the payment of an article processing\ncharge (APC) has become one of the dominant models in open access publication.\nThis paper examines an extreme hypothetical case in which the APC model is the\nonly model and the systematic issues that could develop in such a scenario.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 17:50:02 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Hook", "Daniel W.", ""], ["Hahnel", "Mark", ""], ["Herzog", "Christian", ""]]}, {"id": "1905.01725", "submitter": "Loet Leydesdorff", "authors": "Gangan Prathap and Loet Leydesdorff", "title": "Within-Journal Self-citations and the Pinski-Narin Influence Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Journal Impact Factor (JIF) is linearly sensitive to self-citations\nbecause each self-citation adds to the numerator, whereas the denominator is\nnot affected. Pinski & Narin (1976) derived the Influence Weight (IW) as an\nalternative to Garfield's JIF. Whereas the JIF is based on raw citation counts\nnormalized by the number of publications, IWs are based on the eigenvectors in\nthe matrix of aggregated journal-journal citations without a reference to size:\nthe cited and citing sides are combined by a matrix approach. IWs emerge as a\nvector after recursive iteration of the normalized matrix. Before recursion, IW\nis a (vector-based) non-network indicator of impact, but after recursion (i.e.\nrepeated improvement by iteration), IWs can be considered a network measure of\nprestige among the journals in the (sub)graph as a representation of a field of\nscience. As a consequence (not intended by Pinski & Narin in 1976), the\nself-citations are integrated at the field level and no longer disturb the\nanalysis as outliers. In our opinion, this is a very desirable property of a\nmeasure of quality or impact. As illustrations, we use data of journal citation\nmatrices already studied in the literature, and also the complete set of data\nin the Journal Citation Reports 2017 (n = 11,579 journals). The values of IWs\nare sometimes counter-intuitive and difficult to interpret. Furthermore,\niterations do not always converge. Routines for the computation of IWs are made\navailable at http://www.leydesdorff.net/iw.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 18:03:37 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 05:19:51 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Prathap", "Gangan", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1905.01943", "submitter": "Philip Hofmann", "authors": "Anna Tietze, Serge Galam, Philip Hofmann", "title": "Crediting multi-authored papers to single authors", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.physa.2020.124652", "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fair assignment of credit for multi-authored publications is a\nlong-standing issue in scientometrics. In the calculation of the $h$-index, for\ninstance, all co-authors receive equal credit for a given publication,\nindependent of a given author's contribution to the work or of the total number\nof co-authors. Several attempts have been made to distribute the credit in a\nmore appropriate manner. In a recent paper, Hirsch has suggested a new way of\ncredit assignment that is fundamentally different from the previous ones: All\ncredit for a multi-author paper goes to a single author, the called\n``$\\alpha$-author'', defined as the person with the highest current $h$-index\nnot the highest $h$-index at the time of the paper's publication) (J. E.\nHirsch, Scientometrics 118, 673 (2019)). The collection of papers this author\nhas received credit for as $\\alpha$-author is then used to calculate a new\nindex, $h_{\\alpha}$, following the same recipe as for the usual $h$ index. The\nobjective of this new assignment is not a fairer distribution of credit, but\nrather the determination of an altogether different property, the degree of a\nperson's scientific leadership. We show that given the complex time dependence\nof $h$ for individual scientists, the approach of using the current $h$ value\ninstead of the historic one is problematic, and we argue that it would be\nfeasible to determine the $\\alpha$-author at the time of the paper's\npublication instead. On the other hand, there are other practical\nconsiderations that make the calculation of the proposed $h_{\\alpha}$ very\ndifficult. As an alternative, we explore other ways of crediting papers to a\nsingle author in order to test early career achievement or scientific\nleadership.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 12:01:18 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tietze", "Anna", ""], ["Galam", "Serge", ""], ["Hofmann", "Philip", ""]]}, {"id": "1905.02464", "submitter": "Ciriaco Andrea D'Angelo", "authors": "Giovanni Abramo, Ciriaco Andrea D'Angelo, Flavia Di Costa", "title": "Authorship analysis of specialized vs diversified research output", "comments": null, "journal-ref": "Journal of Informetrics, 13(2), 564-573 (2019)", "doi": "10.1016/j.joi.2019.03.004", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work investigates the relations between amplitude and type of\ncollaboration (intramural, extramural domestic or international) and output of\nspecialized versus diversified research. By specialized or diversified\nresearch, we mean within or beyond the author's dominant research topic. The\nfield of observation is the scientific production over five years from about\n23,500 academics. The analyses are conducted at the aggregate and disciplinary\nlevel. The results lead to the conclusion that in general, the output of\ndiversified research is no more frequently the fruit of collaboration than is\nspecialized research. At the level of the particular collaboration types,\ninternational collaborations weakly underlie the specialized kind of research\noutput; on the contrary, extramural domestic and intramural collaborations are\nweakly associated with diversified research. While the weakness of association\nremains, exceptions are observed at the level of the individual disciplines.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 10:54:43 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Abramo", "Giovanni", ""], ["D'Angelo", "Ciriaco Andrea", ""], ["Di Costa", "Flavia", ""]]}, {"id": "1905.02875", "submitter": "Ronald Rousseau", "authors": "Xiaojun Hu, Ronald Rousseau, Sandra Rousseau", "title": "Does Environmental Economics lead to patentable research?", "comments": "10 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this feasibility study, the impact of academic research from social\nsciences and humanities on technological innovation is explored through a study\nof citations patterns of journal articles in patents. Specifically we focus on\ncitations of journals from the field of environmental economics in patents\nincluded in an American patent database (USPTO). Three decades of patents have\nled to a small set of journal articles (85) that are being cited from the field\nof environmental economics. While this route of measuring how academic research\nis validated through its role in stimulating technological progress may be\nrather limited (based on this first exploration), it may still point to a\nvaluable and interesting topic for further research.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 07:17:24 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Hu", "Xiaojun", ""], ["Rousseau", "Ronald", ""], ["Rousseau", "Sandra", ""]]}, {"id": "1905.02896", "submitter": "Yu Luo", "authors": "Yu Luo, Beth Plale", "title": "Pilot evaluation of Collection API with PID Kernel Information", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As digital data become increasingly available for research, there is a\ngrowing awareness of the value of domain agnostic Persistent Identifiers (PIDs)\nfor data. A PID is a globally unique reference to a digital object, which in\nour case is data. In an ecosystem of connected digital objects, a PID will\nreference a digital object, and the digital object will be a simple entity, a\ncollection of homogeneous objects, or a set of heterogeneous objects.\n  In this paper, we study two recent recommendations from the Research Data\nAlliance (RDA) that both address pieces of an ecosystem of connected digital\nobjects. The recommendations address Persistent ID records and representations\nof collections of data. We evaluate different approaches in where to locate key\ninformation about a data collection between these two component solutions.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 04:11:41 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 20:19:13 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Luo", "Yu", ""], ["Plale", "Beth", ""]]}, {"id": "1905.03298", "submitter": "Paulo Eduardo Pinto Burke", "authors": "Paulo E. P. Burke and Luciano da F. Costa", "title": "Interdisciplinary Relationships Between Biological and Physical Sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several interdisciplinary areas have appeared at the interface between\nbiological and physical sciences. In this work, we suggest a complex\nnetwork-based methodology for analyzing the interrelationships between some of\nthese interdisciplinary areas, including Bioinformatics, Computational Biology,\nBiochemistry, among others. This approach has been applied over respective data\nderived from Wikipedia. Related reviews from the scientific literature are also\nconsidered as a reference, yielding a respective bipartite hypergraph which can\nbe used to gain insights about the interrelationships underlying the considered\ninterdisciplinary areas. Several interesting results are obtained, including\ngreater interconnection between the considered interdisciplinary areas with\nbiological than with physical sciences. A good agreement was also found between\nthe network obtained from Wikipedia and the interrelationships revealed by the\nliterature reviews. At the same time, the former network was found to exhibit\nmore intricate relationships than in the hypergraph derived from the literature\nreview.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 19:02:55 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Burke", "Paulo E. P.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1905.03322", "submitter": "Moritz Schubotz", "authors": "Moritz Schubotz, Olaf Teschke, Vincent Stange, Norman Meuschke and\n  Bela Gipp", "title": "Forms of Plagiarism in Digital Mathematical Libraries", "comments": null, "journal-ref": "Intelligent Computer Mathematics - 12th International Conference,\n  {CICM} 2019, Prague, Czech Republic, July 8-12, 2019, Proceedings", "doi": "10.1007/978-3-030-23250-4_18", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We report on an exploratory analysis of the forms of plagiarism observable in\nmathematical publications, which we identified by investigating editorial notes\nfrom zbMATH. While most cases we encountered were simple copies of earlier\nwork, we also identified several forms of disguised plagiarism. We investigated\n11 cases in detail and evaluate how current plagiarism detection systems\nperform in identifying these cases. Moreover, we describe the steps required to\ndiscover these and potentially undiscovered cases in the future.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 20:32:58 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 07:28:41 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Schubotz", "Moritz", ""], ["Teschke", "Olaf", ""], ["Stange", "Vincent", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "1905.03461", "submitter": "Qiang Wu", "authors": "Qiang Wu, Zhaoyang Yan", "title": "Solo citations, duet citations, and prelude citations: New measures of\n  the disruption of academic papers", "comments": "8 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to measure the disruption of academic papers. According to\nthe characteristics of three different kinds of citations, this paper borrows\nmusical vocabulary and names them solo citations (SC), duet citations (DC), and\nprelude citations (PC) respectively. Studying how to measure the disruption of\na published work effectively, this study analyzes nine indicators and suggests\na general evaluation formula. Seven of the nine indicators are innovations\nintroduced by this paper: SC, SC-DC, SC-PC, SC-DC-PC, (SC-DC)/(SC+DC),\n(SC-PC)/(SC+DC), and (SC-DC-PC)/(SC+DC), as is the general formula. These\nindices are discussed considering two cases: One case concerns the Citation\nIndexes for Science and the other concerns Co-citations. The results show that,\ncompared with other indicators, four indicators (SC, SC-DC, SC/(SC+DC), and\n(SC-DC)/(SC+DC)) are logically and empirically reasonable. Future research may\nconsider combining these indices, for example, using SC multiplied by\nSC/(SC+DC) or SC-DC multiplied by (SC-DC)/(SC+DC), to get final evaluation\nresults that contain desirable characteristics of two types of indicators.\nConfirming which of the evaluation results from these indicators can best\nreflect the innovation of research papers requires much empirical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 06:52:45 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 07:13:19 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Wu", "Qiang", ""], ["Yan", "Zhaoyang", ""]]}, {"id": "1905.03485", "submitter": "Matthias Held", "authors": "Matthias Held, Theresa Velden", "title": "How to interpret algorithmically constructed topical structures of\n  research specialties? A case study comparing an internal and an external\n  mapping of the topical structure of invasion biology", "comments": "conference Paper, 10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our paper we seek to address a shortcoming in the scientometric\nliterature, namely that, given the proliferation of algorithmic approaches to\ntopic detection from bibliometric data, there is a relative lack of studies\nthat validate and create a deeper understanding of the topical structures these\nalgorithmic approaches generate. To take a closer look at this issue, we\ninvestigate the results of the new Leiden algorithm when applied to the direct\ncitation network of a field-level data set. We compare this internal\nperspective which is constructed from the citation links within a data set of\n30,000 publications in invasion biology, with an external perspective onto the\ntopic structures in this research specialty, which is based on a global science\nmap in form of the CWTS microfield classification underlying the Leiden\nRanking. We present an initial comparative analysis of the results and lay out\nour next steps that will involve engaging with domain experts to examine how\nthe algorithmically identified topics relate to understandings of topics and\ntopical perspectives that operate within this research specialty.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 08:29:15 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Held", "Matthias", ""], ["Velden", "Theresa", ""]]}, {"id": "1905.03836", "submitter": "Mohamed Aturban", "authors": "Mohamed Aturban, Michael L. Nelson, Michele C. Weigle, Martin Klein\n  and Herbert Van de Sompel", "title": "Collecting 16K archived web pages from 17 public web archives", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We document the creation of a data set of 16,627 archived web pages, or\nmementos, of 3,698 unique live web URIs (Uniform Resource Identifiers) from 17\npublic web archives. We used four different methods to collect the dataset.\nFirst, we used the Los Alamos National Laboratory (LANL) Memento Aggregator to\ncollect mementos of an initial set of URIs obtained from four sources: (a) the\nMoz Top 500, (b) the dataset used in our previous study, (c) the HTTP Archive,\nand (d) the Web Archives for Historical Research group. Second, we extracted\nURIs from the HTML of already collected mementos. These URIs were then used to\nlook up mementos in LANL's aggregator. Third, we downloaded web archives'\npublished lists of URIs of both original pages and their associated mementos.\nFourth, we collected more mementos from archives that support the Memento\nprotocol by requesting TimeMaps directly from archives, not through the Memento\naggregator. Finally, we downsampled the collected mementos to 16,627 due to our\nconstraints of a maximum of 1,600 mementos per archive and being able to\ndownload all mementos from each archive in less than 40 hours.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 20:00:23 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Aturban", "Mohamed", ""], ["Nelson", "Michael L.", ""], ["Weigle", "Michele C.", ""], ["Klein", "Martin", ""], ["Van de Sompel", "Herbert", ""]]}, {"id": "1905.05377", "submitter": "Anh Duc Le Dr.", "authors": "Anh Duc Le, Tarin Clanuwat, Asanobu Kitamoto", "title": "A human-inspired recognition system for premodern Japanese historical\n  documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of historical documents is a challenging problem due to the\nnoised, damaged characters and background. However, in Japanese historical\ndocuments, not only contains the mentioned problems, pre-modern Japanese\ncharacters were written in cursive and are connected. Therefore, character\nsegmentation based methods do not work well. This leads to the idea of creating\na new recognition system. In this paper, we propose a human-inspired document\nreading system to recognize multiple lines of premodern Japanese historical\ndocuments. During the reading, people employ eyes movement to determine the\nstart of a text line. Then, they move the eyes from the current character/word\nto the next character/word. They can also determine the end of a line or skip a\nfigure to move to the next line. The eyes movement integrates with visual\nprocessing to operate the reading process in the brain. We employ\nattention-based encoder-decoder to implement this recognition system. First,\nthe recognition system detects where to start a text line. Second, the system\nscans and recognize character by character until the text line is completed.\nThen, the system continues to detect the start of the next text line. This\nprocess is repeated until reading the whole document. We tested our\nhuman-inspired recognition system on the pre-modern Japanese historical\ndocument provide by the PRMU Kuzushiji competition. The results of the\nexperiments demonstrate the superiority and effectiveness of our proposed\nsystem by achieving Sequence Error Rate of 9.87% and 53.81% on level 2 and\nlevel 3 of the dataset, respectively. These results outperform to any other\nsystems participated in the PRMU Kuzushiji competition.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:26:25 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Le", "Anh Duc", ""], ["Clanuwat", "Tarin", ""], ["Kitamoto", "Asanobu", ""]]}, {"id": "1905.05615", "submitter": "Na Pang", "authors": "Na Pang, Li Qian, Weimin Lyu, Jin-Dong Yang", "title": "Transfer Learning for Scientific Data Chain Extraction in Small Chemical\n  Corpus with BERT-CRF Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational chemistry develops fast in recent years due to the rapid growth\nand breakthroughs in AI. Thanks for the progress in natural language\nprocessing, researchers can extract more fine-grained knowledge in publications\nto stimulate the development in computational chemistry. While the works and\ncorpora in chemical entity extraction have been restricted in the biomedicine\nor life science field instead of the chemistry field, we build a new corpus in\nchemical bond field annotated for 7 types of entities: compound, solvent,\nmethod, bond, reaction, pKa and pKa value. This paper presents a novel BERT-CRF\nmodel to build scientific chemical data chains by extracting 7 chemical\nentities and relations from publications. And we propose a joint model to\nextract the entities and relations simultaneously. Experimental results on our\nChemical Special Corpus demonstrate that we achieve state-of-art and\ncompetitive NER performance.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 03:18:38 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Pang", "Na", ""], ["Qian", "Li", ""], ["Lyu", "Weimin", ""], ["Yang", "Jin-Dong", ""]]}, {"id": "1905.05635", "submitter": "Jean-Christophe Mourrat", "authors": "Jean-Christophe Mourrat", "title": "On the share of mathematics published by Elsevier and Springer", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For-profit editors such as Elsevier and Springer have been subject to\nsustained criticism from academics and university libraries, including calls to\nboycott, and discontinued subscriptions. Mathematicians have played a\nparticularly active role in this critique, and have endeavored to imagine new\npublication practices and create new journals. This motivates the monitoring of\nthe share of articles published by different editors. I used data from\nMathSciNet over the period 2000-2017, and focused on the 100 journals with\nhighest citations per article. Within this category, the share of articles\npublished by Elsevier and Springer has steadily increased over this period,\nfrom about a third to almost half of the total.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 15:24:56 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Mourrat", "Jean-Christophe", ""]]}, {"id": "1905.06365", "submitter": "Jiawei Zhang", "authors": "Bowen Dong and Jiawei Zhang and Chenwei Zhang and Yang Yang and Philip\n  S. Yu", "title": "Missing Movie Synergistic Completion across Multiple Isomeric Online\n  Movie Knowledge Libraries", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online knowledge libraries refer to the online data warehouses that\nsystematically organize and categorize the knowledge-based information about\ndifferent kinds of concepts and entities. In the era of big data, the setup of\nonline knowledge libraries is an extremely challenging and laborious task, in\nterms of efforts, time and expense required in the completion of knowledge\nentities. Especially nowadays, a large number of new knowledge entities, like\nmovies, are keeping on being produced and coming out at a continuously\naccelerating speed, which renders the knowledge library setup and completion\nproblem more difficult to resolve manually. In this paper, we will take the\nonline movie knowledge libraries as an example, and study the \"Multiple aligned\nISomeric Online Knowledge LIbraries Completion problem\" (Miso-Klic) problem\nacross multiple online knowledge libraries. Miso-Klic aims at identifying the\nmissing entities for multiple knowledge libraries synergistically and ranking\nthem for editing based on certain ranking criteria. To solve the problem, a\nthorough investigation of two isomeric online knowledge libraries, Douban and\nIMDB, have been carried out in this paper. Based on analyses results, a novel\ndeep online knowledge library completion framework \"Integrated Deep alignEd\nAuto-encoder\" (IDEA) is introduced to solve the problem. By projecting the\nentities from multiple isomeric knowledge libraries to a shared feature space,\nIDEA solves the Miso-Klic problem via three steps: (1) entity feature space\nunification via embedding, (2) knowledge library fusion based missing entity\nidentification, and (3) missing entity ranking. Extensive experiments done on\nthe real-world online knowledge library dataset have demonstrated the\neffectiveness of IDEA in addressing the problem.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 18:15:44 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 17:21:16 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Dong", "Bowen", ""], ["Zhang", "Jiawei", ""], ["Zhang", "Chenwei", ""], ["Yang", "Yang", ""], ["Yu", "Philip S.", ""]]}, {"id": "1905.06480", "submitter": "Rafael S. Gon\\c{c}alves", "authors": "Rafael S. Gon\\c{c}alves, Martin J. O'Connor, Marcos Mart\\'inez-Romero,\n  Attila L. Egyedi, Debra Willrett, John Graybeal and Mark A. Musen", "title": "The CEDAR Workbench: An Ontology-Assisted Environment for Authoring\n  Metadata that Describe Scientific Experiments", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-68204-4_10", "report-no": null, "categories": "cs.DB cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Center for Expanded Data Annotation and Retrieval (CEDAR) aims to\nrevolutionize the way that metadata describing scientific experiments are\nauthored. The software we have developed--the CEDAR Workbench--is a suite of\nWeb-based tools and REST APIs that allows users to construct metadata\ntemplates, to fill in templates to generate high-quality metadata, and to share\nand manage these resources. The CEDAR Workbench provides a versatile,\nREST-based environment for authoring metadata that are enriched with terms from\nontologies. The metadata are available as JSON, JSON-LD, or RDF for easy\nintegration in scientific applications and reusability on the Web. Users can\nleverage our APIs for validating and submitting metadata to external\nrepositories. The CEDAR Workbench is freely available and open-source.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 00:19:49 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Gon\u00e7alves", "Rafael S.", ""], ["O'Connor", "Martin J.", ""], ["Mart\u00ednez-Romero", "Marcos", ""], ["Egyedi", "Attila L.", ""], ["Willrett", "Debra", ""], ["Graybeal", "John", ""], ["Musen", "Mark A.", ""]]}, {"id": "1905.07141", "submitter": "Nicolas Robinson-Garcia", "authors": "Nicolas Robinson-Garcia, Daniel Torres-Salinas, Enrique Herrera-Viedma\n  and Domingo Docampo", "title": "Mining university rankings: Publication output and citation impact as\n  their basis", "comments": "Paper accepted for publication in Research Evaluation", "journal-ref": null, "doi": "10.1093/reseval/rvz014", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World University rankings have become well-established tools that students,\nuniversity managers and policy makers read and use. Each ranking claims to have\na unique methodology capable of measuring the 'quality' of universities. The\npurpose of this paper is to analyze to which extent these different rankings\nmeasure the same phenomenon and what it is that they are measuring. For this,\nwe selected a total of seven world-university rankings and performed a\nprincipal component analysis. After ensuring that despite their methodological\ndifferences, they all come together to a single component, we hypothesized that\nbibliometric indicators could explain what is being measured. Our analyses show\nthat ranking scores from whichever of the seven league tables under study can\nbe explained by the number of publications and citations received by the\ninstitution. We conclude by discussing policy implications and opportunities on\nhow a nuanced and responsible use of rankings can help decision making at the\ninstitutional level\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 07:29:09 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Robinson-Garcia", "Nicolas", ""], ["Torres-Salinas", "Daniel", ""], ["Herrera-Viedma", "Enrique", ""], ["Docampo", "Domingo", ""]]}, {"id": "1905.08359", "submitter": "Moritz Schubotz", "authors": "Andr\\'e Greiner-Petter, Terry Ruas, Moritz Schubotz, Akiko Aizawa,\n  William Grosky, Bela Gipp", "title": "Why Machines Cannot Learn Mathematics, Yet", "comments": "Submitted to 4th Joint Workshop on Bibliometric-enhanced Information\n  Retrieval and Natural Language Processing for Digital Libraries colocated at\n  the 42nd International ACM SIGIR Conference", "journal-ref": "2019 http://ceur-ws.org/Vol-2414/paper14.pdf", "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Machine Learning (ML) is seen as the universal solution to improve\nthe effectiveness of information retrieval (IR) methods. However, while\nmathematics is a precise and accurate science, it is usually expressed by less\naccurate and imprecise descriptions, contributing to the relative dearth of\nmachine learning applications for IR in this domain. Generally, mathematical\ndocuments communicate their knowledge with an ambiguous, context-dependent, and\nnon-formal language. Given recent advances in ML, it seems canonical to apply\nML techniques to represent and retrieve mathematics semantically. In this work,\nwe apply popular text embedding techniques to the arXiv collection of STEM\ndocuments and explore how these are unable to properly understand mathematics\nfrom that corpus. In addition, we also investigate the missing aspects that\nwould allow mathematics to be learned by computers.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 21:54:26 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Greiner-Petter", "Andr\u00e9", ""], ["Ruas", "Terry", ""], ["Schubotz", "Moritz", ""], ["Aizawa", "Akiko", ""], ["Grosky", "William", ""], ["Gipp", "Bela", ""]]}, {"id": "1905.08674", "submitter": "Daniel S. Katz", "authors": "Daniel S. Katz, Daina Bouquin, Neil P. Chue Hong, Jessica Hausman,\n  Catherine Jones, Daniel Chivvis, Tim Clark, Merc\\`e Crosas, Stephan Druskat,\n  Martin Fenner, Tom Gillespie, Alejandra Gonzalez-Beltran, Morane Gruenpeter,\n  Ted Habermann, Robert Haines, Melissa Harrison, Edwin Henneken, Lorraine\n  Hwang, Matthew B. Jones, Alastair A. Kelly, David N. Kennedy, Katrin\n  Leinweber, Fernando Rios, Carly B. Robinson, Ilian Todorov, Mingfang Wu, Qian\n  Zhang", "title": "Software Citation Implementation Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main output of the FORCE11 Software Citation working group\n(https://www.force11.org/group/software-citation-working-group) was a paper on\nsoftware citation principles (https://doi.org/10.7717/peerj-cs.86) published in\nSeptember 2016. This paper laid out a set of six high-level principles for\nsoftware citation (importance, credit and attribution, unique identification,\npersistence, accessibility, and specificity) and discussed how they could be\nused to implement software citation in the scholarly community. In a series of\ntalks and other activities, we have promoted software citation using these\nincreasingly accepted principles. At the time the initial paper was published,\nwe also provided guidance and examples on how to make software citable, though\nwe now realize there are unresolved problems with that guidance. The purpose of\nthis document is to provide an explanation of current issues impacting\nscholarly attribution of research software, organize updated implementation\nguidance, and identify where best practices and solutions are still needed.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 14:46:50 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Katz", "Daniel S.", ""], ["Bouquin", "Daina", ""], ["Hong", "Neil P. Chue", ""], ["Hausman", "Jessica", ""], ["Jones", "Catherine", ""], ["Chivvis", "Daniel", ""], ["Clark", "Tim", ""], ["Crosas", "Merc\u00e8", ""], ["Druskat", "Stephan", ""], ["Fenner", "Martin", ""], ["Gillespie", "Tom", ""], ["Gonzalez-Beltran", "Alejandra", ""], ["Gruenpeter", "Morane", ""], ["Habermann", "Ted", ""], ["Haines", "Robert", ""], ["Harrison", "Melissa", ""], ["Henneken", "Edwin", ""], ["Hwang", "Lorraine", ""], ["Jones", "Matthew B.", ""], ["Kelly", "Alastair A.", ""], ["Kennedy", "David N.", ""], ["Leinweber", "Katrin", ""], ["Rios", "Fernando", ""], ["Robinson", "Carly B.", ""], ["Todorov", "Ilian", ""], ["Wu", "Mingfang", ""], ["Zhang", "Qian", ""]]}, {"id": "1905.08880", "submitter": "Anshul Kanakia", "authors": "Anshul Kanakia (1), Zhihong Shen (1), Darrin Eide (1), Kuansan Wang\n  (1) ((1) Microsoft Research)", "title": "A Scalable Hybrid Research Paper Recommender System for Microsoft\n  Academic", "comments": "7 pages, 7 figures. Short paper at The Web Conference 2019, San\n  Francisco, USA", "journal-ref": "In The World Wide Web Conference (WWW '19). ACM, New York, NY,\n  USA, 2893-2899", "doi": "10.1145/3308558.3313700", "report-no": null, "categories": "cs.DL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and methodology for the large scale hybrid paper\nrecommender system used by Microsoft Academic. The system provides\nrecommendations for approximately 160 million English research papers and\npatents. Our approach handles incomplete citation information while also\nalleviating the cold-start problem that often affects other recommender\nsystems. We use the Microsoft Academic Graph (MAG), titles, and available\nabstracts of research papers to build a recommendation list for all documents,\nthereby combining co-citation and content based approaches. Tuning system\nparameters also allows for blending and prioritization of each approach which,\nin turn, allows us to balance paper novelty versus authority in recommendation\nresults. We evaluate the generated recommendations via a user study of 40\nparticipants, with over 2400 recommendation pairs graded and discuss the\nquality of the results using P@10 and nDCG scores. We see that there is a\nstrong correlation between participant scores and the similarity rankings\nproduced by our system but that additional focus needs to be put towards\nimproving recommender precision, particularly for content based\nrecommendations. The results of the user survey and associated analysis scripts\nare made available via GitHub and the recommendations produced by our system\nare available as part of the MAG on Azure to facilitate further research and\nlight up novel research paper recommendation applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 21:46:33 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Kanakia", "Anshul", "", "Microsoft Research"], ["Shen", "Zhihong", "", "Microsoft Research"], ["Eide", "Darrin", "", "Microsoft Research"], ["Wang", "Kuansan", "", "Microsoft Research"]]}, {"id": "1905.08988", "submitter": "Damien Vurpillot", "authors": "Damien Vurpillot (CESR), Perrine Pittet (CESR), Johann Forte (CESR),\n  Benoist Pierre (CESR)", "title": "From heterogeneous data to heterogeneous public: thoughts on transmedia\n  applications for digital heritage research and dissemination", "comments": "Computer Applications and quantitative methods in Archaeology 2019:\n  Check Object Integrity, Apr 2019, Cracovie, Poland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have seen a tenfold increase in volume and complexity of\ndigital data acquired for cultural heritage documentation. Meanwhile, open data\nand open science have become leading trends in digital humanities. The\nconvergence of those two parameters compels us to deliver, in an interoperable\nfashion, datasets that are vastly heterogeneous both in content and format and,\nmoreover, in such a way that they fit the expectation of a broad array of\nresearchers and an even broader public audience. Tackling those issues is one\nof the main goal of the \"HeritageS\" digital platform project supported by the\n\"Intelligence des Patrimoines\" research program. This platform is designed to\nallow research projects from many interdisciplinary fields to share, integrate\nand valorize cultural and natural heritage datasets related to the Loire\nValley. In this regard, one of our main project is the creation of the\n\"Renaissance Transmedia Lab\". Its core element is a website which acts as a hub\nto access various interactive experiences linked to project about the\nRenaissance period: augmented web-documentary, serious game, virtual reality,\n3D application. We expect to leverage those transmedia experiences to foster\nbetter communication between researchers and the public while keeping the\nquality of scientific discourse. By presenting the current and upcoming\nproductions, we intend to share our experience with other participants:\npreparatory work and how we cope with researchers to produce, in concertation,\ntailor-made experiences that convey the desired scientific discourse while\nremaining appealing to the general public.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 07:07:34 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Vurpillot", "Damien", "", "CESR"], ["Pittet", "Perrine", "", "CESR"], ["Forte", "Johann", "", "CESR"], ["Pierre", "Benoist", "", "CESR"]]}, {"id": "1905.09095", "submitter": "Robin Haunschild", "authors": "Robin Haunschild, Lutz Bornmann, and Jonathan Adams", "title": "R package for producing beamplots as a preferred alternative to the h\n  index when assessing single researchers (based on downloads from Web of\n  Science)", "comments": "6 pages, 1 figure", "journal-ref": "Scientometrics 120, 925-927 (2019)", "doi": "10.1007/s11192-019-03147-3", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of beamplots - which can be produced by using the R\npackage BibPlots and WoS downloads - as a preferred alternative to h index\nvalues for assessing single researchers.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 12:09:36 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Haunschild", "Robin", ""], ["Bornmann", "Lutz", ""], ["Adams", "Jonathan", ""]]}, {"id": "1905.10022", "submitter": "Taoran Ji", "authors": "Taoran Ji, Zhiqian Chen, Nathan Self, Kaiqun Fu, Chang-Tien Lu, Naren\n  Ramakrishnan", "title": "Patent Citation Dynamics Modeling via Multi-Attention Recurrent Networks", "comments": null, "journal-ref": "IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and forecasting forward citations to a patent is a central task for\nthe discovery of emerging technologies and for measuring the pulse of inventive\nprogress. Conventional methods for forecasting these forward citations cast the\nproblem as analysis of temporal point processes which rely on the conditional\nintensity of previously received citations. Recent approaches model the\nconditional intensity as a chain of recurrent neural networks to capture memory\ndependency in hopes of reducing the restrictions of the parametric form of the\nintensity function. For the problem of patent citations, we observe that\nforecasting a patent's chain of citations benefits from not only the patent's\nhistory itself but also from the historical citations of assignees and\ninventors associated with that patent. In this paper, we propose a\nsequence-to-sequence model which employs an attention-of-attention mechanism to\ncapture the dependencies of these multiple time sequences. Furthermore, the\nproposed model is able to forecast both the timestamp and the category of a\npatent's next citation. Extensive experiments on a large patent citation\ndataset collected from USPTO demonstrate that the proposed model outperforms\nstate-of-the-art models at forward citation forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 21:11:31 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Ji", "Taoran", ""], ["Chen", "Zhiqian", ""], ["Self", "Nathan", ""], ["Fu", "Kaiqun", ""], ["Lu", "Chang-Tien", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1905.10960", "submitter": "Marie Katsurai", "authors": "Marie Katsurai, Shunsuke Ono", "title": "TrendNets: Mapping Emerging Research Trends From Dynamic Co-Word\n  Networks via Sparse Representation", "comments": "This is a pre-print of an article published in Scientometrics, 2019", "journal-ref": null, "doi": "10.1007/s11192-019-03241-6", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping the knowledge structure from word co-occurrences in a collection of\nacademic papers has been widely used to provide insight into the topic\nevolution in an arbitrary research field. In a traditional approach, the paper\ncollection is first divided into temporal subsets, and then a co-word network\nis independently depicted in a 2D map to characterize each period's trend. To\neffectively map emerging research trends from such a time-series of co-word\nnetworks, this paper presents TrendNets, a novel visualization methodology that\nhighlights the rapid changes in edge weights over time. Specifically, we\nformulated a new convex optimization framework that decomposes the matrix\nconstructed from dynamic co-word networks into a smooth part and a sparse part:\nthe former represents stationary research topics, while the latter corresponds\nto bursty research topics. Simulation results on synthetic data demonstrated\nthat our matrix decomposition approach achieved the best burst detection\nperformance over four baseline methods. In experiments conducted using papers\npublished in the past 16 years at three conferences in different fields, we\nshowed the effectiveness of TrendNets compared to the traditional co-word\nrepresentation. We have made our codes available on the Web to encourage\nscientific mapping in all research fields.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 03:53:19 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 03:31:27 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Katsurai", "Marie", ""], ["Ono", "Shunsuke", ""]]}, {"id": "1905.10975", "submitter": "Cole Freeman", "authors": "Cole Freeman, Mrinal Kanti Roy, Michele Fattoruso, Hamed Alhoori", "title": "Shared Feelings: Understanding Facebook Reactions to Scholarly Articles", "comments": "4 pages, 5 figures, JCDL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on social-media platforms has tended to rely on textual analysis to\nperform research tasks. While text-based approaches have significantly\nincreased our understanding of online behavior and social dynamics, they\noverlook features on these platforms that have grown in prominence in the past\nfew years: click-based responses to content. In this paper, we present a new\ndataset of Facebook Reactions to scholarly content. We give an overview of its\nstructure, analyze some of the statistical trends in the data, and use it to\ntrain and test two supervised learning algorithms. Our preliminary tests\nsuggest the presence of stratification in the number of users following pages,\ndivisions that seem to fall in line with distinctions in the subject matter of\nthose pages.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 05:00:59 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Freeman", "Cole", ""], ["Roy", "Mrinal Kanti", ""], ["Fattoruso", "Michele", ""], ["Alhoori", "Hamed", ""]]}, {"id": "1905.11052", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Christian Ganser, Alexander Tekles, Loet Leydesdorff", "title": "Does the $h_\\alpha$ index reinforce the Matthew effect in science?\n  Agent-based simulations using Stata and R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Hirsch (2019a) proposed a new variant of the h index called the\n$h_\\alpha$ index. He formulated as follows: \"we define the $h_\\alpha$ index of\na scientist as the number of papers in the h-core of the scientist (i.e. the\nset of papers that contribute to the h-index of the scientist) where this\nscientist is the $\\alpha$-author\" (p. 673). The $h_\\alpha$ index was criticized\nby Leydesdorff, Bornmann, and Opthof (2019). One of their most important points\nis that the index reinforces the Matthew effect in science. We address this\npoint in the current study using a recently developed Stata command (h_index)\nand R package (hindex), which can be used to simulate h index and\n$h_\\alpha$index applications in research evaluation. The user can investigate\nunder which conditions $h_\\alpha$ reinforces the Matthew effect. The results of\nour study confirm what Leydesdorff et al. (2019) expected: the $h_\\alpha$ index\nreinforces the Matthew effect. This effect can be intensified if strategic\nbehavior of the publishing scientists and cumulative advantage effects are\nadditionally considered in the simulation.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 08:57:50 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Bornmann", "Lutz", ""], ["Ganser", "Christian", ""], ["Tekles", "Alexander", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1905.11096", "submitter": "Juli\\'an Urbano", "authors": "Juli\\'an Urbano, Harlley Lima, Alan Hanjalic", "title": "Statistical Significance Testing in Information Retrieval: An Empirical\n  Analysis of Type I, Type II and Type III Errors", "comments": "10 pages, 6 figures, SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331259", "report-no": null, "categories": "cs.IR cs.DL cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical significance testing is widely accepted as a means to assess how\nwell a difference in effectiveness reflects an actual difference between\nsystems, as opposed to random noise because of the selection of topics.\nAccording to recent surveys on SIGIR, CIKM, ECIR and TOIS papers, the t-test is\nthe most popular choice among IR researchers. However, previous work has\nsuggested computer intensive tests like the bootstrap or the permutation test,\nbased mainly on theoretical arguments. On empirical grounds, others have\nsuggested non-parametric alternatives such as the Wilcoxon test. Indeed, the\nquestion of which tests we should use has accompanied IR and related fields for\ndecades now. Previous theoretical studies on this matter were limited in that\nwe know that test assumptions are not met in IR experiments, and empirical\nstudies were limited in that we do not have the necessary control over the null\nhypotheses to compute actual Type I and Type II error rates under realistic\nconditions. Therefore, not only is it unclear which test to use, but also how\nmuch trust we should put in them. In contrast to past studies, in this paper we\nemploy a recent simulation methodology from TREC data to go around these\nlimitations. Our study comprises over 500 million p-values computed for a range\nof tests, systems, effectiveness measures, topic set sizes and effect sizes,\nand for both the 2-tail and 1-tail cases. Having such a large supply of IR\nevaluation data with full knowledge of the null hypotheses, we are finally in a\nposition to evaluate how well statistical significance tests really behave with\nIR data, and make sound recommendations for practitioners.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 10:02:29 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 22:18:34 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Urbano", "Juli\u00e1n", ""], ["Lima", "Harlley", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1905.11123", "submitter": "Roberto Di Cosmo", "authors": "Pierre Alliez (TITANE), Roberto Di Cosmo (UPD7), Benjamin Guedj\n  (UCL-CS), Alain Girault (SPADES), Mohand-Said Hacid (LIRIS), Arnaud Legrand\n  (LIG), Nicolas P. Rougier (Mnemosyne)", "title": "Attributing and Referencing (Research) Software: Best Practices and\n  Outlook from Inria", "comments": null, "journal-ref": null, "doi": "10.1109/MCSE.2019.2949413", "report-no": null, "categories": "cs.DL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software is a fundamental pillar of modern scientiic research, not only in\ncomputer science, but actually across all elds and disciplines. However, there\nis a lack of adequate means to cite and reference software, for many reasons.\nAn obvious rst reason is software authorship, which can range from a single\ndeveloper to a whole team, and can even vary in time. The panorama is even more\ncomplex than that, because many roles can be involved in software development:\nsoftware architect, coder, debugger, tester, team manager, and so on. Arguably,\nthe researchers who have invented the key algorithms underlying the software\ncan also claim a part of the authorship. And there are many other reasons that\nmake this issue complex. We provide in this paper a contribution to the ongoing\neeorts to develop proper guidelines and recommendations for software citation,\nbuilding upon the internal experience of Inria, the French research institute\nfor digital sciences. As a central contribution, we make three key\nrecommendations. (1) We propose a richer taxonomy for software contributions\nwith a qualitative scale. (2) We claim that it is essential to put the human at\nthe heart of the evaluation. And (3) we propose to distinguish citation from\nreference.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 11:19:33 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 18:30:19 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Alliez", "Pierre", "", "TITANE"], ["Di Cosmo", "Roberto", "", "UPD7"], ["Guedj", "Benjamin", "", "UCL-CS"], ["Girault", "Alain", "", "SPADES"], ["Hacid", "Mohand-Said", "", "LIRIS"], ["Legrand", "Arnaud", "", "LIG"], ["Rougier", "Nicolas P.", "", "Mnemosyne"]]}, {"id": "1905.11244", "submitter": "Andrew Collins Mr", "authors": "Andrew Collins, Joeran Beel", "title": "Document Embeddings vs. Keyphrases vs. Terms: An Online Evaluation in\n  Digital Library Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recommendation algorithms are available to digital library recommender\nsystem operators. The effectiveness of algorithms is largely unreported by way\nof online evaluation. We compare a standard term-based recommendation approach\nto two promising approaches for related-article recommendation in digital\nlibraries: document embeddings, and keyphrases. We evaluate the consistency of\ntheir performance across multiple scenarios. Through our\nrecommender-as-a-service Mr. DLib, we delivered 33.5M recommendations to users\nof Sowiport and Jabref over the course of 19 months, from March 2017 to October\n2018. The effectiveness of the algorithms differs significantly between\nSowiport and Jabref (Wilcoxon rank-sum test; p < 0.05). There is a ~400%\ndifference in effectiveness between the best and worst algorithm in both\nscenarios separately. The best performing algorithm in Sowiport (terms) is the\nworst performing in Jabref. The best performing algorithm in Jabref\n(keyphrases) is 70% worse in Sowiport, than Sowiport`s best algorithm\n(click-through rate; 0.1% terms, 0.03% keyphrases).\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 14:05:50 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Collins", "Andrew", ""], ["Beel", "Joeran", ""]]}, {"id": "1905.11342", "submitter": "Shawn Jones", "authors": "Shawn M. Jones and Michele C. Weigle and Michael L. Nelson", "title": "Social Cards Probably Provide For Better Understanding Of Web Archive\n  Collections", "comments": "58 pages, 53 figures", "journal-ref": null, "doi": "10.1145/3357384.3358039", "report-no": null, "categories": "cs.DL cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Used by a variety of researchers, web archive collections have become\ninvaluable sources of evidence. If a researcher is presented with a web archive\ncollection that they did not create, how do they know what is inside so that\nthey can use it for their own research? Search engine results and social media\nlinks are represented as surrogates, small easily digestible summaries of the\nunderlying page. Search engines and social media have a different focus, and\nhence produce different surrogates than web archives. Search engine surrogates\nhelp a user answer the question \"Will this link meet my information need?\"\nSocial media surrogates help a user decide \"Should I click on this?\" Our use\ncase is subtly different. We hypothesize that groups of surrogates together are\nuseful for summarizing a collection. We want to help users answer the question\nof \"What does the underlying collection contain?\" But which surrogate should we\nuse? With Mechanical Turk participants, we evaluate six different surrogate\ntypes against each other. We find that the type of surrogate does not influence\nthe time to complete the task we presented the participants. Of particular\ninterest are social cards, surrogates typically found on social media, and\nbrowser thumbnails, screen captures of web pages rendered in a browser. At\n$p=0.0569$, and $p=0.0770$, respectively, we find that social cards and social\ncards paired side-by-side with browser thumbnails probably provide better\ncollection understanding than the surrogates currently used by the popular\nArchive-It web archiving platform. We measure user interactions with each\nsurrogate and find that users interact with social cards less than other types.\nThe results of this study have implications for our web archive summarization\nwork, live web curation platforms, social media, and more.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 17:00:54 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 16:55:15 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 02:06:42 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jones", "Shawn M.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1905.11924", "submitter": "Ari Kobren", "authors": "Ari Kobren, Barna Saha, Andrew McCallum", "title": "Paper Matching with Local Fairness Constraints", "comments": "Appears at KDD 2019 Research Track, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically matching reviewers to papers is a crucial step of the peer\nreview process for venues receiving thousands of submissions. Unfortunately,\ncommon paper matching algorithms often construct matchings suffering from two\ncritical problems: (1) the group of reviewers assigned to a paper do not\ncollectively possess sufficient expertise, and (2) reviewer workloads are\nhighly skewed. In this paper, we propose a novel local fairness formulation of\npaper matching that directly addresses both of these issues. Since optimizing\nour formulation is not always tractable, we introduce two new algorithms,\nFairIR and FairFlow, for computing fair matchings that approximately optimize\nthe new formulation. FairIR solves a relaxation of the local fairness\nformulation and then employs a rounding technique to construct a valid matching\nthat provably maximizes the objective and only compromises on fairness with\nrespect to reviewer loads and papers by a small constant. In contrast, FairFlow\nis not provably guaranteed to produce fair matchings, however it can be 2x as\nefficient as FairIR and an order of magnitude faster than matching algorithms\nthat directly optimize for fairness. Empirically, we demonstrate that both\nFairIR and FairFlow improve fairness over standard matching algorithms on real\nconference data. Moreover, in comparison to state-of-the-art matching\nalgorithms that optimize for fairness only, FairIR achieves higher objective\nscores, FairFlow achieves competitive fairness, and both are capable of more\nevenly allocating reviewers.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 16:36:51 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Kobren", "Ari", ""], ["Saha", "Barna", ""], ["McCallum", "Andrew", ""]]}, {"id": "1905.12220", "submitter": "Alexander Nwala", "authors": "Alexander C. Nwala, Michele C. Weigle, Michael L. Nelson", "title": "Using Micro-collections in Social Media to Generate Seeds for Web\n  Archive Collections", "comments": "This is an extended version of the ACM/IEEE Joint Conference on\n  Digital Libraries (JCDL 2019) full paper. Some figures have been enlarged,\n  and appendices of additional figures included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In a Web plagued by disappearing resources, Web archive collections provide a\nvaluable means of preserving Web resources important to the study of past\nevents ranging from elections to disease outbreaks. These archived collections\nstart with seed URIs (Uniform Resource Identifiers) hand-selected by curators.\nCurators produce high quality seeds by removing non-relevant URIs and adding\nURIs from credible and authoritative sources, but it is time consuming to\ncollect these seeds. Two main strategies adopted by curators for discovering\nseeds include scraping Web (e.g., Google) Search Engine Result Pages (SERPs)\nand social media (e.g., Twitter) SERPs. In this work, we studied three social\nmedia platforms in order to provide insight on the characteristics of seeds\ngenerated from different sources. First, we developed a simple vocabulary for\ndescribing social media posts across different platforms. Second, we introduced\na novel source for generating seeds from URIs in the threaded conversations of\nsocial media posts created by single or multiple users. Users on social media\nsites routinely create and share posts about news events consisting of\nhand-selected URIs of news stories, tweets, videos, etc. In this work, we call\nthese posts micro-collections, and we consider them as an important source for\nseeds because the effort taken to create micro-collections is an indication of\neditorial activity, and a demonstration of domain expertise. Third, we\ngenerated 23,112 seed collections with text and hashtag queries from 449,347\nsocial media posts from Reddit, Twitter, and Scoop.it. We collected in total\n120,444 URIs from the conventional scraped SERP posts and micro-collections. We\ncharacterized the resultant seed collections across multiple dimensions\nincluding the distribution of URIs, precision, ages, diversity of webpages,\netc...\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 05:19:58 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Nwala", "Alexander C.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1905.12410", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, K. Brad Wray, Robin Haunschild", "title": "Citation concept analysis (CCA) - A new form of citation analysis\n  revealing the usefulness of concepts for other researchers illustrated by two\n  exemplary case studies including classic books by Thomas S. Kuhn and Karl R.\n  Popper", "comments": null, "journal-ref": null, "doi": "10.1007/s11192-019-03326-2", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the full text of papers are increasingly available\nelectronically which opens up the possibility of quantitatively investigating\ncitation contexts in more detail. In this study, we introduce a new form of\ncitation analysis, which we call citation concept analysis (CCA). CCA is\nintended to reveal the cognitive impact certain concepts -- published in a\ndocument -- have on the citing authors. It counts the number of times the\nconcepts are mentioned (cited) in the citation context of citing publications.\nWe demonstrate the method using three classical examples: (1) The structure of\nscientific revolutions by Thomas S. Kuhn, (2) The logic of scientific discovery\n- Logik der Forschung: Zur Erkenntnistheorie der modernen Naturwissenschaft in\nGerman -, and (3) Conjectures and refutations: the growth of scientific\nknowledge by Karl R. Popper. It is not surprising -- as our results show --\nthat Kuhn's \"paradigm\" concept has had a significant impact. What is surprising\nis that it has had such a disproportionately larger impact than Kuhn's other\nconcepts, e.g., \"scientific revolution\". The paradigm concept accounts for over\n80% of the concept-related citations to Kuhn's work, and its impact is\nresilient across all disciplines and over time. With respect to Popper,\n\"falsification\" is the most used concept derived from his books. Falsification,\nafter all, is the cornerstone of Popper's critical rationalism.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 07:53:58 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Bornmann", "Lutz", ""], ["Wray", "K. Brad", ""], ["Haunschild", "Robin", ""]]}, {"id": "1905.12565", "submitter": "Mohamed Aturban", "authors": "Mohamed Aturban, Sawood Alam, Michael L. Nelson, Michele C. Weigle", "title": "Archive Assisted Archival Fixity Verification Framework", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of public and private web archives has increased, and we\nimplicitly trust content delivered by these archives. Fixity is checked to\nensure an archived resource has remained unaltered since the time it was\ncaptured. Some web archives do not allow users to access fixity information\nand, more importantly, even if fixity information is available, it is provided\nby the same archive from which the archived resources are requested. In this\nresearch, we propose two approaches, namely Atomic and Block, to establish and\ncheck fixity of archived resources. In the Atomic approach, the fixity\ninformation of each archived web page is stored in a JSON file (or a manifest),\nand published in a well-known web location (an Archival Fixity server) before\nit is disseminated to several on-demand web archives. In the Block approach, we\nfirst batch together fixity information of multiple archived pages in a single\nbinary-searchable file (or a block) before it is published and disseminated to\narchives. In both approaches, the fixity information is not obtained directly\nfrom archives. Instead, we compute the fixity information (e.g., hash values)\nbased on the playback of archived resources. One advantage of the Atomic\napproach is the ability to verify fixity of archived pages even with the\nabsence of the Archival Fixity server. The Block approach requires pushing\nfewer resources into archives, and it performs fixity verification faster than\nthe Atomic approach. On average, it takes about 1.25X, 4X, and 36X longer to\ndisseminate a manifest to perma.cc, archive.org, and webcitation.org,\nrespectively, than archive.is, while it takes 3.5X longer to disseminate a\nblock to archive.org than perma.cc. The Block approach performs 4.46X faster\nthan the Atomic approach on verifying the fixity of archived pages.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 16:16:59 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Aturban", "Mohamed", ""], ["Alam", "Sawood", ""], ["Nelson", "Michael L.", ""], ["Weigle", "Michele C.", ""]]}, {"id": "1905.12607", "submitter": "Sawood Alam", "authors": "Sawood Alam, Michele C. Weigle, Michael L. Nelson, Fernando Melo,\n  Daniel Bicho, Daniel Gomes", "title": "MementoMap Framework for Flexible and Adaptive Web Archive Profiling", "comments": "In Proceedings of JCDL 2019; 13 pages, 9 tables, 13 figures, 3 code\n  samples, and 1 equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we propose MementoMap, a flexible and adaptive framework to\nefficiently summarize holdings of a web archive. We described a simple, yet\nextensible, file format suitable for MementoMap. We used the complete index of\nthe Arquivo.pt comprising 5B mementos (archived web pages/files) to understand\nthe nature and shape of its holdings. We generated MementoMaps with varying\namount of detail from its HTML pages that have an HTTP status code of 200 OK.\nAdditionally, we designed a single-pass, memory-efficient, and\nparallelization-friendly algorithm to compact a large MementoMap into a small\none and an in-file binary search method for efficient lookup. We analyzed more\nthan three years of MemGator (a Memento aggregator) logs to understand the\nresponse behavior of 14 public web archives. We evaluated MementoMaps by\nmeasuring their Accuracy using 3.3M unique URIs from MemGator logs. We found\nthat a MementoMap of less than 1.5% Relative Cost (as compared to the\ncomprehensive listing of all the unique original URIs) can correctly identify\nthe presence or absence of 60% of the lookup URIs in the corresponding archive\nwhile maintaining 100% Recall (i.e., zero false negatives).\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 17:41:21 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Alam", "Sawood", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""], ["Melo", "Fernando", ""], ["Bicho", "Daniel", ""], ["Gomes", "Daniel", ""]]}, {"id": "1905.12834", "submitter": "Ba Nguyen", "authors": "Ba Xuan Nguyen, Markus Luczak-Roesch and Jesse David Dinneen", "title": "Exploring the Effects of Data Set Choice on Measuring International\n  Research Collaboration: an Example Using the ACM Digital Library and\n  Microsoft Academic Graph", "comments": "This paper was accepted for publication at the 17th INTERNATIONAL\n  CONFERENCE ON SCIENTOMETRICS & INFORMETRICS (ISSI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  International research collaboration (IRC) measurement is important because\ncountries can and want to benefit from international collaboration but\nperforming the same measurement procedure on different data sets can lead to\ndifferent results. This study aims to explore the effects of data set choice on\nIRC measurement.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 02:45:07 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Nguyen", "Ba Xuan", ""], ["Luczak-Roesch", "Markus", ""], ["Dinneen", "Jesse David", ""]]}, {"id": "1905.13226", "submitter": "Ba Nguyen", "authors": "Ba Xuan Nguyen, Jesse David Dinneen and Markus Luczak-Roesch", "title": "Enriching Bibliographic Data by Combining String Matching and the\n  Wikidata Knowledge Graph to Improve the Measurement of International Research\n  Collaboration", "comments": "This paper was accepted for publication at the 17th INTERNATIONAL\n  CONFERENCE ON SCIENTOMETRICS & INFORMETRICS (ISSI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring international research collaboration is necessary when evaluating,\nfor example, the efficacy of policy meant to increase cooperation between\ncountries, but is currently very difficult as bibliographic records contain\nonly affiliation data from which there is no standard method to identify the\nrelevant countries. In this paper we describe a method to address this\ndifficulty, and evaluate it using both general and domain-specific data sets.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 02:24:35 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Nguyen", "Ba Xuan", ""], ["Dinneen", "Jesse David", ""], ["Luczak-Roesch", "Markus", ""]]}, {"id": "1905.13363", "submitter": "Yasith Jayawardana", "authors": "Yasith Jayawardana, Sampath Jayarathna", "title": "DFS: A Dataset File System for Data Discovering Users", "comments": null, "journal-ref": null, "doi": "10.1109/JCDL.2019.00068", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research questions can be answered quickly and efficiently using data\nalready collected for previous research. This practice is called secondary data\nanalysis (SDA), and has gained popularity due to lower costs and improved\nresearch efficiency. In this paper we propose DFS, a file system to standardize\nthe metadata representation of datasets, and DDU, a scalable architecture based\non DFS for semi-automated metadata generation and data recommendation on the\ncloud. We discuss how DFS and DDU lays groundwork for automatic dataset\naggregation, how it integrates with existing data wrangling and machine\nlearning tools, and explores their implications on datasets stored in digital\nlibraries.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 00:23:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Jayawardana", "Yasith", ""], ["Jayarathna", "Sampath", ""]]}]