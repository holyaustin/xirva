[{"id": "1406.0216", "submitter": "Mathias Niepert", "authors": "Jakob Huber, Timo Sztyler, Jan Noessner, Jaimie Murdock, Colin Allen,\n  Mathias Niepert", "title": "LODE: Linking Digital Humanities Content to the Web of Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous digital humanities projects maintain their data collections in the\nform of text, images, and metadata. While data may be stored in many formats,\nfrom plain text to XML to relational databases, the use of the resource\ndescription framework (RDF) as a standardized representation has gained\nconsiderable traction during the last five years. Almost every digital\nhumanities meeting has at least one session concerned with the topic of digital\nhumanities, RDF, and linked data. While most existing work in linked data has\nfocused on improving algorithms for entity matching, the aim of the\nLinkedHumanities project is to build digital humanities tools that work \"out of\nthe box,\" enabling their use by humanities scholars, computer scientists,\nlibrarians, and information scientists alike. With this paper, we report on the\nLinked Open Data Enhancer (LODE) framework developed as part of the\nLinkedHumanities project. With LODE we support non-technical users to enrich a\nlocal RDF repository with high-quality data from the Linked Open Data cloud.\nLODE links and enhances the local RDF repository without compromising the\nquality of the data. In particular, LODE supports the user in the enhancement\nand linking process by providing intuitive user-interfaces and by suggesting\nhigh-quality linking candidates using tailored matching algorithms. We hope\nthat the LODE framework will be useful to digital humanities scholars\ncomplementing other digital humanities tools.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 23:37:39 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Huber", "Jakob", ""], ["Sztyler", "Timo", ""], ["Noessner", "Jan", ""], ["Murdock", "Jaimie", ""], ["Allen", "Colin", ""], ["Niepert", "Mathias", ""]]}, {"id": "1406.0936", "submitter": "Chuang Liu", "authors": "Feng Hu, Hai-Xing Zhao, Xiu-Xiu Zhan, Chuang Liu, Zi-Ke Zhang", "title": "Evolution of citation networks with the hypergraph formalism", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed an evolving model via the hypergraph to illustrate\nthe evolution of the citation network. In the evolving model, we consider the\nmechanism combined with preferential attachment and the aging influence.\nSimulation results show that the proposed model can characterize the citation\ndistribution of the real system very well. In addition, we give the analytical\nresult of the citation distribution using the master equation. Detailed\nanalysis showed that the time decay factor should be the origin of the same\ncitation distribution between the proposed model and the empirical result. The\nproposed model might shed some lights in understanding the underlying laws\ngoverning the structure of real citation networks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 04:15:01 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Hu", "Feng", ""], ["Zhao", "Hai-Xing", ""], ["Zhan", "Xiu-Xiu", ""], ["Liu", "Chuang", ""], ["Zhang", "Zi-Ke", ""]]}, {"id": "1406.1143", "submitter": "Jimmy Lin", "authors": "Sarah Weissman, Samet Ayhan, Joshua Bradley, and Jimmy Lin", "title": "Identifying Duplicate and Contradictory Information in Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our study identifies sentences in Wikipedia articles that are either\nidentical or highly similar by applying techniques for near-duplicate detection\nof web pages. This is accomplished with a MapReduce implementation of minhash\nto identify clusters of sentences with high Jaccard similarity. We show that\nthese clusters can be categorized into six different types, two of which are\nparticularly interesting: identical sentences quantify the extent to which\ncontent in Wikipedia is copied and pasted, and near-duplicate sentences that\nstate contradictory facts point to quality issues in Wikipedia.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 18:59:00 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Weissman", "Sarah", ""], ["Ayhan", "Samet", ""], ["Bradley", "Joshua", ""], ["Lin", "Jimmy", ""]]}, {"id": "1406.2746", "submitter": "Ekin Oguz", "authors": "Mishari Almishari, Mohamed Ali Kaafar, Gene Tsudik, Ekin Oguz", "title": "Are 140 Characters Enough? A Large-Scale Linkability Study of Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogging is a very popular Internet activity that informs and entertains\ngreat multitudes of people world-wide via quickly and scalably disseminated\nterse messages containing all kinds of newsworthy utterances. Even though\nmicroblogging is neither designed nor meant to emphasize privacy, numerous\ncontributors hide behind pseudonyms and compartmentalize their different\nincarnations via multiple accounts within the same, or across multiple,\nsite(s). Prior work has shown that stylometric analysis is a very powerful tool\ncapable of linking product or service reviews and blogs that are produced by\nthe same author when the number of authors is large. In this paper, we explore\nlinkability of tweets. Our results, based on a very large corpus of tweets,\nclearly demonstrate that, at least for relatively active tweeters, linkability\nof tweets by the same author is easily attained even when the number of\ntweeters is large. We also show that our linkability results hold for a set of\nactual Twitter users who tweet from multiple accounts. This has some obvious\nprivacy implications, both positive and negative.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 00:13:59 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 03:32:41 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Almishari", "Mishari", ""], ["Kaafar", "Mohamed Ali", ""], ["Tsudik", "Gene", ""], ["Oguz", "Ekin", ""]]}, {"id": "1406.2793", "submitter": "Song Gao", "authors": "Song Gao", "title": "Towards a Frontier of Spatial Scientometric Studies", "comments": "10 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research field of spatial scientometrics is dedicated to measuring and\nanalyzing science with spatial components (e.g., location, place, mapping).\nBecause of the dynamic nature of this field, researchers from multidisciplinary\ndomains constantly contribute qualitative, quantitative and computational\napproaches and technologies into scientometric analysis. This article aims to\ngiving a brief overview about this field by analyzing the publications in\n(spatial) scientometrics collected from the Scopus database and introduces\nrecent frontier researches which integrate advanced spatial analysis and\ngeovisualization with Semantic Web technologies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 06:51:31 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Gao", "Song", ""]]}, {"id": "1406.2880", "submitter": "Ulf Sch\\\"oneberg", "authors": "Ulf Sch\\\"oneberg and Wolfram Sperber", "title": "POS Tagging and its Applications for Mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content analysis of scientific publications is a nontrivial task, but a\nuseful and important one for scientific information services. In the Gutenberg\nera it was a domain of human experts; in the digital age many machine-based\nmethods, e.g., graph analysis tools and machine-learning techniques, have been\ndeveloped for it. Natural Language Processing (NLP) is a powerful\nmachine-learning approach to semiautomatic speech and language processing,\nwhich is also applicable to mathematics. The well established methods of NLP\nhave to be adjusted for the special needs of mathematics, in particular for\nhandling mathematical formulae. We demonstrate a mathematics-aware part of\nspeech tagger and give a short overview about our adaptation of NLP methods for\nmathematical publications. We show the use of the tools developed for key\nphrase extraction and classification in the database zbMATH.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 12:25:26 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Sch\u00f6neberg", "Ulf", ""], ["Sperber", "Wolfram", ""]]}, {"id": "1406.2886", "submitter": "Sta\\v{s}a Milojevi\\'c", "authors": "Sta\\v{s}a Milojevi\\'c, Cassidy R. Sugimoto, Vincent Larivi\\`ere, Mike\n  Thelwall and Ying Ding", "title": "The role of handbooks in knowledge creation and diffusion: A case of\n  science and technology studies", "comments": "Accepted for publication in Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genre is considered to be an important element in scholarly communication and\nin the practice of scientific disciplines. However, scientometric studies have\ntypically focused on a single genre, the journal article. The goal of this\nstudy is to understand the role that handbooks play in knowledge creation and\ndiffusion and their relationship with the genre of journal articles,\nparticularly in highly interdisciplinary and emergent social science and\nhumanities disciplines. To shed light on these questions we focused on\nhandbooks and journal articles published over the last four decades belonging\nto the research area of Science and Technology Studies (STS), broadly defined.\nTo get a detailed picture we used the full-text of five handbooks (500,000\nwords) and a well-defined set of 11,700 STS articles. We confirmed the\nmethodological split of STS into qualitative and quantitative (scientometric)\napproaches. Even when the two traditions explore similar topics (e.g., science\nand gender) they approach them from different starting points. The change in\ncognitive foci in both handbooks and articles partially reflects the changing\ntrends in STS research, often driven by technology. Using text similarity\nmeasures we found that, in the case of STS, handbooks play no special role in\neither focusing the research efforts or marking their decline. In general, they\ndo not represent the summaries of research directions that have emerged since\nthe previous edition of the handbook.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 12:36:07 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Milojevi\u0107", "Sta\u0161a", ""], ["Sugimoto", "Cassidy R.", ""], ["Larivi\u00e8re", "Vincent", ""], ["Thelwall", "Mike", ""], ["Ding", "Ying", ""]]}, {"id": "1406.3110", "submitter": "Lior Rokach", "authors": "Yuval Elovici, Lior Rokach", "title": "Reaction to New Security Threat Class", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each new identified security threat class triggers new research and\ndevelopment efforts by the scientific and professional communities. In this\nstudy, we investigate the rate at which the scientific and professional\ncommunities react to new identified threat classes as it is reflected in the\nnumber of patents, scientific articles and professional publications over a\nlong period of time. The following threat classes were studied: Phishing; SQL\nInjection; BotNet; Distributed Denial of Service; and Advanced Persistent\nThreat. Our findings suggest that in most cases it takes a year for the\nscientific community and more than two years for industry to react to a new\nthreat class with patents. Since new products follow patents, it is reasonable\nto expect that there will be a window of approximately two to three years in\nwhich no effective product is available to cope with the new threat class.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 03:36:38 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Elovici", "Yuval", ""], ["Rokach", "Lior", ""]]}, {"id": "1406.3876", "submitter": "Shawn Jones", "authors": "Shawn M. Jones, Michael L. Nelson, Harihar Shankar, Herbert Van de\n  Sompel", "title": "Bringing Web Time Travel to MediaWiki: An Assessment of the Memento\n  MediaWiki Extension", "comments": "23 pages, 18 figures, 9 tables, 17 listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have implemented the Memento MediaWiki Extension Version 2.0, which brings\nthe Memento Protocol to MediaWiki, used by Wikipedia and the Wikimedia\nFoundation. Test results show that the extension has a negligible impact on\nperformance. Two 302 status code datetime negotiation patterns, as defined by\nMemento, have been examined for the extension: Pattern 1.1, which requires 2\nrequests, versus Pattern 2.1, which requires 3 requests. Our test results and\nmathematical review find that, contrary to intuition, Pattern 2.1 performs\nbetter than Pattern 1.1 due to idiosyncrasies in MediaWiki. In addition to\nimplementing Memento, Version 2.0 allows administrators to choose the optional\n200-style datetime negotiation Pattern 1.2 instead of Pattern 2.1. It also\npermits administrators the ability to have the Memento MediaWiki Extension\nreturn full HTTP 400 and 500 status codes rather than using standard MediaWiki\nerror pages. Finally, version 2.0 permits administrators to turn off\nrecommended Memento headers if desired. Seeing as much of our work focuses on\nproducing the correct revision of a wiki page in response to a user's datetime\ninput, we also examine the problem of finding the correct revisions of the\nembedded resources, including images, stylesheets, and JavaScript; identifying\nthe issues and discussing whether or not MediaWiki must be changed to support\nthis functionality.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 01:01:54 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Jones", "Shawn M.", ""], ["Nelson", "Michael L.", ""], ["Shankar", "Harihar", ""], ["Van de Sompel", "Herbert", ""]]}, {"id": "1406.4020", "submitter": "Grigori Fursin", "authors": "Grigori Fursin (INRIA Saclay - Ile de France), Christophe Dubach\n  (ICSA)", "title": "Community-driven reviewing and validation of publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we share our practical experience on crowdsourcing evaluation\nof research artifacts and reviewing of publications since 2008. We also briefly\ndiscuss encountered problems including reproducibility of experimental results\nand possible solutions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 14:08:19 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Fursin", "Grigori", "", "INRIA Saclay - Ile de France"], ["Dubach", "Christophe", "", "ICSA"]]}, {"id": "1406.4161", "submitter": "Daniele Rotolo", "authors": "Daniele Rotolo and Loet Leydesdorff", "title": "Matching MEDLINE/PubMed Data with Web of Science (WoS): A Routine in R\n  language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel routine, namely medlineR, based on R-language, that\nenables the user to match data from MEDLINE/PubMed with records indexed in the\nISI Web of Science (WoS) database. The matching allows exploiting the rich and\ncontrolled vocabulary of Medical Subject Headings (MeSH) of MEDLINE/PubMed with\nadditional fields of WoS. The integration provides data (e.g. citation data,\nlist of cited reference, full list of the addresses of authors' host\norganisations, WoS subject categories) to perform a variety of scientometric\nanalyses. This brief communication describes medlineR, the methodology on which\nit relies, and the steps the user should follow to perform the matching across\nthe two databases. In order to specify the differences from Leydesdorff and\nOpthof (2013), we conclude the brief communication by testing the routine on\nthe case of the \"Burgada Syndrome\".\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 20:27:50 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 16:51:43 GMT"}, {"version": "v3", "created": "Wed, 9 Jul 2014 16:24:23 GMT"}, {"version": "v4", "created": "Thu, 10 Jul 2014 16:14:10 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Rotolo", "Daniele", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1406.4331", "submitter": "Delgado Lopez-Cozar emilio", "authors": "Enrique Ordu\\~na-Malea, Emilio Delgado Lopez-Cozar", "title": "The dark side of Open Access in Google and Google Scholar: the case of\n  Latin-American repositories", "comments": "16 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Since repositories are a key tool in making scholarly knowledge open access,\ndetermining their presence and impact on the Web is essential, particularly in\nGoogle (search engine par excellence) and Google Scholar (a tool increasingly\nused by researchers to search for academic information). The few studies\nconducted so far have been limited to very specific geographic areas (USA),\nwhich makes it necessary to find out what is happening in other regions that\nare not part of mainstream academia, and where repositories play a decisive\nrole in the visibility of scholarly production. The main objective of this\nstudy is to ascertain the presence and visibility of Latin American\nrepositories in Google and Google Scholar through the application of page count\nand visibility indicators. For a sample of 137 repositories, the results\nindicate that the indexing ratio is low in Google, and virtually nonexistent in\nGoogle Scholar; they also indicate a complete lack of correspondence between\nthe repository records and the data produced by these two search tools. These\nresults are mainly attributable to limitations arising from the use of\ndescription schemas that are incompatible with Google Scholar (repository\ndesign) and the reliability of web indicators (search engines). We conclude\nthat neither Google nor Google Scholar accurately represent the actual size of\nopen access content published by Latin American repositories; this may indicate\na non-indexed, hidden side to open access, which could be limiting the\ndissemination and consumption of open access scholarly literature.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 11:45:30 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Ordu\u00f1a-Malea", "Enrique", ""], ["Lopez-Cozar", "Emilio Delgado", ""]]}, {"id": "1406.4542", "submitter": "Edwin Henneken", "authors": "Edwin A. Henneken, Alberto Accomazzi, Michael J. Kurtz, Carolyn S.\n  Grant, Donna Thompson, Jay Luker, Roman Chyla, Alexandra Holachek and Stephen\n  S. Murray", "title": "Computing and Using Metrics in the ADS", "comments": "to appear in proceedings of LISA VII conference, Naples, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL astro-ph.IM", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Finding measures for research impact, be it for individuals, institutions,\ninstruments or projects, has gained a lot of popularity. More papers than ever\nare being written on new impact measures, and problems with existing measures\nare being pointed out on a regular basis. Funding agencies require impact\nstatistics in their reports, job candidates incorporate them in their resumes,\nand publication metrics have even been used in at least one recent court case.\nTo support this need for research impact indicators, the SAO/NASA Astrophysics\nData System (ADS) has developed a service which provides a broad overview of\nvarious impact measures. In this presentation we discuss how the ADS can be\nused to quench the thirst for impact measures. We will also discuss a couple of\nthe lesser known indicators in the metrics overview and the main issues to be\naware of when compiling publication-based metrics in the ADS, namely author\nname ambiguity and citation incompleteness.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 21:06:48 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Henneken", "Edwin A.", ""], ["Accomazzi", "Alberto", ""], ["Kurtz", "Michael J.", ""], ["Grant", "Carolyn S.", ""], ["Thompson", "Donna", ""], ["Luker", "Jay", ""], ["Chyla", "Roman", ""], ["Holachek", "Alexandra", ""], ["Murray", "Stephen S.", ""]]}, {"id": "1406.5520", "submitter": "Henk Moed", "authors": "Henk F. Moed and Gali Halevi", "title": "The Multidimensional Assessment of Scholarly Research Impact", "comments": "Author copy version accepted for publication, JASIST (Journal of the\n  Association for Information Science and Technology) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the Multidimensional Research Assessment Matrix of\nscientific output. Its base notion holds that the choice of metrics to be\napplied in a research assessment process depends upon the unit of assessment,\nthe research dimension to be assessed, and the purposes and policy context of\nthe assessment. An indicator may by highly useful within one assessment\nprocess, but less so in another. For instance, publication counts are useful\ntools to help discriminating between those staff members who are research\nactive, and those who are not, but are of little value if active scientists are\nto be compared one another according to their research performance. This paper\ngives a systematic account of the potential usefulness and limitations of a set\nof 10 important metrics including altmetrics, applied at the level of\nindividual articles, individual researchers, research groups and institutions.\nIt presents a typology of research impact dimensions, and indicates which\nmetrics are the most appropriate to measure each dimension. It introduces the\nconcept of a meta-analysis of the units under assessment in which metrics are\nnot used as tools to evaluate individual units, but to reach policy inferences\nregarding the objectives and general setup of an assessment process.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 20:13:52 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Moed", "Henk F.", ""], ["Halevi", "Gali", ""]]}, {"id": "1406.5688", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff", "title": "Information, Meaning, and Intellectual Organization in Networks of\n  Inter-Human Communication", "comments": "Pp. 280-303 in: Cassidy R. Sugimoto (Ed.), Theories of Informetrics\n  and Scholarly Communication, Berlin/Boston MA: De Gruyter, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shannon-Weaver model of linear information transmission is extended with\ntwo loops potentially generating redundancies: (i) meaning is provided locally\nto the information from the perspective of hindsight, and (ii) meanings can be\ncodified differently and then refer to other horizons of meaning. Thus, three\nlayers are distinguished: variations in the communications, historical\norganization at each moment of time, and evolutionary self-organization of the\ncodes of communication over time. Furthermore, the codes of communication can\nfunctionally be different and then the system is both horizontally and\nvertically differentiated. All these subdynamics operate in parallel and\nnecessarily generate uncertainty. However, meaningful information can be\nconsidered as the specific selection of a signal from the noise; the codes of\ncommunication are social constructs that can generate redundancy by giving\ndifferent meanings to the same information. Reflexively, one can translate\namong codes in more elaborate discourses. The second (instantiating) layer can\nbe operationalized in terms of semantic maps using the vector space model; the\nthird in terms of mutual redundancy among the latent dimensions of the vector\nspace. Using Blaise Cronin's {\\oe}uvre, the different operations of the three\nlayers are demonstrated empirically.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 09:18:18 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 11:54:59 GMT"}, {"version": "v3", "created": "Sat, 20 Oct 2018 11:11:00 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Leydesdorff", "Loet", ""]]}, {"id": "1406.6126", "submitter": "Ross Moore", "authors": "Ross Moore", "title": "PDF/A-3u as an archival format for Accessible mathematics", "comments": "This is a post-print version of original in volume: S.M. Watt et al.\n  (Eds.): CICM 2014, LNAI 8543, pp.184-199, 2014; available at\n  http://link.springer.com/search?query=LNAI+8543, along with supplementary\n  PDF. This version, with supplement as attachment, is enriched to validate as\n  PDF/A-3u modulo an error in white-space handling in the pdfTeX version used\n  to generate it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Including LaTeX source of mathematical expressions, within the PDF document\nof a text-book or research paper, has definite benefits regarding\n`Accessibility' considerations. Here we describe three ways in which this can\nbe done, fully compatibly with international standards ISO 32000, ISO 19005-3,\nand the forthcoming ISO 32000-2 (PDF 2.0). Two methods use embedded files, also\nknown as `attachments', holding information in either LaTeX or MathML formats,\nbut use different PDF structures to relate these attachments to regions of the\ndocument window. One uses structure, so is applicable to a fully `Tagged PDF'\ncontext, while the other uses /AF tagging of the relevant content. The third\nmethod requires no tagging at all, instead including the source coding as the\n/ActualText replacement of a so-called `fake space'. Information provided this\nway is extracted via simple Select/Copy/Paste actions, and is available to\nexisting screen-reading software and assistive technologies.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 02:48:42 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Moore", "Ross", ""]]}, {"id": "1406.6840", "submitter": "Hardik Joshi Mr.", "authors": "Hardik Joshi", "title": "From Citation count to Argumentation count: a new metric to indicate the\n  usefulness of an article", "comments": "Technical Conference cum Workshop on Digital Library Using DSpace\n  hosted by Gujarat National Law University on 21-23 March, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation count is a quantifiable measure to indicate the number of times an\narticle is cited by other articles. It is believed that if an article is cited\noften then it must be an important or influential article; however, there is no\nguarantee that the most cited articles are good in quality. In this paper, the\nauthor suggests argumentation count, a new metric for citation analysis. The\nproposed metric, argumentation count is a triplet of quantities for each\nconcept of an article that helps in providing a quantifiable measure about the\nusefulness of an article.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 11:01:11 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Joshi", "Hardik", ""]]}, {"id": "1406.7091", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann", "title": "Do altmetrics point to the broader impact of research? An overview of\n  benefits and disadvantages of altmetrics", "comments": "Accepted for publication in the Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, it is not clear how the impact of research on other areas of society\nthan science should be measured. While peer review and bibliometrics have\nbecome standard methods for measuring the impact of research in science, there\nis not yet an accepted framework within which to measure societal impact.\nAlternative metrics (called altmetrics to distinguish them from bibliometrics)\nare considered an interesting option for assessing the societal impact of\nresearch, as they offer new ways to measure (public) engagement with research\noutput. Altmetrics is a term to describe web-based metrics for the impact of\npublications and other scholarly material by using data from social media\nplatforms (e.g. Twitter or Mendeley). This overview of studies explores the\npotential of altmetrics for measuring societal impact. It deals with the\ndefinition and classification of altmetrics. Furthermore, their benefits and\ndisadvantages for measuring impact are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 07:31:12 GMT"}, {"version": "v2", "created": "Wed, 10 Sep 2014 13:08:51 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Bornmann", "Lutz", ""]]}, {"id": "1406.7582", "submitter": "Walter Lasecki", "authors": "Simona Doboli, Fanshu Zhao, and Alex Doboli", "title": "New measures for evaluating creativity in scientific publications", "comments": null, "journal-ref": null, "doi": null, "report-no": "ci-2014/127", "categories": "cs.SI cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our research is to understand how ideas propagate, combine and\nare created in large social networks. In this work, we look at a sample of\nrelevant scientific publications in the area of high-frequency analog circuit\ndesign and their citation distribution. A novel aspect of our work is the way\nin which we categorize citations based on the reason and place of it in a\npublication. We created seven citation categories from general domain\nreferences, references to specific methods used in the same domain problem,\nreferences to an analysis method, references for experimental comparison and so\non. This added information allows us to define two new measures to characterize\nthe creativity (novelty and usefulness) of a publication based on its pattern\nof citations clustered by reason, place and citing scientific group. We\nanalyzed 30 publications in relevant journals since 2000 and their about 300\ncitations, all in the area of high-frequency analog circuit design. We observed\nthat the number of citations a publication receives from different scientific\ngroups matches a Levy type distribution: with a large number of groups citing a\npublication relatively few times, and a very small number of groups citing a\npublication a large number of times. We looked at the motifs a publication is\ncited differently by different scientific groups.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 02:41:22 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Doboli", "Simona", ""], ["Zhao", "Fanshu", ""], ["Doboli", "Alex", ""]]}, {"id": "1406.7611", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann", "title": "Validity of altmetrics data for measuring societal impact: A study using\n  data from Altmetric and F1000Prime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can altmetric data be validly used for the measurement of societal impact?\nThe current study seeks to answer this question with a comprehensive dataset\n(about 100,000 records) from very disparate sources (F1000, Altmetric, and an\nin-house database based on Web of Science). In the F1000 peer review system,\nexperts attach particular tags to scientific papers which indicate whether a\npaper could be of interest for science or rather for other segments of society.\nThe results show that papers with the tag \"good for teaching\" do achieve higher\naltmetric counts than papers without this tag - if the quality of the papers is\ncontrolled. At the same time, a higher citation count is shown especially by\npapers with a tag that is specifically scientifically oriented (\"new finding\").\nThe findings indicate that papers tailored for a readership outside the area of\nresearch should lead to societal impact. If altmetric data is to be used for\nthe measurement of societal impact, the question arises of its normalization.\nIn bibliometrics, citations are normalized for the papers' subject area and\npublication year. This study has taken a second analytic step involving a\npossible normalization of altmetric data. As the results show there are\nparticular scientific topics which are of especial interest for a wide\naudience. Since these more or less interesting topics are not completely\nreflected in Thomson Reuters' journal sets, a normalization of altmetric data\nshould not be based on the level of subject categories, but on the level of\ntopics.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 06:19:24 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 08:37:42 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Bornmann", "Lutz", ""]]}, {"id": "1406.7749", "submitter": "Walter Lasecki", "authors": "Richard Absalom, Marcus Luczak-Rosch, Dap Hartmann, and Aske Plaat", "title": "Crowd-Sourcing Fuzzy and Faceted Classification for Concept Search", "comments": null, "journal-ref": null, "doi": null, "report-no": "ci-2014/82", "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for concepts in science and technology is often a difficult task.\nTo facilitate concept search, different types of human-generated metadata have\nbeen created to define the content of scientific and technical disclosures.\nClassification schemes such as the International Patent Classification (IPC)\nand MEDLINE's MeSH are structured and controlled, but require trained experts\nand central management to restrict ambiguity (Mork, 2013). While unstructured\ntags of folksonomies can be processed to produce a degree of structure\n(Kalendar, 2010; Karampinas, 2012; Sarasua, 2012; Bragg, 2013) the freedom\nenjoyed by the crowd typically results in less precision (Stock 2007).\n  Existing classification schemes suffer from inflexibility and ambiguity.\nSince humans understand language, inference, implication, abstraction and hence\nconcepts better than computers, we propose to harness the collective wisdom of\nthe crowd. To do so, we propose a novel classification scheme that is\nsufficiently intuitive for the crowd to use, yet powerful enough to facilitate\nsearch by analogy, and flexible enough to deal with ambiguity. The system will\nenhance existing classification information. Linking up with the semantic web\nand computer intelligence, a Citizen Science effort (Good, 2013) would support\ninnovation by improving the quality of granted patents, reducing duplicitous\nresearch, and stimulating problem-oriented solution design.\n  A prototype of our design is in preparation. A crowd-sourced fuzzy and\nfaceted classification scheme will allow for better concept search and improved\naccess to prior art in science and technology.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 14:18:29 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Absalom", "Richard", ""], ["Luczak-Rosch", "Marcus", ""], ["Hartmann", "Dap", ""], ["Plaat", "Aske", ""]]}]