[{"id": "2005.00485", "submitter": "Josiane Mothe", "authors": "Bernard Dousset and Josiane Mothe", "title": "Getting Insights from a Large Corpus of Scientific Papers on\n  Specialisted Comprehensive Topics -- the Case of COVID-19", "comments": "14 pages; 5 figures and 3 tables submitted to KES 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 is one of the most important topic these days, specifically on\nsearch engines and news. While fake news are easily shared, scientific papers\nare reliable sources where information can be extracted. With about 24,000\nscientific publications on COVID-19 and related research on PUBMED, automatic\ncomputer-assisted analysis is required. In this paper, we develop two\nmethodologies to get insights on specific sub-topics of interest and latest\nresearch sub-topics. They rely on natural language processing and graph-based\nvisualizations. We run these methodologies on two cases: the virus origin and\nthe uses of existing drugs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:59:25 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Dousset", "Bernard", ""], ["Mothe", "Josiane", ""]]}, {"id": "2005.00554", "submitter": "David Coeurjolly", "authors": "Nicolas Bonneel and David Coeurjolly and Julie Digne and Nicolas\n  Mellado", "title": "Code Replicability in Computer Graphics", "comments": "8 pages. ACM Trans. on Graphics (Proceedings of SIGGRAPH 2020)", "journal-ref": null, "doi": "10.1145/3386569.3392413", "report-no": null, "categories": "cs.DL cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to duplicate published research results is an important process of\nconducting research whether to build upon these findings or to compare with\nthem. This process is called \"replicability\" when using the original authors'\nartifacts (e.g., code), or \"reproducibility\" otherwise (e.g., re-implementing\nalgorithms). Reproducibility and replicability of research results have gained\na lot of interest recently with assessment studies being led in various fields,\nand they are often seen as a trigger for better result diffusion and\ntransparency. In this work, we assess replicability in Computer Graphics, by\nevaluating whether the code is available and whether it works properly. As a\nproxy for this field we compiled, ran and analyzed 151 codes out of 374 papers\nfrom 2014, 2016 and 2018 SIGGRAPH conferences. This analysis shows a clear\nincrease in the number of papers with available and operational research codes\nwith a dependency on the subfields, and indicates a correlation between code\nreplicability and citation count. We further provide an interactive tool to\nexplore our results and evaluation data.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:03:13 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 11:44:46 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Bonneel", "Nicolas", ""], ["Coeurjolly", "David", ""], ["Digne", "Julie", ""], ["Mellado", "Nicolas", ""]]}, {"id": "2005.00912", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad", "title": "Examining Citations of Natural Language Processing Literature", "comments": null, "journal-ref": "Proceedings of the 58th Annual Meeting of the Association of\n  Computational Linguistics (ACL 2020). July 2020. Seattle, USA", "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extracted information from the ACL Anthology (AA) and Google Scholar (GS)\nto examine trends in citations of NLP papers. We explore questions such as: how\nwell cited are papers of different types (journal articles, conference papers,\ndemo papers, etc.)? how well cited are papers from different areas of within\nNLP? etc. Notably, we show that only about 56\\% of the papers in AA are cited\nten or more times. CL Journal has the most cited papers, but its citation\ndominance has lessened in recent years. On average, long papers get almost\nthree times as many citations as short papers; and papers on sentiment\nclassification, anaphora resolution, and entity recognition have the highest\nmedian citations. The analyses presented here, and the associated dataset of\nNLP papers mapped to citations, have a number of uses including: understanding\nhow the field is growing and quantifying the impact of different types of\npapers.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 20:01:59 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mohammad", "Saif M.", ""]]}, {"id": "2005.00962", "submitter": "Saif M. Mohammad Dr.", "authors": "Saif M. Mohammad", "title": "Gender Gap in Natural Language Processing Research: Disparities in\n  Authorship and Citations", "comments": null, "journal-ref": "Proceedings of the 58th Annual Meeting of the Association of\n  Computational Linguistics (ACL 2020). July 2020. Seattle, USA", "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disparities in authorship and citations across gender can have substantial\nadverse consequences not just on the disadvantaged genders, but also on the\nfield of study as a whole. Measuring gender gaps is a crucial step towards\naddressing them. In this work, we examine female first author percentages and\nthe citations to their papers in Natural Language Processing (1965 to 2019). We\ndetermine aggregate-level statistics using an existing manually curated\nauthor--gender list as well as first names strongly associated with a gender.\nWe find that only about 29% of first authors are female and only about 25% of\nlast authors are female. Notably, this percentage has not improved since the\nmid 2000s. We also show that, on average, female first authors are cited less\nthan male first authors, even when controlling for experience and area of\nresearch. Finally, we discuss the ethical considerations involved in automatic\ndemographic analysis.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 01:31:12 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 20:00:08 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Mohammad", "Saif M.", ""]]}, {"id": "2005.01008", "submitter": "Bo-Christer Bj\\\"ork", "authors": "Bo-Christer Bjork and Timo Korkeamaki", "title": "Adoption of the open access business model in scientific journal\n  publishing: A cross-disciplinary study", "comments": "This is the accepted manuscript version (personal version) of an\n  article to be published in the College and Research Libraries Journal\n  (scheduled for the November 2020 issue). CRL is an Open Access journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific journal publishers have over the past twenty-five years rapidly\nconverted to predominantly electronic dissemination, but the reader-pays\nbusiness model continues to dominate the market. Open Access (OA) publishing,\nwhere the articles are freely readable on the net, has slowly increased its\nmarket share to near 20%, but has failed to fulfill the visions of rapid\nproliferation predicted by many early proponents. The growth of OA has also\nbeen very uneven across fields of science. We report market shares of open\naccess in eighteen Scopus-indexed disciplines ranging from 27% (agriculture) to\n7% (business). The differences become far more pronounced for journals\npublished in the four countries, which dominate commercial scholarly publishing\n(US, UK, Germany and the Netherlands). We present contrasting developments\nwithin six academic disciplines. Availability of funding to pay publication\ncharges, pressure from research funding agencies, and the diversity of\ndiscipline-specific research communication cultures arise as potential\nexplanations for the observed differences.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 07:00:44 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bjork", "Bo-Christer", ""], ["Korkeamaki", "Timo", ""]]}, {"id": "2005.02614", "submitter": "Fabian Kirstein", "authors": "Fabian Kirstein, Kyriakos Stefanidis, Benjamin Dittwald, Simon\n  Dutkowski, Sebastian Urbanek, Manfred Hauswirth", "title": "Piveau: A Large-scale Open Data Management Platform based on Semantic\n  Web Technologies", "comments": "16 pages, 2 figures, preprint of an in-use paper at Extended Semantic\n  Web Conference (ESWC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The publication and (re)utilization of Open Data is still facing multiple\nbarriers on technical, organizational and legal levels. This includes\nlimitations in interfaces, search capabilities, provision of quality\ninformation and the lack of definite standards and implementation guidelines.\nMany Semantic Web specifications and technologies are specifically designed to\naddress the publication of data on the web. In addition, many official\npublication bodies encourage and foster the development of Open Data standards\nbased on Semantic Web principles. However, no existing solution for managing\nOpen Data takes full advantage of these possibilities and benfits. In this\npaper, we present our solution \"Piveau\", a fully-fledged Open Data management\nsolution, based on Semantic Web technologies. It harnesses a variety of\nstandards, like RDF, DCAT, DQV, and SKOS, to overcome the barriers in Open Data\npublication. The solution puts a strong focus on assuring data quality and\nscalability. We give a detailed description of the underlying, highly scalable,\nservice-oriented architecture, how we integrated the aforementioned standards,\nand used a triplestore as our primary database. We have evaluated our work in a\ncomprehensive feature comparison to established solutions and through a\npractical application in a production environment, the European Data Portal.\nOur solution is available as Open Source.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 06:46:27 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Kirstein", "Fabian", ""], ["Stefanidis", "Kyriakos", ""], ["Dittwald", "Benjamin", ""], ["Dutkowski", "Simon", ""], ["Urbanek", "Sebastian", ""], ["Hauswirth", "Manfred", ""]]}, {"id": "2005.02726", "submitter": "Metin Orbay", "authors": "Keziban Orbay, Ruben Miranda and Metin Orbay", "title": "Building Journal Impact Factor Quartile into the Assessment of Academic\n  Performance: A Case Study", "comments": "13 pages, 2 figures, 4 tables", "journal-ref": "Participatory Educational Research, 7(2), 1-13 (2020)", "doi": "10.17275/per.20.26.7.2", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to provide information about the Q Concept defined as the\ndivision of journal impact factors into quartiles based on given field\ncategories so that the disadvantages resulting from the direct use of journal\nimpact factors can be eliminated. While the number of \"Original articles\npublished in the Web of Science (WoS) database-indexed journals like SCI, SSCI\nand A&HCI\" is an important indicator for research assessment in Turkey, neither\nthe journal impact factors nor the Q Concept of these papers have been taken\ninto account. Present study analyzes the scientific production of the Amasya\nUniversity researchers in journals indexed in WoS database in the period\n2014-2018 using the Q concept. The share of publications by Q category journals\nas well as the average citations received by the works from Amasya University\nwere compared to the average situation in Turkey and other different countries\nin the world. Results indicate that the articles published by Amasya University\nresearchers were mostly published in low impact factor journals (Q4 journals)\n(36.49%), in fact, only a small share of papers were published in high impact\njournals (14.32% in Q1 journals). The share of papers published in low impact\njournals by researchers from Amasya University is higher than the Turkish\naverage and much higher than the scientific leading countries. The average\ncitations received by papers published in Q1 journals was around six times\nhigher than papers published in Q4 journals (8.92 vs. 1.56), thus papers\npublished in Q1 journals received 30.02% citations despite only 14.32% of the\npapers was published in these journals. The share of papers published which\nwere never cited in WoS was 27.48%, increasing from 9.68% in Q1 to almost half\n(48.10%) in Q4. The study concludes with some suggestions on how and where the\nQ Concept can be used.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 10:56:34 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Orbay", "Keziban", ""], ["Miranda", "Ruben", ""], ["Orbay", "Metin", ""]]}, {"id": "2005.02985", "submitter": "Ana Trisovic", "authors": "Ana Trisovic, Philip Durbin, Tania Schlatter, Gustavo Durand, Sonia\n  Barbosa, Danny Brooke, and Merc\\`e Crosas", "title": "Advancing computational reproducibility in the Dataverse data repository\n  platform", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent reproducibility case studies have raised concerns showing that much of\nthe deposited research has not been reproducible. One of their conclusions was\nthat the way data repositories store research data and code cannot fully\nfacilitate reproducibility due to the absence of a runtime environment needed\nfor the code execution. New specialized reproducibility tools provide\ncloud-based computational environments for code encapsulation, thus enabling\nresearch portability and reproducibility. However, they do not often enable\nresearch discoverability, standardized data citation, or long-term archival\nlike data repositories do. This paper addresses the shortcomings of data\nrepositories and reproducibility tools and how they could be overcome to\nimprove the current lack of computational reproducibility in published and\narchived research outputs.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:39:49 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 21:10:16 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Trisovic", "Ana", ""], ["Durbin", "Philip", ""], ["Schlatter", "Tania", ""], ["Durand", "Gustavo", ""], ["Barbosa", "Sonia", ""], ["Brooke", "Danny", ""], ["Crosas", "Merc\u00e8", ""]]}, {"id": "2005.03324", "submitter": "Pranab K. Muhuri Dr.", "authors": "Aparna Basu, Deepika Malhotra, Taniya Seth, and Pranab K. Muhuri", "title": "Global Distribution of Google Scholar Citations: A Size-independent\n  Institution-based Analysis", "comments": null, "journal-ref": "Journal of Scientometric Research 8, no. 2 (2019): 72-78", "doi": "10.5530/jscires.8.2.12", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most currently available schemes for performance based ranking of\nUniversities or Research organizations, such as, Quacarelli Symonds (QS), Times\nHigher Education (THE), Shanghai University based All Research of World\nUniversities (ARWU) use a variety of criteria that include productivity,\ncitations, awards, reputation, etc., while Leiden and Scimago use only\nbibliometric indicators. The research performance evaluation in the aforesaid\ncases is based on bibliometric data from Web of Science or Scopus, which are\ncommercially available priced databases. The coverage includes peer reviewed\njournals and conference proceedings. Google Scholar (GS) on the other hand,\nprovides a free and open alternative to obtaining citations of papers available\non the net, (though it is not clear exactly which journals are covered.)\nCitations are collected automatically from the net and also added to self\ncreated individual author profiles under Google Scholar Citations (GSC). This\ndata was used by Webometrics Lab, Spain to create a ranked list of 4000+\ninstitutions in 2016, based on citations from only the top 10 individual GSC\nprofiles in each organization. (GSC excludes the top paper for reasons\nexplained in the text; the simple selection procedure makes the ranked list\nsize-independent as claimed by the Cybermetrics Lab). Using this data\n(Transparent Ranking TR, 2016), we find the regional and country wise\ndistribution of GS-TR Citations. The size independent ranked list is subdivided\ninto deciles of 400 institutions each and the number of institutions and\ncitations of each country obtained for each decile. We test for correlation\nbetween institutional ranks between GS TR and the other ranking schemes for the\ntop 20 institutions.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 08:47:21 GMT"}], "update_date": "2020-06-28", "authors_parsed": [["Basu", "Aparna", ""], ["Malhotra", "Deepika", ""], ["Seth", "Taniya", ""], ["Muhuri", "Pranab K.", ""]]}, {"id": "2005.04308", "submitter": "Jian Xu", "authors": "Jian Xu, Sunkyu Kim, Min Song, Minbyul Jeong, Donghyeon Kim, Jaewoo\n  Kang, Justin F. Rousseau, Xin Li, Weijia Xu, Vetle I. Torvik, Yi Bu, Chongyan\n  Chen, Islam Akef Ebeid, Daifeng Li, Ying Ding", "title": "Building a PubMed knowledge graph", "comments": "19 pages, 5 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PubMed is an essential resource for the medical domain, but useful concepts\nare either difficult to extract or are ambiguated, which has significantly\nhindered knowledge discovery. To address this issue, we constructed a PubMed\nknowledge graph (PKG) by extracting bio-entities from 29 million PubMed\nabstracts, disambiguating author names, integrating funding data through the\nNational Institutes of Health (NIH) ExPORTER, collecting affiliation history\nand educational background of authors from ORCID, and identifying fine-grained\naffiliation data from MapAffil. Through the integration of the credible\nmulti-source data, we could create connections among the bio-entities, authors,\narticles, affiliations, and funding. Data validation revealed that the BioBERT\ndeep learning method of bio-entity extraction significantly outperformed the\nstate-of-the-art models based on the F1 score (by 0.51%), with the author name\ndisambiguation (AND) achieving a F1 score of 98.09%. PKG can trigger broader\ninnovations, not only enabling us to measure scholarly impact, knowledge usage,\nand knowledge transfer, but also assisting us in profiling authors and\norganizations based on their connections with bio-entities. The PKG is freely\navailable on Figshare (https://figshare.com/s/6327a55355fc2c99f3a2, simplified\nversion that exclude PubMed raw data) and TACC website\n(http://er.tacc.utexas.edu/datasets/ped, full version).\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 22:36:49 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 06:50:39 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Xu", "Jian", ""], ["Kim", "Sunkyu", ""], ["Song", "Min", ""], ["Jeong", "Minbyul", ""], ["Kim", "Donghyeon", ""], ["Kang", "Jaewoo", ""], ["Rousseau", "Justin F.", ""], ["Li", "Xin", ""], ["Xu", "Weijia", ""], ["Torvik", "Vetle I.", ""], ["Bu", "Yi", ""], ["Chen", "Chongyan", ""], ["Ebeid", "Islam Akef", ""], ["Li", "Daifeng", ""], ["Ding", "Ying", ""]]}, {"id": "2005.04398", "submitter": "Claudiu Herteliu", "authors": "Catalin Emilian Boja, Claudiu Herteliu, Marian Dardala, Bogdan Vasile\n  Ileanu", "title": "Day of the week submission effect for accepted papers in Physica A, PLOS\n  ONE, Nature and Cell", "comments": "113 pages, 5 figures, 7 tables, 42 references, 4 SI figures, 3 SI\n  tables, 4 SI sections", "journal-ref": "Published, Scientometrics, Volume 117, Issue 2, 2018, pp. 887-918", "doi": "10.1007/s11192-018-2911-7", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The particular day of the week when an event occurs seems to have unexpected\nconsequences. For example, the day of the week when a paper is submitted to a\npeer reviewed journal correlates with whether that paper is accepted. Using an\neconometric analysis (a mix of log-log and semi-log based on undated and panel\nstructured data) we find that more papers are submitted to certain peer review\njournals on particular weekdays than others, with fewer papers being submitted\non weekends. Seasonal effects, geographical information as well as potential\nchanges over time are examined. This finding rests on a large (178 000) and\nreliable sample; the journals polled are broadly recognized (Nature, Cell, PLOS\nONE and Physica A). Day of the week effect in the submission of accepted papers\nshould be of interest to many researchers, editors and publishers, and perhaps\nalso to managers and psychologists.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:23:44 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Boja", "Catalin Emilian", ""], ["Herteliu", "Claudiu", ""], ["Dardala", "Marian", ""], ["Ileanu", "Bogdan Vasile", ""]]}, {"id": "2005.04432", "submitter": "Michele Starnini", "authors": "Marta Tuninetti, Alberto Aleta, Daniela Paolotti, Yamir Moreno, and\n  Michele Starnini", "title": "Prediction of scientific collaborations through multiplex interaction\n  networks", "comments": null, "journal-ref": "Phys. Rev. Research 2, 042029 (2020)", "doi": "10.1103/PhysRevResearch.2.042029", "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction algorithms can help to understand the structure and dynamics\nof scientific collaborations and the evolution of Science. However, available\nalgorithms based on similarity between nodes of collaboration networks are\nbounded by the limited amount of links present in these networks. In this work,\nwe reduce the latter intrinsic limitation by generalizing the Adamic-Adar\nmethod to multiplex networks composed by an arbitrary number of layers, that\nencode diverse forms of scientific interactions. We show that the new metric\noutperforms other single-layered, similarity-based scores and that scientific\ncredit, represented by citations, and common interests, measured by the usage\nof common keywords, can be predictive of new collaborations. Our work paves the\nway for a deeper understanding of the dynamics driving scientific\ncollaborations, and provides a new algorithm for link prediction in multiplex\nnetworks that can be applied to a plethora of systems.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 12:48:51 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Tuninetti", "Marta", ""], ["Aleta", "Alberto", ""], ["Paolotti", "Daniela", ""], ["Moreno", "Yamir", ""], ["Starnini", "Michele", ""]]}, {"id": "2005.04512", "submitter": "Diego Amancio", "authors": "Ana C. M. Brito, Filipi N. Silva, Henrique F. de Arruda, Cesar H.\n  Comin, Diego R. Amancio and Luciano da F. Costa", "title": "Classification of abrupt changes along viewing profiles of scientific\n  articles", "comments": null, "journal-ref": "Journal of Informetrics, 2021", "doi": "10.1016/j.joi.2021.101158", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the expansion of electronic publishing, a new dynamics of scientific\narticles dissemination was initiated. Nowadays, many works are widely\ndisseminated even before publication, in the form of preprints. Another\nimportant new element concerns the views of published articles. Thanks to the\navailability of respective data by some journals, such as PLoS ONE, it became\npossible to develop investigations on how scientific works are viewed along\ntime, often before the first citations appear. This provides the main theme of\nthe present work. More specifically, our research was motivated by preliminary\nobservations that the view profiles along time tend to present a piecewise\nlinear nature. A methodology was then delineated in order to identify the main\nsegments in the view profiles, which allowed several related measurements to be\nderived. In particular, we focused on the inclination and length of each\nsubsequent segment. Basic statistics indicated that the inclination can vary\nsubstantially along subsequent segments, while the segment lengths resulted\nmore stable. Complementary joint statistics analysis, considering pairwise\ncorrelations, provided further information about the properties of the views.\nIn order to better understand the view profiles, we performed respective\nmultivariate statistical analysis, including principal component analysis and\nhierarchical clustering. The results suggest that a portion of the polygonal\nviews are organized into clusters or groups. These groups were characterized in\nterms of prototypes indicating the relative increase or decrease along\nsubsequent segments. Four respective distinct models were then developed for\nrepresenting the observed segments. It was found that models incorporating\njoint dependencies between the properties of the segments provided the most\naccurate results among the considered alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 20:50:44 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 14:06:03 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 03:41:34 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Brito", "Ana C. M.", ""], ["Silva", "Filipi N.", ""], ["de Arruda", "Henrique F.", ""], ["Comin", "Cesar H.", ""], ["Amancio", "Diego R.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "2005.04534", "submitter": "Kumar Ravi", "authors": "Vishal Vyas, Kumar Ravi, Vadlamani Ravi, V.Uma, Srirangaraj Setlur,\n  Venu Govindaraju", "title": "Article citation study: Context enhanced citation sentiment detection", "comments": "39 pages, 12 Tables, 5 Figures, Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation sentimet analysis is one of the little studied tasks for\nscientometric analysis. For citation analysis, we developed eight datasets\ncomprising citation sentences, which are manually annotated by us into three\nsentiment polarities viz. positive, negative, and neutral. Among eight\ndatasets, three were developed by considering the whole context of citations.\nFurthermore, we proposed an ensembled feature engineering method comprising\nword embeddings obtained for texts, parts-of-speech tags, and dependency\nrelationships together. Ensembled features were considered as input to deep\nlearning based approaches for citation sentiment classification, which is in\nturn compared with Bag-of-Words approach. Experimental results demonstrate that\ndeep learning is useful for higher number of samples, whereas support vector\nmachine is the winner for smaller number of samples. Moreover, context-based\nsamples are proved to be more effective than context-less samples for citation\nsentiment analysis.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 00:27:19 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Vyas", "Vishal", ""], ["Ravi", "Kumar", ""], ["Ravi", "Vadlamani", ""], ["Uma", "V.", ""], ["Setlur", "Srirangaraj", ""], ["Govindaraju", "Venu", ""]]}, {"id": "2005.04702", "submitter": "Daniel Ucko", "authors": "Daniel Ucko", "title": "Peer Review: Objectivity, Anonymity, Trust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.hist-ph cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This dissertation is focused on the role of objectivity in peer review.\nThrough an examination of aspects of peer review including anonymity, trust,\nexpertise, and the question of who has standing to evaluate research, we find\nthat objectivity in peer review differs significantly from other uses of the\nterm objectivity in science. In peer review it is not required for this\nobjectivity to have correspondence to an outside world, instead it is enough\nfor it to operate inside the \"rules\" of the community. Neither is the\nobjectivity here empirical in the sense of using data about the scientific\nproblem in question. Rather, the objectivity is one of judgment, cleaving to\nthe epistemological standards of a community that are formed by background\nassumptions and beliefs. As a consequence, we highlight the role of\nsubjectivity in what is usually taken as a practice of objectivity, and arrive\nat the insight that objectivity is not defined by one core value, but a balance\nof transparency, confidentiality, trust, representation, and living up to\ncommunity standards. As such, objectivity in peer review is a highly specific\nsense of the term that is not reducible to that used in other aspects of\nscientific practice.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 15:48:40 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ucko", "Daniel", ""]]}, {"id": "2005.04793", "submitter": "George Chacko", "authors": "Sitaram Devarakonda, James Bradley, Dmitriy Korobskiy, Tandy Warnow\n  and George Chacko", "title": "Frequently Co-cited Publications: Features and Kinetics", "comments": null, "journal-ref": null, "doi": "10.1162/qss_a_00075", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-citation measurements can reveal the extent to which a concept\nrepresenting a novel combination of existing ideas evolves towards a specialty.\nThe strength of co-citation is represented by its frequency, which accumulates\nover time. Of interest is whether underlying features associated with the\nstrength of co-citation can be identified. We use the proximal citation network\nfor a given pair of articles (x, y) to compute theta, an a priori estimate of\nthe probability of co-citation between x and y, prior to their first\nco-citation.Thus, low values for theta reflect pairs of articles for which\nco-citation is presumed less likely. We observe that co-citation frequencies\nare a composite of power-law and lognormal distributions, and that very high\nco-citation frequencies are more likely to be composed of pairs with low values\nof theta, reflecting the impact of a novel combination of ideas. Furthermore,\nwe note that the occurrence of a direct citation between two members of a\nco-cited pair increases with co-citation frequency. Finally, we identify cases\nof frequently co-cited publications that accumulate co-citations after an\nextended period of dormancy.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 21:37:18 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Devarakonda", "Sitaram", ""], ["Bradley", "James", ""], ["Korobskiy", "Dmitriy", ""], ["Warnow", "Tandy", ""], ["Chacko", "George", ""]]}, {"id": "2005.05389", "submitter": "Lawrence Smolinsky", "authors": "Lawrence Smolinsky, Daniel S. Sage, Aaron J. Lercher, and Aaron Cao", "title": "Citations versus expert opinions: Citation analysis of Featured Reviews\n  of the American Mathematical Society", "comments": "21 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer review and citation metrics are two means of gauging the value of\nscientific research, but the lack of publicly available peer review data makes\nthe comparison of these methods difficult. Mathematics can serve as a useful\nlaboratory for considering these questions because as an exact science, there\nis a narrow range of reasons for citations. In mathematics, virtually all\npublished articles are post-publication reviewed by mathematicians in\nMathematical Reviews (MathSciNet) and so the data set was essentially the Web\nof Science mathematics publications from 1993 to 2004. For a decade, especially\nimportant articles were singled out in Mathematical Reviews for featured\nreviews. In this study, we analyze the bibliometrics of elite articles selected\nby peer review and by citation count. We conclude that the two notions of\nsignificance described by being a featured review article and being highly\ncited are distinct. This indicates that peer review and citation counts give\nlargely independent determinations of highly distinguished articles. We also\nconsider whether hiring patterns of subfields and mathematicians' interest in\nsubfields reflect subfields of featured review or highly cited articles. We\nreexamine data from two earlier studies in light of our methods for\nimplications on the peer review/citation count relationship to a diversity of\ndisciplines.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 19:18:08 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 06:14:59 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Smolinsky", "Lawrence", ""], ["Sage", "Daniel S.", ""], ["Lercher", "Aaron J.", ""], ["Cao", "Aaron", ""]]}, {"id": "2005.05471", "submitter": "Lawrence Smolinsky", "authors": "Lawrence Smolinsky and Aaron J. Lercher", "title": "Co-author weighting in bibliometric methodology and subfields of a\n  scientific discipline", "comments": "11 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative work and co-authorship are fundamental to the advancement of\nmodern science. However, it is not clear how collaboration should be measured\nin achievement-based metrics. Co-author weighted credit introduces distortions\ninto the bibliometric description of a discipline. It puts great weight on\ncollaboration - not based on the results of collaboration - but purely because\nof the existence of collaborations. In terms of publication and citation\nimpact, it artificially favors some subdisciplines. In order to understand how\ncredit is given in a co-author weighted system (like the NRC's method), we\nintroduced credit spaces. We include a study of the discipline of physics to\nillustrate the method. Indicators are introduced to measure the proportion of a\ncredit space awarded to a subfield or a set of authors.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 22:40:21 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Smolinsky", "Lawrence", ""], ["Lercher", "Aaron J.", ""]]}, {"id": "2005.05474", "submitter": "Micah Altman", "authors": "Micah Altman, Karen Cariani, Bradley Daigle, Christie Moffatt, Sibyl\n  Schaefer, Bethany Scott, Lauren Work", "title": "2020 NDSA Agenda for Digital Stewardship", "comments": null, "journal-ref": null, "doi": "10.17605/OSF.IO/BCETD", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The NDSA Agenda is a comprehensive overview of the state of global digital\npreservation. It casts its eye over current research trends, grants, projects,\nand various efforts spanning the preservation ecosystem. The agenda identifies\nsuccesses and ongoing challenges in addition to providing some tangible\nrecommendations to both researcher and practitioner alike. As both an overview\nand comprehensive dive into digital preservation issues, the audience ranges\nfrom high level to hands on experts. Funders can use this report as a signpost\nfor the overall state of the profession.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 22:49:05 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Altman", "Micah", ""], ["Cariani", "Karen", ""], ["Daigle", "Bradley", ""], ["Moffatt", "Christie", ""], ["Schaefer", "Sibyl", ""], ["Scott", "Bethany", ""], ["Work", "Lauren", ""]]}, {"id": "2005.05954", "submitter": "Md. Tawkat Islam Khondaker", "authors": "Junaed Younus Khan, Md. Tawkat Islam Khondaker, Iram Tazim Hoque,\n  Hamada Al-Absi, Mohammad Saifur Rahman, Tanvir Alam, M. Sohel Rahman", "title": "COVID-19Base: A knowledgebase to explore biomedical entities related to\n  COVID-19", "comments": "10 pages, 3 figures", "journal-ref": "JMIR Med Inform 2020;8(11):e21648", "doi": "10.2196/21648", "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are presenting COVID-19Base, a knowledgebase highlighting the biomedical\nentities related to COVID-19 disease based on literature mining. To develop\nCOVID-19Base, we mine the information from publicly available scientific\nliterature and related public resources. We considered seven topic-specific\ndictionaries, including human genes, human miRNAs, human lncRNAs, diseases,\nProtein Databank, drugs, and drug side effects, are integrated to mine all\nscientific evidence related to COVID-19. We have employed an automated\nliterature mining and labeling system through a novel approach to measure the\neffectiveness of drugs against diseases based on natural language processing,\nsentiment analysis, and deep learning. To the best of our knowledge, this is\nthe first knowledgebase dedicated to COVID-19, which integrates such large\nvariety of related biomedical entities through literature mining. Proper\ninvestigation of the mined biomedical entities along with the identified\ninteractions among those, reported in COVID-19Base, would help the research\ncommunity to discover possible ways for the therapeutic treatment of COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:55:00 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Khan", "Junaed Younus", ""], ["Khondaker", "Md. Tawkat Islam", ""], ["Hoque", "Iram Tazim", ""], ["Al-Absi", "Hamada", ""], ["Rahman", "Mohammad Saifur", ""], ["Alam", "Tanvir", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "2005.06259", "submitter": "Johannes Stegmann Dr.", "authors": "Johannes Stegmann", "title": "MeSH descriptors indicate the knowledge growth in the\n  SARS-CoV-2/COVID-19 pandemic", "comments": "6 pages, 3 fgures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific papers dealing with the novel betacoronavirus SARS-CoV-2 and\nthe coronavirus disease 2019 (COVID-19) caused by this virus, published in 2020\nand recorded in the database PUBMED, were retrieved on April 27, 2020. About\n20\\% of the records contain Medical Subject Headings (MeSH), keywords assigned\nto records in the course of the indexing process in order to summarise the\narticles' contents. The temporal sequence of the first occurrences of the\nkeywords was determined, thus giving insight into the growth of the knowledge\nbase of the pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 11:40:51 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Stegmann", "Johannes", ""]]}, {"id": "2005.06303", "submitter": "Jens Peter Andersen", "authors": "Jens Peter Andersen, Mathias Wullum Nielsen, Nicole L. Simone, Resa E.\n  Lewiss, Reshma Jagsi", "title": "Meta-Research: COVID-19 medical papers have fewer women first authors\n  than expected", "comments": "Submitted to eLife. First revision", "journal-ref": "eLife 2020;9:e58807", "doi": "10.7554/eLife.58807", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The COVID-19 pandemic has resulted in school closures and distancing\nrequirements that have disrupted both work and family life for many. Concerns\nexist that these disruptions caused by the pandemic may not have influenced men\nand women researchers equally. Many medical journals have published papers on\nthe pandemic, which were generated by researchers facing the challenges of\nthese disruptions. Here we report the results of an analysis that compared the\ngender distribution of authors on 1,893 medical papers related to the pandemic\nwith that on papers published in the same journals in 2019, for papers with\nfirst authors and last authors from the United States. Using mixed-effects\nregression models, we estimated that the proportion of COVID-19 papers with a\nwoman first author was 19% lower than that for papers published in the same\njournals in 2019, while our comparisons for last authors and overall proportion\nof women authors per paper were inconclusive. A closer examination suggested\nthat women's representation as first authors of COVID-19 research was\nparticularly low for papers published in March and April 2020. Our findings are\nconsistent with the idea that the research productivity of women, especially\nearly-career women, has been affected more than the research productivity of\nmen.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 13:23:07 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 15:25:49 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 18:09:07 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Andersen", "Jens Peter", ""], ["Nielsen", "Mathias Wullum", ""], ["Simone", "Nicole L.", ""], ["Lewiss", "Resa E.", ""], ["Jagsi", "Reshma", ""]]}, {"id": "2005.06605", "submitter": "Oren Halvani", "authors": "Oren Halvani and Lukas Graner", "title": "POSNoise: An Effective Countermeasure Against Topic Biases in Authorship\n  Analysis", "comments": "Paper has been accepted for publication in: The 16th International\n  Conference on Availability, Reliability and Security (ARES 2021)", "journal-ref": null, "doi": "10.1145/3465481.3470050", "report-no": null, "categories": "cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship verification (AV) is a fundamental research task in digital text\nforensics, which addresses the problem of whether two texts were written by the\nsame person. In recent years, a variety of AV methods have been proposed that\nfocus on this problem and can be divided into two categories: The first\ncategory refers to such methods that are based on explicitly defined features,\nwhere one has full control over which features are considered and what they\nactually represent. The second category, on the other hand, relates to such AV\nmethods that are based on implicitly defined features, where no control\nmechanism is involved, so that any character sequence in a text can serve as a\npotential feature. However, AV methods belonging to the second category bear\nthe risk that the topic of the texts may bias their classification predictions,\nwhich in turn may lead to misleading conclusions regarding their results. To\ntackle this problem, we propose a preprocessing technique called POSNoise,\nwhich effectively masks topic-related content in a given text. In this way, AV\nmethods are forced to focus on such text units that are more related to the\nwriting style. Our empirical evaluation based on six AV methods (falling into\nthe second category) and seven corpora shows that POSNoise leads to better\nresults compared to a well-known topic masking approach in 34 out of 42 cases,\nwith an increase in accuracy of up to 10%.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 21:10:24 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 09:16:06 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Halvani", "Oren", ""], ["Graner", "Lukas", ""]]}, {"id": "2005.06611", "submitter": "Dominique Mercier", "authors": "Dominique Mercier, Syed Tahseen Raza Rizvi, Vikas Rajashekar, Andreas\n  Dengel, Sheraz Ahmed", "title": "ImpactCite: An XLNet-based method for Citation Impact Analysis", "comments": "12 pages (10 + 2 references), 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citations play a vital role in understanding the impact of scientific\nliterature. Generally, citations are analyzed quantitatively whereas\nqualitative analysis of citations can reveal deeper insights into the impact of\na scientific artifact in the community. Therefore, citation impact analysis\n(which includes sentiment and intent classification) enables us to quantify the\nquality of the citations which can eventually assist us in the estimation of\nranking and impact. The contribution of this paper is two-fold. First, we\nbenchmark the well-known language models like BERT and ALBERT along with\nseveral popular networks for both tasks of sentiment and intent classification.\nSecond, we provide ImpactCite, which is XLNet-based method for citation impact\nanalysis. All evaluations are performed on a set of publicly available citation\nanalysis datasets. Evaluation results reveal that ImpactCite achieves a new\nstate-of-the-art performance for both citation intent and sentiment\nclassification by outperforming the existing approaches by 3.44% and 1.33% in\nF1-score. Therefore, we emphasize ImpactCite (XLNet-based solution) for both\ntasks to better understand the impact of a citation. Additional efforts have\nbeen performed to come up with CSC-Clean corpus, which is a clean and reliable\ndataset for citation sentiment classification.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 08:31:54 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mercier", "Dominique", ""], ["Rizvi", "Syed Tahseen Raza", ""], ["Rajashekar", "Vikas", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2005.06989", "submitter": "Fairouz Malek", "authors": "Juan Pedro Araque Espinosa, Gabriel Baldi Levcovitz, Riccardo-Maria\n  Bianchi, Ian Brock, Tancredi Carli, Nuno Filipe Castro, Alessandra Ciocio,\n  Maurizio Colautti, Ana Carolina Da Silva Menezes, Gabriel De Oliveira da\n  Fonseca, Leandro Domingues Macedo Alves, Andreas Hoecker, Bruno Lange Ramos,\n  Gabriela Lemos L\\'ucidi Pinh\\~ao, Carmen Maidantchik, Fairouz Malek, Robert\n  McPherson, Gianluca Picco, Marcelo Teixeira Dos Santos", "title": "A continuous integration and web framework in support of the ATLAS\n  Publication Process", "comments": "22 pages in total,11 figures, submitted to JINST. All figures\n  including auxiliary figures are available at\n  https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/GENR-2018-01/", "journal-ref": null, "doi": "10.1088/1748-0221/16/05/T05006", "report-no": "CERN-OPEN-2020-007", "categories": "cs.DL hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ATLAS collaboration defines methods, establishes procedures, and\norganises advisory groups to manage the publication processes of scientific\npapers, conference papers, and public notes. All stages are managed through web\nsystems, computing programs, and tools that are designed and developed by the\ncollaboration. A framework called FENCE is integrated into the CERN GitLab\nsoftware repository, to automatically configure workspaces where each analysis\ncan be documented by the analysis team and managed by the relevant\ncoordinators. Continuous integration is used to guide the writers in applying\nconsistent and correct formatting when preparing papers to be submitted to\nscientific journals. Additional software assures the correctness of other\naspects of each paper, such as the lists of collaboration authors, funding\nagencies, and foundations. The framework and the workflow therein provide\nautomatic and easy support to the researchers and facilitates each phase of the\npublication process, allowing authors to focus on the article contents. The\nframework and its integration with the most up to date and efficient tools has\nconsequently provided a more professional and efficient automatized work\nenvironment to the whole collaboration.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 13:59:02 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 11:35:09 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 14:15:29 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Espinosa", "Juan Pedro Araque", ""], ["Levcovitz", "Gabriel Baldi", ""], ["Bianchi", "Riccardo-Maria", ""], ["Brock", "Ian", ""], ["Carli", "Tancredi", ""], ["Castro", "Nuno Filipe", ""], ["Ciocio", "Alessandra", ""], ["Colautti", "Maurizio", ""], ["Menezes", "Ana Carolina Da Silva", ""], ["da Fonseca", "Gabriel De Oliveira", ""], ["Alves", "Leandro Domingues Macedo", ""], ["Hoecker", "Andreas", ""], ["Ramos", "Bruno Lange", ""], ["Pinh\u00e3o", "Gabriela Lemos L\u00facidi", ""], ["Maidantchik", "Carmen", ""], ["Malek", "Fairouz", ""], ["McPherson", "Robert", ""], ["Picco", "Gianluca", ""], ["Santos", "Marcelo Teixeira Dos", ""]]}, {"id": "2005.07349", "submitter": "Mikhail Simkin", "authors": "M.V. Simkin", "title": "Success in creative careers depends little on product quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent article Janosov, Battiston, & Sinatra report that they\nseparated the inputs of talent and luck in creative careers. They build on the\nprevious work of Sinatra et al which introduced the Q-model. Under the model\nthe popularity of different elements of culture is a product of two factors: a\nrandom factor and a Qfactor, or talent. The latter is fixed for an individual\nbut randomly distributed among different people. This way they explain how some\nindividuals can consistently produce high-impact work. They extract the\nQ-factors for different scientists, writers, and movie makers from statistical\ndata on popularity of their work. However, in their article they reluctantly\nstate that there is little correlation between popularity and quality ratings\nof of books and movies (correlation coefficients 0.022 and 0.15). I analyzed\nthe data of the original Q-factor article and obtained a correlation between\nthe citation-based Q-factor and Nobel Prize winning of merely 0.19. I also\nbriefly review few other experiments that found a meager, sometimes even\nnegative, correlation between popularity and quality of cultural products. I\nconclude that, if there is an ability associated with a high Q-factor it should\nbe more of a marketing ability than an ability to produce a higher quality\nproduct. Janosov,\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 03:58:08 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Simkin", "M. V.", ""]]}, {"id": "2005.07382", "submitter": "Ya-Hui An", "authors": "Ya-Hui An and Muthu Kumar Chandresekaran and Min-Yen Kan and Yan Fu", "title": "The MUIR Framework: Cross-Linking MOOC Resources to Enhance Discussion\n  Forums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New learning resources are created and minted in Massive Open Online Courses\nevery week -- new videos, quizzes, assessments and discussion threads are\ndeployed and interacted with -- in the era of on-demand online learning.\nHowever, these resources are often artificially siloed between platforms and\nartificial web application models. Facilitating the linking between such\nresources facilitates learning and multimodal understanding, bettering\nlearners' experience.\n  We create a framework for MOOC Uniform Identifier for Resources (MUIR). MUIR\nenables applications to refer and link to such resources in a cross-platform\nway, allowing the easy minting of identifiers to MOOC resources, akin to\n#hashtags. We demonstrate the feasibility of this approach to the automatic\nidentification, linking and resolution -- a task known as Wikification -- of\nlearning resources mentioned on MOOC discussion forums, from a harvested\ncollection of 100K+ resources. Our Wikification system achieves a high initial\nrate of 54.6% successful resolutions on key resource mentions found in\ndiscussion forums, demonstrating the utility of the MUIR framework. Our\nanalysis on this new problem shows that context is a key factor in determining\nthe correct resolution of such mentions.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 07:09:01 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["An", "Ya-Hui", ""], ["Chandresekaran", "Muthu Kumar", ""], ["Kan", "Min-Yen", ""], ["Fu", "Yan", ""]]}, {"id": "2005.07544", "submitter": "Frances Skinner", "authors": "Frances M. Skinner, Iouli E. Gordon, Christian Hill, Robert J.\n  Hargreaves, Kelly E. Lockhart and Laurence S. Rothman", "title": "Referencing Sources of Molecular Spectroscopic Data in the Era of Data\n  Science: Application to the HITRAN and AMBDAS Databases", "comments": "11 pages, 5 figures, already published online at\n  https://doi.org/10.3390/atoms8020016", "journal-ref": "Journal: Atoms Special Issue Development and Perspectives of\n  Atomic and Molecular Databases, Volume 8, Issue 2, 30 April 2020", "doi": "10.3390/atoms8020016", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The application described has been designed to create bibliographic entries\nin large databases with diverse sources automatically, which reduces both the\nfrequency of mistakes and the workload for the administrators. This new system\nuniquely identifies each reference from its digital object identifier (DOI) and\nretrieves the corresponding bibliographic information from any of several\nonline services, including the SAO/NASA Astrophysics Data Systems (ADS) and\nCrossRef APIs. Once parsed into a relational database, the software is able to\nproduce bibliographies in any of several formats, including HTML and BibTeX,\nfor use on websites or printed articles. The application is provided\nfree-of-charge for general use by any scientific database. The power of this\napplication is demonstrated when used to populate reference data for the HITRAN\nand AMBDAS databases as test cases. HITRAN contains data that is provided by\nresearchers and collaborators throughout the spectroscopic community. These\ncontributors are accredited for their contributions through the bibliography\nproduced alongside the data returned by an online search in HITRAN. Prior to\nthe work presented here, HITRAN and AMBDAS created these bibliographies\nmanually, which is a tedious, time-consuming and error-prone process. The\ncomplete code for the new referencing system can be found at\n\\url{https://github.com/hitranonline/refs}.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 13:48:37 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Skinner", "Frances M.", ""], ["Gordon", "Iouli E.", ""], ["Hill", "Christian", ""], ["Hargreaves", "Robert J.", ""], ["Lockhart", "Kelly E.", ""], ["Rothman", "Laurence S.", ""]]}, {"id": "2005.08254", "submitter": "Diego Amancio", "authors": "Jorge A. V. Tohalino and Laura V. C. Quispe and Diego R. Amancio", "title": "Analyzing the relationship between text features and research proposal\n  productivity", "comments": "Experimental results and text features analysis have been updated and\n  improved", "journal-ref": "Scientometrics, 2021", "doi": "10.1007/s11192-021-03926-x", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the output of research grants is of considerable relevance to\nresearch funding bodies, scientific entities and government agencies. In this\nstudy, we investigate whether text features extracted from projects title and\nabstracts are able to identify productive grants. Our analysis was conducted in\nthree distinct areas, namely Medicine, Dentistry and Veterinary Medicine.\nTopical and complexity text features were used to identify predictors of\nproductivity. The results indicate that there is a statistically significant\nrelationship between text features and grants productivity, however such a\ndependence is weak. A feature relevance analysis revealed that the abstract\ntext length and metrics derived from lexical diversity are among the most\ndiscriminative features. We also found that the prediction accuracy has a\ndependence on the considered project language and that topical features are\nmore discriminative than text complexity measurements. Our findings suggest\nthat text features should be used in combination with other features to assist\nthe identification of relevant research ideas.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 13:51:55 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 23:33:57 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Tohalino", "Jorge A. V.", ""], ["Quispe", "Laura V. C.", ""], ["Amancio", "Diego R.", ""]]}, {"id": "2005.08753", "submitter": "Henk Moed", "authors": "Nuria Bautista-Puig, Carmen Lopez-Illescas, Felix de Moya-Anegon,\n  Vicente Guerrero-Bote, and Henk F. Moed", "title": "Do journals flipping to Gold Open Access show an OA Citation or\n  Publication Advantage?", "comments": "Author copy of a manuscript accepted for publication in the journal\n  Scientometrics (18 May 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The effects of Open Access (OA) upon journal performance are investigated.\nThe key research question holds: How does the citation impact and publication\noutput of journals switching ('flipping') from non-OA to Gold-OA develop after\ntheir switch to Gold-OA? A review is given of the literature, with an emphasis\non studies dealing with flipping journals. Two study sets with 119 and 100\nflipping journals, derived from two different OA data sources (DOAJ and OAD),\nare compared with two control groups, one based on a standard bibliometric\ncriterion, and a second controlling for a journal's national orientation.\nComparing post-switch indicators with pre-switch ones in paired T-tests,\nevidence was obtained of an OA Citation advantage but not of an OA Publication\nAdvantage. Shifts in the affiliation countries of publishing and citing authors\nare characterized in terms of countries' income class and geographical world\nregion. Suggestions are made for qualitative follow-up studies to obtain more\ninsight into OA flipping or reverse-flipping\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:26:56 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bautista-Puig", "Nuria", ""], ["Lopez-Illescas", "Carmen", ""], ["de Moya-Anegon", "Felix", ""], ["Guerrero-Bote", "Vicente", ""], ["Moed", "Henk F.", ""]]}, {"id": "2005.08823", "submitter": "Wolf-Tilo Balke", "authors": "Hermann Kroll, Jan Pirklbauer, Johannes Ruthmann, Wolf-Tilo Balke", "title": "A Semantically Enriched Dataset based on Biomedical NER for the COVID19\n  Open Research Dataset Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research into COVID-19 is a big challenge and highly relevant at the moment.\nNew tools are required to assist medical experts in their research with\nrelevant and valuable information. The COVID-19 Open Research Dataset Challenge\n(CORD-19) is a \"call to action\" for computer scientists to develop these\ninnovative tools. Many of these applications are empowered by entity\ninformation, i. e. knowing which entities are used within a sentence. For this\npaper, we have developed a pipeline upon the latest Named Entity Recognition\ntools for Chemicals, Diseases, Genes and Species. We apply our pipeline to the\nCOVID-19 research challenge and share the resulting entity mentions with the\ncommunity.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:56:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kroll", "Hermann", ""], ["Pirklbauer", "Jan", ""], ["Ruthmann", "Johannes", ""], ["Balke", "Wolf-Tilo", ""]]}, {"id": "2005.09133", "submitter": "Boxiang Liu", "authors": "Boxiang Liu and Liang Huang", "title": "NEJM-enzh: A Parallel Corpus for English-Chinese Translation in the\n  Biomedical Domain", "comments": "11 pages, 11 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine translation requires large amounts of parallel text. While such\ndatasets are abundant in domains such as newswire, they are less accessible in\nthe biomedical domain. Chinese and English are two of the most widely spoken\nlanguages, yet to our knowledge a parallel corpus in the biomedical domain does\nnot exist for this language pair. In this study, we develop an effective\npipeline to acquire and process an English-Chinese parallel corpus, consisting\nof about 100,000 sentence pairs and 3,000,000 tokens on each side, from the New\nEngland Journal of Medicine (NEJM). We show that training on out-of-domain data\nand fine-tuning with as few as 4,000 NEJM sentence pairs improve translation\nquality by 25.3 (13.4) BLEU for en$\\to$zh (zh$\\to$en) directions. Translation\nquality continues to improve at a slower pace on larger in-domain datasets,\nwith an increase of 33.0 (24.3) BLEU for en$\\to$zh (zh$\\to$en) directions on\nthe full dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 23:25:15 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Liu", "Boxiang", ""], ["Huang", "Liang", ""]]}, {"id": "2005.09381", "submitter": "Pablo Dorta-Gonzalez", "authors": "Pablo Dorta-Gonz\\'alez, Rafael Su\\'arez-Vega, Mar\\'ia Isabel\n  Dorta-Gonz\\'alez", "title": "Open Access effect on uncitedness: A large-scale study controlling by\n  discipline, source type and visibility", "comments": "33 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are many factors that affect the probability of being uncited during\nthe first years after publication. In this study, we analyze three of these\nfactors for journals, conference proceedings and book series: the field (in 316\nsubject categories of the Scopus database), the access modality (open access\nvs. paywalled), and the visibility of the source (through the percentile of the\naverage impact in the subject category). We quantify the effect of these\nfactors on the probability of being uncited. This probability is measured\nthrough the percentage of uncited documents in the serial sources of the Scopus\ndatabase at about two years after publication. As a main result, we do not find\nany strong correlation between open access and uncitedness. Within the group of\nmost cited journals (Q1 and top 10%), open access journals generally have\nsomewhat lower uncited rates. However, in the intermediate quartiles (Q2 and\nQ3) almost no differences are observed, while for Q4 the uncited rate is again\nsomewhat lower in the case of the OA group. This is important because it\nprovides new evidence in the debate about open access citation advantage.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:14:31 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Dorta-Gonz\u00e1lez", "Pablo", ""], ["Su\u00e1rez-Vega", "Rafael", ""], ["Dorta-Gonz\u00e1lez", "Mar\u00eda Isabel", ""]]}, {"id": "2005.09433", "submitter": "Weishu Liu", "authors": "Junwen Zhu and Weishu Liu", "title": "Comparing like with like: China ranks first in SCI-indexed research\n  articles since 2018", "comments": "Forthcoming in Scientometrics", "journal-ref": null, "doi": "10.1007/s11192-020-03525-2", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  China's rising in scientific research output is impressive. The academic\ncommunity is curious about the time when the cross-over in the number of annual\nscientific publication production between China and the USA can happen. By\nusing Web of Science Core Collection's Science Citation Index Expanded\ndatabase, this study finds that China still ranks the second in the production\nof SCI-indexed publications in 2019 but may leapfrog the USA to be the first in\n2020 or 2021, if all document types are considered. Comparatively, China has\nalready overtaken the USA and been the largest SCI-indexed original research\narticle producer since 2018. However, China still lags behind the USA regarding\nthe number of review paper production. In general, quantitative advantage does\nnot equal quality or impact advantage. We think that the USA will continue to\nbe the global scientific leader for a long time.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 13:23:23 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zhu", "Junwen", ""], ["Liu", "Weishu", ""]]}, {"id": "2005.10082", "submitter": "Petar Radanliev", "authors": "Petar Radanliev, David De Roure, Rob Walton, Max Van Kleek, Omar\n  Santos, Rafael Mantilla Montalvo, La Treall Maddox", "title": "What country, university or research institute, performed the best on\n  COVID-19? Bibliometric analysis of scientific literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we conduct data mining to discover the countries,\nuniversities and companies, produced or collaborated the most research on\nCovid-19 since the pandemic started. We present some interesting findings, but\ndespite analysing all available records on COVID-19 from the Web of Science\nCore Collection, we failed to reach any significant conclusions on how the\nworld responded to the COVID-19 pandemic. Therefore, we increased our analysis\nto include all available data records on pandemics and epidemics from 1900 to\n2020. We discover some interesting results on countries, universities and\ncompanies, that produced collaborated most the most in research on pandemic and\nepidemics. Then we compared the results with the analysing on COVID-19 data\nrecords. This has created some interesting findings that are explained and\ngraphically visualised in the article.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 14:36:12 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Radanliev", "Petar", ""], ["De Roure", "David", ""], ["Walton", "Rob", ""], ["Van Kleek", "Max", ""], ["Santos", "Omar", ""], ["Montalvo", "Rafael Mantilla", ""], ["Maddox", "La Treall", ""]]}, {"id": "2005.10321", "submitter": "Marko Stamenovic", "authors": "Marko Stamenovic, Jeibo Luo", "title": "Machine Identification of High Impact Research through Text and Image\n  Analysis", "comments": null, "journal-ref": "2017 IEEE Third International Conference on Multimedia Big Data\n  (BigMM)", "doi": "10.1109/BigMM.2017.63", "report-no": null, "categories": "cs.IR cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume of academic paper submissions and publications is growing at an\never increasing rate. While this flood of research promises progress in various\nfields, the sheer volume of output inherently increases the amount of noise. We\npresent a system to automatically separate papers with a high from those with a\nlow likelihood of gaining citations as a means to quickly find high impact,\nhigh quality research. Our system uses both a visual classifier, useful for\nsurmising a document's overall appearance, and a text classifier, for making\ncontent-informed decisions. Current work in the field focuses on small datasets\ncomposed of papers from individual conferences. Attempts to use similar\ntechniques on larger datasets generally only considers excerpts of the\ndocuments such as the abstract, potentially throwing away valuable data. We\nrectify these issues by providing a dataset composed of PDF documents and\ncitation counts spanning a decade of output within two separate academic\ndomains: computer science and medicine. This new dataset allows us to expand on\ncurrent work in the field by generalizing across time and academic domain.\nMoreover, we explore inter-domain prediction models - evaluating a classifier's\nperformance on a domain it was not trained on - to shed further insight on this\nimportant problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:12:24 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Stamenovic", "Marko", ""], ["Luo", "Jeibo", ""]]}, {"id": "2005.10334", "submitter": "Arthur Brack", "authors": "Arthur Brack, Anett Hoppe, Markus Stocker, S\\\"oren Auer, Ralph Ewerth", "title": "Requirements Analysis for an Open Research Knowledge Graph", "comments": "Accepted for publishing in 24th International Conference on Theory\n  and Practice of Digital Libraries, TPDL 2020", "journal-ref": "Digital Libraries for Open Knowledge. TPDL 2020. Lecture Notes in\n  Computer Science, vol 12246. Springer, Cham", "doi": "10.1007/978-3-030-54956-5_1", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current science communication has a number of drawbacks and bottlenecks which\nhave been subject of discussion lately: Among others, the rising number of\npublished articles makes it nearly impossible to get an overview of the state\nof the art in a certain field, or reproducibility is hampered by fixed-length,\ndocument-based publications which normally cannot cover all details of a\nresearch work. Recently, several initiatives have proposed knowledge graphs\n(KGs) for organising scientific information as a solution to many of the\ncurrent issues. The focus of these proposals is, however, usually restricted to\nvery specific use cases. In this paper, we aim to transcend this limited\nperspective by presenting a comprehensive analysis of requirements for an Open\nResearch Knowledge Graph (ORKG) by (a) collecting daily core tasks of a\nscientist, (b) establishing their consequential requirements for a KG-based\nsystem, (c) identifying overlaps and specificities, and their coverage in\ncurrent solutions. As a result, we map necessary and desirable requirements for\nsuccessful KG-based science communication, derive implications and outline\npossible solutions.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:56:58 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Brack", "Arthur", ""], ["Hoppe", "Anett", ""], ["Stocker", "Markus", ""], ["Auer", "S\u00f6ren", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2005.10537", "submitter": "Weishu Liu", "authors": "Weishu Liu", "title": "China's SCI-indexed publications: facts, feelings, and future directions", "comments": "forthcoming in ECNU Review of Education", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose:In relation to the boom in China's SCI-indexed publications, this\nopinion piece examines this phenomenon and looks at future possible directions\nfor the reform of China's research evaluation processes.\nDesign/Approach/Methods:This opinion piece uses bibliographic data for the past\ndecade (2010-2019) from the Science Citation Index Expanded in the Web of\nScience Core Collection to examine the rise in China's SCI-indexed\npublications. Findings: China has surpassed the United States and been the\nlargest contributor of SCI publications since 2018. However, while the impact\nof China's SCI publications is rising, the scale of this impact still lags\nbehind that of other major contributing countries. China's SCI publications are\nalso overrepresented in some journals. Originality/Value: Reporting the latest\nfacts about China's SCI-indexed publications, this article will benefit the\nreform of China's research evaluation system.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 09:26:27 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Liu", "Weishu", ""]]}, {"id": "2005.10732", "submitter": "Ludo Waltman", "authors": "Martijn Visser, Nees Jan van Eck, and Ludo Waltman", "title": "Large-scale comparison of bibliographic data sources: Scopus, Web of\n  Science, Dimensions, Crossref, and Microsoft Academic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large-scale comparison of five multidisciplinary bibliographic\ndata sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft\nAcademic. The comparison considers scientific documents from the period\n2008-2017 covered by these data sources. Scopus is compared in a pairwise\nmanner with each of the other data sources. We first analyze differences\nbetween the data sources in the coverage of documents, focusing for instance on\ndifferences over time, differences per document type, and differences per\ndiscipline. We then study differences in the completeness and accuracy of\ncitation links. Based on our analysis, we discuss strengths and weaknesses of\nthe different data sources. We emphasize the importance of combining a\ncomprehensive coverage of the scientific literature with a flexible set of\nfilters for making selections of the literature.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 15:42:15 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 21:08:09 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Visser", "Martijn", ""], ["van Eck", "Nees Jan", ""], ["Waltman", "Ludo", ""]]}, {"id": "2005.11021", "submitter": "Moritz Schubotz", "authors": "Philipp Scharpf, Moritz Schubotz, Abdou Youssef, Felix Hamborg, Norman\n  Meuschke, Bela Gipp", "title": "Classification and Clustering of arXiv Documents, Sections, and\n  Abstracts, Comparing Encodings of Natural and Mathematical Language", "comments": null, "journal-ref": "Proceedings of the ACM/IEEE Joint Conference on Digital Libraries\n  JCDL 2020", "doi": "10.1145/3383583.3398529", "report-no": null, "categories": "cs.DL cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we show how selecting and combining encodings of natural and\nmathematical language affect classification and clustering of documents with\nmathematical content. We demonstrate this by using sets of documents, sections,\nand abstracts from the arXiv preprint server that are labeled by their subject\nclass (mathematics, computer science, physics, etc.) to compare different\nencodings of text and formulae and evaluate the performance and runtimes of\nselected classification and clustering algorithms. Our encodings achieve\nclassification accuracies up to $82.8\\%$ and cluster purities up to $69.4\\%$\n(number of clusters equals number of classes), and $99.9\\%$ (unspecified number\nof clusters) respectively. We observe a relatively low correlation between text\nand math similarity, which indicates the independence of text and formulae and\nmotivates treating them as separate features of a document. The classification\nand clustering can be employed, e.g., for document search and recommendation.\nFurthermore, we show that the computer outperforms a human expert when\nclassifying documents. Finally, we evaluate and discuss multi-label\nclassification and formula semantification.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:16:32 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Scharpf", "Philipp", ""], ["Schubotz", "Moritz", ""], ["Youssef", "Abdou", ""], ["Hamborg", "Felix", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2005.11504", "submitter": "Cornelius Ihle", "authors": "Cornelius Ihle, Moritz Schubotz, Norman Meuschke, Bela Gipp", "title": "A First Step Towards Content Protecting Plagiarism Detection", "comments": "Submitted to JCDL 2020: Proceedings of the ACM/ IEEE Joint Conference\n  on Digital Libraries in 2020 (JCDL '20), August 1-5, 2020, Virtual Event,\n  China", "journal-ref": null, "doi": "10.1145/3383583.3398620", "report-no": null, "categories": "cs.CR cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plagiarism detection systems are essential tools for safeguarding academic\nand educational integrity. However, today's systems require disclosing the full\ncontent of the input documents and the document collection to which the input\ndocuments are compared. Moreover, the systems are centralized and under the\ncontrol of individual, typically commercial providers. This situation raises\nprocedural and legal concerns regarding the confidentiality of sensitive data,\nwhich can limit or prohibit the use of plagiarism detection services. To\neliminate these weaknesses of current systems, we seek to devise a plagiarism\ndetection approach that does not require a centralized provider nor exposing\nany content as cleartext. This paper presents the initial results of our\nresearch. Specifically, we employ Private Set Intersection to devise a\ncontent-protecting variant of the citation-based similarity measure\nBibliographic Coupling implemented in our plagiarism detection system HyPlag.\nOur evaluation shows that the content-protecting method achieves the same\ndetection effectiveness as the original method while making common attacks to\ndisclose the protected content practically infeasible. Our future work will\nextend this successful proof-of-concept by devising plagiarism detection\nmethods that can analyze the entire content of documents without disclosing it\nas cleartext.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 09:58:07 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ihle", "Cornelius", ""], ["Schubotz", "Moritz", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2005.11512", "submitter": "Mike Thelwall Prof", "authors": "Mike Thelwall, Ruth Fairclough", "title": "All downhill from the PhD? The typical impact trajectory of US academic\n  careers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within academia, mature researchers tend to be more senior, but do they also\ntend to write higher impact articles? This article assesses long-term\npublishing (16+ years) United States (US) researchers, contrasting them with\nshorter-term publishing researchers (1, 6 or 10 years). A long-term US\nresearcher is operationalised as having a first Scopus-indexed journal article\nin exactly 2001 and one in 2016-2019, with US main affiliations in their first\nand last articles. Researchers publishing in large teams (11+ authors) were\nexcluded. The average field and year normalised citation impact of long- and\nshorter-term US researchers' journal articles decreases over time relative to\nthe national average, with especially large falls to the last articles\npublished that may be at least partly due to a decline in self-citations. In\nmany cases researchers start by publishing above US average citation impact\nresearch and end by publishing below US average citation impact research. Thus,\nresearch managers should not assume that senior researchers will usually write\nthe highest impact papers.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 10:41:31 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Thelwall", "Mike", ""], ["Fairclough", "Ruth", ""]]}, {"id": "2005.11981", "submitter": "Marilena Daquino", "authors": "Marilena Daquino, Silvio Peroni, David Shotton, Giovanni Colavizza,\n  Behnam Ghavimi, Anne Lauscher, Philipp Mayr, Matteo Romanello, and Philipp\n  Zumstein", "title": "The OpenCitations Data Model", "comments": "ISWC 2020 Conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of schemas and ontologies are currently used for the\nmachine-readable description of bibliographic entities and citations. This\ndiversity, and the reuse of the same ontology terms with different nuances,\ngenerates inconsistencies in data. Adoption of a single data model would\nfacilitate data integration tasks regardless of the data supplier or context\napplication. In this paper we present the OpenCitations Data Model (OCDM), a\ngeneric data model for describing bibliographic entities and citations,\ndeveloped using Semantic Web technologies. We also evaluate the effective\nreusability of OCDM according to ontology evaluation practices, mention\nexisting users of OCDM, and discuss the use and impact of OCDM in the wider\nopen science community.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 08:52:02 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 10:14:43 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Daquino", "Marilena", ""], ["Peroni", "Silvio", ""], ["Shotton", "David", ""], ["Colavizza", "Giovanni", ""], ["Ghavimi", "Behnam", ""], ["Lauscher", "Anne", ""], ["Mayr", "Philipp", ""], ["Romanello", "Matteo", ""], ["Zumstein", "Philipp", ""]]}, {"id": "2005.12099", "submitter": "Moritz Schubotz", "authors": "Moritz Schubotz and Philipp Scharpf and Olaf Teschke and Andreas\n  Kuehnemund and Corinna Breitinger and Bela Gipp", "title": "AutoMSC: Automatic Assignment of Mathematics Subject Classification\n  Labels", "comments": null, "journal-ref": "Intelligent Computer Mathematics - 13thInternational Conference,\n  {CICM} 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings", "doi": "10.1007/978-3-030-53518-6_15", "report-no": null, "categories": "cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Authors of research papers in the fields of mathematics, and other math-heavy\ndisciplines commonly employ the Mathematics Subject Classification (MSC) scheme\nto search for relevant literature. The MSC is a hierarchical alphanumerical\nclassification scheme that allows librarians to specify one or multiple codes\nfor publications. Digital Libraries in Mathematics, as well as reviewing\nservices, such as zbMATH and Mathematical Reviews (MR) rely on these MSC labels\nin their workflows to organize the abstracting and reviewing process.\nEspecially, the coarse-grained classification determines the subject editor who\nis responsible for the actual reviewing process.\n  In this paper, we investigate the feasibility of automatically assigning a\ncoarse-grained primary classification using the MSC scheme, by regarding the\nproblem as a multi-class classification machine learning task. We find that our\nmethod achieves an (F_1)-score of over 77%, which is remarkably close to the\nagreement of zbMATH and MR ((F_1)-score of 81%). Moreover, we find that the\nmethod's confidence score allows for reducing the effort by 86% compared to the\nmanual coarse-grained classification effort while maintaining a precision of\n81% for automatically classified articles.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:26:45 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 07:12:34 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Schubotz", "Moritz", ""], ["Scharpf", "Philipp", ""], ["Teschke", "Olaf", ""], ["Kuehnemund", "Andreas", ""], ["Breitinger", "Corinna", ""], ["Gipp", "Bela", ""]]}, {"id": "2005.12668", "submitter": "Tom Hope", "authors": "Tom Hope, Jason Portenoy, Kishore Vasan, Jonathan Borchardt, Eric\n  Horvitz, Daniel S. Weld, Marti A. Hearst, Jevin West", "title": "SciSight: Combining faceted navigation and research group detection for\n  COVID-19 exploratory scientific search", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has sparked unprecedented mobilization of scientists,\ngenerating a deluge of papers that makes it hard for researchers to keep track\nand explore new directions. Search engines are designed for targeted queries,\nnot for discovery of connections across a corpus. In this paper, we present\nSciSight, a system for exploratory search of COVID-19 research integrating two\nkey capabilities: first, exploring associations between biomedical facets\nautomatically extracted from papers (e.g., genes, drugs, diseases, patient\noutcomes); second, combining textual and network information to search and\nvisualize groups of researchers and their ties. SciSight has so far served over\n$15K$ users with over $42K$ page views and $13\\%$ returns.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 08:56:21 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 03:05:32 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 15:43:20 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Hope", "Tom", ""], ["Portenoy", "Jason", ""], ["Vasan", "Kishore", ""], ["Borchardt", "Jonathan", ""], ["Horvitz", "Eric", ""], ["Weld", "Daniel S.", ""], ["Hearst", "Marti A.", ""], ["West", "Jevin", ""]]}, {"id": "2005.13240", "submitter": "Marion Maisonobe", "authors": "Michel Grossetti (LISST), Marion Maisonobe (GC), Laurent J\\'egou\n  (CIEU), B\\'eatrice Milard (LISST), Guillaume Cabanac (IRIT)", "title": "Spatial organisation of French research from the scholarly publication\n  standpoint (1999-2017): Long-standing dynamics and policy-induced disorder", "comments": null, "journal-ref": null, "doi": "10.1051/epjconf/202024401005", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In social processes, long-term trends can be influenced or disrupted by\nvarious factors, including public policy. When public policies depend on a\nmisrepresentation of trends in the areas they are aimed at, they become random\nand disruptive, which can be interpreted as a source of disorder. Here we\nconsider policies on the spatial organization of the French Higher Education\nand Research system, which reflects the authorities' hypothesis that scientific\nexcellence is the prerogative of a few large urban agglomerations. By\ngeographically identifying all the French publications listed in the Web of\nScience databases between 1999 and 2017, we highlight a spatial deconcentration\ntrend, which has slowed down in recent years due to a freezed growth of the\nteaching force. This deconcentration continues, however, to sustain the growth\nof scientific production in small and medium-sized towns. An examination of the\nlarge conurbations shows the relative decline of sites that nevertheless have\nbeen highlighted as examples to be followed by the Excellence policies\n(Strasbourg among others). The number of students and faculty has grown less\nthere, and it is a plaussible explanation for the relative decline in\nscientific production. We show that the publication output of a given site\ndepends directly and strongly on the number of researchers hosted there. Based\non precise data at the French level, our results confirm what is already known\nat world scale. In conclusion, we question the amount of disorder resulting\nfrom policies aligned with poorly assessed trends.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 08:48:11 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 09:38:12 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 08:12:40 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Grossetti", "Michel", "", "LISST"], ["Maisonobe", "Marion", "", "GC"], ["J\u00e9gou", "Laurent", "", "CIEU"], ["Milard", "B\u00e9atrice", "", "LISST"], ["Cabanac", "Guillaume", "", "IRIT"]]}, {"id": "2005.13342", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff, Till Blume, Saskia Ostendorff", "title": "Towards an Open Platform for Legal Information", "comments": "Accepted at ACM/IEEE Joint Conference on Digital Libraries (JCDL)\n  2020", "journal-ref": null, "doi": "10.1145/3383583.3398616", "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the area of legal information systems have led to a\nvariety of applications that promise support in processing and accessing legal\ndocuments. Unfortunately, these applications have various limitations, e.g.,\nregarding scope or extensibility. Furthermore, we do not observe a trend\ntowards open access in digital libraries in the legal domain as we observe in\nother domains, e.g., economics of computer science. To improve open access in\nthe legal domain, we present our approach for an open source platform to\ntransparently process and access Legal Open Data. This enables the sustainable\ndevelopment of legal applications by offering a single technology stack.\nMoreover, the approach facilitates the development and deployment of new\ntechnologies. As proof of concept, we implemented six technologies and\ngenerated metadata for more than 250,000 German laws and court decisions. Thus,\nwe can provide users of our platform not only access to legal documents, but\nalso the contained information.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:16:19 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ostendorff", "Malte", ""], ["Blume", "Till", ""], ["Ostendorff", "Saskia", ""]]}, {"id": "2005.14021", "submitter": "Julia Heuritsch", "authors": "Julia Heuritsch", "title": "Knowledge Utilization and Open Science Policies: Noble aims that ensure\n  quality research or Ordering discoveries like a pizza?", "comments": "This is a pre-print version. The original paper was published at the\n  International Astronautical Congress in Washington DC (USA), October 2019.\n  arXiv admin note: text overlap with arXiv:1801.08033", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Science has been a rising theme in the landscape of science policy in\nrecent years. The goal is to make research that emerges from publicly funded\nscience to become findable, accessible, interoperable and reusable (FAIR) for\nuse by other researchers. Knowledge utilization policies aim to efficiently\nmake scientific knowledge beneficial for society at large. This paper\ndemonstrates how Astronomy aspires to be open and transparent given their\ncriteria for high research quality, which aim at pushing knowledge forward and\nclear communication of findings. However, the use of quantitative metrics in\nresearch evaluation puts pressure on the researcher, such that taking the extra\ntime for transparent publishing of data and results is difficult, given that\nastronomers are not rewarded for the quality of research papers, but rather\ntheir quantity. This paper explores the current mode of openness in Astronomy\nand how incentives due to funding, publication practices and indicators affect\nthis field. The paper concludes with some recommendations on how policies such\nas making science more open have the potential to contribute to scientific\nquality in Astronomy.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:03:55 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Heuritsch", "Julia", ""]]}, {"id": "2005.14024", "submitter": "Felix Hamborg", "authors": "Lukas Gebhard and Felix Hamborg", "title": "The POLUSA Dataset: 0.9M Political News Articles Balanced by Time and\n  Outlet Popularity", "comments": "2 pages, 1 table", "journal-ref": null, "doi": "10.1145/3383583.3398567", "report-no": null, "categories": "cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News articles covering policy issues are an essential source of information\nin the social sciences and are also frequently used for other use cases, e.g.,\nto train NLP language models. To derive meaningful insights from the analysis\nof news, large datasets are required that represent real-world distributions,\ne.g., with respect to the contained outlets' popularity, topically, or across\ntime. Information on the political leanings of media publishers is often\nneeded, e.g., to study differences in news reporting across the political\nspectrum, which is one of the prime use cases in the social sciences when\nstudying media bias and related societal issues. Concerning these requirements,\nexisting datasets have major flaws, resulting in redundant and cumbersome\neffort in the research community for dataset creation. To fill this gap, we\npresent POLUSA, a dataset that represents the online media landscape as\nperceived by an average US news consumer. The dataset contains 0.9M articles\ncovering policy topics published between Jan. 2017 and Aug. 2019 by 18 news\noutlets representing the political spectrum. Each outlet is labeled by its\npolitical leaning, which we derive using a systematic aggregation of eight data\nsources. The news dataset is balanced with respect to publication date and\noutlet popularity. POLUSA enables studying a variety of subjects, e.g., media\neffects and political partisanship. Due to its size, the dataset allows to\nutilize data-intense deep learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 14:24:11 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Gebhard", "Lukas", ""], ["Hamborg", "Felix", ""]]}, {"id": "2005.14343", "submitter": "Baani Leen Kaur Jolly", "authors": "Baani Leen Kaur Jolly, Lavina Jain, Debajyoti Bera, Tanmoy Chakraborty", "title": "Unsupervised Anomaly Detection in Journal-Level Citation Networks", "comments": "Accepted: ACM/IEEE Joint Conference on Digital Libraries (JCDL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Journal Impact Factor is a popular metric for determining the quality of a\njournal in academia. The number of citations received by a journal is a crucial\nfactor in determining the impact factor, which may be misused in multiple ways.\nTherefore, it is crucial to detect citation anomalies for further identifying\nmanipulation and inflation of impact factor. Citation network models the\ncitation relationship between journals in terms of a directed graph. Detecting\nanomalies in the citation network is a challenging task which has several\napplications in spotting citation cartels and citation stack and understanding\nthe intentions behind the citations. In this paper, we present a novel approach\nto detect the anomalies in a journal-level scientific citation network, and\ncompare the results with the existing graph anomaly detection algorithms. Due\nto the lack of proper ground-truth, we introduce a journal-level citation\nanomaly dataset which consists of synthetically injected citation anomalies and\nuse it to evaluate our methodology. Our method is able to predict the anomalous\ncitation pairs with a precision of 100\\% and an F1-score of 86%. We further\ncategorize the detected anomalies into various types and reason out possible\ncauses. We also analyze our model on the Microsoft Academic Search dataset - a\nreal-world citation dataset and interpret our results using a case study,\nwherein our results resemble the citations and SCImago Journal Rank (SJR)\nrating-change charts, thus indicating the usefulness of our method. We further\ndesign `Journal Citation Analysis Tool', an interactive web portal which, given\nthe citation network as an input, shows the journal-level anomalous citation\npatterns and helps users analyze citation patterns of a given journal over the\nyears.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 23:59:57 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Jolly", "Baani Leen Kaur", ""], ["Jain", "Lavina", ""], ["Bera", "Debajyoti", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2005.14569", "submitter": "Lukas Pukelis Dr", "authors": "Lukas Pukelis, Nuria Bautista Puig, Mykola Skrynik, Vilius\n  Stanciauskas", "title": "OSDG -- Open-Source Approach to Classify Text Data by UN Sustainable\n  Development Goals (SDGs)", "comments": "10 pages , 1 Figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sustainable Development Goals (SDGs) bring together the diverse development\ncommunity and provide a clear set of development targets for 2030. Given a\nlarge number of actors and initiatives related to these goals, there is a need\nto have a way to accurately and reliably assign text to different input:\nscientific research, research projects, technological output or documents to\nspecific SDGs. In this paper we present Open Source SDG (OSDG) project and tool\nwhich does so by integrating existing research and previous classification into\na robust and coherent framework. This integration is based on linking the\nfeatures from the variety of previous approaches, like ontology items, keywords\nor features from machine-learning models, to the topics in Microsoft Academic\nGraph.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 13:42:42 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Pukelis", "Lukas", ""], ["Puig", "Nuria Bautista", ""], ["Skrynik", "Mykola", ""], ["Stanciauskas", "Vilius", ""]]}]