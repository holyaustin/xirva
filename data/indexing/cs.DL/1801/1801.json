[{"id": "1801.00004", "submitter": "Jenny Novacescu", "authors": "Jenny Novacescu, Joshua E.G. Peek, Sarah Weissman, Scott W. Fleming,\n  Karen Levay and Elizabeth Fraser", "title": "A Model for Data Citation in Astronomical Research using Digital Object\n  Identifiers (DOIs)", "comments": "13 pages, 3 figures. Accepted on Dec 19, 2017 for publication in\n  Astrophysical Journal Supplement Series", "journal-ref": null, "doi": "10.3847/1538-4365/aab76a", "report-no": null, "categories": "cs.DL astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardizing and incentivizing the use of digital object identifiers (DOIs)\nto aggregate and identify both data analyzed and data generated by a research\nproject will advance the field of astronomy to match best practices in other\nresearch fields like geosciences and medicine. Increase in the use of DOIs will\nprepare the discipline for changing expectations among funding agencies and\npublishers, who increasingly expect accurate and thorough data citation to\naccompany scientific outputs. The use of DOIs ensures a robust, sustainable,\nand interoperable approach to data citation in which due credit is given to\nresearchers and institutions who produce and maintain the primary data. We\ndescribe in this work the advantages of DOIs for data citation and best\npractices for integrating a DOI service in an astronomical archive. We report\non a pilot project carried out in collaboration with AAS Journals. During the\ncourse of the 1.5 year pilot, over 75% of submitting authors opted to use the\nintegrated DOI service to clearly identify data analyzed during their research\nproject when prompted at the time of paper submission.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 03:05:57 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Novacescu", "Jenny", ""], ["Peek", "Joshua E. G.", ""], ["Weissman", "Sarah", ""], ["Fleming", "Scott W.", ""], ["Levay", "Karen", ""], ["Fraser", "Elizabeth", ""]]}, {"id": "1801.00725", "submitter": "Robert B. Allen", "authors": "Robert B. Allen, Yoonhwan Kim", "title": "Semantic Modeling with Foundries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze challenges for the development of the Human Activities and\nInfrastructures Foundry. We explore a rich semantic modeling approach to\ndescribe two Korean ceramic water droppers used to mix ink for calligraphy, how\nthey were produced and the reasons for their differing aesthetic. Our modeling\nsupports schema and allows for transitions of Entities based on the\nrelationships to other Entities with which they are associated. We explore the\nsimilarity of our approach to object-oriented analysis and modeling.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 18:52:19 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 18:21:32 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Allen", "Robert B.", ""], ["Kim", "Yoonhwan", ""]]}, {"id": "1801.00912", "submitter": "Yun-Cheng Tsai", "authors": "Hsuan-Lei Shao, Sieh-Chuen Huang, Yun-Cheng Tsai", "title": "How the Taiwanese Do China Studies: Applications of Text Mining", "comments": "10 pages, 8 figures, 1 table", "journal-ref": "Journal of Data Mining & Digital Humanities, 2018 (May 4, 2018)\n  jdmdh:4470", "doi": "10.46298/jdmdh.4178", "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid evolution of cross-strait situation, \"Mainland China\" as a\nsubject of social science study has evoked the voice of \"Rethinking China\nStudy\" among intelligentsia recently. This essay tried to apply an automatic\ncontent analysis tool (CATAR) to the journal \"Mainland China Studies\"\n(1998-2015) in order to observe the research trends based on the clustering of\ntext from the title and abstract of each paper in the journal. The results\nshowed that the 473 articles published by the journal were clustered into seven\nsalient topics. From the publication number of each topic over time (including\n\"volume of publications\", \"percentage of publications\"), there are two major\ntopics of this journal while other topics varied over time widely. The\ncontribution of this study includes: 1. We could group each \"independent\" study\ninto a meaningful topic, as a small scale experiment verified that this topic\nclustering is feasible. 2. This essay reveals the salient research topics and\ntheir trends for the Taiwan journal \"Mainland China Studies\". 3. Various\ntopical keywords were identified, providing easy access to the past study. 4.\nThe yearly trends of the identified topics could be viewed as signature of\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 06:18:14 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 00:53:27 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 08:32:10 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Shao", "Hsuan-Lei", ""], ["Huang", "Sieh-Chuen", ""], ["Tsai", "Yun-Cheng", ""]]}, {"id": "1801.01021", "submitter": "Katie Frey", "authors": "Katie Frey and Alberto Accomazzi", "title": "The Unified Astronomy Thesaurus: Semantic Metadata for Astronomy and\n  Astrophysics", "comments": "Submitted to the Astrophysical Journal Supplements, 10 pages, 3\n  tables", "journal-ref": null, "doi": "10.3847/1538-4365/aab760", "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several different controlled vocabularies have been developed and used by the\nastronomical community, each designed to serve a specific need and a specific\ngroup. The Unified Astronomy Thesaurus (UAT) attempts to provide a highly\nstructured controlled vocabulary that will be relevant and useful across the\nentire discipline, regardless of content or platform. As two major use cases\nfor the UAT include classifying articles and data, we examine the UAT in\ncomparison with the Astronomical Subject Keywords used by major publications\nand the JWST Science Keywords used by STScI's Astronomer's Proposal Tool.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 14:37:39 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Frey", "Katie", ""], ["Accomazzi", "Alberto", ""]]}, {"id": "1801.01316", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Mu Jung Cho, Anupriya Gagneja, Xiao Yang, Miriam\n  Brinberg, Katie Roehrick, Sagnik Ray Choudhury, Nilam Ram, Byron Reeves and\n  C. Lee Giles", "title": "Text Extraction and Retrieval from Smartphone Screenshots: Building a\n  Repository for Life in Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daily engagement in life experiences is increasingly interwoven with mobile\ndevice use. Screen capture at the scale of seconds is being used in behavioral\nstudies and to implement \"just-in-time\" health interventions. The increasing\npsychological breadth of digital information will continue to make the actual\nscreens that people view a preferred if not required source of data about life\nexperiences. Effective and efficient Information Extraction and Retrieval from\ndigital screenshots is a crucial prerequisite to successful use of screen data.\nIn this paper, we present the experimental workflow we exploited to: (i)\npre-process a unique collection of screen captures, (ii) extract unstructured\ntext embedded in the images, (iii) organize image text and metadata based on a\nstructured schema, (iv) index the resulting document collection, and (v) allow\nfor Image Retrieval through a dedicated vertical search engine application. The\nadopted procedure integrates different open source libraries for traditional\nimage processing, Optical Character Recognition (OCR), and Image Retrieval. Our\naim is to assess whether and how state-of-the-art methodologies can be applied\nto this novel data set. We show how combining OpenCV-based pre-processing\nmodules with a Long short-term memory (LSTM) based release of Tesseract OCR,\nwithout ad hoc training, led to a 74% character-level accuracy of the extracted\ntext. Further, we used the processed repository as baseline for a dedicated\nImage Retrieval system, for the immediate use and application for behavioral\nand prevention scientists. We discuss issues of Text Information Extraction and\nRetrieval that are particular to the screenshot image case and suggest\nimportant future work.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 11:51:26 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Chiatti", "Agnese", ""], ["Cho", "Mu Jung", ""], ["Gagneja", "Anupriya", ""], ["Yang", "Xiao", ""], ["Brinberg", "Miriam", ""], ["Roehrick", "Katie", ""], ["Choudhury", "Sagnik Ray", ""], ["Ram", "Nilam", ""], ["Reeves", "Byron", ""], ["Giles", "C. Lee", ""]]}, {"id": "1801.02094", "submitter": "Alice Allen", "authors": "Alice Allen, Peter J. Teuben, and P. Wesley Ryan", "title": "Schroedinger's code: A preliminary study on research source code\n  availability and link persistence in astrophysics", "comments": "Accepted to ApJS", "journal-ref": null, "doi": "10.3847/1538-4365/aab764", "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examined software usage in a sample set of astrophysics research articles\npublished in 2015 and searched for source code for the software mentioned in\nthese research papers. We categorized the software to indicate whether source\ncode is available for download and whether there are restrictions to accessing\nit, and if source code is not available, whether some other form of the\nsoftware, such as a binary, is. We also extracted hyperlinks from one journal's\n2015 research articles, as links in articles can serve as an acknowledgment of\nsoftware use and lead to data used in the research, and tested them to\ndetermine which of these URLs are still accessible. For our sample of 715\nsoftware instances in the 166 articles we examined, we were able to categorize\n418 records as to availability of source code and found that 285 unique codes\nwere used, 58% of which offer source code available online for download. Of the\n2,558 hyperlinks extracted from 1,669 research articles, at best, 90% of them\nwere available over our testing period.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 21:27:32 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 08:45:09 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Allen", "Alice", ""], ["Teuben", "Peter J.", ""], ["Ryan", "P. Wesley", ""]]}, {"id": "1801.02383", "submitter": "Xianwen Wang", "authors": "Xianwen Wang, Yunxue Cui, Qingchun Li and Xinhui Guo", "title": "Social Media Attention Increases Article Visits: An Investigation on\n  Article-Level Referral Data of PeerJ", "comments": null, "journal-ref": "Frontiers in Research Metrics and Analytics, 2017", "doi": "10.3389/frma.2017.00011", "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to better understand the effect of social media in the dissemination\nof scholarly articles, employing the daily updated referral data of 110 PeerJ\narticles collected over a period of 345 days, we analyze the relationship\nbetween social media attention and article visitors directed by social media.\nOur results show that social media presence of PeerJ articles is high. About\n68.18% of the papers receive at least one tweet from Twitter accounts other\nthan @PeerJ, the official account of the journal. Social media attention\nincreases the dissemination of scholarly articles. Altmetrics could not only\nact as the complement of traditional citation measures but also play an\nimportant role in increasing the article downloads and promoting the impacts of\nscholarly articles. There also exists a significant correlation among the\nonline attention from different social media platforms. Articles with more\nFacebook shares tend to get more tweets. The temporal trends show that social\nattention comes immediately following publication but does not last long, so do\nthe social media directed article views.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 11:17:21 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Wang", "Xianwen", ""], ["Cui", "Yunxue", ""], ["Li", "Qingchun", ""], ["Guo", "Xinhui", ""]]}, {"id": "1801.02466", "submitter": "Peter Sj\\\"og{\\aa}rde", "authors": "Peter Sj\\\"og{\\aa}rde, Per Ahlgren", "title": "Granularity of algorithmically constructed publication-level\n  classifications of research publications: Identification of topics", "comments": null, "journal-ref": "Journal of Informetrics. 2018. Volume 12, Issue 1, Pages 133-152", "doi": "10.1016/j.joi.2017.12.006", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to find a theoretically grounded, practically\napplicable and useful granularity level of an algorithmically constructed\npublication-level classification of research publications (ACPLC). The level\naddressed is the level of research topics. The methodology we propose uses\nsynthesis papers and their reference articles to construct a baseline\nclassification. A dataset of about 31 million publications, and their mutual\ncitations relations, is used to obtain several ACPLCs of different granularity.\nEach ACPLC is compared to the baseline classification and the best performing\nACPLC is identified. The results of two case studies show that the topics of\nthe cases are closely associated with different classes of the identified\nACPLC, and that these classes tend to treat only one topic. Further, the class\nsize variation is moderate, and only a small proportion of the publications\nbelong to very small classes. For these reasons, we conclude that the proposed\nmethodology is suitable to determine the topic granularity level of an ACPLC\nand that the ACPLC identified by this methodology is useful for bibliometric\nanalyses.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 15:06:54 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Sj\u00f6g\u00e5rde", "Peter", ""], ["Ahlgren", "Per", ""]]}, {"id": "1801.03106", "submitter": "Wolfgang Orthuber", "authors": "Wolfgang Orthuber (Kiel University)", "title": "Why informatics and general science need a conjoint basic definition of\n  information", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First the basic definition of information as a selection from a set of\npossibilities resp. domain is recalled. This also applies to digital\ninformation. The bits of digital information are parts of number sequences\nwhich represent a selection from a set of possibilities resp. domain. For\nfaultless conversation sender and receiver of information must have the same\ndefinition of the domain (e.g. of language vocabulary). Up to now the\ndefinition of the domain and of its elements is derived from context and\nknowledge. The internet provides an additional important possibility: A link to\na conjoint uniform definition of the domain at unique location on the internet.\nThe associated basic information structure is called \"Domain Vector\" (DV) and\nhas the structure \"UL (of the domain definition) plus sequence of numbers\". The\n\"UL\" is not only \"Uniform Locator\" of the domain definition. It also identifies\na certain kind of information for later comparison and search. It can be a\nUniform Resource Locator (URL) or an abbreviated equivalent, e.g. a hierarchic\nnumeric pointer or a short local pointer to a table with global internet\npointers. The DV structure can be used as general carrier of information which\nis language independent and more precise than language. A domain which contains\nDVs is called \"Domain Space\" (DS) and is defined as metric space. This allows\nsimilarity search according to user defined criteria, so that any kind of\ndefinable information can be made comparable and searchable according to user\nselected (relevant) and objectifiable (globally uniform) criteria. DS\ndefinitions can be reused in new DS definitions. Their elements, the DVs, are\nautomatically globally uniformly identified and defined. Obviously such\nconjoint definition of comparable information has great potential. It also can\navoid interoperability problems and redundant programming and so save high\ncosts.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 17:25:00 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Orthuber", "Wolfgang", "", "Kiel University"]]}, {"id": "1801.03366", "submitter": "Uta Grothkopf", "authors": "Uta Grothkopf, Dominic Bordelon, Silvia Meakins, Eric Emsellem", "title": "On the Availability of ESO Data Papers on arXiv/astro-ph", "comments": "4 pages, 3 figures, 2 tables", "journal-ref": "The Messenger 170 (December 2017): 58-61", "doi": "10.18727/0722-6691/5056", "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the ESO Telescope Bibliography database telbib, we have investigated\nthe percentage of ESO data papers that were submitted to the arXiv/astro-ph\ne-print server and that are therefore free to read. Our study revealed an\navailability of up to 96% of telbib papers on arXiv over the years 2010 to\n2017. We also compared the citation counts of arXiv vs. non-arXiv papers and\nfound that on average, papers submitted to arXiv are cited 2.8 times more often\nthan those not on arXiv. While simulations suggest that these findings are\nstatistically significant, we cannot yet draw firm conclusions as to the main\ncause of these differences.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 13:23:36 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Grothkopf", "Uta", ""], ["Bordelon", "Dominic", ""], ["Meakins", "Silvia", ""], ["Emsellem", "Eric", ""]]}, {"id": "1801.03417", "submitter": "Mikko Packalen", "authors": "Mikko Packalen", "title": "Edge Factors: Scientific Frontier Positions of Nations", "comments": "6 pages, 2 figures, 1 table (main text); 57 pages (full pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key decision in scientific work is whether to build on novel or\nwell-established ideas. Because exploiting new ideas is often harder than more\nconventional science, novel work can be especially dependent on interactions\nwith colleagues, the training environment, and ready access to potential\ncollaborators. Location may thus influence the tendency to pursue work that is\nclose to the edge of the scientific frontier in the sense that it builds on\nrecent ideas. We calculate for each nation its position relative to the edge of\nthe scientific frontier by measuring its propensity to build on relatively new\nideas in biomedical research. Text analysis of 20+ million publications shows\nthat the United States and South Korea have the highest tendencies for novel\nscience. China has become a leader in favoring newer ideas when working with\nbasic science ideas and research tools, but is still slow to adopt new clinical\nideas. Many locations remain far behind the leaders in terms of their tendency\nto work with novel ideas, indicating that the world is far from flat in this\nregard.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 15:11:40 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 15:20:16 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Packalen", "Mikko", ""]]}, {"id": "1801.03872", "submitter": "Valentin Kuznetsov", "authors": "Valentin Kuznetsov, Nils Leif Fischer, Yuyi Guo", "title": "The archive solution for distributed workflow management agents of the\n  CMS experiment at LHC", "comments": "This is a pre-print of an article published in Computing and Software\n  for Big Science. The final authenticated version is available online at:\n  https://doi.org/10.1007/s41781-018-0005-0", "journal-ref": null, "doi": "10.1007/s41781-018-0005-0", "report-no": null, "categories": "hep-ex cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CMS experiment at the CERN LHC developed the Workflow Management Archive\nsystem to persistently store unstructured framework job report documents\nproduced by distributed workflow management agents. In this paper we present\nits architecture, implementation, deployment, and integration with the CMS and\nCERN computing infrastructures, such as central HDFS and Hadoop Spark cluster.\nThe system leverages modern technologies such as a document oriented database\nand the Hadoop eco-system to provide the necessary flexibility to reliably\nprocess, store, and aggregate $\\mathcal{O}$(1M) documents on a daily basis. We\ndescribe the data transformation, the short and long term storage layers, the\nquery language, along with the aggregation pipeline developed to visualize\nvarious performance metrics to assist CMS data operators in assessing the\nperformance of the CMS computing system.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 17:02:30 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Kuznetsov", "Valentin", ""], ["Fischer", "Nils Leif", ""], ["Guo", "Yuyi", ""]]}, {"id": "1801.03928", "submitter": "YunFeng Chang", "authors": "Yunfeng Chang, Jihui Han", "title": "The efficiency of community detection by most similar node pairs", "comments": "arXiv admin note: text overlap with arXiv:1706.04829", "journal-ref": null, "doi": "10.1142/S0129183119500311", "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community analysis is an important way to ascertain whether or not a complex\nsystem consists of sub-structures with different properties. In this paper, we\ngive a two level community structure analysis for the SSCI journal system by\nmost similar co-citation pattern. Five different strategies for the selection\nof most similar node (journal) pairs are introduced. The efficiency is checked\nby the normalized mutual information technique. Statistical properties and\ncomparisons of the community results show that both of the two level detection\ncould give instructional information for the community structure of complex\nsystems. Further comparisons of the five strategies indicates that, the most\nefficient strategy is to assign nodes with maximum similarity into the same\ncommunity whether the similarity information is complete or not, while random\nselection generates small world local community with no inside order. These\nresults give valuable indication for efficient community detection by most\nsimilar node pairs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 01:34:17 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chang", "Yunfeng", ""], ["Han", "Jihui", ""]]}, {"id": "1801.04437", "submitter": "Rodrigo Costas", "authors": "Rodrigo Costas", "title": "Towards the social media studies of science: social media metrics,\n  present and future", "comments": "Spanish version:\n  http://revistas.bnjm.cu/index.php/anales/article/view/4172", "journal-ref": "Bibliotecas. Anales de Investigacion. 2017. 13(1), 1-5", "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we aim at providing a general reflection around the present and\nfuture of social media metrics (or altmetrics) and how they could evolve into a\nnew discipline focused on the study of the relationships and interactions\nbetween science and social media, in what could be seen as the social media\nstudies of science.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 13:26:06 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Costas", "Rodrigo", ""]]}, {"id": "1801.04479", "submitter": "Philipp Mayr", "authors": "Marcia Lei Zeng, Philipp Mayr", "title": "Knowledge Organization Systems (KOS) in the Semantic Web: A\n  Multi-Dimensional Review", "comments": "31 pages, 12 figures, accepted paper in International Journal on\n  Digital Libraries", "journal-ref": null, "doi": "10.1007/s00799-018-0241-2", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the Simple Knowledge Organization System (SKOS) specification and its\nSKOS eXtension for Labels (SKOS-XL) became formal W3C recommendations in 2009 a\nsignificant number of conventional knowledge organization systems (KOS)\n(including thesauri, classification schemes, name authorities, and lists of\ncodes and terms, produced before the arrival of the ontology-wave) have made\ntheir journeys to join the Semantic Web mainstream. This paper uses \"LOD KOS\"\nas an umbrella term to refer to all of the value vocabularies and lightweight\nontologies within the Semantic Web framework. The paper provides an overview of\nwhat the LOD KOS movement has brought to various communities and users. These\nare not limited to the colonies of the value vocabulary constructors and\nproviders, nor the catalogers and indexers who have a long history of applying\nthe vocabularies to their products. The LOD dataset producers and LOD service\nproviders, the information architects and interface designers, and researchers\nin sciences and humanities, are also direct beneficiaries of LOD KOS. The paper\nexamines a set of the collected cases (experimental or in real applications)\nand aims to find the usages of LOD KOS in order to share the practices and\nideas among communities and users. Through the viewpoints of a number of\ndifferent user groups, the functions of LOD KOS are examined from multiple\ndimensions. This paper focuses on the LOD dataset producers, vocabulary\nproducers, and researchers (as end-users of KOS).\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 18:58:44 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Zeng", "Marcia Lei", ""], ["Mayr", "Philipp", ""]]}, {"id": "1801.04971", "submitter": "Kathleen Gregory", "authors": "Kathleen Gregory, Helena Cousijn, Paul Groth, Andrea Scharnhorst,\n  Sally Wyatt", "title": "Understanding Data Search as a Socio-technical Practice", "comments": "19 pages, 3 figures, 7 tables", "journal-ref": "Journal of Information Science. (2019). 0165551519837182", "doi": "10.1177/0165551519837182", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open research data are heralded as having the potential to increase\neffectiveness, productivity, and reproducibility in science, but little is\nknown about the actual practices involved in data search. The socio-technical\nproblem of locating data for reuse is often reduced to the technological\ndimension of designing data search systems. We combine a bibliometric study of\nthe current academic discourse around data search with interviews with data\nseekers. In this article, we explore how adopting a contextual, socio-technical\nperspective can help to understand user practices and behavior and ultimately\nhelp to improve the design of data discovery systems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 20:09:56 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 09:42:25 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 09:36:28 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Gregory", "Kathleen", ""], ["Cousijn", "Helena", ""], ["Groth", "Paul", ""], ["Scharnhorst", "Andrea", ""], ["Wyatt", "Sally", ""]]}, {"id": "1801.05208", "submitter": "Stephan Stahlschmidt", "authors": "Stephan Stahlschmidt and Sybille Hinze", "title": "The dynamically changing publication universe as a reference point in\n  national impact evaluation: A counterfactual case study on the Chinese\n  publication growth", "comments": null, "journal-ref": null, "doi": "10.3389/frma.2018.00030", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  National bibliometric performance is commonly measured via relative impact\nindicators which appraise absolute national values through a global\nenvironment. Consequenty the resulting impact values mirror changes in the\nnational performance as well as in its embedding. In order to assess the\nimportance of the environment in this ratio, we analyse the increase in Chinese\npublications as an example for a structural change altering the whole database.\nVia a counterfactual comparison we quantify how Chinese publications benefit a\nlarge set of countries on their impact values, identify explanatory factors and\ndescribe the underelying mechanism due to longer reference lists and a\nnon-uniform citation distribution among recipient countries. We argue that such\nstructural changes in the environment have to be taken into account for an\nunbiased measurement of national bibliometric performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 11:08:51 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 10:09:05 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Stahlschmidt", "Stephan", ""], ["Hinze", "Sybille", ""]]}, {"id": "1801.05367", "submitter": "Ekta Vats", "authors": "Anders Hast, Per Cullhed, and Ekta Vats", "title": "TexT - Text Extractor Tool for Handwritten Document Transcription and\n  Annotation", "comments": null, "journal-ref": "Digital Libraries and Multimedia Archives. IRCDL 2018.\n  Communications in Computer and Information Science, vol 806. Springer, Cham", "doi": "10.1007/978-3-319-73165-0_8", "report-no": null, "categories": "cs.DL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for semi-automatic transcription of\nlarge-scale historical handwritten documents and proposes a simple\nuser-friendly text extractor tool, TexT for transcription. The proposed\napproach provides a quick and easy transcription of text using computer\nassisted interactive technique. The algorithm finds multiple occurrences of the\nmarked text on-the-fly using a word spotting system. TexT is also capable of\nperforming on-the-fly annotation of handwritten text with automatic generation\nof ground truth labels, and dynamic adjustment and correction of user generated\nbounding box annotations with the word being perfectly encapsulated. The user\ncan view the document and the found words in the original form or with\nbackground noise removed for easier visualization of transcription results. The\neffectiveness of TexT is demonstrated on an archival manuscript collection from\nwell-known publicly available dataset.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 12:05:33 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Hast", "Anders", ""], ["Cullhed", "Per", ""], ["Vats", "Ekta", ""]]}, {"id": "1801.05400", "submitter": "Shivakumar Jolad", "authors": "Chakresh Kumar Singh, Shivakumar Jolad", "title": "Structure and Evolution of Indian Physics Co-authorship Networks", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We trace the evolution of Indian physics community from 1919 to 2013 by\nanalysing the coauthorship network constructed from papers published by authors\nin India in American Physical Society journals. We make inferences on Indias\ncontribution to different branches of Physics and identify the most influential\nIndian physicists at different time periods. The relative contribution of India\nto global physics publication(research) and its variation across subfields of\nPhysics is assessed. We extract the changing collaboration pattern of authors\nbetween Indian physicists through various network measures. We study the\nevolution of Indian physics communities and trace the mean life and\nstationarity of communities by size in different APS journals. We map the\ntransition of authors between communities of different sizes from 1970 to 2013,\ncapturing their birth, growth, merger and collapse. We find that Indian-Foreign\ncollaborations are increasing at a faster pace compared to the Indian-Indian.\nWe observe that the degree distribution of Indian collaboration networks\nfollows the power law, with distinct patterns between Physical Review A, B and\nE, and high energy physics journals Physical Review C and D, and Physical\nReview Letters. In almost every measure, we observe strong structural\ndifferences between low-energy and high-energy physics journals.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 18:33:51 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Singh", "Chakresh Kumar", ""], ["Jolad", "Shivakumar", ""]]}, {"id": "1801.05916", "submitter": "Shuhua Liu", "authors": "Shuhua Monica Liu (1), Liting Pan (1), Xiaowei Chen (1) ((1)\n  Department of Public Administration, Fudan University, Shanghai, China)", "title": "Citation Analysis of Innovative ICT and Advances of Governance\n  (2008-2017)", "comments": "Corrected first author's name spelling and added authors' affiliation\n  in the metadata", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper opens by introducing the Internet Plus Government (IPG), a new\ngovernment initiative emerging in the last decade. To understand benefits and\nchallenges associated with this initiative worldwide, we conducted analyses on\nresearch articles published in the e-governance area between 2008 and 2017.\nContent analysis and citation analysis were performed on 2105 articles to\naddress three questions: (1) What types of new ICT have been adopted in the IPG\ninitiative in the past decade? (2) How did scholars investigate interactions\nbetween the new ICTs and governance core to IPG? (3) How did the new ICTs\ninteract and shape while also being shaped by the evolution of governance in\nthe past decade? Our analysis suggests that IPG initiative has enriched the\ngovernment information infrastructure. It presented opportunities to accumulate\nand use huge volume of data for better decision making and proactive\ngovernment-citizen interaction. At the same time, the advance of open data, the\nwidespread use of social media and the potential of data analytics also\ngenerated great pressure to address challenging questions and issues in the\ndomain of e-democracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 02:57:25 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 12:32:25 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Liu", "Shuhua Monica", ""], ["Pan", "Liting", ""], ["Chen", "Xiaowei", ""]]}, {"id": "1801.06552", "submitter": "Jim Hahn", "authors": "Jim Hahn", "title": "The Bibliotelemetry of Information and Environment: an Evaluation of\n  IoT-Powered Recommender Systems", "comments": "10 pages, 8 figures, 6 tables", "journal-ref": "Proceedings of the ASIS&T 2018 Annual Meeting, p.151-160", "doi": "10.1002/pra2.2018.14505501017", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) infrastructure within the physical library\nenvironment is the basis for an integrative, hybrid approach to digital\nresource recommenders. The IoT infrastructure provides mobile, dynamic\nwayfinding support for items in the collection, which includes features for\nlocation-based recommendations. A modular evaluation and analysis herein\nclarified the nature of users' requests for recommendations based on their\nlocation and describes subject areas of the library for which users request\nrecommendations. The modular mobile design allowed for deep exploration of\nusers' bibliographic identifiers throughout the global module system, serving\nto provide context to the browsing data that are the focus of this study.\nBibliotelemetry is introduced as an evaluation method for IoT middleware within\nlibrary collections.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 19:04:10 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 04:15:19 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 17:49:48 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Hahn", "Jim", ""]]}, {"id": "1801.06664", "submitter": "Noel Nuo Wi Tay", "authors": "Noel Nuo Wi Tay, Sheng-Chi Yang, Chang-Shing Lee, Naoyuki Kubota", "title": "Ontology-based Adaptive e-Textbook Platform for Student and Machine\n  Co-Learning", "comments": "This paper is submitted to IEEE WCCI 2018 Conference for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of electronic textbooks (e-book) has been heavily studied over the\nyears due to their flexibility, accessibility, interactivity and extensibility.\nYet current shortcomings of e-book, which is often just a digitized version of\nthe original book, does not encourage adoption. Consequently, this leads to a\nrethinking of e-book that should incorporate current technologies to augment\nits capabilities, where inclusion of information search and organization tools\nhave shown to be favorable. This paper is on a preliminary work to add\nintelligence into such tools in terms of information retrieval. Construction of\nknowledge graph for e-book material with little overhead is first introduced.\nInformation retrieval through typed similarity query is then performed via\nrandom walk. Case study demonstrate the applicability of the e-book platform,\nwith promising application and advancement in the area of electronic textbooks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 12:07:36 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Tay", "Noel Nuo Wi", ""], ["Yang", "Sheng-Chi", ""], ["Lee", "Chang-Shing", ""], ["Kubota", "Naoyuki", ""]]}, {"id": "1801.06717", "submitter": "Florian Mai", "authors": "Florian Mai, Lukas Galke, Ansgar Scherp", "title": "Using Deep Learning for Title-Based Semantic Subject Indexing to Reach\n  Competitive Performance to Full-Text", "comments": "Presented at JCDL 2018, 10 pages, code and data at\n  https://github.com/florianmai/Quadflor", "journal-ref": "JCDL '18: The 18th ACM/IEEE Joint Conference on Digital Libraries,\n  June 3--7, 2018, Fort Worth, TX, USA", "doi": "10.1145/3197026.3197039", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For (semi-)automated subject indexing systems in digital libraries, it is\noften more practical to use metadata such as the title of a publication instead\nof the full-text or the abstract. Therefore, it is desirable to have good text\nmining and text classification algorithms that operate well already on the\ntitle of a publication. So far, the classification performance on titles is not\ncompetitive with the performance on the full-texts if the same number of\ntraining samples is used for training. However, it is much easier to obtain\ntitle data in large quantities and to use it for training than full-text data.\nIn this paper, we investigate the question how models obtained from training on\nincreasing amounts of title training data compare to models from training on a\nconstant number of full-texts. We evaluate this question on a large-scale\ndataset from the medical domain (PubMed) and from economics (EconBiz). In these\ndatasets, the titles and annotations of millions of publications are available,\nand they outnumber the available full-texts by a factor of 20 and 15,\nrespectively. To exploit these large amounts of data to their full potential,\nwe develop three strong deep learning classifiers and evaluate their\nperformance on the two datasets. The results are promising. On the EconBiz\ndataset, all three classifiers outperform their full-text counterparts by a\nlarge margin. The best title-based classifier outperforms the best full-text\nmethod by 9.4%. On the PubMed dataset, the best title-based method almost\nreaches the performance of the best full-text classifier, with a difference of\nonly 2.9%.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 19:26:20 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 10:20:34 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Mai", "Florian", ""], ["Galke", "Lukas", ""], ["Scherp", "Ansgar", ""]]}, {"id": "1801.08720", "submitter": "Lutz Bornmann Dr.", "authors": "Andreas Thor, Lutz Bornmann, Werner Marx, R\\\"udiger Mutz", "title": "Identifying single influential publications in a research field: New\n  analysis opportunities of the CRExplorer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference Publication Year Spectroscopy (RPYS) has been developed for\nidentifying the cited references (CRs) with the greatest influence in a given\npaper set (mostly sets of papers on certain topics or fields). The program\nCRExplorer (see www.crexplorer.net) was specifically developed by Thor, Marx,\nLeydesdorff, and Bornmann (2016a, 2016b) for applying RPYS to publication sets\ndownloaded from Scopus or Web of Science. In this study, we present some\nadvanced methods which have been newly developed for CRExplorer. These methods\nare able to identify and characterize the CRs which have been influential\nacross a longer period (many citing years). The new methods are demonstrated in\nthis study using all the papers published in Scientometrics between 1978 and\n2016. The indicators N_TOP50, N_TOP25, and N_TOP10 can be used to identify\nthose CRs which belong to the 50%, 25%, or 10% most frequently cited\npublications (CRs) over many citing publication years. In the Scientometrics\ndataset, for example, Lotka's (1926) paper on the distribution of scientific\nproductivity belongs to the top 10% publications (CRs) in 36 citing years.\nFurthermore, the new version of CRExplorer analyzes the impact sequence of CRs\nacross citing years. CRs can have below average (-), average (0), or above\naverage (+) impact in citing years (whereby average is meant in the sense of\nexpected values). The sequence (e.g. 00++---0--00) is used by the program to\nidentify papers with typical impact distributions. For example, CRs can have\nearly, but not late impact (\"hot papers\", e.g. +++---) or vice versa (\"sleeping\nbeauties\", e.g. ---0000---++).\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 09:14:21 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 09:38:03 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Thor", "Andreas", ""], ["Bornmann", "Lutz", ""], ["Marx", "Werner", ""], ["Mutz", "R\u00fcdiger", ""]]}, {"id": "1801.08992", "submitter": "Vincent Lariviere", "authors": "Vincent Lariviere, Cassidy R. Sugimoto", "title": "The Journal Impact Factor: A brief history, critique, and discussion of\n  adverse effects", "comments": "33 pages, Forthcoming in Glanzel, W., Moed, H.F., Schmoch U.,\n  Thelwall, M. (2018). Springer Handbook of Science and Technology Indicators.\n  Cham (Switzerland): Springer International Publishing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Journal Impact Factor (JIF) is, by far, the most discussed bibliometric\nindicator. Since its introduction over 40 years ago, it has had enormous\neffects on the scientific ecosystem: transforming the publishing industry,\nshaping hiring practices and the allocation of resources, and, as a result,\nreorienting the research activities and dissemination practices of scholars.\nGiven both the ubiquity and impact of the indicator, the JIF has been widely\ndissected and debated by scholars of every disciplinary orientation. Drawing on\nthe existing literature as well as on original research, this chapter provides\na brief history of the indicator and highlights well-known limitations-such as\nthe asymmetry between the numerator and the denominator, differences across\ndisciplines, the insufficient citation window, and the skewness of the\nunderlying citation distributions. The inflation of the JIF and the weakening\npredictive power is discussed, as well as the adverse effects on the behaviors\nof individual actors and the research enterprise. Alternative journal-based\nindicators are described and the chapter concludes with a call for responsible\napplication and a commentary on future developments in journal indicators.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 22:12:06 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 15:39:59 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Lariviere", "Vincent", ""], ["Sugimoto", "Cassidy R.", ""]]}, {"id": "1801.09121", "submitter": "Jiangen He", "authors": "Jiangen He and Chaomei Chen", "title": "Predictive Effects of Novelty Measured by Temporal Embeddings on the\n  Growth of Scientific Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel scientific knowledge is constantly produced by the scientific\ncommunity. Understanding the level of novelty characterized by scientific\nliterature is key for modeling scientific dynamics and analyzing the growth\nmechanisms of scientific knowledge. Metrics derived from bibliometrics and\ncitation analysis were effectively used to characterize the novelty in\nscientific development. However, time is required before we can observe links\nbetween documents such as citation links or patterns derived from the links,\nwhich makes these techniques more effective for retrospective analysis than\npredictive analysis. In this study, we present a new approach to measuring the\nnovelty of a research topic in a scientific community over a specific period by\ntracking semantic changes of the terms and characterizing the research topic in\ntheir usage context. The semantic changes are derived from the text data of\nscientific literature by temporal embedding learning techniques. We validated\nthe effects of the proposed novelty metric on predicting the future growth of\nscientific publications and investigated the relations between novelty and\ngrowth by panel data analysis applied in a large-scale publication dataset\n(MEDLINE/PubMed). Key findings based on the statistical investigation indicate\nthat the novelty metric has significant predictive effects on the growth of\nscientific literature and the predictive effects may last for more than ten\nyears. We demonstrated the effectiveness and practical implications of the\nnovelty metric in three case studies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 18:38:43 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["He", "Jiangen", ""], ["Chen", "Chaomei", ""]]}, {"id": "1801.09472", "submitter": "AmirAbbas Davari", "authors": "AmirAbbas Davari, Nikolaos Sakaltras, Armin Haeberle, Sulaiman Vesal,\n  Vincent Christlein, Andreas Maier, Christian Riess", "title": "Hyper-Hue and EMAP on Hyperspectral Images for Supervised Layer\n  Decomposition of Old Master Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Old master drawings were mostly created step by step in several layers using\ndifferent materials. To art historians and restorers, examination of these\nlayers brings various insights into the artistic work process and helps to\nanswer questions about the object, its attribution and its authenticity.\nHowever, these layers typically overlap and are oftentimes difficult to\ndifferentiate with the unaided eye. For example, a common layer combination is\nred chalk under ink.\n  In this work, we propose an image processing pipeline that operates on\nhyperspectral images to separate such layers. Using this pipeline, we show that\nhyperspectral images enable better layer separation than RGB images, and that\nspectral focus stacking aids the layer separation. In particular, we propose to\nuse two descriptors in hyperspectral historical document analysis, namely\nhyper-hue and extended multi-attribute profile (EMAP). Our comparative results\nwith other features underline the efficacy of the three proposed improvements.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 12:29:44 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 12:30:48 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Davari", "AmirAbbas", ""], ["Sakaltras", "Nikolaos", ""], ["Haeberle", "Armin", ""], ["Vesal", "Sulaiman", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""]]}, {"id": "1801.09479", "submitter": "Loet Leydesdorff", "authors": "Jordan Comins and Loet Leydesdorff", "title": "Data-mining the Foundational Patents of Photovoltaic Materials: An\n  application of Patent Citation Spectroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply Patent Citation Spectroscopy (PCS)--originally developed as\nReference Publication Year Spectroscopy for studying landmarks and milestones\nin scientific literature--to patent literature classified into the nine\nY-subclasses of the Cooperative Patent Classification (CPC) that describe\nmaterial photovoltaic technologies. For this study we extended the routine with\nthe option to use the advanced search queries at PatentsView. On the basis of\ntwo normalizations of the longitudinal distribution of the publication years of\nthe patents cited by the retrieved patents, the routine (at\nhttp://www.leydesdorff.net/comins/pcs/index.html) provides a best guess of the\nfoundational patent for the subject specified in the string. In five of the\nnine cases, we found corroborating evidence for the foundational character of\nthe patent indicated by the routine.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 12:41:06 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 07:35:58 GMT"}, {"version": "v3", "created": "Sun, 8 Apr 2018 17:57:39 GMT"}, {"version": "v4", "created": "Tue, 10 Apr 2018 04:40:25 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Comins", "Jordan", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1801.09496", "submitter": "Gaurav Singh", "authors": "Gaurav Singh, James Thomas and John Shawe-Taylor", "title": "Improving Active Learning in Systematic Reviews", "comments": "10 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic reviews are essential to summarizing the results of different\nclinical and social science studies. The first step in a systematic review task\nis to identify all the studies relevant to the review. The task of identifying\nrelevant studies for a given systematic review is usually performed manually,\nand as a result, involves substantial amounts of expensive human resource.\nLately, there have been some attempts to reduce this manual effort using active\nlearning. In this work, we build upon some such existing techniques, and\nvalidate by experimenting on a larger and comprehensive dataset than has been\nattempted until now. Our experiments provide insights on the use of different\nfeature extraction models for different disciplines. More importantly, we\nidentify that a naive active learning based screening process is biased in\nfavour of selecting similar documents. We aimed to improve the performance of\nthe screening process using a novel active learning algorithm with success.\nAdditionally, we propose a mechanism to choose the best feature extraction\nmethod for a given review.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 13:26:48 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Singh", "Gaurav", ""], ["Thomas", "James", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1801.09985", "submitter": "Ludo Waltman", "authors": "Ludo Waltman and Nees Jan van Eck", "title": "Field normalization of scientometric indicators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When scientometric indicators are used to compare research units active in\ndifferent scientific fields, there often is a need to make corrections for\ndifferences between fields, for instance differences in publication,\ncollaboration, and citation practices. Field-normalized indicators aim to make\nsuch corrections. The design of these indicators is a significant challenge. We\ndiscuss the main issues in the design of field-normalized indicators, and we\npresent an overview of different approaches that have been developed for\ndealing with the problem of field normalization. We also discuss how\nfield-normalized indicators can be evaluated, and we consider the sensitivity\nof scientometric analyses to the choice of a field normalization approach.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 13:58:16 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Waltman", "Ludo", ""], ["van Eck", "Nees Jan", ""]]}, {"id": "1801.10080", "submitter": "Haimonti Dutta", "authors": "Aayushee Gupta, Haimonti Dutta, Srikanta Bedathur, Lipika Dey", "title": "A Machine Learning Approach to Quantitative Prosopography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prosopography is an investigation of the common characteristics of a group of\npeople in history, by a collective study of their lives. It involves a study of\nbiographies to solve historical problems. If such biographies are unavailable,\nsurviving documents and secondary biographical data are used. Quantitative\nprosopography involves analysis of information from a wide variety of sources\nabout \"ordinary people\". In this paper, we present a machine learning framework\nfor automatically designing a people gazetteer which forms the basis of\nquantitative prosopographical research. The gazetteer is learnt from the noisy\ntext of newspapers using a Named Entity Recognizer (NER). It is capable of\nidentifying influential people from it by making use of a custom designed\nInfluential Person Index (IPI). Our corpus comprises of 14020 articles from a\nlocal newspaper, \"The Sun\", published from New York in 1896. Some influential\npeople identified by our algorithm include Captain Donald Hankey (an English\nsoldier), Dame Nellie Melba (an Australian operatic soprano), Hugh Allan (a\nCanadian shipping magnate) and Sir Hugh John McDonald (the first Prime Minister\nof Canada).\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 16:13:55 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Gupta", "Aayushee", ""], ["Dutta", "Haimonti", ""], ["Bedathur", "Srikanta", ""], ["Dey", "Lipika", ""]]}, {"id": "1801.10311", "submitter": "Mike Thelwall Prof", "authors": "Mike Thelwall, Tamara Nevill", "title": "Could scientists use Altmetric.com scores to predict longer term\n  citation counts?", "comments": null, "journal-ref": "Journal of Informetrics, 12(1), 237-248 (2018)", "doi": "10.1016/j.joi.2018.01.008", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Altmetrics from Altmetric.com are widely used by publishers and researchers\nto give earlier evidence of attention than citation counts. This article\nassesses whether Altmetric.com scores are reliable early indicators of likely\nfuture impact and whether they may also reflect non-scholarly impacts. A\npreliminary factor analysis suggests that the main altmetric indicator of\nscholarly impact is Mendeley reader counts, with weaker news, informational and\nsocial network discussion/promotion dimensions in some fields. Based on a\nregression analysis of Altmetric.com data from November 2015 and Scopus\ncitation counts from October 2017 for articles in 30 narrow fields, only\nMendeley reader counts are consistent predictors of future citation impact.\nMost other Altmetric.com scores can help predict future impact in some fields.\nOverall, the results confirm that early Altmetric.com scores can predict later\ncitation counts, although less well than journal impact factors, and the\noptimal strategy is to consider both Altmetric.com scores and journal impact\nfactors. Altmetric.com scores can also reflect dimensions of non-scholarly\nimpact in some fields.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 06:18:55 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Thelwall", "Mike", ""], ["Nevill", "Tamara", ""]]}, {"id": "1801.10396", "submitter": "Emiliano De Cristofaro", "authors": "Savvas Zannettou, Jeremy Blackburn, Emiliano De Cristofaro, Michael\n  Sirivianos, Gianluca Stringhini", "title": "Understanding Web Archiving Services and Their (Mis)Use on Social Media", "comments": "Proceedings of the 12th International AAAI Conference on Web and\n  Social Media (ICWSM 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archiving services play an increasingly important role in today's\ninformation ecosystem, by ensuring the continuing availability of information,\nor by deliberately caching content that might get deleted or removed. Among\nthese, the Wayback Machine has been proactively archiving, since 2001, versions\nof a large number of Web pages, while newer services like archive.is allow\nusers to create on-demand snapshots of specific Web pages, which serve as time\ncapsules that can be shared across the Web. In this paper, we present a\nlarge-scale analysis of Web archiving services and their use on social media,\nshedding light on the actors involved in this ecosystem, the content that gets\narchived, and how it is shared. We crawl and study: 1) 21M URLs from\narchive.is, spanning almost two years, and 2) 356K archive.is plus 391K Wayback\nMachine URLs that were shared on four social networks: Reddit, Twitter, Gab,\nand 4chan's Politically Incorrect board (/pol/) over 14 months. We observe that\nnews and social media posts are the most common types of content archived,\nlikely due to their perceived ephemeral and/or controversial nature. Moreover,\nURLs of archiving services are extensively shared on \"fringe\" communities\nwithin Reddit and 4chan to preserve possibly contentious content. Lastly, we\nfind evidence of moderators nudging or even forcing users to use archives,\ninstead of direct links, for news sources with opposing ideologies, potentially\ndepriving them of ad revenue.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 10:53:46 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 14:36:27 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Zannettou", "Savvas", ""], ["Blackburn", "Jeremy", ""], ["De Cristofaro", "Emiliano", ""], ["Sirivianos", "Michael", ""], ["Stringhini", "Gianluca", ""]]}]