[{"id": "1806.00089", "submitter": "Chaomei Chen", "authors": "Chaomei Chen", "title": "Cascading Citation Expansion", "comments": "16 figures", "journal-ref": "Journal of Information Science Theory and Practice, 6(2), 6-23\n  (2018)", "doi": "10.1633/JISTaP.2018.6.2.1", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Science's Dimensions is envisaged as a next-generation research and\ndiscovery platform for a better and more efficient access to cross-referenced\nscholarly publications, grants, patents, and clinical trials. As a new addition\nto the growing open citation resources, it offers opportunities that may\nbenefit a wide variety of stakeholders of scientific publications from\nresearchers, policy makers, and the general public. In this article, we explore\nand demonstrate some of the practical potentials in terms of cascading citation\nexpansions. Given a set of publications, the cascading citation expansion\nprocess can be successively applied to a set of articles so as to extend the\ncoverage to more and more relevant articles through citation links. Although\nthe conceptual origin can be traced back to Garfield's citation indexing, it\nhas been largely limited, until recently, to the few who have unrestricted\naccess to a citation database that is large enough to sustain such iterative\nexpansions. Building on the open API of Dimensions, we integrate cascading\ncitation expansion functions in CiteSpace and demonstrate how one may benefit\nfrom these new capabilities. In conclusion, cascading citation expansion has\nthe potential to improve our understanding of the structure and dynamics of\nscientific knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 20:33:29 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Chen", "Chaomei", ""]]}, {"id": "1806.00224", "submitter": "Iman Tahamtan", "authors": "Iman Tahamtan, Lutz Bornmann", "title": "Creativity in Science and the Link to Cited References: Is the Creative\n  Potential of Papers Reflected in their Cited References?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several authors have proposed that a large number of unusual combinations of\ncited references in a paper point to its high creative potential (or novelty).\nHowever, it is still not clear whether the number of unusual combinations can\nreally measure the creative potential of papers. The current study addresses\nthis question on the basis of several case studies from the field of\nscientometrics. We identified some landmark papers in this field. Study\nsubjects were the corresponding authors of these papers. We asked them where\nthe ideas for the papers came from and which role the cited publications\nplayed. The results revealed that the creative ideas might not necessarily have\nbeen inspired by past publications. The literature seems to be important for\nthe contextualization of the idea in the field of scientometrics. Instead, we\nfound that creative ideas are the result of finding solutions to practical\nproblems, result from discussions with colleagues, and profit from\ninterdisciplinary exchange. The roots of the studied landmark papers are\ndiscussed in detail.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 07:43:31 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 12:03:45 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Tahamtan", "Iman", ""], ["Bornmann", "Lutz", ""]]}, {"id": "1806.00287", "submitter": "Ning Cai", "authors": "Zong-Yuan Tan, Ning Cai, Jian Zhou, Sheng-Guo Zhang", "title": "On Performance of Peer Review for Academic Journals: Analysis Based on\n  Distributed Parallel System", "comments": "This paper is already published in the journal IEEE Access and its\n  copyright is fully open. The title was adjusted according to the review\n  comments", "journal-ref": "IEEE Access, 2019, 7, pp. 19024-19032", "doi": "10.1109/ACCESS.2019.2896978", "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simulation model based on parallel systems is established, aiming to\nexplore the relation between the number of submissions and the overall standard\nof academic journals within a similar discipline under peer review. The model\ncan effectively simulate the submission, review, and acceptance behaviors of\nacademic journals in a distributed manner. According to the simulation\nexperiments, it could possibly happen that the overall standard of academic\njournals deteriorates due to excessive submissions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 11:09:30 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 02:44:06 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Tan", "Zong-Yuan", ""], ["Cai", "Ning", ""], ["Zhou", "Jian", ""], ["Zhang", "Sheng-Guo", ""]]}, {"id": "1806.00871", "submitter": "Mat Kelly", "authors": "Mat Kelly and Michael L. Nelson and Michele C. Weigle", "title": "A Framework for Aggregating Private and Public Web Archives", "comments": "Preprint version of the ACM/IEEE Joint Conference on Digital\n  Libraries (JCDL 2018) full paper, accessible at the DOI", "journal-ref": null, "doi": "10.1145/3197026.3197045", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Personal and private Web archives are proliferating due to the increase in\nthe tools to create them and the realization that Internet Archive and other\npublic Web archives are unable to capture personalized (e.g., Facebook) and\nprivate (e.g., banking) Web pages. We introduce a framework to mitigate issues\nof aggregation in private, personal, and public Web archives without\ncompromising potential sensitive information contained in private captures. We\namend Memento syntax and semantics to allow TimeMap enrichment to account for\nadditional attributes to be expressed inclusive of the requirements for\ndereferencing private Web archive captures. We provide a method to involve the\nuser further in the negotiation of archival captures in dimensions beyond time.\nWe introduce a model for archival querying precedence and short-circuiting, as\nneeded when aggregating private and personal Web archive captures with those\nfrom public Web archives through Memento. Negotiation of this sort is novel to\nWeb archiving and allows for the more seamless aggregation of various types of\nWeb archives to convey a more accurate picture of the past Web.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 20:30:09 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Kelly", "Mat", ""], ["Nelson", "Michael L.", ""], ["Weigle", "Michele C.", ""]]}, {"id": "1806.01507", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn", "title": "Using the AIDA Language to Formally Organize Scientific Claims", "comments": "To appear in the Proceedings of the Sixth International Workshop on\n  Controlled Natural Language (CNL 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific communication still mainly relies on natural language written in\nscientific papers, which makes the described knowledge very difficult to access\nwith automatic means. We can therefore only make limited use of formal\nknowledge organization methods to support researchers and other interested\nparties with features such as automatic aggregations, fact checking,\nconsistency checking, question answering, and powerful semantic search.\nExisting approaches to solve this problem by improving the scientific\ncommunication methods have either very restricted coverage, require formal\nlogic skills on the side of the researchers, or depend on unreliable machine\nlearning for the formalization of knowledge. Here, I propose an approach to\nthis problem that is general, intuitive, and flexible. It is based on a unique\nkind of controlled natural language, called AIDA, consisting of English\nsentences that are atomic, independent, declarative, and absolute. Such\nsentences can then serve as nodes in a network of scientific claims linked to\npublications, researchers, and domain elements. I present here some small\nstudies on preliminary applications of this language. The results indicate that\nit is well accepted by users and provides a good basis for the creation of a\nknowledge graph of scientific findings.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 06:01:06 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Kuhn", "Tobias", ""]]}, {"id": "1806.02284", "submitter": "Michele Dolfi", "authors": "Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas", "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest\n  Documents at Scale", "comments": "Accepted paper at KDD 2018 conference", "journal-ref": null, "doi": "10.1145/3219819.3219834", "report-no": null, "categories": "cs.DL cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, the amount of scientific articles and technical\nliterature has increased exponentially in size. Consequently, there is a great\nneed for systems that can ingest these documents at scale and make the\ncontained knowledge discoverable. Unfortunately, both the format of these\ndocuments (e.g. the PDF format or bitmap images) as well as the presentation of\nthe data (e.g. complex tables) make the extraction of qualitative and\nquantitive data extremely challenging. In this paper, we present a modular,\ncloud-based platform to ingest documents at scale. This platform, called the\nCorpus Conversion Service (CCS), implements a pipeline which allows users to\nparse and annotate documents (i.e. collect ground-truth), train\nmachine-learning classification algorithms and ultimately convert any type of\nPDF or bitmap-documents to a structured content representation format. We will\nshow that each of the modules is scalable due to an asynchronous microservice\narchitecture and can therefore handle massive amounts of documents.\nFurthermore, we will show that our capability to gather ground-truth is\naccelerated by machine-learning algorithms by at least one order of magnitude.\nThis allows us to both gather large amounts of ground-truth in very little time\nand obtain very good precision/recall metrics in the range of 99\\% with regard\nto content conversion to structured output. The CCS platform is currently\ndeployed on IBM internal infrastructure and serving more than 250 active users\nfor knowledge-engineering project engagements.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 09:44:07 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Staar", "Peter W J", ""], ["Dolfi", "Michele", ""], ["Auer", "Christoph", ""], ["Bekas", "Costas", ""]]}, {"id": "1806.02743", "submitter": "Martin Toepfer", "authors": "Martin Toepfer and Christin Seifert", "title": "Content-Based Quality Estimation for Automatic Subject Indexing of Short\n  Texts under Precision and Recall Constraints", "comments": "authors' manuscript, paper submitted to TPDL-2018 conference, 12\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic annotations have to satisfy quality constraints to be useful for\ndigital libraries, which is particularly challenging on large and diverse\ndatasets. Confidence scores of multi-label classification methods typically\nrefer only to the relevance of particular subjects, disregarding indicators of\ninsufficient content representation at the document-level. Therefore, we\npropose a novel approach that detects documents rather than concepts where\nquality criteria are met. Our approach uses a deep, multi-layered regression\narchitecture, which comprises a variety of content-based indicators. We\nevaluated multiple configurations using text collections from law and\neconomics, where the available content is restricted to very short texts.\nNotably, we demonstrate that the proposed quality estimation technique can\ndetermine subsets of the previously unseen data where considerable gains in\ndocument-level recall can be achieved, while upholding precision at the same\ntime. Hence, the approach effectively performs a filtering that ensures high\ndata quality standards in operative information retrieval systems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 15:58:59 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Toepfer", "Martin", ""], ["Seifert", "Christin", ""]]}, {"id": "1806.03694", "submitter": "Yuxiao Dong", "authors": "Yuxiao Dong, Hao Ma, Jie Tang, Kuansan Wang", "title": "Collaboration Diversity and Scientific Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shift from individual effort to collaborative output has benefited\nscience, with scientific work pursued collaboratively having increasingly led\nto more highly impactful research than that pursued individually. However,\nunderstanding of how the diversity of a collaborative team influences the\nproduction of knowledge and innovation is sorely lacking. Here, we study this\nquestion by breaking down the process of scientific collaboration of 32.9\nmillion papers over the last five decades. We find that the probability of\nproducing a top-cited publication increases as a function of the diversity of a\nteam of collaborators---namely, the distinct number of institutions represented\nby the team. We discover striking phenomena where a smaller, yet more diverse\nteam is more likely to generate highly innovative work than a relatively larger\nteam within one institution. We demonstrate that the synergy of collaboration\ndiversity is universal across different generations, research fields, and tiers\nof institutions and individual authors. Our findings suggest that collaboration\ndiversity strongly and positively correlates with the production of scientific\ninnovation, giving rise to the potential revolution of the policies used by\nfunding agencies and authorities to fund research projects, and broadly the\nprinciples used to organize teams, organizations, and societies.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 17:31:56 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Dong", "Yuxiao", ""], ["Ma", "Hao", ""], ["Tang", "Jie", ""], ["Wang", "Kuansan", ""]]}, {"id": "1806.04004", "submitter": "Nicolas Fiorini", "authors": "Nicolas Fiorini, Kathi Canese, Rostyslav Bryzgunov, Ievgeniia\n  Radetska, Asta Gindulyte, Martin Latterner, Vadim Miller, Maxim Osipov,\n  Michael Kholodov, Grisha Starchenko, Evgeny Kireev, Zhiyong Lu", "title": "PubMed Labs: An experimental platform for improving biomedical\n  literature search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PubMed is a freely accessible system for searching the biomedical literature,\nwith approximately 2.5 million users worldwide on an average workday. We have\nrecently developed PubMed Labs (www.pubmed.gov/labs), an experimental platform\nfor users to test new features/tools and provide feedback, which enables us to\nmake more informed decisions about potential changes to improve the search\nquality and overall usability of PubMed. In doing so, we hope to better meet\nour user needs in an era of information overload. Another novel aspect of\nPubMed Labs lies in its mobile-first and responsive layout, which offers better\nsupport for accessing PubMed on the increasingly popular use of mobile and\nsmall-screen devices. Currently, PubMed Labs only includes a core subset of\nPubMed functionalities, e.g. search, facets. We encourage users to test PubMed\nLabs and share their experience with us, based on which we expect to\ncontinuously improve PubMed Labs with more advanced features and better user\nexperience.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 14:10:01 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Fiorini", "Nicolas", ""], ["Canese", "Kathi", ""], ["Bryzgunov", "Rostyslav", ""], ["Radetska", "Ievgeniia", ""], ["Gindulyte", "Asta", ""], ["Latterner", "Martin", ""], ["Miller", "Vadim", ""], ["Osipov", "Maxim", ""], ["Kholodov", "Michael", ""], ["Starchenko", "Grisha", ""], ["Kireev", "Evgeny", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1806.04406", "submitter": "Vasyl Palchykov", "authors": "Vasyl Palchykov and Yurij Holovatch", "title": "Bipartite graph analysis as an alternative to reveal clusterization in\n  complex systems", "comments": "5 pages, 2 figures, submitted to IEEE Second International Conference\n  Data Stream Mining & Processing (Dsmp2018)", "journal-ref": "IEEE Second International Conference on Data Stream Mining &\n  Processing (Dsmp2018) IEEE Catalog Number: CFP18J13-POD, ISBN:\n  978-1-5386-2875-1 (2018) pp. 84-87", "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how analysis of co-clustering in bipartite networks may be\nused as a bridge to connect, compare and complement clustering results about\ncommunity structure in two different spaces: single-mode bipartite network\nprojections. As a case study we consider scientific knowledge, which is\nrepresented as a complex bipartite network of articles and related concepts.\nConnecting clusters of articles and clusters of concepts via article-to-concept\nbipartite co-clustering, we demonstrate how concept features (e.g. subject\nclasses) may be inferred from the article ones.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 09:24:27 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Palchykov", "Vasyl", ""], ["Holovatch", "Yurij", ""]]}, {"id": "1806.04435", "submitter": "Alberto Mart\\'in-Mart\\'in", "authors": "Emilio Delgado L\\'opez-C\\'ozar, Enrique Orduna-Malea, Alberto\n  Mart\\'in-Mart\\'in", "title": "Google Scholar as a data source for research assessment", "comments": "42 pages. Forthcoming in: Springer Handbook of Science and Technology\n  Indicators (Editors: Wolfgang Glaenzel, Henk Moed, Ulrich Schmoch, Michael\n  Thelwall)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The launch of Google Scholar (GS) marked the beginning of a revolution in the\nscientific information market. This search engine, unlike traditional\ndatabases, automatically indexes information from the academic web. Its ease of\nuse, together with its wide coverage and fast indexing speed, have made it the\nfirst tool most scientists currently turn to when they need to carry out a\nliterature search. Additionally, the fact that its search results were\naccompanied from the beginning by citation counts, as well as the later\ndevelopment of secondary products which leverage this citation data (such as\nGoogle Scholar Metrics and Google Scholar Citations), made many scientists\nwonder about its potential as a source of data for bibliometric analyses. The\ngoal of this chapter is to lay the foundations for the use of GS as a\nsupplementary source (and in some disciplines, arguably the best alternative)\nfor scientific evaluation. First, we present a general overview of how GS\nworks. Second, we present empirical evidences about its main characteristics\n(size, coverage, and growth rate). Third, we carry out a systematic analysis of\nthe main limitations this search engine presents as a tool for the evaluation\nof scientific performance. Lastly, we discuss the main differences between GS\nand other more traditional bibliographic databases in light of the correlations\nfound between their citation data. We conclude that Google Scholar presents a\nbroader view of the academic world because it has brought to light a great\namount of sources that were not previously visible.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 11:06:36 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 10:23:54 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""], ["Orduna-Malea", "Enrique", ""], ["Mart\u00edn-Mart\u00edn", "Alberto", ""]]}, {"id": "1806.04641", "submitter": "Sabine Hossenfelder", "authors": "Tobias Mistele, Tom Price, Sabine Hossenfelder", "title": "Predicting Citation Counts with a Neural Network", "comments": "12 figure, 17 pages, typo fixed, reference updated", "journal-ref": "Scientometrics (2019) 120: 87", "doi": "10.1007/s11192-019-03110-2", "report-no": null, "categories": "cs.DL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here describe and present results of a simple neural network that predicts\nindividual researchers' future citation counts based on a variety of data from\nthe researchers' past. For publications available on the open access-server\narXiv.org we find a higher predictability than previous studies.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 16:54:12 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 06:08:45 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Mistele", "Tobias", ""], ["Price", "Tom", ""], ["Hossenfelder", "Sabine", ""]]}, {"id": "1806.05029", "submitter": "Alberto Mart\\'in-Mart\\'in", "authors": "Alberto Mart\\'in-Mart\\'in, Rodrigo Costas, Thed N. van Leeuwen, Emilio\n  Delgado L\\'opez-C\\'ozar", "title": "Unbundling Open Access dimensions: a conceptual discussion to reduce\n  terminology inconsistencies", "comments": "8 pages, 1 figure. Accepted as oral presentation in 23rd STI\n  conference (2018)", "journal-ref": null, "doi": "10.17605/OSF.IO/7B4AJ", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current ways in which documents are made freely accessible in the Web no\nlonger adhere to the models established Budapest/Bethesda/Berlin (BBB)\ndefinitions of Open Access (OA). Since those definitions were established,\nOA-related terminology has expanded, trying to keep up with all the variants of\nOA publishing that are out there. However, the inconsistent and arbitrary\nterminology that is being used to refer to these variants are complicating\ncommunication about OA-related issues. This study intends to initiate a\ndiscussion on this issue, by proposing a conceptual model of OA. Our model\nfeatures six different dimensions (prestige, user rights, stability, immediacy,\npeer-review, and cost). Each dimension allows for a range of different options.\nWe believe that by combining the options in these six dimensions, we can arrive\nat all the current variants of OA, while avoiding ambiguous and/or arbitrary\nterminology. This model can be an useful tool for funders and policy makers who\nneed to decide exactly which aspects of OA are necessary for each specific\nscenario.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:36:59 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 07:39:53 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Mart\u00edn-Mart\u00edn", "Alberto", ""], ["Costas", "Rodrigo", ""], ["van Leeuwen", "Thed N.", ""], ["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""]]}, {"id": "1806.05259", "submitter": "Philipp Mayr", "authors": "Ameni Kacem, Philipp Mayr", "title": "Analysis of Search Stratagem Utilisation", "comments": "20 pages, 3 figures, accepted paper in Scientometrics", "journal-ref": null, "doi": "10.1007/s11192-018-2821-8", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Interactive IR, researchers consider the user behaviour towards systems\nand search tasks in order to adapt search results and to improve the search\nexperience of users. Analysing the users' past interactions with the system is\none typical approach. In this paper, we analyse the user behaviour in retrieval\nsessions towards Marcia Bates' search stratagems such as Footnote Chasing,\nCitation Searching, Keyword Searching, Author Searching and Journal Run in a\nreal-life academic search engine. In fact, search stratagems represent\nhigh-level search behaviour as the users go beyond simple execution of queries\nand investigate more of the system functionalities. We performed analyses of\nthese five search stratagems using two datasets extracted from the social\nsciences search engine sowiport. A specific focus was the detection of the\nsearch phase and frequency of the usage of these stratagems. In addition, we\nexplored the impact of these stratagems on the whole search process\nperformance. We addressed mainly the usage patterns' observation of the\nstratagems, their impact on the conduct of retrieval sessions and explore\nwhether they are used similarly in both datasets. From the observation and\nmetrics proposed, we can conclude that the utilisation of search stratagems in\nreal retrieval sessions leads to an improvement of the precision in terms of\npositive interactions. However, the difference is that Footnote Chasing,\nCitation Searching and Journal Run appear mostly at the end of a session while\nKeyword and Author Searching appear typically at the beginning. Thus, we can\nconclude from the log analysis that the improvement of search functionalities\nincluding personalisation and/or recommendation could be achieved by\nconsidering references, citations, and journals in the ranking process.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 20:29:52 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Kacem", "Ameni", ""], ["Mayr", "Philipp", ""]]}, {"id": "1806.06017", "submitter": "Marcel R. Ackermann", "authors": "Marcel R. Ackermann, Florian Reitz", "title": "Homonym Detection in Curated Bibliographies: Learning from dblp's\n  Experience (full version)", "comments": "13 pages, 5 figures, short paper version accepted at TPDL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying (and fixing) homonymous and synonymous author profiles is one of\nthe major tasks of curating personalized bibliographic metadata repositories\nlike the dblp computer science bibliography. In this paper, we present and\nevaluate a machine learning approach to identify homonymous author\nbibliographies using a simple multilayer perceptron setup. We train our model\non a novel gold-standard data set derived from the past years of active, manual\ncuration at the dblp computer science bibliography.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 15:31:10 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Ackermann", "Marcel R.", ""], ["Reitz", "Florian", ""]]}, {"id": "1806.06351", "submitter": "Enrique Ordu\\~na-Malea", "authors": "Emilio Delgado Lopez-Cozar, Enrique Orduna-Malea, Alberto\n  Martin-Martin, Juan M. Ayllon", "title": "Google Scholar: the 'big data' bibliographic tool", "comments": "31 pages with 6 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The launch of Google Scholar back in 2004 meant a revolution not only in the\nscientific information search market but also in research evaluation processes.\nIts dynamism, unparalleled coverage, and uncontrolled indexing make of Google\nScholar an unusual product, especially when compared to traditional\nbibliographic databases. Conceived primarily as a discovery tool for academic\ninformation, it presents a number of limitations as a bibliometric tool. The\nmain objective of this chapter is to show how Google Scholar operates and how\nits core database may be used for bibliometric purposes. To do this, the\ngeneral features of the search engine (in terms of document typologies,\ndisciplines, and coverage) are analysed. Lastly, several bibliometric tools\nbased on Google Scholar data, both official (Google Scholar Metrics, Google\nScholar Citations), and some developed by third parties (H Index Scholar,\nPublishers Scholar Metrics, Proceedings Scholar Metrics, Journal Scholar\nMetrics, Scholar Mirrors), as well as software to collect and process data from\nthis source (Publish or Perish, Scholarometer) are introduced, aiming to\nillustrate the potential bibliometric uses of this source.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 09:00:46 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lopez-Cozar", "Emilio Delgado", ""], ["Orduna-Malea", "Enrique", ""], ["Martin-Martin", "Alberto", ""], ["Ayllon", "Juan M.", ""]]}, {"id": "1806.06452", "submitter": "Dai-Hai Ton That", "authors": "Zhihao Yuan, Dai Hai Ton That, Siddhant Kothari, Gabriel Fils, Tanu\n  Malik", "title": "Utilizing Provenance in Reusable Research Objects", "comments": "25 pages", "journal-ref": "Informatics 2018, 5(1), 14", "doi": "10.3390/informatics5010014", "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science is conducted collaboratively, often requiring the sharing of\nknowledge about computational experiments. When experiments include only\ndatasets, they can be shared using Uniform Resource Identifiers (URIs) or\nDigital Object Identifiers (DOIs). An experiment, however, seldom includes only\ndatasets, but more often includes software, its past execution, provenance, and\nassociated documentation. The Research Object has recently emerged as a\ncomprehensive and systematic method for aggregation and identification of\ndiverse elements of computational experiments. While a necessary method, mere\naggregation is not sufficient for the sharing of computational experiments.\nOther users must be able to easily recompute on these shared research objects.\nComputational provenance is often the key to enable such reuse. In this paper,\nwe show how reusable research objects can utilize provenance to correctly\nrepeat a previous reference execution, to construct a subset of a research\nobject for partial reuse, and to reuse existing contents of a research object\nfor modified reuse. We describe two methods to summarize provenance that aid in\nunderstanding the contents and past executions of a research object. The first\nmethod obtains a process-view by collapsing low-level system information, and\nthe second method obtains a summary graph by grouping related nodes and edges\nwith the goal to obtain a graph view similar to application workflow. Through\ndetailed experiments, we show the efficacy and efficiency of our algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 21:47:58 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Yuan", "Zhihao", ""], ["That", "Dai Hai Ton", ""], ["Kothari", "Siddhant", ""], ["Fils", "Gabriel", ""], ["Malik", "Tanu", ""]]}, {"id": "1806.06796", "submitter": "Matthias Springstein", "authors": "Matthias Springstein, Huu Hung Nguyen, Anett Hoppe, Ralph Ewerth", "title": "TIB-arXiv: An Alternative Search Portal for the arXiv Pre-print Server", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  arXiv is a popular pre-print server focusing on natural science disciplines\n(e.g. physics, computer science, quantitative biology). As a platform with\nfocus on easy publishing services it does not provide enhanced search\nfunctionality -- but offers programming interfaces which allow external parties\nto add these services. This paper presents extensions of the open source\nframework arXiv Sanity Preserver (SP). With respect to the original framework,\nit derestricts the topical focus and allows for text-based search and\nvisualisation of all papers in arXiv. To this end, all papers are stored in a\nunified back-end; the extension provides enhanced search and ranking facilities\nand allows the exploration of arXiv papers by a novel user interface.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 16:05:22 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Springstein", "Matthias", ""], ["Nguyen", "Huu Hung", ""], ["Hoppe", "Anett", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1806.06870", "submitter": "Shawn Jones", "authors": "Shawn M. Jones, Michele C. Weigle, Michael L. Nelson", "title": "The Off-Topic Memento Toolkit", "comments": "10 pages, 14 figures, to appear in the proceedings of the 15th\n  International Conference on Digital Preservation (iPres 2018)", "journal-ref": null, "doi": "10.17605/OSF.IO/UBW87", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archive collections are created with a particular purpose in mind. A\ncurator selects seeds, or original resources, which are then captured by an\narchiving system and stored as archived web pages, or mementos. The systems\nthat build web archive collections are often configured to revisit the same\noriginal resource multiple times. This is incredibly useful for understanding\nan unfolding news story or the evolution of an organization. Unfortunately,\nover time, some of these original resources can go off-topic and no longer suit\nthe purpose for which the collection was originally created. They can go\noff-topic due to web site redesigns, changes in domain ownership, financial\nissues, hacking, technical problems, or because their content has moved on from\nthe original topic. Even though they are off-topic, the archiving system will\nstill capture them, thus it becomes imperative to anyone performing research on\nthese collections to identify these off-topic mementos. Hence, we present the\nOff-Topic Memento Toolkit, which allows users to detect off-topic mementos\nwithin web archive collections. The mementos identified by this toolkit can\nthen be separately removed from a collection or merely excluded from downstream\nanalysis. The following similarity measures are available: byte count, word\ncount, cosine similarity, Jaccard distance, S{\\o}rensen-Dice distance, Simhash\nusing raw text content, Simhash using term frequency, and Latent Semantic\nIndexing via the gensim library. We document the implementation of each of\nthese similarity measures. We possess a gold standard dataset generated by\nmanual analysis, which contains both off-topic and on-topic mementos. Using\nthis gold standard dataset, we establish a default threshold corresponding to\nthe best F1 score for each measure. We also provide an overview of potential\nfuture directions that the toolkit may take.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 18:05:12 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 17:52:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jones", "Shawn M.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1806.06878", "submitter": "Shawn Jones", "authors": "Shawn M. Jones, Alexander Nwala, Michele C. Weigle, Michael L. Nelson", "title": "The Many Shapes of Archive-It", "comments": "10 pages, 12 figures, to appear in the proceedings of the 15th\n  International Conference on Digital Preservation (iPres 2018)", "journal-ref": null, "doi": "10.17605/OSF.IO/EV42P", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archives, a key area of digital preservation, meet the needs of\njournalists, social scientists, historians, and government organizations. The\nuse cases for these groups often require that they guide the archiving process\nthemselves, selecting their own original resources, or seeds, and creating\ntheir own web archive collections. We focus on the collections within\nArchive-It, a subscription service started by the Internet Archive in 2005 for\nthe purpose of allowing organizations to create their own collections of\narchived web pages, or mementos. Understanding these collections could be done\nvia their user-supplied metadata or via text analysis, but the metadata is\napplied inconsistently between collections and some Archive-It collections\nconsist of hundreds of thousands of seeds, making it costly in terms of time to\ndownload each memento. Our work proposes using structural metadata as an\nadditional way to understand these collections. We explore structural features\ncurrently existing in these collections that can unveil curation and crawling\nbehaviors. We adapt the concept of the collection growth curve for\nunderstanding Archive-It collection curation and crawling behavior. We also\nintroduce several seed features and come to an understanding of the diversity\nof resources that make up a collection. Finally, we use the descriptions of\neach collection to identify four semantic categories of Archive-It collections.\nUsing the identified structural features, we reviewed the results of runs with\n20 classifiers and are able to predict the semantic category of a collection\nusing a Random Forest classifier with a weighted average F1 score of 0.720,\nthus bridging the structural to the descriptive. Our method is useful because\nit saves the researcher time and bandwidth. Identifying collections by their\nsemantic category allows further downstream processing to be tailored to these\ncategories.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 18:27:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jones", "Shawn M.", ""], ["Nwala", "Alexander", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1806.07122", "submitter": "Seyyed Mehdi Hosseini Jenab", "authors": "S. M. Hosseini Jenab", "title": "Cross-country comparisons of scientific performance by focusing on\n  post-apartheid South Africa", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the scientific performance of South Africa since 1994\n(post-apartheid) until 2014 in comparisons with the rest of the world,\nutilizing relative indicator. It provides a view over current standing of South\nAfrica in the scientific world as well as its temporal evolution after the\napartheid. This study focuses on four major aspects of scientific performance,\nnamely quantity, productivity, impact and quality, as the main attributes of\nscientific perfomance on national level. These are measured by re-based\n(relative) publication, publication per population or GDP, citations and\ncitations per publication respectively. The study focuses on scientific outputs\n(in the form of papers published in peer-reviewed journals) and their impact\n(measured by the citations they have received) to bring into a light a\nhomogeneous comprehension of South Africa's scientific performance in all these\nfour aspects. Indicators are adopted cautiously by considering the measures put\nforward recently for scientometrics indicators and their usage in the long-term\ncomparisons studies. The temporal evolution of these indicators for South\nAfrica are discussed in the context of three major groups of countries, namely\nAfrican countries, developing countries, and developed (including BRICS)\ncountries. It aims to examine the process of transition of South Africa from a\ndeveloping world economy system into a knowledge-based and innovation-driven\none of the developed world. The study reveals that South Africa has shown\nsteady increase in its scientific performance during the studied period when\ncompared to the rest of the world. However, due to the increasing competition\nfrom the other developing countries, South Africa's position stands the same\nduring this period, while countries such as China, Iran, Turkey and Malaysia\nhave shown great jump at least in the quantity of their scientific performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 09:20:55 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Jenab", "S. M. Hosseini", ""]]}, {"id": "1806.07309", "submitter": "Christian Otto", "authors": "Justyna Medrek, Christian Otto, Ralph Ewerth", "title": "Recommending Scientific Videos based on Metadata Enrichment using Linked\n  Open Data", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of available videos in the Web has significantly increased not\nonly for entertainment etc., but also to convey educational or scientific\ninformation in an effective way. There are several web portals that offer\naccess to the latter kind of video material. One of them is the TIB AV-Portal\nof the Leibniz Information Centre for Science and Technology (TIB), which hosts\nscientific and educational video content. In contrast to other video portals,\nautomatic audiovisual analysis (visual concept classification, optical\ncharacter recognition, speech recognition) is utilized to enhance metadata\ninformation and semantic search. In this paper, we propose to further exploit\nand enrich this automatically generated information by linking it to the\nIntegrated Authority File (GND) of the German National Library. This\ninformation is used to derive a measure to compare the similarity of two videos\nwhich serves as a basis for recommending semantically similar videos. A user\nstudy demonstrates the feasibility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 15:34:45 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 08:23:08 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Medrek", "Justyna", ""], ["Otto", "Christian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1806.07524", "submitter": "Yichen Hu Mr", "authors": "Yichen Hu, Qing Wang, Peter Christen", "title": "Developing a Temporal Bibliographic Data Set for Entity Resolution", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution is the process of identifying groups of records within or\nacross data sets where each group represents a real-world entity. Novel\ntechniques that consider temporal features to improve the quality of entity\nresolution have recently attracted significant attention. However, there are\ncurrently no large data sets available that contain both temporal information\nas well as ground truth information to evaluate the quality of temporal entity\nresolution approaches. In this paper, we describe the preparation of a temporal\ndata set based on author profiles extracted from the Digital Bibliography and\nLibrary Project (DBLP). We completed missing links between publications and\nauthor profiles in the DBLP data set using the DBLP public API. We then used\nthe Microsoft Academic Graph (MAG) to link temporal affiliation information for\nDBLP authors. We selected around 80K (1%) of author profiles that cover 2\nmillion (50%) publications using information in DBLP such as alternative author\nnames and personal web profile to improve the reliability of the resulting\nground truth, while at the same time keeping the data set challenging for\ntemporal entity resolution research.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 02:14:09 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Hu", "Yichen", ""], ["Wang", "Qing", ""], ["Christen", "Peter", ""]]}, {"id": "1806.07603", "submitter": "Anett Hoppe", "authors": "Anett Hoppe and Jascha Hagen and Helge Holzmann and G\\\"unter Kniesel\n  and Ralph Ewerth", "title": "An Analytics Tool for Exploring Scientific Software and Related\n  Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific software is one of the key elements for reproducible research.\nHowever, classic publications and related scientific software are typically not\n(sufficiently) linked, and it lacks tools to jointly explore these artefacts.\nIn this paper, we report on our work on developing an analytics tool for\njointly exploring software and publications. The presented prototype, a concept\nfor automatic code discovery, and two use cases demonstrate the feasibility and\nusefulness of the proposal.\n  Submitted to TPDL 2018 as Demonstration Paper.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 08:17:19 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Hoppe", "Anett", ""], ["Hagen", "Jascha", ""], ["Holzmann", "Helge", ""], ["Kniesel", "G\u00fcnter", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1806.07815", "submitter": "Nicolas Robinson-Garcia", "authors": "Nicolas Robinson-Garcia, Cassidy R. Sugimoto, Dakota Murray, Alfredo\n  Yegros-Yegros, Vincent Larivi\\`ere and Rodrigo Costas", "title": "Scientific mobility indicators in practice: International mobility\n  profiles at the country level", "comments": null, "journal-ref": "Robinson-Garcia, N. et al. Scientific mobility indicators in\n  practice: International mobility profiles at the country level. El\n  profesional de la informaci\\'on, 27(3), 511-520. doi:10.3145/epi.2018.may.05", "doi": "10.3145/epi.2018.may.05", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents and describes the methodological opportunities offered by\nbibliometric data to produce indicators of scientific mobility. Large\nbibliographic datasets of disambiguated authors and their affiliations allow\nfor the possibility of tracking the affiliation changes of scientists. Using\nthe Web of Science as data source, we analyze the distribution of types of\nmobile scientists for a selection of countries. We explore the possibility of\ncreating profiles of international mobility at the country level, and discuss\npotential interpretations and caveats. Five countries (Canada, The Netherlands,\nSouth Africa, Spain, and the United States) are used as examples. These\nprofiles enable us to characterize these countries in terms of their strongest\nlinks with other countries. This type of analysis reveals circulation among and\nbetween countries with strong policy implications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:13:37 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Robinson-Garcia", "Nicolas", ""], ["Sugimoto", "Cassidy R.", ""], ["Murray", "Dakota", ""], ["Yegros-Yegros", "Alfredo", ""], ["Larivi\u00e8re", "Vincent", ""], ["Costas", "Rodrigo", ""]]}, {"id": "1806.07842", "submitter": "Yuri G. Gordienko", "authors": "Olga Barkova, Natalia Pysarevska, Oleg Allenin, Serhii Hamotsky,\n  Nikita Gordienko, Vladyslav Sarnatskyi, Vadym Ovcharenko, Mariia Tkachenko,\n  Yurii Gordienko, Sergei Stirenko", "title": "Gamification for Education of the Digitally Native Generation by Means\n  of Virtual Reality, Augmented Reality, Machine Learning, and Brain-Computing\n  Interfaces in Museums", "comments": "16 pages, 8 figures,\n  http://uncommonculture.org/ojs/index.php/UC/article/view/9238", "journal-ref": "Uncommon Culture, vol. 7, no.1/2(13/14), pp.86-101 (2018)", "doi": null, "report-no": null, "categories": "cs.CY cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particularly close attention is being paid today among researchers in social\nscience disciplines to aspects of learning in the digital age, especially for\nthe Digitally Native Generation. In the context of museums, the question is:\nhow can rich learning experiences be provided for increasingly technologically\nadvanced young visitors in museums? Which high-tech platforms and solutions do\nmuseums need to focus on? At the same time, the software games business is\ngrowing fast and now finding its way into non-entertainment contexts, helping\nto deliver substantial benefits, particularly in education, training, research,\nand health. This article outlines some aspects facing Digitally Native learners\nin museums through an analysis of several radically new key technologies:\nInteractivity, Wearables, Virtual Reality, and Augmented Reality. Special\nattention is paid to use cases for application of games-based scenarios via\nthese technologies in non-leisure contexts and specifically for educational\npurposes in museums.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:03:52 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Barkova", "Olga", ""], ["Pysarevska", "Natalia", ""], ["Allenin", "Oleg", ""], ["Hamotsky", "Serhii", ""], ["Gordienko", "Nikita", ""], ["Sarnatskyi", "Vladyslav", ""], ["Ovcharenko", "Vadym", ""], ["Tkachenko", "Mariia", ""], ["Gordienko", "Yurii", ""], ["Stirenko", "Sergei", ""]]}, {"id": "1806.08202", "submitter": "Hussein AL-Natsheh", "authors": "Hussein T. Al-Natsheh, Lucie Martinet, Fabrice Muhlenbach, Fabien\n  Rico, Djamel A. Zighed", "title": "Metadata Enrichment of Multi-Disciplinary Digital Library: A\n  Semantic-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the scientific digital libraries, some papers from different research\ncommunities can be described by community-dependent keywords even if they share\na semantically similar topic. Articles that are not tagged with enough keyword\nvariations are poorly indexed in any information retrieval system which limits\npotentially fruitful exchanges between scientific disciplines. In this paper,\nwe introduce a novel experimentally designed pipeline for multi-label\nsemantic-based tagging developed for open-access metadata digital libraries.\nThe approach starts by learning from a standard scientific categorization and a\nsample of topic tagged articles to find semantically relevant articles and\nenrich its metadata accordingly. Our proposed pipeline aims to enable\nresearchers reaching articles from various disciplines that tend to use\ndifferent terminologies. It allows retrieving semantically relevant articles\ngiven a limited known variation of search terms. In addition to achieving an\naccuracy that is higher than an expanded query based method using a topic\nsynonym set extracted from a semantic network, our experiments also show a\nhigher computational scalability versus other comparable techniques. We created\na new benchmark extracted from the open-access metadata of a scientific digital\nlibrary and published it along with the experiment code to allow further\nresearch in the topic.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 12:35:23 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Al-Natsheh", "Hussein T.", ""], ["Martinet", "Lucie", ""], ["Muhlenbach", "Fabrice", ""], ["Rico", "Fabien", ""], ["Zighed", "Djamel A.", ""]]}, {"id": "1806.08246", "submitter": "Eric M\\\"uller-Budack", "authors": "Eric M\\\"uller-Budack, Kader Pustu-Iren, Sebastian Diering, Ralph\n  Ewerth", "title": "Finding Person Relations in Image Data of the Internet Archive", "comments": null, "journal-ref": "In: M\\'endez E., Crestani F., Ribeiro C., David G., Lopes J. (eds)\n  Digital Libraries for Open Knowledge. TPDL 2018. Lecture Notes in Computer\n  Science, vol 11057. Springer, Cham", "doi": "10.1007/978-3-030-00066-0_20", "report-no": null, "categories": "cs.DL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multimedia content in the World Wide Web is rapidly growing and contains\nvaluable information for many applications in different domains. For this\nreason, the Internet Archive initiative has been gathering billions of\ntime-versioned web pages since the mid-nineties. However, the huge amount of\ndata is rarely labeled with appropriate metadata and automatic approaches are\nrequired to enable semantic search. Normally, the textual content of the\nInternet Archive is used to extract entities and their possible relations\nacross domains such as politics and entertainment, whereas image and video\ncontent is usually neglected. In this paper, we introduce a system for person\nrecognition in image content of web news stored in the Internet Archive. Thus,\nthe system complements entity recognition in text and allows researchers and\nanalysts to track media coverage and relations of persons more precisely. Based\non a deep learning face recognition approach, we suggest a system that\nautomatically detects persons of interest and gathers sample material, which is\nsubsequently used to identify them in the image data of the Internet Archive.\nWe evaluate the performance of the face recognition system on an appropriate\nstandard benchmark dataset and demonstrate the feasibility of the approach with\ntwo use cases.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:48:21 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 13:04:14 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["M\u00fcller-Budack", "Eric", ""], ["Pustu-Iren", "Kader", ""], ["Diering", "Sebastian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1806.08663", "submitter": "Jinfeng Zhang", "authors": "Albert Steppi, Jinchan Qu, Minjing Tao, Tingting Zhao, Xiaodong Pang,\n  Jinfeng Zhang", "title": "Simulation Study on a New Peer Review Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing volume of scientific publications and grant proposals has\ngenerated an unprecedentedly high workload to scientific communities.\nConsequently, review quality has been decreasing and review outcomes have\nbecome less correlated with the real merits of the papers and proposals. A\nnovel distributed peer review (DPR) approach has recently been proposed to\naddress these issues. The new approach assigns principal investigators (PIs)\nwho submitted proposals (or papers) to the same program as reviewers. Each PI\nreviews and ranks a small number (such as seven) of other PIs' proposals. The\nindividual rankings are then used to estimate a global ranking of all proposals\nusing the Modified Borda Count (MBC). In this study, we perform simulation\nstudies to investigate several parameters important for the decision making\nwhen adopting this new approach. We also propose a new method called\nConcordance Index-based Global Ranking (CIGR) to estimate global ranking from\nindividual rankings. An efficient simulated annealing algorithm is designed to\nsearch the optimal Concordance Index (CI). Moreover, we design a new balanced\nreview assignment procedure, which can result in significantly better\nperformance for both MBC and CIGR methods. We found that CIGR performs better\nthan MBC when the review quality is relatively high. As review quality and\nreview difficulty are tightly correlated, we constructed a boundary in the\nspace of review quality vs review difficulty that separates the CIGR-superior\nand MBC-superior regions. Finally, we propose a multi-stage DPR strategy based\non CIGR, which has the potential to substantially improve the overall review\nperformance while reducing the review workload.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 23:34:59 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 18:06:53 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 18:06:39 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Steppi", "Albert", ""], ["Qu", "Jinchan", ""], ["Tao", "Minjing", ""], ["Zhao", "Tingting", ""], ["Pang", "Xiaodong", ""], ["Zhang", "Jinfeng", ""]]}, {"id": "1806.08746", "submitter": "Uta Grothkopf", "authors": "Uta Grothkopf, Silvia Meakins, Dominic Bordelon", "title": "ESO telbib: learning from experience, preparing for the future", "comments": "6 pages, 2 figures. To be published in SPIE conference proceedings\n  10704 (10704-29), Observatory Operations: Strategies, Processes, and Systems\n  VII (June 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ESO telescope bibliography (telbib) dates back to 1996. During the 20+\nyears of its existence, it has undergone many changes. Most importantly, the\ntelbib system has been enhanced to cater to new use cases and demands from its\nstakeholders. Based on achievements of the past, we will show how a system like\ntelbib can not only stay relevant through the decades, but gain importance, and\nprovide an essential tool for the observatory's management and the wider user\ncommunity alike.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 16:14:40 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Grothkopf", "Uta", ""], ["Meakins", "Silvia", ""], ["Bordelon", "Dominic", ""]]}, {"id": "1806.09082", "submitter": "Grant Atkins", "authors": "Grant C. Atkins, Alexander Nwala, Michele C. Weigle, Michael L. Nelson", "title": "Measuring News Similarity Across Ten U.S. News Sites", "comments": "This is an extended version of the paper to appear in the proceedings\n  of the 15th International Conference on Digital Preservation (iPres 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  News websites make editorial decisions about what stories to include on their\nwebsite homepages and what stories to emphasize (e.g., large font size for main\nstory). The emphasized stories on a news website are often highly similar to\nmany other news websites (e.g, a terrorist event story). The selective emphasis\nof a top news story and the similarity of news across different news\norganizations are well-known phenomena but not well-measured. We provide a\nmethod for identifying the top news story for a select set of U.S.-based news\nwebsites and then quantify the similarity across them. To achieve this, we\nfirst developed a headline and link extractor that parses select websites, and\nthen examined ten United States based news website homepages during a three\nmonth period, November 2016 to January 2017. Using archived copies, retrieved\nfrom the Internet Archive (IA), we discuss the methods and difficulties for\nparsing these websites, and how events such as a presidential election can lead\nnews websites to alter their document representation just for these events. We\nuse our parser to extract k = 1, 3, 10 maximum number of stories for each news\nsite. Second, we used the cosine similarity measure to calculate news\nsimilarity at 8PM Eastern Time for each day in the three months. The similarity\nscores show a buildup (0.335) before Election Day, with a declining value\n(0.328) on Election Day, and an increase (0.354) after Election Day. Our method\nshows that we can effectively identity top stories and quantify news\nsimilarity.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 04:44:36 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 00:01:57 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Atkins", "Grant C.", ""], ["Nwala", "Alexander", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1806.09804", "submitter": "Anand Bihari Mr.", "authors": "Anand Bihari, Sudhakar Tripathi, Akshay Deepak and Prabhat Kumar", "title": "EM and EM'-index sequence: Construction and application in scientific\n  assessment of scholars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the scientometric indicators use only the total number of citations\nof an article and produce a single number for scientific assessment of\nscholars. Although this concept is very simple to compute, it fails to show the\nscientific productivity and impact of scholars during a time-span or in a year.\nTo overcome this, several time series indicators have been proposed that\nconsider the citations from the entire research career of a scholar. However,\nthese indicators fail to give a comparative assessment of two scholars having\nsame or very similar index value. To overcome this shortcoming, h-index\nsequence was proposed to assess the impact of scholars during a particular\ntime-span and to compare multiple scholars at a similar stage in their careers.\nThe h-index sequence is based on the h-index formulation. One of the main\nissues related to the h-index is that it completely ignores the excess citation\nin scientific assessment; h-index sequence also exhibits a similar behaviour.\nTo overcome these limitations, in this article, we have discussed the EM and\n$EM^{'}$-index sequence, and performed an empirical study based on yearly\ncitation count earned from all publications of 89 scholars' publication data.\nThe element of the EM and $EM^{'}$-index sequence for a given year shows the\nimpact of a scholar for that year. We conclude that the EM and $EM^{'}$-index\nsequence could be used as an alternative metrics to asses the impact of\nscholars.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 06:04:44 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Bihari", "Anand", ""], ["Tripathi", "Sudhakar", ""], ["Deepak", "Akshay", ""], ["Kumar", "Prabhat", ""]]}, {"id": "1806.09899", "submitter": "Giovanni Colavizza", "authors": "Giovanni Colavizza, Thomas Franssen, Thed van Leeuwen", "title": "An empirical investigation of the Tribes and their Territories: are\n  research specialisms rural and urban?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an operationalization of the rural and urban analogy introduced in\nBecher and Trowler [2001]. According to them, a specialism is rural if it is\norganized into many, smaller topics of research, with higher author mobility\namong them, lower rate of collaboration and productivity, lower competition for\nresources and citation recognitions compared to an urban specialism. It is\nassumed that most humanities specialisms are rural while science specialisms\nare in general urban: we set to test this hypothesis empirically. We first\npropose an operationalization of the theory in most of its quantifiable\naspects. We then consider specialisms from history, literature, computer\nscience, biology, astronomy. Our results show that specialisms in the\nhumanities present a sensibly lower citation and textual connectivity, in\nagreement with their organization into more, smaller topics per specialism, as\nsuggested by the analogy. We argue that the intellectual organization of rural\nspecialisms might indeed be qualitative different from urban ones, discouraging\nthe straightforward application of citation-based indicators commonly applied\nto urban specialisms without a dedicated re-design in acknowledgement of these\ndifferences.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 11:00:07 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 21:27:14 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 11:18:47 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Colavizza", "Giovanni", ""], ["Franssen", "Thomas", ""], ["van Leeuwen", "Thed", ""]]}, {"id": "1806.09989", "submitter": "Olesya Mryglod", "authors": "O. Mryglod", "title": "Scientometric analysis of Condensed Matter Physics journal", "comments": "18 pages, 15 figures, 3 tables", "journal-ref": "Condens. Matter Phys., 2018, vol. 21, No. 2, 22801", "doi": "10.5488/CMP.21.22801", "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper is dedicated to 25th anniversary of Condensed Matter Physics\njournal (CMP). It contains the results of comprehensive analysis of different\njournal-related data. CMP co-authorship relationships are studied analysing the\ncollaboration network. Its cumulative statical and dynamical properties as well\nas the structure are discussed. The international contribution to the journal\nis assessed using the authors' affiliation data. The network of the countries\ncollaborating within CMP is considered. Another kind of network is used to\ninvestigate the topical spectrum: two PACS indices assigned to one paper are\nconnected by link here. The structure of the most significant interdisciplinary\nconnections is analysed. Finally, the download statistics and the corresponding\nrecords of the papers' citations are used to discuss the journal's impact.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 13:47:22 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Mryglod", "O.", ""]]}, {"id": "1806.10540", "submitter": "Jinseok Kim", "authors": "Jinseok Kim", "title": "Evaluating author name disambiguation for digital libraries: A case of\n  DBLP", "comments": "Scientometrics (2018)", "journal-ref": null, "doi": "10.1007/s11192-018-2824-5", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author name ambiguity in a digital library may affect the findings of\nresearch that mines authorship data of the library. This study evaluates author\nname disambiguation in DBLP, a widely used but insufficiently evaluated digital\nlibrary for its disambiguation performance. In doing so, this study takes a\ntriangulation approach that author name disambiguation for a digital library\ncan be better evaluated when its performance is assessed on multiple labeled\ndatasets with comparison to baselines. Tested on three types of labeled data\ncontaining 5,000 ~ 700K disambiguated names and 6M pairs of disambiguated\nnames, DBLP is shown to assign author names quite accurately to distinct\nauthors, resulting in pairwise precision, recall, and F1 measures around 0.90\nor above overall. DBLP's author name disambiguation performs well even on large\nambiguous name blocks but deficiently on distinguishing authors with the same\nnames. When compared to other disambiguation algorithms, DBLP's disambiguation\nperformance is quite competitive, possibly due to its hybrid disambiguation\napproach combining algorithmic disambiguation and manual error correction. A\ndiscussion follows on strengths and weaknesses of labeled datasets used in this\nstudy for future efforts to evaluate author name disambiguation on a digital\nlibrary scale.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 15:49:27 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 13:45:32 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Kim", "Jinseok", ""]]}, {"id": "1806.10541", "submitter": "Rodrigo Costas", "authors": "Paul Wouters, Zohreh Zahedi, Rodrigo Costas", "title": "Social media metrics for new research evaluation", "comments": "Forthcoming in Glanzel, W., Moed, H.F., Schmoch U., Thelwall, M.\n  (2018). Springer Handbook of Science and Technology Indicators. Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IT cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter approaches, both from a theoretical and practical perspective,\nthe most important principles and conceptual frameworks that can be considered\nin the application of social media metrics for scientific evaluation. We\npropose conceptually valid uses for social media metrics in research\nevaluation. The chapter discusses frameworks and uses of these metrics as well\nas principles and recommendations for the consideration and application of\ncurrent (and potentially new) metrics in research evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 15:50:05 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Wouters", "Paul", ""], ["Zahedi", "Zohreh", ""], ["Costas", "Rodrigo", ""]]}, {"id": "1806.10674", "submitter": "Jinseok Kim", "authors": "Jinseok Kim", "title": "Author-Based Analysis of Conference versus Journal Publication in\n  Computer Science", "comments": null, "journal-ref": "Kim, J. (2018). Author-Based Analysis of Conference versus Journal\n  Publication in Computer Science. Journal of the Association for Information\n  Science and Technology", "doi": "10.1002/asi.24079", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conference publications in computer science (CS) have attracted scholarly\nattention due to their unique status as a main research outlet unlike other\nscience fields where journals are dominantly used for communicating research\nfindings. One frequent research question has been how different conference and\njournal publications are, considering a paper as a unit of analysis. This study\ntakes an author-based approach to analyze publishing patterns of 517,763\nscholars who have ever published both in CS conferences and journals for the\nlast 57 years, as recorded in DBLP. The analysis shows that the majority of CS\nscholars tend to make their scholarly debut, publish more papers, and\ncollaborate with more coauthors in conferences than in journals. Importantly,\nconference papers seem to serve as a distinct channel of scholarly\ncommunication, not a mere preceding step to journal publications: coauthors and\ntitle words of authors across conferences and journals tend not to overlap\nmuch. This study corroborates findings of previous studies on this topic from a\ndistinctive perspective and suggests that conference authorship in CS calls for\nmore special attention from scholars and administrators outside CS who have\nfocused on journal publications to mine authorship data and evaluate scholarly\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 20:06:17 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Kim", "Jinseok", ""]]}, {"id": "1806.11433", "submitter": "David Guillermo Fajardo Ortiz Dr.", "authors": "Carmen Garc\\'ia-Pe\\~na, Luis Miguel Guti\\'errez-Robledo, Augusto\n  Cabrera-Becerril and David Fajardo-Ortiz", "title": "Team assembly mechanisms and the knowledge produced in the Mexico's\n  National Institute of Geriatrics: a network analysis and agent-based\n  modelling approach", "comments": "8 pages, 5 figures", "journal-ref": "Scientifica, vol. 2019, Article ID 9127657, 7 pages, 2019", "doi": "10.1155/2019/9127657", "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mexico's National Institute of Geriatrics (INGER) is the national research\ncenter of reference for matters related to human aging. INGER scientists\nperform basic, clinical and demographic research which may imply different\nscientific cultures working together in the same specialized institution. In\nthis paper, by a combination of text mining, co-authorship network analysis and\nagent-based modeling we analyzed and modeled the team assembly practices and\nthe structure of the knowledge produced by scientists from INGER. Our results\nshowed a weak connection between basic and clinical research, and the emergence\nof a highly connected academic leadership. Importantly, basic and\nclinical-demographic researchers exhibited different team assembly strategies:\nBasic researchers tended to form larger teams mainly with external\ncollaborators while clinical and demographic researchers formed smaller teams\nthat very often incorporated internal (INGER) collaborators. We showed how\nthese two different ways to form research teams impacted the organization of\nknowledge produced at INGER. Following these observations, we modeled, via\nagent-based modeling, the coexistence of different scientific cultures (basic\nand clinical research) exhibiting different team assembly strategies in the\nsame institution. Our agent model successfully reproduced the current situation\nof INGER. Moreover, by modifying the values of homophily we obtain alternative\nscenarios in which multidisciplinary and interdisciplinary research could be\ndone.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 03:59:35 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 21:48:03 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Garc\u00eda-Pe\u00f1a", "Carmen", ""], ["Guti\u00e9rrez-Robledo", "Luis Miguel", ""], ["Cabrera-Becerril", "Augusto", ""], ["Fajardo-Ortiz", "David", ""]]}]