[{"id": "2003.00537", "submitter": "Marek Kwiek", "authors": "Marek Kwiek and Wojciech Roszka", "title": "Gender Disparities in International Research Collaboration: A\n  Large-scale Bibliometric Study of 25,000 University Professors", "comments": "37 pages, 8 figures,6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we examine the hypothesis that gender disparities in\ninternational research collaboration differ by collaboration intensity,\nacademic position, age, and academic discipline. The following are the major\nfindings: (1) while female scientists exhibit a higher rate of general,\nnational, and institutional collaboration, male scientists exhibit a higher\nrate of international collaboration, a finding critically important in\nexplaining gender disparities in impact, productivity, and access to large\ngrants. (2) An aggregated picture of gender disparities hides a more nuanced\ncross-disciplinary picture of them. (3) An analysis of international research\ncollaboration at three separate intensity levels (low, medium, and high)\nreveals that male scientists dominate in international collaboration at each\nlevel. However, at each level, there are specific disciplines in which females\ncollaborate more than males. Further (4), gender disparities are clearly linked\nwith age. Until about the age of 40, they are marginal and then they begin to\ngrow. Finally, we estimate the odds of being involved in international research\ncollaboration using an analytical linear logistic model. The examined sample\nincludes 25,463 internationally productive Polish university professors from 85\nuniversities, grouped into 27 disciplines, who authored 159,943 Scopus-indexed\narticles.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 17:52:25 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 20:59:08 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Kwiek", "Marek", ""], ["Roszka", "Wojciech", ""]]}, {"id": "2003.01006", "submitter": "Jennifer D'Souza", "authors": "Jennifer D'Souza, Anett Hoppe, Arthur Brack, Mohamad Yaser Jaradeh,\n  S\\\"oren Auer, Ralph Ewerth", "title": "The STEM-ECR Dataset: Grounding Scientific Entity References in STEM\n  Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources", "comments": "Published in LREC 2020. Publication URL\n  https://www.aclweb.org/anthology/2020.lrec-1.268/; Dataset DOI\n  https://doi.org/10.25835/0017546", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the STEM (Science, Technology, Engineering, and Medicine)\nDataset for Scientific Entity Extraction, Classification, and Resolution,\nversion 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to\nprovide a benchmark for the evaluation of scientific entity extraction,\nclassification, and resolution tasks in a domain-independent fashion. It\ncomprises abstracts in 10 STEM disciplines that were found to be the most\nprolific ones on a major publishing platform. We describe the creation of such\na multidisciplinary corpus and highlight the obtained findings in terms of the\nfollowing features: 1) a generic conceptual formalism for scientific entities\nin a multidisciplinary scientific context; 2) the feasibility of the\ndomain-independent human annotation of scientific entities under such a generic\nformalism; 3) a performance benchmark obtainable for automatic extraction of\nmultidisciplinary scientific entities using BERT-based neural models; 4) a\ndelineated 3-step entity resolution procedure for human annotation of the\nscientific entities via encyclopedic entity linking and lexicographic word\nsense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic\nlinks and lexicographic senses for our entities. Our findings cumulatively\nindicate that human annotation and automatic learning of multidisciplinary\nscientific concepts as well as their semantic disambiguation in a wide-ranging\nsetting as STEM is reasonable.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 16:35:17 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 11:38:40 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 08:58:48 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 09:45:52 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["D'Souza", "Jennifer", ""], ["Hoppe", "Anett", ""], ["Brack", "Arthur", ""], ["Jaradeh", "Mohamad Yaser", ""], ["Auer", "S\u00f6ren", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2003.01108", "submitter": "Tao Jia", "authors": "Linlin Liu, Jianfei Yu, Junming Huang, Feng Xia and Tao Jia", "title": "The dominance of big teams in China's scientific output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern science is dominated by scientific productions from teams. A recent\nfinding shows that teams with both large and small sizes are essential in\nresearch, prompting us to analyze the extent to which a country's scientific\nwork is carried out by big/small teams. Here, using over 26 million\npublications from Web of Science, we find that China's research output is more\ndominated by big teams than the rest of the world, which is particularly the\ncase in fields of natural science. Despite the global trend that more papers\nare done by big teams, China's drop in small team output is much steeper. As\nteams in China shift from small to large size, the team diversity that is\nessential for innovative works does not increase as much as that in other\ncountries. Using the national average as the baseline, we find that the\nNational Natural Science Foundation of China (NSFC) supports fewer small team\nworks than the National Science Foundation of U.S. (NSF) does, implying that\nbig teams are more preferred by grant agencies in China. Our finding provides\nnew insights into the concern of originality and innovation in China, which\nurges a need to balance small and big teams.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 08:49:41 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 14:22:34 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 13:19:20 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Liu", "Linlin", ""], ["Yu", "Jianfei", ""], ["Huang", "Junming", ""], ["Xia", "Feng", ""], ["Jia", "Tao", ""]]}, {"id": "2003.01958", "submitter": "Federico Simonetta", "authors": "Federico Simonetta, Stavros Ntalampiras, Federico Avanzini", "title": "ASMD: an automatic framework for compiling multimodal datasets with\n  audio and scores", "comments": "Accepted at the Sound and Music Computing Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes an open-source Python framework for handling datasets\nfor music processing tasks, built with the aim of improving the reproducibility\nof research projects in music computing and assessing the generalization\nabilities of machine learning models. The framework enables the automatic\ndownload and installation of several commonly used datasets for multimodal\nmusic processing. Specifically, we provide a Python API to access the datasets\nthrough Boolean set operations based on particular attributes, such as\nintersections and unions of composers, instruments, and so on. The framework is\ndesigned to ease the inclusion of new datasets and the respective ground-truth\nannotations so that one can build, convert, and extend one's own collection as\nwell as distribute it by means of a compliant format to take advantage of the\nAPI. All code and ground-truth are released under suitable open licenses.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:57:59 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 10:44:46 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Simonetta", "Federico", ""], ["Ntalampiras", "Stavros", ""], ["Avanzini", "Federico", ""]]}, {"id": "2003.02135", "submitter": "Jian Du", "authors": "Yong Zhao, Jian Du, Yishan Wu", "title": "Impact of JD Bernal Thoughts in the Science of Science upon China:\n  Implications for Quantitative Studies of Science Today", "comments": "12 pages", "journal-ref": "Quantitative Science Studies, 2020", "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL physics.hist-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  John Desmond Bernal (1901-1970) was one of the most eminent scientists in\nmolecular biology, and also regarded as the founding father of the Science of\nScience. His book The Social Function of Science laid the theoretical\nfoundations for the discipline. In this article, we summarize four chief\ncharacteristics of his ideas in the Science of Science: the socio-historical\nperspective, theoretical models, qualitative and quantitative approaches, and\nstudies of science planning and policy. China has constantly reformed its\nscientific and technological system based on research evidence of the Science\nof Science. Therefore, we analyze the impact of Bernal Science-of-Science\nthoughts on the development of Science of Science in China, and discuss how\nthey might be usefully taken still further in quantitative studies of science.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 02:29:11 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 06:38:16 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Zhao", "Yong", ""], ["Du", "Jian", ""], ["Wu", "Yishan", ""]]}, {"id": "2003.02763", "submitter": "Daniel Ish", "authors": "Daniel Ish, Andrew Lohn, Christian Curriden", "title": "A Quantitative History of A.I. Research in the United States and China", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent interest in the status and consequences of competition\nbetween the U.S. and China in A.I. research, we analyze 60 years of abstract\ndata scraped from Scopus to explore and quantify trends in publications on A.I.\ntopics from institutions affiliated with each country. We find the total volume\nof publications produced in both countries grows with a remarkable regularity\nover tens of years. While China initially experienced faster growth in\npublication volume than the U.S., growth slowed in China when it reached parity\nwith the U.S. and the growth rates of both countries are now similar. We also\nsee both countries undergo a seismic shift in topic choice around 1990, and\nconnect this to an explosion of interest in neural network methods. Finally, we\nsee evidence that between 2000 and 2010, China's topic choice tended to lag\nthat of the U.S. but that in recent decades the topic portfolios have come into\ncloser alignment.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 16:55:31 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 14:35:39 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Ish", "Daniel", ""], ["Lohn", "Andrew", ""], ["Curriden", "Christian", ""]]}, {"id": "2003.03845", "submitter": "Simon Fowler", "authors": "Simon Fowler, Simon D. Harding, Joanna Sharman, and James Cheney", "title": "Cross-tier web programming for curated databases: A case study", "comments": "Accepted to International Journal of Digital Curation", "journal-ref": "International Journal of Digital Curation 2021 , Vol. 16 , Iss. 1\n  , 21 pp", "doi": "10.2218/ijdc.v16i1.735", "report-no": null, "categories": "cs.PL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curated databases have become important sources of information across\nscientific disciplines, and due to the manual work of experts, often become\nimportant reference works. Features such as provenance tracking, archiving, and\ndata citation are widely regarded as important features for curated databases,\nbut implementing such features is challenging, and small database projects\noften lack the resources to do so. A scientific database application is not\njust the database itself, but also an ecosystem of web applications to display\nthe data, and applications supporting data curation. Supporting advanced\ncuration features requires changing all of these components, and there is\ncurrently no way to provide such capabilities in a reusable way. Cross-tier\nprogramming languages have been proposed to simplify the creation of web\napplications, where developers write an application in a single, uniform\nlanguage. Consequently, database queries and updates can be written in the same\nlanguage as the rest of the program, and at least in principle, it should be\npossible to provide curation features reusably via program transformations. As\na first step, it is important to establish that realistic curated databases can\nbe implemented in a cross-tier programming language. In this paper, we describe\nsuch a case study: reimplementing the web frontend of a real-world scientific\ndatabase, the IUPHAR/BPS Guide to Pharmacology (GtoPdb), in the Links\nprogramming language. We show how features such as language-integrated query\nsimplify the development process, and rule out common errors. We show that the\nLinks implementation performs fewer database queries, while the time needed to\nhandle the queries is comparable to the Java version. While there is some\noverhead to using Links because of its comparative immaturity compared to Java,\nthe Links version is viable as a proof-of-concept case study.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 20:35:21 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 12:29:36 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Fowler", "Simon", ""], ["Harding", "Simon D.", ""], ["Sharman", "Joanna", ""], ["Cheney", "James", ""]]}, {"id": "2003.04590", "submitter": "Ois\\'in Creaner", "authors": "Ois\\'in Creaner, Kevin Nolan, David Grennan, Niall Smith and Eugene\n  Hickey", "title": "A Catalogue of Locus Algorithm Pointings for Optimal Differential\n  Photometry for 23,779 Quasars", "comments": "7 pages, 5 figures, cross-references to other papers in the series\n  edited with arxiv links", "journal-ref": null, "doi": "10.1093/mnras/staa2494", "report-no": null, "categories": "astro-ph.GA astro-ph.IM cs.DC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a catalogue of optimised pointings for differential\nphotometry of 23,779 quasars extracted from the Sloan Digital Sky Survey (SDSS)\nCatalogue and a score for each indicating the quality of the Field of View\n(FoV) associated with that pointing. Observation of millimagnitude variability\non a timescale of minutes typically requires differential observations with\nreference to an ensemble of reference stars. For optimal performance, these\nreference stars should have similar colour and magnitude to the target quasar.\nIn addition, the greatest quantity and quality of suitable reference stars may\nbe found by using a telescope pointing which offsets the target object from the\ncentre of the field of view. By comparing each quasar with the stars which\nappear close to it on the sky in the SDSS Catalogue, an optimum pointing can be\ncalculated, and a figure of merit, referred to as the \"score\" calculated for\nthat pointing. Highly flexible software has been developed to enable this\nprocess to be automated and implemented in a distributed computing paradigm,\nwhich enables the creation of catalogues of pointings given a set of input\ntargets. Applying this technique to a sample of 40,000 targets from the 4th\nSDSS quasar catalogue resulted in the production of pointings and scores for\n23,779 quasars. This catalogue is a useful resource for observers planning\ndifferential photometry studies and surveys of quasars to select those which\nhave many suitable celestial neighbours for differential photometry\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 09:08:10 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 08:25:33 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Creaner", "Ois\u00edn", ""], ["Nolan", "Kevin", ""], ["Grennan", "David", ""], ["Smith", "Niall", ""], ["Hickey", "Eugene", ""]]}, {"id": "2003.05258", "submitter": "Manolis Peponakis", "authors": "Manolis Peponakis, Anna Mastora, Sarantos Kapidakis, Martin Doerr", "title": "Expressiveness and machine processability of Knowledge Organization\n  Systems (KOS): An analysis of concepts and relations", "comments": "34 pages, 2 tables, 2 figures", "journal-ref": "International Journal on Digital Libraries, 20(4), 433-452 (2019)", "doi": "10.1007/s00799-019-00269-0", "report-no": null, "categories": "cs.DL cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study considers the expressiveness (that is the expressive power or\nexpressivity) of different types of Knowledge Organization Systems (KOS) and\ndiscusses its potential to be machine-processable in the context of the\nSemantic Web. For this purpose, the theoretical foundations of KOS are reviewed\nbased on conceptualizations introduced by the Functional Requirements for\nSubject Authority Data (FRSAD) and the Simple Knowledge Organization System\n(SKOS); natural language processing techniques are also implemented. Applying a\ncomparative analysis, the dataset comprises a thesaurus (Eurovoc), a subject\nheadings system (LCSH) and a classification scheme (DDC). These are compared\nwith an ontology (CIDOC-CRM) by focusing on how they define and handle concepts\nand relations. It was observed that LCSH and DDC focus on the formalism of\ncharacter strings (nomens) rather than on the modelling of semantics; their\ndefinition of what constitutes a concept is quite fuzzy, and they comprise a\nlarge number of complex concepts. By contrast, thesauri have a coherent\ndefinition of what constitutes a concept, and apply a systematic approach to\nthe modelling of relations. Ontologies explicitly define diverse types of\nrelations, and are by their nature machine-processable. The paper concludes\nthat the potential of both the expressiveness and machine processability of\neach KOS is extensively regulated by its structural rules. It is harder to\nrepresent subject headings and classification schemes as semantic networks with\nnodes and arcs, while thesauri are more suitable for such a representation. In\naddition, a paradigm shift is revealed which focuses on the modelling of\nrelations between concepts, rather than the concepts themselves.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 12:35:52 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Peponakis", "Manolis", ""], ["Mastora", "Anna", ""], ["Kapidakis", "Sarantos", ""], ["Doerr", "Martin", ""]]}, {"id": "2003.06126", "submitter": "Dan Bi", "authors": "Dan Bi, Ju-e Guo, Shouyang Wang, Shaolong Sun", "title": "New Research Trends in Unconventional Oil and Gas Environmental Issue: A\n  Bibliometric Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the booming of unconventional gas production in the world, how to\nbalance environment pollution risk and economy of unconventional gas have\nbecome a common dilemma around the world. The aim of this study is to elucidate\nthe research about environmental issue brought with development of\nunconventional oil and gas industry. To achieve this goal, we present a\nbibliometrics overview of this field from 1990 to 2018. Firstly, this study\noutlines a basic statistical analysis over journals, publications, authors,\ninstitutions and documents. Secondly, VOSviewer is employed to visualize the\ncollaborative relationship to show the link between different author,\ninstitutions, regions and journals. Finally, document bibliographic coupling,\ncooccurrence and keyword burst detection are analyzed to reveal the emerging\ntrend and hot topic. The results indicate that among all countries, America was\nthe most productive country as well as cooperated the most with other\ncountries, followed by China, while the China University of Petroleum is the\nmost productive institution in the world, with 105 publications. Additionally,\nmost articles were classified as energy fuels, environmental sciences and\ngeosciences multidisciplinary. Furthermore, based on emerging trends analysis,\nit was concluded that hydraulic fracturing technology has become a hot topic,\nother popular research topics include: energy policy and regulation of\nunconventional gas development, greenhouse gas emissions, energy and water\nconsumption of unconventional gas life cycle assessment.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 06:07:57 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Bi", "Dan", ""], ["Guo", "Ju-e", ""], ["Wang", "Shouyang", ""], ["Sun", "Shaolong", ""]]}, {"id": "2003.07097", "submitter": "Bahram Kalhor", "authors": "Bahram Kalhor, Alireza Nikravanshalmani", "title": "Correlation between Content and Traffic of the Universities Website", "comments": "16 pagges", "journal-ref": "International Journal of Information Science and Management Vol.\n  13, No. 2, 2015, 61-76", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to analyse the correlation between content and\ntraffic of 21,485 academic websites (universities and research institutes). The\nachieved result is used as an indicator which shows the performance of the\nwebsites for attracting more visitors. This inspires a best practice for\ndeveloping new websites or promoting the traffic of the existing websites. At\nthe first step, content of the site is divided into three major items which\nare: Size, Papers and Rich Files. Then, the Spearman correlation between\ntraffic of the websites and these items are calculated for each country and for\nthe world, respectively. At the next step, countries are ranked based on their\ncorrelations, also a new indicator is proposed from combining these three\ncorrelations of the countries. Results show that in most countries, correlation\nbetween traffic of the websites and Papers is less than correlations between\ntraffic of the websites and Rich Files and Size.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 10:17:27 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Kalhor", "Bahram", ""], ["Nikravanshalmani", "Alireza", ""]]}, {"id": "2003.08283", "submitter": "Tatiana Savina", "authors": "Tatiana Savina and Ivan Sterligov (National Research University Higher\n  School of Economics, Moscow, Russian Federation)", "title": "Prevalence of Potentially Predatory Publishing in Scopus on the Country\n  Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of a large-scale study of potentially predatory\njournals (PPJ) represented in the Scopus database, which is widely used for\nresearch evaluation. Both journal metrics and country, disciplinary data have\nbeen evaluated for different groups of PPJ: those listed by Jeffrey Beall and\nthose delisted by Scopus because of \"publication concerns\". Our results show\nthat even after years of delisting, PPJ are still highly visible in the Scopus\ndatabase with hundreds of active potentially predatory journals. PPJ papers are\ncontinuously produced by all major countries, but with different shares. All\nmajor subject areas are affected. The largest number of PPJ papers are in\nengineering and medicine. On average, PPJ have much lower citation metrics than\nother Scopus-indexed journals. We conclude with a brief survey of the case of\nKazakhstan where the share of PPJ papers at one time amounted to almost a half\nof all Kazakhstan papers in Scopus, and propose a link between PPJ share and\nnational research evaluation policies (in particular, rules of awarding\nacademic degrees). The progress of potentially predatory journal research will\nbe increasingly important because such evaluation methods are becoming more\nwidespread in times of the Metric Tide.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:44:23 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 07:00:08 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 16:15:49 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Savina", "Tatiana", "", "National Research University Higher\n  School of Economics, Moscow, Russian Federation"], ["Sterligov", "Ivan", "", "National Research University Higher\n  School of Economics, Moscow, Russian Federation"]]}, {"id": "2003.08437", "submitter": "Siyao Peng", "authors": "Siyao Peng, Yang Liu, Yilun Zhu, Austin Blodgett, Yushi Zhao, Nathan\n  Schneider", "title": "A Corpus of Adpositional Supersenses for Mandarin Chinese", "comments": "LREC 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adpositions are frequent markers of semantic relations, but they are highly\nambiguous and vary significantly from language to language. Moreover, there is\na dearth of annotated corpora for investigating the cross-linguistic variation\nof adposition semantics, or for building multilingual disambiguation systems.\nThis paper presents a corpus in which all adpositions have been semantically\nannotated in Mandarin Chinese; to the best of our knowledge, this is the first\nChinese corpus to be broadly annotated with adposition semantics. Our approach\nadapts a framework that defined a general set of supersenses according to\nostensibly language-independent semantic criteria, though its development\nfocused primarily on English prepositions (Schneider et al., 2018). We find\nthat the supersense categories are well-suited to Chinese adpositions despite\nsyntactic differences from English. On a Mandarin translation of The Little\nPrince, we achieve high inter-annotator agreement and analyze semantic\ncorrespondences of adposition tokens in bitext.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:59:55 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Peng", "Siyao", ""], ["Liu", "Yang", ""], ["Zhu", "Yilun", ""], ["Blodgett", "Austin", ""], ["Zhao", "Yushi", ""], ["Schneider", "Nathan", ""]]}, {"id": "2003.09315", "submitter": "Zheng Xie", "authors": "Zheng Xie", "title": "Predicting the number of coauthors for researchers: A learning model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the number of coauthors for researchers contributes to\nunderstanding the development of team science. However, it is an elusive task\ndue to diversity in the collaboration patterns of researchers. This study\nprovides a learning model for the dynamics of this variable; the parameters are\nlearned from empirical data that consist of the number of publications and the\nnumber of coauthors at given time intervals. The model is based on relationship\nbetween the annual number of new coauthors and time given an annual number of\npublications, the relationship between the annual number of publications and\ntime given a historical number of publications, and Lotka's law. The\nassumptions of the model are validated by applying it on the high-quality dblp\ndataset. The effectiveness of the model is tested on the dataset by\nsatisfactory fittings on the evolutionary trend of the number of coauthors for\nresearchers, the distribution of this variable, and the occurrence probability\nof collaboration events. Due to its regression nature, the model has the\npotential to be extended to assess the confidence level of the prediction\nresults and thus has applicability to other empirical research.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 15:05:25 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Xie", "Zheng", ""]]}, {"id": "2003.09417", "submitter": "Moritz Schubotz", "authors": "Moritz Schubotz and Andr\\'e Greiner-Petter and Norman Meuschke and\n  Olaf Teschke and Bela Gipp", "title": "Mathematical Formulae in Wikimedia Projects 2020", "comments": "Submitted to JCDL 2020: Proceedings of the ACM/ IEEE Joint Conference\n  on Digital Libraries in 2020 (JCDL '20), August 1-5, 2020, Virtual Event,\n  China", "journal-ref": null, "doi": "10.1145/3383583.3398557", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This poster summarizes our contributions to Wikimedia's processing pipeline\nfor mathematical formulae. We describe how we have supported the transition\nfrom rendering formulae as course-grained PNG images in 2001 to providing\nmodern semantically enriched language-independent MathML formulae in 2020.\nAdditionally, we describe our plans to improve the accessibility and\ndiscoverability of mathematical knowledge in Wikimedia projects further.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 17:56:26 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 19:25:19 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Schubotz", "Moritz", ""], ["Greiner-Petter", "Andr\u00e9", ""], ["Meuschke", "Norman", ""], ["Teschke", "Olaf", ""], ["Gipp", "Bela", ""]]}, {"id": "2003.09881", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff, Terry Ruas, Moritz Schubotz, Georg Rehm, Bela Gipp", "title": "Pairwise Multi-Class Document Classification for Semantic Relations\n  between Wikipedia Articles", "comments": "Accepted at ACM/IEEE Joint Conference on Digital Libraries (JCDL\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many digital libraries recommend literature to their users considering the\nsimilarity between a query document and their repository. However, they often\nfail to distinguish what is the relationship that makes two documents alike. In\nthis paper, we model the problem of finding the relationship between two\ndocuments as a pairwise document classification task. To find the semantic\nrelation between documents, we apply a series of techniques, such as GloVe,\nParagraph-Vectors, BERT, and XLNet under different configurations (e.g.,\nsequence length, vector concatenation scheme), including a Siamese architecture\nfor the Transformer-based systems. We perform our experiments on a newly\nproposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that\ndefine the semantic document relations. Our results show vanilla BERT as the\nbest performing system with an F1-score of 0.93, which we manually examine to\nbetter understand its applicability to other domains. Our findings suggest that\nclassifying semantic relations between documents is a solvable task and\nmotivates the development of recommender systems based on the evaluated\ntechniques. The discussions in this paper serve as first steps in the\nexploration of documents through SPARQL-like queries such that one could find\ndocuments that are similar in one aspect but dissimilar in another.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 12:52:56 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ostendorff", "Malte", ""], ["Ruas", "Terry", ""], ["Schubotz", "Moritz", ""], ["Rehm", "Georg", ""], ["Gipp", "Bela", ""]]}, {"id": "2003.10289", "submitter": "Yurij Holovatch", "authors": "Serhii Brodiuk, Vasyl Palchykov, and Yurij Holovatch", "title": "Embedding technique and network analysis of scientific innovations\n  emergence in an arXiv-based concept network", "comments": "6 pages, 1 figure, submitted to IEEE Third International Conference\n  Data Stream Mining & Processing (Dsmp2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty is an inherent part of innovations and discoveries. Such processes\nmay be considered as an appearance of new ideas or as an emergence of atypical\nconnections between the existing ones. The importance of such connections hints\nfor investigation of innovations through network or graph representation in the\nspace of ideas. In such representation, a graph node corresponds to the\nrelevant concept (idea), whereas an edge between two nodes means that the\ncorresponding concepts have been used in a common context. In this study we\naddress the question about a possibility to identify the edges between existing\nconcepts where the innovations may emerge. To this end, we use a\nwell-documented scientific knowledge landscape of 1.2M arXiv.org manuscripts\ndated starting from April 2007 and until September 2019. We extract relevant\nconcepts for them using the ScienceWISE.info platform. Combining approaches\ndeveloped in complex networks science and graph embedding, we discuss the\npredictability of edges (links) on the scientific knowledge landscape where the\ninnovations may appear.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 14:18:39 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Brodiuk", "Serhii", ""], ["Palchykov", "Vasyl", ""], ["Holovatch", "Yurij", ""]]}, {"id": "2003.10295", "submitter": "Pablo Dorta-Gonzalez", "authors": "Juan Mar\\'ia Hern\\'andez and Pablo Dorta-Gonz\\'alez", "title": "Interdisciplinarity metric based on the co-citation network", "comments": "9 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantifying the interdisciplinarity of a research is a relevant problem in\nthe evaluative bibliometrics. The concept of interdisciplinarity is ambiguous\nand multidimensional. Thus, different measures of interdisciplinarity have been\npropose in the literature. However, few studies have proposed interdisciplinary\nmetrics without previously defining classification sets, and no one use the\nco-citation network for this purpose. In this study we propose an\ninterdisciplinary metric based on the co-citation network. This is a way to\ndefine the publication's field without resorting to pre-defined classification\nsets. We present a characterization of a publication's field and then we use\nthis definition to propose a new metric of the interdisciplinarity degree for\npublications (papers) and journals as units of analysis. The proposed measure\nhas an aggregative property that makes it scalable from a paper individually to\na set of them (journal) without more than adding the numerators and\ndenominators in the proportions that define this new indicator. Moreover, the\naggregated value of two or more units is strictly among all the individual\nvalues.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 14:26:48 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Hern\u00e1ndez", "Juan Mar\u00eda", ""], ["Dorta-Gonz\u00e1lez", "Pablo", ""]]}, {"id": "2003.10434", "submitter": "Victor Castano", "authors": "Cesar Aguado-Cort\\'es and Victor M. Casta\\~no", "title": "Translational Knowledge Map of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A translational knowledge map of COVID-19, based on the analysis of\nscientific papers and networks citation concurrence of terms and keywords of\nthe terms: covid- 19, 2019-ncov and sars-cov-2 in leading databases (MEDLINE,\nweb of Science and Scopus), was constructed. Some fields of the research on\ncovid-19 are connected together, differing in structure, content and evolution.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 00:04:10 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Aguado-Cort\u00e9s", "Cesar", ""], ["Casta\u00f1o", "Victor M.", ""]]}, {"id": "2003.10508", "submitter": "Xiaozan Lyu", "authors": "Xiaozan Lyu and Rodrigo Costas", "title": "How do academic topics shift across altmetric sources? A case study of\n  the research area of Big Data", "comments": null, "journal-ref": null, "doi": "10.1007/s11192-020-03415-7", "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking the research area of Big Data as a case study, we propose an approach\nfor exploring how academic topics shift through the interactions among\naudiences across different altmetric sources. Data used is obtained from Web of\nScience (WoS) and Altmetric.com, with a focus on Blog, News, Policy, Wikipedia,\nand Twitter. Author keywords from publications and terms from online events are\nextracted as the main topics of the publications and the online discussion of\ntheir audiences at Altmetric. Different measures are applied to determine the\n(dis)similarities between the topics put forward by the publication authors and\nthose by the online audiences. Results show that overall there are substantial\ndifferences between the two sets of topics around Big Data scientific research.\nThe main exception is Twitter, where high-frequency hashtags in tweets have a\nstronger concordance with the author keywords in publications. Among the online\ncommunities, Blogs and News show a strong similarity in the terms commonly\nused, while Policy documents and Wikipedia articles exhibit the strongest\ndissimilarity in considering and interpreting Big Data related research.\nSpecifically, the audiences not only focus on more easy-to-understand academic\ntopics related to social or general issues, but also extend them to a broader\nrange of topics in their online discussions. This study lays the foundations\nfor further investigations about the role of online audiences in the\ntransformation of academic topics across altmetric sources, and the degree of\nconcern and reception of scholarly contents by online communities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 19:37:36 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Lyu", "Xiaozan", ""], ["Costas", "Rodrigo", ""]]}, {"id": "2003.11090", "submitter": "Mike Thelwall Prof", "authors": "Mike Thelwall, Saheeda Thelwall", "title": "Covid-19 Tweeting in English: Gender Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the start of 2020, COVID-19 became the most urgent threat to global public\nhealth. Uniquely in recent times, governments have imposed partly voluntary,\npartly compulsory restrictions on the population to slow the spread of the\nvirus. In this context, public attitudes and behaviors are vitally important\nfor reducing the death rate. Analyzing tweets about the disease may therefore\ngive insights into public reactions that may help guide public information\ncampaigns. This article analyses 3,038,026 English tweets about COVID-19 from\nMarch 10 to 23, 2020. It focuses on one relevant aspect of public reaction:\ngender differences. The results show that females are more likely to tweet\nabout the virus in the context of family, social distancing and healthcare\nwhereas males are more likely to tweet about sports cancellations, the global\nspread of the virus and political reactions. Thus, women seem to be taking a\ndisproportionate share of the responsibility for directly keeping the\npopulation safe. The detailed results may be useful to inform public\ninformation announcements and to help understand the spread of the virus. For\nexample, failure to impose a sporting bans whilst encouraging social distancing\nmay send mixed messages to males.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 19:45:57 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Thelwall", "Mike", ""], ["Thelwall", "Saheeda", ""]]}, {"id": "2003.11318", "submitter": "Robin Haunschild", "authors": "Robin Haunschild and Lutz Bornmann", "title": "Which papers cited which tweets? An empirical analysis based on Scopus\n  data", "comments": "7 pages, 4 figures, and 1 table, submitted to STI 2020 in Aarhus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many altmetric studies analyze which papers were mentioned how often in\nspecific altmetrics sources. In order to study the potential policy relevance\nof tweets from another perspective, we investigate which tweets were cited in\npapers. If many tweets were cited in publications, this might demonstrate that\ntweets have substantial and useful content. Overall, a rather low number of\ntweets (n=5506) were cited by less than 3000 papers. Most tweets do not seem to\nbe cited because of any cognitive influence they might have had on studies;\nthey rather were study objects. Most of the papers citing tweets are from the\nsubject areas Social Sciences, Arts and Humanities, and Computer Sciences. Most\nof the papers cited only one tweet. Up to 55 tweets cited in a single paper\nwere found. This research-in-progress does not support a high policy-relevance\nof tweets. However, a content analysis of the tweets and/or papers might lead\nto a more detailed conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 10:54:41 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Haunschild", "Robin", ""], ["Bornmann", "Lutz", ""]]}, {"id": "2003.11650", "submitter": "Asia Biega", "authors": "Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier", "title": "Overview of the TREC 2019 Fair Ranking Track", "comments": "Published in The Twenty-Eighth Text REtrieval Conference Proceedings\n  (TREC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the TREC Fair Ranking track was to develop a benchmark for\nevaluating retrieval systems in terms of fairness to different content\nproviders in addition to classic notions of relevance. As part of the\nbenchmark, we defined standardized fairness metrics with evaluation protocols\nand released a dataset for the fair ranking problem. The 2019 task focused on\nreranking academic paper abstracts given a query. The objective was to fairly\nrepresent relevant authors from several groups that were unknown at the system\nsubmission time. Thus, the track emphasized the development of systems which\nhave robust performance across a variety of group definitions. Participants\nwere provided with querylog data (queries, documents, and relevance) from\nSemantic Scholar. This paper presents an overview of the track, including the\ntask definition, descriptions of the data and the annotation process, as well\nas a comparison of the performance of submitted systems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 21:34:58 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Biega", "Asia J.", ""], ["Diaz", "Fernando", ""], ["Ekstrand", "Michael D.", ""], ["Kohlmeier", "Sebastian", ""]]}, {"id": "2003.12042", "submitter": "Xovee Xu", "authors": "Fan Zhou, Xovee Xu, Ce Li, Goce Trajcevski, Ting Zhong, Kunpeng Zhang", "title": "A Heterogeneous Dynamical Graph Neural Networks Approach to Quantify\n  Scientific Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying and predicting the long-term impact of scientific writings or\nindividual scholars has important implications for many policy decisions, such\nas funding proposal evaluation and identifying emerging research fields. In\nthis work, we propose an approach based on Heterogeneous Dynamical Graph Neural\nNetwork (HDGNN) to explicitly model and predict the cumulative impact of papers\nand authors. HDGNN extends heterogeneous GNNs by incorporating temporally\nevolving characteristics and capturing both structural properties of attributed\ngraph and the growing sequence of citation behavior. HDGNN is significantly\ndifferent from previous models in its capability of modeling the node impact in\na dynamic manner while taking into account the complex relations among nodes.\nExperiments conducted on a real citation dataset demonstrate its superior\nperformance of predicting the impact of both papers and authors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:15:36 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhou", "Fan", ""], ["Xu", "Xovee", ""], ["Li", "Ce", ""], ["Trajcevski", "Goce", ""], ["Zhong", "Ting", ""], ["Zhang", "Kunpeng", ""]]}, {"id": "2003.12273", "submitter": "Nicolas Robinson-Garcia", "authors": "Nicolas Robinson-Garcia, Rodrigo Costas and Thed N. van Leeuwen", "title": "Open Access uptake by universities worldwide", "comments": "Paper accepted for publication in PeerJ. Supplemental material at\n  doi:10.5281/zenodo.3874959", "journal-ref": null, "doi": "10.5281/zenodo.3874959", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of policies promoting the adoption of an Open Science\nculture must be accompanied by indicators that allow monitoring the penetration\nof such policies and their potential effects on research publishing and sharing\npractices. This study presents indicators of Open Access (OA) penetration at\nthe institutional level for universities worldwide. By combining data from Web\nof Science, Unpaywall and the Leiden Ranking disambiguation of institutions, we\ntrack OA coverage of universities' output for 963 institutions. This paper\npresents the methodological challenges, conceptual discrepancies and\nlimitations and discusses further steps needed to move forward the discussion\non fostering Open Access and Open Science practices and policies.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 08:19:22 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 15:16:42 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Robinson-Garcia", "Nicolas", ""], ["Costas", "Rodrigo", ""], ["van Leeuwen", "Thed N.", ""]]}, {"id": "2003.12303", "submitter": "Daniel Hain PhD.", "authors": "Daniel Hain, Roman Jurowetzki, Tobias Buchmann, Patrick Wolf", "title": "Text-based Technological Signatures and Similarities: How to create them\n  and what to do with them", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an efficiently scalable approach to measure\ntechnological similarity between patents by combining embedding techniques from\nnatural language processing with nearest-neighbor approximation. Using this\nmethodology we are able to compute existing similarities between all patents,\nwhich in turn enables us to represent the whole patent universe as a\ntechnological network. We validate both technological signature and similarity\nin various ways, and demonstrate at the case of electric vehicle technologies\ntheir usefulness to measure knowledge flows, map technological change, and\ncreate patent quality indicators. Thereby the paper contributes to the growing\nliterature on text-based indicators for patent landscaping.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 09:58:09 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 14:33:17 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 11:42:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hain", "Daniel", ""], ["Jurowetzki", "Roman", ""], ["Buchmann", "Tobias", ""], ["Wolf", "Patrick", ""]]}, {"id": "2003.12519", "submitter": "Jichao Li", "authors": "Jichao Li, Yian Yin, Santo Fortunato and Dashun Wang", "title": "Scientific elite revisited: Patterns of productivity, collaboration,\n  authorship and impact", "comments": null, "journal-ref": null, "doi": "10.1098/rsif.2020.0135", "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout history, a relatively small number of individuals have made a\nprofound and lasting impact on science and society. Despite long-standing,\nmulti-disciplinary interests in understanding careers of elite scientists,\nthere have been limited attempts for a quantitative, career-level analysis.\nHere, we leverage a comprehensive dataset we assembled, allowing us to trace\nthe entire career histories of nearly all Nobel laureates in physics,\nchemistry, and physiology or medicine over the past century. We find that,\nalthough Nobel laureates were energetic producers from the outset, producing\nworks that garner unusually high impact, their careers before winning the prize\nfollow relatively similar patterns as ordinary scientists, being characterized\nby hot streaks and increasing reliance on collaborations. We also uncovered\nnotable variations along their careers, often associated with the Nobel prize,\nincluding shifting coauthorship structure in the prize-winning work, and a\nsignificant but temporary dip in the impact of work they produce after winning\nthe Nobel. Together, these results document quantitative patterns governing the\ncareers of scientific elites, offering an empirical basis for a deeper\nunderstanding of the hallmarks of exceptional careers in science.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 16:31:02 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Li", "Jichao", ""], ["Yin", "Yian", ""], ["Fortunato", "Santo", ""], ["Wang", "Dashun", ""]]}, {"id": "2003.12611", "submitter": "Angelo Salatino", "authors": "Angelo A. Salatino, Francesco Osborne, Enrico Motta", "title": "Ontology Extraction and Usage in the Scholarly Knowledge Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontologies of research areas have been proven to be useful in many\napplication for analysing and making sense of scholarly data. In this chapter,\nwe present the Computer Science Ontology (CSO), which is the largest ontology\nof research areas in the field of Computer Science, and discuss a number of\napplications that build on CSO, to support high-level tasks, such as topic\nclassification, metadata extraction, and recommendation of books.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 19:35:47 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 15:18:52 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Salatino", "Angelo A.", ""], ["Osborne", "Francesco", ""], ["Motta", "Enrico", ""]]}, {"id": "2003.12830", "submitter": "Victor Castano", "authors": "C. Aguado, A. Silva and V.M. Castano", "title": "Opioids for pain treatment of cancer: a knowledge maturity mapping", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conceptual structure of opioids, based on the bibliometric analysis of\n4,935 articles of the Web of Science was constructed. The results were\nprocessed identifying the most cited articles to extract the main connections\nand frequencies of key words, authors, journals, countries, institutions, and\ntheir tendencies and their connection and degree of collaboration. The temporal\ntendencies, the word cloud, the keyword network, the evolution of words, author\nproduction and the scientific production by country are analyzed in terms of\nthe increasing frequency in which opioids are employed to treat both cancerous\nand non-cancerous pain.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 16:10:43 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Aguado", "C.", ""], ["Silva", "A.", ""], ["Castano", "V. M.", ""]]}, {"id": "2003.12958", "submitter": "Markus Stocker", "authors": "Markus Stocker, Louise Darroch, Rolf Krahl, Ted Habermann, Anusuriya\n  Devaraju, Ulrich Schwardmann, Claudio D'Onofrio, Ingemar H\\\"aggstr\\\"om", "title": "Persistent Identification Of Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Instruments play an essential role in creating research data. Given the\nimportance of instruments and associated metadata to the assessment of data\nquality and data reuse, globally unique, persistent and resolvable\nidentification of instruments is crucial. The Research Data Alliance Working\nGroup Persistent Identification of Instruments (PIDINST) developed a\ncommunity-driven solution for persistent identification of instruments which we\npresent and discuss in this paper. Based on an analysis of 10 use cases,\nPIDINST developed a metadata schema and prototyped schema implementation with\nDataCite and ePIC as representative persistent identifier infrastructures and\nwith HZB (Helmholtz-Zentrum Berlin f\\\"ur Materialien und Energie) and BODC\n(British Oceanographic Data Centre) as representative institutional instrument\nproviders. These implementations demonstrate the viability of the proposed\nsolution in practice. Moving forward, PIDINST will further catalyse adoption\nand consolidate the schema by addressing new stakeholder requirements.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 07:12:43 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Stocker", "Markus", ""], ["Darroch", "Louise", ""], ["Krahl", "Rolf", ""], ["Habermann", "Ted", ""], ["Devaraju", "Anusuriya", ""], ["Schwardmann", "Ulrich", ""], ["D'Onofrio", "Claudio", ""], ["H\u00e4ggstr\u00f6m", "Ingemar", ""]]}, {"id": "2003.13030", "submitter": "Miguel Guevara", "authors": "Paulina Hurtado-Arenas, Miguel R. Guevara", "title": "A bibliometric analysis of research based on the Roy Adaptation Model: a\n  contribution to Nursing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective. To perform a modern bibliometric analysis of the research based on\nthe Roy Adaptation Model, a founding nursing model proposed by Sor Callista Roy\nin the1970s. Method. A descriptive and longitudinal study. We used information\nfrom the two dominant scientific databases, Web Of Science and SCOPUS. We\nobtained 137 publications from the Core Collection of WoS, and 338 publications\nfrom SCOPUS. We conducted our analysis using the software Bibliometrix, an\nR-package specialized in creating bibliometric analyses from a perspective of\ndescriptive statistics and network analysis, including co-citation, co-keyword\noccurrence and collaboration networks. Results. Our quantitative results show\nthe main actors around the research based on the model and the founding\nliterature or references on which this research was based. We analyze the main\nkeywords and how they are linked. Furthermore, we present the most prolific\nauthors both in number of publications and in centrality in the network of\ncoauthors. We present the most central institutions in the global network of\ncollaboration. Conclusions. We highlight the relevance of this theoretical\nmodel in nursing and detail its evolution. The United States is the dominant\ncountry in production of documents on the topic, and the University of\nMassachusetts Boston and Boston College are the most influential institutions.\nThe network of collaboration also describes clusters in Mexico, Turkey and\nSpain. Our findings are useful to acquire a general vision of the field.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 14:02:16 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Hurtado-Arenas", "Paulina", ""], ["Guevara", "Miguel R.", ""]]}, {"id": "2003.13084", "submitter": "Daniel Garijo", "authors": "Daniel Garijo and Mar\\'ia Poveda-Villal\\'on", "title": "Best Practices for Implementing FAIR Vocabularies and Ontologies on the\n  Web", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the adoption of Semantic Web technologies, an increasing number of\nvocabularies and ontologies have been developed in different domains, ranging\nfrom Biology to Agronomy or Geosciences. However, many of these ontologies are\nstill difficult to find, access and understand by researchers due to a lack of\ndocumentation, URI resolving issues, versioning problems, etc. In this chapter\nwe describe guidelines and best practices for creating accessible,\nunderstandable and reusable ontologies on the Web, using standard practices and\npointing to existing tools and frameworks developed by the Semantic Web\ncommunity. We illustrate our guidelines with concrete examples, in order to\nhelp researchers implement these practices in their future vocabularies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 17:40:04 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Garijo", "Daniel", ""], ["Poveda-Villal\u00f3n", "Mar\u00eda", ""]]}, {"id": "2003.13236", "submitter": "Georg Rehm", "authors": "Penny Labropoulou and Katerina Gkirtzou and Maria Gavriilidou and\n  Miltos Deligiannis and Dimitrios Galanis and Stelios Piperidis and Georg Rehm\n  and Maria Berger and Val\\'erie Mapelli and Micka\\\"el Rigault and Victoria\n  Arranz and Khalid Choukri and Gerhard Backfried and Jos\\'e Manuel G\\'omez\n  P\\'erez and Andres Garcia Silva", "title": "Making Metadata Fit for Next Generation Language Technology Platforms:\n  The Metadata Schema of the European Language Grid", "comments": "Proceedings of the 12th Language Resources and Evaluation Conference\n  (LREC 2020). To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current scientific and technological landscape is characterised by the\nincreasing availability of data resources and processing tools and services. In\nthis setting, metadata have emerged as a key factor facilitating management,\nsharing and usage of such digital assets. In this paper we present ELG-SHARE, a\nrich metadata schema catering for the description of Language Resources and\nTechnologies (processing and generation services and tools, models, corpora,\nterm lists, etc.), as well as related entities (e.g., organizations, projects,\nsupporting documents, etc.). The schema powers the European Language Grid\nplatform that aims to be the primary hub and marketplace for industry-relevant\nLanguage Technology in Europe. ELG-SHARE has been based on various metadata\nschemas, vocabularies, and ontologies, as well as related recommendations and\nguidelines.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 06:46:35 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Labropoulou", "Penny", ""], ["Gkirtzou", "Katerina", ""], ["Gavriilidou", "Maria", ""], ["Deligiannis", "Miltos", ""], ["Galanis", "Dimitrios", ""], ["Piperidis", "Stelios", ""], ["Rehm", "Georg", ""], ["Berger", "Maria", ""], ["Mapelli", "Val\u00e9rie", ""], ["Rigault", "Micka\u00ebl", ""], ["Arranz", "Victoria", ""], ["Choukri", "Khalid", ""], ["Backfried", "Gerhard", ""], ["P\u00e9rez", "Jos\u00e9 Manuel G\u00f3mez", ""], ["Silva", "Andres Garcia", ""]]}, {"id": "2003.13833", "submitter": "Georg Rehm", "authors": "Georg Rehm and Katrin Marheinecke and Stefanie Hegele and Stelios\n  Piperidis and Kalina Bontcheva and Jan Haji\\v{c} and Khalid Choukri and\n  Andrejs Vasi\\c{l}jevs and Gerhard Backfried and Christoph Prinz and Jos\\'e\n  Manuel G\\'omez P\\'erez and Luc Meertens and Paul Lukowicz and Josef van\n  Genabith and Andrea L\\\"osch and Philipp Slusallek and Morten Irgens and\n  Patrick Gatellier and Joachim K\\\"ohler and Laure Le Bars and Dimitra\n  Anastasiou and Albina Auksori\\=ut\\.e and N\\'uria Bel and Ant\\'onio Branco and\n  Gerhard Budin and Walter Daelemans and Koenraad De Smedt and Radovan\n  Garab\\'ik and Maria Gavriilidou and Dagmar Gromann and Svetla Koeva and Simon\n  Krek and Cvetana Krstev and Krister Lind\\'en and Bernardo Magnini and Jan\n  Odijk and Maciej Ogrodniczuk and Eir\\'ikur R\\\"ognvaldsson and Mike Rosner and\n  Bolette Sandford Pedersen and Inguna Skadi\\c{n}a and Marko Tadi\\'c and Dan\n  Tufi\\c{s} and Tam\\'as V\\'aradi and Kadri Vider and Andy Way and Fran\\c{c}ois\n  Yvon", "title": "The European Language Technology Landscape in 2020: Language-Centric and\n  Human-Centric AI for Cross-Cultural Communication in Multilingual Europe", "comments": "Proceedings of the 12th Language Resources and Evaluation Conference\n  (LREC 2020). To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingualism is a cultural cornerstone of Europe and firmly anchored in\nthe European treaties including full language equality. However, language\nbarriers impacting business, cross-lingual and cross-cultural communication are\nstill omnipresent. Language Technologies (LTs) are a powerful means to break\ndown these barriers. While the last decade has seen various initiatives that\ncreated a multitude of approaches and technologies tailored to Europe's\nspecific needs, there is still an immense level of fragmentation. At the same\ntime, AI has become an increasingly important concept in the European\nInformation and Communication Technology area. For a few years now, AI,\nincluding many opportunities, synergies but also misconceptions, has been\novershadowing every other topic. We present an overview of the European LT\nlandscape, describing funding programmes, activities, actions and challenges in\nthe different countries with regard to LT, including the current state of play\nin industry and the LT market. We present a brief overview of the main\nLT-related activities on the EU level in the last ten years and develop\nstrategic guidance with regard to four key dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:42:45 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Rehm", "Georg", ""], ["Marheinecke", "Katrin", ""], ["Hegele", "Stefanie", ""], ["Piperidis", "Stelios", ""], ["Bontcheva", "Kalina", ""], ["Haji\u010d", "Jan", ""], ["Choukri", "Khalid", ""], ["Vasi\u013cjevs", "Andrejs", ""], ["Backfried", "Gerhard", ""], ["Prinz", "Christoph", ""], ["P\u00e9rez", "Jos\u00e9 Manuel G\u00f3mez", ""], ["Meertens", "Luc", ""], ["Lukowicz", "Paul", ""], ["van Genabith", "Josef", ""], ["L\u00f6sch", "Andrea", ""], ["Slusallek", "Philipp", ""], ["Irgens", "Morten", ""], ["Gatellier", "Patrick", ""], ["K\u00f6hler", "Joachim", ""], ["Bars", "Laure Le", ""], ["Anastasiou", "Dimitra", ""], ["Auksori\u016bt\u0117", "Albina", ""], ["Bel", "N\u00faria", ""], ["Branco", "Ant\u00f3nio", ""], ["Budin", "Gerhard", ""], ["Daelemans", "Walter", ""], ["De Smedt", "Koenraad", ""], ["Garab\u00edk", "Radovan", ""], ["Gavriilidou", "Maria", ""], ["Gromann", "Dagmar", ""], ["Koeva", "Svetla", ""], ["Krek", "Simon", ""], ["Krstev", "Cvetana", ""], ["Lind\u00e9n", "Krister", ""], ["Magnini", "Bernardo", ""], ["Odijk", "Jan", ""], ["Ogrodniczuk", "Maciej", ""], ["R\u00f6gnvaldsson", "Eir\u00edkur", ""], ["Rosner", "Mike", ""], ["Pedersen", "Bolette Sandford", ""], ["Skadi\u0146a", "Inguna", ""], ["Tadi\u0107", "Marko", ""], ["Tufi\u015f", "Dan", ""], ["V\u00e1radi", "Tam\u00e1s", ""], ["Vider", "Kadri", ""], ["Way", "Andy", ""], ["Yvon", "Fran\u00e7ois", ""]]}, {"id": "2003.14046", "submitter": "Zhiwu Xie", "authors": "Xinyue Wang, Zhiwu Xie", "title": "The Case For Alternative Web Archival Formats To Expedite The\n  Data-To-Insight Cycle", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": "10.1145/3383583.3398542", "report-no": null, "categories": "cs.DL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The WARC file format is widely used by web archives to preserve collected web\ncontent for future use. With the rapid growth of web archives and the\nincreasing interest to reuse these archives as big data sources for statistical\nand analytical research, the speed to turn these data into insights becomes\ncritical. In this paper we show that the WARC format carries significant\nperformance penalties for batch processing workload. We trace the root cause of\nthese penalties to its data structure, encoding, and addressing method. We then\nrun controlled experiments to illustrate how severe these problems can be.\nIndeed, performance gain of one to two orders of magnitude can be achieved\nsimply by reformatting WARC files into Parquet or Avro formats. While these\nresults do not necessarily constitute an endorsement for Avro or Parquet, the\ntime has come for the web archiving community to consider replacing WARC with\nmore efficient web archival formats.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 09:23:35 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Wang", "Xinyue", ""], ["Xie", "Zhiwu", ""]]}]