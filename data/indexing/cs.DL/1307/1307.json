[{"id": "1307.0110", "submitter": "Nicolas Robinson-Garcia", "authors": "Alvaro Cabezas-Clavijo, Nicol\\'as Robinson-Garcia, Manuel Escabias and\n  Evaristo Jim\\'enez-Contreras", "title": "Reviewers' ratings and bibliometric indicators: hand in hand when\n  assessing over research proposals?", "comments": null, "journal-ref": "PLoS ONE 8(6): e68258", "doi": "10.1371/journal.pone.0068258", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The peer review system has been traditionally challenged due to its many\nlimitations especially for allocating funding. Bibliometric indicators may well\npresent themselves as a complement. Objective: We analyze the relationship\nbetween peers' ratings and bibliometric indicators for Spanish researchers in\nthe 2007 National R&D Plan for 23 research fields. We analyze peers' ratings\nfor 2333 applications. We also gathered principal investigators' research\noutput and impact and studied the differences between accepted and rejected\napplications. We used the Web of Science database and focused on the 2002-2006\nperiod. First, we analyzed the distribution of granted and rejected proposals\nconsidering a given set of bibliometric indicators to test if there are\nsignificant differences. Then, we applied a multiple logistic regression\nanalysis to determine if bibliometric indicators can explain by themselves the\nconcession of grant proposals. 63.4% of the applications were funded.\nBibliometric indicators for accepted proposals showed a better previous\nperformance than for those rejected; however the correlation between peer\nreview and bibliometric indicators is very heterogeneous among most areas. The\nlogistic regression analysis showed that the main bibliometric indicators that\nexplain the granting of research proposals in most cases are the output (number\nof published articles) and the number of papers published in journals that\nbelong to the first quartile ranking of the Journal Citations Report.\nBibliometric indicators predict the concession of grant proposals at least as\nwell as peer ratings. Social Sciences and Education are the only areas where no\nrelation was found, although this may be due to the limitations of the Web of\nScience's coverage. These findings encourage the use of bibliometric indicators\nas a complement to peer review in most of the analyzed areas.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 14:38:12 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Cabezas-Clavijo", "Alvaro", ""], ["Robinson-Garcia", "Nicol\u00e1s", ""], ["Escabias", "Manuel", ""], ["Jim\u00e9nez-Contreras", "Evaristo", ""]]}, {"id": "1307.0667", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Ruediger Mutz", "title": "From P100 to P100_: Conception and improvement of a new citation-rank\n  approach in bibliometrics", "comments": "Accepted for publication in the Journal of the American Society for\n  Information Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Properties of a percentile-based rating scale needed in bibliometrics are\nformulated. Based on these properties, P100 was recently introduced as a new\ncitation-rank approach (Bornmann, Leydesdorff, & Wang, in press). In this\npaper, we conceptualize P100 and propose an improvement which we call P100_.\nAdvantages and disadvantages of citation-rank indicators are noted.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 10:24:45 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 13:36:22 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Bornmann", "Lutz", ""], ["Mutz", "Ruediger", ""]]}, {"id": "1307.0788", "submitter": "Julian Sienkiewicz", "authors": "Julian Sienkiewicz, Krzysztof Soja, Janusz A. Holyst, Peter M. A.\n  Sloot", "title": "Categorical and Geographical Separation in Science", "comments": "17 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform the analysis of scientific collaboration at the level of\nuniversities. The scope of this study is to answer two fundamental questions:\n(i) can one indicate a category (i.e., a scientific discipline) that has the\ngreatest impact on the rank of the university and (ii) do the best universities\ncollaborate with the best ones only? Using two university ranking lists (ARWU\nand QS) as well as data from the Science Citation Index we show how the number\nof publications in certain categories correlates with the university rank.\nMoreover, using complex networks analysis, we give hints that the scientific\ncollaboration is highly embedded in the physical space and the number of common\npapers decays with the distance between them. We also show the strength of the\nties between universities is proportional to product of their total number of\npublications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 18:37:34 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2013 09:28:02 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Sienkiewicz", "Julian", ""], ["Soja", "Krzysztof", ""], ["Holyst", "Janusz A.", ""], ["Sloot", "Peter M. A.", ""]]}, {"id": "1307.1271", "submitter": "Jorge Ma\\~nana-Rodr\\'iguez", "authors": "Elea Gimenez-Toledo, Jorge Manana-Rodriguez and Emilio\n  Delgado-Lopez-Cozar", "title": "Quality indicators for scientific journals based on experts opinion", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results and further development of a survey sent to\n11,799 Spanish faculty members and researchers from various fields of the\nsocial sciences and the humanities, obtaining a total of 45.6% (5,368\nresponses) usable answers. Respondents were asked (a) to indicate the three\nmost important journals in their field and (b) to rate them on a 0-10 scale\naccording to their quality. The information obtained has been synthesized in\ntwo indicators which reflect the perceived quality of journals. Once the values\nwere obtained, the journals were categorized according to each indicator and\nthe ordinal positions were compared. Different profiles of journals are\nanalyzed in connection with experts opinion, such as regional orientation, and\nthe consensus among researchers is studied. Finally, the possibilities of\nextending the research and indicators to sets of international journals are\nexplored.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 11:06:28 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Gimenez-Toledo", "Elea", ""], ["Manana-Rodriguez", "Jorge", ""], ["Delgado-Lopez-Cozar", "Emilio", ""]]}, {"id": "1307.1330", "submitter": "George Lozano A", "authors": "George A. Lozano", "title": "The elephant in the room: multi-authorship and the assessment of\n  individual researchers", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a group of individuals creates something, credit is usually divided\namong them. Oddly, that does not apply to scientific papers. The most commonly\nused performance measure for individual researchers is the h-index, which does\nnot correct for multi-authorship. Each author claims full credit for each paper\nand each ensuing citation. This mismeasure of achievement is fuelling a\nflagrant increase in multi-authorship. Several alternatives to the h-index have\nbeen devised, and one of them, the individual h-index (hI), is logical,\nintuitive and easily calculated. Correcting for multi-authorship would end\ngratuitous authorship and allow proper attribution and unbiased comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 13:50:28 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2013 20:30:44 GMT"}, {"version": "v3", "created": "Fri, 26 Jul 2013 12:53:07 GMT"}, {"version": "v4", "created": "Mon, 29 Jul 2013 20:00:52 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Lozano", "George A.", ""]]}, {"id": "1307.3616", "submitter": "Loet Leydesdorff", "authors": "Fred Y. Ye and Loet Leydesdorff", "title": "The \"Academic Trace\" of the Performance Matrix: A Mathematical Synthesis\n  of the h-Index and the Integrated Impact Indicator (I3)", "comments": "Journal of the American Society for Information Science and\n  Technology (forthcoming)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The h-index provides us with nine natural classes which can be written as a\nmatrix of three vectors. The three vectors are: X=(X1, X2, X3) indicate\npublication distribution in the h-core, the h-tail, and the uncited ones,\nrespectively; Y=(Y1, Y2, Y3) denote the citation distribution of the h-core,\nthe h-tail and the so-called \"excess\" citations (above the h-threshold),\nrespectively; and Z=(Z1, Z2, Z3)= (Y1-X1, Y2-X2, Y3-X3). The matrix V=(X,Y,Z)T\nconstructs a measure of academic performance, in which the nine numbers can all\nbe provided with meanings in different dimensions. The \"academic trace\" tr(V)\nof this matrix follows naturally, and contributes a unique indicator for total\nacademic achievements by summarizing and weighting the accumulation of\npublications and citations. This measure can also be used to combine the\nadvantages of the h-index and the Integrated Impact Indicator (I3) into a\nsingle number with a meaningful interpretation of the values. We illustrate the\nuse of tr(V) for the cases of two journal sets, two universities, and ourselves\nas two individual authors.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 06:34:44 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Ye", "Fred Y.", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1307.4329", "submitter": "Delgado Lopez-Cozar emilio", "authors": "Liliana Marcela Reina Leal, Rafael Repiso, Emilio Delgado Lopez-Cozar", "title": "H Index of scientific Nursing journals according to Google Scholar\n  Metrics (2007-2011)", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": "EC35", "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The aim of this report is to present a ranking of Nursing journals covered in\nGoogle Scholar Metrics (GSM), a Google product launched in 2012 to assess the\nimpact of scientific journals from citation counts this receive on Google\nScholar. Google has chosen to include only those journals that have published\nat least 100 papers and have at least one citation in a period of five years\n(2007-2011). Journal rankings are sorted by languages (showing the 100 papers\nwith the greatest impact). This tool allows to sort by subject areas and\ndisciplines, but only in the case of journals in English. In this case, it only\nshows the 20 journals with the highest h index. This option is not available\nfor journals in the other nine languages present in Google (Chinese,\nPortuguese, German, Spanish, French, Korean, Japanese, Dutch and Italian).\n  Google Scholar Metrics doesnt currently allow to group and sort all journals\nbelonging to a scientific discipline. In the case of Nursing, in the ten\nlistings displayed by GSM we can only locate 34 journals. Therefore, in an\nattempt to overcome this limitation, we have used the diversity of search\nprocedures allowed by GSM to identify the greatest number of scientific\njournals of Nursing with h index calculated by this bibliometric tool.\nBibliographic searches were conducted between 10th and 30th May 2013.\n  The result is a ranking of 337 nursing journals sorted by the same h index,\nand mean as discriminating value. Journals are also grouped by quartiles.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 16:19:47 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Leal", "Liliana Marcela Reina", ""], ["Repiso", "Rafael", ""], ["Lopez-Cozar", "Emilio Delgado", ""]]}, {"id": "1307.5647", "submitter": "Roberto Piazza", "authors": "Roberto Piazza", "title": "On house renovation and coauthoring (with a little excursus on the Holy\n  Grail of bibliometrics)", "comments": "Because of the reasons stated in the abstract, I am not planning to\n  submit this paper to any journal. But, of course, I am not against it. (4\n  pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than a paper, this is just a little divertissement about coauthoring,\nthe Hirsch h-index, and bibliometric evaluation in general. Without pretending\nto yield any general conclusions, what I found rummaging through the physics\nliterature made me think quite a bit. I hope the same will happen to my\nreaders, even it they will likely be much less than 25, which is the audience\none of the greatest Italian writers (whom, is left to the reader to single out)\naddresses to.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 10:19:01 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Piazza", "Roberto", ""]]}, {"id": "1307.5685", "submitter": "Justin F Brunelle", "authors": "Justin F. Brunelle and Michael L. Nelson", "title": "An Evaluation of Caching Policies for Memento TimeMaps", "comments": "JCDL2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As defined by the Memento Framework, TimeMaps are ma-chine-readable lists of\ntime-specific copies -- called \"mementos\" -- of an archived original resource.\nIn theory, as an archive acquires additional mementos over time, a TimeMap\nshould be monotonically increasing. However, there are reasons why the number\nof mementos in a TimeMap would decrease, for example: archival redaction of\nsome or all of the mementos, archival restructuring, and transient errors on\nthe part of one or more archives. We study TimeMaps for 4,000 original\nresources over a three month period, note their change patterns, and develop a\ncaching algorithm for TimeMaps suitable for a reverse proxy in front of a\nMemento aggregator. We show that TimeMap cardinality is constant or\nmonotonically increasing for 80.2% of all TimeMap downloads observed in the\nobservation period. The goal of the caching algorithm is to exploit the ideally\nmonotonically increasing nature of TimeMaps and not cache responses with fewer\nmementos than the already cached TimeMap. This new caching algorithm uses\nconditional cache replacement and a Time To Live (TTL) value to ensure the user\nhas access to the most complete TimeMap available. Based on our empirical data,\na TTL of 15 days will minimize the number of mementos missed by users, and\nminimize the load on archives contributing to TimeMaps.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 12:53:10 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Brunelle", "Justin F.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1307.5964", "submitter": "Michael Schreiber", "authors": "Michael Schreiber", "title": "The predictability of the Hirsch index evolution", "comments": "6 pages, 3 figures, to be published in the Proceedings of 18th Int.\n  Conf. Science and Technology Indicators (STI2013), Berlin", "journal-ref": "in Translational twists and turns: Science as a socio-economic\n  endeavor, Hrsg.: S. Hinze, A. Lottmann, Proc. STI 2013 Berlin (Institute for\n  Research Information and Quality Assurance, Berlin), S. 366-372 (2013)", "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The h-index can be used as a predictor of itself. However, the evolution of\nthe h-index with time is shown in the present investigation to be dominated for\nseveral years by citations to previous publications rather than by new\nscientific achievements. This inert behaviour of the h-index raises questions,\nwhether the h-index can be used profitably in academic appointment processes or\nfor the allocation of research resources.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 07:23:29 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Schreiber", "Michael", ""]]}, {"id": "1307.6307", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann", "title": "Is there currently a scientific revolution in scientometrics?", "comments": "Accepted for publication in the Journal of the American Society for\n  Information Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The author of this letter to the editor would like to set forth the argument\nthat scientometrics is currently in a phase in which a taxonomic change, and\nhence a revolution, is taking place. One of the key terms in scientometrics is\nscientific impact which nowadays is understood to mean not only the impact on\nscience but the impact on every area of society.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 06:41:35 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Bornmann", "Lutz", ""]]}, {"id": "1307.6760", "submitter": "Nadine Rons", "authors": "Nadine Rons", "title": "Quantitative CV-based indicators for research quality, validated by peer\n  review", "comments": "2 pages, 1 figure", "journal-ref": "Proceedings of ISSI 2007, 11th International Conference of the\n  International Society for Scientometrics and Informetrics, CSIC, Madrid,\n  Spain, 25-27 June 2007, 930-931", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a university, research assessments are organized at different policy\nlevels (faculties, research council) in different contexts (funding, council\nmembership, personnel evaluations). Each evaluation requires its own focus and\nmethodology. To conduct a coherent research policy however, data on which\ndifferent assessments are based should be well coordinated. A common set of\ncore indicators for any type of research assessment can provide a supportive\nand objectivating tool for evaluations at different institutional levels and at\nthe same time promote coherent decision-making. The same indicators can also\nform the basis for a 'light touch' monitoring instrument, signalling when and\nwhere a more thorough evaluation could be considered. This poster paper shows\nhow peer review results were used to validate a set of quantitative indicators\nfor research quality for a first series of disciplines. The indicators\ncorrespond to categories in the university's standard CV-format. Per\ndiscipline, specific indicators are identified corresponding to their own\npublication and funding characteristics. Also more globally valid indicators\nare identified after normalization for discipline-characteristic performance\nlevels. The method can be applied to any system where peer ratings and\nquantitative performance measures, both reliable and sufficiently detailed, can\nbe combined for the same entities.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 14:46:21 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Rons", "Nadine", ""]]}, {"id": "1307.6765", "submitter": "Nadine Rons", "authors": "Lucy Amez, Nadine Rons", "title": "Composing a Publication List for Individual Researcher Assessment by\n  Merging Information from Different Sources", "comments": "3 pages", "journal-ref": "Book of Abstracts, 10th International Conference on Science and\n  Technology Indicators, Vienna, Austria, 17-20 September 2008, 435-437", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation and publication profiles are gaining importance for the evaluation\nof top researchers when it comes to the appropriation of funding for excellence\nprograms or career promotion judgments. Indicators like the Normalized Mean\nCitation Rate, the hindex or other distinguishing measures are increasingly\nused to picture the characteristics of individual scholars. Using bibliometric\ntechniques for individual assessment is known to be particularly delicate, as\nthe chance of errors being averaged away becomes smaller whereas a minor\nincompleteness can have a significant influence on the evaluation outcome. The\nquality of the data becomes as such crucial to the legitimacy of the methods\nused.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 14:57:55 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Amez", "Lucy", ""], ["Rons", "Nadine", ""]]}, {"id": "1307.6770", "submitter": "Nadine Rons", "authors": "Nadine Rons, Lucy Amez", "title": "Impact Vitality - A Measure for Excellent Scientists", "comments": "3 pages", "journal-ref": "Book of Abstracts, 10th International Conference on Science and\n  Technology Indicators, Vienna, Austria, 17-20 September 2008, 211-213", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many countries and at European level, research policy increasingly focuses\non 'excellent' researchers. The concept of excellence however is complex and\nmultidimensional. For individual scholars it involves talents for innovative\nknowledge creation and successful transmission to peers, as well as management\ncapacities. Excellence is also a comparative concept, implying the ability to\nsurpass others [TIJSSEN, 2003]. Grants are in general awarded based on\nassessments by expert committees. While peer review is a widely accepted\npractice, it nevertheless is also subject to criticism. At higher aggregation\nlevels, peer assessments are often supported by quantitative measures. At\nindividual level, most of these measures are much less appropriate and there is\na need for new, dedicated indicators.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:04:43 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Rons", "Nadine", ""], ["Amez", "Lucy", ""]]}, {"id": "1307.6773", "submitter": "Nadine Rons", "authors": "Nadine Rons, Arlette De Bruyn", "title": "Quality related publication categories in social sciences and\n  humanities, based on a university's peer review assessments", "comments": "2 pages", "journal-ref": "Book of Abstracts, 11th International Conference on Science and\n  Technology Indicators \"Creating Value for Users\", Leiden, The Netherlands,\n  9-11 September 2010, 229-230", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliometric analysis has firmly conquered its place as an instrument for\nevaluation and international comparison of performance levels. Consequently,\ndifferences in coverage by standard bibliometric databases installed a\ndichotomy between on the one hand the well covered 'exact' sciences, and on the\nother hand most of the social sciences and humanities with a more limited\ncoverage (Nederhof, 2006). Also the latter domains need to be able to soundly\ndemonstrate their level of performance and claim or legitimate funding\naccordingly. An important part of the output volume in social sciences appears\nas books, book chapters and national literature (Hicks, 2004). To proceed from\npublication data to performance measurement, quantitative publication counts\nneed to be combined with qualitative information, for example from peer\nassessment or validation (European Expert Group on Assessment of\nUniversity-Based Research, 2010), to identify those categories that represent\nresearch quality as perceived by peers. An accurate focus is crucial in order\nto stimulate, recognize and reward high quality achievements only. This paper\ndemonstrates how such a selection of publication categories can be based on\ncorrelations with peer judgments. It is also illustrated that the selection\nshould be sufficiently precise, to avoid subcategories negatively correlated\nwith peer judgments. The findings indicate that, also in social sciences and\nhumanities, publications in journals with an international referee system are\nthe most important category for evaluating quality. Book chapters with\ninternational referee system and contributions in international conference\nproceedings follow them.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:10:14 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Rons", "Nadine", ""], ["De Bruyn", "Arlette", ""]]}, {"id": "1307.6778", "submitter": "Nadine Rons", "authors": "Nadine Rons", "title": "Output and citation impact of interdisciplinary networks: Experiences\n  from a dedicated funding program", "comments": "2 pages, 1 figure", "journal-ref": "Book of Abstracts, 11th International Conference on Science and\n  Technology Indicators \"Creating Value for Users\", Leiden, The Netherlands,\n  9-11 September 2010, 227-228", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a context of ever more specialized scientists, interdisciplinarity\nreceives increasing attention as innovating ideas are often situated where the\ndisciplines meet. In many countries science policy makers installed dedicated\nfunding programs and policies. This induces a need for specific tools for their\nsupport. There is however not yet a generally accepted quantitative method or\nset of criteria to recognize and evaluate interdisciplinary research outputs\n(Tracking and evaluating interdisciplinary research: metrics and maps, 12th\nISSI Conference, 2009). Interdisciplinarity also takes on very different forms,\nas distinguished in overviews from the first codifications (Klein, 1990) to the\nlatest reference work (Frodeman et al., 2010). In the specific context of\nresearch measurement and evaluation, interdisciplinarity was discussed e.g. by\nRinia (2007) and Porter et al. (2006). This empirical study aims to contribute\nto the understanding and the measuring of interdisciplinary research at the\nmicro level, in the form of new synergies between disciplines. Investigation of\na specialized funding program shows how a new interdisciplinary synergy and its\ncitation impact are visible in co-publications and cocitations, and that these\nare important parameters for assessment. The results also demonstrate the\neffect of funding, which is clearly present after about three years.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:14:50 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Rons", "Nadine", ""]]}, {"id": "1307.6784", "submitter": "Nadine Rons", "authors": "Nadine Rons", "title": "Interdisciplinary Research Collaborations: Evaluation of a Funding\n  Program", "comments": "14 pages, 4 figures; Article in Collnet Journal of Scientometrics and\n  Information Management based on the paper presented at the Sixth\n  International Conference on Webometrics, Informetrics and Scientometrics &\n  Eleventh COLLNET Meeting held in Mysore, India, 19-22 October 2010", "journal-ref": "Collnet Journal of Scientometrics and Information Management,\n  5(1), 17-32, 2011", "doi": "10.1080/09737766.2011.10700900", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovative ideas are often situated where disciplines meet, and\nsocio-economic problems generally require contributions from several\ndisciplines. Ways to stimulate interdisciplinary research collaborations are\ntherefore an increasing point of attention for science policy. There is concern\nthat 'regular' funding programs, involving advice from disciplinary experts and\ndiscipline-bound viewpoints, may not adequately stimulate, select or evaluate\nthis kind of research. This has led to specific policies aimed at\ninterdisciplinary research in many countries. There is however at this moment\nno generally accepted method to adequately select and evaluate\ninterdisciplinary research. In the vast context of different forms of\ninterdisciplinarity, this paper aims to contribute to the debate on best\npractices to stimulate and support interdisciplinary research collaborations.\nIt describes the selection procedures and results of a university program\nsupporting networks formed 'bottom up', integrating expertise from different\ndisciplines. The program's recent evaluation indicates that it is successful in\nselecting and supporting the interdisciplinary synergies aimed for, responding\nto a need experienced in the field. The analysis further confirms that\npotential for interdisciplinary collaboration is present in all disciplines.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:20:42 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2013 12:34:23 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Rons", "Nadine", ""]]}, {"id": "1307.6791", "submitter": "Nadine Rons", "authors": "Nadine Rons", "title": "Research Excellence Milestones of BRIC and N-11 Countries", "comments": "3 pages, 1 figure", "journal-ref": "Proceedings of ISSI 2011, 13th Conference of the International\n  Society for Scientometrics and Informetrics, Durban, South Africa, 04-07 July\n  2011. Ed Noyons, Patrick Ngulube and Jacqueline Leta (Eds.), Vol. 2,\n  1049-1051", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While scientific performance is an important aspect of a stable and healthy\neconomy, measures for it have yet to gain their place in economic country\nprofiles. As useful indicators for this performance dimension, this paper\nintroduces the concept of milestones for research excellence, as points of\ntransition to higher-level contributions at the leading edge of science. The\nproposed milestones are based on two indicators associated with research\nexcellence, the impact vitality profile and the production of review type\npublications, both applied to a country's publications in the top journals\nNature and Science. The milestones are determined for two distinct groups of\nemerging market economies: the BRIC countries, which outperformed the relative\ngrowth expected at their identification in 2001, and the N-11 or Next Eleven\ncountries, identified in 2005 as potential candidates for a BRIC-like\nevolution. Results show how these two groups at different economic levels can\nbe clearly distinguished based on the research milestones, indicating a\npotential utility as parameters in an economic context.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:26:45 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Rons", "Nadine", ""]]}, {"id": "1307.6792", "submitter": "Nadine Rons", "authors": "Nadine Rons", "title": "Characteristics of International versus Non-International Scientific\n  Publication Media in Team- and Author-Based Data", "comments": "2 pages", "journal-ref": "Proceedings of STI 2012 Montr\\'eal, 17th International Conference\n  on Science and Technology Indicators, Montr\\'eal, Qu\\'ebec, Canada, 05-08\n  September 2012, Eric Archambault, Yves Gingras and Vincent Larivi\\`ere\n  (Eds.), Vol. 2, 888-889", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The enlarged coverage of the international publication and citation databases\nWeb of Science and Scopus towards local media in social sciences was a welcome\nresponse to an increased usage of these databases in evaluation and funding\nsystems. The mostly international journals available earlier were the basis for\nthe development of current standard bibliometric indicators. The same\nindicators may no longer measure exactly the same concepts when applied to\nnewly introduced or extended media categories, with possibly different\ncharacteristics than those of international journals. This paper investigates\ndifferences between media with and without international dimension in\npublication data at team and author level. The findings relate the\ninternational publication categories to research quality, important for\nvalidation of their usage in evaluation or funding models that aim to stimulate\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:30:44 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Rons", "Nadine", ""]]}, {"id": "1307.6797", "submitter": "Nadine Rons", "authors": "Nadine Rons", "title": "Groups of Highly Cited Publications: Stability in Content with Citation\n  Window Length", "comments": "2 pages, 1 figure", "journal-ref": "Proceedings of ISSI 2013, 14th International Society of\n  Scientometrics and Informetrics Conference, Vienna, Austria, 15-19 July 2013,\n  Vol. 2, 1998-2000", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing focus in research policy worldwide on top scientists makes it\nincreasingly important to define adequate supporting measures to help identify\nexcellent scientists. Highly cited publications have since long been associated\nto research excellence. At the same time, the analysis of the high-end of\ncitation distributions still is a challenging topic in evaluative\nbibliometrics. Evaluations typically require indicators that generate\nsufficiently stable results when applied to recent publication records of\nlimited size. Highly cited publications have been identified using two\ntechniques in particular: pre-set percentiles, and the parameter free\nCharacteristic Scores and Scales (CSS) (Gl\\\"anzel & Schubert, 1988). The\nstability required in assessments of relatively small publication records,\nconcerns size as well as content of groups of highly cited publications.\nInfluencing factors include domain delineation and citation window length.\nStability in size is evident for the pre-set percentiles, and has been\ndemonstrated for the CSS-methodology beyond an initial citation period of about\nthree years (Gl\\\"anzel, 2007). Stability in content is less straightforward,\nconsidering for instance that more highly cited publications can have a later\ncitation peak, as observed by Abt (1981) for astronomical papers. This paper\ninvestigates the stability in content of groups of highly cited publications,\ni.e. the extent to which individual publications enter and leave the group as\nthe citation window is enlarged.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:39:41 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Rons", "Nadine", ""]]}, {"id": "1307.6804", "submitter": "Nadine Rons", "authors": "Nadine Rons", "title": "Partition-based Field Normalization: An approach to highly specialized\n  publication records", "comments": "14 pages, 1 figure", "journal-ref": "Journal of Informetrics, 6(1), 1-10, 2012", "doi": "10.1016/j.joi.2011.09.008 10.1016/j.joi.2012.09.001", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field normalized citation rates are well-established indicators for research\nperformance from the broadest aggregation levels such as countries, down to\ninstitutes and research teams. When applied to still more specialized\npublication sets at the level of individual scientists, also a more accurate\ndelimitation is required of the reference domain that provides the expectations\nto which a performance is compared. This necessity for sharper accuracy\nchallenges standard methodology based on predefined subject categories. This\npaper proposes a way to define a reference domain that is more strongly\ndelimited than in standard methodology, by building it up out of cells of the\npartition created by the pre-defined subject categories and their\nintersections. This partition approach can be applied to different existing\nfield normalization variants. The resulting reference domain lies between those\ngenerated by standard field normalization and journal normalization. Examples\nbased on fictive and real publication records illustrate how the potential\nimpact on results can exceed or be smaller than the effect of other currently\ndebated normalization variants, depending on the case studied. The proposed\nPartition-based Field Normalization is expected to offer advantages in\nparticular at the level of individual scientists and other very specific\npublication records, such as publication output from interdisciplinary\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 16:22:21 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Rons", "Nadine", ""]]}, {"id": "1307.6941", "submitter": "Delgado Lopez-Cozar emilio", "authors": "Alvaro Cabezas-Clavijo and Emilio Delgado Lopez-Cozar", "title": "Google Scholar Metrics 2013: nothing new under the sun", "comments": "10 pages, in Spanish, 8 figures", "journal-ref": null, "doi": null, "report-no": "EC3 12", "categories": "cs.DL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Main characteristics of Google Scholar Metrics new version (july 2013) are\npresented. We outline the novelties and the weaknesses detected after a first\nanalysis. As main conclusion, we remark the lack of new functionalities with\nrespect to last editions, as the only modification is the update of the\ntimeframe (2008-2012). Hence, problems pointed out in our last reviews still\nremain active. Finally, it seems Google Scholar Metrics will be updated in a\nyearly basis\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 07:20:44 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Cabezas-Clavijo", "Alvaro", ""], ["Lopez-Cozar", "Emilio Delgado", ""]]}, {"id": "1307.7031", "submitter": "Nadine Rons", "authors": "Nadine Rons, Eric Spruyt", "title": "Reliability and Comparability of Peer Review Results", "comments": "14 pages, 3 figures; Conference papers, New Frontiers in Evaluation,\n  Vienna, Austria, 24-25 April 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper peer review reliability is investigated based on peer ratings\nof research teams at two Belgian universities. It is found that outcomes can be\nsubstantially influenced by the different ways in which experts attribute\nratings. To increase reliability of peer ratings, procedures creating a uniform\nreference level should be envisaged. One should at least check for signs of low\nreliability, which can be obtained from an analysis of the outcomes of the peer\nevaluation itself. The peer review results are compared to outcomes from a\ncitation analysis of publications by the same teams, in subject fields well\ncovered by citation indexes. It is illustrated how, besides reliability,\ncomparability of results depends on the nature of the indicators, on the\nsubject area and on the intrinsic characteristics of the methods. The results\nfurther confirm what is currently considered as good practice: the presentation\nof results for not one but for a series of indicators.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 13:38:13 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Rons", "Nadine", ""], ["Spruyt", "Eric", ""]]}, {"id": "1307.7033", "submitter": "Nadine Rons", "authors": "Nadine Rons, Arlette De Bruyn, Jan Cornelis", "title": "Research evaluation per discipline: a peer-review method and its\n  outcomes", "comments": "21 pages, 4 figures", "journal-ref": "Research Evaluation, 17(1), 45-57, 2008", "doi": "10.3152/095820208X240208", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the method for ex-post peer review evaluation per\nresearch discipline used at the Vrije Universiteit Brussel (VUB) and summarizes\nthe outcomes obtained from it. The method produces pertinent advice and\ntriggers responses - at the level of the individual researcher, the research\nteam and the university's research management - for the benefit of research\nquality, competitivity and visibility. Imposed reflection and contacts during\nand after the evaluation procedure modify the individual researcher's attitude,\nimprove the research teams' strategies and allow for the extraction of general\nrecommendations that are used as discipline-dependent guidelines in the\nuniversity's research management. The deep insights gained in the different\nresearch disciplines and the substantial data sets on their research, support\nthe university management in its policy decisions and in building policy\ninstruments. Moreover, the results are used as a basis for comparison with\nother assessments, leading to a better understanding of the possibilities and\nlimitations of different evaluation processes. The peer review method can be\napplied systematically in a pluri-annual cycle of research discipline\nevaluations to build up a complete overview, or it can be activated on an ad\nhoc basis for a particular discipline, based on demands from research teams or\non strategic or policy arguments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 13:44:08 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Rons", "Nadine", ""], ["De Bruyn", "Arlette", ""], ["Cornelis", "Jan", ""]]}, {"id": "1307.7035", "submitter": "Nadine Rons", "authors": "Nadine Rons, Lucy Amez", "title": "Impact vitality: an indicator based on citing publications in search of\n  excellent scientists", "comments": "12 pages", "journal-ref": "Research Evaluation, 18(3), 233-241, 2009", "doi": "10.3152/095820209X470563", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the quest for an operational definition of\n'research excellence' and proposes a translation of the excellence concept into\na bibliometric indicator. Starting from a textual analysis of funding program\ncalls aimed at individual researchers and from the challenges for an indicator\nat this level in particular, a new type of indicator is proposed. The Impact\nVitality indicator [RONS & AMEZ, 2008] reflects the vitality of the impact of a\nresearcher's publication output, based on the change in volume over time of the\nciting publications. The introduced metric is shown to posses attractive\noperational characteristics and meets a number of criteria which are desirable\nwhen comparing individual researchers. The validity of one of the possible\nindicator variants is tested using a small dataset of applicants for a senior\nfull time Research Fellowship. Options for further research involve testing\nvarious indicator variants on larger samples linked to different kinds of\nevaluations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 13:48:36 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Rons", "Nadine", ""], ["Amez", "Lucy", ""]]}, {"id": "1307.7172", "submitter": "Brian Keegan", "authors": "Brian Keegan, Dan Horn, Thomas A. Finholt, Joseph \"Jofish\" Kaye", "title": "Structure and Dynamics of Coauthorship, Citation, and Impact within CSCW", "comments": "14 pages; 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  CSCW has stabilized as an interdisciplinary venue for computer, information,\ncognitive, and social scientists but has also undergone significant changes in\nits format in recent years. This paper uses methods from social network\nanalysis and bibliometrics to re-examine the structures of CSCW a decade after\nits last systematic analysis. Using data from the ACM Digital Library, we\nanalyze changes in structures of coauthorship and citation between 1986 and\n2013. Statistical models reveal significant but distinct patterns between\npapers and authors in how brokerage and closure in these networks affects\nimpact as measured by citations and downloads. Specifically, impact is unduly\ninfluenced by structural position, such that ideas introduced by those in the\ncore of the CSCW community (e.g., elite researchers) are advantaged over those\nintroduced by peripheral participants (e.g., newcomers). This finding is\nexamined in the context of recent changes to the CSCW conference that may have\nthe effect of upsetting the preference for contributions from the core.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 21:28:15 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 19:35:24 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Keegan", "Brian", ""], ["Horn", "Dan", ""], ["Finholt", "Thomas A.", ""], ["Kaye", "Joseph \"Jofish\"", ""]]}, {"id": "1307.7331", "submitter": "Xianwen Wang", "authors": "Xianwen Wang, Zhi Wang, Wenli Mao, Chen Liu", "title": "How far does scientific community look back?", "comments": "11 pages, 7 figures", "journal-ref": "Journal of Informetrics, 2014, 8(3), 562-568", "doi": "10.1016/j.joi.2014.04.009", "report-no": null, "categories": "cs.DL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does the published scientific literature used by scientific community?\nMany previous studies make analysis on the static usage data. In this research,\nwe propose the concept of dynamic usage data. Based on the platform of\nrealtime.springer.com, we have been monitoring and recording the dynamic usage\ndata of Scientometrics articles round the clock. Our analysis find that papers\npublished in recent four years have many more downloads than papers published\nfour years ago. According to our quantitative calculation, papers down-loaded\non one day have an average lifetime of 4.1 years approximately. Classic papers\nare still being downloaded frequently even long after their publication.\nAdditionally, we find that social media may reboot the attention of old\nscientific literature in a short time.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2013 04:02:28 GMT"}, {"version": "v2", "created": "Sun, 22 Jun 2014 12:02:18 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Wang", "Xianwen", ""], ["Wang", "Zhi", ""], ["Mao", "Wenli", ""], ["Liu", "Chen", ""]]}, {"id": "1307.7498", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Werner Marx, Andreas Barth", "title": "The normalization of citation counts based on classification systems", "comments": "Accepted for publication in the journal publications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If we want to assess whether the paper in question has had a particularly\nhigh or low citation impact compared to other papers, the standard practice in\nbibliometrics is to normalize citations in respect of the subject category and\npublication year. A number of proposals for an improved procedure in the\nnormalization of citation impact have been put forward in recent years. Against\nthe background of these proposals this study describes an ideal solution for\nthe normalization of citation impact: in a first step, the reference set for\nthe publication in question is collated by means of a classification scheme,\nwhere every publication is associated with a single principal research field or\nsubfield entry (e. g. via Chemical Abstracts sections) and a publication year.\nIn a second step, percentiles of citation counts are calculated for this set\nand used to assign the normalized citation impact score to the publications\n(and also to the publication in question).\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 08:42:07 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Bornmann", "Lutz", ""], ["Marx", "Werner", ""], ["Barth", "Andreas", ""]]}, {"id": "1307.8067", "submitter": "Mat Kelly", "authors": "Mat Kelly, Justin F. Brunelle, Michele C. Weigle, Michael L. Nelson", "title": "On the Change in Archivability of Websites Over Time", "comments": "12 pages, 8 figures, Theory and Practice of Digital Libraries (TPDL)\n  2013, Valletta, Malta", "journal-ref": null, "doi": "10.1007/978-3-642-40501-3_5", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  As web technologies evolve, web archivists work to keep up so that our\ndigital history is preserved. Recent advances in web technologies have\nintroduced client-side executed scripts that load data without a referential\nidentifier or that require user interaction (e.g., content loading when the\npage has scrolled). These advances have made automating methods for capturing\nweb pages more difficult. Because of the evolving schemes of publishing web\npages along with the progressive capability of web preservation tools, the\narchivability of pages on the web has varied over time. In this paper we show\nthat the archivability of a web page can be deduced from the type of page being\narchived, which aligns with that page's accessibility in respect to dynamic\ncontent. We show concrete examples of when these technologies were introduced\nby referencing mementos of pages that have persisted through a long evolution\nof available technologies. Identifying these reasons for the inability of these\nweb pages to be archived in the past in respect to accessibility serves as a\nguide for ensuring that content that has longevity is published using good\npractice methods that make it available for preservation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 17:52:38 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Kelly", "Mat", ""], ["Brunelle", "Justin F.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1307.8239", "submitter": "Lutz Bornmann Dr.", "authors": "Werner Marx, Lutz Bornmann, Andreas Barth, Loet Leydesdorff", "title": "Detecting the historical roots of research fields by reference\n  publication year spectroscopy (RPYS)", "comments": "Accepted for publication in the Journal of the American Society for\n  Information Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the quantitative method named \"reference publication year\nspectroscopy\" (RPYS). With this method one can determine the historical roots\nof research fields and quantify their impact on current research. RPYS is based\non the analysis of the frequency with which references are cited in the\npublications of a specific research field in terms of the publication years of\nthese cited references. The origins show up in the form of more or less\npronounced peaks mostly caused by individual publications which are cited\nparticularly frequently. In this study, we use research on graphene and on\nsolar cells to illustrate how RPYS functions, and what results it can deliver.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 07:38:30 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Marx", "Werner", ""], ["Bornmann", "Lutz", ""], ["Barth", "Andreas", ""], ["Leydesdorff", "Loet", ""]]}]