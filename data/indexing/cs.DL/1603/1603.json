[{"id": "1603.00812", "submitter": "Chaomei Chen", "authors": "Chaomei Chen", "title": "Grand Challenges in Measuring and Characterizing Scholarly Impact", "comments": "6 pages", "journal-ref": "Front. Res. Metr. Anal 1:4 (2016)", "doi": "10.3389/frma.2016.00004", "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constantly growing body of scholarly knowledge of science, technology,\nand humanities is an asset of the mankind. While new discoveries expand the\nexisting knowledge, they may simultaneously render some of it obsolete. It is\ncrucial for scientists and other stakeholders to keep their knowledge up to\ndate. Policy makers, decision makers, and the general public also need an\nefficient communication of scientific knowledge. Several grand challenges\nconcerning the creation, adaptation, and diffusion of scholarly knowledge, and\nadvance quantitative and qualitative approaches to the study of scholarly\nknowledge are identified.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 18:03:55 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chen", "Chaomei", ""]]}, {"id": "1603.01204", "submitter": "Mihailo Backovic", "authors": "Mihailo Backovi\\'c", "title": "A Theory of Ambulance Chasing", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL hep-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambulance chasing is a common socio-scientific phenomenon in particle\nphysics. I argue that despite the seeming complexity, it is possible to gain\ninsight into both the qualitative and quantitative features of ambulance\nchasing dynamics. Compound-Poisson statistics suffices to accommodate the time\nevolution of the cumulative number of papers on a topic, where basic\nassumptions that the interest in the topic as well as the number of available\nideas decrease with time appear to drive the time evolution. It follows that if\nthe interest scales as an inverse power law in time, the cumulative number of\npapers on a topic is well described by a di-gamma function, with a distinct\nlogarithmic behavior at large times. In cases where the interest decreases\nexponentially with time, the model predicts that the total number of papers on\nthe topic will converge to a fixed value as time goes to infinity. I\ndemonstrate that the two models are able to fit at least 9 specific instances\nof ambulance chasing in particle physics using only two free parameters. In\ncase of the most recent ambulance chasing instance, the ATLAS {\\gamma}{\\gamma}\nexcess, fits to the current data predict that the total number of papers on the\ntopic will not exceed 310 papers by the June 1. 2016, and prior to the natural\ncut-off for the validity of the theory.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 18:05:36 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Backovi\u0107", "Mihailo", ""]]}, {"id": "1603.01207", "submitter": "Nathan Gibson", "authors": "Nathan P. Gibson, David A. Michelson, Daniel L. Schwartz", "title": "From manuscript catalogues to a handbook of Syriac literature: Modeling\n  an infrastructure for Syriaca.org", "comments": "Part of special issue: Computer-Aided Processing of Intertextuality\n  in Ancient Languages. 15 pages, 4 figures", "journal-ref": "Journal of Data Mining & Digital Humanities, Special Issue on\n  Computer-Aided Processing of Intertextuality in Ancient Languages (May 30,\n  2017) jdmdh:1395", "doi": "10.46298/jdmdh.1395", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite increasing interest in Syriac studies and growing digital\navailability of Syriac texts, there is currently no up-to-date infrastructure\nfor discovering, identifying, classifying, and referencing works of Syriac\nliterature. The standard reference work (Baumstark's Geschichte) is over ninety\nyears old, and the perhaps 20,000 Syriac manuscripts extant worldwide can be\naccessed only through disparate catalogues and databases. The present article\nproposes a tentative data model for Syriaca.org's New Handbook of Syriac\nLiterature, an open-access digital publication that will serve as both an\nauthority file for Syriac works and a guide to accessing their manuscript\nrepresentations, editions, and translations. The authors hope that by\npublishing a draft data model they can receive feedback and incorporate\nsuggestions into the next stage of the project.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 18:14:52 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gibson", "Nathan P.", ""], ["Michelson", "David A.", ""], ["Schwartz", "Daniel L.", ""]]}, {"id": "1603.01336", "submitter": "Sabir Ribas", "authors": "Sabir Ribas, Alberto Ueda, Rodrygo L. T. Santos, Berthier\n  Ribeiro-Neto, Nivio Ziviani", "title": "Simplified Relative Citation Ratio for Static Paper Ranking: UFMG/LATIN\n  at WSDM Cup 2016", "comments": "WSDM Cup. The 9th ACM International Conference on Web Search and Data\n  Mining San Francisco, California, USA. February 22-25, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static rankings of papers play a key role in the academic search setting.\nMany features are commonly used in the literature to produce such rankings,\nsome examples are citation-based metrics, distinct applications of PageRank,\namong others. More recently, learning to rank techniques have been successfully\napplied to combine sets of features producing effective results. In this work,\nwe propose the metric S-RCR, which is a simplified version of a metric called\nRelative Citation Ratio --- both based on the idea of a co-citation network.\nWhen compared to the classical version, our simplification S-RCR leads to\nimproved efficiency with a reasonable effectiveness. We use S-RCR to rank over\n120 million papers in the Microsoft Academic Graph dataset. By using this\nsingle feature, which has no parameters and does not need to be tuned, our team\nwas able to reach the 3rd position in the first phase of the WSDM Cup 2016.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 03:00:46 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Ribas", "Sabir", ""], ["Ueda", "Alberto", ""], ["Santos", "Rodrygo L. T.", ""], ["Ribeiro-Neto", "Berthier", ""], ["Ziviani", "Nivio", ""]]}, {"id": "1603.01774", "submitter": "Behnam Ghavimi", "authors": "Behnam Ghavimi (1,2), Philipp Mayr (1), Sahar Vahdati (2) and\n  Christoph Lange (2,3) ((1) GESIS Leibniz Institute for the Social Sciences,\n  (2) University of Bonn, (3) Fraunhofer IAIS)", "title": "Identifying and Improving Dataset References in Social Sciences Full\n  Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific full text papers are usually stored in separate places than their\nunderlying research datasets. Authors typically make references to datasets by\nmentioning them for example by using their titles and the year of publication.\nHowever, in most cases explicit links that would provide readers with direct\naccess to referenced datasets are missing. Manually detecting references to\ndatasets in papers is time consuming and requires an expert in the domain of\nthe paper. In order to make explicit all links to datasets in papers that have\nbeen published already, we suggest and evaluate a semi-automatic approach for\nfinding references to datasets in social sciences papers. Our approach does not\nneed a corpus of papers (no cold start problem) and it performs well on a small\ntest corpus (gold standard). Our approach achieved an F-measure of 0.84 for\nidentifying references in full texts and an F-measure of 0.83 for finding\ncorrect matches of detected references in the da|ra dataset registry.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 01:09:08 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 12:36:27 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Ghavimi", "Behnam", ""], ["Mayr", "Philipp", ""], ["Vahdati", "Sahar", ""], ["Lange", "Christoph", ""]]}, {"id": "1603.01979", "submitter": "Haewoon Kwak", "authors": "Haewoon Kwak and Jisun An", "title": "Two Tales of the World: Comparison of Widely Used World News Datasets\n  GDELT and EventRegistry", "comments": "To be appeared in ICWSM'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we compare GDELT and Event Registry, which monitor news\narticles worldwide and provide big data to researchers regarding scale, news\nsources, and news geography. We found significant differences in scale and news\nsources, but surprisingly, we observed high similarity in news geography\nbetween the two datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 09:25:57 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Kwak", "Haewoon", ""], ["An", "Jisun", ""]]}, {"id": "1603.03014", "submitter": "Christina Kraus", "authors": "Christina Kraus", "title": "Plagiarism Detection - State-of-the-art systems (2016) and evaluation\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Plagiarism detection systems comprise various approaches that aim to create a\nfair environment for academic publications and appropriately acknowledge the\nauthors' works. While the need for a reliable and performant plagiarism\ndetection system increases with an increasing amount of publications, current\nsystems still have shortcomings. Particularly intelligent research plagiarism\ndetection still leaves room for improvement. An important factor for progress\nin research is a suitable evaluation framework. In this paper, we give an\noverview on the evaluation of plagiarism detection. We then use a taxonomy\nprovided in former research, to classify recent approaches of plagiarism\ndetection. Based on this, we asses the current research situation in the field\nof plagiarism detection and derive further research questions and approaches to\nbe tackled in the future.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 18:31:13 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Kraus", "Christina", ""]]}, {"id": "1603.03144", "submitter": "Yi Yang", "authors": "Yi Yang and Jacob Eisenstein", "title": "Part-of-Speech Tagging for Historical English", "comments": "Accepted to NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more historical texts are digitized, there is interest in applying natural\nlanguage processing tools to these archives. However, the performance of these\ntools is often unsatisfactory, due to language change and genre differences.\nSpelling normalization heuristics are the dominant solution for dealing with\nhistorical texts, but this approach fails to account for changes in usage and\nvocabulary. In this empirical paper, we assess the capability of domain\nadaptation techniques to cope with historical texts, focusing on the classic\nbenchmark task of part-of-speech tagging. We evaluate several domain adaptation\nmethods on the task of tagging Early Modern English and Modern British English\ntexts in the Penn Corpora of Historical English. We demonstrate that the\nFeature Embedding method for unsupervised domain adaptation outperforms word\nembeddings and Brown clusters, showing the importance of embedding the entire\nfeature space, rather than just individual words. Feature Embeddings also give\nbetter performance than spelling normalization, but the combination of the two\nmethods is better still, yielding a 5% raw improvement in tagging accuracy on\nEarly Modern English texts.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 04:27:15 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 16:59:38 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Yang", "Yi", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1603.03170", "submitter": "Laurent Romary", "authors": "Laurent Romary (CMB, ALPAGE), Mike Mertens, Anne Baillot (CMB, ALPAGE)", "title": "Data fluidity in DARIAH -- pushing the agenda forward", "comments": null, "journal-ref": "BIBLIOTHEK Forschung und Praxis, De Gruyter, 2016, 39 (3),\n  pp.350-357", "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides both an update concerning the setting up of the European\nDARIAH infrastructure and a series of strong action lines related to the\ndevelopment of a data centred strategy for the humanities in the coming years.\nIn particular we tackle various aspect of data management: data hosting, the\nsetting up of a DARIAH seal of approval, the establishment of a charter between\ncultural heritage institutions and scholars and finally a specific view on\ncertification mechanisms for data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 07:43:15 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 07:54:58 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Romary", "Laurent", "", "CMB, ALPAGE"], ["Mertens", "Mike", "", "CMB, ALPAGE"], ["Baillot", "Anne", "", "CMB, ALPAGE"]]}, {"id": "1603.03824", "submitter": "Luis Reyes-Galindo", "authors": "Luis Reyes-Galindo", "title": "Automating the Horae: Boundary-work in the age of computers", "comments": "19 pages", "journal-ref": "Social Studies of Science, August 2016 vol. 46 no. 4: 586-606", "doi": "10.1177/0306312716642317", "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the intense software filtering that has allowed the\narXiv eprint repository to sort and process large numbers of submissions with\nminimal human intervention, making it one of the most important and influential\ncases of open access repositories to date. The paper narrates arXiv's\ntransformation, using sophisticated sorting-filtering algorithms to decrease\nhuman workload, from a small mailing list used by a few hundred researchers to\na site that processes thousands of papers per month. However there are\nsignificant negative consequences for authors who have been filtered out of the\nmain categories. There is thus a continued need to check and balance arXiv's\nboundaries, based in the essential tension between stability and innovation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 23:44:49 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Reyes-Galindo", "Luis", ""]]}, {"id": "1603.04395", "submitter": "Joseph Paul Cohen", "authors": "Henry Z. Lo and Joseph Paul Cohen", "title": "Academic Torrents: Scalable Data Distribution", "comments": "Presented at Neural Information Processing Systems 2015 Challenges in\n  Machine Learning (CiML) workshop http://ciml.chalearn.org/home/schedule", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As competitions get more popular, transferring ever-larger data sets becomes\ninfeasible and costly. For example, downloading the 157.3 GB 2012 ImageNet data\nset incurs about $4.33 in bandwidth costs per download. Downloading the full\nImageNet data set takes 33 days. ImageNet has since become popular beyond the\ncompetition, and many papers and models now revolve around this data set. For\nsharing such an important resource to the machine learning community, the\nsharers of ImageNet must shoulder a large bandwidth burden. Academic Torrents\nreduces this burden for disseminating competition data, and also increases\ndownload speeds for end users. Academic Torrents is run by a pending\nnonprofit.. By augmenting an existing HTTP server with a peer-to-peer swarm,\nrequests get re-routed to get data from downloaders. While existing systems\nslow down with more users, the benefits of Academic Torrents grow, with\nnoticeable effects even when only one other person is downloading.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 19:08:54 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Lo", "Henry Z.", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "1603.04939", "submitter": "Stefanie Haustein", "authors": "Stefanie Haustein", "title": "Grand challenges in altmetrics: heterogeneity, data quality and\n  dependencies", "comments": null, "journal-ref": null, "doi": "10.1007/s11192-016-1910-9", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As uptake among researchers is constantly increasing, social media are\nfinding their way into scholarly communication and, under the umbrella term\naltmetrics, were introduced to research evaluation. Fueled by technological\npossibilities and an increasing demand to demonstrate impact beyond the\nscientific community, altmetrics received great attention as potential\ndemocratizers of the scientific reward system and indicators of societal\nimpact. This paper focuses on current challenges of altmetrics. Heterogeneity,\ndata quality and particular dependencies are identified as the three major\nissues and discussed in detail with a particular emphasis on past developments\nin bibliometrics. The heterogeneity of altmetrics mirrors the diversity of the\ntypes of underlying acts, most of which take place on social media platforms.\nThis heterogeneity has made it difficult to establish a common definition or\nconceptual framework. Data quality issues become apparent in the lack of\naccuracy, consistency and replicability of various altmetrics, which is largely\naffected by the dynamic nature of social media events. It is further\nhighlighted that altmetrics are shaped by technical possibilities and depend\nparticularly on the availability of APIs and DOIs, are strongly dependent on\ndata providers and aggregators, and potentially influenced by technical\naffordances of underlying platforms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 02:02:15 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Haustein", "Stefanie", ""]]}, {"id": "1603.05078", "submitter": "Mike Thelwall", "authors": "Mike Thelwall", "title": "Are the discretised lognormal and hooked power law distributions\n  plausible for citation data?", "comments": "Thelwall, M. (in press). Are the discretised lognormal and hooked\n  power law distributions plausible for citation data? Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no agreement over which statistical distribution is most appropriate\nfor modelling citation count data. This is important because if one\ndistribution is accepted then the relative merits of different citation-based\nindicators, such as percentiles, arithmetic means and geometric means, can be\nmore fully assessed. In response, this article investigates the plausibility of\nthe discretised lognormal and hooked power law distributions for modelling the\nfull range of citation counts, with an offset of 1. The citation counts from 23\nScopus subcategories were fitted to hooked power law and discretised lognormal\ndistributions but both distributions failed a Kolmogorov-Smirnov goodness of\nfit test in over three quarters of cases. The discretised lognormal\ndistribution also seems to have the wrong shape for citation distributions,\nwith too few zeros and not enough medium values for all subjects. The cause of\npoor fits could be the impurity of the subject subcategories or the presence of\ninterdisciplinary research. Although it is possible to test for subject\nsubcategory purity indirectly through a goodness of fit test in theory with\nlarge enough sample sizes, it is probably not possible in practice. Hence it\nseems difficult to get conclusive evidence about the theoretically most\nappropriate statistical distribution.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 13:04:59 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Thelwall", "Mike", ""]]}, {"id": "1603.05212", "submitter": "Carlos Luis Gonz\\'alez-Valiente", "authors": "C. L. Gonz\\'alez-Valiente, S. N\\'u\\~nez Amaro, J. R. Santovenia\n  D\\'iaz, M. P. Linares Herrera", "title": "Analysis of the Cuban journal Bibliotecas: Anales de Investigacion", "comments": "in Spanish, Biblios, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this article is to describe the academic impact, the\neditorial process quality, and the editorial and visibility strategies of\nBibliotecas. Anales de Investigacion (BAI), a scientific Cuban journal edited\nby National Library of Cuba Jose Marti. The academic impact is determined\nthrough a citation analysis, which considers Google Scholar database as\nreference source. The bibliometric indicators applied are: citation per year,\ncitation vs. self-citation, citable journals vs. non-citable documents, Hirsch\nIndex, and impact factor. The editorial process quality and the visibility\nstrategies are determined through a self-evaluation which takes into account\nthe SciELO, Scopus, CLASE, Redalyc, Latindex, Dialnet, and ERIH PLUS\nmethodologies. The results reveal an ascending citation line that highlights\nciting journals from the field of Library and Information Science, Medicine and\nHealth Sciences, and Education. Aspects related content and format have\nnegatively influenced on editorial process quality. Some strategies are\nproposed to improve scientific visibility through the inclusion in databases,\ndirectories, and social and academic networks. In general, this study\ncontributes to the editorial decision taking, an issue that could augment the\nimpact and scientific visibility of BAI.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 18:37:38 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Gonz\u00e1lez-Valiente", "C. L.", ""], ["Amaro", "S. N\u00fa\u00f1ez", ""], ["D\u00edaz", "J. R. Santovenia", ""], ["Herrera", "M. P. Linares", ""]]}, {"id": "1603.06485", "submitter": "Lisa Posch", "authors": "Lisa Posch and Philipp Schaer and Arnim Bleier and Markus Strohmaier", "title": "A System for Probabilistic Linking of Thesauri and Classification\n  Systems", "comments": null, "journal-ref": "KI - K\\\"unstliche Intelligenz, 2015", "doi": "10.1007/s13218-015-0413-9", "report-no": null, "categories": "cs.AI cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a system which creates and visualizes probabilistic\nsemantic links between concepts in a thesaurus and classes in a classification\nsystem. For creating the links, we build on the Polylingual Labeled Topic Model\n(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in\nthe classification system by using information from the natural language text\nof documents, their assigned thesaurus descriptors and their designated\nclasses. The links are then presented to users of the system in an interactive\nvisualization, providing them with an automatically generated overview of the\nrelations between the thesaurus and the classification system.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:34:13 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Posch", "Lisa", ""], ["Schaer", "Philipp", ""], ["Bleier", "Arnim", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1603.06494", "submitter": "Lisa Posch", "authors": "Lisa Posch", "title": "Enriching Ontologies with Encyclopedic Background Knowledge for Document\n  Indexing", "comments": null, "journal-ref": "13th International Semantic Web Conference, Riva del Garda, Italy,\n  October 19-23, 2014. Proceedings, Part II", "doi": "10.1007/978-3-319-11915-1_36", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly increasing number of scientific documents available publicly on\nthe Internet creates the challenge of efficiently organizing and indexing these\ndocuments. Due to the time consuming and tedious nature of manual\nclassification and indexing, there is a need for better methods to automate\nthis process. This thesis proposes an approach which leverages encyclopedic\nbackground knowledge for enriching domain-specific ontologies with textual and\nstructural information about the semantic vicinity of the ontologies' concepts.\nThe proposed approach aims to exploit this information for improving both\nontology-based methods for classifying and indexing documents and methods based\non supervised machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:53:32 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Posch", "Lisa", ""]]}, {"id": "1603.07016", "submitter": "Chifumi Nishioka", "authors": "Chifumi Nishioka and Ansgar Scherp", "title": "Profiling vs. Time vs. Content: What does Matter for Top-k Publication\n  Recommendation based on Twitter Profiles? - An Extended Technical Report", "comments": null, "journal-ref": null, "doi": "10.1145/2910896.2910898", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far it is unclear how different factors of a scientific publication\nrecommender system based on users' tweets have an influence on the\nrecommendation performance. We examine three different factors, namely\nprofiling method, temporal decay, and richness of content. Regarding profiling,\nwe compare CF-IDF that replaces terms in TF-IDF by semantic concepts, HCF-IDF\nas novel hierarchical variant of CF-IDF, and topic modeling. As temporal decay\nfunctions, we apply sliding window and exponential decay. In terms of the\nrichness of content, we compare recommendations using both full-texts and\ntitles of publications and using only titles. Overall, the three factors make\ntwelve recommendation strategies. We have conducted an online experiment with\n123 participants and compared the strategies in a within-group design. The best\nrecommendations are achieved by the strategy combining CF-IDF, sliding window,\nand with full-texts. However, the strategies using the novel HCF-IDF profiling\nmethod achieve similar results with just using the titles of the publications.\nTherefore, HCF-IDF can make recommendations when only short and sparse data is\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 22:43:50 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 08:54:09 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2016 07:43:44 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Nishioka", "Chifumi", ""], ["Scherp", "Ansgar", ""]]}, {"id": "1603.07150", "submitter": "Martyn Harris", "authors": "Martyn Harris, Mark Levene, Dell Zhang, Dan Levene", "title": "The Anatomy of a Search and Mining System for Digital Archives", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Samtla (Search And Mining Tools with Linguistic Analysis) is a digital\nhumanities system designed in collaboration with historians and linguists to\nassist them with their research work in quantifying the content of any textual\ncorpora through approximate phrase search and document comparison. The\nretrieval engine uses a character-based n-gram language model rather than the\nconventional word-based one so as to achieve great flexibility in language\nagnostic query processing.\n  The index is implemented as a space-optimised character-based suffix tree\nwith an accompanying database of document content and metadata. A number of\ntext mining tools are integrated into the system to allow researchers to\ndiscover textual patterns, perform comparative analysis, and find out what is\ncurrently popular in the research community.\n  Herein we describe the system architecture, user interface, models and\nalgorithms, and data storage of the Samtla system. We also present several case\nstudies of its usage in practice together with an evaluation of the systems'\nranking performance through crowdsourcing.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 12:02:12 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Harris", "Martyn", ""], ["Levene", "Mark", ""], ["Zhang", "Dell", ""], ["Levene", "Dan", ""]]}, {"id": "1603.07313", "submitter": "Jes\\'us Tramullas", "authors": "Piedad Garrido, Jesus Tramullas, Manuel Coll", "title": "CONDITOR1: Topic Maps and DITA labelling tool for textual documents with\n  historical information", "comments": null, "journal-ref": "Journal of Digital Information, 10, 4, 2009", "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conditor is a software tool which works with textual documents containing\nhistorical information. The purpose of this work two-fold: firstly to show the\nvalidity of the developed engine to correctly identify and label the entities\nof the universe of discourse with a labelled-combined XTM-DITA model. Secondly\nto explain the improvements achieved in the information retrieval process\nthanks to the use of a object-oriented database (JPOX) as well as its\nintegration into the Lucene-type database search process to not only accomplish\nmore accurate searches, but to also help the future development of a\nrecommender system. We finish with a brief demo in a 3D-graph of the results of\nthe aforementioned search.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 19:26:20 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Garrido", "Piedad", ""], ["Tramullas", "Jesus", ""], ["Coll", "Manuel", ""]]}, {"id": "1603.07992", "submitter": "Saeed-Ul Hassan", "authors": "Saeed-Ul Hassan, Uzair Ahmed Gillani", "title": "Altmetrics of \"altmetrics\" using Google Scholar, Twitter, Mendeley,\n  Facebook, Google-plus, CiteULike, Blogs and Wiki", "comments": "19 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We measure the impact of \"altmetrics\" field by deploying altmetrics\nindicators using the data from Google Scholar, Twitter, Mendeley, Facebook,\nGoogle-plus, CiteULike, Blogs and Wiki during 2010- 2014. To capture the social\nimpact of scientific publications, we propose an index called alt-index,\nanalogues to h-index. Across the deployed indices, our results have shown high\ncorrelation among the indicators that capture social impact. While we observe\nmedium Pearson's correlation (\\r{ho}= .247) among the alt-index and h-index, a\nrelatively high correlation is observed between social citations and scholarly\ncitations (\\r{ho}= .646). Interestingly, we find high turnover of social\ncitations in the field compared with the traditional scholarly citations, i.e.\nsocial citations are 42.2 % more than traditional citations. The social mediums\nsuch as Twitter and Mendeley appear to be the most effective channels of social\nimpact followed by Facebook and Google-plus. Overall, altmetrics appears to be\nworking well in the field of \"altmetrics\".\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 19:42:31 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Hassan", "Saeed-Ul", ""], ["Gillani", "Uzair Ahmed", ""]]}, {"id": "1603.08091", "submitter": "Qingqing Zhou", "authors": "Qingqing Zhou, Chengzhi Zhang, Star X. Zhao, Bikun Chen", "title": "Measuring Book Impact Based on the Multi-granularity Online Review\n  Mining", "comments": "21pages,3 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As with articles and journals, the customary methods for measuring books'\nacademic impact mainly involve citations, which is easy but limited to\ninterrogating traditional citation databases and scholarly book reviews,\nResearchers have attempted to use other metrics, such as Google Books,\nlibcitation, and publisher prestige. However, these approaches lack\ncontent-level information and cannot determine the citation intentions of\nusers. Meanwhile, the abundant online review resources concerning academic\nbooks can be used to mine deeper information and content utilizing altmetric\nperspectives. In this study, we measure the impacts of academic books by\nmulti-granularity mining online reviews, and we identify factors that affect a\nbook's impact. First, online reviews of a sample of academic books on Amazon.cn\nare crawled and processed. Then, multi-granularity review mining is conducted\nto identify review sentiment polarities and aspects' sentiment values. Lastly,\nthe numbers of positive reviews and negative reviews, aspect sentiment values,\nstar values, and information regarding helpfulness are integrated via the\nentropy method, and lead to the calculation of the final book impact scores.\nThe results of a correlation analysis of book impact scores obtained via our\nmethod versus traditional book citations show that, although there are\nsubstantial differences between subject areas, online book reviews tend to\nreflect the academic impact. Thus, we infer that online reviews represent a\npromising source for mining book impact within the altmetric perspective and at\nthe multi-granularity content level. Moreover, our proposed method might also\nbe a means by which to measure other books besides academic publications.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 09:25:16 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Zhou", "Qingqing", ""], ["Zhang", "Chengzhi", ""], ["Zhao", "Star X.", ""], ["Chen", "Bikun", ""]]}, {"id": "1603.08452", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff, Lutz Bornmann, Jordan Comins, and Sta\\v{s}a\n  Milojevi\\'c", "title": "Citations: Indicators of Quality? The Impact Fallacy", "comments": "accepted for publication in Frontiers in Research Metrics and\n  Analysis; doi: 10.3389/frma.2016.00001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that citation is a composed indicator: short-term citations can be\nconsidered as currency at the research front, whereas long-term citations can\ncontribute to the codification of knowledge claims into concept symbols.\nKnowledge claims at the research front are more likely to be transitory and are\ntherefore problematic as indicators of quality. Citation impact studies focus\non short-term citation, and therefore tend to measure not epistemic quality,\nbut involvement in current discourses in which contributions are positioned by\nreferencing. We explore this argument using three case studies: (1) citations\nof the journal Soziale Welt as an example of a venue that tends not to publish\npapers at a research front, unlike, for example, JACS; (2) Robert Merton as a\nconcept symbol across theories of citation; and (3) the Multi-RPYS\n(\"Multi-Referenced Publication Year Spectroscopy\") of the journals\nScientometrics, Gene, and Soziale Welt. We show empirically that the\nmeasurement of \"quality\" in terms of citations can further be qualified:\nshort-term citation currency at the research front can be distinguished from\nlonger-term processes of incorporation and codification of knowledge claims\ninto bodies of knowledge. The recently introduced Multi-RPYS can be used to\ndistinguish between short-term and long-term impacts.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 17:37:20 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 08:01:34 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 14:30:52 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Bornmann", "Lutz", ""], ["Comins", "Jordan", ""], ["Milojevi\u0107", "Sta\u0161a", ""]]}, {"id": "1603.09111", "submitter": "Enrique Ordu\\~na-Malea", "authors": "Alberto Martin-Martin, Enrique Orduna-Malea, Juan M. Ayllon and Emilio\n  Delgado Lopez-Cozar", "title": "Back to the past: on the shoulders of an academic search engine giant", "comments": "12 pages, 2 tables", "journal-ref": null, "doi": "10.1007/s11192-016-1917-2", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A study released by the Google Scholar team found an apparently increasing\nfraction of citations to old articles from studies published in the last 24\nyears (1990-2013). To demonstrate this finding we conducted a complementary\nstudy using a different data source (Journal Citation Reports), metric\n(aggregate cited half-life), time spam (2003-2013), and set of categories (53\nSocial Science subject categories and 167 Science subject categories). Although\nthe results obtained confirm and reinforce the previous findings, the possible\ncauses of this phenomenon keep unclear. We finally hypothesize that first page\nresults syndrome in conjunction with the fact that Google Scholar favours the\nmost cited documents are suggesting the growing trend of citing old documents\nis partly caused by Google Scholar.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 10:25:16 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Martin-Martin", "Alberto", ""], ["Orduna-Malea", "Enrique", ""], ["Ayllon", "Juan M.", ""], ["Lopez-Cozar", "Emilio Delgado", ""]]}]