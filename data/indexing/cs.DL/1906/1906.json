[{"id": "1906.00754", "submitter": "Ciriaco Andrea D'Angelo", "authors": "Giovanni Abramo, Ciriaco Andrea D'Angelo, Flavia Di Costa", "title": "A gender analysis of top scientists' collaboration behavior: evidence\n  from Italy", "comments": null, "journal-ref": null, "doi": "10.1007/s11192-019-03136-6", "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work analyzes the differences in collaboration behavior between males\nand females among a particular type of scholars: top scientists, and as\ncompared to non top scientists. The field of observation consists of the\nItalian academic system and the co-authorships of scientific publications by\n11,145 professors. The results obtained from a cross-sectional analysis\ncovering the five-year period 2006-2010 show that there are no significant\ndifferences in the overall propensity to collaborate in the top scientists of\nthe two genders. At the level of single disciplines there are no differences in\ncollaboration behavior, except in the case of: i) international collaborations,\nfor Mathematics and Chemistry - where the propensity for collaboration is\ngreater for males; and ii) extramural domestic collaborations in Physics, in\nwhich it is the females that show greater propensity for collaboration. Because\ninternational collaboration is positively correlated to research performance,\nfindings can inform science policy aimed at increasing the representation of\nfemale top performers.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 12:40:17 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Abramo", "Giovanni", ""], ["D'Angelo", "Ciriaco Andrea", ""], ["Di Costa", "Flavia", ""]]}, {"id": "1906.01440", "submitter": "Rocco Tripodi", "authors": "Rocco Tripodi, Massimo Warglien, Simon Levis Sullam and Deborah Paci", "title": "Tracing Antisemitic Language Through Diachronic Embedding Projections:\n  France 1789-1914", "comments": "Accepted to the 1st International Workshop on Computational\n  Approaches to Historical Language Change 2019 (ACL 2019). 11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate some aspects of the history of antisemitism in France, one of\nthe cradles of modern antisemitism, using diachronic word embeddings. We\nconstructed a large corpus of French books and periodicals issues that contain\na keyword related to Jews and performed a diachronic word embedding over the\n1789-1914 period. We studied the changes over time in the semantic spaces of 4\ntarget words and performed embedding projections over 6 streams of antisemitic\ndiscourse. This allowed us to track the evolution of antisemitic bias in the\nreligious, economic, socio-politic, racial, ethic and conspiratorial domains.\nProjections show a trend of growing antisemitism, especially in the years\nstarting in the mid-80s and culminating in the Dreyfus affair. Our analysis\nalso allows us to highlight the peculiar adverse bias towards Judaism in the\nbroader context of other religions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 13:54:47 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Tripodi", "Rocco", ""], ["Warglien", "Massimo", ""], ["Sullam", "Simon Levis", ""], ["Paci", "Deborah", ""]]}, {"id": "1906.01849", "submitter": "Mike Thelwall Prof", "authors": "Mike Thelwall", "title": "Large publishing consortia produce higher citation impact research but\n  co-author contributions are hard to evaluate", "comments": "Quantitative Science Studies, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple agglomerative clustering method to identify\nlarge publishing consortia with at least 20 authors and 80% shared authorship\nbetween articles. Based on Scopus journal articles 1996-2018, under these\ncriteria, nearly all (88%) of the large consortia published research with\ncitation impact above the world average, with the exceptions being mainly the\nnewer consortia for which average citation counts are unreliable. On average,\nconsortium research had almost double (1.95) the world average citation impact\non the log scale used (Mean Normalised Log Citation Score). At least partial\nalphabetical author ordering was the norm in most consortia. The 250 largest\nconsortia were for nuclear physics and astronomy around expensive equipment,\nand for predominantly health-related issues in genomics, medicine, public\nhealth, microbiology and neuropsychology. For the health-related issues, except\nfor the first and last few authors, authorship seem to primary indicate\ncontributions to the shared project infrastructure necessary to gather the raw\ndata. It is impossible for research evaluators to identify the contributions of\nindividual authors in the huge alphabetical consortia of physics and astronomy,\nand problematic for the middle and end authors of health-related consortia. For\nsmall scale evaluations, authorship contribution statements could be used, when\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 06:44:15 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Thelwall", "Mike", ""]]}, {"id": "1906.02660", "submitter": "Manolis Antonoyiannakis", "authors": "Manolis Antonoyiannakis", "title": "How a Single Paper Affects the Impact Factor: Implications for Scholarly\n  Publishing", "comments": "8 pages, 1 figure, 4 tables, paper accepted for publication at the\n  17th INTERNATIONAL CONFERENCE ON SCIENTOMETRICS & INFORMETRICS (ISSI 2019)", "journal-ref": "Proceedings of the 17th Conference of the International Society on\n  Scientometrics & Informetrics, vol. II, 2306-2313 (2019)", "doi": null, "report-no": null, "categories": "cs.DL physics.data-an physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because the Impact Factor (IF) is an average quantity and most journals are\nsmall, IFs are volatile. We study how a single paper affects the IF using data\nfrom 11639 journals in the 2017 Journal Citation Reports. We define as\nvolatility the IF gain (or loss) caused by a single paper, and this is\ninversely proportional to journal size. We find high volatilities for hundreds\nof journals annually due to their top-cited paper: whether it is a highly-cited\npaper in a small journal, or a moderately (or even low) cited paper in a small\nand low-cited journal. For example, 1218 journals had their most cited paper\nboost their IF by more than 20%, while for 231 journals the boost exceeded 50%.\nWe find that small journals are rewarded much more than large journals for\npublishing a highly-cited paper, and are also penalized more for publishing a\nlow-cited paper, especially if they have a high IF. This produces a strong\nincentive for prestigious, high-IF journals to stay small, to remain\ncompetitive in IF rankings. We discuss the implications for breakthrough papers\nto appear in prestigious journals. We also question the practice of ranking\njournals by IF given this uneven reward mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 16:00:49 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Antonoyiannakis", "Manolis", ""]]}, {"id": "1906.02927", "submitter": "Miguel A. Fortuna", "authors": "Miguel A. Fortuna", "title": "Please, no more scientific journals! The strategy of the scientific\n  publication system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the same way ecosystems tend to increase maturity by decreasing the flow\nof energy per unit biomass, we should move towards a more mature science by\npublishing less but high-quality papers and getting away from joining large\nteams in small roles. That is, we should decrease our scientific productivity\nfor good.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 07:12:26 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Fortuna", "Miguel A.", ""]]}, {"id": "1906.03300", "submitter": "Kensuke Ito", "authors": "Kensuke Ito, Hideyuki Tanaka", "title": "Token-Curated Registry with Citation Graph", "comments": "16 pages, 5 figures", "journal-ref": "LEDGER, Vol.4, (2019) 191-209", "doi": "10.5195/ledger.2019.182", "report-no": null, "categories": "cs.DL cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we aim to incorporate the expertise of anonymous curators into\na token-curated registry (TCR), a decentralized recommender system for\ncollecting a list of high-quality content. This registry is important, because\nprevious studies on TCRs have not specifically focused on technical content,\nsuch as academic papers and patents, whose effective curation requires\nexpertise in relevant fields. To measure expertise, curation in our model\nfocuses on both the content and its citation relationships, for which curator\nassignment uses the Personalized PageRank (PPR) algorithm while reward\ncomputation uses a multi-task peer-prediction mechanism. Our proposed CitedTCR\nbridges the literature on network-based and token-based recommender systems and\ncontributes to the autonomous development of an evolving citation graph for\nhigh-quality content. Moreover, we experimentally confirm the incentive for\nregistration and curation in CitedTCR using the simplification of a one-to-one\ncorrespondence between users and content (nodes).\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 15:30:21 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Ito", "Kensuke", ""], ["Tanaka", "Hideyuki", ""]]}, {"id": "1906.03307", "submitter": "Drahomira Herrmannova", "authors": "Drahomira Herrmannova, Nancy Pontika, Petr Knoth", "title": "Do Authors Deposit on Time? Tracking Open Access Policy Compliance", "comments": "\\c{opyright} 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": "10.1109/JCDL.2019.00037", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen fast growth in the number of policies mandating Open\nAccess (OA) to research outputs. We conduct a large-scale analysis of over 800\nthousand papers from repositories around the world published over a period of 5\nyears to investigate: a) if the time lag between the date of publication and\ndate of deposit in a repository can be effectively tracked across thousands of\nrepositories globally, and b) if introducing deposit deadlines is associated\nwith a reduction of time from acceptance to public availability of research\noutputs. We show that after the introduction of the UK REF 2021 OA policy, this\ntime lag has decreased significantly in the UK and that the policy introduction\nmight have accelerated the UK's move towards immediate OA compared to other\ncountries. This supports the argument for the inclusion of a time-limited\ndeposit requirement in OA policies.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 19:44:27 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Herrmannova", "Drahomira", ""], ["Pontika", "Nancy", ""], ["Knoth", "Petr", ""]]}, {"id": "1906.03840", "submitter": "Nicolas Robinson-Garcia", "authors": "Nicolas Robinson-Garcia, Rodrigo Costas and Thed N. van Leeuwen", "title": "Indicators of Open Access for universities", "comments": "Paper accepted for oral presentation at the ISSI 2019 Conference held\n  in Rome 2-5 September, 2019. Corrected figure 2 and renumbered some figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a first attempt to analyse Open Access integration at the\ninstitutional level. For this, we combine information from Unpaywall and the\nLeiden Ranking to offer basic OA indicators for universities. We calculate the\noverall number of Open Access publications for 930 universities worldwide. OA\nindicators are also disaggregated by green, gold and hybrid Open Access. We\nthen explore differences between and within countries and offer a general\nranking of universities based on the proportion of their output which is openly\naccessible.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 08:44:40 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 11:22:42 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Robinson-Garcia", "Nicolas", ""], ["Costas", "Rodrigo", ""], ["van Leeuwen", "Thed N.", ""]]}, {"id": "1906.04206", "submitter": "Guoqiang Liang", "authors": "Guoqiang Liang, Xiaodan Lou, Haiyan Hou, Zhigang Hu", "title": "Qualifying threshold of take off stage for successfully disseminated\n  creative ideas", "comments": "17 pages", "journal-ref": "Scientometrics, 2019", "doi": "10.1007/s11192-019-03154-4", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The creative process is essentially Darwinian and only a small proportion of\ncreative ideas are selected for further development. However, the threshold\nthat identifies this small fraction of successfully disseminated creative ideas\nat their early stage has not been thoroughly analyzed through the lens of\nRogers innovation diffusion theory. Here, we take highly cited (top 1%)\nresearch papers as an example of the most successfully disseminated creative\nideas and explore the time it takes and citations it receives at their take off\nstage, which play a crucial role in the dissemination of creativity. Results\nshow the majority of highly cited papers will reach 10% and 25% of their total\ncitations within two years and four years, respectively. Interestingly, our\nresults also present a minimal number of articles that attract their first\ncitation before publication. As for the discipline, number of references, and\nPrice index, we find a significant difference exists: Clinical, Pre-Clinical &\nHealth and Life Sciences are the first two disciplines to reach the C10% and\nC25% in a shorter amount of time. Highly cited papers with limited references\nusually take more time to reach 10% and 25% of their total citations. In\naddition, highly cited papers will attract citations rapidly when they cite\nmore recent references. These results provide insights into the timespan and\ncitations for a research paper to become highly cited at the take off stage in\nits diffusion process, as well as the factors that may influence it.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 18:05:27 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Liang", "Guoqiang", ""], ["Lou", "Xiaodan", ""], ["Hou", "Haiyan", ""], ["Hu", "Zhigang", ""]]}, {"id": "1906.04484", "submitter": "Behnam Ghavimi", "authors": "Behnam Ghavimi, Wolfgang Otto, and Philipp Mayr", "title": "EXmatcher: Combining Features Based on Reference Strings and Segments to\n  Enhance Citation Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation matching is a challenging task due to different problems such as the\nvariety of citation styles, mistakes in reference strings and the quality of\nidentified reference segments. The classic citation matching configuration used\nin this paper is the combination of blocking technique and a binary classifier.\nThree different possible inputs (reference strings, reference segments and a\ncombination of reference strings and segments) were tested to find the most\nefficient strategy for citation matching. In the classification step, we\ndescribe the effect which the probabilities of reference segments can have in\ncitation matching. Our evaluation on a manually curated gold standard showed\nthat the input data consisting of the combination of reference segments and\nreference strings lead to the best result. In addition, the usage of the\nprobabilities of the segmentation slightly improves the result.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:19:58 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Ghavimi", "Behnam", ""], ["Otto", "Wolfgang", ""], ["Mayr", "Philipp", ""]]}, {"id": "1906.04588", "submitter": "Iman Tahamtan", "authors": "Iman Tahamtan and Lutz Bornmann", "title": "What Do Citation Counts Measure? An Updated Review of Studies on\n  Citations in Scientific Documents Published between 2006 and 2018", "comments": "56 pages, 4 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to update the review of Bornmann and Daniel\n(2008) presenting a narrative review of studies on citations in scientific\ndocuments. The current review covers 41 studies published between 2006 and\n2018. Bornmann and Daniel (2008) focused on earlier years. The current review\ndescribes the (new) studies on citation content and context analyses as well as\nthe studies that explore the citation motivation of scholars through surveys or\ninterviews. One focus in this paper is on the technical developments in the\nlast decade, such as the richer meta-data available and machine-readable\nformats of scientific papers. These developments have resulted in citation\ncontext analyses of large datasets in comprehensive studies (which was not\npossible previously). Many studies in recent years have used computational and\nmachine learning techniques to determine citation functions and polarities,\nsome of which have attempted to overcome the methodological weaknesses of\nprevious studies. The automated recognition of citation functions seems to have\nthe potential to greatly enhance citation indices and information retrieval\ncapabilities. Our review of the empirical studies demonstrates that a paper may\nbe cited for very different scientific and non-scientific reasons. This result\naccords with the finding by Bornmann and Daniel (2008). The current review also\nshows that to better understand the relationship between citing and cited\ndocuments, a variety of features should be analyzed, primarily the citation\ncontext, the semantics and linguistic patterns in citations, citation locations\nwithin the citing document, and citation polarity (negative, neutral,\npositive).\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 17:40:21 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 10:29:46 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Tahamtan", "Iman", ""], ["Bornmann", "Lutz", ""]]}, {"id": "1906.04619", "submitter": "Giacomo Livan", "authors": "Weihua Li, Tomaso Aste, Fabio Caccioli, Giacomo Livan", "title": "Achieving competitive advantage in academia through early career\n  coauthorship with top scientists", "comments": "17 pages, 7 figures, 2 tables", "journal-ref": "Nature Communications 10, 5170 (2019)", "doi": "10.1038/s41467-019-13130-4", "report-no": null, "categories": "physics.soc-ph cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify the long term impact that the coauthorship with established\ntop-cited scientists has on the career of junior researchers in four different\nscientific disciplines. Through matched pair analysis, we find that junior\nresearchers who coauthor work with top scientists enjoy a persistent\ncompetitive advantage throughout the rest of their careers with respect to\npeers with similar early career profiles. Such a competitive advantage\nmaterialises as a higher probability of repeatedly coauthoring work with\ntop-cited scientists, and, ultimately, as a higher probability of becoming one.\nNotably, we find that the coauthorship with a top scientist has the strongest\nimpact on the careers of junior researchers affiliated with less prestigious\ninstitutions. As a consequence, we argue that such institutions may hold vast\namounts of untapped potential, which may be realised by improving access to top\nscientists.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 14:21:20 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Li", "Weihua", ""], ["Aste", "Tomaso", ""], ["Caccioli", "Fabio", ""], ["Livan", "Giacomo", ""]]}, {"id": "1906.04800", "submitter": "Chaomei Chen", "authors": "Chaomei Chen and Min Song", "title": "Visualizing a Field of Research: A Methodology of Systematic\n  Scientometric Reviews", "comments": "17 figures, 3 tables", "journal-ref": "PLoS ONE 14(10): e0223994 (2019)", "doi": "10.1371/journal.pone.0223994", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic scientometric reviews, empowered by scientometric and visual\nanalytic techniques, offer opportunities to improve the timeliness,\naccessibility, and reproducibility of conventional systematic reviews. While\nincreasingly accessible science mapping tools enable end users to visualize the\nstructure and dynamics of a research field, a common bottleneck in the current\npractice is the construction of a collection of scholarly publications as the\ninput of the subsequent scientometric analysis and visualization. End users\noften have to face a dilemma in the preparation process: the more they know\nabout a knowledge domain, the easier it is for them to find the relevant data\nto meet their needs adequately; the little they know, the harder the problem\nis. What can we do to avoid missing something valuable but beyond our initial\ndescription? In this article, we introduce a flexible and generic methodology,\ncascading citation expansion, to increase the quality of constructing a\nbibliographic dataset for systematic reviews. Furthermore, the methodology\nsimplifies the conceptualization of globalism and localism in science mapping\nand unifies them on a consistent and continuous spectrum. We demonstrate an\napplication of the methodology to the research of literature-based discovery\nand compare five datasets constructed based on three use scenarios, namely a\nconventional keyword-based search (one dataset), an expansion process starting\nwith a groundbreaking article of the knowledge domain (two datasets), and an\nexpansion process starting with a recently published review article by a\nprominent expert in the domain (two datasets). The unique coverage of each of\nthe datasets is inspected through network visualization overlays with reference\nto other datasets in a broad and integrated context.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 20:22:13 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Chen", "Chaomei", ""], ["Song", "Min", ""]]}, {"id": "1906.06039", "submitter": "Silvio Peroni", "authors": "Yongjun Zhu, Erjia Yan, Silvio Peroni and Chao Che", "title": "Nine Million Book Items and Eleven Million Citations: A Study of\n  Book-Based Scholarly Communication Using OpenCitations", "comments": null, "journal-ref": null, "doi": "10.1007/s11192-019-03311-9", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Books have been widely used to share information and contribute to human\nknowledge. However, the quantitative use of books as a method of scholarly\ncommunication is relatively unexamined compared to journal articles and\nconference papers. This study uses the COCI dataset (a comprehensive open\ncitation dataset provided by OpenCitations) to explore books' roles in\nscholarly communication. The COCI data we analyzed includes 445,826,118\ncitations from 46,534,705 bibliographic entities. By analyzing such a large\namount of data, we provide a thorough, multifaceted understanding of books.\nAmong the investigated factors are 1) temporal changes to book citations; 2)\nbook citation distributions; 3) years to citation peak; 4) citation half-life;\nand 5) characteristics of the most-cited books. Results show that books have\nreceived less than 4% of total citations, and have been cited mainly by journal\narticles. Moreover, 97.96% of books have been cited fewer than ten times. Books\ntake longer than other bibliographic materials to reach peak citation levels,\nyet are cited for the same duration as journal articles. Most-cited books tend\nto cover general (yet essential) topics, theories, and technological concepts\nin mathematics and statistics.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 06:11:52 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 08:51:11 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Zhu", "Yongjun", ""], ["Yan", "Erjia", ""], ["Peroni", "Silvio", ""], ["Che", "Chao", ""]]}, {"id": "1906.06132", "submitter": "Christin Katharina Kreutz", "authors": "Christin Katharina Kreutz, Michael Wolz, Ralf Schenkel", "title": "SchenQL -- A Domain-Specific Query Language on Bibliographic Metadata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information access needs to be uncomplicated, users rather use incorrect data\nwhich is easily received than correct information which is harder to obtain.\nQuerying bibliographic metadata from digital libraries mainly supports simple\ntextual queries. A user's demand for answering more sophisticated queries could\nbe fulfilled by the usage of SQL. As such means are highly complex and\nchallenging even for trained programmers, a domain-specific query language is\nneeded to provide a straightforward way to access data.\n  In this paper we present SchenQL, a simple query language focused on\nbibliographic metadata in the area of computer science while using the\nvocabulary of domain-experts. By facilitating a plain syntax and fundamental\naggregate functions, we propose an easy-to-learn domain-specific query language\ncapable of search and exploration. It is suitable for domain-experts as well as\ncasual users while still providing the possibility to answer complicated\nqueries. A user study with computer scientists directly compared our query\nlanguage to SQL and clearly demonstrated SchenQL's suitability and usefulness\nfor given queries as well as users' acceptance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 11:23:28 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 08:51:54 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Kreutz", "Christin Katharina", ""], ["Wolz", "Michael", ""], ["Schenkel", "Ralf", ""]]}, {"id": "1906.06141", "submitter": "Stephan Druskat", "authors": "Stephan Druskat", "title": "Software and Dependencies in Research Citation Graphs", "comments": "10 pages, 2 figures. Accepted for a special issue of IEEE Computing\n  in Science & Engineering on \"Software and Data Citations\", published early\n  access on 2019-11-11", "journal-ref": null, "doi": "10.1109/MCSE.2019.2952840", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following the widespread digitalization of scholarship, software has become\nessential for research, but the current sociotechnical system of citation does\nnot reflect this sufficiently. Citation provides context for research, but the\ncurrent model for the respective research citation graphs does not integrate\nsoftware. In this paper, I develop a directed graph model to alleviate this,\ndescribe challenges for its instantiation, and give an outlook of useful\napplications of research citation graphs, including transitive credit.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 11:50:16 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 08:08:13 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 09:45:27 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Druskat", "Stephan", ""]]}, {"id": "1906.06437", "submitter": "S\\'ergio De Sousa Sr.", "authors": "S\\'ergio Jos\\'e de Sousa, Thiago Magela Rodrigues Dias and Adilson\n  Luiz Pinto", "title": "A Strategy for Expert Recommendation From Open Data Available on the\n  Lattes Platform", "comments": "7 pages, in Portuguese, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing volume of data and users of curriculum systems, the\ndifficulty of finding specialists is increasing.This work proposes an open data\nextraction methodology of the Lattes Platform curricula, a treatment for this\ndata and investigates a Recommendation Agent approach based on deep neural\nnetworks with autoencoder.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 23:36:53 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["de Sousa", "S\u00e9rgio Jos\u00e9", ""], ["Dias", "Thiago Magela Rodrigues", ""], ["Pinto", "Adilson Luiz", ""]]}, {"id": "1906.06843", "submitter": "Mario Krenn", "authors": "Mario Krenn, Anton Zeilinger", "title": "Predicting Research Trends with Semantic and Neural Networks with an\n  application in Quantum Physics", "comments": "9+6 pages, 6 figures", "journal-ref": "PNAS 117(4), 1910-1916 (2020)", "doi": "10.1073/pnas.1914370116", "report-no": null, "categories": "cs.DL cs.LG physics.hist-ph physics.soc-ph quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast and growing number of publications in all disciplines of science\ncannot be comprehended by a single human researcher. As a consequence,\nresearchers have to specialize in narrow sub-disciplines, which makes it\nchallenging to uncover scientific connections beyond the own field of research.\nThus access to structured knowledge from a large corpus of publications could\nhelp pushing the frontiers of science. Here we demonstrate a method to build a\nsemantic network from published scientific literature, which we call SemNet. We\nuse SemNet to predict future trends in research and to inspire new,\npersonalized and surprising seeds of ideas in science. We apply it in the\ndiscipline of quantum physics, which has seen an unprecedented growth of\nactivity in recent years. In SemNet, scientific knowledge is represented as an\nevolving network using the content of 750,000 scientific papers published since\n1919. The nodes of the network correspond to physical concepts, and links\nbetween two nodes are drawn when two physical concepts are concurrently studied\nin research articles. We identify influential and prize-winning research topics\nfrom the past inside SemNet thus confirm that it stores useful semantic\nknowledge. We train a deep neural network using states of SemNet of the past,\nto predict future developments in quantum physics research, and confirm high\nquality predictions using historic data. With the neural network and\ntheoretical network tools we are able to suggest new, personalized,\nout-of-the-box ideas, by identifying pairs of concepts which have unique and\nextremal semantic network properties. Finally, we consider possible future\ndevelopments and implications of our findings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 05:10:52 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 07:40:13 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Krenn", "Mario", ""], ["Zeilinger", "Anton", ""]]}, {"id": "1906.06856", "submitter": "Mayank Singh", "authors": "Naman Jain, Mayank Singh", "title": "The Evolving Ecosystem of Predatory Journals: A Case Study in Indian\n  Perspective", "comments": "Submitted in ICADL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital advancement in scholarly repositories has led to the emergence of a\nlarge number of open access predatory publishers that charge high article\nprocessing fees from authors but fail to provide necessary editorial and\npublishing services. Identifying and blacklisting such publishers has remained\na research challenge due to the highly volatile scholarly publishing ecosystem.\nThis paper presents a data-driven approach to study how potential predatory\npublishers are evolving and bypassing several regularity constraints. We\nempirically show the close resemblance of predatory publishers against reputed\npublishing groups. In addition to verifying standard constraints, we also\npropose distinctive signals gathered from network-centric properties to\nunderstand this evolving ecosystem better.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 06:21:12 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Jain", "Naman", ""], ["Singh", "Mayank", ""]]}, {"id": "1906.07011", "submitter": "Nees Jan van Eck", "authors": "Nees Jan van Eck, Ludo Waltman", "title": "Accuracy of citation data in Web of Science and Scopus", "comments": "Paper published in the Proceedings of the 16th International\n  Conference of the International Society for Scientometrics and Informetrics\n  (pp. 1087-1092)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large-scale analysis of the accuracy of citation data in the Web\nof Science and Scopus databases. The analysis is based on citations given in\npublications in Elsevier journals. We reveal significant data quality problems\nfor both databases. Missing and incorrect references are important problems in\nWeb of Science. Duplicate publications are a serious problem in Scopus.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 13:03:45 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["van Eck", "Nees Jan", ""], ["Waltman", "Ludo", ""]]}, {"id": "1906.07104", "submitter": "Sawood Alam", "authors": "Sawood Alam, Michele C. Weigle, Michael L. Nelson, Martin Klein, and\n  Herbert Van de Sompel", "title": "Supporting Web Archiving via Web Packaging", "comments": "This is a position paper accepted at the ESCAPE Workshop 2019.\n  https://www.iab.org/activities/workshops/escape-workshop/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.CY cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe challenges related to web archiving, replaying archived web\nresources, and verifying their authenticity. We show that Web Packaging has\nsignificant potential to help address these challenges and identify areas in\nwhich changes are needed in order to fully realize that potential.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 16:12:46 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Alam", "Sawood", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""], ["Klein", "Martin", ""], ["Van de Sompel", "Herbert", ""]]}, {"id": "1906.07141", "submitter": "Sawood Alam", "authors": "Sawood Alam, Plinio Vargas, Michele C. Weigle, and Michael L. Nelson", "title": "Impact of HTTP Cookie Violations in Web Archives", "comments": "Presented at WADL 2019 (http://fox.cs.vt.edu/wadl2019.html). Slides:\n  https://www.slideshare.net/ibnesayeed/impact-of-http-cookie-violations-in-web-archives", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Certain HTTP Cookies on certain sites can be a source of content bias in\narchival crawls. Accommodating Cookies at crawl time, but not utilizing them at\nreplay time may cause cookie violations, resulting in defaced composite\nmementos that never existed on the live web. To address these issues, we\npropose that crawlers store Cookies with short expiration time and archival\nreplay systems account for values in the Vary header along with URIs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 17:29:54 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Alam", "Sawood", ""], ["Vargas", "Plinio", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1906.07883", "submitter": "Lucy Lu Wang", "authors": "Lucy Lu Wang, Gabriel Stanovsky, Luca Weihs, Oren Etzioni", "title": "Gender trends in computer science authorship", "comments": "13 pages, 8 figures, 2 tables, 4 appendices; Communications of the\n  ACM", "journal-ref": null, "doi": "10.1145/3430803", "report-no": null, "categories": "cs.DL cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A large-scale, up-to-date analysis of Computer Science literature (11.8M\npapers through 2019) reveals that, if trends from the last 50 years continue,\nparity between the number of male and female authors will not be reached in\nthis century. In contrast, parity is projected to be reached within two to\nthree decades or may have already been reached in other fields of study like\nMedicine or Sociology. Our analysis of collaboration trends in Computer Science\nreveals shifts in the size of the collaboration gap between authors of\ndifferent perceived genders. The gap is persistent but shrinking, corresponding\nto a slow increase in the rate of cross-gender collaborations over time.\nTogether, these trends describe a persistent gender gap in the authorship of\nComputer Science literature that may not close without systematic intervention.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 02:32:04 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 17:40:24 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wang", "Lucy Lu", ""], ["Stanovsky", "Gabriel", ""], ["Weihs", "Luca", ""], ["Etzioni", "Oren", ""]]}, {"id": "1906.07953", "submitter": "Robin Haunschild", "authors": "Jian Du, Peixin Li, Robin Haunschild, Yinan Sun, and Xiaoli Tang", "title": "Paper-Patent Citation Linkages as Early Signs for Predicting Delayed\n  Recognized Knowledge: Macro and Micro Evidence", "comments": "21 pages, 8 figures, and 4 tables; previous version was presented at\n  the ISSI 2019 in Rome, Italy; current version has been accepted for\n  publication in Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate the extent to which patent citations to papers\ncan serve as early signs for predicting delayed recognized knowledge in science\nusing a comparative study with a control group, i.e., instant recognition\npapers. We identify the two opposite groups of papers by the Bcp measure, a\nparameter-free index for identifying papers which were recognized with delay.\nWe provide a macro (Science/Nature papers dataset) and micro (a case chosen\nfrom the dataset) evidence on paper-patent citation linkages as early signs for\npredicting delayed recognized knowledge in science. It appears that papers with\ndelayed recognition show a stronger and longer technical impact than instant\nrecognition papers. We provide indication that in the more recent years papers\nwith delayed recognition are awakened more often and earlier by a patent rather\nthan by a scientific paper (also called \"prince\"). We also found that patent\ncitations seem to play an important role to avoid instant recognition papers to\nlevel off or to become a so called \"flash in the pan\", i.e., instant\nrecognition. It also appears that the sleeping beauties may firstly encounter\nnegative citations and then patent citations and finally get widely recognized.\nIn contrast to the two focused fields (biology and chemistry) for instant\nrecognition papers, delayed recognition papers are rather evenly distributed in\nbiology, chemistry, psychology, geology, materials science, and physics. We\ndiscovered several pairs of \"science sleeping\"-\"technology [...]. We propose in\nfurther research to discover the potential ahead of time and transformative\nresearch by using citation delay analysis, patent & NPL analysis, and citation\ncontext analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 07:45:43 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 13:19:43 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Du", "Jian", ""], ["Li", "Peixin", ""], ["Haunschild", "Robin", ""], ["Sun", "Yinan", ""], ["Tang", "Xiaoli", ""]]}, {"id": "1906.08244", "submitter": "Abdul Rahman Shaikh", "authors": "Abdul Rahman Shaikh and Hamed Alhoori", "title": "Predicting Patent Citations to measure Economic Impact of Scholarly\n  Research", "comments": "2 Pages, 1 figure, JCDL conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial goal of funding research and development has always been to advance\neconomic development. On this basis, a consider-able body of research\nundertaken with the purpose of determining what exactly constitutes economic\nimpact and how to accurately measure that impact has been published. Numerous\nindicators have been used to measure economic impact, although no single\nindicator has been widely adapted. Based on patent data collected from\nAltmetric we predict patent citations through various social media features\nusing several classification models. Patents citing a research paper implies\nthe potential it has for direct application inits field. These predictions can\nbe utilized by researchers in deter-mining the practical applications for their\nwork when applying for patents.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 18:25:32 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Shaikh", "Abdul Rahman", ""], ["Alhoori", "Hamed", ""]]}, {"id": "1906.08470", "submitter": "Athar Sefid", "authors": "Athar Sefid, Jian Wu, Allen C. Ge, Jing Zhao, Lu Liu, Cornelia\n  Caragea, Prasenjit Mitra, C. Lee Giles", "title": "Cleaning Noisy and Heterogeneous Metadata for Record Linking Across\n  Scholarly Big Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically extracted metadata from scholarly documents in PDF formats is\nusually noisy and heterogeneous, often containing incomplete fields and\nerroneous values. One common way of cleaning metadata is to use a bibliographic\nreference dataset. The challenge is to match records between corpora with high\nprecision. The existing solution which is based on information retrieval and\nstring similarity on titles works well only if the titles are cleaned. We\nintroduce a system designed to match scholarly document entities with noisy\nmetadata against a reference dataset. The blocking function uses the classic\nBM25 algorithm to find the matching candidates from the reference data that has\nbeen indexed by ElasticSearch. The core components use supervised methods which\ncombine features extracted from all available metadata fields. The system also\nleverages available citation information to match entities. The combination of\nmetadata and citation achieves high accuracy that significantly outperforms the\nbaseline method on the same test dataset. We apply this system to match the\ndatabase of CiteSeerX against Web of Science, PubMed, and DBLP. This method\nwill be deployed in the CiteSeerX system to clean metadata and link records to\nother scholarly big datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 07:21:33 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Sefid", "Athar", ""], ["Wu", "Jian", ""], ["Ge", "Allen C.", ""], ["Zhao", "Jing", ""], ["Liu", "Lu", ""], ["Caragea", "Cornelia", ""], ["Mitra", "Prasenjit", ""], ["Giles", "C. Lee", ""]]}, {"id": "1906.08933", "submitter": "Aurelio Fernandez Bariviera", "authors": "Ignasi Merediz-Sol\\`a, Aurelio F. Bariviera", "title": "A bibliometric analysis of Bitcoin scientific production", "comments": null, "journal-ref": "Research in International Business and Finance, 50, 294-305 (2019)", "doi": "10.1016/J.RIBAF.2019.06.008", "report-no": null, "categories": "cs.DL q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology, and more specifically Bitcoin (one of its foremost\napplications), have been receiving increasing attention in the scientific\ncommunity. The first publications with Bitcoin as a topic, can be traced back\nto 2012. In spite of this short time span, the production magnitude (1162\npapers) makes it necessary to make a bibliometric study in order to observe\nresearch clusters, emerging topics, and leading scholars. Our paper is aimed at\nstudying the scientific production only around bitcoin, excluding other\nblockchain applications. Thus, we restricted our search to papers indexed in\nthe Web of Science Core Collection, whose topic is \"bitcoin\". This database is\nsuitable for such diverse disciplines such as economics, engineering,\nmathematics, and computer science. This bibliometric study draws the landscape\nof the current state and trends of Bitcoin-related research in different\nscientific disciplines.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 03:33:09 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Merediz-Sol\u00e0", "Ignasi", ""], ["Bariviera", "Aurelio F.", ""]]}, {"id": "1906.09380", "submitter": "Omer Anjum", "authors": "Omer Anjum, Wen-Mei Hwu, Jinjun Xiong", "title": "A Retrospective Recount of Computer Architecture Research with a\n  Data-Driven Study of Over Four Decades of ISCA Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study began with a research project, called DISCvR, conducted at the\nIBM-ILLINOIS Center for Cognitive Computing Systems Reseach. The goal of DISCvR\nwas to build a practical NLP based AI pipeline for document understanding which\nwill help us better understand the computation patterns and requirements of\nmodern computing systems. While building such a prototype, an early use case\ncame to us thanks to the 2017 IEEE/ACM International Symposium on\nMicroarchitecture (MICRO-50) Program Co-chairs, Drs. Hillery Hunter and Jaime\nMoreno. They asked us if we can perform some data-driven analysis of the past\n50 years of MICRO papers and show some interesting historical perspectives on\nMICRO's 50 years of publication. We learned two important lessons from that\nexperience: (1) building an AI solution to truly understand unstructured data\nis hard in spite of the many claimed successes in natural language\nunderstanding; and (2) providing a data-driven perspective on computer\narchitecture research is a very interesting and fun project. Recently we\ndecided to conduct a more thorough study based on all past papers of\nInternational Symposium on Computer Architecture (ISCA) from 1973 to 2018,\nwhich resulted this article. We recognize that we have just scratched the\nsurface of natural language understanding of unstructured data, and there are\nmany more aspects that we can improve. But even with our current study, we felt\nthere were enough interesting findings that may be worthwhile to share with the\ncommunity. Hence we decided to write this article to summarize our findings so\nfar based only on ISCA publications. Our hope is to generate further interests\nfrom the community in this topic, and we welcome collaboration from the\ncommunity to deepen our understanding both of the computer architecture\nresearch and of the challenges of NLP-based AI solutions.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 03:38:41 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Anjum", "Omer", ""], ["Hwu", "Wen-Mei", ""], ["Xiong", "Jinjun", ""]]}, {"id": "1906.09569", "submitter": "Nim Dvir Mr.", "authors": "Nim Dvir, Ruti Gafni", "title": "Systematic improvement of user engagement with academic titles using\n  computational linguistics", "comments": "Dvir, N., & Gafni, R. (2019). Systematic improvement of user\n  engagement with academic titles using computational linguistics. Proceedings\n  of the Informing Science and Information Technology Education Conference,\n  Jerusalem, Israel, pp. 501-512 Santa Rosa, CA: Informing Science Institute.\n  https://doi.org/10.28945/4338", "journal-ref": null, "doi": "10.28945/4338", "report-no": null, "categories": "cs.CL cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a novel approach to systematically improve information\ninteractions based solely on its wording. Following an interdisciplinary\nliterature review, we recognized three key attributes of words that drive user\nengagement: (1) Novelty (2) Familiarity (3) Emotionality. Based on these\nattributes, we developed a model to systematically improve a given content\nusing computational linguistics, natural language processing (NLP) and text\nanalysis (word frequency, sentiment analysis and lexical substitution). We\nconducted a pilot study (n=216) in which the model was used to formalize\nevaluation and optimization of academic titles. A between-group design (A/B\ntesting) was used to compare responses to the original and modified (treatment)\ntitles. Data was collected for selection and evaluation (User Engagement\nScale). The pilot results suggest that user engagement with digital information\nis fostered by, and perhaps dependent upon, the wording being used. They also\nprovide empirical support that engaging content can be systematically evaluated\nand produced. The preliminary results show that the modified (treatment) titles\nhad significantly higher scores for information use and user engagement\n(selection and evaluation). We propose that computational linguistics is a\nuseful approach for optimizing information interactions. The empirically based\ninsights can inform the development of digital content strategies, thereby\nimproving the success of information interactions.elop more sophisticated\ninteraction measures.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 09:23:08 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Dvir", "Nim", ""], ["Gafni", "Ruti", ""]]}, {"id": "1906.09822", "submitter": "Mark Levene", "authors": "Mark Levene, Trevor Fenner and Judit Bar-Ilan", "title": "Characterisation of the $\\chi$-index and the $rec$-index", "comments": "14 pages, 3 figures. This is a pre-print of an article published in\n  Scientometrics. The final authenticated version is available online at:\n  https://doi.org/10.1007/s11192-019-03151-7", "journal-ref": null, "doi": "10.1007/s11192-019-03151-7", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Axiomatic characterisation of a bibliometric index provides insight into the\nproperties that the index satisfies and facilitates the comparison of different\nindices. A geometric generalisation of the $h$-index, called the $\\chi$-index,\nhas recently been proposed to address some of the problems with the $h$-index,\nin particular, the fact that it is not scale invariant, i.e., multiplying the\nnumber of citations of each publication by a positive constant may change the\nrelative ranking of two researchers. While the square of the $h$-index is the\narea of the largest square under the citation curve of a researcher, the square\nof the $\\chi$-index, which we call the $rec$-index (or {\\em rectangle}-index),\nis the area of the largest rectangle under the citation curve. Our main\ncontribution here is to provide a characterisation of the $rec$-index via three\nproperties: {\\em monotonicity}, {\\em uniform citation} and {\\em uniform\nequivalence}. Monotonicity is a natural property that we would expect any\nbibliometric index to satisfy, while the other two properties constrain the\nvalue of the $rec$-index to be the area of the largest rectangle under the\ncitation curve. The $rec$-index also allows us to distinguish between {\\em\ninfluential} researchers who have relatively few, but highly-cited,\npublications and {\\em prolific} researchers who have many, but less-cited,\npublications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 09:59:38 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Levene", "Mark", ""], ["Fenner", "Trevor", ""], ["Bar-Ilan", "Judit", ""]]}, {"id": "1906.09996", "submitter": "Unai Lopez-Novoa", "authors": "Unai Lopez-Novoa, Cyril Charron, John Evans, Leandro Beltrachini", "title": "The BIDS Toolbox: A web service to manage brain imaging datasets", "comments": "Paper for the Workshop on Data Preprocessing for Big Biomedical Data\n  2019, held in conjunction with the IEEE Smart World Congress 2019, Leicester,\n  UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sharing is a key factor for ensuring reproducibility and transparency of\nscientific experiments, and neuroimaging is no exception. The vast\nheterogeneity of data formats and imaging modalities utilised in the field\nmakes it a very challenging problem. In this context, the Brain Imaging Data\nStructure (BIDS) appears as a solution for organising and describing\nneuroimaging datasets. Since its publication in 2015, BIDS has gained\nwidespread attention in the field, as it provides a common way to arrange and\nshare multimodal brain images. Although the evident benefits it presents, BIDS\nhas not been widely adopted in the field of MRI yet and we believe that this is\ndue to the lack of a go-to tool to create and managed BIDS datasets. Motivated\nby this, we present the BIDS Toolbox, a web service to manage brain imaging\ndatasets in BIDS format. Different from other tools, the BIDS Toolbox allows\nthe creation and modification of BIDS-compliant datasets based on MRI data. It\nprovides both a web interface and REST endpoints for its use. In this paper we\ndescribe its design and early prototype, and provide a link to the public\nsource code repository.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 14:34:38 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Lopez-Novoa", "Unai", ""], ["Charron", "Cyril", ""], ["Evans", "John", ""], ["Beltrachini", "Leandro", ""]]}, {"id": "1906.10969", "submitter": "Mirco Sch\\\"onfeld", "authors": "Mirco Sch\\\"onfeld, Steffen Eckhard, Ronny Patz, Hilde van Meegdenburg", "title": "The UN Security Council debates 1995-2017", "comments": "The UN Security Council Debates corpus is available online at\n  https://doi.org/10.7910/DVN/KGVSYH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new dataset containing 65,393 speeches held in the\npublic meetings of the UN Security Council (UNSC) between 1995 and 2017. The\ndataset is based on publicly available meeting transcripts with the S/PV\ndocument symbol and includes the full substance of individual speeches as well\nas automatically extracted and manually corrected metadata on the speaker, the\nposition of the speech in the sequence of speeches of a meeting, and the date\nof the speech. After contextualizing the dataset in recent research on the\nUNSC, the paper presents descriptive statistics on UNSC meetings and speeches\nthat characterize the period covered by the dataset. Data highlight the\nextensive presence of the UN bureaucracy in UNSC meetings as well as an\nemerging trend towards more lengthy open UNSC debates. These open debates cover\nkey issues that have emerged only during the period that is covered by the\ndataset, for example the debates relating to Women, Peace and Security or\nClimate-related Disasters.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 10:57:34 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 12:30:38 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Sch\u00f6nfeld", "Mirco", ""], ["Eckhard", "Steffen", ""], ["Patz", "Ronny", ""], ["van Meegdenburg", "Hilde", ""]]}, {"id": "1906.11217", "submitter": "Mohsen Ahmadvand", "authors": "Mohsen Ahmadvand, Amjad Ibrahim, Felix Huber", "title": "Taxonomy-as-a-Service: How To Structure Your Related Work", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structuring related work is a daunting task encompassing literature review,\nclassification, comparison (primarily in the form of concepts), and gap\nanalysis. Building taxonomies is a compelling way to structure concepts in the\nliterature yielding reusable and extensible models. However, constructing\ntaxonomies as a product of literature reviews could become, to our experiences,\nimmensely complex and error-prone. Including new literature or addressing\nerrors may cause substantial changes (ripple effects) in taxonomies coping with\nwhich requires adequate tools. To this end, we propose a\n\\emph{Taxonomy-as-a-Service (TaaS)} platform. TaaS combines the systematic\npaper review process with taxonomy development, visualization, and analysis\ncapabilities. We evaluate the effectiveness and efficiency of our platform by\nemploying it in the development of a real-world taxonomy. Our results indicate\nthat our TaaS can be used to effectively craft and maintain UML-conforming\ntaxonomies and thereby structure related work. The screencast of our tool\ndemonstration is available at \\url{https://goo.gl/GsTjsP}.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 09:17:20 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Ahmadvand", "Mohsen", ""], ["Ibrahim", "Amjad", ""], ["Huber", "Felix", ""]]}, {"id": "1906.11405", "submitter": "Mayank Singh", "authors": "Heer Ambavi, Ayush Garg, Ayush Garg, Nitiksha, Mridul Sharma, Rohit\n  Sharma, Jayesh Choudhari, Mayank Singh", "title": "BioGen: Automated Biography Generation", "comments": "Accepted at JCDL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A biography of a person is the detailed description of several life events\nincluding his education, work, relationships, and death. Wikipedia, the free\nweb-based encyclopedia, consists of millions of manually curated biographies of\neminent politicians, film and sports personalities, etc. However, manual\ncuration efforts, even though efficient, suffers from significant delays. In\nthis work, we propose an automatic biography generation framework BioGen.\nBioGen generates a short collection of biographical sentences clustered into\nmultiple events of life. Evaluation results show that biographies generated by\nBioGen are significantly closer to manually written biographies in Wikipedia. A\nworking model of this framework is available at nlpbiogen.herokuapp.com/home/.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 01:24:33 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Ambavi", "Heer", ""], ["Garg", "Ayush", ""], ["Garg", "Ayush", ""], ["Nitiksha", "", ""], ["Sharma", "Mridul", ""], ["Sharma", "Rohit", ""], ["Choudhari", "Jayesh", ""], ["Singh", "Mayank", ""]]}, {"id": "1906.11485", "submitter": "Moritz Schubotz", "authors": "Andre Greiner-Petter, Moritz Schubotz, Howard S. Cohl and Bela Gipp", "title": "Semantic Preserving Bijective Mappings for Expressions involving Special\n  Functions in Computer Algebra Systems and Document Preparation Systems", "comments": "This work was supported by the German Research Foundation (DFG, grant\n  GI-1259-1)", "journal-ref": null, "doi": "10.1108/AJIM-08-2018-0185", "report-no": null, "categories": "cs.DL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Modern mathematicians and scientists of math-related disciplines\noften use Document Preparation Systems (DPS) to write and Computer Algebra\nSystems (CAS) to calculate mathematical expressions. Usually, they translate\nthe expressions manually between DPS and CAS. This process is time-consuming\nand error-prone. Our goal is to automate this translation. This paper uses\nMaple and Mathematica as the CAS, and LaTeX as our DPS.\n  Design/methodology/approach: Bruce Miller at the National Institute of\nStandards and Technology (NIST) developed a collection of special LaTeX macros\nthat create links from mathematical symbols to their definitions in the NIST\nDigital Library of Mathematical Functions (DLMF). We are using these macros to\nperform rule-based translations between the formulae in the DLMF and CAS.\nMoreover, we develop software to ease the creation of new rules and to discover\ninconsistencies.\n  Findings: We created 396 mappings and translated 58.8% of DLMF formulae\n(2,405 expressions) successfully between Maple and DLMF. For a significant\npercentage, the special function definitions in Maple and the DLMF were\ndifferent. Therefore, an atomic symbol in one system maps to a composite\nexpression in the other system. The translator was also successfully used for\nautomatic verification of mathematical online compendia and CAS. Our evaluation\ntechniques discovered two errors in the DLMF and one defect in Maple.\n  Originality: This paper introduces the first translation tool for special\nfunctions between LaTeX and CAS. The approach improves error-prone manual\ntranslations and can be used to verify mathematical online compendia and CAS.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 08:08:09 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Greiner-Petter", "Andre", ""], ["Schubotz", "Moritz", ""], ["Cohl", "Howard S.", ""], ["Gipp", "Bela", ""]]}, {"id": "1906.11761", "submitter": "Moritz Schubotz", "authors": "Norman Meuschke, Vincent Stange, Moritz Schubotz, Michael Karmer, Bela\n  Gipp", "title": "Improving Academic Plagiarism Detection for STEM Documents by Analyzing\n  Mathematical Content and Citations", "comments": "Proceedings of the ACM/IEEE-CS Joint Conference on Digital Libraries\n  (JCDL) 2019. The data and code of our study are openly available at\n  https://purl.org/hybridPD", "journal-ref": null, "doi": "10.1109/JCDL.2019.00026", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying academic plagiarism is a pressing task for educational and\nresearch institutions, publishers, and funding agencies. Current plagiarism\ndetection systems reliably find instances of copied and moderately reworded\ntext. However, reliably detecting concealed plagiarism, such as strong\nparaphrases, translations, and the reuse of nontextual content and ideas is an\nopen research problem. In this paper, we extend our prior research on analyzing\nmathematical content and academic citations. Both are promising approaches for\nimproving the detection of concealed academic plagiarism primarily in Science,\nTechnology, Engineering and Mathematics (STEM). We make the following\ncontributions: i) We present a two-stage detection process that combines\nsimilarity assessments of mathematical content, academic citations, and text.\nii) We introduce new similarity measures that consider the order of\nmathematical features and outperform the measures in our prior research. iii)\nWe compare the effectiveness of the math-based, citation-based, and text-based\ndetection approaches using confirmed cases of academic plagiarism. iv) We\ndemonstrate that the combined analysis of math-based and citation-based content\nfeatures allows identifying potentially suspicious cases in a collection of\n102K STEM documents. Overall, we show that analyzing the similarity of\nmathematical content and academic citations is a striking supplement for\nconventional text-based detection approaches for academic literature in the\nSTEM disciplines.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 16:07:47 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Meuschke", "Norman", ""], ["Stange", "Vincent", ""], ["Schubotz", "Moritz", ""], ["Karmer", "Michael", ""], ["Gipp", "Bela", ""]]}, {"id": "1906.11964", "submitter": "Silvio Peroni", "authors": "Silvio Peroni, David Shotton", "title": "OpenCitations, an infrastructure organization for open scholarship", "comments": null, "journal-ref": null, "doi": "10.1162/qss_a_00023", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  OpenCitations is an infrastructure organization for open scholarship\ndedicated to the publication of open citation data as Linked Open Data using\nSemantic Web technologies, thereby providing a disruptive alternative to\ntraditional proprietary citation indexes. Open citation data are valuable for\nbibliometric analysis, increasing the reproducibility of large-scale analyses\nby enabling publication of the source data. Following brief introductions to\nthe development and benefits of open scholarship and to Semantic Web\ntechnologies, this paper describes OpenCitations and its datasets, tools,\nservices and activities. These include the OpenCitations Data Model; the SPAR\n(Semantic Publishing and Referencing) Ontologies; OpenCitations' open software\nof generic applicability for searching, browsing and providing REST APIs over\nRDF triplestores; Open Citation Identifiers (OCIs) and the OpenCitations OCI\nResolution Service; the OpenCitations Corpus (OCC), a database of open\ndownloadable bibliographic and citation data made available in RDF under a\nCreative Commons public domain dedication; and the OpenCitations Indexes of\nopen citation data, of which the first and largest is COCI, the OpenCitations\nIndex of Crossref Open DOI-to-DOI Citations, which currently contains over 445\nmillion bibliographic citations and is receiving considerable usage by the\nscholarly community.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 21:04:13 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 08:41:30 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 21:40:56 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Peroni", "Silvio", ""], ["Shotton", "David", ""]]}]