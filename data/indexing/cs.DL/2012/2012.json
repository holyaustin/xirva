[{"id": "2012.00456", "submitter": "Allard Oelen", "authors": "Allard Oelen, Markus Stocker, S\\\"oren Auer", "title": "Creating a Scholarly Knowledge Graph from Survey Article Tables", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-64452-9_35", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the lack of structure, scholarly knowledge remains hardly accessible\nfor machines. Scholarly knowledge graphs have been proposed as a solution.\nCreating such a knowledge graph requires manual effort and domain experts, and\nis therefore time-consuming and cumbersome. In this work, we present a\nhuman-in-the-loop methodology used to build a scholarly knowledge graph\nleveraging literature survey articles. Survey articles often contain manually\ncurated and high-quality tabular information that summarizes findings published\nin the scientific literature. Consequently, survey articles are an excellent\nresource for generating a scholarly knowledge graph. The presented methodology\nconsists of five steps, in which tables and references are extracted from PDF\narticles, tables are formatted and finally ingested into the knowledge graph.\nTo evaluate the methodology, 92 survey articles, containing 160 survey tables,\nhave been imported in the graph. In total, 2,626 papers have been added to the\nknowledge graph using the presented methodology. The results demonstrate the\nfeasibility of our approach, but also indicate that manual effort is required\nand thus underscore the important role of human experts.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:04:27 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Oelen", "Allard", ""], ["Stocker", "Markus", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2012.02325", "submitter": "Simon J D Cox", "authors": "Simon J D Cox, Alejandra N Gonzalez-Beltran, Barbara Magagna,\n  Maria-Cristina Marinescu", "title": "Ten Simple Rules for making a vocabulary FAIR", "comments": "13 pages", "journal-ref": null, "doi": "10.1371/journal.pcbi.1009041", "report-no": null, "categories": "cs.DL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ten simple rules that support converting a legacy vocabulary -- a\nlist of terms available in a print-based glossary or table not accessible using\nweb standards -- into a FAIR vocabulary. Various pathways may be followed to\npublish the FAIR vocabulary, but we emphasise particularly the goal of\nproviding a distinct IRI for each term or concept. A standard representation of\nthe concept should be returned when the individual IRI is de-referenced, using\nSKOS or OWL serialised in an RDF-based representation for machine-interchange,\nor in a web-page for human consumption. Guidelines for vocabulary and item\nmetadata are provided, as well as development and maintenance considerations.\nBy following these rules you can achieve the outcome of converting a legacy\nvocabulary into a standalone FAIR vocabulary, which can be used for unambiguous\ndata annotation. In turn, this increases data interoperability and enables data\nintegration.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 23:21:31 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Cox", "Simon J D", ""], ["Gonzalez-Beltran", "Alejandra N", ""], ["Magagna", "Barbara", ""], ["Marinescu", "Maria-Cristina", ""]]}, {"id": "2012.02413", "submitter": "Moritz Schubotz", "authors": "Philipp Scharpf, Moritz Schubotz, Andre Greiner-Petter, Malte\n  Ostendorff, Olaf Teschke, Bela Gipp", "title": "ARQMath Lab: An Incubator for Semantic Formula Search in zbMATH Open?", "comments": "in Working Notes of {CLEF} 2020 - Conference and Labs of the\n  Evaluation Forum, Thessaloniki, Greece, September 22-25, 2020\n  http://ceur-ws.org/Vol-2696/paper_200.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The zbMATH database contains more than 4 million bibliographic entries. We\naim to provide easy access to these entries. Therefore, we maintain different\nindex structures, including a formula index. To optimize the findability of the\nentries in our database, we continuously investigate new approaches to satisfy\nthe information needs of our users. We believe that the findings from the\nARQMath evaluation will generate new insights into which index structures are\nmost suitable to satisfy mathematical information needs. Search engines,\nrecommender systems, plagiarism checking software, and many other added-value\nservices acting on databases such as the arXiv and zbMATH need to combine\nnatural and formula language. One initial approach to address this challenge is\nto enrich the mostly unstructured document data via Entity Linking. The ARQMath\nTask at CLEF 2020 aims to tackle the problem of linking newly posted questions\nfrom Math Stack Exchange (MSE) to existing ones that were already answered by\nthe community. To deeply understand MSE information needs, answer-, and formula\ntypes, we performed manual runs for tasks 1 and 2. Furthermore, we explored\nseveral formula retrieval methods: For task 2, such as fuzzy string search,\nk-nearest neighbors, and our recently introduced approach to retrieve\nMathematical Objects of Interest (MOI) with textual search queries. The task\nresults show that neither our automated methods nor our manual runs archived\ngood scores in the competition. However, the perceived quality of the hits\nreturned by the MOI search particularly motivates us to conduct further\nresearch about MOI.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 05:51:01 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 07:09:56 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Scharpf", "Philipp", ""], ["Schubotz", "Moritz", ""], ["Greiner-Petter", "Andre", ""], ["Ostendorff", "Malte", ""], ["Teschke", "Olaf", ""], ["Gipp", "Bela", ""]]}, {"id": "2012.03397", "submitter": "Yasith Jayawardana", "authors": "Yasith Jayawardana, Alexander C. Nwala, Gavindya Jayawardena, Jian Wu,\n  Sampath Jayarathna, Michael L. Nelson, C. Lee Giles", "title": "Modeling Updates of Scholarly Webpages Using Archived Data", "comments": "12 pages, 2 appendix pages, 18 figures, to be published in\n  Proceedings of IEEE Big Data 2020 - 5th Computational Archival Science (CAS)\n  Workshop", "journal-ref": null, "doi": "10.1109/BigData50022.2020.9377796", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vastness of the web imposes a prohibitive cost on building large-scale\nsearch engines with limited resources. Crawl frontiers thus need to be\noptimized to improve the coverage and freshness of crawled content. In this\npaper, we propose an approach for modeling the dynamics of change in the web\nusing archived copies of webpages. To evaluate its utility, we conduct a\npreliminary study on the scholarly web using 19,977 seed URLs of authors'\nhomepages obtained from their Google Scholar profiles. We first obtain archived\ncopies of these webpages from the Internet Archive (IA), and estimate when\ntheir actual updates occurred. Next, we apply maximum likelihood to estimate\ntheir mean update frequency ($\\lambda$) values. Our evaluation shows that\n$\\lambda$ values derived from a short history of archived data provide a good\nestimate for the true update frequency in the short-term, and that our method\nprovides better estimations of updates at a fraction of resources compared to\nthe baseline models. Based on this, we demonstrate the utility of archived data\nto optimize the crawling strategy of web crawlers, and uncover important\nchallenges that inspire future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 00:22:00 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Jayawardana", "Yasith", ""], ["Nwala", "Alexander C.", ""], ["Jayawardena", "Gavindya", ""], ["Wu", "Jian", ""], ["Jayarathna", "Sampath", ""], ["Nelson", "Michael L.", ""], ["Giles", "C. Lee", ""]]}, {"id": "2012.03649", "submitter": "Eugenio Petrovich", "authors": "Eugenio Petrovich", "title": "Bibliometrics in Press. Representations and Uses of Bibliometric\n  Indicators in the Italian Daily Newspapers", "comments": "39 pages, 4 tables, 11 figures, 3 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scholars in science and technology studies and bibliometricians are\nincreasingly revealing the performative nature of bibliometric indicators. Far\nfrom being neutral technical measures, indicators such as the Impact Factor and\nthe h-index are deeply transforming the social and epistemic structures of\ncontemporary science. At the same time, scholars have highlighted how\nbibliometric indicators are endowed with social meanings that go beyond their\npurely technical definitions. These social representations of bibliometric\nindicators are constructed and negotiated between different groups of actors\nwithin several arenas. This study aims to investigate how bibliometric\nindicators are used in a context, which, so far, has not yet been covered by\nresearchers, that of daily newspapers. By a content analysis of a corpus of 583\narticles that appeared in four major Italian newspapers between 1990 and 2020,\nwe chronicle the main functions that bibliometrics and bibliometric indicators\nplayed in the Italian press. Our material shows, among other things, that the\npublic discourse developed in newspapers creates a favorable environment for\nbibliometrics-centered science policies, that bibliometric indicators\ncontribute to the social construction of scientific facts in the press,\nespecially in science news related to medicine, and that professional\nbibliometric expertise struggles to be represented in newspapers and hence\nreach the general public.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:06:49 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Petrovich", "Eugenio", ""]]}, {"id": "2012.03891", "submitter": "Amalie Trewartha", "authors": "Amalie Trewartha, John Dagdelen, Haoyan Huo, Kevin Cruse, Zheren Wang,\n  Tanjin He, Akshay Subramanian, Yuxing Fei, Benjamin Justus, Kristin Persson,\n  Gerbrand Ceder", "title": "COVIDScholar: An automated COVID-19 research aggregation and analysis\n  platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing COVID-19 pandemic has had far-reaching effects throughout\nsociety, and science is no exception. The scale, speed, and breadth of the\nscientific community's COVID-19 response has lead to the emergence of new\nresearch literature on a remarkable scale -- as of October 2020, over 81,000\nCOVID-19 related scientific papers have been released, at a rate of over 250\nper day. This has created a challenge to traditional methods of engagement with\nthe research literature; the volume of new research is far beyond the ability\nof any human to read, and the urgency of response has lead to an increasingly\nprominent role for pre-print servers and a diffusion of relevant research\nacross sources. These factors have created a need for new tools to change the\nway scientific literature is disseminated. COVIDScholar is a knowledge portal\ndesigned with the unique needs of the COVID-19 research community in mind,\nutilizing NLP to aid researchers in synthesizing the information spread across\nthousands of emergent research articles, patents, and clinical trials into\nactionable insights and new knowledge. The search interface for this corpus,\nhttps://covidscholar.org, now serves over 2000 unique users weekly. We present\nalso an analysis of trends in COVID-19 research over the course of 2020.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:17:11 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Trewartha", "Amalie", ""], ["Dagdelen", "John", ""], ["Huo", "Haoyan", ""], ["Cruse", "Kevin", ""], ["Wang", "Zheren", ""], ["He", "Tanjin", ""], ["Subramanian", "Akshay", ""], ["Fei", "Yuxing", ""], ["Justus", "Benjamin", ""], ["Persson", "Kristin", ""], ["Ceder", "Gerbrand", ""]]}, {"id": "2012.04986", "submitter": "Marianne Gauffriau", "authors": "Marianne Gauffriau", "title": "Counting methods introduced into the bibliometric research literature\n  1970-2018: A review", "comments": null, "journal-ref": null, "doi": "10.1162/qss_a_00141", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The present review of bibliometric counting methods investigates 1) the\nnumber of unique counting methods in the bibliometric research literature, 2)\nto what extent the counting methods can be categorized according to selected\ncharacteristics of the counting methods, 3) methods and elements to assess the\ninternal validity of the counting methods, and 4) to what extent and with which\ncharacteristics the counting methods are used in research evaluations.\n  The review identifies 32 counting methods introduced during the period 1981 -\n2018. Two frameworks categorize these counting methods. Framework 1 describes\nselected mathematical properties of counting methods, and Framework 2 describes\narguments for choosing a counting method. Twenty of the 32 counting methods are\nrank-dependent, fractionalized, and introduced to measure contribution,\nparticipation, etc. of an object of study. Next, three criteria for internal\nvalidity are used to identify five methods that test the adequacy of counting\nmethods, two elements that test sensitivity, and three elements that test\nhomogeneity of the counting methods. These methods and elements may be used to\nassess the internal validity of counting methods. Finally, a literature search\nfinds research evaluations that use the counting methods. Only three of the 32\ncounting methods are used by four research evaluations or more. Of these three\ncounting methods, two are used with the same characteristics as defined in the\nstudies that introduced the counting methods.\n  The review provides practitioners in research evaluation and researchers in\nbibliometrics with a detailed foundation for working with counting methods. At\nthe same time, many of the findings in the review provide bases for future\ninvestigations of counting methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 11:29:35 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gauffriau", "Marianne", ""]]}, {"id": "2012.05739", "submitter": "Chiawei Tang", "authors": "Chia-Wei Tang, Chao-Lin Liu and Po-Sen Chiu", "title": "HRCenterNet: An Anchorless Approach to Chinese Character Segmentation in\n  Historical Documents", "comments": null, "journal-ref": null, "doi": "10.1109/BigData50022.2020.9378051", "report-no": null, "categories": "cs.CV cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The information provided by historical documents has always been\nindispensable in the transmission of human civilization, but it has also made\nthese books susceptible to damage due to various factors. Thanks to recent\ntechnology, the automatic digitization of these documents are one of the\nquickest and most effective means of preservation. The main steps of automatic\ntext digitization can be divided into two stages, mainly: character\nsegmentation and character recognition, where the recognition results depend\nlargely on the accuracy of segmentation. Therefore, in this study, we will only\nfocus on the character segmentation of historical Chinese documents. In this\nresearch, we propose a model named HRCenterNet, which is combined with an\nanchorless object detection method and parallelized architecture. The MTHv2\ndataset consists of over 3000 Chinese historical document images and over 1\nmillion individual Chinese characters; with these enormous data, the\nsegmentation capability of our model achieves IoU 0.81 on average with the best\nspeed-accuracy trade-off compared to the others. Our source code is available\nat https://github.com/Tverous/HRCenterNet.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:21:02 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tang", "Chia-Wei", ""], ["Liu", "Chao-Lin", ""], ["Chiu", "Po-Sen", ""]]}, {"id": "2012.07060", "submitter": "Ji Han Dr", "authors": "Ji Han, Serhad Sarica, Feng Shi, Jianxi Luo", "title": "Semantic Networks for Engineering Design: A Survey", "comments": "12 pages, 2 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been growing uses of semantic networks in the past decade, such as\nleveraging large-scale pre-trained graph knowledge databases for various\nnatural language processing (NLP) tasks in engineering design research.\nTherefore, the paper provides a survey of the research that has employed\nsemantic networks in the engineering design research community. The survey\nreveals that engineering design researchers have primarily relied on WordNet,\nConceptNet, and other common-sense semantic network databases trained on\nnon-engineering data sources to develop methods or tools for engineering\ndesign. Meanwhile, there are emerging efforts to mine large scale technical\npublication and patent databases to construct engineering-contextualized\nsemantic network databases, e.g., B-Link and TechNet, to support NLP in\nengineering design. On this basis, we recommend future research directions for\nthe construction and applications of engineering-related semantic networks in\nengineering design research and practice.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 13:36:20 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Han", "Ji", ""], ["Sarica", "Serhad", ""], ["Shi", "Feng", ""], ["Luo", "Jianxi", ""]]}, {"id": "2012.07675", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Ruediger Mutz, Robin Haunschild", "title": "Growth rates of modern science: A latent piecewise growth curve approach\n  to model publication numbers from established and new literature databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Growth of science is a prevalent issue in science of science studies. In\nrecent years, two new bibliographic databases have been introduced which can be\nused to study growth processes in science from centuries back: Dimensions from\nDigital Science and Microsoft Academic. In this study, we used publication data\nfrom these new databases and added publication data from two established\ndatabases (Web of Science from Clarivate Analytics and Scopus from Elsevier) to\ninvestigate scientific growth processes from the beginning of the modern\nscience system until today. We estimated regression models that included\nsimultaneously the publication counts from the four databases. The results of\nthe unrestricted growth of science calculations show that the overall growth\nrate amounts to 4.02% with a doubling time of 16.8 years. As the comparison of\nvarious segmented regression models in the current study revealed, the model\nwith five segments fits the publication data best. We demonstrated that these\nsegments with different growth rates can be interpreted very well, since they\nare related to either phases of economic (e.g., industrialization) and / or\npolitical developments (e.g., Second World War). In this study, we additionally\nanalyzed scientific growth in two broad fields and the relationship of\nscientific and economic growth in UK. We focused on this country, since\nlong-time series for publication counts and economic growth indices were\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 16:25:01 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Bornmann", "Lutz", ""], ["Mutz", "Ruediger", ""], ["Haunschild", "Robin", ""]]}, {"id": "2012.08178", "submitter": "Marcos Baez", "authors": "Maisie Badami, Marcos Baez, Shayan Zamanirad, Wei Kang", "title": "On how Cognitive Computing will plan your next Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic literature reviews (SLRs) are at the heart of evidence-based\nresearch, setting the foundation for future research and practice. However,\nproducing good quality timely contributions is a challenging and highly\ncognitive endeavor, which has lately motivated the exploration of automation\nand support in the SLR process. In this paper we address an often overlooked\nphase in this process, that of planning literature reviews, and explore under\nthe lenses of cognitive process augmentation how to overcome its most salient\nchallenges. In doing so, we report on the insights from 24 SLR authors on\nplanning practices, its challenges as well as feedback on support strategies\ninspired by recent advances in cognitive computing. We frame our findings under\nthe cognitive augmentation framework, and report on a prototype implementation\nand evaluation focusing on further informing the technical feasibility.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:56:09 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Badami", "Maisie", ""], ["Baez", "Marcos", ""], ["Zamanirad", "Shayan", ""], ["Kang", "Wei", ""]]}, {"id": "2012.09269", "submitter": "Ching Jin", "authors": "Ching Jin, Yifang Ma, Brian Uzzi", "title": "Prizes Signal Scientific Revolutions", "comments": "47 pages, 14 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific revolutions affect funding, investments, and technological\nadvances, yet predicting their onset and projected size and impact remains a\npuzzle. We investigated a possible signal predicting a topic's revolutionary\ngrowth - its association with a scientific prize. Our analysis used original\ndata on nearly all recognized prizes associated with 11,539 scientific topics\nawarded between 1960 and 2017 to examine the link between prizes and a topic's\nunexpected growth in productivity, impact, and talent. Using\ndifference-in-differences regressions and counterfactuals of matched\nprizewinning and non-prizewinning topics, we found that in the year following\nthe receipt of a prize, a topic experiences an onset of extraordinary growth in\nimpact and talent that continues into the future. At between five to 10 years\nafter the prize year, prizewinning topics are 38% more productive and 31% more\nimpactful in citations, retain 53% more incumbents, and gain 35% more new\nentrants and 46% more star scientists than their non-prizewinning peer topics.\nWhile prizewinning topics grow unexpectedly fast in talent and impact, funding\ndoes not drive growth; rather, growth is positively associated with the recency\nof work on the topic, discipline-specific rather than general awards, and prize\nmoney. These findings advance understanding of scientific revolutions and\nidentify variations in prize characteristics that predict the timing and size\nof a topic's revolutionary growth. We discuss the implications of these\nfindings on how funding agencies and universities make investments and\nscientists commit time and resources to one topic versus another, as well as on\nthe quality of research.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 21:16:25 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Jin", "Ching", ""], ["Ma", "Yifang", ""], ["Uzzi", "Brian", ""]]}, {"id": "2012.10925", "submitter": "Zheng Xie", "authors": "Zheng Xie", "title": "A distributed hypergraph model for the full-scale simulation of the\n  collaborations in dblp", "comments": "30 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposed a model to give a full-scale simulation for the dynamics\nof the collaborations in the dblp dataset. It is a distributed model with the\ncapability of simulating large hypergraphs, namely systems with heterogeneously\nmultinary relationship. Its assembly mechanism of hyperedges is driven by\nLotka's law and a cooperative game that maximizes benefit-cost ratio for\ncollaborations. The model is built on a circle to express the game, expressing\nthe cost by the distance between nodes. The benefit of coauthoring with a\nproductive researcher or one with many coauthors is expressed by the cumulative\ndegree or hyperdegree of nodes. The model successfully captures the\nmultimodality of collaboration patterns emerged in the dblp dataset, and\nreproduces the evolutionary trends of collaboration pattern, degree,\nhyperdegree, clustering, and giant component over thirty years remarkably well.\nThis model has the potential to be extended to understand the complexity of\nself-organized systems that evolve mainly driven by specific cooperative games,\nand would be capable of predicting the behavior patterns of system nodes.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 13:42:37 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Xie", "Zheng", ""]]}, {"id": "2012.11475", "submitter": "Ivan Heibi", "authors": "Ivan Heibi, Silvio Peroni", "title": "A qualitative and quantitative analysis of open citations to retracted\n  articles: the Wakefield et al.'s case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we show the results of a quantitative and qualitative\nanalysis of open citations on a popular and highly cited retracted paper:\n\"Ileal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive\ndevelopmental disorder in children\" by Wakefield et al., published in 1998. The\nmain purpose of our study is to understand the behavior of the publications\nciting retracted articles and the characteristics of the citations the\nretracted articles accumulated over time. Our analysis is based on a\nmethodology which illustrates how we gathered the data, extracted the topics of\nthe citing articles, and visualized the results. The data and services used are\nall open and free to foster the reproducibility of the analysis. The outcomes\nconcerned the analysis of the entities citing Wakefield et al.'s article and\ntheir related in-text citations. We observed a constant increasing number of\ncitations in the last 20 years, accompanied with a constant increment in the\npercentage of those acknowledging its retraction. Citing articles have started\neither discussing or dealing with the retraction of Wakefield et al.'s article\neven before its full retraction, happened in 2010. Articles in the social\nsciences domain citing the Wakefield et al.'s one were among those that have\nmostly discussed its retraction. In addition, when observing the in-text\ncitations, we noticed that a large part of the citations received by Wakefield\net al.'s article has focused on general discussions without recalling strictly\nmedical details, especially after the full retraction. Medical studies did not\nhesitate in acknowledging the retraction and often provided strong negative\nstatements on it.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 16:45:41 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 12:44:19 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Heibi", "Ivan", ""], ["Peroni", "Silvio", ""]]}, {"id": "2012.12439", "submitter": "Alex Junior Nunes Da Silva", "authors": "Alex Junior Nunes da Silva, Matheus Montanini Breve, Jes\\'us Pascual\n  Mena-Chalco, Fabr\\'icio Martins Lopes", "title": "Analysis of co-authorship networks among Brazilian graduate programs in\n  computer science", "comments": "17 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growth and popularization of platforms on scientific production have been\nthe subject of several studies, producing relevant analyses of coauthorship\nbehavior among groups of researchers. Researchers and their scientific\nproductions can be analyzed as coauthorship social networks, so researchers are\nlinked through common publications. In this context, coauthoring networks can\nbe analyzed to find patterns that can describe or characterize them. This work\npresents the analysis and characterization of co-authorship networks of\nacademic Brazilian graduate programs in computer science. To this end, data\nfrom the curricula of Brazilian researchers were collected and modeled as\ncoauthoring networks among the graduate programs that researchers participate\nin. Each network topology was analyzed regarding complex network measurements\nand three qualitative indices that evaluate the publications quality. In\naddition, the coauthorship networks of the graduate programs were characterized\nin relation to the evaluation received by CAPES, which attributes a qualitative\ngrade to the graduate programs in Brazil. The results indicate some of the most\nrelevant topological measures for the programs characterization and evaluate at\ndifferent qualitative rates and indicate a pattern of the graduate programs\nbest evaluated by CAPES.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 01:29:15 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["da Silva", "Alex Junior Nunes", ""], ["Breve", "Matheus Montanini", ""], ["Mena-Chalco", "Jes\u00fas Pascual", ""], ["Lopes", "Fabr\u00edcio Martins", ""]]}, {"id": "2012.12506", "submitter": "Jerome Niyirora", "authors": "Jerome Niyirora", "title": "Entropic Measures of Complexity in a New Medical Coding System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DL math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: Transitioning from an old medical coding system to a new one can\nbe challenging, especially when the two coding systems are significantly\ndifferent. The US experienced such a transition in 2015. Objective: This\nresearch aims to introduce entropic measures to help users prepare for the\nmigration to a new medical coding system by identifying and focusing\npreparation initiatives on clinical concepts with more likelihood of transition\nchallenges. Methods: Two entropic measures of coding complexity are introduced.\nThe first measure is a function of the variation in the alphabets of new codes.\nThe second measure is based on the possible number of valid representations of\nan old code. Results: A demonstration of how to implement the proposed\ntechniques is carried out using the 2015 mappings between ICD-9-CM and\nICD-10-CM/PCS. The significance of the resulting entropic measures is discussed\nin the context of clinical concepts that were likely to pose challenges\nregarding documentation, coding errors, and longitudinal data comparisons.\nConclusion: The proposed entropic techniques are suitable to assess the\ncomplexity between any two medical coding systems where mappings or crosswalks\nexist. The more the entropy, the more likelihood of transition challenges.\nUsers can utilize the suggested techniques as a guide to prioritize training\nefforts to improve documentation and increase the chances of accurate coding,\ncode validity, and longitudinal data comparisons.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 06:16:09 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Niyirora", "Jerome", ""]]}, {"id": "2012.12526", "submitter": "Alice Allen", "authors": "Alice Allen, Siddha Mavuram, Robert J. Nemiroff, Judy Schmidt, and\n  Peter Teuben", "title": "Making organizational software easier to find in ASCL and ADS", "comments": "4 pages; to be published in the proceedings of the ADASS XXX meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software is the most used instrument in astronomy, and organizations such as\nNASA and the Heidelberg Institute for Theoretical Physics (HITS) fund, develop,\nand release research software. NASA, for example, has created sites such as\ncode.nasa.gov to share its software with the world, but how easy is it to see\nwhat NASA has? Until recently, searching NASA's Astrophysics Data System (ADS)\nfor NASA astronomy research software has not been fruitful. Through its ADAP\nprogram, NASA funded the Astrophysics Source Code Library to improve the\ndiscoverability of these codes. Adding institutional tags to ASCL entries makes\nit easy to find this software not only in the ASCL but also in ADS and other\nservices that index the ASCL. This presentation covered the changes the ASCL\nmade as a result of this funding and how you can use the results of this work\nto better find organizational software in ASCL and ADS.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 07:38:04 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Allen", "Alice", ""], ["Mavuram", "Siddha", ""], ["Nemiroff", "Robert J.", ""], ["Schmidt", "Judy", ""], ["Teuben", "Peter", ""]]}, {"id": "2012.13117", "submitter": "Michael Hucka", "authors": "Task Force on Best Practices for Software Registries: Alain Monteil\n  (INRIA), Alejandra Gonzalez-Beltran (Science and Technology Facilities\n  Council, UK Research and Innovation), Alexandros Ioannidis (CERN), Alice\n  Allen (University of Maryland), Allen Lee (Arizona State University), Anita\n  Bandrowski (University of California at San Diego), Bruce E. Wilson (Oak\n  Ridge National Laboratory), Bryce Mecum (University of California at Santa\n  Barbara), Cai Fan Du (University of Texas at Austin), Carly Robinson\n  (DOE-OSTI), Daniel Garijo (University of Southern California), Daniel S. Katz\n  (University of Illinois at Urbana-Champaign), David Long (Brigham Young\n  University), Genevieve Milliken (NYU Bobst Library), Herv\\'e M\\'enager\n  (Institut Pasteur), Jessica Hausman (NASA Jet Propulsion Laboratory),\n  Jurriaan H. Spaaks (Netherlands eScience Center), Katrina Fenlon (University\n  of Maryland), Kristin Vanderbilt (University of New Mexico), Lorraine Hwang\n  (University of California at Davis), Lynn Davis (DOE-OSTI), Martin Fenner\n  (DataCite), Michael R. Crusoe (CWL), Michael Hucka (California Institute of\n  Technology), Mingfang Wu (Australian Research Data Commons), Neil Chue Hong\n  (University of Edinburgh), Peter Teuben (University of Maryland), Shelley\n  Stall (American Geophysical Union), Stephan Druskat (German Aerospace Center\n  (DLR)/University Jena/Humboldt-Universit\\\"at zu Berlin), Ted Carnevale (Yale\n  University), Thomas Morrell (California Institute of Technology)", "title": "Nine Best Practices for Research Software Registries and Repositories: A\n  Concise Guide", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific software registries and repositories serve various roles in their\nrespective disciplines. These resources improve software discoverability and\nresearch transparency, provide information for software citations, and foster\npreservation of computational methods that might otherwise be lost over time,\nthereby supporting research reproducibility and replicability. However,\ndeveloping these resources takes effort, and few guidelines are available to\nhelp prospective creators of registries and repositories. To address this need,\nwe present a set of nine best practices that can help managers define the\nscope, practices, and rules that govern individual registries and repositories.\nThese best practices were distilled from the experiences of the creators of\nexisting resources, convened by a Task Force of the FORCE11 Software Citation\nImplementation Working Group during the years 2019-2020. We believe that\nputting in place specific policies such as those presented here will help\nscientific software registries and repositories better serve their users and\ntheir disciplines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:37:54 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Registries", "Task Force on Best Practices for Software", "", "INRIA"], [":", "", "", "INRIA"], ["Monteil", "Alain", "", "INRIA"], ["Gonzalez-Beltran", "Alejandra", "", "Science and Technology Facilities\n  Council, UK Research and Innovation"], ["Ioannidis", "Alexandros", "", "CERN"], ["Allen", "Alice", "", "University of Maryland"], ["Lee", "Allen", "", "Arizona State University"], ["Bandrowski", "Anita", "", "University of California at San Diego"], ["Wilson", "Bruce E.", "", "Oak\n  Ridge National Laboratory"], ["Mecum", "Bryce", "", "University of California at Santa\n  Barbara"], ["Du", "Cai Fan", "", "University of Texas at Austin"], ["Robinson", "Carly", "", "DOE-OSTI"], ["Garijo", "Daniel", "", "University of Southern California"], ["Katz", "Daniel S.", "", "University of Illinois at Urbana-Champaign"], ["Long", "David", "", "Brigham Young\n  University"], ["Milliken", "Genevieve", "", "NYU Bobst Library"], ["M\u00e9nager", "Herv\u00e9", "", "Institut Pasteur"], ["Hausman", "Jessica", "", "NASA Jet Propulsion Laboratory"], ["Spaaks", "Jurriaan H.", "", "Netherlands eScience Center"], ["Fenlon", "Katrina", "", "University\n  of Maryland"], ["Vanderbilt", "Kristin", "", "University of New Mexico"], ["Hwang", "Lorraine", "", "University of California at Davis"], ["Davis", "Lynn", "", "DOE-OSTI"], ["Fenner", "Martin", "", "DataCite"], ["Crusoe", "Michael R.", "", "CWL"], ["Hucka", "Michael", "", "California Institute of\n  Technology"], ["Wu", "Mingfang", "", "Australian Research Data Commons"], ["Hong", "Neil Chue", "", "University of Edinburgh"], ["Teuben", "Peter", "", "University of Maryland"], ["Stall", "Shelley", "", "American Geophysical Union"], ["Druskat", "Stephan", "", "German Aerospace Center"], ["Carnevale", "Ted", "", "Yale\n  University"], ["Morrell", "Thomas", "", "California Institute of Technology"]]}, {"id": "2012.13367", "submitter": "Juste Raimbault", "authors": "Juste Raimbault", "title": "A systematic review and meta-analysis of interaction models between\n  transportation networks and territories", "comments": "18 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and simulation in urban and regional studies has always given a\nsignificant place to models relating the dynamics of territories with\ntransportation networks. These include for example Land-use Transport\nInteraction models, but this question has been investigated from different\nviewpoints and disciplines. We propose in this paper a systematic review to\nconstruct a corpus of such models, followed by a meta-analysis of model\ncharacteristics. A statistical analysis provides links between temporal and\nspatial scale of models, their level of interdisciplinarity, and the paper\nyear, with disciplines, type of model and methodology. We unveil in particular\nstrong disciplinary discrepancies in the type of approach taken. This study\nprovides a basis for novel and interdisciplinary approaches to modeling\ninteractions between transportation networks and territories.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:49:08 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Raimbault", "Juste", ""]]}, {"id": "2012.13427", "submitter": "Anirudh Prabhu", "authors": "Anirudh Prabhu and Peter Fox", "title": "Reproducible Workflow", "comments": "7 pages, no figures. Submitted as an entry to the \"Encyclopedia of\n  Mathematical Geosciences.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DL math.IT stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reproducibility has been consistently identified as an important component of\nscientific research. Although there is a general consensus on the importance of\nreproducibility along with the other commonly used 'R' terminology (i.e.,\nReplicability, Repeatability etc.), there is some disagreement on the usage of\nthese terms, including conflicting definitions used by different parts of the\nresearch community. In this encyclopedia article, we explore the different\ndefinitions used in scientific literature (specifically pertaining to\ncomputational research), whether there is a need for a single standardized\ndefinition and provide an alternative based on non-functional requirements. We\nalso describe the role of reproducibility (and other R's) in scientific\nworkflows.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 19:22:27 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Prabhu", "Anirudh", ""], ["Fox", "Peter", ""]]}, {"id": "2012.13560", "submitter": "Xuli Tang", "authors": "Xuli Tang, Xin Li, Ying Ding, Feicheng Ma", "title": "Understanding Team Collaboration in Artificial Intelligence from the\n  perspective of Geographic Distance", "comments": "Accepted short paper submission to iConference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes team collaboration in the field of Artificial\nIntelligence (AI) from the perspective of geographic distance. We obtained\n1,584,175 AI related publications during 1950-2019 from the Microsoft Academic\nGraph. Three latitude-and-longitude-based indicators were employed to quantify\nthe geographic distance of collaborations in AI over time at domestic and\ninternational levels. The results show team collaborations in AI has been more\npopular in the field over time with around 42,000 (38.4%) multiple-affiliation\nAI publications in 2019. The changes in geographic distances of team\ncollaborations indicate the increase of breadth and density for both domestic\nand international collaborations in AI over time. In addition, the United\nStates produced the largest number of single-country and internationally\ncollaborated AI publications, and China has played an important role in\ninternational collaborations in AI after 2010.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 11:06:38 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Tang", "Xuli", ""], ["Li", "Xin", ""], ["Ding", "Ying", ""], ["Ma", "Feicheng", ""]]}, {"id": "2012.13599", "submitter": "Hamed Alhoori", "authors": "Akhil Pandey Akella, Hamed Alhoori, Pavan Ravikanth Kondamudi, Cole\n  Freeman, Haiming Zhou", "title": "Early Indicators of Scientific Impact: Predicting Citations with\n  Altmetrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying important scholarly literature at an early stage is vital to the\nacademic research community and other stakeholders such as technology companies\nand government bodies. Due to the sheer amount of research published and the\ngrowth of ever-changing interdisciplinary areas, researchers need an efficient\nway to identify important scholarly work. The number of citations a given\nresearch publication has accrued has been used for this purpose, but these take\ntime to occur and longer to accumulate. In this article, we use altmetrics to\npredict the short-term and long-term citations that a scholarly publication\ncould receive. We build various classification and regression models and\nevaluate their performance, finding neural networks and ensemble models to\nperform best for these tasks. We also find that Mendeley readership is the most\nimportant factor in predicting the early citations, followed by other factors\nsuch as the academic status of the readers (e.g., student, postdoc, professor),\nfollowers on Twitter, online post length, author count, and the number of\nmentions on Twitter, Wikipedia, and across different countries.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 16:25:07 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Akella", "Akhil Pandey", ""], ["Alhoori", "Hamed", ""], ["Kondamudi", "Pavan Ravikanth", ""], ["Freeman", "Cole", ""], ["Zhou", "Haiming", ""]]}, {"id": "2012.13919", "submitter": "Julian Risch", "authors": "Julian Risch, Nicolas Alder, Christoph Hewel, Ralf Krestel", "title": "PatentMatch: A Dataset for Matching Patent Claims & Prior Art", "comments": "https://hpi.de/naumann/s/patentmatch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patent examiners need to solve a complex information retrieval task when they\nassess the novelty and inventive step of claims made in a patent application.\nGiven a claim, they search for prior art, which comprises all relevant publicly\navailable information. This time-consuming task requires a deep understanding\nof the respective technical domain and the patent-domain-specific language. For\nthese reasons, we address the computer-assisted search for prior art by\ncreating a training dataset for supervised machine learning called PatentMatch.\nIt contains pairs of claims from patent applications and semantically\ncorresponding text passages of different degrees from cited patent documents.\nEach pair has been labeled by technically-skilled patent examiners from the\nEuropean Patent Office. Accordingly, the label indicates the degree of semantic\ncorrespondence (matching), i.e., whether the text passage is prejudicial to the\nnovelty of the claimed invention or not. Preliminary experiments using a\nbaseline system show that PatentMatch can indeed be used for training a binary\ntext pair classifier on this challenging information retrieval task. The\ndataset is available online: https://hpi.de/naumann/s/patentmatch.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 11:22:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Risch", "Julian", ""], ["Alder", "Nicolas", ""], ["Hewel", "Christoph", ""], ["Krestel", "Ralf", ""]]}, {"id": "2012.13990", "submitter": "Mitsuo Yoshida", "authors": "Kenshin Sekimoto, Yoshifumi Seki, Mitsuo Yoshida, Kyoji Umemura", "title": "The metrics of keywords to understand the difference between Retweet and\n  Like in each category", "comments": "The 2020 IEEE/WIC/ACM International Joint Conference on Web\n  Intelligence and Intelligent Agent Technology (WI-IAT '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to clarify what kind of news is easily retweeted\nand what kind of news is easily Liked. We believe these actions, retweeting and\nLiking, have different meanings for users. Understanding this difference is\nimportant for understanding people's interest in Twitter. To analyze the\ndifference between retweets (RT) and Likes on Twitter in detail, we focus on\nword appearances in news titles. First, we calculate basic statistics and\nconfirm that tweets containing news URLs have different RT and Like tendencies\ncompared to other tweets. Next, we compared RTs and Likes for each category and\nconfirmed that the tendency of categories is different. Therefore, we propose\nmetrics for clarifying the differences in each action for each category used in\nthe $\\chi$-square test in order to perform an analysis focusing on the topic.\nThe proposed metrics are more useful than simple counts and TF-IDF for\nextracting meaningful words to understand the difference between RTs and Likes.\nWe analyzed each category using the proposed metrics and quantitatively\nconfirmed that the difference in the role of retweeting and Liking appeared in\nthe content depending on the category. Moreover, by aggregating tweets\nchronologically, the results showed the trend of RT and Like as a list of words\nand clarified how the characteristic words of each week were related to current\nevents for retweeting and Liking.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 18:32:19 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Sekimoto", "Kenshin", ""], ["Seki", "Yoshifumi", ""], ["Yoshida", "Mitsuo", ""], ["Umemura", "Kyoji", ""]]}, {"id": "2012.13992", "submitter": "Mitsuo Yoshida", "authors": "Ryosuke Homma, Yoshifumi Seki, Mitsuo Yoshida, Kyoji Umemura", "title": "Analysis of Short Dwell Time in Relation to User Interest in a News\n  Application", "comments": "The 2020 IEEE/WIC/ACM International Joint Conference on Web\n  Intelligence and Intelligent Agent Technology (WI-IAT '20), Best in Practice\n  Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dwell time has been widely used in various fields to evaluate content quality\nand user engagement. Although many studies shown that content with long dwell\ntime is good quality, contents with short dwell time have not been discussed in\ndetail. We hypothesize that content with short dwell time is not always low\nquality and does not always have low user engagement, but is instead related to\nuser interest. The purpose of this study is to clarify the meanings of short\ndwell time browsing in mobile news application. First, we analyze the relation\nof short dwell time to user interest using large scale user behavior logs from\na mobile news application. This analysis was conducted on a vector space based\non users click histories and then users and articles were mapped in the same\nspace. The users with short dwell time are concentrated on a specific position\nin this space; thus, the length of dwell time is related to their interest.\nMoreover, we also analyze the characteristics of short dwell time browsing by\nexcluding these browses from their click histories. Surprisingly, excluding\nshort dwell time click history, it was found that short dwell time click\nhistory included some aspect of user interest in 30.87% of instances where the\ncluster of users changed. These findings demonstrate that short dwell time does\nnot always indicate a low level of user engagement, but also level of user\ninterest.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 18:36:52 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Homma", "Ryosuke", ""], ["Seki", "Yoshifumi", ""], ["Yoshida", "Mitsuo", ""], ["Umemura", "Kyoji", ""]]}, {"id": "2012.15553", "submitter": "Ruben Miranda", "authors": "Ruben Miranda, Esther Garcia-Carpintero", "title": "Bibliometric analysis of world scientific production in Chemical\n  Engineering during 2000-2011. Part 1: Analysis of total scientific production", "comments": "https://dergipark.org.tr/tr/pub/jauist/issue/59235/839409", "journal-ref": "Journal of Amasya University the Institute of Sciences and\n  Technology 1(2) (2020), 1-39", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive bibliometric analysis of Chemical Engineering area has been\ncarried out through the analysis of the scientific production covered in Web of\nScience during the period 2000-2011. Three complementary studies have been\ncarried out. Part 1 analyzes total scientific production in the area. An\nimportant displacement of the scientific production to the Far East has\noccurred, mainly by the increase in publications from China (the world most\nproductive country since 2008) but also from countries such as India and Iran.\nAlthough the share of publications from Europe, and especially from North\nAmerica, have been decreased significantly, United States is still the country\nwith the highest number of articles among the 1,000 most cited (31.5%),\nfollowed by Germany (8.4%) and China (7.5%). Switzerland, Italy, Netherlands,\nSingapore and Spain outstands as the countries with the highest number of cites\nper article and year and average impact factors of their publications. The\ninternational collaboration in the area is considerably high, especially within\nEuropean countries (>40% publications are in international collaboration). The\nscientific production of the area is concentrated in a few journals (top 10\njournals publishing around 30% publications and top 25 journals, around 50%)\nand publishers (Elsevier, Wiley-Blackwell, Taylor & Francis and the American\nChemical Society). The countries with the highest number of institutions among\nthe top 100 most productive are China and United States (12 each), followed by\nFrance (9), Japan (7) and United Kingdom (7). The top five institutions were\nfrom France, China, India, United States and Russia.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:25:50 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Miranda", "Ruben", ""], ["Garcia-Carpintero", "Esther", ""]]}, {"id": "2012.15556", "submitter": "Ruben Miranda", "authors": "Ruben Miranda, Esther Garcia-Carpintero", "title": "Bibliometric analysis of the world scientific production in Chemical\n  Engineering during 2000-2011. Part 2: Analysis of the 1,000 most cited\n  publications", "comments": "https://dergipark.org.tr/tr/pub/jauist/issue/59235/839424", "journal-ref": "Journal of Amasya University the Institute of Sciences and\n  Technology 1(2) (2020), 40-74", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive bibliometric analysis of the scientific production of\nChemical Engineering area has been carried out using the Web of Science\ndatabase for the period 2000-2011 through three complementary studies. Part 2\ndemonstrated a displacement of the most cited publications to the Far East,\nespecially due to China, however, this displacement is less important to that\nobserved for total scientific production (Part 1). United States is still the\ncountry with the highest number of articles among the 1,000 most cited (31.5%),\nlargely above what expected from their number of publications, followed by\nGermany (8.4%) and China (7.5%). The international collaboration, at least\nglobally, seems not being an important issue for producing highly cited papers.\nIn fact, only two from the top 25 most cited papers were international\ncollaborations (8%). Furthermore, a large share of reviews among the 1,000 most\ncited papers (65%) has been observed. Although the number of institutions with\nmore publications among the most cited in the area are from United States, the\ntwo institutions with the highest cited papers are CNRS (France) and CSIC\n(Spain). The most cited papers are highly concentrated in a few journals:\naround half of the most cited papers were published in five journals.\nGenerally, the most cited papers are published in journals with high impact\nfactors, however, there is also a significant number of highly cited papers\npublished in journals with low or not having impact factor.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:29:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Miranda", "Ruben", ""], ["Garcia-Carpintero", "Esther", ""]]}, {"id": "2012.15558", "submitter": "Ruben Miranda", "authors": "Ruben Miranda, Esther Garcia-Carpintero", "title": "Bibliometric analysis of the world scientific production in Chemical\n  Engineering during 2000-2011. Part 3: Analysis of research trends and hot\n  topics", "comments": "https://dergipark.org.tr/tr/pub/jauist/issue/59235/839429", "journal-ref": "Journal of Amasya University the Institute of Sciences and\n  Technology 1(2) (2020), 75-100", "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive bibliometric analysis of the scientific production of\nChemical Engineering area has been carried out using the Web of Science\ndatabase for the period 2000-2011 through three complementary studies. Part 3\nhas analyzed the distribution of words in article titles, keyword plus and\nauthor keywords of both total scientific production and the 1,000 most cited\npublications. The main areas of Chemical Engineering have been identified; they\nare mainly related to chemical reaction engineering such as catalysis,\nreactors, kinetics, and unit operations such as adsorption. Furthermore, a\ntotal of ten hotspots in the area have been identified: hydrogen as a new\nenergy vector, wastewater treatments, carbon dioxide (capture and\nsequestration), photocatalysis, nanoparticles, biodiesel, nanotubes, ionic\nliquids, advanced oxidation processes, membranes, fuel cells and the use of\nbiomass as raw material (e.g. bioethanol, energy production, etc.). Results\nobtained suggest thematic areas and research trends can be easier analyzed\nthrough the most cited publications, decreasing significantly the time\nnecessary for these analyses. Words in article title, keyword plus and author\nkeywords are complementary, however, author keywords are suggested as the\nsource of most useful data. Compared to other strategies, the author keywords\nare more valuable for identifying research areas and trends, and the total\nnumber of words to be analyzed is lower. Whatever the case, authors recommend a\nrevision of the results obtained by experts in the area to avoid inaccurate\nresults and get the most meaningful information.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:33:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Miranda", "Ruben", ""], ["Garcia-Carpintero", "Esther", ""]]}]