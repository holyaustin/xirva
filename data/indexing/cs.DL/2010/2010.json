[{"id": "2010.00639", "submitter": "Massimo Germano", "authors": "Massimo Germano", "title": "The h-index and the Harmonic Mean", "comments": "3pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Harmonic Mean between the number of papers and the citation number per\npaper is proposed as a simple single-value index to quantify an individual's\nresearch output. Two simple comparisons with the Hirsch h-index are performed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 18:40:24 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Germano", "Massimo", ""]]}, {"id": "2010.01418", "submitter": "Michael J. Kurtz", "authors": "Michael J. Kurtz, Roman Chyla (for the ADS Team)", "title": "Second Order Operators in the NASA Astrophysics Data System", "comments": "ADS Bibcode:2020BAAS...52b0207K, author's version", "journal-ref": "Bulletin of the American Astronomical Society, Vol. 52, No. 2, id.\n  0207 2020", "doi": "10.3847/25c2cfeb.8d12c399", "report-no": null, "categories": "cs.DL astro-ph.IM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second Order Operators (SOOs) are database functions which form secondary\nqueries based on attributes of the objects returned in an initial query; they\ncan provide powerful methods to investigate complex, multipartite information\ngraphs. The NASA Astrophysics Data System (ADS) has implemented four SOOs,\nreviews, useful, trending, and similar which use the citations, references,\ndownloads, and abstract text.\n  This tutorial describes these operators in detail, both alone and in\nconjunction with other functions. It is intended for scientists and others who\nwish to make fuller use of the ADS database. Basic knowledge of the ADS is\nassumed.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 19:57:01 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Kurtz", "Michael J.", "", "for the ADS Team"], ["Chyla", "Roman", "", "for the ADS Team"]]}, {"id": "2010.01540", "submitter": "Aliaksandr Birukou", "authors": "Dmitry Kochetkov, Aliaksandr Birukou, Anna Ermolayeva", "title": "The Importance of Conference Proceedings in Research Evaluation: a\n  Methodology for Assessing Conference Impact", "comments": "Submitted to Scientometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conferences are an essential tool for scientific communication. In\ndisciplines such as Computer Science, over 50% of original research results are\npublished in conference proceedings. In this study, we have analysed the role\nof conference proceedings in various disciplines and propose an alternative\napproach to research evaluation based on conference proceedings and Scimago\nJournal Rank (SJR). The result of the study is a list of conference\nproceedings, categorised Q1 - Q4 in several disciplines by analogy with SJR\njournal quartiles. The comparison of this bibliometric-driven ranking with the\nexpert-driven CORE ranking in Computer Science showed a 62% overlap, as well as\na significant average rank correlation of the category distribution. Moreover,\n38 conference proceedings in Engineering (45% of the list) and 23 in Computer\nScience (32% of the list) have an SJR level corresponding to the first quartile\njournals in these areas. This again emphasises the exceptional importance of\nconferences in these disciplines.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:23:15 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Kochetkov", "Dmitry", ""], ["Birukou", "Aliaksandr", ""], ["Ermolayeva", "Anna", ""]]}, {"id": "2010.02594", "submitter": "Shir Aviv-Reuven", "authors": "Shir Aviv-Reuven and Ariel Rosenfeld (Department of Information\n  Sciences, Bar-Ilan University, Israel)", "title": "Publication Patterns' Changes due to the COVID-19 Pandemic: A\n  longitudinal and short-term scientometric analysis", "comments": "26 pages, 9 figures, 11 tables", "journal-ref": "Scientometrics 126 (2021) 6761-6784", "doi": "10.1007/s11192-021-04059-x", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent months the COVID-19 (also known as SARS-CoV-2 and Coronavirus)\npandemic has spread throughout the world. In parallel, extensive scholarly\nresearch regarding various aspects of the pandemic has been published. In this\nwork, we analyse the changes in biomedical publishing patterns due to the\npandemic. We study the changes in the volume of publications in both peer\nreviewed journals and preprint servers, average time to acceptance of papers\nsubmitted to biomedical journals, international (co-)authorship of these papers\n(expressed by diversity and volume), and the possible association between\njournal metrics and said changes. We study these possible changes using two\napproaches: a short-term analysis through which changes during the first six\nmonths of the outbreak are examined for both COVID-19 related papers and\nnon-COVID-19 related papers; and a longitudinal approach through which changes\nare examined in comparison to the previous four years. Our results show that\nthe pandemic has so far had a tremendous effect on all examined accounts of\nscholarly publications: A sharp increase in publication volume has been\nwitnessed and it can be almost entirely attributed to the pandemic; a\nsignificantly faster mean time to acceptance for COVID-19 papers is apparent,\nand it has (partially) come at the expense of non-COVID-19 papers; and a\nsignificant reduction in international collaboration for COVID-19 papers has\nalso been identified. As the pandemic continues to spread, these changes may\ncause a slow down in research in non-COVID-19 biomedical fields and bring about\na lower rate of international collaboration.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:09:41 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 08:17:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Aviv-Reuven", "Shir", "", "Department of Information\n  Sciences, Bar-Ilan University, Israel"], ["Rosenfeld", "Ariel", "", "Department of Information\n  Sciences, Bar-Ilan University, Israel"]]}, {"id": "2010.03083", "submitter": "Olga Zagovora", "authors": "Olga Zagovora, Roberto Ulloa, Katrin Weller, Fabian Fl\\\"ock", "title": "'I Updated the <ref>': The Evolution of References in the English\n  Wikipedia and the Implications for Altmetrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With this work, we present a publicly available dataset of the history of all\nthe references (more than 55 million) ever used in the English Wikipedia until\nJune 2019. We have applied a new method for identifying and monitoring\nreferences in Wikipedia, so that for each reference we can provide data about\nassociated actions: creation, modifications, deletions, and reinsertions. The\nhigh accuracy of this method and the resulting dataset was confirmed via a\ncomprehensive crowdworker labelling campaign. We use the dataset to study the\ntemporal evolution of Wikipedia references as well as users' editing behaviour.\nWe find evidence of a mostly productive and continuous effort to improve the\nquality of references: (1) there is a persistent increase of reference and\ndocument identifiers (DOI, PubMedID, PMC, ISBN, ISSN, ArXiv ID), and (2) most\nof the reference curation work is done by registered humans (not bots or\nanonymous editors). We conclude that the evolution of Wikipedia references,\nincluding the dynamics of the community processes that tend to them should be\nleveraged in the design of relevance indexes for altmetrics, and our dataset\ncan be pivotal for such effort.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 23:26:12 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zagovora", "Olga", ""], ["Ulloa", "Roberto", ""], ["Weller", "Katrin", ""], ["Fl\u00f6ck", "Fabian", ""]]}, {"id": "2010.03847", "submitter": "Serhii Nazarovets", "authors": "Serhii Nazarovets", "title": "Quantitative analysis of the co-publications of Ukrainian scientists\n  with the Nobel laureates 1994-2018 in Science", "comments": null, "journal-ref": "Sci. innov. 2020. V. 16, no. 5", "doi": "10.15407/scin16.05.110", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nobel Prize is awarded annually for outstanding scientific discoveries\nand inventions. Most scientific papers today are co-authored by a large number\nof researchers. However, very few scientists can receive the Nobel Prize\naccording to the Statutes of the Nobel Foundation. An analysis of the\nco-authorship of the Nobel laureates will make it possible to identify\nemployees of Ukrainian institutions who have collaborated with leading\nscientists of the world, whose scientific works were noted by Nobel. For the\ndevelopment of science in Ukraine it is important to study the successful\nexperience of cooperation of domestic research institutions with leading world\nscientists and research centers, because international scientific collaboration\nfacilitates the process of acquiring new knowledge, promotes mutual enrichment\nof ideas, efficient use of resources and expands opportunities for further use\nof research results. Using the Scopus database, selected publications of Nobel\nPrize winners, which were written in collaboration with scientists who worked\nin Ukrainian institutions. The data obtained indicate that the employees of\nscientific institutions of Ukraine published very few papers in collaborations\nwith Nobel Prize winners of 1994-2018 in comparison with employees of\ninstitutions in leading countries in publishing activity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 09:02:49 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nazarovets", "Serhii", ""]]}, {"id": "2010.04388", "submitter": "Jennifer D'Souza", "authors": "Jennifer D'Souza, S\\\"oren Auer", "title": "Sentence, Phrase, and Triple Annotations to Build a Knowledge Graph of\n  Natural Language Processing Contributions -- A Trial Dataset", "comments": "22 pages, 9 figures, 4 tables", "journal-ref": "Journal of Data and Information Science, 6(3) (2021)", "doi": "10.2478/jdis-2021-0023", "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Purpose: The aim of this work is to normalize the NLPCONTRIBUTIONS scheme\n(henceforward, NLPCONTRIBUTIONGRAPH) to structure, directly from article\nsentences, the contributions information in Natural Language Processing (NLP)\nscholarly articles via a two-stage annotation methodology: 1) pilot stage - to\ndefine the scheme (described in prior work); and 2) adjudication stage - to\nnormalize the graphing model (the focus of this paper).\n  Design/methodology/approach: We re-annotate, a second time, the\ncontributions-pertinent information across 50 prior-annotated NLP scholarly\narticles in terms of a data pipeline comprising: contribution-centered\nsentences, phrases, and triple statements. To this end, specifically, care was\ntaken in the adjudication annotation stage to reduce annotation noise while\nformulating the guidelines for our proposed novel NLP contributions structuring\nand graphing scheme.\n  Findings: The application of NLPCONTRIBUTIONGRAPH on the 50 articles resulted\nfinally in a dataset of 900 contribution-focused sentences, 4,702\ncontribution-information-centered phrases, and 2,980 surface-structured\ntriples. The intra-annotation agreement between the first and second stages, in\nterms of F1, was 67.92% for sentences, 41.82% for phrases, and 22.31% for\ntriple statements indicating that with increased granularity of the\ninformation, the annotation decision variance is greater.\n  Practical Implications: We demonstrate NLPCONTRIBUTIONGRAPH data integrated\ninto the Open Research Knowledge Graph (ORKG), a next-generation KG-based\ndigital library with intelligent computations enabled over structured scholarly\nknowledge, as a viable aid to assist researchers in their day-to-day tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 06:45:35 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 14:24:45 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 06:08:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["D'Souza", "Jennifer", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2010.04836", "submitter": "Jiahao Chen", "authors": "Jiahao Chen and Manuela Veloso", "title": "Paying down metadata debt: learning the representation of concepts using\n  topic models", "comments": "8 pages, 4 figures. Data set available in paper source", "journal-ref": "Proceedings of the 1st ACM International Conference on AI in\n  Finance (ICAIF '20), October 15-16, 2020, New York, NY, USA", "doi": "10.1145/3383455.3422537", "report-no": null, "categories": "cs.LG cs.AI cs.DL cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data management problem called metadata debt, to identify the\nmapping between data concepts and their logical representations. We describe\nhow this mapping can be learned using semisupervised topic models based on\nlow-rank matrix factorizations that account for missing and noisy labels,\ncoupled with sparsity penalties to improve localization and interpretability.\nWe introduce a gauge transformation approach that allows us to construct\nexplicit associations between topics and concept labels, and thus assign\nmeaning to topics. We also show how to use this topic model for semisupervised\nlearning tasks like extrapolating from known labels, evaluating possible errors\nin existing labels, and predicting missing features. We show results from this\ntopic model in predicting subject tags on over 25,000 datasets from Kaggle.com,\ndemonstrating the ability to learn semantically meaningful features.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 22:42:38 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Jiahao", ""], ["Veloso", "Manuela", ""]]}, {"id": "2010.05365", "submitter": "Dmytro Mishkin", "authors": "Dmytro Mishkin and Amy Tabb and Jiri Matas", "title": "ArXiving Before Submission Helps Everyone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We claim, and present evidence, that allowing arXiv publication before a\nconference or journal submission benefits researchers, especially early career,\nas well as the whole scientific community. Specifically, arXiving helps\nprofessional identity building, protects against independent re-discovery, idea\ntheft and gate-keeping; it facilitates open research result distribution and\nreduces inequality. The advantages dwarf the drawbacks -- mainly the relative\nincrease in acceptance rate of papers of well-known authors -- which studies\nshow to be marginal. Analyzing the pros and cons of arXiving papers, we\nconclude that requiring preprints be anonymous is nearly as detrimental as not\nallowing them. We see no reasons why anyone but the authors should decide\nwhether to arXiv or not.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 22:26:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Mishkin", "Dmytro", ""], ["Tabb", "Amy", ""], ["Matas", "Jiri", ""]]}, {"id": "2010.05640", "submitter": "Panagiotis Podiotis", "authors": "Panagiotis Podiotis", "title": "Towards International Relations Data Science: Mining the CIA World\n  Factbook", "comments": "A Computational International Relations Project. For code and\n  datasets see Github link in 2nd page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a three-component work. The first component sets the\noverall theoretical context which lies in the argument that the increasing\ncomplexity of the world has made it more difficult for International Relations\n(IR) to succeed both in theory and practice. The era of information and the\nevents of the 21st century have moved IR theory and practice away from real\npolicy making (Walt, 2016) and have made it entrenched in opinions and\npolitical theories difficult to prove. At the same time, the rise of the\n\"Fourth Paradigm - Data Intensive Scientific Discovery\" (Hey et al., 2009) and\nthe strengthening of data science offer an alternative: \"Computational\nInternational Relations\" (Unver, 2018). The use of traditional and contemporary\ndata-centered tools can help to update the field of IR by making it more\nrelevant to reality (Koutsoupias, Mikelis, 2020). The \"wedding\" between Data\nScience and IR is no panacea though. Changes are required both in perceptions\nand practices. Above all, for Data Science to enter IR, the relevant data must\nexist. This is where the second component comes into play. I mine the CIA World\nFactbook which provides cross-domain data covering all countries of the world.\nThen, I execute various data preprocessing tasks peaking in simple machine\nlearning which imputes missing values providing with a more complete dataset.\nLastly, the third component presents various projects making use of the\nproduced dataset in order to illustrate the relevance of Data Science to IR\nthrough practical examples. Then, ideas regarding the future development of\nthis project are discussed in order to optimize it and ensure continuity.\nOverall, I hope to contribute to the \"fourth paradigm\" discussion in IR by\nproviding practical examples while providing at the same time the fuel for\nfuture research.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:28:01 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Podiotis", "Panagiotis", ""]]}, {"id": "2010.06657", "submitter": "Hancheng Cao", "authors": "Hancheng Cao, Mengjie Cheng, Zhepeng Cen, Daniel A. McFarland, Xiang\n  Ren", "title": "Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer\n  of Scientific Concepts across Text Corpora", "comments": "EMNLP 2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What kind of basic research ideas are more likely to get applied in practice?\nThere is a long line of research investigating patterns of knowledge transfer,\nbut it generally focuses on documents as the unit of analysis and follow their\ntransfer into practice for a specific scientific domain. Here we study\ntranslational research at the level of scientific concepts for all scientific\nfields. We do this through text mining and predictive modeling using three\ncorpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28\nmillion clinical trials. We extract scientific concepts (i.e., phrases) from\ncorpora as instantiations of \"research ideas\", create concept-level features as\nmotivated by literature, and then follow the trajectories of over 450,000 new\nconcepts (emerged from 1995-2014) to identify factors that lead only a small\nproportion of these ideas to be used in inventions and drug trials. Results\nfrom our analysis suggest several mechanisms that distinguish which scientific\nconcept will be adopted in practice, and which will not. We also demonstrate\nthat our derived features can be used to explain and predict knowledge transfer\nwith high accuracy. Our work provides greater understanding of knowledge\ntransfer for researchers, practitioners, and government agencies interested in\nencouraging translational research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:46:59 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Cao", "Hancheng", ""], ["Cheng", "Mengjie", ""], ["Cen", "Zhepeng", ""], ["McFarland", "Daniel A.", ""], ["Ren", "Xiang", ""]]}, {"id": "2010.06969", "submitter": "Soumya Sarkar", "authors": "Bhanu Prakash Reddy, Sasi Bhusan, Soumya Sarkar, Animesh Mukherjee", "title": "NwQM: A neural quality assessment framework for Wikipedia", "comments": "EMNLP 2020: Long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of people irrespective of socioeconomic and demographic backgrounds,\ndepend on Wikipedia articles everyday for keeping themselves informed regarding\npopular as well as obscure topics. Articles have been categorized by editors\ninto several quality classes, which indicate their reliability as encyclopedic\ncontent. This manual designation is an onerous task because it necessitates\nprofound knowledge about encyclopedic language, as well navigating circuitous\nset of wiki guidelines. In this paper we propose Neural wikipedia\nQualityMonitor (NwQM), a novel deep learning model which accumulates signals\nfrom several key information sources such as article text, meta data and images\nto obtain improved Wikipedia article representation. We present comparison of\nour approach against a plethora of available solutions and show 8% improvement\nover state-of-the-art approaches with detailed ablation studies.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 11:26:25 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Reddy", "Bhanu Prakash", ""], ["Bhusan", "Sasi", ""], ["Sarkar", "Soumya", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "2010.08157", "submitter": "Yanbo Zhou", "authors": "Yanbo Zhou and Qu Li and Xuhua Yang and Hongbing cheng", "title": "Predicting the popularity of scientific publications by an age-based\n  diffusion model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the popularity of scientific publications has attracted many\nattentions from various disciplines. In this paper, we focus on the popularity\nprediction problem of scientific papers, and propose an age-based diffusion\n(AD) model to identify which paper will receive more citations in the near\nfuture and will be popular. The AD model is a mimic of the attention diffusion\nprocess along the citation networks. The experimental study shows that the AD\nmodel can achieve better prediction accuracy than other networkbased methods.\nFor some newly published papers that have not accumulated many citations but\nwill be popular in the near future, the AD model can substantially improve\ntheir rankings. This is really critical, because identifying the future highly\ncited papers from massive numbers of new papers published each month would\nprovide very valuable references for researchers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 04:25:39 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhou", "Yanbo", ""], ["Li", "Qu", ""], ["Yang", "Xuhua", ""], ["cheng", "Hongbing", ""]]}, {"id": "2010.08381", "submitter": "Harang Ju", "authors": "Harang Ju, Dale Zhou, Ann S. Blevins, David M. Lydon-Staley, Judith\n  Kaplan, Julio R. Tuma, Danielle S. Bassett", "title": "The network structure of scientific revolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.hist-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Philosophers of science have long postulated how collective scientific\nknowledge grows. Empirical validation has been challenging due to limitations\nin collecting and systematizing large historical records. Here, we capitalize\non the largest online encyclopedia to formulate knowledge as growing networks\nof articles and their hyperlinked inter-relations. We demonstrate that concept\nnetworks grow not by expanding from their core but rather by creating and\nfilling knowledge gaps, a process which produces discoveries that are more\nfrequently awarded Nobel prizes than others. Moreover, we operationalize\nparadigms as network modules to reveal a temporal signature in structural\nstability across scientific subjects. In a network formulation of scientific\ndiscovery, data-driven conditions underlying breakthroughs depend just as much\non identifying uncharted gaps as on advancing solutions within scientific\ncommunities.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:32:37 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 20:46:59 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Ju", "Harang", ""], ["Zhou", "Dale", ""], ["Blevins", "Ann S.", ""], ["Lydon-Staley", "David M.", ""], ["Kaplan", "Judith", ""], ["Tuma", "Julio R.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "2010.08912", "submitter": "Kristina Lerman", "authors": "Clara O Ross, Aditya Gupta, Ninareh Mehrabi, Goran Muric, Kristina\n  Lerman", "title": "The Leaky Pipeline in Physics Publishing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Women make up a shrinking portion of physics faculty in senior positions, a\nphenomenon known as a \"leaky pipeline.\" While fixing this problem has been a\npriority in academic institutions, efforts have been stymied by the diverse\nsources of leaks. In this paper we identify a bias potentially contributing to\nthe leaky pipeline. We analyze bibliographic data provided by the American\nPhysical Society (APS), a leading publisher of physics research. By inferring\nthe gender of authors from names, we are able to measure the fraction of women\nauthors over past decades. We show that the more selective, higher impact APS\njournals have lower fractions of women authors compared to other APS journals.\nCorrecting this bias may help more women publish in prestigious APS journals,\nand in turn help improve their academic promotion cases.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 03:31:55 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ross", "Clara O", ""], ["Gupta", "Aditya", ""], ["Mehrabi", "Ninareh", ""], ["Muric", "Goran", ""], ["Lerman", "Kristina", ""]]}, {"id": "2010.09157", "submitter": "Ryoma Sato", "authors": "Ryoma Sato, Makoto Yamada, Hisashi Kashima", "title": "Poincare: Recommending Publication Venues via Treatment Effect\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing a publication venue for an academic paper is a crucial step in the\nresearch process. However, in many cases, decisions are based on the experience\nof researchers, which often leads to suboptimal results. Although some existing\nmethods recommend publication venues, they just recommend venues where a paper\nis likely to be published. In this study, we aim to recommend publication\nvenues from a different perspective. We estimate the number of citations a\npaper will receive if the paper is published in each venue and recommend the\nvenue where the paper has the most potential impact. However, there are two\nchallenges to this task. First, a paper is published in only one venue, and\nthus, we cannot observe the number of citations the paper would receive if the\npaper were published in another venue. Secondly, the contents of a paper and\nthe publication venue are not statistically independent; that is, there exist\nselection biases in choosing publication venues. In this paper, we propose to\nuse a causal inference method to estimate the treatment effects of choosing a\npublication venue effectively and to recommend venues based on the potential\ninfluence of papers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 00:50:48 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sato", "Ryoma", ""], ["Yamada", "Makoto", ""], ["Kashima", "Hisashi", ""]]}, {"id": "2010.11101", "submitter": "Kai Li", "authors": "Kai Li", "title": "The (re-)instrumentalization of the Diagnostic and Statistical Manual of\n  Mental Disorders (DSM) in psychological publications: a citation context\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research instruments play significant roles in the construction of scientific\nknowledge, even though we have only acquired very limited knowledge about their\nlifecycles from quantitative studies. This paper aims to address this gap by\nquantitatively examining the citation contexts of an exemplary research\ninstrument, the Diagnostic and Statistical Manual of Mental Disorders (DSM), in\nfull-text psychological publications. We investigated the relationship between\nthe citation contexts of the DSM and its status as a valid instrument being\nused and described by psychological researchers. We specifically focused on how\nthis relationship has changed over the DSM's citation histories, especially\nthrough the temporal framework of its versions. We found that a new version of\nthe DSM is increasingly regarded as a valid instrument after its publication;\nthis is reflected in various key citation contexts, such as the use of hedges,\nattention markers, and the verb profile in sentences where the DSM is cited. We\ncall this process the re-instrumentalization of the DSM in the space of\nscientific publications. Our findings bridge an important gap between\nquantitative and qualitative science studies and shed light on an aspect of the\nsocial process of scientific instrument development that is not addressed by\nthe current qualitative literature.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:11:05 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Li", "Kai", ""]]}, {"id": "2010.12294", "submitter": "Bastian Sch\\\"afermeier", "authors": "Bastian Sch\\\"afermeier and Gerd Stumme and Tom Hanika", "title": "Topic Space Trajectories: A case study on machine learning literature", "comments": "41 pages, 8 figures", "journal-ref": "Scientometrics (2021)", "doi": "10.1007/s11192-021-03931-0", "report-no": null, "categories": "cs.LG cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The annual number of publications at scientific venues, for example,\nconferences and journals, is growing quickly. Hence, even for researchers it\nbecomes harder and harder to keep track of research topics and their progress.\nIn this task, researchers can be supported by automated publication analysis.\nYet, many such methods result in uninterpretable, purely numerical\nrepresentations. As an attempt to support human analysts, we present topic\nspace trajectories, a structure that allows for the comprehensible tracking of\nresearch topics. We demonstrate how these trajectories can be interpreted based\non eight different analysis approaches. To obtain comprehensible results, we\nemploy non-negative matrix factorization as well as suitable visualization\ntechniques. We show the applicability of our approach on a publication corpus\nspanning 50 years of machine learning research from 32 publication venues. Our\nnovel analysis method may be employed for paper classification, for the\nprediction of future research topics, and for the recommendation of fitting\nconferences and journals for submitting unpublished work.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 10:53:42 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 14:11:19 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 12:09:47 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Sch\u00e4fermeier", "Bastian", ""], ["Stumme", "Gerd", ""], ["Hanika", "Tom", ""]]}, {"id": "2010.14588", "submitter": "Robert Leaman", "authors": "Robert Leaman and Zhiyong Lu", "title": "A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and\n  SARS-CoV-2", "comments": "Accepted EMNLP NLP-COVID Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of unique terms in the scientific literature used to refer to\neither SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase\nrapidly despite well-established standardized terms. This high degree of term\nvariation makes high recall identification of these important entities\ndifficult. In this manuscript we present an extensive dictionary of terms used\nin the literature to refer to SARS-CoV-2 and COVID-19. We use a rule-based\napproach to iteratively generate new term variants, then locate these variants\nin a large text corpus. We compare our dictionary to an extensive collection of\nterminological resources, demonstrating that our resource provides a\nsubstantial number of additional terms. We use our dictionary to analyze the\nusage of SARS-CoV-2 and COVID-19 terms over time and show that the number of\nunique terms continues to grow rapidly. Our dictionary is freely available at\nhttps://github.com/ncbi-nlp/CovidTermVar.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:51:53 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Leaman", "Robert", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2010.14640", "submitter": "Peter Organisciak", "authors": "Peter Organisciak, Maggie Ryan", "title": "Improving Text Relationship Modeling with Artificial Data", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation uses artificially-created examples to support supervised\nmachine learning, adding robustness to the resulting models and helping to\naccount for limited availability of labelled data. We apply and evaluate a\nsynthetic data approach to relationship classification in digital libraries,\ngenerating artificial books with relationships that are common in digital\nlibraries but not easier inferred from existing metadata. We find that for\nclassification on whole-part relationships between books, synthetic data\nimproves a deep neural network classifier by 91%. Further, we consider the\nability of synthetic data to learn a useful new text relationship class from\nfully artificial training data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 22:04:54 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Organisciak", "Peter", ""], ["Ryan", "Maggie", ""]]}]