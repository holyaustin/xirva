[{"id": "1709.01142", "submitter": "Sebastian Neef", "authors": "Sebastian Neef", "title": "Implementation and Evaluation of a Framework to calculate Impact\n  Measures for Wikipedia Authors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia, an open collaborative website, can be edited by anyone, even\nanonymously, thus becoming victim to ill-intentioned changes. Therefore,\nranking Wikipedia authors by calculating impact measures based on the edit\nhistory can help to identify reputational users or harmful activity such as\nvandalism \\cite{Adler:2008:MAC:1822258.1822279}. However, processing millions\nof edits on one system can take a long time. The author implements an open\nsource framework to calculate such rankings in a distributed way (MapReduce)\nand evaluates its performance on various sized datasets. A reimplementation of\nthe contribution measures by \\citeauthor{Adler:2008:MAC:1822258.1822279}\ndemonstrates its extensibility and usability, as well as problems of handling\nhuge datasets and their possible resolutions. The results put different\nperformance optimizations into perspective and show that horizontal scaling can\ndecrease the total processing time.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 21:33:02 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Neef", "Sebastian", ""]]}, {"id": "1709.01154", "submitter": "Remi Rampin", "authors": "Fernando Chirigati, Rebecca Capone, Dennis Shasha, Remi Rampin,\n  Juliana Freire", "title": "A Collaborative Approach to Computational Reproducibility", "comments": null, "journal-ref": "The Journal of Information Systems, Volume 59, Pages 95-97, ISSN\n  0306-4379 (2016)", "doi": "10.1016/j.is.2016.03.002", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a standard in natural science, reproducibility has been only\nepisodically applied in experimental computer science. Scientific papers often\npresent a large number of tables, plots and pictures that summarize the\nobtained results, but then loosely describe the steps taken to derive them. Not\nonly can the methods and the implementation be complex, but also their\nconfiguration may require setting many parameters and/or depend on particular\nsystem configurations. While many researchers recognize the importance of\nreproducibility, the challenge of making it happen often outweigh the benefits.\nFortunately, a plethora of reproducibility solutions have been recently\ndesigned and implemented by the community. In particular, packaging tools\n(e.g., ReproZip) and virtualization tools (e.g., Docker) are promising\nsolutions towards facilitating reproducibility for both authors and reviewers.\nTo address the incentive problem, we have implemented a new publication model\nfor the Reproducibility Section of Information Systems Journal. In this\nsection, authors submit a reproducibility paper that explains in detail the\ncomputational assets from a previous published manuscript in Information\nSystems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 20:33:45 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Chirigati", "Fernando", ""], ["Capone", "Rebecca", ""], ["Shasha", "Dennis", ""], ["Rampin", "Remi", ""], ["Freire", "Juliana", ""]]}, {"id": "1709.01609", "submitter": "Yuriy Brun", "authors": "Claire Le Goues, Yuriy Brun, Sven Apel, Emery Berger, Sarfraz\n  Khurshid, Yannis Smaragdakis", "title": "Effectiveness of Anonymization in Double-Blind Review", "comments": "Effectiveness of Anonymization in Double-Blind Review. Communications\n  of the ACM. 2017", "journal-ref": "Communications of the ACM, vol. 61, no. 6, June 2018, pp. 34-37", "doi": "10.1145/3208157", "report-no": null, "categories": "cs.DL cs.GL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double-blind review relies on the authors' ability and willingness to\neffectively anonymize their submissions. We explore anonymization effectiveness\nat ASE 2016, OOPSLA 2016, and PLDI 2016 by asking reviewers if they can guess\nauthor identities. We find that 74%-90% of reviews contain no correct guess and\nthat reviewers who self-identify as experts on a paper's topic are more likely\nto attempt to guess, but no more likely to guess correctly. We present our\nfindings, summarize the PC chairs' comments about administering double-blind\nreview, discuss the advantages and disadvantages of revealing author identities\npart of the way through the process, and conclude by advocating for the\ncontinued use of double-blind review.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 22:08:22 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Goues", "Claire Le", ""], ["Brun", "Yuriy", ""], ["Apel", "Sven", ""], ["Berger", "Emery", ""], ["Khurshid", "Sarfraz", ""], ["Smaragdakis", "Yannis", ""]]}, {"id": "1709.01775", "submitter": "Ekta Vats", "authors": "Ekta Vats and Anders Hast", "title": "On-the-fly Historical Handwritten Text Annotation", "comments": null, "journal-ref": "14th IAPR International Conference on Document Analysis and\n  Recognition (ICDAR), Volume 8, IEEE, Kyoto, Japan, 2017, pp. 10-14", "doi": "10.1109/ICDAR.2017.374", "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of information retrieval algorithms depends upon the\navailability of ground truth labels annotated by experts. This is an important\nprerequisite, and difficulties arise when the annotated ground truth labels are\nincorrect or incomplete due to high levels of degradation. To address this\nproblem, this paper presents a simple method to perform on-the-fly annotation\nof degraded historical handwritten text in ancient manuscripts. The proposed\nmethod aims at quick generation of ground truth and correction of inaccurate\nannotations such that the bounding box perfectly encapsulates the word, and\ncontains no added noise from the background or surroundings. This method will\npotentially be of help to historians and researchers in generating and\ncorrecting word labels in a document dynamically. The effectiveness of the\nannotation method is empirically evaluated on an archival manuscript collection\nfrom well-known publicly available datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:27:41 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Vats", "Ekta", ""], ["Hast", "Anders", ""]]}, {"id": "1709.02261", "submitter": "Chris Hartgerink", "authors": "Chris Hartgerink and Peter Murray-Rust", "title": "Extracting data from vector figures in scholarly articles", "comments": "4 figures, 1 table", "journal-ref": null, "doi": "10.5281/zenodo.839536", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is common for authors to communicate their results in graphical figures,\nbut those data are frequently unavailable for reanalysis. Reconstructing data\npoints from a figure manually requires the author to measure the coordinates\neither on printed pages using a ruler, or from the display screen using a\ncursor. This is time-consuming (often hours) and error-prone, and limited by\nthe precision of the display or ruler. What is often not realised is that the\ndata themselves are held in the PDF document to much higher precision (usually\n0.0-0.01 pixels), if the figure is stored in vector format. We developed alpha\nsoftware to automatically reconstruct data from vector figures and tested it on\nfunnel plots in the meta-analysis literature. Our results indicate that\nreconstructing data from vector based figures is promising, where we correctly\nextracted data for 12 out of 24 funnel plots with extracted data (50%).\nHowever, we observed that vector based figures are relatively sparse (15 out of\n136 papers with funnel plots) and strongly insist publishers to provide more\nvector based data figures in the near future for the benefit of the scholarly\ncommunity.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 14:18:36 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 10:40:16 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 14:01:06 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Hartgerink", "Chris", ""], ["Murray-Rust", "Peter", ""]]}, {"id": "1709.02445", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Dashun Wang, James A. Evans", "title": "Large Teams Have Developed Science and Technology; Small Teams Have\n  Disrupted It", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teams dominate the production of high-impact science and technology.\nAnalyzing teamwork from more than 50 million papers, patents, and software\nproducts, 1954-2014, we demonstrate across this period that larger teams\ndeveloped recent, popular ideas, while small teams disrupted the system by\ndrawing on older and less prevalent ideas. Attention to work from large teams\ncame immediately, while advances by small teams succeeded further into the\nfuture. Differences between small and large teams magnify with impact - small\nteams have become known for disruptive work and large teams for developing\nwork. Differences in topic and re- search design account for part of the\nrelationship between team size and disruption, but most of the effect occurs\nwithin people, controlling for detailed subject and article type. Our findings\nsuggest the importance of supporting both small and large teams for the\nsustainable vitality of science and technology.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 20:43:03 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 19:59:04 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Wu", "Lingfei", ""], ["Wang", "Dashun", ""], ["Evans", "James A.", ""]]}, {"id": "1709.02897", "submitter": "Samin Aref", "authors": "Samin Aref, David Friggens and Shaun Hendy", "title": "Analysing Scientific Collaborations of New Zealand Institutions using\n  Scopus Bibliometric Data", "comments": "10 pages, 15 figures, accepted author copy with link to research\n  data, Analysing Scientific Collaborations of New Zealand Institutions using\n  Scopus Bibliometric Data. In Proceedings of ACSW 2018: Australasian Computer\n  Science Week 2018, January 29-February 2, 2018, Brisbane, QLD, Australia", "journal-ref": null, "doi": "10.1145/3167918.3167920", "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific collaborations are among the main enablers of development in small\nnational science systems. Although analysing scientific collaborations is a\nwell-established subject in scientometrics, evaluations of scientific\ncollaborations within a country remain speculative with studies based on a\nlimited number of fields or using data too inadequate to be representative of\ncollaborations at a national level. This study represents a unique view on the\ncollaborative aspect of scientific activities in New Zealand. We perform a\nquantitative study based on all Scopus publications in all subjects for more\nthan 1500 New Zealand institutions over a period of 6 years to generate an\nextensive mapping of scientific collaboration at a national level. The\ncomparative results reveal the level of collaboration between New Zealand\ninstitutions and business enterprises, government institutions, higher\neducation providers, and private not for profit organisations in 2010-2015.\nConstructing a collaboration network of institutions, we observe a power-law\ndistribution indicating that a small number of New Zealand institutions account\nfor a large proportion of national collaborations. Network centrality concepts\nare deployed to identify the most central institutions of the country in terms\nof collaboration. We also provide comparative results on 15 universities and\nCrown research institutes based on 27 subject classifications.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 03:07:03 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 09:44:49 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 22:09:49 GMT"}, {"version": "v4", "created": "Thu, 18 Jan 2018 08:16:19 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Aref", "Samin", ""], ["Friggens", "David", ""], ["Hendy", "Shaun", ""]]}, {"id": "1709.02955", "submitter": "Robin Haunschild", "authors": "Robin Haunschild, Hermann Schier, Werner Marx, and Lutz Bornmann", "title": "Algorithmically generated subject categories based on citation\n  relations: An empirical micro study using papers on overall water splitting", "comments": "28 pages, 6 figures, and 3 tables", "journal-ref": "Journal of Informetrics, 12, 436-447 (2018)", "doi": "10.1016/j.joi.2018.03.004", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important reason for the use of field categorization in bibliometrics is\nthe necessity to make citation impact of papers published in different\nscientific fields comparable with each other. Raw citations are normalized by\nusing field-normalization schemes to achieve comparable citation scores. There\nare different approaches to field categorization available. They can be broadly\nclassified as intellectual and algorithmic approaches. A paper-based\nalgorithmically constructed classification system (ACCS) was proposed which is\nbased on citation relations. Using a few ACCS field-specific clusters, we\ninvestigate the discriminatory power of the ACCS. The micro study focusses on\nthe topic \"overall water splitting\" and related topics. The first part of the\nstudy investigates intellectually whether the ACCS is able to identify papers\non overall water splitting reliably and validly. Next, we compare the ACCS with\n(1) a paper-based intellectual (INSPEC) classification and (2) a journal-based\nintellectual classification (Web of Science, WoS, subject categories). In the\nlast part of our case study, we compare the average number of citations in\nselected ACCS clusters (on overall water splitting and related topics) with the\naverage citation count of publications in WoS subject categories related to\nthese clusters. The results of this micro study question the discriminatory\npower of the ACCS. We recommend larger follow-up studies on broad datasets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 13:45:21 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 09:52:33 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 09:14:52 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Haunschild", "Robin", ""], ["Schier", "Hermann", ""], ["Marx", "Werner", ""], ["Bornmann", "Lutz", ""]]}, {"id": "1709.02968", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu and Hongsu Wang", "title": "Matrix and Graph Operations for Relationship Inference: An Illustration\n  with the Kinship Inference in the China Biographical Database", "comments": "3 pages, 3 figures, 2017 Annual Meeting of the Japanese Association\n  for Digital Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biographical databases contain diverse information about individuals. Person\nnames, birth information, career, friends, family and special achievements are\nsome possible items in the record for an individual. The relationships between\nindividuals, such as kinship and friendship, provide invaluable insights about\nhidden communities which are not directly recorded in databases. We show that\nsome simple matrix and graph-based operations are effective for inferring\nrelationships among individuals, and illustrate the main ideas with the China\nBiographical Database (CBDB).\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 16:01:08 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Wang", "Hongsu", ""]]}, {"id": "1709.03319", "submitter": "Tao Jia", "authors": "Tao Jia, Dashun Wang, Boleslaw K. Szymanski", "title": "Quantifying patterns of research interest evolution", "comments": null, "journal-ref": "Nature Human Behaviour, 1, 0078, (2017)", "doi": "10.1038/s41562-017-0078", "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our quantitative understanding of how scientists choose and shift their\nresearch focus over time is highly consequential, because it affects the ways\nin which scientists are trained, science is funded, knowledge is organized and\ndiscovered, and excellence is recognized and rewarded. Despite extensive\ninvestigations of various factors that influence a scientist's choice of\nresearch topics, quantitative assessments of mechanisms that give rise to\nmacroscopic patterns characterizing research interest evolution of individual\nscientists remain limited. Here we perform a large-scale analysis of\npublication records, finding that research interest change follows a\nreproducible pattern characterized by an exponential distribution. We identify\nthree fundamental features responsible for the observed exponential\ndistribution, which arise from a subtle interplay between exploitation and\nexploration in research interest evolution. We develop a random walk based\nmodel, allowing us to accurately reproduce the empirical observations. This\nwork presents a quantitative analysis of macroscopic patterns governing\nresearch interest change, discovering a high degree of regularity underlying\nscientific research and individual careers.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 10:13:04 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Jia", "Tao", ""], ["Wang", "Dashun", ""], ["Szymanski", "Boleslaw K.", ""]]}, {"id": "1709.03453", "submitter": "Kevin Boyack", "authors": "Richard Klavans and Kevin W. Boyack", "title": "Research Portfolio Analysis and Topic Prominence", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stakeholders in the science system need to decide where to place their bets.\nExample questions include: Which areas of research should get more funding? Who\nshould we hire? Which projects should we abandon and which new projects should\nwe start? Making informed choices requires knowledge about these research\noptions. Unfortunately, to date research portfolio options have not been\ndefined in a consistent, transparent and relevant manner. Furthermore, we don't\nknow how to define demand for these options. In this article, we address the\nissues of consistency, transparency, relevance and demand by using a model of\nscience consisting of 91,726 topics (or research options) that contain over 58\nmillion documents. We present a new indicator of topic prominence - a measure\nof visibility, momentum and, ultimately, demand. We assign over $203 billion of\nproject-level funding data from STAR METRICS to individual topics in science,\nand show that the indicator of topic prominence, explains over one-third of the\nvariance in current (or future) funding by topic. We also show that highly\nprominent topics receive far more funding per researcher than topics that are\nnot prominent. Implications of these results for research planning and\nportfolio analysis by institutions and researchers are emphasized.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 15:53:21 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 13:45:57 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Klavans", "Richard", ""], ["Boyack", "Kevin W.", ""]]}, {"id": "1709.04481", "submitter": "Ryan Rossi", "authors": "James P. Canning, Emma E. Ingram, Sammantha Nowak-Wolff, Adriana M.\n  Ortiz, Nesreen K. Ahmed, Ryan A. Rossi, Karl R. B. Schmitt, Sucheta\n  Soundarajan", "title": "Network Classification and Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To the best of our knowledge, this paper presents the first large-scale study\nthat tests whether network categories (e.g., social networks vs. web graphs)\nare distinguishable from one another (using both categories of real-world\nnetworks and synthetic graphs). A classification accuracy of $94.2\\%$ was\nachieved using a random forest classifier with both real and synthetic\nnetworks. This work makes two important findings. First, real-world networks\nfrom various domains have distinct structural properties that allow us to\npredict with high accuracy the category of an arbitrary network. Second,\nclassifying synthetic networks is trivial as our models can easily distinguish\nbetween synthetic graphs and the real-world networks they are supposed to\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 18:02:09 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Canning", "James P.", ""], ["Ingram", "Emma E.", ""], ["Nowak-Wolff", "Sammantha", ""], ["Ortiz", "Adriana M.", ""], ["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan A.", ""], ["Schmitt", "Karl R. B.", ""], ["Soundarajan", "Sucheta", ""]]}, {"id": "1709.05576", "submitter": "Manolis Peponakis", "authors": "Anna Mastora, Manolis Peponakis, Sarantos Kapidakis", "title": "SKOS Concepts and Natural Language Concepts: an Analysis of Latent\n  Relationships in KOSs", "comments": "18 pages, 5 tables", "journal-ref": "Journal of Information Science, 43(4), 492-508 (2017)", "doi": "10.1177/0165551516648108", "report-no": null, "categories": "cs.DL cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The vehicle to represent Knowledge Organization Systems (KOSs) in the\nenvironment of the Semantic Web and linked data is the Simple Knowledge\nOrganization System (SKOS). SKOS provides a way to assign a URI to each\nconcept, and this URI functions as a surrogate for the concept. This fact makes\nof main concern the need to clarify the URIs' ontological meaning. The aim of\nthis study is to investigate the relation between the ontological substance of\nKOS concepts and concepts revealed through the grammatical and syntactic\nformalisms of natural language. For this purpose, we examined the dividableness\nof concepts in specific KOSs (i.e. a thesaurus, a subject headings system and a\nclassification scheme) by applying Natural Language Processing (NLP) techniques\n(i.e. morphosyntactic analysis) to the lexical representations (i.e. RDF\nliterals) of SKOS concepts. The results of the comparative analysis reveal\nthat, despite the use of multi-word units, thesauri tend to represent concepts\nin a way that can hardly be further divided conceptually, while Subject\nHeadings and Classification Schemes - to a certain extent - comprise terms that\ncan be decomposed into more conceptual constituents. Consequently, SKOS\nconcepts deriving from thesauri are more likely to represent atomic conceptual\nunits and thus be more appropriate tools for inference and reasoning. Since\nidentifiers represent the meaning of a concept, complex concepts are neither\nthe most appropriate nor the most efficient way of modelling a KOS for the\nSemantic Web.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 22:58:13 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Mastora", "Anna", ""], ["Peponakis", "Manolis", ""], ["Kapidakis", "Sarantos", ""]]}, {"id": "1709.05587", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu, Shuhua Zhang, Yuanli Geng, Huei-ling Lai, Hongsu Wang", "title": "Character Distributions of Classical Chinese Literary Texts: Zipf's Law,\n  Genres, and Epochs", "comments": "12 pages, 2 tables, 6 figures, 2017 International Conference on\n  Digital Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We collect 14 representative corpora for major periods in Chinese history in\nthis study. These corpora include poetic works produced in several dynasties,\nnovels of the Ming and Qing dynasties, and essays and news reports written in\nmodern Chinese. The time span of these corpora ranges between 1046 BCE and 2007\nCE. We analyze their character and word distributions from the viewpoint of the\nZipf's law, and look for factors that affect the deviations and similarities\nbetween their Zipfian curves. Genres and epochs demonstrated their influences\nin our analyses. Specifically, the character distributions for poetic works of\nbetween 618 CE and 1644 CE exhibit striking similarity. In addition, although\ntexts of the same dynasty may tend to use the same set of characters, their\ncharacter distributions still deviate from each other.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 01:15:03 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Zhang", "Shuhua", ""], ["Geng", "Yuanli", ""], ["Lai", "Huei-ling", ""], ["Wang", "Hongsu", ""]]}, {"id": "1709.05729", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu", "title": "Flexible Computing Services for Comparisons and Analyses of Classical\n  Chinese Poetry", "comments": "6 pages, 2 tables, 1 figure, 2017 International Conference on Digital\n  Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We collect nine corpora of representative Chinese poetry for the time span of\n1046 BCE and 1644 CE for studying the history of Chinese words, collocations,\nand patterns. By flexibly integrating our own tools, we are able to provide new\nperspectives for approaching our goals. We illustrate the ideas with two\nexamples. The first example show a new way to compare word preferences of\npoets, and the second example demonstrates how we can utilize our corpora in\nhistorical studies of the Chinese words. We show the viability of the tools for\nacademic research, and we wish to make it helpful for enriching existing\nChinese dictionary as well.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 00:01:07 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Liu", "Chao-Lin", ""]]}, {"id": "1709.06479", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Caroline Wagner, Loet Leydesdorff", "title": "The geography of references in elite articles: What countries contribute\n  to the archives of knowledge", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0194805", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is intended to find an answer for the question on which national\n\"shoulders\" the worldwide top-level research stands. Traditionally, national\nscientific standings are evaluated in terms of the number of citations to their\npapers. We raise a different question: instead of analyzing the citations to\nthe countries' articles (the forward view), we examine referenced publications\nfrom specific countries cited in the most elite publications (the\nbackward-citing-view). \"Elite publications\" are operationalized as the top-1%\nmost-highly cited articles. Using the articles published during the years 2004\nto 2013, we examine the research referenced in these works. Our results confirm\nthe well-known fact that China has emerged to become a major player in science.\nHowever, China still belongs to the low contributors when countries are ranked\nas contributors to the cited references in top-1% articles. Using this\nperspective, the results do not point to a decreasing trend for the USA; in\nfact, the USA exceeds expectations (compared to its publication share) in terms\nof contributions to cited references in the top-1% articles. Switzerland,\nSweden, and the Netherlands also are shown at the top of the list. However, the\nresults for Germany are lower than statistically expected.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 15:07:04 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Bornmann", "Lutz", ""], ["Wagner", "Caroline", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1709.06556", "submitter": "Matjaz Perc", "authors": "Kristina Ban, Matjaz Perc, Zoran Levnajic", "title": "Robust clustering of languages across Wikipedia growth", "comments": "9 two-column pages, 7 figures; accepted for publication in Royal\n  Society Open Science", "journal-ref": "R. Soc. Open Sci. 4, 171217 (2017)", "doi": "10.1098/rsos.171217", "report-no": null, "categories": "cs.DL cs.SI physics.data-an physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia is the largest existing knowledge repository that is growing on a\ngenuine crowdsourcing support. While the English Wikipedia is the most\nextensive and the most researched one with over five million articles,\ncomparatively little is known about the behavior and growth of the remaining\n283 smaller Wikipedias, the smallest of which, Afar, has only one article. Here\nwe use a subset of this data, consisting of 14962 different articles, each of\nwhich exists in 26 different languages, from Arabic to Ukrainian. We study the\ngrowth of Wikipedias in these languages over a time span of 15 years. We show\nthat, while an average article follows a random path from one language to\nanother, there exist six well-defined clusters of Wikipedias that share common\ngrowth patterns. The make-up of these clusters is remarkably robust against the\nmethod used for their determination, as we verify via four different clustering\nmethods. Interestingly, the identified Wikipedia clusters have little\ncorrelation with language families and groups. Rather, the growth of Wikipedia\nacross different languages is governed by different factors, ranging from\nsimilarities in culture to information literacy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:35:52 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Ban", "Kristina", ""], ["Perc", "Matjaz", ""], ["Levnajic", "Zoran", ""]]}, {"id": "1709.07020", "submitter": "Matteo Cantiello Dr.", "authors": "Alberto Pepe, Matteo Cantiello, Josh Nicholson", "title": "The arXiv of the future will not look like the arXiv", "comments": "The authors of this document welcome public comments and ideas from\n  its readers, at the online version of this article\n  (https://www.authorea.com/173764)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The arXiv is the most popular preprint repository in the world. Since its\ninception in 1991, the arXiv has allowed researchers to freely share\npublication-ready articles prior to formal peer review. The growth and the\npopularity of the arXiv emerged as a result of new technologies that made\ndocument creation and dissemination easy, and cultural practices where\ncollaboration and data sharing were dominant. The arXiv represents a unique\nplace in the history of research communication and the Web itself, however it\nhas arguably changed very little since its creation. Here we look at the\nstrengths and weaknesses of arXiv in an effort to identify what possible\nimprovements can be made based on new technologies not previously available.\nBased on this, we argue that a modern arXiv might in fact not look at all like\nthe arXiv of today.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 18:12:05 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Pepe", "Alberto", ""], ["Cantiello", "Matteo", ""], ["Nicholson", "Josh", ""]]}, {"id": "1709.07183", "submitter": "Gy\\\"orgy Csom\\'os", "authors": "Gyorgy Csomos", "title": "A spatial scientometric analysis of the publication output of cities\n  worldwide", "comments": null, "journal-ref": "Journal of Informetrics, 12 (2), 547-566 (2018)", "doi": "10.1016/j.joi.2018.05.003", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tandem with the rapid globalisation of science, spatial scientometrics has\nbecome an important research sub-field in scientometric studies. Recently,\nnumerous spatial scientometric contributions have focused on the examination of\ncities' scientific output by using various scientometric indicators. In this\npaper, I analyse cities' scientific output worldwide in terms of the number of\njournal articles indexed by the Scopus database, in the period from 1986 to\n2015. Furthermore, I examine which countries are the most important\ncollaborators of cities. Finally, I identify the most productive disciplines in\neach city. I use GPS Visualizer to illustrate the scientometric data of nearly\n2,200 cities on maps. Results show that cities with the highest scientific\noutput are mostly located in developed countries and China. Between 1986 and\n2015, the greatest number of scientific articles were created in Beijing. The\ninternational hegemony of the United States in science has been described by\nmany studies, and is also reinforced by the fact that the United States is the\nmost important collaborator to more than 75 percent of all cities. Medicine is\nthe most productive discipline in two-thirds of cities. Furthermore, cities\nhaving the highest scientific output in specific disciplines show well-defined\ngeographical patterns.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 07:11:22 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Csomos", "Gyorgy", ""]]}, {"id": "1709.07782", "submitter": "Anne Chardonnens", "authors": "Anne Chardonnens, Ettore Rizza, Mathias Coeckelbergs, Seth van Hooland", "title": "Mining User Queries with Information Extraction Methods and Linked Data", "comments": "Preprint (17 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Advanced usage of Web Analytics tools allows to capture the content\nof user queries. Despite their relevant nature, the manual analysis of large\nvolumes of user queries is problematic. This paper demonstrates the potential\nof using information extraction techniques and Linked Data to gather a better\nunderstanding of the nature of user queries in an automated manner.\n  Design/methodology/approach: The paper presents a large-scale case-study\nconducted at the Royal Library of Belgium consisting of a data set of 83 854\nqueries resulting from 29 812 visits over a 12 month period of the historical\nnewspapers platform BelgicaPress. By making use of information extraction\nmethods, knowledge bases and various authority files, this paper presents the\npossibilities and limits to identify what percentage of end users are looking\nfor person and place names.\n  Findings: Based on a quantitative assessment, our method can successfully\nidentify the majority of person and place names from user queries. Due to the\nspecific character of user queries and the nature of the knowledge bases used,\na limited amount of queries remained too ambiguous to be treated in an\nautomated manner.\n  Originality/value: This paper demonstrates in an empirical manner both the\npossibilities and limits of gaining more insights from user queries extracted\nfrom a Web Analytics tool and analysed with the help of information extraction\ntools and knowledge bases. Methods and tools used are generalisable and can be\nreused by other collection holders.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:35:44 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Chardonnens", "Anne", ""], ["Rizza", "Ettore", ""], ["Coeckelbergs", "Mathias", ""], ["van Hooland", "Seth", ""]]}, {"id": "1709.07810", "submitter": "David Melero-Fuentes", "authors": "Rafael Aleixandre-Benavent, Carolina Navarro-Molina, Remedios\n  Aguilar-Moya, David Melero-Fuentes and Juan-Carlos Valderrama-Zuri\\'an", "title": "Trends in scientific research in Online Information Review. Part 1.\n  Production, impact and research collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The study, based on the Web of Science, analyses 758 articles published from\n2000 to 2014. Our analysis includes the publications' output, authorship,\ninstitutional and country patterns of production, citations and collaboration.\nA Social Network Analysis was conducted to identify primary groups of\nresearchers and institutions and the collaboration between countries. The study\nreveals that 1097 authors and 453 Institutions have contributed to the journal.\nThe collaboration index has increased progressively, and the average degree of\ncollaboration during the study period was 1.98. The majority of the papers were\ncontributed by professionals affiliated with a university. Highly cited papers\naddress online and digital environments, e-learning systems, mobile services,\nweb 2.0 and citation analyses. This work is a bibliometric analysis of a\nleading journal in library and information science, Online Information Review.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 15:33:07 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 07:20:11 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Aleixandre-Benavent", "Rafael", ""], ["Navarro-Molina", "Carolina", ""], ["Aguilar-Moya", "Remedios", ""], ["Melero-Fuentes", "David", ""], ["Valderrama-Zuri\u00e1n", "Juan-Carlos", ""]]}, {"id": "1709.07817", "submitter": "David Melero-Fuentes", "authors": "Juan-Carlos Valderrama-Zurian, Carolina Navarro-Molina, Remedios\n  Aguilar-Moya, David Melero-Fuentes and Rafael Aleixandre-Benavent", "title": "Trends in scientific research in Online Information Review. Part 2.\n  Mapping the scientific knowledge through bibliometric and social network\n  analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective. The purpose of this work is to analyse the knowledge structure and\ntrends in scientific research in the Online Information Reviews journal by\nbibliometric analysis of key words and social network analysis of co-words.\nMethods. Key words included in a set of 758 papers included in the Web of\nScience database from 2000 to 2014 were analysed. We conducted a subject\nanalysis considering the key words assigned to papers. A social network\nanalysis was also conducted to identify the number of co-occurrences between\nkey words (co-words). The Pajek software was used to create and graphically\nvisualize the networks. Results. Internet is the most frequent key word (n=219)\nand the most central in the network of co-words, strongly associated with\nInformation retrieval, search engines, the World Wide Web, libraries and users\nConclusions. Information science, as represented by Online Information Review\nin the present study, is an evolving discipline that draws on literature from a\nrelatively wide range of subjects. Although Online Information Review appears\nto have well-defined and established research topics, the journal also changes\nrapidly to embrace new lines of research.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 15:46:21 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 07:21:43 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Valderrama-Zurian", "Juan-Carlos", ""], ["Navarro-Molina", "Carolina", ""], ["Aguilar-Moya", "Remedios", ""], ["Melero-Fuentes", "David", ""], ["Aleixandre-Benavent", "Rafael", ""]]}, {"id": "1709.09021", "submitter": "Nizal Alshammry", "authors": "Nizal Alshammry and Phillip Lord", "title": "Identitas: A Better Way To Be Meaningless", "comments": "2 pages, accepted at ICBO 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is often recommended that identifiers for ontology terms should be\nsemantics-free or meaningless. In practice, ontology developers tend to use\nnumeric identifiers, starting at 1 and working upwards. In this paper we\npresent a critique of current ontology semantics-free identifiers;\nmonotonically increasing numbers have a number of significant usability flaws\nwhich make them unsuitable as a default option, and we present a series of\nalternatives. We have provide an implementation of these alternatives which can\nbe freely combined.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 13:56:47 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 12:54:47 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Alshammry", "Nizal", ""], ["Lord", "Phillip", ""]]}, {"id": "1709.09119", "submitter": "Paul Christian Sommerhoff", "authors": "Paul Christian Sommerhoff", "title": "Integration of Japanese Papers Into the DBLP Data Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If someone is looking for a certain publication in the field of computer\nscience, the searching person is likely to use the DBLP to find the desired\npublication. The DBLP data set is continuously extended with new publications,\nor rather their metadata, for example the names of involved authors, the title\nand the publication date. While the size of the data set is already remarkable,\nspecific areas can still be improved. The DBLP offers a huge collection of\nEnglish papers because most papers concerning computer science are published in\nEnglish. Nevertheless, there are official publications in other languages which\nare supposed to be added to the data set. One kind of these are Japanese\npapers. This diploma thesis will show a way to automatically process\npublication lists of Japanese papers and to make them ready for an import into\nthe DBLP data set. Especially important are the problems along the way of\nprocessing, such as transcription handling and Personal Name Matching with\nJapanese names.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 16:33:59 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Sommerhoff", "Paul Christian", ""]]}, {"id": "1709.09186", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga", "title": "A Longitudinal Assessment of the Persistence of Twitter Datasets", "comments": "Accepted for publication in JASIST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With social media datasets being increasingly shared by researchers, it also\npresents the caveat that those datasets are not always completely replicable.\nHaving to adhere to requirements of platforms like Twitter, researchers cannot\nrelease the raw data and instead have to release a list of unique identifiers,\nwhich others can then use to recollect the data from the platform themselves.\nThis leads to the problem that subsets of the data may no longer be available,\nas content can be deleted or user accounts deactivated. To quantify the impact\nof content deletion in the replicability of datasets in a long term, we perform\na longitudinal analysis of the persistence of 30 Twitter datasets, which\ninclude over 147 million tweets. Having the original datasets collected between\n2012 and 2016, and recollecting them later by using the tweet IDs, we look at\nfour different factors that quantify the extent to which recollected datasets\nresemble original ones: completeness, representativity, similarity and\nchangingness. Even though the ratio of available tweets keeps decreasing as the\ndataset gets older, we find that the textual content of the recollected subset\nis still largely representative of the whole dataset that was originally\ncollected. The representativity of the metadata, however, keeps decreasing over\ntime, both because the dataset shrinks and because certain metadata, such as\nthe users' number of followers, keeps changing. Our study has important\nimplications for researchers sharing and using publicly shared Twitter datasets\nin their research.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 18:00:33 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 21:58:52 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Zubiaga", "Arkaitz", ""]]}, {"id": "1709.09450", "submitter": "Mohammad Halawani", "authors": "Mohammad K. Halawani, Rob Forsyth and Phillip Lord", "title": "A Literature Based Approach to Define the Scope of Biomedical\n  Ontologies: A Case Study on a Rehabilitation Therapy Ontology", "comments": "Accepted at the International Conference for Biomedical Ontologies\n  2017(ICBO 2017), 4 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we investigate our early attempts at building an ontology\ndescribing rehabilitation therapies following brain injury. These therapies are\nwide-ranging, involving interventions of many different kinds. As a result,\nthese therapies are hard to describe. As well as restricting actual practice,\nthis is also a major impediment to evidence-based medicine as it is hard to\nmeaningfully compare two treatment plans.\n  Ontology development requires significant effort from both ontologists and\ndomain experts. Knowledge elicited from domain experts forms the scope of the\nontology. The process of knowledge elicitation is expensive, consumes experts'\ntime and might have biases depending on the selection of the experts. Various\nmethodologies and techniques exist for enabling this knowledge elicitation,\nincluding community groups and open development practices. A related problem is\nthat of defining scope. By defining the scope, we can decide whether a concept\n(i.e. term) should be represented in the ontology. This is the opposite of\nknowledge elicitation, in the sense that it defines what should not be in the\nontology. This can be addressed by pre-defining a set of competency questions.\n  These approaches are, however, expensive and time-consuming. Here, we\ndescribe our work toward an alternative approach, bootstrapping the ontology\nfrom an initially small corpus of literature that will define the scope of the\nontology, expanding this to a set covering the domain, then using information\nextraction to define an initial terminology to provide the basis and the\ncompetencies for the ontology. Here, we discuss four approaches to building a\nsuitable corpus that is both sufficiently covering and precise.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 11:11:54 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Halawani", "Mohammad K.", ""], ["Forsyth", "Rob", ""], ["Lord", "Phillip", ""]]}, {"id": "1709.09657", "submitter": "Kunho Kim", "authors": "Kunho Kim, Athar Sefid, C. Lee Giles", "title": "Scaling Author Name Disambiguation with CNF Blocking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An author name disambiguation (AND) algorithm identifies a unique author\nentity record from all similar or same publication records in scholarly or\nsimilar databases. Typically, a clustering method is used that requires\ncalculation of similarities between each possible record pair. However, the\ntotal number of pairs grows quadratically with the size of the author database\nmaking such clustering difficult for millions of records. One remedy for this\nis a blocking function that reduces the number of pairwise similarity\ncalculations. Here, we introduce a new way of learning blocking schemes by\nusing a conjunctive normal form (CNF) in contrast to the disjunctive normal\nform (DNF). We demonstrate on PubMed author records that CNF blocking reduces\nmore pairs while preserving high pairs completeness compared to the previous\nmethods that use a DNF with the computation time significantly reduced. Thus,\nthese concepts in scholarly data can be better represented with CNFs. Moreover,\nwe also show how to ensure that the method produces disjoint blocks so that the\nrest of the AND algorithm can be easily paralleled. Our CNF blocking tested on\nthe entire PubMed database of 80 million author mentions efficiently removes\n82.17% of all author record pairs in 10 minutes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 17:48:21 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Kim", "Kunho", ""], ["Sefid", "Athar", ""], ["Giles", "C. Lee", ""]]}]