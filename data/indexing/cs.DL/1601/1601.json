[{"id": "1601.00141", "submitter": "Bakthavachalam Elango", "authors": "Bakthavachalam Elango, Lutz Bornmann and Govindaraju Kannan", "title": "Detecting the historical roots of tribology research: a bibliometric\n  analysis", "comments": "Accepted for publicaion in the Scientometrics", "journal-ref": null, "doi": "10.1007/s11192-016-1877-6", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the historical roots of tribology are investigated using a\nnewly developed scientometric method called Referenced Publication Years\nSpectroscopy. The study is based on cited references in tribology research\npublications. The Science Citation Index Expanded is used as data source. The\nresults show that RPYS has the potential to identify the important publications\n: Most of the publications which have been identified in this study as highly\ncited (referenced) publications are landmark publications in the field of\ntribology.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 05:43:25 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 04:54:18 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Elango", "Bakthavachalam", ""], ["Bornmann", "Lutz", ""], ["Kannan", "Govindaraju", ""]]}, {"id": "1601.00245", "submitter": "Weishu Liu", "authors": "Li Tang, Guangyuan Hu, Weishu Liu", "title": "Funding acknowledgment analysis:Queries and Caveats", "comments": "8 pages 2 tables, accepted by JASIST", "journal-ref": null, "doi": "10.1002/asi.23713", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thomson Reuters' Web of Science (WoS) began systematically collecting\nacknowledgment information in August 2008. Since then, bibliometric analysis of\nfunding acknowledgment (FA) has been growing and has aroused intense interest\nand attention from both academia and policy makers. Examining the distribution\nof FA by citation index database, by language, and by acknowledgment type, we\nnoted coverage limitations and potential biases in each analysis. We argue that\nin spite of its great value, bibliometric analysis of FA should be used with\ncaution.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 05:00:30 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Tang", "Li", ""], ["Hu", "Guangyuan", ""], ["Liu", "Weishu", ""]]}, {"id": "1601.00288", "submitter": "Loet Leydesdorff", "authors": "Jordan A. Comins and Loet Leydesdorff", "title": "Identification of long-term concept-symbols among citations: Can\n  documents be clustered in terms of common intellectual histories?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Citation classics\" are not only highly cited, but also cited during several\ndecades. We test whether the peaks in the spectrograms generated by Reference\nPublication Years Spectroscopy (RPYS) indicate such long-term impact by\ncomparing across RPYS for subsequent time intervals. Multi-RPYS enables us to\ndistinguish between short-term citation peaks at the research front that decay\nwithin ten years versus historically constitutive (long-term) citations that\nfunction as concept symbols (Small, 1978). Using these constitutive citations,\none is able to cluster document sets (e.g., journals) in terms of\nintellectually shared histories. We test this premise by clustering 40 journals\nin the Web of Science Category of Information and Library Science using\nmulti-RPYS. It follows that RPYS can not only be used for retrieving roots of\nsets under study (cited), but also for algorithmic historiography of the citing\nsets. Significant references are historically rooted symbols among other\ncitations that function as currency.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 12:52:27 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Comins", "Jordan A.", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1601.00473", "submitter": "Mike Thelwall", "authors": "Mike Thelwall", "title": "The discretised lognormal and hooked power law distributions for\n  complete citation data: Best options for modelling and regression", "comments": "Thelwall, M. (in press). The discretised lognormal and hooked power\n  law distributions for complete citation data: Best options for modelling and\n  regression. Journal of Informetrics", "journal-ref": "Journal of Informetrics, 10(2), 336-346.\n  doi:10.1016/j.joi.2015.12.007", "doi": "10.1016/j.joi.2015.12.007", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the statistical distribution that best fits citation data is\nimportant to allow robust and powerful quantitative analyses. Whilst previous\nstudies have suggested that both the hooked power law and discretised lognormal\ndistributions fit better than the power law and negative binomial\ndistributions, no comparisons so far have covered all articles within a\ndiscipline, including those that are uncited. Based on an analysis of 26\ndifferent Scopus subject areas in seven different years, this article reports\ncomparisons of the discretised lognormal and the hooked power law with citation\ndata, adding 1 to citation counts in order to include zeros. The hooked power\nlaw fits better in two thirds of the subject/year combinations tested for\njournal articles that are at least three years old, including most medical,\nlife and natural sciences, and for virtually all subject areas for younger\narticles. Conversely, the discretised lognormal tends to fit best for arts,\nhumanities, social science and engineering fields. The difference between the\nfits of the distributions is mostly small, however, and so either could\nreasonably be used for modelling citation data. For regression analyses,\nhowever, the best option is to use ordinary least squares regression applied to\nthe natural logarithm of citation counts plus one, especially for sets of\nyounger articles, because of the increased precision of the parameters.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 12:16:09 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Thelwall", "Mike", ""]]}, {"id": "1601.00626", "submitter": "Tim Weninger PhD", "authors": "Baoxu Shi and Tim Weninger", "title": "Scalable Models for Computing Hierarchies in Information Networks", "comments": "Preprint for \"Knowledge and Information Systems\" paper, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information hierarchies are organizational structures that often used to\norganize and present large and complex information as well as provide a\nmechanism for effective human navigation. Fortunately, many statistical and\ncomputational models exist that automatically generate hierarchies; however,\nthe existing approaches do not consider linkages in information {\\em networks}\nthat are increasingly common in real-world scenarios. Current approaches also\ntend to present topics as an abstract probably distribution over words, etc\nrather than as tangible nodes from the original network. Furthermore, the\nstatistical techniques present in many previous works are not yet capable of\nprocessing data at Web-scale. In this paper we present the Hierarchical\nDocument Topic Model (HDTM), which uses a distributed vertex-programming\nprocess to calculate a nonparametric Bayesian generative model. Experiments on\nthree medium size data sets and the entire Wikipedia dataset show that HDTM can\ninfer accurate hierarchies even over large information networks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 20:05:19 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Shi", "Baoxu", ""], ["Weninger", "Tim", ""]]}, {"id": "1601.01058", "submitter": "Gilad Katz", "authors": "Gilad Katz, Lior Rokach", "title": "Wikiometrics: A Wikipedia Based Ranking System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new concept - Wikiometrics - the derivation of metrics and\nindicators from Wikipedia. Wikipedia provides an accurate representation of the\nreal world due to its size, structure, editing policy and popularity. We\ndemonstrate an innovative mining methodology, where different elements of\nWikipedia - content, structure, editorial actions and reader reviews - are used\nto rank items in a manner which is by no means inferior to rankings produced by\nexperts or other methods. We test our proposed method by applying it to two\nreal-world ranking problems: top world universities and academic journals. Our\nproposed ranking methods were compared to leading and widely accepted\nbenchmarks, and were found to be extremely correlative but with the advantage\nof the data being publically available.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 02:44:42 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 05:25:27 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Katz", "Gilad", ""], ["Rokach", "Lior", ""]]}, {"id": "1601.01199", "submitter": "Lutz Bornmann Dr.", "authors": "Andreas Thor, Werner Marx, Loet Leydesdorff, Lutz Bornmann", "title": "Introducing CitedReferencesExplorer (CRExplorer): A program for\n  Reference Publication Year Spectroscopy with Cited References Standardization", "comments": "Accepted for publication in the Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new tool - the CitedReferencesExplorer (CRExplorer,\nwww.crexplorer.net) - which can be used to disambiguate and analyze the cited\nreferences (CRs) of a publication set downloaded from the Web of Science (WoS).\nThe tool is especially suitable to identify those publications which have been\nfrequently cited by the researchers in a field and thereby to study for example\nthe historical roots of a research field or topic. CRExplorer simplifies the\nidentification of key publications by enabling the user to work with both a\ngraph for identifying most frequently cited reference publication years (RPYs)\nand the list of references for the RPYs which have been most frequently cited.\nA further focus of the program is on the standardization of CRs. It is a\nserious problem in bibliometrics that there are several variants of the same CR\nin the WoS. In this study, CRExplorer is used to study the CRs of all papers\npublished in the Journal of Informetrics. The analyses focus on the most\nimportant papers published between 1980 and 1990.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 14:56:59 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 10:13:31 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Thor", "Andreas", ""], ["Marx", "Werner", ""], ["Leydesdorff", "Loet", ""], ["Bornmann", "Lutz", ""]]}, {"id": "1601.01203", "submitter": "Lovro \\v{S}ubelj", "authors": "Lovro \\v{S}ubelj, Dalibor Fiala", "title": "Publication boost in Web of Science journals and its effect on citation\n  distributions", "comments": "14 pages, 5 figures, J. Assoc. Inf. Sci. Tec. (2016)", "journal-ref": "J. Assoc. Inf. Sci. Tec. 68(4), 1018-1023 (2017)", "doi": "10.1002/asi.23718", "report-no": null, "categories": "cs.DL cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that the dramatic increase in the number of research\narticles indexed in the Web of Science database impacts the commonly observed\ndistributions of citations within these articles. First, we document that the\ngrowing number of physics articles in recent years is due to existing journals\npublishing more and more papers rather than more new journals coming into being\nas it happens in computer science. And second, even though the references from\nthe more recent papers generally cover a longer time span, the newer papers are\ncited more frequently than the older ones if the uneven paper growth is not\ncorrected for. Nevertheless, despite this change in the distribution of\ncitations, the citation behavior of scientists does not seem to have changed.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 15:10:57 GMT"}, {"version": "v2", "created": "Sat, 11 Jun 2016 11:00:38 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["\u0160ubelj", "Lovro", ""], ["Fiala", "Dalibor", ""]]}, {"id": "1601.01582", "submitter": "Manolis Antonoyiannakis", "authors": "Manolis Antonoyiannakis", "title": "Highlighting Impact and the Impact of Highlighting: PRB Editors'\n  Suggestions", "comments": "Editorial", "journal-ref": "Physical Review B 92, 210001 (2015)", "doi": "10.1103/PhysRevB.92.210001", "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associate Editor Manolis Antonoyiannakis discusses the highlighting, as\nEditors' Suggestions, of a small percentage of the papers published each week.\nWe highlight papers primarily for their importance and impact in their\nrespective fields, or because we find them particularly interesting or elegant.\nIt turns out that the additional layer of scrutiny involved in the selection of\npapers as Editors' Suggestions is associated with a significantly elevated and\nsustained citation impact.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 16:20:18 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Antonoyiannakis", "Manolis", ""]]}, {"id": "1601.01887", "submitter": "Rustam Tagiew", "authors": "Rustam Tagiew", "title": "Research Project: Text Engineering Tool for Ontological Scientometry", "comments": "5 pages, 2 figure", "journal-ref": null, "doi": "10.13140/RG.2.1.1619.1847", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of scientific papers grows exponentially in many disciplines. The\nshare of online available papers grows as well. At the same time, the period of\ntime for a paper to loose at chance to be cited anymore shortens. The decay of\nthe citing rate shows similarity to ultradiffusional processes as for other\nonline contents in social networks. The distribution of papers per author shows\nsimilarity to the distribution of posts per user in social networks. The rate\nof uncited papers for online available papers grows while some papers 'go\nviral' in terms of being cited. Summarized, the practice of scientific\npublishing moves towards the domain of social networks. The goal of this\nproject is to create a text engineering tool, which can semi-automatically\ncategorize a paper according to its type of contribution and extract\nrelationships between them into an ontological database. Semi-automatic\ncategorization means that the mistakes made by automatic pre-categorization and\nrelationship-extraction will be corrected through a wikipedia-like front-end by\nvolunteers from general public. This tool should not only help researchers and\nthe general public to find relevant supplementary material and peers faster,\nbut also provide more information for research funding agencies.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 14:29:44 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Tagiew", "Rustam", ""]]}, {"id": "1601.01963", "submitter": "Greg Morrison", "authors": "Greg Morrison, Massimo Riccaboni, and Fabio Pammolli", "title": "Disambiguation of Patent Inventors and Assignees Using High-Resolution\n  Geolocation Data", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patent data represent a significant source of information on innovation and\nthe evolution of technology through networks of citations, co-invention and\nco-assignment of new patents. A major obstacle to extracting useful information\nfrom this data is the problem of name disambiguation: linking alternate\nspellings of individuals or institutions to a single identifier to uniquely\ndetermine the parties involved in the creation of a technology. In this paper,\nwe describe a new algorithm that uses high-resolution geolocation to\ndisambiguate both inventor and assignees on more than 3.6 million patents found\nin the European Patent Office (EPO), under the Patent Cooperation treaty (PCT),\nand in the US Patent and Trademark Office (USPTO). We show that our algorithm\nhas both high precision and recall in comparison to a manual disambiguation of\nEPO assignee names in Boston and Paris, and show it performs well for a\nbenchmark of USPTO inventor names that can be linked to a high-resolution\naddress (but poorly for inventors that never provided a high quality address).\nThe most significant benefit of this work is the high quality assignee\ndisambiguation with worldwide coverage coupled with an inventor disambiguation\nthat is competitive with other state of the art approaches. To our knowledge\nthis is the broadest and most accurate simultaneous disambiguation and\ncross-linking of the inventor and assignee names for a significant fraction of\npatents in these three major patent collections.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 14:16:51 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Morrison", "Greg", ""], ["Riccaboni", "Massimo", ""], ["Pammolli", "Fabio", ""]]}, {"id": "1601.02927", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Fakhri Momeni, Christoph Lange", "title": "Opening Scholarly Communication in Social Sciences: Supporting Open Peer\n  Review with Fidus Writer", "comments": "4 pages, 1 figure, poster paper accepted at the 2016 Annual EA\n  Conference: \"Innovating the Gutenberg Galaxis. The role of peer review and\n  open access in university knowledge dissemination and evaluation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our system will initially provide readers, authors and reviewers with an\nalternative, thus having the potential to gain wider acceptance and gradually\nreplace the old, incoherent publication process of our journals and of others\nin related fields. It will make journals more \"open\" (in terms of reusability)\nthat are open access already, and it has the potential to serve as an incentive\nfor turning \"closed\" journals into open access ones. In this poster paper we\nwill present the framework of the OSCOSS system and highlight the reviewer use\ncase.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 15:58:40 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Mayr", "Philipp", ""], ["Momeni", "Fakhri", ""], ["Lange", "Christoph", ""]]}, {"id": "1601.03746", "submitter": "Stefano Carrazza", "authors": "Stefano Carrazza, Alfio Ferrara, Silvia Salini", "title": "Research infrastructures in the LHC era: a scientometric approach", "comments": "39 pages, 9 figures, final version published in TFS Special Issue\n  with updated references", "journal-ref": null, "doi": "10.1016/j.techfore.2016.02.005", "report-no": "CERN-PH-TH-2015-246, TIF-UNIMI-2015-17", "categories": "physics.soc-ph cs.DL hep-ex hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a research infrastructure is funded and implemented, new information and\nnew publications are created. This new information is the measurable output of\ndiscovery process. In this paper, we describe the impact of infrastructure for\nphysics experiments in terms of publications and citations. In particular, we\nconsider the Large Hadron Collider (LHC) experiments (ATLAS, CMS, ALICE, LHCb)\nand compare them to the Large Electron Positron Collider (LEP) experiments\n(ALEPH, DELPHI, L3, OPAL) and the Tevatron experiments (CDF, D0). We provide an\noverview of the scientific output of these projects over time and highlight the\nrole played by remarkable project results in the publication-citation\ndistribution trends. The methodological and technical contribution of this work\nprovides a starting point for the development of a theoretical model of modern\nscientific knowledge propagation over time.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 21:00:06 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 13:04:12 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 08:36:24 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Carrazza", "Stefano", ""], ["Ferrara", "Alfio", ""], ["Salini", "Silvia", ""]]}, {"id": "1601.04734", "submitter": "Kyle Niemeyer", "authors": "Kyle E. Niemeyer, Arfon M. Smith, and Daniel S. Katz", "title": "The challenge and promise of software citation for credit,\n  identification, discovery, and reuse", "comments": "Challenge paper submitted to ACM Journal of Data and Information\n  Quality. v4: 2nd revised submission", "journal-ref": "ACM Journal of Data and Information Quality 7 (2016) 16", "doi": "10.1145/2968452", "report-no": null, "categories": "cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present the challenge of software citation as a method to\nensure credit for and identification, discovery, and reuse of software in\nscientific and engineering research. We discuss related work and key\nchallenges/research directions, including suggestions for metadata necessary\nfor software citation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 21:50:30 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 16:56:08 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 16:56:09 GMT"}, {"version": "v4", "created": "Wed, 6 Jul 2016 17:03:42 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Niemeyer", "Kyle E.", ""], ["Smith", "Arfon M.", ""], ["Katz", "Daniel S.", ""]]}, {"id": "1601.04950", "submitter": "Stephen Bensman", "authors": "Stephen J. Bensman and Lawrence J. Smolinsky", "title": "Lotka's Inverse Square Law of Scientific Productivity: Its Methods and\n  Statistics", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief communication analyzes the statistics and methods Lotka used to\nderive his inverse square law of scientific productivity from the standpoint of\nmodern theory. It finds that he violated the norms of this theory by extremely\ntruncating his data on the right. It also proves that Lotka himself played an\nimportant role in establishing the commonly used method of identifying\npower-law behavior by the R^2 fit to a regression line on a log-log plot that\nmodern theory considers unreliable by basing the derivation of his law on this\nvery method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 15:24:22 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Bensman", "Stephen J.", ""], ["Smolinsky", "Lawrence J.", ""]]}, {"id": "1601.05104", "submitter": "Ryan Womack", "authors": "Ryan P. Womack", "title": "ARL Libraries and Research: Correlates of Grant Funding", "comments": "revised June 2016 version of original January 2016 submission.\n  Revision contains no substantive changes, made exclusively to correct typos\n  detected in the original manuscript", "journal-ref": null, "doi": "10.1016/j.acalib.2016.06.006", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While providing the resources and tools that make advanced research possible\nis a primary mission of academic libraries at large research universities, many\nother elements also contribute to the success of the research enterprise, such\nas institutional funding, staffing, labs, and equipment. This study focuses on\nU.S. members of the ARL, the Association for Research Libraries. Research\nsuccess is measured by the total grant funding received by the University,\ncreating an ordered set of categories. Combining data from the NSF National\nCenter for Science and Engineering Statistics, ARL Statistics, and IPEDS, the\nprimary explanatory factors for research success are examined. Using linear\nregression, logistic regression, and the cumulative logit model, the\nbest-fitting models generated by ARL data, NSF data, and the combined data set\nfor both nominal and per capita funding are compared. These models produce the\nmost relevant explanatory variables for research funding, which do not include\nlibrary-related variables in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 05:17:51 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 04:13:36 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Womack", "Ryan P.", ""]]}, {"id": "1601.05142", "submitter": "Justin F Brunelle", "authors": "Justin F. Brunelle and Michele C. Weigle and Michael L. Nelson", "title": "Adapting the Hypercube Model to Archive Deferred Representations and\n  Their Descendants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web is today's primary publication medium, making web archiving an\nimportant activity for historical and analytical purposes. Web pages are\nincreasingly interactive, resulting in pages that are increasingly difficult to\narchive. Client-side technologies (e.g., JavaScript) enable interactions that\ncan potentially change the client-side state of a representation. We refer to\nrepresentations that load embedded resources via JavaScript as deferred\nrepresentations. It is difficult to archive all of the resources in deferred\nrepresentations and the result is archives with web pages that are either\nincomplete or that erroneously load embedded resources from the live web.\n  We propose a method of discovering and crawling deferred representations and\ntheir descendants (representation states that are only reachable through\nclient-side events). We adapt the Dincturk et al. Hypercube model to construct\na model for archiving descendants, and we measure the number of descendants and\nrequisite embedded resources discovered in a proof-of-concept crawl. Our\napproach identified an average of 38.5 descendants per seed URI crawled, 70.9%\nof which are reached through an onclick event. This approach also added 15.6\ntimes more embedded resources than Heritrix to the crawl frontier, but at a\nrate that was 38.9 times slower than simply using Heritrix. We show that our\ndataset has two levels of descendants. We conclude with proposed crawl policies\nand an analysis of the storage requirements for archiving descendants.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 00:48:39 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Brunelle", "Justin F.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1601.05271", "submitter": "Xianwen Wang", "authors": "Xianwen Wang, Shenmeng Xu and Zhichao Fang", "title": "Tracing Digital Footprints to Academic Articles: An Investigation of\n  PeerJ Publication Referral Data", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.SI physics.bio-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a novel way to explore the patterns of people's\nvisits to academic articles. About 3.4 million links to referral source of\nvisitors of 1432 papers published in the journal of PeerJ are collected and\nanalyzed. We find that at least 57% visits are from external referral sources,\namong which General Search Engine, Social Network, and News & Blog are the top\nthree categories of referrals. Academic Resource, including academic search\nengines and academic publishers' sites, is the fourth largest category of\nreferral sources. In addition, our results show that Google contributes\nsignificantly the most in directing people to scholarly articles. This\nencompasses the usage of Google the search engine, Google Scholar the academic\nsearch engine, and diverse specific country domains of them. Focusing on\nsimilar disciplines to PeerJ's publication scope, NCBI is the academic search\nengine on which people are the most frequently directed to PeerJ. Correlation\nanalysis and regression analysis indicates that papers with more mentions are\nexpected to have more visitors, and Facebook, Twitter and Reddit are the most\ncommonly used social networking tools that refer people to PeerJ.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 13:56:04 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Wang", "Xianwen", ""], ["Xu", "Shenmeng", ""], ["Fang", "Zhichao", ""]]}, {"id": "1601.06075", "submitter": "Elisa Omodei", "authors": "Elisa Omodei, Manlio De Domenico and Alex Arenas", "title": "Evaluating the impact of interdisciplinary research: a multilayer\n  network approach", "comments": null, "journal-ref": null, "doi": "10.1017/nws.2016.15", "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, scientific challenges usually require approaches that cross\ntraditional boundaries between academic disciplines, driving many researchers\ntowards interdisciplinarity. Despite its obvious importance, there is a lack of\nstudies on how to quantify the influence of interdisciplinarity on the research\nimpact, posing uncertainty in a proper evaluation for hiring and funding\npurposes. Here we propose a method based on the analysis of bipartite\ninterconnected multilayer networks of citations and disciplines, to assess\nscholars, institutions and countries interdisciplinary importance. Using data\nabout physics publications and US patents, we show that our method allows to\nreward, using a quantitative approach, scholars and institutions that have\ncarried out interdisciplinary work and have had an impact in different\nscientific areas. The proposed method could be used by funding agencies,\nuniversities and scientific policy decision makers for hiring and funding\npurposes, and to complement existing methods to rank universities and\ncountries.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 17:12:20 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 13:38:22 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Omodei", "Elisa", ""], ["De Domenico", "Manlio", ""], ["Arenas", "Alex", ""]]}, {"id": "1601.06296", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Katrin Weller", "title": "Think before you collect: Setting up a data collection approach for\n  social media studies", "comments": "33 pages, updated final version, The SAGE Handbook of Social Media\n  Research Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter discusses important challenges of designing the data collection\nsetup for social media studies. It outlines how it is necessary to carefully\nthink about which data to collect and to use, and to recognize the effects that\na specific data collection approach may have on the types of analyses that can\nbe carried out and the results that can be expected in a study. We will\nhighlight important questions one should ask before setting up a data\ncollection framework and relate them to the different options for accessing\nsocial media data. The chapter will mainly be illustrated with examples from\nstudying Twitter and Facebook. A case study studying political communication\naround the 2013 elections in Germany should serve as a practical application\nscenario. In this case study we constructed several social media datasets based\non different collection approaches, using data from Facebook and Twitter.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 17:28:25 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 11:00:30 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 08:16:41 GMT"}, {"version": "v4", "created": "Mon, 30 Oct 2017 20:17:46 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Mayr", "Philipp", ""], ["Weller", "Katrin", ""]]}, {"id": "1601.07252", "submitter": "Anshul Gupta", "authors": "Anshul Gupta, Ricardo Gutierrez-Osuna, Matthew Christy, Richard\n  Furuta, Laura Mandell", "title": "Font Identification in Historical Documents Using Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DL stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Identifying the type of font (e.g., Roman, Blackletter) used in historical\ndocuments can help optical character recognition (OCR) systems produce more\naccurate text transcriptions. Towards this end, we present an active-learning\nstrategy that can significantly reduce the number of labeled samples needed to\ntrain a font classifier. Our approach extracts image-based features that\nexploit geometric differences between fonts at the word level, and combines\nthem into a bag-of-word representation for each page in a document. We evaluate\nsix sampling strategies based on uncertainty, dissimilarity and diversity\ncriteria, and test them on a database containing over 3,000 historical\ndocuments with Blackletter, Roman and Mixed fonts. Our results show that a\ncombination of uncertainty and diversity achieves the highest predictive\naccuracy (89% of test cases correctly classified) while requiring only a small\nfraction of the data (17%) to be labeled. We discuss the implications of this\nresult for mass digitization projects of historical documents.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 03:24:05 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Gupta", "Anshul", ""], ["Gutierrez-Osuna", "Ricardo", ""], ["Christy", "Matthew", ""], ["Furuta", "Richard", ""], ["Mandell", "Laura", ""]]}, {"id": "1601.07858", "submitter": "Alberto Accomazzi", "authors": "Alberto Accomazzi, Michael J. Kurtz, Edwin A. Henneken, Carolyn S.\n  Grant, Donna M. Thompson, Roman Chyla, Alexandra Holachek, Jonathan Elliott", "title": "Aggregation and Linking of Observational Metadata in the ADS", "comments": "4 pages, Proceedings of the ADASS XXV conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss current efforts behind the curation of observing proposals,\narchive bibliographies, and data links in the NASA Astrophysics Data System\n(ADS). The primary data in the ADS is the bibliographic content from scholarly\narticles in Astronomy and Physics, which ADS aggregates from publishers, arXiv\nand conference proceeding sites. This core bibliographic information is then\nfurther enriched by ADS via the generation of citations and usage data, and\nthrough the aggregation of external resources from astronomy data archives and\nlibraries. Important sources of such additional information are the metadata\ndescribing observing proposals and high level data products, which, once\ningested in ADS, become easily discoverable and citeable by the science\ncommunity. Bibliographic studies have shown that the integration of links\nbetween data archives and the ADS provides greater visibility to data products\nand increased citations to the literature associated with them.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 18:53:14 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Accomazzi", "Alberto", ""], ["Kurtz", "Michael J.", ""], ["Henneken", "Edwin A.", ""], ["Grant", "Carolyn S.", ""], ["Thompson", "Donna M.", ""], ["Chyla", "Roman", ""], ["Holachek", "Alexandra", ""], ["Elliott", "Jonathan", ""]]}, {"id": "1601.08049", "submitter": "Juan Gorraiz", "authors": "Juan Gorraiz, Martin Wieland and Christian Gumpenberger", "title": "Individual Bibliometric Assessment @ University of Vienna: From Numbers\n  to Multidimensional Profiles", "comments": "Preprint", "journal-ref": null, "doi": "10.5281/zenodo.45402", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper shows how bibliometric assessment can be implemented at individual\nlevel. This has been successfully done at the University of Vienna carried out\nby the Bibliometrics and Publication Strategies Department of the Vienna\nUniversity Library. According to the department's philosophy, bibliometrics is\nnot only a helpful evaluation instrument in order to complement the peer review\nsystem. It is also meant as a compass for researchers in the \"publish or\nperish\" dilemma in order to increase general visibility and to optimize\npublication strategies. The individual assessment comprises of an interview\nwith the researcher under evaluation, the elaboration of a bibliometric report\nof the researcher's publication output, the discussion and validation of the\nobtained results with the researcher under evaluation as well as further\noptional analyses. The produced bibliometric reports are provided to the\nresearchers themselves and inform them about the quantitative aspects of their\nresearch output. They also serve as a basis for further discussion concerning\ntheir publication strategies. These reports are eventually intended for\ninformed peer review practices, and are therefore forwarded to the quality\nassurance and the rector's office and finally sent to the peers. The most\nimportant feature of the generated bibliometric report is its multidimensional\nand individual character. It relies on a variety of basic indicators and\nfurther control parameters in order to foster comprehensibility. Researchers,\nadministrative staff and peers alike have confirmed the usefulness of this\nbibliometric approach. An increasing demand is noticeable. In total, 33\nbibliometric reports have been delivered so far. Moreover, similar reports have\nalso been produced for the bibliometric assessment of two faculties with great\nsuccess.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 10:54:56 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 12:45:12 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Gorraiz", "Juan", ""], ["Wieland", "Martin", ""], ["Gumpenberger", "Christian", ""]]}]