[{"id": "1912.00532", "submitter": "Marion Maisonobe", "authors": "Marion Maisonobe (GC UMR 8504 CNRS)", "title": "The future of urban models in the Big Data and AI era: a bibliometric\n  analysis (2000-2019)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article questions the effects on urban research dynamics of the Big Data\nand AI turn in urban management. To identify these effects, we use two\ncomplementary materials: bibliometric data and interviews. We consider two\nareas in urban research: one, covering the academic research dealing with\ntransportation systems and the other, with water systems. First, we measure the\nevolution of AI and Big Data keywords in these two areas. Second, we measure\nthe evolution of the share of publications published in computer science\njournals about urban traffic and water quality. To guide these bibliometric\nanalyses, we rely on the content of interviews conducted with academics and\nhigher education officials in Paris and Edinburgh at the beginning of 2018.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:47:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Maisonobe", "Marion", "", "GC UMR 8504 CNRS"]]}, {"id": "1912.01527", "submitter": "Qing Ke", "authors": "Qing Ke", "title": "The citation disadvantage of clinical research", "comments": null, "journal-ref": "Journal of Informetrics 14, 100998 (2020)", "doi": "10.1016/j.joi.2019.100998", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical research encompasses diverse types of activities, from basic\nscience (\"bench\") to clinical medicine (\"bedside\") to bench-to-bedside\ntranslational research. It, however, remains unclear whether different types of\nresearch receive citations at varying rates. Here we aim to answer this\nquestion by using a newly proposed paper-level indicator that quantifies the\nextent to which a paper is basic science or clinical medicine. Applying this\nmeasure to 5 million biomedical papers, we find a systematic citation\ndisadvantage of clinical oriented papers; they tend to garner far fewer\ncitations and are less likely to be hit works than papers oriented towards\nbasic science. At the same time, clinical research has a higher variance in its\ncitation. We also find that the citation difference between basic and clinical\nresearch decreases, yet still persists, if longer citation-window is used.\nGiven the increasing adoption of short-term, citation-based bibliometric\nindicators in funding decisions, the under-cited effect of clinical research\nmay provide disincentives for bio-researchers to venture into the translation\nof basic scientific discoveries into clinical applications, thus providing\nexplanations of reasons behind the existence of the gap between basic and\nclinical research that is commented as \"valley of death\" and the commentary of\n\"extinction\" risk of translational researchers. Our work may provide insights\nto policy-makers on how to evaluate different types of biomedical research.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 17:03:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Ke", "Qing", ""]]}, {"id": "1912.02611", "submitter": "Hossein Mohammadi Rouzbahani", "authors": "Hossein Mohammadi Rouzbahani, Hadis Karimipour, Ali Dehghantanha, Reza\n  M. Parizi", "title": "Blockchain Applications in Power Systems: A Bibliometric Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power systems are growing rapidly, due to the ever-increasing demand for\nelectrical power. These systems require novel methodologies and modern tools\nand technologies, to better perform, particularly for communication among\ndifferent parts. Therefore, power systems are facing new challenges such as\nenergy trading and marketing and cyber threats. Using blockchain in power\nsystems, as a solution, is one of the newest methods. Most studies aim to\ninvestigate innovative approach-es of blockchain application in power systems.\nEven though, many articles published to support the research activities, there\nhas not been any bibliometric analysis which specifies the research trends.\nThis paper aims to present a bibliographic analysis of the blockchain\napplication in power systems related literature, in the Web of Science (WoS)\ndatabase between January 2009 and July 2019. This paper discusses the research\nactivities and performed a detailed analysis by looking at the number of\narticles published, citations, institutions, research areas, and authors. From\nthe analysis, it was concluded that there are several significant impacts of\nresearch activities in China and the USA, in comparison to other countries.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:47:04 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Rouzbahani", "Hossein Mohammadi", ""], ["Karimipour", "Hadis", ""], ["Dehghantanha", "Ali", ""], ["Parizi", "Reza M.", ""]]}, {"id": "1912.05082", "submitter": "Caroline Schroeder", "authors": "Caroline T. Schroeder, Amir Zeldes", "title": "A Collaborative Ecosystem for Digital Coptic Studies", "comments": "9 pages; paper presented at the Stanford University CESTA Workshop\n  \"Collecting, Preserving and Disseminating Endangered Cultural Heritage for\n  New Understandings Through Multilingual Approaches\"", "journal-ref": "Journal of Data Mining & Digital Humanities, Special Issue on\n  Collecting, Preserving, and Disseminating Endangered Cultural Heritage for\n  New Understandings through Multilingual Approaches (September 23, 2020)\n  jdmdh:6797", "doi": "10.46298/jdmdh.5969", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scholarship on underresourced languages bring with them a variety of\nchallenges which make access to the full spectrum of source materials and their\nevaluation difficult. For Coptic in particular, large scale analyses and any\nkind of quantitative work become difficult due to the fragmentation of\nmanuscripts, the highly fusional nature of an incorporational morphology, and\nthe complications of dealing with influences from Hellenistic era Greek, among\nother concerns. Many of these challenges, however, can be addressed using\nDigital Humanities tools and standards. In this paper, we outline some of the\nlatest developments in Coptic Scriptorium, a DH project dedicated to bringing\nCoptic resources online in uniform, machine readable, and openly available\nformats. Collaborative web-based tools create online 'virtual departments' in\nwhich scholars dispersed sparsely across the globe can collaborate, and natural\nlanguage processing tools counterbalance the scarcity of trained editors by\nenabling machine processing of Coptic text to produce searchable, annotated\ncorpora.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 02:03:31 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 19:39:30 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 19:22:24 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Schroeder", "Caroline T.", ""], ["Zeldes", "Amir", ""]]}, {"id": "1912.05576", "submitter": "Hanna Hottenrott", "authors": "Hanna Hottenrott, Michael Rose, Cornelia Lawson", "title": "The Rise of Multiple Institutional Affiliations in Academia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.DL q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study provides the first systematic, international, large-scale evidence\non the extent and nature of multiple institutional affiliations on journal\npublications. Studying more than 15 million authors and 22 million articles\nfrom 40 countries we document that: In 2019, almost one in three articles was\n(co-)authored by authors with multiple affiliations and the share of authors\nwith multiple affiliations increased from around 10% to 16% since 1996. The\ngrowth of multiple affiliations is prevalent in all fields and it is stronger\nin high impact journals. About 60% of multiple affiliations are between\ninstitutions from within the academic sector. International co-affiliations,\nwhich account for about a quarter of multiple affiliations, most often involve\ninstitutions from the United States, China, Germany and the United Kingdom,\nsuggesting a core-periphery network. Network analysis also reveals a number\ncommunities of countries that are more likely to share affiliations. We discuss\npotential causes and show that the timing of the rise in multiple affiliations\ncan be linked to the introduction of more competitive funding structures such\nas 'excellence initiatives' in a number of countries. We discuss implications\nfor science and science policy.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:08:01 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 06:52:23 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 10:40:05 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Hottenrott", "Hanna", ""], ["Rose", "Michael", ""], ["Lawson", "Cornelia", ""]]}, {"id": "1912.06231", "submitter": "Fei Shu", "authors": "Fei Shu, Wei Quan, Bikun Chen, Junping Qiu, Cassidy Sugimoto and\n  Vincent Larivi\\`ere", "title": "The role of Web of Science publications in China's tenure system", "comments": "Accepted by Scientometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tenure provides a permanent position to faculty in higher education\ninstitutions. In North America, it is granted to those who have established a\nrecord of excellence in research, teaching and services in a limited period.\nHowever, in China, research excellence represented by the number of Web of\nScience publications is highly weighted in the tenure assessment compared to\nexcellence in teaching and services, but this has never been systematically\ninvestigated. By analyzing the tenure assessment documents from Chinese\nuniversities, this study reveals the role of Web of Science publications in\nChina tenure system and presents the landscape of the tenure assessment process\nin Chinese higher education institutions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 21:42:42 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Shu", "Fei", ""], ["Quan", "Wei", ""], ["Chen", "Bikun", ""], ["Qiu", "Junping", ""], ["Sugimoto", "Cassidy", ""], ["Larivi\u00e8re", "Vincent", ""]]}, {"id": "1912.06858", "submitter": "Neslihan Suzen", "authors": "Neslihan Suzen, Evgeny M. Mirkes, Alexander N. Gorban", "title": "LScDC-new large scientific dictionary", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a scientific corpus of abstracts of academic papers\nin English -- Leicester Scientific Corpus (LSC). The LSC contains 1,673,824\nabstracts of research articles and proceeding papers indexed by Web of Science\n(WoS) in which publication year is 2014. Each abstract is assigned to at least\none of 252 subject categories. Paper metadata include these categories and the\nnumber of citations. We then develop scientific dictionaries named Leicester\nScientific Dictionary (LScD) and Leicester Scientific Dictionary-Core (LScDC),\nwhere words are extracted from the LSC. The LScD is a list of 974,238 unique\nwords (lemmas). The LScDC is a core list (sub-list) of the LScD with 104,223\nlemmas. It was created by removing LScD words appearing in not greater than 10\ntexts in the LSC. LScD and LScDC are available online. Both the corpus and\ndictionaries are developed to be later used for quantification of meaning in\nacademic texts.\n  Finally, the core list LScDC was analysed by comparing its words and word\nfrequencies with a classic academic word list 'New Academic Word List (NAWL)'\ncontaining 963 word families, which is also sampled from an academic corpus.\nThe major sources of the corpus where NAWL is extracted are Cambridge English\nCorpus (CEC), oral sources and textbooks. We investigate whether two\ndictionaries are similar in terms of common words and ranking of words. Our\ncomparison leads us to main conclusion: most of words of NAWL (99.6%) are\npresent in the LScDC but two lists differ in word ranking. This difference is\nmeasured.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 14:55:59 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Suzen", "Neslihan", ""], ["Mirkes", "Evgeny M.", ""], ["Gorban", "Alexander N.", ""]]}, {"id": "1912.07090", "submitter": "Dimitrios Katsaros", "authors": "Dimitrios Katsaros", "title": "Hunting for supernovae articles in the universe of scientometrics", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note records an unusual situation with some Google Scholar's\nprofiles that imply the existence of \"supernovae\" articles, i.e., articles\nwhose impact -- in terms of number of citations -- in a single year gets\n(almost) an order of magnitude higher than the previous year and immediate\ndrops (and remains steady) to a very low level after the next year. We analyse\nthe issue and resolve the situation providing an answer whether there exist\nsupernovae articles.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 18:54:36 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Katsaros", "Dimitrios", ""]]}, {"id": "1912.07266", "submitter": "Syed Tahseen Raza Rizvi", "authors": "Syed Tahseen Raza Rizvi, Andreas Dengel, Sheraz Ahmed", "title": "A Hybrid Approach and Unified Framework for Bibliographic Reference\n  Extraction", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3042455", "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publications are an integral part in a scientific community. Bibliographic\nreference extraction from scientific publication is a challenging task due to\ndiversity in referencing styles and document layout. Existing methods perform\nsufficiently on one dataset however, applying these solutions to a different\ndataset proves to be challenging. Therefore, a generic solution was anticipated\nwhich could overcome the limitations of the previous approaches. The\ncontribution of this paper is three-fold. First, it presents a novel approach\ncalled DeepBiRD which is inspired by human visual perception and exploits\nlayout features to identify individual references in a scientific publication.\nSecond, we release a large dataset for image-based reference detection with\n2401 scans containing 38863 references, all manually annotated for individual\nreference. Third, we present a unified and highly configurable end-to-end\nautomatic bibliographic reference extraction framework called BRExSys which\nemploys DeepBiRD along with state-of-the-art text-based models to detect and\nvisualize references from a bibliographic document. Our proposed approach\npre-processes the images in which a hybrid representation is obtained by\nprocessing the given image using different computer vision techniques. Then, it\nperforms layout driven reference detection using Mask R-CNN on a given\nscientific publication. DeepBiRD was evaluated on two different datasets to\ndemonstrate the generalization of this approach. The proposed system achieved\nan AP50 of 98.56% on our dataset. DeepBiRD significantly outperformed the\ncurrent state-of-the-art approach on their dataset. Therefore, suggesting that\nDeepBiRD is significantly superior in performance, generalized, and independent\nof any domain or referencing style.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 09:47:50 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 14:28:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Rizvi", "Syed Tahseen Raza", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "1912.07908", "submitter": "Micah Altman", "authors": "Micah Altman and Richard Landau", "title": "Selecting efficient and reliable preservation strategies: modeling\n  long-term information integrity using large-scale hierarchical discrete event\n  simulation", "comments": "Fortcoming IDCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the problem of formulating efficient and reliable\noperational preservation policies that ensure bit-level information integrity\nover long periods, and in the presence of a diverse range of real-world\ntechnical, legal, organizational, and economic threats. We develop a\nsystematic, quantitative prediction framework that combines formal modeling,\ndiscrete-event-based simulation, hierarchical modeling, and then use\nempirically calibrated sensitivity analysis to identify effective strategies.\nThe framework offers flexibility for the modeling of a wide range of\npreservation policies and threats. Since this framework is open source and\neasily deployed in a cloud computing environment, it can be used to produce\nanalysis based on independent estimates of scenario-specific costs,\nreliability, and risks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:05:50 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Altman", "Micah", ""], ["Landau", "Richard", ""]]}, {"id": "1912.08320", "submitter": "R.Stuart Geiger", "authors": "R. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah\n  Tang, Jenny Huang", "title": "Garbage In, Garbage Out? Do Machine Learning Application Papers in\n  Social Computing Report Where Human-Labeled Training Data Comes From?", "comments": "18 pages, includes appendix", "journal-ref": "Proc ACM FAT* 2020", "doi": "10.1145/3351095.3372862", "report-no": null, "categories": "cs.CY cs.CL cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many machine learning projects for new application areas involve teams of\nhumans who label data for a particular purpose, from hiring crowdworkers to the\npaper's authors labeling the data themselves. Such a task is quite similar to\n(or a form of) structured content analysis, which is a longstanding methodology\nin the social sciences and humanities, with many established best practices. In\nthis paper, we investigate to what extent a sample of machine learning\napplication papers in social computing --- specifically papers from ArXiv and\ntraditional publications performing an ML classification task on Twitter data\n--- give specific details about whether such best practices were followed. Our\nteam conducted multiple rounds of structured content analysis of each paper,\nmaking determinations such as: Does the paper report who the labelers were,\nwhat their qualifications were, whether they independently labeled the same\nitems, whether inter-rater reliability metrics were disclosed, what level of\ntraining and/or instructions were given to labelers, whether compensation for\ncrowdworkers is disclosed, and if the training data is publicly available. We\nfind a wide divergence in whether such practices were followed and documented.\nMuch of machine learning research and education focuses on what is done once a\n\"gold standard\" of training data is available, but we discuss issues around the\nequally-important aspect of whether such data is reliable in the first place.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 23:49:19 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Geiger", "R. Stuart", ""], ["Yu", "Kevin", ""], ["Yang", "Yanlai", ""], ["Dai", "Mindy", ""], ["Qiu", "Jie", ""], ["Tang", "Rebekah", ""], ["Huang", "Jenny", ""]]}, {"id": "1912.08582", "submitter": "Giorgio Maria Di Nunzio", "authors": "Nataliya Sira, Giorgio Maria Di Nunzio, Viviana Nosilia", "title": "Towards an automatic recognition of mixed languages: The\n  Ukrainian-Russian hybrid language Surzhyk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language interference is common in today's multilingual societies where more\nlanguages are being in contact and as a global final result leads to the\ncreation of hybrid languages. These, together with doubts on their right to be\nofficially recognised made emerge in the area of computational linguistics the\nproblem of their automatic identification and further elaboration. In this\npaper, we propose a first attempt to identify the elements of a\nUkrainian-Russian hybrid language, Surzhyk, through the adoption of the\nexample-based rules created with the instruments of programming language R. Our\nexample-based study consists of: 1) analysis of spoken samples of Surzhyk\nregistered by Del Gaudio (2010) in Kyiv area and creation of the written\ncorpus; 2) production of specific rules on the identification of Surzhyk\npatterns and their implementation; 3) testing the code and analysing the\neffectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:13:29 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Sira", "Nataliya", ""], ["Di Nunzio", "Giorgio Maria", ""], ["Nosilia", "Viviana", ""]]}, {"id": "1912.08648", "submitter": "Vincent A Traag", "authors": "V.A. Traag", "title": "Inferring the causal effect of journals on citations", "comments": null, "journal-ref": null, "doi": "10.1162/qss_a_00128", "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Articles in high-impact journals are, on average, more frequently cited. But\nare they cited more often because those articles are somehow more \"citable\"? Or\nare they cited more often simply because they are published in a high-impact\njournal? Although some evidence suggests the latter, the causal relationship is\nnot clear. We here compare citations of preprints to citations of the published\nversion to uncover the causal mechanism. We build on an earlier model of\ncitation dynamics to infer the causal effect of journals on citations. We find\nthat high-impact journals select articles that tend to attract more citations.\nAt the same time, we find that high-impact journals augment the citation rate\nof published articles. Our results yield a deeper understanding of the role of\njournals in the research system. The use of journal metrics in research\nevaluation has been increasingly criticized in recent years and article-level\ncitations are sometimes suggested as an alternative. Our results show that\nremoving impact factors from evaluation does not negate the influence of\njournals. This insight has important implications for changing practices of\nresearch evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:55:18 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 09:19:20 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Traag", "V. A.", ""]]}, {"id": "1912.08694", "submitter": "Andrew Collins Mr", "authors": "Andrew Collins, Joeran Beel", "title": "Meta-Learned Per-Instance Algorithm Selection in Scholarly Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of recommender system algorithms varies in different\nreal-world scenarios. It is difficult to choose a best algorithm for a scenario\ndue to the quantity of algorithms available, and because of their varying\nperformances. Furthermore, it is not possible to choose one single algorithm\nthat will work optimally for all recommendation requests. We apply\nmeta-learning to this problem of algorithm selection for scholarly article\nrecommendation. We train a random forest, gradient boosting machine, and\ngeneralized linear model, to predict a best-algorithm from a pool of content\nsimilarity-based algorithms. We evaluate our approach on an offline dataset for\nscholarly article recommendation and attempt to predict the best algorithm\nper-instance. The best meta-learning model achieved an average increase in F1\nof 88% when compared to the average F1 of all base-algorithms (F1; 0.0708 vs\n0.0376) and was significantly able to correctly select each base-algorithm\n(Paired t-test; p < 0.1). The meta-learner had a 3% higher F1 when compared to\nthe single-best base-algorithm (F1; 0.0739 vs 0.0717). We further perform an\nonline evaluation of our approach, conducting an A/B test through our\nrecommender-as-a-service platform Mr. DLib. We deliver 148K recommendations to\nusers between January and March 2019. User engagement was significantly\nincreased for recommendations generated using our meta-learning approach when\ncompared to a random selection of algorithm (Click-through rate (CTR); 0.51%\nvs. 0.44%, Chi-Squared test; p < 0.1), however our approach did not produce a\nhigher CTR than the best algorithm alone (CTR; MoreLikeThis (Title): 0.58%).\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 16:13:27 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Collins", "Andrew", ""], ["Beel", "Joeran", ""]]}, {"id": "1912.08812", "submitter": "Teofilo de Campos", "authors": "Frederico Guth and Teofilo Emidio de-Campos", "title": "Research Frontiers in Transfer Learning -- a systematic and bibliometric\n  review", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can learn from very few samples, demonstrating an outstanding\ngeneralization ability that learning algorithms are still far from reaching.\nCurrently, the most successful models demand enormous amounts of well-labeled\ndata, which are expensive and difficult to obtain, becoming one of the biggest\nobstacles to the use of machine learning in practice. This scenario shows the\nmassive potential for Transfer Learning, which aims to harness previously\nacquired knowledge to the learning of new tasks more effectively and\nefficiently. In this systematic review, we apply a quantitative method to\nselect the main contributions to the field and make use of bibliographic\ncoupling metrics to identify research frontiers. We further analyze the\nlinguistic variation between the classics of the field and the frontier and map\npromising research directions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 15:08:19 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Guth", "Frederico", ""], ["de-Campos", "Teofilo Emidio", ""]]}, {"id": "1912.08861", "submitter": "Bianca Minetto Napole\\~ao", "authors": "Bianca Minetto Napoleao, Katia Romero Felizardo, Erica Ferreira de\n  Souza, Fabio Petrillo, Nandamudi L. Vijaykumar, Elisa Yumi Nakagawa, Sylvain\n  Halle", "title": "Establishing a Search String to Detect Secondary Studies in Software\n  Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search for secondary studies is essential to establish whether the review on\nthe intended topic has already been done, avoiding waste time. In addition,\nsecondary studies are the inputs of a tertiary study. However, one critical\nstep in searching for secondary studies is to elaborate a search string. The\nmain goal of this work is to analyze search strings to establish directions to\nbetter detect secondary studies in Software Engineering (SE). We analyzed seven\ntertiary studies under two perspectives: (1) structure - strings' terms to\ndetect secondary studies; and (2) field: where searching - titles alone or\nabstracts alone or titles and abstracts together, among others. We also\nperformed a validation of the results found. The suitable search string for\nfinding secondary studies in SE contain the terms \"systematic review\",\n\"literature review\", \"systematic mapping\", \"mapping study\", \"systematic map\",\n\"meta-analysis\", \"survey\" and \"literature analysis\". Furthermore, we recommend\n(1) researchers use the title, abstract and keywords search fields in their\nsearches to increase studies recall; (2) researchers choose carefully their\npaper title, abstract and keyword terms to increase the chance of having such\nstudies found on digital libraries.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:54:26 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Napoleao", "Bianca Minetto", ""], ["Felizardo", "Katia Romero", ""], ["de Souza", "Erica Ferreira", ""], ["Petrillo", "Fabio", ""], ["Vijaykumar", "Nandamudi L.", ""], ["Nakagawa", "Elisa Yumi", ""], ["Halle", "Sylvain", ""]]}, {"id": "1912.08928", "submitter": "Angelo Salatino", "authors": "Angelo Antonio Salatino", "title": "Early Detection of Research Trends", "comments": "This dissertation is submitted for the Degree of Doctor of Philosophy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to rapidly recognise new research trends is strategic for many\nstakeholders, including universities, institutional funding bodies, academic\npublishers and companies. The literature presents several approaches to\nidentifying the emergence of new research topics, which rely on the assumption\nthat the topic is already exhibiting a certain degree of popularity and\nconsistently referred to by a community of researchers. However, detecting the\nemergence of a new research area at an embryonic stage, i.e., before the topic\nhas been consistently labelled by a community of researchers and associated\nwith a number of publications, is still an open challenge. In this\ndissertation, we begin to address this challenge by performing a study of the\ndynamics preceding the creation of new topics. This study indicates that the\nemergence of a new topic is anticipated by a significant increase in the pace\nof collaboration between relevant research areas, which can be seen as the\n'ancestors' of the new topic. Based on this understanding, we developed Augur,\na novel approach to effectively detecting the emergence of new research topics.\nAugur analyses the diachronic relationships between research areas and is able\nto detect clusters of topics that exhibit dynamics correlated with the\nemergence of new research topics. Here we also present the Advanced Clique\nPercolation Method (ACPM), a new community detection algorithm developed\nspecifically for supporting this task. Augur was evaluated on a gold standard\nof 1,408 debutant topics in the 2000-2011 timeframe and outperformed four\nalternative approaches in terms of both precision and recall.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 20:20:20 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Salatino", "Angelo Antonio", ""]]}, {"id": "1912.09094", "submitter": "Anton Akusok", "authors": "Anton Akusok, Mirka Saarela, Tommi K\\\"arkk\\\"ainen, Kaj-Mikael Bj\\\"ork\n  and Amaury Lendasse", "title": "Mislabel Detection of Finnish Publication Ranks", "comments": null, "journal-ref": "International Conference on Extreme Learning Machine 2017 Oct 4\n  (pp. 240-248). Springer, Cham", "doi": null, "report-no": null, "categories": "cs.LG cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes to analyze a data set of Finnish ranks of academic\npublication channels with Extreme Learning Machine (ELM). The purpose is to\nintroduce and test recently proposed ELM-based mislabel detection approach with\na rich set of features characterizing a publication channel. We will compare\nthe architecture, accuracy, and, especially, the set of detected mislabels of\nthe ELM-based approach to the corresponding reference results on the reference\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:56:50 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Akusok", "Anton", ""], ["Saarela", "Mirka", ""], ["K\u00e4rkk\u00e4inen", "Tommi", ""], ["Bj\u00f6rk", "Kaj-Mikael", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1912.10170", "submitter": "Joeran Beel", "authors": "Dominika Tkaczyk, Andrew Collins, Joeran Beel", "title": "Na\\\"iveRole: Author-Contribution Extraction and Parsing from Biomedical\n  Manuscripts", "comments": "arXiv admin note: substantial text overlap with arXiv:1802.01174", "journal-ref": "27th AIAI Irish Conference on Artificial Intelligence and\n  Cognitive Science, 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about the contributions of individual authors to scientific\npublications is important for assessing authors' achievements. Some biomedical\npublications have a short section that describes authors' roles and\ncontributions. It is usually written in natural language and hence author\ncontributions cannot be trivially extracted in a machine-readable format. In\nthis paper, we present 1) A statistical analysis of roles in author\ncontributions sections, and 2) Na\\\"iveRole, a novel approach to extract\nstructured authors' roles from author contribution sections. For the first\npart, we used co-clustering techniques, as well as Open Information Extraction,\nto semi-automatically discover the popular roles within a corpus of 2,000\ncontributions sections from PubMed Central. The discovered roles were used to\nautomatically build a training set for Na\\\"iveRole, our role extractor\napproach, based on Na\\\"ive Bayes. Na\\\"iveRole extracts roles with a\nmicro-averaged precision of 0.68, recall of 0.48 and F1 of 0.57. It is, to the\nbest of our knowledge, the first attempt to automatically extract author roles\nfrom research papers. This paper is an extended version of a previous poster\npublished at JCDL 2018.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 14:37:06 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Tkaczyk", "Dominika", ""], ["Collins", "Andrew", ""], ["Beel", "Joeran", ""]]}, {"id": "1912.10228", "submitter": "Bo-Christer Bj\\\"ork", "authors": "Bo-Christer Bj\\\"ork, Sari Kanto-Karvonen and J. Tuomas Harviainen", "title": "How Frequently are Articles in Predatory Open Access Journals Cited", "comments": "Working paper, later to submitted to a journal", "journal-ref": "Publications, 2020, 8(2) e17", "doi": "10.3390/publications8020017", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predatory journals are Open Access journals of highly questionable scientific\nquality. Such journals pretend to use peer review for quality assurance, and\nspam academics with requests for submissions, in order to collect author\npayments. In recent years predatory journals have received a lot of negative\nmedia. While much has been said about the harm that such journals cause to\nacademic publishing in general, an overlooked aspect is how much articles in\nsuch journals are actually read and in particular cited, that is if they have\nany significant impact on the research in their fields. Other studies have\nalready demonstrated that only some of the articles in predatory journals\ncontain faulty and directly harmful results, while a lot of the articles\npresent mediocre and poorly reported studies. We studied citation statistics\nover a five-year period in Google Scholar for 250 random articles published in\nsuch journals in 2014, and found an average of 2,6 citations per article and\nthat 60 % of the articles had no citations at all. For comparison a random\nsample of articles published in the approximately 25,000 peer reviewed journals\nincluded in the Scopus index had an average of 18,1 citations in the same\nperiod with only 9 % receiving no citations. We conclude that articles\npublished in predatory journals have little scientific impact.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 09:22:00 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Bj\u00f6rk", "Bo-Christer", ""], ["Kanto-Karvonen", "Sari", ""], ["Harviainen", "J. Tuomas", ""]]}, {"id": "1912.10521", "submitter": "George Chacko", "authors": "Sitaram Devarakonda, Dmitriy Korobskiy, Tandy Warnow, and George\n  Chacko", "title": "Viewing Computer Science through Citation Analysis; Salton and Bergmark\n  Redux", "comments": null, "journal-ref": null, "doi": "10.1007/s11192-020-03624-0", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer science has experienced dramatic growth and diversification over the\nlast twenty years. Towards a current understanding of the structure of this\ndiscipline, we analyze a cohort of the computer science literature using the\nDBLP database. For insight on the features of this cohort and the relationship\nwithin its components, we constructed article level clusters based on either\ndirect citations or co-citations, and reconciled them to major and minor\nsubject categories in the Scopus All Science Journal Classification (ASJC). We\ndescribed complementary insights from clustering by direct citation and\nco-citation, and both point to the increase in computer science publications\nand their scope. Our analysis shows cross-category clusters, some that interact\nwith external fields, such as the biological sciences, while others remain\ninward looking.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 19:56:49 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Devarakonda", "Sitaram", ""], ["Korobskiy", "Dmitriy", ""], ["Warnow", "Tandy", ""], ["Chacko", "George", ""]]}, {"id": "1912.10809", "submitter": "Christian Otto", "authors": "Hang Zhou, Christian Otto, Ralph Ewerth", "title": "Visual Summarization of Scholarly Videos using Word Embeddings and\n  Keyphrase Extraction", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-3-030-30760-8_28", "report-no": null, "categories": "cs.MM cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective learning with audiovisual content depends on many factors. Besides\nthe quality of the learning resource's content, it is essential to discover the\nmost relevant and suitable video in order to support the learning process most\neffectively. Video summarization techniques facilitate this goal by providing a\nquick overview over the content. It is especially useful for longer recordings\nsuch as conference presentations or lectures. In this paper, we present an\napproach that generates a visual summary of video content based on semantic\nword embeddings and keyphrase extraction. For this purpose, we exploit video\nannotations that are automatically generated by speech recognition and video\nOCR (optical character recognition).\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 12:02:15 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zhou", "Hang", ""], ["Otto", "Christian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1912.11084", "submitter": "Gian Maria Campedelli", "authors": "Gian Maria Campedelli", "title": "Where Are We? Using Scopus to Map the Literature at the Intersection\n  Between Artificial Intelligence and Research on Crime", "comments": "25 pages, 12 figures, pre-print (currently R&R in JCSS)", "journal-ref": "J Comput Soc Sc (2020)", "doi": "10.1007/s42001-020-00082-9", "report-no": null, "categories": "cs.DL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research on Artificial Intelligence (AI) applications has spread over many\nscientific disciplines. Scientists have tested the power of intelligent\nalgorithms developed to predict (or learn from) natural, physical and social\nphenomena. This also applies to crime-related research problems. Nonetheless,\nstudies that map the current state of the art at the intersection between AI\nand crime are lacking. What are the current research trends in terms of topics\nin this area? What is the structure of scientific collaboration when\nconsidering works investigating criminal issues using machine learning, deep\nlearning, and AI in general? What are the most active countries in this\nspecific scientific sphere? Using data retrieved from the Scopus database, this\nwork quantitatively analyzes 692 published works at the intersection between AI\nand crime employing network science to respond to these questions. Results show\nthat researchers are mainly focusing on cyber-related criminal topics and that\nrelevant themes such as algorithmic discrimination, fairness, and ethics are\nconsiderably overlooked. Furthermore, data highlight the extremely disconnected\nstructure of co-authorship networks. Such disconnectedness may represent a\nsubstantial obstacle to a more solid community of scientists interested in\nthese topics. Additionally, the graph of scientific collaboration indicates\nthat countries that are more prone to engage in international partnerships are\ngenerally less central in the network. This means that scholars working in\nhighly productive countries (e.g. the United States, China) tend to mostly\ncollaborate domestically. Finally, current issues and future developments\nwithin this scientific area are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 19:55:52 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 23:23:57 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Campedelli", "Gian Maria", ""]]}, {"id": "1912.11894", "submitter": "Mayank Singh", "authors": "Pradumn Kumar Pandey, Mayank Singh, Pawan Goyal, Animesh Mukherjee,\n  Soumen Chakrabarti", "title": "Analysis of Reference and Citation Copying in Evolving Bibliographic\n  Networks", "comments": "Accepted in Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive literature demonstrates how the copying of references (links) can\nlead to the emergence of various structural properties (e.g., power-law degree\ndistribution and bipartite cores) in bibliographic and other similar directed\nnetworks. However, it is also well known that the copying process is incapable\nof mimicking the number of directed triangles in such networks; neither does it\nhave the power to explain the obsolescence of older papers. In this paper, we\npropose RefOrCite, a new model that allows for copying of both the references\nfrom (i.e., out-neighbors of) as well as the citations to (i.e., in-neighbors\nof) an existing node. In contrast, the standard copying model (CP) only copies\nreferences. While retaining its spirit, RefOrCite differs from the Forest Fire\n(FF) model in ways that makes RefOrCite amenable to mean-field analysis for\ndegree distribution, triangle count, and densification. Empirically, RefOrCite\ngives the best overall agreement with observed degree distribution, triangle\ncount, diameter, h-index, and the growth of citations to newer papers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 16:31:52 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Pandey", "Pradumn Kumar", ""], ["Singh", "Mayank", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "1912.12646", "submitter": "Bo-Christer Bj\\\"ork", "authors": "Bo-Christer Bj\\\"ork", "title": "Scholarly journal publishing in transition: from restricted to open\n  access", "comments": null, "journal-ref": "Electronic Markets, The International Journal on Networked\n  Business, Vol 7, 2017,Issue 2, 101-109", "doi": "10.1007/s12525-017-0249-2", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the business models used in most segments of the media industry have\nbeen profoundly changed by the Internet surprisingly little has been changed in\nthe publishing of scholarly peer reviewed journals. Electronic delivery has\nbecome the norm, but the same publishers as before are dominating the market,\nselling content to subscribers. This article asks the question why Open Access\n(OA) to the output of mainly publicly funded research hasn't yet become the\nmainstream business model. OA implies a reversal of business logic from readers\npaying for content to authors paying fro dissemination via universa free\naccess. The current situation is analyzed using Porter's five forces model. The\nanalysis demonstrates a lack of competitive pressure in this industry, leading\nto so high profit levels of the leading publishers that they have yet to feel a\nstrong need to change the way they operate.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 13:29:59 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bj\u00f6rk", "Bo-Christer", ""]]}, {"id": "1912.13349", "submitter": "Alexandre H. Abdo", "authors": "Alexandre Hannud Abdo, Jean-Philippe Cointet, Pascale Bourret, Alberto\n  Cambrosio", "title": "Domain-topic models with chained dimensions: charting an emergent domain\n  of a major oncology conference", "comments": "44 pages, 8 figures, for supporting information see\n  <https://doi.org/10.5281/zenodo.3596036>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.data-an physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a contribution to the study of bibliographic corpora in\nthe context of science mapping. Starting from a graph representation of\ndocuments and their textual dimension, we observe that stochastic block models\n(SBMs) can provide a simultaneous clustering of documents and words that we\ncall a domain-topic model. Previous work by (Gerlach et al., 2018) investigated\nthe resulting topics, or word clusters, while ours focuses on the study of the\ndocument clusters, which we call domains. To enable the synthetic description\nand interactive navigation of domains, we introduce measures and interfaces\nrelating both types of clusters, which reflect the structure of the graph and\nthe model. We then present a procedure that, starting from the document\nclusters, extends the block model to also cluster arbitrary metadata attributes\nof the documents. We call this procedure a domain-chained model, and our\nprevious measures and interfaces can be directly transposed to read the\nmetadata clusters. We provide an example application to a corpus that is\nrelevant to current STS research, and an interesting case for our approach: the\n1995-2017 collection of abstracts presented at ASCO, the main annual oncology\nresearch conference. Through a sequence of domain-topic and domain-chained\nmodels, we identify and describe a particular group of domains in ASCO that\nhave notably grown through the last decades, and which we relate to the\nestablishment of \"oncopolicy\" as a major concern in oncology.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:17:26 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 04:48:42 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 12:07:47 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Abdo", "Alexandre Hannud", ""], ["Cointet", "Jean-Philippe", ""], ["Bourret", "Pascale", ""], ["Cambrosio", "Alberto", ""]]}]