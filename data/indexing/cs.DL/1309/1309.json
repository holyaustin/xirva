[{"id": "1309.0277", "submitter": "Antonis Sidiropoulos", "authors": "Antonis Sidiropoulos, Dimitrios Katsaros, Yannis Manolopoulos", "title": "Categorizing Influential Authors Using Penalty Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of h-index has been proposed to easily assess a researcher's\nperformance with a single two-dimensional number. However, by using only this\nsingle number, we lose significant information about the distribution of the\nnumber of citations per article of an author's publication list. Two authors\nwith the same h-index may have totally different distributions of the number of\ncitations per article. One may have a very long \"tail\" in the citation curve,\ni.e. he may have published a great number of articles, which did not receive\nrelatively many citations. Another researcher may have a short tail, i.e.\nalmost all his publications got a relatively large number of citations. In this\narticle, we study an author's citation curve and we define some areas appearing\nin this curve. These areas are used to further evaluate authors' research\nperformance from quantitative and qualitative point of view. We call these\nareas as \"penalty\" ones, since the greater they are, the more an author's\nperformance is penalized. Moreover, we use these areas to establish new metrics\naiming at categorizing researchers in two distinct categories: \"influential\"\nones vs. \"mass producers\".\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 23:35:52 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Sidiropoulos", "Antonis", ""], ["Katsaros", "Dimitrios", ""], ["Manolopoulos", "Yannis", ""]]}, {"id": "1309.0326", "submitter": "Micha{\\l} {\\L}opuszy\\'nski", "authors": "Micha{\\l} {\\L}opuszy\\'nski, {\\L}ukasz Bolikowski", "title": "Tagging Scientific Publications using Wikipedia and Natural Language\n  Processing Tools. Comparison on the ArXiv Dataset", "comments": null, "journal-ref": "Communications in Computer and Information Science Volume 416,\n  Springer 2014, pp 16-27", "doi": "10.1007/978-3-319-08425-1_3", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we compare two simple methods of tagging scientific\npublications with labels reflecting their content. As a first source of labels\nWikipedia is employed, second label set is constructed from the noun phrases\noccurring in the analyzed corpus. We examine the statistical properties and the\neffectiveness of both approaches on the dataset consisting of abstracts from\n0.7 million of scientific documents deposited in the ArXiv preprint collection.\nWe believe that obtained tags can be later on applied as useful document\nfeatures in various machine learning tasks (document similarity, clustering,\ntopic modelling, etc.).\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 09:09:27 GMT"}, {"version": "v2", "created": "Wed, 13 Aug 2014 14:30:21 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 14:48:29 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["\u0141opuszy\u0144ski", "Micha\u0142", ""], ["Bolikowski", "\u0141ukasz", ""]]}, {"id": "1309.1066", "submitter": "Mathieu Andro", "authors": "Mathieu Andro (DV/IST), Ga\\\"etan Tr\\\"oger (ENPC)", "title": "Statistiques et visibilit\\'e des biblioth\\`eques num\\'eriques : quelles\n  strat\\'egies de diffusion ?", "comments": "7 p, in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compared statistics of major digital libraries and we tried to see if\nthere is a relationship between the volume of digital libraries and online\nvisibility of each digitized document. Finally, we analyzed the consequences of\nthe diffusion strategies of French digital libraries. The statistics were\nobtained by survey, gray literature, alexa.com, and Google Trends.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 15:12:25 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Andro", "Mathieu", "", "DV/IST"], ["Tr\u00f6ger", "Ga\u00ebtan", "", "ENPC"]]}, {"id": "1309.1805", "submitter": "Lynn Zentner", "authors": "Lynn Zentner, Michael Zentner, Victoria Farnsworth, Michael McLennan,\n  Krishna Madhavan, and Gerhard Klimeck", "title": "nanoHUB.org: Experiences and Challenges in Software Sustainability for a\n  Large Scientific Community", "comments": "4 pages, 1 figure, this version contains minor revisions to correct\n  an acronym, update a quotation, improve grammar, and add a reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CE cs.DL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Managing and growing a successful cyberinfrastructure such as nanoHUB.org\npresents a variety of opportunities and challenges, particularly in regard to\nsoftware. This position paper details a number of those issues and how we have\napproached them.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 02:17:57 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 17:18:17 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Zentner", "Lynn", ""], ["Zentner", "Michael", ""], ["Farnsworth", "Victoria", ""], ["McLennan", "Michael", ""], ["Madhavan", "Krishna", ""], ["Klimeck", "Gerhard", ""]]}, {"id": "1309.1825", "submitter": "Asefeh Asemi", "authors": "Fatemeh Anari, Asefeh Asemi, Adeleh Asemi, Munir Abu Bakar", "title": "Social Interactive Media Tools and Knowledge Sharing: A Case Study", "comments": "17 pages, 23 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Social Media Tools (SMT) have provided new opportunities for\nlibraries and librarians in the world. In academic libraries, we can use of\nthem as a powerful tool for communication. This study is to determine the use\nof the social interactive media tools [Social Networking Tools (SNT), Social\nBookmarking Tools (SBT), Image or Video Sharing Tools (IVShT), and Mashup Tools\n(MT)] in disseminating knowledge and information among librarians in the\nLimerick University, Ireland. Methodology: The study was a descriptive survey.\nThe research population included all librarians in Glucksman library. A\nquestionnaire survey was done to collect data. Statistical software, SPSS16 was\nused at two levels (descriptive and inferential statistics) for data analyzing.\nFindings: The findings show that the mean (out of 5.00) of using each of SMT in\nsharing knowledge by the librarians of Glucksman library is as the following:\nSNT (2.49), SBT (2.92), IVShT (2.99) and MT (2.5). It shows that most of their\ninteraction related to share of image or video. Originality: SMT provides an\nexcellent platform for the exchange information between students, faculty\nmembers, and the librarians themselves. The Glucksman library at the University\nof Limerick is using this technology. This paper gives an example of how using\nthese tools in the field of Library and Information Science in Ireland. The\nissues expressed could be beneficial for the development of library services in\ngeneral and knowledge sharing among librarians in particular.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 06:10:09 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Anari", "Fatemeh", ""], ["Asemi", "Asefeh", ""], ["Asemi", "Adeleh", ""], ["Bakar", "Munir Abu", ""]]}, {"id": "1309.2409", "submitter": "Nicolas Robinson-Garcia", "authors": "Nicolas Robinson-Garcia, Daniel Torres-Salinas, J.M. Campanario and\n  Emilio Delgado L\\'opez-C\\'ozar", "title": "Letter to the editor: Against the Resilience of Rejected Manuscripts", "comments": "This letter is accepted for publication in the Journal of the\n  American Society for Information Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter we propose the development of guidelines by the main editors\nassociations as well as protocols within online journal management systems for\nkeeping track of rejected manuscripts that are resubmitted as well as for the\ninterchange of referees reports between journals.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 08:28:12 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Robinson-Garcia", "Nicolas", ""], ["Torres-Salinas", "Daniel", ""], ["Campanario", "J. M.", ""], ["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""]]}, {"id": "1309.2413", "submitter": "Nicolas Robinson-Garcia", "authors": "Emilio Delgado L\\'opez-C\\'ozar, Nicol\\'as Robinson-Garcia and Daniel\n  Torres-Salinas", "title": "The Google Scholar Experiment: how to index false papers and manipulate\n  bibliometric indicators", "comments": "This paper has been accepted for publication in the Journal of the\n  American Society for Information Science and Technology. It is based on a\n  previous working paper available at arXiv:1212.0638.pdf. arXiv admin note:\n  substantial text overlap with arXiv:1212.0638", "journal-ref": null, "doi": "10.1002/asi.23056", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google Scholar has been well received by the research community. Its promises\nof free, universal and easy access to scientific literature as well as the\nperception that it covers better than other traditional multidisciplinary\ndatabases the areas of the Social Sciences and the Humanities have contributed\nto the quick expansion of Google Scholar Citations and Google Scholar Metrics:\ntwo new bibliometric products that offer citation data at the individual level\nand at journal level. In this paper we show the results of a experiment\nundertaken to analyze Google Scholar's capacity to detect citation counting\nmanipulation. For this, six documents were uploaded to an institutional web\ndomain authored by a false researcher and referencing all the publications of\nthe members of the EC3 research group at the University of Granada. The\ndetection of Google Scholar of these papers outburst the citations included in\nthe Google Scholar Citations profiles of the authors. We discuss the effects of\nsuch outburst and how it could affect the future development of such products\nnot only at individual level but also at journal level, especially if Google\nScholar persists with its lack of transparency.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 08:33:57 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""], ["Robinson-Garcia", "Nicol\u00e1s", ""], ["Torres-Salinas", "Daniel", ""]]}, {"id": "1309.2434", "submitter": "Max Kemman", "authors": "Max Kemman, Martijn Kleppe, Stef Scagliola", "title": "Just Google It - Digital Research Practices of Humanities Scholars", "comments": "Final publication Please cite as Kemman, Max, Martijn Kleppe and Stef\n  Scagliola. 'Just Google It'. In: Clare Mills, Michael Pidd and Esther Ward.\n  Proceedings of the Digital Humanities Congress 2012. Studies in the Digital\n  Humanities. Sheffield: HRI Online Publications, 2014. Available online at:\n  <http://www.hrionline.ac.uk/openbook/chapter/dhc2012-kemman>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transition from analogue to digital archives and the recent explosion of\nonline content offers researchers novel ways of engaging with data. The crucial\nquestion for ensuring a balance between the supply and demand-side of data, is\nwhether this trend connects to existing scholarly practices and to the average\nsearch skills of researchers. To gain insight into this process we conducted a\nsurvey among nearly three hundred (N= 288) humanities scholars in the\nNetherlands and Belgium with the aim of finding answers to the following\nquestions: 1) To what extent are digital databases and archives used? 2) What\nare the preferences in search functionalities 3) Are there differences in\nsearch strategies between novices and experts of information retrieval? Our\nresults show that while scholars actively engage in research online they mainly\nsearch for text and images. General search systems such as Google and JSTOR are\npredominant, while large-scale collections such as Europeana are rarely\nconsulted. Searching with keywords is the dominant search strategy and advanced\nsearch options are rarely used. When comparing novice and more experienced\nsearchers, the first tend to have a more narrow selection of search engines,\nand mostly use keywords. Our overall findings indicate that Google is the key\nplayer among available search engines. This dominant use illustrates the\nparadoxical attitude of scholars toward Google: while provenance and context\nare deemed key academic requirements, the workings of the Google algorithm\nremain unclear. We conclude that Google introduces a black box into digital\nscholarly practices, indicating scholars will become increasingly dependent on\nsuch black boxed algorithms. This calls for a reconsideration of the academic\nprinciples of provenance and context.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 09:52:44 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 11:20:14 GMT"}, {"version": "v3", "created": "Tue, 22 Apr 2014 13:50:10 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Kemman", "Max", ""], ["Kleppe", "Martijn", ""], ["Scagliola", "Stef", ""]]}, {"id": "1309.2486", "submitter": "Min Song", "authors": "Ying Ding, Min Song, Jia Han, Qi Yu, Erjia Yan, Lili Lin, Tamy\n  Chambers", "title": "Entitymetrics: Measuring the Impact of Entities", "comments": null, "journal-ref": "PLOS ONE 8(8): e71416, 2013", "doi": "10.1371/journal.pone.0071416", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes entitymetrics to measure the impact of knowledge units.\nEntitymetrics highlight the importance of entities embedded in scientific\nliterature for further knowledge discovery. In this paper, we use Metformin, a\ndrug for diabetes, as an example to form an entity-entity citation network\nbased on literature related to Metformin. We then calculate the network\nfeatures and compare the centrality ranks of biological entities with results\nfrom Comparative Toxicogenomics Database (CTD). The comparison demonstrates the\nusefulness of entitymetrics to detect most of the outstanding interactions\nmanually curated in CTD.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 12:45:47 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Ding", "Ying", ""], ["Song", "Min", ""], ["Han", "Jia", ""], ["Yu", "Qi", ""], ["Yan", "Erjia", ""], ["Lin", "Lili", ""], ["Chambers", "Tamy", ""]]}, {"id": "1309.2546", "submitter": "Erjia Yan", "authors": "Erjia Yan", "title": "Finding knowledge paths among scientific disciplines", "comments": "31 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discovers patterns of knowledge dissemination among scientific\ndisciplines. While the transfer of knowledge is largely unobservable, citations\nfrom one discipline to another have been proven to be an effective proxy to\nstudy disciplinary knowledge flow. This study constructs a knowledge flow\nnetwork in that a node represents a Journal Citation Report subject category\nand a link denotes the citations from one subject category to another. Using\nthe concept of shortest path, several quantitative measurements are proposed\nand applied to a knowledge flow network. Based on an examination of subject\ncategories in Journal Citation Report, this paper finds that social science\ndomains tend to be more self-contained and thus it is more difficult for\nknowledge from other domains to flow into them; at the same time, knowledge\nfrom science domains, such as biomedicine-, chemistry-, and physics-related\ndomains can access and be accessed by other domains more easily. This paper\nalso finds that social science domains are more disunified than science\ndomains, as three fifths of the knowledge paths from one social science domain\nto another need at least one science domain to serve as an intermediate. This\npaper contributes to discussions on disciplinarity and interdisciplinarity by\nproviding empirical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 15:23:01 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Yan", "Erjia", ""]]}, {"id": "1309.2648", "submitter": "Hany SalahEldeen", "authors": "Hany M. SalahEldeen and Michael L. Nelson", "title": "Resurrecting My Revolution: Using Social Link Neighborhood in Bringing\n  Context to the Disappearing Web", "comments": "Published IN TPDL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work we reported that resources linked in tweets disappeared at\nthe rate of 11% in the first year followed by 7.3% each year afterwards. We\nalso found that in the first year 6.7%, and 14.6% in each subsequent year, of\nthe resources were archived in public web archives. In this paper we revisit\nthe same dataset of tweets and find that our prior model still holds and the\ncalculated error for estimating percentages missing was about 4%, but we found\nthe rate of archiving produced a higher error of about 11.5%. We also\ndiscovered that resources have disappeared from the archives themselves (7.89%)\nas well as reappeared on the live web after being declared missing (6.54%). We\nhave also tested the availability of the tweets themselves and found that\n10.34% have disappeared from the live web. To mitigate the loss of resources on\nthe live web, we propose the use of a \"tweet signature\". Using the Topsy API,\nwe extract the top five most frequent terms from the union of all tweets about\na resource, and use these five terms as a query to Google. We found that using\ntweet signatures results in discovering replacement resources with 70+% textual\nsimilarity to the missing resource 41% of the time.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 20:00:55 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["SalahEldeen", "Hany M.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1309.2788", "submitter": "Andrea Scharnhorst", "authors": "Ingrid Dillo and Rene van Horik and Andrea Scharnhorst", "title": "Training in Data Curation as Service in a Federated Data Infrastructure\n  - the FrontOffice-BackOffice Model", "comments": "TPDL 2013, accepted for workshop Education in Data Curation, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing volume and importance of research data leads to the emergence\nof research data infrastructures in which data management plays an important\nrole. As a consequence, practices at digital archives and libraries change. In\nthis paper, we focus on a possible alliance between archives and libraries\naround training activities in data curation. We introduce a so-called\n\\emph{FrontOffice--BackOffice model} and discuss experiences of its\nimplementation in the Netherlands. In this model, an efficient division of\ntasks relies on a distributed infrastructure in which research institutions\n(i.e., universities) use centralized storage and data curation services\nprovided by national research data archives. The training activities are aimed\nat information professionals working at those research institutions, for\ninstance as digital librarians. We describe our experiences with the course\n\\emph{DataIntelligence4Librarians}. Eventually, we reflect about the\ninternational dimension of education and training around data curation and\nstewardship.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 11:19:50 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Dillo", "Ingrid", ""], ["van Horik", "Rene", ""], ["Scharnhorst", "Andrea", ""]]}, {"id": "1309.2797", "submitter": "Hiroyasu Inoue Dr.", "authors": "Hiroyasu Inoue and Yang-Yu Liu", "title": "Revealing the intricate effect of collaboration on innovation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Japan and U.S. patent records of several decades to demonstrate\nthe effect of collaboration on innovation. We find that statistically inventor\nteams slightly outperform solo inventors while company teams perform equally\nwell as solo companies. By tracking the performance record of individual teams\nwe find that inventor teams' performance generally degrades with more repeat\ncollaborations. Though company teams' performance displays strongly bursty\nbehavior, long-term collaboration does not significantly help innovation at\nall. To systematically study the effect of repeat collaboration, we define the\nrepeat collaboration number of a team as the average number of collaborations\nover all the teammate pairs. We find that mild repeat collaboration improves\nthe performance of Japanese inventor teams and U.S. company teams. Yet,\nexcessive repeat collaboration does not significantly help innovation at both\nthe inventor and company levels in both countries. To control for unobserved\nheterogeneity, we perform a detailed regression analysis and the results are\nconsistent with our simple observations. The presented results reveal the\nintricate effect of collaboration on innovation, which may also be observed in\nother creative projects.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 09:05:54 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Inoue", "Hiroyasu", ""], ["Liu", "Yang-Yu", ""]]}, {"id": "1309.3197", "submitter": "Arthur Carvalho", "authors": "Arthur Carvalho, Stanko Dimitrov, Kate Larson", "title": "Inducing Honest Reporting Without Observing Outcomes: An Application to\n  the Peer-Review Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DL math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When eliciting opinions from a group of experts, traditional devices used to\npromote honest reporting assume that there is an observable future outcome. In\npractice, however, this assumption is not always reasonable. In this paper, we\npropose a scoring method built on strictly proper scoring rules to induce\nhonest reporting without assuming observable outcomes. Our method provides\nscores based on pairwise comparisons between the reports made by each pair of\nexperts in the group. For ease of exposition, we introduce our scoring method\nby illustrating its application to the peer-review process. In order to do so,\nwe start by modeling the peer-review process using a Bayesian model where the\nuncertainty regarding the quality of the manuscript is taken into account.\nThereafter, we introduce our scoring method to evaluate the reported reviews.\nUnder the assumptions that reviewers are Bayesian decision-makers and that they\ncannot influence the reviews of other reviewers, we show that risk-neutral\nreviewers strictly maximize their expected scores by honestly disclosing their\nreviews. We also show how the group's scores can be used to find a consensual\nreview. Experimental results show that encouraging honest reporting through the\nproposed scoring method creates more accurate reviews than the traditional\npeer-review process.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 15:34:21 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 13:39:51 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Carvalho", "Arthur", ""], ["Dimitrov", "Stanko", ""], ["Larson", "Kate", ""]]}, {"id": "1309.3323", "submitter": "Ted Underwood", "authors": "Ted Underwood, Michael L. Black, Loretta Auvil, Boris Capitanu", "title": "Mapping Mutable Genres in Structurally Complex Volumes", "comments": "Preprint accepted for the 2013 IEEE International Conference on Big\n  Data. Revised to include corroborating evidence from a smaller workset", "journal-ref": null, "doi": "10.1109/BigData.2013.6691676", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mine large digital libraries in humanistically meaningful ways, scholars\nneed to divide them by genre. This is a task that classification algorithms are\nwell suited to assist, but they need adjustment to address the specific\nchallenges of this domain. Digital libraries pose two problems of scale not\nusually found in the article datasets used to test these algorithms. 1) Because\nlibraries span several centuries, the genres being identified may change\ngradually across the time axis. 2) Because volumes are much longer than\narticles, they tend to be internally heterogeneous, and the classification task\nneeds to begin with segmentation. We describe a multi-layered solution that\ntrains hidden Markov models to segment volumes, and uses ensembles of\noverlapping classifiers to address historical change. We test this approach on\na collection of 469,200 volumes drawn from HathiTrust Digital Library. To\ndemonstrate the humanistic value of these methods, we extract 32,209 volumes of\nfiction from the digital library, and trace the changing proportions of first-\nand third-person narration in the corpus. We note that narrative points of view\nseem to have strong associations with particular themes and genres.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 22:27:59 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2013 17:37:27 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Underwood", "Ted", ""], ["Black", "Michael L.", ""], ["Auvil", "Loretta", ""], ["Capitanu", "Boris", ""]]}, {"id": "1309.4008", "submitter": "Ahmed AlSum", "authors": "Ahmed AlSum, Michele C. Weigle, Michael L. Nelson, Herbert Van de\n  Sompel", "title": "Profiling Web Archive Coverage for Top-Level Domain and Content Language", "comments": "Appeared in TPDL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Memento aggregator currently polls every known public web archive when\nserving a request for an archived web page, even though some web archives focus\non only specific domains and ignore the others. Similar to query routing in\ndistributed search, we investigate the impact on aggregated Memento TimeMaps\n(lists of when and where a web page was archived) by only sending queries to\narchives likely to hold the archived page. We profile twelve public web\narchives using data from a variety of sources (the web, archives' access logs,\nand full-text queries to archives) and discover that only sending queries to\nthe top three web archives (i.e., a 75% reduction in the number of queries) for\nany request produces the full TimeMaps on 84% of the cases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 15:39:35 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["AlSum", "Ahmed", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""], ["Van de Sompel", "Herbert", ""]]}, {"id": "1309.4009", "submitter": "Yasmin AlNoamany", "authors": "Yasmin AlNoamany, Michele C. Weigle, Michael L. Nelson", "title": "Access Patterns for Robots and Humans in Web Archives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although user access patterns on the live web are well-understood, there has\nbeen no corresponding study of how users, both humans and robots, access web\narchives. Based on samples from the Internet Archive's public Wayback Machine,\nwe propose a set of basic usage patterns: Dip (a single access), Slide (the\nsame page at different archive times), Dive (different pages at approximately\nthe same archive time), and Skim (lists of what pages are archived, i.e.,\nTimeMaps). Robots are limited almost exclusively to Dips and Skims, but human\naccesses are more varied between all four types. Robots outnumber humans 10:1\nin terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of\nmegabytes transferred. Robots almost always access TimeMaps (95% of accesses),\nbut humans predominately access the archived web pages themselves (82% of\naccesses). In terms of unique archived web pages, there is no overall\npreference for a particular time, but the recent past (within the last year)\nshows significant repeat accesses.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 15:40:16 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["AlNoamany", "Yasmin", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1309.4016", "submitter": "Yasmin AlNoamany", "authors": "Yasmin AlNoamany, Ahmed AlSum, Michele C. Weigle, and Michael L.\n  Nelson", "title": "Who and What Links to the Internet Archive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet Archive's (IA) Wayback Machine is the largest and oldest public\nweb archive and has become a significant repository of our recent history and\ncultural heritage. Despite its importance, there has been little research about\nhow it is discovered and used. Based on web access logs, we analyze what users\nare looking for, why they come to IA, where they come from, and how pages link\nto IA. We find that users request English pages the most, followed by the\nEuropean languages. Most human users come to web archives because they do not\nfind the requested pages on the live web. About 65% of the requested archived\npages no longer exist on the live web. We find that more than 82% of human\nsessions connect to the Wayback Machine via referrals from other web sites,\nwhile only 15% of robots have referrers. Most of the links (86%) from websites\nare to individual archived pages at specific points in time, and of those 83%\nno longer exist on the live web.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 16:01:37 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["AlNoamany", "Yasmin", ""], ["AlSum", "Ahmed", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1309.4932", "submitter": "Andrew N. Jackson", "authors": "Angela Dappert, Andrew N. Jackson, Akiko Kimura", "title": "Developing a Robust Migration Workflow for Preserving and Curating\n  Hand-held Media", "comments": "11 pages, presented at iPres 2011. Also publishing in corresponding\n  conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many memory institutions hold large collections of hand-held media, which can\ncomprise hundreds of terabytes of data spread over many thousands of\ndata-carriers. Many of these carriers are at risk of significant physical\ndegradation over time, depending on their composition. Unfortunately, handling\nthem manually is enormously time consuming and so a full and frequent\nevaluation of their condition is extremely expensive. It is, therefore,\nimportant to develop scalable processes for stabilizing them onto backed-up\nonline storage where they can be subject to highquality digital preservation\nmanagement. This goes hand in hand with the need to establish efficient,\nstandardized ways of recording metadata and to deal with defective\ndata-carriers. This paper discusses processing approaches, workflows, technical\nset-up, software solutions and touches on staffing needs for the stabilization\nprocess. We have experimented with different disk copying robots, defined our\nmetadata, and addressed storage issues to scale stabilization to the vast\nquantities of digital objects on hand-held data-carriers that need to be\npreserved. Working closely with the content curators, we have been able to\nbuild a robust data migration workflow and have stabilized over 16 terabytes of\ndata in a scalable and economical manner.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 11:10:13 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Dappert", "Angela", ""], ["Jackson", "Andrew N.", ""], ["Kimura", "Akiko", ""]]}, {"id": "1309.4962", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk and Josef Urban", "title": "HOL(y)Hammer: Online ATP Service for HOL Light", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG cs.LO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable)\nmathematics encoded in the HOL Light system. The service allows its users to\nupload and automatically process an arbitrary formal development (project)\nbased on HOL Light, and to attack arbitrary conjectures that use the concepts\ndefined in some of the uploaded projects. For that, the service uses several\nautomated reasoning systems combined with several premise selection methods\ntrained on all the project proofs. The projects that are readily available on\nthe server for such query answering include the recent versions of the\nFlyspeck, Multivariate Analysis and Complex Analysis libraries. The service\nruns on a 48-CPU server, currently employing in parallel for each task 7 AI/ATP\ncombinations and 4 decision procedures that contribute to its overall\nperformance. The system is also available for local installation by interested\nusers, who can customize it for their own proof development. An Emacs interface\nallowing parallel asynchronous queries to the service is also provided. The\noverall structure of the service is outlined, problems that arise and their\nsolutions are discussed, and an initial account of using the system is given.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 13:22:31 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1309.5256", "submitter": "Arnim Bleier", "authors": "Andreas Strotmann and Arnim Bleier", "title": "Author Name Co-Mention Analysis: Testing a Poor Man's Author Co-Citation\n  Analysis Method", "comments": "14th International Society of Scientometrics and Informetrics\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a social science information service for the German language countries, we\ndocument research projects, publications, and data in relevant fields. At the\nsame time, we aim to provide well-founded bibliometric studies of these fields.\nPerforming a citation analysis on an area of the German social sciences is,\nhowever, a serious challenge given the low and likely significantly biased\ncoverage of these fields in the standard citation databases. Citations, and\nespecially author citations, play a highly significant role in that literature,\nhowever. In this work in progress, we report preliminary methods and results\nfor an author name co-mention analysis of a large fragment of a particularly\ninteresting corpus of German sociology: a quarter century's worth of the\nfull-text proceedings of the Deutsche Gesellschaft fuer Soziologie (DGS), which\ncelebrated its 100th anniversary meeting in 2012. Results are encouraging for\nthis poor cousin of author co-citation analysis, but considerable refinements,\nespecially of the underlying computational infrastructure for full-text\nanalysis, appear advisable for full-scale deployment of this method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 12:54:17 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Strotmann", "Andreas", ""], ["Bleier", "Arnim", ""]]}, {"id": "1309.5275", "submitter": "Dan Stowell", "authors": "Dan Stowell and Mark D. Plumbley", "title": "An open dataset for research on audio field recording archives:\n  freefield1010", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.DL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We introduce a free and open dataset of 7690 audio clips sampled from the\nfield-recording tag in the Freesound audio archive. The dataset is designed for\nuse in research related to data mining in audio archives of field recordings /\nsoundscapes. Audio is standardised, and audio and metadata are Creative Commons\nlicensed. We describe the data preparation process, characterise the dataset\ndescriptively, and illustrate its use through an auto-tagging experiment.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 14:12:04 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 21:29:13 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Stowell", "Dan", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1309.5372", "submitter": "Reagan Moore", "authors": "Reagan W. Moore", "title": "Extensible Generic Data Management Software", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Extensibility mechanisms constitute a form of knowledge capture that is\nessential for software re-use. The Data Intensive Cyber Environments (DICE)\ngroup has collaborated with twenty-five science and engineering domains on the\napplication of the iRODS policy-based data management system. Based on these\ncollaborations, three types of extensibility mechanisms are sufficient to\ncapture the domain knowledge needed for interaction with domain resources:\ncomputer actionable rules that control management policies; computer executable\nmicro-services that encapsulate operations or interaction protocols; and\nmiddleware servers that apply standard operations at remote locations. These\nmechanisms enable the creation of generic data management software that is\ncapable of implementing collections, data grids for sharing data, digital\nlibraries for publishing data, processing pipelines, and archives.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 20:43:06 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Moore", "Reagan W.", ""]]}, {"id": "1309.5503", "submitter": "Scott Ainsworth", "authors": "Scott G. Ainsworth and Michael L. Nelson", "title": "Evaluating Sliding and Sticky Target Policies by Measuring Temporal\n  Drift in Acyclic Walks Through a Web Archive", "comments": "10 pages, JCDL 2013", "journal-ref": null, "doi": "10.1145/2467696.2467718", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a user views an archived page using the archive's user interface (UI),\nthe user selects a datetime to view from a list. The archived web page, if\navailable, is then displayed. From this display, the web archive UI attempts to\nsimulate the web browsing experience by smoothly transitioning between archived\npages. During this process, the target datetime changes with each link\nfollowed; drifting away from the datetime originally selected. When browsing\nsparsely-archived pages, this nearly-silent drift can be many years in just a\nfew clicks. We conducted 200,000 acyclic walks of archived pages, following up\nto 50 links per walk, comparing the results of two target datetime policies.\nThe Sliding Target policy allows the target datetime to change as it does in\narchive UIs such as the Internet Archive's Wayback Machine. The Sticky Target\npolicy, represented by the Memento API, keeps the target datetime the same\nthroughout the walk. We found that the Sliding Target policy drift increases\nwith the number of walk steps, number of domains visited, and choice (number of\nlinks available). However, the Sticky Target policy controls temporal drift,\nholding it to less than 30 days on average regardless of walk length or number\nof domains visited. The Sticky Target policy shows some increase as choice\nincreases, but this may be caused by other factors. We conclude that based on\nwalk length, the Sticky Target policy generally produces at least 30 days less\ndrift than the Sliding Target policy.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 17:27:04 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Ainsworth", "Scott G.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1309.5680", "submitter": "Xin-Jian Xu", "authors": "Zheng Yao, Xiao-Long Peng, Li-Jie Zhang, Xin-Jian Xu", "title": "Modeling nonuniversal citation distributions: the role of scientific\n  journals", "comments": "14 pages, 6 figures, Latex", "journal-ref": "J. Stat. Mech. (2014) P04029", "doi": "10.1088/1742-5468/2014/04/P04029", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether a scientific paper is cited is related not only to the influence of\nits author(s) but also to the journal publishing it. Scientists, either\nproficient or tender, usually submit their most important work to prestigious\njournals which receives higher citations than the ordinary. How to model the\nrole of scientific journals in citation dynamics is of great importance. In\nthis paper we address this issue through two folds. One is the intrinsic\nheterogeneity of a paper determined by the impact factor of the journal\npublishing it. The other is the mechanism of a paper being cited which depends\non its citations and prestige. We develop a model for citation networks via an\nintrinsic nodal weight function and an intuitive ageing mechanism. The node's\nweight is drawn from the distribution of impact factors of journals and the\nageing transition is a function of the citation and the prestige. The\nnode-degree distribution of resulting networks shows nonuniversal scaling: the\ndistribution decays exponentially for small degree and has a power-law tail for\nlarge degree, hence the dual behaviour. The higher the impact factor of the\njournal, the larger the tipping point and the smaller the power exponent that\nare obtained. With the increase of the journal rank, this phenomenon will fade\nand evolve to pure power laws.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 01:41:07 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2014 02:04:54 GMT"}, {"version": "v3", "created": "Fri, 9 May 2014 02:17:41 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Yao", "Zheng", ""], ["Peng", "Xiao-Long", ""], ["Zhang", "Li-Jie", ""], ["Xu", "Xin-Jian", ""]]}, {"id": "1309.5706", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff, Lutz Bornmann, Werner Marx, and Sta\\v{s}a\n  Milojevi\\'c", "title": "Referenced Publication Years Spectroscopy applied to iMetrics:\n  Scientometrics, Journal of Informetrics, and a relevant subset of JASIST", "comments": "Journal of Informetrics, accepted for publication (20 Nov. 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a (freeware) routine for \"referenced publication years\nspectroscopy\" (RPYS) and apply this method to the historiography of \"iMetrics,\"\nthat is, the junction of the journals Scientometrics, Informetrics, and the\nrelevant subset of JASIST (approx. 20%) that shapes the intellectual space for\nthe development of information metrics (bibliometrics, scientometrics,\ninformetrics, and webometrics). The application to information metrics (our own\nfield of research) provides us with the opportunity to validate this\nmethodology, and to add a reflection about using citations for the historical\nreconstruction. The results show that the field is rooted in individual\ncontributions of the 1920s-1950s (e.g., Alfred J. Lotka), and was then shaped\nintellectually in the early 1960s by a confluence of the history of science\n(Derek de Solla Price), documentation (e.g., Michael M. Kessler's\n\"bibliographic coupling\"), and \"citation indexing\" (Eugene Garfield).\nInstitutional development at the interfaces between science studies and\ninformation science has been reinforced by the new journal Informetrics since\n2007. In a concluding reflection, we return to the question of how the\nhistoriography of science using algorithmic means--in terms of citation\npractices--can be different from an intellectual history of the field based,\nfor example, on reading source materials.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 06:44:24 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 08:46:45 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Bornmann", "Lutz", ""], ["Marx", "Werner", ""], ["Milojevi\u0107", "Sta\u0161a", ""]]}, {"id": "1309.6527", "submitter": "Yordan Kalmukov", "authors": "Yordan Kalmukov", "title": "Describing Papers and Reviewers' Competences by Taxonomy of Keywords", "comments": null, "journal-ref": "Computer Science and Information Systems, Vol. 9(2), pp. 763-789,\n  2012, ISSN 1820-0214", "doi": "10.2298/CSIS110906012K", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the importance of the precise calculation of\nsimilarity factors between papers and reviewers for performing a fair and\naccurate automatic assignment of reviewers to papers. It suggests that papers\nand reviewers' competences should be described by taxonomy of keywords so that\nthe implied hierarchical structure allows similarity measures to take into\naccount not only the number of exactly matching keywords, but in case of\nnon-matching ones to calculate how semantically close they are. The paper also\nsuggests a similarity measure derived from the well-known and widely-used\nDice's coefficient, but adapted in a way it could be also applied between sets\nwhose elements are semantically related to each other (as concepts in taxonomy\nare). It allows a non-zero similarity factor to be accurately calculated\nbetween a paper and a reviewer even if they do not share any keyword in common.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 14:43:31 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Kalmukov", "Yordan", ""]]}, {"id": "1309.6650", "submitter": "Adrian Paschke", "authors": "Haytham Al-Feel, Ralph Schafermeier, Adrian Paschke", "title": "An Inter-lingual Reference Approach For Multi-Lingual Ontology Matching", "comments": "http://www.ijcsi.org/papers/IJCSI-10-2-1-497-503.pdf", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 2, No 1, March 2013 ISSN (Print): 1694-0814 | ISSN (Online): 1694-0784", "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies are considered as the backbone of the Semantic Web. With the\nrising success of the Semantic Web, the number of participating communities\nfrom different countries is constantly increasing. The growing number of\nontologies available in different natural languages leads to an\ninteroperability problem. In this paper, we discuss several approaches for\nontology matching; examine similarities and differences, identify weaknesses,\nand compare the existing automated approaches with the manual approaches for\nintegrating multilingual ontologies. In addition to that, we propose a new\narchitecture for a multilingual ontology matching service. As a case study we\nused an example of two multilingual enterprise ontologies - the university\nontology of Freie Universitaet Berlin and the ontology for Fayoum University in\nEgypt.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 20:13:28 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Al-Feel", "Haytham", ""], ["Schafermeier", "Ralph", ""], ["Paschke", "Adrian", ""]]}, {"id": "1309.7099", "submitter": "Lawrence Cram", "authors": "Domingo Docampo and Lawrence Cram", "title": "On the Internal Dynamics of the Shanghai Ranking", "comments": null, "journal-ref": null, "doi": "10.1007/s11192-013-1143-0", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Academic Ranking of World Universities (ARWU) published by researchers at\nShanghai Jiao Tong University has become a major source of information for\nuniversity administrators, country officials, students and the public at large.\nRecent discoveries regarding its internal dynamics allow the inversion of\npublished ARWU indicator scores to reconstruct raw scores for five hundred\nworld class universities. This paper explores raw scores in the ARWU and in\nother contests to contrast the dynamics of rank-driven and score-driven tables,\nand to explain why the ARWU ranking is a score-driven procedure. We show that\nthe ARWU indicators constitute sub-scales of a single factor accounting for\nresearch performance, and provide an account of the system of gains and\nnon-linearities used by ARWU. The paper discusses the non-linearities selected\nby ARWU, concluding that they are designed to represent the regressive\ncharacter of indicators measuring research performance. We propose that the\nutility and usability of the ARWU could be greatly improved by replacing the\nunwanted dynamical effects of the annual re-scaling based on raw scores of the\nbest performers.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 01:47:37 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Docampo", "Domingo", ""], ["Cram", "Lawrence", ""]]}, {"id": "1309.7640", "submitter": "Georgios Pitsilis", "authors": "Mohamed El-Hadedy, Georgios Pitsilis, Svein J. Knapskog", "title": "An Efficient Authorship Protection Scheme for Shared Multimedia Content", "comments": "Extensive technical report of paper published in Sixth International\n  Conference on Image and Graphics (ICIG), pp.914-919, Hefei, Anhui, China,\n  August 12-15, 2011. ISBN: 978-0-7695-4541-7", "journal-ref": null, "doi": "10.1109/ICIG.2011.183", "report-no": null, "categories": "cs.MM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many electronic content providers today like Flickr and Google, offer space\nto users to publish their electronic media (e.g. photos and videos) in their\ncloud infrastructures, so that they can be publicly accessed. Features like\nincluding other information, such as keywords or owner information into the\ndigital material is already offered by existing providers. Despite the useful\nfeatures made available to users by such infrastructures, the authorship of the\npublished content is not protected against various attacks such as compression.\nIn this paper we propose a robust scheme that uses digital invisible\nwatermarking and hashing to protect the authorship of the digital content and\nprovide resistance against malicious manipulation of multimedia content. The\nscheme is enhanced by an algorithm called MMBEC, that is an extension of an\nestablished scheme MBEC, towards higher resistance.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 19:17:18 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["El-Hadedy", "Mohamed", ""], ["Pitsilis", "Georgios", ""], ["Knapskog", "Svein J.", ""]]}, {"id": "1309.7747", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann", "title": "Research misconduct: definitions, manifestations and extent", "comments": "Accepted for publication in publications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the international scientific community has been rocked by a\nnumber of serious cases of research misconduct. In one of these, Woo Suk Hwang,\na Korean stem cell researcher published two articles on research with\nground-breaking results in Science in 2004 and 2005. Both articles were later\nrevealed to be fakes. This paper provides an overview of what research\nmisconduct is generally understood to be, its manifestations and the extent to\nwhich they are thought to exist.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 08:13:42 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Bornmann", "Lutz", ""]]}, {"id": "1309.7949", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Peter Mutschke", "title": "Bibliometric-enhanced Retrieval Models for Big Scholarly Information\n  Systems", "comments": "4 pages, IEEE BigData 2013, Workshop on Scholarly Big Data:\n  Challenges and Ideas", "journal-ref": null, "doi": "10.1109/BigData.2013.6691762", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliometric techniques are not yet widely used to enhance retrieval\nprocesses in digital libraries, although they offer value-added effects for\nusers. In this paper we will explore how statistical modelling of scholarship,\nsuch as Bradfordizing or network analysis of coauthorship network, can improve\nretrieval services for specific communities, as well as for large, cross-domain\nlarge collections. This paper aims to raise awareness of the missing link\nbetween information retrieval (IR) and bibliometrics / scientometrics and to\ncreate a common ground for the incorporation of bibliometric-enhanced services\ninto retrieval at the digital library interface.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 18:32:14 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Mayr", "Philipp", ""], ["Mutschke", "Peter", ""]]}]