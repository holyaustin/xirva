[{"id": "2001.00350", "submitter": "Brian Ryu", "authors": "Brian K. Ryu", "title": "The Demise of Single-Authored Publications in Computer Science: A\n  Citation Network Analysis", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, I analyze the DBLP bibliographic database to study role of\nsingle author publications in the computer science literature between 1940 and\n2019. I examine the demographics and reception by computing the population\nfraction, citation statistics, and PageRank scores of single author\npublications over the years. Both the population fraction and reception have\nbeen continuously declining since the 1940s. The overall decaying trend of\nsingle author publications is qualitatively consistent with those observed in\nother scientific disciplines, though the diminution is taking place several\ndecades later than those in the natural sciences. Additionally, I analyze the\nscope and volume of single author publications, using page length and reference\ncount as first-order approximations of the scope of publications. Although both\nmetrics on average show positive correlations with citation count, single\nauthor papers show no significant difference in page or reference counts\ncompared to the rest of the publications, suggesting that there exist other\nfactors that impact the citations of single author publications.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 07:47:44 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Ryu", "Brian K.", ""]]}, {"id": "2001.00484", "submitter": "Markus Konkol", "authors": "Markus Konkol, Daniel N\\\"ust, Laura Goulier", "title": "Publishing computational research -- A review of infrastructures for\n  reproducible and transparent scholarly communication", "comments": "8 pages, 2 tables", "journal-ref": "Res Integr Peer Rev 5, 10 (2020)", "doi": "10.1186/s41073-020-00095-y", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The trend toward open science increases the pressure on authors to provide\naccess to the source code and data they used to compute the results reported in\ntheir scientific papers. Since sharing materials reproducibly is challenging,\nseveral projects have developed solutions to support the release of executable\nanalyses alongside articles. We reviewed 11 applications that can assist\nresearchers in adhering to reproducibility principles. The applications were\nfound through a literature search and interactions with the reproducible\nresearch community. An application was included in our analysis if it was\nactively maintained at the time the data for this paper was collected, supports\nthe publication of executable code and data, is connected to the scholarly\npublication process. By investigating the software documentation and published\narticles, we compared the applications across 19 criteria, e.g. features that\nsupport authors in creating and readers in studying executable papers. From the\n11 applications, eight allow publishers to self-host the system for free,\nwhereas three provide paid services. Authors can submit an executable analysis\nusing Jupyter Notebooks or R Markdown documents (10 applications support these\nformats). All approaches provide features to assist readers in studying the\nmaterials, e.g., one-click reproducible results or tools for manipulating the\nanalysis parameters. Six applications allow for modifying materials after\npublication. The applications support authors to publish reproducible research\npredominantly with literate programming. Concerning readers, most applications\nprovide user interfaces to inspect and manipulate the computational analysis.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 15:16:54 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 08:11:13 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Konkol", "Markus", ""], ["N\u00fcst", "Daniel", ""], ["Goulier", "Laura", ""]]}, {"id": "2001.01002", "submitter": "Jordan Dworkin", "authors": "Jordan D. Dworkin, Kristin A. Linn, Erin G. Teich, Perry Zurn, Russell\n  T. Shinohara, Danielle S. Bassett", "title": "The extent and drivers of gender imbalance in neuroscience reference\n  lists", "comments": null, "journal-ref": "Nature Neuroscience 23 (2020) 918-926", "doi": "10.1038/s41593-020-0658-y", "report-no": null, "categories": "cs.SI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like many scientific disciplines, neuroscience has increasingly attempted to\nconfront pervasive gender imbalances within the field. While much of the\nconversation has centered around publishing and conference participation,\nrecent research in other fields has called attention to the prevalence of\ngender bias in citation practices. Because of the downstream effects that\ncitations can have on visibility and career advancement, understanding and\neliminating gender bias in citation practices is vital for addressing inequity\nin a scientific community. In this study, we sought to determine whether there\nis evidence of gender bias in the citation practices of neuroscientists. Using\ndata from five top neuroscience journals, we find that reference lists tend to\ninclude more papers with men as first and last author than would be expected if\ngender were not a factor in referencing. Importantly, we show that this\novercitation of men and undercitation of women is driven largely by the\ncitation practices of men, and is increasing over time as the field becomes\nmore diverse. We develop a co-authorship network to assess homophily in\nresearchers' social networks, and we find that men tend to overcite men even\nwhen their social networks are representative. We discuss possible mechanisms\nand consider how individual researchers might address these findings in their\nown practices.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 22:15:46 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 20:24:41 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Dworkin", "Jordan D.", ""], ["Linn", "Kristin A.", ""], ["Teich", "Erin G.", ""], ["Zurn", "Perry", ""], ["Shinohara", "Russell T.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "2001.01040", "submitter": "Sourish Das", "authors": "Bharathi Manjula .K and Sourish Das and Jehadeesan .R", "title": "Causal Impact of Web Browsing and Other Factors on Research Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the causal impact of the web-search activity on the\nresearch publication. We considered observational prospective study design,\nwhere research activity of 267 scientists is being studied. We considered the\nPoisson and negative binomial regression model for our analysis. Based on the\nAkaike's Model selection criterion, we found the negative binomial regression\nperforms better than the Poisson regression. Detailed analysis indicates that\nthe higher web-search activity of 2016 related to the sci-indexed website has a\npositive significant impact on the research publication of 2017. We observed\nthat unique collaborations of 2016 and web-search activity of 2016 have a\nnon-linear but significant positive impact on the research publication of 2017.\nWhat-if analysis indicates the high web browsing activity leads to more number\nof the publication. However, interestingly we see a scientist with low web\nactivity can be as productive as others if her/his maximum hits are the\nsci-indexed journal. That is if the scientist uses web browsing only for\nresearch-related activity, then she/he can be equally productive even if\nher/his web activity is lower than fellow scientists.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 05:43:44 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["K", "Bharathi Manjula .", ""], ["Das", "Sourish", ""], ["R", "Jehadeesan .", ""]]}, {"id": "2001.01673", "submitter": "Jan R\\\"orden", "authors": "Jan R\\\"orden (1), Doris Gruber (2), Martin Krickl (3), Bernhard\n  Haslhofer (1) ((1) AIT Austrian Insitute of Technology, (2) Austrian Academy\n  of Sciences, (3) Austrian National Library)", "title": "Identifying Historical Travelogues in Large Text Corpora Using Machine\n  Learning", "comments": "14 pages, accepted for presentation at iConference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Travelogues represent an important and intensively studied source for\nscholars in the humanities, as they provide insights into people, cultures, and\nplaces of the past. However, existing studies rarely utilize more than a dozen\nprimary sources, since the human capacities of working with a large number of\nhistorical sources are naturally limited. In this paper, we define the notion\nof travelogue and report upon an interdisciplinary method that, using machine\nlearning as well as domain knowledge, can effectively identify German\ntravelogues in the digitized inventory of the Austrian National Library with F1\nscores between 0.94 and 1.00. We applied our method on a corpus of 161,522\nGerman volumes and identified 345 travelogues that could not be identified\nusing traditional search methods, resulting in the most extensive collection of\nearly modern German travelogues ever created. To our knowledge, this is the\nfirst time such a method was implemented for the bibliographic indexing of a\ntext corpus on this scale, improving and extending the traditional methods in\nthe humanities. Overall, we consider our technique to be an important first\nstep in a broader effort of developing a novel mixed-method approach for the\nlarge-scale serial analysis of travelogues.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 17:20:10 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["R\u00f6rden", "Jan", ""], ["Gruber", "Doris", ""], ["Krickl", "Martin", ""], ["Haslhofer", "Bernhard", ""]]}, {"id": "2001.01972", "submitter": "Steffen Herbold", "authors": "Steffen Herbold", "title": "With Registered Reports Towards Large Scale Data Curation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scale of manually validated data is currently limited by the effort that\nsmall groups of researchers can invest for the curation of such data. Within\nthis paper, we propose the use of registered reports to scale the curation of\nmanually validated data. The idea is inspired by the mechanical turk and\nreplaces monetary payment with authorship of data set publication.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 11:17:09 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Herbold", "Steffen", ""]]}, {"id": "2001.02293", "submitter": "Henry Blanchette", "authors": "Henry Blanchette", "title": "A Network-Level View of Author Influence", "comments": "9 pages, 3 figures, student paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  I compare several network-level measures of centrality to common measures of\nauthor reputation and influence (e.g. hindex, i10index), all taken over the\ndata set of papers published in 2017 at major computer systems conferences and\nsome controls. I hypothesize that centrality measures will correlate strongly\nwith the reputation and influence measures. My results confirm several expected\ncorrelations and exhibit a few surprising absences of correlation. In\nparticular, there was an absence of statistically significant correlation\nbetween degree centrality and hindex,\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 04:21:30 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Blanchette", "Henry", ""]]}, {"id": "2001.02733", "submitter": "Sta\\v{s}a Milojevi\\'c", "authors": "Sta\\v{s}a Milojevi\\'c", "title": "Practical method to reclassify Web of Science articles into unique\n  subject categories and broad disciplines", "comments": "Quantitative Science Studies", "journal-ref": null, "doi": "10.1162/qss_a_00014", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of bibliographic items into subjects and disciplines in large\ndatabases is essential for many quantitative science studies. The Web of\nScience classification of journals into ~250 subject categories, which has\nserved as a basis for many studies, is known to have some fundamental problems\nand several practical limitations that may affect the results from such\nstudies. Here we present an easily reproducible method to perform\nreclassification of the Web of Science into existing subject categories and\ninto 14 broad areas. Our reclassification is at a level of articles, so it\npreserves disciplinary differences that may exist among individual articles\npublished in the same journal. Reclassification also eliminates ambiguous\n(multiple) categories that are found for 50% of items, and assigns a\ndiscipline/field category to all articles that come from broad-coverage\njournals such as Nature and Science. The correctness of the assigned subject\ncategories is evaluated manually and is found to be ~95%.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 20:44:11 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Milojevi\u0107", "Sta\u0161a", ""]]}, {"id": "2001.03067", "submitter": "Arthur Brack", "authors": "Arthur Brack, Jennifer D'Souza, Anett Hoppe, S\\\"oren Auer, Ralph\n  Ewerth", "title": "Domain-independent Extraction of Scientific Concepts from Research\n  Articles", "comments": "Accepted for publishing in 42nd European Conference on IR Research,\n  ECIR 2020", "journal-ref": "Advances in Information Retrieval. 2020", "doi": "10.1007/978-3-030-45439-5_17", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the novel task of domain-independent scientific concept extraction\nfrom abstracts of scholarly articles and present two contributions. First, we\nsuggest a set of generic scientific concepts that have been identified in a\nsystematic annotation process. This set of concepts is utilised to annotate a\ncorpus of scientific abstracts from 10 domains of Science, Technology and\nMedicine at the phrasal level in a joint effort with domain experts. The\nresulting dataset is used in a set of benchmark experiments to (a) provide\nbaseline performance for this task, (b) examine the transferability of concepts\nbetween domains. Second, we present two deep learning systems as baselines. In\nparticular, we propose active learning to deal with different domains in our\ntask. The experimental results show that (1) a substantial agreement is\nachievable by non-experts after consultation with domain experts, (2) the\nbaseline system achieves a fairly high F1 score, (3) active learning enables us\nto nearly halve the amount of required training data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 15:42:22 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Brack", "Arthur", ""], ["D'Souza", "Jennifer", ""], ["Hoppe", "Anett", ""], ["Auer", "S\u00f6ren", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2001.04002", "submitter": "Gy\\\"orgy Csom\\'os", "authors": "Gyorgy Csomos", "title": "On the challenges ahead of spatial scientometrics focusing on the city\n  level", "comments": null, "journal-ref": "Aslib Journal of Information Management, 72(1), 67-87. (2020)", "doi": "10.1108/AJIM-06-2019-0152", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the mid-1970s, it has become highly acknowledged to measure and\nevaluate changes in international research collaborations and the scientific\nperformance of institutions and countries through the prism of bibliometric and\nscientometric data. Spatial bibliometrics and scientometrics (henceforward\nspatial scientometrics) have traditionally focused on examining both country\nand regional levels; however, in recent years, numerous spatial analyses on the\ncity level have been carried out. While city-level scientometric analyses have\ngained popularity among policymakers and statistical/economic research\norganizations, researchers in the field of bibliometrics are divided regarding\nwhether it is possible to observe the spatial unit 'city' through bibliometric\nand scientometric tools. After systematically scrutinizing relevant studies in\nthe field, three major problems have been identified: 1) there is no\nstandardized method of how cities should be defined and how metropolitan areas\nshould be delineated, 2) there is no standardized method of how bibliometric\nand scientometric data on the city level should be collected and processed and\n3) it is not clearly defined how cities can profit from the results of\nbibliometric and scientometric analysis focusing on them. This paper\ninvestigates major challenges ahead of spatial scientometrics, focusing on the\ncity level and presents some possible solutions.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 21:20:52 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Csomos", "Gyorgy", ""]]}, {"id": "2001.04232", "submitter": "Takeshi Terui", "authors": "Takeshi Terui, Yasuyuki Minamiyama, Kazutsuna Yamaji", "title": "Possibility and prevention of inappropriate data manipulation in Polar\n  Data Journal", "comments": "5 pages, 1 figure, 1 table", "journal-ref": "2019 8th International Congress on Advanced Applied Informatics\n  (IIAI-AAI)", "doi": "10.1109/IIAI-AAI.2019.00087", "report-no": null, "categories": "cs.CR cs.DL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stakeholders in the scientific field must always maintain transparency in the\nprocess of publishing research results in journals. Unfortunately, although\nresearch misconduct has stopped, certain forms of manipulation continue to\nappear in other forms. As new techniques of scientific publishing develop,\nscience stakeholders need to examine the possibility of inappropriate activity\nin these new platforms. The National Institute of Polar Research in Japan\nlaunched a new data journal Polar Data Journal (PDJ) in 2017 to review the\nquality of data obtained in the polar region. To maintain transparency in this\nnew data journal, we investigated the possibility of inappropriate data\nmanipulation in peer reviews before the inception of this journal. We clarified\ninappropriate activity for the data in the peer review and considered\npreventive measures. We designed a specific workflow for PDJ. This included two\nmeasures: (i) the comparison of hash values in the review process and (ii) open\npeer review report publishing. Using the hash value comparison, we detected two\ninstances of inappropriate data manipulation after the start of the journal.\nThis research will help improve workflow in data journals and data\nrepositories.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:29:34 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Terui", "Takeshi", ""], ["Minamiyama", "Yasuyuki", ""], ["Yamaji", "Kazutsuna", ""]]}, {"id": "2001.04244", "submitter": "Diego Amancio", "authors": "Xiomara S. Q. Chac\\'on and Thiago C. Silva and Diego R. Amancio", "title": "Comparing the impact of subfields in scientific journals", "comments": "Scientometrics, 2020", "journal-ref": "Scientometrics 125, 625-639 (2020)", "doi": "10.1007/s11192-020-03651-x", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact factor has been extensively used in the last years to assess\njournals visibility and prestige. While the impact factor is useful to compare\njournals, the specificities of subfields visibility in journals are overlooked\nwhenever visibility is measured only at the journal level. In this paper, we\nanalyze the subfields visibility in a subset of over 450,000 Physics papers. We\nshow that the visibility of subfields is not regular in the considered dataset.\nIn particular years, the variability in subfields impact factor in a journal\nreached 75% of the average subfields impact factor. We also found that the\ndifference of subfields visibility in the same journal can be even higher than\nthe difference of visibility between different journals. Our results show that\nsubfields impact is an important factor accounting for journals visibility.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:53:15 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 01:46:32 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 13:24:03 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Chac\u00f3n", "Xiomara S. Q.", ""], ["Silva", "Thiago C.", ""], ["Amancio", "Diego R.", ""]]}, {"id": "2001.04290", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Richard Williams", "title": "An Evaluation of Percentile Measures of Citation Impact, and a Proposal\n  for Making Them Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Percentiles are statistics pointing to the standing of a paper's citation\nimpact relative to other papers in a given citation distribution. Percentile\nRanks (PRs) often play an important role in evaluating the impact of scholars,\ninstitutions, and lines of study. Because PRs are so important for the\nassessment of scholarly impact, and because citation practices differ greatly\nacross time and fields, various percentile approaches have been proposed to\ntime- and field-normalize citations. Unfortunately, current popular methods\noften face significant problems in time- and field-normalization, including\nwhen papers are assigned to multiple fields or have been published by more than\none unit (e.g., researchers or countries). They also face problems for\nestimating citation counts (CCs) for pre-defined PRs (e.g., the 90th PR). We\noffer a series of guidelines and procedures that, we argue, address these\nproblems and others and provide a superior means to make the use of percentile\nmethods more accurate and informative. In particular, we introduce two\napproaches, CP-IN and CP-EX, that should be preferred in bibliometric studies\nbecause they consider the complete citation distribution. Both approaches are\nbased on cumulative frequencies in percentages (CPs). The paper further shows\nhow bar graphs and beamplots can present PRs in a more meaningful and accurate\nmanner.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:36:40 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 12:33:17 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 06:22:30 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Bornmann", "Lutz", ""], ["Williams", "Richard", ""]]}, {"id": "2001.04697", "submitter": "Weishu Liu", "authors": "Weishu Liu, Li Tang, Guangyuan Hu", "title": "Funding information in Web of Science: An updated overview", "comments": "forthcoming in Scientometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the limitations of funding acknowledgment (FA) data in Web of Science\n(WoS), studies using FA information have increased rapidly over the last\nseveral years. Considering this WoS'recent practice of updating funding data,\nthis paper further investigates the characteristics and distribution of FA data\nin four WoS journal citation indexes. The research reveals that FA information\ncoverage variances persist cross all four citation indexes by time coverage,\nlanguage and document type. Our evidence suggests an improvement in FA\ninformation collection in humanity and social science research. Departing from\nprevious studies, we argue that FA text (FT) alone no longer seems an\nappropriate field to retrieve and analyze funding information, since a\nsubstantial number of documents only report funding agency or grant number\ninformation in respective fields. Articles written in Chinese have a higher FA\npresence rate than other non-English WoS publications. This updated study\nconcludes with a discussion of new findings and practical guidance for the\nfuture retrieval and analysis of funded research.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 10:18:38 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Liu", "Weishu", ""], ["Tang", "Li", ""], ["Hu", "Guangyuan", ""]]}, {"id": "2001.05399", "submitter": "Jimmy Lin", "authors": "Nick Ruest, Jimmy Lin, Ian Milligan, and Samantha Fritz", "title": "The Archives Unleashed Project: Technology, Process, and Community to\n  Improve Scholarly Access to Web Archives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Archives Unleashed project aims to improve scholarly access to web\narchives through a multi-pronged strategy involving tool creation, process\nmodeling, and community building - all proceeding concurrently in\nmutually-reinforcing efforts. As we near the end of our initially-conceived\nthree-year project, we report on our progress and share lessons learned along\nthe way. The main contribution articulated in this paper is a process model\nthat decomposes scholarly inquiries into four main activities: filter, extract,\naggregate, and visualize. Based on the insight that these activities can be\ndisaggregated across time, space, and tools, it is possible to generate\n\"derivative products\", using our Archives Unleashed Toolkit, that serve as\nuseful starting points for scholarly inquiry. Scholars can download these\nproducts from the Archives Unleashed Cloud and manipulate them just like any\nother dataset, thus providing access to web archives without requiring any\nspecialized knowledge. Over the past few years, our platform has processed over\na thousand different collections from about two hundred users, totaling over\n280 terabytes of web archives.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:09:47 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Ruest", "Nick", ""], ["Lin", "Jimmy", ""], ["Milligan", "Ian", ""], ["Fritz", "Samantha", ""]]}, {"id": "2001.05414", "submitter": "Matus Medo", "authors": "Shuqi Xu, Manuel Sebastian Mariani, Linyuan L\\\"u, Mat\\'u\\v{s} Medo", "title": "Unbiased evaluation of ranking metrics reveals consistent performance in\n  science and technology citation data", "comments": "21 pages, 11 figures, 5 tables", "journal-ref": null, "doi": "10.1016/j.joi.2019.101005", "report-no": "Journal of Informetrics 14(1), 101005 (2020)", "categories": "cs.SI cs.DL cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasing use of citation-based metrics for research evaluation\npurposes, we do not know yet which metrics best deliver on their promise to\ngauge the significance of a scientific paper or a patent. We assess 17\nnetwork-based metrics by their ability to identify milestone papers and patents\nin three large citation datasets. We find that traditional\ninformation-retrieval evaluation metrics are strongly affected by the interplay\nbetween the age distribution of the milestone items and age biases of the\nevaluated metrics. Outcomes of these metrics are therefore not representative\nof the metrics' ranking ability. We argue in favor of a modified evaluation\nprocedure that explicitly penalizes biased metrics and allows us to reveal\nmetrics' performance patterns that are consistent across the datasets. PageRank\nand LeaderRank turn out to be the best-performing ranking metrics when their\nage bias is suppressed by a simple transformation of the scores that they\nproduce, whereas other popular metrics, including citation count, HITS and\nCollective Influence, produce significantly worse ranking results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:34:21 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Xu", "Shuqi", ""], ["Mariani", "Manuel Sebastian", ""], ["L\u00fc", "Linyuan", ""], ["Medo", "Mat\u00fa\u0161", ""]]}, {"id": "2001.06803", "submitter": "Zhesi Shen", "authors": "Sichao Tong, Ting Yue, Zhesi Shen, Liying Yang", "title": "The effect of national and international multiple affiliations on\n  citation impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Researchers affiliated with multiple institutions are increasingly seen in\ncurrent scientific environment. In this paper we systematically analyze the\nmulti-affiliated authorship and its effect on citation impact, with focus on\nthe scientific output of research collaboration. By considering the nationality\nof each institutions, we further differentiate the national multi-affiliated\nauthorship and international multi-affiliated authorship and reveal their\ndifferent patterns across disciplines and countries. We observe a large share\nof publications with multi-affiliated authorship (45.6%) in research\ncollaboration, with a larger share of publications containing national\nmulti-affiliated authorship in medicine related and biology related\ndisciplines, and a larger share of publications containing international type\nin Space Science, Physics and Geosciences. To a country-based view, we\ndistinguish between domestic and foreign multi-affiliated authorship to a\nspecific country. Taking G7 and BRICS countries as samples from different S&T\nlevel, we find that the domestic national multi-affiliated authorship relate to\nmore on citation impact for most disciplines of G7 countries, while domestic\ninternational multi-affiliated authorships are more positively influential for\nmost BRICS countries.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 10:16:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tong", "Sichao", ""], ["Yue", "Ting", ""], ["Shen", "Zhesi", ""], ["Yang", "Liying", ""]]}, {"id": "2001.07038", "submitter": "Ana Freire", "authors": "Ana Freire, Lorenzo Porcaro and Emilia G\\'omez", "title": "Measuring Diversity of Artificial Intelligence Conferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of diversity of the Artificial Intelligence (AI) field is nowadays a\nconcern, and several initiatives such as funding schemes and mentoring programs\nhave been designed to overcome it. However, there is no indication on how these\ninitiatives actually impact AI diversity in the short and long term. This work\nstudies the concept of diversity in this particular context and proposes a\nsmall set of diversity indicators (i.e. indexes) of AI scientific events. These\nindicators are designed to quantify the diversity of the AI field and monitor\nits evolution. We consider diversity in terms of gender, geographical location\nand business (understood as the presence of academia versus industry). We\ncompute these indicators for the different communities of a conference:\nauthors, keynote speakers and organizing committee. From these components we\ncompute a summarized diversity indicator for each AI event. We evaluate the\nproposed indexes for a set of recent major AI conferences and we discuss their\nvalues and limitations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 10:09:50 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 10:34:58 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 15:45:35 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 17:08:41 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Freire", "Ana", ""], ["Porcaro", "Lorenzo", ""], ["G\u00f3mez", "Emilia", ""]]}, {"id": "2001.07199", "submitter": "Ye Sun", "authors": "Ye Sun and Vito Latora", "title": "The evolution of knowledge within and across fields in modern physics", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exchange of knowledge across different areas and disciplines plays a key\nrole in the process of knowledge creation, and can stimulate innovation and the\nemergence of new fields. We develop here a quantitative framework to extract\nsignificant dependencies among scientific disciplines and turn them into a\ntime-varying network whose nodes are the different fields, while the weighted\nlinks represent the flow of knowledge from one field to another at a given\nperiod of time. Drawing on a comprehensive data set on scientific production in\nmodern physics and on the patterns of citations between articles published in\nthe various fields in the last thirty years, we are then able to map, over\ntime, how the ideas developed in a given field in a certain time period have\ninfluenced later discoveries in the same field or in other fields. The analysis\nof knowledge flows internal to each field displays a remarkable variety of\ntemporal behaviours, with some fields of physics showing to be more\nself-referential than others. The temporal networks of knowledge exchanges\nacross fields reveal cases of one field continuously absorbing knowledge from\nanother field in the entire observed period, pairs of fields mutually\ninfluencing each other, but also cases of evolution from absorbing to mutual or\neven to back-nurture behaviors.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 18:17:40 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Sun", "Ye", ""], ["Latora", "Vito", ""]]}, {"id": "2001.07491", "submitter": "Zhichao Fang", "authors": "Zhichao Fang, Jonathan Dudek, Rodrigo Costas", "title": "The stability of Twitter metrics: A study on unavailable Twitter\n  mentions of scientific publications", "comments": null, "journal-ref": "Journal of the Association for Information Science and Technology,\n  2020", "doi": "10.1002/asi.24344", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the stability of Twitter counts of scientific\npublications over time. For this, we conducted an analysis of the availability\nstatuses of over 2.6 million Twitter mentions received by the 1,154 most\ntweeted scientific publications recorded by Altmetric.com up to October 2017.\nResults show that of the Twitter mentions for these highly tweeted\npublications, about 14.3% have become unavailable by April 2019. Deletion of\ntweets by users is the main reason for unavailability, followed by suspension\nand protection of Twitter user accounts. This study proposes two measures for\ndescribing the Twitter dissemination structures of publications: Degree of\nOriginality (i.e., the proportion of original tweets received by a paper) and\nDegree of Concentration (i.e., the degree to which retweets concentrate on a\nsingle original tweet). Twitter metrics of publications with relatively low\nDegree of Originality and relatively high Degree of Concentration are observed\nto be at greater risk of becoming unstable due to the potential disappearance\nof their Twitter mentions. In light of these results, we emphasize the\nimportance of paying attention to the potential risk of unstable Twitter\ncounts, and the significance of identifying the different Twitter dissemination\nstructures when studying the Twitter metrics of scientific publications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 12:59:50 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Fang", "Zhichao", ""], ["Dudek", "Jonathan", ""], ["Costas", "Rodrigo", ""]]}, {"id": "2001.08199", "submitter": "Hao Peng", "authors": "Hao Peng, Qing Ke, Ceren Budak, Daniel M. Romero, Yong-Yeol Ahn", "title": "Neural Embeddings of Scholarly Periodicals Reveal Complex Disciplinary\n  Organizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the structure of knowledge domains is one of the foundational\nchallenges in science of science. Here, we propose a neural embedding technique\nthat leverages the information contained in the citation network to obtain\ncontinuous vector representations of scientific periodicals. We demonstrate\nthat our periodical embeddings encode nuanced relationships between periodicals\nas well as the complex disciplinary and interdisciplinary structure of science,\nallowing us to make cross-disciplinary analogies between periodicals.\nFurthermore, we show that the embeddings capture meaningful \"axes\" that\nencompass knowledge domains, such as an axis from \"soft\" to \"hard\" sciences or\nfrom \"social\" to \"biological\" sciences, which allow us to quantitatively ground\nperiodicals on a given dimension. By offering novel quantification in science\nof science, our framework may in turn facilitate the study of how knowledge is\ncreated and organized.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 18:40:47 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 16:45:06 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Peng", "Hao", ""], ["Ke", "Qing", ""], ["Budak", "Ceren", ""], ["Romero", "Daniel M.", ""], ["Ahn", "Yong-Yeol", ""]]}, {"id": "2001.08647", "submitter": "Morane Gruenpeter", "authors": "Roberto Di Cosmo (UPD7), Morane Gruenpeter (UNIVAQ), Stefano\n  Zacchiroli (UPD7)", "title": "Referencing Source Code Artifacts: a Separate Concern in Software\n  Citation", "comments": "Computing in Science & Engineering, IEEE, In press", "journal-ref": null, "doi": "10.1109/MCSE.2019.2963148", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the entities involved in software citation, software source code\nrequires special attention, due to the role it plays in ensuring scientific\nreproducibility. To reference source code we need identifiers that are not only\nunique and persistent, but also support \\emph{integrity} checking\nintrinsically. Suitable identifiers must guarantee that denotedobjects will\nalways stay the same, without relying on external third parties and\nadministrative processes. We analyze the role of identifiers for digital\nobjects (IDOs), whose properties are different from, and complementary to,\nthose of the various digital identifiers of objects (DIOs) that are today\npopular building blocks of software and data citation toolchains.We argue that\nboth kinds of identifiers are needed and detail the syntax, semantics, and\npractical implementation of the persistent identifiers (PIDs) adopted by the\nSoftware Heritage project to reference billions of softwaresource code\nartifacts such as source code files, directories, and commits.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 16:36:34 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Di Cosmo", "Roberto", "", "UPD7"], ["Gruenpeter", "Morane", "", "UNIVAQ"], ["Zacchiroli", "Stefano", "", "UPD7"]]}, {"id": "2001.08687", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira, Zhiying Jiang, Kyunghyun Cho, Jimmy Lin", "title": "Navigation-Based Candidate Expansion and Pretrained Language Models for\n  Citation Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation recommendation systems for the scientific literature, to help\nauthors find papers that should be cited, have the potential to speed up\ndiscoveries and uncover new routes for scientific exploration. We treat this\ntask as a ranking problem, which we tackle with a two-stage approach: candidate\ngeneration followed by re-ranking. Within this framework, we adapt to the\nscientific domain a proven combination based on \"bag of words\" retrieval\nfollowed by re-scoring with a BERT model. We experimentally show the effects of\ndomain adaptation, both in terms of pretraining on in-domain data and\nexploiting in-domain vocabulary. In addition, we introduce a novel\nnavigation-based document expansion strategy to enrich the candidate documents\nprocessed by our neural models. On three different collections from different\nscientific disciplines, we achieve the best-reported results in the citation\nrecommendation task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:29:04 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Nogueira", "Rodrigo", ""], ["Jiang", "Zhiying", ""], ["Cho", "Kyunghyun", ""], ["Lin", "Jimmy", ""]]}, {"id": "2001.08988", "submitter": "Glen Martin Dr", "authors": "Glen P. Martin, David Jenkins, Lucy Bull, Rose Sisk, Lijing Lin,\n  William Hulme, Anthony Wilson, Wenjuan Wang, Michael Barrowman, Camilla\n  Sammut-Powell, Alexander Pate, Matthew Sperrin and Niels Peek", "title": "Towards a Framework for the Design, Implementation and Reporting of\n  Methodology Scoping Reviews", "comments": "22 pages, 2 tables", "journal-ref": "Journal of Clinical Epidemiology. (2020)", "doi": "10.1016/j.jclinepi.2020.07.014", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: In view of the growth of published papers, there is an increasing\nneed for studies that summarise scientific research. An increasingly common\nreview is a 'Methodology scoping review', which provides a summary of existing\nanalytical methods, techniques and software, proposed or applied in research\narticles, which address an analytical problem or further an analytical\napproach. However, guidelines for their design, implementation and reporting\nare limited.\n  Methods: Drawing on the experiences of the authors, which were consolidated\nthrough a series of face-to-face workshops, we summarise the challenges\ninherent in conducting a methodology scoping review and offer suggestions of\nbest practice to promote future guideline development.\n  Results: We identified three challenges of conducting a methodology scoping\nreview. First, identification of search terms; one cannot usually define the\nsearch terms a priori and the language used for a particular method can vary\nacross the literature. Second, the scope of the review requires careful\nconsideration since new methodology is often not described (in full) within\nabstracts. Third, many new methods are motivated by a specific clinical\nquestion, where the methodology may only be documented in supplementary\nmaterials. We formulated several recommendations that build upon existing\nreview guidelines. These recommendations ranged from an iterative approach to\ndefining search terms through to screening and data extraction processes.\n  Conclusion: Although methodology scoping reviews are an important aspect of\nresearch, there is currently a lack of guidelines to standardise their design,\nimplementation and reporting. We recommend a wider discussion on this topic.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 10:43:26 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Martin", "Glen P.", ""], ["Jenkins", "David", ""], ["Bull", "Lucy", ""], ["Sisk", "Rose", ""], ["Lin", "Lijing", ""], ["Hulme", "William", ""], ["Wilson", "Anthony", ""], ["Wang", "Wenjuan", ""], ["Barrowman", "Michael", ""], ["Sammut-Powell", "Camilla", ""], ["Pate", "Alexander", ""], ["Sperrin", "Matthew", ""], ["Peek", "Niels", ""]]}, {"id": "2001.09006", "submitter": "Nagy Marcell", "authors": "Roland Molontay, Marcell Nagy", "title": "Twenty Years of Network Science: A Bibliographic and Co-Authorship\n  Network Analysis", "comments": "20 pages, 18 figures. arXiv admin note: substantial text overlap with\n  arXiv:1908.08478", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two decades ago three pioneering papers turned the attention to complex\nnetworks and initiated a new era of research, establishing an interdisciplinary\nfield called network science. Namely, these highly-cited seminal papers were\nwritten by Watts&Strogatz, Barab\\'asi&Albert, and Girvan&Newman on small-world\nnetworks, on scale-free networks and on the community structure of complex\nnetworks, respectively. In the past 20 years - due to the multidisciplinary\nnature of the field - a diverse but not divided network science community has\nemerged. In this paper, we investigate how this community has evolved over time\nwith respect to speed, diversity and interdisciplinary nature as seen through\nthe growing co-authorship network of network scientists (here the notion refers\nto a scholar with at least one paper citing at least one of the three\naforementioned milestone papers). After providing a bibliographic analysis of\n31,763 network science papers, we construct the co-authorship network of 56,646\nnetwork scientists and we analyze its topology and dynamics. We shed light on\nthe collaboration patterns of the last 20 years of network science by\ninvestigating numerous structural properties of the co-authorship network and\nby using enhanced data visualization techniques. We also identify the most\ncentral authors, the largest communities, investigate the spatiotemporal\nchanges, and compare the properties of the network to scientometric indicators.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 02:13:08 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 21:33:42 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 10:04:34 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Molontay", "Roland", ""], ["Nagy", "Marcell", ""]]}, {"id": "2001.09914", "submitter": "Julian Sienkiewicz", "authors": "Robert Jankowski and Julian Sienkiewicz", "title": "Determining crucial factors for the popularity of scientific articles", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a set of over 70.000 records from PLOS One journal consisting of 37\nlexical, sentiment and bibliographic variables we perform analysis backed with\nmachine learning methods to predict the class of popularity of scientific\npapers defined by the number of times they have been viewed. Our study shows\ncorrelations among the features and recovers a threshold for the number of\nviews that results in the best prediction results in terms of Matthew's\ncorrelation coefficient. Moreover, by creating a variable importance plot for\nrandom forest classifier, we are able to reduce the number of features while\nkeeping similar predictability and determine crucial factors responsible for\nthe popularity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 17:10:39 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Jankowski", "Robert", ""], ["Sienkiewicz", "Julian", ""]]}, {"id": "2001.10336", "submitter": "Philipp Mayr", "authors": "Guillaume Cabanac and Ingo Frommholz and Philipp Mayr", "title": "Bibliometric-enhanced Information Retrieval 10th Anniversary Workshop\n  Edition", "comments": "Overview paper submitted to ECIR 2020, Lisbon, PT. arXiv admin note:\n  substantial text overlap with arXiv:1909.04954", "journal-ref": null, "doi": "10.1007/978-3-030-45442-5_85", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bibliometric-enhanced Information Retrieval workshop series (BIR) was\nlaunched at ECIR in 2014 \\cite{MayrEtAl2014} and it was held at ECIR each year\nsince then. This year we organize the 10th iteration of BIR. The workshop\nseries at ECIR and JCDL/SIGIR tackles issues related to academic search, at the\ncrossroads between Information Retrieval, Natural Language Processing and\nBibliometrics. In this overview paper, we summarize the past workshops, present\nthe workshop topics for 2020 and reflect on some future steps for this workshop\nseries.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:16:12 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Cabanac", "Guillaume", ""], ["Frommholz", "Ingo", ""], ["Mayr", "Philipp", ""]]}]