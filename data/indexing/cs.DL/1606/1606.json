[{"id": "1606.00153", "submitter": "Nicolas Robinson-Garcia", "authors": "Nicolas Robinson-Garcia, Thed N. van Leeuwen and Ismael Rafols", "title": "Using almetrics for contextualised mapping of societal impact: From hits\n  to networks", "comments": "This paper has been accepted for publication in Science and Public\n  Policy", "journal-ref": "Robinson-Garcia, N., van Leeuwen, T.N., Rafols, I. Using almetrics\n  for contextualised mapping of societal impact: From hits to networks. Science\n  and Public Policy (2017)", "doi": "10.1093/scipol/scy024", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this article, we develop a method that uses altmetric data to analyse\nresearchers' interactions, as a way of mapping the contexts of potential\nsocietal impact. In the face of an increasing policy demand for quantitative\nmethodologies to assess societal impact, social media data (altmetrics) has\nbeen presented as a potential method to capture broader forms of impact.\nHowever, current altmetric indicators were extrapolated from traditional\ncitation approaches and are seen as problematic for assessing societal impact.\nIn contrast, established qualitative methodologies for societal impact\nassessment are based on interaction approaches. These argue that assessment\nshould focus on mapping the contexts in which engagement among researchers and\nstakeholders take place, as a means to understand the pathways to societal\nimpact. Following these approaches, we propose to shift the use of altmetric\ndata towards network analysis of researchers and stakeholders. We carry out two\ncase studies, analysing researchers' networks with Twitter data. The comparison\nillustrates the potential of Twitter networks to capture disparate degrees of\npolicy engagement. We propose that this mapping method can be used as an input\nwithin broader methodologies in case studies of societal impact assessment.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 07:50:45 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 10:56:38 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Robinson-Garcia", "Nicolas", ""], ["van Leeuwen", "Thed N.", ""], ["Rafols", "Ismael", ""]]}, {"id": "1606.00155", "submitter": "Nicolas Robinson-Garcia", "authors": "Nicolas Robinson-Garcia, Carolina Ca\\~nibano, Richard Woolley and\n  Rodrigo Costas", "title": "Tracing scientific mobility of Early Career Researchers in Spain and The\n  Netherlands through their publications", "comments": "This paper has been accepted at the STI 2016 Conference held in\n  Valencia, September 14-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  International scientific mobility is acknowledged to be a key mechanism for\nthe diffusion of knowledge, particularly tacit or 'sticky' knowledge that\ncannot be transferred without geographical proximity and personal contact, for\nthe incorporation of young researchers into elite transnational scientific\nnetworks, and for accessing additional resources or infrastructures that are\nessential to the research process but located in other places. The inadequacy\nand lack of appropriate data to assess the phenomenon of researcher mobility\nhas been repeatedly pointed out by scholars and policy makers. This paper\npresents an exploratory analysis of different typologies of researchers\naccording to their traceable mobility using scientific publications covered in\nthe Web of Science (WoS). We compare two populations of researchers, of the\nsame 'scientific age', based in Spain and The Netherlands. We observe\ndifferences in the degree of mobility of Spain and Netherlands based\nresearchers. Factors associated with the different institutional conditions\ncharacterizing the two national systems need to be taken into account. First,\nthe Spanish and Dutch university and research systems are different in many\nways. Second, there may be very different institutional incentives for mobility\nin the two systems. More sophisticated bibliometric analyses and comparisons\nwith different 'generations' of researchers, possibly combined with qualitative\ninvestigation, will be required to better understand the role and function of\nnational institutional context in both research mobility and research careers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 08:03:07 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Robinson-Garcia", "Nicolas", ""], ["Ca\u00f1ibano", "Carolina", ""], ["Woolley", "Richard", ""], ["Costas", "Rodrigo", ""]]}, {"id": "1606.00193", "submitter": "Mike Thelwall", "authors": "Mike Thelwall", "title": "Not dead, just resting: The practical value of per publication citation\n  indicators", "comments": null, "journal-ref": "Journal of Informetrics, 10(2), 667-670, (2016)", "doi": "10.1016/j.joi.2015.12.009", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the final analysis citation-based indicators are inferior to effective\npeer review and even peer review is flawed. It is impossible to accurately\nmeasure the value or impact of scientific research and a key task of\nscientometricians should be to produce figures for policy makers and others\nthat are as informative as it is practical to make them and to ensure that\nusers are fully aware of their limitations. Although the Abramo and D'Angelo\n(2016) suggestions make a lot of theoretical sense and so are a goal that is\nworth aiming for, it is unrealistic in practice to advocate their universal use\nin the contexts discussed above. This is because the indicators would still\nhave flaws in addition to the generic limitations of citation-based indicators\nand would still be inadequate for replacing peer review. Thus, the expense of\nthe data gathering does not always justify the value in practice of the extra\naccuracy. In the longer term, the restructuring of education needed in order to\nget the homogeneity necessary for genuinely comparable statistics would be too\nexpensive and probably damaging to the research mission, in addition to being\nout of proportion to the likely value of any citation-based indicator.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 09:43:36 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Thelwall", "Mike", ""]]}, {"id": "1606.00232", "submitter": "Nicolas Robinson-Garcia", "authors": "Daniel Torres-Salinas, Nicolas Robinson-Garcia and Evaristo\n  Jim\\'enez-Contreras", "title": "Can we use altmetrics at the institutional level? A case study analysing\n  the coverage by research areas of four Spanish universities", "comments": "Paper accepted for the STI 2016 Conference held in Valencia,\n  September 14-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social media based indicators or altmetrics have been under scrutiny for the\nlast seven years. Their promise as alternative metrics for measuring scholarly\nimpact is still far from becoming a reality. Up to now, most studies have\nfocused on the understanding of the nature and relation of altmetric indicators\nwith citation data. Few papers have analysed research profiles based on\naltmetric data. Most of these have related to researcher profiles and the\nexpansion of these tools among researchers. This paper aims at exploring the\ncoverage of the Altmetric.com database and its potential use in order to show\nuniversities' research profiles in relationship with other databases. We\nanalyse a sample of four different Spanish universities.First, we observe a low\ncoverage of altmetric indicators with only 36 percent of all documents\nretrieved from the Web of Science having an 'altmetric' score. Second, we\nobserve that for the four universities analysed, the area of Science shows\nhigher 'altmetric' scores that the rest of the research areas. Finally,\nconsidering the low coverage of altmetric data at the institutional level, it\ncould be interesting for research policy makers to consider the development of\nguidelines and best practices guides to ensure that researchers disseminate\nadequately their research findings through social media.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 11:26:19 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Torres-Salinas", "Daniel", ""], ["Robinson-Garcia", "Nicolas", ""], ["Jim\u00e9nez-Contreras", "Evaristo", ""]]}, {"id": "1606.00240", "submitter": "Nicolas Robinson-Garcia", "authors": "Alesia Zuccala, Nicolas Robinson-Garcia, Rafael Repiso and Daniel\n  Torres-Salinas", "title": "Using network centrality measures to improve national journal\n  classification lists", "comments": "Paper accepted at the STI 2016 Conference held in Valencia, September\n  14-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In countries like Denmark and Spain classified journal lists are now being\nproduced and used in the calculation of nationwide performance indicators. As a\nresult, Danish and Spanish scholars are advised to contribute to journals of\nhigh 'authority' (as in the former) or those within a high class (as in the\nlatter). This can create a few problems. The aim of this paper is to analyse\nthe potential use of network centrality measures to identify possible\nmismatches of journal categories. It analysis the Danish National Authority\nList and the Spanish CIRC Classification. Based on a sample of Library and\nInformation Science publications, it analyses centrality measures that can\nassess on the importance of journals to given fields, correcting mismatches in\nthese classifications. We conclude by emphasising the use of these measures to\nbetter calibrate journal classifications as we observe a general bias in these\nlists towards older journals. Centrality measures can allow to identify\nperiphery-to-core journals' transitions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 11:45:02 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Zuccala", "Alesia", ""], ["Robinson-Garcia", "Nicolas", ""], ["Repiso", "Rafael", ""], ["Torres-Salinas", "Daniel", ""]]}, {"id": "1606.00405", "submitter": "Carlo Maria Zw\\\"olf", "authors": "Carlo Maria Zw\\\"olf, Nicolas Moreau, Marie-Lise Dubernet", "title": "New model for datasets citation and extraction reproducibility in VAMDC", "comments": "48 pages", "journal-ref": null, "doi": "10.1016/j.jms.2016.04.009", "report-no": null, "categories": "cs.DL physics.atom-ph physics.chem-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we present a new paradigm for the identification of datasets\nextracted from the Virtual Atomic and Molecular Data Centre (VAMDC) e-science\ninfrastructure. Such identification includes information on the origin and\nversion of the datasets, references associated to individual data in the\ndatasets, as well as timestamps linked to the extraction procedure. This\nparadigm is described through the modifications of the language used to\nexchange data within the VAMDC and through the services that will implement\nthose modifications. This new paradigm should enforce traceability of datasets,\nfavour reproducibility of datasets extraction, and facilitate the systematic\ncitation of the authors having originally measured and/or calculated the\nextracted atomic and molecular data.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 19:14:35 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Zw\u00f6lf", "Carlo Maria", ""], ["Moreau", "Nicolas", ""], ["Dubernet", "Marie-Lise", ""]]}, {"id": "1606.01452", "submitter": "Marta Vos", "authors": "Marta Vos", "title": "Maturity of the Internet of Things Research Field: Or Why Choose\n  Rigorous Keywords", "comments": "ISBN# 978-0-646-95337-3 Presented at the Australasian Conference on\n  Information Systems 2015 (arXiv:1605.01032)", "journal-ref": null, "doi": null, "report-no": "ACIS/2015/147", "categories": "cs.DL cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Periodically researchers examine their fields of research in order to test\nthe fields maturity, to determine theories and methods used, and to uncover\nareas of research emphasis. Research articles for these studies are generally\nlocated through keyword search, then manually read and coded in order to\nsummarise the research field. This research tests using metadata analysis\nalone, on the internet of things (IoT) research field, using data extracted\nfrom three academic databases. The metadata analysis was proposed to reduce\ntime in analysis, and allow for testing of a much larger sample of articles.\nFindings indicated that the IoT research field was immature, with experimental\nmethods dominating research outputs. However, difficulties with keyword\nmetadata included synonyms, abbreviations, lack of accuracy, and method and\ntheory not being included in metadata. A keyword schema is suggested in order\nto relieve these problems in future articles, and to assist researchers in\nlocating relevant literature.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 02:30:24 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Vos", "Marta", ""]]}, {"id": "1606.01808", "submitter": "Mansaf Alam Dr", "authors": "Samiya Khan, Kashish A. Shakil, and Mansaf Alam", "title": "Cloud-Based Big Data Management and Analytics for Scholarly Resources:\n  Current Trends, Challenges and Scope for Future Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the shifting focus of organizations and governments towards digitization\nof academic and technical documents, there has been an increasing need to use\nthis reserve of scholarly documents for developing applications that can\nfacilitate and aid in better management of research. In addition to this, the\nevolving nature of research problems has made them essentially\ninterdisciplinary. As a result, there is a growing need for scholarly\napplications like collaborator discovery, expert finding and research\nrecommendation systems. This research paper reviews the current trends and\nidentifies the challenges existing in the architecture, services and\napplications of big scholarly data platform with a specific focus on directions\nfor future research.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 16:11:48 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Khan", "Samiya", ""], ["Shakil", "Kashish A.", ""], ["Alam", "Mansaf", ""]]}, {"id": "1606.03857", "submitter": "Fakhri Momeni", "authors": "Fakhri Momeni and Philipp Mayr", "title": "Evaluating Co-Authorship Networks in Author Name Disambiguation for\n  Common Names", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing size of digital libraries it has become a challenge to\nidentify author names correctly. The situation becomes more critical when\ndifferent persons share the same name (homonym problem) or when the names of\nauthors are presented in several different ways (synonym problem). This paper\nfocuses on homonym names in the computer science bibliography DBLP. The goal of\nthis study is to evaluate a method which uses co-authorship networks and\nanalyze the effect of common names on it. For this purpose we clustered the\npublications of authors with the same name and measured the effectiveness of\nthe method against a gold standard of manually assigned DBLP records. The\nresults show that despite the good performance of implemented method for most\nnames, we should optimize for common names. Hence community detection was\nemployed to optimize the method. Results prove that the applied method improves\nthe performance for these names.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 08:38:01 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Momeni", "Fakhri", ""], ["Mayr", "Philipp", ""]]}, {"id": "1606.03963", "submitter": "Igor Barahona Dr", "authors": "Igor Barahona, Daria Micaela Hernandez and Hector Hugo\n  Perez-Villarreal", "title": "How marketing vocabulary was evolving from 2005 to 2014? An illustrative\n  application of statistical methods on text mining", "comments": "Marketing, Textual Statistics, Vocabulary evolving, Influential\n  articles, Correspondence analysis", "journal-ref": "JSM Proceedings (2015) Section on Statistics in Marketing.\n  Alexandria, VA. American Statistical Association. 1121-1131", "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here a collection of 1169 abstracts, which corresponds to articles that the\nJournal of Marketing Research has published from 2005 to 2014, are analysed\nunder a novel approach. We apply several statistical methods, such as Principal\nComponents Analysis and Correspondence Analysis to identify the way Marketing\nvocabulary is evolving. Similarly those articles that introduce new vocabulary\nare identified and the preferred words by authors are also detected. In order\nto provide an easy-to-understand explanation, we provide our results\ngraphically. A word-cloud with the most frequent words is given first. Secondly\nabstracts-words are represented on the factorial plane. Finally one\nrepresentation of word-years allows us to detect changes on the vocabulary\nthrough the passing of time.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 18:06:26 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Barahona", "Igor", ""], ["Hernandez", "Daria Micaela", ""], ["Perez-Villarreal", "Hector Hugo", ""]]}, {"id": "1606.04139", "submitter": "Javier Contreras-Reyes", "authors": "Javier E. Contreras-Reyes", "title": "Credit allocation based on journal impact factor and coauthorship\n  contribution", "comments": "9 pages; 3 figures; 2 tables", "journal-ref": "Journal of Social and Administrative Sciences (2016), 3(2),\n  111-118", "doi": "10.1453/jsas.v3i2.809", "report-no": null, "categories": "cs.DL q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some research institutions demand researchers to distribute the incomes they\nearn from publishing papers to their researchers and/or co-authors. In this\nstudy, we deal with the Impact Factor-based ranking journal as a criteria for\nthe correct distribution of these incomes. We also include the Authorship\nCredit factor for distribution of the incomes among authors, using the\ngeometric progression of Cantor's theory and the Harmonic Credit Index.\nDepending on the ranking of the journal, the proposed model develops a proper\npublication credit allocation among all authors. Moreover, our tool can be\ndeployed in the evaluation of an institution for a funding program, as well as\ncalculating the amounts necessary to incentivize research among personnel.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 02:12:26 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Contreras-Reyes", "Javier E.", ""]]}, {"id": "1606.04319", "submitter": "Matus Medo", "authors": "Matus Medo, Giulio Cimini", "title": "Model-based evaluation of scientific impact indicators", "comments": "12 pages, 8 figures", "journal-ref": "Phys. Rev. E 94, 032312 (2016)", "doi": "10.1103/PhysRevE.94.032312", "report-no": null, "categories": "physics.soc-ph cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using bibliometric data artificially generated through a model of citation\ndynamics calibrated on empirical data, we compare several indicators for the\nscientific impact of individual researchers. The use of such a controlled setup\nhas the advantage of avoiding the biases present in real databases, and allows\nus to assess which aspects of the model dynamics and which traits of individual\nresearchers a particular indicator actually reflects. We find that the simple\ncitation average performs well in capturing the intrinsic scientific ability of\nresearchers, whatever the length of their career. On the other hand, when\nproductivity complements ability in the evaluation process, the notorious $h$\nand $g$ indices reveal their potential, yet their normalized variants do not\nalways yield a fair comparison between researchers at different career stages.\nNotably, the use of logarithmic units for citation counts allows us to build\nsimple indicators with performance equal to that of $h$ and $g$. Our analysis\nmay provide useful hints for a proper use of bibliometric indicators.\nAdditionally, our framework can be extended by including other aspects of the\nscientific production process and citation dynamics, with the potential to\nbecome a standard tool for the assessment of impact metrics.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 11:45:41 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Medo", "Matus", ""], ["Cimini", "Giulio", ""]]}, {"id": "1606.04972", "submitter": "Byung Mook Weon", "authors": "Yeseul Kim, Kun Cho, and Byung Mook Weon", "title": "Physical aging in article page views", "comments": "3 pages with 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics of article page views is useful for measuring the impact of\nindividual articles. Analyzing the temporal evolution of article page views, we\nfind that article page views usually decay over time after reaching a peak,\nespecially exhibiting relaxation with nonexponentiality. This finding suggests\nthat relaxation in article page views resembles physical aging as frequently\nfound in complex systems.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 06:24:36 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Kim", "Yeseul", ""], ["Cho", "Kun", ""], ["Weon", "Byung Mook", ""]]}, {"id": "1606.05157", "submitter": "Uwe Springmann", "authors": "U. Springmann, F. Fink, K. U. Schulz", "title": "Automatic quality evaluation and (semi-) automatic improvement of OCR\n  models for historical printings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good OCR results for historical printings rely on the availability of\nrecognition models trained on diplomatic transcriptions as ground truth, which\nis both a scarce resource and time-consuming to generate. Instead of having to\ntrain a separate model for each historical typeface, we propose a strategy to\nstart from models trained on a combined set of available transcriptions in a\nvariety of fonts. These \\emph{mixed models} result in character accuracy rates\nover 90\\% on a test set of printings from the same period of time, but without\nany representation in the training data, demonstrating the possibility to\novercome the typography barrier by generalizing from a few typefaces to a\nlarger set of (similar) fonts in use over a period of time. The output of these\nmixed models is then used as a baseline to be further improved by both fully\nautomatic methods and semi-automatic methods involving a minimal amount of\nmanual transcriptions. In order to evaluate the recognition quality of each\nmodel in a series of models generated during the training process in the\nabsence of any ground truth, we introduce two readily observable quantities\nthat correlate well with true accuracy. These quantities are \\emph{mean\ncharacter confidence C} (as given by the OCR engine OCRopus) and \\emph{mean\ntoken lexicality L} (a distance measure of OCR tokens from modern wordforms\ntaking historical spelling patterns into account, which can be calculated for\nany OCR engine). Whereas the fully automatic method is able to improve upon the\nresult of a mixed model by only 1-2 percentage points, already 100-200\nhand-corrected lines lead to much better OCR results with character error rates\nof only a few percent. This procedure minimizes the amount of ground truth\nproduction and does not depend on the previous construction of a specific\ntypographic model.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 12:15:14 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 14:09:05 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Springmann", "U.", ""], ["Fink", "F.", ""], ["Schulz", "K. U.", ""]]}, {"id": "1606.05341", "submitter": "Delgado Lopez-Cozar emilio", "authors": "Alberto Mart\\'in-Mart\\'in, Juan Manuel Ayll\\'on, Enrique\n  Ordu\\~na-Malea, Emilio Delgado L\\'opez-C\\'ozar", "title": "Proceedings Scholar Metrics: H Index of proceedings on Computer Science,\n  Electrical & Electronic Engineering, and Communications according to Google\n  Scholar Metrics (2010-2014)", "comments": "36 pages, EC3 reports 15", "journal-ref": null, "doi": "10.13140/RG.2.1.4504.9681", "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The objective of this report is to present a list of proceedings\n(conferences, workshops, symposia, meetings) in the areas of Computer Science,\nElectrical & Electronic Engineering, and Communications covered by Google\nScholar Metrics and ranked according to their h-index. Google Scholar Metrics\nonly displays publications that have published at least 100 papers and have\nreceived at least one citation in the last five years (2010-2014). The searches\nwere conducted between the 8th and 10th of December, 2015. A total of 1501\nproceedings have been identified\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 12:12:47 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Mart\u00edn-Mart\u00edn", "Alberto", ""], ["Ayll\u00f3n", "Juan Manuel", ""], ["Ordu\u00f1a-Malea", "Enrique", ""], ["L\u00f3pez-C\u00f3zar", "Emilio Delgado", ""]]}, {"id": "1606.05609", "submitter": "Jes\\'us Tramullas", "authors": "Jes\\'us Tramullas, Piedad Garrido-Picazo, Ana I. S\\'anchez-Casab\\'on", "title": "Research on Wikipedia Vandalism: a brief literature review", "comments": "Preprint", "journal-ref": "4th Spanish Conference on Information Retrieval CERI 2016", "doi": "10.1145/2934732.2934748", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on vandalism in Wikipedia has been of interest for the last decade.\nThis paper performs a literature review on the subject, with the goal of\nidentifying the main research topics and approaches, methods and techniques\nused. 67 papers have been reviewed. Main topic is the detection of vandalism,\nalthough there is a increasing interest about content quality. The most\ncommonly used technique is machine learning, based on feature analysis. It\ndraws attention to the lack of research on information behavior of vandals.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 17:50:55 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Tramullas", "Jes\u00fas", ""], ["Garrido-Picazo", "Piedad", ""], ["S\u00e1nchez-Casab\u00f3n", "Ana I.", ""]]}, {"id": "1606.05741", "submitter": "Christian Jacobs", "authors": "Christian T. Jacobs, Alexandros Avdis", "title": "Connecting web-based mapping services with scientific data repositories:\n  collaborative curation and retrieval of simulation data via a geospatial\n  interface", "comments": "Submission withdrawn from the International Journal of Digital\n  Curation on 9 September 2016 in order to prepare a joint paper with\n  additional colleagues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increasing quantities of scientific data are becoming readily accessible via\nonline repositories such as those provided by Figshare and Zenodo.\nGeoscientific simulations in particular generate large quantities of data, with\nseveral research groups studying many, often overlapping areas of the world.\nWhen studying a particular area, being able to keep track of one's own\nsimulations as well as those of collaborators can be challenging. This paper\ndescribes the design, implementation, and evaluation of a new tool for visually\ncataloguing and retrieving data associated with a given geographical location\nthrough a web-based Google Maps interface. Each data repository is pin-pointed\non the map with a marker based on the geographical location that the dataset\ncorresponds to. By clicking on the markers, users can quickly inspect the\nmetadata of the repositories and download the associated data files. The crux\nof the approach lies in the ability to easily query and retrieve data from\nmultiple sources via a common interface. While many advances are being made in\nterms of scientific data repositories, the development of this new tool has\nuncovered several issues and limitations of the current state-of-the-art which\nare discussed herein, along with some ideas for the future.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 11:46:10 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 19:56:01 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 08:01:24 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Jacobs", "Christian T.", ""], ["Avdis", "Alexandros", ""]]}, {"id": "1606.05752", "submitter": "Chuxu Zhang", "authors": "Chuxu Zhang, Chuang Liu, Lu Yu, Zi-Ke Zhang and Tao Zhou", "title": "Identifying the Academic Rising Stars", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the fast-rising young researchers (Academic Rising Stars) in the\nfuture provides useful guidance to the research community, e.g., offering\ncompetitive candidates to university for young faculty hiring as they are\nexpected to have success academic careers. In this work, given a set of young\nresearchers who have published the first first-author paper recently, we solve\nthe problem of how to effectively predict the top k% researchers who achieve\nthe highest citation increment in \\Delta t years. We explore a series of\nfactors that can drive an author to be fast-rising and design a novel impact\nincrement ranking learning (IIRL) algorithm that leverages those factors to\npredict the academic rising stars. Experimental results on the large ArnetMiner\ndataset with over 1.7 million authors demonstrate the effectiveness of IIRL.\nSpecifically, it outperforms all given benchmark methods, with over 8% average\nimprovement. Further analysis demonstrates that the prediction models for\ndifferent research topics follow the similar pattern. We also find that\ntemporal features are the best indicators for rising stars prediction, while\nvenue features are less relevant.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 14:01:55 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zhang", "Chuxu", ""], ["Liu", "Chuang", ""], ["Yu", "Lu", ""], ["Zhang", "Zi-Ke", ""], ["Zhou", "Tao", ""]]}, {"id": "1606.05905", "submitter": "Yuxiao Dong", "authors": "Yuxiao Dong, Reid A. Johnson, Nitesh V. Chawla", "title": "Can Scientific Impact Be Predicted?", "comments": "An extension of the ACM WSDM 2016 paper (Will This Paper Increase\n  Your h-index? Scientific Impact Prediction, Cf. arXiv:1412.4754), IEEE\n  Transactions on Big Data, Vol. 2, No. 1, 2016", "journal-ref": null, "doi": "10.1109/TBDATA.2016.2521657", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely used measure of scientific impact is citations. However, due to\ntheir heavy-tailed distribution, citations are fundamentally difficult to\npredict. Instead, to characterize scientific impact, we address two analogous\nquestions asked by many scientific researchers: \"How will my h-index evolve\nover time, and which of my previously or newly published papers will contribute\nto it?\" To answer these questions, we perform two related tasks. First, we\ndevelop a model to predict authors' future h-indices based on their current\nscientific impact. Second, we examine the factors that drive papers---either\npreviously or newly published---to increase their authors' predicted future\nh-indices. By leveraging relevant factors, we can predict an author's h-index\nin five years with an R2 value of 0.92 and whether a previously (newly)\npublished paper will contribute to this future h-index with an F1 score of 0.99\n(0.77). We find that topical authority and publication venue are crucial to\nthese effective predictions, while topic popularity is surprisingly\ninconsequential. Further, we develop an online tool that allows users to\ngenerate informed h-index predictions. Our work demonstrates the predictability\nof scientific impact, and can help scholars to effectively leverage their\nposition of \"standing on the shoulders of giants.\"\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 19:39:49 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Dong", "Yuxiao", ""], ["Johnson", "Reid A.", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1606.06674", "submitter": "S. R. Kulkarni", "authors": "S. R. Kulkarni", "title": "Instruments on large optical telescopes -- A case study", "comments": "Revised from previous submission (typos fixed, table 6 was garbled).\n  Submitted to PASP", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the distant past, telescopes were known, first and foremost, for the sizes\nof their apertures. Advances in technology are now enabling astronomers to\nbuild extremely powerful instruments to the extent that instruments have now\nachieved importance comparable or even exceeding the usual importance accorded\nto the apertures of the telescopes. However, the cost of successive generations\nof instruments has risen at a rate noticeably above that of the rate of\ninflation. Here, given the vast sums of money now being expended on optical\ntelescopes and their instrumentation, I argue that astronomers must undertake\n\"cost-benefit\" analysis for future planning. I use the scientific output of the\nfirst two decades of the W. M. Keck Observatory as a laboratory for this\npurpose. I find, in the absence of upgrades, that the time to reach peak paper\nproduction for an instrument is about six years. The prime lifetime of\ninstruments (sans upgrades), as measured by citations returns, is about a\ndecade. Well thought out and timely upgrades increase and sometimes even double\nthe useful lifetime. I investigate how well instrument builders are rewarded. I\nfind acknowledgements ranging from almost 100% to as low as 60%. Next, given\nthe increasing cost of operating optical telescopes, the management of existing\nobservatories continue to seek new partnerships. This naturally raises the\nquestion \"What is the cost of a single night of telescope time\". I provide a\nrational basis to compute this quantity. I then end the paper with some\nthoughts on the future of large ground-based optical telescopes, bearing in\nmind the explosion of synoptic precision photometric, astrometric and imaging\nsurveys across the electromagnetic spectrum, the increasing cost of\ninstrumentation and the rise of mega instruments.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 17:23:32 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 05:04:33 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Kulkarni", "S. R.", ""]]}, {"id": "1606.08534", "submitter": "Ian Wesley-Smith", "authors": "Ian Wesley-Smith, Carl T. Bergstrom, Jevin D. West", "title": "Static Ranking of Scholarly Papers using Article-Level Eigenfactor\n  (ALEF)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsoft Research hosted the 2016 WSDM Cup Challenge based on the Microsoft\nAcademic Graph. The goal was to provide static rankings for the articles that\nmake up the graph, with the rankings to be evaluated against those of human\njudges. While the Microsoft Academic Graph provided metadata about many aspects\nof each scholarly document, we focused more narrowly on citation data and used\nthis contest as an opportunity to test the Article Level Eigenfactor (ALEF), a\nnovel citation-based ranking algorithm, and evaluate its performance against\ncompeting algorithms that drew upon multiple facets of the data from a large,\nreal world dataset (122M papers and 757M citations). Our final submission to\nthis contest was scored at 0.676, earning second place.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 01:55:56 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Wesley-Smith", "Ian", ""], ["Bergstrom", "Carl T.", ""], ["West", "Jevin D.", ""]]}, {"id": "1606.09136", "submitter": "Herbert Van De Sompel", "authors": "Nicolas J. Bornand, Lyudmila Balakireva, Herbert Van de Sompel", "title": "Routing Memento Requests Using Binary Classifiers", "comments": "10 pages, 6 figures, 11 tables, accepted to be published at JCDL 2016", "journal-ref": null, "doi": "10.1145/2910896.2910899", "report-no": null, "categories": "cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Memento protocol provides a uniform approach to query individual web\narchives. Soon after its emergence, Memento Aggregator infrastructure was\nintroduced that supports querying across multiple archives simultaneously. An\nAggregator generates a response by issuing the respective Memento request\nagainst each of the distributed archives it covers. As the number of archives\ngrows, it becomes increasingly challenging to deliver aggregate responses while\nkeeping response times and computational costs under control. Ad-hoc heuristic\napproaches have been introduced to address this challenge and research has been\nconducted aimed at optimizing query routing based on archive profiles. In this\npaper, we explore the use of binary, archive-specific classifiers generated on\nthe basis of the content cached by an Aggregator, to determine whether or not\nto query an archive for a given URI. Our results turn out to be readily\napplicable and can help to significantly decrease both the number of requests\nand the overall response times without compromising on recall. We find, among\nothers, that classifiers can reduce the average number of requests by 77%\ncompared to a brute force approach on all archives, and the overall response\ntime by 42% while maintaining a recall of 0.847.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 14:54:43 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Bornand", "Nicolas J.", ""], ["Balakireva", "Lyudmila", ""], ["Van de Sompel", "Herbert", ""]]}]