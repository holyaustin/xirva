[{"id": "1407.0085", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall", "title": "Improved Quantum Algorithm for Triangle Finding via Combinatorial\n  Arguments", "comments": "17 pages, to appear in FOCS'14; v2: minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a quantum algorithm solving the triangle finding\nproblem in unweighted graphs with query complexity $\\tilde O(n^{5/4})$, where\n$n$ denotes the number of vertices in the graph. This improves the previous\nupper bound $O(n^{9/7})=O(n^{1.285...})$ recently obtained by Lee, Magniez and\nSantha. Our result shows, for the first time, that in the quantum query\ncomplexity setting unweighted triangle finding is easier than its edge-weighted\nversion, since for finding an edge-weighted triangle Belovs and Rosmanis proved\nthat any quantum algorithm requires $\\Omega(n^{9/7}/\\sqrt{\\log n})$ queries.\nOur result also illustrates some limitations of the non-adaptive learning graph\napproach used to obtain the previous $O(n^{9/7})$ upper bound since, even over\nunweighted graphs, any quantum algorithm for triangle finding obtained using\nthis approach requires $\\Omega(n^{9/7}/\\sqrt{\\log n})$ queries as well. To\nbypass the obstacles characterized by these lower bounds, our quantum algorithm\nuses combinatorial ideas exploiting the graph-theoretic properties of triangle\nfinding, which cannot be used when considering edge-weighted graphs or the\nnon-adaptive learning graph approach.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 00:46:14 GMT"}, {"version": "v2", "created": "Mon, 8 Sep 2014 06:35:43 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1407.0114", "submitter": "Travis Gagie", "authors": "Travis Gagie", "title": "Suffix Arrays for Spaced-SNP Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-nucleotide polymorphisms (SNPs) account for most variations between\nhuman genomes. We show how, if the genomes in a database differ only by a\nreasonable number of SNPs and the substrings between those SNPs are unique,\nthen we can store a fast compressed suffix array for that database.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 06:38:45 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Gagie", "Travis", ""]]}, {"id": "1407.0375", "submitter": "Charalampos Tsourakakis", "authors": "Charalampos E. Tsourakakis", "title": "Mathematical and Algorithmic Analysis of Network and Biological Data", "comments": "Doctorial thesis, 306 pages, Carnegie Mellon University 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM cs.SI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This dissertation contributes to mathematical and algorithmic problems that\narise in the analysis of network and biological data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 02:06:51 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1407.0491", "submitter": "Igor Razgon", "authors": "Igor Razgon", "title": "No small nondeterministic read-once branching programs for CNFs of\n  bounded treewidth", "comments": "Prepared as a 12 pages conference version, thus some proofs are\n  postponed to the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, given a parameter $k$, we demonstrate an infinite class of\n{\\sc cnf}s of treewidth at most $k$ of their primary graphs such that the\nequivalent nondeterministic read-once branching programs ({\\sc nrobp}s) are of\nsize at least $n^{ck}$ for some universal constant $c$. Thus we rule out the\npossibility of fixed-parameter space complexity of {\\sc nrobp}s parameterized\nby the smallest treewidth of the equivalent {\\sc cnf}.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 09:18:36 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Razgon", "Igor", ""]]}, {"id": "1407.0522", "submitter": "Tatiana Starikovskaya", "authors": "Tomasz Kociumaka, Tatiana Starikovskaya, Hjalte Wedel Vildh{\\o}j", "title": "Sublinear Space Algorithms for the Longest Common Substring Problem", "comments": "Accepted to 22nd European Symposium on Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $m$ documents of total length $n$, we consider the problem of finding a\nlongest string common to at least $d \\geq 2$ of the documents. This problem is\nknown as the \\emph{longest common substring (LCS) problem} and has a classic\n$O(n)$ space and $O(n)$ time solution (Weiner [FOCS'73], Hui [CPM'92]).\nHowever, the use of linear space is impractical in many applications. In this\npaper we show that for any trade-off parameter $1 \\leq \\tau \\leq n$, the LCS\nproblem can be solved in $O(\\tau)$ space and $O(n^2/\\tau)$ time, thus providing\nthe first smooth deterministic time-space trade-off from constant to linear\nspace. The result uses a new and very simple algorithm, which computes a\n$\\tau$-additive approximation to the LCS in $O(n^2/\\tau)$ time and $O(1)$\nspace. We also show a time-space trade-off lower bound for deterministic\nbranching programs, which implies that any deterministic RAM algorithm solving\nthe LCS problem on documents from a sufficiently large alphabet in $O(\\tau)$\nspace must use $\\Omega(n\\sqrt{\\log(n/(\\tau\\log n))/\\log\\log(n/(\\tau\\log n)})$\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 11:22:11 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Starikovskaya", "Tatiana", ""], ["Vildh\u00f8j", "Hjalte Wedel", ""]]}, {"id": "1407.0628", "submitter": "Stefano Leucci", "authors": "Davide Bil\\`o Luciano Gual\\`a and Stefano Leucci and Guido Proietti", "title": "Exact and approximate algorithms for movement problems on (special\n  classes of) graphs", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a large collection of objects (e.g., robots, sensors, etc.) has to be\ndeployed in a given environment, it is often required to plan a coordinated\nmotion of the objects from their initial position to a final configuration\nenjoying some global property. In such a scenario, the problem of minimizing\nsome function of the distance travelled, and therefore energy consumption, is\nof vital importance. In this paper we study several motion planning problems\nthat arise when the objects must be moved on a graph, in order to reach certain\ngoals which are of interest for several network applications. Among the others,\nthese goals include broadcasting messages and forming connected or\ninterference-free networks. We study these problems with the aim of minimizing\na number of natural measures such as the average/overall distance travelled,\nthe maximum distance travelled, or the number of objects that need to be moved.\nTo this respect, we provide several approximability and inapproximability\nresults, most of which are tight.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 16:12:45 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Gual\u00e0", "Davide Bil\u00f2 Luciano", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""]]}, {"id": "1407.0637", "submitter": "Stefano Leucci", "authors": "Davide Bil\\`o and Luciano Gual\\`a and Stefano Leucci and Guido\n  Proietti", "title": "Fault-Tolerant Approximate Shortest-Path Trees", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resiliency of a network is its ability to remain \\emph{effectively}\nfunctioning also when any of its nodes or links fails. However, to reduce\noperational and set-up costs, a network should be small in size, and this\nconflicts with the requirement of being resilient. In this paper we address\nthis trade-off for the prominent case of the {\\em broadcasting} routing scheme,\nand we build efficient (i.e., sparse and fast) \\emph{fault-tolerant approximate\nshortest-path trees}, for both the edge and vertex \\emph{single-failure} case.\nIn particular, for an $n$-vertex non-negatively weighted graph, and for any\nconstant $\\varepsilon >0$, we design two structures of size $O(\\frac{n \\log\nn}{\\varepsilon^2})$ which guarantee $(1+\\varepsilon)$-stretched paths from the\nselected source also in the presence of an edge/vertex failure. This favorably\ncompares with the currently best known solutions, which are for the\nedge-failure case of size $O(n)$ and stretch factor 3, and for the\nvertex-failure case of size $O(n \\log n)$ and stretch factor 3. Moreover, we\nalso focus on the unweighted case, and we prove that an ordinary\n$(\\alpha,\\beta)$-spanner can be slightly augmented in order to build efficient\nfault-tolerant approximate \\emph{breadth-first-search trees}.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 16:30:58 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 10:53:20 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Bil\u00f2", "Davide", ""], ["Gual\u00e0", "Luciano", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""]]}, {"id": "1407.0699", "submitter": "Nasr Mohamed", "authors": "Nasr Mohamed", "title": "Enumeration of Spanning Trees Using Edge Exchange with Minimal\n  Partitioning", "comments": "Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, Minimal Partitioning (MP) algorithm, an innovative algorithm\nfor enumerating all the spanning trees in an undirected graph is presented.\n  While MP algorithm uses a computational tree graph to traverse all possible\nspanning trees by the edge exchange technique, it has two unique properties\ncompared to previous algorithms. In the first place, the algorithm maintains a\nstate of minimal partition size in the spanning tree due to edge deletion. This\nis realized by swapping peripheral edges, more precisely leaf edges, in most of\nedge exchange operations. Consequently, the main structure of the spanning\ntrees is preserved during the steps of the enumeration process. This extra\nconstraint proves to be advantageous in many applications where the partition\nsize is a factor in the solution cost. Secondly, we introduce, and utilize, the\nnew concept of edge promotion: the exchanged edges always share one end.\nPractically, and as a result of this property, the interface between the two\npartitions of the spanning tree during edge exchange has to be maintained from\none side only.\n  For a graph $G(V,E)$, MP algorithm requires $O(log V+E/V)$ expected time and\n$OV log V)$ worst case time for generating each spanning tree. MP algorithm\nrequires a total expected space limit of $O(E log V)$ with worst case limit of\n$O(EV)$. Like all edge exchange algorithms, MP algorithm retains the advantage\nof compacted output of $O(1)$ per spanning tree by listing the relative\ndifferences only.\n  Three sample real-world applications of spanning trees enumeration are\nexplored and the effects of using MP algorithm are studied. Namely:\nconstruction of nets of polyhedra, multi-robots spanning tree routing, and\ncomputing the electric current in edges of a network. We report that MP\nalgorithm outperforms other algorithm by $O(V)$ time complexity.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 17:15:28 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Mohamed", "Nasr", ""]]}, {"id": "1407.0892", "submitter": "Sebastian Ott", "authors": "Antonios Antoniadis, Chien-Chung Huang and Sebastian Ott", "title": "A Fully Polynomial-Time Approximation Scheme for Speed Scaling with\n  Sleep State", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study classical deadline-based preemptive scheduling of tasks in a\ncomputing environment equipped with both dynamic speed scaling and sleep state\ncapabilities: Each task is specified by a release time, a deadline and a\nprocessing volume, and has to be scheduled on a single, speed-scalable\nprocessor that is supplied with a sleep state. In the sleep state, the\nprocessor consumes no energy, but a constant wake-up cost is required to\ntransition back to the active state. In contrast to speed scaling alone, the\naddition of a sleep state makes it sometimes beneficial to accelerate the\nprocessing of tasks in order to transition the processor to the sleep state for\nlonger amounts of time and incur further energy savings. The goal is to output\na feasible schedule that minimizes the energy consumption. Since the\nintroduction of the problem by Irani et al. [16], its exact computational\ncomplexity has been repeatedly posed as an open question (see e.g. [2,8,15]).\nThe currently best known upper and lower bounds are a 4/3-approximation\nalgorithm and NP-hardness due to [2] and [2,17], respectively. We close the\naforementioned gap between the upper and lower bound on the computational\ncomplexity of speed scaling with sleep state by presenting a fully\npolynomial-time approximation scheme for the problem. The scheme is based on a\ntransformation to a non-preemptive variant of the problem, and a discretization\nthat exploits a carefully defined lexicographical ordering among schedules.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 12:41:07 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Antoniadis", "Antonios", ""], ["Huang", "Chien-Chung", ""], ["Ott", "Sebastian", ""]]}, {"id": "1407.0950", "submitter": "Carl Barton", "authors": "Carl Barton", "title": "On the Average-case Complexity of Pattern Matching with Wildcards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching with wildcards is the problem of finding all factors of a\ntext $t$ of length $n$ that match a pattern $x$ of length $m$, where wildcards\n(characters that match everything) may be present. In this paper we present a\nnumber of fast average-case algorithms for pattern matching where wildcards are\nrestricted to either the pattern or the text, however, the results are easily\nadapted to the case where wildcards are allowed in both. We analyse the\n\\textit{average-case} complexity of these algorithms and show the first\nnon-trivial time bounds. These are the first results on the average-case\ncomplexity of pattern matching with wildcards which, as a by product, provide\nwith first provable separation in complexity between exact pattern matching and\npattern matching with wildcards in the word RAM model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 15:29:40 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 13:19:24 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Barton", "Carl", ""]]}, {"id": "1407.0958", "submitter": "Vance Faber", "authors": "Vance Faber", "title": "Transpose on vertex symmetric digraphs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss transpose (sometimes called universal exchange or all-to-all) on\nvertex symmetric networks. We provide a method to compare the efficiency of\ntranspose schemes on two different networks with a cost function based on the\nnumber processors and wires needed to complete a given algorithm in a given\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 15:43:36 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Faber", "Vance", ""]]}, {"id": "1407.0961", "submitter": "Feng Shi", "authors": "Feng Shi, Zhiyuan Yan, and Meghanad Wagh", "title": "An Enhanced Multiway Sorting Network Based on n-Sorters", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Merging-based sorting networks are an important family of sorting networks.\nMost merge sorting networks are based on 2-way or multi-way merging algorithms\nusing 2-sorters as basic building blocks. An alternative is to use n-sorters,\ninstead of 2-sorters, as the basic building blocks so as to greatly reduce the\nnumber of sorters as well as the latency. Based on a modified Leighton's\ncolumnsort algorithm, an n-way merging algorithm, referred to as SS-Mk, that\nuses n-sorters as basic building blocks was proposed. In this work, we first\npropose a new multiway merging algorithm with n-sorters as basic building\nblocks that merges n sorted lists of m values each in 1 + ceil(m/2) stages (n\n<= m). Based on our merging algorithm, we also propose a sorting algorithm,\nwhich requires O(N log2 N) basic sorters to sort N inputs. While the asymptotic\ncomplexity (in terms of the required number of sorters) of our sorting\nalgorithm is the same as the SS-Mk, for wide ranges of N, our algorithm\nrequires fewer sorters than the SS-Mk. Finally, we consider a binary sorting\nnetwork, where the basic sorter is implemented in threshold logic and scales\nlinearly with the number of inputs, and compare the complexity in terms of the\nrequired number of gates. For wide ranges of N, our algorithm requires fewer\ngates than the SS-Mk.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 15:47:23 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Shi", "Feng", ""], ["Yan", "Zhiyuan", ""], ["Wagh", "Meghanad", ""]]}, {"id": "1407.1116", "submitter": "C. Seshadhri", "authors": "Jonathan W. Berry and Luke A. Fostvedt and Daniel J. Nordman and\n  Cynthia A. Phillips and C. Seshadhri and Alyson G. Wilson", "title": "Why do simple algorithms for triangle enumeration work in the real\n  world?", "comments": "Conference version in Innovations of Theoretical Computer Science\n  (ITCS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Listing all triangles is a fundamental graph operation. Triangles can have\nimportant interpretations in real-world graphs, especially social and other\ninteraction networks. Despite the lack of provably efficient (linear, or\nslightly super-linear) worst-case algorithms for this problem, practitioners\nrun simple, efficient heuristics to find all triangles in graphs with millions\nof vertices. How are these heuristics exploiting the structure of these special\ngraphs to provide major speedups in running time?\n  We study one of the most prevalent algorithms used by practitioners. A\ntrivial algorithm enumerates all paths of length $2$, and checks if each such\npath is incident to a triangle. A good heuristic is to enumerate only those\npaths of length $2$ where the middle vertex has the lowest degree. It is easily\nimplemented and is empirically known to give remarkable speedups over the\ntrivial algorithm.\n  We study the behavior of this algorithm over graphs with heavy-tailed degree\ndistributions, a defining feature of real-world graphs. The erased\nconfiguration model (ECM) efficiently generates a graph with asymptotically\n(almost) any desired degree sequence. We show that the expected running time of\nthis algorithm over the distribution of graphs created by the ECM is controlled\nby the $\\ell_{4/3}$-norm of the degree sequence. Norms of the degree sequence\nare a measure of the heaviness of the tail, and it is precisely this feature\nthat allows non-trivial speedups of simple triangle enumeration algorithms. As\na corollary of our main theorem, we prove expected linear-time performance for\ndegree sequences following a power law with exponent $\\alpha \\geq 7/3$, and\nnon-trivial speedup whenever $\\alpha \\in (2,3)$.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 03:07:54 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Berry", "Jonathan W.", ""], ["Fostvedt", "Luke A.", ""], ["Nordman", "Daniel J.", ""], ["Phillips", "Cynthia A.", ""], ["Seshadhri", "C.", ""], ["Wilson", "Alyson G.", ""]]}, {"id": "1407.1121", "submitter": "Qiang Ma", "authors": "Qiang Ma, S. Muthukrishnan, Mark Sandler", "title": "Frugal Streaming for Estimating Quantiles:One (or two) memory suffices", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications require processing streams of data for estimating\nstatistical quantities such as quantiles with small amount of memory. In many\nsuch applications, in fact, one needs to compute such statistical quantities\nfor each of a large number of groups, which additionally restricts the amount\nof memory available for the stream for any particular group. We address this\nchallenge and introduce frugal streaming, that is algorithms that work with\ntiny -- typically, sub-streaming -- amount of memory per group.\n  We design a frugal algorithm that uses only one unit of memory per group to\ncompute a quantile for each group. For stochastic streams where data items are\ndrawn from a distribution independently, we analyze and show that the algorithm\nfinds an approximation to the quantile rapidly and remains stably close to it.\nWe also propose an extension of this algorithm that uses two units of memory\nper group. We show with extensive experiments with real world data from HTTP\ntrace and Twitter that our frugal algorithms are comparable to existing\nstreaming algorithms for estimating any quantile, but these existing algorithms\nuse far more space per group and are unrealistic in frugal applications;\nfurther, the two memory frugal algorithm converges significantly faster than\nthe one memory algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 05:12:04 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Ma", "Qiang", ""], ["Muthukrishnan", "S.", ""], ["Sandler", "Mark", ""]]}, {"id": "1407.1167", "submitter": "Stefano Leucci", "authors": "Davide Bil\\`o and Luciano Gual\\`a and Stefano Leucci and Guido\n  Proietti", "title": "Specializations and Generalizations of the Stackelberg Minimum Spanning\n  Tree Game", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let be given a graph $G=(V,E)$ whose edge set is partitioned into a set $R$\nof \\emph{red} edges and a set $B$ of \\emph{blue} edges, and assume that red\nedges are weighted and form a spanning tree of $G$. Then, the \\emph{Stackelberg\nMinimum Spanning Tree} (\\stack) problem is that of pricing (i.e., weighting)\nthe blue edges in such a way that the total weight of the blue edges selected\nin a minimum spanning tree of the resulting graph is maximized. \\stack \\ is\nknown to be \\apx-hard already when the number of distinct red weights is 2. In\nthis paper we analyze some meaningful specializations and generalizations of\n\\stack, which shed some more light on the computational complexity of the\nproblem. More precisely, we first show that if $G$ is restricted to be\n\\emph{complete}, then the following holds: (i) if there are only 2 distinct red\nweights, then the problem can be solved optimally (this contrasts with the\ncorresponding \\apx-hardness of the general problem); (ii) otherwise, the\nproblem can be approximated within $7/4 + \\epsilon$, for any $\\epsilon > 0$.\nAfterwards, we define a natural extension of \\stack, namely that in which blue\nedges have a non-negative \\emph{activation cost} associated, and it is given a\nglobal \\emph{activation budget} that must not be exceeded when pricing blue\nedges. Here, after showing that the very same approximation ratio as that of\nthe original problem can be achieved, we prove that if the spanning tree of red\nedges can be rooted so as that any root-leaf path contains at most $h$ edges,\nthen the problem admits a $(2h+\\epsilon)$-approximation algorithm, for any\n$\\epsilon > 0$.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 09:36:32 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Bil\u00f2", "Davide", ""], ["Gual\u00e0", "Luciano", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""]]}, {"id": "1407.1209", "submitter": "Ricardo Corr\\^ea", "authors": "Ricardo C. Corr\\^ea and Philippe Michelon and Bertrand Le Cun and\n  Thierry Mautor and Diego Delle Donne", "title": "A Bit-Parallel Russian Dolls Search for a Maximum Cardinality Clique in\n  a Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the clique of maximum cardinality in an arbitrary graph is an NP-Hard\nproblem that has many applications, which has motivated studies to solve it\nexactly despite its difficulty. The great majority of algorithms proposed in\nthe literature are based on the Branch and Bound method. In this paper, we\npropose an exact algorithm for the maximum clique problem based on the Russian\nDolls Search method. When compared to Branch and Bound, the main difference of\nthe Russian Dolls method is that the nodes of its search tree correspond to\ndecision subproblems, instead of the optimization subproblems of the Branch and\nBound method. In comparison to a first implementation of this Russian Dolls\nmethod from the literature, several improvements are presented. Some of them\nare adaptations of techniques already employed successfully in Branch and Bound\nalgorithms, like the use of approximate coloring for pruning purposes and\nbit-parallel operations. Two different coloring heuristics are tested: the\nstandard greedy and the greedy with recoloring. Other improvements are directly\nrelated to the Russian Dolls scheme: the adoption of recursive calls where each\nsubproblem (doll) is solved itself via the same principles than the Russian\nDolls Search and the application of an elimination rule allowing not to\ngenerate a significant number of dolls. Results of computational experiments\nshow that the algorithm outperforms the best exact combinatorial algorithms in\nthe literature for the great majority of the dense graphs tested, being more\nthan twice faster in several cases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 12:54:44 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2014 11:15:32 GMT"}, {"version": "v3", "created": "Thu, 28 May 2015 00:30:26 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Corr\u00eaa", "Ricardo C.", ""], ["Michelon", "Philippe", ""], ["Cun", "Bertrand Le", ""], ["Mautor", "Thierry", ""], ["Donne", "Diego Delle", ""]]}, {"id": "1407.1289", "submitter": "Christopher Musco", "authors": "Michael Kapralov and Yin Tat Lee and Cameron Musco and Christopher\n  Musco and Aaron Sidford", "title": "Single Pass Spectral Sparsification in Dynamic Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first single pass algorithm for computing spectral sparsifiers\nof graphs in the dynamic semi-streaming model. Given a single pass over a\nstream containing insertions and deletions of edges to a graph G, our algorithm\nmaintains a randomized linear sketch of the incidence matrix of G into\ndimension O((1/epsilon^2) n polylog(n)). Using this sketch, at any point, the\nalgorithm can output a (1 +/- epsilon) spectral sparsifier for G with high\nprobability.\n  While O((1/epsilon^2) n polylog(n)) space algorithms are known for computing\n\"cut sparsifiers\" in dynamic streams [AGM12b, GKP12] and spectral sparsifiers\nin \"insertion-only\" streams [KL11], prior to our work, the best known single\npass algorithm for maintaining spectral sparsifiers in dynamic streams required\nsketches of dimension Omega((1/epsilon^2) n^(5/3)) [AGM14].\n  To achieve our result, we show that, using a coarse sparsifier of G and a\nlinear sketch of G's incidence matrix, it is possible to sample edges by\neffective resistance, obtaining a spectral sparsifier of arbitrary precision.\nSampling from the sketch requires a novel application of ell_2/ell_2 sparse\nrecovery, a natural extension of the ell_0 methods used for cut sparsifiers in\n[AGM12b]. Recent work of [MP12] on row sampling for matrix approximation gives\na recursive approach for obtaining the required coarse sparsifiers.\n  Under certain restrictions, our approach also extends to the problem of\nmaintaining a spectral approximation for a general matrix A^T A given a stream\nof updates to rows in A.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 18:34:03 GMT"}, {"version": "v2", "created": "Tue, 23 Sep 2014 18:46:08 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 01:52:10 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Kapralov", "Michael", ""], ["Lee", "Yin Tat", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Sidford", "Aaron", ""]]}, {"id": "1407.1428", "submitter": "Avery Miller", "authors": "Avery Miller, Andrzej Pelc", "title": "Fast Rendezvous with Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two mobile agents, starting from different nodes of an $n$-node network at\npossibly different times, have to meet at the same node. This problem is known\nas rendezvous. Agents move in synchronous rounds using a deterministic\nalgorithm. In each round, an agent decides to either remain idle or to move to\none of the adjacent nodes. Each agent has a distinct integer label from the set\n$\\{1,...,L\\}$, which it can use in the execution of the algorithm, but it does\nnot know the label of the other agent.\n  If $D$ is the distance between the initial positions of the agents, then\n$\\Omega(D)$ is an obvious lower bound on the time of rendezvous. However, if\neach agent has no initial knowledge other than its label, time $O(D)$ is\nusually impossible to achieve. We study the minimum amount of information that\nhas to be available a priori to the agents to achieve rendezvous in optimal\ntime $\\Theta(D)$. This information is provided to the agents at the start by an\noracle knowing the entire instance of the problem, i.e., the network, the\nstarting positions of the agents, their wake-up rounds, and both of their\nlabels. The oracle helps the agents by providing them with the same binary\nstring called advice, which can be used by the agents during their navigation.\nThe length of this string is called the size of advice. Our goal is to find the\nsmallest size of advice which enables the agents to meet in time $\\Theta(D)$.\nWe show that this optimal size of advice is $\\Theta(D\\log(n/D)+\\log\\log L)$.\nThe upper bound is proved by constructing an advice string of this size, and\nproviding a natural rendezvous algorithm using this advice that works in time\n$\\Theta(D)$ for all networks. The matching lower bound, which is the main\ncontribution of this paper, is proved by exhibiting classes of networks for\nwhich it is impossible to achieve rendezvous in time $\\Theta(D)$ with smaller\nadvice.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jul 2014 19:33:08 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 13:08:32 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Miller", "Avery", ""], ["Pelc", "Andrzej", ""]]}, {"id": "1407.1507", "submitter": "Sebastian Deorowicz", "authors": "Sebastian Deorowicz and Marek Kokot and Szymon Grabowski and Agnieszka\n  Debudaj-Grabysz", "title": "KMC 2: Fast and resource-frugal $k$-mer counting", "comments": null, "journal-ref": "Bioinformatics 31 (10): 1569-1576 (2015)", "doi": "10.1093/bioinformatics/btv022", "report-no": null, "categories": "cs.DS cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Building the histogram of occurrences of every $k$-symbol long\nsubstring of nucleotide data is a standard step in many bioinformatics\napplications, known under the name of $k$-mer counting. Its applications\ninclude developing de Bruijn graph genome assemblers, fast multiple sequence\nalignment and repeat detection. The tremendous amounts of NGS data require fast\nalgorithms for $k$-mer counting, preferably using moderate amounts of memory.\n  Results: We present a novel method for $k$-mer counting, on large datasets at\nleast twice faster than the strongest competitors (Jellyfish~2, KMC~1), using\nabout 12\\,GB (or less) of RAM memory. Our disk-based method bears some\nresemblance to MSPKmerCounter, yet replacing the original minimizers with\nsignatures (a carefully selected subset of all minimizers) and using $(k,\nx)$-mers allows to significantly reduce the I/O, and a highly parallel overall\narchitecture allows to achieve unprecedented processing speeds. For example,\nKMC~2 allows to count the 28-mers of a human reads collection with 44-fold\ncoverage (106\\,GB of compressed size) in about 20 minutes, on a 6-core Intel i7\nPC with an SSD.\n  Availability: KMC~2 is freely available at http://sun.aei.polsl.pl/kmc.\n  Contact: sebastian.deorowicz@polsl.pl\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 15:39:05 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Deorowicz", "Sebastian", ""], ["Kokot", "Marek", ""], ["Grabowski", "Szymon", ""], ["Debudaj-Grabysz", "Agnieszka", ""]]}, {"id": "1407.1521", "submitter": "Marek Chrobak", "authors": "Marek Chrobak, Kevin Costello, Leszek Gasieniec, Dariusz R. Kowalski", "title": "Information Gathering in Ad-Hoc Radio Networks with Tree Topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of information gathering in ad-hoc radio networks\nwithout collision detection, focussing on the case when the network forms a\ntree, with edges directed towards the root. Initially, each node has a piece of\ninformation that we refer to as a rumor. Our goal is to design protocols that\ndeliver all rumors to the root of the tree as quickly as possible. The protocol\nmust complete this task within its allotted time even though the actual tree\ntopology is unknown when the computation starts. In the deterministic case,\nassuming that the nodes are labeled with small integers, we give an O(n)-time\nprotocol that uses unbounded messages, and an O(n log n)-time protocol using\nbounded messages, where any message can include only one rumor. We also\nconsider fire-and-forward protocols, in which a node can only transmit its own\nrumor or the rumor received in the previous step. We give a deterministic\nfire-and- forward protocol with running time O(n^1.5), and we show that it is\nasymptotically optimal. We then study randomized algorithms where the nodes are\nnot labelled. In this model, we give an O(n log n)-time protocol and we prove\nthat this bound is asymptotically optimal.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 18:24:27 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Chrobak", "Marek", ""], ["Costello", "Kevin", ""], ["Gasieniec", "Leszek", ""], ["Kowalski", "Dariusz R.", ""]]}, {"id": "1407.1525", "submitter": "Iyad Kanj", "authors": "Iyad Kanj, Eric Sedgwick, and Ge Xia", "title": "Computing the flip distance between triangulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${\\cal T}$ be a triangulation of a set ${\\cal P}$ of $n$ points in the\nplane, and let $e$ be an edge shared by two triangles in ${\\cal T}$ such that\nthe quadrilateral $Q$ formed by these two triangles is convex. A {\\em flip} of\n$e$ is the operation of replacing $e$ by the other diagonal of $Q$ to obtain a\nnew triangulation of ${\\cal P}$ from ${\\cal T}$. The {\\em flip distance}\nbetween two triangulations of ${\\cal P}$ is the minimum number of flips needed\nto transform one triangulation into the other. The Flip Distance problem asks\nif the flip distance between two given triangulations of ${\\cal P}$ is at most\n$k$, for some given $k \\in N$. It is a fundamental and a challenging problem.\n  We present an algorithm for the {\\sc Flip Distance} problem that runs in time\n$O(n + k \\cdot c^{k})$, for a constant $c \\leq 2 \\cdot 14^{11}$, which implies\nthat the problem is fixed-parameter tractable. We extend our results to\ntriangulations of polygonal regions with holes, and to labeled triangulated\ngraphs.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 18:46:17 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 22:00:38 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Kanj", "Iyad", ""], ["Sedgwick", "Eric", ""], ["Xia", "Ge", ""]]}, {"id": "1407.1537", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Lorenzo Orecchia", "title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent", "comments": "A new section added; polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order methods play a central role in large-scale machine learning. Even\nthough many variations exist, each suited to a particular problem, almost all\nsuch methods fundamentally rely on two types of algorithmic steps: gradient\ndescent, which yields primal progress, and mirror descent, which yields dual\nprogress.\n  We observe that the performances of gradient and mirror descent are\ncomplementary, so that faster algorithms can be designed by LINEARLY COUPLING\nthe two. We show how to reconstruct Nesterov's accelerated gradient methods\nusing linear coupling, which gives a cleaner interpretation than Nesterov's\noriginal proofs. We also discuss the power of linear coupling by extending it\nto many other settings that Nesterov's methods cannot apply to.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:11:48 GMT"}, {"version": "v2", "created": "Sat, 9 Aug 2014 01:48:01 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 06:59:10 GMT"}, {"version": "v4", "created": "Fri, 2 Jan 2015 17:41:24 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 19:30:37 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1407.1543", "submitter": "David Steurer", "authors": "Boaz Barak, Jonathan A. Kelner, David Steurer", "title": "Dictionary Learning and Tensor Decomposition via the Sum-of-Squares\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new approach to the dictionary learning (also known as \"sparse\ncoding\") problem of recovering an unknown $n\\times m$ matrix $A$ (for $m \\geq\nn$) from examples of the form \\[ y = Ax + e, \\] where $x$ is a random vector in\n$\\mathbb R^m$ with at most $\\tau m$ nonzero coordinates, and $e$ is a random\nnoise vector in $\\mathbb R^n$ with bounded magnitude. For the case $m=O(n)$,\nour algorithm recovers every column of $A$ within arbitrarily good constant\naccuracy in time $m^{O(\\log m/\\log(\\tau^{-1}))}$, in particular achieving\npolynomial time if $\\tau = m^{-\\delta}$ for any $\\delta>0$, and time $m^{O(\\log\nm)}$ if $\\tau$ is (a sufficiently small) constant. Prior algorithms with\ncomparable assumptions on the distribution required the vector $x$ to be much\nsparser---at most $\\sqrt{n}$ nonzero coordinates---and there were intrinsic\nbarriers preventing these algorithms from applying for denser $x$.\n  We achieve this by designing an algorithm for noisy tensor decomposition that\ncan recover, under quite general conditions, an approximate rank-one\ndecomposition of a tensor $T$, given access to a tensor $T'$ that is\n$\\tau$-close to $T$ in the spectral norm (when considered as a matrix). To our\nknowledge, this is the first algorithm for tensor decomposition that works in\nthe constant spectral-norm noise regime, where there is no guarantee that the\nlocal optima of $T$ and $T'$ have similar structures.\n  Our algorithm is based on a novel approach to using and analyzing the Sum of\nSquares semidefinite programming hierarchy (Parrilo 2000, Lasserre 2001), and\nit can be viewed as an indication of the utility of this very general and\npowerful tool for unsupervised learning problems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:42:05 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 21:32:44 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Barak", "Boaz", ""], ["Kelner", "Jonathan A.", ""], ["Steurer", "David", ""]]}, {"id": "1407.1571", "submitter": "Jonathan Ullman", "authors": "Jonathan Ullman", "title": "Private Multiplicative Weights Beyond Linear Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of fundamental data analyses in machine learning, such as\nlinear and logistic regression, require minimizing a convex function defined by\nthe data. Since the data may contain sensitive information about individuals,\nand these analyses can leak that sensitive information, it is important to be\nable to solve convex minimization in a privacy-preserving way.\n  A series of recent results show how to accurately solve a single convex\nminimization problem in a differentially private manner. However, the same data\nis often analyzed repeatedly, and little is known about solving multiple convex\nminimization problems with differential privacy. For simpler data analyses,\nsuch as linear queries, there are remarkable differentially private algorithms\nsuch as the private multiplicative weights mechanism (Hardt and Rothblum, FOCS\n2010) that accurately answer exponentially many distinct queries. In this work,\nwe extend these results to the case of convex minimization and show how to give\naccurate and differentially private solutions to *exponentially many* convex\nminimization problems on a sensitive dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 02:51:37 GMT"}, {"version": "v2", "created": "Fri, 26 Sep 2014 18:43:19 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2015 19:21:33 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Ullman", "Jonathan", ""]]}, {"id": "1407.1689", "submitter": "Ragesh Jaiswal", "authors": "Anup Bhattacharya, Davis Issac, Ragesh Jaiswal, Amit Kumar", "title": "Sampling in Space Restricted Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space efficient algorithms play a central role in dealing with large amount\nof data. In such settings, one would like to analyse the large data using small\namount of \"working space\". One of the key steps in many algorithms for\nanalysing large data is to maintain a (or a small number) random sample from\nthe data points. In this paper, we consider two space restricted settings --\n(i) streaming model, where data arrives over time and one can use only a small\namount of storage, and (ii) query model, where we can structure the data in low\nspace and answer sampling queries. In this paper, we prove the following\nresults in above two settings:\n  - In the streaming setting, we would like to maintain a random sample from\nthe elements seen so far. We prove that one can maintain a random sample using\n$O(\\log n)$ random bits and $O(\\log n)$ space, where $n$ is the number of\nelements seen so far. We can extend this to the case when elements have weights\nas well.\n  - In the query model, there are $n$ elements with weights $w_1, ..., w_n$\n(which are $w$-bit integers) and one would like to sample a random element with\nprobability proportional to its weight. Bringmann and Larsen (STOC 2013) showed\nhow to sample such an element using $nw +1 $ space (whereas, the information\ntheoretic lower bound is $n w$). We consider the approximate sampling problem,\nwhere we are given an error parameter $\\varepsilon$, and the sampling\nprobability of an element can be off by an $\\varepsilon$ factor. We give\nmatching upper and lower bounds for this problem.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 12:48:06 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 08:38:51 GMT"}, {"version": "v3", "created": "Fri, 16 Jan 2015 04:53:18 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Bhattacharya", "Anup", ""], ["Issac", "Davis", ""], ["Jaiswal", "Ragesh", ""], ["Kumar", "Amit", ""]]}, {"id": "1407.1706", "submitter": "Ignasi Sau", "authors": "Henri Perret du Cray and Ignasi Sau", "title": "Improved FPT algorithms for weighted independent set in bull-free graphs", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very recently, Thomass\\'e, Trotignon and Vuskovic [WG 2014] have given an FPT\nalgorithm for Weighted Independent Set in bull-free graphs parameterized by the\nweight of the solution, running in time $2^{O(k^5)} \\cdot n^9$. In this article\nwe improve this running time to $2^{O(k^2)} \\cdot n^7$. As a byproduct, we also\nimprove the previous Turing-kernel for this problem from $O(k^5)$ to $O(k^2)$.\nFurthermore, for the subclass of bull-free graphs without holes of length at\nmost $2p-1$ for $p \\geq 3$, we speed up the running time to $2^{O(k \\cdot\nk^{\\frac{1}{p-1}})} \\cdot n^7$. As $p$ grows, this running time is\nasymptotically tight in terms of $k$, since we prove that for each integer $p\n\\geq 3$, Weighted Independent Set cannot be solved in time $2^{o(k)} \\cdot\nn^{O(1)}$ in the class of $\\{bull,C_4,\\ldots,C_{2p-1}\\}$-free graphs unless the\nETH fails.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 13:28:22 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Cray", "Henri Perret du", ""], ["Sau", "Ignasi", ""]]}, {"id": "1407.1746", "submitter": "Adam Kurpisz", "authors": "Adam Kurpisz, Samuli Lepp\\\"anen, Monaldo Mastrolilli", "title": "Sum-of-squares hierarchy lower bounds for symmetric formulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for proving Sum-of-Squares (SoS)/ Lasserre hierarchy\nlower bounds when the initial problem formulation exhibits a high degree of\nsymmetry. Our main technical theorem allows us to reduce the study of the\npositive semidefiniteness to the analysis of \"well-behaved\" univariate\npolynomial inequalities.\n  We illustrate the technique on two problems, one unconstrained and the other\nwith constraints. More precisely, we give a short elementary proof of\nGrigoriev/Laurent lower bound for finding the integer cut polytope of the\ncomplete graph. We also show that the SoS hierarchy requires a non-constant\nnumber of rounds to improve the initial integrality gap of 2 for the\nMin-Knapsack linear program strengthened with cover inequalities.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 15:37:56 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 16:43:25 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Kurpisz", "Adam", ""], ["Lepp\u00e4nen", "Samuli", ""], ["Mastrolilli", "Monaldo", ""]]}, {"id": "1407.1910", "submitter": "Seth Pettie", "authors": "Seth Pettie", "title": "Sensitivity Analysis of Minimum Spanning Trees in Sub-Inverse-Ackermann\n  Time", "comments": "Extended abstract appeared in ISAAC 2005", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic algorithm for computing the sensitivity of a\nminimum spanning tree (MST) or shortest path tree in $O(m\\log\\alpha(m,n))$\ntime, where $\\alpha$ is the inverse-Ackermann function. This improves upon a\nlong standing bound of $O(m\\alpha(m,n))$ established by Tarjan. Our algorithms\nare based on an efficient split-findmin data structure, which maintains a\ncollection of sequences of weighted elements that may be split into smaller\nsubsequences. As far as we are aware, our split-findmin algorithm is the first\nwith superlinear but sub-inverse-Ackermann complexity. We also give a reduction\nfrom MST sensitivity to the MST problem itself. Together with the randomized\nlinear time MST algorithm of Karger, Klein, and Tarjan, this gives another\nrandomized linear time MST sensitivity algoritm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 00:18:02 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Pettie", "Seth", ""]]}, {"id": "1407.1925", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Lorenzo Orecchia", "title": "Using Optimization to Solve Positive LPs Faster in Parallel", "comments": "polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive linear programs (LP), also known as packing and covering linear\nprograms, are an important class of problems that bridges computer science,\noperations research, and optimization. Despite the consistent efforts on this\nproblem, all known nearly-linear-time algorithms require\n$\\tilde{O}(\\varepsilon^{-4})$ iterations to converge to $1\\pm \\varepsilon$\napproximate solutions. This $\\varepsilon^{-4}$ dependence has not been improved\nsince 1993, and limits the performance of parallel implementations for such\nalgorithms. Moreover, previous algorithms and their analyses rely on update\nsteps and convergence arguments that are combinatorial in nature and do not\nseem to arise naturally from an optimization viewpoint.\n  In this paper, we leverage new insights from optimization theory to construct\na novel algorithm that breaks the longstanding $\\varepsilon^{-4}$ barrier. Our\nalgorithm has a simple analysis and a clear motivation. Our work introduces a\nnumber of novel techniques, such as the combined application of gradient\ndescent and mirror descent, and a truncated, smoothed version of the standard\nmultiplicative weight update, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 01:58:11 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 07:03:14 GMT"}, {"version": "v3", "created": "Sun, 13 Nov 2016 05:23:54 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1407.1931", "submitter": "Shaileshh Bojja Venkatakrishnan", "authors": "Shaileshh Bojja Venkatakrishnan, Pramod Viswanath", "title": "Deterministic Near-Optimal P2P Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider streaming over a peer-to-peer network with homogeneous nodes in\nwhich a single source broadcasts a data stream to all the users in the system.\nPeers are allowed to enter or leave the system (adversarially) arbitrarily.\nPrevious approaches for streaming in this setting have either used randomized\ndistribution graphs or structured trees with randomized maintenance algorithms.\nRandomized graphs handle peer churn well but have poor connectivity guarantees,\nwhile structured trees have good connectivity but have proven hard to maintain\nunder peer churn. We improve upon both approaches by presenting a novel\ndistribution structure with a deterministic and distributed algorithm for\nmaintenance under peer churn; our result is inspired by a recent work proposing\ndeterministic algorithms for rumor spreading in graphs. A key innovation in our\napproach is in having redundant links in the distribution structure. While this\nleads to a reduction in the maximum streaming rate possible, we show that for\nthe amount of redundancy used, the delay guarantee of the proposed algorithm is\nnear optimal. We introduce a tolerance parameter that captures the worst-case\ntransient streaming rate received by the peers during churn events and\ncharacterize the fundamental tradeoff between rate, delay and tolerance. A\nnatural generalization of the deterministic algorithm achieves this tradeoff\nnear optimally. Finally, the proposed deterministic algorithm is robust enough\nto handle various generalizations: ability to deal with heterogeneous node\ncapacities of the peers and more complicated streaming patterns where multiple\nsource transmissions are present.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 02:37:17 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Venkatakrishnan", "Shaileshh Bojja", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1407.2033", "submitter": "Meirav Zehavi", "authors": "Hadas Shachnai and Meirav Zehavi", "title": "A Multivariate Framework for Weighted FPT Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel multivariate approach for solving weighted parameterized\nproblems. In our model, given an instance of size $n$ of a minimization\n(maximization) problem, and a parameter $W \\geq 1$, we seek a solution of\nweight at most (or at least) $W$. We use our general framework to obtain\nefficient algorithms for such fundamental graph problems as Vertex Cover,\n3-Hitting Set, Edge Dominating Set and Max Internal Out-Branching. The best\nknown algorithms for these problems admit running times of the form $c^W\nn^{O(1)}$, for some constant $c>1$. We improve these running times to $c^s\nn^{O(1)}$, where $s\\leq W$ is the minimum size of a solution of weight at most\n(at least) $W$. If no such solution exists, $s=\\min\\{W,m\\}$, where $m$ is the\nmaximum size of a solution. Clearly, $s$ can be substantially smaller than $W$.\nIn particular, the running times of our algorithms are (almost) the same as the\nbest known $O^*$ running times for the unweighted variants. Thus, we solve the\nweighted versions of\n  * Vertex Cover in $1.381^s n^{O(1)}$ time and $n^{O(1)}$ space.\n  * 3-Hitting Set in $2.168^s n^{O(1)}$ time and $n^{O(1)}$ space.\n  * Edge Dominating Set in $2.315^s n^{O(1)}$ time and $n^{O(1)}$ space.\n  * Max Internal Out-Branching in $6.855^s n^{O(1)}$ time and space.\n  We further show that Weighted Vertex Cover and Weighted Edge Dominating Set\nadmit fast algorithms whose running times are of the form $c^t n^{O(1)}$, where\n$t \\leq s$ is the minimum size of a solution.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 11:00:00 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 11:48:16 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Shachnai", "Hadas", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1407.2036", "submitter": "Mamadou Moustapha Kant\\'e", "authors": "Mamadou Moustapha Kant\\'e and Vincent Limouzy and Arnaud Mary and\n  Lhouari Nourine and Takeaki Uno", "title": "A Polynomial Delay Algorithm for Enumerating Minimal Dominating Sets in\n  Chordal Graphs", "comments": "13 pages, 1 figure, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An output-polynomial algorithm for the listing of minimal dominating sets in\ngraphs is a challenging open problem and is known to be equivalent to the\nwell-known Transversal problem which asks for an output-polynomial algorithm\nfor listing the set of minimal hitting sets in hypergraphs. We give a\npolynomial delay algorithm to list the set of minimal dominating sets in\nchordal graphs, an important and well-studied graph class where such an\nalgorithm was open for a while.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 11:17:19 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Kant\u00e9", "Mamadou Moustapha", ""], ["Limouzy", "Vincent", ""], ["Mary", "Arnaud", ""], ["Nourine", "Lhouari", ""], ["Uno", "Takeaki", ""]]}, {"id": "1407.2053", "submitter": "Mamadou Moustapha Kant\\'e", "authors": "Mamadou Moustapha Kant\\'e and Vincent Limouzy and Arnaud Mary and\n  Lhouari Nourine", "title": "On the Enumeration of Minimal Dominating Sets and Related Notions", "comments": "15 pages, 3 figures, In revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dominating set $D$ in a graph is a subset of its vertex set such that each\nvertex is either in $D$ or has a neighbour in $D$. In this paper, we are\ninterested in the enumeration of (inclusion-wise) minimal dominating sets in\ngraphs, called the Dom-Enum problem. It is well known that this problem can be\npolynomially reduced to the Trans-Enum problem in hypergraphs, i.e., the\nproblem of enumerating all minimal transversals in a hypergraph. Firstly we\nshow that the Trans-Enum problem can be polynomially reduced to the Dom-Enum\nproblem. As a consequence there exists an output-polynomial time algorithm for\nthe Trans-Enum problem if and only if there exists one for the Dom-Enum\nproblem. Secondly, we study the Dom-Enum problem in some graph classes. We give\nan output-polynomial time algorithm for the Dom-Enum problem in split graphs,\nand introduce the completion of a graph to obtain an output-polynomial time\nalgorithm for the Dom-Enum problem in $P_6$-free chordal graphs, a proper\nsuperclass of split graphs. Finally, we investigate the complexity of the\nenumeration of (inclusion-wise) minimal connected dominating sets and minimal\ntotal dominating sets of graphs. We show that there exists an output-polynomial\ntime algorithm for the Dom-Enum problem (or equivalently Trans-Enum problem) if\nand only if there exists one for the following enumeration problems: minimal\ntotal dominating sets, minimal total dominating sets in split graphs, minimal\nconnected dominating sets in split graphs, minimal dominating sets in\nco-bipartite graphs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 12:03:01 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Kant\u00e9", "Mamadou Moustapha", ""], ["Limouzy", "Vincent", ""], ["Mary", "Arnaud", ""], ["Nourine", "Lhouari", ""]]}, {"id": "1407.2109", "submitter": "Krzysztof Onak", "authors": "Artur Czumaj, Morteza Monemizadeh, Krzysztof Onak, Christian Sohler", "title": "Planar Graphs: Random Walks and Bipartiteness Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of property testing in arbitrary planar graphs. We\nprove that bipartiteness can be tested in constant time, improving on the\nprevious bound of $\\tilde{O}(\\sqrt{n})$ for graphs on $n$ vertices. The\nconstant-time testability was only known for planar graphs with bounded degree.\n  Our algorithm is based on random walks. Since planar graphs have good\nseparators, i.e., bad expansion, our analysis diverges from standard techniques\nthat involve the fast convergence of random walks on expanders. We reduce the\nproblem to the task of detecting an odd-parity cycle in a multigraph induced by\nconstant-length cycles. We iteratively reduce the length of cycles while\npreserving the detection probability, until the multigraph collapses to a\ncollection of easily discoverable self-loops.\n  Our approach extends to arbitrary minor-free graphs. We also believe that our\ntechniques will find applications to testing other properties in arbitrary\nminor-free graphs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 14:33:52 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 20:36:41 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 13:57:11 GMT"}, {"version": "v4", "created": "Fri, 21 Dec 2018 20:13:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Czumaj", "Artur", ""], ["Monemizadeh", "Morteza", ""], ["Onak", "Krzysztof", ""], ["Sohler", "Christian", ""]]}, {"id": "1407.2151", "submitter": "Jelani Nelson", "authors": "Kasper Green Larsen, Jelani Nelson, Huy L. Nguyen", "title": "Time lower bounds for nonadaptive turnstile streaming algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We say a turnstile streaming algorithm is \"non-adaptive\" if, during updates,\nthe memory cells written and read depend only on the index being updated and\nrandom coins tossed at the beginning of the stream (and not on the memory\ncontents of the algorithm). Memory cells read during queries may be decided\nupon adaptively. All known turnstile streaming algorithms in the literature are\nnon-adaptive.\n  We prove the first non-trivial update time lower bounds for both randomized\nand deterministic turnstile streaming algorithms, which hold when the\nalgorithms are non-adaptive. While there has been abundant success in proving\nspace lower bounds, there have been no non-trivial update time lower bounds in\nthe turnstile model. Our lower bounds hold against classically studied problems\nsuch as heavy hitters, point query, entropy estimation, and moment estimation.\nIn some cases of deterministic algorithms, our lower bounds nearly match known\nupper bounds.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 16:09:01 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Larsen", "Kasper Green", ""], ["Nelson", "Jelani", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1407.2178", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Rati Gelashvili, Ilya Razenshteyn", "title": "Restricted Isometry Property for General p-Norms", "comments": "An extended abstract of this paper is to appear at the 31st\n  International Symposium on Computational Geometry (SoCG 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT math.IT math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Restricted Isometry Property (RIP) is a fundamental property of a matrix\nwhich enables sparse recovery. Informally, an $m \\times n$ matrix satisfies RIP\nof order $k$ for the $\\ell_p$ norm, if $\\|Ax\\|_p \\approx \\|x\\|_p$ for every\nvector $x$ with at most $k$ non-zero coordinates.\n  For every $1 \\leq p < \\infty$ we obtain almost tight bounds on the minimum\nnumber of rows $m$ necessary for the RIP property to hold. Prior to this work,\nonly the cases $p = 1$, $1 + 1 / \\log k$, and $2$ were studied. Interestingly,\nour results show that the case $p = 2$ is a \"singularity\" point: the optimal\nnumber of rows $m$ is $\\widetilde{\\Theta}(k^{p})$ for all $p\\in\n[1,\\infty)\\setminus \\{2\\}$, as opposed to $\\widetilde{\\Theta}(k)$ for $k=2$.\n  We also obtain almost tight bounds for the column sparsity of RIP matrices\nand discuss implications of our results for the Stable Sparse Recovery problem.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 17:16:49 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 18:00:12 GMT"}, {"version": "v3", "created": "Sun, 22 Feb 2015 08:16:33 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Gelashvili", "Rati", ""], ["Razenshteyn", "Ilya", ""]]}, {"id": "1407.2407", "submitter": "Filip Paveti\\'c", "authors": "Filip Paveti\\'c, Goran \\v{Z}u\\v{z}i\\'c, Mile \\v{S}iki\\'c", "title": "$LCSk$++: Practical similarity metric for long strings", "comments": null, "journal-ref": "Proceedings of the Prague Stringology Conference 2018, (2018)\n  50-60", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present $LCSk$++: a new metric for measuring the similarity\nof long strings, and provide an algorithm for its efficient computation. With\never increasing size of strings occuring in practice, e.g. large genomes of\nplants and animals, classic algorithms such as Longest Common Subsequence (LCS)\nfail due to demanding computational complexity. Recently, Benson et al. defined\na similarity metric named $LCSk$. By relaxing the requirement that the\n$k$-length substrings should not overlap, we extend their definition into a new\nmetric. An efficient algorithm is presented which computes $LCSk$++ with\ncomplexity of $O((|X|+|Y|)\\log(|X|+|Y|))$ for strings $X$ and $Y$ under a\nrealistic random model. The algorithm has been designed with implementation\nsimplicity in mind. Additionally, we describe how it can be adjusted to compute\n$LCSk$ as well, which gives an improvement of the $O(|X|\\dot|Y|)$ algorithm\npresented in the original $LCSk$ paper.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 09:42:36 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Paveti\u0107", "Filip", ""], ["\u017du\u017ei\u0107", "Goran", ""], ["\u0160iki\u0107", "Mile", ""]]}, {"id": "1407.2479", "submitter": "Zhiyi Huang", "authors": "Zhiyi Huang and Yishay Mansour and Tim Roughgarden", "title": "Making the Most of Your Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of setting a price for a potential buyer with a\nvaluation drawn from an unknown distribution $D$. The seller has \"data\"' about\n$D$ in the form of $m \\ge 1$ i.i.d. samples, and the algorithmic challenge is\nto use these samples to obtain expected revenue as close as possible to what\ncould be achieved with advance knowledge of $D$.\n  Our first set of results quantifies the number of samples $m$ that are\nnecessary and sufficient to obtain a $(1-\\epsilon)$-approximation. For example,\nfor an unknown distribution that satisfies the monotone hazard rate (MHR)\ncondition, we prove that $\\tilde{\\Theta}(\\epsilon^{-3/2})$ samples are\nnecessary and sufficient. Remarkably, this is fewer samples than is necessary\nto accurately estimate the expected revenue obtained by even a single reserve\nprice. We also prove essentially tight sample complexity bounds for regular\ndistributions, bounded-support distributions, and a wide class of irregular\ndistributions. Our lower bound approach borrows tools from differential privacy\nand information theory, and we believe it could find further applications in\nauction theory.\n  Our second set of results considers the single-sample case. For regular\ndistributions, we prove that no pricing strategy is better than\n$\\tfrac{1}{2}$-approximate, and this is optimal by the Bulow-Klemperer theorem.\nFor MHR distributions, we show how to do better: we give a simple pricing\nstrategy that guarantees expected revenue at least $0.589$ times the maximum\npossible. We also prove that no pricing strategy achieves an approximation\nguarantee better than $\\frac{e}{4} \\approx .68$.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 13:46:38 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 14:56:29 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Huang", "Zhiyi", ""], ["Mansour", "Yishay", ""], ["Roughgarden", "Tim", ""]]}, {"id": "1407.2511", "submitter": "Adrian Kosowski", "authors": "Dariusz Dereniowski, Adrian Kosowski (INRIA Paris-Rocquencourt,\n  LIAFA), Dominik Pajak", "title": "Distinguishing Views in Symmetric Networks: A Tight Lower Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The view of a node in a port-labeled network is an infinite tree encoding all\nwalks in the network originating from this node. We prove that for any integers\n$n\\geq D\\geq 1$, there exists a port-labeled network with at most $n$ nodes and\ndiameter at most $D$ which contains a pair of nodes whose (infinite) views are\ndifferent, but whose views truncated to depth $\\Omega(D\\log (n/D))$ are\nidentical.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 06:06:33 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 09:54:27 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Dereniowski", "Dariusz", "", "INRIA Paris-Rocquencourt,\n  LIAFA"], ["Kosowski", "Adrian", "", "INRIA Paris-Rocquencourt,\n  LIAFA"], ["Pajak", "Dominik", ""]]}, {"id": "1407.2524", "submitter": "Alantha Newman", "authors": "Alantha Newman", "title": "An improved analysis of the M\\\"omke-Svensson algorithm for graph-TSP on\n  subquartic graphs", "comments": "Journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  M\\\"omke and Svensson presented a beautiful new approach for the traveling\nsalesman problem on a graph metric (graph-TSP), which yields a\n$4/3$-approximation guarantee on subcubic graphs as well as a substantial\nimprovement over the $3/2$-approximation guarantee of Christofides' algorithm\non general graphs. The crux of their approach is to compute an upper bound on\nthe minimum cost of a circulation in a particular network, $C(G,T)$, where $G$\nis the input graph and $T$ is a carefully chosen spanning tree. The cost of\nthis circulation is directly related to the number of edges in a tour output by\ntheir algorithm. Mucha subsequently improved the analysis of the circulation\ncost, proving that M\\\"omke and Svensson's algorithm for graph-TSP has an\napproximation ratio of at most $13/9$ on general graphs.\n  This analysis of the circulation is local, and vertices with degree four and\nfive can contribute the most to its cost. Thus, hypothetically, there could\nexist a subquartic graph (a graph with degree at most four at each vertex) for\nwhich Mucha's analysis of the M\\\"omke-Svensson algorithm is tight. We show that\nthis is not the case and that M\\\"omke and Svensson's algorithm for graph-TSP\nhas an approximation guarantee of at most $25/18$ on subquartic graphs. To\nprove this, we present different methods to upper bound the minimum cost of a\ncirculation on the network $C(G,T)$. Our approximation guarantee holds for all\ngraphs that have an optimal solution to a standard linear programming\nrelaxation of graph-TSP with subquartic support.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 15:32:51 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 08:40:33 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 11:07:42 GMT"}, {"version": "v4", "created": "Tue, 3 Mar 2020 13:32:53 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Newman", "Alantha", ""]]}, {"id": "1407.2569", "submitter": "John Peebles", "authors": "Jerry Li and John Peebles", "title": "Replacing Mark Bits with Randomness in Fibonacci Heaps", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Fibonacci heap is a deterministic data structure implementing a priority\nqueue with optimal amortized operation costs. An unfortunate aspect of\nFibonacci heaps is that they must maintain a \"mark bit\" which serves only to\nensure efficiency of heap operations, not correctness. Karger proposed a simple\nrandomized variant of Fibonacci heaps in which mark bits are replaced by coin\nflips. This variant still has expected amortized cost $O(1)$ for insert,\ndecrease-key, and merge. Karger conjectured that this data structure has\nexpected amortized cost $O(\\log s)$ for delete-min, where $s$ is the number of\nheap operations.\n  We give a tight analysis of Karger's randomized Fibonacci heaps, resolving\nKarger's conjecture. Specifically, we obtain matching upper and lower bounds of\n$\\Theta(\\log^2 s / \\log \\log s)$ for the runtime of delete-min. We also prove a\ntight lower bound of $\\Omega(\\sqrt{n})$ on delete-min in terms of the number of\nheap elements $n$. The request sequence used to prove this bound also solves an\nopen problem of Fredman on whether cascading cuts are necessary. Finally, we\ngive a simple additional modification to these heaps which yields a tight\nruntime $O(\\log^2 n / \\log \\log n)$ for delete-min.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 17:43:25 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 18:12:07 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 16:21:16 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Li", "Jerry", ""], ["Peebles", "John", ""]]}, {"id": "1407.2575", "submitter": "Ali Pourmiri", "authors": "Ali Pourmiri", "title": "Balanced Allocation on Graphs: A Random Walk Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose algorithms for allocating $n$ sequential balls into\n$n$ bins that are interconnected as a $d$-regular $n$-vertex graph $G$, where\n$d\\ge3$ can be any integer.Let $l$ be a given positive integer. In each round\n$t$, $1\\le t\\le n$, ball $t$ picks a node of $G$ uniformly at random and\nperforms a non-backtracking random walk of length $l$ from the chosen node.Then\nit allocates itself on one of the visited nodes with minimum load (ties are\nbroken uniformly at random). Suppose that $G$ has a sufficiently large girth\nand $d=\\omega(\\log n)$. Then we establish an upper bound for the maximum number\nof balls at any bin after allocating $n$ balls by the algorithm, called {\\it\nmaximum load}, in terms of $l$ with high probability. We also show that the\nupper bound is at most an $O(\\log\\log n)$ factor above the lower bound that is\nproved for the algorithm. In particular, we show that if we set $l=\\lfloor(\\log\nn)^{\\frac{1+\\epsilon}{2}}\\rfloor$, for every constant $\\epsilon\\in (0, 1)$, and\n$G$ has girth at least $\\omega(l)$, then the maximum load attained by the\nalgorithm is bounded by $O(1/\\epsilon)$ with high probability.Finally, we\nslightly modify the algorithm to have similar results for balanced allocation\non $d$-regular graph with $d\\in[3, O(\\log n)]$ and sufficiently large girth.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 18:03:56 GMT"}, {"version": "v2", "created": "Sun, 20 Jul 2014 12:45:51 GMT"}, {"version": "v3", "created": "Thu, 19 Feb 2015 20:06:01 GMT"}, {"version": "v4", "created": "Sat, 27 Feb 2016 09:38:05 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Pourmiri", "Ali", ""]]}, {"id": "1407.2774", "submitter": "Will Perkins", "authors": "Vitaly Feldman, Will Perkins, Santosh Vempala", "title": "Subsampled Power Iteration: a Unified Algorithm for Block Models and\n  Planted CSP's", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for recovering planted solutions in two well-known\nmodels, the stochastic block model and planted constraint satisfaction\nproblems, via a common generalization in terms of random bipartite graphs. Our\nalgorithm matches up to a constant factor the best-known bounds for the number\nof edges (or constraints) needed for perfect recovery and its running time is\nlinear in the number of edges used. The time complexity is significantly better\nthan both spectral and SDP-based approaches.\n  The main contribution of the algorithm is in the case of unequal sizes in the\nbipartition (corresponding to odd uniformity in the CSP). Here our algorithm\nsucceeds at a significantly lower density than the spectral approaches,\nsurpassing a barrier based on the spectral norm of a random matrix.\n  Other significant features of the algorithm and analysis include (i) the\ncritical use of power iteration with subsampling, which might be of independent\ninterest; its analysis requires keeping track of multiple norms of an evolving\nsolution (ii) it can be implemented statistically, i.e., with very limited\naccess to the input distribution (iii) the algorithm is extremely simple to\nimplement and runs in linear time, and thus is practical even for very large\ninstances.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 13:12:38 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 06:54:15 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 21:01:03 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Feldman", "Vitaly", ""], ["Perkins", "Will", ""], ["Vempala", "Santosh", ""]]}, {"id": "1407.2844", "submitter": "Alantha Newman", "authors": "Satoru Iwata, Alantha Newman, R. Ravi", "title": "Graph-TSP from Steiner Cycles", "comments": "Proceedings of WG 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for the traveling salesman problem with graph metric\nbased on Steiner cycles. A Steiner cycle is a cycle that is required to contain\nsome specified subset of vertices. For a graph $G$, if we can find a spanning\ntree $T$ and a simple cycle that contains the vertices with odd-degree in $T$,\nthen we show how to combine the classic \"double spanning tree\" algorithm with\nChristofides' algorithm to obtain a TSP tour of length at most $\\frac{4n}{3}$.\nWe use this approach to show that a graph containing a Hamiltonian path has a\nTSP tour of length at most $4n/3$.\n  Since a Hamiltonian path is a spanning tree with two leaves, this motivates\nthe question of whether or not a graph containing a spanning tree with few\nleaves has a short TSP tour. The recent techniques of M\\\"omke and Svensson\nimply that a graph containing a depth-first-search tree with $k$ leaves has a\nTSP tour of length $4n/3 + O(k)$. Using our approach, we can show that a\n$2(k-1)$-vertex connected graph that contains a spanning tree with at most $k$\nleaves has a TSP tour of length $4n/3$. We also explore other conditions under\nwhich our approach results in a short tour.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 16:12:01 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Iwata", "Satoru", ""], ["Newman", "Alantha", ""], ["Ravi", "R.", ""]]}, {"id": "1407.2907", "submitter": "Kasper Green Larsen", "authors": "Mayank Goswami, Allan Gr{\\o}nlund, Kasper Green Larsen, Rasmus Pagh", "title": "Approximate Range Emptiness in Constant Time and Optimal Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the \\emph{$\\varepsilon$-approximate range emptiness}\nproblem, where the task is to represent a set $S$ of $n$ points from\n$\\{0,\\ldots,U-1\\}$ and answer emptiness queries of the form \"$[a ; b]\\cap S\n\\neq \\emptyset$ ?\" with a probability of \\emph{false positives} allowed. This\ngeneralizes the functionality of \\emph{Bloom filters} from single point queries\nto any interval length $L$. Setting the false positive rate to $\\varepsilon/L$\nand performing $L$ queries, Bloom filters yield a solution to this problem with\nspace $O(n \\lg(L/\\varepsilon))$ bits, false positive probability bounded by\n$\\varepsilon$ for intervals of length up to $L$, using query time $O(L\n\\lg(L/\\varepsilon))$. Our first contribution is to show that the space/error\ntrade-off cannot be improved asymptotically: Any data structure for answering\napproximate range emptiness queries on intervals of length up to $L$ with false\npositive probability $\\varepsilon$, must use space $\\Omega(n\n\\lg(L/\\varepsilon)) - O(n)$ bits. On the positive side we show that the query\ntime can be improved greatly, to constant time, while matching our space lower\nbound up to a lower order additive term. This result is achieved through a\nsuccinct data structure for (non-approximate 1d) range emptiness/reporting\nqueries, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 19:14:59 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Goswami", "Mayank", ""], ["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1407.2912", "submitter": "Enrico Malizia", "authors": "Georg Gottlob and Enrico Malizia", "title": "Achieving New Upper Bounds for the Hypergraph Duality Problem through\n  Logic", "comments": "Restructured the presentation in order to be the extended version of\n  a paper that will shortly appear in SIAM Journal on Computing", "journal-ref": "SIAM Journal on Computing, vol. 47(2), pp. 456-492, 2018", "doi": "10.1137/15M1027267", "report-no": null, "categories": "cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hypergraph duality problem DUAL is defined as follows: given two simple\nhypergraphs $\\mathcal{G}$ and $\\mathcal{H}$, decide whether $\\mathcal{H}$\nconsists precisely of all minimal transversals of $\\mathcal{G}$ (in which case\nwe say that $\\mathcal{G}$ is the dual of $\\mathcal{H}$). This problem is\nequivalent to deciding whether two given non-redundant monotone DNFs are dual.\nIt is known that non-DUAL, the complementary problem to DUAL, is in\n$\\mathrm{GC}(\\log^2 n,\\mathrm{PTIME})$, where $\\mathrm{GC}(f(n),\\mathcal{C})$\ndenotes the complexity class of all problems that after a nondeterministic\nguess of $O(f(n))$ bits can be decided (checked) within complexity class\n$\\mathcal{C}$. It was conjectured that non-DUAL is in $\\mathrm{GC}(\\log^2\nn,\\mathrm{LOGSPACE})$. In this paper we prove this conjecture and actually\nplace the non-DUAL problem into the complexity class $\\mathrm{GC}(\\log^2\nn,\\mathrm{TC}^0)$ which is a subclass of $\\mathrm{GC}(\\log^2\nn,\\mathrm{LOGSPACE})$. We here refer to the logtime-uniform version of\n$\\mathrm{TC}^0$, which corresponds to $\\mathrm{FO(COUNT)}$, i.e., first order\nlogic augmented by counting quantifiers. We achieve the latter bound in two\nsteps. First, based on existing problem decomposition methods, we develop a new\nnondeterministic algorithm for non-DUAL that requires to guess $O(\\log^2 n)$\nbits. We then proceed by a logical analysis of this algorithm, allowing us to\nformulate its deterministic part in $\\mathrm{FO(COUNT)}$. From this result, by\nthe well known inclusion $\\mathrm{TC}^0\\subseteq\\mathrm{LOGSPACE}$, it follows\nthat DUAL belongs also to $\\mathrm{DSPACE}[\\log^2 n]$. Finally, by exploiting\nthe principles on which the proposed nondeterministic algorithm is based, we\ndevise a deterministic algorithm that, given two hypergraphs $\\mathcal{G}$ and\n$\\mathcal{H}$, computes in quadratic logspace a transversal of $\\mathcal{G}$\nmissing in $\\mathcal{H}$.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 19:28:11 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 00:28:03 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 23:05:56 GMT"}, {"version": "v4", "created": "Mon, 20 Nov 2017 12:07:03 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Gottlob", "Georg", ""], ["Malizia", "Enrico", ""]]}, {"id": "1407.2929", "submitter": "Radu Curticapean", "authors": "Radu Curticapean and D\\'aniel Marx", "title": "Complexity of counting subgraphs: only the boundedness of the\n  vertex-cover number counts", "comments": "42 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a class $\\mathcal{H}$ of graphs, #Sub$(\\mathcal{H})$ is the counting\nproblem that, given a graph $H\\in \\mathcal{H}$ and an arbitrary graph $G$, asks\nfor the number of subgraphs of $G$ isomorphic to $H$. It is known that if\n$\\mathcal{H}$ has bounded vertex-cover number (equivalently, the size of the\nmaximum matching in $\\mathcal{H}$ is bounded), then #Sub$(\\mathcal{H})$ is\npolynomial-time solvable. We complement this result with a corresponding lower\nbound: if $\\mathcal{H}$ is any recursively enumerable class of graphs with\nunbounded vertex-cover number, then #Sub$(\\mathcal{H})$ is #W[1]-hard\nparameterized by the size of $H$ and hence not polynomial-time solvable and not\neven fixed-parameter tractable, unless FPT = #W[1].\n  As a first step of the proof, we show that counting $k$-matchings in\nbipartite graphs is #W[1]-hard. Recently, Curticapean [ICALP 2013] proved the\n#W[1]-hardness of counting $k$-matchings in general graphs; our result\nstrengthens this statement to bipartite graphs with a considerably simpler\nproof and even shows that, assuming the Exponential Time Hypothesis (ETH),\nthere is no $f(k)n^{o(k/\\log k)}$ time algorithm for counting $k$-matchings in\nbipartite graphs for any computable function $f(k)$. As a consequence, we\nobtain an independent and somewhat simpler proof of the classical result of\nFlum and Grohe [SICOMP 2004] stating that counting paths of length $k$ is\n#W[1]-hard, as well as a similar almost-tight ETH-based lower bound on the\nexponent.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 19:59:19 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Curticapean", "Radu", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1407.3008", "submitter": "Neal E. Young", "authors": "Claire Mathieu and Carl Staelin and Neal E. Young and Arman Yousefi", "title": "Bigtable Merge Compaction", "comments": null, "journal-ref": "SUPERSEDED BY https://arxiv.org/abs/2011.02615", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NoSQL databases are widely used for massive data storage and real-time web\napplications. Yet important aspects of these data structures are not well\nunderstood. For example, NoSQL databases write most of their data to a\ncollection of files on disk, meanwhile periodically compacting subsets of these\nfiles. A compaction policy must choose which files to compact, and when to\ncompact them, without knowing the future workload. Although these choices can\naffect computational efficiency by orders of magnitude, existing literature\nlacks tools for designing and analyzing online compaction policies --- policies\nare now chosen largely by trial and error.\n  Here we introduce tools for the design and analysis of compaction policies\nfor Google Bigtable, propose new policies, give average-case and worst-case\ncompetitive analyses, and present preliminary empirical benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 02:30:15 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 07:56:31 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2015 19:05:05 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Mathieu", "Claire", ""], ["Staelin", "Carl", ""], ["Young", "Neal E.", ""], ["Yousefi", "Arman", ""]]}, {"id": "1407.3015", "submitter": "Neal E. Young", "authors": "Neal E. Young", "title": "Nearly Linear-Work Algorithms for Mixed Packing/Covering and\n  Facility-Location Linear Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the first nearly linear-time approximation algorithms for\nexplicitly given mixed packing/covering linear programs, and for (non-metric)\nfractional facility location. We also describe the first parallel algorithms\nrequiring only near-linear total work and finishing in polylog time. The\nalgorithms compute $(1+\\epsilon)$-approximate solutions in time (and work)\n$O^*(N/\\epsilon^2)$, where $N$ is the number of non-zeros in the constraint\nmatrix. For facility location, $N$ is the number of eligible client/facility\npairs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 03:17:57 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 00:52:41 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 06:39:30 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Young", "Neal E.", ""]]}, {"id": "1407.3041", "submitter": "Luigi Laura", "authors": "Loukas Georgiadis and Giuseppe F. Italiano and Luigi Laura and Nikos\n  Parotsidis", "title": "2-Edge Connectivity in Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge and vertex connectivity are fundamental concepts in graph theory. While\nthey have been thoroughly studied in the case of undirected graphs,\nsurprisingly not much has been investigated for directed graphs. In this paper\nwe study $2$-edge connectivity problems in directed graphs and, in particular,\nwe consider the computation of the following natural relation: We say that two\nvertices $v$ and $w$ are $2$-edge-connected if there are two edge-disjoint\npaths from $v$ to $w$ and two edge-disjoint paths from $w$ to $v$. This\nrelation partitions the vertices into blocks such that all vertices in the same\nblock are $2$-edge-connected. Differently from the undirected case, those\nblocks do not correspond to the $2$-edge-connected components of the graph. We\nshow how to compute this relation in linear time so that we can report in\nconstant time if two vertices are $2$-edge-connected. We also show how to\ncompute in linear time a sparse certificate for this relation, i.e., a subgraph\nof the input graph that has $O(n)$ edges and maintains the same\n$2$-edge-connected blocks as the input graph.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 06:58:07 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 19:04:16 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Italiano", "Giuseppe F.", ""], ["Laura", "Luigi", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "1407.3144", "submitter": "Matteo Ceccarello", "authors": "Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci and Eli Upfal", "title": "Space and Time Efficient Parallel Graph Decomposition, Clustering, and\n  Diameter Approximation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel parallel decomposition strategy for unweighted, undirected\ngraphs, based on growing disjoint connected clusters from batches of centers\nprogressively selected from yet uncovered nodes. With respect to similar\nprevious decompositions, our strategy exercises a tighter control on both the\nnumber of clusters and their maximum radius.\n  We present two important applications of our parallel graph decomposition:\n(1) $k$-center clustering approximation; and (2) diameter approximation. In\nboth cases, we obtain algorithms which feature a polylogarithmic approximation\nfactor and are amenable to a distributed implementation that is geared for\nmassive (long-diameter) graphs. The total space needed for the computation is\nlinear in the problem size, and the parallel depth is substantially sublinear\nin the diameter for graphs with low doubling dimension. To the best of our\nknowledge, ours are the first parallel approximations for these problems which\nachieve sub-diameter parallel time, for a relevant class of graphs, using only\nlinear space. Besides the theoretical guarantees, our algorithms allow for a\nvery simple implementation on clustered architectures: we report on extensive\nexperiments which demonstrate their effectiveness and efficiency on large\ngraphs as compared to alternative known approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 13:12:19 GMT"}, {"version": "v2", "created": "Tue, 21 Oct 2014 16:11:08 GMT"}, {"version": "v3", "created": "Fri, 6 Feb 2015 14:38:45 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Ceccarello", "Matteo", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Upfal", "Eli", ""]]}, {"id": "1407.3200", "submitter": "Przemyslaw Uznanski", "authors": "J\\'er\\'emie Chalopin (LIF), Shantanu Das (LIF), Pawel Gawrychowski\n  (MPII), Adrian Kosowski (INRIA Paris-Rocquencourt, LIAFA), Arnaud Labourel\n  (LIF), Przemyslaw Uzna\\'nski", "title": "Lock-in Problem for Parallel Rotor-router Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rotor-router model, also called the Propp machine, was introduced as a\ndeterministic alternative to the random walk. In this model, a group of\nidentical tokens are initially placed at nodes of the graph. Each node\nmaintains a cyclic ordering of the outgoing arcs, and during consecutive turns\nthe tokens are propagated along arcs chosen according to this ordering in\nround-robin fashion. The behavior of the model is fully deterministic. Yanovski\net al.(2003) proved that a single rotor-router walk on any graph with m edges\nand diameter $D$ stabilizes to a traversal of an Eulerian circuit on the set of\nall 2m directed arcs on the edge set of the graph, and that such periodic\nbehaviour of the system is achieved after an initial transient phase of at most\n2mD steps. The case of multiple parallel rotor-routers was studied\nexperimentally, leading Yanovski et al. to the conjecture that a system of $k\n\\textgreater{} 1$ parallel walks also stabilizes with a period of length at\nmost $2m$ steps. In this work we disprove this conjecture, showing that the\nperiod of parallel rotor-router walks can in fact, be superpolynomial in the\nsize of graph. On the positive side, we provide a characterization of the\nperiodic behavior of parallel router walks, in terms of a structural property\nof stable states called a subcycle decomposition. This property provides us the\ntools to efficiently detect whether a given system configuration corresponds to\nthe transient or to the limit behavior of the system. Moreover, we provide\npolynomial upper bounds of $O(m^4 D^2 + mD \\log k)$ and $O(m^5 k^2)$ on the\nnumber of steps it takes for the system to stabilize. Thus, we are able to\npredict any future behavior of the system using an algorithm that takes\npolynomial time and space. In addition, we show that there exists a separation\nbetween the stabilization time of the single-walk and multiple-walk\nrotor-router systems, and that for some graphs the latter can be asymptotically\nlarger even for the case of $k = 2$ walks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 06:12:51 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 08:03:07 GMT"}, {"version": "v3", "created": "Thu, 28 May 2015 06:15:01 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Chalopin", "J\u00e9r\u00e9mie", "", "LIF"], ["Das", "Shantanu", "", "LIF"], ["Gawrychowski", "Pawel", "", "MPII"], ["Kosowski", "Adrian", "", "INRIA Paris-Rocquencourt, LIAFA"], ["Labourel", "Arnaud", "", "LIF"], ["Uzna\u0144ski", "Przemyslaw", ""]]}, {"id": "1407.3239", "submitter": "Daniel McCormack Mr.", "authors": "Daniel McCormack", "title": "Boolean Algebraic Programs as a Methodology for Symbolically\n  Demonstrating Lower and Upper Bounds of Algorithms and Determinism", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lower and upper bound of any given algorithm is one of the most crucial\npieces of information needed when evaluating the computational effectiveness\nfor said algorithm. Here a novel method of Boolean Algebraic Programming for\nsymbolic manipulation of Machines, Functions, and Inputs is presented which\nallows for direct analysis of time complexities and proof of deterministic\nmethodologies. It is demonstrated through the analysis of a particular problem\nwhich is proven and solved through the application of Boolean algebraic\nprogramming.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 13:47:15 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["McCormack", "Daniel", ""]]}, {"id": "1407.3242", "submitter": "Marcello La Rocca", "authors": "Marcello La Rocca", "title": "Density Adaptive Parallel Clustering", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are going to introduce a new nearest neighbours based\napproach to clustering, and compare it with previous solutions; the resulting\nalgorithm, which takes inspiration from both DBscan and minimum spanning tree\napproaches, is deterministic but proves simpler, faster and doesnt require to\nset in advance a value for k, the number of clusters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 18:24:15 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["La Rocca", "Marcello", ""]]}, {"id": "1407.3263", "submitter": "Hyung-Chan An", "authors": "Hyung-Chan An, Mohit Singh, Ola Svensson", "title": "LP-Based Algorithms for Capacitated Facility Location", "comments": "25 pages, 6 figures; minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear programming has played a key role in the study of algorithms for\ncombinatorial optimization problems. In the field of approximation algorithms,\nthis is well illustrated by the uncapacitated facility location problem. A\nvariety of algorithmic methodologies, such as LP-rounding and primal-dual\nmethod, have been applied to and evolved from algorithms for this problem.\nUnfortunately, this collection of powerful algorithmic techniques had not yet\nbeen applicable to the more general capacitated facility location problem. In\nfact, all of the known algorithms with good performance guarantees were based\non a single technique, local search, and no linear programming relaxation was\nknown to efficiently approximate the problem.\n  In this paper, we present a linear programming relaxation with constant\nintegrality gap for capacitated facility location. We demonstrate that the\nfundamental theories of multi-commodity flows and matchings provide key\ninsights that lead to the strong relaxation. Our algorithmic proof of\nintegrality gap is obtained by finally accessing the rich toolbox of LP-based\nmethodologies: we present a constant factor approximation algorithm based on\nLP-rounding.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 19:51:06 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 19:51:52 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["An", "Hyung-Chan", ""], ["Singh", "Mohit", ""], ["Svensson", "Ola", ""]]}, {"id": "1407.3342", "submitter": "Srinivasa Rao Satti", "authors": "Amr Elmasry and Daniel Dahl Juhl and Jyrki Katajainen and Srinivasa\n  Rao Satti", "title": "Selection from read-only memory with limited workspace", "comments": "16 pages, 1 figure, Preliminary version appeared in COCOON-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an unordered array of $N$ elements drawn from a totally ordered set and\nan integer $k$ in the range from $1$ to $N$, in the classic selection problem\nthe task is to find the $k$-th smallest element in the array. We study the\ncomplexity of this problem in the space-restricted random-access model: The\ninput array is stored on read-only memory, and the algorithm has access to a\nlimited amount of workspace. We prove that the linear-time prune-and-search\nalgorithm---presented in most textbooks on algorithms---can be modified to use\n$\\Theta(N)$ bits instead of $\\Theta(N)$ words of extra space. Prior to our\nwork, the best known algorithm by Frederickson could perform the task with\n$\\Theta(N)$ bits of extra space in $O(N \\lg^{*} N)$ time. Our result separates\nthe space-restricted random-access model and the multi-pass streaming model,\nsince we can surpass the $\\Omega(N \\lg^{*} N)$ lower bound known for the latter\nmodel. We also generalize our algorithm for the case when the size of the\nworkspace is $\\Theta(S)$ bits, where $\\lg^3{N} \\leq S \\leq N$. The running time\nof our generalized algorithm is $O(N \\lg^{*}(N/S) + N (\\lg N) / \\lg{} S)$,\nslightly improving over the $O(N \\lg^{*}(N (\\lg N)/S) + N (\\lg N) / \\lg{} S)$\nbound of Frederickson's algorithm. To obtain the improvements mentioned above,\nwe developed a new data structure, called the wavelet stack, that we use for\nrepeated pruning. We expect the wavelet stack to be a useful tool in other\napplications as well.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 04:35:37 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Elmasry", "Amr", ""], ["Juhl", "Daniel Dahl", ""], ["Katajainen", "Jyrki", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1407.3377", "submitter": "Stefan Edelkamp", "authors": "Stefan Edelkamp, Jyrki Katajainen, Amr Elmasry", "title": "Strengthened Lazy Heaps: Surpassing the Lower Bounds for Binary Heaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $n$ denote the number of elements currently in a data structure. An\nin-place heap is stored in the first $n$ locations of an array, uses $O(1)$\nextra space, and supports the operations: minimum, insert, and extract-min. We\nintroduce an in-place heap, for which minimum and insert take $O(1)$ worst-case\ntime, and extract-min takes $O(\\lg{} n)$ worst-case time and involves at most\n$\\lg{} n + O(1)$ element comparisons. The achieved bounds are optimal to within\nadditive constant terms for the number of element comparisons. In particular,\nthese bounds for both insert and extract-min -and the time bound for insert-\nsurpass the corresponding lower bounds known for binary heaps, though our data\nstructure is similar. In a binary heap, when viewed as a nearly complete binary\ntree, every node other than the root obeys the heap property, i.e. the element\nat a node is not smaller than that at its parent. To surpass the lower bound\nfor extract-min, we reinforce a stronger property at the bottom levels of the\nheap that the element at any right child is not smaller than that at its left\nsibling. To surpass the lower bound for insert, we buffer insertions and allow\n$O(\\lg^2{} n)$ nodes to violate heap order in relation to their parents.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 11:50:48 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Edelkamp", "Stefan", ""], ["Katajainen", "Jyrki", ""], ["Elmasry", "Amr", ""]]}, {"id": "1407.3462", "submitter": "Justin Thaler", "authors": "Justin Thaler", "title": "Semi-Streaming Algorithms for Annotated Graph Streams", "comments": "This update includes some additional discussion of the results\n  proven. The result on counting triangles was previously included in an ECCC\n  technical report by Chakrabarti et al. available at\n  http://eccc.hpi-web.de/report/2013/180/. That report has been superseded by\n  this manuscript, and the CCC 2015 paper \"Verifiable Stream Computation and\n  Arthur-Merlin Communication\" by Chakrabarti et al", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable effort has been devoted to the development of streaming\nalgorithms for analyzing massive graphs. Unfortunately, many results have been\nnegative, establishing that a wide variety of problems require $\\Omega(n^2)$\nspace to solve. One of the few bright spots has been the development of\nsemi-streaming algorithms for a handful of graph problems -- these algorithms\nuse space $O(n\\cdot\\text{polylog}(n))$.\n  In the annotated data streaming model of Chakrabarti et al., a\ncomputationally limited client wants to compute some property of a massive\ninput, but lacks the resources to store even a small fraction of the input, and\nhence cannot perform the desired computation locally. The client therefore\naccesses a powerful but untrusted service provider, who not only performs the\nrequested computation, but also proves that the answer is correct.\n  We put forth the notion of semi-streaming algorithms for annotated graph\nstreams (semi-streaming annotation schemes for short). These are protocols in\nwhich both the client's space usage and the length of the proof are $O(n \\cdot\n\\text{polylog}(n))$. We give evidence that semi-streaming annotation schemes\nrepresent a substantially more robust solution concept than does the standard\nsemi-streaming model. On the positive side, we give semi-streaming annotation\nschemes for two dynamic graph problems that are intractable in the standard\nmodel: (exactly) counting triangles, and (exactly) computing maximum matchings.\nThe former scheme answers a question of Cormode. On the negative side, we\nidentify for the first time two natural graph problems (connectivity and\nbipartiteness in a certain edge update model) that can be solved in the\nstandard semi-streaming model, but cannot be solved by annotation schemes of\n\"sub-semi-streaming\" cost. That is, these problems are just as hard in the\nannotations model as they are in the standard model.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jul 2014 13:19:07 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 16:09:52 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Thaler", "Justin", ""]]}, {"id": "1407.3631", "submitter": "Yi Zhang", "authors": "David Cariolaro, Zhaiming Shen and Yi Zhang", "title": "Group Testing with Pools of Fixed Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical combinatorial (adaptive) group testing problem, one is given\ntwo integers \\(d\\) and \\(n\\), where \\(0\\le d\\le n\\), and a population of \\(n\\)\nitems, exactly \\(d\\) of which are known to be defective. The question is to\ndevise an optimal sequential algorithm that, at each step, tests a subset of\nthe population and determines whether such subset is contaminated (i.e.\ncontains defective items) or otherwise. The problem is solved only when the\n\\(d\\) defective items are identified. The minimum number of steps that an\noptimal sequential algorithm takes in general (i.e. in the worst case) to solve\nthe problem is denoted by \\(M(d, n)\\). The computation of \\(M(d, n)\\) appears\nto be very difficult and a general formula is known only for \\(d = 1\\). We\nconsider here a variant of the original problem, where the size of the subsets\nto be tested is restricted to be a fixed positive integer \\(k\\). The\ncorresponding minimum number of tests by a sequential optimal algorithm is\ndenoted by \\(M^{\\lbrack k\\rbrack}(d, n)\\). In this paper we start the\ninvestigation of the function \\(M^{\\lbrack k\\rbrack}(d, n)\\).\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 13:11:15 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Cariolaro", "David", ""], ["Shen", "Zhaiming", ""], ["Zhang", "Yi", ""]]}, {"id": "1407.3740", "submitter": "Justin Thaler", "authors": "Edo Liberty, Michael Mitzenmacher, Justin Thaler, Jonathan Ullman", "title": "Space Lower Bounds for Itemset Frequency Sketches", "comments": "Minor edits for clarity, and added some discussion of subsequent\n  work. To appear in PODS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a database, computing the fraction of rows that contain a query itemset\nor determining whether this fraction is above some threshold are fundamental\noperations in data mining. A uniform sample of rows is a good sketch of the\ndatabase in the sense that all sufficiently frequent itemsets and their\napproximate frequencies are recoverable from the sample, and the sketch size is\nindependent of the number of rows in the original database. For many seemingly\nsimilar problems there are better sketching algorithms than uniform sampling.\nIn this paper we show that for itemset frequency sketching this is not the\ncase. That is, we prove that there exist classes of databases for which uniform\nsampling is a space optimal sketch for approximate itemset frequency analysis,\nup to constant or iterated-logarithmic factors.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 17:41:26 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 15:59:14 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2015 20:32:37 GMT"}, {"version": "v4", "created": "Wed, 9 Mar 2016 15:47:40 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Liberty", "Edo", ""], ["Mitzenmacher", "Michael", ""], ["Thaler", "Justin", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1407.3857", "submitter": "Takeaki Uno", "authors": "Takeaki Uno", "title": "A New Approach to Efficient Enumeration by Push-out Amortization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enumeration algorithms have been one of recent hot topics in theoretical\ncomputer science. Different from other problems, enumeration has many\ninteresting aspects, such as the computation time can be shorter than the total\noutput size, by sophisticated ordering of output solutions. One more example is\nthat the recursion of the enumeration algorithm is often structured well, thus\nwe can have good amortized analysis, and interesting algorithms for reducing\nthe amortized complexity. However, there is a lack of deep studies from these\npoints of views; there are only few results on the fundamentals of enumeration,\nsuch as a basic design of an algorithm that is applicable to many problems. In\nthis paper, we address new approaches on the complexity analysis, and propose a\nnew way of amortized analysis Push-Out Amortization for enumeration algorithms,\nwhere the computation time of an iteration is amortized by using all its\ndescendant iterations. We clarify sufficient conditions on the enumeration\nalgorithm so that the amortized analysis works. By the amortization, we show\nthat many elimination orderings, matchings in a graph, connected vertex induced\nsubgraphs in a graph, and spanning trees can be enumerated in O(1) time for\neach solution by simple algorithms with simple proofs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 01:41:18 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Uno", "Takeaki", ""]]}, {"id": "1407.4070", "submitter": "Mary Wootters", "authors": "Moritz Hardt and Mary Wootters", "title": "Fast matrix completion without the condition number", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first algorithm for Matrix Completion whose running time and\nsample complexity is polynomial in the rank of the unknown target matrix,\nlinear in the dimension of the matrix, and logarithmic in the condition number\nof the matrix. To the best of our knowledge, all previous algorithms either\nincurred a quadratic dependence on the condition number of the unknown matrix\nor a quadratic dependence on the dimension of the matrix in the running time.\nOur algorithm is based on a novel extension of Alternating Minimization which\nwe show has theoretical guarantees under standard assumptions even in the\npresence of noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 17:47:44 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Hardt", "Moritz", ""], ["Wootters", "Mary", ""]]}, {"id": "1407.4094", "submitter": "Nika Haghtalab", "authors": "Avrim Blum, John P. Dickerson, Nika Haghtalab, Ariel D. Procaccia,\n  Tuomas Sandholm, Ankit Sharma", "title": "Ignorance is Almost Bliss: Near-Optimal Stochastic Matching With Few\n  Queries", "comments": null, "journal-ref": null, "doi": "10.1145/2764468.2764479", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic matching problem deals with finding a maximum matching in a\ngraph whose edges are unknown but can be accessed via queries. This is a\nspecial case of stochastic $k$-set packing, where the problem is to find a\nmaximum packing of sets, each of which exists with some probability. In this\npaper, we provide edge and set query algorithms for these two problems,\nrespectively, that provably achieve some fraction of the omniscient optimal\nsolution.\n  Our main theoretical result for the stochastic matching (i.e., $2$-set\npacking) problem is the design of an \\emph{adaptive} algorithm that queries\nonly a constant number of edges per vertex and achieves a $(1-\\epsilon)$\nfraction of the omniscient optimal solution, for an arbitrarily small\n$\\epsilon>0$. Moreover, this adaptive algorithm performs the queries in only a\nconstant number of rounds. We complement this result with a \\emph{non-adaptive}\n(i.e., one round of queries) algorithm that achieves a $(0.5 - \\epsilon)$\nfraction of the omniscient optimum. We also extend both our results to\nstochastic $k$-set packing by designing an adaptive algorithm that achieves a\n$(\\frac{2}{k} - \\epsilon)$ fraction of the omniscient optimal solution, again\nwith only $O(1)$ queries per element. This guarantee is close to the best known\npolynomial-time approximation ratio of $\\frac{3}{k+1} -\\epsilon$ for the\n\\emph{deterministic} $k$-set packing problem [Furer and Yu, 2013]\n  We empirically explore the application of (adaptations of) these algorithms\nto the kidney exchange problem, where patients with end-stage renal failure\nswap willing but incompatible donors. We show on both generated data and on\nreal data from the first 169 match runs of the UNOS nationwide kidney exchange\nthat even a very small number of non-adaptive edge queries per vertex results\nin large gains in expected successful matches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 18:52:32 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 18:51:32 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Blum", "Avrim", ""], ["Dickerson", "John P.", ""], ["Haghtalab", "Nika", ""], ["Procaccia", "Ariel D.", ""], ["Sandholm", "Tuomas", ""], ["Sharma", "Ankit", ""]]}, {"id": "1407.4235", "submitter": "Tatsuhiko Hatanaka", "authors": "Tatsuhiko Hatanaka, Takehiro Ito, Xiao Zhou", "title": "The List Coloring Reconfiguration Problem for Bounded Pathwidth Graphs", "comments": null, "journal-ref": null, "doi": "10.1587/transfun.E98.A.1168", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of transforming one list (vertex) coloring of a graph\ninto another list coloring by changing only one vertex color assignment at a\ntime, while at all times maintaining a list coloring, given a list of allowed\ncolors for each vertex. This problem is known to be PSPACE-complete for\nbipartite planar graphs. In this paper, we first show that the problem remains\nPSPACE-complete even for bipartite series-parallel graphs, which form a proper\nsubclass of bipartite planar graphs. We note that our reduction indeed shows\nthe PSPACE-completeness for graphs with pathwidth two, and it can be extended\nfor threshold graphs. In contrast, we give a polynomial-time algorithm to solve\nthe problem for graphs with pathwidth one. Thus, this paper gives precise\nanalyses of the problem with respect to pathwidth.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 08:55:12 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Hatanaka", "Tatsuhiko", ""], ["Ito", "Takehiro", ""], ["Zhou", "Xiao", ""]]}, {"id": "1407.4286", "submitter": "Markus Lohrey", "authors": "Moses Ganardi, Danny Hucke, Artur Jez, Markus Lohrey and Eric Noeth", "title": "Constructing small tree grammars and small circuits for formulas", "comments": "A short version of this paper appeared in the Proceedings of FSTTCS\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that every tree of size $n$ over a fixed set of $\\sigma$\ndifferent ranked symbols can be decomposed (in linear time as well as in\nlogspace) into $O\\big(\\frac{n}{\\log_\\sigma n}\\big) = O\\big(\\frac{n \\log\n\\sigma}{\\log n}\\big)$ many hierarchically defined pieces. Formally, such a\nhierarchical decomposition has the form of a straight-line linear context-free\ntree grammar of size $O\\big(\\frac{n}{\\log_\\sigma n}\\big)$, which can be used as\na compressed representation of the input tree. This generalizes an analogous\nresult for strings. Previous grammar-based tree compressors were not analyzed\nfor the worst-case size of the computed grammar, except for the top dag of\nBille et al., for which only the weaker upper bound of\n$O\\big(\\frac{n}{\\log_\\sigma^{0.19} n}\\big)$ (which was very recently improved\nto $O\\big(\\frac{n \\cdot \\log \\log_\\sigma n}{\\log_\\sigma n}\\big)$ by\nH\\\"ubschle-Schneider and Raman) for unranked and unlabelled trees has been\nderived. The main result is used to show that every arithmetical formula of\nsize $n$, in which only $m \\leq n$ different variables occur, can be\ntransformed (in linear time as well as in logspace) into an arithmetical\ncircuit of size $O\\big(\\frac{n \\cdot \\log m}{\\log n}\\big)$ and depth $O(\\log\nn)$. This refines a classical result of Brent from 1974, according to which an\narithmetical formula of size $n$ can be transformed into a logarithmic depth\ncircuit of size $O(n)$.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 12:48:48 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 20:21:15 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 13:55:03 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Ganardi", "Moses", ""], ["Hucke", "Danny", ""], ["Jez", "Artur", ""], ["Lohrey", "Markus", ""], ["Noeth", "Eric", ""]]}, {"id": "1407.4416", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "In Defense of MinHash Over SimHash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MinHash and SimHash are the two widely adopted Locality Sensitive Hashing\n(LSH) algorithms for large-scale data processing applications. Deciding which\nLSH to use for a particular problem at hand is an important question, which has\nno clear answer in the existing literature. In this study, we provide a\ntheoretical answer (validated by experiments) that MinHash virtually always\noutperforms SimHash when the data are binary, as common in practice such as\nsearch.\n  The collision probability of MinHash is a function of resemblance similarity\n($\\mathcal{R}$), while the collision probability of SimHash is a function of\ncosine similarity ($\\mathcal{S}$). To provide a common basis for comparison, we\nevaluate retrieval results in terms of $\\mathcal{S}$ for both MinHash and\nSimHash. This evaluation is valid as we can prove that MinHash is a valid LSH\nwith respect to $\\mathcal{S}$, by using a general inequality $\\mathcal{S}^2\\leq\n\\mathcal{R}\\leq \\frac{\\mathcal{S}}{2-\\mathcal{S}}$. Our worst case analysis can\nshow that MinHash significantly outperforms SimHash in high similarity region.\n  Interestingly, our intensive experiments reveal that MinHash is also\nsubstantially better than SimHash even in datasets where most of the data\npoints are not too similar to each other. This is partly because, in practical\ndata, often $\\mathcal{R}\\geq \\frac{\\mathcal{S}}{z-\\mathcal{S}}$ holds where $z$\nis only slightly larger than 2 (e.g., $z\\leq 2.1$). Our restricted worst case\nanalysis by assuming $\\frac{\\mathcal{S}}{z-\\mathcal{S}}\\leq \\mathcal{R}\\leq\n\\frac{\\mathcal{S}}{2-\\mathcal{S}}$ shows that MinHash indeed significantly\noutperforms SimHash even in low similarity region.\n  We believe the results in this paper will provide valuable guidelines for\nsearch in practice, especially when the data are sparse.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:27:02 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1407.4423", "submitter": "Sarah Allen", "authors": "Sarah R. Allen and Ryan O'Donnell", "title": "Conditioning and covariance on caterpillars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC cs.DS math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1, \\dots, X_n$ be joint $\\{ \\pm 1\\}$-valued random variables. It is\nknown that conditioning on a random subset of $O(1/\\epsilon^2)$ of them reduces\ntheir average pairwise covariance to below $\\epsilon$ (in expectation). We\nconjecture that $O(1/\\epsilon^2)$ can be improved to $O(1/\\epsilon)$. The\nmotivation for the problem and our conjectured improvement comes from the\ntheory of global correlation rounding for convex relaxation hierarchies. We\nsuggest attempting the conjecture in the case that $X_1, \\dots, X_n$ are the\nleaves of an information flow tree. We prove the conjecture in the case that\nthe information flow tree is a caterpillar graph (similar to a two-state hidden\nMarkov model).\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:51:12 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Allen", "Sarah R.", ""], ["O'Donnell", "Ryan", ""]]}, {"id": "1407.4498", "submitter": "Moti Medina", "authors": "Guy Even and Moti Medina", "title": "Online Packet-Routing in Grids with Bounded Buffers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present deterministic and randomized algorithms for the problem of online\npacket routing in grids in the competitive network throughput model\n\\cite{AKOR}. In this model the network has nodes with bounded buffers and\nbounded link capacities. The goal in this model is to maximize the throughput,\ni.e., the number of delivered packets.\n  Our deterministic algorithm is the first online algorithm with an\n$O\\left(\\log^{O(1)}(n)\\right)$ competitive ratio for uni-directional grids\n(where $n$ denotes the size of the network). The deterministic online algorithm\nis centralized and handles packets with deadlines. This algorithm is applicable\nto various ranges of values of buffer sizes and communication link capacities.\nIn particular, it holds for buffer size and communication link capacity in the\nrange $[3 \\ldots \\log n]$.\n  Our randomized algorithm achieves an expected competitive ratio of $O(\\log\nn)$ for the uni-directional line. This algorithm is applicable to a wide range\nof buffer sizes and communication link capacities. In particular, it holds also\nfor unit size buffers and unit capacity links. This algorithm improves the best\nprevious $O(\\log^2 n)$-competitive ratio of Azar and Zachut \\cite{AZ}.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 20:58:03 GMT"}, {"version": "v2", "created": "Sat, 30 Aug 2014 13:27:52 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Even", "Guy", ""], ["Medina", "Moti", ""]]}, {"id": "1407.4640", "submitter": "Valerii Sopin", "authors": "Valerii Sopin", "title": "A new algorithm for solving the rSUM problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A determined algorithm is presented for solving the rSUM problem for any\nnatural r with a sub-quadratic assessment of time complexity in some cases. In\nterms of an amount of memory used the obtained algorithm is the nlog^3(n)\norder.\n  The idea of the obtained algorithm is based not considering integer numbers,\nbut rather k (is a natural) successive bits of these numbers in the binary\nnumeration system. It is shown that if a sum of integer numbers is equal to\nzero, then the sum of numbers presented by any k successive bits of these\nnumbers must be sufficiently \"close\" to zero. This makes it possible to discard\nthe numbers, which a fortiori, do not establish the solution.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 11:27:30 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 19:02:34 GMT"}, {"version": "v3", "created": "Fri, 15 Aug 2014 00:24:38 GMT"}, {"version": "v4", "created": "Mon, 18 Aug 2014 22:18:07 GMT"}, {"version": "v5", "created": "Mon, 9 Feb 2015 09:59:04 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Sopin", "Valerii", ""]]}, {"id": "1407.5011", "submitter": "S{\\o}ren Dahlgaard", "authors": "S{\\o}ren Dahlgaard and Mathias B{\\ae}k Tejs Knudsen and Noy Rotbart", "title": "A simple and optimal ancestry labeling scheme for trees", "comments": "12 pages, 1 figure. To appear at ICALP'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a $\\lg n + 2 \\lg \\lg n+3$ ancestry labeling scheme for trees. The\nproblem was first presented by Kannan et al. [STOC 88'] along with a simple $2\n\\lg n$ solution. Motivated by applications to XML files, the label size was\nimproved incrementally over the course of more than 20 years by a series of\npapers. The last, due to Fraigniaud and Korman [STOC 10'], presented an\nasymptotically optimal $\\lg n + 4 \\lg \\lg n+O(1)$ labeling scheme using\nnon-trivial tree-decomposition techniques. By providing a framework\ngeneralizing interval based labeling schemes, we obtain a simple, yet\nasymptotically optimal solution to the problem. Furthermore, our labeling\nscheme is attained by a small modification of the original $2 \\lg n$ solution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 14:36:13 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2015 14:34:00 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Rotbart", "Noy", ""]]}, {"id": "1407.5080", "submitter": "Kaarthik Sundar", "authors": "Kaarthik Sundar and Sivakumar Rathinam", "title": "Multiple Depot Ring Star Problem: A polyhedral study and exact algorithm", "comments": null, "journal-ref": null, "doi": "10.1007/s10898-016-0431-7", "report-no": null, "categories": "cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multiple Depot Ring-Star Problem (MDRSP) is an important combinatorial\noptimization problem that arises in the context of optical fiber network\ndesign, and in applications pertaining to collecting data using stationary\nsensing devices and autonomous vehicles. Given the locations of a set of\ncustomers and a set of depots, the goal is to (i) find a set of simple cycles\nsuch that each cycle (ring) passes through a subset of customers and exactly\none depot, (ii) assign each non-visited customer to a visited customer or a\ndepot, and (iii) minimize the sum of the routing costs, i.e., the cost of the\ncycles and the assignment costs. We present a mixed integer linear programming\nformulation for the MDRSP and propose valid inequalities to strengthen the\nlinear programming relaxation. Furthermore, we present a polyhedral analysis\nand derive facet-inducing results for the MDRSP. All these results are then\nused to develop a branch-and-cut algorithm to obtain optimal solutions to the\nMDRSP. The performance of the branch-and-cut algorithm is evaluated through\nextensive computational experiments on several classes of test instances.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 18:59:36 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 20:30:17 GMT"}, {"version": "v3", "created": "Sun, 25 Apr 2021 00:23:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sundar", "Kaarthik", ""], ["Rathinam", "Sivakumar", ""]]}, {"id": "1407.5111", "submitter": "Sumit Agarwal", "authors": "Harsh Ranjan, Sumit Agarwal, Niraj Kumar Singh", "title": "Design and Analysis of RS Sort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new comparison base stable sorting algorithm, named\nRS sort. RS Sort involves only the comparison of pair of elements in an array\nwhich ultimately sorts the array and does not involve the comparison of each\nelement with every other element. RS sort tries to build upon the relationship\nestablished between the elements in each pass. Suppose there is an array\ncontaining three elements a1, a2, a3 and if a relationship exist such that\na1<a2 and a2<a3 then it can be established that a1<a3 and so there is no need\nto compare a1 and a3. Sorting is a fundamental operation in computer science.\nRS sort is analyzed both theoretically and empirically. We have performed its\nEmpirical analysis and compared its performance with the well-known quick sort\nfor various input types.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 20:38:18 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 13:54:56 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Ranjan", "Harsh", ""], ["Agarwal", "Sumit", ""], ["Singh", "Niraj Kumar", ""]]}, {"id": "1407.5218", "submitter": "Cameron Mura", "authors": "Marcin Cieslik, Zygmunt Derewenda, Cameron Mura", "title": "Abstractions, Algorithms and Data Structures for Structural\n  Bioinformatics in PyCogent", "comments": "36 pages, 4 figures (including supplemental information)", "journal-ref": "Journal of Applied Crystallography (2011), 44(2), 424-428", "doi": "10.1107/S0021889811004481", "report-no": null, "categories": "q-bio.BM cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate flexible and efficient structural bioinformatics analyses, new\nfunctionality for three-dimensional structure processing and analysis has been\nintroduced into PyCogent -- a popular feature-rich framework for sequence-based\nbioinformatics, but one which has lacked equally powerful tools for handling\nstuctural/coordinate-based data. Extensible Python modules have been developed,\nwhich provide object-oriented abstractions (based on a hierarchical\nrepresentation of macromolecules), efficient data structures (e.g. kD-trees),\nfast implementations of common algorithms (e.g. surface-area calculations),\nread/write support for Protein Data Bank-related file formats and wrappers for\nexternal command-line applications (e.g. Stride). Integration of this code into\nPyCogent is symbiotic, allowing sequence-based work to benefit from\nstructure-derived data and, reciprocally, enabling structural studies to\nleverage PyCogent's versatile tools for phylogenetic and evolutionary analyses.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jul 2014 19:41:02 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Cieslik", "Marcin", ""], ["Derewenda", "Zygmunt", ""], ["Mura", "Cameron", ""]]}, {"id": "1407.5298", "submitter": "Marco Molinaro", "authors": "Anupam Gupta and Marco Molinaro", "title": "How the Experts Algorithm Can Help Solve LPs Online", "comments": "An extended abstract appears in the 22nd European Symposium on\n  Algorithms (ESA 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of solving packing/covering LPs online, when the\ncolumns of the constraint matrix are presented in random order. This problem\nhas received much attention and the main focus is to figure out how large the\nright-hand sides of the LPs have to be (compared to the entries on the\nleft-hand side of the constraints) to allow $(1+\\epsilon)$-approximations\nonline. It is known that the right-hand sides have to be $\\Omega(\\epsilon^{-2}\n\\log m)$ times the left-hand sides, where $m$ is the number of constraints.\n  In this paper we give a primal-dual algorithm that achieve this bound for\nmixed packing/covering LPs. Our algorithms construct dual solutions using a\nregret-minimizing online learning algorithm in a black-box fashion, and use\nthem to construct primal solutions. The adversarial guarantee that holds for\nthe constructed duals helps us to take care of most of the correlations that\narise in the algorithm; the remaining correlations are handled via martingale\nconcentration and maximal inequalities. These ideas lead to conceptually simple\nand modular algorithms, which we hope will be useful in other contexts.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 14:37:32 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 00:04:44 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Gupta", "Anupam", ""], ["Molinaro", "Marco", ""]]}, {"id": "1407.5336", "submitter": "Florent Foucaud", "authors": "Edouard Bonnet, Florent Foucaud, Eun Jung Kim, Florian Sikora", "title": "Complexity of Grundy coloring and its variants", "comments": "24 pages, 7 figures. This version contains some new results and\n  improvements. A short paper based on version v2 appeared in COCOON'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Grundy number of a graph is the maximum number of colors used by the\ngreedy coloring algorithm over all vertex orderings. In this paper, we study\nthe computational complexity of GRUNDY COLORING, the problem of determining\nwhether a given graph has Grundy number at least $k$. We also study the\nvariants WEAK GRUNDY COLORING (where the coloring is not necessarily proper)\nand CONNECTED GRUNDY COLORING (where at each step of the greedy coloring\nalgorithm, the subgraph induced by the colored vertices must be connected).\n  We show that GRUNDY COLORING can be solved in time $O^*(2.443^n)$ and WEAK\nGRUNDY COLORING in time $O^*(2.716^n)$ on graphs of order $n$. While GRUNDY\nCOLORING and WEAK GRUNDY COLORING are known to be solvable in time\n$O^*(2^{O(wk)})$ for graphs of treewidth $w$ (where $k$ is the number of\ncolors), we prove that under the Exponential Time Hypothesis (ETH), they cannot\nbe solved in time $O^*(2^{o(w\\log w)})$. We also describe an\n$O^*(2^{2^{O(k)}})$ algorithm for WEAK GRUNDY COLORING, which is therefore\n$\\fpt$ for the parameter $k$. Moreover, under the ETH, we prove that such a\nrunning time is essentially optimal (this lower bound also holds for GRUNDY\nCOLORING). Although we do not know whether GRUNDY COLORING is in $\\fpt$, we\nshow that this is the case for graphs belonging to a number of standard graph\nclasses including chordal graphs, claw-free graphs, and graphs excluding a\nfixed minor. We also describe a quasi-polynomial time algorithm for GRUNDY\nCOLORING and WEAK GRUNDY COLORING on apex-minor graphs. In stark contrast with\nthe two other problems, we show that CONNECTED GRUNDY COLORING is\n$\\np$-complete already for $k=7$ colors.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 21:31:35 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 09:20:49 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2015 20:16:27 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Bonnet", "Edouard", ""], ["Foucaud", "Florent", ""], ["Kim", "Eun Jung", ""], ["Sikora", "Florian", ""]]}, {"id": "1407.5374", "submitter": "Lefteris  Kirousis", "authors": "Ioannis Giotis, Lefteris Kirousis, Kostas I. Psaromiligkos, and\n  Dimitrios M. Thilikos", "title": "Acyclic Edge Coloring through the Lov\\'asz Local Lemma", "comments": "The proof of Lemma 5 has been corrected", "journal-ref": "Theoretical Computer Science 665 (2017): 40-50", "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a probabilistic analysis of a Moser-type algorithm for the Lov\\'{a}sz\nLocal Lemma (LLL), adjusted to search for acyclic edge colorings of a graph. We\nthus improve the best known upper bound to acyclic chromatic index, also\nobtained by analyzing a similar algorithm, but through the entropic method\n(basically counting argument). Specifically we show that a graph with maximum\ndegree $\\Delta$ has an acyclic proper edge coloring with at most $\\lceil\n3.74(\\Delta-1)\\rceil+1 $ colors, whereas, previously, the best bound was\n$4(\\Delta-1)$. The main contribution of this work is that it comprises a\nprobabilistic analysis of a Moser-type algorithm applied to events pertaining\nto dependent variables.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 05:01:25 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 20:09:14 GMT"}, {"version": "v3", "created": "Fri, 25 Jul 2014 13:53:25 GMT"}, {"version": "v4", "created": "Tue, 29 Jul 2014 08:23:35 GMT"}, {"version": "v5", "created": "Tue, 12 Aug 2014 16:01:43 GMT"}, {"version": "v6", "created": "Sun, 12 Oct 2014 16:04:51 GMT"}, {"version": "v7", "created": "Wed, 13 May 2015 18:14:22 GMT"}, {"version": "v8", "created": "Thu, 14 Jul 2016 12:23:43 GMT"}, {"version": "v9", "created": "Sun, 7 Jan 2018 18:15:01 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Giotis", "Ioannis", ""], ["Kirousis", "Lefteris", ""], ["Psaromiligkos", "Kostas I.", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1407.5396", "submitter": "EPTCS", "authors": "Aaron Bohy (Universit\\'e de Mons), V\\'eronique Bruy\\`ere (Universit\\'e\n  de Mons), Jean-Fran\\c{c}ois Raskin (Universit\\'e Libre de Bruxelles)", "title": "Symblicit algorithms for optimal strategy synthesis in monotonic Markov\n  decision processes", "comments": "In Proceedings SYNT 2014, arXiv:1407.4937", "journal-ref": "EPTCS 157, 2014, pp. 51-67", "doi": "10.4204/EPTCS.157.8", "report-no": null, "categories": "cs.LO cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When treating Markov decision processes (MDPs) with large state spaces, using\nexplicit representations quickly becomes unfeasible. Lately, Wimmer et al. have\nproposed a so-called symblicit algorithm for the synthesis of optimal\nstrategies in MDPs, in the quantitative setting of expected mean-payoff. This\nalgorithm, based on the strategy iteration algorithm of Howard and Veinott,\nefficiently combines symbolic and explicit data structures, and uses binary\ndecision diagrams as symbolic representation. The aim of this paper is to show\nthat the new data structure of pseudo-antichains (an extension of antichains)\nprovides another interesting alternative, especially for the class of monotonic\nMDPs. We design efficient pseudo-antichain based symblicit algorithms (with\nopen source implementations) for two quantitative settings: the expected\nmean-payoff and the stochastic shortest path. For two practical applications\ncoming from automated planning and LTL synthesis, we report promising\nexperimental results w.r.t. both the run time and the memory consumption.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 07:28:33 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Bohy", "Aaron", "", "Universit\u00e9 de Mons"], ["Bruy\u00e8re", "V\u00e9ronique", "", "Universit\u00e9\n  de Mons"], ["Raskin", "Jean-Fran\u00e7ois", "", "Universit\u00e9 Libre de Bruxelles"]]}, {"id": "1407.5609", "submitter": "Sanguthevar Rajasekaran", "authors": "Sanguthevar Rajasekaran and Sudipta Pathak", "title": "Efficient Algorithms for the Closest Pair Problem and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The closest pair problem (CPP) is one of the well studied and fundamental\nproblems in computing. Given a set of points in a metric space, the problem is\nto identify the pair of closest points. Another closely related problem is the\nfixed radius nearest neighbors problem (FRNNP). Given a set of points and a\nradius $R$, the problem is, for every input point $p$, to identify all the\nother input points that are within a distance of $R$ from $p$. A naive\ndeterministic algorithm can solve these problems in quadratic time. CPP as well\nas FRNNP play a vital role in computational biology, computational finance,\nshare market analysis, weather prediction, entomology, electro cardiograph,\nN-body simulations, molecular simulations, etc. As a result, any improvements\nmade in solving CPP and FRNNP will have immediate implications for the solution\nof numerous problems in these domains. We live in an era of big data and\nprocessing these data take large amounts of time. Speeding up data processing\nalgorithms is thus much more essential now than ever before. In this paper we\npresent algorithms for CPP and FRNNP that improve (in theory and/or practice)\nthe best-known algorithms reported in the literature for CPP and FRNNP. These\nalgorithms also improve the best-known algorithms for related applications\nincluding time series motif mining and the two locus problem in Genome Wide\nAssociation Studies (GWAS).\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 19:39:46 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Rajasekaran", "Sanguthevar", ""], ["Pathak", "Sudipta", ""]]}, {"id": "1407.5750", "submitter": "Haim Kaplan", "authors": "Haim Kaplan and Robert E. Tarjan and Uri Zwick", "title": "Fibonacci Heaps Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fibonacci heap is a classic data structure that supports deletions in\nlogarithmic amortized time and all other heap operations in O(1) amortized\ntime. We explore the design space of this data structure. We propose a version\nwith the following improvements over the original: (i) Each heap is represented\nby a single heap-ordered tree, instead of a set of trees. (ii) Each\ndecrease-key operation does only one cut and a cascade of rank changes, instead\nof doing a cascade of cuts. (iii) The outcomes of all comparisons done by the\nalgorithm are explicitly represented in the data structure, so none are wasted.\nWe also give an example to show that without cascading cuts or rank changes,\nboth the original data structure and the new version fail to have the desired\nefficiency, solving an open problem of Fredman. Finally, we illustrate the\nrichness of the design space by proposing several alternative ways to do\ncascading rank changes, including a randomized strategy related to one\npreviously proposed by Karger. We leave the analysis of these alternatives as\nintriguing open problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 06:30:53 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Kaplan", "Haim", ""], ["Tarjan", "Robert E.", ""], ["Zwick", "Uri", ""]]}, {"id": "1407.6140", "submitter": "Kunihiro Wasa", "authors": "Kunihiro Wasa, Hiroki Arimura, Takeaki Uno", "title": "Efficient Enumeration of Induced Subtrees in a K-Degenerate Graph", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-13075-0_8", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of enumerating all induced subtrees in\nan input k-degenerate graph, where an induced subtree is an acyclic and\nconnected induced subgraph. A graph G = (V, E) is a k-degenerate graph if for\nany its induced subgraph has a vertex whose degree is less than or equal to k,\nand many real-world graphs have small degeneracies, or very close to small\ndegeneracies. Although, the studies are on subgraphs enumeration, such as\ntrees, paths, and matchings, but the problem addresses the subgraph\nenumeration, such as enumeration of subgraphs that are trees. Their induced\nsubgraph versions have not been studied well. One of few example is for\nchordless paths and cycles. Our motivation is to reduce the time complexity\nclose to O(1) for each solution. This type of optimal algorithms are proposed\nmany subgraph classes such as trees, and spanning trees. Induced subtrees are\nfundamental object thus it should be studied deeply and there possibly exist\nsome efficient algorithms. Our algorithm utilizes nice properties of\nk-degeneracy to state an effective amortized analysis. As a result, the time\ncomplexity is reduced to O(k) time per induced subtree. The problem is solved\nin constant time for each in planar graphs, as a corollary.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 08:51:27 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Wasa", "Kunihiro", ""], ["Arimura", "Hiroki", ""], ["Uno", "Takeaki", ""]]}, {"id": "1407.6144", "submitter": "Jakub Radoszewski", "authors": "Tomasz Kociumaka, Jakub W. Pachocki, Jakub Radoszewski, Wojciech\n  Rytter, and Tomasz Wale\\'n", "title": "On the String Consensus Problem and the Manhattan Sequence Consensus\n  Problem", "comments": "accepted to SPIRE 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Manhattan Sequence Consensus problem (MSC problem) we are given $k$\ninteger sequences, each of length $l$, and we are to find an integer sequence\n$x$ of length $l$ (called a consensus sequence), such that the maximum\nManhattan distance of $x$ from each of the input sequences is minimized. For\nbinary sequences Manhattan distance coincides with Hamming distance, hence in\nthis case the string consensus problem (also called string center problem or\nclosest string problem) is a special case of MSC. Our main result is a\npractically efficient $O(l)$-time algorithm solving MSC for $k\\le 5$ sequences.\nPracticality of our algorithms has been verified experimentally. It improves\nupon the quadratic algorithm by Amir et al.\\ (SPIRE 2012) for string consensus\nproblem for $k=5$ binary strings. Similarly as in Amir's algorithm we use a\ncolumn-based framework. We replace the implied general integer linear\nprogramming by its easy special cases, due to combinatorial properties of the\nMSC for $k\\le 5$. We also show that for a general parameter $k$ any instance\ncan be reduced in linear time to a kernel of size $k!$, so the problem is\nfixed-parameter tractable. Nevertheless, for $k\\ge 4$ this is still too large\nfor any naive solution to be feasible in practice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 09:03:27 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Pachocki", "Jakub W.", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1407.6178", "submitter": "Raed Jaberi", "authors": "Raed Jaberi", "title": "Computing the $2$-blocks of directed graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G$ be a directed graph. A \\textit{$2$-directed block} in $G$ is a\nmaximal vertex set $C^{2d}\\subseteq V$ with $|C^{2d}|\\geq 2$ such that for each\npair of distinct vertices $x,y \\in C^{2d}$, there exist two vertex-disjoint\npaths from $x$ to $y$ and two vertex-disjoint paths from $y$ to $x$ in $G$. In\ncontrast to the $2$-vertex-connected components of $G$, the subgraphs induced\nby the $2$-directed blocks may consist of few or no edges. In this paper we\npresent two algorithms for computing the $2$-directed blocks of $G$ in\n$O(\\min\\lbrace m,(t_{sap}+t_{sb})n\\rbrace n)$ time, where $t_{sap}$ is the\nnumber of the strong articulation points of $G$ and $t_{sb}$ is the number of\nthe strong bridges of $G$. Furthermore, we study two related concepts: the\n$2$-strong blocks and the $2$-edge blocks of $G$. We give two algorithms for\ncomputing the $2$-strong blocks of $G$ in $O( \\min \\lbrace m,t_{sap} n\\rbrace\nn)$ time and we show that the $2$-edge blocks of $G$ can be computed in $O(\\min\n\\lbrace m, t_{sb} n \\rbrace n)$ time. In this paper we also study some\noptimization problems related to the strong articulation points and the\n$2$-blocks of a directed graph. Given a strongly connected graph $G=(V,E)$,\nfind a minimum cardinality set $E^{*}\\subseteq E$ such that $G^{*}=(V,E^{*})$\nis strongly connected and the strong articulation points of $G$ coincide with\nthe strong articulation points of $G^{*}$. This problem is called minimum\nstrongly connected spanning subgraph with the same strong articulation points.\nWe show that there is a linear time $17/3$ approximation algorithm for this\nNP-hard problem. We also consider the problem of finding a minimum strongly\nconnected spanning subgraph with the same $2$-blocks in a strongly connected\ngraph $G$. We present approximation algorithms for three versions of this\nproblem, depending on the type of $2$-blocks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 11:20:24 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Jaberi", "Raed", ""]]}, {"id": "1407.6183", "submitter": "Marcello La Rocca", "authors": "Marcello La Rocca and Domenico Cantone", "title": "NeatSort - A practical adaptive algorithm", "comments": "23 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new adaptive sorting algorithm which is optimal for most\ndisorder metrics and, more important, has a simple and quick implementation. On\ninput $X$, our algorithm has a theoretical $\\Omega (|X|)$ lower bound and a\n$\\mathcal{O}(|X|\\log|X|)$ upper bound, exhibiting amazing adaptive properties\nwhich makes it run closer to its lower bound as disorder (computed on different\nmetrics) diminishes. From a practical point of view, \\textit{NeatSort} has\nproven itself competitive with (and often better than) \\textit{qsort} and any\n\\textit{Random Quicksort} implementation, even on random arrays.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 11:58:11 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["La Rocca", "Marcello", ""], ["Cantone", "Domenico", ""]]}, {"id": "1407.6327", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "Compressed representation of Learning Spaces", "comments": "33 pages, 8 figures, 11 Tables. Section 8 on query learning is\n  thoroughly revised", "journal-ref": "Journal of Mathematical Psychology 79, 64-76 (2017)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Spaces are certain set systems that are applied in the mathematical\nmodeling of education. We propose a suitable compression (without loss of\ninformation) of such set systems to facilitate their logical and statistical\nanalysis. Under certain circumstances compression is the prerequisite to\ncalculate the Learning Space in the first place. There are connections to the\ndual framework of Formal Concept Analysis and in particular to so called\nattribute exploration.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 13:46:23 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 13:17:42 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2015 08:33:14 GMT"}, {"version": "v4", "created": "Fri, 2 Dec 2016 10:42:49 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Wild", "Marcel", ""]]}, {"id": "1407.6328", "submitter": "Rani Izsak", "authors": "Moran Feldman and Rani Izsak", "title": "Constrained Monotone Function Maximization and the Supermodular Degree", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of maximizing a constrained monotone set function has many\npractical applications and generalizes many combinatorial problems.\nUnfortunately, it is generally not possible to maximize a monotone set function\nup to an acceptable approximation ratio, even subject to simple constraints.\nOne highly studied approach to cope with this hardness is to restrict the set\nfunction. An outstanding disadvantage of imposing such a restriction on the set\nfunction is that no result is implied for set functions deviating from the\nrestriction, even slightly. A more flexible approach, studied by Feige and\nIzsak, is to design an approximation algorithm whose approximation ratio\ndepends on the complexity of the instance, as measured by some complexity\nmeasure. Specifically, they introduced a complexity measure called supermodular\ndegree, measuring deviation from submodularity, and designed an algorithm for\nthe welfare maximization problem with an approximation ratio that depends on\nthis measure.\n  In this work, we give the first (to the best of our knowledge) algorithm for\nmaximizing an arbitrary monotone set function, subject to a k-extendible\nsystem. This class of constraints captures, for example, the intersection of\nk-matroids (note that a single matroid constraint is sufficient to capture the\nwelfare maximization problem). Our approximation ratio deteriorates gracefully\nwith the complexity of the set function and k. Our work can be seen as\ngeneralizing both the classic result of Fisher, Nemhauser and Wolsey, for\nmaximizing a submodular set function subject to a k-extendible system, and the\nresult of Feige and Izsak for the welfare maximization problem. Moreover, when\nour algorithm is applied to each one of these simpler cases, it obtains the\nsame approximation ratio as of the respective original work.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 18:37:25 GMT"}, {"version": "v2", "created": "Thu, 28 Aug 2014 11:08:18 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Feldman", "Moran", ""], ["Izsak", "Rani", ""]]}, {"id": "1407.6559", "submitter": "Markus Jalsenius", "authors": "Raphael Clifford, Markus Jalsenius, Benjamin Sach", "title": "Cell-Probe Bounds for Online Edit Distance and Other Pattern Matching\n  Problems", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give cell-probe bounds for the computation of edit distance, Hamming\ndistance, convolution and longest common subsequence in a stream. In this\nmodel, a fixed string of $n$ symbols is given and one $\\delta$-bit symbol\narrives at a time in a stream. After each symbol arrives, the distance between\nthe fixed string and a suffix of most recent symbols of the stream is reported.\nThe cell-probe model is perhaps the strongest model of computation for showing\ndata structure lower bounds, subsuming in particular the popular word-RAM\nmodel.\n  * We first give an $\\Omega((\\delta \\log n)/(w+\\log\\log n))$ lower bound for\nthe time to give each output for both online Hamming distance and convolution,\nwhere $w$ is the word size. This bound relies on a new encoding scheme and for\nthe first time holds even when $w$ is as small as a single bit.\n  * We then consider the online edit distance and longest common subsequence\nproblems in the bit-probe model ($w=1$) with a constant sized input alphabet.\nWe give a lower bound of $\\Omega(\\sqrt{\\log n}/(\\log\\log n)^{3/2})$ which\napplies for both problems. This second set of results relies both on our new\nencoding scheme as well as a carefully constructed hard distribution.\n  * Finally, for the online edit distance problem we show that there is an\n$O((\\log n)^2/w)$ upper bound in the cell-probe model. This bound gives a\ncontrast to our new lower bound and also establishes an exponential gap between\nthe known cell-probe and RAM model complexities.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 13:01:17 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Clifford", "Raphael", ""], ["Jalsenius", "Markus", ""], ["Sach", "Benjamin", ""]]}, {"id": "1407.6665", "submitter": "\\\"Ozg\\\"ur  \\\"Ozkan", "authors": "John Iacono, \\\"Ozg\\\"ur \\\"Ozkan", "title": "A Tight Lower Bound for Decrease-Key in the Pure Heap Model", "comments": "arXiv admin note: substantial text overlap with arXiv:1302.6641", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the lower bound on the amortized cost of the decrease-key\noperation in the pure heap model and show that any pure-heap-model heap (that\nhas a \\bigoh{\\log n} amortized-time extract-min operation) must spend\n\\bigom{\\log\\log n} amortized time on the decrease-key operation. Our result\nshows that sort heaps as well as pure-heap variants of numerous other heaps\nhave asymptotically optimal decrease-key operations in the pure heap model. In\naddition, our improved lower bound matches the lower bound of Fredman [J. ACM\n46(4):473-501 (1999)] for pairing heaps [M.L. Fredman, R. Sedgewick, D.D.\nSleator, and R.E. Tarjan. Algorithmica 1(1):111-129 (1986)] and surpasses it\nfor pure-heap variants of numerous other heaps with augmented data such as\npointer rank-pairing heaps.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 23:11:29 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Iacono", "John", ""], ["\u00d6zkan", "\u00d6zg\u00fcr", ""]]}, {"id": "1407.6730", "submitter": "Roei Tov", "authors": "Liam Roditty and Roei Tov", "title": "New routing techniques and their applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be an undirected graph with $n$ vertices and $m$ edges. We\nobtain the following new routing schemes:\n  - A routing scheme for unweighted graphs that uses $\\tilde\nO(\\frac{1}{\\epsilon} n^{2/3})$ space at each vertex and $\\tilde\nO(1/\\epsilon)$-bit headers, to route a message between any pair of vertices\n$u,v\\in V$ on a $(2 + \\epsilon,1)$-stretch path, i.e., a path of length at most\n$(2+\\epsilon)\\cdot d(u,v)+1$. This should be compared to the $(2,1)$-stretch\nand $\\tilde O(n^{5/3})$ space distance oracle of Patrascu and Roditty [FOCS'10\nand SIAM J. Comput. 2014] and to the $(2,1)$-stretch routing scheme of Abraham\nand Gavoille [DISC'11] that uses $\\tilde O( n^{3/4})$ space at each vertex.\n  - A routing scheme for weighted graphs with normalized diameter $D$, that\nuses $\\tilde O(\\frac{1}{\\epsilon} n^{1/3}\\log D)$ space at each vertex and\n$\\tilde O(\\frac{1}{\\epsilon}\\log D)$-bit headers, to route a message between\nany pair of vertices on a $(5+\\epsilon)$-stretch path. This should be compared\nto the $5$-stretch and $\\tilde O(n^{4/3})$ space distance oracle of Thorup and\nZwick [STOC'01 and J. ACM. 2005] and to the $7$-stretch routing scheme of\nThorup and Zwick [SPAA'01] that uses $\\tilde O( n^{1/3})$ space at each vertex.\nSince a $5$-stretch routing scheme must use tables of $\\Omega( n^{1/3})$ space\nour result is almost tight.\n  - For an integer $\\ell>1$, a routing scheme for unweighted graphs that uses\n$\\tilde O(\\ell\\frac{1}{\\epsilon} n^{\\ell/(2\\ell \\pm 1)})$ space at each vertex\nand $\\tilde O(\\frac{1}{\\epsilon})$-bit headers, to route a message between any\npair of vertices on a $(3\\pm2/\\ell+\\epsilon,2)$-stretch path.\n  - A routing scheme for weighted graphs, that uses $\\tilde\nO(\\frac{1}{\\epsilon}n^{1/k}\\log D)$ space at each vertex and $\\tilde\nO(\\frac{1}{\\epsilon}\\log D)$-bit headers, to route a message between any pair\nof vertices on a $(4k-7+\\epsilon)$-stretch path.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 20:32:09 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 11:18:16 GMT"}, {"version": "v3", "created": "Mon, 4 Aug 2014 07:46:52 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Roditty", "Liam", ""], ["Tov", "Roei", ""]]}, {"id": "1407.6753", "submitter": "Luis Meira", "authors": "Rog\\'erio H. B. de Lima and Luis A. A. Meira", "title": "Ordena\\c{c}\\~ao Baseada em \\'Arvores de Fus\\~ao", "comments": "10 pages, portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorting is one of the most important problem in the computer science. After\nmore than 60 years of studies, there are still many research devoted to develop\nfaster sorting algorithms. This work aims to explain the Fusion Tree data\nstructure. Fusion Tree was responsible for the first sorting algorithm with\ntime $o(n \\ lg n) $. -----\n  O problema da ordena\\c{c}\\~ao \\'e sem d\\'uvida um dos mais estudados na\nCi\\^encia da Computa\\c{c}\\~ao. No escopo da computa\\c{c}\\~ao moderna, depois de\nmais de 60 anos de estudos, ainda existem muitas pesquisas que objetivam o\ndesenvolvimento de algoritmos que solucionem uma ordena\\c{c}\\~ao mais r\\'apida\nou com menos recursos comparados a outros algoritmos j\\'a conhecidos. H\\'a\nv\\'arios tipos de algoritmos de ordena\\c{c}\\~ao, alguns mais r\\'apidos, outros\nmais econ\\^omicos em rela\\c{c}\\~ao ao espa\\c{c}o e outros com algumas\nrestri\\c{c}\\~oes com rela\\c{c}\\~ao \\`a entrada de dados. O objetivo deste\ntrabalho \\'e explicar a estrutura de dados \\'Avore de Fus\\~ao, respons\\'avel\npelo primeiro algoritmo de ordena\\c{c}\\~ao com tempo inferior a $ n \\lg n $,\ntempo esse que criou certa confus\\~ao, gerando uma errada cren\\c{c}a de ser o\nmenor poss\\'ivel para esse tipo de problema.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 22:44:12 GMT"}, {"version": "v2", "created": "Sat, 4 Oct 2014 02:07:35 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["de Lima", "Rog\u00e9rio H. B.", ""], ["Meira", "Luis A. A.", ""]]}, {"id": "1407.6755", "submitter": "Tsvi Kopelowitz", "authors": "Tsvi Kopelowitz, Seth Pettie, Ely Porat", "title": "Dynamic Set Intersection", "comments": "Accepted to WADS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of maintaining a family $F$ of dynamic sets subject to\ninsertions, deletions, and set-intersection reporting queries: given $S,S'\\in\nF$, report every member of $S\\cap S'$ in any order. We show that in the word\nRAM model, where $w$ is the word size, given a cap $d$ on the maximum size of\nany set, we can support set intersection queries in $O(\\frac{d}{w/\\log^2 w})$\nexpected time, and updates in $O(\\log w)$ expected time. Using this algorithm\nwe can list all $t$ triangles of a graph $G=(V,E)$ in\n$O(m+\\frac{m\\alpha}{w/\\log^2 w} +t)$ expected time, where $m=|E|$ and $\\alpha$\nis the arboricity of $G$. This improves a 30-year old triangle enumeration\nalgorithm of Chiba and Nishizeki running in $O(m \\alpha)$ time.\n  We provide an incremental data structure on $F$ that supports intersection\n{\\em witness} queries, where we only need to find {\\em one} $e\\in S\\cap S'$.\nBoth queries and insertions take $O\\paren{\\sqrt \\frac{N}{w/\\log^2 w}}$ expected\ntime, where $N=\\sum_{S\\in F} |S|$. Finally, we provide time/space tradeoffs for\nthe fully dynamic set intersection reporting problem. Using $M$ words of space,\neach update costs $O(\\sqrt {M \\log N})$ expected time, each reporting query\ncosts $O(\\frac{N\\sqrt{\\log N}}{\\sqrt M}\\sqrt{op+1})$ expected time where $op$\nis the size of the output, and each witness query costs $O(\\frac{N\\sqrt{\\log\nN}}{\\sqrt M} + \\log N)$ expected time.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 23:02:09 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 02:25:09 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Kopelowitz", "Tsvi", ""], ["Pettie", "Seth", ""], ["Porat", "Ely", ""]]}, {"id": "1407.6756", "submitter": "Tsvi Kopelowitz", "authors": "Tsvi Kopelowitz, Seth Pettie, Ely Porat", "title": "Higher Lower Bounds from the 3SUM Conjecture", "comments": "Full version of SODA 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3SUM conjecture has proven to be a valuable tool for proving conditional\nlower bounds on dynamic data structures and graph problems. This line of work\nwas initiated by P\\v{a}tra\\c{s}cu (STOC 2010) who reduced 3SUM to an offline\nSetDisjointness problem. However, the reduction introduced by P\\v{a}tra\\c{s}cu\nsuffers from several inefficiencies, making it difficult to obtain tight\nconditional lower bounds from the 3SUM conjecture.\n  In this paper we address many of the deficiencies of P\\v{a}tra\\c{s}cu's\nframework. We give new and efficient reductions from 3SUM to offline\nSetDisjointness and offline SetIntersection (the reporting version of\nSetDisjointness) which leads to polynomially higher lower bounds on several\nproblems. Using our reductions, we are able to show the essential optimality of\nseveral algorithms, assuming the 3SUM conjecture.\n  - Chiba and Nishizeki's $O(m\\alpha)$-time algorithm (SICOMP 1985) for\nenumerating all triangles in a graph with arboricity/degeneracy $\\alpha$ is\nessentially optimal, for any $\\alpha$.\n  - Bj{\\o}rklund, Pagh, Williams, and Zwick's algorithm (ICALP 2014) for\nlisting $t$ triangles is essentially optimal (assuming the matrix\nmultiplication exponent is $\\omega=2$).\n  - Any static data structure for SetDisjointness that answers queries in\nconstant time must spend $\\Omega(N^{2-o(1)})$ time in preprocessing, where $N$\nis the size of the set system.\n  These statements were unattainable via P\\v{a}tra\\c{s}cu's reductions.\n  We also introduce several new reductions from 3SUM to pattern matching\nproblems and dynamic graph problems. Of particular interest are new conditional\nlower bounds for dynamic versions of Maximum Cardinality Matching, which\nintroduce a new technique for obtaining amortized lower bounds.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 23:03:04 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 07:35:12 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 17:30:47 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Kopelowitz", "Tsvi", ""], ["Pettie", "Seth", ""], ["Porat", "Ely", ""]]}, {"id": "1407.6761", "submitter": "Gengchun Xu", "authors": "Qian-Ping Gu and Gengchun Xu", "title": "Near-Linear Time Constant-Factor Approximation Algorithm for\n  Branch-Decomposition of Planar Graphs", "comments": "The mainly revision is the $O(nk^2)$ algorithm part (Section 4):\n  added proofs for graphs with edge weights 1/2 and 1, and modified the proofs\n  for finding the minimum separating cycles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm which for an input planar graph $G$ of $n$ vertices and\ninteger $k$, in $\\min\\{O(n\\log^3n),O(nk^2)\\}$ time either constructs a\nbranch-decomposition of $G$ with width at most $(2+\\delta)k$, $\\delta>0$ is a\nconstant, or a $(k+1)\\times \\lceil{\\frac{k+1}{2}\\rceil}$ cylinder minor of $G$\nimplying $bw(G)>k$, $bw(G)$ is the branchwidth of $G$. This is the first\n$\\tilde{O}(n)$ time constant-factor approximation for branchwidth/treewidth and\nlargest grid/cylinder minors of planar graphs and improves the previous\n$\\min\\{O(n^{1+\\epsilon}),O(nk^2)\\}$ ($\\epsilon>0$ is a constant) time\nconstant-factor approximations. For a planar graph $G$ and $k=bw(G)$, a\nbranch-decomposition of width at most $(2+\\delta)k$ and a $g\\times \\frac{g}{2}$\ncylinder/grid minor with $g=\\frac{k}{\\beta}$, $\\beta>2$ is constant, can be\ncomputed by our algorithm in $\\min\\{O(n\\log^3n\\log k),O(nk^2\\log k)\\}$ time.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 23:52:54 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 05:24:18 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 21:03:47 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Gu", "Qian-Ping", ""], ["Xu", "Gengchun", ""]]}, {"id": "1407.6794", "submitter": "Shri Prakash Dwivedi", "authors": "Shri Prakash Dwivedi", "title": "GCD Computation of n Integers", "comments": "RAECS 2014", "journal-ref": null, "doi": "10.1109/RAECS.2014.6799612", "report-no": null, "categories": "cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greatest Common Divisor (GCD) computation is one of the most important\noperation of algorithmic number theory. In this paper we present the algorithms\nfor GCD computation of $n$ integers. We extend the Euclid's algorithm and\nbinary GCD algorithm to compute the GCD of more than two integers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 06:55:58 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Dwivedi", "Shri Prakash", ""]]}, {"id": "1407.6832", "submitter": "Christian Wulff-Nilsen", "authors": "Jacob Holm, Eva Rotenberg, Christian Wulff-Nilsen", "title": "Faster Fully-Dynamic Minimum Spanning Forest", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new data structure for the fully-dynamic minimum spanning forest\nproblem in simple graphs. Edge updates are supported in $O(\\log^4n/\\log\\log n)$\namortized time per operation, improving the $O(\\log^4n)$ amortized bound of\nHolm et al. (STOC'98, JACM'01). We assume the Word-RAM model with standard\ninstructions.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 09:33:30 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Holm", "Jacob", ""], ["Rotenberg", "Eva", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1407.6846", "submitter": "S{\\o}ren Dahlgaard", "authors": "S{\\o}ren Dahlgaard, Mathias B{\\ae}k Tejs Knudsen, Eva Rotenberg, and\n  Mikkel Thorup", "title": "The Power of Two Choices with Simple Tabulation", "comments": "SODA'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power of two choices is a classic paradigm for load balancing when\nassigning $m$ balls to $n$ bins. When placing a ball, we pick two bins\naccording to two hash functions $h_0$ and $h_1$, and place the ball in the\nleast loaded bin. Assuming fully random hash functions, when $m=O(n)$, Azar et\nal.~[STOC'94] proved that the maximum load is $\\lg \\lg n + O(1)$ with high\nprobability.\n  In this paper, we investigate the power of two choices when the hash\nfunctions $h_0$ and $h_1$ are implemented with simple tabulation, which is a\nvery efficient hash function evaluated in constant time. Following their\nanalysis of Cuckoo hashing [J.ACM'12], P\\v{a}tra\\c{s}cu and Thorup claimed that\nthe expected maximum load with simple tabulation is $O(\\lg\\lg n)$. This did not\ninclude any high probability guarantee, so the load balancing was not yet to be\ntrusted.\n  Here, we show that with simple tabulation, the maximum load is $O(\\lg\\lg n)$\nwith high probability, giving the first constant time hash function with this\nguarantee. We also give a concrete example where, unlike with fully random\nhashing, the maximum load is not bounded by $\\lg \\lg n + O(1)$, or even\n$(1+o(1))\\lg \\lg n$ with high probability. Finally, we show that the expected\nmaximum load is $\\lg \\lg n + O(1)$, just like with fully random hashing.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 10:56:37 GMT"}, {"version": "v2", "created": "Thu, 28 Aug 2014 12:07:04 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2016 15:24:43 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Rotenberg", "Eva", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1407.6869", "submitter": "Christian Wulff-Nilsen", "authors": "Christian Wulff-Nilsen", "title": "Faster Separators for Shallow Minor-Free Graphs via Dynamic Approximate\n  Distance Oracles", "comments": "16 pages. Full version of the paper that appeared at ICALP'14. Minor\n  fixes regarding the time bounds such that these bounds hold also for\n  non-sparse graphs", "journal-ref": "Automata, Languages, and Programming, Lecture Notes in Computer\n  Science Volume 8572, 2014, pp 1063-1074", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plotkin, Rao, and Smith (SODA'97) showed that any graph with $m$ edges and\n$n$ vertices that excludes $K_h$ as a depth $O(\\ell\\log n)$-minor has a\nseparator of size $O(n/\\ell + \\ell h^2\\log n)$ and that such a separator can be\nfound in $O(mn/\\ell)$ time. A time bound of $O(m + n^{2+\\epsilon}/\\ell)$ for\nany constant $\\epsilon > 0$ was later given (W., FOCS'11) which is an\nimprovement for non-sparse graphs. We give three new algorithms. The first has\nthe same separator size and running time $O(\\mbox{poly}(h)\\ell\nm^{1+\\epsilon})$. This is a significant improvement for small $h$ and $\\ell$.\nIf $\\ell = \\Omega(n^{\\epsilon'})$ for an arbitrarily small chosen constant\n$\\epsilon' > 0$, we get a time bound of $O(\\mbox{poly}(h)\\ell n^{1+\\epsilon})$.\nThe second algorithm achieves the same separator size (with a slightly larger\npolynomial dependency on $h$) and running time $O(\\mbox{poly}(h)(\\sqrt\\ell\nn^{1+\\epsilon} + n^{2+\\epsilon}/\\ell^{3/2}))$ when $\\ell =\n\\Omega(n^{\\epsilon'})$. Our third algorithm has running time\n$O(\\mbox{poly}(h)\\sqrt\\ell n^{1+\\epsilon})$ when $\\ell =\n\\Omega(n^{\\epsilon'})$. It finds a separator of size $O(n/\\ell) + \\tilde\nO(\\mbox{poly}(h)\\ell\\sqrt n)$ which is no worse than previous bounds when $h$\nis fixed and $\\ell = \\tilde O(n^{1/4})$. A main tool in obtaining our results\nis a novel application of a decremental approximate distance oracle of Roditty\nand Zwick.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 12:43:12 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Wulff-Nilsen", "Christian", ""]]}, {"id": "1407.7061", "submitter": "Ciaran McCreesh", "authors": "Ciaran McCreesh and Patrick Prosser", "title": "A Parallel Branch and Bound Algorithm for the Maximum Labelled Clique\n  Problem", "comments": "Author-final version. Accepted to Optimization Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum labelled clique problem is a variant of the maximum clique\nproblem where edges in the graph are given labels, and we are not allowed to\nuse more than a certain number of distinct labels in a solution. We introduce a\nnew branch-and-bound algorithm for the problem, and explain how it may be\nparallelised. We evaluate an implementation on a set of benchmark instances,\nand show that it is consistently faster than previously published results,\nsometimes by four or five orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 21:11:13 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 14:46:09 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["McCreesh", "Ciaran", ""], ["Prosser", "Patrick", ""]]}, {"id": "1407.7125", "submitter": "Asish Mukhopadhyay", "authors": "Asish Mukhopadhyay and Puspal Bhabak", "title": "A 3-factor approximation algorithm for a Maximum Acyclic Agreement\n  Forest on k rooted, binary phylogenetic trees", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic trees are leaf-labelled trees, where the leaves correspond to\nextant species (taxa), and the internal vertices represent ancestral species.\nThe evolutionary history of a set of species can be explained by more than one\nphylogenetic tree, giving rise to the problem of comparing phylogenetic trees\nfor similarity. Various distance metrics, like the subtree prune-and-regraft\n(SPR), tree bisection reconnection (TBR) and nearest neighbour interchange\n(NNI) have been proposed to capture this similarity. The distance between two\nphylogenetic trees can also be measured by the size of a Maximum Agreement\nForest (MAF) on these trees, as it has been shown that the rooted subtree\nprune-and-regraft distance is 1 less than the size of a MAF. Since computing a\nMAF of minimum size is an NP-hard problem, approximation algorithms are of\ninterest. Recently, it has been shown that the MAF on k(>=2) trees can be\napproximated to within a factor of 8. In this paper, we improve this ratio to\n3. For certain species, however, the evolutionary history is not completely\ntree-like. Due to reticulate evolution two gene trees, though related, appear\ndifferent, making a phylogenetic network a more appropriate representation of\nreticulate evolution. A phylogenetic network contains hybrid nodes for the\nspecies evolved from two parents. The number of such nodes is its hybridization\nnumber. It has been shown that this number is 1 less than the size of a Maximum\nAcyclic Agreement Forest (MAAF). We show that the MAAF for k(>= 2) phylogenetic\ntrees can be approximated to within a factor of 3.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 12:40:11 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 17:29:45 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Mukhopadhyay", "Asish", ""], ["Bhabak", "Puspal", ""]]}, {"id": "1407.7156", "submitter": "R.B. Sandeep", "authors": "N. R. Aravind, R. B. Sandeep and Naveen Sivadasan", "title": "On Polynomial Kernelization of $\\mathcal{H}$-free Edge Deletion", "comments": "12 pages. IPEC 2014 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a set of graphs $\\mathcal{H}$, the \\textsc{$\\mathcal{H}$-free Edge\nDeletion} problem asks to find whether there exist at most $k$ edges in the\ninput graph whose deletion results in a graph without any induced copy of\n$H\\in\\mathcal{H}$. In \\cite{cai1996fixed}, it is shown that the problem is\nfixed-parameter tractable if $\\mathcal{H}$ is of finite cardinality. However,\nit is proved in \\cite{cai2013incompressibility} that if $\\mathcal{H}$ is a\nsingleton set containing $H$, for a large class of $H$, there exists no\npolynomial kernel unless $coNP\\subseteq NP/poly$. In this paper, we present a\npolynomial kernel for this problem for any fixed finite set $\\mathcal{H}$ of\nconnected graphs and when the input graphs are of bounded degree. We note that\nthere are \\textsc{$\\mathcal{H}$-free Edge Deletion} problems which remain\nNP-complete even for the bounded degree input graphs, for example\n\\textsc{Triangle-free Edge Deletion}\\cite{brugmann2009generating} and\n\\textsc{Custer Edge Deletion($P_3$-free Edge\nDeletion)}\\cite{komusiewicz2011alternative}. When $\\mathcal{H}$ contains\n$K_{1,s}$, we obtain a stronger result - a polynomial kernel for $K_t$-free\ninput graphs (for any fixed $t> 2$). We note that for $s>9$, there is an\nincompressibility result for \\textsc{$K_{1,s}$-free Edge Deletion} for general\ngraphs \\cite{cai2012polynomial}. Our result provides first polynomial kernels\nfor \\textsc{Claw-free Edge Deletion} and \\textsc{Line Edge Deletion} for\n$K_t$-free input graphs which are NP-complete even for $K_4$-free\ngraphs\\cite{yannakakis1981edge} and were raised as open problems in\n\\cite{cai2013incompressibility,open2013worker}.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 20:17:53 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 06:26:17 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Aravind", "N. R.", ""], ["Sandeep", "R. B.", ""], ["Sivadasan", "Naveen", ""]]}, {"id": "1407.7161", "submitter": "Arkaidiusz Socala", "authors": "{\\L}ukasz Kowalik and Arkadiusz Soca{\\l}a", "title": "Assigning channels via the meet-in-the-middle approach", "comments": "SWAT 2014: 282-293", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the Channel Assignment problem. By applying the\nmeet-in-the-middle approach we get an algorithm for the $\\ell$-bounded Channel\nAssignment (when the edge weights are bounded by $\\ell$) running in time\n$O^*((2\\sqrt{\\ell+1})^n)$. This is the first algorithm which breaks the\n$(O(\\ell))^n$ barrier. We extend this algorithm to the counting variant, at the\ncost of slightly higher polynomial factor.\n  A major open problem asks whether Channel Assignment admits a $O(c^n)$-time\nalgorithm, for a constant $c$ independent of $\\ell$. We consider a similar\nquestion for Generalized T-Coloring, a CSP problem that generalizes \\CA. We\nshow that Generalized T-Coloring does not admit a\n$2^{2^{o\\left(\\sqrt{n}\\right)}} {\\rm poly}(r)$-time algorithm, where $r$ is the\nsize of the instance.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 22:17:53 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 05:26:53 GMT"}, {"version": "v3", "created": "Mon, 4 Aug 2014 02:46:18 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Kowalik", "\u0141ukasz", ""], ["Soca\u0142a", "Arkadiusz", ""]]}, {"id": "1407.7162", "submitter": "Arkaidiusz Socala", "authors": "Arkadiusz Socala", "title": "Tight lower bound for the channel assignment problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the Channel Assignment problem. A major open\nproblem asks whether Channel Assignment admits an $O(c^n)$-time algorithm, for\na constant $c$ independent of the weights on the edges. We answer this question\nin the negative i.e. we show that there is no $2^{o(n\\log n)}$-time algorithm\nsolving Channel Assignment unless the Exponential Time Hypothesis fails. Note\nthat the currently best known algorithm works in time $O^*(n!) = 2^{O(n\\log\nn)}$ so our lower bound is tight.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 22:21:35 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Socala", "Arkadiusz", ""]]}, {"id": "1407.7216", "submitter": "Krzysztof Sornat", "authors": "Jaroslaw Byrka, Krzysztof Sornat (Institute of Computer Science,\n  University of Wroclaw, Poland)", "title": "PTAS for Minimax Approval Voting", "comments": "15 pages, 1 figure", "journal-ref": "Proceedings of the 10th International Conference on Web and\n  Internet Economics, WINE 2014, pages 203--217, 2014", "doi": "10.1007/978-3-319-13129-0_15", "report-no": null, "categories": "cs.DS cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Approval Voting systems where each voter decides on a subset to\ncandidates he/she approves. We focus on the optimization problem of finding the\ncommittee of fixed size k minimizing the maximal Hamming distance from a vote.\nIn this paper we give a PTAS for this problem and hence resolve the open\nquestion raised by Carragianis et al. [AAAI'10]. The result is obtained by\nadapting the techniques developed by Li et al. [JACM'02] originally used for\nthe less constrained Closest String problem. The technique relies on extracting\ninformation and structural properties of constant size subsets of votes.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 13:02:08 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 14:04:14 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Byrka", "Jaroslaw", "", "Institute of Computer Science,\n  University of Wroclaw, Poland"], ["Sornat", "Krzysztof", "", "Institute of Computer Science,\n  University of Wroclaw, Poland"]]}, {"id": "1407.7269", "submitter": "Keren Cohavi", "authors": "Keren Cohavi, Shahar Dobzinski", "title": "Faster and Simpler Sketches of Valuation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present fast algorithms for sketching valuation functions. Let $N$\n($|N|=n$) be some ground set and $v:2^N\\rightarrow \\mathbb R$ be a function. We\nsay that $\\tilde v:2^N\\rightarrow \\mathbb R$ is an $\\alpha$-sketch of $v$ if\nfor every set $S$ we have that $\\frac {v(S)} {\\alpha} \\leq \\tilde v(S) \\leq\nv(S)$ and $\\tilde v$ can be described in $poly(n)$ bits.\n  Goemans et al. [SODA'09] showed that if $v$ is submodular then there exists\nan $\\tilde O(\\sqrt n)$-sketch that can be constructed using polynomially many\nvalue queries (this is the best possible, as Balcan and Harvey [STOC'11] show\nthat no submodular function admit an $n^{\\frac 1 3 - \\epsilon}$-sketch). Based\non their work, Balcan et al. [COLT'12] and Badanidiyuru et al. [SODA'12] show\nthat if $v$ is subadditive then there exists an $\\tilde O(\\sqrt n)$-sketch that\ncan be constructed using polynomially many demand queries. All previous\nsketches are based on complicated geometric constructions. The first step in\ntheir constructions is proving the existence of a good sketch by finding an\nellipsoid that ``approximates'' $v$ well (this is done by applying John's\ntheorem to ensure the existence of an ellipsoid that is ``close'' to the\npolymatroid that is associated with $v$). The second step is showing this\nellipsoid can be found efficiently, and this is done by repeatedly solving a\ncertain convex program to obtain better approximations of John's ellipsoid.\n  In this paper, we give a much simpler, non-geometric proof for the existence\nof good sketches, and utilize the proof to obtain much faster algorithms that\nmatch the previously obtained approximation bounds. Specifically, we provide an\nalgorithm that finds $\\tilde O(\\sqrt n)$-sketch of a submodular function with\nonly $\\tilde O(n^\\frac{3}{2})$ value queries, and an algorithm that finds\n$\\tilde O(\\sqrt n)$-sketch of a subadditive function with $O(n)$ demand and\nvalue queries.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 18:15:23 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Cohavi", "Keren", ""], ["Dobzinski", "Shahar", ""]]}, {"id": "1407.7279", "submitter": "Elliot Meyerson", "authors": "Eric Aaron, Danny Krizanc, Elliot Meyerson", "title": "DMVP: Foremost Waypoint Coverage of Time-Varying Graphs", "comments": "24 pages. Full version of paper from Proceedings of WG 2014, LNCS,\n  Springer-Verlag", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Dynamic Map Visitation Problem (DMVP), in which a team of\nagents must visit a collection of critical locations as quickly as possible, in\nan environment that may change rapidly and unpredictably during the agents'\nnavigation. We apply recent formulations of time-varying graphs (TVGs) to DMVP,\nshedding new light on the computational hierarchy $\\mathcal{R} \\supset\n\\mathcal{B} \\supset \\mathcal{P}$ of TVG classes by analyzing them in the\ncontext of graph navigation. We provide hardness results for all three classes,\nand for several restricted topologies, we show a separation between the classes\nby showing severe inapproximability in $\\mathcal{R}$, limited approximability\nin $\\mathcal{B}$, and tractability in $\\mathcal{P}$. We also give topologies in\nwhich DMVP in $\\mathcal{R}$ is fixed parameter tractable, which may serve as a\nfirst step toward fully characterizing the features that make DMVP difficult.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 20:28:11 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Aaron", "Eric", ""], ["Krizanc", "Danny", ""], ["Meyerson", "Elliot", ""]]}, {"id": "1407.7294", "submitter": "Lili Dworkin", "authors": "Kareem Amin, Rachel Cummings, Lili Dworkin, Michael Kearns, Aaron Roth", "title": "Online Learning and Profit Maximization from Revealed Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from revealed preferences in an online\nsetting. In our framework, each period a consumer buys an optimal bundle of\ngoods from a merchant according to her (linear) utility function and current\nprices, subject to a budget constraint. The merchant observes only the\npurchased goods, and seeks to adapt prices to optimize his profits. We give an\nefficient algorithm for the merchant's problem that consists of a learning\nphase in which the consumer's utility function is (perhaps partially) inferred,\nfollowed by a price optimization step. We also consider an alternative online\nlearning algorithm for the setting where prices are set exogenously, but the\nmerchant would still like to predict the bundle that will be bought by the\nconsumer for purposes of inventory or supply chain management. In contrast with\nmost prior work on the revealed preferences problem, we demonstrate that by\nmaking stronger assumptions on the form of utility functions, efficient\nalgorithms for both learning and profit maximization are possible, even in\nadaptive, online settings.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 23:38:09 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 21:45:08 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Amin", "Kareem", ""], ["Cummings", "Rachel", ""], ["Dworkin", "Lili", ""], ["Kearns", "Michael", ""], ["Roth", "Aaron", ""]]}, {"id": "1407.7423", "submitter": "Micha{\\l}  Karpi\\'nski", "authors": "Micha{\\l} Karpi\\'nski", "title": "Vertex 2-coloring without monochromatic cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a problem of vertex two-coloring of undirected graph\nsuch that there is no monochromatic cycle of given length. We show that this\nproblem is hard to solve. We give a proof by presenting a reduction from\nvariation of satisfiability (SAT) problem. We show nice properties of coloring\ncliques with two colors which plays pivotal role in the reduction construction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 14:07:09 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Karpi\u0144ski", "Micha\u0142", ""]]}, {"id": "1407.7459", "submitter": "Vasileios Iliopoulos", "authors": "Vasileios Iliopoulos", "title": "A note on multipivot Quicksort", "comments": "Author's accepted manuscript, 7 pages", "journal-ref": "J. Inform. Optim. Sci. 39 (5): 1139-1147, 2018", "doi": "10.1080/02522667.2017.1303947", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse a generalisation of the Quicksort algorithm, where k uniformly at\nrandom chosen pivots are used for partitioning an array of n distinct keys.\nSpecifically, the expected cost of this scheme is obtained, under the\nassumption of linearity of the cost needed for the partition process. The\nintegration constants of the expected cost are computed using Vandermonde\nmatrices.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 13:58:57 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 16:54:56 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 16:26:01 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Iliopoulos", "Vasileios", ""]]}, {"id": "1407.7498", "submitter": "Jason Teutsch", "authors": "Robert F. Erbacher, Trent Jaeger, Nirupama Talele, Jason Teutsch", "title": "Directed Multicut with linearly ordered terminals", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an application in network security, we investigate the following\n\"linear\" case of Directed Mutlicut. Let $G$ be a directed graph which includes\nsome distinguished vertices $t_1, \\ldots, t_k$. What is the size of the\nsmallest edge cut which eliminates all paths from $t_i$ to $t_j$ for all $i <\nj$? We show that this problem is fixed-parameter tractable when parametrized in\nthe cutset size $p$ via an algorithm running in $O(4^p p n^4)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 19:01:26 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Erbacher", "Robert F.", ""], ["Jaeger", "Trent", ""], ["Talele", "Nirupama", ""], ["Teutsch", "Jason", ""]]}, {"id": "1407.7654", "submitter": "Giorgio Lucarelli", "authors": "Evripidis Bampis, Dimitrios Letsios, and Giorgio Lucarelli", "title": "Speed-scaling with no Preemptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the non-preemptive speed-scaling problem, in which a set of jobs\nhave to be executed on a single or a set of parallel speed-scalable\nprocessor(s) between their release dates and deadlines so that the energy\nconsumption to be minimized. We adopt the speed-scaling mechanism first\nintroduced in [Yao et al., FOCS 1995] according to which the power dissipated\nis a convex function of the processor's speed. Intuitively, the higher is the\nspeed of a processor, the higher is the energy consumption. For the\nsingle-processor case, we improve the best known approximation algorithm by\nproviding a $(1+\\epsilon)^{\\alpha}\\tilde{B}_{\\alpha}$-approximation algorithm,\nwhere $\\tilde{B}_{\\alpha}$ is a generalization of the Bell number. For the\nmultiprocessor case, we present an approximation algorithm of ratio\n$\\tilde{B}_{\\alpha}((1+\\epsilon)(1+\\frac{w_{\\max}}{w_{\\min}}))^{\\alpha}$\nimproving the best known result by a factor of\n$(\\frac{5}{2})^{\\alpha-1}(\\frac{w_{\\max}}{w_{\\min}})^{\\alpha}$. Notice that our\nresult holds for the fully heterogeneous environment while the previous known\nresult holds only in the more restricted case of parallel processors with\nidentical power functions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 07:55:23 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Bampis", "Evripidis", ""], ["Letsios", "Dimitrios", ""], ["Lucarelli", "Giorgio", ""]]}, {"id": "1407.7671", "submitter": "EPTCS", "authors": "Dragan Bo\\v{s}na\\v{c}ki (Eindhoven University of Technology), Stefan\n  Edelkamp (University of Bremen), Alberto Lluch Lafuente (Technical University\n  of Denmark), Anton Wijs (Eindhoven University of Technology)", "title": "Proceedings 3rd Workshop on GRAPH Inspection and Traversal Engineering", "comments": null, "journal-ref": "EPTCS 159, 2014", "doi": "10.4204/EPTCS.159", "report-no": null, "categories": "cs.LO cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are the proceedings of the Third Workshop on GRAPH Inspection and\nTraversal Engineering (GRAPHITE 2014), which took place on April 5, 2014 in\nGrenoble, France, as a satellite event of the 17th European Joint Conferences\non Theory and Practice of Software (ETAPS 2014).\n  The aim of GRAPHITE is to foster the convergence on research interests from\nseveral communities dealing with graph analysis in all its forms in computer\nscience, with a particular attention to software development and analysis.\nGraphs are used to represent data and processes in many application areas, and\nthey are subjected to various computational algorithms in order to analyze\nthem. Just restricting the attention to the analysis of software, graph\nanalysis algorithms are used, for instance, to verify properties using model\nchecking techniques that explore the system's state space graph or static\nanalysis techniques based on control flow graphs. Further application domains\ninclude games, planning, and network analysis. Very often, graph problems and\ntheir algorithmic solutions have common characteristics, independent of their\napplication domain. The goal of this event is to gather scientists from\ndifferent communities, who do research on graph analysis algorithms, such that\nawareness of each others' work is increased. More information can be found at\nhttp://sysma.imtlucca.it/graphite.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 09:25:55 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Bo\u0161na\u010dki", "Dragan", "", "Eindhoven University of Technology"], ["Edelkamp", "Stefan", "", "University of Bremen"], ["Lafuente", "Alberto Lluch", "", "Technical University\n  of Denmark"], ["Wijs", "Anton", "", "Eindhoven University of Technology"]]}, {"id": "1407.7740", "submitter": "Zhiwei Steven Wu", "authors": "Rachel Cummings, Michael Kearns, Aaron Roth, Zhiwei Steven Wu", "title": "Privacy and Truthful Equilibrium Selection for Aggregative Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a very general class of games --- multi-dimensional aggregative\ngames --- which in particular generalize both anonymous games and weighted\ncongestion games. For any such game that is also large, we solve the\nequilibrium selection problem in a strong sense. In particular, we give an\nefficient weak mediator: a mechanism which has only the power to listen to\nreported types and provide non-binding suggested actions, such that (a) it is\nan asymptotic Nash equilibrium for every player to truthfully report their type\nto the mediator, and then follow its suggested action; and (b) that when\nplayers do so, they end up coordinating on a particular asymptotic pure\nstrategy Nash equilibrium of the induced complete information game. In fact,\ntruthful reporting is an ex-post Nash equilibrium of the mediated game, so our\nsolution applies even in settings of incomplete information, and even when\nplayer types are arbitrary or worst-case (i.e. not drawn from a common prior).\nWe achieve this by giving an efficient differentially private algorithm for\ncomputing a Nash equilibrium in such games. The rates of convergence to\nequilibrium in all of our results are inverse polynomial in the number of\nplayers $n$. We also apply our main results to a multi-dimensional market game.\n  Our results can be viewed as giving, for a rich class of games, a more robust\nversion of the Revelation Principle, in that we work with weaker informational\nassumptions (no common prior), yet provide a stronger solution concept (ex-post\nNash versus Bayes Nash equilibrium). In comparison to previous work, our main\nconceptual contribution is showing that weak mediators are a game theoretic\nobject that exist in a wide variety of games -- previously, they were only\nknown to exist in traffic routing games.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 14:37:53 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 18:27:57 GMT"}, {"version": "v3", "created": "Tue, 24 Feb 2015 21:01:58 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Cummings", "Rachel", ""], ["Kearns", "Michael", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1407.7759", "submitter": "Amey Bhangale", "authors": "Amey Bhangale, Swastik Kopparty and Sushant Sachdeva", "title": "Simultaneous Approximation of Constraint Satisfaction Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $k$ collections of 2SAT clauses on the same set of variables $V$, can\nwe find one assignment that satisfies a large fraction of clauses from each\ncollection? We consider such simultaneous constraint satisfaction problems, and\ndesign the first nontrivial approximation algorithms in this context.\n  Our main result is that for every CSP $F$, for $k < \\tilde{O}(\\log^{1/4} n)$,\nthere is a polynomial time constant factor Pareto approximation algorithm for\n$k$ simultaneous Max-$F$-CSP instances. Our methods are quite general, and we\nalso use them to give an improved approximation factor for simultaneous\nMax-w-SAT (for $k <\\tilde{O}(\\log^{1/3} n)$). In contrast, for $k = \\omega(\\log\nn)$, no nonzero approximation factor for $k$ simultaneous Max-$F$-CSP instances\ncan be achieved in polynomial time (assuming the Exponential Time Hypothesis).\n  These problems are a natural meeting point for the theory of constraint\nsatisfaction problems and multiobjective optimization. We also suggest a number\nof interesting directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 15:48:38 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Bhangale", "Amey", ""], ["Kopparty", "Swastik", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1407.7882", "submitter": "Moti Medina", "authors": "Guy Even, Moti Medina, and Dana Ron", "title": "Distributed Maximum Matching in Bounded Degree Graphs", "comments": "arXiv admin note: substantial text overlap with arXiv:1402.3796", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present deterministic distributed algorithms for computing approximate\nmaximum cardinality matchings and approximate maximum weight matchings. Our\nalgorithm for the unweighted case computes a matching whose size is at least\n$(1-\\eps)$ times the optimal in $\\Delta^{O(1/\\eps)} +\nO\\left(\\frac{1}{\\eps^2}\\right) \\cdot\\log^*(n)$ rounds where $n$ is the number\nof vertices in the graph and $\\Delta$ is the maximum degree. Our algorithm for\nthe edge-weighted case computes a matching whose weight is at least $(1-\\eps)$\ntimes the optimal in\n$\\log(\\min\\{1/\\wmin,n/\\eps\\})^{O(1/\\eps)}\\cdot(\\Delta^{O(1/\\eps)}+\\log^*(n))$\nrounds for edge-weights in $[\\wmin,1]$.\n  The best previous algorithms for both the unweighted case and the weighted\ncase are by Lotker, Patt-Shamir, and Pettie~(SPAA 2008). For the unweighted\ncase they give a randomized $(1-\\eps)$-approximation algorithm that runs in\n$O((\\log(n)) /\\eps^3)$ rounds. For the weighted case they give a randomized\n$(1/2-\\eps)$-approximation algorithm that runs in $O(\\log(\\eps^{-1}) \\cdot\n\\log(n))$ rounds. Hence, our results improve on the previous ones when the\nparameters $\\Delta$, $\\eps$ and $\\wmin$ are constants (where we reduce the\nnumber of runs from $O(\\log(n))$ to $O(\\log^*(n))$), and more generally when\n$\\Delta$, $1/\\eps$ and $1/\\wmin$ are sufficiently slowly increasing functions\nof $n$. Moreover, our algorithms are deterministic rather than randomized.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 20:42:30 GMT"}, {"version": "v2", "created": "Mon, 4 Aug 2014 06:35:50 GMT"}, {"version": "v3", "created": "Tue, 11 Nov 2014 20:57:53 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Even", "Guy", ""], ["Medina", "Moti", ""], ["Ron", "Dana", ""]]}, {"id": "1407.7887", "submitter": "Grigory Yaroslavtsev", "authors": "Grigory Yaroslavtsev", "title": "Going for Speed: Sublinear Algorithms for Dense r-CSPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new sublinear and parallel algorithms for the extensively studied\nproblem of approximating n-variable r-CSPs (constraint satisfaction problems\nwith constraints of arity r up to an additive error. The running time of our\nalgorithms is O(n/\\epsilon^2) + 2^O(1/\\epsilon^2) for Boolean r-CSPs and O(k^4\nn / \\epsilon^2) + 2^O(log k / \\epsilon^2) for r-CSPs with constraints on\nvariables over an alphabet of size k. For any constant k this gives optimal\ndependence on n in the running time unconditionally, while the exponent in the\ndependence on 1/\\epsilon is polynomially close to the lower bound under the\nexponential-time hypothesis, which is 2^\\Omega(\\epsilon^(-1/2)).\n  For Max-Cut this gives an exponential improvement in dependence on 1/\\epsilon\ncompared to the sublinear algorithms of Goldreich, Goldwasser and Ron (JACM'98)\nand a linear speedup in n compared to the algorithms of Mathieu and Schudy\n(SODA'08). For the maximization version of k-Correlation Clustering problem our\nrunning time is O(k^4 n / \\epsilon^2) + k^O(1/\\epsilon^2), improving the\npreviously best n k^{O(1/\\epsilon^3 log k/\\epsilon) by Guruswami and Giotis\n(SODA'06).\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 21:12:39 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Yaroslavtsev", "Grigory", ""]]}, {"id": "1407.7917", "submitter": "Pat Morin", "authors": "Luis Barba and Pat Morin", "title": "Top-Down Skiplists", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe todolists (top-down skiplists), a variant of skiplists (Pugh\n1990) that can execute searches using at most $\\log_{2-\\varepsilon} n + O(1)$\nbinary comparisons per search and that have amortized update time\n$O(\\varepsilon^{-1}\\log n)$. A variant of todolists, called working-todolists,\ncan execute a search for any element $x$ using $\\log_{2-\\varepsilon} w(x) +\no(\\log w(x))$ binary comparisons and have amortized search time\n$O(\\varepsilon^{-1}\\log w(w))$. Here, $w(x)$ is the \"working-set number\" of\n$x$. No previous data structure is known to achieve a bound better than\n$4\\log_2 w(x)$ comparisons. We show through experiments that, if implemented\ncarefully, todolists are comparable to other common dictionary implementations\nin terms of insertion times and outperform them in terms of search times.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 01:24:34 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Barba", "Luis", ""], ["Morin", "Pat", ""]]}, {"id": "1407.7930", "submitter": "EPTCS", "authors": "Roi Blanco (Yahoo! Research Barcelona, Spain), Paolo Boldi\n  (Dipartimento di informatica, Universit\\`a degli Studi di Milano), Andrea\n  Marino (Dipartimento di informatica, Universit\\`a degli Studi di Milano)", "title": "Entity-Linking via Graph-Distance Minimization", "comments": "In Proceedings GRAPHITE 2014, arXiv:1407.7671. The second and third\n  authors were supported by the EU-FET grant NADINE (GA 288956)", "journal-ref": "EPTCS 159, 2014, pp. 30-43", "doi": "10.4204/EPTCS.159.4", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity-linking is a natural-language-processing task that consists in\nidentifying the entities mentioned in a piece of text, linking each to an\nappropriate item in some knowledge base; when the knowledge base is Wikipedia,\nthe problem comes to be known as wikification (in this case, items are\nwikipedia articles). One instance of entity-linking can be formalized as an\noptimization problem on the underlying concept graph, where the quantity to be\noptimized is the average distance between chosen items. Inspired by this\napplication, we define a new graph problem which is a natural variant of the\nMaximum Capacity Representative Set. We prove that our problem is NP-hard for\ngeneral graphs; nonetheless, under some restrictive assumptions, it turns out\nto be solvable in linear time. For the general case, we propose two heuristics:\none tries to enforce the above assumptions and another one is based on the\nnotion of hitting distance; we show experimentally how these approaches perform\nwith respect to some baselines on a real-world dataset.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 03:22:51 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Blanco", "Roi", "", "Yahoo! Research Barcelona, Spain"], ["Boldi", "Paolo", "", "Dipartimento di informatica, Universit\u00e0 degli Studi di Milano"], ["Marino", "Andrea", "", "Dipartimento di informatica, Universit\u00e0 degli Studi di Milano"]]}, {"id": "1407.7998", "submitter": "Kevin Schewior", "authors": "Lin Chen, Nicole Megow, Kevin Schewior", "title": "New Results on Online Resource Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online resource minimization problem in which jobs with hard\ndeadlines arrive online over time at their release dates. The task is to\ndetermine a feasible schedule on a minimum number of machines. We rigorously\nstudy this problem and derive various algorithms with small constant\ncompetitive ratios for interesting restricted problem variants. As the most\nimportant special case, we consider scheduling jobs with agreeable deadlines.\nWe provide the first constant ratio competitive algorithm for the\nnon-preemptive setting, which is of particular interest with regard to the\nknown strong lower bound of n for the general problem. For the preemptive\nsetting, we show that the natural algorithm LLF achieves a constant ratio for\nagreeable jobs, while for general jobs it has a lower bound of Omega(n^(1/3)).\nWe also give an O(log n)-competitive algorithm for the general preemptive\nproblem, which improves upon the known O(p_max/p_min)-competitive algorithm.\nOur algorithm maintains a dynamic partition of the job set into loose and tight\njobs and schedules each (temporal) subset individually on separate sets of\nmachines. The key is a characterization of how the decrease in the relative\nlaxity of jobs influences the optimum number of machines. To achieve this we\nderive a compact expression of the optimum value, which might be of independent\ninterest. We complement the general algorithmic result by showing lower bounds\nthat rule out that other known algorithms may yield a similar performance\nguarantee.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 11:03:11 GMT"}, {"version": "v2", "created": "Sat, 27 Sep 2014 19:46:04 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 17:09:55 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Chen", "Lin", ""], ["Megow", "Nicole", ""], ["Schewior", "Kevin", ""]]}, {"id": "1407.8088", "submitter": "Alejandro Edera", "authors": "Alejandro Edera, Yanela Strappa and Facundo Bromberg", "title": "The Grow-Shrink strategy for learning Markov network structures\n  constrained by context-specific independences", "comments": "12 pages, and 8 figures. This works was presented in IBERAMIA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks are models for compactly representing complex probability\ndistributions. They are composed by a structure and a set of numerical weights.\nThe structure qualitatively describes independences in the distribution, which\ncan be exploited to factorize the distribution into a set of compact functions.\nA key application for learning structures from data is to automatically\ndiscover knowledge. In practice, structure learning algorithms focused on\n\"knowledge discovery\" present a limitation: they use a coarse-grained\nrepresentation of the structure. As a result, this representation cannot\ndescribe context-specific independences. Very recently, an algorithm called\nCSPC was designed to overcome this limitation, but it has a high computational\ncomplexity. This work tries to mitigate this downside presenting CSGS, an\nalgorithm that uses the Grow-Shrink strategy for reducing unnecessary\ncomputations. On an empirical evaluation, the structures learned by CSGS\nachieve competitive accuracies and lower computational complexity with respect\nto those obtained by CSPC.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 15:24:46 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Edera", "Alejandro", ""], ["Strappa", "Yanela", ""], ["Bromberg", "Facundo", ""]]}, {"id": "1407.8142", "submitter": "Julian Shun", "authors": "Julian Shun", "title": "Parallel Wavelet Tree Construction", "comments": "This is a longer version of the paper that appears in the Proceedings\n  of the IEEE Data Compression Conference, 2015", "journal-ref": null, "doi": "10.1109/DCC.2015.7", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present parallel algorithms for wavelet tree construction with\npolylogarithmic depth, improving upon the linear depth of the recent parallel\nalgorithms by Fuentes-Sepulveda et al. We experimentally show on a 40-core\nmachine with two-way hyper-threading that we outperform the existing parallel\nalgorithms by 1.3--5.6x and achieve up to 27x speedup over the sequential\nalgorithm on a variety of real-world and artificial inputs. Our algorithms show\ngood scalability with increasing thread count, input size and alphabet size. We\nalso discuss extensions to variants of the standard wavelet tree.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 17:55:42 GMT"}, {"version": "v2", "created": "Tue, 19 Aug 2014 03:32:05 GMT"}, {"version": "v3", "created": "Sun, 24 Aug 2014 02:36:08 GMT"}, {"version": "v4", "created": "Tue, 13 Jan 2015 21:00:53 GMT"}, {"version": "v5", "created": "Fri, 13 Mar 2015 15:02:26 GMT"}, {"version": "v6", "created": "Tue, 17 Mar 2015 07:21:11 GMT"}, {"version": "v7", "created": "Wed, 1 Apr 2015 06:28:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Shun", "Julian", ""]]}, {"id": "1407.8295", "submitter": "Dennis Schieferdecker", "authors": "Moritz Kobitzsch, Samitha Samaranayake, Dennis Schieferdecker", "title": "Pruning Techniques for the Stochastic on-time Arrival\n  Problem\\texorpdfstring -- An Experimental Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing shortest paths is one of the most researched topics in algorithm\nengineering. Currently available algorithms compute shortest paths in mere\nfractions of a second on continental sized road networks. In the presence of\nunreliability, however, current algorithms fail to achieve results as\nimpressive as for the static setting. In contrast to speed-up techniques for\nstatic route planning, current implementations for the stochastic on-time\narrival problem require the computationally expensive step of solving\nconvolution products. Running times can reach hours when considering large\nscale networks. We present a novel approach to reduce this immense\ncomputational effort of stochastic routing based on existing techniques for\nalternative routes. In an extensive experimental study, we show that the\nprocess of stochastic route planning can be speed-up immensely, without\nsacrificing much in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 07:09:27 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Kobitzsch", "Moritz", ""], ["Samaranayake", "Samitha", ""], ["Schieferdecker", "Dennis", ""]]}, {"id": "1407.8315", "submitter": "Chun-Shien Lu", "authors": "Sung-Hsien Hsieh, Chun-Shien Lu, Soo-Chang Pei", "title": "Sparse Fast Fourier Transform for Exactly and Generally K-Sparse Signals\n  by Downsampling and Sparse Recovery", "comments": "31 pages, 6 figures, the preliminary version was published in ICASSP\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Fourier Transform (FFT) is one of the most important tools in digital\nsignal processing. FFT costs O(N \\log N) for transforming a signal of length N.\nRecently, Sparse Fourier Transform (SFT) has emerged as a critical issue\naddressing how to compute a compressed Fourier transform of a signal with\ncomplexity being related to the sparsity of its spectrum. In this paper, a new\nSFT algorithm is proposed for both exactly K-sparse signals (with K non-zero\nfrequencies) and generally K-sparse signals (with K significant frequencies),\nwith the assumption that the distribution of the non-zero frequencies is\nuniform. The nuclear idea is to downsample the input signal at the beginning;\nthen, subsequent processing operates under downsampled signals, where signal\nlengths are proportional to O(K). Downsampling, however, possibly leads to\n\"aliasing.\" By the shift property of DFT, we recast the aliasing problem as\ncomplex Bose-Chaudhuri-Hocquenghem (BCH) codes solved by syndrome decoding. The\nproposed SFT algorithm for exactly K-sparse signals recovers 1-\\tau frequencies\nwith computational complexity O(K \\log K) and probability at least\n1-O(\\frac{c}{\\tau})^{\\tau K} under K=O(N), where c is a user-controlled\nparameter.\n  For generally K-sparse signals, due to the fact that BCH codes are sensitive\nto noise, we combine a part of syndrome decoding with a compressive\nsensing-based solver for obtaining $K$ significant frequencies. The\ncomputational complexity of our algorithm is \\max \\left( O(K \\log K), O(N)\n\\right), where the Big-O constant of O(N) is very small and only a simple\noperation involves O(N). Our simulations reveal that O(N) does not dominate the\ncomputational cost of sFFT-DT.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 08:45:19 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 04:57:04 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Hsieh", "Sung-Hsien", ""], ["Lu", "Chun-Shien", ""], ["Pei", "Soo-Chang", ""]]}, {"id": "1407.8373", "submitter": "Mathias Weller", "authors": "Mathias Weller", "title": "Optimal Hub Labeling is NP-complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance labeling is a preprocessing technique introduced by Peleg [Journal\nof Graph Theory, 33(3)] to speed up distance queries in large networks. Herein,\neach vertex receives a (short) label and, the distance between two vertices can\nbe inferred from their two labels. One such preprocessing problem occurs in the\nhub labeling algorithm [Abraham et al., SODA'10]: the label of a vertex v is a\nset of vertices x (the \"hubs\") with their distance d(x,v) to v and the distance\nbetween any two vertices u and v is the sum of their distances to a common hub.\nThe problem of assigning as few such hubs as possible was conjectured to be\nNP-hard, but no proof was known to date. We give a reduction from the\nwell-known Vertex Cover problem on graphs to prove that finding an optimal hub\nlabeling is indeed NP-hard.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 12:13:46 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Weller", "Mathias", ""]]}, {"id": "1407.8474", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay, Aritra Banik, Sandip Das, Hirak Sarkar", "title": "Voronoi Game on Graphs", "comments": "Journal preprint version, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textit{Voronoi game} is a geometric model of competitive facility location\nproblem played between two players. Users are generally modeled as points\nuniformly distributed on a given underlying space. Each player chooses a set of\npoints in the underlying space to place their facilities. Each user avails\nservice from its nearest facility. Service zone of a facility consists of the\nset of users which are closer to it than any other facility. Payoff of each\nplayer is defined by the quantity of users served by all of its facilities. The\nobjective of each player is to maximize their respective payoff. In this paper\nwe consider the two players {\\it Voronoi game} where the underlying space is a\nroad network modeled by a graph. In this framework we consider the problem of\nfinding $k$ optimal facility locations of Player 2 given any placement of $m$\nfacilities by Player 1. Our main result is a dynamic programming based\npolynomial time algorithm for this problem on tree network. On the other hand,\nwe show that the problem is strongly $\\mathcal{NP}$-complete for graphs. This\nproves that finding a winning strategy of P2 is $\\mathcal{NP}$-complete.\nConsequently, we design an $1-\\frac{1}{e}$ factor approximation algorithm,\nwhere $e \\approx 2.718$.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 16:14:07 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Banik", "Aritra", ""], ["Das", "Sandip", ""], ["Sarkar", "Hirak", ""]]}]