[{"id": "2005.00010", "submitter": "Gautam Kamath", "authors": "Gautam Kamath, Jonathan Ullman", "title": "A Primer on Private Statistics", "comments": "20 pages. Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially private statistical estimation has seen a flurry of\ndevelopments over the last several years. Study has been divided into two\nschools of thought, focusing on empirical statistics versus population\nstatistics. We suggest that these two lines of work are more similar than\ndifferent by giving examples of methods that were initially framed for\nempirical statistics, but can be applied just as well to population statistics.\nWe also provide a thorough coverage of recent work in this area.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:00:00 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Kamath", "Gautam", ""], ["Ullman", "Jonathan", ""]]}, {"id": "2005.00134", "submitter": "Vaishali Surianarayanan", "authors": "Daniel Lokshtanov, Saket Saurabh, Vaishali Surianarayanan", "title": "A Parameterized Approximation Scheme for Min $k$-Cut", "comments": "32 pages, 5 figures, to appear in FOCS '20. Typos from previous\n  version fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Min $k$-Cut problem, input is an edge weighted graph $G$ and an\ninteger $k$, and the task is to partition the vertex set into $k$ non-empty\nsets, such that the total weight of the edges with endpoints in different parts\nis minimized. When $k$ is part of the input, the problem is NP-complete and\nhard to approximate within any factor less than $2$. Recently, the problem has\nreceived significant attention from the perspective of parameterized\napproximation. Gupta et al.~[SODA 2018] initiated the study of\nFPT-approximation for the Min $k$-Cut problem and gave an\n$1.9997$-approximation algorithm running in time\n$2^{\\mathcal{O}(k^6)}n^{\\mathcal{O}(1)}$. Later, the same set of authors~[FOCS\n2018] designed an $(1 +\\epsilon)$-approximation algorithm that runs in time\n$(k/\\epsilon)^{\\mathcal{O}(k)}n^{k+\\mathcal{O}(1)}$, and a $1.81$-approximation\nalgorithm running in time $2^{\\mathcal{O}(k^2)}n^{\\mathcal{O}(1)}$. More,\nrecently, Kawarabayashi and Lin~[SODA 2020] gave a $(5/3 +\n\\epsilon)$-approximation for Min $k$-Cut running in time $2^{\\mathcal{O}(k^2\n\\log k)}n^{\\mathcal{O}(1)}$.\n  In this paper we give a parameterized approximation algorithm with best\npossible approximation guarantee, and best possible running time dependence on\nsaid guarantee (up to Exponential Time Hypothesis (ETH) and constants in the\nexponent). In particular, for every $\\epsilon > 0$, the algorithm obtains a $(1\n+\\epsilon)$-approximate solution in time\n$(k/\\epsilon)^{\\mathcal{O}(k)}n^{\\mathcal{O}(1)}$. The main ingredients of our\nalgorithm are: a simple sparsification procedure, a new polynomial time\nalgorithm for decomposing a graph into highly connected parts, and a new exact\nalgorithm with running time $s^{\\mathcal{O}(k)}n^{\\mathcal{O}(1)}$ on\nunweighted (multi-) graphs. Here, $s$ denotes the number of edges in a minimum\n$k$-cut. The latter two are of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 22:55:25 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 03:48:37 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 01:12:39 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Saurabh", "Saket", ""], ["Surianarayanan", "Vaishali", ""]]}, {"id": "2005.00144", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Dariusz Dereniowski, Aleksander {\\L}ukasiewicz, Przemys{\\l}aw\n  Uzna\\'nski", "title": "An Efficient Noisy Binary Search in Graphs via Median Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a generalization of the classical binary search problem in linearly\nsorted data to the graph-theoretic setting. The goal is to design an adaptive\nquery algorithm, called a strategy, that identifies an initially unknown target\nvertex in a graph by asking queries. Each query is conducted as follows: the\nstrategy selects a vertex $q$ and receives a reply $v$: if $q$ is the target,\nthen $v=q$, and if $q$ is not the target, then $v$ is a neighbor of $q$ that\nlies on a shortest path to the target. Furthermore, there is a noise parameter\n$0\\leq p<\\frac{1}{2}$, which means that each reply can be incorrect with\nprobability $p$. The optimization criterion to be minimized is the overall\nnumber of queries asked by the strategy, called the query complexity. The query\ncomplexity is well understood to be $O(\\varepsilon^{-2}\\log n)$ for general\ngraphs, where $n$ is the order of the graph and $\\varepsilon=\\frac{1}{2}-p$.\nHowever, implementing such a strategy is computationally expensive, with each\nquery requiring possibly $O(n^2)$ operations.\n  In this work we propose two efficient strategies that keep the optimal query\ncomplexity. The first strategy achieves the overall complexity of\n$O(\\varepsilon^{-1}n\\log n)$ per a single query. The second strategy is\ndedicated to graphs of small diameter $D$ and maximum degree $\\Delta$ and has\nthe average complexity of $O(n+\\varepsilon^{-2}D\\Delta\\log n)$ per query. We\nstress out that we develop an algorithmic tool of graph median approximation\nthat is of independent interest: the median can be efficiently approximated by\nfinding a vertex minimizing the sum of distances to a randomly sampled vertex\nsubset of size $O(\\varepsilon^{-2}\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 23:51:09 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Dereniowski", "Dariusz", ""], ["\u0141ukasiewicz", "Aleksander", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "2005.00168", "submitter": "Stefano Leucci", "authors": "Davide Bil\\`o, Luciano Gual\\`a, Stefano Leucci, Guido Proietti,\n  Giacomo Scornavacca", "title": "Cutting Bamboo Down to Size", "comments": "17 pages, 5 figures, FUN 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.FUN.2020.5", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of programming a robotic panda gardener to\nkeep a bamboo garden from obstructing the view of the lake by your house.\n  The garden consists of $n$ bamboo stalks with known daily growth rates and\nthe gardener can cut at most one bamboo per day. As a computer scientist, you\nfound out that this problem has already been formalized in [G\\k{a}sieniec et\nal., SOFSEM'17] as the Bamboo Garden Trimming (BGT) problem, where the goal is\nthat of computing a perpetual schedule (i.e., the sequence of bamboos to cut)\nfor the robotic gardener to follow in order to minimize the makespan, i.e., the\nmaximum height ever reached by a bamboo.\n  Two natural strategies are Reduce-Max and Reduce-Fastest(x). Reduce-Max trims\nthe tallest bamboo of the day, while Reduce-Fastest(x) trims the fastest\ngrowing bamboo among the ones that are taller than $x$. It is known that\nReduce-Max and Reduce-Fastest(x) achieve a makespan of $O(\\log n)$ and $4$ for\nthe best choice of $x=2$, respectively. We prove the first constant upper bound\nof $9$ for Reduce-Max and improve the one for Reduce-Fastest(x) to\n$\\frac{3+\\sqrt{5}}{2} < 2.62$ for $x=1+\\frac{1}{\\sqrt{5}}$.\n  Another critical aspect stems from the fact that your robotic gardener has a\nlimited amount of processing power and memory. It is then important for the\nalgorithm to be able to quickly determine the next bamboo to cut while\nrequiring at most linear space. We formalize this aspect as the problem of\ndesigning a Trimming Oracle data structure, and we provide three efficient\nTrimming Oracles implementing different perpetual schedules, including those\nproduced by Reduce-Max and Reduce-Fastest(x).\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 01:42:16 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Bil\u00f2", "Davide", ""], ["Gual\u00e0", "Luciano", ""], ["Leucci", "Stefano", ""], ["Proietti", "Guido", ""], ["Scornavacca", "Giacomo", ""]]}, {"id": "2005.00198", "submitter": "EPTCS", "authors": "Artjoms {\\v{S}}inkarovs (Heriot-Watt University)", "title": "Multi-dimensional Arrays with Levels", "comments": "In Proceedings MSFP 2020, arXiv:2004.14735", "journal-ref": "EPTCS 317, 2020, pp. 57-71", "doi": "10.4204/EPTCS.317.4", "report-no": null, "categories": "cs.DS cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a data structure that generalises rectangular multi-dimensional\narrays. The shape of an n-dimensional array is typically given by a tuple of n\nnatural numbers. Each element in that tuple defines the length of the\ncorresponding axis. If we treat this tuple as an array, the shape of that array\nis described by the single natural number n. A natural number itself can be\nalso treated as an array with the shape described by the natural number 1 (or\nthe element of any singleton set). This observation gives rise to the hierarchy\nof array types where the shape of an array of level l+1 is a level-l array of\nnatural numbers. Such a hierarchy occurs naturally when treating arrays as\ncontainers, which makes it possible to define both rank- and level-polymorphic\noperations. The former can be found in most array languages, whereas the latter\ngives rise to partial selections on a large set of hyperplanes, which is often\nuseful in practice. In this paper we present an Agda formalisation of arrays\nwith levels. We show that the proposed formalism supports standard\nrank-polymorphic array operations, while type system gives static guarantees\nthat indexing is within bounds. We generalise the notion of ranked operator so\nthat it becomes applicable on arrays of arbitrary levels and we show why this\nmay be useful in practice.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 03:42:41 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["{\u0160}inkarovs", "Artjoms", "", "Heriot-Watt University"]]}, {"id": "2005.00417", "submitter": "Aaron Bernstein", "authors": "Aaron Bernstein", "title": "Improved Bound for Matching in Random-Order Streams", "comments": "To Appear in ICALP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing an approximate maximum cardinality matching\nin the semi-streaming model when edges arrive in a \\emph{random} order. In the\nsemi-streaming model, the edges of the input graph G = (V,E) are given as a\nstream e_1, ..., e_m, and the algorithm is allowed to make a single pass over\nthis stream while using $O(n \\textrm{polylog}(n))$ space ($m = |E|$ and $n =\n|V|$). If the order of edges is adversarial, a simple single-pass greedy\nalgorithm yields a $1/2$-approximation in $O(n)$ space; achieving a better\napproximation in adversarial streams remains an elusive open question.\n  A line of recent work shows that one can improve upon the $1/2$-approximation\nif the edges of the stream arrive in a random order. The state of the art for\nthis model is two-fold: Assadi et al. [SODA 2019] show how to compute a\n$2/3(\\sim.66)$-approximate matching, but the space requirement is $O(n^{1.5}\n\\textrm{polylog}(n))$. Very recently, Farhadi et al. [SODA 2020] presented an\nalgorithm with the desired space usage of $O(n \\textrm{polylog}(n))$, but a\nworse approximation ratio of $6/11(\\sim.545)$, or $3/5(=.6)$ in bipartite\ngraphs.\n  In this paper, we present an algorithm that computes a\n$2/3(\\sim.66)$-approximate matching using only $O(n \\log(n))$ space, improving\nupon both results above. We also note that for adversarial streams, a lower\nbound of Kapralov [SODA 2013] shows that any algorithm that achieves a\n$1-1/e(\\sim.63)$-approximation requires $(n^{1+\\Omega(1/\\log\\log(n))})$ space.\nOur result for random-order streams is the first to go beyond the\nadversarial-order lower bound, thus establishing that computing a maximum\nmatching is provably easier in random-order streams.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:41:57 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Bernstein", "Aaron", ""]]}, {"id": "2005.00448", "submitter": "Nikolaos Tziavelis", "authors": "Nikolaos Tziavelis, Wolfgang Gatterbauer, Mirek Riedewald", "title": "Optimal Join Algorithms Meet Top-k", "comments": "To be published in Proceedings ofthe 2020 ACM SIGMOD International\n  Conference on Management of Data (SIGMOD'20), June 14-19, 2020, Portland, OR,\n  USA, 7 pages", "journal-ref": null, "doi": "10.1145/3318464.3383132", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k queries have been studied intensively in the database community and\nthey are an important means to reduce query cost when only the \"best\" or \"most\ninteresting\" results are needed instead of the full output. While some\noptimality results exist, e.g., the famous Threshold Algorithm, they hold only\nin a fairly limited model of computation that does not account for the cost\nincurred by large intermediate results and hence is not aligned with typical\ndatabase-optimizer cost models. On the other hand, the idea of avoiding large\nintermediate results is arguably the main goal of recent work on optimal join\nalgorithms, which uses the standard RAM model of computation to determine\nalgorithm complexity. This research has created a lot of excitement due to its\npromise of reducing the time complexity of join queries with cycles, but it has\nmostly focused on full-output computation. We argue that the two areas can and\nshould be studied from a unified point of view in order to achieve optimality\nin the common model of computation for a very general class of top-k-style join\nqueries. This tutorial has two main objectives. First, we will explore and\ncontrast the main assumptions, concepts, and algorithmic achievements of the\ntwo research areas. Second, we will cover recent, as well as some older,\napproaches that emerged at the intersection to support efficient ranked\nenumeration of join-query results. These are related to classic work on\nk-shortest path algorithms and more general optimization problems, some of\nwhich dates back to the 1950s. We demonstrate that this line of research\nwarrants renewed attention in the challenging context of ranked enumeration for\ngeneral join queries.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:33:23 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Tziavelis", "Nikolaos", ""], ["Gatterbauer", "Wolfgang", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2005.00515", "submitter": "Andreia P. Guerreiro", "authors": "Andreia P. Guerreiro, Carlos M. Fonseca, Lu\\'is Paquete", "title": "The Hypervolume Indicator: Problems and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hypervolume indicator is one of the most used set-quality indicators for\nthe assessment of stochastic multiobjective optimizers, as well as for\nselection in evolutionary multiobjective optimization algorithms. Its\ntheoretical properties justify its wide acceptance, particularly the strict\nmonotonicity with respect to set dominance which is still unique of\nhypervolume-based indicators. This paper discusses the computation of\nhypervolume-related problems, highlighting the relations between them,\nproviding an overview of the paradigms and techniques used, a description of\nthe main algorithms for each problem, and a rundown of the fastest algorithms\nregarding asymptotic complexity and runtime. By providing a complete overview\nof the computational problems associated to the hypervolume indicator, this\npaper serves as the starting point for the development of new algorithms, and\nsupports users in the identification of the most appropriate implementations\navailable for each problem.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:38:48 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Guerreiro", "Andreia P.", ""], ["Fonseca", "Carlos M.", ""], ["Paquete", "Lu\u00eds", ""]]}, {"id": "2005.00518", "submitter": "Sean Cleary", "authors": "Sean Cleary and Haris Nadeem", "title": "Distributions of restricted rotation distances", "comments": "17 pages, 11 figures, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation distances measure the differences in structure between rooted\nordered binary trees. The one-dimensional skeleta of associahedra are rotation\ngraphs, where two vertices representing trees are connected by an edge if they\ndiffer by a single rotation. There are no known efficient algorithms to compute\nrotation distance between trees and thus distances in rotation graphs. Limiting\nthe allowed locations of where rotations are permitted gives rise to a number\nof notions of restricted rotation distances. Allowing rotations at a minimal\nsuch set of locations gives restricted rotation distance. There are linear-time\nalgorithms to compute restricted rotation distance, where there are only two\npermitted locations for rotations to occur. The associated restricted rotation\ngraph has an efficient distance algorithm. There are linear upper and lower\nbounds on restricted rotation distance with respect to the sizes of the reduced\ntree pairs. Here, we experimentally investigate the expected restricted\nrotation distance between two trees selected at random of increasing size and\nfind that it lies typically in a narrow band well within the earlier proven\nlinear upper and lower bounds.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:45:48 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 21:47:12 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Cleary", "Sean", ""], ["Nadeem", "Haris", ""]]}, {"id": "2005.00575", "submitter": "Mathieu Mari", "authors": "Chien-chung Huang, Mathieu Mari, Claire Mathieu, Jens Vygen", "title": "Approximating maximum integral multiflows on bounded genus graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise the first constant-factor approximation algorithm for finding an\nintegral multi-commodity flow of maximum total value for instances where the\nsupply graph together with the demand edges can be embedded on an orientable\nsurface of bounded genus. This extends recent results for planar instances. Our\ntechniques include an uncrossing algorithm, which is significantly more\ndifficult than in the planar case, a partition of the cycles in the support of\nan LP solution into free homotopy classes, and a new rounding procedure for\nfreely homotopic non-separating cycles.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 19:10:31 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:06:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Huang", "Chien-chung", ""], ["Mari", "Mathieu", ""], ["Mathieu", "Claire", ""], ["Vygen", "Jens", ""]]}, {"id": "2005.00681", "submitter": "Shunsuke Inenaga", "authors": "Shunsuke Inenaga", "title": "Pointer-Machine Algorithms for Fully-Online Construction of Suffix Trees\n  and DAWGs on Multiple Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the problem of maintaining the suffix tree indexing structure\nfor a fully-online collection of multiple strings, where a new character can be\nprepended to any string in the collection at any time. The only previously\nknown algorithm for the problem, recently proposed by Takagi et al.\n[Algorithmica 82(5): 1346-1377 (2020)], runs in $O(N \\log \\sigma)$ time and\n$O(N)$ space on the word RAM model, where $N$ denotes the total length of the\nstrings and $\\sigma$ denotes the alphabet size. Their algorithm makes heavy use\nof the nearest marked ancestor (NMA) data structure on semi-dynamic trees, that\ncan answer queries and supports insertion of nodes in $O(1)$ amortized time on\nthe word RAM model. In this paper, we present a simpler fully-online\nright-to-left algorithm that builds the suffix tree for a given string\ncollection in $O(N (\\log \\sigma + \\log d))$ time and $O(N)$ space, where $d$ is\nthe maximum number of in-coming Weiner links to a node of the suffix tree. We\nnote that $d$ is bounded by the height of the suffix tree, which is further\nbounded by the length of the longest string in the collection. The advantage of\nthis new algorithm is that it works on the pointer machine model, namely, it\ndoes not use the complicated NMA data structures that involve table look-ups.\nAs a byproduct, we also obtain a pointer-machine algorithm for building the\ndirected acyclic word graph (DAWG) for a fully-online left-to-right collection\nof multiple strings, which runs in $O(N (\\log \\sigma + \\log d))$ time and\n$O(N)$ space again without the aid of the NMA data structures.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 02:24:03 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Inenaga", "Shunsuke", ""]]}, {"id": "2005.00690", "submitter": "Peter Gartland", "authors": "Peter Gartland and Daniel Lokshtanov", "title": "Independent Set on P$_k$-Free Graphs in Quasi-Polynomial Time", "comments": "17 pages long. No figures. Typos fixed and constants used in measure\n  analysis have been corrected from first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that takes as input a graph $G$ with weights on the\nvertices, and computes a maximum weight independent set $S$ of $G$. If the\ninput graph $G$ excludes a path $P_k$ on $k$ vertices as an induced subgraph,\nthe algorithm runs in time $n^{O(k^2 \\log^3 n)}$. Hence, for every fixed $k$\nour algorithm runs in quasi-polynomial time. This resolves in the affirmative\nan open problem of [Thomass\\'{e}, SODA'20 invited presentation]. Previous to\nthis work, polynomial time algorithms were only known for $P_4$-free graphs\n[Corneil et al., DAM'81], $P_5$-free graphs [Lokshtanov et al., SODA'14], and\n$P_6$-free graphs [Grzesik et al., SODA'19]. For larger values of $t$, only\n$2^{O(\\sqrt{kn\\log n})}$ time algorithms [Basc\\'{o} et al., Algorithmica'19]\nand quasi-polynomial time approximation schemes [Chudnovsky et al., SODA'20]\nwere known. Thus, our work is the first to offer conclusive evidence that\nIndependent Set on $P_k$-free graphs is not NP-complete for any integer $k$.\n  Additionally we show that for every graph $H$, if there exists a\nquasi-polynomial time algorithm for Independent Set on $C$-free graphs for\nevery connected component $C$ of $H$, then there also exists a quasi-polynomial\ntime algorithm for {\\sc Independent Set} on $H$-free graphs. This lifts our\nquasi-polynomial time algorithm to $T_k$-free graphs, where $T_k$ has one\ncomponent that is a $P_k$, and $k-1$ components isomorphic to a fork (the\nunique $5$-vertex tree with a degree $3$ vertex).\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 03:46:55 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 03:59:39 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Gartland", "Peter", ""], ["Lokshtanov", "Daniel", ""]]}, {"id": "2005.00853", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "Lower Bounds for Non-Elitist Evolutionary Algorithms via Negative\n  Multiplicative Drift", "comments": "Full version of a paper that appeared in the proceedings of PPSN\n  2020. Difference to the previous version: Added a section on standard bit\n  mutation with random mutation rate. Additional difference to the conference\n  version: All proofs are given since ArXiV does not have a page limit", "journal-ref": "Evolutionary Computation (2021) 29 (2): 305-329", "doi": "10.1162/evco_a_00283", "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A decent number of lower bounds for non-elitist population-based evolutionary\nalgorithms has been shown by now. Most of them are technically demanding due to\nthe (hard to avoid) use of negative drift theorems -- general results which\ntranslate an expected progress away from the target into a high hitting time.\n  We propose a simple negative drift theorem for multiplicative drift scenarios\nand show that it can simplify existing analyses. We discuss in more detail\nLehre's (PPSN 2010) \\emph{negative drift in populations} method, one of the\nmost general tools to prove lower bounds on the runtime of non-elitist\nmutation-based evolutionary algorithms for discrete search spaces. Together\nwith other arguments, we obtain an alternative and simpler proof, which also\nstrengthens and simplifies this method. In particular, now only three of the\nfive technical conditions of the previous result have to be verified. The lower\nbounds we obtain are explicit instead of only asymptotic. This allows to\ncompute concrete lower bounds for concrete algorithms, but also enables us to\nshow that super-polynomial runtimes appear already when the reproduction rate\nis only a $(1 - \\omega(n^{-1/2}))$ factor below the threshold. For the special\ncase of algorithms using standard bit mutation with a random mutation rate\n(called uniform mixing in the language of hyper-heuristics), we prove the\nresult stated by Dang and Lehre (PPSN 2016) and extend it to mutation rates\nother than $\\Theta(1/n)$, which includes the heavy-tailed mutation operator\nproposed by Doerr, Le, Makhmara, and Nguyen (GECCO 2017). We finally apply our\nmethod and a novel domination argument to show an exponential lower bound for\nthe runtime of the mutation-only simple genetic algorithm on \\onemax for\narbitrary population size.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 15:10:09 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 14:18:07 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 16:17:34 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 09:04:46 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "2005.00875", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Yoann Dieudonn\\'e, Andrzej Pelc, Franck Petit", "title": "Deterministic Treasure Hunt in the Plane with Angular Hints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile agent equipped with a compass and a measure of length has to find an\ninert treasure in the Euclidean plane. Both the agent and the treasure are\nmodeled as points. In the beginning, the agent is at a distance at most $D>0$\nfrom the treasure, but knows neither the distance nor any bound on it. Finding\nthe treasure means getting at distance at most 1 from it. The agent makes a\nseries of moves. Each of them consists in moving straight in a chosen direction\nat a chosen distance. In the beginning and after each move the agent gets a\nhint consisting of a positive angle smaller than $2\\pi$ whose vertex is at the\ncurrent position of the agent and within which the treasure is contained. We\ninvestigate the problem of how these hints permit the agent to lower the cost\nof finding the treasure, using a deterministic algorithm, where the cost is the\nworst-case total length of the agent's trajectory. It is well known that\nwithout any hint the optimal (worst case) cost is $\\Theta(D^2)$. We show that\nif all angles given as hints are at most $\\pi$, then the cost can be lowered to\n$O(D)$, which is optimal. If all angles are at most $\\beta$, where $\\beta<2\\pi$\nis a constant unknown to the agent, then the cost is at most\n$O(D^{2-\\epsilon})$, for some $\\epsilon>0$. For both these positive results we\npresent deterministic algorithms achieving the above costs. Finally, if angles\ngiven as hints can be arbitrary, smaller than $2\\pi$, then we show that cost\n$\\Theta(D^2)$ cannot be beaten.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 16:12:36 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Pelc", "Andrzej", ""], ["Petit", "Franck", ""]]}, {"id": "2005.00880", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Yoann Dieudonn\\'e, Andrzej Pelc, Franck Petit", "title": "Almost Universal Anonymous Rendezvous in the Plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two mobile agents represented by points freely moving in the plane and\nstarting at two distinct positions, have to meet. The meeting, called\nrendezvous, occurs when agents are at distance at most $r$ of each other and\nnever move after this time, where $r$ is a positive real unknown to them,\ncalled the visibility radius. Agents are anonymous and execute the same\ndeterministic algorithm. Each agent has a set of private attributes, some or\nall of which can differ between agents. These attributes are: the initial\nposition of the agent, its system of coordinates (orientation and chirality),\nthe rate of its clock, its speed when it moves, and the time of its wake-up. If\nall attributes (except the initial positions) are identical and agents start at\ndistance larger than $r$ then they can never meet. However, differences between\nattributes make it sometimes possible to break the symmetry and accomplish\nrendezvous. Such instances of the rendezvous problem (formalized as lists of\nattributes), are called feasible.\n  Our contribution is three-fold. We first give an exact characterization of\nfeasible instances. Thus it is natural to ask whether there exists a single\nalgorithm that guarantees rendezvous for all these instances. We give a strong\nnegative answer to this question: we show two sets $S_1$ and $S_2$ of feasible\ninstances such that none of them admits a single rendezvous algorithm valid for\nall instances of the set. On the other hand, we construct a single algorithm\nthat guarantees rendezvous for all feasible instances outside of sets $S_1$ and\n$S_2$. We observe that these exception sets $S_1$ and $S_2$ are geometrically\nvery small, compared to the set of all feasible instances: they are included in\nlow-dimension subspaces of the latter. Thus, our rendezvous algorithm handling\nall feasible instances other than these small sets of exceptions can be justly\ncalled almost universal.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 16:30:14 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Pelc", "Andrzej", ""], ["Petit", "Franck", ""]]}, {"id": "2005.00947", "submitter": "Rui Sun", "authors": "David Simchi-Levi, Rui Sun, Huanan Zhang", "title": "Online Learning and Optimization for Revenue Management Problems with\n  Add-on Discounts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper a revenue management problem with add-on discounts.\nThe problem is motivated by the practice in the video game industry, where a\nretailer offers discounts on selected supportive products (e.g. video games) to\ncustomers who have also purchased the core products (e.g. video game consoles).\nWe formulate this problem as an optimization problem to determine the prices of\ndifferent products and the selection of products with add-on discounts. To\novercome the computational challenge of this optimization problem, we propose\nan efficient FPTAS algorithm that can solve the problem approximately to any\ndesired accuracy. Moreover, we consider the revenue management problem in the\nsetting where the retailer has no prior knowledge of the demand functions of\ndifferent products. To resolve this problem, we propose a UCB-based learning\nalgorithm that uses the FPTAS optimization algorithm as a subroutine. We show\nthat our learning algorithm can converge to the optimal algorithm that has\naccess to the true demand functions, and we prove that the convergence rate is\ntight up to a certain logarithmic term. In addition, we conduct numerical\nexperiments with the real-world transaction data we collect from a popular\nvideo gaming brand's online store on Tmall.com. The experiment results\nillustrate our learning algorithm's robust performance and fast convergence in\nvarious scenarios. We also compare our algorithm with the optimal policy that\ndoes not use any add-on discount, and the results show the advantages of using\nthe add-on discount strategy in practice.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:54:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Simchi-Levi", "David", ""], ["Sun", "Rui", ""], ["Zhang", "Huanan", ""]]}, {"id": "2005.01098", "submitter": "Ioana O. Bercea", "authors": "Ioana Oriana Bercea and Guy Even", "title": "A Dynamic Space-Efficient Filter with Constant Time Operations", "comments": "To be published in SWAT 2020. Earlier version of the paper available\n  at arXiv:1911.05060", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic dictionary is a data structure that maintains sets of cardinality\nat most $n$ from a given universe and supports insertions, deletions, and\nmembership queries. A filter approximates membership queries with a one-sided\nerror that occurs with probability at most $\\epsilon$. The goal is to obtain\ndynamic filters that are space-efficient (the space is $1+o(1)$ times the\ninformation-theoretic lower bound) and support all operations in constant time\nwith high probability. One approach to designing filters is to reduce to the\nretrieval problem. When the size of the universe is polynomial in $n$, this\napproach yields a space-efficient dynamic filter as long as the error parameter\n$\\epsilon$ satisfies $\\log(1/\\epsilon) = \\omega(\\log\\log n)$.\n  For the case that $\\log(1/\\epsilon) = O(\\log\\log n)$, we present the first\nspace-efficient dynamic filter with constant time operations in the worst case\n(whp). In contrast, the space-efficient dynamic filter of Pagh, Pagh, Rao (SODA\n2005) supports insertions and deletions in amortized expected constant time.\nOur approach employs the classic reduction of Carter et al. (STOC 1978) on a\nnew type of dictionary construction that supports random multisets.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 14:29:35 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 13:21:42 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bercea", "Ioana Oriana", ""], ["Even", "Guy", ""]]}, {"id": "2005.01112", "submitter": "Maria Kosche", "authors": "Pawel Gawrychowski, Maria Kosche, Tore Koss, Florin Manea, Stefan\n  Siemer", "title": "Efficiently Testing Simon's Congruence", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2021.34", "report-no": null, "categories": "cs.FL cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simon's congruence $\\sim_k$ is defined as follows: two words are\n$\\sim_k$-equivalent if they have the same set of subsequences of length at most\n$k$. We propose an algorithm which computes, given two words $s$ and $t$, the\nlargest $k$ for which $s\\sim_k t$. Our algorithm runs in linear time\n$O(|s|+|t|)$ when the input words are over the integer alphabet\n$\\{1,\\ldots,|s|+|t|\\}$ (or other alphabets which can be sorted in linear time).\nThis approach leads to an optimal algorithm in the case of general alphabets as\nwell. Our results are based on a novel combinatorial approach and a series of\nefficient data structures.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 15:15:47 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 16:31:53 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Kosche", "Maria", ""], ["Koss", "Tore", ""], ["Manea", "Florin", ""], ["Siemer", "Stefan", ""]]}, {"id": "2005.01182", "submitter": "Yihe Dong", "authors": "Yihe Dong, Yu Gao, Richard Peng, Ilya Razenshteyn, Saurabh Sawlani", "title": "A Study of Performance of Optimal Transport", "comments": null, "journal-ref": "Optimal Transport & Machine learning Workshop at NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of efficiently computing optimal transport (OT)\ndistances, which is equivalent to the node-capacitated minimum cost maximum\nflow problem in a bipartite graph. We compare runtimes in computing OT\ndistances on data from several domains, such as synthetic data of geometric\nshapes, embeddings of tokens in documents, and pixels in images. We show that\nin practice, combinatorial methods such as network simplex and augmenting path\nbased algorithms can consistently outperform numerical matrix-scaling based\nmethods such as Sinkhorn [Cuturi'13] and Greenkhorn [Altschuler et al'17], even\nin low accuracy regimes, with up to orders of magnitude speedups. Lastly, we\npresent a new combinatorial algorithm that improves upon the classical\nKuhn-Munkres algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 20:37:05 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Dong", "Yihe", ""], ["Gao", "Yu", ""], ["Peng", "Richard", ""], ["Razenshteyn", "Ilya", ""], ["Sawlani", "Saurabh", ""]]}, {"id": "2005.01242", "submitter": "Konrad Anand", "authors": "Konrad Anand, Luc Devroye", "title": "Probabilistic Analysis of RRT Trees", "comments": "29 pages, 10 figures, submitted to The International Journal of\n  Robotics Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis presents analysis of the properties and run-time of the\nRapidly-exploring Random Tree (RRT) algorithm. It is shown that the time for\nthe RRT with stepsize $\\epsilon$ to grow close to every point in the\n$d$-dimensional unit cube is $\\Theta\\left(\\frac1{\\epsilon^d} \\log\n\\left(\\frac1\\epsilon\\right)\\right)$. Also, the time it takes for the tree to\nreach a region of positive probability is $O\\left(\\epsilon^{-\\frac32}\\right)$.\nFinally, a relationship is shown to the Nearest Neighbour Tree (NNT). This\nrelationship shows that the total Euclidean path length after $n$ steps is\n$O(\\sqrt n)$ and the expected height of the tree is bounded above by $(e +\no(1)) \\log n$.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 03:02:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Anand", "Konrad", ""], ["Devroye", "Luc", ""]]}, {"id": "2005.01299", "submitter": "Zhigang Jia", "authors": "Zhigang Jia, Xuan Liu and Mei-Xiang Zhao", "title": "The Multi-Symplectic Lanczos Algorithm and Its Applications to Color\n  Image Processing", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank approximations of original samples are playing more and more an\nimportant role in many recently proposed mathematical models from data science.\nA natural and initial requirement is that these representations inherit\noriginal structures or properties. With this aim, we propose a new\nmulti-symplectic method based on the Lanzcos bidiagonalization to compute the\npartial singular triplets of JRS-symmetric matrices. These singular triplets\ncan be used to reconstruct optimal low-rank approximations while preserving the\nintrinsic multi-symmetry. The augmented Ritz and harmonic Ritz vectors are used\nto perform implicit restarting to obtain a satisfactory bidiagonal matrix for\ncalculating the $k$ largest or smallest singular triplets, respectively. We\nalso apply the new multi-symplectic Lanczos algorithms to color face\nrecognition and color video compressing and reconstruction. Numerical\nexperiments indicate their superiority over the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 07:13:14 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Jia", "Zhigang", ""], ["Liu", "Xuan", ""], ["Zhao", "Mei-Xiang", ""]]}, {"id": "2005.01359", "submitter": "Rian Neogi", "authors": "Rian Neogi, M. S. Ramanujan, Saket Saurabh, Roohani Sharma", "title": "On the Parameterized Complexity of Deletion to $\\mathcal{H}$-free Strong\n  Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\sc Directed Feedback Vertex Set (DFVS)} is a fundamental computational\nproblem that has received extensive attention in parameterized complexity. In\nthis paper, we initiate the study of a wide generalization, the {\\sc ${\\cal\nH}$-free SCC Deletion} problem. Here, one is given a digraph $D$, an integer\n$k$ and the objective is to decide whether there is a vertex set of size at\nmost $k$ whose deletion leaves a digraph where every strong component excludes\ngraphs in the fixed finite family ${\\cal H}$ as (not necessarily induced)\nsubgraphs. When ${\\cal H}$ comprises only the digraph with a single arc, then\nthis problem is precisely DFVS.\n  Our main result is a proof that this problem is fixed-parameter tractable\nparameterized by the size of the deletion set if ${\\cal H}$ only contains\nrooted graphs or if ${\\cal H}$ contains at least one directed path. Along with\ngeneralizing the fixed-parameter tractability result for DFVS, our result also\ngeneralizes the recent results of G\\\"{o}ke et al. [CIAC 2019] for the {\\sc\n1-Out-Regular Vertex Deletion} and {\\sc Bounded Size Strong Component Vertex\nDeletion} problems. Moreover, we design algorithms for the two above mentioned\nproblems, whose running times are better and match with the best bounds for\n{\\sc DFVS}, without using the heavy machinery of shadow removal as is done by\nG\\\"{o}ke et al. [CIAC 2019].\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:04:01 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 11:22:25 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Neogi", "Rian", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""], ["Sharma", "Roohani", ""]]}, {"id": "2005.01378", "submitter": "Yu Cheng", "authors": "Yu Cheng, Ilias Diakonikolas, Rong Ge, Mahdi Soltanolkotabi", "title": "High-Dimensional Robust Mean Estimation via Gradient Descent", "comments": "Under submission to ICML'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of high-dimensional robust mean estimation in the\npresence of a constant fraction of adversarial outliers. A recent line of work\nhas provided sophisticated polynomial-time algorithms for this problem with\ndimension-independent error guarantees for a range of natural distribution\nfamilies.\n  In this work, we show that a natural non-convex formulation of the problem\ncan be solved directly by gradient descent. Our approach leverages a novel\nstructural lemma, roughly showing that any approximate stationary point of our\nnon-convex objective gives a near-optimal solution to the underlying robust\nestimation task. Our work establishes an intriguing connection between\nalgorithmic high-dimensional robust statistics and non-convex optimization,\nwhich may have broader applications to other robust estimation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:48:04 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Cheng", "Yu", ""], ["Diakonikolas", "Ilias", ""], ["Ge", "Rong", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "2005.01460", "submitter": "Ignasi Sau", "authors": "Paloma T. Lima, Vinicius F. dos Santos, Ignasi Sau, U\\'everton S.\n  Souza", "title": "Reducing graph transversals via edge contractions", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph invariant $\\pi$, the Contraction($\\pi$) problem consists in,\ngiven a graph $G$ and two positive integers $k,d$, deciding whether one can\ncontract at most $k$ edges of $G$ to obtain a graph in which $\\pi$ has dropped\nby at least $d$. Galby et al. [ISAAC 2019, MFCS 2019] recently studied the case\nwhere $\\pi$ is the size of a minimum dominating set. We focus on graph\ninvariants defined as the minimum size of a vertex set that hits all the\noccurrences of graphs in a collection ${\\cal H}$ according to a fixed\ncontainment relation. We prove co-NP-hardness results under some assumptions on\nthe graphs in ${\\cal H}$, which in particular imply that Contraction($\\pi$) is\nco-NP-hard even for fixed $k=d=1$ when $\\pi$ is the size of a minimum feedback\nvertex set or an odd cycle transversal. In sharp contrast, we show that when\n$\\pi$ is the size of a minimum vertex cover, the problem is in XP parameterized\nby $d$.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:18:35 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 09:59:22 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lima", "Paloma T.", ""], ["Santos", "Vinicius F. dos", ""], ["Sau", "Ignasi", ""], ["Souza", "U\u00e9verton S.", ""]]}, {"id": "2005.01586", "submitter": "Ma{\\l}gorzata Sulkowska", "authors": "Krzysztof Grining, Marek Klonowski, Ma{\\l}gorzata Sulkowska", "title": "What Do Our Choices Say About Our Preferences?", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking online decisions is a part of everyday life. Think of buying a house,\nparking a car or taking part in an auction. We often take those decisions\npublicly, which may breach our privacy - a party observing our choices may\nlearn a lot about our preferences. In this paper we investigate the online\nstopping algorithms from the privacy preserving perspective, using a\nmathematically rigorous differential privacy notion.\n  In differentially private algorithms there is usually an issue of balancing\nthe privacy and utility. In this regime, in most cases, having both optimality\nand high level of privacy at the same time is impossible. We propose a natural\nmechanism to achieve a controllable trade-off, quantified by a parameter,\nbetween the accuracy of the online algorithm and its privacy. Depending on the\nparameter, our mechanism can be optimal with weaker differential privacy or\nsuboptimal, yet more privacy-preserving. We conduct a detailed accuracy and\nprivacy analysis of our mechanism applied to the optimal algorithm for the\nclassical secretary problem. Thereby the classical notions from two distinct\nareas - optimal stopping and differential privacy - meet for the first time.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:57:12 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 15:39:29 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Grining", "Krzysztof", ""], ["Klonowski", "Marek", ""], ["Sulkowska", "Ma\u0142gorzata", ""]]}, {"id": "2005.01757", "submitter": "Lee Cohen", "authors": "Eliran Shabat, Lee Cohen and Yishay Mansour", "title": "Sample Complexity of Uniform Convergence for Multicalibration", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in societal concerns in machine learning systems,\nespecially in fairness. Multicalibration gives a comprehensive methodology to\naddress group fairness. In this work, we address the multicalibration error and\ndecouple it from the prediction error. The importance of decoupling the\nfairness metric (multicalibration) and the accuracy (prediction error) is due\nto the inherent trade-off between the two, and the societal decision regarding\nthe \"right tradeoff\" (as imposed many times by regulators). Our work gives\nsample complexity bounds for uniform convergence guarantees of multicalibration\nerror, which implies that regardless of the accuracy, we can guarantee that the\nempirical and (true) multicalibration errors are close. We emphasize that our\nresults: (1) are more general than previous bounds, as they apply to both\nagnostic and realizable settings, and do not rely on a specific type of\nalgorithm (such as deferentially private), (2) improve over previous\nmulticalibration sample complexity bounds and (3) implies uniform convergence\nguarantees for the classical calibration error.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 18:01:38 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 15:28:21 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shabat", "Eliran", ""], ["Cohen", "Lee", ""], ["Mansour", "Yishay", ""]]}, {"id": "2005.01778", "submitter": "Mathias Soeken", "authors": "Mathias Soeken", "title": "Determining the Multiplicative Complexity of Boolean Functions using SAT", "comments": "8 pages, 2 tables, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a constructive SAT-based algorithm to determine the multiplicative\ncomplexity of a Boolean function, i.e., the smallest number of AND gates in any\nlogic network that consists of 2-input AND gates, 2-input XOR gates, and\ninverters. In order to speed-up solving time, we make use of several symmetry\nbreaking constraints; these exploit properties of XAGs that may be useful\nbeyond the proposed SAT-based algorithm. We further propose a heuristic\npost-optimization algorithm to reduce the number of XOR gates once the optimum\nnumber of AND gates has been obtained, which also makes use of SAT solvers. Our\nalgorithm is capable to find all optimum XAGs for representatives of all\n5-input affine-equivalent classes, and for a set of frequently occurring\n6-input functions.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 18:29:48 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Soeken", "Mathias", ""]]}, {"id": "2005.01824", "submitter": "Pawe{\\l} Rz\\k{a}\\.zewski", "authors": "Maria Chudnovsky, Shenwei Huang, Pawe{\\l} Rz\\k{a}\\.zewski, Sophie\n  Spirkl, Mingxian Zhong", "title": "Complexity of $C_k$-coloring in hereditary classes of graphs", "comments": "The extended abstract of the paper was presented on ESA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $F$, a graph $G$ is \\emph{$F$-free} if it does not contain an\ninduced subgraph isomorphic to $F$. For two graphs $G$ and $H$, an\n\\emph{$H$-coloring} of $G$ is a mapping $f:V(G)\\rightarrow V(H)$ such that for\nevery edge $uv\\in E(G)$ it holds that $f(u)f(v)\\in E(H)$. We are interested in\nthe complexity of the problem $H$-{\\sc Coloring}, which asks for the existence\nof an $H$-coloring of an input graph $G$. In particular, we consider $H$-{\\sc\nColoring} of $F$-free graphs, where $F$ is a fixed graph and $H$ is an odd\ncycle of length at least 5. This problem is closely related to the well known\nopen problem of determining the complexity of 3-{\\sc Coloring} of $P_t$-free\ngraphs.\n  We show that for every odd $k \\geq 5$ the $C_k$-{\\sc Coloring} problem, even\nin the list variant, can be solved in polynomial time in $P_9$-free graphs. The\nalgorithm extends for the case of list version of $C_k$-{\\sc Coloring}, where\n$k$ is an even number of length at least 10.\n  On the other hand, we prove that if some component of $F$ is not a subgraph\nof a subdividecd claw, then the following problems are NP-complete in $F$-free\ngraphs: a)extension version of $C_k$-{\\sc Coloring} for every odd $k \\geq 5$,\nb) list version of $C_k$-{\\sc Coloring} for every even $k \\geq 6$.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 20:08:50 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Chudnovsky", "Maria", ""], ["Huang", "Shenwei", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""], ["Spirkl", "Sophie", ""], ["Zhong", "Mingxian", ""]]}, {"id": "2005.01861", "submitter": "Pan Peng", "authors": "Hendrik Fichtenberger, Mingze Gao, Pan Peng", "title": "Sampling Arbitrary Subgraphs Exactly Uniformly in Sublinear Time", "comments": "ICALP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple sublinear-time algorithm for sampling an arbitrary\nsubgraph $H$ \\emph{exactly uniformly} from a graph $G$ with $m$ edges, to which\nthe algorithm has access by performing the following types of queries: (1)\ndegree queries, (2) neighbor queries, (3) pair queries and (4) edge sampling\nqueries. The query complexity and running time of our algorithm are\n$\\tilde{O}(\\min\\{m, \\frac{m^{\\rho(H)}}{\\# H}\\})$ and\n$\\tilde{O}(\\frac{m^{\\rho(H)}}{\\# H})$, respectively, where $\\rho(H)$ is the\nfractional edge-cover of $H$ and $\\# H$ is the number of copies of $H$ in $G$.\nFor any clique on $r$ vertices, i.e., $H=K_r$, our algorithm is almost optimal\nas any algorithm that samples an $H$ from any distribution that has $\\Omega(1)$\ntotal probability mass on the set of all copies of $H$ must perform\n$\\Omega(\\min\\{m, \\frac{m^{\\rho(H)}}{\\# H\\cdot (cr)^r}\\})$ queries.\n  Together with the query and time complexities of the $(1\\pm\n\\varepsilon)$-approximation algorithm for the number of subgraphs $H$ by\nAssadi, Kapralov and Khanna [ITCS 2018] and the lower bound by Eden and\nRosenbaum [APPROX 2018] for approximately counting cliques, our results suggest\nthat in our query model, approximately counting cliques is \"equivalent to\"\nexactly uniformly sampling cliques, in the sense that the query and time\ncomplexities of exactly uniform sampling and randomized approximate counting\nare within a polylogarithmic factor of each other. This stands in interesting\ncontrast to an analogous relation between approximate counting and almost\nuniformly sampling for self-reducible problems in the polynomial-time regime by\nJerrum, Valiant and Vazirani [TCS 1986].\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:44:42 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 13:45:38 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 21:58:59 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Fichtenberger", "Hendrik", ""], ["Gao", "Mingze", ""], ["Peng", "Pan", ""]]}, {"id": "2005.01867", "submitter": "Fabian Frei", "authors": "Hans-Joachim B\\\"ockenhauer and Jan Dreier and Fabian Frei and Peter\n  Rossmanith", "title": "Advice for Online Knapsack With Removable Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the proportional knapsack problem, we are given a knapsack of some\ncapacity and a set of variably sized items. The goal is to pack some of these\nitems such that they fill the knapsack as much as possible without ever\nexceeding the capacity. The online version of this problem reveals the items\nand their sizes not all at once but one by one. For each item, the algorithm\nhas to decide immediately whether to pack it or not. We consider a natural\nvariant of this online knapsack problem, which has been coined removable\nknapsack and we denote by RemKnap. It differs from the classical variant by\nallowing the removal of any packed item from the knapsack. Repacking is\nimpossible, however: Once an item is removed, it is gone for good.\n  We analyze the advice complexity of this problem. It measures how many advice\nbits an omniscient oracle needs to provide for an online algorithm to reach any\ngiven competitive ratio, which is--understood in its strict sense--just the\nalgorithm's approximation factor. The online knapsack problem without\nremovability is known for its peculiar advice behavior involving three jumps in\ncompetitivity. We show that the advice complexity of RemKnap is quite different\nbut just as interesting. The competitivity starts from the golden ratio when no\nadvice is given. It then drops down in small increments to (1 + epsilon) for a\nconstant amount of advice already, which requires logarithmic advice in the\nclassical version. Removability comes as no relief to the perfectionist,\nhowever: Optimality still requires one full advice bit for every single item in\nthe instance as before.\n  These results are particularly noteworthy from a structural viewpoint for the\nexceptionally slow transition from near-optimality to optimality; such a steep\njump up from constant to full linear advice for just an infinitesimally small\nimprovement is unique among the online problems examined so far.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 22:00:11 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["B\u00f6ckenhauer", "Hans-Joachim", ""], ["Dreier", "Jan", ""], ["Frei", "Fabian", ""], ["Rossmanith", "Peter", ""]]}, {"id": "2005.01921", "submitter": "Heather Guarnera", "authors": "Feodor F. Dragan and Heather M. Guarnera", "title": "Helly-gap of a graph and vertex eccentricities", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new metric parameter for a graph, Helly-gap, is introduced. A graph $G$ is\ncalled $\\alpha$-weakly-Helly if any system of pairwise intersecting disks in\n$G$ has a nonempty common intersection when the radius of each disk is\nincreased by an additive value $\\alpha$. The minimum $\\alpha$ for which a graph\n$G$ is $\\alpha$-weakly-Helly is called the Helly-gap of $G$ and denoted by\n$\\alpha(G)$. The Helly-gap of a graph $G$ is characterized by distances in the\ninjective hull $\\mathcal{H}(G)$, which is a (unique) minimal Helly graph which\ncontains $G$ as an isometric subgraph. This characterization is used as a tool\nto generalize many eccentricity related results known for Helly graphs\n($\\alpha(G)=0$), as well as for chordal graphs ($\\alpha(G)\\le 1$),\ndistance-hereditary graphs ($\\alpha(G)\\le 1$) and $\\delta$-hyperbolic graphs\n($\\alpha(G)\\le 2\\delta$), to all graphs, parameterized by their Helly-gap\n$\\alpha(G)$. Several additional graph classes are shown to have a bounded\nHelly-gap, including AT-free graphs and graphs with bounded tree-length,\nbounded chordality or bounded $\\alpha_i$-metric.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 02:45:49 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Dragan", "Feodor F.", ""], ["Guarnera", "Heather M.", ""]]}, {"id": "2005.01929", "submitter": "Matthew Fahrbach", "authors": "Matthew Fahrbach, Zhiyi Huang, Runzhou Tao, Morteza Zadimoghaddam", "title": "Edge-Weighted Online Bipartite Matching", "comments": "36 pages, 5 figures. This work merges and refines the results in\n  arXiv:1704.05384, arXiv:1910.02569, and arXiv:1910.03287. In particular, we\n  fix a bug in arXiv:1910.03287 and have a smaller competitive ratio as a\n  result", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online bipartite matching and its variants are among the most fundamental\nproblems in the online algorithms literature. Karp, Vazirani, and Vazirani\n(STOC 1990) introduced an elegant algorithm for the unweighted problem that\nachieves an optimal competitive ratio of $1-1/e$. Later, Aggarwal et al. (SODA\n2011) generalized their algorithm and analysis to the vertex-weighted case.\nLittle is known, however, about the most general edge-weighted problem aside\nfrom the trivial $1/2$-competitive greedy algorithm. In this paper, we present\nthe first online algorithm that breaks the long-standing $1/2$ barrier and\nachieves a competitive ratio of at least $0.5086$. In light of the hardness\nresult of Kapralov, Post, and Vondr\\'ak (SODA 2013) that restricts beating a\n$1/2$ competitive ratio for the more general problem of monotone submodular\nwelfare maximization, our result can be seen as strong evidence that\nedge-weighted bipartite matching is strictly easier than submodular welfare\nmaximization in the online setting.\n  The main ingredient in our online matching algorithm is a novel subroutine\ncalled online correlated selection (OCS), which takes a sequence of pairs of\nvertices as input and selects one vertex from each pair. Instead of using a\nfresh random bit to choose a vertex from each pair, the OCS negatively\ncorrelates decisions across different pairs and provides a quantitative measure\non the level of correlation. We believe our OCS technique is of independent\ninterest and will find further applications in other online optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 03:18:56 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Huang", "Zhiyi", ""], ["Tao", "Runzhou", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "2005.02082", "submitter": "Fabrizio Montecchiani", "authors": "Michael A. Bekos, Martin Gronemann, Fabrizio Montecchiani,\n  D\\\"om\\\"ot\\\"or P\\'alv\\\"olgyi, Antonios Symvonis, Leonidas Theocharous", "title": "Grid Drawings of Graphs with Constant Edge-Vertex Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the algorithmic problem of computing drawings of graphs in which\n$(i)$ each vertex is a disk with fixed radius $\\rho$, $(ii)$ each edge is a\nstraight-line segment connecting the centers of the two disks representing its\nend-vertices, $(iii)$ no two disks intersect, and $(iv)$ the distance between\nan edge segment and the center of a non-incident disk, called \\emph{edge-vertex\nresolution}, is at least $\\rho$. We call such drawings \\emph{disk-link\ndrawings}.\n  In this paper we focus on the case of constant edge-vertex resolution, namely\n$\\rho=\\frac{1}{2}$ (i.e., disks of unit diameter). We prove that star graphs,\nwhich trivially admit straight-line drawings in linear area, require quadratic\narea in any such disk-link drawing. On the positive side, we present\nconstructive techniques that yield improved upper bounds for the area\nrequirements of disk-link drawings for several (planar and nonplanar) graph\nclasses, including bounded bandwidth, complete, and planar graphs. In\nparticular, the presented bounds for complete and planar graphs are\nasymptotically tight.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 11:55:11 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 10:55:50 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 13:35:47 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bekos", "Michael A.", ""], ["Gronemann", "Martin", ""], ["Montecchiani", "Fabrizio", ""], ["P\u00e1lv\u00f6lgyi", "D\u00f6m\u00f6t\u00f6r", ""], ["Symvonis", "Antonios", ""], ["Theocharous", "Leonidas", ""]]}, {"id": "2005.02143", "submitter": "Ioana O. Bercea", "authors": "Ioana Oriana Bercea and Guy Even", "title": "A Space-Efficient Dynamic Dictionary for Multisets with Constant Time\n  Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dynamic dictionary problem for multisets. Given an upper\nbound $n$ on the total cardinality of the multiset (i.e., including\nmultiplicities) at any point in time, the goal is to design a data structure\nthat supports multiplicity queries and allows insertions and deletions to the\nmultiset (i.e., the dynamic setting). The data structure must be\nspace-efficient (the space is $1+o(1)$ times the information-theoretic lower\nbound) and support all operations in constant time with high probability.\n  In this paper, we present the first dynamic dictionary for multisets that\nachieves these performance guarantees. This answers an open problem of\nArbitman, Naor and Segev (FOCS 2010). The previously best-known construction of\nPagh, Pagh and Rao (SODA 2005) supports membership in constant time,\nmultiplicity queries in $O(\\log n)$ time in the worst case, and insertions and\ndeletions in constant expected amortized time. The main technical component of\nour solution is a strategy for efficiently storing variable-length binary\ncounters using weighted balls-into-bins experiments in which balls have\nlogarithmic weights.\n  We also obtain a counting filter that approximates multiplicity queries with\na one sided error, using the reduction of Carter et al. (STOC 1978). Counting\nfilters have received significant attention over the years due to their\napplicability in practice.We present the first counting filter with constant\ntime operations.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 13:32:32 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Bercea", "Ioana Oriana", ""], ["Even", "Guy", ""]]}, {"id": "2005.02218", "submitter": "Matthias Bentert", "authors": "Luis M\\\"uller and Matthias Bentert", "title": "On Reachable Assignments in Cycles and Cliques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient and fair distribution of indivisible resources among agents is\na common problem in the field of \\emph{Multi-Agent-Systems}. We consider a\ngraph-based version of this problem called Reachable Assignments, introduced by\nGourves, Lesca, and Wilczynski [AAAI, 2017]. The input for this problem\nconsists of a set of agents, a set of objects, the agent's preferences over the\nobjects, a graph with the agents as vertices and edges encoding which agents\ncan trade resources with each other, and an initial and a target distribution\nof the objects, where each agent owns exactly one object in each distribution.\nThe question is then whether the target distribution is reachable via a\nsequence of rational trades. A trade is rational when the two participating\nagents are neighbors in the graph and both obtain an object they prefer over\nthe object they previously held. We show that Reachable Assignments is NP-hard\neven when restricting the input graph to be a clique and develop an\n$O(n^3)$-time algorithm for the case where the input graph is a cycle with $n$\nvertices.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:23:55 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["M\u00fcller", "Luis", ""], ["Bentert", "Matthias", ""]]}, {"id": "2005.02300", "submitter": "Till Fluschnik", "authors": "Robert Bredereck, Till Fluschnik, Andrzej Kaczmarczyk", "title": "Multistage Committee Election", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electing a single committee of a small size is a classical and\nwell-understood voting situation. Being interested in a sequence of committees,\nwe introduce and study two time-dependent multistage models based on simple\nPlurality voting. Therein, we are given a sequence of voting profiles (stages)\nover the same set of agents and candidates, and our task is to find a small\ncommittee for each stage of high score. In the conservative model we\nadditionally require that any two consecutive committees have a small symmetric\ndifference. Analogously, in the revolutionary model we require large symmetric\ndifferences. We prove both models to be NP-hard even for a constant number of\nagents, and, based on this, initiate a parameterized complexity analysis for\nthe most natural parameters and combinations thereof. Among other results, we\nprove both models to be in XP yet W[1]-hard regarding the number of stages, and\nthat being revolutionary seems to be \"easier\" than being conservative: If the\n(upper- resp. lower-) bound on the size of symmetric differences is constant,\nthe conservative model remains NP-hard while the revolutionary model becomes\npolynomial-time solvable.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 15:55:16 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Bredereck", "Robert", ""], ["Fluschnik", "Till", ""], ["Kaczmarczyk", "Andrzej", ""]]}, {"id": "2005.02329", "submitter": "Wojciech Nadara", "authors": "{\\L}ukasz Kowalik, Shaohua Li, Wojciech Nadara, Marcin Smulewicz,\n  Magnus Wahlstr\\\"om", "title": "Many visits TSP revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Many Visits TSP problem, where given a number $k(v)$ for each of\n$n$ cities and pairwise (possibly asymmetric) integer distances, one has to\nfind an optimal tour that visits each city $v$ exactly $k(v)$ times. The\ncurrently fastest algorithm is due to Berger, Kozma, Mnich and Vincze [SODA\n2019, TALG 2020] and runs in time and space $\\mathcal{O}^*(5^n)$. They also\nshow a polynomial space algorithm running in time $\\mathcal{O}^*(16^{n+o(n)})$.\n  In this work, we show three main results: (i) A randomized polynomial space\nalgorithm in time $\\mathcal{O}^*(2^nD)$, where $D$ is the maximum distance\nbetween two cities. By using standard methods, this results in\n$(1+\\epsilon)$-approximation in time $\\mathcal{O}^*(2^n\\epsilon^{-1})$.\nImproving the constant $2$ in these results would be a major breakthrough, as\nit would result in improving the $\\mathcal{O}^*(2^n)$-time algorithm for\nDirected Hamiltonian Cycle, which is a 50 years old open problem. (ii) A tight\nanalysis of Berger et al.'s exponential space algorithm, resulting in\n$\\mathcal{O}^*(4^n)$ running time bound. (iii) A new polynomial space\nalgorithm, running in time $\\mathcal{O}(7.88^n)$.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 16:54:58 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Kowalik", "\u0141ukasz", ""], ["Li", "Shaohua", ""], ["Nadara", "Wojciech", ""], ["Smulewicz", "Marcin", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "2005.02368", "submitter": "Gramoz Goranci", "authors": "Li Chen, Gramoz Goranci, Monika Henzinger, Richard Peng, Thatchaphol\n  Saranurak", "title": "Fast Dynamic Cuts, Distances and Effective Resistances via Vertex\n  Sparsifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework of designing efficient dynamic approximate\nalgorithms for optimization on undirected graphs. In particular, we develop a\ntechnique that, given any problem that admits a certain notion of vertex\nsparsifiers, gives data structures that maintain approximate solutions in\nsub-linear update and query time. We illustrate the applicability of our\nparadigm to the following problems.\n  (1) A fully-dynamic algorithm that approximates all-pair\nmaximum-flows/minimum-cuts up to a nearly logarithmic factor in\n$\\tilde{O}(n^{2/3})$ amortized time against an oblivious adversary, and\n$\\tilde{O}(m^{3/4})$ time against an adaptive adversary.\n  (2) An incremental data structure that maintains $O(1)$-approximate shortest\npath in $n^{o(1)}$ time per operation, as well as fully dynamic approximate\nall-pair shortest path and transshipment in $\\tilde{O}(n^{2/3+o(1)})$ amortized\ntime per operation.\n  (3) A fully-dynamic algorithm that approximates all-pair effective resistance\nup to an $(1+\\epsilon)$ factor in $\\tilde{O}(n^{2/3+o(1)} \\epsilon^{-O(1)})$\namortized update time per operation.\n  The key tool behind result (1) is the dynamic maintenance of an algorithmic\nconstruction due to Madry [FOCS' 10], which partitions a graph into a\ncollection of simpler graph structures (known as j-trees) and approximately\ncaptures the cut-flow and metric structure of the graph. The\n$O(1)$-approximation guarantee of (2) is by adapting the distance oracles by\n[Thorup-Zwick JACM `05]. Result (3) is obtained by invoking the random-walk\nbased spectral vertex sparsifier by [Durfee et al. STOC `19] in a hierarchical\nmanner, while carefully keeping track of the recourse among levels in the\nhierarchy.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:52:01 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Chen", "Li", ""], ["Goranci", "Gramoz", ""], ["Henzinger", "Monika", ""], ["Peng", "Richard", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "2005.02369", "submitter": "Gramoz Goranci", "authors": "Gramoz Goranci, Harald R\\\"acke, Thatchaphol Saranurak, Zihan Tan", "title": "The Expander Hierarchy and its Applications to Dynamic Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a notion for hierarchical graph clustering which we call the\nexpander hierarchy and show a fully dynamic algorithm for maintaining such a\nhierarchy on a graph with $n$ vertices undergoing edge insertions and deletions\nusing $n^{o(1)}$ update time. An expander hierarchy is a tree representation of\ngraphs that faithfully captures the cut-flow structure and consequently our\ndynamic algorithm almost immediately implies several results including:\n  (1) The first fully dynamic algorithm with $n^{o(1)}$ worst-case update time\nthat allows querying $n^{o(1)}$-approximate conductance, $s$-$t$ maximum flows,\nand $s$-$t$ minimum cuts for any given $(s,t)$ in $O(\\log^{1/6} n)$ time. Our\nresults are deterministic and extend to multi-commodity cuts and flows. The key\nidea behind these results is a fully dynamic algorithm for maintaining a tree\nflow sparsifier, a notion introduced by R\\\"acke [FOCS'02] for constructing\ncompetitive oblivious routing schemes.\n  (2) A deterministic fully dynamic connectivity algorithm with $n^{o(1)}$\nworst-case update time. This significantly simplifies the recent algorithm by\nChuzhoy et al.~that uses the framework of Nanongkai et al. [FOCS'17].\n  (3) The first non-trivial deterministic fully dynamic treewidth decomposition\nalgorithm on constant-degree graphs with $n^{o(1)}$ worst-case update time that\nmaintains a treewidth decomposition of width $\\text{tw}(G)\\cdot n^{o(1)}$ where\n$\\text{tw}(G)$ denotes the treewidth of the current graph.\n  Our technique is based on a new stronger notion of the expander\ndecomposition, called the boundary-linked expander decomposition. This\ndecomposition is more robust against updates and better captures the clustering\nstructure of graphs. Given that the expander decomposition has proved extremely\nuseful in many fields, we expect that our new notion will find more future\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:52:11 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 17:58:10 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Goranci", "Gramoz", ""], ["R\u00e4cke", "Harald", ""], ["Saranurak", "Thatchaphol", ""], ["Tan", "Zihan", ""]]}, {"id": "2005.02530", "submitter": "Hao-Tsung Yang", "authors": "Peyman Afshani, Mark De Berg, Kevin Buchin, Jie Gao, Maarten Loffler,\n  Amir Nayyeri, Benjamin Raichel, Rik Sarkar, Haotian Wang, Hao-Tsung Yang", "title": "Approximation Algorithms for Multi-Robot Patrol-Scheduling with Min-Max\n  Latency", "comments": "Proceedings of the 14th International Workshop on the Algorithmic\n  Foundations of Robotics (WAFR 20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding patrol schedules for $k$ robots to visit a\ngiven set of $n$ sites in a metric space. Each robot has the same maximum speed\nand the goal is to minimize the weighted maximum latency of any site, where the\nlatency of a site is defined as the maximum time duration between consecutive\nvisits of that site. The problem is NP-hard, as it has the traveling salesman\nproblem as a special case (when $k=1$ and all sites have the same weight). We\npresent a polynomial-time algorithm with an approximation factor of $O(k^2 \\log\n\\frac{w_{\\max}}{w_{\\min}})$ to the optimal solution, where $w_{\\max}$ and\n$w_{\\min}$ are the maximum and minimum weight of the sites respectively.\nFurther, we consider the special case where the sites are in 1D. When all sites\nhave the same weight, we present a polynomial-time algorithm to solve the\nproblem exactly. If the sites may have different weights, we present a\n$12$-approximate solution, which runs in polynomial time when the number of\nrobots, $k$, is a constant.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 23:18:53 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 15:28:03 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 02:39:07 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Afshani", "Peyman", ""], ["De Berg", "Mark", ""], ["Buchin", "Kevin", ""], ["Gao", "Jie", ""], ["Loffler", "Maarten", ""], ["Nayyeri", "Amir", ""], ["Raichel", "Benjamin", ""], ["Sarkar", "Rik", ""], ["Wang", "Haotian", ""], ["Yang", "Hao-Tsung", ""]]}, {"id": "2005.02537", "submitter": "Rick Cole", "authors": "Daniel Ting, Rick Cole", "title": "Conditional Cuckoo Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bloom filters, cuckoo filters, and other approximate set membership sketches\nhave a wide range of applications. Oftentimes, expensive operations can be\nskipped if an item is not in a data set. These filters provide an inexpensive,\nmemory efficient way to test if an item is in a set and avoid unnecessary\noperations. Existing sketches only allow membership testing for single set.\nHowever, in some applications such as join processing, the relevant set is not\nfixed and is determined by a set of predicates.\n  We propose the Conditional Cuckoo Filter, a simple modification of the cuckoo\nfilter that allows for set membership testing given predicates on a\npre-computed sketch. This filter also introduces a novel chaining technique\nthat enables cuckoo filters to handle insertion of duplicate keys. We evaluate\nour methods on a join processing application and show that they significantly\nreduce the number of tuples that a join must process.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 23:46:08 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Ting", "Daniel", ""], ["Cole", "Rick", ""]]}, {"id": "2005.02578", "submitter": "Shinsaku Sakaue", "authors": "Shinsaku Sakaue", "title": "Differentiable Greedy Submodular Maximization: Guarantees, Gradient\n  Estimators, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by, e.g., sensitivity analysis and end-to-end learning, the demand\nfor differentiable optimization algorithms has been significantly increasing.\nIn this paper, we establish a theoretically guaranteed versatile framework that\nmakes the greedy algorithm for monotone submodular function maximization\ndifferentiable. We smooth the greedy algorithm via randomization, and prove\nthat it almost recovers original approximation guarantees in expectation for\nthe cases of cardinality and $\\kappa$-extensible system constrains. We also\nshow how to efficiently compute unbiased gradient estimators of any expected\noutput-dependent quantities. We demonstrate the usefulness of our framework by\ninstantiating it for various applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 03:33:46 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 10:29:47 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 02:52:24 GMT"}, {"version": "v4", "created": "Fri, 12 Jun 2020 01:07:08 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Sakaue", "Shinsaku", ""]]}, {"id": "2005.02725", "submitter": "Lu\\'is M. S. Russo", "authors": "Lu\\'is M. S. Russo, Alexandre P. Francisco, Tatiana Rocher", "title": "Incremental Multiple Longest Common Sub-Sequences", "comments": "The work reported in this article was supported by national funds\n  through Funda\\c{c}\\~ao para a Ci\\^encia e Tecnologia (FCT) through projects\n  NGPHYLO PTDC/CCI-BIO/29676/2017 and UID/CEC/50021/2019. Funded in part by\n  European Union Horizon 2020 research and innovation programme under the Marie\n  Sk{\\l}odowska-Curie Actions grant agreement No 690941", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of updating the information about multiple longest\ncommon sub-sequences. This kind of sub-sequences is used to highlight\ninformation that is shared across several information sequences, therefore it\nis extensively used namely in bioinformatics and computational genomics. In\nthis paper we propose a way to maintain this information when the underlying\nsequences are subject to modifications, namely when letters are added and\nremoved from the extremes of the sequence. Experimentally our data structure\nobtains significant improvements over the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 10:52:18 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Russo", "Lu\u00eds M. S.", ""], ["Francisco", "Alexandre P.", ""], ["Rocher", "Tatiana", ""]]}, {"id": "2005.02751", "submitter": "Ayan Chattopadhyay", "authors": "Vikram Menon, Ayan Chattopadhyay", "title": "Quantum pattern matching Oracle construction", "comments": null, "journal-ref": null, "doi": "10.1007/s12043-020-02062-0", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a couple of oracle construction methods for quantum pattern\nmatching. We in turn show that one of the construct can be used with the\nGrover's search algorithm for exact and partial pattern matching,\ndeterministically. The other one also points to the matched indices, but\nprimarily provides a means to generate the Hamming distance between the pattern\nto be searched and all the possible sub strings in the input string, in a\nprobabilistic way.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 11:57:55 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Menon", "Vikram", ""], ["Chattopadhyay", "Ayan", ""]]}, {"id": "2005.02853", "submitter": "David Bremner", "authors": "David Avis and David Bremner", "title": "Sparktope: linear programs from algorithms", "comments": "Updated 2020-09-27. Update reference to now published paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM math.OC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In a recent paper Avis, Bremner, Tiwary and Watanabe gave a method for\nconstructing linear programs (LPs) based on algorithms written in a simple\nprogramming language called Sparks. If an algorithm produces the solution $x$\nto a problem in polynomial time and space then the LP constructed is also of\npolynomial size and its optimum solution contains $x$ as well as a complete\nexecution trace of the algorithm. Their method led us to the construction of a\ncompiler called Sparktope which we describe in this paper. This compiler allows\none to generate polynomial sized LPs for problems in P that have exponential\nextension complexity, such as matching problems in non-bipartite graphs.\n  In this paper we describe Sparktope, the language Sparks, and the assembler\ninstructions and LP constraints it produces. This is followed by two concrete\nexamples, the makespan problem and the problem of testing if a matching in a\ngraph is maximum, both of which are known to have exponential extension\ncomplexity. Computational results are given. In discussing these examples we\nmake use of visualization techniques included in Sparktope that may be of\nindependent interest. The extremely large linear programs produced by the\ncompiler appear to be quite challenging to solve using currently available\nsoftware. Since the optimum LP solutions can be computed independently they may\nbe useful as benchmarks. Further enhancements of the compiler and its\napplication are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 14:27:42 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 18:35:31 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 01:52:59 GMT"}, {"version": "v4", "created": "Sun, 27 Sep 2020 22:53:50 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Avis", "David", ""], ["Bremner", "David", ""]]}, {"id": "2005.02970", "submitter": "Pravesh K Kothari", "authors": "Ainesh Bakshi and Pravesh Kothari", "title": "Outlier-Robust Clustering of Non-Spherical Mixtures", "comments": "This version fixes a few typos and includes detailed proofs of the\n  certifiable bounded variance property in Section 8 for natural distributions\n  classes (fixing an issue with a generic lemma that proved such a property for\n  a class of distributions in the previous version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first outlier-robust efficient algorithm for clustering a mixture\nof $k$ statistically separated d-dimensional Gaussians (k-GMMs). Concretely,\nour algorithm takes input an $\\epsilon$-corrupted sample from a $k$-GMM and whp\nin $d^{\\text{poly}(k/\\eta)}$ time, outputs an approximate clustering that\nmisclassifies at most $k^{O(k)}(\\epsilon+\\eta)$ fraction of the points whenever\nevery pair of mixture components are separated by\n$1-\\exp(-\\text{poly}(k/\\eta)^k)$ in total variation (TV) distance. Such a\nresult was not previously known even for $k=2$. TV separation is the\nstatistically weakest possible notion of separation and captures important\nspecial cases such as mixed linear regression and subspace clustering.\n  Our main conceptual contribution is to distill simple analytic properties -\n(certifiable) hypercontractivity and bounded variance of degree 2 polynomials\nand anti-concentration of linear projections - that are necessary and\nsufficient for mixture models to be (efficiently) clusterable. As a\nconsequence, our results extend to clustering mixtures of arbitrary affine\ntransforms of the uniform distribution on the $d$-dimensional unit sphere. Even\nthe information-theoretic clusterability of separated distributions satisfying\nthese two analytic assumptions was not known prior to our work and is likely to\nbe of independent interest.\n  Our algorithms build on the recent sequence of works relying on certifiable\nanti-concentration first introduced in the works of Karmarkar, Klivans, and\nKothari and Raghavendra, and Yau in 2019. Our techniques expand the\nsum-of-squares toolkit to show robust certifiability of TV-separated Gaussian\nclusters in data. This involves giving a low-degree sum-of-squares proof of\nstatements that relate parameter (i.e. mean and covariances) distance to total\nvariation distance by relying only on hypercontractivity and\nanti-concentration.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:24:27 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 17:37:14 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 18:00:59 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Kothari", "Pravesh", ""]]}, {"id": "2005.03185", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski and Michael W. Mahoney", "title": "Determinantal Point Processes in Randomized Numerical Linear Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized Numerical Linear Algebra (RandNLA) uses randomness to develop\nimproved algorithms for matrix problems that arise in scientific computing,\ndata science, machine learning, etc. Determinantal Point Processes (DPPs), a\nseemingly unrelated topic in pure and applied mathematics, is a class of\nstochastic point processes with probability distribution characterized by\nsub-determinants of a kernel matrix. Recent work has uncovered deep and\nfruitful connections between DPPs and RandNLA which lead to new guarantees and\nimproved algorithms that are of interest to both areas. We provide an overview\nof this exciting new line of research, including brief introductions to RandNLA\nand DPPs, as well as applications of DPPs to classical linear algebra tasks\nsuch as least squares regression, low-rank approximation and the Nystr\\\"om\nmethod. For example, random sampling with a DPP leads to new kinds of unbiased\nestimators for least squares, enabling more refined statistical and inferential\nunderstanding of these algorithms; a DPP is, in some sense, an optimal\nrandomized algorithm for the Nystr\\\"om method; and a RandNLA technique called\nleverage score sampling can be derived as the marginal distribution of a DPP.\nWe also discuss recent algorithmic developments, illustrating that, while not\nquite as efficient as standard RandNLA techniques, DPP-based algorithms are\nonly moderately more expensive.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 00:39:52 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2005.03246", "submitter": "Nicolas Langren\\'e", "authors": "Nicolas Langren\\'e, Xavier Warin", "title": "Fast multivariate empirical cumulative distribution function with\n  connection to kernel density estimation", "comments": "26 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the problem of computing empirical cumulative\ndistribution functions (ECDF) efficiently on large, multivariate datasets.\nComputing an ECDF at one evaluation point requires $\\mathcal{O}(N)$ operations\non a dataset composed of $N$ data points. Therefore, a direct evaluation of\nECDFs at $N$ evaluation points requires a quadratic $\\mathcal{O}(N^2)$\noperations, which is prohibitive for large-scale problems. Two fast and exact\nmethods are proposed and compared. The first one is based on fast summation in\nlexicographical order, with a $\\mathcal{O}(N{\\log}N)$ complexity and requires\nthe evaluation points to lie on a regular grid. The second one is based on the\ndivide-and-conquer principle, with a $\\mathcal{O}(N\\log(N)^{(d-1){\\vee}1})$\ncomplexity and requires the evaluation points to coincide with the input\npoints. The two fast algorithms are described and detailed in the general\n$d$-dimensional case, and numerical experiments validate their speed and\naccuracy. Secondly, the paper establishes a direct connection between\ncumulative distribution functions and kernel density estimation (KDE) for a\nlarge class of kernels. This connection paves the way for fast exact algorithms\nfor multivariate kernel density estimation and kernel regression. Numerical\ntests with the Laplacian kernel validate the speed and accuracy of the proposed\nalgorithms. A broad range of large-scale multivariate density estimation,\ncumulative distribution estimation, survival function estimation and regression\nproblems can benefit from the proposed numerical methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 04:38:42 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 13:14:34 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Langren\u00e9", "Nicolas", ""], ["Warin", "Xavier", ""]]}, {"id": "2005.03394", "submitter": "Christoph D\\\"urr", "authors": "Fanny Dufoss\\'e, Christoph D\\\"urr, No\\\"el Nadal, Denis Trystram and\n  \\'Oscar C. V\\'asquez", "title": "Scheduling with a processing time oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we study a single machine scheduling problem with the objective\nof minimizing the sum of completion times. Each of the given jobs is either\nshort or long. However the processing times are initially hidden to the\nalgorithm, but can be tested. This is done by executing a processing time\noracle, which reveals the processing time of a given job. Each test occupies a\ntime unit in the schedule, therefore the algorithm must decide for which jobs\nit will call the processing time oracle. The objective value of the resulting\nschedule is compared with the objective value of an optimal schedule, which is\ncomputed using full information. The resulting competitive ratio measures the\nprice of hidden processing times, and the goal is to design an algorithm with\nminimal competitive ratio.\n  Two models are studied in this paper. In the non-adaptive model, the\nalgorithm needs to decide beforehand which jobs to test, and which jobs to\nexecute untested. However in the adaptive model, the algorithm can make these\ndecisions adaptively depending on the outcomes of the job tests. In both models\nwe provide optimal polynomial time algorithms following a two-phase strategy,\nwhich consist of a first phase where jobs are tested, and a second phase where\njobs are executed obliviously. Experiments give strong evidence that optimal\nalgorithms have this structure. Proving this property is left as an open\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 11:41:08 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 08:51:24 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 14:52:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Dufoss\u00e9", "Fanny", ""], ["D\u00fcrr", "Christoph", ""], ["Nadal", "No\u00ebl", ""], ["Trystram", "Denis", ""], ["V\u00e1squez", "\u00d3scar C.", ""]]}, {"id": "2005.03552", "submitter": "Sandy Heydrich", "authors": "Christoph Hertrich, Christian Wei{\\ss}, Heiner Ackermann, Sandy\n  Heydrich, Sven O. Krumke", "title": "Online Algorithms to Schedule a Proportionate Flexible Flow Shop of\n  Batching Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first to consider online algorithms to schedule a\nproportionate flexible flow shop of batching machines (PFFB). The scheduling\nmodel is motivated by manufacturing processes of individualized medicaments,\nwhich are used in modern medicine to treat some serious illnesses. We provide\ntwo different online algorithms, proving also lower bounds for the offline\nproblem to compute their competitive ratios. The first algorithm is an\neasy-to-implement, general local scheduling heuristic. It is 2-competitive for\nPFFBs with an arbitrary number of stages and for several natural scheduling\nobjectives. We also show that for total/average flow time, no deterministic\nalgorithm with better competitive ratio exists. For the special case with two\nstages and the makespan or total completion time objective, we describe an\nimproved algorithm that achieves the best possible competitive ratio\n$\\varphi=\\frac{1+\\sqrt{5}}{2}$, the golden ratio. All our results also hold for\nproportionate (non-flexible) flow shops of batching machines (PFB) for which\nthis is also the first paper to study online algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:26:29 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Hertrich", "Christoph", ""], ["Wei\u00df", "Christian", ""], ["Ackermann", "Heiner", ""], ["Heydrich", "Sandy", ""], ["Krumke", "Sven O.", ""]]}, {"id": "2005.03584", "submitter": "Manuel Penschuck", "authors": "Petra Berenbrink and David Hammer and Dominik Kaaser and Ulrich Meyer\n  and Manuel Penschuck and Hung Tran", "title": "Simulating Population Protocols in Sub-Constant Time per Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficiently simulating population protocols. In\nthe population model, we are given a distributed system of $n$ agents modeled\nas identical finite-state machines. In each time step, a pair of agents is\nselected uniformly at random to interact. In an interaction, agents update\ntheir states according to a common transition function. We empirically and\nanalytically analyze two classes of simulators for this model.\n  First, we consider sequential simulators executing one interaction after the\nother. Key to the performance of these simulators is the data structure storing\nthe agents' states. For our analysis, we consider plain arrays, binary search\ntrees, and a novel Dynamic Alias Table data structure.\n  Secondly, we consider batch processing to efficiently update the states of\nmultiple independent agents in one step. For many protocols considered in\nliterature, our simulator requires amortized sub-constant time per interaction\nand is fast in practice: given a fixed time budget, the implementation of our\nbatched simulator is able to simulate population protocols several orders of\nmagnitude larger compared to the sequential competitors, and can carry out\n$2^{50}$ interactions among the same number of agents in less than 400s.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 16:14:55 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Berenbrink", "Petra", ""], ["Hammer", "David", ""], ["Kaaser", "Dominik", ""], ["Meyer", "Ulrich", ""], ["Penschuck", "Manuel", ""], ["Tran", "Hung", ""]]}, {"id": "2005.03800", "submitter": "Neeldhara Misra", "authors": "Neeldhara Misra and Harshil Mittal", "title": "Imbalance Parameterized by Twin Cover Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of Imbalance parameterized by the twin cover of a graph.\nWe show that Imbalance is XP parameterized by twin cover, and FPT when\nparameterized by the twin cover and the size of the largest clique outside the\ntwin cover. In contrast, we introduce a notion of succinct representations of\ngraphs in terms of their twin cover and demonstrate that Imbalance is NP-hard\nin the setting of succinct representations, even for graphs that have a twin\ncover of size one.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 23:58:21 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Misra", "Neeldhara", ""], ["Mittal", "Harshil", ""]]}, {"id": "2005.03810", "submitter": "Govind Ramnarayan", "authors": "Younhun Kim, Elchanan Mossel, Govind Ramnarayan, Paxton Turner", "title": "Efficient Reconstruction of Stochastic Pedigrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG q-bio.PE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm called {\\sc Rec-Gen} for reconstructing the\ngenealogy or \\textit{pedigree} of an extant population purely from its genetic\ndata. We justify our approach by giving a mathematical proof of the\neffectiveness of {\\sc Rec-Gen} when applied to pedigrees from an idealized\ngenerative model that replicates some of the features of real-world pedigrees.\nOur algorithm is iterative and provides an accurate reconstruction of a large\nfraction of the pedigree while having relatively low \\emph{sample complexity},\nmeasured in terms of the length of the genetic sequences of the population. We\npropose our approach as a prototype for further investigation of the pedigree\nreconstruction problem toward the goal of applications to real-world examples.\nAs such, our results have some conceptual bearing on the increasingly important\nissue of genomic privacy.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 01:08:36 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Kim", "Younhun", ""], ["Mossel", "Elchanan", ""], ["Ramnarayan", "Govind", ""], ["Turner", "Paxton", ""]]}, {"id": "2005.04214", "submitter": "Raphael Clifford", "authors": "Peter Clifford and Rapha\\\"el Clifford", "title": "Faster classical Boson Sampling", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1706.01260", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction Boson Sampling has been the subject of intense study\nin the world of quantum computing. The task is to sample independently from the\nset of all $n \\times n$ submatrices built from possibly repeated rows of a\nlarger $m \\times n$ complex matrix according to a probability distribution\nrelated to the permanents of the submatrices. Experimental systems exploiting\nquantum photonic effects can in principle perform the task at great speed. In\nthe framework of classical computing, Aaronson and Arkhipov (2011) showed that\nexact Boson Sampling problem cannot be solved in polynomial time unless the\npolynomial hierarchy collapses to the third level. Indeed for a number of years\nthe fastest known exact classical algorithm ran in $O({m+n-1 \\choose n} n 2^n\n)$ time per sample, emphasising the potential speed advantage of quantum\ncomputation. The advantage was reduced by Clifford and Clifford (2018) who gave\na significantly faster classical solution taking $O(n 2^n +\n\\operatorname{poly}(m,n))$ time and linear space, matching the complexity of\ncomputing the permanent of a single matrix when $m$ is polynomial in $n$.\n  We continue by presenting an algorithm for Boson Sampling whose average-case\ntime complexity is much faster when $m$ is proportional to $n$. In particular,\nwhen $m = n$ our algorithm runs in approximately $O(n\\cdot1.69^n)$ time on\naverage. This result further increases the problem size needed to establish\nquantum computational supremacy via Boson Sampling.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 20:01:02 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 13:03:38 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Clifford", "Peter", ""], ["Clifford", "Rapha\u00ebl", ""]]}, {"id": "2005.04455", "submitter": "Martin Kouteck\\'y", "authors": "Martin Kouteck\\'y, Nimrod Talmon", "title": "Multi-Party Campaigning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a social choice setting of manipulation in elections and extend the\nusual model in two major ways: first, instead of considering a single\nmanipulating agent, in our setting there are several, possibly competing ones;\nsecond, instead of evaluating an election after the first manipulative action,\nwe allow several back-and-forth rounds to take place. We show that in certain\nsituations, such as in elections with only a few candidates, optimal strategies\nfor each of the manipulating agents can be computed efficiently.\n  Our algorithmic results rely on formulating the problem of finding an optimal\nstrategy as sentences of Presburger arithmetic that are short and only involve\nsmall coefficients, which we show is fixed-parameter tractable -- indeed, one\nof our contributions is a general result regarding fixed-parameter tractability\nof Presburger arithmetic that might be useful in other settings. Following our\ngeneral theorem, we design quite general algorithms; in particular, we describe\nhow to design efficient algorithms for various settings, including settings in\nwhich we model diffusion of opinions in a social network, complex budgeting\nschemes available to the manipulating agents, and various realistic\nrestrictions on adversary actions.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 14:45:01 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kouteck\u00fd", "Martin", ""], ["Talmon", "Nimrod", ""]]}, {"id": "2005.04533", "submitter": "Anya Chaturvedi", "authors": "Anya Chaturvedi, Andr\\'ea W. Richa, Mattias Rost, Stefan Schmid and\n  Jamison Weber", "title": "Improved Bi-criteria Approximation for the All-or-Nothing Multicommodity\n  Flow Problem in Arbitrary Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the following fundamental maximum throughput routing\nproblem: Given an arbitrary edge-capacitated $n$-node directed network and a\nset of $k$ commodities, with source-destination pairs $(s_i,t_i)$ and demands\n$d_i> 0$, admit and route the largest possible number of commodities -- i.e.,\nthe maximum {\\em throughput} -- to satisfy their demands. The main\ncontributions of this paper are two-fold: First, we present a bi-criteria\napproximation algorithm for this all-or-nothing multicommodity flow (ANF)\nproblem. Our algorithm is the first to achieve a {\\em constant approximation of\nthe maximum throughput} with an {\\em edge capacity violation ratio that is at\nmost logarithmic in $n$}, with high probability. Our approach is based on a\nversion of randomized rounding that keeps splittable flows, rather than\napproximating those via a non-splittable path for each commodity: This allows\nour approach to work for {\\em arbitrary directed edge-capacitated graphs},\nunlike most of the prior work on the ANF problem. Our algorithm also works if\nwe consider the weighted throughput, where the benefit gained by fully\nsatisfying the demand for commodity $i$ is determined by a given weight\n$w_i>0$. Second, we present a derandomization of our algorithm that maintains\nthe same approximation bounds, using novel pessimistic estimators for\nBernstein's inequality. In addition, we show how our framework can be adapted\nto achieve a polylogarithmic fraction of the maximum throughput while\nmaintaining a constant edge capacity violation, if the network capacity is\nlarge enough. One important aspect of our randomized and derandomized\nalgorithms is their {\\em simplicity}, which lends to efficient implementations\nin practice.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 00:02:25 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 22:25:16 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 03:52:33 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chaturvedi", "Anya", ""], ["Richa", "Andr\u00e9a W.", ""], ["Rost", "Mattias", ""], ["Schmid", "Stefan", ""], ["Weber", "Jamison", ""]]}, {"id": "2005.04635", "submitter": "Ayan Chattopadhyay", "authors": "Ayan Chattopadhyay, Vikram Menon", "title": "Fast simulation of Grover's quantum search on classical computer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research community has been actively working on the realization of\nquantum computer. But the large scale commercial quantum computers are not a\nreality yet quantum computing field has become richer by day with the advent of\nalgorithms and the avenue of its application in multiple domains. Availability\nof efficient quantum simulators will enable the researchers to quickly verify\ntheir results and concepts in order to establish a working proof of\ncorrectness. One important algorithm that has become one of the basic\ningredients to build other algorithms and models is the Grover's search\nAlgorithm which is known to be the most compute intensive. Our approach\nhighlights the design principles for the fast simulation of Grover's search\nwhich can be implemented on a general purpose personal computer. The\nperformance obtained are encouraging when compared to the existing simulators.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 12:17:39 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 15:51:26 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chattopadhyay", "Ayan", ""], ["Menon", "Vikram", ""]]}, {"id": "2005.04733", "submitter": "Lars Jaffke", "authors": "Lars Jaffke, Paloma T. Lima, Geevarghese Philip", "title": "Structural Parameterizations of Clique Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A clique coloring of a graph is an assignment of colors to its vertices such\nthat no maximal clique is monochromatic. We initiate the study of structural\nparameterizations of the Clique Coloring problem which asks whether a given\ngraph has a clique coloring with $q$ colors. For fixed $q \\ge 2$, we give an\n$\\mathcal{O}^{\\star}(q^{tw})$-time algorithm when the input graph is given\ntogether with one of its tree decompositions of width $tw$. We complement this\nresult with a matching lower bound under the Strong Exponential Time\nHypothesis. We furthermore show that (when the number of colors is unbounded)\nClique Coloring is XP parameterized by clique-width.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 17:58:06 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Jaffke", "Lars", ""], ["Lima", "Paloma T.", ""], ["Philip", "Geevarghese", ""]]}, {"id": "2005.04740", "submitter": "Marius Lombard-Platet", "authors": "R\\'emi G\\'eraud-Stewart and Marius Lombard-Platet and David Naccache", "title": "Approaching Optimal Duplicate Detection in a Sliding Window", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duplicate detection is the problem of identifying whether a given item has\npreviously appeared in a (possibly infinite) stream of data, when only a\nlimited amount of memory is available.\n  Unfortunately the infinite stream setting is ill-posed, and error rates of\nduplicate detection filters turn out to be heavily constrained: consequently\nthey appear to provide no advantage, asymptotically, over a biased coin toss\n[8].\n  In this paper we formalize the sliding window setting introduced by [13,16],\nand show that a perfect (zero error) solution can be used up to a maximal\nwindow size $w_\\text{max}$. Above this threshold we show that some existing\nduplicate detection filters (designed for the $\\textit{non-windowed}$ setting)\nperform better that those targeting the windowed problem. Finally, we introduce\na \"queuing construction\" that improves on the performance of some duplicate\ndetection filters in the windowed setting.\n  We also analyse the security of our filters in an adversarial setting.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 18:24:54 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["G\u00e9raud-Stewart", "R\u00e9mi", ""], ["Lombard-Platet", "Marius", ""], ["Naccache", "David", ""]]}, {"id": "2005.04800", "submitter": "Itai Dinur", "authors": "Itai Dinur", "title": "Improved Algorithms for Solving Polynomial Systems over GF(2) by\n  Multiple Parity-Counting", "comments": "Several (mostly small) changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a solution to a multivariate polynomial\nequation system of degree $d$ in $n$ variables over $\\mathbb{F}_2$. For $d=2$,\nthe best-known algorithm for the problem is by Bardet et al. [J. Complexity,\n2013] and was shown to run in time $O(2^{0.792n})$ under assumptions that were\nexperimentally found to hold for random equation systems. The best-known\nworst-case algorithm for the problem is due to Bj\\\"{o}rklund et al. [ICALP'19].\nIt runs in time $O(2^{0.804n})$ for $d = 2$ and $O(2^{(1 - 1/(2.7d))n})$ for $d\n> 2$.\n  In this paper, we devise a worst-case algorithm that improves the one by\nBj\\\"{o}rklund et al. It runs in time $O(2^{0.6943n})$ (or $O(1.6181^n)$) for $d\n= 2$ and $O(2^{(1 - 1/(2d))n})$ for $d > 2$. Our algorithm thus outperforms all\nknown worst-case algorithms, as well as ones analyzed for random equation\nsystems. We also devise a second algorithm that outputs all solutions to a\npolynomial system and has similar complexity to the first (provided that the\nnumber of solutions is not too large).\n  A central idea in the work of Bj\\\"{o}rklund et al. was to reduce the problem\nof finding a solution to a polynomial system over $\\mathbb{F}_2$ to the problem\nof counting the parity of all solutions. A parity-counting instance was then\nreduced to many smaller parity-counting instances. Our main observation is that\nthese smaller instances are related and can be solved more efficiently by a new\nalgorithm to a problem which we call multiple parity-counting.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 22:17:18 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 22:08:32 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Dinur", "Itai", ""]]}, {"id": "2005.04818", "submitter": "Thomas Hujsa", "authors": "Thomas Hujsa, Bernard Berthomieu, Silvano Dal Zilio, Didier Le Botlan", "title": "On the Petri Nets with a Single Shared Place and Beyond", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Petri nets proved useful to describe various real-world systems, but many of\ntheir properties are very hard to check. To alleviate this difficulty,\nsubclasses are often considered. The class of weighted marked graphs with\nrelaxed place constraint (WMG=< for short), in which each place has at most one\ninput and one output, and the larger class of choice-free (CF) nets, in which\neach place has at most one output, have been extensively studied to this end,\nwith various applications.\n  In this work, we develop new properties related to the fundamental and\nintractable problems of reachability, liveness and reversibility in weighted\nPetri nets. We focus mainly on the homogeneous Petri nets with a single shared\nplace (H1S nets for short), which extend the expressiveness of CF nets by\nallowing one shared place (i.e. a place with at least two outputs and possibly\nseveral inputs) under the homogeneity constraint (i.e. all the output weights\nof the shared place are equal). Indeed, this simple generalization already\nyields new challenging problems and is expressive enough for modeling existing\nuse-cases, justifying a dedicated study.\n  One of our central results is the first characterization of liveness in a\nsubclass of H1S nets more expressive than WMG=< that is expressed by the\ninfeasibility of an integer linear program (ILP) of polynomial size. This trims\ndown the complexity to co-NP, contrasting with the known EXPSPACE-hardness of\nliveness in the more general case of weighted Petri nets. In the same subclass,\nwe obtain a new reachability property related to the live markings, which is a\nvariant of the well-known Keller's theorem. Another central result is a new\nreversibility characterization for the live H1S class, simplifying its\nchecking. Finally, we apply our results to use-cases, highlight their\nscalability and discuss their extensibility to more expressive classes.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 00:29:11 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hujsa", "Thomas", ""], ["Berthomieu", "Bernard", ""], ["Zilio", "Silvano Dal", ""], ["Botlan", "Didier Le", ""]]}, {"id": "2005.04907", "submitter": "Du\\v{s}an Knop", "authors": "Robert Bredereck, Andrzej Kaczmarczyk, Du\\v{s}an Knop, Rolf\n  Niedermeier", "title": "High-Multiplicity Fair Allocation Using Parametric Integer Linear\n  Programming", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using insights from parametric integer linear programming, we significantly\nimprove on our previous work [Proc. ACM EC 2019] on high-multiplicity fair\nallocation. Therein, answering an open question from previous work, we proved\nthat the problem of finding envy-free Pareto-efficient allocations of\nindivisible items is fixed-parameter tractable with respect to the combined\nparameter \"number of agents\" plus \"number of item types.\" Our central\nimprovement, compared to this result, is to break the condition that the\ncorresponding utility and multiplicity values have to be encoded in unary\nrequired there. Concretely, we show that, while preserving fixed-parameter\ntractability, these values can be encoded in binary, thus greatly expanding the\nrange of feasible values.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:02:39 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Bredereck", "Robert", ""], ["Kaczmarczyk", "Andrzej", ""], ["Knop", "Du\u0161an", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "2005.04957", "submitter": "Moritz Venzin", "authors": "Friedrich Eisenbrand, Moritz Venzin", "title": "Approximate $\\mathrm{CVP}_{p}$ in time $2^{0.802 \\, n}$", "comments": "We improved the introduction and added the case $p \\in [1,2)$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a constant factor approximation of the shortest and closest\nlattice vector problem w.r.t. any $\\ell_p$-norm can be computed in time\n$2^{(0.802 +{\\epsilon})\\, n}$. This matches the currently fastest constant\nfactor approximation algorithm for the shortest vector problem w.r.t. $\\ell_2$.\nTo obtain our result, we combine the latter algorithm w.r.t. $\\ell_2$ with\ngeometric insights related to coverings.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 09:32:06 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 17:31:39 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Eisenbrand", "Friedrich", ""], ["Venzin", "Moritz", ""]]}, {"id": "2005.05070", "submitter": "David Richerby", "authors": "Leslie Ann Goldberg, John Lapinskas and David Richerby", "title": "Faster Exponential-time Algorithms for Approximately Counting\n  Independent Sets", "comments": "50pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the independent sets of a graph is a classical #P-complete problem,\neven in the bipartite case. We give an exponential-time approximation scheme\nfor this problem which is faster than the best known algorithm for the exact\nproblem. The running time of our algorithm on general graphs with error\ntolerance $\\varepsilon$ is at most $O(2^{0.2680n})$ times a polynomial in\n$1/\\varepsilon$. On bipartite graphs, the exponential term in the running time\nis improved to $O(2^{0.2372n})$. Our methods combine techniques from exact\nexponential algorithms with techniques from approximate counting. Along the way\nwe generalise (to the multivariate case) the FPTAS of Sinclair, Srivastava,\n\\v{S}tefankovi\\v{c} and Yin for approximating the hard-core partition function\non graphs with bounded connective constant. Also, we obtain an FPTAS for\ncounting independent sets on graphs with no vertices with degree at least 6\nwhose neighbours' degrees sum to 27 or more. By a result of Sly, there is no\nFPTAS that applies to all graphs with maximum degree 6 unless\n$\\mbox{P}=\\mbox{NP}$.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 13:04:28 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Goldberg", "Leslie Ann", ""], ["Lapinskas", "John", ""], ["Richerby", "David", ""]]}, {"id": "2005.05143", "submitter": "Kevin Pratt", "authors": "Cornelius Brand, Kevin Pratt", "title": "An Algorithmic Method of Partial Derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following problem and its applications: given a homogeneous\ndegree-$d$ polynomial $g$ as an arithmetic circuit, and a $d \\times d$ matrix\n$X$ whose entries are homogeneous linear polynomials, compute\n$g(\\partial/\\partial x_1, \\ldots, \\partial/\\partial x_n) \\det X$. By\nconsidering special cases of this problem we obtain faster parameterized\nalgorithms for several problems, including the matroid $k$-parity and\n$k$-matroid intersection problems, faster \\emph{deterministic} algorithms for\ntesting if a linear space of matrices contains an invertible matrix (Edmonds's\nproblem) and detecting $k$-internal outbranchings, and more. We also match the\nruntime of the fastest known deterministic algorithm for detecting subgraphs of\nbounded pathwidth, while using a new approach.\n  Our approach raises questions in algebraic complexity related to Waring rank\nand the exponent of matrix multiplication $\\omega$. In particular, we study a\nnew complexity measure on the space of homogeneous polynomials, namely the\nbilinear complexity of a polynomial's apolar algebra. Our algorithmic\nimprovements are reflective of the fact that for the degree-$n$ determinant\npolynomial this quantity is at most $O(n 2^{\\omega n})$, whereas all known\nupper bounds on the Waring rank of this polynomial exceed $n!$.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 14:36:22 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Brand", "Cornelius", ""], ["Pratt", "Kevin", ""]]}, {"id": "2005.05295", "submitter": "Arthur Goldberg", "authors": "Arthur P. Goldberg (1) and David R. Jefferson (2) and John A. P. Sekar\n  (1) and Jonathan R. Karr (1) ((1) Icahn Institute for Data Science and\n  Genomic Technology, and Department of Genetics and Genomic Sciences, Icahn\n  School of Medicine at Mount Sinai, (2) Lawrence Livermore National\n  Laboratory)", "title": "Exact Parallelization of the Stochastic Simulation Algorithm for\n  Scalable Simulation of Large Biochemical Networks", "comments": "21 pages, 4 figures; 2020-05-20 submission: updated authors,\n  affiliations, emails, acknowledgments and layout", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.DC cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive simulations of the entire biochemistry of cells have great\npotential to help physicians treat disease and help engineers design biological\nmachines. But such simulations must model networks of millions of molecular\nspecies and reactions.\n  The Stochastic Simulation Algorithm (SSA) is widely used for simulating\nbiochemistry, especially systems with species populations small enough that\ndiscreteness and stochasticity play important roles. However, existing serial\nSSA methods are prohibitively slow for comprehensive networks, and existing\nparallel SSA methods, which use periodic synchronization, sacrifice accuracy.\n  To enable fast, accurate, and scalable simulations of biochemistry, we\npresent an exact parallel algorithm for SSA that partitions a biochemical\nnetwork into many SSA processes that simulate in parallel. Our parallel SSA\nalgorithm exactly coordinates the interactions among these SSA processes and\nthe species state they share by structuring the algorithm as a parallel\ndiscrete event simulation (DES) application and using an optimistic parallel\nDES simulator to synchronize the interactions. We anticipate that our method\nwill enable unprecedented biochemical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:56:21 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 21:27:01 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Goldberg", "Arthur P.", ""], ["Jefferson", "David R.", ""], ["Sekar", "John A. P.", ""], ["Karr", "Jonathan R.", ""]]}, {"id": "2005.05325", "submitter": "Alireza Samadian", "authors": "Mahmoud Abo-Khamis, Sungjin Im, Benjamin Moseley, Kirk Pruhs, Alireza\n  Samadian", "title": "A Relational Gradient Descent Algorithm For Support Vector Machine\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider gradient descent like algorithms for Support Vector Machine (SVM)\ntraining when the data is in relational form. The gradient of the SVM objective\ncan not be efficiently computed by known techniques as it suffers from the\n``subtraction problem''. We first show that the subtraction problem can not be\nsurmounted by showing that computing any constant approximation of the gradient\nof the SVM objective function is $\\#P$-hard, even for acyclic joins. We,\nhowever, circumvent the subtraction problem by restricting our attention to\nstable instances, which intuitively are instances where a nearly optimal\nsolution remains nearly optimal if the points are perturbed slightly. We give\nan efficient algorithm that computes a ``pseudo-gradient'' that guarantees\nconvergence for stable instances at a rate comparable to that achieved by using\nthe actual gradient. We believe that our results suggest that this sort of\nstability the analysis would likely yield useful insight in the context of\ndesigning algorithms on relational data for other learning problems in which\nthe subtraction problem arises.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:50:36 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Abo-Khamis", "Mahmoud", ""], ["Im", "Sungjin", ""], ["Moseley", "Benjamin", ""], ["Pruhs", "Kirk", ""], ["Samadian", "Alireza", ""]]}, {"id": "2005.05547", "submitter": "Jan Bok", "authors": "Jan Bok, Richard Brewster, Tom\\'as Feder, Pavol Hell and Nikola\n  Jedli\\v{c}kov\\'a", "title": "List homomorphism problems for signed graphs", "comments": "various enhancements, full graph-theoretical classification of\n  complexity for cycle-separable graphs, 44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider homomorphisms of signed graphs from a computational perspective.\nIn particular, we study the list homomorphism problem seeking a homomorphism of\nan input signed graph $(G,\\sigma)$, equipped with lists $L(v) \\subseteq V(H), v\n\\in V(G)$, of allowed images, to a fixed target signed graph $(H,\\pi)$. The\ncomplexity of the similar homomorphism problem without lists (corresponding to\nall lists being $L(v)=V(H)$) has been previously classified by Brewster and\nSiggers, but the list version remains open and appears difficult. We illustrate\nthis difficulty by classifying the complexity of the problem when $H$ is a tree\n(with possible loops). The tools we develop will be useful for classifications\nof other classes of signed graphs, and we illustrate this by classifying the\ncomplexity of irreflexive signed graphs in which the unicoloured edges form\nsome simple structures, namely paths or cycles. The structure of the signed\ngraphs in the polynomial cases is interesting, suggesting they may constitute a\nnice class of signed graphs analogous to the so-called bi-arc graphs (which\ncharacterized the polynomial cases of list homomorphisms to unsigned graphs).\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 04:57:23 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 08:54:18 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 13:56:25 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Bok", "Jan", ""], ["Brewster", "Richard", ""], ["Feder", "Tom\u00e1s", ""], ["Hell", "Pavol", ""], ["Jedli\u010dkov\u00e1", "Nikola", ""]]}, {"id": "2005.05681", "submitter": "Juliusz Straszy\\'nski", "authors": "Panagiotis Charalampopoulos, Tomasz Kociumaka, Manal Mohamed, Jakub\n  Radoszewski, Wojciech Rytter, Juliusz Straszy\\'nski, Tomasz Wale\\'n and\n  Wiktor Zuba", "title": "Counting Distinct Patterns in Internal Dictionary Matching", "comments": "Accepted to CPM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of preprocessing a text $T$ of length $n$ and a\ndictionary $\\mathcal{D}$ in order to be able to efficiently answer queries\n$CountDistinct(i,j)$, that is, given $i$ and $j$ return the number of patterns\nfrom $\\mathcal{D}$ that occur in the fragment $T[i \\mathinner{.\\,.} j]$. The\ndictionary is internal in the sense that each pattern in $\\mathcal{D}$ is given\nas a fragment of $T$. This way, the dictionary takes space proportional to the\nnumber of patterns $d=|\\mathcal{D}|$ rather than their total length, which\ncould be $\\Theta(n\\cdot d)$. An $\\tilde{\\mathcal{O}}(n+d)$-size data structure\nthat answers $CountDistinct(i,j)$ queries $\\mathcal{O}(\\log n)$-approximately\nin $\\tilde{\\mathcal{O}}(1)$ time was recently proposed in a work that\nintroduced internal dictionary matching [ISAAC 2019]. Here we present an\n$\\tilde{\\mathcal{O}}(n+d)$-size data structure that answers\n$CountDistinct(i,j)$ queries $2$-approximately in $\\tilde{\\mathcal{O}}(1)$\ntime. Using range queries, for any $m$, we give an\n$\\tilde{\\mathcal{O}}(\\min(nd/m,n^2/m^2)+d)$-size data structure that answers\n$CountDistinct(i,j)$ queries exactly in $\\tilde{\\mathcal{O}}(m)$ time. We also\nconsider the special case when the dictionary consists of all square factors of\nthe string. We design an $\\mathcal{O}(n \\log^2 n)$-size data structure that\nallows us to count distinct squares in a text fragment $T[i \\mathinner{.\\,.}\nj]$ in $\\mathcal{O}(\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 10:49:46 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Kociumaka", "Tomasz", ""], ["Mohamed", "Manal", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Straszy\u0144ski", "Juliusz", ""], ["Wale\u0144", "Tomasz", ""], ["Zuba", "Wiktor", ""]]}, {"id": "2005.05863", "submitter": "Laurent Feuilloley", "authors": "Laurent Feuilloley, Pierre Fraigniaud, Ivan Rapaport, \\'Eric R\\'emila,\n  Pedro Montealegre and Ioan Todinca", "title": "Compact Distributed Certification of Planar Graphs", "comments": "To appear at PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naor, Parter, and Yogev (SODA 2020) have recently demonstrated the existence\nof a \\emph{distributed interactive proof} for planarity (i.e., for certifying\nthat a network is planar), using a sophisticated generic technique for\nconstructing distributed IP protocols based on sequential IP protocols. The\ninteractive proof for planarity is based on a distributed certification of the\ncorrect execution of any given sequential linear-time algorithm for planarity\ntesting. It involves three interactions between the prover and the randomized\ndistributed verifier (i.e., it is a \\dMAM\\/ protocol), and uses small\ncertificates, on $O(\\log n)$ bits in $n$-node networks. We show that a single\ninteraction from the prover suffices, and randomization is unecessary, by\nproviding an explicit description of a \\emph{proof-labeling scheme} for\nplanarity, still using certificates on just $O(\\log n)$ bits. We also show that\nthere are no proof-labeling schemes -- in fact, even no \\emph{locally checkable\nproofs} -- for planarity using certificates on $o(\\log n)$ bits.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 15:34:04 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Feuilloley", "Laurent", ""], ["Fraigniaud", "Pierre", ""], ["Rapaport", "Ivan", ""], ["R\u00e9mila", "\u00c9ric", ""], ["Montealegre", "Pedro", ""], ["Todinca", "Ioan", ""]]}, {"id": "2005.06046", "submitter": "Neeldhara Misra", "authors": "Neeldhara Misra, Harshil Mittal, Aditi Sethia", "title": "Red-Blue Point Separation for Points on a Circle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set R of red points and a set B of blue points in the plane, the\nRed-Blue point separation problem asks if there are at most k lines that\nseparate R from B, that is, each cell induced by the lines of the solution is\neither empty or monochromatic (containing points of only one color). A common\nvariant of the problem is when the lines are required to be axis-parallel. The\nproblem is known to be NP-complete for both scenarios, and W[1]-hard\nparameterized by k in the former setting and FPT in the latter. We demonstrate\na polynomial-time algorithm for the special case when the points lie on a\ncircle. Further, we also demonstrate the W-hardness of a related problem in the\naxis-parallel setting, where the question is if there are p horizontal and q\nvertical lines that separate R from B. The hardness here is shown in the\nparameter p.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 20:54:54 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Misra", "Neeldhara", ""], ["Mittal", "Harshil", ""], ["Sethia", "Aditi", ""]]}, {"id": "2005.06100", "submitter": "Cheng Lu", "authors": "Cheng Lu", "title": "Structure and Algorithm for Path of Solutions to a Class of Fused Lasso\n  Problems", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of fused lasso problems where the estimated parameters in a\nsequence are regressed toward their respective observed values (fidelity loss),\nwith $\\ell_1$ norm penalty (regularization loss) on the differences between\nsuccessive parameters, which promotes local constancy. In many applications,\nthere is a coefficient, often denoted as $\\lambda$, on the regularization term,\nwhich adjusts the relative importance between the two losses.\n  In this paper, we characterize how the optimal solution evolves with the\nincrement of $\\lambda$. We show that, if all fidelity loss functions are convex\npiecewise linear, the optimal value for \\emph{each} variable changes at most\n$O(nq)$ times for a problem of $n$ variables and total $q$ breakpoints. On the\nother hand, we present an algorithm that solves the path of solutions of\n\\emph{all} variables in $\\tilde{O}(nq)$ time for all $\\lambda \\geq 0$.\nInterestingly, we find that the path of solutions for each variable can be\ndivided into up to $n$ locally convex-like segments. For problems of arbitrary\nconvex loss functions, for a given solution accuracy, one can transform the\nloss functions into convex piecewise linear functions and apply the above\nresults, giving pseudo-polynomial bounds as $q$ becomes a pseudo-polynomial\nquantity.\n  To our knowledge, this is the first work to solve the path of solutions for\nfused lasso of non-quadratic fidelity loss functions.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 01:10:10 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Lu", "Cheng", ""]]}, {"id": "2005.06156", "submitter": "Daogao Liu", "authors": "Yaonan Jin, Daogao Liu, Zhao Song", "title": "A robust multi-dimensional sparse Fourier transform in the continuous\n  setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sparse Fourier transform (Sparse FT) is the problem of learning an unknown\nsignal, whose frequency spectrum is dominated by a small amount of $k$\nindividual frequencies, through fast algorithms that use as few samples as\npossible in the time domain. The last two decades have seen an extensive study\non such problems, either in the one-/multi-dimensional discrete setting\n[Hassanieh, Indyk, Katabi, and Price STOC'12; Kapralov STOC'16] or in the\none-dimensional continuous setting [Price and Song FOCS'15]. Despite this rich\nliterature, the most general multi-dimensional continuous case remains\nmysterious.\n  This paper initiates the study on the Sparse FT problem in the\nmulti-dimensional continuous setting. Our main result is a randomized\nnon-adaptive algorithm that uses sublinear samples and runs in sublinear time.\nIn particular, the sample duration bound required by our algorithm gives a\nnon-trivial improvement over [Price and Song FOCS'15], which studies the same\nproblem in the one-dimensional continuous setting.\n  The dimensionality in the continuous setting, different from both the\ndiscrete cases and the one-dimensional continuous case, turns out to incur many\nnew challenges. To overcome these issues, we develop a number of new techniques\nfor constructing the filter functions, designing the permutation-then-hashing\nschemes, sampling the Fourier measurements, and locating the frequencies. We\nbelieve these techniques can find their applications in the future studies on\nthe Sparse FT problem.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 05:29:36 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Jin", "Yaonan", ""], ["Liu", "Daogao", ""], ["Song", "Zhao", ""]]}, {"id": "2005.06225", "submitter": "Rosario Scatamacchia", "authors": "Federico Della Croce, Marco Ghirardi, Rosario Scatamacchia", "title": "An improved solution approach for the Budget constrained Fuel Treatment\n  Scheduling problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the budget constrained fuel treatment scheduling (BFTS)\nproblem where, in the context of wildfire mitigation, the goal is to inhibit\nthe potential of fire spread in a landscape by proper fuel treatment\nactivities. Given a time horizon represented by consecutive unit periods, the\nlandscape is divided into cells and represented as a grid graph where each cell\nhas a fuel age that increases over time and becomes old if no treatment is\napplied in the meantime: this induces a potential high fire risk whenever two\ncontiguous cells are old. Cells fuel ages can be reset to zero under\nappropriate fuel treatments but there is a limited budget for treatment in each\nperiod. The problem calls for finding a suitable selection of cells to be\ntreated so as to minimize the presence of old contiguous cells over the whole\ntime horizon. We prove that problem BFTS is strongly NP-complete on paths and\nthus on grid graphs and show that no polynomial time approximation algorithm\nexists unless P = NP. We provide an enhanced integer linear programming\nformulation of the problem with respect to the relevant literature that shows\nup to be efficiently solved by an ILP solver on reasonably large size\ninstances. Finally, we consider a harder periodic variant of the problem with\nthe aim of finding a cyclic treatment plan with cycles of length T and propose\na matheuristic approach capable of efficiently tackling those instances where\nan ILP solver applied to the ILP formulation runs into difficulties.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 09:34:14 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Della Croce", "Federico", ""], ["Ghirardi", "Marco", ""], ["Scatamacchia", "Rosario", ""]]}, {"id": "2005.06311", "submitter": "Zhihao Gavin Tang", "authors": "Zhiyi Huang, Zhihao Gavin Tang, Xiaowei Wu and Yuhao Zhang", "title": "Fully Online Matching II: Beating Ranking and Water-filling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Karp, Vazirani, and Vazirani (STOC 1990) initiated the study of online\nbipartite matching, which has held a central role in online algorithms ever\nsince. Of particular importance are the Ranking algorithm for integral matching\nand the Water-filling algorithm for fractional matching. Most algorithms in the\nliterature can be viewed as adaptations of these two in the corresponding\nmodels. Recently, Huang et al.~(STOC 2018, SODA 2019) introduced a more general\nmodel called \\emph{fully online matching}, which considers general graphs and\nallows all vertices to arrive online. They also generalized Ranking and\nWater-filling to fully online matching and gave some tight analysis: Ranking is\n$\\Omega \\approx 0.567$-competitive on bipartite graphs where the\n$\\Omega$-constant satisfies $\\Omega e^\\Omega = 1$, and Water-filling is\n$2-\\sqrt{2} \\approx 0.585$-competitive on general graphs.\n  We propose fully online matching algorithms strictly better than Ranking and\nWater-filling. For integral matching on bipartite graphs, we build on the\nonline primal dual analysis of Ranking and Water-filling to design a\n$0.569$-competitive hybrid algorithm called Balanced Ranking. To our knowledge,\nit is the first integral algorithm in the online matching literature that\nsuccessfully integrates ideas from Water-filling. For fractional matching on\ngeneral graphs, we give a $0.592$-competitive algorithm called Eager\nWater-filling, which may match a vertex on its arrival. By contrast, the\noriginal Water-filling algorithm always matches vertices at their deadlines.\nOur result for fractional matching further shows a separation between fully\nonline matching and the general vertex arrival model by Wang and Wong (ICALP\n2015), due to an upper bound of $0.5914$ in the latter model by Buchbinder,\nSegev, and Tkach (ESA 2017).\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 13:33:21 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Huang", "Zhiyi", ""], ["Tang", "Zhihao Gavin", ""], ["Wu", "Xiaowei", ""], ["Zhang", "Yuhao", ""]]}, {"id": "2005.06329", "submitter": "Jakub Radoszewski", "authors": "Aleksander K\\k{e}dzierski and Jakub Radoszewski", "title": "k-Approximate Quasiperiodicity under Hamming and Edit Distance", "comments": "accepted to CPM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasiperiodicity in strings was introduced almost 30 years ago as an\nextension of string periodicity. The basic notions of quasiperiodicity are\ncover and seed. A cover of a text $T$ is a string whose occurrences in $T$\ncover all positions of $T$. A seed of text $T$ is a cover of a superstring of\n$T$. In various applications exact quasiperiodicity is still not sufficient due\nto the presence of errors. We consider approximate notions of quasiperiodicity,\nfor which we allow approximate occurrences in $T$ with a small Hamming,\nLevenshtein or weighted edit distance.\n  In previous work Sip et al. (2002) and Christodoulakis et al. (2005) showed\nthat computing approximate covers and seeds, respectively, under weighted edit\ndistance is NP-hard. They, therefore, considered restricted approximate covers\nand seeds which need to be factors of the original string $T$ and presented\npolynomial-time algorithms for computing them. Further algorithms, considering\napproximate occurrences with Hamming distance bounded by $k$, were given in\nseveral contributions by Guth et al. They also studied relaxed approximate\nquasiperiods that do not need to cover all positions of $T$.\n  In case of large data the exponents in polynomial time complexity play a\ncrucial role. We present more efficient algorithms for computing restricted\napproximate covers and seeds. In particular, we improve upon the complexities\nof many of the aforementioned algorithms, also for relaxed quasiperiods. Our\nsolutions are especially efficient if the number (or total cost) of allowed\nerrors is bounded. We also show NP-hardness of computing non-restricted\napproximate covers and seeds under Hamming distance.\n  Approximate covers were studied in three recent contributions at CPM over the\nlast three years. However, these works consider a different definition of an\napproximate cover of $T$.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 13:58:13 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["K\u0119dzierski", "Aleksander", ""], ["Radoszewski", "Jakub", ""]]}, {"id": "2005.06344", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok", "title": "A remark on approximating permanents of positive definite matrices", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $A$ be an $n \\times n$ positive definite Hermitian matrix with all\neigenvalues between 1 and 2. We represent the permanent of $A$ as the integral\nof some explicit log-concave function on ${\\Bbb R}^{2n}$. Consequently, there\nis a fully polynomial randomized approximation scheme (FPRAS) for the permanent\nof $A$.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 14:39:12 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Barvinok", "Alexander", ""]]}, {"id": "2005.06417", "submitter": "Sushrut Karmalkar", "authors": "Ilias Diakonikolas, Samuel B. Hopkins, Daniel Kane, Sushrut Karmalkar", "title": "Robustly Learning any Clusterable Mixture of Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the efficient learnability of high-dimensional Gaussian mixtures in\nthe outlier-robust setting, where a small constant fraction of the data is\nadversarially corrupted. We resolve the polynomial learnability of this problem\nwhen the components are pairwise separated in total variation distance.\nSpecifically, we provide an algorithm that, for any constant number of\ncomponents $k$, runs in polynomial time and learns the components of an\n$\\epsilon$-corrupted $k$-mixture within information theoretically near-optimal\nerror of $\\tilde{O}(\\epsilon)$, under the assumption that the overlap between\nany pair of components $P_i, P_j$ (i.e., the quantity $1-TV(P_i, P_j)$) is\nbounded by $\\mathrm{poly}(\\epsilon)$.\n  Our separation condition is the qualitatively weakest assumption under which\naccurate clustering of the samples is possible. In particular, it allows for\ncomponents with arbitrary covariances and for components with identical means,\nas long as their covariances differ sufficiently. Ours is the first polynomial\ntime algorithm for this problem, even for $k=2$.\n  Our algorithm follows the Sum-of-Squares based proofs to algorithms approach.\nOur main technical contribution is a new robust identifiability proof of\nclusters from a Gaussian mixture, which can be captured by the constant-degree\nSum of Squares proof system. The key ingredients of this proof are a novel use\nof SoS-certifiable anti-concentration and a new characterization of pairs of\nGaussians with small (dimension-independent) overlap in terms of their\nparameter distance.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:44:12 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Hopkins", "Samuel B.", ""], ["Kane", "Daniel", ""], ["Karmalkar", "Sushrut", ""]]}, {"id": "2005.06441", "submitter": "Rajesh Jayaram", "authors": "Ainesh Bakshi, Nadiia Chepurko, Rajesh Jayaram", "title": "Testing Positive Semi-Definiteness via Random Submatrices", "comments": "Minor Edits, highlighted connection between \\ell_\\infty gap and\n  spectral norm gap", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing whether a matrix $\\mathbf{A} \\in\n\\mathbb{R}^{n \\times n}$ with bounded entries ($\\|\\mathbf{A}\\|_\\infty \\leq 1$)\nis positive semi-definite (PSD), or $\\epsilon$-far in Euclidean distance from\nthe PSD cone, meaning that $\\min_{\\mathbf{B} \\succeq 0} \\|\\mathbf{A} -\n\\mathbf{B}\\|_F^2 > \\epsilon n^2$, where $\\mathbf{B} \\succeq 0$ denotes that\n$\\mathbf{B}$ is PSD. Our main algorithmic contribution is a non-adaptive tester\nwhich distinguishes between these cases using only $\\tilde{O}(1/\\epsilon^4)$\nqueries to the entries of $\\mathbf{A}$. If instead of the Euclidean norm we\nconsidered the distance in spectral norm, we obtain the \"$\\ell_\\infty$-gap\nproblem\", where $\\mathbf{A}$ is either PSD or satisfies\n$\\min_{\\mathbf{B}\\succeq 0} \\|\\mathbf{A}- \\mathbf{B}\\|_2 > \\epsilon n$. For\nthis related problem, we give a $\\tilde{O}(1/\\epsilon^2)$ query tester, which\nwe show is optimal up to $\\log(1/\\epsilon)$ factors. Our testers randomly\nsample a collection of principal submatrices and check whether these\nsubmatrices are PSD. Consequentially, our algorithms achieve one-sided error:\nwhenever they output that $\\mathbf{A}$ is not PSD, they return a certificate\nthat $\\mathbf{A}$ has negative eigenvalues.\n  We complement our upper bound for PSD testing with Euclidean norm distance by\ngiving a $\\tilde{\\Omega}(1/\\epsilon^2)$ lower bound for any non-adaptive\nalgorithm. Our lower bound construction is general, and can be used to derive\nlower bounds for a number of spectral testing problems. As an example of the\napplicability of our construction, we obtain a new\n$\\tilde{\\Omega}(1/\\epsilon^4)$ sampling lower bound for testing the\nSchatten-$1$ norm with a $\\epsilon n^{1.5}$ gap, extending a result of Balcan,\nLi, Woodruff, and Zhang [SODA'19]. In addition, it yields new sampling lower\nbounds for estimating the Ky-Fan Norm, and the cost of the best rank-$k$\napproximation.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 17:30:12 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 14:23:07 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 19:10:23 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Chepurko", "Nadiia", ""], ["Jayaram", "Rajesh", ""]]}, {"id": "2005.06528", "submitter": "Magnus M. Halldorsson", "authors": "Magnus M. Halldorsson, Fabian Kuhn, Yannic Maus", "title": "Distance-2 Coloring in the CONGEST Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give efficient randomized and deterministic distributed algorithms for\ncomputing a distance-$2$ vertex coloring of a graph $G$ in the CONGEST model.\nIn particular, if $\\Delta$ is the maximum degree of $G$, we show that there is\na randomized CONGEST model algorithm to compute a distance-$2$ coloring of $G$\nwith $\\Delta^2+1$ colors in $O(\\log\\Delta\\cdot\\log n)$ rounds. Further if the\nnumber of colors is slightly increased to $(1+\\epsilon)\\Delta^2$ for some\n$\\epsilon>1/{\\rm polylog}(n)$, we show that it is even possible to compute a\ndistance-$2$ coloring deterministically in polylog$(n)$ time in the CONGEST\nmodel. Finally, we give a $O(\\Delta^2 + \\log^* n)$-round deterministic CONGEST\nalgorithm to compute distance-$2$ coloring with $\\Delta^2+1$ colors.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:58:48 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Halldorsson", "Magnus M.", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "2005.06665", "submitter": "Diego Pennino", "authors": "Gianmaria Del Monte, Diego Pennino, Maurizio Pizzonia", "title": "Scaling Blockchains Without Giving up Decentralization and Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public blockchains should be able to scale with respect to the number of\nnodes and to the transactions workload. The blockchain scalability trilemma has\nbeen informally conjectured. This is related to scalability, security and\ndecentralization, stating that any improvement in one of these aspects should\nnegatively impact on at least one of the other twos. In fact, despite the large\nresearch and experimental effort, all known approaches turn out to be\ntradeoffs. We theoretically describe a new blockchain architecture that scales\nto arbitrarily high workload provided that a corresponding proportional\nincrement of nodes is provisioned. We show that, under reasonable assumptions,\nour approach does not require tradeoffs on security or decentralization. To the\nbest of our knowledge, this is the first result that disprove the trilemma\nconsidering the scalability of all architectural elements of a blockchain and\nnot only the consensus protocol. While our result is currently only theoretic,\nwe believe that ot our approach may stimulate significant practical\ncontributions.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 23:39:40 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 19:20:00 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Del Monte", "Gianmaria", ""], ["Pennino", "Diego", ""], ["Pizzonia", "Maurizio", ""]]}, {"id": "2005.06827", "submitter": "Stefan Neubert", "authors": "Katrin Casel, Tobias Friedrich, Stefan Neubert and Markus L. Schmid", "title": "Shortest Distances as Enumeration Problem", "comments": "Updated version adds the study of space complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the single source shortest distance (SSSD) and all pairs\nshortest distance (APSD) problems as enumeration problems (on unweighted and\ninteger weighted graphs), meaning that the elements $(u, v, d(u, v))$ -- where\n$u$ and $v$ are vertices with shortest distance $d(u, v)$ -- are produced and\nlisted one by one without repetition. The performance is measured in the RAM\nmodel of computation with respect to preprocessing time and delay, i.e., the\nmaximum time that elapses between two consecutive outputs. This point of view\nreveals that specific types of output (e.g., excluding the non-reachable pairs\n$(u, v, \\infty)$, or excluding the self-distances $(u, u, 0)$) and the order of\nenumeration (e.g., sorted by distance, sorted row-wise with respect to the\ndistance matrix) have a huge impact on the complexity of APSD while they appear\nto have no effect on SSSD.\n  In particular, we show for APSD that enumeration without output restrictions\nis possible with delay in the order of the average degree. Excluding\nnon-reachable pairs, or requesting the output to be sorted by distance,\nincreases this delay to the order of the maximum degree. Further, for weighted\ngraphs, a delay in the order of the average degree is also not possible without\npreprocessing or considering self-distances as output. In contrast, for SSSD we\nfind that a delay in the order of the maximum degree without preprocessing is\nattainable and unavoidable for any of these requirements.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 09:14:58 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 12:50:20 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Casel", "Katrin", ""], ["Friedrich", "Tobias", ""], ["Neubert", "Stefan", ""], ["Schmid", "Markus L.", ""]]}, {"id": "2005.07030", "submitter": "Juan Ignacio Mulero-Mart\\'inez", "authors": "Juan Ignacio Mulero-Mart\\'inez", "title": "A Polynomial-Time Algorithm for Unconstrained Binary Quadratic\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an exact algorithm in polynomial time is developed to solve\nunrestricted binary quadratic programs. The computational complexity is\n$O\\left( n^{\\frac{15}{2}}\\right) $, although very conservative, it is\nsufficient to prove that this minimization problem is in the complexity class\n$P$. The implementation aspects are also described in detail with a special\nemphasis on the transformation of the quadratic program into a linear program\nthat can be solved in polynomial time. The algorithm was implemented in MATLAB\nand checked by generating five million matrices of arbitrary dimensions up to\n30 with random entries in the range $\\left[ -50,50\\right] $. All the\nexperiments carried out have revealed that the method works correctly.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 09:56:21 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 21:17:52 GMT"}, {"version": "v3", "created": "Sat, 23 May 2020 14:58:38 GMT"}, {"version": "v4", "created": "Sat, 30 May 2020 09:08:35 GMT"}, {"version": "v5", "created": "Sat, 13 Jun 2020 16:00:27 GMT"}, {"version": "v6", "created": "Sat, 30 Jan 2021 18:31:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Mulero-Mart\u00ednez", "Juan Ignacio", ""]]}, {"id": "2005.07235", "submitter": "Antonio Mora Dr.", "authors": "A.M. Mora, A.I. Esparcia-Alc\\'azar", "title": "Evo* 2020 -- Late-Breaking Abstracts Volume", "comments": "LBAs accepted in Evo* 2020. Part of the Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the Late-Breaking Abstracts submitted to the Evo* 2020\nConference, that took place online, from 15 to 17 of April 2020. These papers\nwhere presented as short talks and also at the poster session of the conference\ntogether with other regular submissions. All of them present ongoing research\nand preliminary results investigating on the application of different\napproaches of Bioinspired Methods (mainly Evolutionary Computation) to\ndifferent problems, most of them real world ones.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 19:37:34 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Mora", "A. M.", ""], ["Esparcia-Alc\u00e1zar", "A. I.", ""]]}, {"id": "2005.07373", "submitter": "Reza Fathi", "authors": "Reza Fathi, Anisur Rahaman Molla, Gopal Pandurangan", "title": "Efficient Distributed Algorithms for the $K$-Nearest Neighbors Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $K$-nearest neighbors is a basic problem in machine learning with\nnumerous applications. In this problem, given a (training) set of $n$ data\npoints with labels and a query point $p$, we want to assign a label to $p$\nbased on the labels of the $K$-nearest points to the query. We study this\nproblem in the {\\em $k$-machine model}, (Note that parameter $k$ stands for the\nnumber of machines in the $k$-machine model and is independent of $K$-nearest\npoints.) a model for distributed large-scale data. In this model, we assume\nthat the $n$ points are distributed (in a balanced fashion) among the $k$\nmachines and the goal is to quickly compute answer given a query point to a\nmachine.\n  Our main result is a simple randomized algorithm in the $k$-machine model\nthat runs in $O(\\log K)$ communication rounds with high probability success\n(regardless of the number of machines $k$ and the number of points $n$). The\nmessage complexity of the algorithm is small taking only $O(k\\log K)$ messages.\nOur bounds are essentially the best possible for comparison-based algorithms\n(Algorithms that use only comparison operations ($\\leq, \\geq, =$) between\nelements to distinguish the ordering among them). This is due to the existence\nof a lower bound of $\\Omega(\\log n)$ communication rounds for finding the {\\em\nmedian} of $2n$ elements distributed evenly among two processors by Rodeh\n\\cite{rodeh}.\n  We also implemented our algorithm and show that it performs well compared to\nan algorithm (used in practice) that sends $K$ nearest points from each machine\nto a single machine which then computes the answer.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 06:24:43 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 01:52:20 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2020 02:53:00 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Fathi", "Reza", ""], ["Molla", "Anisur Rahaman", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "2005.07644", "submitter": "Sebastian Wild", "authors": "Meng He, J. Ian Munro, Yakov Nekrich, Sebastian Wild, Kaiyu Wu", "title": "Distance Oracles for Interval Graphs via Breadth-First Rank/Select in\n  Succinct Trees", "comments": "to appear in ISAAC 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.ISAAC.2020.57", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first succinct distance oracles for (unweighted) interval\ngraphs and related classes of graphs, using a novel succinct data structure for\nordinal trees that supports the mapping between preorder (i.e., depth-first)\nranks and level-order (breadth-first) ranks of nodes in constant time. Our\ndistance oracles for interval graphs also support navigation queries -- testing\nadjacency, computing node degrees, neighborhoods, and shortest paths -- all in\noptimal time. Our technique also yields optimal distance oracles for proper\ninterval graphs (unit-interval graphs) and circular-arc graphs. Our tree data\nstructure supports all operations provided by different approaches in previous\nwork, as well as mapping to and from level-order ranks and retrieving the last\n(first) internal node before (after) a given node in a level-order traversal,\nall in constant time.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 16:58:51 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 20:09:30 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["He", "Meng", ""], ["Munro", "J. Ian", ""], ["Nekrich", "Yakov", ""], ["Wild", "Sebastian", ""], ["Wu", "Kaiyu", ""]]}, {"id": "2005.07652", "submitter": "Omar Montasser", "authors": "Omar Montasser, Surbhi Goel, Ilias Diakonikolas, Nathan Srebro", "title": "Efficiently Learning Adversarially Robust Halfspaces with Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning adversarially robust halfspaces in the\ndistribution-independent setting. In the realizable setting, we provide\nnecessary and sufficient conditions on the adversarial perturbation sets under\nwhich halfspaces are efficiently robustly learnable. In the presence of random\nlabel noise, we give a simple computationally efficient algorithm for this\nproblem with respect to any $\\ell_p$-perturbation.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:13:54 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Montasser", "Omar", ""], ["Goel", "Surbhi", ""], ["Diakonikolas", "Ilias", ""], ["Srebro", "Nathan", ""]]}, {"id": "2005.07678", "submitter": "Alexandr Andoni", "authors": "Alexandr Andoni, Negev Shekel Nosatzki", "title": "Edit Distance in Near-Linear Time: it's a Constant Factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for approximating the edit distance between two\nstrings of length $n$ in time $n^{1+\\epsilon}$, for any $\\epsilon>0$, up to a\nconstant factor. Our result completes the research direction set forth in the\nrecent breakthrough paper [Chakraborty-Das-Goldenberg-Koucky-Saks, FOCS'18],\nwhich showed the first constant-factor approximation algorithm with a\n(strongly) sub-quadratic running time. Several recent results have shown\nnear-linear complexity under different restrictions on the inputs (eg, when the\nedit distance is close to maximal, or when one of the inputs is pseudo-random).\nIn contrast, our algorithm obtains a constant-factor approximation in\nnear-linear running time for any input strings.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:48:44 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Andoni", "Alexandr", ""], ["Nosatzki", "Negev Shekel", ""]]}, {"id": "2005.07859", "submitter": "Ali Pourmiri", "authors": "Ali Pourmiri, Bernard Mans", "title": "Tight Analysis of Asynchronous Rumor Spreading in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asynchronous rumor algorithm spreading propagates a piece of information,\nthe so-called rumor, in a network. Starting with a single informed node, each\nnode is associated with an exponential time clock with rate $1$ and calls a\nrandom neighbor in order to possibly exchange the rumor. Spread time is the\nfirst time when all nodes of a network are informed with high probability. We\nconsider spread time of the algorithm in any dynamic evolving network,\n$\\mathcal{G}=\\{G^{(t)}\\}_{t=0}^{\\infty}$, which is a sequence of graphs exposed\nat discrete time step $t=0,1\\ldots$. We observe that besides the expansion\nprofile of a dynamic network, the degree distribution of nodes over time effect\nthe spread time. We establish upper bounds for the spread time in terms of\ngraph conductance and diligence. For a given connected simple graph $G=(V,E)$,\nthe diligence of cut set $E(S, \\overline{S})$ is defined as\n$\\rho(S)=\\min_{\\{u,v\\}\\in E(S,\\overline{S})}\\max\\{\\bar{d}/d_u, \\bar{d}/d_v\\}$\nwhere $d_u$ is the degree of $u$ and $\\bar{d}$ is the average degree of nodes\nin the one side of the cut with smaller volume (i.e.,\n${\\mathtt{vol}}{(S)}=\\sum_{u\\in S}d_u$). The diligence of $G$ is also defined\nas $\\rho(G)=\\min_{ \\emptyset\\neq S\\subset V}\\rho(S)$. We show that the spread\ntime of the algorithm in $\\mathcal{G}$ is bounded by $T$, where $T$ is the\nfirst time that $\\sum_{t=0}^T\\Phi(G^{(t)})\\cdot\\rho(G^{(t)})$ exceeds $C\\log\nn$, where $\\Phi(G^{(t)})$ denotes the conductance of $G^{(t)}$ and $C$ is a\nspecified constant. We also define the absolute diligence as\n$\\overline{\\rho}(G)=\\min_{\\{u,v\\}\\in E}\\max\\{1/d_u,1/d_v\\}$ and establish upper\nbound $T$ for the spread time in terms of absolute diligence, which is the\nfirst time when $\\sum_{t=0}^T\\lceil\\Phi(G^{(t)})\\rceil\\cdot\n\\overline{\\rho}(G^{(t)})\\ge 2n$. We present dynamic networks where the given\nupper bounds are almost tight.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 03:42:37 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Pourmiri", "Ali", ""], ["Mans", "Bernard", ""]]}, {"id": "2005.07944", "submitter": "Mark Jerrum", "authors": "Martin Dyer, Marc Heinrich, Mark Jerrum and Haiko M\\\"uller", "title": "Polynomial-time approximation algorithms for the antiferromagnetic Ising\n  model on line graphs", "comments": "Minor revisions. The version is accepted for publication in\n  Combinatorics, Probability and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a polynomial-time Markov chain Monte Carlo algorithm for\nestimating the partition function of the antiferromagnetic Ising model on any\nline graph. The analysis of the algorithm exploits the \"winding\" technology\ndevised by McQuillan [CoRR abs/1301.2880 (2013)] and developed by Huang, Lu and\nZhang [Proc. 27th Symp. on Disc. Algorithms (SODA16), 514-527]. We show that\nexact computation of the partition function is #P-hard, even for line graphs,\nindicating that an approximation algorithm is the best that can be expected. We\nalso show that Glauber dynamics for the Ising model is rapidly mixing on line\ngraphs, an example being the kagome lattice.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 10:42:54 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 13:25:21 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dyer", "Martin", ""], ["Heinrich", "Marc", ""], ["Jerrum", "Mark", ""], ["M\u00fcller", "Haiko", ""]]}, {"id": "2005.08058", "submitter": "Jasine Babu", "authors": "Jasine Babu and Veena Prabhakaran and Arko Sharma", "title": "A Linear Time Algorithm for Computing the Eternal Vertex Cover Number of\n  Cactus Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eternal vertex cover problem is a dynamic variant of the classical vertex\ncover problem. It is NP-hard to compute the eternal vertex cover number of\ngraphs and known algorithmic results for the problem are very few. This paper\npresents a linear time recursive algorithm for computing the eternal vertex\ncover number of cactus graphs. Unlike other graph classes for which polynomial\ntime algorithms for eternal vertex cover number are based on efficient\ncomputability of a known lower bound directly derived from minimum vertex\ncover, we show that it is a certain substructure property that helps the\nefficient computation of eternal vertex cover number of cactus graphs. An\nextension of the result to graphs in which each block is an edge, a cycle or a\nbiconnected chordal graph is also presented.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 18:10:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Babu", "Jasine", ""], ["Prabhakaran", "Veena", ""], ["Sharma", "Arko", ""]]}, {"id": "2005.08137", "submitter": "Arun Ganesh", "authors": "Arun Ganesh, Bruce M. Maggs, Debmalya Panigrahi", "title": "Robust Algorithms for TSP and Steiner Tree", "comments": "39 pages. An extended abstract of this paper appeared in the\n  Proceedings of the 47th International Colloquium on Automata, Languages, and\n  Programming (ICALP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust optimization is a widely studied area in operations research, where\nthe algorithm takes as input a range of values and outputs a single solution\nthat performs well for the entire range. Specifically, a robust algorithm aims\nto minimize regret, defined as the maximum difference between the solution's\ncost and that of an optimal solution in hindsight once the input has been\nrealized. For graph problems in P, such as shortest path and minimum spanning\ntree, robust polynomial-time algorithms that obtain a constant approximation on\nregret are known. In this paper, we study robust algorithms for minimizing\nregret in NP-hard graph optimization problems, and give constant approximations\non regret for the classical traveling salesman and Steiner tree problems.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 00:40:32 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ganesh", "Arun", ""], ["Maggs", "Bruce M.", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "2005.08150", "submitter": "Pallavi Jain", "authors": "Sushmita Gupta, Pallavi Jain, Sanjukta Roy, Saket Saurabh, Meirav\n  Zehavi", "title": "On the (Parameterized) Complexity of Almost Stable Marriage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Stable Marriage problem. when the preference lists are complete, all\nagents of the smaller side can be matched. However, this need not be true when\npreference lists are incomplete. In most real-life situations, where agents\nparticipate in the matching market voluntarily and submit their preferences, it\nis natural to assume that each agent wants to be matched to someone in his/her\npreference list as opposed to being unmatched. In light of the Rural Hospital\nTheorem, we have to relax the \"no blocking pair\" condition for stable matchings\nin order to match more agents. In this paper, we study the question of matching\nmore agents with fewest possible blocking edges. In particular, we find a\nmatching whose size exceeds that of stable matching in the graph by at least t\nand has at most k blocking edges. We study this question in the realm of\nparameterized complexity with respect to several natural parameters, k,t,d,\nwhere d is the maximum length of a preference list. Unfortunately, the problem\nremains intractable even for the combined parameter k+t+d. Thus, we extend our\nstudy to the local search variant of this problem, in which we search for a\nmatching that not only fulfills each of the above conditions but is \"closest\",\nin terms of its symmetric difference to the given stable matching, and obtain\nan FPT algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 02:51:19 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Gupta", "Sushmita", ""], ["Jain", "Pallavi", ""], ["Roy", "Sanjukta", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "2005.08190", "submitter": "Shunsuke Inenaga", "authors": "Akihiro Nishi, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai,\n  Masayuki Takeda", "title": "Towards Efficient Interactive Computation of Dynamic Time Warping\n  Distance", "comments": "Accepted for SPIRE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic time warping (DTW) is a widely-used method that allows us to\nefficiently compare two time series that can vary in speed. Given two strings\n$A$ and $B$ of respective lengths $m$ and $n$, there is a fundamental dynamic\nprogramming algorithm that computes the DTW distance for $A$ and $B$ together\nwith an optimal alignment in $\\Theta(mn)$ time and space. In this paper, we\ntackle the problem of interactive computation of the DTW distance for dynamic\nstrings, denoted $\\mathrm{D^2TW}$, where character-wise edit operation\n(insertion, deletion, substitution) can be performed at an arbitrary position\nof the strings. Let $M$ and $N$ be the sizes of the run-length encoding (RLE)\nof $A$ and $B$, respectively. We present an algorithm for $\\mathrm{D^2TW}$ that\noccupies $\\Theta(mN+nM)$ space and uses $O(m+n+\\#_{\\mathrm{chg}}) \\subseteq\nO(mN + nM)$ time to update a compact differential representation $\\mathit{DS}$\nof the DP table per edit operation, where $\\#_{\\mathrm{chg}}$ denotes the\nnumber of cells in $\\mathit{DS}$ whose values change after the edit operation.\nOur method is at least as efficient as the algorithm recently proposed by\nFroese et al. running in $\\Theta(mN + nM)$ time, and is faster when\n$\\#_{\\mathrm{chg}}$ is smaller than $O(mN + nM)$ which, as our preliminary\nexperiments suggest, is likely to be the case in the majority of instances.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 08:14:43 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 03:15:34 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 14:04:42 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Nishi", "Akihiro", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "2005.08232", "submitter": "Dana Shapira", "authors": "Aharon Fruchtman, Yoav Gross, Shmuel T. Klein and Dana Shapira", "title": "Weighted Adaptive Coding", "comments": "18 pages, 8 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huffman coding is known to be optimal, yet its dynamic version may be even\nmore efficient in practice. A new variant of Huffman encoding has been proposed\nrecently, that provably always performs better than static Huffman coding by at\nleast $m-1$ bits, where $m$ denotes the size of the alphabet, and has a better\nworst case than the standard dynamic Huffman coding. This paper introduces a\nnew generic coding method, extending the known static and dynamic variants and\nincluding them as special cases. In fact, the generalization is applicable to\nall statistical methods, including arithmetic coding. This leads then to the\nformalization of a new adaptive coding method, which is provably always at\nleast as good as the best dynamic variant known to date. Moreover, we present\nempirical results that show improvements over static and dynamic Huffman and\narithmetic coding achieved by the proposed method, even when the encoded file\nincludes the model description.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 12:00:26 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Fruchtman", "Aharon", ""], ["Gross", "Yoav", ""], ["Klein", "Shmuel T.", ""], ["Shapira", "Dana", ""]]}, {"id": "2005.08243", "submitter": "Gunnar Brinkmann", "authors": "G. Brinkmann", "title": "A Practical Algorithm for the Computation of the Genus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a practical algorithm to compute the (oriented) genus of a graph,\ngive results of the program implementing this algorithm, and compare the\nperformance to existing algorithms. The aim of this algorithm is to be fast\nenough for many applications instead of focusing on the theoretical asymptotic\ncomplexity. Apart from the specific problem and the results, the article can\nalso be seen as an example how some design principles used to carefully develop\nand implement standard backtracking algorithms can still result in very\ncompetitive programs.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 12:39:25 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Brinkmann", "G.", ""]]}, {"id": "2005.08263", "submitter": "George Mertzios", "authors": "Eleni C. Akrida, Argyrios Deligkas, George B. Mertzios, Paul G.\n  Spirakis, and Viktor Zamaraev", "title": "Matching in Stochastically Evolving Graphs", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the maximum cardinality matching problem in stochastically\nevolving graphs. We formally define the arrival-departure model with stochastic\ndepartures. There, a graph is sampled from a specific probability distribution\nand it is revealed as a series of snapshots. Our goal is to study algorithms\nthat create a large matching in the sampled graphs. We define the price of\nstochasticity for this problem which intuitively captures the loss of any\nalgorithm in the worst case in the size of the matching due to the uncertainty\nof the model. Furthermore, we prove the existence of a deterministic optimal\nalgorithm for the problem. In our second set of results we show that we can\nefficiently approximate the expected size of a maximum cardinality matching by\nderiving a fully randomized approximation scheme (FPRAS) for it. The FPRAS is\nthe backbone of a probabilistic algorithm that is optimal when the model is\ndefined over two timesteps. Our last result is an upper bound of $\\frac{2}{3}$\non the price of stochasticity. This means that there is no algorithm that can\nmatch more than $\\frac{2}{3}$ of the edges of an optimal matching in hindsight.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 14:26:46 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Akrida", "Eleni C.", ""], ["Deligkas", "Argyrios", ""], ["Mertzios", "George B.", ""], ["Spirakis", "Paul G.", ""], ["Zamaraev", "Viktor", ""]]}, {"id": "2005.08301", "submitter": "David Harris", "authors": "Anupam Gupta, David G. Harris, Euiwoong Lee, Jason Li", "title": "Optimal Bounds for the $k$-cut Problem", "comments": "Final version of arXiv:1911.09165 with new and more general proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-cut problem, we want to find the smallest set of edges whose\ndeletion breaks a given (multi)graph into $k$ connected components. Algorithms\nof Karger & Stein and Thorup showed how to find such a minimum $k$-cut in time\napproximately $O(n^{2k})$. The best lower bounds come from conjectures about\nthe solvability of the $k$-clique problem, and show that solving $k$-cut is\nlikely to require time $\\Omega(n^k)$. Recent results of Gupta, Lee & Li have\ngiven special-purpose algorithms that solve the problem in time $n^{1.98k +\nO(1)}$, and ones that have better performance for special classes of graphs\n(e.g., for small integer weights).\n  In this work, we resolve the problem for general graphs, by showing that the\nContraction Algorithm of Karger outputs any fixed $k$-cut of weight $\\alpha\n\\lambda_k$ with probability $\\Omega_k(n^{-\\alpha k})$, where $\\lambda_k$\ndenotes the minimum $k$-cut size. This also gives an extremal bound of\n$O_k(n^k)$ on the number of minimum $k$-cuts and an algorithm to compute a\nminimum $k$-cut in similar runtime. Both are tight up to lower-order factors,\nwith the algorithmic lower bound assuming hardness of max-weight $k$-clique.\n  The first main ingredient in our result is a fine-grained analysis of how the\ngraph shrinks -- and how the average degree evolves -- in the Karger process.\nThe second ingredient is an extremal bound on the number of cuts of size less\nthan $2 \\lambda_k/k$, using the Sunflower lemma.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 17:08:35 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 21:19:19 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Gupta", "Anupam", ""], ["Harris", "David G.", ""], ["Lee", "Euiwoong", ""], ["Li", "Jason", ""]]}, {"id": "2005.08351", "submitter": "Abdullah Almethen", "authors": "Abdullah Almethen, Othon Michail, Igor Potapov", "title": "On Efficient Connectivity-Preserving Transformations in a Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a discrete system of $n$ devices lying on a 2-dimensional square\ngrid and forming an initial connected shape $S_I$. Each device is equipped with\na linear-strength mechanism which enables it to move a whole line of\nconsecutive devices in a single time-step. We study the problem of transforming\n$S_I$ into a given connected target shape $S_F$ of the same number of devices,\nvia a finite sequence of \\emph{line moves}. Our focus is on designing\n\\emph{centralised} transformations aiming at \\emph{minimising the total number\nof moves} subject to the constraint of \\emph{preserving connectivity} of the\nshape throughout the course of the transformation. We first give very fast\nconnectivity-preserving transformations for the case in which the\n\\emph{associated graphs} of $ S_I $ and $ S_F $ are isomorphic to a Hamiltonian\nline. In particular, our transformations make $ O(n \\log n $) moves, which is\nasymptotically equal to the best known running time of connectivity-breaking\ntransformations. Our most general result is then a connectivity-preserving\n\\emph{universal transformation} that can transform any initial connected shape\n$ S_I $ into any target connected shape $ S_F $, through a sequence of\n$O(n\\sqrt{n})$ moves. Finally, we establish $\\Omega(n \\log n)$ lower bounds for\ntwo restricted sets of transformations. These are the first lower bounds for\nthis model and are matching the best known $ O(n \\log n) $ upper bounds.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 19:34:00 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Almethen", "Abdullah", ""], ["Michail", "Othon", ""], ["Potapov", "Igor", ""]]}, {"id": "2005.08391", "submitter": "Till Knollmann", "authors": "Jannik Castenow, Bj\\\"orn Feldkord, Till Knollmann, Manuel Malatyali,\n  Friedhelm Meyer auf der Heide", "title": "The Online Multi-Commodity Facility Location Problem", "comments": "A conference version of this paper was accepted at the 32nd ACM\n  Symposium on Parallelism in Algorithms and Architectures (SPAA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a natural extension to the metric uncapacitated Facility Location\nProblem (FLP) in which requests ask for different commodities out of a finite\nset $S$ of commodities. Ravi and Sinha (SODA'04) introduced the model as the\nMulti-Commodity Facility Location Problem (MFLP) and considered it an offline\noptimization problem. The model itself is similar to the FLP: i.e., requests\nare located at points of a finite metric space and the task of an algorithm is\nto construct facilities and assign requests to facilities while minimizing the\nconstruction cost and the sum over all assignment distances. In addition,\nrequests and facilities are heterogeneous; they request or offer multiple\ncommodities out of $S$. A request has to be connected to a set of facilities\njointly offering the commodities demanded by it. In comparison to the FLP, an\nalgorithm has to decide not only if and where to place facilities, but also\nwhich commodities to offer at each.\n  To the best of our knowledge we are the first to study the problem in its\nonline variant in which requests, their positions and their commodities are not\nknown beforehand but revealed over time. We present results regarding the\ncompetitive ratio. On the one hand, we show that heterogeneity influences the\ncompetitive ratio by developing a lower bound on the competitive ratio for any\nrandomized online algorithm of $\\Omega(\\sqrt{|S|}+\\frac{\\log n}{\\log\\log n})$\nthat already holds for simple line metrics. Here, $n$ is the number of\nrequests. On the other side, we establish a deterministic\n$O(\\sqrt{|S|}\\cdot\\log n)$-competitive algorithm and a randomized\n$O(\\sqrt{|S|}\\cdot\\frac{\\log n}{\\log\\log n})$-competitive algorithm. Further,\nwe show that when considering a more special class of cost functions for the\nconstruction cost of a facility, the competitive ratio decreases given by our\ndeterministic algorithm depending on the function.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 23:01:39 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 08:56:26 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Castenow", "Jannik", ""], ["Feldkord", "Bj\u00f6rn", ""], ["Knollmann", "Till", ""], ["Malatyali", "Manuel", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "2005.08584", "submitter": "Simon Mauras", "authors": "Simon Mauras", "title": "Two-Sided Random Matching Markets: Ex-Ante Equivalence of the Deferred\n  Acceptance Procedures", "comments": "Accepted for publication in the 21st ACM Conference on Economics and\n  Computation (EC'20)", "journal-ref": null, "doi": "10.1145/3391403.3399448", "report-no": null, "categories": "cs.GT cs.DM cs.DS econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable matching in a community consisting of $N$ men and $N$ women is a\nclassical combinatorial problem that has been the subject of intense\ntheoretical and empirical study since its introduction in 1962 in a seminal\npaper by Gale and Shapley.\n  When the input preference profile is generated from a distribution, we study\nthe output distribution of two stable matching procedures:\nwomen-proposing-deferred-acceptance and men-proposing-deferred-acceptance. We\nshow that the two procedures are ex-ante equivalent: that is, under certain\nconditions on the input distribution, their output distributions are identical.\n  In terms of technical contributions, we generalize (to the non-uniform case)\nan integral formula, due to Knuth and Pittel, which gives the probability that\na fixed matching is stable. Using an inclusion-exclusion principle on the set\nof rotations, we give a new formula which gives the probability that a fixed\nmatching is the women/men-optimal stable matching. We show that those two\nprobabilities are equal with an integration by substitution.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 10:49:39 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mauras", "Simon", ""]]}, {"id": "2005.08614", "submitter": "Debasis Dwibedy", "authors": "Debasis Dwibedy, Rakesh Mohanty", "title": "Semi-online Scheduling: A Survey", "comments": "48 pages, 7 figures and 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online scheduling, jobs are available one by one and each job must be\nscheduled irrevocably before the availability of the next job. Semi-online\nscheduling is a relaxed variant of online scheduling, where an additional\nmemory in terms of buffer or an Extra Piece of Information(EPI) is provided\nalong with input data. The EPI may include one or more of the parameter(s) such\nas size of the largest job, total size of all jobs, arrival sequence of the\njobs, optimum makespan value or range of job's processing time. A semi-online\nscheduling algorithm was first introduced in 1997 by Kellerer et al. They\nenvisioned semi-online scheduling as a practically significant model and\nobtained improved results for $2$-identical machine setting. This paper surveys\nscholarly contributions in the design of semi-online scheduling algorithms in\nparallel machine models such as identical and uniformly related by considering\njob's processing formats such as preemptive and non-preemptive with the\noptimality criteria such as Min-Max and Max-Min. The main focus is to present\nstate of the art competitive analysis results of well-known semi-online\nscheduling algorithms in a chronological overview. The survey first introduces\nthe online and semi-online algorithmic frameworks for the multi-processor\nscheduling problem with important applications and research motivation,\nfollowed by a taxonomy for semi-online scheduling. Fifteen well-known\nsemi-online scheduling algorithms are stated. Important competitive analysis\nresults are presented in a chronological way by highlighting the critical ideas\nand intuition behind the results. An evolution time-line of semi-online\nscheduling setups and a classification of the references based on EPI are\noutlined. Finally, the survey concludes with the exploration of some of the\ninteresting research challenges and open problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:53:23 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Dwibedy", "Debasis", ""], ["Mohanty", "Rakesh", ""]]}, {"id": "2005.08856", "submitter": "Maciej Bendkowski", "authors": "Maciej Bendkowski", "title": "How to generate random lambda terms?", "comments": "Fixed typo in the S-combinator", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey several methods of generating large random lambda-terms, focusing\non their closed and simply-typed variants. We discuss methods of exact- and\napproximate-size generation, as well as methods of achieving size-uniform and\nnon-uniform outcome distributions.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:30:07 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 08:01:28 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Bendkowski", "Maciej", ""]]}, {"id": "2005.08918", "submitter": "Suprovat Ghoshal", "authors": "Suprovat Ghoshal and Anand Louis", "title": "Approximation Algorithms and Hardness for Strong Unique Games", "comments": "67 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UNIQUE GAMES problem is a central problem in algorithms and complexity\ntheory. Given an instance of UNIQUE GAMES, the STRONG UNIQUE GAMES problem asks\nto find the largest subset of vertices, such that the UNIQUE GAMES instance\ninduced on them is completely satisfiable. In this paper, we give new\nalgorithmic and hardness results for STRONG UNIQUE GAMES. Given an instance\nwith label set size $k$ where a set of $(1 - \\epsilon)$-fraction of the\nvertices induce an instance that is completely satisfiable, our first algorithm\nproduces a set of $1 - \\widetilde{O}({k^2}) \\epsilon \\sqrt{\\log n}$ fraction of\nthe vertices such that the UNIQUE GAMES induced on them is completely\nsatisfiable. In the same setting, our second algorithm produces a set of $1 -\n\\widetilde{O}({k^2}) \\sqrt{\\epsilon \\log d}$ (here $d$ is the largest vertex\ndegree of the graph) fraction of the vertices such that the UNIQUE GAMES\ninduced on them is completely satisfiable. The technical core of our results is\na new connection between STRONG UNIQUE GAMES and Small-Set-Vertex-Expansion in\ngraphs. Complementing this, assuming the Unique Games Conjecture, we prove that\nit is NP-hard to compute a set of size larger than $1 - \\Omega( \\sqrt{\\epsilon\n\\log k \\log d})$ for which all the constraints induced on this set are\nsatisfied.\n  Given an undirected graph $G(V,E)$ the ODD CYCLE TRANSVERSAL problem asks to\ndelete the least fraction of vertices to make the induced graph on the\nremaining vertices bipartite. As a corollary to our main algorithmic results,\nwe obtain an algorithm that outputs a set $S$ such the graph induced on $V\n\\setminus S$ is bipartite, and $|S|/n \\leq O(\\sqrt{\\epsilon \\log d})$ (here $d$\nis the largest vertex degree and $\\epsilon$ is the optimal fraction of vertices\nthat need to be deleted). Assuming the Unique Games Conjecture, we prove a\nmatching (up to constant factors) hardness.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:44:58 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ghoshal", "Suprovat", ""], ["Louis", "Anand", ""]]}, {"id": "2005.08950", "submitter": "Ayan Chattopadhyay", "authors": "Vikram Menon and Ayan Chattopadhyay", "title": "Quantum string comparison method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a quantum string comparison method whose main building blocks are\na specially designed oracle construction followed by Grover's search algorithm.\nThe purpose of the oracle is to compare all alphabets of the string in\nparallel. This requires a unique input state preparation, which when combined\nwith some ancillas will result in a deterministic binary success and failure\ncompare outcome.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 19:22:05 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Menon", "Vikram", ""], ["Chattopadhyay", "Ayan", ""]]}, {"id": "2005.08959", "submitter": "Alessandro Provetti", "authors": "Pasquale De Meo, Mark Levene and Alessandro Provetti", "title": "Potential gain as a centrality measure", "comments": "In Proceedings of Web Intelligence 2019 (WI19), the IEEE/WIC/ACM\n  International Conference on Web Intelligence, pages 418--422. arXiv admin\n  note: text overlap with arXiv:1812.08012", "journal-ref": null, "doi": "10.1145/3350546.3352559", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigability is a distinctive features of graphs associated with artificial\nor natural systems whose primary goal is the transportation of information or\ngoods. We say that a graph $\\mathcal{G}$ is navigable when an agent is able to\nefficiently reach any target node in $\\mathcal{G}$ by means of local routing\ndecisions. In a social network navigability translates to the ability of\nreaching an individual through personal contacts. Graph navigability is\nwell-studied, but a fundamental question is still open: why are some\nindividuals more likely than others to be reached via short,\nfriend-of-a-friend, communication chains? In this article we answer the\nquestion above by proposing a novel centrality metric called the potential\ngain, which, in an informal sense, quantifies the easiness at which a target\nnode can be reached. We define two variants of the potential gain, called the\ngeometric and the exponential potential gain, and present fast algorithms to\ncompute them. The geometric and the potential gain are the first instances of a\nnovel class of composite centrality metrics, i.e., centrality metrics which\ncombine the popularity of a node in $\\mathcal{G}$ with its similarity to all\nother nodes. As shown in previous studies, popularity and similarity are two\nmain criteria which regulate the way humans seek for information in large\nnetworks such as Wikipedia. We give a formal proof that the potential gain of a\nnode is always equivalent to the product of its degree centrality (which\ncaptures popularity) and its Katz centrality (which captures similarity).\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 13:17:27 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["De Meo", "Pasquale", ""], ["Levene", "Mark", ""], ["Provetti", "Alessandro", ""]]}, {"id": "2005.09152", "submitter": "Anqi Dong", "authors": "Anqi Dong, Amirhossein Taghvaei, Tryphon T. Georgiou", "title": "Lasso formulation of the shortest path problem", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The shortest path problem is formulated as an $l_1$-regularized regression\nproblem, known as lasso. Based on this formulation, a connection is established\nbetween Dijkstra's shortest path algorithm and the least angle regression\n(LARS) for the lasso problem. Specifically, the solution path of the lasso\nproblem, obtained by varying the regularization parameter from infinity to zero\n(the regularization path), corresponds to shortest path trees that appear in\nthe bi-directional Dijkstra algorithm. Although Dijkstra's algorithm and the\nLARS formulation provide exact solutions, they become impractical when the size\nof the graph is exceedingly large. To overcome this issue, the alternating\ndirection method of multipliers (ADMM) is proposed to solve the lasso\nformulation. The resulting algorithm produces good and fast approximations of\nthe shortest path by sacrificing exactness that may not be absolutely essential\nin many applications. Numerical experiments are provided to illustrate the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:16:01 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 18:04:43 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Dong", "Anqi", ""], ["Taghvaei", "Amirhossein", ""], ["Georgiou", "Tryphon T.", ""]]}, {"id": "2005.09169", "submitter": "Shunsuke Inenaga", "authors": "Yoshifumi Sakai and Shunsuke Inenaga", "title": "A faster reduction of the dynamic time warping distance to the longest\n  increasing subsequence length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The similarity between a pair of time series, i.e., sequences of indexed\nvalues in time order, is often estimated by the dynamic time warping (DTW)\ndistance, instead of any in the well-studied family of measures including the\nlongest common subsequence (LCS) length and the edit distance. Although it may\nseem as if the DTW and the LCS(-like) measures are essentially different, we\nreveal that the DTW distance can be represented by the longest increasing\nsubsequence (LIS) length of a sequence of integers, which is the LCS length\nbetween the integer sequence and itself sorted. For a given pair of time series\nof length $n$ such that the dissimilarity between any elements is an integer\nbetween zero and $c$, we propose an integer sequence that represents any\nsubstring-substring DTW distance as its band-substring LIS length. The length\nof the produced integer sequence is $O(c n^2)$, which can be translated to\n$O(n^2)$ for constant dissimilarity functions. To demonstrate that techniques\ndeveloped under the LCS(-like) measures are directly applicable to analysis of\ntime series via our reduction of DTW to LIS, we present time-efficient\nalgorithms for DTW-related problems utilizing the semi-local sequence\ncomparison technique developed for LCS-related problems.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 02:21:18 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 11:39:40 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sakai", "Yoshifumi", ""], ["Inenaga", "Shunsuke", ""]]}, {"id": "2005.09262", "submitter": "Manoj  Gupta", "authors": "Manoj Gupta, Rahul Jain, Nitiksha Modi", "title": "Multiple Source Replacement Path Problem", "comments": "Accepted in PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the classical line of work in graph algorithms has been the\nReplacement Path Problem: given a graph $G$, $s$ and $t$, find shortest paths\nfrom $s$ to $t$ avoiding each edge $e$ on the shortest path from $s$ to $t$.\nThese paths are called replacement paths in literature. For an undirected and\nunweighted graph, (Malik, Mittal, and Gupta, Operation Research Letters, 1989)\nand (Hershberger and Suri, FOCS 2001) designed an algorithm that solves the\nreplacement path problem in $\\tilde O(m+n)$ time. It is natural to ask whether\nwe can generalize the replacement path problem: {\\em can we find all\nreplacement paths from a source $s$ to all vertices in $G$?} This problem is\ncalled the Single Source Replacement Path Problem.\n  Recently (Chechik and Cohen, SODA 2019) designed a randomized combinatorial\nalgorithm that solves the Single Source Replacement Path Problem in $\\tilde\nO(m\\sqrt n\\ + n^2)$ time. One of the questions left unanswered by their work is\nthe case when there are many sources, not one. When there are $n$ sources, the\ncombinatorial algorithm of (Bernstein and Karger, STOC 2009) can be used to\nfind all pair replacement path in $\\tilde O(mn + n^3)$ time. However, there is\nno result known for any general $\\sigma$. Thus, the problem we study is defined\nas follows: given a set of $\\sigma$ sources, we want to find the replacement\npath from these sources to all vertices in $G$. We give a randomized\ncombinatorial algorithm for this problem that takes $\\tilde O(m\\sqrt{n \\sigma}\n+\\ \\sigma n^2)$ time. This result generalizes both results known for this\nproblem. Our algorithm is much different and arguably simpler than (Chechik and\nCohen, SODA 2019). Like them, we show a matching conditional lower bound using\nthe Boolean Matrix Multiplication conjecture.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 07:44:56 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 16:56:56 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Gupta", "Manoj", ""], ["Jain", "Rahul", ""], ["Modi", "Nitiksha", ""]]}, {"id": "2005.09342", "submitter": "Veli M\\\"akinen", "authors": "Veli M\\\"akinen, Bastien Cazaux, Massimo Equi, Tuukka Norri, and\n  Alexandru I. Tomescu", "title": "Linear Time Construction of Indexable Founder Block Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a compact pangenome representation based on an optimal\nsegmentation concept that aims to reconstruct founder sequences from a multiple\nsequence alignment (MSA). Such founder sequences have the feature that each row\nof the MSA is a recombination of the founders. Several linear time dynamic\nprogramming algorithms have been previously devised to optimize segmentations\nthat induce founder blocks that then can be concatenated into a set of founder\nsequences. All possible concatenation orders can be expressed as a founder\nblock graph. We observe a key property of such graphs: if the node labels\n(founder segments) do not repeat in the paths of the graph, such graphs can be\nindexed for efficient string matching. We call such graphs segment repeat-free\nfounder block graphs.\n  We give a linear time algorithm to construct a segment repeat-free founder\nblock graph given an MSA. The algorithm combines techniques from the founder\nsegmentation algorithms (Cazaux et al. SPIRE 2019) and fully-functional\nbidirectional Burrows-Wheeler index (Belazzougui and Cunial, CPM 2019). We\nderive a succinct index structure to support queries of arbitrary length in the\npaths of the graph.\n  Experiments on an MSA of SAR-CoV-2 strains are reported. An MSA of size\n$410\\times 29811$ is compacted in one minute into a segment repeat-free founder\nblock graph of 3900 nodes and 4440 edges. The maximum length and total length\nof node labels is 12 and 34968, respectively. The index on the graph takes only\n$3\\%$ of the size of the MSA.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 10:11:33 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["M\u00e4kinen", "Veli", ""], ["Cazaux", "Bastien", ""], ["Equi", "Massimo", ""], ["Norri", "Tuukka", ""], ["Tomescu", "Alexandru I.", ""]]}, {"id": "2005.09524", "submitter": "Hideo Bannai", "authors": "Kanaru Kutsukake, Takuya Matsumoto, Yuto Nakashima, Shunsuke Inenaga,\n  Hideo Bannai, Masayuki Takeda", "title": "On repetitiveness measures of Thue-Morse words", "comments": "accepted to SPIRE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the size $\\gamma(t_n)$ of the smallest string attractor of the\n$n$th Thue-Morse word $t_n$ is 4 for any $n\\geq 4$, disproving the conjecture\nby Mantaci et al. [ICTCS 2019] that it is $n$. We also show that $\\delta(t_n) =\n\\frac{10}{3+2^{4-n}}$ for $n \\geq 3$, where $\\delta(w)$ is the maximum over all\n$k = 1,\\ldots,|w|$, the number of distinct substrings of length $k$ in $w$\ndivided by $k$, which is a measure of repetitiveness recently studied by\nKociumaka et al. [LATIN 2020]. Furthermore, we show that the number $z(t_n)$ of\nfactors in the self-referencing Lempel-Ziv factorization of $t_n$ is exactly\n$2n$.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:34:18 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 18:06:40 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 04:49:36 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Kutsukake", "Kanaru", ""], ["Matsumoto", "Takuya", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "2005.09588", "submitter": "Udit Agarwal", "authors": "Udit Agarwal, Vijaya Ramachandran", "title": "Faster Deterministic All Pairs Shortest Paths in Congest Model", "comments": "An extended abstract of this paper will appear in the Proceedings of\n  the ACM Symposium on Parallel Algorithms and Architectures (SPAA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deterministic algorithm for distributed weighted all pairs\nshortest paths (APSP) in both undirected and directed graphs. Our algorithm\nruns in $\\tilde{O}(n^{4/3})$ rounds in the Congest models on graphs with\narbitrary edge weights, and it improves on the previous $\\tilde{O}(n^{3/2})$\nbound of Agarwal et al. [ARKP18]. The main components of our new algorithm are\na new faster technique for constructing blocker set deterministically and a new\npipelined method for deterministically propagating distance values from source\nnodes to the blocker set nodes in the network. Both of these techniques have\npotential applications to other distributed algorithms.\n  Our new deterministic algorithm for computing blocker set adapts the NC\napproximate hypergraph set cover algorithm in [BRS94] to the distributed\nconstruction of a blocker set. It follows the two-step process of first\ndesigning a randomized algorithm that uses only pairwise independence, and then\nderandomizes this algorithm using a sample space of linear size. This algorithm\nruns in almost the same number of rounds as the initial step in our APSP\nalgorithm that computes $h$-hops shortest paths, and significantly improves on\nthe deterministic blocker set algorithms in [ARKP18, AR19] by removing an\nadditional $n\\cdot |Q|$ term in the round bound, where Q is the blocker set.\n  The other new component in our APSP algorithm is a deterministic pipelined\napproach to propagate distance values from source nodes to blocker nodes. We\nuse a simple natural round-robin method for this step, and we show using a\nsuitable progress measure that it achieve the $\\tilde{O}(n^{4/3})$ bound on the\nnumber of rounds. It appears that the standard deterministic methods for\nefficiently broadcasting multiple values, and for sending or receiving messages\nusing the routing schedule in [HPDG+19,LSP19] do not apply to this setting.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:09:41 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "2005.09595", "submitter": "Min Jae Song", "authors": "Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang", "title": "Continuous LWE", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a continuous analogue of the Learning with Errors (LWE) problem,\nwhich we name CLWE. We give a polynomial-time quantum reduction from worst-case\nlattice problems to CLWE, showing that CLWE enjoys similar hardness guarantees\nto those of LWE. Alternatively, our result can also be seen as opening new\navenues of (quantum) attacks on lattice problems. Our work resolves an open\nproblem regarding the computational complexity of learning mixtures of\nGaussians without separability assumptions (Diakonikolas 2016, Moitra 2018). As\nan additional motivation, (a slight variant of) CLWE was considered in the\ncontext of robust machine learning (Diakonikolas et al.~FOCS 2017), where\nhardness in the statistical query (SQ) model was shown; our work addresses the\nopen question regarding its computational hardness (Bubeck et al.~ICML 2019).\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:16:12 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 20:55:35 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bruna", "Joan", ""], ["Regev", "Oded", ""], ["Song", "Min Jae", ""], ["Tang", "Yi", ""]]}, {"id": "2005.09599", "submitter": "Joseph Ross", "authors": "Joseph Ross", "title": "Asymmetric scale functions for $t$-digests", "comments": "18 pages, 8 figured; submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $t$-digest is a data structure that can be queried for approximate\nquantiles, with greater accuracy near the minimum and maximum of the\ndistribution. We develop a $t$-digest variant with accuracy asymmetric about\nthe median, thereby making possible alternative tradeoffs between computational\nresources and accuracy which may be of particular interest for distributions\nwith significant skew. After establishing some theoretical properties of scale\nfunctions for $t$-digests, we show that a tangent line construction on the\nfamiliar scale functions preserves the crucial properties that allow\n$t$-digests to operate online and be mergeable. We conclude with an empirical\nstudy demonstrating the asymmetric variant preserves accuracy on one side of\nthe distribution with a much smaller memory footprint.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:21:54 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ross", "Joseph", ""]]}, {"id": "2005.09724", "submitter": "David Stalfa", "authors": "Hamidreza Jahanjou and Rajmohan Rajaraman and David Stalfa", "title": "Scheduling Flows on a Switch to Optimize Response Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the scheduling of flows on a switch with the goal of optimizing\nmetrics related to the response time of the flows. The input to the problem is\na sequence of flow requests on a switch, where the switch is represented by a\nbipartite graph with a capacity on each vertex (or port), and a flow request is\nan edge with associated demand. In each round, a subset of edges can be\nscheduled subject to the constraint that the total demand of the scheduled\nedges incident on any vertex is at most the capacity of the vertex. Previous\nwork has essentially settled the complexity of metrics based on {\\em completion\ntime}. The objective of average or maximum {\\em response time}, however, is\nmuch more challenging.\n  We present approximation algorithms for flow scheduling over a switch to\noptimize response time based metrics. For the average response time metric,\nwhose NP-hardness follows directly from past work, we present an offline $O(1 +\nO(\\log(n))/c)$ approximation algorithm for unit flows, assuming that the port\ncapacities of the switch can be increased by a factor of $1 + c$, for any given\npositive integer $c$. For the maximum response time metric, we first establish\nthat it is NP-hard to achieve an approximation factor of better than 4/3\nwithout augmenting capacity. We then present an offline algorithm that achieves\n{\\em optimal maximum response time}, assuming the capacity of each port is\nincreased by at most $2 d_{max} - 1$, where $d_{max}$ is the maximum demand of\nany flow. Both algorithms are based on linear programming relaxations. We also\nstudy the online version of flow scheduling using the lens of competitive\nanalysis, and present preliminary results along with experiments that evaluate\nthe performance of fast online heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 19:46:10 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 17:53:22 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 17:20:20 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Jahanjou", "Hamidreza", ""], ["Rajaraman", "Rajmohan", ""], ["Stalfa", "David", ""]]}, {"id": "2005.09796", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Sidhanth Mohanty, Morris Yau", "title": "List Decodable Mean Estimation in Nearly Linear Time", "comments": "Minor corrections in Appendix A.3 and Algorithm 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from data in the presence of outliers is a fundamental problem in\nstatistics. Until recently, no computationally efficient algorithms were known\nto compute the mean of a high dimensional distribution under natural\nassumptions in the presence of even a small fraction of outliers. In this\npaper, we consider robust statistics in the presence of overwhelming outliers\nwhere the majority of the dataset is introduced adversarially. With only an\n$\\alpha < 1/2$ fraction of \"inliers\" (clean data) the mean of a distribution is\nunidentifiable. However, in their influential work, [CSV17] introduces a\npolynomial time algorithm recovering the mean of distributions with bounded\ncovariance by outputting a succinct list of $O(1/\\alpha)$ candidate solutions,\none of which is guaranteed to be close to the true distributional mean; a\ndirect analog of 'List Decoding' in the theory of error correcting codes. In\nthis work, we develop an algorithm for list decodable mean estimation in the\nsame setting achieving up to constants the information theoretically optimal\nrecovery, optimal sample complexity, and in nearly linear time up to\npolylogarithmic factors in dimension. Our conceptual innovation is to design a\ndescent style algorithm on a nonconvex landscape, iteratively removing minima\nto generate a succinct list of solutions. Our runtime bottleneck is a\nsaddle-point optimization for which we design custom primal dual solvers for\ngeneralized packing and covering SDP's under Ky-Fan norms, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 00:05:35 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 06:57:57 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Mohanty", "Sidhanth", ""], ["Yau", "Morris", ""]]}, {"id": "2005.10029", "submitter": "Marcelo Arenas", "authors": "Marcelo Arenas, Luis Alberto Croquevielle, Rajesh Jayaram, Cristian\n  Riveros", "title": "When is Approximate Counting for Conjunctive Queries Tractable?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjunctive queries are one of the most common class of queries used in\ndatabase systems, and the best studied in the literature. A seminal result of\nGrohe, Schwentick, and Segoufin (STOC 2001) demonstrates that for every class\n$G$ of graphs, the evaluation of all conjunctive queries whose underlying graph\nis in $G$ is tractable if, and only if, $G$ has bounded treewidth. In this\nwork, we extend this characterization to the counting problem for conjunctive\nqueries. Specifically, for every class $C$ of conjunctive queries with bounded\ntreewidth, we introduce the first fully polynomial-time randomized\napproximation scheme (FPRAS) for counting answers to a query in $C$, and the\nfirst polynomial-time algorithm for sampling answers uniformly from a query in\n$C$. As a corollary, it follows that for every class $G$ of graphs, the\ncounting problem for conjunctive queries whose underlying graph is in $G$\nadmits an FPRAS if, and only if, $G$ has bounded treewidth (unless $\\text{BPP}\n\\neq \\text{P}$)}. In fact, our FPRAS is more general, and also applies to\nconjunctive queries with bounded hypertree width, as well as unions of such\nqueries.\n  The key ingredient in our proof is the resolution of a fundamental counting\nproblem from automata theory. Specifically, we demonstrate the first FPRAS and\npolynomial time sampler for the set of trees of size $n$ accepted by a tree\nautomaton, which improves the prior quasi-polynomial time randomized\napproximation scheme (QPRAS) and sampling algorithm of Gore, Jerrum, Kannan,\nSweedyk, and Mahaney '97. We demonstrate how this algorithm can be used to\nobtain an FPRAS for many hitherto open problems, such as counting solutions to\nconstraint satisfaction problems (CSP) with bounded hypertree-width, counting\nthe number of error threads in programs with nested call subroutines, and\ncounting valid assignments to structured DNNF circuits.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:21:46 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 15:02:36 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 13:34:49 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Arenas", "Marcelo", ""], ["Croquevielle", "Luis Alberto", ""], ["Jayaram", "Rajesh", ""], ["Riveros", "Cristian", ""]]}, {"id": "2005.10095", "submitter": "Duncan Adamson", "authors": "Duncan Adamson, Argyrios Deligkas, Vladimir V. Gusev, Igor Potapov", "title": "The K-Centre Problem for Necklaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph theory, the objective of the k-centre problem is to find a set of\n$k$ vertices for which the largest distance of any vertex to its closest vertex\nin the $k$-set is minimised. In this paper, we introduce the $k$-centre problem\nfor sets of necklaces, i.e. the equivalence classes of words under the cyclic\nshift. This can be seen as the k-centre problem on the complete weighted graph\nwhere every necklace is represented by a vertex, and each edge has a weight\ngiven by the overlap distance between any pair of necklaces. Similar to the\ngraph case, the goal is to choose $k$ necklaces such that the distance from any\nword in the language and its nearest centre is minimised. However, in a case of\nk-centre problem for languages the size of associated graph maybe exponential\nin relation to the description of the language, i.e., the length of the words l\nand the size of the alphabet q. We derive several approximation algorithms for\nthe $k$-centre problem on necklaces, with logarithmic approximation factor in\nthe context of l and k, and within a constant factor for a more restricted\ncase.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:56:59 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Adamson", "Duncan", ""], ["Deligkas", "Argyrios", ""], ["Gusev", "Vladimir V.", ""], ["Potapov", "Igor", ""]]}, {"id": "2005.10427", "submitter": "Peter Ahrens", "authors": "Suzanne Mueller, Peter Ahrens, Stephen Chou, Fredrik Kjolstad, Saman\n  Amarasinghe", "title": "Sparse Tensor Transpositions", "comments": "This work will be the subject of a brief announcement at the 32nd ACM\n  Symposium on Parallelism in Algorithms and Architectures (SPAA '20)", "journal-ref": null, "doi": "10.1145/3350755.3400245", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for transposing sparse tensors called Quesadilla.\nThe algorithm converts the sparse tensor data structure to a list of\ncoordinates and sorts it with a fast multi-pass radix algorithm that exploits\nknowledge of the requested transposition and the tensors input partial\ncoordinate ordering to provably minimize the number of parallel partial sorting\npasses. We evaluate both a serial and a parallel implementation of Quesadilla\non a set of 19 tensors from the FROSTT collection, a set of tensors taken from\nscientific and data analytic applications. We compare Quesadilla and a\ngeneralization, Top-2-sadilla to several state of the art approaches, including\nthe tensor transposition routine used in the SPLATT tensor factorization\nlibrary. In serial tests, Quesadilla was the best strategy for 60% of all\ntensor and transposition combinations and improved over SPLATT by at least 19%\nin half of the combinations. In parallel tests, at least one of Quesadilla or\nTop-2-sadilla was the best strategy for 52% of all tensor and transposition\ncombinations.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 02:24:10 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Mueller", "Suzanne", ""], ["Ahrens", "Peter", ""], ["Chou", "Stephen", ""], ["Kjolstad", "Fredrik", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "2005.10566", "submitter": "Ce Jin", "authors": "Mohsen Ghaffari, Ce Jin, Daan Nilis", "title": "A Massively Parallel Algorithm for Minimum Weight Vertex Cover", "comments": "To appear in SPAA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a massively parallel algorithm, with near-linear memory per\nmachine, that computes a $(2+\\varepsilon)$-approximation of minimum-weight\nvertex cover in $O(\\log\\log d)$ rounds, where $d$ is the average degree of the\ninput graph.\n  Our result fills the key remaining gap in the state-of-the-art MPC algorithms\nfor vertex cover and matching problems; two classic optimization problems,\nwhich are duals of each other. Concretely, a recent line of work---by Czumaj et\nal. [STOC'18], Ghaffari et al. [PODC'18], Assadi et al. [SODA'19], and Gamlath\net al. [PODC'19]---provides $O(\\log\\log n)$ time algorithms for\n$(1+\\varepsilon)$-approximate maximum weight matching as well as for\n$(2+\\varepsilon)$-approximate minimum cardinality vertex cover. However, the\nlatter algorithm does not work for the general weighted case of vertex cover,\nfor which the best known algorithm remained at $O(\\log n)$ time complexity.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 10:59:33 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Jin", "Ce", ""], ["Nilis", "Daan", ""]]}, {"id": "2005.10610", "submitter": "Adam Kasperski", "authors": "Marc Goerigk, Adam Kasperski, Pawel Zielinski", "title": "Combinatorial two-stage minmax regret problems under interval\n  uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a class of combinatorial optimization problems is discussed. It\nis assumed that a feasible solution can be constructed in two stages. In the\nfirst stage the objective function costs are known while in the second stage\nthey are uncertain and belong to an interval uncertainty set. In order to\nchoose a solution, the minmax regret criterion is used. Some general properties\nof the problem are established and results for two particular problems, namely\nthe shortest path and the selection problem, are shown.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 12:42:14 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Goerigk", "Marc", ""], ["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "2005.10800", "submitter": "Katarzyna Paluch", "authors": "Katarzyna Paluch", "title": "New Approximation Algorithms for Maximum Asymmetric Traveling Salesman\n  and Shortest Superstring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the maximum asymmetric traveling salesman problem (Max ATSP) we are given\na complete directed graph with nonnegative weights on the edges and we wish to\ncompute a traveling salesman tour of maximum weight. In this paper we give a\nfast combinatorial $\\frac{7}{10}$-approximation algorithm for Max ATSP. It is\nbased on techniques of {\\em eliminating} and {\\em diluting} problematic\nsubgraphs with the aid of {\\it half-edges} and a method of edge coloring. (A\n{\\it half-edge} of edge $(u,v)$ is informally speaking \"either a head or a tail\nof $(u,v)$\".) A novel technique of {\\em diluting} a problematic subgraph $S$\nconsists in a seeming reduction of its weight, which allows its better\nhandling.\n  The current best approximation algorithms for Max ATSP, achieving the\napproximation guarantee of $\\frac 23$, are due to Kaplan, Lewenstein, Shafrir,\nSviridenko (2003) and Elbassioni, Paluch, van Zuylen (2012). Using a result by\nMucha, which states that an $\\alpha$-approximation algorithm for Max ATSP\nimplies a $(2+\\frac{11(1-\\alpha)}{9-2\\alpha})$-approximation algorithm for the\nshortest superstring problem (SSP), we obtain also a $(2 \\frac{33}{76} \\approx\n2,434)$-approximation algorithm for SSP, beating the previously best known\n(having an approximation factor equal to $2 \\frac{11}{23} \\approx 2,4782$.)\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:29:40 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 16:35:44 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Paluch", "Katarzyna", ""]]}, {"id": "2005.10852", "submitter": "Yaqiao Li", "authors": "Yaqiao Li, Vishnu V. Narayan, Denis Pankratov", "title": "Online Coloring and a New Type of Adversary for Online Graph Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of adversary for online graph problems. The new\nadversary is parameterized by a single integer $\\kappa$, which upper bounds the\nnumber of connected components that the adversary can use at any time during\nthe presentation of the online graph $G$. We call this adversary \"$\\kappa$\ncomponents bounded\", or $\\kappa$-CB for short. On one hand, this adversary is\nrestricted compared to the classical adversary because of the $\\kappa$-CB\nconstraint. On the other hand, we seek competitive ratios parameterized only by\n$\\kappa$ with no dependence on the input length $n$, thereby giving the new\nadversary power to use arbitrarily large inputs.\n  We study online coloring under the $\\kappa$-CB adversary. We obtain finer\nanalysis of the existing algorithms $FirstFit$ and $CBIP$ by computing their\ncompetitive ratios on trees and bipartite graphs under the new adversary.\nSurprisingly, $FirstFit$ outperforms $CBIP$ on trees. When it comes to\nbipartite graphs $FirstFit$ is no longer competitive under the new adversary,\nwhile $CBIP$ uses at most $2\\kappa$ colors. We also study several well known\nclasses of graphs, such as $3$-colorable, $C_k$-free, $d$-inductive, planar,\nand bounded treewidth, with respect to online coloring under the $\\kappa$-CB\nadversary. We demonstrate that the extra adversarial power of unbounded input\nlength outweighs the restriction on the number of connected components leading\nto non existence of competitive algorithms for these classes.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 18:25:03 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Li", "Yaqiao", ""], ["Narayan", "Vishnu V.", ""], ["Pankratov", "Denis", ""]]}, {"id": "2005.10917", "submitter": "Shunsuke Kanda", "authors": "Shunsuke Kanda, Koh Takeuchi, Keisuke Fujii and Yasuo Tabei", "title": "Succinct Trit-array Trie for Scalable Trajectory Similarity Search", "comments": "Accepted by ACM SIGSPATIAL 2020 as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive datasets of spatial trajectories representing the mobility of a\ndiversity of moving objects are ubiquitous in research and industry. Similarity\nsearch of a large collection of trajectories is indispensable for turning these\ndatasets into knowledge. Locality sensitive hashing (LSH) is a powerful\ntechnique for fast similarity searches. Recent methods employ LSH and attempt\nto realize an efficient similarity search of trajectories; however, those\nmethods are inefficient in terms of search time and memory when applied to\nmassive datasets. To address this problem, we present the trajectory-indexing\nsuccinct trit-array trie (tSTAT), which is a scalable method leveraging LSH for\ntrajectory similarity searches. tSTAT quickly performs the search on a tree\ndata structure called trie. We also present two novel techniques that enable to\ndramatically enhance the memory efficiency of tSTAT. One is a node reduction\ntechnique that substantially omits redundant trie nodes while maintaining the\ntime performance. The other is a space-efficient representation that leverages\nthe idea behind succinct data structures (i.e., a compressed data structure\nsupporting fast data operations). We experimentally test tSTAT on its ability\nto retrieve similar trajectories for a query from large collections of\ntrajectories and show that tSTAT performs superiorly in comparison to\nstate-of-the-art similarity search methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 21:42:30 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 21:01:47 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Kanda", "Shunsuke", ""], ["Takeuchi", "Koh", ""], ["Fujii", "Keisuke", ""], ["Tabei", "Yasuo", ""]]}, {"id": "2005.11026", "submitter": "Zhida Pan", "authors": "Xingwu Liu, Zhida Pan, Yuyi Wang", "title": "Target Location Problem for Multi-commodity Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by scheduling in Geo-distributed data analysis, we propose a target\nlocation problem for multi-commodity flow (LoMuF for short). Given commodities\nto be sent from their resources, LoMuF aims at locating their targets so that\nthe multi-commodity flow is optimized in some sense. LoMuF is a combination of\ntwo fundamental problems, namely, the facility location problem and the network\nflow problem. We study the hardness and algorithmic issues of the problem in\nvarious settings. The findings lie in three aspects. First, a series of\nNP-hardness and APX-hardness results are obtained, uncovering the inherent\ndifficulty in solving this problem. Second, we propose an approximation\nalgorithm for general undirected networks and an exact algorithm for undirected\ntrees, which naturally induce efficient approximation algorithms on directed\nnetworks. Third, we observe separations between directed networks and\nundirected ones, indicating that imposing direction on edges makes the problem\nstrictly harder. These results show the richness of the problem and pave the\nway to further studies.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:42:29 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Liu", "Xingwu", ""], ["Pan", "Zhida", ""], ["Wang", "Yuyi", ""]]}, {"id": "2005.11116", "submitter": "Christian Konrad", "authors": "Jacques Dark, Christian Konrad", "title": "Optimal Lower Bounds for Matching and Vertex Cover in Dynamic Graph\n  Streams", "comments": "to appear in CCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give simple optimal lower bounds on the one-way two-party\ncommunication complexity of approximate Maximum Matching and Minimum Vertex\nCover with deletions. In our model, Alice holds a set of edges and sends a\nsingle message to Bob. Bob holds a set of edge deletions, which form a subset\nof Alice's edges, and needs to report a large matching or a small vertex cover\nin the graph spanned by the edges that are not deleted. Our results imply\noptimal space lower bounds for insertion-deletion streaming algorithms for\nMaximum Matching and Minimum Vertex Cover.\n  Previously, Assadi et al. [SODA 2016] gave an optimal space lower bound for\ninsertion-deletion streaming algorithms for Maximum Matching via the\nsimultaneous model of communication. Our lower bound is simpler and stronger in\nseveral aspects: The lower bound of Assadi et al. only holds for algorithms\nthat (1) are able to process streams that contain a triple exponential number\nof deletions in $n$, the number of vertices of the input graph; (2) are able to\nprocess multi-graphs; and (3) never output edges that do not exist in the input\ngraph when the randomized algorithm errs. In contrast, our lower bound even\nholds for algorithms that (1) rely on short ($O(n^2)$-length) input streams;\n(2) are only able to process simple graphs; and (3) may output non-existing\nedges when the algorithm errs.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 11:34:14 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Dark", "Jacques", ""], ["Konrad", "Christian", ""]]}, {"id": "2005.11188", "submitter": "Torben Hagerup", "authors": "Torben Hagerup", "title": "Still Simpler Static Level Ancestors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A level-ancestor or LA query about a rooted tree $T$ takes as arguments a\nnode $v$ in $T$, of depth $d_v$, say, and an integer $d$ with $0\\le d\\le d_v$\nand returns the ancestor of $v$ in $T$ of depth $d$. The static LA problem is\nto process a given rooted tree $T$ so as to support efficient subsequent\nprocessing of LA queries about $T$. All previous efficient solutions to the\nstatic LA problem work by reducing a given instance of the problem to a smaller\ninstance of the same or a related problem, solved with a less efficient data\nstructure, and a collection of small micro-instances for which a different\nsolution is provided. We indicate the first efficient solution to the static LA\nproblem that works directly, without resorting to reductions or\nmicro-instances.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 13:45:23 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Hagerup", "Torben", ""]]}, {"id": "2005.11195", "submitter": "Rui Yao", "authors": "Rui Yao, Shlomo Bekhor", "title": "A Dynamic Tree Algorithm for On-demand Peer-to-peer Ride-sharing\n  Matching", "comments": "hEART 2020 : 9th Symposium of the European Association for Research\n  in Transportation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovative shared mobility services provide on-demand flexible mobility\noptions and have the potential to alleviate traffic congestion. These\nattractive services are challenging from different perspectives. One major\nchallenge in such systems is to find suitable ride-sharing matchings between\ndrivers and passengers with respect to the system objective and constraints,\nand to provide optimal pickup and drop-off sequence to the drivers. In this\npaper, we develop an efficient dynamic tree algorithm to find the optimal\npickup and drop-off sequence. The algorithm finds an initial solution to the\nproblem, keeps track of previously explored feasible solutions, and reduces the\nsolution search space when considering new requests. In addition, an efficient\npre-processing procedure to select candidate passenger requests is proposed,\nwhich further improves the algorithm performance. Numerical experiments are\nconducted on a real size network to illustrate the efficiency of our algorithm.\nSensitivity analysis suggests that small vehicle capacities and loose excess\ntravel time constraints do not guarantee overall savings in vehicle kilometer\ntraveled.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 14:02:39 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Yao", "Rui", ""], ["Bekhor", "Shlomo", ""]]}, {"id": "2005.11232", "submitter": "Alexander Barvinok", "authors": "Alexander Barvinok and Nicholas Barvinok", "title": "More on zeros and approximation of the Ising partition function", "comments": "Several improvements", "journal-ref": "Forum of Mathematics, Sigma 9 (2021) e46", "doi": "10.1017/fms.2021.40", "report-no": null, "categories": "math.PR cs.DS math-ph math.CO math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the partition function $\\sum_x\ne^{f(x)}$, where $f: \\{-1, 1\\}^n \\longrightarrow {\\Bbb R}$ is a quadratic or\ncubic polynomial on the Boolean cube $\\{-1, 1\\}^n$. In the case of a quadratic\npolynomial $f$, we show that the partition function can be approximated within\nrelative error $0 < \\epsilon < 1$ in quasi-polynomial $n^{O(\\ln n - \\ln\n\\epsilon)}$ time if the Lipschitz constant of the non-linear part of $f$ with\nrespect to the $\\ell^1$ metric on the Boolean cube does not exceed $1-\\delta$,\nfor any $\\delta >0$, fixed in advance. For a cubic polynomial $f$, we get the\nsame result under a somewhat stronger condition.\n  We apply the method of polynomial interpolation, for which we prove that\n$\\sum_x e^{\\tilde{f}(x)} \\ne 0$ for complex-valued polynomials $\\tilde{f}$ in a\nneighborhood of a real-valued $f$ satisfying the above mentioned conditions.\nThe bounds are asymptotically optimal. Results on the zero-free region are\ninterpreted as the absence of a phase transition in the Lee - Yang sense in the\ncorresponding Ising model. The novel feature of the bounds is that they control\nthe total interaction of each vertex but not every single interaction of sets\nof vertices.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 15:22:10 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 13:29:37 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 15:12:52 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Barvinok", "Alexander", ""], ["Barvinok", "Nicholas", ""]]}, {"id": "2005.11541", "submitter": "Marvin K\\\"unnemann", "authors": "Marvin K\\\"unnemann and D\\'aniel Marx", "title": "Finding Small Satisfying Assignments Faster Than Brute Force: A\n  Fine-grained Perspective into Boolean Constraint Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the question under which circumstances small solutions can be found\nfaster than by exhaustive search (and by how much), we study the fine-grained\ncomplexity of Boolean constraint satisfaction with size constraint exactly $k$.\nMore precisely, we aim to determine, for any finite constraint family, the\noptimal running time $f(k)n^{g(k)}$ required to find satisfying assignments\nthat set precisely $k$ of the $n$ variables to $1$.\n  Under central hardness assumptions on detecting cliques in graphs and\n3-uniform hypergraphs, we give an almost tight characterization of $g(k)$ into\nfour regimes: (1) Brute force is essentially best-possible, i.e., $g(k) = (1\\pm\no(1))k$, (2) the best algorithms are as fast as current $k$-clique algorithms,\ni.e., $g(k)=(\\omega/3\\pm o(1))k$, (3) the exponent has sublinear dependence on\n$k$ with $g(k) \\in [\\Omega(\\sqrt[3]{k}), O(\\sqrt{k})]$, or (4) the problem is\nfixed-parameter tractable, i.e., $g(k) = O(1)$.\n  This yields a more fine-grained perspective than a previous FPT/W[1]-hardness\ndichotomy (Marx, Computational Complexity 2005). Our most interesting technical\ncontribution is a $f(k)n^{4\\sqrt{k}}$-time algorithm for SubsetSum with\nprecedence constraints parameterized by the target $k$ -- particularly the\napproach, based on generalizing a bound on the Frobenius coin problem to a\nsetting with precedence constraints, might be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 14:05:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["K\u00fcnnemann", "Marvin", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "2005.11542", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Xiaoqi Chen, Gil Einziger, Shir Landau Feibish, Danny\n  Raz, Minlan Yu", "title": "Routing Oblivious Measurement Analytics", "comments": "To appear in IFIP Networking 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network-wide traffic analytics are often needed for various network\nmonitoring tasks. These measurements are often performed by collecting samples\nat network switches, which are then sent to the controller for aggregation.\nHowever, performing such analytics without ``overcounting'' flows or packets\nthat traverse multiple measurement switches is challenging. Therefore, existing\nsolutions often simplify the problem by making assumptions on the routing or\nmeasurement switch placement.\n  We introduce AROMA, a measurement infrastructure that generates a uniform\nsample of packets and flows regardless of the topology, workload and routing.\nTherefore, AROMA can be deployed in many settings, and can also work in the\ndata plane using programmable PISA switches. The AROMA infrastructure includes\ncontroller algorithms that approximate a variety of essential measurement tasks\nwhile providing formal accuracy guarantees. Using extensive simulations on\nreal-world network traces, we show that our algorithms are competitively\naccurate compared to the best existing solutions despite the fact that they\nmake no assumptions on the underlying network or the placement of measurement\nswitches.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 14:14:01 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 01:22:41 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Basat", "Ran Ben", ""], ["Chen", "Xiaoqi", ""], ["Einziger", "Gil", ""], ["Feibish", "Shir Landau", ""], ["Raz", "Danny", ""], ["Yu", "Minlan", ""]]}, {"id": "2005.11547", "submitter": "Tobias Christiani", "authors": "Tobias Christiani", "title": "DartMinHash: Fast Sketching for Weighted Sets", "comments": "See https://github.com/tobc/dartminhash for the code accompanying the\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted minwise hashing is a standard dimensionality reduction technique\nwith applications to similarity search and large-scale kernel machines. We\nintroduce a simple algorithm that takes a weighted set $x \\in \\mathbb{R}_{\\geq\n0}^{d}$ and computes $k$ independent minhashes in expected time $O(k \\log k +\n\\Vert x \\Vert_{0}\\log( \\Vert x \\Vert_1 + 1/\\Vert x \\Vert_1))$, improving upon\nthe state-of-the-art BagMinHash algorithm (KDD '18) and representing the\nfastest weighted minhash algorithm for sparse data. Our experiments show\nrunning times that scale better with $k$ and $\\Vert x \\Vert_0$ compared to ICWS\n(ICDM '10) and BagMinhash, obtaining $10$x speedups in common use cases. Our\napproach also gives rise to a technique for computing fully independent\nlocality-sensitive hash values for $(L, K)$-parameterized approximate near\nneighbor search under weighted Jaccard similarity in optimal expected time\n$O(LK + \\Vert x \\Vert_0)$, improving on prior work even in the case of\nunweighted sets.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 14:59:25 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Christiani", "Tobias", ""]]}, {"id": "2005.11614", "submitter": "Mariam Zomorodi-Moghadam", "authors": "Omid Daei, Keivan Navi, Mariam Zomorodi-Moghadam", "title": "Optimized Quantum Circuit Partitioning", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": "10.1007/s10773-020-04633-8", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of this paper is to improve the communication costs in\ndistributed quantum circuits. To this end, we present a method for generating\ndistributed quantum circuits from monolithic quantum circuits in such a way\nthat communication between partitions of a distributed quantum circuit is\nminimized. Thus, the communication between distributed components is performed\nat a lower cost. Compared to existing works, our approach can effectively map a\nquantum circuit into an appropriate number of distributed components. Since\nteleportation is usually the protocol used to connect components in a\ndistributed quantum circuit, our approach ultimately reduces the number of\nteleportations. The results of applying our approach to the benchmark quantum\ncircuits determine its effectiveness and show that partitioning is a necessary\nstep in constructing distributed quantum circuit.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 22:35:18 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Daei", "Omid", ""], ["Navi", "Keivan", ""], ["Zomorodi-Moghadam", "Mariam", ""]]}, {"id": "2005.11654", "submitter": "Eldon Chung", "authors": "Divesh Aggarwal and Eldon Chung", "title": "A Note on the Concrete Hardness of the Shortest Independent Vectors\n  Problem in Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bl\\\"omer and Seifert showed that $\\mathsf{SIVP}_2$ is NP-hard to approximate\nby giving a reduction from $\\mathsf{CVP}_2$ to $\\mathsf{SIVP}_2$ for constant\napproximation factors as long as the $\\mathsf{CVP}$ instance has a certain\nproperty. In order to formally define this requirement on the $\\mathsf{CVP}$\ninstance, we introduce a new computational problem called the Gap Closest\nVector Problem with Bounded Minima. We adapt the proof of Bl\\\"omer and Seifert\nto show a reduction from the Gap Closest Vector Problem with Bounded Minima to\n$\\mathsf{SIVP}$ for any $\\ell_p$ norm for some constant approximation factor\ngreater than $1$.\n  In a recent result, Bennett, Golovnev and Stephens-Davidowitz showed that\nunder Gap-ETH, there is no $2^{o(n)}$-time algorithm for approximating\n$\\mathsf{CVP}_p$ up to some constant factor $\\gamma \\geq 1$ for any $1 \\leq p\n\\leq \\infty$. We observe that the reduction in their paper can be viewed as a\nreduction from $\\mathsf{Gap3SAT}$ to the Gap Closest Vector Problem with\nBounded Minima. This, together with the above mentioned reduction, implies\nthat, under Gap-ETH, there is no $2^{o(n)}$-time algorithm for approximating\n$\\mathsf{SIVP}_p$ up to some constant factor $\\gamma \\geq 1$ for any $1 \\leq p\n\\leq \\infty$.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 04:22:02 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 05:26:49 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Chung", "Eldon", ""]]}, {"id": "2005.11736", "submitter": "Raghavendra Addanki", "authors": "Raghavendra Addanki, Shiva Prasad Kasiviswanathan, Andrew McGregor,\n  Cameron Musco", "title": "Efficient Intervention Design for Causal Discovery with Latents", "comments": "International Conference on Machine Learning 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider recovering a causal graph in presence of latent variables, where\nwe seek to minimize the cost of interventions used in the recovery process. We\nconsider two intervention cost models: (1) a linear cost model where the cost\nof an intervention on a subset of variables has a linear form, and (2) an\nidentity cost model where the cost of an intervention is the same, regardless\nof what variables it is on, i.e., the goal is just to minimize the number of\ninterventions. Under the linear cost model, we give an algorithm to identify\nthe ancestral relations of the underlying causal graph, achieving within a\n$2$-factor of the optimal intervention cost. This approximation factor can be\nimproved to $1+\\epsilon$ for any $\\epsilon > 0$ under some mild restrictions.\nUnder the identity cost model, we bound the number of interventions needed to\nrecover the entire causal graph, including the latent variables, using a\nparameterization of the causal graph through a special type of colliders. In\nparticular, we introduce the notion of $p$-colliders, that are colliders\nbetween pair of nodes arising from a specific type of conditioning in the\ncausal graph, and provide an upper bound on the number of interventions as a\nfunction of the maximum number of $p$-colliders between any two nodes in the\ncausal graph.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 12:53:48 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 16:53:18 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Addanki", "Raghavendra", ""], ["Kasiviswanathan", "Shiva Prasad", ""], ["McGregor", "Andrew", ""], ["Musco", "Cameron", ""]]}, {"id": "2005.11818", "submitter": "Steve Hanneke", "authors": "Olivier Bousquet, Steve Hanneke, Shay Moran, and Nikita Zhivotovskiy", "title": "Proper Learning, Helly Number, and an Optimal SVM Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical PAC sample complexity bounds are stated for any Empirical Risk\nMinimizer (ERM) and contain an extra logarithmic factor $\\log(1/{\\epsilon})$\nwhich is known to be necessary for ERM in general. It has been recently shown\nby Hanneke (2016) that the optimal sample complexity of PAC learning for any VC\nclass C is achieved by a particular improper learning algorithm, which outputs\na specific majority-vote of hypotheses in C. This leaves the question of when\nthis bound can be achieved by proper learning algorithms, which are restricted\nto always output a hypothesis from C.\n  In this paper we aim to characterize the classes for which the optimal sample\ncomplexity can be achieved by a proper learning algorithm. We identify that\nthese classes can be characterized by the dual Helly number, which is a\ncombinatorial parameter that arises in discrete geometry and abstract\nconvexity. In particular, under general conditions on C, we show that the dual\nHelly number is bounded if and only if there is a proper learner that obtains\nthe optimal joint dependence on $\\epsilon$ and $\\delta$.\n  As further implications of our techniques we resolve a long-standing open\nproblem posed by Vapnik and Chervonenkis (1974) on the performance of the\nSupport Vector Machine by proving that the sample complexity of SVM in the\nrealizable case is $\\Theta((n/{\\epsilon})+(1/{\\epsilon})\\log(1/{\\delta}))$,\nwhere $n$ is the dimension. This gives the first optimal PAC bound for\nHalfspaces achieved by a proper learning algorithm, and moreover is\ncomputationally efficient.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 18:11:57 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bousquet", "Olivier", ""], ["Hanneke", "Steve", ""], ["Moran", "Shay", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "2005.11912", "submitter": "Robert Carr", "authors": "Robert D. Carr, Jennifer Iglesias, Giuseppe Lanciac, Benjamin Moseley", "title": "Symmetric Linear Programming Formulations for Minimum Cut with\n  Applications to TSP", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce multiple symmetric LP relaxations for minimum cut problems. The\nrelaxations give optimal and approximate solutions when the input is a\nHamiltonian cycle. We show that this leads to one of two interesting results.\nIn one case, these LPs always give optimal and near optimal solutions, and then\nthey would be the smallest known symmetric LPs for the problems considered.\nOtherwise, these LP formulations give strictly better LP relaxations for the\ntraveling salesperson problem than the subtour relaxation. We have the smallest\nknown LP formulation that is a 9/8-approximation or better for min-cut. In\naddition, the LP relaxation of min-cut investigated in this paper has\ninteresting constraints; the LP contains only a single typical min-cut\nconstraint and all other constraints are typically only used for max-cut\nrelaxations.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 03:55:41 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Carr", "Robert D.", ""], ["Iglesias", "Jennifer", ""], ["Lanciac", "Giuseppe", ""], ["Moseley", "Benjamin", ""]]}, {"id": "2005.12065", "submitter": "Thomas Dybdahl Ahle", "authors": "Thomas Dybdahl Ahle", "title": "On the Problem of $p_1^{-1}$ in Locality-Sensitive Hashing", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A Locality-Sensitive Hash (LSH) function is called\n$(r,cr,p_1,p_2)$-sensitive, if two data-points with a distance less than $r$\ncollide with probability at least $p_1$ while data points with a distance\ngreater than $cr$ collide with probability at most $p_2$. These functions form\nthe basis of the successful Indyk-Motwani algorithm (STOC 1998) for nearest\nneighbour problems. In particular one may build a $c$-approximate nearest\nneighbour data structure with query time $\\tilde O(n^\\rho/p_1)$ where\n$\\rho=\\frac{\\log1/p_1}{\\log1/p_2}\\in(0,1)$. That is, sub-linear time, as long\nas $p_1$ is not too small. This is significant since most high dimensional\nnearest neighbour problems suffer from the curse of dimensionality, and can't\nbe solved exact, faster than a brute force linear-time scan of the database.\n  Unfortunately, the best LSH functions tend to have very low collision\nprobabilities, $p_1$ and $p_2$. Including the best functions for Cosine and\nJaccard Similarity. This means that the $n^\\rho/p_1$ query time of LSH is often\nnot sub-linear after all, even for approximate nearest neighbours!\n  In this paper, we improve the general Indyk-Motwani algorithm to reduce the\nquery time of LSH to $\\tilde O(n^\\rho/p_1^{1-\\rho})$ (and the space usage\ncorrespondingly.) Since $n^\\rho p_1^{\\rho-1} < n \\Leftrightarrow p_1 > n^{-1}$,\nour algorithm always obtains sublinear query time, for any collision\nprobabilities at least $1/n$. For $p_1$ and $p_2$ small enough, our improvement\nover all previous methods can be \\emph{up to a factor $n$} in both query time\nand space.\n  The improvement comes from a simple change to the Indyk-Motwani algorithm,\nwhich can easily be implemented in existing software packages.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 12:14:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ahle", "Thomas Dybdahl", ""]]}, {"id": "2005.12245", "submitter": "Alexandre Salles Da Cunha", "authors": "Alexandre Salles da Cunha", "title": "Improved Formulations and Branch-and-cut Algorithms for the Angular\n  Constrained Minimum Spanning Tree Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Angular Constrained Minimum Spanning Tree Problem ($\\alpha$-MSTP) is\ndefined in terms of a complete undirected graph $G=(V,E)$ and an angle $\\alpha\n\\in (0,2\\pi]$. Vertices of $G$ define points in the Euclidean plane while\nedges, the line segments connecting them, are weighted by the Euclidean\ndistance between their endpoints. A spanning tree is an $\\alpha$-spanning tree\n($\\alpha$-ST) of $G$ if, for any $i \\in V$, the smallest angle that encloses\nall line segments corresponding to its $i$-incident edges does not exceed\n$\\alpha$. $\\alpha$-MSTP consists in finding an $\\alpha$-ST with the least\nweight. We introduce two $\\alpha-$MSTP integer programming formulations,\n${\\mathcal F}_{xy}^*$ and $\\mathcal{F}_x^{++}$ and their accompanying\nBranch-and-cut (BC) algorithms, BCFXY$^*$ and BCFX$^{++}$. Both formulations\ncan be seen as improvements over formulations coming from the literature. The\nstrongest of them, $\\mathcal{F}_x^{++}$, was obtained by: (i) lifting an\nexisting set of inequalities in charge of enforcing $\\alpha$ angular\nconstraints and (ii) characterizing $\\alpha$-MSTP valid inequalities from the\nStable Set polytope, a structure behind $\\alpha-$STs, that we disclosed here.\nThese formulations and their predecessors in the literature were compared from\na polyhedral perspective. From a numerical standpoint, we observed that\nBCFXY$^*$ and BCFX$^{++}$ compare favorably to their competitors in the\nliterature. In fact, thanks to the quality of the bounds provided by\n$\\mathcal{F}_x^{++}$, BCFX$^{++}$ seems to outperform the other existing\n$\\alpha-$MSTP algorithms. It is able to solve more instances to proven\noptimality and to provide sharper lower bounds, when optimality is not attested\nwithin an imposed time limit. As a by-product, BCFX$^{++}$ provided 8 new\noptimality certificates for instances coming from the literature.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 17:41:54 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["da Cunha", "Alexandre Salles", ""]]}, {"id": "2005.12414", "submitter": "Peter Ahrens", "authors": "Peter Ahrens and Erik G. Boman", "title": "On Optimal Partitioning For Sparse Matrices In Variable Block Row Format", "comments": "22 pages; added experimental results for VBR, updated presentation of\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Variable Block Row (VBR) format is an influential blocked sparse matrix\nformat designed for matrices with shared sparsity structure between adjacent\nrows and columns. VBR groups adjacent rows and columns, storing the resulting\nblocks that contain nonzeros in a dense format. This reduces the memory\nfootprint and enables optimizations such as register blocking and\ninstruction-level parallelism. Existing approaches use heuristics to determine\nwhich rows and columns should be grouped together. We show that finding the\noptimal grouping of rows and columns for VBR is NP-hard under several\nreasonable cost models. In light of this finding, we propose a 1-dimensional\nvariant of VBR, called 1D-VBR, which achieves better performance than VBR by\nonly grouping rows. We describe detailed cost models for runtime and memory\nconsumption. Then, we describe a linear time dynamic programming solution for\noptimally grouping the rows for 1D-VBR format. We extend our algorithm to\nproduce a heuristic VBR partitioner which alternates between optimally\npartitioning rows and columns, assuming the columns or rows to be fixed,\nrespectively. Our alternating heuristic produces VBR matrices with the smallest\nmemory footprint of any partitioner we tested.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 21:30:03 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 17:53:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ahrens", "Peter", ""], ["Boman", "Erik G.", ""]]}, {"id": "2005.12648", "submitter": "Berenger Bramas", "authors": "Berenger Bramas (Inria, ICube, CAMUS), Quentin Bramas (ICube, UNISTRA)", "title": "On the improvement of the in-place merge algorithm parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present several improvements in the parallelization of the\nin-place merge algorithm, which merges two contiguous sorted arrays into one\nwith an O(T) space complexity (where T is the number of threads). The approach\ndivides the two arrays into as many pairs of partitions as there are threads\navailable; such that each thread can later merge a pair of partitions\nindependently of the others. We extend the existing method by proposing a new\nalgorithm to find the median of two partitions. Additionally, we provide a new\nstrategy to divide the input arrays where we minimize the data movement, but at\nthe cost of making this stage sequential. Finally, we provide the so-called\nlinear shifting algorithm that swaps two partitions in-place with contiguous\ndata access. We emphasize that our approach is straightforward to implement and\nthat it can also be used for external (out of place) merging. The results\ndemonstrate that it provides a significant speedup compared to sequential\nexecutions, when the size of the arrays is greater than a thousand elements.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 12:08:04 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Bramas", "Berenger", "", "Inria, ICube, CAMUS"], ["Bramas", "Quentin", "", "ICube, UNISTRA"]]}, {"id": "2005.12844", "submitter": "Sushrut Karmalkar", "authors": "Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R. Klivans,\n  Mahdi Soltanolkotabi", "title": "Approximation Schemes for ReLU Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of ReLU regression, where the goal is to\noutput the best fitting ReLU with respect to square loss given access to draws\nfrom some unknown distribution. We give the first efficient, constant-factor\napproximation algorithm for this problem assuming the underlying distribution\nsatisfies some weak concentration and anti-concentration conditions (and\nincludes, for example, all log-concave distributions). This solves the main\nopen problem of Goel et al., who proved hardness results for any exact\nalgorithm for ReLU regression (up to an additive $\\epsilon$). Using more\nsophisticated techniques, we can improve our results and obtain a\npolynomial-time approximation scheme for any subgaussian distribution. Given\nthe aforementioned hardness results, these guarantees can not be substantially\nimproved.\n  Our main insight is a new characterization of surrogate losses for nonconvex\nactivations. While prior work had established the existence of convex\nsurrogates for monotone activations, we show that properties of the underlying\ndistribution actually induce strong convexity for the loss, allowing us to\nrelate the global minimum to the activation's Chow parameters.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 16:26:17 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 18:08:38 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Goel", "Surbhi", ""], ["Karmalkar", "Sushrut", ""], ["Klivans", "Adam R.", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "2005.12997", "submitter": "Antoine Genitrini", "authors": "Olivier Bodini and Antoine Genitrini and Bernhard Gittenberger and\n  Isabella Larcher and Mehdi Naima", "title": "Compaction for two models of logarithmic-depth trees: Analysis and\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in the quantitative analysis of the\ncompaction ratio for two classical families of trees: recursive trees and plane\nbinary increasing trees. These families are typical representatives of tree\nmodels with a small depth. More formally, asymptotically, for a random tree\nwith $n$ nodes, its depth is of order $\\ln n$. Once a tree of size $n$ is\ncompacted by keeping only one occurrence of all fringe subtrees appearing in\nthe tree the resulting graph contains only $O(n / \\ln n)$ nodes. This result\nmust be compared to classical results of compaction in the families of simply\ngenerated trees, where the analogous result states that the compacted structure\nis of size of order $n / \\sqrt{\\ln n}$. The result about the plane binary\nincreasing trees has already been proved, but we propose a new and generic\napproach to get the result in this paper. We end the paper with an experimental\nquantitative study, based on a prototype implementation of compacted binary\nsearch trees, that are modeled by plane binary increasing trees.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 19:27:14 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 08:06:09 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Bodini", "Olivier", ""], ["Genitrini", "Antoine", ""], ["Gittenberger", "Bernhard", ""], ["Larcher", "Isabella", ""], ["Naima", "Mehdi", ""]]}, {"id": "2005.13628", "submitter": "Neal E. Young", "authors": "Christos Koufogiannakis, Neal E. Young", "title": "Distributed algorithms for covering, packing and maximum weighted\n  matching", "comments": null, "journal-ref": "Distributed Computing 24, 45--63 (2011)", "doi": "10.1007/s00446-011-0127-7", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives poly-logarithmic-round, distributed D-approximation\nalgorithms for covering problems with submodular cost and monotone covering\nconstraints (Submodular-cost Covering). The approximation ratio D is the\nmaximum number of variables in any constraint. Special cases include Covering\nMixed Integer Linear Programs (CMIP), and Weighted Vertex Cover (with D=2). Via\nduality, the paper also gives poly-logarithmic-round, distributed\nD-approximation algorithms for Fractional Packing linear programs (where D is\nthe maximum number of constraints in which any variable occurs), and for Max\nWeighted c-Matching in hypergraphs (where D is the maximum size of any of the\nhyperedges; for graphs D=2). The paper also gives parallel (RNC)\n2-approximation algorithms for CMIP with two variables per constraint and\nWeighted Vertex Cover. The algorithms are randomized. All of the approximation\nratios exactly match those of comparable centralized algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:20:04 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Koufogiannakis", "Christos", ""], ["Young", "Neal E.", ""]]}, {"id": "2005.13634", "submitter": "Fabio Henrique Viduani Martinez", "authors": "Diego P Rubert, Eloi Araujo, Marco A Stefanes, Jens Stoye, F\\'abio V\n  Martinez", "title": "On motifs in colored graphs", "comments": "28 pages, 9 figures, to be published in Journal of Combinatorial\n  Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most important concepts in biological network analysis is that of\nnetwork motifs, which are patterns of interconnections that occur in a given\nnetwork at a frequency higher than expected in a random network. In this work\nwe are interested in searching and inferring network motifs in a class of\nbiological networks that can be represented by vertex-colored graphs. We show\nthe computational complexity for many problems related to colorful topological\nmotifs and present efficient algorithms for special cases. We also present a\nprobabilistic strategy to detect highly frequent motifs in vertex-colored\ngraphs. Experiments on real data sets show that our algorithms are very\ncompetitive both in efficiency and in quality of the solutions.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:27:44 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Rubert", "Diego P", ""], ["Araujo", "Eloi", ""], ["Stefanes", "Marco A", ""], ["Stoye", "Jens", ""], ["Martinez", "F\u00e1bio V", ""]]}, {"id": "2005.13645", "submitter": "Neal E. Young", "authors": "Qi Fu, Elizabeth Bent, James Borneman, Marek Chrobak, Neal E. Young", "title": "Algorithmic approaches to selecting control clones in DNA array\n  hybridization experiments", "comments": null, "journal-ref": "Journal of Bioinformatics and Computational Biology 5(4) 937-961,\n  2007", "doi": "10.1142/S0219720007002977", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of selecting control clones in DNA array hybridization\nexperiments. The problem arises in the OFRG method for analyzing microbial\ncommunities. The OFRG method performs classification of rRNA gene clones using\nbinary fingerprints created from a series of hybridization experiments, where\neach experiment consists of hybridizing a collection of arrayed clones with a\nsingle oligonucleotide probe. This experiment produces analog signals, one for\neach clone, which then need to be classified, that is, converted into binary\nvalues 1 and 0 that represent hybridization and non-hybridization events. In\naddition to the sample rRNA gene clones, the array contains a number of control\nclones needed to calibrate the classification procedure of the hybridization\nsignals. These control clones must be selected with care to optimize the\nclassification process. We formulate this as a combinatorial optimization\nproblem called Balanced Covering. We prove that the problem is NP-hard, and we\nshow some results on hardness of approximation. We propose approximation\nalgorithms based on randomized rounding and we show that, with high\nprobability, our algorithms approximate well the optimum solution. The\nexperimental results confirm that the algorithms find high quality control\nclones. The algorithms have been implemented and are publicly available as part\nof the software package called CloneTools.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:50:29 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Fu", "Qi", ""], ["Bent", "Elizabeth", ""], ["Borneman", "James", ""], ["Chrobak", "Marek", ""], ["Young", "Neal E.", ""]]}, {"id": "2005.13716", "submitter": "Alexander Wei", "authors": "Alexander Wei", "title": "Better and Simpler Learning-Augmented Online Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lykouris and Vassilvitskii (ICML 2018) introduce a model of online caching\nwith machine-learned advice, where each page request additionally comes with a\nprediction of when that page will next be requested. In this model, a natural\ngoal is to design algorithms that (1) perform well when the advice is accurate\nand (2) remain robust in the worst case a la traditional competitive analysis.\nLykouris and Vassilvitskii give such an algorithm by adapting the Marker\nalgorithm to the learning-augmented setting. In a recent work, Rohatgi (SODA\n2020) improves on their result with an approach also inspired by randomized\nmarking. We continue the study of this problem, but with a somewhat different\napproach: We consider combining the BlindOracle algorithm, which just na\\\"ively\nfollows the predictions, with an optimal competitive algorithm for online\ncaching in a black-box manner. The resulting algorithm outperforms all existing\napproaches while being significantly simpler. Moreover, we show that combining\nBlindOracle with LRU is in fact optimal among deterministic algorithms for this\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 00:32:52 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Wei", "Alexander", ""]]}, {"id": "2005.13773", "submitter": "Martin P. Seybold", "authors": "Joachim Gudmundsson, Michael Horton, John Pfeifer, Martin P. Seybold", "title": "A Practical Index Structure Supporting Fr\\'echet Proximity Queries Among\n  Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable approach for range and $k$ nearest neighbor queries\nunder computationally expensive metrics, like the continuous Fr\\'echet distance\non trajectory data. Based on clustering for metric indexes, we obtain a dynamic\ntree structure whose size is linear in the number of trajectories, regardless\nof the trajectory's individual sizes or the spatial dimension, which allows one\nto exploit low `intrinsic dimensionality' of data sets for effective search\nspace pruning.\n  Since the distance computation is expensive, generic metric indexing methods\nare rendered impractical. We present strategies that (i) improve on known upper\nand lower bound computations, (ii) build cluster trees without any or very few\ndistance calls, and (iii) search using bounds for metric pruning, interval\norderings for reduction, and randomized pivoting for reporting the final\nresults.\n  We analyze the efficiency and effectiveness of our methods with extensive\nexperiments on diverse synthetic and real-world data sets. The results show\nimprovement over state-of-the-art methods for exact queries, and even further\nspeed-ups are achieved for queries that may return approximate results.\nSurprisingly, the majority of exact nearest-neighbor queries on real data sets\nare answered without any distance computations.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 04:12:43 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Gudmundsson", "Joachim", ""], ["Horton", "Michael", ""], ["Pfeifer", "John", ""], ["Seybold", "Martin P.", ""]]}, {"id": "2005.13913", "submitter": "Sangram Kishor Jena Mr", "authors": "Ramesh K. Jallu, Sangram K. Jena and Gautam K. Das", "title": "Liar's Domination in Unit Disk Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study a variant of the minimum dominating set problem\nknown as the minimum liar's dominating set (MLDS) problem. We prove that the\nMLDS problem is NP-hard in unit disk graphs. Next, we show that the recent\nsub-quadratic time $\\frac{11}{2}$-factor approximation algorithm \\cite{bhore}\nfor the MLDS problem is erroneous and propose a simple $O(n + m)$ time\n7.31-factor approximation algorithm, where $n$ and $m$ are the number of\nvertices and edges in the input unit disk graph, respectively. Finally, we\nprove that the MLDS problem admits a polynomial-time approximation scheme.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 11:27:40 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Jallu", "Ramesh K.", ""], ["Jena", "Sangram K.", ""], ["Das", "Gautam K.", ""]]}, {"id": "2005.13938", "submitter": "Matthew Johnson", "authors": "Nick Brettell, Matthew Johnson, Giacomo Paesani, Dani\\\"el Paulusma", "title": "Computing Subset Transversals in $H$-Free Graphs", "comments": "Minor changes incorporating suggestions of reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of two well-known graph transversal\nproblems, namely Subset Feedback Vertex Set and Subset Odd Cycle Transversal,\nby restricting the input to $H$-free graphs, that is, to graphs that do not\ncontain some fixed graph~$H$ as an induced subgraph. By combining known and new\nresults, we determine the computational complexity of both problems on $H$-free\ngraphs for every graph $H$ except when $H=sP_1+P_4$ for some $s\\geq 1$. As part\nof our approach, we introduce the Subset Vertex Cover problem and prove that it\nis polynomial-time solvable for $(sP_1+P_4)$-free graphs for every $s\\geq 1$.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 12:09:04 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 23:26:50 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Brettell", "Nick", ""], ["Johnson", "Matthew", ""], ["Paesani", "Giacomo", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "2005.14058", "submitter": "Charles Argue", "authors": "C.J. Argue, Anupam Gupta, Guru Guruganesh", "title": "Dimension-Free Bounds on Chasing Convex Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of chasing convex functions, where functions arrive\nover time. The player takes actions after seeing the function, and the goal is\nto achieve a small function cost for these actions, as well as a small cost for\nmoving between actions. While the general problem requires a polynomial\ndependence on the dimension, we show how to get dimension-independent bounds\nfor well-behaved functions. In particular, we consider the case where the\nconvex functions are $\\kappa$-well-conditioned, and give an algorithm that\nachieves an $O(\\sqrt \\kappa)$-competitiveness. Moreover, when the functions are\nsupported on $k$-dimensional affine subspaces--e.g., when the function are the\nindicators of some affine subspaces--we get $O(\\min(k, \\sqrt{k \\log\nT}))$-competitive algorithms for request sequences of length $T$. We also show\nsome lower bounds, that well-conditioned functions require\n$\\Omega(\\kappa^{1/3})$-competitiveness, and $k$-dimensional functions require\n$\\Omega(\\sqrt{k})$-competitiveness.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:45:58 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Argue", "C. J.", ""], ["Gupta", "Anupam", ""], ["Guruganesh", "Guru", ""]]}, {"id": "2005.14098", "submitter": "Takahisa Toda", "authors": "Kohei Nishikawa, Takahisa Toda", "title": "Exact Method for Generating Strategy-Solvable Sudoku Clues", "comments": null, "journal-ref": "Algorithms 2020, 13(7), 171", "doi": "10.3390/a13070171", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Sudoku puzzle often has a regular pattern in the arrangement of initial\ndigits and it is typically made solvable with known solving techniques, called\nstrategies. In this paper, we consider the problem of generating such Sudoku\ninstances. We introduce a rigorous framework to discuss solvability for Sudoku\ninstances with respect to strategies. This allows us to handle not only known\nstrategies but also general strategies under a few reasonable assumptions. We\npropose an exact method for determining Sudoku clues for a given set of clue\npositions that is solvable with a given set of strategies. This is the first\nexact method except for a trivial brute-force search. Besides the clue\ngeneration, we present an application of our method to the problem of\ndetermining the minimum number of strategy-solvable Sudoku clues. We conduct\nexperiments to evaluate our method, varying the position and the number of\nclues at random. Our method terminates within $1$ minutes for many grids.\nHowever, as the number of clues gets closer to $20$, the running time rapidly\nincreases and exceeds the time limit set to $600$ seconds. We also evaluate our\nmethod for several instances with $17$ clue positions taken from known minimum\nSudokus to see the efficiency for deciding unsolvability.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:47:11 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Nishikawa", "Kohei", ""], ["Toda", "Takahisa", ""]]}, {"id": "2005.14111", "submitter": "Mihalis Yannakakis", "authors": "Mihalis Yannakakis", "title": "Planar Graphs that Need Four Pages", "comments": "To be published in Journal of Combinatorial Theory, Series B", "journal-ref": "Journal of Combinatorial Theory, series B, 145, pp. 241-263, 2020", "doi": "10.1016/j.jctb.2020.05.008", "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there are planar graphs that require four pages in any book\nembedding.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:06:02 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Yannakakis", "Mihalis", ""]]}, {"id": "2005.14195", "submitter": "Adil Erzin I", "authors": "Adil Erzin, Gregory Melidi, Stepan Nazarenko, Roman Plotnikov", "title": "Two-Bar Charts Packing Problem", "comments": null, "journal-ref": null, "doi": "10.1007/s11590-020-01657-1", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bar Charts Packing Problem (BCPP), in which it is necessary to\npack bar charts (BCs) in a strip of minimum length. The problem is, on the one\nhand, a generalization of the Bin Packing Problem (BPP), and, on the other\nhand, a particular case of the Project Scheduling Problem with\nmultidisciplinary jobs and one limited non-accumulative resource. Earlier, we\nproposed a polynomial algorithm that constructs the optimal package for a given\norder of non-increasing BCs. This result generalizes a similar result for BPP.\nFor Two-Bar Charts Packing Problem (2-BCPP), when each BC consists of two bars,\nthe algorithm we have proposed constructs a package in polynomial time, the\nlength of which does not exceed $2\\ OPT+1$, where $OPT$ is the minimum possible\nlength of the packing. As far as we know, this is the first guaranteed estimate\nfor 2-BCPP. We also conducted a numerical experiment in which we compared the\nsolutions built by our approximate algorithms with the optimal solutions built\nby the CPLEX package. The experimental results confirmed the high efficiency of\nthe developed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 10:07:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Erzin", "Adil", ""], ["Melidi", "Gregory", ""], ["Nazarenko", "Stepan", ""], ["Plotnikov", "Roman", ""]]}, {"id": "2005.14335", "submitter": "Kamil Khadiev", "authors": "Kamil Khadiev and Vladislav Remidovskii", "title": "Classical and Quantum Algorithms for Constructing Text from Dictionary\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms for solving the problem of constructing a text (long\nstring) from a dictionary (sequence of small strings). The problem has an\napplication in bioinformatics and has a connection with the Sequence assembly\nmethod for reconstructing a long DNA sequence from small fragments. The problem\nis constructing a string $t$ of length $n$ from strings $s^1,\\dots, s^m$ with\npossible intersections. We provide a classical algorithm with running time\n$O\\left(n+L +m(\\log n)^2\\right)=\\tilde{O}(n+L)$ where $L$ is the sum of lengths\nof $s^1,\\dots,s^m$. We provide a quantum algorithm with running time $O\\left(n\n+\\log n\\cdot(\\log m+\\log\\log n)\\cdot \\sqrt{m\\cdot L}\\right)=\\tilde{O}\\left(n\n+\\sqrt{m\\cdot L}\\right)$. Additionally, we show that the lower bound for the\nclassical algorithm is $\\Omega(n+L)$. Thus, our classical algorithm is optimal\nup to a log factor, and our quantum algorithm shows speed-up comparing to any\nclassical algorithm in a case of non-constant length of strings in the\ndictionary.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 22:44:01 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Khadiev", "Kamil", ""], ["Remidovskii", "Vladislav", ""]]}, {"id": "2005.14425", "submitter": "Srinivas Kota Reddy", "authors": "Sahasrajit Sarmasarkar, Kota Srinivas Reddy, and Nikhil Karamchandani", "title": "Query complexity of heavy hitter estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying the subset\n$\\mathcal{S}^{\\gamma}_{\\mathcal{P}}$ of elements in the support of an\nunderlying distribution $\\mathcal{P}$ whose probability value is larger than a\ngiven threshold $\\gamma$, by actively querying an oracle to gain information\nabout a sequence $X_1, X_2, \\ldots$ of $i.i.d.$ samples drawn from\n$\\mathcal{P}$. We consider two query models: $(a)$ each query is an index $i$\nand the oracle return the value $X_i$ and $(b)$ each query is a pair $(i,j)$\nand the oracle gives a binary answer confirming if $X_i = X_j$ or not. For each\nof these query models, we design sequential estimation algorithms which at each\nround, either decide what query to send to the oracle depending on the entire\nhistory of responses or decide to stop and output an estimate of\n$\\mathcal{S}^{\\gamma}_{\\mathcal{P}}$, which is required to be correct with some\npre-specified large probability. We provide upper bounds on the query\ncomplexity of the algorithms for any distribution $\\mathcal{P}$ and also derive\nlower bounds on the optimal query complexity under the two query models. We\nalso consider noisy versions of the two query models and propose robust\nestimators which can effectively counter the noise in the oracle responses.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 07:15:46 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 12:18:38 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Sarmasarkar", "Sahasrajit", ""], ["Reddy", "Kota Srinivas", ""], ["Karamchandani", "Nikhil", ""]]}, {"id": "2005.14493", "submitter": "Yanhao Wang", "authors": "Yanhao Wang, Yuchen Li, Raymond Chi-Wing Wong, Kian-Lee Tan", "title": "A Fully Dynamic Algorithm for k-Regret Minimizing Sets", "comments": "15 pages, 11 figures; to appear in ICDE 2021", "journal-ref": null, "doi": "10.1109/ICDE51399.2021.00144", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Selecting a small set of representatives from a large database is important\nin many applications such as multi-criteria decision making, web search, and\nrecommendation. The $k$-regret minimizing set ($k$-RMS) problem was recently\nproposed for representative tuple discovery. Specifically, for a large database\n$P$ of tuples with multiple numerical attributes, the $k$-RMS problem returns a\nsize-$r$ subset $Q$ of $P$ such that, for any possible ranking function, the\nscore of the top-ranked tuple in $Q$ is not much worse than the score of the\n$k$\\textsuperscript{th}-ranked tuple in $P$. Although the $k$-RMS problem has\nbeen extensively studied in the literature, existing methods are designed for\nthe static setting and cannot maintain the result efficiently when the database\nis updated. To address this issue, we propose the first fully-dynamic algorithm\nfor the $k$-RMS problem that can efficiently provide the up-to-date result\nw.r.t.~any insertion and deletion in the database with a provable guarantee.\nExperimental results on several real-world and synthetic datasets demonstrate\nthat our algorithm runs up to four orders of magnitude faster than existing\n$k$-RMS algorithms while returning results of nearly equal quality.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 10:36:15 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 06:21:27 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 09:02:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Yanhao", ""], ["Li", "Yuchen", ""], ["Wong", "Raymond Chi-Wing", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "2005.14620", "submitter": "Tomohiro Koana", "authors": "Matthias Bentert, Roman Haag, Christian Hofer, Tomohiro Koana, Andr\\'e\n  Nichterlein", "title": "Parameterized Complexity of Min-Power Asymmetric Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate parameterized algorithms for the NP-hard problem Min-Power\nAsymmetric Connectivity (MinPAC) that has applications in wireless sensor\nnetworks. Given a directed arc-weighted graph, MinPAC asks for a strongly\nconnected spanning subgraph minimizing the summed vertex costs. Here, the cost\nof each vertex is the weight of its heaviest outgoing arc in the chosen\nsubgraph. We present linear-time algorithms for the cases where the number of\nstrongly connected components in a so-called obligatory subgraph or the\nfeedback edge number in the underlying undirected graph is constant.\nComplementing these results, we prove that the problem is W[2]-hard with\nrespect to the solution cost, even on restricted graphs with one feedback arc\nand binary arc weights.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 15:25:49 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Bentert", "Matthias", ""], ["Haag", "Roman", ""], ["Hofer", "Christian", ""], ["Koana", "Tomohiro", ""], ["Nichterlein", "Andr\u00e9", ""]]}, {"id": "2005.14648", "submitter": "Maximilian Teegen", "authors": "Christian Elbracht, Jakob Kneip and Maximilian Teegen", "title": "A Note on Generic Tangle Algorithms", "comments": "Early draft; 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we gather the theoretical outlines of three basic algorithms for\ntangles in abstract separation systems: a naive tree search for finding\ntangles; an algorithm which outputs a certificate for the non-existence of\ntangles if possible, and otherwise a way to jump-start the naive tree search;\nand a way to obtain a tree-of-tangles.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 16:19:21 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Elbracht", "Christian", ""], ["Kneip", "Jakob", ""], ["Teegen", "Maximilian", ""]]}, {"id": "2005.14717", "submitter": "Lydia Zakynthinou", "authors": "Anamay Chaturvedi, Huy Nguyen, Lydia Zakynthinou", "title": "Differentially Private Decomposable Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of differentially private constrained maximization of\ndecomposable submodular functions. A submodular function is decomposable if it\ntakes the form of a sum of submodular functions. The special case of maximizing\na monotone, decomposable submodular function under cardinality constraints is\nknown as the Combinatorial Public Projects (CPP) problem [Papadimitriou et al.,\n2008]. Previous work by Gupta et al. [2010] gave a differentially private\nalgorithm for the CPP problem. We extend this work by designing differentially\nprivate algorithms for both monotone and non-monotone decomposable submodular\nmaximization under general matroid constraints, with competitive utility\nguarantees. We complement our theoretical bounds with experiments demonstrating\nempirical performance, which improves over the differentially private\nalgorithms for the general case of submodular maximization and is close to the\nperformance of non-private algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:59:46 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Chaturvedi", "Anamay", ""], ["Nguyen", "Huy", ""], ["Zakynthinou", "Lydia", ""]]}]