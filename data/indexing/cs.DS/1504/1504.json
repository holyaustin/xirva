[{"id": "1504.00065", "submitter": "Fragkiskos Koufogiannis", "authors": "Fragkiskos Koufogiannis, Shuo Han, George J. Pappas", "title": "Optimality of the Laplace Mechanism in Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the highly interconnected realm of Internet of Things, exchange of\nsensitive information raises severe privacy concerns. The Laplace mechanism --\nadding Laplace-distributed artificial noise to sensitive data -- is one of the\nwidely used methods of providing privacy guarantees within the framework of\ndifferential privacy. In this work, we present Lipschitz privacy, a slightly\ntighter version of differential privacy. We prove that the Laplace mechanism is\noptimal in the sense that it minimizes the mean-squared error for identity\nqueries which provide privacy with respect to the $\\ell_{1}$-norm. In addition\nto the $\\ell_{1}$-norm which respects individuals' participation, we focus on\nthe use of the $\\ell_{2}$-norm which provides privacy of high-dimensional data.\nA variation of the Laplace mechanism is proven to have the optimal mean-squared\nerror from the identity query. Finally, the optimal mechanism for the scenario\nin which individuals submit their high-dimensional sensitive data is derived.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 23:28:16 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 20:49:32 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Koufogiannis", "Fragkiskos", ""], ["Han", "Shuo", ""], ["Pappas", "George J.", ""]]}, {"id": "1504.00231", "submitter": "Stefania Petra Mrs.", "authors": "Stefania Petra, Constantin Popa", "title": "Single Projection Kaczmarz Extended Algorithms", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To find the least squares solution of a very large and inconsistent system of\nequations, one can employ the extended Kaczmarz algorithm. This method\nsimultaneously removes the error term, such that a consistent system is\nasymptotically obtained, and applies Kaczmarz iterations for the current\napproximation of this system. For random corrections of the right hand side and\nKaczmarz updates selected at random, convergence to the least squares solution\nhas been shown. We consider the deterministic control strategies, and show\nconvergence to a least squares solution when row and column updates are chosen\naccording to the almost-cyclic or maximal-residual choice.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 13:49:42 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Petra", "Stefania", ""], ["Popa", "Constantin", ""]]}, {"id": "1504.00341", "submitter": "Gabriele Farina", "authors": "Gabriele Farina", "title": "A linear time algorithm to compute the impact of all the articulation\n  points", "comments": "4 pages, 3 figures. Accepted for YR-ICALP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The articulation points of an undirected connected graphs are those vertices\nwhose removal increases the number of connected components of the graph, i.e.\nthe vertices whose removal disconnects the graph. However, not all the\narticulation points are equal: the removal of some of them might end in a\nsingle vertex disconnected from the graph, whilst in other cases the graph can\nbe split in several small pieces. In order to measure the effect of the removal\nof an articulation point, in \\cite{AFL12} has been proposed the impact, defined\nas the number of vertices that get disconnected from the main (largest)\nsurviving connected component (CC). In this paper we present the first linear\ntime algorithm ($\\mathcal{O}(m+n)$ for a graph with $n$ vertices and $m$ edges)\nto compute the impact of all the articulation points of the graph, thus\nimproving from the $\\mathcal{O}(a(m+n))\\approx\\mathcal{O}(nm+n^2)$ of the\nna\\\"ive algorithm, with $a$ being the number of articulation points of the\ngraph.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 19:21:42 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 09:47:18 GMT"}, {"version": "v3", "created": "Sun, 10 May 2015 13:32:47 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Farina", "Gabriele", ""]]}, {"id": "1504.00429", "submitter": "Fragkiskos Koufogiannis", "authors": "Fragkiskos Koufogiannis, Shuo Han, George J. Pappas", "title": "Gradual Release of Sensitive Data under Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of releasing sensitive data under differential\nprivacy when the privacy level is subject to change over time. Existing work\nassumes that privacy level is determined by the system designer as a fixed\nvalue before sensitive data is released. For certain applications, however,\nusers may wish to relax the privacy level for subsequent releases of the same\ndata after either a re-evaluation of the privacy concerns or the need for\nbetter accuracy. Specifically, given a database containing sensitive data, we\nassume that a response $y_1$ that preserves $\\epsilon_{1}$-differential privacy\nhas already been published. Then, the privacy level is relaxed to $\\epsilon_2$,\nwith $\\epsilon_2 > \\epsilon_1$, and we wish to publish a more accurate response\n$y_2$ while the joint response $(y_1, y_2)$ preserves $\\epsilon_2$-differential\nprivacy. How much accuracy is lost in the scenario of gradually releasing two\nresponses $y_1$ and $y_2$ compared to the scenario of releasing a single\nresponse that is $\\epsilon_{2}$-differentially private? Our results show that\nthere exists a composite mechanism that achieves \\textit{no loss} in accuracy.\nWe consider the case in which the private data lies within $\\mathbb{R}^{n}$\nwith an adjacency relation induced by the $\\ell_{1}$-norm, and we focus on\nmechanisms that approximate identity queries. We show that the same accuracy\ncan be achieved in the case of gradual release through a mechanism whose\noutputs can be described by a \\textit{lazy Markov stochastic process}. This\nstochastic process has a closed form expression and can be efficiently sampled.\nOur results are applicable beyond identity queries. To this end, we demonstrate\nthat our results can be applied in several cases, including Google's RAPPOR\nproject, trading of sensitive data, and controlled transmission of private data\nin a social network.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 02:03:20 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Koufogiannis", "Fragkiskos", ""], ["Han", "Shuo", ""], ["Pappas", "George J.", ""]]}, {"id": "1504.00513", "submitter": "Natali Ruchansky", "authors": "Natali Ruchansky, Francesco Bonchi, David Garcia-Soriano, Francesco\n  Gullo, Nicolas Kourtellis", "title": "The Minimum Wiener Connector", "comments": "Published in Proceedings of the 2015 ACM SIGMOD International\n  Conference on Management of Data", "journal-ref": null, "doi": "10.1145/2723372.2749449", "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wiener index of a graph is the sum of all pairwise shortest-path\ndistances between its vertices. In this paper we study the novel problem of\nfinding a minimum Wiener connector: given a connected graph $G=(V,E)$ and a set\n$Q\\subseteq V$ of query vertices, find a subgraph of $G$ that connects all\nquery vertices and has minimum Wiener index.\n  We show that The Minimum Wiener Connector admits a polynomial-time (albeit\nimpractical) exact algorithm for the special case where the number of query\nvertices is bounded. We show that in general the problem is NP-hard, and has no\nPTAS unless $\\mathbf{P} = \\mathbf{NP}$. Our main contribution is a\nconstant-factor approximation algorithm running in time\n$\\widetilde{O}(|Q||E|)$.\n  A thorough experimentation on a large variety of real-world graphs confirms\nthat our method returns smaller and denser solutions than other methods, and\ndoes so by adding to the query set $Q$ a small number of important vertices\n(i.e., vertices with high centrality).\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 11:36:56 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 04:40:23 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Ruchansky", "Natali", ""], ["Bonchi", "Francesco", ""], ["Garcia-Soriano", "David", ""], ["Gullo", "Francesco", ""], ["Kourtellis", "Nicolas", ""]]}, {"id": "1504.00545", "submitter": "Timo Bingmann", "authors": "Timo Bingmann, Thomas Keh, Peter Sanders", "title": "A Bulk-Parallel Priority Queue in External Memory with STXXL", "comments": "extended version of SEA'15 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the design and an implementation of a bulk-parallel external\nmemory priority queue to take advantage of both shared-memory parallelism and\nhigh external memory transfer speeds to parallel disks. To achieve higher\nperformance by decoupling item insertions and extractions, we offer two\nparallelization interfaces: one using \"bulk\" sequences, the other by defining\n\"limit\" items. In the design, we discuss how to parallelize insertions using\nmultiple heaps, and how to calculate a dynamic prediction sequence to prefetch\nblocks and apply parallel multiway merge for extraction. Our experimental\nresults show that in the selected benchmarks the priority queue reaches 75% of\nthe full parallel I/O bandwidth of rotational disks and and 65% of SSDs, or the\nspeed of sorting in external memory when bounded by computation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 13:23:20 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Bingmann", "Timo", ""], ["Keh", "Thomas", ""], ["Sanders", "Peter", ""]]}, {"id": "1504.00616", "submitter": "Fabian Peternek", "authors": "Sebastian Maneth and Fabian Peternek", "title": "A Survey on Methods and Systems for Graph Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an informal survey (meant to accompany another paper) on graph\ncompression methods. We focus on lossless methods, briefly list available\npproaches, and compare them where possible or give some indicators on their\ncompression ratios. We also mention some relevant results from the field of\nlossy compression and algorithms specialized for the use on large graphs. ---\nNote: The comparison is by no means complete. This document is a first draft\nand will be updated and extended.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 17:08:09 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Maneth", "Sebastian", ""], ["Peternek", "Fabian", ""]]}, {"id": "1504.00627", "submitter": "Bruce  Shepherd", "authors": "F. Bruce Shepherd and Adrian Vetta", "title": "The Inapproximability of Maximum Single-Sink Unsplittable, Priority and\n  Confluent Flow Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider single-sink network flow problems. An instance consists of a\ncapacitated graph (directed or undirected), a sink node $t$ and a set of\ndemands that we want to send to the sink. Here demand $i$ is located at a node\n$s_i$ and requests an amount $d_i$ of flow capacity in order to route\nsuccessfully. Two standard objectives are to maximise (i) the number of demands\n(cardinality) and (ii) the total demand (throughput) that can be routed subject\nto the capacity constraints. Furthermore, we examine these maximisation\nproblems for three specialised types of network flow: unsplittable, confluent\nand priority flows.\n  In the {\\em unsplittable flow} problem (UFP), we have edge capacities, and\nthe demand for $s_i$ must be routed on a single path. In the {\\em confluent\nflow} problem, we have node capacities, and the final flow must induce a tree.\nBoth of these problems have been studied extensively, primarily in the\nsingle-sink setting. However, most of this work imposed the {\\em no-bottleneck\nassumption} (that the maximum demand $d_{max}$ is at most the minimum capacity\n$u_{min}$). Given the no-bottleneck assumption (NBA), there is a factor\n$4.43$-approximation algorithm due to Dinitz et al. for the unsplittable flow\nproblem. Under the stronger assumption of uniform capacities, there is a factor\n$3$-approximation algorithm due to Chen et al. for the confluent flow problem.\nHowever, unlike the UFP, we show that a constant factor approximation algorithm\ncannot be obtained for the single-sink confluent flows even {\\bf with} the NBA.\n  Without NBA, we show that maximum cardinality single-sink UFP is hard to\napproximate to within a factor $n^{.5-\\epsilon}$ even when all demands lie in a\nsmall interval $[1,1+\\Delta]$ where $\\Delta>0$ (but has polynomial input size).\nThis is very sharp since when $\\Delta=0$, this becomes a maximum flow problem.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 17:55:03 GMT"}, {"version": "v2", "created": "Fri, 15 May 2015 18:37:50 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Shepherd", "F. Bruce", ""], ["Vetta", "Adrian", ""]]}, {"id": "1504.00681", "submitter": "Alexandra Kolla", "authors": "Guy Kindler, Alexandra Kolla, Luca Trevisan", "title": "Approximation of non-boolean 2CSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a polynomial time $\\Omega\\left ( \\frac 1R \\log R \\right)$\napproximate algorithm for Max 2CSP-$R$, the problem where we are given a\ncollection of constraints, each involving two variables, where each variable\nranges over a set of size $R$, and we want to find an assignment to the\nvariables that maximizes the number of satisfied constraints. Assuming the\nUnique Games Conjecture, this is the best possible approximation up to constant\nfactors.\n  Previously, a $1/R$-approximate algorithm was known, based on linear\nprogramming. Our algorithm is based on semidefinite programming (SDP) and on a\nnovel rounding technique. The SDP that we use has an almost-matching\nintegrality gap.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 20:05:58 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2015 02:55:27 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Kindler", "Guy", ""], ["Kolla", "Alexandra", ""], ["Trevisan", "Luca", ""]]}, {"id": "1504.00686", "submitter": "Tsz Chiu Kwok", "authors": "Tsz Chiu Kwok, Lap Chi Lau, Yin Tat Lee", "title": "Improved Cheeger's Inequality and Analysis of Local Graph Partitioning\n  using Vertex Expansion and Expansion Profile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove two generalizations of the Cheeger's inequality. The first\ngeneralization relates the second eigenvalue to the edge expansion and the\nvertex expansion of the graph G, $\\lambda_2 = \\Omega(\\phi^V(G) \\phi(G))$, where\n$\\phi^V(G)$ denotes the robust vertex expansion of G and $\\phi(G)$ denotes the\nedge expansion of G. The second generalization relates the second eigenvalue to\nthe edge expansion and the expansion profile of G, for all $k \\ge 2$,\n$\\lambda_2 = \\Omega(\\phi_k(G) \\phi(G) / k)$, where $\\phi_k(G)$ denotes the\nk-way expansion of G. These show that the spectral partitioning algorithm has\nbetter performance guarantees when $\\phi^V(G)$ is large (e.g. planted random\ninstances) or $\\phi_k(G)$ is large (instances with few disjoint non-expanding\nsets). Both bounds are tight up to a constant factor.\n  Our approach is based on a method to analyze solutions of Laplacian systems,\nand this allows us to extend the results to local graph partitioning\nalgorithms. In particular, we show that our approach can be used to analyze\npersonal pagerank vectors, and to give a local graph partitioning algorithm for\nthe small-set expansion problem with performance guarantees similar to the\ngeneralizations of Cheeger's inequality. We also present a spectral approach to\nprove similar results for the truncated random walk algorithm. These show that\nlocal graph partitioning algorithms almost match the performance of the\nspectral partitioning algorithm, with the additional advantages that they apply\nto the small-set expansion problem and their running time could be sublinear.\nOur techniques provide common approaches to analyze the spectral partitioning\nalgorithm and local graph partitioning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 20:45:39 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Kwok", "Tsz Chiu", ""], ["Lau", "Lap Chi", ""], ["Lee", "Yin Tat", ""]]}, {"id": "1504.00766", "submitter": "Hiro Ito", "authors": "Hiro Ito (UEC, Japan and CREST, JST, Japan)", "title": "Every property is testable on a natural class of scale-free multigraphs", "comments": "13 pages, one figure. Difference from ver. 1: Definitions of HSF and\n  SF become more general. Typos were fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a natural class of multigraphs called\nhierarchical-scale-free (HSF) multigraphs, and consider constant-time\ntestability on the class. We show that a very wide subclass, specifically, that\nin which the power-law exponent is greater than two, of HSF is hyperfinite.\nBased on this result, an algorithm for a deterministic partitioning oracle can\nbe constructed. We conclude by showing that every property is constant-time\ntestable on the above subclass of HSF. This algorithm utilizes findings by\nNewman and Sohler of STOC'11. However, their algorithm is based on the\nbounded-degree model, while it is known that actual scale-free networks usually\ninclude hubs, which have a very large degree. HSF is based on scale-free\nproperties and includes such hubs. This is the first universal result of\nconstant-time testability on the general graph model, and it has the potential\nto be applicable on a very wide range of scale-free networks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 07:52:07 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 17:23:38 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Ito", "Hiro", "", "UEC, Japan and CREST, JST, Japan"]]}, {"id": "1504.00774", "submitter": "Hiro Ito", "authors": "Hiro Ito (UEC, Japan and CREST, JST, Japan) and Takahiro Ueda (Komatsu\n  Ltd., Japan)", "title": "How to solve the cake-cutting problem in sublinear time", "comments": "15 pages, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show algorithms for solving the cake-cutting problem in\nsublinear-time. More specifically, we preassign (simple) fair portions to o(n)\nplayers in o(n)-time, and minimize the damage to the rest of the players. All\ncurrently known algorithms require Omega(n)-time, even when assigning a portion\nto just one player, and it is nontrivial to revise these algorithms to run in\n$o(n)$-time since many of the remaining players, who have not been asked any\nqueries, may not be satisfied with the remaining cake. To challenge this\nproblem, we begin by providing a framework for solving the cake-cutting problem\nin sublinear-time. Generally speaking, solving a problem in sublinear-time\nrequires the use of approximations. However, in our framework, we introduce the\nconcept of \"eps n-victims,\" which means that eps n players (victims) may not\nget fair portions, where 0< eps =< 1 is an arbitrary constant. In our\nframework, an algorithm consists of the following two parts: In the first\n(Preassigning) part, it distributes fair portions to r < n players in\no(n)-time. In the second (Completion) part, it distributes fair portions to the\nremaining n-r players except for the eps n victims in poly}(n)-time. There are\ntwo variations on the r players in the first part. Specifically, whether they\ncan or cannot be designated. We will then present algorithms in this framework.\nIn particular, an O(r/eps)-time algorithm for r =< eps n/127 undesignated\nplayers with eps n-victims, and an O~(r^2/eps)-time algorithm for r =< eps\ne^{{sqrt{ln{n}}}/{7}} designated players and eps =< 1/e with eps n-victims are\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 08:24:31 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 10:48:52 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Ito", "Hiro", "", "UEC, Japan and CREST, JST, Japan"], ["Ueda", "Takahiro", "", "Komatsu\n  Ltd., Japan"]]}, {"id": "1504.00785", "submitter": "Marcos Assuncao", "authors": "Marcos Dias de Assuncao", "title": "Enhanced Red-Black-Tree Data Structure for Facilitating the Scheduling\n  of Reservations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper details a data structure for managing and scheduling requests for\ncomputing resources of clusters and virtualised infrastructure such as private\nclouds. The data structure uses a red-black tree whose nodes represent the\nstart times and/or completion times of requests. The tree is enhanced by a\ndouble-linked list that facilitates the iteration of nodes once the start time\nof a request is determined by using the tree. We describe the data structure\nmain features, provide an example of use, and discuss experiments that\ndemonstrate that the average complexity of two operations are often below 10%\nof their respective theoretical worst cases.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 08:59:21 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["de Assuncao", "Marcos Dias", ""]]}, {"id": "1504.00834", "submitter": "Raphael Clifford", "authors": "Raphael Clifford, Markus Jalsenius, Benjamin Sach", "title": "The complexity of computation in bit streams", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the complexity of online computation in the cell probe model. We\nconsider a class of problems where we are first given a fixed pattern or vector\n$F$ of $n$ symbols and then one symbol arrives at a time in a stream. After\neach symbol has arrived we must output some function of $F$ and the $n$-length\nsuffix of the arriving stream. Cell probe bounds of $\\Omega(\\delta\\lg{n}/w)$\nhave previously been shown for both convolution and Hamming distance in this\nsetting, where $\\delta$ is the size of a symbol in bits and\n$w\\in\\Omega(\\lg{n})$ is the cell size in bits. However, when $\\delta$ is a\nconstant, as it is in many natural situations, these previous results no longer\ngive us non-trivial bounds.\n  We introduce a new lop-sided information transfer proof technique which\nenables us to prove meaningful lower bounds even for constant size input\nalphabets. We use our new framework to prove an amortised cell probe lower\nbound of $\\Omega(\\lg^2 n/(w\\cdot \\lg \\lg n))$ time per arriving bit for an\nonline version of a well studied problem known as pattern matching with address\nerrors. This is the first non-trivial cell probe lower bound for any online\nproblem on bit streams that still holds when the cell sizes are large. We also\nshow the same bound for online convolution conditioned on a new combinatorial\nconjecture related to Toeplitz matrices.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 13:10:58 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Clifford", "Raphael", ""], ["Jalsenius", "Markus", ""], ["Sach", "Benjamin", ""]]}, {"id": "1504.00954", "submitter": "Talya Eden", "authors": "Talya Eden, Amit Levi, Dana Ron, C. Seshadhri", "title": "Approximately Counting Triangles in Sublinear Time", "comments": "To appear in the 56th Annual IEEE Symposium on Foundations of\n  Computer Science (FOCS 2015)", "journal-ref": null, "doi": "10.1109/FOCS.2015.44", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the number of triangles in a graph.\nThis problem has been extensively studied in both theory and practice, but all\nexisting algorithms read the entire graph. In this work we design a {\\em\nsublinear-time\\/} algorithm for approximating the number of triangles in a\ngraph, where the algorithm is given query access to the graph. The allowed\nqueries are degree queries, vertex-pair queries and neighbor queries.\n  We show that for any given approximation parameter $0<\\epsilon<1$, the\nalgorithm provides an estimate $\\widehat{t}$ such that with high constant\nprobability, $(1-\\epsilon)\\cdot t< \\widehat{t}<(1+\\epsilon)\\cdot t$, where $t$\nis the number of triangles in the graph $G$. The expected query complexity of\nthe algorithm is $\\!\\left(\\frac{n}{t^{1/3}} + \\min\\left\\{m,\n\\frac{m^{3/2}}{t}\\right\\}\\right)\\cdot {\\rm poly}(\\log n, 1/\\epsilon)$, where\n$n$ is the number of vertices in the graph and $m$ is the number of edges, and\nthe expected running time is $\\!\\left(\\frac{n}{t^{1/3}} +\n\\frac{m^{3/2}}{t}\\right)\\cdot {\\rm poly}(\\log n, 1/\\epsilon)$. We also prove\nthat $\\Omega\\!\\left(\\frac{n}{t^{1/3}} + \\min\\left\\{m,\n\\frac{m^{3/2}}{t}\\right\\}\\right)$ queries are necessary, thus establishing that\nthe query complexity of this algorithm is optimal up to polylogarithmic factors\nin $n$ (and the dependence on $1/\\epsilon$).\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 22:40:41 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 06:19:43 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2015 08:39:07 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Eden", "Talya", ""], ["Levi", "Amit", ""], ["Ron", "Dana", ""], ["Seshadhri", "C.", ""]]}, {"id": "1504.01033", "submitter": "Zhiwei Steven Wu", "authors": "Aaron Roth, Jonathan Ullman, Zhiwei Steven Wu", "title": "Watch and Learn: Optimizing from Revealed Preferences Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Stackelberg game is played between a leader and a follower. The leader\nfirst chooses an action, then the follower plays his best response. The goal of\nthe leader is to pick the action that will maximize his payoff given the\nfollower's best response. In this paper we present an approach to solving for\nthe leader's optimal strategy in certain Stackelberg games where the follower's\nutility function (and thus the subsequent best response of the follower) is\nunknown.\n  Stackelberg games capture, for example, the following interaction between a\nproducer and a consumer. The producer chooses the prices of the goods he\nproduces, and then a consumer chooses to buy a utility maximizing bundle of\ngoods. The goal of the seller here is to set prices to maximize his\nprofit---his revenue, minus the production cost of the purchased bundle. It is\nquite natural that the seller in this example should not know the buyer's\nutility function. However, he does have access to revealed preference\nfeedback---he can set prices, and then observe the purchased bundle and his own\nprofit. We give algorithms for efficiently solving, in terms of both\ncomputational and query complexity, a broad class of Stackelberg games in which\nthe follower's utility function is unknown, using only \"revealed preference\"\naccess to it. This class includes in particular the profit maximization\nproblem, as well as the optimal tolling problem in nonatomic congestion games,\nwhen the latency functions are unknown. Surprisingly, we are able to solve\nthese problems even though the optimization problems are non-convex in the\nleader's actions.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 18:13:15 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 03:34:42 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Roth", "Aaron", ""], ["Ullman", "Jonathan", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1504.01076", "submitter": "Ilya Razenshteyn", "authors": "Arturs Backurs, Piotr Indyk, Eric Price, Ilya Razenshteyn, David P.\n  Woodruff", "title": "Nearly-optimal bounds for sparse recovery in generic norms, with\n  applications to $k$-median sketching", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of trade-offs between sparsity and the number of\nmeasurements in sparse recovery schemes for generic norms. Specifically, for a\nnorm $\\|\\cdot\\|$, sparsity parameter $k$, approximation factor $K>0$, and\nprobability of failure $P>0$, we ask: what is the minimal value of $m$ so that\nthere is a distribution over $m \\times n$ matrices $A$ with the property that\nfor any $x$, given $Ax$, we can recover a $k$-sparse approximation to $x$ in\nthe given norm with probability at least $1-P$? We give a partial answer to\nthis problem, by showing that for norms that admit efficient linear sketches,\nthe optimal number of measurements $m$ is closely related to the doubling\ndimension of the metric induced by the norm $\\|\\cdot\\|$ on the set of all\n$k$-sparse vectors. By applying our result to specific norms, we cast known\nmeasurement bounds in our general framework (for the $\\ell_p$ norms, $p \\in\n[1,2]$) as well as provide new, measurement-efficient schemes (for the\nEarth-Mover Distance norm). The latter result directly implies more succinct\nlinear sketches for the well-studied planar $k$-median clustering problem.\nFinally, our lower bound for the doubling dimension of the EMD norm enables us\nto address the open question of [Frahling-Sohler, STOC'05] about the space\ncomplexity of clustering problems in the dynamic streaming model.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 02:59:25 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Backurs", "Arturs", ""], ["Indyk", "Piotr", ""], ["Price", "Eric", ""], ["Razenshteyn", "Ilya", ""], ["Woodruff", "David P.", ""]]}, {"id": "1504.01093", "submitter": "Alon Eden", "authors": "Ilan Reuven Cohen, Alon Eden, Amos Fiat, {\\L}ukasz Je\\.z", "title": "Pricing Online Decisions: Beyond Auctions", "comments": "Appeared in the ACM-SIAM Symposium on Discrete Algorithms (SODA),\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider dynamic pricing schemes in online settings where selfish agents\ngenerate online events. Previous work on online mechanisms has dealt almost\nentirely with the goal of maximizing social welfare or revenue in an auction\nsettings. This paper deals with quite general settings and minimizing social\ncosts. We show that appropriately computed posted prices allow one to achieve\nessentially the same performance as the best online algorithm. This holds in a\nwide variety of settings. Unlike online algorithms that learn about the event,\nand then make enforceable decisions, prices are posted without knowing the\nfuture events or even the current event, and are thus inherently dominant\nstrategy incentive compatible.\n  In particular we show that one can give efficient posted price mechanisms for\nmetrical task systems, some instances of the $k$-server problem, and metrical\nmatching problems. We give both deterministic and randomized algorithms. Such\nposted price mechanisms decrease the social cost dramatically over selfish\nbehavior where no decision incurs a charge. One alluring application of this is\nreducing the social cost of free parking exponentially.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 07:47:58 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Cohen", "Ilan Reuven", ""], ["Eden", "Alon", ""], ["Fiat", "Amos", ""], ["Je\u017c", "\u0141ukasz", ""]]}, {"id": "1504.01118", "submitter": "Krzysztof Choromanski", "authors": "Krzysztof Choromanski", "title": "Learning how to rank from heavily perturbed statistics - digraph\n  clustering approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking is one of the most fundamental problems in machine learning with\napplications in many branches of computer science such as: information\nretrieval systems, recommendation systems, machine translation and\ncomputational biology. Ranking objects based on possibly conflicting\npreferences is a central problem in voting research and social choice theory.\nIn this paper we present a new simple combinatorial ranking algorithm adapted\nto the preference-based setting. We apply this new algorithm to the well-known\nscenario where the edges of the preference tournament are determined by the\nmajority-voting model. It outperforms existing methods when it cannot be\nassumed that there exists global ranking of good enough quality and applies\ncombinatorial techniques that havent been used in the ranking context before.\nPerformed experiments show the superiority of the new algorithm over existing\nmethods, also over these that were designed to handle heavily perturbed\nstatistics. By combining our techniques with those presented in \\cite{mohri},\nwe obtain a purely combinatorial algorithm that answers correctly most of the\nqueries in the heterogeneous scenario, where the preference tournament is only\nlocally of good quality but is not necessarily pseudotransitive. As a byproduct\nof our methods, we obtain the algorithm solving clustering problem for the\ndirected planted partition model. To the best of our knowledge, it is the first\npurely combinatorial algorithm tackling this problem.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 13:06:37 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Choromanski", "Krzysztof", ""]]}, {"id": "1504.01130", "submitter": "Radu Grigore", "authors": "Maria Bruna, Radu Grigore, Stefan Kiefer, Jo\\\"el Ouaknine, James\n  Worrell", "title": "Proving the Herman-Protocol Conjecture", "comments": "ICALP 2016", "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2016.104", "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Herman's self-stabilisation algorithm, introduced 25 years ago, is a\nwell-studied synchronous randomised protocol for enabling a ring of $N$\nprocesses collectively holding any odd number of tokens to reach a stable state\nin which a single token remains. Determining the worst-case expected time to\nstabilisation is the central outstanding open problem about this protocol. It\nis known that there is a constant $h$ such that any initial configuration has\nexpected stabilisation time at most $h N^2$. Ten years ago, McIver and Morgan\nestablished a lower bound of $4/27 \\approx 0.148$ for $h$, achieved with three\nequally-spaced tokens, and conjectured this to be the optimal value of $h$. A\nseries of papers over the last decade gradually reduced the upper bound on $h$,\nwith the present record (achieved in 2014) standing at approximately $0.156$.\nIn this paper, we prove McIver and Morgan's conjecture and establish that $h =\n4/27$ is indeed optimal.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 15:55:55 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 15:17:53 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 22:34:01 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Bruna", "Maria", ""], ["Grigore", "Radu", ""], ["Kiefer", "Stefan", ""], ["Ouaknine", "Jo\u00ebl", ""], ["Worrell", "James", ""]]}, {"id": "1504.01304", "submitter": "Yuzhen Ye", "authors": "Yuzhen Ye and Haixu Tang", "title": "Utilizing de Bruijn graph of metagenome assembly for metatranscriptome\n  analysis", "comments": "8 pages, 4 figures, accepted in RECOMB-Seq 2015, under consideration\n  in Bioinformatics (a special issue for RECOMB-Seq/CBB)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metagenomics research has accelerated the studies of microbial organisms,\nproviding insights into the composition and potential functionality of various\nmicrobial communities. Metatranscriptomics (studies of the transcripts from a\nmixture of microbial species) and other meta-omics approaches hold even greater\npromise for providing additional insights into functional and regulatory\ncharacteristics of the microbial communities. Current metatranscriptomics\nprojects are often carried out without matched metagenomic datasets (of the\nsame microbial communities). For the projects that produce both\nmetatranscriptomic and metagenomic datasets, their analyses are often not\nintegrated. Metagenome assemblies are far from perfect, partially explaining\nwhy metagenome assemblies are not used for the analysis of metatranscriptomic\ndatasets. Here we report a reads mapping algorithm for mapping of short reads\nonto a de Bruijn graph of assemblies. A hash table of junction k-mers (k-mers\nspanning branching structures in the de Bruijn graph) is used to facilitate\nfast mapping of reads to the graph. We developed an application of this mapping\nalgorithm: a reference based approach to metatranscriptome assembly using\ngraphs of metagenome assembly as the reference. Our results show that this new\napproach (called TAG) helps to assemble substantially more transcripts that\notherwise would have been missed or truncated because of the fragmented nature\nof the reference metagenome. TAG was implemented in C++ and has been tested\nextensively on the linux platform. It is available for download as open source\nat http://omics.informatics.indiana.edu/TAG.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 15:17:29 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Ye", "Yuzhen", ""], ["Tang", "Haixu", ""]]}, {"id": "1504.01352", "submitter": "Sai Praneeth Reddy K", "authors": "Sai Praneeth Reddy, Dariusz R. Kowalski, and Shailesh Vaya", "title": "Multi-Broadcasting under the SINR Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-broadcast problem in multi-hop wireless networks under the\nSINR model deployed in the 2D Euclidean plane. In multi-broadcast, there are\n$k$ initial rumours, potentially belonging to different nodes, that must be\nforwarded to all $n$ nodes of the network. Furthermore, in each round a node\ncan only transmit a small message that could contain at most one initial rumor\nand $O(\\log n)$ control bits. In order to be successfully delivered to a node,\ntransmissions must satisfy the (Signal-to-Inference-and-Noise-Ratio) SINR\ncondition and have sufficiently strong signal at the receiver. We present\ndeterministic algorithms for multi-broadcast for different settings that\nreflect the different types of knowledge about the topology of the network\navailable to the nodes: (i) the whole network topology (ii) their own\ncoordinates and coordinates of their neighbors (iii) only their own\ncoordinates, and (iv) only their own ids and the ids of their neighbors. For\nthe former two settings, we present solutions that are scalable with respect to\nthe diameter of the network and the polylogarithm of the network size, i.e.,\n$\\log^c n$ for some constant $c> 0$, while the solutions for the latter two\nhave round complexity that is superlinear in the number of nodes. The last\nresult is of special significance, as it is the first result for the SINR model\nthat does not require nodes to know their coordinates in the plane (a very\nspecialized type of knowledge), but intricately exploits the understanding that\nnodes are implanted in the 2D Euclidean plane.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 18:55:28 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Reddy", "Sai Praneeth", ""], ["Kowalski", "Dariusz R.", ""], ["Vaya", "Shailesh", ""]]}, {"id": "1504.01431", "submitter": "Arturs Backurs", "authors": "Amir Abboud, Arturs Backurs, Virginia Vassilevska Williams", "title": "If the Current Clique Algorithms are Optimal, so is Valiant's Parser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CFG recognition problem is: given a context-free grammar $\\mathcal{G}$\nand a string $w$ of length $n$, decide if $w$ can be obtained from\n$\\mathcal{G}$. This is the most basic parsing question and is a core computer\nscience problem. Valiant's parser from 1975 solves the problem in\n$O(n^{\\omega})$ time, where $\\omega<2.373$ is the matrix multiplication\nexponent. Dozens of parsing algorithms have been proposed over the years, yet\nValiant's upper bound remains unbeaten. The best combinatorial algorithms have\nmildly subcubic $O(n^3/\\log^3{n})$ complexity.\n  Lee (JACM'01) provided evidence that fast matrix multiplication is needed for\nCFG parsing, and that very efficient and practical algorithms might be hard or\neven impossible to obtain. Lee showed that any algorithm for a more general\nparsing problem with running time $O(|\\mathcal{G}|\\cdot n^{3-\\varepsilon})$ can\nbe converted into a surprising subcubic algorithm for Boolean Matrix\nMultiplication. Unfortunately, Lee's hardness result required that the grammar\nsize be $|\\mathcal{G}|=\\Omega(n^6)$. Nothing was known for the more relevant\ncase of constant size grammars.\n  In this work, we prove that any improvement on Valiant's algorithm, even for\nconstant size grammars, either in terms of runtime or by avoiding the\ninefficiencies of fast matrix multiplication, would imply a breakthrough\nalgorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide if\nthere are $k$ that form a clique.\n  Besides classifying the complexity of a fundamental problem, our reduction\nhas led us to similar lower bounds for more modern and well-studied cubic time\nproblems for which faster algorithms are highly desirable in practice: RNA\nFolding, a central problem in computational biology, and Dyck Language Edit\nDistance, answering an open question of Saha (FOCS'14).\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 22:21:59 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 18:02:03 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Abboud", "Amir", ""], ["Backurs", "Arturs", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "1504.01459", "submitter": "Marek Suchenek", "authors": "Marek A. Suchenek", "title": "A Complete Worst-Case Analysis of Heapsort with Experimental\n  Verification of Its Results, A manuscript (MS)", "comments": "115 pages 41 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rigorous proof is presented that the number of comparisons of keys\nperformed in the worst case by ${\\tt Heapsort}$ on any array of size $N \\geq 2$\nis equal to: $ 2 (N-1)\\, ( \\, \\lg \\frac{N-1}{2} +\\varepsilon \\, ) - 2s_2(N) -\ne_2(N) + \\min (\\lfloor \\lg (N-1) \\rfloor, 2) + 6 + c, $ where $ \\varepsilon $,\ngiven by: $\\varepsilon = 1 + \\lceil \\lg \\, (N-1) \\rceil - \\lg \\, (N-1) -\n2^{\\lceil \\lg \\, (N-1) \\rceil - \\lg \\, (N-1)} ,$ is a function of $ N $ with\nthe minimum value 0 and and the supremum value $\\delta = 1 - \\lg e + \\lg \\lg e\n\\approx 0.0860713320559342$, $s_2(N)$ is the sum of all digits of the binary\nrepresentation of $N$, $e_2(N)$ is the exponent of $2$ in the prime\nfactorization of $N$, and $ c $ is a binary function on the set of integers\ndefined by: $c = 1$, if $N \\leq 2 ^{\\lceil \\lg N \\rceil} - 4$, and $c = 0$,\notherwise. An algorithm that generates worst-case input arrays of any size $ N\n\\geq 2 $ for ${\\tt Heapsort}$ is offered. The algorithm has been implemented in\nJava, runs in $O( N \\log N )$ time, and allows for precise experimental\nverification of the above formula.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 02:07:00 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Suchenek", "Marek A.", ""]]}, {"id": "1504.01497", "submitter": "Alexandros Efentakis", "authors": "Alexandros Efentakis, Dieter Pfoser", "title": "ReHub. Extending Hub Labels for Reverse k-Nearest Neighbor Queries on\n  Large-Scale networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quite recently, the algorithmic community has focused on solving multiple\nshortest-path query problems beyond simple vertex-to-vertex queries, especially\nin the context of road networks. Unfortunately, this research cannot be\ngeneralized for large-scale graphs, e.g., social or collaboration networks, or\nto efficiently answer Reverse k-Nearest Neighbor (RkNN) queries, which are of\npractical relevance to a wide range of applications. To remedy this, we propose\nReHub, a novel main-memory algorithm that extends the Hub Labeling technique to\nefficiently answer RkNN queries on large-scale networks. Our experimentation\nwill show that ReHub is the best overall solution for this type of queries,\nrequiring only minimal preprocessing and providing very fast query times.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 07:07:18 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2015 09:57:37 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Efentakis", "Alexandros", ""], ["Pfoser", "Dieter", ""]]}, {"id": "1504.01584", "submitter": "Chris Schwiegelshohn", "authors": "Marc Bury, Chris Schwiegelshohn", "title": "Random Projections for k-Means: Maintaining Coresets Beyond Merge &\n  Reduce", "comments": "This paper has been withdrawn due to an error in Theorem 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new construction for a small space summary satisfying the coreset\nguarantee of a data set with respect to the $k$-means objective function. The\nnumber of points required in an offline construction is in $\\tilde{O}(k\n\\epsilon^{-2}\\min(d,k\\epsilon^{-2}))$ which is minimal among all available\nconstructions.\n  Aside from two constructions with exponential dependence on the dimension,\nall known coresets are maintained in data streams via the merge and reduce\nframework, which incurs are large space dependency on $\\log n$. Instead, our\nconstruction crucially relies on Johnson-Lindenstrauss type embeddings which\ncombined with results from online algorithms give us a new technique for\nefficiently maintaining coresets in data streams without relying on merge and\nreduce. The final number of points stored by our algorithm in a data stream is\nin $\\tilde{O}(k^2 \\epsilon^{-2} \\log^2 n \\min(d,k\\epsilon^{-2}))$.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 12:45:29 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 11:17:08 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 08:04:32 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2015 14:20:48 GMT"}, {"version": "v5", "created": "Tue, 18 Feb 2020 16:44:34 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bury", "Marc", ""], ["Schwiegelshohn", "Chris", ""]]}, {"id": "1504.01623", "submitter": "Yoann Dieudonn\\'e", "authors": "S\\'ebastien Bouchard, Yoann Dieudonn\\'e and Bertrand Ducourthial", "title": "Byzantine Gathering in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates an open problem introduced in [14]. Two or more\nmobile agents start from different nodes of a network and have to accomplish\nthe task of gathering which consists in getting all together at the same node\nat the same time. An adversary chooses the initial nodes of the agents and\nassigns a different positive integer (called label) to each of them. Initially,\neach agent knows its label but does not know the labels of the other agents or\ntheir positions relative to its own. Agents move in synchronous rounds and can\ncommunicate with each other only when located at the same node. Up to f of the\nagents are Byzantine. A Byzantine agent can choose an arbitrary port when it\nmoves, can convey arbitrary information to other agents and can change its\nlabel in every round, in particular by forging the label of another agent or by\ncreating a completely new one.\n  What is the minimum number M of good agents that guarantees deterministic\ngathering of all of them, with termination?\n  We provide exact answers to this open problem by considering the case when\nthe agents initially know the size of the network and the case when they do\nnot. In the former case, we prove M=f+1 while in the latter, we prove M=f+2.\nMore precisely, for networks of known size, we design a deterministic algorithm\ngathering all good agents in any network provided that the number of good\nagents is at least f+1. For networks of unknown size, we also design a\ndeterministic algorithm ensuring the gathering of all good agents in any\nnetwork but provided that the number of good agents is at least f+2. Both of\nour algorithms are optimal in terms of required number of good agents, as each\nof them perfectly matches the respective lower bound on M shown in [14], which\nis of f+1 when the size of the network is known and of f+2 when it is unknown.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 14:40:31 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 12:56:52 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2015 06:38:07 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Bouchard", "S\u00e9bastien", ""], ["Dieudonn\u00e9", "Yoann", ""], ["Ducourthial", "Bertrand", ""]]}, {"id": "1504.01649", "submitter": "Ishay Haviv", "authors": "Ishay Haviv and Oded Regev", "title": "The List-Decoding Size of Fourier-Sparse Boolean Functions", "comments": "16 pages, CCC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function defined on the Boolean hypercube is $k$-Fourier-sparse if it has\nat most $k$ nonzero Fourier coefficients. For a function $f: \\mathbb{F}_2^n\n\\rightarrow \\mathbb{R}$ and parameters $k$ and $d$, we prove a strong upper\nbound on the number of $k$-Fourier-sparse Boolean functions that disagree with\n$f$ on at most $d$ inputs. Our bound implies that the number of uniform and\nindependent random samples needed for learning the class of $k$-Fourier-sparse\nBoolean functions on $n$ variables exactly is at most $O(n \\cdot k \\log k)$.\n  As an application, we prove an upper bound on the query complexity of testing\nBooleanity of Fourier-sparse functions. Our bound is tight up to a logarithmic\nfactor and quadratically improves on a result due to Gur and Tamuz (Chicago J.\nTheor. Comput. Sci., 2013).\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 15:46:17 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Haviv", "Ishay", ""], ["Regev", "Oded", ""]]}, {"id": "1504.01836", "submitter": "Kasper Green Larsen", "authors": "Raphael Clifford, Allan Gr{\\o}nlund, Kasper Green Larsen", "title": "New Unconditional Hardness Results for Dynamic and Online Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a resurgence of interest in lower bounds whose truth rests on\nthe conjectured hardness of well known computational problems. These\nconditional lower bounds have become important and popular due to the painfully\nslow progress on proving strong unconditional lower bounds. Nevertheless, the\nlong term goal is to replace these conditional bounds with unconditional ones.\nIn this paper we make progress in this direction by studying the cell probe\ncomplexity of two conjectured to be hard problems of particular importance:\nmatrix-vector multiplication and a version of dynamic set disjointness known as\nPatrascu's Multiphase Problem. We give improved unconditional lower bounds for\nthese problems as well as introducing new proof techniques of independent\ninterest. These include a technique capable of proving strong threshold lower\nbounds of the following form: If we insist on having a very fast query time,\nthen the update time has to be slow enough to compute a lookup table with the\nanswer to every possible query. This is the first time a lower bound of this\ntype has been proven.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 06:03:56 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Clifford", "Raphael", ""], ["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1504.01995", "submitter": "Noah Stephens-Davidowitz", "authors": "Divesh Aggarwal, Daniel Dadush, Noah Stephens-Davidowitz", "title": "Solving the Closest Vector Problem in $2^n$ Time--- The Discrete\n  Gaussian Strikes Again!", "comments": null, "journal-ref": "FOCS 2015", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a $2^{n+o(n)}$-time and space randomized algorithm for solving the\nexact Closest Vector Problem (CVP) on $n$-dimensional Euclidean lattices. This\nimproves on the previous fastest algorithm, the deterministic\n$\\widetilde{O}(4^{n})$-time and $\\widetilde{O}(2^{n})$-space algorithm of\nMicciancio and Voulgaris.\n  We achieve our main result in three steps. First, we show how to modify the\nsampling algorithm from [ADRS15] to solve the problem of discrete Gaussian\nsampling over lattice shifts, $L- t$, with very low parameters. While the\nactual algorithm is a natural generalization of [ADRS15], the analysis uses\nsubstantial new ideas. This yields a $2^{n+o(n)}$-time algorithm for\napproximate CVP for any approximation factor $\\gamma = 1+2^{-o(n/\\log n)}$.\nSecond, we show that the approximate closest vectors to a target vector $t$ can\nbe grouped into \"lower-dimensional clusters,\" and we use this to obtain a\nrecursive reduction from exact CVP to a variant of approximate CVP that\n\"behaves well with these clusters.\" Third, we show that our discrete Gaussian\nsampling algorithm can be used to solve this variant of approximate CVP.\n  The analysis depends crucially on some new properties of the discrete\nGaussian distribution and approximate closest vectors, which might be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 14:54:39 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 03:29:12 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Aggarwal", "Divesh", ""], ["Dadush", "Daniel", ""], ["Stephens-Davidowitz", "Noah", ""]]}, {"id": "1504.02035", "submitter": "Mohit Garg", "authors": "Mohit Garg, Jaikumar Radhakrishnan", "title": "Set Membership with a Few Bit Probes", "comments": "19 pages, expanded version of 'Set membership with a few bit probes.\n  SODA 2015: 776-784' (with additional results)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the bit-probe complexity of the set membership problem, where a\nset S of size at most n from a universe of size m is to be represented as a\nshort bit vector in order to answer membership queries of the form \"Is x in S?\"\nby adaptively probing the bit vector at t places. Let s(m,n,t) be the minimum\nnumber of bits of storage needed for such a scheme. Several recent works\ninvestigate s(m,n,t) for various ranges of the parameter; we obtain\nimprovements over some of the bounds shown by Buhrman, Miltersen,\nRadhakrishnan, and Srinivasan (2002) and Alon and Feige (2009).\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 17:02:30 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Garg", "Mohit", ""], ["Radhakrishnan", "Jaikumar", ""]]}, {"id": "1504.02044", "submitter": "Nicholas Harvey", "authors": "Nicholas Harvey and Jan Vondrak", "title": "An Algorithmic Proof of the Lovasz Local Lemma via Resampling Oracles", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lovasz Local Lemma is a seminal result in probabilistic combinatorics. It\ngives a sufficient condition on a probability space and a collection of events\nfor the existence of an outcome that simultaneously avoids all of those events.\nFinding such an outcome by an efficient algorithm has been an active research\ntopic for decades. Breakthrough work of Moser and Tardos (2009) presented an\nefficient algorithm for a general setting primarily characterized by a product\nstructure on the probability space.\n  In this work we present an efficient algorithm for a much more general\nsetting. Our main assumption is that there exist certain functions, called\nresampling oracles, that can be invoked to address the undesired occurrence of\nthe events. We show that, in all scenarios to which the original Lovasz Local\nLemma applies, there exist resampling oracles, although they are not\nnecessarily efficient. Nevertheless, for essentially all known applications of\nthe Lovasz Local Lemma and its generalizations, we have designed efficient\nresampling oracles. As applications of these techniques, we present new results\nfor packings of Latin transversals, rainbow matchings and rainbow spanning\ntrees.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 17:34:35 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 19:00:18 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 23:25:20 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Harvey", "Nicholas", ""], ["Vondrak", "Jan", ""]]}, {"id": "1504.02063", "submitter": "Thomas Courtade", "authors": "Ashwin Pananjady and Thomas A. Courtade", "title": "Compressing Sparse Sequences under Local Decodability Constraints", "comments": "8 pages, 1 figure. First five pages to appear in 2015 International\n  Symposium on Information Theory. This version contains supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variable-length source coding problem subject to local\ndecodability constraints. In particular, we investigate the blocklength scaling\nbehavior attainable by encodings of $r$-sparse binary sequences, under the\nconstraint that any source bit can be correctly decoded upon probing at most\n$d$ codeword bits. We consider both adaptive and non-adaptive access models,\nand derive upper and lower bounds that often coincide up to constant factors.\nNotably, such a characterization for the fixed-blocklength analog of our\nproblem remains unknown, despite considerable research over the last three\ndecades. Connections to communication complexity are also briefly discussed.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 18:19:27 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Pananjady", "Ashwin", ""], ["Courtade", "Thomas A.", ""]]}, {"id": "1504.02146", "submitter": "Patrick Lin", "authors": "Lisa Hellerstein, Devorah Kletenik, Patrick Lin", "title": "Discrete Stochastic Submodular Maximization: Adaptive vs. Non-Adaptive\n  vs. Offline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of stochastic monotone submodular function\nmaximization, subject to constraints. We give results on adaptivity gaps, and\non the gap between the optimal offline and online solutions. We present a\nprocedure that transforms a decision tree (adaptive algorithm) into a\nnon-adaptive chain. We prove that this chain achieves at least ${\\tau}$ times\nthe utility of the decision tree, over a product distribution and binary state\nspace, where ${\\tau} = \\min_{i,j} \\Pr[x_i=j]$. This proves an adaptivity gap of\n$1/{\\tau}$ (which is $2$ in the case of a uniform distribution) for the problem\nof stochastic monotone submodular maximization subject to state-independent\nconstraints. For a cardinality constraint, we prove that a simple adaptive\ngreedy algorithm achieves an approximation factor of $(1-1/e^{\\tau})$ with\nrespect to the optimal offline solution; previously, it has been proven that\nthe algorithm achieves an approximation factor of $(1-1/e)$ with respect to the\noptimal adaptive online solution. Finally, we show that there exists a\nnon-adaptive solution for the stochastic max coverage problem that is within a\nfactor $(1-1/e)$ of the optimal adaptive solution and within a factor of\n${\\tau}(1-1/e)$ of the optimal offline solution.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:26:28 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 22:30:01 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Hellerstein", "Lisa", ""], ["Kletenik", "Devorah", ""], ["Lin", "Patrick", ""]]}, {"id": "1504.02151", "submitter": "Brad Woods", "authors": "Brad Woods, Abraham Punnen and Tamon Stephen", "title": "A Linear Time Algorithm for the $3$-neighbour Traveling Salesman Problem\n  on Halin graphs and extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Quadratic Travelling Salesman Problem (QTSP) is to find a least cost\nHamilton cycle in an edge-weighted graph, where costs are defined on all pairs\nof edges contained in the Hamilton cycle. This is a more general version than\nthe commonly studied QTSP which only considers pairs of adjacent edges. We\ndefine a restricted version of QTSP, the $k$-neighbour TSP (TSP($k$)), and give\na linear time algorithm to solve TSP($k$) on a Halin graph for $k\\leq 3$. This\nalgorithm can be extended to solve TSP($k$) on any fully reducible class of\ngraphs for any fixed $k$ in polynomial time. This result generalizes\ncorresponding results for the standard TSP. TSP($k$) can be used to model\nvarious machine scheduling problems as well as an optimal routing problem for\nunmanned aerial vehicles (UAVs).\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 22:48:21 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 02:55:18 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2015 23:15:21 GMT"}, {"version": "v4", "created": "Sat, 19 Dec 2015 06:29:23 GMT"}, {"version": "v5", "created": "Mon, 4 Sep 2017 17:03:32 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Woods", "Brad", ""], ["Punnen", "Abraham", ""], ["Stephen", "Tamon", ""]]}, {"id": "1504.02268", "submitter": "Sayan Bhattacharya", "authors": "Sayan Bhattacharya and Monika Henzinger and Danupon Nanongkai and\n  Charalampos E. Tsourakakis", "title": "Space- and Time-Efficient Algorithm for Maintaining Dense Subgraphs on\n  One-Pass Dynamic Streams", "comments": "A preliminary version of this paper appeared in STOC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While in many graph mining applications it is crucial to handle a stream of\nupdates efficiently in terms of {\\em both} time and space, not much was known\nabout achieving such type of algorithm. In this paper we study this issue for a\nproblem which lies at the core of many graph mining applications called {\\em\ndensest subgraph problem}. We develop an algorithm that achieves time- and\nspace-efficiency for this problem simultaneously. It is one of the first of its\nkind for graph problems to the best of our knowledge.\n  In a graph $G = (V, E)$, the \"density\" of a subgraph induced by a subset of\nnodes $S \\subseteq V$ is defined as $|E(S)|/|S|$, where $E(S)$ is the set of\nedges in $E$ with both endpoints in $S$. In the densest subgraph problem, the\ngoal is to find a subset of nodes that maximizes the density of the\ncorresponding induced subgraph. For any $\\epsilon>0$, we present a dynamic\nalgorithm that, with high probability, maintains a $(4+\\epsilon)$-approximation\nto the densest subgraph problem under a sequence of edge insertions and\ndeletions in a graph with $n$ nodes. It uses $\\tilde O(n)$ space, and has an\namortized update time of $\\tilde O(1)$ and a query time of $\\tilde O(1)$. Here,\n$\\tilde O$ hides a $O(\\poly\\log_{1+\\epsilon} n)$ term. The approximation ratio\ncan be improved to $(2+\\epsilon)$ at the cost of increasing the query time to\n$\\tilde O(n)$. It can be extended to a $(2+\\epsilon)$-approximation\nsublinear-time algorithm and a distributed-streaming algorithm. Our algorithm\nis the first streaming algorithm that can maintain the densest subgraph in {\\em\none pass}. The previously best algorithm in this setting required $O(\\log n)$\npasses [Bahmani, Kumar and Vassilvitskii, VLDB'12]. The space required by our\nalgorithm is tight up to a polylogarithmic factor.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 11:43:52 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 08:20:42 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2015 10:22:13 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Bhattacharya", "Sayan", ""], ["Henzinger", "Monika", ""], ["Nanongkai", "Danupon", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1504.02306", "submitter": "S{\\o}ren Dahlgaard", "authors": "Stephen Alstrup and S{\\o}ren Dahlgaard and Mathias B{\\ae}k Tejs\n  Knudsen", "title": "Optimal induced universal graphs and adjacency labeling for trees", "comments": "A preliminary version of this paper appeared at FOCS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there exists a graph $G$ with $O(n)$ nodes, where any forest of\n$n$ nodes is a node-induced subgraph of $G$. Furthermore, for constant\narboricity $k$, the result implies the existence of a graph with $O(n^k)$ nodes\nthat contains all $n$-node graphs as node-induced subgraphs, matching a\n$\\Omega(n^k)$ lower bound. The lower bound and previously best upper bounds\nwere presented in Alstrup and Rauhe (FOCS'02). Our upper bounds are obtained\nthrough a $\\log_2 n +O(1)$ labeling scheme for adjacency queries in forests.\n  We hereby solve an open problem being raised repeatedly over decades, e.g. in\nKannan, Naor, Rudich (STOC 1988), Chung (J. of Graph Theory 1990), Fraigniaud\nand Korman (SODA 2010).\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 13:34:53 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 21:09:20 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Alstrup", "Stephen", ""], ["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""]]}, {"id": "1504.02420", "submitter": "Gregory Gutin", "authors": "D. Cohen, J. Crampton, A. Gagarin, G. Gutin and M. Jones", "title": "Algorithms for the workflow satisfiability problem engineered for\n  counting constraints", "comments": null, "journal-ref": null, "doi": "10.1007/s10878-015-9877-7", "report-no": null, "categories": "cs.DS cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The workflow satisfiability problem (WSP) asks whether there exists an\nassignment of authorized users to the steps in a workflow specification that\nsatisfies the constraints in the specification. The problem is NP-hard in\ngeneral, but several subclasses of the problem are known to be fixed-parameter\ntractable (FPT) when parameterized by the number of steps in the specification.\nIn this paper, we consider the WSP with user-independent counting constraints,\na large class of constraints for which the WSP is known to be FPT. We describe\nan efficient implementation of an FPT algorithm for solving this subclass of\nthe WSP and an experimental evaluation of this algorithm. The algorithm\niteratively generates all equivalence classes of possible partial solutions\nuntil, whenever possible, it finds a complete solution to the problem. We also\nprovide a reduction from a WSP instance to a pseudo-Boolean SAT instance. We\napply this reduction to the instances used in our experiments and solve the\nresulting PB SAT problems using SAT4J, a PB SAT solver. We compare the\nperformance of our algorithm with that of SAT4J and discuss which of the two\napproaches would be more effective in practice.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 18:54:57 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Cohen", "D.", ""], ["Crampton", "J.", ""], ["Gagarin", "A.", ""], ["Gutin", "G.", ""], ["Jones", "M.", ""]]}, {"id": "1504.02448", "submitter": "EPTCS", "authors": "Arend Rensink, Eduardo Zambon", "title": "Proceedings Graphs as Models", "comments": null, "journal-ref": "EPTCS 181, 2015", "doi": "10.4204/EPTCS.181", "report-no": null, "categories": "cs.SE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the (first) Graphs as Models (GaM)\n2015 workshop, held on 10-11 April 2015 in London, U.K., as a satellite\nworkshop of ETAPS 2015, the European Joint Conferences on Theory and Practice\nof Software. This new workshop combines the strengths of two pre-existing\nworkshop series: GT-VMT (Graph Transformation and Visual Modelling Techniques)\nand GRAPHITE (Graph Inspection and Traversal Engineering).\n  Graphs are used as models in all areas of computer science: examples are\nstate space graphs, control flow graphs, syntax graphs, UML-type models of all\nkinds, network layouts, social networks, dependency graphs, and so forth. Used\nto model a particular phenomenon or process, graphs are then typically analysed\nto find out properties of the modelled subject, or transformed to construct\nother types of models.\n  The workshop aimed at attracting and stimulating research on the techniques\nfor graph analysis, inspection and transformation, on a general level rather\nthan in any specific domain. In total, we received 15 submissions covering\nseveral different areas. Of these 15 submissions, nine were eventually accepted\nand appear in this volume.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 19:37:36 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Rensink", "Arend", ""], ["Zambon", "Eduardo", ""]]}, {"id": "1504.02526", "submitter": "Jian Li", "authors": "Jian Li, Yuval Rabani, Leonard J. Schulman, Chaitanya Swamy", "title": "Learning Arbitrary Statistical Mixtures of Discrete Distributions", "comments": "23 pages. Preliminary version in the Proceeding of the 47th ACM\n  Symposium on the Theory of Computing (STOC15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning from unlabeled samples very general\nstatistical mixture models on large finite sets. Specifically, the model to be\nlearned, $\\vartheta$, is a probability distribution over probability\ndistributions $p$, where each such $p$ is a probability distribution over $[n]\n= \\{1,2,\\dots,n\\}$. When we sample from $\\vartheta$, we do not observe $p$\ndirectly, but only indirectly and in very noisy fashion, by sampling from $[n]$\nrepeatedly, independently $K$ times from the distribution $p$. The problem is\nto infer $\\vartheta$ to high accuracy in transportation (earthmover) distance.\n  We give the first efficient algorithms for learning this mixture model\nwithout making any restricting assumptions on the structure of the distribution\n$\\vartheta$. We bound the quality of the solution as a function of the size of\nthe samples $K$ and the number of samples used. Our model and results have\napplications to a variety of unsupervised learning scenarios, including\nlearning topic models and collaborative filtering.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 01:17:28 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Li", "Jian", ""], ["Rabani", "Yuval", ""], ["Schulman", "Leonard J.", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1504.02564", "submitter": "Ragesh Jaiswal", "authors": "Anup Bhattacharya, Ragesh Jaiswal, Amit Kumar", "title": "Faster Algorithms for the Constrained k-means Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical center based clustering problems such as\n$k$-means/median/center assume that the optimal clusters satisfy the locality\nproperty that the points in the same cluster are close to each other. A number\nof clustering problems arise in machine learning where the optimal clusters do\nnot follow such a locality property. Consider a variant of the $k$-means\nproblem that may be regarded as a general version of such problems. Here, the\noptimal clusters $O_1, ..., O_k$ are an arbitrary partition of the dataset and\nthe goal is to output $k$-centers $c_1, ..., c_k$ such that the objective\nfunction $\\sum_{i=1}^{k} \\sum_{x \\in O_{i}} ||x - c_{i}||^2$ is minimized. It\nis not difficult to argue that any algorithm (without knowing the optimal\nclusters) that outputs a single set of $k$ centers, will not behave well as far\nas optimizing the above objective function is concerned. However, this does not\nrule out the existence of algorithms that output a list of such $k$ centers\nsuch that at least one of these $k$ centers behaves well. Given an error\nparameter $\\varepsilon > 0$, let $\\ell$ denote the size of the smallest list of\n$k$-centers such that at least one of the $k$-centers gives a $(1+\\varepsilon)$\napproximation w.r.t. the objective function above. In this paper, we show an\nupper bound on $\\ell$ by giving a randomized algorithm that outputs a list of\n$2^{\\tilde{O}(k/\\varepsilon)}$ $k$-centers. We also give a closely matching\nlower bound of $2^{\\tilde{\\Omega}(k/\\sqrt{\\varepsilon})}$. Moreover, our\nalgorithm runs in time $O \\left(n d \\cdot 2^{\\tilde{O}(k/\\varepsilon)}\n\\right)$. This is a significant improvement over the previous result of Ding\nand Xu who gave an algorithm with running time $O \\left(n d \\cdot (\\log{n})^{k}\n\\cdot 2^{poly(k/\\varepsilon)} \\right)$ and output a list of size $O\n\\left((\\log{n})^k \\cdot 2^{poly(k/\\varepsilon)} \\right)$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 07:03:58 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Bhattacharya", "Anup", ""], ["Jaiswal", "Ragesh", ""], ["Kumar", "Amit", ""]]}, {"id": "1504.02605", "submitter": "Dominik K\\\"oppl", "authors": "Johannes Fischer, Tomohiro I, and Dominik K\\\"oppl", "title": "Lempel Ziv Computation In Small Space (LZ-CISS)", "comments": "Full Version of CPM 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For both the Lempel Ziv 77- and 78-factorization we propose algorithms\ngenerating the respective factorization using $(1+\\epsilon) n \\lg n + O(n)$\nbits (for any positive constant $\\epsilon \\le 1$) working space (including the\nspace for the output) for any text of size \\$n\\$ over an integer alphabet in\n$O(n / \\epsilon^{2})$ time.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 09:34:07 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Fischer", "Johannes", ""], ["I", "Tomohiro", ""], ["K\u00f6ppl", "Dominik", ""]]}, {"id": "1504.02610", "submitter": "EPTCS", "authors": "Anton Wijs (Eindhoven University of Technology)", "title": "Confluence Detection for Transformations of Labelled Transition Systems", "comments": "In Proceedings GaM 2015, arXiv:1504.02448", "journal-ref": "EPTCS 181, 2015, pp. 1-15", "doi": "10.4204/EPTCS.181.1", "report-no": null, "categories": "cs.LO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of complex component software systems can be made more\nmanageable by first creating an abstract model and then incrementally adding\ndetails. Model transformation is an approach to add such details in a\ncontrolled way. In order for model transformation systems to be useful, it is\ncrucial that they are confluent, i.e. that when applied on a given model, they\nwill always produce a unique output model, independent of the order in which\nrules of the system are applied on the input. In this work, we consider\nLabelled Transition Systems (LTSs) to reason about the semantics of models, and\nLTS transformation systems to reason about model transformations. In related\nwork, the problem of confluence detection has been investigated for general\ngraph structures. We observe, however, that confluence can be detected more\nefficiently in special cases where the graphs have particular structural\nproperties. In this paper, we present a number of observations to detect\nconfluence of LTS transformation systems, and propose both a new confluence\ndetection algorithm and a conflict resolution algorithm based on them.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 09:39:47 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Wijs", "Anton", "", "Eindhoven University of Technology"]]}, {"id": "1504.02616", "submitter": "EPTCS", "authors": "Luc Moreau (University of Southampton)", "title": "Aggregation by Provenance Types: A Technique for Summarising Provenance\n  Graphs", "comments": "In Proceedings GaM 2015, arXiv:1504.02448", "journal-ref": "EPTCS 181, 2015, pp. 129-144", "doi": "10.4204/EPTCS.181.9", "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As users become confronted with a deluge of provenance data, dedicated\ntechniques are required to make sense of this kind of information. We present\nAggregation by Provenance Types, a provenance graph analysis that is capable of\ngenerating provenance graph summaries. It proceeds by converting provenance\npaths up to some length k to attributes, referred to as provenance types, and\nby grouping nodes that have the same provenance types. The summary also\nincludes numeric values representing the frequency of nodes and edges in the\noriginal graph. A quantitative evaluation and a complexity analysis show that\nthis technique is tractable; with small values of k, it can produce useful\nsummaries and can help detect outliers. We illustrate how the generated\nsummaries can further be used for conformance checking and visualization.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 09:41:37 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Moreau", "Luc", "", "University of Southampton"]]}, {"id": "1504.02644", "submitter": "Johannes Lengler", "authors": "Carola Doerr and Johannes Lengler", "title": "OneMax in Black-Box Models with Several Restrictions", "comments": "This is the full version of a paper accepted to GECCO 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box complexity studies lower bounds for the efficiency of\ngeneral-purpose black-box optimization algorithms such as evolutionary\nalgorithms and other search heuristics. Different models exist, each one being\ndesigned to analyze a different aspect of typical heuristics such as the memory\nsize or the variation operators in use. While most of the previous works focus\non one particular such aspect, we consider in this work how the combination of\nseveral algorithmic restrictions influence the black-box complexity. Our\ntestbed are so-called OneMax functions, a classical set of test functions that\nis intimately related to classic coin-weighing problems and to the board game\nMastermind.\n  We analyze in particular the combined memory-restricted ranking-based\nblack-box complexity of OneMax for different memory sizes. While its isolated\nmemory-restricted as well as its ranking-based black-box complexity for bit\nstrings of length $n$ is only of order $n/\\log n$, the combined model does not\nallow for algorithms being faster than linear in $n$, as can be seen by\nstandard information-theoretic considerations. We show that this linear bound\nis indeed asymptotically tight. Similar results are obtained for other memory-\nand offspring-sizes. Our results also apply to the (Monte Carlo) complexity of\nOneMax in the recently introduced elitist model, in which only the best-so-far\nsolution can be kept in the memory. Finally, we also provide improved lower\nbounds for the complexity of OneMax in the regarded models.\n  Our result enlivens the quest for natural evolutionary algorithms optimizing\nOneMax in $o(n \\log n)$ iterations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 11:56:17 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 12:37:13 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Doerr", "Carola", ""], ["Lengler", "Johannes", ""]]}, {"id": "1504.02671", "submitter": "Hjalte Wedel Vildh{\\o}j", "authors": "Philip Bille, Inge Li G{\\o}rtz, Mathias B{\\ae}k Tejs Knudsen, Moshe\n  Lewenstein, Hjalte Wedel Vildh{\\o}j", "title": "Longest Common Extensions in Sublinear Space", "comments": "An extended abstract of this paper has been accepted to CPM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest common extension problem (LCE problem) is to construct a data\nstructure for an input string $T$ of length $n$ that supports LCE$(i,j)$\nqueries. Such a query returns the length of the longest common prefix of the\nsuffixes starting at positions $i$ and $j$ in $T$. This classic problem has a\nwell-known solution that uses $O(n)$ space and $O(1)$ query time. In this paper\nwe show that for any trade-off parameter $1 \\leq \\tau \\leq n$, the problem can\nbe solved in $O(\\frac{n}{\\tau})$ space and $O(\\tau)$ query time. This\nsignificantly improves the previously best known time-space trade-offs, and\nalmost matches the best known time-space product lower bound.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 13:23:27 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Bille", "Philip", ""], ["G\u00f8rtz", "Inge Li", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Lewenstein", "Moshe", ""], ["Vildh\u00f8j", "Hjalte Wedel", ""]]}, {"id": "1504.02830", "submitter": "Kien Nguyen Trung", "authors": "Kien Trung Nguyen", "title": "The inverse $p$-maxian problem on trees with variable edge lengths", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We concern the problem of modifying the edge lengths of a tree in minimum\ntotal cost so that the prespecified $p$ vertices become the $p$-maxian with\nrespect to the new edge lengths. This problem is called the inverse $p$-maxian\nproblem on trees. \\textbf{Gassner} proposed efficient combinatorial alogrithm\nto solve the the inverse 1-maxian problem on trees in 2008. For the problem\nwith $p \\geq 2$, we claim that the problem can be reduced to finitely many\ninverse $2$-maxian problem. We then develop algorithms to solve the inverse\n$2$-maxian problem for various objective functions. The problem under\n$l_1$-norm can be formulated as a linear program and thus can be solved in\npolynomial time. Particularly, if the underlying tree is a star, then the\nproblem can be solved in linear time. We also devised $O(n\\log n)$ algorithms\nto solve the problems under Chebyshev norm and bottleneck Hamming distance,\nwhere $n$ is the number of vertices of the tree. Finally, the problem under\nweighted sum Hamming distance is $NP$-hard.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 04:17:01 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 17:25:42 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Nguyen", "Kien Trung", ""]]}, {"id": "1504.02876", "submitter": "Arkaidiusz Socala", "authors": "Marek Cygan, Jakub Pachocki, Arkadiusz Soca{\\l}a", "title": "The Hardness of Subgraph Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph Isomorphism is a very basic graph problem, where given two graphs\n$G$ and $H$ one is to check whether $G$ is a subgraph of $H$. Despite its\nsimple definition, the Subgraph Isomorphism problem turns out to be very broad,\nas it generalizes problems such as Clique, $r$-Coloring, Hamiltonicity, Set\nPacking and Bandwidth. However, for all of the mentioned problems\n$2^{\\mathcal{O}(n)}$ time algorithms exist, so a natural and frequently asked\nquestion in the past was whether there exists a $2^{\\mathcal{O}(n)}$ time\nalgorithm for Subgraph Isomorphism. In the monograph of Fomin and Kratsch\n[Springer'10] this question is highlighted as an open problem, among few\nothers.\n  Our main result is a reduction from 3-SAT, producing a subexponential number\nof sublinear instances of the Subgraph Isomorphism problem. In particular, our\nreduction implies a $2^{\\Omega(n \\sqrt{\\log n})}$ lower bound for Subgraph\nIsomorphism under the Exponential Time Hypothesis. This shows that there exist\nclasses of graphs that are strictly harder to embed than cliques or Hamiltonian\ncycles.\n  The core of our reduction consists of two steps. First, we preprocess and\npack variables and clauses of a 3-SAT formula into groups of logarithmic size.\nHowever, the grouping is not arbitrary, since as a result we obtain only a\nlimited interaction between the groups. In the second step, we overcome the\ntechnical hardness of encoding evaluations as permutations by a simple, yet\nfruitful scheme of guessing the sizes of preimages of an arbitrary mapping,\nreducing the case of arbitrary mapping to bijections. In fact, when applying\nthis step to a recent independent result of Fomin et al.[arXiv:1502.05447\n(2015)], who showed hardness of Graph Homomorphism, we can transfer their\nhardness result to Subgraph Isomorphism, implying a nearly tight lower bound of\n$2^{\\Omega(n \\log n / \\log \\log n)}$.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 13:54:51 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Cygan", "Marek", ""], ["Pachocki", "Jakub", ""], ["Soca\u0142a", "Arkadiusz", ""]]}, {"id": "1504.03026", "submitter": "Leonard Schulman", "authors": "Leonard J. Schulman, Alistair Sinclair", "title": "Analysis of a Classical Matrix Preconditioning Algorithm", "comments": "The previous version (1) (see also STOC'15) handled UB (\"unique\n  balance\") input matrices. In this version (2) we extend the work to handle\n  all input matrices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a classical iterative algorithm for balancing matrices in the\n$L_\\infty$ norm via a scaling transformation. This algorithm, which goes back\nto Osborne and Parlett \\& Reinsch in the 1960s, is implemented as a standard\npreconditioner in many numerical linear algebra packages. Surprisingly, despite\nits widespread use over several decades, no bounds were known on its rate of\nconvergence. In this paper we prove that, for any irreducible $n\\times n$ (real\nor complex) input matrix~$A$, a natural variant of the algorithm converges in\n$O(n^3\\log(n\\rho/\\varepsilon))$ elementary balancing operations, where $\\rho$\nmeasures the initial imbalance of~$A$ and $\\varepsilon$ is the target imbalance\nof the output matrix. (The imbalance of~$A$ is $\\max_i\n|\\log(a_i^{\\text{out}}/a_i^{\\text{in}})|$, where\n$a_i^{\\text{out}},a_i^{\\text{in}}$ are the maximum entries in magnitude in the\n$i$th row and column respectively.) This bound is tight up to the $\\log n$\nfactor. A balancing operation scales the $i$th row and column so that their\nmaximum entries are equal, and requires $O(m/n)$ arithmetic operations on\naverage, where $m$ is the number of non-zero elements in~$A$. Thus the running\ntime of the iterative algorithm is $\\tilde{O}(n^2m)$. This is the first time\nbound of any kind on any variant of the Osborne-Parlett-Reinsch algorithm. We\nalso prove a conjecture of Chen that characterizes those matrices for which the\nlimit of the balancing process is independent of the order in which balancing\noperations are performed.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 21:30:05 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2015 07:02:20 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Schulman", "Leonard J.", ""], ["Sinclair", "Alistair", ""]]}, {"id": "1504.03170", "submitter": "Serge Lawrencenko", "authors": "Serge Lawrencenko and Irina A. Duborkina", "title": "Search algorithms for efficient logistics chains", "comments": "10 pages, 1 figure, in Russian", "journal-ref": "Service in Russia and Abroad, Vol. 9 (2015), No. 2 (58), 37-48", "doi": "10.12737/11889", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistics networks arise whenever there is a transfer of material substance\nor objects (such as checked baggage on international flights) as well as\nenergy, information, or finance through links (channels). A general concept of\nlogistics network is suggested and motivated for modeling a service of any kind\nsupplied through links between the nodes of the network. The efficiency of a\nsingle link is defined to be the ratio of the volume of useful service at the\noutput node to the volume of expended service at the input node of the link\n(for a specific period of time). Similarly, the efficiency of a chain is the\nratio of the volume of service at the output to the volume of service at the\ninput of the chain. The overall efficiency of the chain is calculated as the\nproduct of the efficiencies of its links; the more efficiency of the chain, the\nless are the losses in the chain. This paper introduces the notion of\ninadequacy of service in such a way that the overall inadequacy of a chain is\nequal to the sum of the inadequacies of its links. So the efficiencies are\nbeing multiplied, whereas the inadequacies are being added. Thus, the\nantagonistic pair (efficiency, inadequacy) appears to be analogous to the pair\n(reliability, entropy) in communication theory. Various possible\ninterpretations of the proposed logistic model are presented: energy, material,\ninformation and financial networks. Four algorithms are provided for logistics\nchain search: two algorithms for finding the most effective chain from a\nspecified origin to a specified destination, and two algorithms for finding the\nguaranteed minimum level of service between any pair of unspecified nodes in a\ngiven network. An example is shown as to how one of the algorithms finds the\nmost efficient energy chain from the electrical substation to a specified user\nin a concrete energy network.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 11:51:31 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Lawrencenko", "Serge", ""], ["Duborkina", "Irina A.", ""]]}, {"id": "1504.03217", "submitter": "Pedro Henrique Gonz\\'alez", "authors": "Pedro Henrique Gonz\\'alez, Luidi Simonetti, Philippe Michelon, Carlos\n  Martinhon, Edcarllos Santos", "title": "A Variable Fixing Heuristic with Local Branching for the Fixed Charge\n  Uncapacitated Network Design Problem with User-optimal Flow", "comments": "32 pages, 10 figures, submitted to Computers and Operations Research", "journal-ref": null, "doi": "10.1016/j.cor.2016.06.016", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an iterated local search for the fixed-charge\nuncapacitated network design problem with user-optimal flow (FCNDP-UOF), which\nconcerns routing multiple commodities from its origin to its destination by\nsigning a network through selecting arcs, with an objective of minimizing the\nsum of the fixed costs of the selected arcs plus the sum of variable costs\nassociated to the flows on each arc. Besides that, since the FCNDP-UOF is a\nbi-level problem, each commodity has to be transported through a shortest path,\nconcerning the edges length, in the built network. The proposed algorithm\ngenerate a initial solution using a variable fixing heuristic. Then a local\nbranching strategy is applied to improve the quality of the solution. At last,\nan efficient perturbation strategy is presented to perform cycle-based moves to\nexplore different parts of the solution space. Computational experiments shows\nthat the proposed solution method consistently produces high-quality solutions\nin reasonable computational times.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 15:30:15 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Gonz\u00e1lez", "Pedro Henrique", ""], ["Simonetti", "Luidi", ""], ["Michelon", "Philippe", ""], ["Martinhon", "Carlos", ""], ["Santos", "Edcarllos", ""]]}, {"id": "1504.03294", "submitter": "Pan Peng", "authors": "Artur Czumaj, Pan Peng, Christian Sohler", "title": "Testing Cluster Structure of Graphs", "comments": "Full version of STOC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recognizing the cluster structure of a graph in the\nframework of property testing in the bounded degree model. Given a parameter\n$\\varepsilon$, a $d$-bounded degree graph is defined to be $(k,\n\\phi)$-clusterable, if it can be partitioned into no more than $k$ parts, such\nthat the (inner) conductance of the induced subgraph on each part is at least\n$\\phi$ and the (outer) conductance of each part is at most\n$c_{d,k}\\varepsilon^4\\phi^2$, where $c_{d,k}$ depends only on $d,k$. Our main\nresult is a sublinear algorithm with the running time\n$\\widetilde{O}(\\sqrt{n}\\cdot\\mathrm{poly}(\\phi,k,1/\\varepsilon))$ that takes as\ninput a graph with maximum degree bounded by $d$, parameters $k$, $\\phi$,\n$\\varepsilon$, and with probability at least $\\frac23$, accepts the graph if it\nis $(k,\\phi)$-clusterable and rejects the graph if it is $\\varepsilon$-far from\n$(k, \\phi^*)$-clusterable for $\\phi^* = c'_{d,k}\\frac{\\phi^2\n\\varepsilon^4}{\\log n}$, where $c'_{d,k}$ depends only on $d,k$. By the lower\nbound of $\\Omega(\\sqrt{n})$ on the number of queries needed for testing graph\nexpansion, which corresponds to $k=1$ in our problem, our algorithm is\nasymptotically optimal up to polylogarithmic factors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 18:52:02 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Czumaj", "Artur", ""], ["Peng", "Pan", ""], ["Sohler", "Christian", ""]]}, {"id": "1504.03391", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Jan Vondrak", "title": "Tight Bounds on Low-degree Spectral Concentration of Submodular and XOS\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular and fractionally subadditive (or equivalently XOS) functions play\na fundamental role in combinatorial optimization, algorithmic game theory and\nmachine learning. Motivated by learnability of these classes of functions from\nrandom examples, we consider the question of how well such functions can be\napproximated by low-degree polynomials in $\\ell_2$ norm over the uniform\ndistribution. This question is equivalent to understanding of the concentration\nof Fourier weight on low-degree coefficients, a central concept in Fourier\nanalysis. We show that\n  1. For any submodular function $f:\\{0,1\\}^n \\rightarrow [0,1]$, there is a\npolynomial of degree $O(\\log (1/\\epsilon) / \\epsilon^{4/5})$ approximating $f$\nwithin $\\epsilon$ in $\\ell_2$, and there is a submodular function that requires\ndegree $\\Omega(1/\\epsilon^{4/5})$.\n  2. For any XOS function $f:\\{0,1\\}^n \\rightarrow [0,1]$, there is a\npolynomial of degree $O(1/\\epsilon)$ and there exists an XOS function that\nrequires degree $\\Omega(1/\\epsilon)$.\n  This improves on previous approaches that all showed an upper bound of\n$O(1/\\epsilon^2)$ for submodular and XOS functions. The best previous lower\nbound was $\\Omega(1/\\epsilon^{2/3})$ for monotone submodular functions. Our\ntechniques reveal new structural properties of submodular and XOS functions and\nthe upper bounds lead to nearly optimal PAC learning algorithms for these\nclasses of functions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 23:51:45 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2015 22:26:16 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Feldman", "Vitaly", ""], ["Vondrak", "Jan", ""]]}, {"id": "1504.03561", "submitter": "Gregory Gutin", "authors": "Jason Crampton, Andrei Gagarin, Gregory Gutin, Mark Jones and Magnus\n  Wahlstrom", "title": "On the Workflow Satisfiability Problem with Class-Independent\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A workflow specification defines sets of steps and users. An authorization\npolicy determines for each user a subset of steps the user is allowed to\nperform. Other security requirements, such as separation-of-duty, impose\nconstraints on which subsets of users may perform certain subsets of steps. The\n\\emph{workflow satisfiability problem} (WSP) is the problem of determining\nwhether there exists an assignment of users to workflow steps that satisfies\nall such authorizations and constraints. An algorithm for solving WSP is\nimportant, both as a static analysis tool for workflow specifications, and for\nthe construction of run-time reference monitors for workflow management\nsystems. Given the computational difficulty of WSP, it is important,\nparticularly for the second application, that such algorithms are as efficient\nas possible.\n  We introduce class-independent constraints, enabling us to model scenarios\nwhere the set of users is partitioned into groups, and the identities of the\nuser groups are irrelevant to the satisfaction of the constraint. We prove that\nsolving WSP is fixed-parameter tractable (FPT) for this class of constraints\nand develop an FPT algorithm that is useful in practice. We compare the\nperformance of the FPT algorithm with that of SAT4J (a pseudo-Boolean SAT\nsolver) in computational experiments, which show that our algorithm\nsignificantly outperforms SAT4J for many instances of WSP. User-independent\nconstraints, a large class of constraints including many practical ones, are a\nspecial case of class-independent constraints for which WSP was proved to be\nFPT (Cohen {\\em et al.}, J. Artif. Intel. Res. 2014). Thus our results\nconsiderably extend our knowledge of the fixed-parameter tractability of WSP.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 14:25:22 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 15:21:23 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Crampton", "Jason", ""], ["Gagarin", "Andrei", ""], ["Gutin", "Gregory", ""], ["Jones", "Mark", ""], ["Wahlstrom", "Magnus", ""]]}, {"id": "1504.03666", "submitter": "Mordechai Shalom", "authors": "Arman Boyac{\\i} and T{\\i}naz Ekim and Mordechai Shalom", "title": "The Maximum Cut Problem in Co-bipartite Chain Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \\emph{co-bipartite chain} graph is a co-bipartite graph in which the\nneighborhoods of the vertices in each clique can be linearly ordered with\nrespect to inclusion. It is known that the maximum cut problem (MaxCut) is\nNP-Hard in co-bipartite graphs. We consider MaxCut in co-bipartite chain\ngraphs. We first consider the twin-free case and present an explicit solution.\nWe then show that MaxCut is polynomial time solvable in this graph class.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 19:21:05 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Boyac\u0131", "Arman", ""], ["Ekim", "T\u0131naz", ""], ["Shalom", "Mordechai", ""]]}, {"id": "1504.03670", "submitter": "Andreas Bjorklund", "authors": "Andreas Bj\\\"orklund", "title": "Coloring Graphs having Few Colorings over Path Decompositions", "comments": "Strengthened result from uniquely $k$-colorable graphs to graphs with\n  few $k$-colorings. Also improved running time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lokshtanov, Marx, and Saurabh SODA 2011 proved that there is no\n$(k-\\epsilon)^{\\operatorname{pw}(G)}\\operatorname{poly}(n)$ time algorithm for\ndeciding if an $n$-vertex graph $G$ with pathwidth $\\operatorname{pw}(G)$\nadmits a proper vertex coloring with $k$ colors unless the Strong Exponential\nTime Hypothesis (SETH) is false. We show here that nevertheless, when\n$k>\\lfloor \\Delta/2 \\rfloor + 1$, where $\\Delta$ is the maximum degree in the\ngraph $G$, there is a better algorithm, at least when there are few colorings.\nWe present a Monte Carlo algorithm that given a graph $G$ along with a path\ndecomposition of $G$ with pathwidth $\\operatorname{pw}(G)$ runs in $(\\lfloor\n\\Delta/2 \\rfloor + 1)^{\\operatorname{pw}(G)}\\operatorname{poly}(n)s$ time, that\ndistinguishes between $k$-colorable graphs having at most $s$ proper\n$k$-colorings and non-$k$-colorable graphs. We also show how to obtain a\n$k$-coloring in the same asymptotic running time. Our algorithm avoids\nviolating SETH for one since high degree vertices still cost too much and the\nmentioned hardness construction uses a lot of them.\n  We exploit a new variation of the famous Alon--Tarsi theorem that has an\nalgorithmic advantage over the original form. The original theorem shows a\ngraph has an orientation with outdegree less than $k$ at every vertex, with a\ndifferent number of odd and even Eulerian subgraphs only if the graph is\n$k$-colorable, but there is no known way of efficiently finding such an\norientation. Our new form shows that if we instead count another difference of\neven and odd subgraphs meeting modular degree constraints at every vertex\npicked uniformly at random, we have a fair chance of getting a non-zero value\nif the graph has few $k$-colorings. Yet every non-$k$-colorable graph gives a\nzero difference, so a random set of constraints stands a good chance of being\nuseful for separating the two cases.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 19:44:39 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 07:30:23 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2015 14:13:43 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Bj\u00f6rklund", "Andreas", ""]]}, {"id": "1504.03812", "submitter": "Ben Strasser", "authors": "Michael Hamann, Ben Strasser", "title": "Graph Bisection with Pareto-Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce FlowCutter, a novel algorithm to compute a set of edge cuts or\nnode separators that optimize cut size and balance in the Pareto-sense. Our\ncore algorithm solves the balanced connected st-edge-cut problem, where two\ngiven nodes s and t must be separated by removing edges to obtain two connected\nparts. Using the core algorithm we build variants that compute node separators\nand are independent of s and t. Using the Pareto-set we can identify cuts with\na particularly good trade-off between cut size and balance that can be used to\ncompute contraction and minimum fill-in orders, which can be used in\nCustomizable Contraction Hierarchies (CCH), a speed-up technique for shortest\npath computations. Our core algorithm runs in O(cm) time where m is the number\nof edges and c the cut size. This makes it well-suited for large graphs with\nsmall cuts, such as road graphs, which are our primary application. For road\ngraphs we present an extensive experimental study demonstrating that FlowCutter\noutperforms the current state of the art both in terms of cut sizes as well as\nCCH performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 08:06:15 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 11:07:33 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 16:30:28 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Hamann", "Michael", ""], ["Strasser", "Ben", ""]]}, {"id": "1504.03842", "submitter": "Marc Bury", "authors": "Marc Bury", "title": "OBDDs and (Almost) $k$-wise Independent Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OBDD-based graph algorithms deal with the characteristic function of the edge\nset E of a graph $G = (V,E)$ which is represented by an OBDD and solve\noptimization problems by mainly using functional operations. We present an\nOBDD-based algorithm which uses randomization for the first time. In\nparticular, we give a maximal matching algorithm with $O(\\log^3 \\vert V \\vert)$\nfunctional operations in expectation. This algorithm may be of independent\ninterest. The experimental evaluation shows that this algorithm outperforms\nknown OBDD-based algorithms for the maximal matching problem.\n  In order to use randomization, we investigate the OBDD complexity of $2^n$\n(almost) $k$-wise independent binary random variables. We give a OBDD\nconstruction of size $O(n)$ for $3$-wise independent random variables and show\na lower bound of $2^{\\Omega(n)}$ on the OBDD size for $k \\geq 4$. The best\nknown lower bound was $\\Omega(2^n/n)$ for $k \\approx \\log n$ due to Kabanets.\nWe also give a very simple construction of $2^n$ $(\\varepsilon, k)$-wise\nindependent binary random variables by constructing a random OBDD of width $O(n\nk^2/\\varepsilon)$.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 09:47:12 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Bury", "Marc", ""]]}, {"id": "1504.03856", "submitter": "Priyanka Mukhopadhyay Ms", "authors": "Priyanka Mukhopadhyay and Youming Qiao", "title": "Sparse multivariate polynomial interpolation in the basis of Schubert\n  polynomials", "comments": "20 pages; some typos corrected", "journal-ref": "Computational Complexity, 2017 Dec, 26(4), pp. 881-909", "doi": "10.1007/s00037-016-0142-y", "report-no": null, "categories": "cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schubert polynomials were discovered by A. Lascoux and M. Sch\\\"utzenberger in\nthe study of cohomology rings of flag manifolds in 1980's. These polynomials\ngeneralize Schur polynomials, and form a linear basis of multivariate\npolynomials. In 2003, Lenart and Sottile introduced skew Schubert polynomials,\nwhich generalize skew Schur polynomials, and expand in the Schubert basis with\nthe generalized Littlewood-Richardson coefficients.\n  In this paper we initiate the study of these two families of polynomials from\nthe perspective of computational complexity theory. We first observe that skew\nSchubert polynomials, and therefore Schubert polynomials, are in $\\CountP$\n(when evaluating on non-negative integral inputs) and $\\VNP$.\n  Our main result is a deterministic algorithm that computes the expansion of a\npolynomial $f$ of degree $d$ in $\\Z[x_1, \\dots, x_n]$ in the basis of Schubert\npolynomials, assuming an oracle computing Schubert polynomials. This algorithm\nruns in time polynomial in $n$, $d$, and the bit size of the expansion. This\ngeneralizes, and derandomizes, the sparse interpolation algorithm of symmetric\npolynomials in the Schur basis by Barvinok and Fomin (Advances in Applied\nMathematics, 18(3):271--285). In fact, our interpolation algorithm is general\nenough to accommodate any linear basis satisfying certain natural properties.\n  Applications of the above results include a new algorithm that computes the\ngeneralized Littlewood-Richardson coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 10:41:50 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 00:40:07 GMT"}, {"version": "v3", "created": "Sun, 26 Jun 2016 22:02:43 GMT"}, {"version": "v4", "created": "Thu, 29 Sep 2016 08:07:02 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Mukhopadhyay", "Priyanka", ""], ["Qiao", "Youming", ""]]}, {"id": "1504.03878", "submitter": "Yann Busnel", "authors": "Emmanuelle Anceaume (INRIA - SUPELEC, IRISA), Yann Busnel (ENSAI,\n  INRIA - IRISA), Ernst Schulte-Geers, Bruno Sericola (INRIA - IRISA)", "title": "Optimization results for a generalized coupon collector problem", "comments": "arXiv admin note: text overlap with arXiv:1402.5245", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper a generalized coupon collector problem, which consists\nin analyzing the time needed to collect a given number of distinct coupons that\nare drawn from a set of coupons with an arbitrary probability distribution. We\nsuppose that a special coupon called the null coupon can be drawn but never\nbelongs to any collection. In this context, we prove that the almost uniform\ndistribution, for which all the non-null coupons have the same drawing\nprobability, is the distribution which stochastically minimizes the time needed\nto collect a fixed number of distinct coupons. Moreover, we show that in a\ngiven closed subset of probability distributions, the distribution with all its\nentries, but one, equal to the smallest possible value is the one, which\nstochastically maximizes the time needed to collect a fixed number of distinct\ncoupons. An computer science application shows the utility of these results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 12:13:35 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Anceaume", "Emmanuelle", "", "INRIA - SUPELEC, IRISA"], ["Busnel", "Yann", "", "ENSAI,\n  INRIA - IRISA"], ["Schulte-Geers", "Ernst", "", "INRIA - IRISA"], ["Sericola", "Bruno", "", "INRIA - IRISA"]]}, {"id": "1504.03987", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira", "title": "Random Laplacian matrices and convex relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The largest eigenvalue of a matrix is always larger or equal than its largest\ndiagonal entry. We show that for a large class of random Laplacian matrices,\nthis bound is essentially tight: the largest eigenvalue is, up to lower order\nterms, often the size of the largest diagonal entry.\n  Besides being a simple tool to obtain precise estimates on the largest\neigenvalue of a large class of random Laplacian matrices, our main result\nsettles a number of open problems related to the tightness of certain convex\nrelaxation-based algorithms. It easily implies the optimality of the\nsemidefinite relaxation approaches to problems such as $\\mathbb{Z}_2$\nSynchronization and Stochastic Block Model recovery. Interestingly, this result\nreadily implies the connectivity threshold for Erd\\H{o}s-R\\'{e}nyi graphs and\nsuggests that these three phenomena are manifestations of the same underlying\nprinciple. The main tool is a recent estimate on the spectral norm of matrices\nwith independent entries by van Handel and the author.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 18:36:26 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 00:13:05 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Bandeira", "Afonso S.", ""]]}, {"id": "1504.04044", "submitter": "Mahmoud Abo Khamis", "authors": "Mahmoud Abo Khamis, Hung Q. Ngo, Atri Rudra", "title": "FAQ: Questions Asked Frequently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study the Functional Aggregate Query (FAQ) problem, which\nencompasses many frequently asked questions in constraint satisfaction,\ndatabases, matrix operations, probabilistic graphical models and logic. This is\nour main conceptual contribution.\n  We then present a simple algorithm called \"InsideOut\" to solve this general\nproblem. InsideOut is a variation of the traditional dynamic programming\napproach for constraint programming based on variable elimination. Our\nvariation adds a couple of simple twists to basic variable elimination in order\nto deal with the generality of FAQ, to take full advantage of Grohe and Marx's\nfractional edge cover framework, and of the analysis of recent worst-case\noptimal relational join algorithms.\n  As is the case with constraint programming and graphical model inference, to\nmake InsideOut run efficiently we need to solve an optimization problem to\ncompute an appropriate 'variable ordering'. The main technical contribution of\nthis work is a precise characterization of when a variable ordering is\n'semantically equivalent' to the variable ordering given by the input FAQ\nexpression. Then, we design an approximation algorithm to find an equivalent\nvariable ordering that has the best 'fractional FAQ-width'. Our results imply a\nhost of known and a few new results in graphical model inference, matrix\noperations, relational joins, and logic.\n  We also briefly explain how recent algorithms on beyond worst-case analysis\nfor joins and those for solving SAT and #SAT can be viewed as variable\nelimination to solve FAQ over compactly represented input functions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 20:31:00 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 23:38:27 GMT"}, {"version": "v3", "created": "Mon, 11 May 2015 17:49:01 GMT"}, {"version": "v4", "created": "Sun, 9 Aug 2015 04:05:49 GMT"}, {"version": "v5", "created": "Fri, 15 Apr 2016 09:55:45 GMT"}, {"version": "v6", "created": "Mon, 6 Feb 2017 01:14:58 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Khamis", "Mahmoud Abo", ""], ["Ngo", "Hung Q.", ""], ["Rudra", "Atri", ""]]}, {"id": "1504.04073", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "The Parametric Closure Problem", "comments": "22 pages, 8 figures. A preliminary version of this paper appeared at\n  the 14th Algorithms and Data Structures Symposium (WADS), Victoria, BC,\n  August 2015, Springer, Lecture Notes in Comp. Sci. 9214 (2015), pp. 327-338", "journal-ref": "ACM Trans. Algorithms 14 (1): Article 2, 2018", "doi": "10.1145/3147212", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the parametric closure problem, in which the input is a partially\nordered set whose elements have linearly varying weights and the goal is to\ncompute the sequence of minimum-weight lower sets of the partial order as the\nweights vary. We give polynomial time solutions to many important special cases\nof this problem including semiorders, reachability orders of bounded-treewidth\ngraphs, partial orders of bounded width, and series-parallel partial orders.\nOur result for series-parallel orders provides a significant generalization of\na previous result of Carlson and Eppstein on bicriterion subtree problems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 00:41:36 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 01:11:08 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1504.04103", "submitter": "Ananda Theertha Suresh", "authors": "Moein Falahatgar and Ashkan Jafarpour and Alon Orlitsky and\n  Venkatadheeraj Pichapathi and Ananda Theertha Suresh", "title": "Faster Algorithms for Testing under Conditional Sampling", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable recent interest in distribution-tests whose\nrun-time and sample requirements are sublinear in the domain-size $k$. We study\ntwo of the most important tests under the conditional-sampling model where each\nquery specifies a subset $S$ of the domain, and the response is a sample drawn\nfrom $S$ according to the underlying distribution.\n  For identity testing, which asks whether the underlying distribution equals a\nspecific given distribution or $\\epsilon$-differs from it, we reduce the known\ntime and sample complexities from $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ to\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$, thereby matching the information\ntheoretic lower bound. For closeness testing, which asks whether two\ndistributions underlying observed data sets are equal or different, we reduce\nexisting complexity from $\\tilde{\\mathcal{O}}(\\epsilon^{-4} \\log^5 k)$ to an\neven sub-logarithmic $\\tilde{\\mathcal{O}}(\\epsilon^{-5} \\log \\log k)$ thus\nproviding a better bound to an open problem in Bertinoro Workshop on Sublinear\nAlgorithms [Fisher, 2004].\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 05:56:34 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Falahatgar", "Moein", ""], ["Jafarpour", "Ashkan", ""], ["Orlitsky", "Alon", ""], ["Pichapathi", "Venkatadheeraj", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1504.04169", "submitter": "Parter Merav", "authors": "Merav Parter and David Peleg", "title": "Fault Tolerant BFS Structures: A Reinforcement-Backup Tradeoff", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper initiates the study of fault resilient network structures that mix\ntwo orthogonal protection mechanisms: (a) {\\em backup}, namely, augmenting the\nstructure with many (redundant) low-cost but fault-prone components, and (b)\n{\\em reinforcement}, namely, acquiring high-cost but fault-resistant\ncomponents. To study the trade-off between these two mechanisms in a concrete\nsetting, we address the problem of designing a $(b,r)$ {\\em fault-tolerant} BFS\n(or $(b,r)$ FT-BFS for short) structure, namely, a subgraph $H$ of the network\n$G$ consisting of two types of edges: a set $E' \\subseteq E$ of $r(n)$\nfault-resistant {\\em reinforcement} edges, which are assumed to never fail, and\na (larger) set $E(H) \\setminus E'$ of $b(n)$ fault-prone {\\em backup} edges,\nsuch that subsequent to the failure of a single fault-prone backup edge $e \\in\nE \\setminus E'$, the surviving part of $H$ still contains an BFS spanning tree\nfor (the surviving part of) $G$, satisfying $dist(s,v,H\\setminus \\{e\\}) \\leq\ndist(s,v,G\\setminus \\{e\\})$ for every $v \\in V$ and $e \\in E \\setminus E'$. We\nestablish the following tradeoff between $b(n)$ and $r(n)$: For every real\n$\\epsilon \\in (0,1]$, if $r(n) = {\\tilde\\Theta}(n^{1-\\epsilon})$, then $b(n) =\n{\\tilde\\Theta}(n^{1+\\epsilon})$ is necessary and sufficient.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 10:08:25 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Parter", "Merav", ""], ["Peleg", "David", ""]]}, {"id": "1504.04470", "submitter": "Yixin Cao", "authors": "Yixin Cao", "title": "Unit Interval Editing is Fixed-Parameter Tractable", "comments": "An extended abstract of this paper has appeared in the proceedings of\n  ICALP 2015. Update: The proof of Lemma 4.2 has been completely rewritten; an\n  appendix is provided for a brief overview of related graph classes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph~$G$ and integers $k_1$, $k_2$, and~$k_3$, the unit interval\nediting problem asks whether $G$ can be transformed into a unit interval graph\nby at most $k_1$ vertex deletions, $k_2$ edge deletions, and $k_3$ edge\nadditions. We give an algorithm solving this problem in time $2^{O(k\\log\nk)}\\cdot (n+m)$, where $k := k_1 + k_2 + k_3$, and $n, m$ denote respectively\nthe numbers of vertices and edges of $G$. Therefore, it is fixed-parameter\ntractable parameterized by the total number of allowed operations.\n  Our algorithm implies the fixed-parameter tractability of the unit interval\nedge deletion problem, for which we also present a more efficient algorithm\nrunning in time $O(4^k \\cdot (n + m))$. Another result is an $O(6^k \\cdot (n +\nm))$-time algorithm for the unit interval vertex deletion problem,\nsignificantly improving the algorithm of van 't Hof and Villanger, which runs\nin time $O(6^k \\cdot n^6)$.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 09:43:30 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 03:10:31 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Cao", "Yixin", ""]]}, {"id": "1504.04498", "submitter": "Esben Halvorsen", "authors": "Stephen Alstrup, Cyril Gavoille, Esben Bistrup Halvorsen, Holger\n  Petersen", "title": "Simpler, faster and shorter labels for distances in graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider how to assign labels to any undirected graph with n nodes such\nthat, given the labels of two nodes and no other information regarding the\ngraph, it is possible to determine the distance between the two nodes. The\nchallenge in such a distance labeling scheme is primarily to minimize the\nmaximum label lenght and secondarily to minimize the time needed to answer\ndistance queries (decoding). Previous schemes have offered different trade-offs\nbetween label lengths and query time. This paper presents a simple algorithm\nwith shorter labels and shorter query time than any previous solution, thereby\nimproving the state-of-the-art with respect to both label length and query time\nin one single algorithm. Our solution addresses several open problems\nconcerning label length and decoding time and is the first improvement of label\nlength for more than three decades.\n  More specifically, we present a distance labeling scheme with label size (log\n3)/2 + o(n) (logarithms are in base 2) and O(1) decoding time. This outperforms\nall existing results with respect to both size and decoding time, including\nWinkler's (Combinatorica 1983) decade-old result, which uses labels of size\n(log 3)n and O(n/log n) decoding time, and Gavoille et al. (SODA'01), which\nuses labels of size 11n + o(n) and O(loglog n) decoding time. In addition, our\nalgorithm is simpler than the previous ones. In the case of integral edge\nweights of size at most W, we present almost matching upper and lower bounds\nfor label sizes. For r-additive approximation schemes, where distances can be\noff by an additive constant r, we give both upper and lower bounds. In\nparticular, we present an upper bound for 1-additive approximation schemes\nwhich, in the unweighted case, has the same size (ignoring second order terms)\nas an adjacency scheme: n/2. We also give results for bipartite graphs and for\nexact and 1-additive distance oracles.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 13:00:23 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Alstrup", "Stephen", ""], ["Gavoille", "Cyril", ""], ["Halvorsen", "Esben Bistrup", ""], ["Petersen", "Holger", ""]]}, {"id": "1504.04616", "submitter": "Paul Medvedev", "authors": "Rayan Chikhi and Paul Medvedev and Martin Milanic and Sofya\n  Raskhodnikova", "title": "On the readability of overlap digraphs", "comments": "This is a full version of a conference paper of the same title at the\n  26th Annual Symposium on Combinatorial Pattern Matching (CPM 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the graph parameter readability and study it as a function of\nthe number of vertices in a graph. Given a digraph D, an injective overlap\nlabeling assigns a unique string to each vertex such that there is an arc from\nx to y if and only if x properly overlaps y. The readability of D is the\nminimum string length for which an injective overlap labeling exists. In\napplications that utilize overlap digraphs (e.g., in bioinformatics),\nreadability reflects the length of the strings from which the overlap digraph\nis constructed. We study the asymptotic behaviour of readability by casting it\nin purely graph theoretic terms (without any reference to strings). We prove\nupper and lower bounds on readability for certain graph families and general\ngraphs\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 19:52:27 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Chikhi", "Rayan", ""], ["Medvedev", "Paul", ""], ["Milanic", "Martin", ""], ["Raskhodnikova", "Sofya", ""]]}, {"id": "1504.04650", "submitter": "Stefan Kraft", "authors": "Klaus Jansen and Stefan Erich Julius Kraft", "title": "A Faster FPTAS for the Unbounded Knapsack Problem", "comments": "30 pages, pdfLaTeX; typos corrected, additional smaller explanations\n  to improve readability and to avoid confusion; full version of paper\n  presented at IWOCA 2015, reviewer comments were taken into account", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Unbounded Knapsack Problem (UKP) is a well-known variant of the famous\n0-1 Knapsack Problem (0-1 KP). In contrast to 0-1 KP, an arbitrary number of\ncopies of every item can be taken in UKP. Since UKP is NP-hard, fully\npolynomial time approximation schemes (FPTAS) are of great interest. Such\nalgorithms find a solution arbitrarily close to the optimum $\\mathrm{OPT}(I)$,\ni.e. of value at least $(1-\\varepsilon) \\mathrm{OPT}(I)$ for $\\varepsilon > 0$,\nand have a running time polynomial in the input length and\n$\\frac{1}{\\varepsilon}$. For over thirty years, the best FPTAS was due to\nLawler with a running time in $O(n + \\frac{1}{\\varepsilon^3})$ and a space\ncomplexity in $O(n + \\frac{1}{\\varepsilon^2})$, where $n$ is the number of\nknapsack items. We present an improved FPTAS with a running time in $O(n +\n\\frac{1}{\\varepsilon^2} \\log^3 \\frac{1}{\\varepsilon})$ and a space bound in\n$O(n + \\frac{1}{\\varepsilon} \\log^2 \\frac{1}{\\varepsilon})$. This directly\nimproves the running time of the fastest known approximation schemes for Bin\nPacking and Strip Packing, which have to approximately solve UKP instances as\nsubproblems.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 22:13:48 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 16:37:48 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Jansen", "Klaus", ""], ["Kraft", "Stefan Erich Julius", ""]]}, {"id": "1504.04679", "submitter": "Longkun Guo l", "authors": "Longkun Guo, Hong Shen, Wenxing Zhu", "title": "Efficient Approximation Algorithms for Multi-Antennae Largest Weight\n  Data Retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/TMC.2017.2696009", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a mobile network, wireless data broadcast over $m$ channels (frequencies)\nis a powerful means for distributed dissemination of data to clients who access\nthe channels through multi-antennae equipped on their mobile devices. The\n$\\delta$-antennae largest weight data retrieval ($\\delta$ALWDR) problem is to\ncompute a schedule for downloading a subset of data items that has a maximum\ntotal weight using $\\delta$ antennae in a given time interval. In this paper,\nwe propose a ratio $1-\\frac{1}{e}-\\epsilon$ approximation algorithm for the\n$\\delta$-antennae largest weight data retrieval ($\\delta$ALWDR) problem that\nhas the same ratio as the known result but a significantly improved time\ncomplexity of $O(2^{\\frac{1}{\\epsilon}}\\frac{1}{\\epsilon}m^{7}T^{3.5}L)$ from\n$O(\\epsilon^{3.5}m^{\\frac{3.5}{\\epsilon}}T^{3.5}L)$ when $\\delta=1$\n\\cite{lu2014data}. To our knowledge, our algorithm represents the first ratio\n$1-\\frac{1}{e}-\\epsilon$ approximation solution to $\\delta$ALWDR for the\ngeneral case of arbitrary $\\delta$. To achieve this, we first give a ratio\n$1-\\frac{1}{e}$ algorithm for the $\\gamma$-separated $\\delta$ALWDR\n($\\delta$A$\\gamma$LWDR) with runtime $O(m^{7}T^{3.5}L)$, under the assumption\nthat every data item appears at most once in each segment of\n$\\delta$A$\\gamma$LWDR, for any input of maximum length $L$ on $m$ channels in\n$T$ time slots. Then, we show that we can retain the same ratio for\n$\\delta$A$\\gamma$LWDR without this assumption at the cost of increased time\ncomplexity to $O(2^{\\gamma}m^{7}T^{3.5}L)$. This result immediately yields an\napproximation solution of same ratio and time complexity for $\\delta$ALWDR,\npresenting a significant improvement of the known time complexity of ratio\n$1-\\frac{1}{e}-\\epsilon$ approximation to the problem.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 03:26:01 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 14:09:46 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 01:59:05 GMT"}, {"version": "v4", "created": "Tue, 6 Jun 2017 07:00:37 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Guo", "Longkun", ""], ["Shen", "Hong", ""], ["Zhu", "Wenxing", ""]]}, {"id": "1504.04686", "submitter": "Raef Bassily", "authors": "Raef Bassily and Adam Smith", "title": "Local, Private, Efficient Protocols for Succinct Histograms", "comments": null, "journal-ref": null, "doi": "10.1145/2746539.2746632", "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give efficient protocols and matching accuracy lower bounds for frequency\nestimation in the local model for differential privacy. In this model,\nindividual users randomize their data themselves, sending differentially\nprivate reports to an untrusted server that aggregates them.\n  We study protocols that produce a succinct histogram representation of the\ndata. A succinct histogram is a list of the most frequent items in the data\n(often called \"heavy hitters\") along with estimates of their frequencies; the\nfrequency of all other items is implicitly estimated as 0.\n  If there are $n$ users whose items come from a universe of size $d$, our\nprotocols run in time polynomial in $n$ and $\\log(d)$. With high probability,\nthey estimate the accuracy of every item up to error\n$O\\left(\\sqrt{\\log(d)/(\\epsilon^2n)}\\right)$ where $\\epsilon$ is the privacy\nparameter. Moreover, we show that this much error is necessary, regardless of\ncomputational efficiency, and even for the simple setting where only one item\nappears with significant frequency in the data set.\n  Previous protocols (Mishra and Sandler, 2006; Hsu, Khanna and Roth, 2012) for\nthis task either ran in time $\\Omega(d)$ or had much worse error (about\n$\\sqrt[6]{\\log(d)/(\\epsilon^2n)}$), and the only known lower bound on error was\n$\\Omega(1/\\sqrt{n})$.\n  We also adapt a result of McGregor et al (2010) to the local setting. In a\nmodel with public coins, we show that each user need only send 1 bit to the\nserver. For all known local protocols (including ours), the transformation\npreserves computational efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 06:23:34 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Bassily", "Raef", ""], ["Smith", "Adam", ""]]}, {"id": "1504.04690", "submitter": "Eyal Skop", "authors": "Shay Mozes, Eyal E. Skop", "title": "Efficient Vertex-Label Distance Oracles for Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distance queries in vertex-labeled planar graphs. For any fixed\n$0 < \\epsilon \\leq 1/2$ we show how to preprocess a directed planar graph with\nvertex labels and arc lengths into a data structure that answers queries of the\nfollowing form. Given a vertex $u$ and a label $\\lambda$ return a\n$(1+\\epsilon)$-approximation of the distance from $u$ to its closest vertex\nwith label $\\lambda$. For a directed planar graph with $n$ vertices, such that\nthe ratio of the largest to smallest arc length is bounded by $N$, the\npreprocessing time is $O(\\epsilon^{-2}n\\lg^{3}{n}\\lg(nN))$, the data structure\nsize is $O(\\epsilon^{-1}n\\lg{n}\\lg(nN))$, and the query time is\n$O(\\lg\\lg{n}\\lg\\lg(nN) + \\epsilon^{-1})$. We also point out that a vertex label\ndistance oracle for undirected planar graphs suggested in an earlier version of\nthis paper is incorrect.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 07:24:00 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 07:29:35 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Mozes", "Shay", ""], ["Skop", "Eyal E.", ""]]}, {"id": "1504.04757", "submitter": "Radu Grigore", "authors": "Radu Grigore and Stefan Kiefer", "title": "Tree Buffers", "comments": "CAV 2015 (The final publication is available at link.springer.com)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In runtime verification, the central problem is to decide if a given program\nexecution violates a given property. In online runtime verification, a monitor\nobserves a program's execution as it happens. If the program being observed has\nhard real-time constraints, then the monitor inherits them. In the presence of\nhard real-time constraints it becomes a challenge to maintain enough\ninformation to produce error traces, should a property violation be observed.\nIn this paper we introduce a data structure, called tree buffer, that solves\nthis problem in the context of automata-based monitors: If the monitor itself\nrespects hard real-time constraints, then enriching it by tree buffers makes it\npossible to provide error traces, which are essential for diagnosing defects.\nWe show that tree buffers are also useful in other application domains. For\nexample, they can be used to implement functionality of capturing groups in\nregular expressions. We prove optimal asymptotic bounds for our data structure,\nand validate them using empirical data from two sources: regular expression\nsearching through Wikipedia, and runtime verification of execution traces\nobtained from the DaCapo test suite.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 20:39:15 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 11:43:56 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Grigore", "Radu", ""], ["Kiefer", "Stefan", ""]]}, {"id": "1504.04767", "submitter": "Guru  Guruganesh", "authors": "Nikhil Bansal and Anupam Gupta and Guru Guruganesh", "title": "On the Lov\\'asz Theta function for Independent Sets in Sparse Graphs", "comments": "Extended abstract to appear at the STOC 2015 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maximum independent set problem on graphs with maximum\ndegree~$d$. We show that the integrality gap of the Lov\\'asz\n$\\vartheta$-function based SDP is $\\widetilde{O}(d/\\log^{3/2} d)$. This\nimproves on the previous best result of $\\widetilde{O}(d/\\log d)$, and almost\nmatches the integrality gap of $\\widetilde{O}(d/\\log^2 d)$ recently shown for\nstronger SDPs, namely those obtained using poly-$(\\log(d))$ levels of the\n$SA^+$ semidefinite hierarchy. The improvement comes from an improved\nRamsey-theoretic bound on the independence number of $K_r$-free graphs for\nlarge values of $r$.\n  We also show how to obtain an algorithmic version of the above-mentioned\n$SA^+$-based integrality gap result, via a coloring algorithm of Johansson. The\nresulting approximation guarantee of $\\widetilde{O}(d/\\log^2 d)$ matches the\nbest unique-games-based hardness result up to lower-order poly-$(\\log\\log d)$\nfactors.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 21:55:07 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 14:17:38 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Bansal", "Nikhil", ""], ["Gupta", "Anupam", ""], ["Guruganesh", "Guru", ""]]}, {"id": "1504.04931", "submitter": "David Eppstein", "authors": "David Eppstein, J. Michael McCarthy, and Brian E. Parrish", "title": "Rooted Cycle Bases", "comments": "12 pages with 10 additional pages of appendices and 10 figures.\n  Extended version of a paper to appear at the 14th Algorithms and Data\n  Structures Symposium (WADS), Victoria, BC, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cycle basis in an undirected graph is a minimal set of simple cycles whose\nsymmetric differences include all Eulerian subgraphs of the given graph. We\ndefine a rooted cycle basis to be a cycle basis in which all cycles contain a\nspecified root edge, and we investigate the algorithmic problem of constructing\nrooted cycle bases. We show that a given graph has a rooted cycle basis if and\nonly if the root edge belongs to its 2-core and the 2-core is\n2-vertex-connected, and that constructing such a basis can be performed\nefficiently. We show that in an unweighted or positively weighted graph, it is\npossible to find the minimum weight rooted cycle basis in polynomial time.\nAdditionally, we show that it is NP-complete to find a fundamental rooted cycle\nbasis (a rooted cycle basis in which each cycle is formed by combining paths in\na fixed spanning tree with a single additional edge) but that the problem can\nbe solved by a fixed-parameter-tractable algorithm when parameterized by\nclique-width.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 04:02:52 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Eppstein", "David", ""], ["McCarthy", "J. Michael", ""], ["Parrish", "Brian E.", ""]]}, {"id": "1504.04993", "submitter": "Marco Kuhlmann", "authors": "Marco Kuhlmann", "title": "Tabulation of Noncrossing Acyclic Digraphs", "comments": "9 pages, several figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present an algorithm that, given a number $n \\geq 1$, computes a compact\nrepresentation of the set of all noncrossing acyclic digraphs with $n$ nodes.\nThis compact representation can be used as the basis for a wide range of\ndynamic programming algorithms on these graphs. As an illustration, along with\nthis note I am releasing the implementation of an algorithm for counting the\nnumber of noncrossing acyclic digraphs of a given size. The same tabulation can\nbe modified to count other classes of combinatorial structures, including\nweakly connected noncrossing acyclic digraphs, general noncrossing digraphs,\nnoncrossing undirected graphs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 09:58:53 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Kuhlmann", "Marco", ""]]}, {"id": "1504.05009", "submitter": "David Adjiashvili", "authors": "David Adjiashvili", "title": "Non-Uniform Robust Network Design in Planar Graphs", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust optimization is concerned with constructing solutions that remain\nfeasible also when a limited number of resources is removed from the solution.\nMost studies of robust combinatorial optimization to date made the assumption\nthat every resource is equally vulnerable, and that the set of scenarios is\nimplicitly given by a single budget constraint. This paper studies a robustness\nmodel of a different kind. We focus on \\textbf{bulk-robustness}, a model\nrecently introduced~\\cite{bulk} for addressing the need to model non-uniform\nfailure patterns in systems.\n  We significantly extend the techniques used in~\\cite{bulk} to design\napproximation algorithm for bulk-robust network design problems in planar\ngraphs. Our techniques use an augmentation framework, combined with linear\nprogramming (LP) rounding that depends on a planar embedding of the input\ngraph. A connection to cut covering problems and the dominating set problem in\ncircle graphs is established. Our methods use few of the specifics of\nbulk-robust optimization, hence it is conceivable that they can be adapted to\nsolve other robust network design problems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 10:53:45 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Adjiashvili", "David", ""]]}, {"id": "1504.05140", "submitter": "Thomas Pajor", "authors": "Hannah Bast, Daniel Delling, Andrew Goldberg, Matthias\n  M\\\"uller-Hannemann, Thomas Pajor, Peter Sanders, Dorothea Wagner, Renato F.\n  Werneck", "title": "Route Planning in Transportation Networks", "comments": "This is an updated version of the technical report MSR-TR-2014-4,\n  previously published by Microsoft Research. This work was mostly done while\n  the authors Daniel Delling, Andrew Goldberg, and Renato F. Werneck were at\n  Microsoft Research Silicon Valley", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey recent advances in algorithms for route planning in transportation\nnetworks. For road networks, we show that one can compute driving directions in\nmilliseconds or less even at continental scale. A variety of techniques provide\ndifferent trade-offs between preprocessing effort, space requirements, and\nquery time. Some algorithms can answer queries in a fraction of a microsecond,\nwhile others can deal efficiently with real-time traffic. Journey planning on\npublic transportation systems, although conceptually similar, is a\nsignificantly harder problem due to its inherent time-dependent and\nmulticriteria nature. Although exact algorithms are fast enough for interactive\nqueries on metropolitan transit systems, dealing with continent-sized instances\nrequires simplifications or heavy preprocessing. The multimodal route planning\nproblem, which seeks journeys combining schedule-based transportation (buses,\ntrains) with unrestricted modes (walking, driving), is even harder, relying on\napproximate solutions even for metropolitan inputs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 18:04:20 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Bast", "Hannah", ""], ["Delling", "Daniel", ""], ["Goldberg", "Andrew", ""], ["M\u00fcller-Hannemann", "Matthias", ""], ["Pajor", "Thomas", ""], ["Sanders", "Peter", ""], ["Wagner", "Dorothea", ""], ["Werneck", "Renato F.", ""]]}, {"id": "1504.05240", "submitter": "Meng-Tsung Tsai", "authors": "Martin Farach-Colton and Meng-Tsung Tsai", "title": "On the complexity of computing prime tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large arithmetic computations rely on tables of all primes less than\n$n$. For example, the fastest algorithms for computing $n!$ takes time\n$O(M(n\\log n) + P(n))$, where $M(n)$ is the time to multiply two $n$-bit\nnumbers, and $P(n)$ is the time to compute a prime table up to $n$. The fastest\nalgorithm to compute $\\binom{n}{n/2}$ also uses a prime table. We show that it\ntakes time $O(M(n) + P(n))$.\n  In various models, the best bound on $P(n)$ is greater than $M(n\\log n)$,\ngiven advances in the complexity of multiplication \\cite{Furer07,De08}. In this\npaper, we give two algorithms to computing prime tables and analyze their\ncomplexity on a multitape Turing machine, one of the standard models for\nanalyzing such algorithms. These two algorithms run in time $O(M(n\\log n))$ and\n$O(n\\log^2 n/\\log \\log n)$, respectively. We achieve our results by speeding up\nAtkin's sieve.\n  Given that the current best bound on $M(n)$ is $n\\log n 2^{O(\\log^*n)}$, the\nsecond algorithm is faster and improves on the previous best algorithm by a\nfactor of $\\log^2\\log n$. Our fast prime-table algorithms speed up both the\ncomputation of $n!$ and $\\binom{n}{n/2}$.\n  Finally, we show that computing the factorial takes $\\Omega(M(n \\log^{4/7 -\n\\epsilon} n))$ for any constant $\\epsilon > 0$ assuming only multiplication is\nallowed.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 21:34:17 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Farach-Colton", "Martin", ""], ["Tsai", "Meng-Tsung", ""]]}, {"id": "1504.05287", "submitter": "Tengyu Ma", "authors": "Rong Ge, Tengyu Ma", "title": "Decomposing Overcomplete 3rd Order Tensors using Sum-of-Squares\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor rank and low-rank tensor decompositions have many applications in\nlearning and complexity theory. Most known algorithms use unfoldings of tensors\nand can only handle rank up to $n^{\\lfloor p/2 \\rfloor}$ for a $p$-th order\ntensor in $\\mathbb{R}^{n^p}$. Previously no efficient algorithm can decompose\n3rd order tensors when the rank is super-linear in the dimension. Using ideas\nfrom sum-of-squares hierarchy, we give the first quasi-polynomial time\nalgorithm that can decompose a random 3rd order tensor decomposition when the\nrank is as large as $n^{3/2}/\\textrm{polylog} n$.\n  We also give a polynomial time algorithm for certifying the injective norm of\nrandom low rank tensors. Our tensor decomposition algorithm exploits the\nrelationship between injective norm and the tensor components. The proof relies\non interesting tools for decoupling random variables to prove better matrix\nconcentration bounds, which can be useful in other settings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 03:21:53 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1504.05457", "submitter": "Yury Kashnitsky", "authors": "Yury Kashnitsky, Sergei O. Kuznetsov", "title": "Graphlet-based lazy associative graph classification", "comments": "This paper has been withdrawn by the author due to the incomplete set\n  of necessary definitions and experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the graph classification problem and introduces a\nmodification of the lazy associative classification method to efficiently\nhandle intersections of graphs. Graph intersections are approximated with all\ncommon subgraphs up to a fixed size similarly to what is done with graphlet\nkernels. We explain the idea of the algorithm with a toy example and describe\nour experiments with a predictive toxicology dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:12:45 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 20:46:47 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Kashnitsky", "Yury", ""], ["Kuznetsov", "Sergei O.", ""]]}, {"id": "1504.05476", "submitter": "Micha{\\l} Pilipczuk", "authors": "D\\'aniel Marx and Micha{\\l} Pilipczuk", "title": "Optimal parameterized algorithms for planar facility location problems\n  using Voronoi diagrams", "comments": "64 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a general family of facility location problems defined on planar\ngraphs and on the 2-dimensional plane. In these problems, a subset of $k$\nobjects has to be selected, satisfying certain packing (disjointness) and\ncovering constraints. Our main result is showing that, for each of these\nproblems, the $n^{O(k)}$ time brute force algorithm of selecting $k$ objects\ncan be improved to $n^{O(\\sqrt{k})}$ time. The algorithm is based on an idea\nthat was introduced recently in the design of geometric QPTASs, but was not yet\nused for exact algorithms and for planar graphs. We focus on the Voronoi\ndiagram of a hypothetical solution of $k$ objects, guess a balanced separator\ncycle of this Voronoi diagram to obtain a set that separates the solution in a\nbalanced way, and then recurse on the resulting subproblems. We complement our\nstudy by giving evidence that packing problems have $n^{O(\\sqrt{k})}$ time\nalgorithms for a much more general class of objects than covering problems\nhave.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:47:32 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1504.05477", "submitter": "Christopher Musco", "authors": "Cameron Musco and Christopher Musco", "title": "Randomized Block Krylov Methods for Stronger and Faster Approximate\n  Singular Value Decomposition", "comments": "Neural Information Processing Systems 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since being analyzed by Rokhlin, Szlam, and Tygert and popularized by Halko,\nMartinsson, and Tropp, randomized Simultaneous Power Iteration has become the\nmethod of choice for approximate singular value decomposition. It is more\naccurate than simpler sketching algorithms, yet still converges quickly for any\nmatrix, independently of singular value gaps. After $\\tilde{O}(1/\\epsilon)$\niterations, it gives a low-rank approximation within $(1+\\epsilon)$ of optimal\nfor spectral norm error.\n  We give the first provable runtime improvement on Simultaneous Iteration: a\nsimple randomized block Krylov method, closely related to the classic Block\nLanczos algorithm, gives the same guarantees in just\n$\\tilde{O}(1/\\sqrt{\\epsilon})$ iterations and performs substantially better\nexperimentally. Despite their long history, our analysis is the first of a\nKrylov subspace method that does not depend on singular value gaps, which are\nunreliable in practice.\n  Furthermore, while it is a simple accuracy benchmark, even $(1+\\epsilon)$\nerror for spectral norm low-rank approximation does not imply that an algorithm\nreturns high quality principal components, a major issue for data applications.\nWe address this problem for the first time by showing that both Block Krylov\nIteration and a minor modification of Simultaneous Iteration give nearly\noptimal PCA for any matrix. This result further justifies their strength over\nnon-iterative sketching methods.\n  Finally, we give insight beyond the worst case, justifying why both\nalgorithms can run much faster in practice than predicted. We clarify how\nsimple techniques can take advantage of common matrix properties to\nsignificantly improve runtime.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:48:44 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 23:43:50 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2015 03:55:11 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2015 19:35:08 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1504.05515", "submitter": "Ignasi Sau", "authors": "Julien Baste, Luerbio Faria, Sulamita Klein, Ignasi Sau", "title": "Parameterized complexity dichotomy for $(r,\\ell)$-Vertex Deletion", "comments": "After the first version of this article appeared in arXiv, we learnt\n  that Kolay and Panolan [abs/1504.08120] obtained simultaneously and\n  independently some of the results of this article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two integers $r, \\ell \\geq 0$, a graph $G = (V, E)$ is an\n$(r,\\ell)$-graph if $V$ can be partitioned into $r$ independent sets and $\\ell$\ncliques. In the parameterized $(r,\\ell)$-Vertex Deletion problem, given a graph\n$G$ and an integer $k$, one has to decide whether at most $k$ vertices can be\nremoved from $G$ to obtain an $(r,\\ell)$-graph. This problem is NP-hard if\n$r+\\ell \\geq 1$ and encompasses several relevant problems such as Vertex Cover\nand Odd Cycle Transversal. The parameterized complexity of $(r,\\ell)$-Vertex\nDeletion was known for all values of $(r,\\ell)$ except for $(2,1)$, $(1,2)$,\nand $(2,2)$. We prove that each of these three cases is FPT and, furthermore,\nsolvable in single-exponential time, which is asymptotically optimal in terms\nof $k$. We consider as well the version of $(r,\\ell)$-Vertex Deletion where the\nset of vertices to be removed has to induce an independent set, and provide\nalso a parameterized complexity dichotomy for this problem.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 17:23:06 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 20:56:49 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Baste", "Julien", ""], ["Faria", "Luerbio", ""], ["Klein", "Sulamita", ""], ["Sau", "Ignasi", ""]]}, {"id": "1504.05535", "submitter": "Markus Lohrey", "authors": "Moses Ganardi, Danny Hucke, Markus Lohrey, and Eric Noeth", "title": "Tree compression using string grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the compressed representation of a ranked tree by a (string)\nstraight-line program (SLP) for its preorder traversal, and compare it with the\nwell-studied representation by straight-line context free tree grammars (which\nare also known as tree straight-line programs or TSLPs). Although SLPs turn out\nto be exponentially more succinct than TSLPs, we show that many simple tree\nqueries can still be performed efficiently on SLPs, such as computing the\nheight and Horton-Strahler number of a tree, tree navigation, or evaluation of\nBoolean expressions. Other problems on tree traversals turn out to be\nintractable, e.g. pattern matching and evaluation of tree automata.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 18:25:29 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 04:12:34 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Ganardi", "Moses", ""], ["Hucke", "Danny", ""], ["Lohrey", "Markus", ""], ["Noeth", "Eric", ""]]}, {"id": "1504.05553", "submitter": "Keith Levin", "authors": "Vladimir Braverman, Harry Lang, Keith Levin and Morteza Monemizadeh", "title": "A Unified Approach for Clustering Problems on Sliding Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore clustering problems in the streaming sliding window model in both\ngeneral metric spaces and Euclidean space. We present the first polylogarithmic\nspace $O(1)$-approximation to the metric $k$-median and metric $k$-means\nproblems in the sliding window model, answering the main open problem posed by\nBabcock, Datar, Motwani and O'Callaghan, which has remained unanswered for over\na decade. Our algorithm uses $O(k^3 \\log^6 n)$ space and\n$\\operatorname{poly}(k, \\log n)$ update time. This is an exponential\nimprovement on the space required by the technique due to Babcock, et al. We\nintroduce a data structure that extends smooth histograms as introduced by\nBraverman and Ostrovsky to operate on a broader class of functions. In\nparticular, we show that using only polylogarithmic space we can maintain a\nsummary of the current window from which we can construct an $O(1)$-approximate\nclustering solution.\n  Merge-and-reduce is a generic method in computational geometry for adapting\noffline algorithms to the insertion-only streaming model. Several well-known\ncoreset constructions are maintainable in the insertion-only streaming model\nusing this method, including well-known coreset techniques for the $k$-median,\n$k$-means in both low-and high-dimensional Euclidean spaces. Previous work has\nadapted these techniques to the insertion-deletion model, but translating them\nto the sliding window model has remained a challenge. We give the first\nalgorithm that, given an insertion-only streaming coreset construction of space\n$s$, maintains a $(1\\pm\\epsilon)$-approximate coreset in the sliding window\nmodel using $O(s^2\\epsilon^{-2}\\log n)$ space.\n  For clustering problems, our results constitute the first significant step\ntowards resolving problem number 20 from the List of Open Problems in Sublinear\nAlgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 19:07:42 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Braverman", "Vladimir", ""], ["Lang", "Harry", ""], ["Levin", "Keith", ""], ["Monemizadeh", "Morteza", ""]]}, {"id": "1504.05773", "submitter": "Kitty Meeks", "authors": "Jessica Enright and Kitty Meeks", "title": "Deleting edges to restrict the size of an epidemic", "comments": "Author final version of article to appear in Algorithmica (funding\n  details updated from previous version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in network epidemiology, we consider the problem of\ndetermining whether it is possible to delete at most $k$ edges from a given\ninput graph (of small treewidth) so that the resulting graph avoids a set\n$\\mathcal{F}$ of forbidden subgraphs; of particular interest is the problem of\ndetermining whether it is possible to delete at most $k$ edges so that the\nresulting graph has no connected component of more than $h$ vertices, as this\nbounds the worst-case size of an epidemic. While even this special case of the\nproblem is NP-complete in general (even when $h=3$), we provide evidence that\nmany of the real-world networks of interest are likely to have small treewidth,\nand we describe an algorithm which solves the general problem in time\n\\genruntime ~on an input graph having $n$ vertices and whose treewidth is\nbounded by a fixed constant $w$, if each of the subgraphs we wish to avoid has\nat most $r$ vertices. For the special case in which we wish only to ensure that\nno component has more than $h$ vertices, we improve on this to give an\nalgorithm running in time $O((wh)^{2w}n)$, which we have implemented and tested\non real datasets based on cattle movements.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 12:58:59 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 11:26:21 GMT"}, {"version": "v3", "created": "Tue, 10 May 2016 16:17:49 GMT"}, {"version": "v4", "created": "Wed, 12 Apr 2017 12:45:24 GMT"}, {"version": "v5", "created": "Wed, 19 Apr 2017 09:30:38 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Enright", "Jessica", ""], ["Meeks", "Kitty", ""]]}, {"id": "1504.05830", "submitter": "Bert Besser", "authors": "Bert Besser and Bastian Werth", "title": "On the Approximation Performance of Degree Heuristics for Matching", "comments": "v2: added inapproximability bounds for two-sided algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the design of greedy algorithms for the maximum cardinality matching\nproblem the utilization of degree information when selecting the next edge is a\nwell established and successful approach.\n  We define the class of \"degree sensitive\" greedy matching algorithms, which\nallows us to analyze many well-known heuristics, and provide tight\napproximation guarantees under worst case tie breaking. We exhibit algorithms\nin this class with optimal approximation guarantee for bipartite graphs. In\nparticular the Karp-Sipser algorithm, which picks an edge incident with a\ndegree-1 node if possible and otherwise an arbitrary edge, turns out to be\noptimal with approximation guarantee D/(2D-2), where D is the maximum degree.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 14:45:32 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 11:54:24 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Besser", "Bert", ""], ["Werth", "Bastian", ""]]}, {"id": "1504.05905", "submitter": "O-joung Kwon", "authors": "Mamadou Moustapha Kant\\'e, Eun Jung Kim, O-joung Kwon, Christophe Paul", "title": "An FPT algorithm and a polynomial kernel for Linear Rankwidth-1 Vertex\n  Deletion", "comments": "29 pages, 9 figures, An extended abstract appeared in IPEC2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear rankwidth is a linearized variant of rankwidth, introduced by Oum and\nSeymour [Approximating clique-width and branch-width. J. Combin. Theory Ser. B,\n96(4):514--528, 2006]. Motivated from recent development on graph modification\nproblems regarding classes of graphs of bounded treewidth or pathwidth, we\nstudy the Linear Rankwidth-1 Vertex Deletion problem (shortly, LRW1-Vertex\nDeletion). In the LRW1-Vertex Deletion problem, given an $n$-vertex graph $G$\nand a positive integer $k$, we want to decide whether there is a set of at most\n$k$ vertices whose removal turns $G$ into a graph of linear rankwidth at most\n$1$ and find such a vertex set if one exists. While the meta-theorem of\nCourcelle, Makowsky, and Rotics implies that LRW1-Vertex Deletion can be solved\nin time $f(k)\\cdot n^3$ for some function $f$, it is not clear whether this\nproblem allows a running time with a modest exponential function.\n  We first establish that LRW1-Vertex Deletion can be solved in time $8^k\\cdot\nn^{\\mathcal{O}(1)}$. The major obstacle to this end is how to handle a long\ninduced cycle as an obstruction. To fix this issue, we define necklace graphs\nand investigate their structural properties. Later, we reduce the polynomial\nfactor by refining the trivial branching step based on a cliquewidth expression\nof a graph, and obtain an algorithm that runs in time $2^{\\mathcal{O}(k)}\\cdot\nn^4$. We also prove that the running time cannot be improved to $2^{o(k)}\\cdot\nn^{\\mathcal{O}(1)}$ under the Exponential Time Hypothesis assumption. Lastly,\nwe show that the LRW1-Vertex Deletion problem admits a polynomial kernel.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 17:59:22 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 21:54:41 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Kant\u00e9", "Mamadou Moustapha", ""], ["Kim", "Eun Jung", ""], ["Kwon", "O-joung", ""], ["Paul", "Christophe", ""]]}, {"id": "1504.06122", "submitter": "Leo N. Geppert", "authors": "Leo N. Geppert, Katja Ickstadt, Alexander Munteanu, Jens Quedenfeld,\n  Christian Sohler", "title": "Random projections for Bayesian regression", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-015-9608-z", "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with random projections applied as a data reduction\ntechnique for Bayesian regression analysis. We show sufficient conditions under\nwhich the entire $d$-dimensional distribution is approximately preserved under\nrandom projections by reducing the number of data points from $n$ to $k\\in\nO(\\operatorname{poly}(d/\\varepsilon))$ in the case $n\\gg d$. Under mild\nassumptions, we prove that evaluating a Gaussian likelihood function based on\nthe projected data instead of the original data yields a\n$(1+O(\\varepsilon))$-approximation in terms of the $\\ell_2$ Wasserstein\ndistance. Our main result shows that the posterior distribution of Bayesian\nlinear regression is approximated up to a small error depending on only an\n$\\varepsilon$-fraction of its defining parameters. This holds when using\narbitrary Gaussian priors or the degenerate case of uniform distributions over\n$\\mathbb{R}^d$ for $\\beta$. Our empirical evaluations involve different\nsimulated settings of Bayesian linear regression. Our experiments underline\nthat the proposed method is able to recover the regression model up to small\nerror while considerably reducing the total running time.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 10:58:34 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 16:22:35 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Geppert", "Leo N.", ""], ["Ickstadt", "Katja", ""], ["Munteanu", "Alexander", ""], ["Quedenfeld", "Jens", ""], ["Sohler", "Christian", ""]]}, {"id": "1504.06242", "submitter": "Allyx Fontaine", "authors": "Raphael Clifford, Allyx Fontaine, Ely Porat, Benjamin Sach, Tatiana\n  Starikovskaya", "title": "Dictionary matching in a stream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of dictionary matching in a stream. Given a set of\nstrings, known as a dictionary, and a stream of characters arriving one at a\ntime, the task is to report each time some string in our dictionary occurs in\nthe stream. We present a randomised algorithm which takes O(log log(k + m))\ntime per arriving character and uses O(k log m) words of space, where k is the\nnumber of strings in the dictionary and m is the length of the longest string\nin the dictionary.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 16:24:15 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Clifford", "Raphael", ""], ["Fontaine", "Allyx", ""], ["Porat", "Ely", ""], ["Sach", "Benjamin", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1504.06316", "submitter": "Varsha Dani", "authors": "Varsha Dani, Thomas P. Hayes, Mahnush Movahedi, Jared Saia, Maxwell\n  Young", "title": "Interactive Communication with Unknown Noise Rate", "comments": "Made substantial improvements to the algorithm and analysis. Previous\n  version had a subtle error involving the adversary's ability to attack\n  fingerprints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alice and Bob want to run a protocol over a noisy channel, where a certain\nnumber of bits are flipped adversarially. Several results take a protocol\nrequiring $L$ bits of noise-free communication and make it robust over such a\nchannel. In a recent breakthrough result, Haeupler described an algorithm that\nsends a number of bits that is conjectured to be near optimal in such a model.\nHowever, his algorithm critically requires $a \\ priori$ knowledge of the number\nof bits that will be flipped by the adversary.\n  We describe an algorithm requiring no such knowledge. If an adversary flips\n$T$ bits, our algorithm sends $L + O\\left(\\sqrt{L(T+1)\\log L} + T\\right)$ bits\nin expectation and succeeds with high probability in $L$. It does so without\nany $a \\ priori$ knowledge of $T$. Assuming a conjectured lower bound by\nHaeupler, our result is optimal up to logarithmic factors.\n  Our algorithm critically relies on the assumption of a private channel. We\nshow that privacy is necessary when the amount of noise is unknown.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 19:57:46 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 23:11:29 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Dani", "Varsha", ""], ["Hayes", "Thomas P.", ""], ["Movahedi", "Mahnush", ""], ["Saia", "Jared", ""], ["Young", "Maxwell", ""]]}, {"id": "1504.06357", "submitter": "Simon Hollis", "authors": "Simon J. Hollis, Steve Kerrison", "title": "Overview of Swallow --- A Scalable 480-core System for Investigating the\n  Performance and Energy Efficiency of Many-core Applications and Operating\n  Systems", "comments": "An open source release of the Swallow system design and code will\n  follow and references to these will be added at a later date", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Swallow, a scalable many-core architecture, with a current\nconfiguration of 480 x 32-bit processors.\n  Swallow is an open-source architecture, designed from the ground up to\ndeliver scalable increases in usable computational power to allow\nexperimentation with many-core applications and the operating systems that\nsupport them.\n  Scalability is enabled by the creation of a tile-able system with a\nlow-latency interconnect, featuring an attractive communication-to-computation\nratio and the use of a distributed memory configuration.\n  We analyse the energy and computational and communication performances of\nSwallow. The system provides 240GIPS with each core consuming 71--193mW,\ndependent on workload. Power consumption per instruction is lower than almost\nall systems of comparable scale.\n  We also show how the use of a distributed operating system (nOS) allows the\neasy creation of scalable software to exploit Swallow's potential. Finally, we\nshow two use case studies: modelling neurons and the overlay of shared memory\non a distributed memory system.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 22:36:46 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Hollis", "Simon J.", ""], ["Kerrison", "Steve", ""]]}, {"id": "1504.06363", "submitter": "Frank Neumann", "authors": "Frank Neumann and Carsten Witt", "title": "On the Runtime of Randomized Local Search and Simple Evolutionary\n  Algorithms for Dynamic Makespan Scheduling", "comments": "Conference version appears at IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms have been frequently used for dynamic optimization\nproblems. With this paper, we contribute to the theoretical understanding of\nthis research area. We present the first computational complexity analysis of\nevolutionary algorithms for a dynamic variant of a classical combinatorial\noptimization problem, namely makespan scheduling. We study the model of a\nstrong adversary which is allowed to change one job at regular intervals.\nFurthermore, we investigate the setting of random changes. Our results show\nthat randomized local search and a simple evolutionary algorithm are very\neffective in dynamically tracking changes made to the problem instance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 23:13:19 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Neumann", "Frank", ""], ["Witt", "Carsten", ""]]}, {"id": "1504.06475", "submitter": "Sebastian Wild", "authors": "Raphael Reitzig and Sebastian Wild", "title": "A Practical and Worst-Case Efficient Algorithm for Divisor Methods of\n  Apportionment", "comments": "(v4 adds missing figures in v3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional apportionment is the problem of assigning seats to parties\naccording to their relative share of votes. Divisor methods are the de-facto\nstandard solution, used in many countries.\n  In recent literature, there are two algorithms that implement divisor\nmethods: one by Cheng and Eppstein (ISAAC, 2014) has worst-case optimal running\ntime but is complex, while the other (Pukelsheim, 2014) is relatively simple\nand fast in practice but does not offer worst-case guarantees.\n  We demonstrate that the former algorithm is much slower than the other in\npractice and propose a novel algorithm that avoids the shortcomings of both. We\ninvestigate the running-time behavior of the three contenders in order to\ndetermine which is most useful in practice.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 11:37:00 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 14:05:14 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 14:39:09 GMT"}, {"version": "v4", "created": "Tue, 5 Dec 2017 16:38:10 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Reitzig", "Raphael", ""], ["Wild", "Sebastian", ""]]}, {"id": "1504.06501", "submitter": "Shikha Singh", "authors": "Michael A. Bender (1), Samuel McCauley (1), Andrew McGregor (2),\n  Shikha Singh (1) and Hoa T. Vu (2) ((1) Stony Brook University, (2)\n  University of Massachusetts, Amherst)", "title": "Run Generation Revisited: What Goes Up May or May Not Come Down", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the classic problem of run generation. Run\ngeneration is the first phase of external-memory sorting, where the objective\nis to scan through the data, reorder elements using a small buffer of size M ,\nand output runs (contiguously sorted chunks of elements) that are as long as\npossible.\n  We develop algorithms for minimizing the total number of runs (or\nequivalently, maximizing the average run length) when the runs are allowed to\nbe sorted or reverse sorted. We study the problem in the online setting, both\nwith and without resource augmentation, and in the offline setting.\n  (1) We analyze alternating-up-down replacement selection (runs alternate\nbetween sorted and reverse sorted), which was studied by Knuth as far back as\n1963. We show that this simple policy is asymptotically optimal. Specifically,\nwe show that alternating-up-down replacement selection is 2-competitive and no\ndeterministic online algorithm can perform better.\n  (2) We give online algorithms having smaller competitive ratios with resource\naugmentation. Specifically, we exhibit a deterministic algorithm that, when\ngiven a buffer of size 4M , is able to match or beat any optimal algorithm\nhaving a buffer of size M . Furthermore, we present a randomized online\nalgorithm which is 7/4-competitive when given a buffer twice that of the\noptimal.\n  (3) We demonstrate that performance can also be improved with a small amount\nof foresight. We give an algorithm, which is 3/2-competitive, with\nforeknowledge of the next 3M elements of the input stream. For the extreme case\nwhere all future elements are known, we design a PTAS for computing the optimal\nstrategy a run generation algorithm must follow.\n  (4) Finally, we present algorithms tailored for nearly sorted inputs which\nare guaranteed to have optimal solutions with sufficiently long runs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 13:36:47 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Bender", "Michael A.", ""], ["McCauley", "Samuel", ""], ["McGregor", "Andrew", ""], ["Singh", "Shikha", ""], ["Vu", "Hoa T.", ""]]}, {"id": "1504.06544", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement Canonne, Themis Gouleakis and Ronitt Rubinfeld", "title": "Sampling Correctors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many situations, sample data is obtained from a noisy or imperfect source.\nIn order to address such corruptions, this paper introduces the concept of a\nsampling corrector. Such algorithms use structure that the distribution is\npurported to have, in order to allow one to make \"on-the-fly\" corrections to\nsamples drawn from probability distributions. These algorithms then act as\nfilters between the noisy data and the end user.\n  We show connections between sampling correctors, distribution learning\nalgorithms, and distribution property testing algorithms. We show that these\nconnections can be utilized to expand the applicability of known distribution\nlearning and property testing algorithms as well as to achieve improved\nalgorithms for those tasks.\n  As a first step, we show how to design sampling correctors using proper\nlearning algorithms. We then focus on the question of whether algorithms for\nsampling correctors can be more efficient in terms of sample complexity than\nlearning algorithms for the analogous families of distributions. When\ncorrecting monotonicity, we show that this is indeed the case when also granted\nquery access to the cumulative distribution function. We also obtain sampling\ncorrectors for monotonicity without this stronger type of access, provided that\nthe distribution be originally very close to monotone (namely, at a distance\n$O(1/\\log^2 n)$). In addition to that, we consider a restricted error model\nthat aims at capturing \"missing data\" corruptions. In this model, we show that\ndistributions that are close to monotone have sampling correctors that are\nsignificantly more efficient than achievable by the learning approach.\n  We also consider the question of whether an additional source of independent\nrandom bits is required by sampling correctors to implement the correction\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 15:39:52 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 03:56:00 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Canonne", "Cl\u00e9ment", ""], ["Gouleakis", "Themis", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1504.06560", "submitter": "Cong Shi", "authors": "Viswanath Nagarajan, Cong Shi", "title": "Approximation Algorithms for Inventory Problems with Submodular or\n  Routing Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following two deterministic inventory optimization problems\nover a finite planning horizon $T$ with non-stationary demands.\n  (a) Submodular Joint Replenishment Problem: This involves multiple item types\nand a single retailer who faces demands. In each time step, any subset of\nitem-types can be ordered incurring a joint ordering cost which is submodular.\nMoreover, items can be held in inventory while incurring a holding cost. The\nobjective is find a sequence of orders that satisfies all demands and minimizes\nthe total ordering and holding costs.\n  (b) Inventory Routing Problem: This involves a single depot that stocks\nitems, and multiple retailer locations facing demands. In each time step, any\nsubset of locations can be visited using a vehicle originating from the depot.\nThere is also cost incurred for holding items at any retailer. The objective\nhere is to satisfy all demands while minimizing the sum of routing and holding\ncosts.\n  We present a unified approach that yields $\\mathcal{O}\\left(\\frac{\\log\nT}{\\log\\log T}\\right)$-factor approximation algorithms for both problems when\nthe holding costs are polynomial functions. A special case is the classic\nlinear holding cost model, wherein this is the first sub-logarithmic\napproximation ratio for either problem.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 16:33:10 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Nagarajan", "Viswanath", ""], ["Shi", "Cong", ""]]}, {"id": "1504.06647", "submitter": "Tomasz Kociumaka", "authors": "Johannes Fischer, Travis Gagie, Pawe{\\l} Gawrychowski, and Tomasz\n  Kociumaka", "title": "Approximating LZ77 via Small-Space Multiple-Pattern Matching", "comments": "preliminary version presented at ESA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize Karp-Rabin string matching to handle multiple patterns in\n$\\mathcal{O}(n \\log n + m)$ time and $\\mathcal{O}(s)$ space, where $n$ is the\nlength of the text and $m$ is the total length of the $s$ patterns, returning\ncorrect answers with high probability. As a prime application of our algorithm,\nwe show how to approximate the LZ77 parse of a string of length $n$. If the\noptimal parse consists of $z$ phrases, using only $\\mathcal{O}(z)$ working\nspace we can return a parse consisting of at most $(1+\\varepsilon)z$ phrases in\n$\\mathcal{O}(\\varepsilon^{-1}n\\log n)$ time, for any $\\varepsilon\\in (0,1]$. As\nprevious quasilinear-time algorithms for LZ77 use $\\Omega(n/\\textrm{polylog\n}n)$ space, but $z$ can be exponentially small in $n$, these improvements in\nspace are substantial.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 21:26:58 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 14:59:37 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Fischer", "Johannes", ""], ["Gagie", "Travis", ""], ["Gawrychowski", "Pawe\u0142", ""], ["Kociumaka", "Tomasz", ""]]}, {"id": "1504.06712", "submitter": "Dmitry Kosolobov", "authors": "Dmitry Kosolobov", "title": "Faster Lightweight Lempel-Ziv Parsing", "comments": "16 pages, 5 figures, accepted to MFCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that computes the Lempel-Ziv decomposition in\n$O(n(\\log\\sigma + \\log\\log n))$ time and $n\\log\\sigma + \\epsilon n$ bits of\nspace, where $\\epsilon$ is a constant rational parameter, $n$ is the length of\nthe input string, and $\\sigma$ is the alphabet size. The $n\\log\\sigma$ bits in\nthe space bound are for the input string itself which is treated as read-only.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 11:21:56 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 12:38:37 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 19:03:03 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Kosolobov", "Dmitry", ""]]}, {"id": "1504.06729", "submitter": "Peilin Zhong", "authors": "Christos Boutsidis, David P. Woodruff, Peilin Zhong", "title": "Optimal Principal Component Analysis in Distributed and Streaming Models", "comments": "STOC2016 full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Principal Component Analysis (PCA) problem in the distributed\nand streaming models of computation. Given a matrix $A \\in R^{m \\times n},$ a\nrank parameter $k < rank(A)$, and an accuracy parameter $0 < \\epsilon < 1$, we\nwant to output an $m \\times k$ orthonormal matrix $U$ for which $$ || A - U U^T\nA ||_F^2 \\le \\left(1 + \\epsilon \\right) \\cdot || A - A_k||_F^2, $$ where $A_k\n\\in R^{m \\times n}$ is the best rank-$k$ approximation to $A$.\n  This paper provides improved algorithms for distributed PCA and streaming\nPCA.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 14:22:21 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 02:42:54 GMT"}, {"version": "v3", "created": "Tue, 5 Jul 2016 12:52:49 GMT"}, {"version": "v4", "created": "Tue, 12 Jul 2016 03:40:58 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Boutsidis", "Christos", ""], ["Woodruff", "David P.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1504.06804", "submitter": "Mikkel Thorup", "authors": "Mikkel Thorup", "title": "High Speed Hashing for Integers and Strings", "comments": "Fixed a few typos from previous version. Also, changed some notation\n  to be more consistent with other literature. Please send comments/typos to me\n  at mikkel2thorup@gmail.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes describe the most efficient hash functions currently known for\nhashing integers and strings. These modern hash functions are often an order of\nmagnitude faster than those presented in standard text books. They are also\nsimpler to implement, and hence a clear win in practice, but their analysis is\nharder. Some of the most practical hash functions have only appeared in theory\npapers, and some of them requires combining results from different theory\npapers. The goal here is to combine the information in lecture-style notes that\ncan be used by theoreticians and practitioners alike, thus making these\npractical fruits of theory more widely accessible.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 11:32:32 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 11:38:11 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 13:24:53 GMT"}, {"version": "v4", "created": "Fri, 27 Apr 2018 13:17:32 GMT"}, {"version": "v5", "created": "Tue, 18 Sep 2018 09:18:27 GMT"}, {"version": "v6", "created": "Wed, 17 Apr 2019 15:26:35 GMT"}, {"version": "v7", "created": "Sun, 12 May 2019 10:02:38 GMT"}, {"version": "v8", "created": "Wed, 29 Apr 2020 15:40:21 GMT"}, {"version": "v9", "created": "Sat, 9 May 2020 07:42:41 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Thorup", "Mikkel", ""]]}, {"id": "1504.06842", "submitter": "Haitao Wang", "authors": "Joseph S.B. Mitchell, Valentin Polishchuk, Mikko Sysikaski, Haitao\n  Wang", "title": "An Optimal Algorithm for Minimum-Link Rectilinear Paths in Triangulated\n  Rectilinear Domains", "comments": "23 pages, 10 figures; an extended abstract to appear in ICALP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding minimum-link rectilinear paths in\nrectilinear polygonal domains in the plane. A path or a polygon is rectilinear\nif all its edges are axis-parallel. Given a set $\\mathcal{P}$ of $h$\npairwise-disjoint rectilinear polygonal obstacles with a total of $n$ vertices\nin the plane, a minimum-link rectilinear path between two points is a\nrectilinear path that avoids all obstacles with the minimum number of edges. In\nthis paper, we present a new algorithm for finding minimum-link rectilinear\npaths among $\\mathcal{P}$. After the plane is triangulated, with respect to any\nsource point $s$, our algorithm builds an $O(n)$-size data structure in\n$O(n+h\\log h)$ time, such that given any query point $t$, the number of edges\nof a minimum-link rectilinear path from $s$ to $t$ can be computed in $O(\\log\nn)$ time and the actual path can be output in additional time linear in the\nnumber of the edges of the path. The previously best algorithm computes such a\ndata structure in $O(n\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 16:24:39 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Mitchell", "Joseph S. B.", ""], ["Polishchuk", "Valentin", ""], ["Sysikaski", "Mikko", ""], ["Wang", "Haitao", ""]]}, {"id": "1504.06954", "submitter": "Takaaki Nishimoto", "authors": "Takaaki Nishimoto, I Tomohiro, Shunsuke Inenaga, Hideo Bannai,\n  Masayuki Takeda", "title": "Dynamic index, LZ factorization, and LCE queries in compressed space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the following results: (1) We propose a new\n\\emph{dynamic compressed index} of $O(w)$ space, that supports searching for a\npattern $P$ in the current text in $O(|P| f(M,w) + \\log w \\log |P| \\log^* M\n(\\log N + \\log |P| \\log^* M) + \\mathit{occ} \\log N)$ time and\ninsertion/deletion of a substring of length $y$ in $O((y+ \\log N\\log^* M)\\log w\n\\log N \\log^* M)$ time, where $N$ is the length of the current text, $M$ is the\nmaximum length of the dynamic text, $z$ is the size of the Lempel-Ziv77 (LZ77)\nfactorization of the current text, $f(a,b) = O(\\min \\{ \\frac{\\log\\log a\n\\log\\log b}{\\log\\log\\log a}, \\sqrt{\\frac{\\log b}{\\log\\log b}} \\})$ and $w = O(z\n\\log N \\log^*M)$. (2) We propose a new space-efficient LZ77 factorization\nalgorithm for a given text of length $N$, which runs in $O(N f(N,w') + z \\log\nw' \\log^3 N (\\log^* N)^2)$ time with $O(w')$ working space, where $w' =O(z \\log\nN \\log^* N)$. (3) We propose a data structure of $O(w)$ space which supports\nlongest common extension (LCE) queries on the text in $O(\\log N + \\log \\ell\n\\log^* N)$ time, where $\\ell$ is the output LCE length. On top of the above\ncontributions, we show several applications of our data structures which\nimprove previous best known results on grammar-compressed string processing.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 07:42:53 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 08:32:43 GMT"}, {"version": "v3", "created": "Wed, 30 Mar 2016 06:02:40 GMT"}, {"version": "v4", "created": "Wed, 6 Apr 2016 06:29:09 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Nishimoto", "Takaaki", ""], ["Tomohiro", "I", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1504.06963", "submitter": "Szabolcs M\\'esz\\'aros", "authors": "Endre Cs\\'oka, Szabolcs M\\'esz\\'aros", "title": "Generalized solution for the Herman Protocol Conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have a cycle of $N$ nodes and there is a token on an odd number of nodes.\nAt each step, each token independently moves to its clockwise neighbor or stays\nat its position with probability $\\frac{1}{2}$. If two tokens arrive to the\nsame node, then we remove both of them. The process ends when only one token\nremains. The question is that for a fixed $N$, which is the initial\nconfiguration that maximizes the expected number of steps $E(T)$. The Herman\nProtocol Conjecture says that the $3$-token configuration with distances\n$\\lfloor\\frac{N}{3}\\rfloor$ and $\\lceil\\frac{N}{3}\\rceil$ maximizes $E(T)$. We\npresent a proof of this conjecture not only for $E(T)$ but also for\n$E\\big(f(T)\\big)$ for some function $f:\\mathbb{N}\\rightarrow\\mathbb{R}^{+}$\nwhich method applies for different generalizations of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 08:14:33 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 06:57:25 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2015 14:01:37 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Cs\u00f3ka", "Endre", ""], ["M\u00e9sz\u00e1ros", "Szabolcs", ""]]}, {"id": "1504.07019", "submitter": "Lior Kamma", "authors": "Lior Kamma and Robert Krauthgamer", "title": "Metric Decompositions of Path-Separable Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prominent tool in many problems involving metric spaces is a notion of\nrandomized low-diameter decomposition. Loosely speaking, $\\beta$-decomposition\nrefers to a probability distribution over partitions of the metric into sets of\nlow diameter, such that nearby points (parameterized by $\\beta>0$) are likely\nto be \"clustered\" together. Applying this notion to the shortest-path metric in\nedge-weighted graphs, it is known that $n$-vertex graphs admit an $O(\\ln\nn)$-padded decomposition (Bartal, 1996), and that excluded-minor graphs admit\n$O(1)$-padded decomposition (Klein, Plotkin and Rao 1993, Fakcharoenphol and\nTalwar 2003, Abraham et al. 2014).\n  We design decompositions to the family of $p$-path-separable graphs, which\nwas defined by Abraham and Gavoille (2006). and refers to graphs that admit\nvertex-separators consisting of at most $p$ shortest paths in the graph.\n  Our main result is that every $p$-path-separable $n$-vertex graph admits an\n$O(\\ln (p \\ln n))$-decomposition, which refines the $O(\\ln n)$ bound for\ngeneral graphs, and provides new bounds for families like bounded-treewidth\ngraphs. Technically, our clustering process differs from previous ones by\nworking in (the shortest-path metric of) carefully chosen subgraphs.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 10:48:12 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 21:13:37 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Kamma", "Lior", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1504.07056", "submitter": "Sebastian Forster", "authors": "Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai", "title": "A Deterministic Almost-Tight Distributed Algorithm for Approximating\n  Single-Source Shortest Paths", "comments": "Accepted to SIAM Journal on Computing. A preliminary version of this\n  paper was presented at the 48th ACM Symposium on Theory of Computing (STOC\n  2016). Abstract shortened to respect the arXiv limit of 1920 characters", "journal-ref": null, "doi": "10.1145/2897518.2897638", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic $(1+o(1))$-approximation\n$(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for solving the single-source\nshortest paths problem on distributed weighted networks (the CONGEST model);\nhere $n$ is the number of nodes in the network and $D$ is its (hop) diameter.\nThis is the first non-trivial deterministic algorithm for this problem. It also\nimproves (i) the running time of the randomized $(1+o(1))$-approximation\n$\\tilde O(n^{1/2}D^{1/4}+D)$-time algorithm of Nanongkai [STOC 2014] by a\nfactor of as large as $n^{1/8}$, and (ii) the $O(\\epsilon^{-1}\\log\n\\epsilon^{-1})$-approximation factor of Lenzen and Patt-Shamir's $\\tilde\nO(n^{1/2+\\epsilon}+D)$-time algorithm [STOC 2013] within the same running time.\nOur running time matches the known time lower bound of $\\Omega(n^{1/2}/\\log n +\nD)$ [Elkin STOC 2004] up to subpolynomial factors, thus essentially settling\nthe status of this problem which was raised at least a decade ago [Elkin SIGACT\nNews 2004]. It also implies a $(2+o(1))$-approximation\n$(n^{1/2+o(1)}+D^{1+o(1)})$-time algorithm for approximating a network's\nweighted diameter which almost matches the lower bound by Holzer and Pinsker\n[OPODIS 2015]. In achieving this result, we develop two techniques which might\nbe of independent interest and useful in other settings: (i) a deterministic\nprocess that replaces the \"hitting set argument\" commonly used for shortest\npaths computation in various settings, and (ii) a simple, deterministic,\nconstruction of an $(n^{o(1)}, o(1))$-hop set of size $n^{1+o(1)}$. We combine\nthese techniques with many distributed algorithmic techniques, some of which\nfrom problems that are not directly related to shortest paths, e.g., ruling\nsets [Goldberg et al. STOC 1987], source detection [Lenzen and Peleg PODC\n2013], and partial distance estimation [Lenzen and Patt-Shamir PODC 2015].\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 12:33:50 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 10:36:51 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 16:30:02 GMT"}, {"version": "v4", "created": "Fri, 7 Oct 2016 10:15:17 GMT"}, {"version": "v5", "created": "Wed, 19 Sep 2018 17:04:54 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1504.07066", "submitter": "Alexander M\\\"acker", "authors": "Alexander M\\\"acker, Manuel Malatyali, Friedhelm Meyer auf der Heide,\n  S\\\"oren Riechers", "title": "Non-Preemptive Scheduling on Machines with Setup Times", "comments": "A conference version of this paper has been accepted for publication\n  in the proceedings of the 14th Algorithms and Data Structures Symposium\n  (WADS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem in which n jobs that are classified into k types are to\nbe scheduled on m identical machines without preemption. A machine requires a\nproper setup taking s time units before processing jobs of a given type. The\nobjective is to minimize the makespan of the resulting schedule. We design and\nanalyze an approximation algorithm that runs in time polynomial in n, m and k\nand computes a solution with an approximation factor that can be made\narbitrarily close to 3/2.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 12:59:35 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["M\u00e4cker", "Alexander", ""], ["Malatyali", "Manuel", ""], ["der Heide", "Friedhelm Meyer auf", ""], ["Riechers", "S\u00f6ren", ""]]}, {"id": "1504.07073", "submitter": "Manuel Mohr", "authors": "Sebastian Buchwald, Manuel Mohr, Ignaz Rutter", "title": "Optimal Shuffle Code with Permutation Instructions", "comments": "20 pages, 5 figures, full version of a paper accepted at WADS'15.\n  Minor update: fixed typos, corrected comma placement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During compilation of a program, register allocation is the task of mapping\nprogram variables to machine registers. During register allocation, the\ncompiler may introduce shuffle code, consisting of copy and swap operations,\nthat transfers data between the registers. Three common sources of shuffle code\nare conflicting register mappings at joins in the control flow of the program,\ne.g, due to if-statements or loops; the calling convention for procedures,\nwhich often dictates that input arguments or results must be placed in certain\nregisters; and machine instructions that only allow a subset of registers to\noccur as operands. Recently, Mohr et al. proposed to speed up shuffle code with\nspecial hardware instructions that arbitrarily permute the contents of up to\nfive registers and gave a heuristic for computing such shuffle codes. In this\npaper, we give an efficient algorithm for generating optimal shuffle code in\nthe setting of Mohr et al. An interesting special case occurs when no register\nhas to be transferred to more than one destination, i.e., it suffices to\npermute the contents of the registers. This case is equivalent to factoring a\npermutation into a minimal product of permutations, each of which permutes up\nto five elements.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 13:14:44 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 07:18:27 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Buchwald", "Sebastian", ""], ["Mohr", "Manuel", ""], ["Rutter", "Ignaz", ""]]}, {"id": "1504.07091", "submitter": "Elisabetta Bergamini", "authors": "Elisabetta Bergamini and Henning Meyerhenke", "title": "Fully-dynamic Approximation of Betweenness Centrality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness is a well-known centrality measure that ranks the nodes of a\nnetwork according to their participation in shortest paths. Since an exact\ncomputation is prohibitive in large networks, several approximation algorithms\nhave been proposed. Besides that, recent years have seen the publication of\ndynamic algorithms for efficient recomputation of betweenness in evolving\nnetworks. In previous work we proposed the first semi-dynamic algorithms that\nrecompute an approximation of betweenness in connected graphs after batches of\nedge insertions.\n  In this paper we propose the first fully-dynamic approximation algorithms\n(for weighted and unweighted undirected graphs that need not to be connected)\nwith a provable guarantee on the maximum approximation error. The transfer to\nfully-dynamic and disconnected graphs implies additional algorithmic problems\nthat could be of independent interest. In particular, we propose a new upper\nbound on the vertex diameter for weighted undirected graphs. For both weighted\nand unweighted graphs, we also propose the first fully-dynamic algorithms that\nkeep track of such upper bound. In addition, we extend our former algorithm for\nsemi-dynamic BFS to batches of both edge insertions and deletions.\n  Using approximation, our algorithms are the first to make in-memory\ncomputation of betweenness in fully-dynamic networks with millions of edges\nfeasible. Our experiments show that they can achieve substantial speedups\ncompared to recomputation, up to several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 13:53:32 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 13:51:54 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Bergamini", "Elisabetta", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1504.07129", "submitter": "Max Klimm", "authors": "Yann Disser and Max Klimm and Elisabeth L\\\"ubbecke", "title": "Scheduling Bidirectional Traffic on a Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of scheduling bidirectional traffic along a\npath composed of multiple segments. The main feature of the problem is that\njobs traveling in the same direction can be scheduled in quick succession on a\nsegment, while jobs in opposing directions cannot cross a segment at the same\ntime. We show that this tradeoff makes the problem significantly harder than\nthe related flow shop problem, by proving that it is NP-hard even for identical\njobs. We complement this result with a PTAS for a single segment and\nnon-identical jobs. If we allow some pairs of jobs traveling in different\ndirections to cross a segment concurrently, the problem becomes APX-hard even\non a single segment and with identical jobs. We give polynomial algorithms for\nthe setting with restricted compatibilities between jobs on a single and any\nconstant number of segments, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 15:27:21 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Disser", "Yann", ""], ["Klimm", "Max", ""], ["L\u00fcbbecke", "Elisabeth", ""]]}, {"id": "1504.07149", "submitter": "Sascha Witt", "authors": "Sascha Witt", "title": "Trip-Based Public Transit Routing", "comments": "Minor corrections, no substantial changes. To be presented at ESA\n  2015", "journal-ref": "Lecture Notes in Computer Science 9294 (2015) 1025-1036", "doi": "10.1007/978-3-662-48350-3_85", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing all Pareto-optimal journeys in a public\ntransit network regarding the two criteria of arrival time and number of\ntransfers taken. We take a novel approach, focusing on trips and transfers\nbetween them, allowing fine-grained modeling. Our experiments on the\nmetropolitan network of London show that the algorithm computes full 24-hour\nprofiles in 70 ms after a preprocessing phase of 30 s, allowing fast queries in\ndynamic scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 16:24:17 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 14:57:31 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Witt", "Sascha", ""]]}, {"id": "1504.07218", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Changho Suh", "title": "Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons", "comments": "accepted to International Conference on Machine Learning (ICML), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the preference-based top-$K$ rank aggregation problem.\nSuppose that a collection of items is repeatedly compared in pairs, and one\nwishes to recover a consistent ordering that emphasizes the top-$K$ ranked\nitems, based on partially revealed preferences. We focus on the\nBradley-Terry-Luce (BTL) model that postulates a set of latent preference\nscores underlying all items, where the odds of paired comparisons depend only\non the relative scores of the items involved.\n  We characterize the minimax limits on identifiability of top-$K$ ranked\nitems, in the presence of random and non-adaptive sampling. Our results\nhighlight a separation measure that quantifies the gap of preference scores\nbetween the $K^{\\text{th}}$ and $(K+1)^{\\text{th}}$ ranked items. The minimum\nsample complexity required for reliable top-$K$ ranking scales inversely with\nthe separation measure irrespective of other preference distribution metrics.\nTo approach this minimax limit, we propose a nearly linear-time ranking scheme,\ncalled \\emph{Spectral MLE}, that returns the indices of the top-$K$ items in\naccordance to a careful score estimate. In a nutshell, Spectral MLE starts with\nan initial score estimate with minimal squared loss (obtained via a spectral\nmethod), and then successively refines each component with the assistance of\ncoordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ item\nidentification under minimal sample complexity. The practical applicability of\nSpectral MLE is further corroborated by numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:30:01 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 06:04:15 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Chen", "Yuxin", ""], ["Suh", "Changho", ""]]}, {"id": "1504.07298", "submitter": "J\\'er\\'emy Barbay", "authors": "J\\'er\\'emy Barbay and Pablo P\\'erez-Lantero", "title": "Adaptive Computation of the Swap-Insert Correction Distance", "comments": "16 pages, no figures, long version of the extended abstract accepted\n  to SPIRE 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Swap-Insert Correction distance from a string $S$ of length $n$ to\nanother string $L$ of length $m\\geq n$ on the alphabet $[1..d]$ is the minimum\nnumber of insertions, and swaps of pairs of adjacent symbols, converting $S$\ninto $L$. Contrarily to other correction distances, computing it is NP-Hard in\nthe size $d$ of the alphabet. We describe an algorithm computing this distance\nin time within $O(d^2 nm g^{d-1})$, where there are $n_\\alpha$ occurrences of\n$\\alpha$ in $S$, $m_\\alpha$ occurrences of $\\alpha$ in $L$, and where\n$g=\\max_{\\alpha\\in[1..d]} \\min\\{n_\\alpha,m_\\alpha-n_\\alpha\\}$ measures the\ndifficulty of the instance. The difficulty $g$ is bounded by above by various\nterms, such as the length of the shortest string $S$, and by the maximum number\nof occurrences of a single character in $S$. Those results illustrate how, in\nmany cases, the correction distance between two strings can be easier to\ncompute than in the worst case scenario.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 23:00:16 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 01:30:51 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Barbay", "J\u00e9r\u00e9my", ""], ["P\u00e9rez-Lantero", "Pablo", ""]]}, {"id": "1504.07379", "submitter": "Michael Hamann", "authors": "Ulrik Brandes, Michael Hamann, Ben Strasser and Dorothea Wagner", "title": "Fast Quasi-Threshold Editing", "comments": "26 pages, 4 figures, submitted to ESA 2015", "journal-ref": null, "doi": "10.1007/978-3-662-48350-3_22", "report-no": null, "categories": "cs.DS cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Quasi-Threshold Mover (QTM), an algorithm to solve the\nquasi-threshold (also called trivially perfect) graph editing problem with edge\ninsertion and deletion. Given a graph it computes a quasi-threshold graph which\nis close in terms of edit count. This edit problem is NP-hard. We present an\nextensive experimental study, in which we show that QTM is the first algorithm\nthat is able to scale to large real-world graphs in practice. As a side result\nwe further present a simple linear-time algorithm for the quasi-threshold\nrecognition problem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 08:45:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Brandes", "Ulrik", ""], ["Hamann", "Michael", ""], ["Strasser", "Ben", ""], ["Wagner", "Dorothea", ""]]}, {"id": "1504.07384", "submitter": "Andreas Pavlogiannis", "authors": "Krishnendu Chatterjee and Rasmus Ibsen-Jensen and Andreas Pavlogiannis", "title": "Faster Algorithms for Quantitative Verification in Constant Treewidth\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the core algorithmic problems related to verification of systems\nwith respect to three classical quantitative properties, namely, the\nmean-payoff property, the ratio property, and the minimum initial credit for\nenergy property. The algorithmic problem given a graph and a quantitative\nproperty asks to compute the optimal value (the infimum value over all traces)\nfrom every node of the graph. We consider graphs with constant treewidth, and\nit is well-known that the control-flow graphs of most programs have constant\ntreewidth. Let $n$ denote the number of nodes of a graph, $m$ the number of\nedges (for constant treewidth graphs $m=O(n)$) and $W$ the largest absolute\nvalue of the weights. Our main theoretical results are as follows. First, for\nconstant treewidth graphs we present an algorithm that approximates the\nmean-payoff value within a multiplicative factor of $\\epsilon$ in time $O(n\n\\cdot \\log (n/\\epsilon))$ and linear space, as compared to the classical\nalgorithms that require quadratic time. Second, for the ratio property we\npresent an algorithm that for constant treewidth graphs works in time $O(n\n\\cdot \\log (|a\\cdot b|))=O(n\\cdot\\log (n\\cdot W))$, when the output is\n$\\frac{a}{b}$, as compared to the previously best known algorithm with running\ntime $O(n^2 \\cdot \\log (n\\cdot W))$. Third, for the minimum initial credit\nproblem we show that (i) for general graphs the problem can be solved in\n$O(n^2\\cdot m)$ time and the associated decision problem can be solved in\n$O(n\\cdot m)$ time, improving the previous known $O(n^3\\cdot m\\cdot \\log\n(n\\cdot W))$ and $O(n^2 \\cdot m)$ bounds, respectively; and (ii) for constant\ntreewidth graphs we present an algorithm that requires $O(n\\cdot \\log n)$ time,\nimproving the previous known $O(n^4 \\cdot \\log (n \\cdot W))$ bound.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 08:53:53 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Ibsen-Jensen", "Rasmus", ""], ["Pavlogiannis", "Andreas", ""]]}, {"id": "1504.07406", "submitter": "Tatiana Starikovskaya", "authors": "Gregory Kucherov, Alexander Loptev, Tatiana Starikovskaya", "title": "On Maximal Unbordered Factors", "comments": "Accepted to the 26th Annual Symposium on Combinatorial Pattern\n  Matching (CPM 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $S$ of length $n$, its maximal unbordered factor is the\nlongest factor which does not have a border. In this work we investigate the\nrelationship between $n$ and the length of the maximal unbordered factor of\n$S$. We prove that for the alphabet of size $\\sigma \\ge 5$ the expected length\nof the maximal unbordered factor of a string of length~$n$ is at least $0.99 n$\n(for sufficiently large values of $n$). As an application of this result, we\npropose a new algorithm for computing the maximal unbordered factor of a\nstring.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 10:08:20 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Kucherov", "Gregory", ""], ["Loptev", "Alexander", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1504.07595", "submitter": "Asbj{\\o}rn Br{\\ae}ndeland", "authors": "Asbj{\\o}rn Br{\\ae}ndeland", "title": "nCk sequences and their difference sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nCk sequence is a sequence of n-bit numbers with k bits set. Given such a\nsequence C, the difference sequence D of C is subject to certain regularities\nthat make it possible to generate D in 2|C| time, and, hence, to generate C in\n3|C| time.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 18:45:30 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Br\u00e6ndeland", "Asbj\u00f8rn", ""]]}, {"id": "1504.07626", "submitter": "Asbj{\\o}rn Br{\\ae}ndeland", "authors": "Asbj{\\o}rn Br{\\ae}ndeland", "title": "Split-by-edges trees", "comments": "The definition of 'ordered SBE-tree' has been added. This corrects an\n  omission in the previous versions but does not change anything essential.\n  Some changes have been made to accommodate the addition, and others have been\n  made to correct minor errors and improve wordings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A split-by-edges tree of a graph G on n vertices is a binary tree T where the\nroot = V(G), every leaf is an independent set in G, and for every other node N\nin T with children L and R there is a pair of vertices {u, v} in N such that L\n= N - v, R = N - u, and uv is an edge in G. It follows from the definition that\nevery maximal independent set in G is a leaf in T, and the maximum independent\nsets of G are the leaves closest to the root of T.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 18:39:17 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 10:55:41 GMT"}, {"version": "v3", "created": "Wed, 13 May 2015 18:59:43 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Br\u00e6ndeland", "Asbj\u00f8rn", ""]]}, {"id": "1504.07672", "submitter": "Jaehyun Park", "authors": "Jaehyun Park, Stephen Boyd", "title": "A Semidefinite Programming Method for Integer Convex Quadratic\n  Minimization", "comments": "25 pages, 3 figures; to appear in Optimization Letters (OPTL)", "journal-ref": null, "doi": "10.1007/s11590-017-1132-y", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the NP-hard problem of minimizing a convex quadratic function\nover the integer lattice ${\\bf Z}^n$. We present a simple semidefinite\nprogramming (SDP) relaxation for obtaining a nontrivial lower bound on the\noptimal value of the problem. By interpreting the solution to the SDP\nrelaxation probabilistically, we obtain a randomized algorithm for finding good\nsuboptimal solutions, and thus an upper bound on the optimal value. The\neffectiveness of the method is shown for numerical problem instances of various\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 22:07:02 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 20:47:01 GMT"}, {"version": "v3", "created": "Mon, 25 May 2015 03:14:19 GMT"}, {"version": "v4", "created": "Sat, 6 Feb 2016 04:00:54 GMT"}, {"version": "v5", "created": "Wed, 28 Sep 2016 20:58:21 GMT"}, {"version": "v6", "created": "Fri, 10 Mar 2017 20:43:48 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Park", "Jaehyun", ""], ["Boyd", "Stephen", ""]]}, {"id": "1504.07697", "submitter": "Anand Kumar Narayanan", "authors": "Anand Kumar Narayanan", "title": "Polynomial Factorization over Finite Fields By Computing Euler-Poincare\n  Characteristics of Drinfeld Modules", "comments": "Proof of theorem 1.4 and section 4 revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and rigorously analyze two randomized algorithms to factor\nunivariate polynomials over finite fields using rank $2$ Drinfeld modules. The\nfirst algorithm estimates the degree of an irreducible factor of a polynomial\nfrom Euler-Poincare characteristics of random Drinfeld modules. Knowledge of a\nfactor degree allows one to rapidly extract all factors of that degree. As a\nconsequence, the problem of factoring polynomials over finite fields in time\nnearly linear in the degree is reduced to finding Euler-Poincare\ncharacteristics of random Drinfeld modules with high probability. Notably, the\nworst case complexity of polynomial factorization over finite fields is reduced\nto the average case complexity of a problem concerning Drinfeld modules. The\nsecond algorithm is a random Drinfeld module analogue of Berlekamp's algorithm.\nDuring the course of its analysis, we prove a new bound on degree distributions\nin factorization patterns of polynomials over finite fields in certain short\nintervals.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 01:48:07 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 07:08:01 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Narayanan", "Anand Kumar", ""]]}, {"id": "1504.07828", "submitter": "Andrzej Kapanowski", "authors": "A. Kapanowski and {\\L}. Ga{\\l}uszka", "title": "Weighted graph algorithms with Python", "comments": "26 pages, no figures", "journal-ref": "The Python Papers 11, 3 (2016)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Python implementation of selected weighted graph algorithms is presented. The\nminimal graph interface is defined together with several classes implementing\nthis interface. Graph nodes can be any hashable Python objects. Directed edges\nare instances of the Edge class. Graphs are instances of the Graph class. It is\nbased on the adjacency-list representation, but with fast lookup of nodes and\nneighbors (dict-of-dict structure). Other implementations of this class are\nalso possible.\n  In this work, many algorithms are implemented using a unified approach. There\nare separate classes and modules devoted to different algorithms. Three\nalgorithms for finding a minimum spanning tree are implemented: the Boruvka's\nalgorithm, the Prim's algorithm (three implementations), and the Kruskal's\nalgorithm. Three algorithms for solving the single-source shortest path problem\nare implemented: the dag shortest path algorithm, the Bellman-Ford algorithm,\nand the Dijkstra's algorithm (two implementations). Two algorithms for solving\nall-pairs shortest path problem are implemented: the Floyd-Warshall algorithm\nand the Johnson's algorithm.\n  All algorithms were tested by means of the unittest module, the Python unit\ntesting framework. Additional computer experiments were done in order to\ncompare real and theoretical computational complexity. The source code is\navailable from the public GitHub repository.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 12:20:20 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Kapanowski", "A.", ""], ["Ga\u0142uszka", "\u0141.", ""]]}, {"id": "1504.07830", "submitter": "Yuni Iwamasa", "authors": "Hiroshi Hirai and Yuni Iwamasa", "title": "On k-Submodular Relaxation", "comments": "11 pages, corrected typos, accepted in SIAM Journal on Discrete\n  Mathematics", "journal-ref": "SIAM Journal on Discrete Mathematics, 30(3):1726-1736, 2016", "doi": "10.1137/15M101926X", "report-no": null, "categories": "math.OC cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $k$-submodular functions, introduced by Huber and Kolmogorov, are functions\ndefined on $\\{0, 1, 2, \\dots, k\\}^n$ satisfying certain submodular-type\ninequalities. $k$-submodular functions typically arise as relaxations of\nNP-hard problems, and the relaxations by $k$-submodular functions play key\nroles in design of efficient, approximation, or fixed-parameter tractable\nalgorithms. Motivated by this, we consider the following problem: Given a\nfunction $f : \\{1, 2, \\dots, k\\}^n \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$,\ndetermine whether $f$ is extended to a $k$-submodular function $g : \\{0, 1, 2,\n\\dots, k\\}^n \\rightarrow \\mathbb{R} \\cup \\{\\infty\\}$, where $g$ is called a\n$k$-submodular relaxation of $f$.\n  We give a polymorphic characterization of those functions which admit a\n$k$-submodular relaxation, and also give a combinatorial $O((k^n)^2)$-time\nalgorithm to find a $k$-submodular relaxation or establish that a\n$k$-submodular relaxation does not exist. Our algorithm has interesting\nproperties: (1) If the input function is integer valued, then our algorithm\noutputs a half-integral relaxation, and (2) if the input function is binary,\nthen our algorithm outputs the unique optimal relaxation. We present\napplications of our algorithm to valued constraint satisfaction problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 12:27:44 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 14:58:03 GMT"}, {"version": "v3", "created": "Fri, 9 Sep 2016 06:22:10 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Hirai", "Hiroshi", ""], ["Iwamasa", "Yuni", ""]]}, {"id": "1504.07834", "submitter": "Thomas Bosman", "authors": "Thomas Bosman", "title": "A Solution Merging Heuristic for the Steiner Problem in Graphs Using\n  Tree Decompositions", "comments": null, "journal-ref": "SEA 2015, LNCS 9125, pp. 391-402, 2015", "doi": "10.1007/978-3-319-20086-6_30", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed parameter tractable algorithms for bounded treewidth are known to exist\nfor a wide class of graph optimization problems. While most research in this\narea has been focused on exact algorithms, it is hard to find decompositions of\ntreewidth sufficiently small to make these al- gorithms fast enough for\npractical use. Consequently, tree decomposition based algorithms have limited\napplicability to large scale optimization. However, by first reducing the input\ngraph so that a small width tree decomposition can be found, we can harness the\npower of tree decomposi- tion based techniques in a heuristic algorithm, usable\non graphs of much larger treewidth than would be tractable to solve exactly. We\npropose a solution merging heuristic to the Steiner Tree Problem that applies\nthis idea. Standard local search heuristics provide a natural way to generate\nsubgraphs with lower treewidth than the original instance, and subse- quently\nwe extract an improved solution by solving the instance induced by this\nsubgraph. As such the fixed parameter tractable algorithm be- comes an\nefficient tool for our solution merging heuristic. For a large class of sparse\nbenchmark instances the algorithm is able to find small width tree\ndecompositions on the union of generated solutions. Subsequently it can often\nimprove on the generated solutions fast.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 12:41:53 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Bosman", "Thomas", ""]]}, {"id": "1504.07846", "submitter": "Christian Schulz", "authors": "Nitin Ahuja, Matthias Bender, Peter Sanders, Christian Schulz and\n  Andreas Wagner", "title": "Incorporating Road Networks into Territory Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of basic areas, the territory design problem asks to create a\npredefined number of territories, each containing at least one basic area, such\nthat an objective function is optimized. Desired properties of territories\noften include a reasonable balance, compact form, contiguity and small average\njourney times which are usually encoded in the objective function or formulated\nas constraints. We address the territory design problem by developing graph\ntheoretic models that also consider the underlying road network. The derived\ngraph models enable us to tackle the territory design problem by modifying\ngraph partitioning algorithms and mixed integer programming formulations so\nthat the objective of the planning problem is taken into account. We test and\ncompare the algorithms on several real world instances.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 13:11:29 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 12:40:01 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Ahuja", "Nitin", ""], ["Bender", "Matthias", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""], ["Wagner", "Andreas", ""]]}, {"id": "1504.07851", "submitter": "Patrick Hagge Cording", "authors": "Philip Bille and Patrick Hagge Cording and Inge Li G{\\o}rtz and\n  Frederik Rye Skjoldjensen and Hjalte Wedel Vildh{\\o}j and S{\\o}ren Vind", "title": "Dynamic Relative Compression, Dynamic Partial Sums, and Substring\n  Concatenation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a static reference string $R$ and a source string $S$, a relative\ncompression of $S$ with respect to $R$ is an encoding of $S$ as a sequence of\nreferences to substrings of $R$. Relative compression schemes are a classic\nmodel of compression and have recently proved very successful for compressing\nhighly-repetitive massive data sets such as genomes and web-data. We initiate\nthe study of relative compression in a dynamic setting where the compressed\nsource string $S$ is subject to edit operations. The goal is to maintain the\ncompressed representation compactly, while supporting edits and allowing\nefficient random access to the (uncompressed) source string. We present new\ndata structures that achieve optimal time for updates and queries while using\nspace linear in the size of the optimal relative compression, for nearly all\ncombinations of parameters. We also present solutions for restricted and\nextended sets of updates. To achieve these results, we revisit the dynamic\npartial sums problem and the substring concatenation problem. We present new\noptimal or near optimal bounds for these problems. Plugging in our new results\nwe also immediately obtain new bounds for the string indexing for patterns with\nwildcards problem and the dynamic text and static pattern matching problem.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 13:31:48 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 10:31:21 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 08:14:58 GMT"}, {"version": "v4", "created": "Fri, 9 Sep 2016 08:58:49 GMT"}, {"version": "v5", "created": "Fri, 16 Sep 2016 12:53:27 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Bille", "Philip", ""], ["Cording", "Patrick Hagge", ""], ["G\u00f8rtz", "Inge Li", ""], ["Skjoldjensen", "Frederik Rye", ""], ["Vildh\u00f8j", "Hjalte Wedel", ""], ["Vind", "S\u00f8ren", ""]]}, {"id": "1504.07863", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Pawel Zielinski", "title": "Using the WOWA operator in robust discrete optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a class of discrete optimization problems with uncertain costs\nis discussed. The uncertainty is modeled by introducing a scenario set\ncontaining a finite number of cost scenarios. A probability distribution in the\nscenario set is available. In order to choose a solution the weighted OWA\ncriterion (WOWA) is applied. This criterion allows decision makers to take into\naccount both probabilities for scenarios and the degree of pessimism/ optimism.\nIn this paper the complexity of the considered class of discrete optimization\nproblems is described and some exact and approximation algorithms for solving\nit are proposed. An application to a selection problem, together with results\nof computational tests are shown.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 13:59:34 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 09:36:09 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1504.07880", "submitter": "Axel Parmentier", "authors": "Axel Parmentier", "title": "Algorithms for Non-Linear and Stochastic Resource Constrained Shortest\n  Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource constrained shortest path problems are usually solved thanks to a\nsmart enumeration of all the non-dominated paths. Recent improvements of these\nenumeration algorithms rely on the use of bounds on path resources to discard\npartial solutions. The quality of the bounds determines the performance of the\nalgorithm. The main contribution of this paper is to introduce a standard\nprocedure to generate bounds on paths resources in a general setting which\ncovers most resource constrained shortest path problems, among which stochastic\nversions.\n  In that purpose, we introduce a generalization of the resource constrained\nshortest path problem where the resources are taken in a monoid. The resource\nof a path is the monoid sum of the resources of its arcs. The problem consists\nin finding a path whose resource minimizes a non-decreasing cost function of\nthe path resource among the paths that respect a given constraint. Enumeration\nalgorithms are generalized to this framework. We use lattice theory to provide\npolynomial procedures to find good quality bounds. These procedures solve a\ngeneralization of the algebraic path problem, where arc resources belong to a\nlattice ordered monoid. The practical efficiency of the approach is proved\nthrough an extensive numerical study on some deterministic and stochastic\nresource constrained shortest path problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 14:55:39 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 16:02:15 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Parmentier", "Axel", ""]]}, {"id": "1504.07912", "submitter": "Adam Smith", "authors": "Sofya Raskhodnikova, Adam Smith", "title": "Efficient Lipschitz Extensions for High-Dimensional Graph Statistics and\n  Node Private Degree Distributions", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipschitz extensions were recently proposed as a tool for designing node\ndifferentially private algorithms. However, efficiently computable Lipschitz\nextensions were known only for 1-dimensional functions (that is, functions that\noutput a single real value). In this paper, we study efficiently computable\nLipschitz extensions for multi-dimensional (that is, vector-valued) functions\non graphs. We show that, unlike for 1-dimensional functions, Lipschitz\nextensions of higher-dimensional functions on graphs do not always exist, even\nwith a non-unit stretch. We design Lipschitz extensions with small stretch for\nthe sorted degree list and for the degree distribution of a graph. Crucially,\nour extensions are efficiently computable.\n  We also develop new tools for employing Lipschitz extensions in the design of\ndifferentially private algorithms. Specifically, we generalize the exponential\nmechanism, a widely used tool in data privacy. The exponential mechanism is\ngiven a collection of score functions that map datasets to real values. It\nattempts to return the name of the function with nearly minimum value on the\ndata set. Our generalized exponential mechanism provides better accuracy when\nthe sensitivity of an optimal score function is much smaller than the maximum\nsensitivity of score functions.\n  We use our Lipschitz extension and the generalized exponential mechanism to\ndesign a node-differentially private algorithm for releasing an approximation\nto the degree distribution of a graph. Our algorithm is much more accurate than\nalgorithms from previous work.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 16:08:57 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Raskhodnikova", "Sofya", ""], ["Smith", "Adam", ""]]}, {"id": "1504.07959", "submitter": "Sebastian Krinninger", "authors": "Monika Henzinger, Sebastian Krinninger, Danupon Nanongkai", "title": "Sublinear-Time Decremental Algorithms for Single-Source Reachability and\n  Shortest Paths on Directed Graphs", "comments": "Preliminary versions of this paper were presented at the 46th ACM\n  Symposium on Theory of Computing (STOC 2014) and the 42nd International\n  Colloquium on Automata, Languages, and Programming (ICALP 2015)", "journal-ref": null, "doi": "10.1145/2591796.2591869", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider dynamic algorithms for maintaining Single-Source Reachability\n(SSR) and approximate Single-Source Shortest Paths (SSSP) on $n$-node $m$-edge\ndirected graphs under edge deletions (decremental algorithms). The previous\nfastest algorithm for SSR and SSSP goes back three decades to Even and Shiloach\n[JACM 1981]; it has $ O(1) $ query time and $ O (mn) $ total update time (i.e.,\nlinear amortized update time if all edges are deleted). This algorithm serves\nas a building block for several other dynamic algorithms. The question whether\nits total update time can be improved is a major, long standing, open problem.\n  In this paper, we answer this question affirmatively. We obtain a randomized\nalgorithm with an expected total update time of $ O(\\min (m^{7/6} n^{2/3 +\no(1)}, m^{3/4} n^{5/4 + o(1)}) ) = O (m n^{9/10 + o(1)}) $ for SSR and\n$(1+\\epsilon)$-approximate SSSP if the edge weights are integers from $ 1 $ to\n$ W \\leq 2^{\\log^c{n}} $ and $ \\epsilon \\geq 1 / \\log^c{n} $ for some constant\n$ c $. We also extend our algorithm to achieve roughly the same running time\nfor Strongly Connected Components (SCC), improving the algorithm of Roditty and\nZwick [FOCS 2002]. Our algorithm is most efficient for sparse and dense graphs.\nWhen $ m = \\Theta(n) $ its running time is $ O (n^{1 + 5/6 + o(1)}) $ and when\n$ m = \\Theta(n^2) $ its running time is $ O (n^{2 + 3/4 + o(1)}) $. For SSR we\nalso obtain an algorithm that is faster for dense graphs and has a total update\ntime of $ O ( m^{2/3} n^{4/3 + o(1)} + m^{3/7} n^{12/7 + o(1)}) $ which is $ O\n(n^{2 + 2/3}) $ when $ m = \\Theta(n^2) $. All our algorithms have constant\nquery time in the worst case and are correct with high probability against an\noblivious adversary.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 18:46:55 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 09:02:57 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1504.07976", "submitter": "Frank Kammer", "authors": "Thomas Erlebach, Michael Hoffmann, Frank Kammer", "title": "On Temporal Graph Exploration", "comments": "This is an extended version of an ICALP 2015 paper", "journal-ref": "Journal of Computer and System Sciences, 119:1-18, 2021", "doi": "10.1016/j.jcss.2021.01.005", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A temporal graph is a graph in which the edge set can change from one time\nstep to the next. The temporal graph exploration problem TEXP is the problem of\ncomputing a foremost exploration schedule for a temporal graph, i.e., a\ntemporal walk that starts at a given start node, visits all nodes of the graph,\nand has the smallest arrival time. In the first part of the paper, we consider\nonly undirected temporal graphs that are connected at each time step. For such\ntemporal graphs with $n$ nodes, we show that it is \\NP-hard to approximate TEXP\nwith ratio $O(n^{1-\\varepsilon})$ for every $\\varepsilon>0$. We also provide an\nexplicit construction of temporal graphs that require $\\Theta(n^2)$ time steps\nto be explored. In the second part of the paper, we still consider temporal\ngraphs that are connected in each time step, but we assume that the underlying\ngraph (i.e. the graph that contains all edges that are present in the temporal\ngraph in at least one time step) belongs to a specific class of graphs. Among\nother results, we show that temporal graphs can be explored in\n$O(n^{1.5}k^{1.5}\\log n)$ time steps if the underlying graph has treewidth $k$,\nin $O(n^{1.8}\\log n)$ time steps if the underlying graph is planar, and in\n$O(n\\log^3 n)$ time steps if the underlying graph is a $2\\times n$ grid. In the\nthird part of the paper, we consider settings where the graphs in future time\nsteps are not known and the exploration schedule is constructed online. We\nreplace the connectedness assumption by a weaker assumption and show that\n$m$-edge temporal graphs with regularly present edges and with\nprobabilistically present edges can be explored online in $O(m)$ time steps and\n$O(m \\log n)$ time steps with high probability, respectively. We finally show\nthat the latter result can be used to obtain a distributed algorithm for the\ngossiping problem in random temporal graphs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 19:41:52 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 13:57:20 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 11:15:11 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Erlebach", "Thomas", ""], ["Hoffmann", "Michael", ""], ["Kammer", "Frank", ""]]}, {"id": "1504.08008", "submitter": "Kyle Fox", "authors": "Kyle Fox, Philip N. Klein, Shay Mozes", "title": "A Polynomial-time Bicriteria Approximation Scheme for Planar Bisection", "comments": "To appear in STOC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph with edge costs and node weights, the minimum\nbisection problem asks for a partition of the nodes into two parts of equal\nweight such that the sum of edge costs between the parts is minimized. We give\na polynomial time bicriteria approximation scheme for bisection on planar\ngraphs.\n  Specifically, let $W$ be the total weight of all nodes in a planar graph $G$.\nFor any constant $\\varepsilon > 0$, our algorithm outputs a bipartition of the\nnodes such that each part weighs at most $W/2 + \\varepsilon$ and the total cost\nof edges crossing the partition is at most $(1+\\varepsilon)$ times the total\ncost of the optimal bisection. The previously best known approximation for\nplanar minimum bisection, even with unit node weights, was $O(\\log n)$. Our\nalgorithm actually solves a more general problem where the input may include a\ntarget weight for the smaller side of the bipartition.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 20:16:31 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Fox", "Kyle", ""], ["Klein", "Philip N.", ""], ["Mozes", "Shay", ""]]}, {"id": "1504.08011", "submitter": "Victoria Horan", "authors": "Victoria Horan, Steve Adachi, and Stanley Bak", "title": "A Comparison of Approaches for Solving Hard Graph-Theoretic Problems", "comments": "23 pages, 13 figures; revised/reformatted: same main results but\n  includes additional references and run times", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO quant-ph", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In order to formulate mathematical conjectures likely to be true, a number of\nbase cases must be determined. However, many combinatorial problems are NP-hard\nand the computational complexity makes this research approach difficult using a\nstandard brute force approach on a typical computer. One sample problem\nexplored is that of finding a minimum identifying code. To work around the\ncomputational issues, a variety of methods are explored and consist of a\nparallel computing approach using Matlab, a quantum annealing approach using\nthe D-Wave computer, and lastly using satisfiability modulo theory (SMT) and\ncorresponding SMT solvers. Each of these methods requires the problem to be\nformulated in a unique manner. In this paper, we address the challenges of\ncomputing solutions to this NP-hard problem with respect to each of these\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 20:22:12 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 12:57:27 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Horan", "Victoria", ""], ["Adachi", "Steve", ""], ["Bak", "Stanley", ""]]}, {"id": "1504.08024", "submitter": "Kent Quanrud", "authors": "Chandra Chekuri, Shalmoli Gupta, Kent Quanrud", "title": "Streaming Algorithms for Submodular Function Maximization", "comments": "29 pages, 7 figures, extended abstract to appear in ICALP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a nonnegative submodular set function\n$f:2^{\\mathcal{N}} \\rightarrow \\mathbb{R}^+$ subject to a $p$-matchoid\nconstraint in the single-pass streaming setting. Previous work in this context\nhas considered streaming algorithms for modular functions and monotone\nsubmodular functions. The main result is for submodular functions that are {\\em\nnon-monotone}. We describe deterministic and randomized algorithms that obtain\na $\\Omega(\\frac{1}{p})$-approximation using $O(k \\log k)$-space, where $k$ is\nan upper bound on the cardinality of the desired set. The model assumes value\noracle access to $f$ and membership oracles for the matroids defining the\n$p$-matchoid constraint.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 21:03:31 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Chekuri", "Chandra", ""], ["Gupta", "Shalmoli", ""], ["Quanrud", "Kent", ""]]}, {"id": "1504.08120", "submitter": "Sudeshna Kolay", "authors": "Sudeshna Kolay and Fahad Panolan", "title": "Parameterized Algorithms for Deletion to (r,l)-graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For fixed integers $r,\\ell \\geq 0$, a graph $G$ is called an {\\em\n$(r,\\ell)$-graph} if the vertex set $V(G)$ can be partitioned into $r$\nindependent sets and $\\ell$ cliques. This brings us to the following natural\nparameterized questions: {\\sc Vertex $(r,\\ell)$-Partization} and {\\sc Edge\n$(r,\\ell)$-Partization}. An input to these problems consist of a graph $G$ and\na positive integer $k$ and the objective is to decide whether there exists a\nset $S\\subseteq V(G)$ ($S\\subseteq E(G)$) such that the deletion of $S$ from\n$G$ results in an $(r,\\ell)$-graph. These problems generalize well studied\nproblems such as {\\sc Odd Cycle Transversal}, {\\sc Edge Odd Cycle Transversal},\n{\\sc Split Vertex Deletion} and {\\sc Split Edge Deletion}. We do not hope to\nget parameterized algorithms for either {\\sc Vertex $(r,\\ell)$-Partization} or\n{\\sc Edge $(r,\\ell)$-Partization} when either of $r$ or $\\ell$ is at least $3$\nas the recognition problem itself is NP-complete. This leaves the case of\n$r,\\ell \\in \\{1,2\\}$. We almost complete the parameterized complexity dichotomy\nfor these problems. Only the parameterized complexity of {\\sc Edge\n$(2,2)$-Partization} remains open. We also give an approximation algorithm and\na Turing kernelization for {\\sc Vertex $(r,\\ell)$-Partization}. We use an\ninteresting finite forbidden induced graph characterization, for a class of\ngraphs known as $(r,\\ell)$-split graphs, properly containing the class of\n$(r,\\ell)$-graphs. This approach to obtain approximation algorithms could be of\nan independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 08:45:19 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Kolay", "Sudeshna", ""], ["Panolan", "Fahad", ""]]}, {"id": "1504.08235", "submitter": "Stefan Fafianie", "authors": "Stefan Fafianie, Stefan Kratsch", "title": "A shortcut to (sun)flowers: Kernels in logarithmic space or linear time", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether kernelization results can be obtained if we restrict\nkernelization algorithms to run in logarithmic space. This restriction for\nkernelization is motivated by the question of what results are attainable for\npreprocessing via simple and/or local reduction rules. We find kernelizations\nfor d-Hitting Set(k), d-Set Packing(k), Edge Dominating Set(k) and a number of\nhitting and packing problems in graphs, each running in logspace. Additionally,\nwe return to the question of linear-time kernelization. For d-Hitting Set(k) a\nlinear-time kernelization was given by van Bevern [Algorithmica (2014)]. We\ngive a simpler procedure and save a large constant factor in the size bound.\nFurthermore, we show that we can obtain a linear-time kernel for d-Set\nPacking(k) as well.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 14:05:04 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Fafianie", "Stefan", ""], ["Kratsch", "Stefan", ""]]}, {"id": "1504.08251", "submitter": "Bodo Manthey", "authors": "Kamiel Cornelissen and Bodo Manthey", "title": "Smoothed Analysis of the Minimum-Mean Cycle Canceling Algorithm and the\n  Network Simplex Algorithm", "comments": "Extended abstract to appear in the proceedings of COCOON 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum-cost flow (MCF) problem is a fundamental optimization problem\nwith many applications and seems to be well understood. Over the last half\ncentury many algorithms have been developed to solve the MCF problem and these\nalgorithms have varying worst-case bounds on their running time. However, these\nworst-case bounds are not always a good indication of the algorithms'\nperformance in practice. The Network Simplex (NS) algorithm needs an\nexponential number of iterations for some instances, but it is considered the\nbest algorithm in practice and performs best in experimental studies. On the\nother hand, the Minimum-Mean Cycle Canceling (MMCC) algorithm is strongly\npolynomial, but performs badly in experimental studies.\n  To explain these differences in performance in practice we apply the\nframework of smoothed analysis. We show an upper bound of\n$O(mn^2\\log(n)\\log(\\phi))$ for the number of iterations of the MMCC algorithm.\nHere $n$ is the number of nodes, $m$ is the number of edges, and $\\phi$ is a\nparameter limiting the degree to which the edge costs are perturbed. We also\nshow a lower bound of $\\Omega(m\\log(\\phi))$ for the number of iterations of the\nMMCC algorithm, which can be strengthened to $\\Omega(mn)$ when\n$\\phi=\\Theta(n^2)$. For the number of iterations of the NS algorithm we show a\nsmoothed lower bound of $\\Omega(m \\cdot \\min \\{ n, \\phi \\} \\cdot \\phi)$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 14:48:15 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Cornelissen", "Kamiel", ""], ["Manthey", "Bodo", ""]]}, {"id": "1504.08265", "submitter": "Erez Kantor", "authors": "Erez Kantor and Shay Kutten", "title": "Optimal competitiveness for the Rectilinear Steiner Arborescence problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present optimal online algorithms for two related known problems involving\nSteiner Arborescence, improving both the lower and the upper bounds. One of\nthem is the well studied continuous problem of the {\\em Rectilinear Steiner\nArborescence} ($RSA$). We improve the lower bound and the upper bound on the\ncompetitive ratio for $RSA$ from $O(\\log N)$ and $\\Omega(\\sqrt{\\log N})$ to\n$\\Theta(\\frac{\\log N}{\\log \\log N})$, where $N$ is the number of Steiner\npoints. This separates the competitive ratios of $RSA$ and the Symetric-$RSA$,\ntwo problems for which the bounds of Berman and Coulston is STOC 1997 were\nidentical. The second problem is one of the Multimedia Content Distribution\nproblems presented by Papadimitriou et al. in several papers and Charikar et\nal. SODA 1998. It can be viewed as the discrete counterparts (or a network\ncounterpart) of $RSA$. For this second problem we present tight bounds also in\nterms of the network size, in addition to presenting tight bounds in terms of\nthe number of Steiner points (the latter are similar to those we derived for\n$RSA$).\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 15:18:08 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Kantor", "Erez", ""], ["Kutten", "Shay", ""]]}, {"id": "1504.08360", "submitter": "Manoj Gupta", "authors": "Manoj Gupta", "title": "Simple and Faster algorithm for Reachability in a Decremental Directed\n  Graph", "comments": "This paper is withdrawn by the author due to a crucial error in Lemma\n  3.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of maintaining source sink\nreachability($st$-Reachability), single source reachability(SSR) and strongly\nconnected component(SCC) in an edge decremental directed graph. In particular,\nwe design a randomized algorithm that maintains with high probability:\n  1) $st$-Reachability in $\\tilde{O}(mn^{4/5})$ total update time. 2)\n$st$-Reachability in a total update time of $\\tilde{O}(n^{8/3})$ in a dense\ngraph. 3) SSR in a total update time of $\\tilde{O}(m n^{9/10})$. 4) SCC in a\ntotal update time of $\\tilde{O}(m n^{9/10})$. For all the above problems, we\nimprove upon the previous best algorithm (by Henzinger et. al. (STOC 2014)).\n  Our main focus is maintaining $st$-Reachability in an edge decremental\ndirected graph (other problems can be reduced to $st$-Reachability). The\nclassical algorithm of Even and Shiloach (JACM 81) solved this problem in\n$O(1)$ query time and $O(mn)$ total update time. Recently, Henzinger,\nKrinninger and Nanongkai (STOC 2014) designed a randomized algorithm which\nachieves an update time of $\\tilde{O}(m n^{0.98})$ and broke the long-standing\n$O(mn)$ bound of Even and Shiloach. However, they designed four algorithms $A_i\n(1\\le i \\le 4)$ such that for graphs having total number of edges between $m_i$\nand $m_{i+1}$ ($m_{i+1} > m_i$), $A_i$ outperforms other three algorithms. That\nis, one of the four algorithms may be faster for a particular density range of\nedges, but it may be too slow asymptotically for the other ranges. Our main\ncontribution is that we design a {\\it single} algorithm which works for all\ntypes of graphs. Not only is our algorithm faster, it is much simpler than the\nalgorithm designed by Henzinger et.al. (STOC 2014).\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 19:42:19 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 06:23:42 GMT"}, {"version": "v3", "created": "Sat, 16 May 2015 13:13:27 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Gupta", "Manoj", ""]]}, {"id": "1504.08363", "submitter": "Gautam Kamath", "authors": "Constantinos Daskalakis and Gautam Kamath and Christos Tzamos", "title": "On the Structure, Covering, and Learning of Poisson Multinomial\n  Distributions", "comments": "49 pages, extended abstract appeared in FOCS 2015", "journal-ref": null, "doi": "10.1109/FOCS.2015.77", "report-no": null, "categories": "cs.DS cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the\nsum of $n$ independent random vectors supported on the set ${\\cal\nB}_k=\\{e_1,\\ldots,e_k\\}$ of standard basis vectors in $\\mathbb{R}^k$. We prove\na structural characterization of these distributions, showing that, for all\n$\\varepsilon >0$, any $(n, k)$-Poisson multinomial random vector is\n$\\varepsilon$-close, in total variation distance, to the sum of a discretized\nmultidimensional Gaussian and an independent $(\\text{poly}(k/\\varepsilon),\nk)$-Poisson multinomial random vector. Our structural characterization extends\nthe multi-dimensional CLT of Valiant and Valiant, by simultaneously applying to\nall approximation requirements $\\varepsilon$. In particular, it overcomes\nfactors depending on $\\log n$ and, importantly, the minimum eigenvalue of the\nPMD's covariance matrix from the distance to a multidimensional Gaussian random\nvariable.\n  We use our structural characterization to obtain an $\\varepsilon$-cover, in\ntotal variation distance, of the set of all $(n, k)$-PMDs, significantly\nimproving the cover size of Daskalakis and Papadimitriou, and obtaining the\nsame qualitative dependence of the cover size on $n$ and $\\varepsilon$ as the\n$k=2$ cover of Daskalakis and Papadimitriou. We further exploit this structure\nto show that $(n,k)$-PMDs can be learned to within $\\varepsilon$ in total\nvariation distance from $\\tilde{O}_k(1/\\varepsilon^2)$ samples, which is\nnear-optimal in terms of dependence on $\\varepsilon$ and independent of $n$. In\nparticular, our result generalizes the single-dimensional result of Daskalakis,\nDiakonikolas, and Servedio for Poisson Binomials to arbitrary dimension.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 19:53:03 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 00:04:51 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 20:58:00 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Kamath", "Gautam", ""], ["Tzamos", "Christos", ""]]}]