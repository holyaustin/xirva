[{"id": "1802.00084", "submitter": "David Eppstein", "authors": "David Eppstein and Vijay V. Vazirani", "title": "NC Algorithms for Computing a Perfect Matching and a Maximum Flow in\n  One-Crossing-Minor-Free Graphs", "comments": "21 pages, 6 figures", "journal-ref": "SIAM J. Computing 50 (3): 1014-1033, 2021", "doi": "10.1137/19M1256221", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 1988, Vazirani gave an NC algorithm for computing the number of perfect\nmatchings in $K_{3,3}$-minor-free graphs by building on Kasteleyn's scheme for\nplanar graphs, and stated that this \"opens up the possibility of obtaining an\nNC algorithm for finding a perfect matching in $K_{3,3}$-free graphs.\" In this\npaper, we finally settle this 30-year-old open problem. Building on recent NC\nalgorithms for planar and bounded-genus perfect matching by Anari and Vazirani\nand later by Sankowski, we obtain NC algorithms for perfect matching in any\nminor-closed graph family that forbids a one-crossing graph. This family\nincludes several well-studied graph families including the $K_{3,3}$-minor-free\ngraphs and $K_5$-minor-free graphs. Graphs in these families not only have\nunbounded genus, but can have genus as high as $O(n)$. Our method applies as\nwell to several other problems related to perfect matching. In particular, we\nobtain NC algorithms for the following problems in any family of graphs (or\nnetworks) with a one-crossing forbidden minor:\n  $\\bullet$ Determining whether a given graph has a perfect matching and if so,\nfinding one.\n  $\\bullet$ Finding a minimum weight perfect matching in the graph, assuming\nthat the edge weights are polynomially bounded.\n  $\\bullet$ Finding a maximum $st$-flow in the network, with arbitrary\ncapacities.\n  The main new idea enabling our results is the definition and use of\nmatching-mimicking networks, small replacement networks that behave the same,\nwith respect to matching problems involving a fixed set of terminals, as the\nlarger network they replace.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 22:10:47 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 06:40:46 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Eppstein", "David", ""], ["Vazirani", "Vijay V.", ""]]}, {"id": "1802.00233", "submitter": "Nader Bshouty", "authors": "Nader H. Bshouty and Waseem Makhoul", "title": "On Polynomial time Constructions of Minimum Height Decision Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a polynomial time algorithms that for an input\n$A\\subseteq {B_m}$ outputs a decision tree for $A$ of minimum depth. This\nproblem has many applications that include, to name a few, computer vision,\ngroup testing, exact learning from membership queries and game theory.\n  Arkin et al. and Moshkov gave a polynomial time $(\\ln |A|)$- approximation\nalgorithm (for the depth). The result of Dinur and Steurer for set cover\nimplies that this problem cannot be approximated with ratio $(1-o(1))\\cdot \\ln\n|A|$, unless P=NP. Moskov the combinatorial measure of extended teaching\ndimension of $A$, $ETD(A)$. He showed that $ETD(A)$ is a lower bound for the\ndepth of the decision tree for $A$ and then gave an {\\it exponential time}\n$ETD(A)/\\log(ETD(A))$-approximation algorithm.\n  In this paper we further study the $ETD(A)$ measure and a new combinatorial\nmeasure, $DEN(A)$, that we call the density of the set $A$. We show that\n$DEN(A)\\le ETD(A)+1$. We then give two results. The first result is that the\nlower bound $ETD(A)$ of Moshkov for the depth of the decision tree for $A$ is\ngreater than the bounds that are obtained by the classical technique used in\nthe literature. The second result is a polynomial time $(\\ln 2)\nDEN(A)$-approximation (and therefore $(\\ln 2) ETD(A)$-approximation) algorithm\nfor the depth of the decision tree of $A$. We also show that a better\napproximation ratio implies P=NP.\n  We then apply the above results to learning the class of disjunctions of\npredicates from membership queries. We show that the $ETD$ of this class is\nbounded from above by the degree $d$ of its Hasse diagram. We then show that\nMoshkov algorithm can be run in polynomial time and is $(d/\\log\nd)$-approximation algorithm. This gives optimal algorithms when the degree is\nconstant. For example, learning axis parallel rays over constant dimension\nspace.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 10:38:30 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Bshouty", "Nader H.", ""], ["Makhoul", "Waseem", ""]]}, {"id": "1802.00347", "submitter": "Saeid Safaei", "authors": "Saeid Safaei, Vahid Safaei, Elizabeth D. Trippe, Karen Aguar, Hamid R.\n  Arabnia", "title": "Solving Minimum k-supplier in Adleman-Lipton model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an algorithm for solving the minimum k supplier\nproblem using the Adleman Lipton model. The procedure works in polynomial steps\nfor the minimum k supplier problem of an undirected graph with n vertices,\nwhich is an NP hard combinatorial optimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 22:44:19 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Safaei", "Saeid", ""], ["Safaei", "Vahid", ""], ["Trippe", "Elizabeth D.", ""], ["Aguar", "Karen", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "1802.00459", "submitter": "Peilin Zhong", "authors": "Wei Hu, Zhao Song, Lin F. Yang, Peilin Zhong", "title": "Nearly Optimal Dynamic $k$-Means Clustering for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the $k$-means clustering problem in the dynamic streaming\nsetting, where points from a discrete Euclidean space $\\{1, 2, \\ldots,\n\\Delta\\}^d$ can be dynamically inserted to or deleted from the dataset. For\nthis problem, we provide a one-pass coreset construction algorithm using space\n$\\tilde{O}(k\\cdot \\mathrm{poly}(d, \\log\\Delta))$, where $k$ is the target\nnumber of centers. To our knowledge, this is the first dynamic geometric data\nstream algorithm for $k$-means using space polynomial in dimension and nearly\noptimal (linear) in $k$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 19:07:51 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 18:47:20 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Hu", "Wei", ""], ["Song", "Zhao", ""], ["Yang", "Lin F.", ""], ["Zhong", "Peilin", ""]]}, {"id": "1802.00624", "submitter": "Filip Malmberg", "authors": "Filip Malmberg and Robin Strand", "title": "When can $l_p$-norm objective functions be minimized via graph cuts?", "comments": "In proceedings of the 19th international workshop on combinatorial\n  image analysis (IWCIA), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques based on minimal graph cuts have become a standard tool for\nsolving combinatorial optimization problems arising in image processing and\ncomputer vision applications. These techniques can be used to minimize\nobjective functions written as the sum of a set of unary and pairwise terms,\nprovided that the objective function is submodular. This can be interpreted as\nminimizing the $l_1$-norm of the vector containing all pairwise and unary\nterms. By raising each term to a power $p$, the same technique can also be used\nto minimize the $l_p$-norm of the vector. Unfortunately, the submodularity of\nan $l_1$-norm objective function does not guarantee the submodularity of the\ncorresponding $l_p$-norm objective function. The contribution of this paper is\nto provide useful conditions under which an $l_p$-norm objective function is\nsubmodular for all $p\\geq 1$, thereby identifying a large class of $l_p$-norm\nobjective functions that can be minimized via minimal graph cuts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 10:09:22 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 08:10:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Malmberg", "Filip", ""], ["Strand", "Robin", ""]]}, {"id": "1802.00884", "submitter": "Michael Mitzenmacher", "authors": "Michael Mitzenmacher", "title": "A Model for Learned Bloom Filters and Related Structures", "comments": "5 pages, commentary on the \"Learned Index Structures\" paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has suggested enhancing Bloom filters by using a pre-filter,\nbased on applying machine learning to model the data set the Bloom filter is\nmeant to represent. Here we model such learned Bloom filters, clarifying what\nguarantees can and cannot be associated with such a structure.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 00:30:52 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Mitzenmacher", "Michael", ""]]}, {"id": "1802.01189", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Qin Zhang, Haixu Tang", "title": "Smooth $q$-Gram, and Its Applications to Detection of Overlaps among\n  Long, Error-Prone Sequencing Reads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose smooth $q$-gram, the first variant of $q$-gram that captures\n$q$-gram pair within a small edit distance. We apply smooth $q$-gram to the\nproblem of detecting overlapping pairs of error-prone reads produced by single\nmolecule real time sequencing (SMRT), which is the first and most critical step\nof the de novo fragment assembly of SMRT reads. We have implemented and tested\nour algorithm on a set of real world benchmarks. Our empirical results\ndemonstrated the significant superiority of our algorithm over the existing\n$q$-gram based algorithms in accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 20:36:57 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Zhang", "Haoyu", ""], ["Zhang", "Qin", ""], ["Tang", "Haixu", ""]]}, {"id": "1802.01239", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Kun Zhang", "title": "Counting and Sampling from Markov Equivalent DAGs Using Clique Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed acyclic graph (DAG) is the most common graphical model for\nrepresenting causal relationships among a set of variables. When restricted to\nusing only observational data, the structure of the ground truth DAG is\nidentifiable only up to Markov equivalence, based on conditional independence\nrelations among the variables. Therefore, the number of DAGs equivalent to the\nground truth DAG is an indicator of the causal complexity of the underlying\nstructure--roughly speaking, it shows how many interventions or how much\nadditional information is further needed to recover the underlying DAG. In this\npaper, we propose a new technique for counting the number of DAGs in a Markov\nequivalence class. Our approach is based on the clique tree representation of\nchordal graphs. We show that in the case of bounded degree graphs, the proposed\nalgorithm is polynomial time. We further demonstrate that this technique can be\nutilized for uniform sampling from a Markov equivalence class, which provides a\nstochastic way to enumerate DAGs in the equivalence class and may be needed for\nfinding the best DAG or for causal inference given the equivalence class as\ninput. We also extend our counting and sampling method to the case where prior\nknowledge about the underlying DAG is available, and present applications of\nthis extension in causal experiment design and estimating the causal effect of\njoint interventions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 02:32:05 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 01:49:04 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Salehkaleybar", "Saber", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""]]}, {"id": "1802.01242", "submitter": "Kent Quanrud", "authors": "Chandra Chekuri and Kent Quanrud", "title": "Fast Approximations for Metric-TSP via Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop faster approximation algorithms for Metric-TSP building on recent,\nnearly linear time approximation schemes for the LP relaxation [Chekuri and\nQuanrud, 2017]. We show that the LP solution can be sparsified via\ncut-sparsification techniques such as those of Benczur and Karger [2015]. Given\na weighted graph $G$ with $m$ edges and $n$ vertices, and $\\epsilon > 0$, our\nrandomized algorithm outputs with high probability a $(1+\\epsilon)$-approximate\nsolution to the LP relaxation whose support has $\\operatorname{O}(n \\log n\n/\\epsilon^2)$ edges. The running time of the algorithm is\n$\\operatorname{\\~O}(m/\\epsilon^2)$. This can be generically used to speed up\nalgorithms that rely on the LP.\n  For Metric-TSP, we obtain the following concrete result. For a weighted graph\n$G$ with $m$ edges and $n$ vertices, and $\\epsilon > 0$, we describe an\nalgorithm that outputs with high probability a tour of $G$ with cost at most\n$(1 + \\epsilon) \\frac{3}{2}$ times the minimum cost tour of $G$ in time\n$\\operatorname{\\~O}(m/\\epsilon^2 + n^{1.5}/\\epsilon^3)$. Previous\nimplementations of Christofides' algorithm [Christofides, 1976] require, for a\n$\\frac{3}{2}$-optimal tour, $\\operatorname{\\~O}(n^{2.5})$ time when the metric\nis explicitly given, or $\\operatorname{\\~O}(\\min\\{m^{1.5}, mn+n^{2.5}\\})$ time\nwhen the metric is given implicitly as the shortest path metric of a weighted\ngraph.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 02:48:36 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""]]}, {"id": "1802.01312", "submitter": "James Saunderson", "authors": "Reza Eghbali, James Saunderson, Maryam Fazel", "title": "Competitive Online Algorithms for Resource Allocation over the Positive\n  Semidefinite Cone", "comments": "23 pages", "journal-ref": null, "doi": "10.1007/s10107-018-1305-1", "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new and general online resource allocation problem, where the\ngoal is to maximize a function of a positive semidefinite (PSD) matrix with a\nscalar budget constraint. The problem data arrives online, and the algorithm\nneeds to make an irrevocable decision at each step. Of particular interest are\nclassic experiment design problems in the online setting, with the algorithm\ndeciding whether to allocate budget to each experiment as new experiments\nbecome available sequentially.\n  We analyze two greedy primal-dual algorithms and provide bounds on their\ncompetitive ratios. Our analysis relies on a smooth surrogate of the objective\nfunction that needs to satisfy a new diminishing returns (PSD-DR) property\n(that its gradient is order-reversing with respect to the PSD cone). Using the\nrepresentation for monotone maps on the PSD cone given by L\\\"owner's theorem,\nwe obtain a convex parametrization of the family of functions satisfying\nPSD-DR. We then formulate a convex optimization problem to directly optimize\nour competitive ratio bound over this set. This design problem can be solved\noffline before the data start arriving. The online algorithm that uses the\ndesigned smoothing is tailored to the given cost function, and enjoys a\ncompetitive ratio at least as good as our optimized bound. We provide examples\nof computing the smooth surrogate for D-optimal and A-optimal experiment\ndesign, and demonstrate the performance of the custom-designed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 09:43:15 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 23:43:14 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Eghbali", "Reza", ""], ["Saunderson", "James", ""], ["Fazel", "Maryam", ""]]}, {"id": "1802.01338", "submitter": "Siddharth Iyer", "authors": "Samir Datta, Siddharth Iyer, Raghav Kulkarni, Anish Mukherjee", "title": "Shortest $k$-Disjoint Paths via Determinants", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known $k$-disjoint path problem ($k$-DPP) asks for pairwise\nvertex-disjoint paths between $k$ specified pairs of vertices $(s_i, t_i)$ in a\ngiven graph, if they exist. The decision version of the shortest $k$-DPP asks\nfor the length of the shortest (in terms of total length) such paths. Similarly\nthe search and counting versions ask for one such and the number of such\nshortest set of paths, respectively.\n  We restrict attention to the shortest $k$-DPP instances on undirected planar\ngraphs where all sources and sinks lie on a single face or on a pair of faces.\nWe provide efficient sequential and parallel algorithms for the search versions\nof the problem answering one of the main open questions raised by Colin de\nVerdiere and Schrijver for the general one-face problem. We do so by providing\na randomised $NC^2$ algorithm along with an $O(n^{\\omega})$ time randomised\nsequential algorithm. We also obtain deterministic algorithms with similar\nresource bounds for the counting and search versions.\n  In contrast, previously, only the sequential complexity of decision and\nsearch versions of the \"well-ordered\" case has been studied. For the one-face\ncase, sequential versions of our routines have better running times for\nconstantly many terminals. In addition, the earlier best known sequential\nalgorithms (e.g. Borradaile et al.) were randomised while ours are also\ndeterministic.\n  The algorithms are based on a bijection between a shortest $k$-tuple of\ndisjoint paths in the given graph and cycle covers in a related digraph. This\nallows us to non-trivially modify established techniques relating counting\ncycle covers to the determinant. We further need to do a controlled\ninclusion-exclusion to produce a polynomial sum of determinants such that all\n\"bad\" cycle covers cancel out in the sum allowing us to count \"good\" cycle\ncovers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 10:44:28 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Datta", "Samir", ""], ["Iyer", "Siddharth", ""], ["Kulkarni", "Raghav", ""], ["Mukherjee", "Anish", ""]]}, {"id": "1802.01453", "submitter": "Ramanujan M. S.", "authors": "Daniel Lokshtanov, M. S. Ramanujan, Saket Saurabh and Meirav Zehavi", "title": "Reducing CMSO Model Checking to Highly Connected Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a Counting Monadic Second Order (CMSO) sentence $\\psi$, the\nCMSO$[\\psi]$ problem is defined as follows. The input to CMSO$[\\psi]$ is a\ngraph $G$, and the objective is to determine whether $G\\models \\psi$. Our main\ntheorem states that for every CMSO sentence $\\psi$, if CMSO$[\\psi]$ is solvable\nin polynomial time on \"globally highly connected graphs\", then CMSO$[\\psi]$ is\nsolvable in polynomial time (on general graphs). We demonstrate the utility of\nour theorem in the design of parameterized algorithms. Specifically we show\nthat technical problem-specific ingredients of a powerful method for designing\nparameterized algorithms, recursive understanding, can be replaced by a\nblack-box invocation of our main theorem. We also show that our theorem can be\neasily deployed to show fixed parameterized tractability of a wide range of\nproblems, where the input is a graph $G$ and the task is to find a connected\ninduced subgraph of $G$ such that \"few\" vertices in this subgraph have\nneighbors outside the subgraph, and additionally the subgraph has a\nCMSO-definable property.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 15:06:32 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Lokshtanov", "Daniel", ""], ["Ramanujan", "M. S.", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1802.01481", "submitter": "Jeffrey Lienert", "authors": "Jeffrey Lienert, Laura Koehly, Felix Reed-Tsochas, Christopher Steven\n  Marcum", "title": "An efficient counting method for the colored triad census", "comments": null, "journal-ref": "Social Networks 59 (2019) 136-142", "doi": "10.1016/j.socnet.2019.04.003", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The triad census is an important approach to understand local structure in\nnetwork science, providing comprehensive assessments of the observed relational\nconfigurations between triples of actors in a network. However, researchers are\noften interested in combinations of relational and categorical nodal\nattributes. In this case, it is desirable to account for the label, or color,\nof the nodes in the triad census. In this paper, we describe an efficient\nalgorithm for constructing the colored triad census, based, in part, on\nexisting methods for the classic triad census. We evaluate the performance of\nthe algorithm using empirical and simulated data for both undirected and\ndirected graphs. The results of the simulation demonstrate that the proposed\nalgorithm reduces computational time many-fold over the naive approach. We also\napply the colored triad census to the Zachary karate club network dataset. We\nsimultaneously show the efficiency of the algorithm, and a way to conduct a\nstatistical test on the census by forming a null distribution from 1,000\nrealizations of a mixing-matrix conditioned graph and comparing the observed\ncolored triad counts to the expected. From this, we demonstrate the method's\nutility in our discussion of results about homophily, heterophily, and\nbridging, simultaneously gained via the colored triad census. In sum, the\nproposed algorithm for the colored triad census brings novel utility to social\nnetwork analysis in an efficient package.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 15:56:30 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 15:21:29 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Lienert", "Jeffrey", ""], ["Koehly", "Laura", ""], ["Reed-Tsochas", "Felix", ""], ["Marcum", "Christopher Steven", ""]]}, {"id": "1802.01754", "submitter": "Xinle Liu", "authors": "Xinle Liu", "title": "How to select the best set of ads: Can we do better than Greedy\n  Algorithm?", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the best set of ads is critical for advertisers for a given set of\nkeywords, which involves the composition of ads from millions of candidates.\nWhile click through rates (CTRs) are important, there could be high correlation\namong different ads, therefore the set of ads with top CTRs does not\nnecessarily maximize the number of clicks. Greedy algorithm has been a standard\nand straightforward way to find out a decent enough solution, however, it is\nnot guaranteed to be the global optimum. In fact, it proves not to be the\nglobal optimum more than 70% of the time across all our simulations, implying\nthat it's very likely to be trapped at a local optimum. In this paper, we\npropose a Greedy-Power Algorithm to find out the best set of creatives, that is\nstarting with the solution from the conventional Greedy Algorithm, one can\nperform another Greedy Algorithm search on top of it, with the option of a few\nor even infinite rounds. The Greedy-Power algorithm is guaranteed to be not\nworse, as it only moves in the direction to increase the goal function. We show\nthat Greedy-Power Algorithm's performance is consistently better, and reach the\nconclusion that it is able to perform better than the Greedy Algorithm\nsystematically.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 01:32:34 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Liu", "Xinle", ""]]}, {"id": "1802.01997", "submitter": "Victor Verdugo", "authors": "Jos\\'e A. Soto, Abner Turkieltaub and Victor Verdugo", "title": "Strong Algorithms for the Ordinal Matroid Secretary Problem", "comments": "A preliminary version appeared at ACM-SIAM SODA 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the ordinal Matroid Secretary Problem (MSP), elements from a weighted\nmatroid are presented in random order to an algorithm that must incrementally\nselect a large weight independent set. However, the algorithm can only compare\npairs of revealed elements without using its numerical value. An algorithm is\n$\\alpha$ probability-competitive if every element from the optimum appears with\nprobability $1/\\alpha$ in the output. We present a technique to design\nalgorithms with strong probability-competitive ratios, improving the guarantees\nfor almost every matroid class considered in the literature: e.g., we get\nratios of 4 for graphic matroids (improving on $2e$ by Korula and P\\'al [ICALP\n2009]) and of 5.19 for laminar matroids (improving on 9.6 by Ma et al. [THEOR\nCOMPUT SYST 2016]). We also obtain new results for superclasses of $k$ column\nsparse matroids, for hypergraphic matroids, certain gammoids and graph packing\nmatroids, and a $1+O(\\sqrt{\\log \\rho/\\rho})$ probability-competitive algorithm\nfor uniform matroids of rank $\\rho$ based on Kleinberg's $1+O(\\sqrt{1/\\rho})$\nutility-competitive algorithm [SODA 2005] for that class. Our second\ncontribution are algorithms for the ordinal MSP on arbitrary matroids of rank\n$\\rho$. We devise an $O(\\log \\rho)$ probability-competitive algorithm and an\n$O(\\log\\log \\rho)$ ordinal-competitive algorithm, a weaker notion of\ncompetitiveness but stronger than the utility variant. These are based on the\n$O(\\log\\log \\rho)$ utility-competitive algorithm by Feldman et al.~[SODA 2015].\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 15:26:37 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Soto", "Jos\u00e9 A.", ""], ["Turkieltaub", "Abner", ""], ["Verdugo", "Victor", ""]]}, {"id": "1802.02050", "submitter": "Astrid Pieterse", "authors": "Bart M. P. Jansen and Astrid Pieterse", "title": "Optimal Data Reduction for Graph Coloring Using Low-Degree Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of kernelization can be used to rigorously analyze data reduction\nfor graph coloring problems. Here, the aim is to reduce a q-Coloring input to\nan equivalent but smaller input whose size is provably bounded in terms of\nstructural properties, such as the size of a minimum vertex cover. In this\npaper we settle two open problems about data reduction for q-Coloring.\n  First, we obtain a kernel of bitsize $O(k^{q-1}\\log{k})$ for q-Coloring\nparameterized by Vertex Cover, for any q >= 3. This size bound is optimal up to\n$k^{o(1)}$ factors assuming NP is not a subset of coNP/poly, and improves on\nthe previous-best kernel of size $O(k^q)$. We generalize this result for\ndeciding q-colorability of a graph G, to deciding the existence of a\nhomomorphism from G to an arbitrary fixed graph H. Furthermore, we can replace\nthe parameter vertex cover by the less restrictive parameter twin-cover. We\nprove that H-Coloring parameterized by Twin-Cover has a kernel of size\n$O(k^{\\Delta(H)}\\log k)$.\n  Our second result shows that 3-Coloring does not admit non-trivial\nsparsification: assuming NP is not a subset of coNP/poly, the parameterization\nby the number of vertices n admits no (generalized) kernel of size $O(n^{2-e})$\nfor any e > 0. Previously, such a lower bound was only known for coloring with\nq >= 4 colors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 16:34:30 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Jansen", "Bart M. P.", ""], ["Pieterse", "Astrid", ""]]}, {"id": "1802.02270", "submitter": "Daniel Roche", "authors": "Daniel S. Roche", "title": "Error correction in fast matrix multiplication and inverse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present new algorithms to detect and correct errors in the product of two\nmatrices, or the inverse of a matrix, over an arbitrary field. Our algorithms\ndo not require any additional information or encoding other than the original\ninputs and the erroneous output. Their running time is softly linear in the\nnumber of nonzero entries in these matrices when the number of errors is\nsufficiently small, and they also incorporate fast matrix multiplication so\nthat the cost scales well when the number of errors is large. These algorithms\nbuild on the recent result of Gasieniec et al (2017) on correcting matrix\nproducts, as well as existing work on verification algorithms, sparse low-rank\nlinear algebra, and sparse polynomial interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 00:20:12 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Roche", "Daniel S.", ""]]}, {"id": "1802.02325", "submitter": "Lijie Chen", "authors": "Lijie Chen", "title": "On The Hardness of Approximate and Exact (Bichromatic) Maximum Inner\n  Product", "comments": "abstract shortened to meet the constraint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the (Bichromatic) Maximum Inner Product Problem\n(Max-IP), in which we are given sets $A$ and $B$ of vectors, and the goal is to\nfind $a \\in A$ and $b \\in B$ maximizing inner product $a \\cdot b$. Max-IP is\nvery basic and serves as the base problem in the recent breakthrough of [Abboud\net al., FOCS 2017] on hardness of approximation for polynomial-time problems.\nIt is also used (implicitly) in the argument for hardness of exact\n$\\ell_2$-Furthest Pair (and other important problems in computational geometry)\nin poly-log-log dimensions in [Williams, SODA 2018]. We have three main results\nregarding this problem.\n  First, we study the best multiplicative approximation ratio for Boolean\nMax-IP in sub-quadratic time. We show that, for Max-IP with two sets of $n$\nvectors from $\\{0,1\\}^{d}$, there is an $n^{2 - \\Omega(1)}$ time $\\left( d/\\log\nn \\right)^{\\Omega(1)}$-multiplicative-approximating algorithm, and we show this\nis conditionally optimal, as such a $\\left(d/\\log\nn\\right)^{o(1)}$-approximating algorithm would refute SETH.\n  Second, we achieve a similar characterization for the best additive\napproximation error to Boolean Max-IP. We show that, for Max-IP with two sets\nof $n$ vectors from $\\{0,1\\}^{d}$, there is an $n^{2 - \\Omega(1)}$ time\n$\\Omega(d)$-additive-approximating algorithm, and this is conditionally\noptimal, as such an $o(d)$-approximating algorithm would refute SETH\n[Rubinstein, STOC 2018].\n  Last, we revisit the hardness of solving Max-IP exactly for vectors with\ninteger entries. We show that, under SETH, for Max-IP with sets of $n$ vectors\nfrom $\\mathbb{Z}^{d}$ for some $d = 2^{O(\\log^{*} n)}$, every exact algorithm\nrequires $n^{2 - o(1)}$ time. With the reduction from [Williams, SODA 2018], it\nfollows that $\\ell_2$-Furthest Pair and Bichromatic $\\ell_2$-Closest Pair in\n$2^{O(\\log^{*} n)}$ dimensions require $n^{2 - o(1)}$ time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 07:04:51 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 03:55:46 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Chen", "Lijie", ""]]}, {"id": "1802.02350", "submitter": "Satoshi Takabe", "authors": "Satoshi Takabe, Koji Hukushima, Alexander K. Hartmann", "title": "Large-deviation Properties of Linear-programming Computational Hardness\n  of the Vertex Cover Problem", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of the computational cost of linear-programming (LP)\nrelaxation for vertex cover problems on Erdos-Renyi random graphs is evaluated\nby using the rare-event sampling method. As a large-deviation property,\ndifferences of the distribution for \"easy\" and \"hard\" problems are found\nreflecting the hardness of approximation by LP relaxation. In particular, by\nevaluating the total variation distance between conditional distributions with\nrespect to the hardness, it is suggested that those distributions are almost\nindistinguishable in the replica symmetric (RS) phase while they asymptotically\ndiffer in the replica symmetry breaking (RSB) phase. In addition, we seek for a\nrelation to graph structure by investigating a similarity to bipartite graphs,\nwhich exhibits a quantitative difference between the RS and RSB phase. These\nresults indicate the nontrivial relation of the typical computational cost of\nLP relaxation to the RS-RSB phase transition as present in the spin-glass\ntheory of models on the corresponding random graph structure.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 08:34:28 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Takabe", "Satoshi", ""], ["Hukushima", "Koji", ""], ["Hartmann", "Alexander K.", ""]]}, {"id": "1802.02379", "submitter": "Federico D'Ambrosio", "authors": "Federico D'Ambrosio, Hans L. Bodlaender and Gerard T. Barkema", "title": "Dynamic Sampling from a Discrete Probability Distribution with a Known\n  Distribution of Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a number of efficient data structures for the\nproblem of sampling from a dynamically changing discrete probability\ndistribution, where some prior information is known on the distribution of the\nrates, in particular the maximum and minimum rate, and where the number of\npossible outcomes N is large.\n  We consider three basic data structures, the Acceptance-Rejection method, the\nComplete Binary Tree and the Alias Method. These can be used as building blocks\nin a multi-level data structure, where at each of the levels, one of the basic\ndata structures can be used.\n  Depending on assumptions on the distribution of the rates of outcomes,\ndifferent combinations of the basic structures can be used. We prove that for\nparticular data structures the expected time of sampling and update is\nconstant, when the rates follow a non-decreasing distribution, log-uniform\ndistribution or an inverse polynomial distribution, and show that for any\ndistribution, an expected time of sampling and update of\n$O\\left(\\log\\log{r_{max}}/{r_{min}}\\right)$ is possible, where $r_{max}$ is the\nmaximum rate and $r_{min}$ the minimum rate.\n  We also present an experimental verification, highlighting the limits given\nby the constraints of a real-life setting.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 10:40:05 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 09:20:28 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 08:41:59 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2020 13:34:19 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["D'Ambrosio", "Federico", ""], ["Bodlaender", "Hans L.", ""], ["Barkema", "Gerard T.", ""]]}, {"id": "1802.02381", "submitter": "Kenjiro Takazawa", "authors": "Naonori Kakimura, Naoyuki Kamiyama, Kenjiro Takazawa", "title": "The $b$-branching problem in digraphs", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the concept of $b$-branchings in digraphs, which\nis a generalization of branchings serving as a counterpart of $b$-matchings.\nHere $b$ is a positive integer vector on the vertex set of a digraph, and a\n$b$-branching is defined as a common independent set of two matroids defined by\n$b$: an arc set is a $b$-branching if it has at most $b(v)$ arcs sharing the\nterminal vertex $v$, and it is an independent set of a certain sparsity matroid\ndefined by $b$. We demonstrate that $b$-branchings yield an appropriate\ngeneralization of branchings by extending several classical results on\nbranchings. We first present a multi-phase greedy algorithm for finding a\nmaximum-weight $b$-branching. We then prove a packing theorem extending\nEdmonds' disjoint branchings theorem, and provide a strongly polynomial\nalgorithm for finding optimal disjoint $b$-branchings. As a consequence of the\npacking theorem, we prove the integer decomposition property of the\n$b$-branching polytope. Finally, we deal with a further generalization in which\na matroid constraint is imposed on the $b(v)$ arcs sharing the terminal vertex\n$v$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 10:44:46 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Kakimura", "Naonori", ""], ["Kamiyama", "Naoyuki", ""], ["Takazawa", "Kenjiro", ""]]}, {"id": "1802.02547", "submitter": "Surbhi Goel", "authors": "Surbhi Goel, Adam Klivans and Raghu Meka", "title": "Learning One Convolutional Layer with Overlapping Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first provably efficient algorithm for learning a one hidden\nlayer convolutional network with respect to a general class of (potentially\noverlapping) patches. Additionally, our algorithm requires only mild conditions\non the underlying distribution. We prove that our framework captures commonly\nused schemes from computer vision, including one-dimensional and\ntwo-dimensional \"patch and stride\" convolutions.\n  Our algorithm-- $Convotron$ -- is inspired by recent work applying isotonic\nregression to learning neural networks. Convotron uses a simple, iterative\nupdate rule that is stochastic in nature and tolerant to noise (requires only\nthat the conditional mean function is a one layer convolutional network, as\nopposed to the realizable setting). In contrast to gradient descent, Convotron\nrequires no special initialization or learning-rate tuning to converge to the\nglobal optimum.\n  We also point out that learning one hidden convolutional layer with respect\nto a Gaussian distribution and just $one$ disjoint patch $P$ (the other patches\nmay be arbitrary) is $easy$ in the following sense: Convotron can efficiently\nrecover the hidden weight vector by updating $only$ in the direction of $P$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:41:25 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Goel", "Surbhi", ""], ["Klivans", "Adam", ""], ["Meka", "Raghu", ""]]}, {"id": "1802.02556", "submitter": "Huan Li", "authors": "Huan Li, Richard Peng, Liren Shan, Yuhao Yi, Zhongzhi Zhang", "title": "Current Flow Group Closeness Centrality for Complex Networks", "comments": "31 pages, 4 figures", "journal-ref": "WWW'2019", "doi": "10.1145/3308558.3313490", "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current flow closeness centrality (CFCC) has a better discriminating ability\nthan the ordinary closeness centrality based on shortest paths. In this paper,\nwe extend this notion to a group of vertices in a weighted graph, and then\nstudy the problem of finding a subset $S$ of $k$ vertices to maximize its CFCC\n$C(S)$, both theoretically and experimentally. We show that the problem is\nNP-hard, but propose two greedy algorithms for minimizing the reciprocal of\n$C(S)$ with provable guarantees using the monotoncity and supermodularity. The\nfirst is a deterministic algorithm with an approximation factor\n$(1-\\frac{k}{k-1}\\cdot\\frac{1}{e})$ and cubic running time; while the second is\na randomized algorithm with a\n$(1-\\frac{k}{k-1}\\cdot\\frac{1}{e}-\\epsilon)$-approximation and nearly-linear\nrunning time for any $\\epsilon > 0$. Extensive experiments on model and real\nnetworks demonstrate that our algorithms are effective and efficient, with the\nsecond algorithm being scalable to massive networks with more than a million\nvertices.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:30:40 GMT"}, {"version": "v2", "created": "Sun, 11 Feb 2018 19:30:05 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Li", "Huan", ""], ["Peng", "Richard", ""], ["Shan", "Liren", ""], ["Yi", "Yuhao", ""], ["Zhang", "Zhongzhi", ""]]}, {"id": "1802.02562", "submitter": "David Garc\\'ia-Soriano", "authors": "David Garc\\'ia-Soriano and Francesco Bonchi", "title": "Fair-by-design matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching algorithms are used routinely to match donors to recipients for\nsolid organs transplantation, for the assignment of medical residents to\nhospitals, record linkage in databases, scheduling jobs on machines, network\nswitching, online advertising, and image recognition, among others. Although\nmany optimal solutions may exist to a given matching problem, when the elements\nthat shall or not be included in a solution correspond to individuals, it\nbecomes of paramount importance that the solution be selected fairly. In this\npaper we study individual fairness in matching problems. Given that many\nmaximum matchings may exist, each one satisfying a different set of\nindividuals, the only way to guarantee fairness is through randomization. Hence\nwe introduce the distributional maxmin fairness framework which provides, for\nany given input instance, the strongest guarantee possible simultaneously for\nall individuals in terms of satisfaction probability (the probability of being\nmatched in the solution). Specifically, a probability distribution over\nfeasible solutions is maxmin-fair if it is not possible to improve the\nsatisfaction probability of any individual without decreasing it for some other\nindividual which is no better off. In the special case of matchings in\nbipartite graphs, our framework is equivalent to the egalitarian mechanism of\nBogomolnaia and Mouline. Our main contribution is a polynomial-time algorithm\nfor fair matching building on techniques from minimum cuts, and edge-coloring\nalgorithms for regular bipartite graphs, and transversal theory. For bipartite\ngraphs, our algorithm runs in $O((|V|^2 + |E||V|^{2/3}) \\cdot (\\log |V|)^2)$\nexpected time and scales to graphs with tens of millions of vertices and\nhundreds of millions of edges. To the best of our knowledge, this provides the\nfirst large-scale implementation of the egalitarian mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:44:43 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 11:09:47 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Garc\u00eda-Soriano", "David", ""], ["Bonchi", "Francesco", ""]]}, {"id": "1802.02638", "submitter": "Jonathan Ullman", "authors": "Jonathan Ullman", "title": "Tight Lower Bounds for Locally Differentially Private Selection", "comments": "The results in this paper have been subsumed by: Alexander Edmonds,\n  Aleksandar Nikolov, Jonathan Ullman. \"The Power of Factorization Mechanisms\n  in Local and Central Differential Privacy.\" STOC 2020 [arXiv:1911.08339].\n  Please cite that paper for the relevant results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a tight lower bound (up to constant factors) on the sample\ncomplexity of any non-interactive local differentially private protocol for\noptimizing a linear function over the simplex. This lower bound also implies a\ntight lower bound (again, up to constant factors) on the sample complexity of\nany non-interactive local differentially private protocol implementing the\nexponential mechanism. These results reveal that any local protocol for these\nproblems has exponentially worse dependence on the dimension than corresponding\nalgorithms in the central model. Previously, Kasiviswanathan et al. (FOCS 2008)\nproved an exponential separation between local and central model algorithms for\nPAC learning the class of parity functions. In contrast, our lower bound are\nquantitatively tight, apply to a simple and natural class of linear\noptimization problems, and our techniques are arguably simpler.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 21:17:53 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 13:55:37 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ullman", "Jonathan", ""]]}, {"id": "1802.02744", "submitter": "Anthony Kim", "authors": "Abhimanyu Das, Sreenivas Gollapudi, Anthony Kim, Debmalya Panigrahi,\n  Chaitanya Swamy", "title": "Minimizing Latency in Online Ride and Delivery Services", "comments": "A short version of the paper is to appear at the 27th Web Conference\n  (formerly, World Wide Web Conference), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the popularity of online ride and delivery services, we study\nnatural variants of classical multi-vehicle minimum latency problems where the\nobjective is to route a set of vehicles located at depots to serve request\nlocated on a metric space so as to minimize the total latency. In this paper,\nwe consider point-to-point requests that come with source-destination pairs and\nrelease-time constraints that restrict when each request can be served. The\npoint-to-point requests and release-time constraints model taxi rides and\ndeliveries. For all the variants considered, we show constant-factor\napproximation algorithms based on a linear programming framework. To the best\nof our knowledge, these are the first set of results for the aforementioned\nvariants of the minimum latency problems. Furthermore, we provide an empirical\nstudy of heuristics based on our theoretical algorithms on a real data set of\ntaxi rides.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 08:23:39 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Das", "Abhimanyu", ""], ["Gollapudi", "Sreenivas", ""], ["Kim", "Anthony", ""], ["Panigrahi", "Debmalya", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1802.02819", "submitter": "Maurice Chandoo", "authors": "Maurice Chandoo", "title": "A Complexity Theory for Labeling Schemes", "comments": "51 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a labeling scheme the vertices of a given graph from a particular class\nare assigned short labels such that adjacency can be algorithmically determined\nfrom these labels. A representation of a graph from that class is given by the\nset of its vertex labels. Due to the shortness constraint on the labels such\nschemes provide space-efficient representations for various graph classes, such\nas planar or interval graphs. We consider what graph classes cannot be\nrepresented by labeling schemes when the algorithm which determines adjacency\nis subjected to computational constraints.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 12:18:52 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Chandoo", "Maurice", ""]]}, {"id": "1802.03160", "submitter": "Michal Dory", "authors": "Keren Censor-Hillel, Michal Dory", "title": "Distributed Spanner Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the fundamental network design problem of constructing approximate\nminimum spanners. Our contributions are for the distributed setting, providing\nboth algorithmic and hardness results.\n  Our main hardness result shows that an $\\alpha$-approximation for the minimum\ndirected $k$-spanner problem for $k \\geq 5$ requires $\\Omega(n\n/\\sqrt{\\alpha}\\log{n})$ rounds using deterministic algorithms or\n$\\Omega(\\sqrt{n }/\\sqrt{\\alpha}\\log{n})$ rounds using randomized ones, in the\nCONGEST model of distributed computing. Combined with the constant-round\n$O(n^{\\epsilon})$-approximation algorithm in the LOCAL model of [Barenboim,\nElkin and Gavoille, 2016], as well as a polylog-round\n$(1+\\epsilon)$-approximation algorithm in the LOCAL model that we show here,\nour lower bounds for the CONGEST model imply a strict separation between the\nLOCAL and CONGEST models. Notably, to the best of our knowledge, this is the\nfirst separation between these models for a local approximation problem.\n  Similarly, a separation between the directed and undirected cases is implied.\nWe also prove a nearly-linear lower bound for the minimum weighted $k$-spanner\nproblem for $k \\geq 4$, and we show lower bounds for the weighted 2-spanner\nproblem.\n  On the algorithmic side, apart from the aforementioned\n$(1+\\epsilon)$-approximation algorithm for minimum $k$-spanners, our main\ncontribution is a new distributed construction of minimum 2-spanners that uses\nonly polynomial local computations. Our algorithm has a guaranteed\napproximation ratio of $O(\\log(m/n))$ for a graph with $n$ vertices and $m$\nedges, which matches the best known ratio for polynomial time sequential\nalgorithms [Kortsarz and Peleg, 1994], and is tight if we restrict ourselves to\npolynomial local computations. Our approach allows us to extend our algorithm\nto work also for the directed, weighted, and client-server variants of the\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 08:01:28 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Dory", "Michal", ""]]}, {"id": "1802.03235", "submitter": "Kenjiro Takazawa", "authors": "Kenjiro Takazawa", "title": "The $b$-bibranching Problem: TDI System, Packing, and Discrete Convexity", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the $b$-bibranching problem in digraphs, which is\na common generalization of the bibranching and $b$-branching problems. The\nbibranching problem, introduced by Schrijver (1982), is a common generalization\nof the branching and bipartite edge cover problems. Previous results on\nbibranchings include polynomial algorithms, a linear programming formulation\nwith total dual integrality, a packing theorem, and an M-convex submodular flow\nformulation. The $b$-branching problem, recently introduced by Kakimura,\nKamiyama, and Takazawa (2018), is a generalization of the branching problem\nadmitting higher indegree, i.e., each vertex $v$ can have indegree at most\n$b(v)$. For $b$-branchings, a combinatorial algorithm, a linear programming\nformulation with total dual integrality, and a packing theorem for branchings\nare extended. A main contribution of this paper is to extend those previous\nresults on bibranchings and $b$-branchings to $b$-bibranchings. That is, we\npresent a linear programming formulation with total dual integrality, a packing\ntheorem, and an M-convex submodular flow formulation for $b$-bibranchings. In\nparticular, the linear program and M-convex submodular flow formulations\nrespectively imply polynomial algorithms for finding a shortest\n$b$-bibranching.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 12:51:20 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Takazawa", "Kenjiro", ""]]}, {"id": "1802.03297", "submitter": "Moritz von Looz-Corswarem", "authors": "Moritz von Looz and Henning Meyerhenke", "title": "Updating Dynamic Random Hyperbolic Graphs in Sublinear Time", "comments": "arXiv admin note: text overlap with arXiv:1509.01990", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative network models play an important role in algorithm development,\nscaling studies, network analysis, and realistic system benchmarks for graph\ndata sets. A complex network model gaining considerable popularity builds\nrandom hyperbolic graphs, generated by distributing points within a disk in the\nhyperbolic plane and then adding edges between points with a probability\ndepending on their hyperbolic distance.\n  We present a dynamic extension to model gradual network change, while\npreserving at each step the point position probabilities. To process the\ndynamic changes efficiently, we formalize the concept of a probabilistic\nneighborhood: Let $P$ be a set of $n$ points in Euclidean or hyperbolic space,\n$q$ a query point, $\\operatorname{dist}$ a distance metric, and $f :\n\\mathbb{R}^+ \\rightarrow [0,1]$ a monotonically decreasing function. Then, the\nprobabilistic neighborhood $N(q, f)$ of $q$ with respect to $f$ is a random\nsubset of $P$ and each point $p \\in P$ belongs to $N(q,f)$ with probability\n$f(\\operatorname{dist}(p,q))$. We present a fast, sublinear-time query\nalgorithm to sample probabilistic neighborhoods from planar point sets. For\ncertain distributions of planar $P$, we prove that our algorithm answers a\nquery in $O((|N(q,f)| + \\sqrt{n})\\log n)$ time with high probability. This\nenables us to process a node movement in random hyperbolic graphs in sublinear\ntime, resulting in a speedup of about one order of magnitude in practice\ncompared to the fastest previous approach. Apart from that, our query algorithm\nis also applicable to Euclidean geometry, making it of independent interest for\nother sampling or probabilistic spreading scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 16:41:57 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["von Looz", "Moritz", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1802.03587", "submitter": "Sebastian Schlag", "authors": "Tobias Heuer, Peter Sanders, Sebastian Schlag", "title": "Network Flow-Based Refinement for Multilevel Hypergraph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a refinement framework for multilevel hypergraph partitioning that\nuses max-flow computations on pairs of blocks to improve the solution quality\nof a $k$-way partition. The framework generalizes the flow-based improvement\nalgorithm of KaFFPa from graphs to hypergraphs and is integrated into the\nhypergraph partitioner KaHyPar. By reducing the size of hypergraph flow\nnetworks, improving the flow model used in KaFFPa, and developing techniques to\nimprove the running time of our algorithm, we obtain a partitioner that\ncomputes the best solutions for a wide range of benchmark hypergraphs from\ndifferent application areas while still having a running time comparable to\nthat of hMetis.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 13:22:29 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 11:39:54 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Heuer", "Tobias", ""], ["Sanders", "Peter", ""], ["Schlag", "Sebastian", ""]]}, {"id": "1802.03611", "submitter": "Anatoly Plotnikov", "authors": "Anatoly D. Plotnikov", "title": "Searching isomorphic graphs", "comments": "17 pages, 11 figures", "journal-ref": "Transactions on Networks and Communications, Volume 5, No. 5,\n  ISSN: 2054 -7420 (2017)", "doi": "10.14738/tnc.55.3551", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To determine that two given undirected graphs are isomorphic, we construct\nfor them auxiliary graphs, using the breadth-first search. This makes\ncapability to position vertices in each digraph with respect to each other. If\nthe given graphs are isomorphic, in each of them we can find such positionally\nequivalent auxiliary digraphs that have the same mutual positioning of\nvertices. Obviously, if the given graphs are isomorphic, then such equivalent\ndigraphs exist. Proceeding from the arrangement of vertices in one of the\ndigraphs, we try to determine the corresponding vertices in another digraph. As\na result we develop the algorithm for constructing a bijective mapping between\nvertices of the given graphs if they are isomorphic. The running time of the\nalgorithm equal to $O(n^5)$, where $n$ is the number of graph vertices.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 15:51:52 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Plotnikov", "Anatoly D.", ""]]}, {"id": "1802.03634", "submitter": "Subrahmanyam Kalyanasundaram", "authors": "Sriram Bhyravarapu, Saurabh Joshi, Subrahmanyam Kalyanasundaram,\n  Anjeneya Swami Kare", "title": "On the Tractability of (k,i)-Coloring", "comments": "This is the full version of the paper, whose preliminary version was\n  published in the CALDAM 2018 conference", "journal-ref": null, "doi": "10.1016/j.dam.2020.08.018", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an undirected graph, a proper (k,i)-coloring is an assignment of a set of\nk colors to each vertex such that any two adjacent vertices have at most i\ncommon colors. The (k,i)-coloring problem is to compute the minimum number of\ncolors required for a proper (k,i)-coloring. This is a generalization of the\nclassic graph coloring problem. We show a parameterized algorithm for the\n(k,i)-coloring problem with the size of the feedback vertex set as a parameter.\nOur algorithm does not use tree-width machinery, thus answering a question of\nMajumdar, Neogi, Raman and Tale [CALDAM 2017]. We also give a faster and\nsimpler exact algorithm for (k, k-1)-coloring. From the hardness perspective,\nwe show that the (k,i)-coloring problem is NP-complete for any fixed values i,\nk, whenever i<k, thereby settling a conjecture of Mendez-Diaz and Zabala [1999]\nand again asked by Majumdar, Neogi, Raman and Tale. The NP-completeness result\nimproves the partial NP-completeness shown in the preliminary version of this\npaper published in CALDAM 2018.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 18:16:58 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 12:05:19 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Bhyravarapu", "Sriram", ""], ["Joshi", "Saurabh", ""], ["Kalyanasundaram", "Subrahmanyam", ""], ["Kare", "Anjeneya Swami", ""]]}, {"id": "1802.03649", "submitter": "Jakub Marecek", "authors": "Jakub Marecek, Stathis Maroulis, Vana Kalogeraki, Dimitrios Gunopulos", "title": "Low-Rank Methods in Event Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring of streamed data to detect abnormal behavior (variously known as\nevent detection, anomaly detection, change detection, or outlier detection)\nunderlies many applications of the Internet of Things. Here, we propose a novel\nframework for event detection in high-dimensional data across a variety of\nsources, with asynchronous sampling and missing data, to instantly predict\nabnormal behavior. We assume that normal observations come from a low-rank\nsubspace, prior to being corrupted by a uniformly distributed noise.\nCorrespondingly, we aim to recover a representation of the subspace and perform\nevent detection by running point-to-subspace distance queries on this subspace\nfor incoming data. In particular, we use a variant of low-rank factorisation,\nwhich considers interval uncertainty sets around \"known entries\", on a suitable\nflattening of the input data to obtain a low-rank model. On-line, we compute\nthe distance of incoming data to the low-rank \"normal\" subspace and update the\nsubspace to keep it consistent with the seasonal changes present. For the\ndistance computation, we present an algorithm with a one-sided error bounded by\na function of the number of coordinates employed. In our experimental\nevaluation, we have tested the ability of the proposed algorithm to identify\nsamples of abnormal behavior in induction-loop data from Dublin, Ireland.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 20:32:28 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 12:07:24 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Marecek", "Jakub", ""], ["Maroulis", "Stathis", ""], ["Kalogeraki", "Vana", ""], ["Gunopulos", "Dimitrios", ""]]}, {"id": "1802.03671", "submitter": "Jason Li", "authors": "Bernhard Haeupler, Jason Li", "title": "Faster Distributed Shortest Path Approximations via Shortcuts", "comments": "To appear in DISC 2018; 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long series of recent results and breakthroughs have led to faster and\nbetter distributed approximation algorithms for single source shortest paths\n(SSSP) and related problems in the CONGEST model. The runtime of all these\nalgorithms, however, is $\\tilde{\\Omega}(\\sqrt{n})$, regardless of the network\ntopology, even on nice networks with a (poly)logarithmic network diameter $D$.\nWhile this is known to be necessary for some pathological networks, most\ntopologies of interest are arguably not of this type.\n  We give the first distributed approximation algorithms for shortest paths\nproblems that adjust to the topology they are run on, thus achieving\nsignificantly faster running times on many topologies of interest. The running\ntime of our algorithms depends on and is close to $Q$, where $Q$ is the quality\nof the best shortcut that exists for the given topology. While $Q =\n\\tilde{\\Theta}(\\sqrt{n} + D)$ for pathological worst-case topologies, many\ntopologies of interest have $Q = \\tilde{\\Theta}(D)$, which results in near\ninstance optimal running times for our algorithm, given the trivial $\\Omega(D)$\nlower bound.\n  The problems we consider are as follows: (1) an approximate shortest path\ntree and SSSP distances, (2) a polylogarithmic size distance label for every\nnode such that from the labels of any two nodes alone one can determine their\ndistance (approximately), and (3) an (approximately) optimal flow for the\ntransshipment problem.\n  Our algorithms have a tunable tradeoff between running time and approximation\nratio. Our fastest algorithms have an arbitrarily good polynomial approximation\nguarantee and an essentially optimal $\\tilde{O}(Q)$ running time. On the other\nend of the spectrum, we achieve polylogarithmic approximations in $\\tilde{O}(Q\n\\cdot n^{\\epsilon})$ rounds for any $\\epsilon > 0$.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 00:50:14 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 22:17:16 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 22:05:25 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Li", "Jason", ""]]}, {"id": "1802.03700", "submitter": "Vaneet Aggarwal", "authors": "Ruijiu Mao and Vaneet Aggarwal and Mung Chiang", "title": "Stochastic Non-preemptive Co-flow Scheduling with Time-Indexed\n  Relaxation", "comments": "Some of the results have been fixed, mainly involving the CoV. The\n  changes compared to the previous version are minor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-flows model a modern scheduling setting that is commonly found in a\nvariety of applications in distributed and cloud computing. A stochastic\nco-flow task contains a set of parallel flows with randomly distributed sizes.\nFurther, many applications require non-preemptive scheduling of co-flow tasks.\nThis paper gives an approximation algorithm for stochastic non-preemptive\nco-flow scheduling. The proposed approach uses a time-indexed linear\nrelaxation, and uses its solution to come up with a feasible schedule. This\nalgorithm is shown to achieve a competitive ratio of\n$(2\\log{m}+1)(1+\\sqrt{m}\\Delta)(1+m{\\Delta}){(3+\\Delta)}/{2}$ for zero-release\ntimes, and $(2\\log{m}+1)(1+\\sqrt{m}\\Delta)(1+m\\Delta)(2+\\Delta)$ for general\nrelease times, where $\\Delta$ represents the upper bound of squared coefficient\nof variation of processing times, and $m$ is the number of servers.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 06:31:59 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 16:09:34 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Mao", "Ruijiu", ""], ["Aggarwal", "Vaneet", ""], ["Chiang", "Mung", ""]]}, {"id": "1802.03734", "submitter": "Jakub Mare\\v{c}ek", "authors": "Jonathan Epperlein, Jaroslaw Legierski, Marcin Luckner, Jakub Marecek,\n  Rahul Nair", "title": "The Use of Presence Data in Modelling Demand for Transportation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the applicability of the data from operators of cellular systems\nto modelling demand for transportation. While individual-level data may contain\nprecise paths of movement, stringent privacy rules prohibit their use without\nconsent. Presence data aggregate the individual-level data to information on\nthe numbers of transactions at each base transceiver station (BTS) per each\ntime period. Our work is aimed at demonstrating value of such aggregate data\nfor mobility management while maintaining privacy of users. In particular,\ngiven mobile subscriber activity aggregated to short time intervals for a zone,\na convex optimisation problem estimates most likely transitions between zones.\nWe demonstrate the method on presence data from Warsaw, Poland, and compare\nwith official demand estimates obtained with classical econometric methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 12:39:24 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Epperlein", "Jonathan", ""], ["Legierski", "Jaroslaw", ""], ["Luckner", "Marcin", ""], ["Marecek", "Jakub", ""], ["Nair", "Rahul", ""]]}, {"id": "1802.03748", "submitter": "Berry Schoenmakers", "authors": "Berry Schoenmakers", "title": "Binary Pebbling Algorithms for In-Place Reversal of One-Way Hash Chains", "comments": "4 figures, 1 table; abridged version of Financial Crypto 2016 paper\n  for NAW special issue on Cryptology", "journal-ref": "Nieuw Archief voor Wiskunde, series 5, volume 18, number 3, pages\n  199-204, September 2017", "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present optimal binary pebbling algorithms for in-place reversal (backward\ntraversal) of one-way hash chains. For a hash chain of length $2^k$, the number\nof hashes performed in each output round does not exceed $\\lceil k/2 \\rceil$,\nwhereas the number of hash values stored (pebbles) throughout is at most $k$.\n  We introduce a framework for rigorous comparison of explicit binary pebbling\nalgorithms, including simple speed-1 binary pebbling, Jakobsson's speed-2\nbinary pebbling, and our optimal binary pebbling algorithm. Explicit schedules\ndescribe for each pebble exactly how many hashes need to be performed in each\nround. The optimal schedule turns out to be essentially unique and exhibits a\nnice recursive structure, which allows for fully optimized implementations that\ncan readily be deployed. In particular, we develop the first in-place\nimplementations with minimal storage overhead (essentially, storing only hash\nvalues), and fast implementations with minimal computational overhead.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 14:58:38 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Schoenmakers", "Berry", ""]]}, {"id": "1802.03844", "submitter": "Pankaj Khanchandani", "authors": "Pankaj Khanchandani and Roger Wattenhofer", "title": "Reducing Compare-and-Swap to Consensus Number One Primitives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consensus number of an object is the maximum number of processes among\nwhich binary consensus can be solved using any number of instances of the\nobject and read-write registers. Herlihy [6] showed in his seminal work that if\nan object has a consensus number of n, then there is a universal construction\nfor a wait-free and linearizable implementation of any non-trivial concurrent\nobject or data structure that is shared among n processes. Thus, a\nsynchronization object such as compare-and-swap with an infinite consensus\nnumber and the corresponding instruction can be viewed as \"strong\". On the\nother hand, a synchronization object such as fetch-and-add with consensus\nnumber two and the corresponding fetch-and-add instruction can be viewed as\n\"weak\".\n  Ellen et al. [2] observed recently that an object supporting two weak\ninstructions can also achieve infinite consensus number like an object that\nsupports one strong instruction. Using Herlihy's universal construction, this\nimplies that ignoring concerns about efficiency, one can design any concurrent\ndata structure or algorithm using only weak instructions. However, is it\npossible that a combination of weak instructions is really powerful enough to\nefficiently replace a strong instruction, like compare-and-swap, without\nincurring a large overhead in time or space? In this paper, we answer this\nquestion by giving an O(1) time wait-free and linearizable implementation of a\ncompare-and-swap register shared among n processes using read-write registers\nand O(1) registers that support two synchronization primitives half-max and\nmax-write, each having consensus number one. Thus, any algorithm that solves\nsome arbitrary synchronization problem using read-write and compare-and-swap\nregisters can be transformed into an algorithm that has the same asymptotic\ntime complexity and only uses consensus number one instructions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 00:00:06 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 19:08:36 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2018 10:01:20 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Khanchandani", "Pankaj", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1802.03866", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of minimizing sum-of-nonconvex functions (i.e., convex functions\nthat are average of non-convex ones) is becoming increasingly important in\nmachine learning, and is the core machinery for PCA, SVD, regularized Newton's\nmethod, accelerated non-convex optimization, and more.\n  We show how to provably obtain an accelerated stochastic algorithm for\nminimizing sum-of-nonconvex functions, by $\\textit{adding one additional line}$\nto the well-known SVRG method. This line corresponds to momentum, and shows how\nto directly apply momentum to the finite-sum stochastic minimization of\nsum-of-nonconvex functions. As a side result, our method enjoys linear parallel\nspeed-up using mini-batch.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 02:49:23 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1802.03905", "submitter": "Xiaowei Wu", "authors": "Zhiyi Huang, Ning Kang, Zhihao Gavin Tang, Xiaowei Wu, Yuhao Zhang and\n  Xue Zhu", "title": "How to Match when All Vertices Arrive Online", "comments": "25 pages, 10 figures, to appear in STOC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a fully online model of maximum cardinality matching in which\nall vertices arrive online. On the arrival of a vertex, its incident edges to\npreviously-arrived vertices are revealed. Each vertex has a deadline that is\nafter all its neighbors' arrivals. If a vertex remains unmatched until its\ndeadline, the algorithm must then irrevocably either match it to an unmatched\nneighbor, or leave it unmatched. The model generalizes the existing one-sided\nonline model and is motivated by applications including ride-sharing platforms,\nreal-estate agency, etc.\n  We show that the Ranking algorithm by Karp et al. (STOC 1990) is\n$0.5211$-competitive in our fully online model for general graphs. Our analysis\nbrings a novel charging mechanic into the randomized primal dual technique by\nDevanur et al. (SODA 2013), allowing a vertex other than the two endpoints of a\nmatched edge to share the gain. To our knowledge, this is the first analysis of\nRanking that beats $0.5$ on general graphs in an online matching problem, a\nfirst step towards solving the open problem by Karp et al. (STOC 1990) about\nthe optimality of Ranking on general graphs. If the graph is bipartite, we show\nthat the competitive ratio of Ranking is between $0.5541$ and $0.5671$.\nFinally, we prove that the fully online model is strictly harder than the\nprevious model as no online algorithm can be $0.6317 <\n1-\\frac{1}{e}$-competitive in our model even for bipartite graphs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 06:31:58 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Huang", "Zhiyi", ""], ["Kang", "Ning", ""], ["Tang", "Zhihao Gavin", ""], ["Wu", "Xiaowei", ""], ["Zhang", "Yuhao", ""], ["Zhu", "Xue", ""]]}, {"id": "1802.03914", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "BagMinHash - Minwise Hashing Algorithm for Weighted Sets", "comments": "10 pages, KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3220089", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing has become a standard tool to calculate signatures which\nallow direct estimation of Jaccard similarities. While very efficient\nalgorithms already exist for the unweighted case, the calculation of signatures\nfor weighted sets is still a time consuming task. BagMinHash is a new algorithm\nthat can be orders of magnitude faster than current state of the art without\nany particular restrictions or assumptions on weights or data dimensionality.\nApplied to the special case of unweighted sets, it represents the first\nefficient algorithm producing independent signature components. A series of\ntests finally verifies the new algorithm and also reveals limitations of other\napproaches published in the recent past.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 07:07:56 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 16:40:08 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "1802.03920", "submitter": "Ishay Haviv", "authors": "Ishay Haviv", "title": "On Minrank and the Lov\\'asz Theta Function", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two classical upper bounds on the Shannon capacity of graphs are the\n$\\vartheta$-function due to Lov\\'asz and the minrank parameter due to Haemers.\nWe provide several explicit constructions of $n$-vertex graphs with a constant\n$\\vartheta$-function and minrank at least $n^\\delta$ for a constant $\\delta>0$\n(over various prime order fields). This implies a limitation on the\n$\\vartheta$-function-based algorithmic approach to approximating the minrank\nparameter of graphs. The proofs involve linear spaces of multivariate\npolynomials and the method of higher incidence matrices.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 07:30:15 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 08:25:50 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Haviv", "Ishay", ""]]}, {"id": "1802.03928", "submitter": "Istv\\'an M\\'odos", "authors": "Istv\\'an M\\'odos, P\\v{r}emysl \\v{S}\\r{u}cha, Zden\\v{e}k Hanz\\'alek", "title": "Algorithms for robust production scheduling with energy consumption\n  limits", "comments": null, "journal-ref": "I. Modos, P. Sucha, and Z. Hanzalek. Algorithms for robust\n  production scheduling with energy consumption limits. Computers & Industrial\n  Engineering, 112:391 - 408, 2017", "doi": "10.1016/j.cie.2017.08.011", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we consider a scheduling problem faced by production companies\nwith large electricity consumption. Due to the contract with the electric\nutility, the production companies are obligated to comply with the total energy\nconsumption limits in the specified time intervals (usually 15-minutes long),\notherwise, the companies pay substantial penalty fees. Although it is possible\nto design production schedules that consider these limits as hard constraints,\nuncertainties occurring during the execution of the schedules are usually not\ntaken into account. This may lead to situations in which the unexpected delays\nof the operations cause the violations of the energy consumption limits. Our\ngoal is to design robust production schedules pro-actively guaranteeing that\nthe energy consumption limits are not violated for the given set of uncertainty\nscenarios. We consider scheduling on one machine with release times of the\noperations and total tardiness as the objective function. To tackle this\nproblem, we first propose a pseudo-polynomial algorithm for finding the optimal\nrobust schedule for the given permutation of the operations. This algorithm is\nthen utilised in three different algorithms for finding the optimal\npermutation: two exact (Branch-and-Bound and logic-based Benders decomposition)\nand one heuristic algorithm (tabu search). All the algorithms were\nexperimentally evaluated on random instances with different sizes of the\nuncertainty scenarios set. Using the tabu search algorithm, we are able to\nsolve large instances within one minute.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 08:22:09 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["M\u00f3dos", "Istv\u00e1n", ""], ["\u0160\u016fcha", "P\u0159emysl", ""], ["Hanz\u00e1lek", "Zden\u011bk", ""]]}, {"id": "1802.04249", "submitter": "Kijung Shin", "authors": "Kijung Shin, Euiwoong Lee, Jinoh Oh, Mohammad Hammoud, Christos\n  Faloutsos", "title": "CoCoS: Fast and Accurate Distributed Triangle Counting in Graph Streams", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph stream, how can we estimate the number of triangles in it using\nmultiple machines with limited storage? Specifically, how should edges be\nprocessed and sampled across the machines for rapid and accurate estimation?\n  The count of triangles (i.e., cliques of size three) has proven useful in\nnumerous applications, including anomaly detection, community detection, and\nlink recommendation. For triangle counting in large and dynamic graphs, recent\nwork has focused largely on streaming algorithms and distributed algorithms but\nlittle on their combinations for \"the best of both worlds\".\n  In this work, we propose CoCoS, a fast and accurate distributed streaming\nalgorithm for estimating the counts of global triangles (i.e., all triangles)\nand local triangles incident to each node. Making one pass over the input\nstream, COCOS carefully processes and stores the edges across multiple machines\nso that the redundant use of computational and storage resources is minimized.\nCompared to baselines, CoCoS is (a) Accurate: giving up to 39X smaller\nestimation error, (b) Fast: up to 10.4X faster, scaling linearly with the size\nof the input stream, and (c) Theoretically sound: yielding unbiased estimates.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:57:57 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 06:28:17 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 22:05:29 GMT"}, {"version": "v4", "created": "Sun, 6 Dec 2020 07:48:15 GMT"}, {"version": "v5", "created": "Sat, 27 Feb 2021 06:23:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Shin", "Kijung", ""], ["Lee", "Euiwoong", ""], ["Oh", "Jinoh", ""], ["Hammoud", "Mohammad", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1802.04367", "submitter": "Pavel Dvurechensky", "authors": "Pavel Dvurechensky and Alexander Gasnikov and Alexey Kroshnin", "title": "Computational Optimal Transport: Complexity by Accelerated Gradient\n  Descent Is Better Than by Sinkhorn's Algorithm", "comments": "Accepted for ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze two algorithms for approximating the general optimal transport\n(OT) distance between two discrete distributions of size $n$, up to accuracy\n$\\varepsilon$. For the first algorithm, which is based on the celebrated\nSinkhorn's algorithm, we prove the complexity bound\n$\\widetilde{O}\\left({n^2/\\varepsilon^2}\\right)$ arithmetic operations. For the\nsecond one, which is based on our novel Adaptive Primal-Dual Accelerated\nGradient Descent (APDAGD) algorithm, we prove the complexity bound\n$\\widetilde{O}\\left(\\min\\left\\{n^{9/4}/\\varepsilon, n^{2}/\\varepsilon^2\n\\right\\}\\right)$ arithmetic operations. Both bounds have better dependence on\n$\\varepsilon$ than the state-of-the-art result given by\n$\\widetilde{O}\\left({n^2/\\varepsilon^3}\\right)$. Our second algorithm not only\nhas better dependence on $\\varepsilon$ in the complexity bound, but also is not\nspecific to entropic regularization and can solve the OT problem with different\nregularizers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 21:33:39 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 14:41:21 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Dvurechensky", "Pavel", ""], ["Gasnikov", "Alexander", ""], ["Kroshnin", "Alexey", ""]]}, {"id": "1802.04477", "submitter": "Zhize Li", "authors": "Zhize Li, Jian Li", "title": "A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex\n  Optimization", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze stochastic gradient algorithms for optimizing nonconvex, nonsmooth\nfinite-sum problems. In particular, the objective function is given by the\nsummation of a differentiable (possibly nonconvex) component, together with a\npossibly non-differentiable but convex component. We propose a proximal\nstochastic gradient algorithm based on variance reduction, called ProxSVRG+.\nOur main contribution lies in the analysis of ProxSVRG+. It recovers several\nexisting convergence results and improves/generalizes them (in terms of the\nnumber of stochastic gradient oracle calls and proximal oracle calls). In\nparticular, ProxSVRG+ generalizes the best results given by the SCSG algorithm,\nrecently proposed by [Lei et al., 2017] for the smooth nonconvex case.\nProxSVRG+ is also more straightforward than SCSG and yields simpler analysis.\nMoreover, ProxSVRG+ outperforms the deterministic proximal gradient descent\n(ProxGD) for a wide range of minibatch sizes, which partially solves an open\nproblem proposed in [Reddi et al., 2016b]. Also, ProxSVRG+ uses much less\nproximal oracle calls than ProxSVRG [Reddi et al., 2016b]. Moreover, for\nnonconvex functions satisfied Polyak-\\L{}ojasiewicz condition, we prove that\nProxSVRG+ achieves a global linear convergence rate without restart unlike\nProxSVRG. Thus, it can \\emph{automatically} switch to the faster linear\nconvergence in some regions as long as the objective function satisfies the PL\ncondition locally in these regions. ProxSVRG+ also improves ProxGD and\nProxSVRG/SAGA, and generalizes the results of SCSG in this case. Finally, we\nconduct several experiments and the experimental results are consistent with\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 06:34:22 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:56:56 GMT"}, {"version": "v3", "created": "Sat, 27 Oct 2018 11:31:17 GMT"}, {"version": "v4", "created": "Sat, 1 Dec 2018 20:10:29 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Li", "Zhize", ""], ["Li", "Jian", ""]]}, {"id": "1802.04555", "submitter": "Wei Chen", "authors": "Wei Chen, Ruihan Wu and Zheng Yu", "title": "Scalable Lattice Influence Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence maximization is the task of finding k seed nodes in a social\nnetwork such that the expected number of activated nodes in the network (under\ncertain influence propagation model), referred to as the influence spread, is\nmaximized. Lattice influence maximization (LIM) generalizes influence\nmaximization such that, instead of selecting k seed nodes, one selects a vector\nx = (x_1, ..., x_d) from a discrete space X called a lattice, where x_j\ncorresponds to the j-th marketing strategy and x represents a marketing\nstrategy mix. Each strategy mix x has probability h_u(x) to activate a node u\nas a seed.LIM is the task of finding a strategy mix under the constraint\nx_1+...+x_d <= k such that its influence spread is maximized. We adapt the\nreverse influence sampling (RIS) approach and design scalable algorithms for\nLIM. We first design the IMM-PRR algorithm based on partial reverse-reachable\nsets as a general solution for LIM, and improve IMM-PRR for a large family of\nmodels where each strategy independently activates seed nodes. We then propose\nan alternative algorithm IMM-VSN based on virtual strategy nodes, for the\nfamily of models with independent strategy activations. We prove that both\nIMM-PRR and IMM-VSN guarantees 1-e-\\epsilon approximation for small \\epsilon>\n0. Empirically, through extensive tests we demonstrate that IMM-VSN runs faster\nthan IMM-PRR and much faster than other baseline algorithms while providing the\nsame level of influence spread. We conclude that IMM-VSN is the best one for\nmodels with independent strategy activations, while IMM-PRR works for general\nmodes without this assumption. Finally, we extend LIM to the partitioned budget\ncase where strategies are partitioned into groups, each of which has a separate\nbudget, and show that a minor variation of our algorithms would achieve 1/2\n-\\epsilon approximation ratio with the same time complexity.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 10:53:59 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 00:40:23 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Chen", "Wei", ""], ["Wu", "Ruihan", ""], ["Yu", "Zheng", ""]]}, {"id": "1802.04632", "submitter": "Eric Rivals", "authors": "Bastien Cazaux, Eric Rivals", "title": "Hierarchical Overlap Graph", "comments": "11 pages, 3 figures, appendix, 6 graphics, 2 algorithms, 13\n  references", "journal-ref": null, "doi": null, "report-no": "lirmm-01674319", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of finite words, the Overlap Graph (OG) is a complete weighted\ndigraph where each word is a node and where the weight of an arc equals the\nlength of the longest overlap of one word onto the other (Overlap is an\nasymmetric notion). The OG serves to assemble DNA fragments or to compute\nshortest superstrings which are a compressed representation of the input. The\nOG requires a space is quadratic in the number of words, which limits its\nscalability. The Hierarchical Overlap Graph (HOG) is an alternative graph that\nalso encodes all maximal overlaps, but uses a space that is linear in the sum\nof the lengths of the input words. We propose the first algorithm to build the\nHOG in linear space for words of equal length.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 14:20:19 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 13:21:49 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Cazaux", "Bastien", ""], ["Rivals", "Eric", ""]]}, {"id": "1802.04659", "submitter": "Daniel Neuen", "authors": "Martin Grohe, Daniel Neuen, Pascal Schweitzer", "title": "A Faster Isomorphism Test for Graphs of Small Degree", "comments": "36 pages; second version significantly improves on the results and\n  gives a faster isomorphism test for all graphs of maximum degree d rather\n  than just graphs of maximum degree d and logarithmic diameter; third version\n  adds additional explanations and corrects several typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent breakthrough, Babai (STOC 2016) gave a quasipolynomial time graph\nisomorphism test. In this work, we give an improved isomorphism test for graphs\nof small degree: our algorithms runs in time $n^{O((\\log d)^{c})}$, where $n$\nis the number of vertices of the input graphs, $d$ is the maximum degree of the\ninput graphs, and $c$ is an absolute constant. The best previous isomorphism\ntest for graphs of maximum degree $d$ due to Babai, Kantor and Luks (FOCS 1983)\nruns in time $n^{O(d/ \\log d)}$.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 14:58:34 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 14:34:17 GMT"}, {"version": "v3", "created": "Sat, 30 Nov 2019 21:00:08 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Grohe", "Martin", ""], ["Neuen", "Daniel", ""], ["Schweitzer", "Pascal", ""]]}, {"id": "1802.04705", "submitter": "Ziteng Sun", "authors": "Jayadev Acharya, Ziteng Sun, Huanyu Zhang", "title": "Hadamard Response: Estimating Distributions Privately, Efficiently, and\n  with Little Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating $k$-ary distributions under\n$\\varepsilon$-local differential privacy. $n$ samples are distributed across\nusers who send privatized versions of their sample to a central server. All\npreviously known sample optimal algorithms require linear (in $k$)\ncommunication from each user in the high privacy regime $(\\varepsilon=O(1))$,\nand run in time that grows as $n\\cdot k$, which can be prohibitive for large\ndomain size $k$.\n  We propose Hadamard Response (HR}, a local privatization scheme that requires\nno shared randomness and is symmetric with respect to the users. Our scheme has\norder optimal sample complexity for all $\\varepsilon$, a communication of at\nmost $\\log k+2$ bits per user, and nearly linear running time of $\\tilde{O}(n +\nk)$.\n  Our encoding and decoding are based on Hadamard matrices, and are simple to\nimplement. The statistical performance relies on the coding theoretic aspects\nof Hadamard matrices, ie, the large Hamming distance between the rows. An\nefficient implementation of the algorithm using the Fast Walsh-Hadamard\ntransform gives the computational gains.\n  We compare our approach with Randomized Response (RR), RAPPOR, and\nsubset-selection mechanisms (SS), both theoretically, and experimentally. For\n$k=10000$, our algorithm runs about 100x faster than SS, and RAPPOR.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:20:56 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 18:03:49 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Acharya", "Jayadev", ""], ["Sun", "Ziteng", ""], ["Zhang", "Huanyu", ""]]}, {"id": "1802.04789", "submitter": "Keren Censor-Hillel", "authors": "Keren Censor-Hillel, Dean Leitersdorf, Elia Turner", "title": "Sparse Matrix Multiplication and Triangle Listing in the Congested\n  Clique Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We multiply two $n \\times n$ matrices $S,T$ over semirings in the Congested\nClique model, where $n$ fully connected nodes communicate synchronously using\n$O(\\log n)$-bit messages, within $O(nz(S)^{1/3} nz(T)^{1/3}/n + 1)$ rounds of\ncommunication, where $nz(A)$ denotes the number of non-zero elements in a\nmatrix $A$. By leveraging the sparsity of the input matrices, our algorithm\ngreatly reduces communication compared with general algorithms [Censor-Hillel\net al., PODC 2015], improving upon the state-of-the-art for matrices with\n$o(n^2)$ non-zero elements. Our algorithm exhibits the additional strength of\nsurpassing previous solutions also when only one matrix is sparse. This allows\nefficiently raising a sparse matrix to a power greater than 2. As applications,\nwe speed up 4-cycle counting and APSP in sparse graphs.\n  Our algorithmic contribution is a new \\emph{deterministic} method of\nrestructuring the input matrices in a sparsity-aware manner, which assigns each\nnode with element-wise multiplication tasks that are not necessarily\nconsecutive but are balanced, yielding communication-efficient multiplication.\n  Moreover, this new deterministic method for restructuring matrices may be\nused to restructure the adjacency matrix of input graphs, enabling faster\nsolutions for graph related problems. As an example, we present a new\ndeterministic algorithm which solves the triangle listing problem in\n$O(m/n^{5/3} + 1)$ rounds, a complexity that was previously obtained by a\n\\emph{randomized} algorithm [Pandurangan et al., SPAA 2018] and matches the\nlower bound of $\\tilde{\\Omega}(n^{1/3})$ when $m=n^2$ of [Izumi and Le Gall,\nPODC 2017, Pandurangan et al., SPAA 2018].\n  Our triangle listing algorithm implies triangle counting with the same\ncomplexity of $O(m/n^{5/3} + 1)$ rounds, which is a \\emph{cubic} improvement\nover the previous $O(m^2/n^3)$-round algorithm [Dolev et al., DISC 2012].\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 18:47:15 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 06:02:53 GMT"}, {"version": "v3", "created": "Thu, 10 May 2018 17:35:36 GMT"}, {"version": "v4", "created": "Wed, 20 Mar 2019 19:05:40 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Leitersdorf", "Dean", ""], ["Turner", "Elia", ""]]}, {"id": "1802.04883", "submitter": "Yifeng Gao", "authors": "Yifeng Gao and Jessica Lin", "title": "Efficient Discovery of Variable-length Time Series Motifs with Large\n  Length Range in Million Scale Time Series", "comments": "Support Page: https://sites.google.com/site/himeicdm/", "journal-ref": "ICDM 2017", "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting repeated variable-length patterns, also called variable-length\nmotifs, has received a great amount of attention in recent years. Current\nstate-of-the-art algorithm utilizes fixed-length motif discovery algorithm as a\nsubroutine to enumerate variable-length motifs. As a result, it may take hours\nor days to execute when enumeration range is large. In this work, we introduce\nan approximate algorithm called HierarchIcal based Motif Enumeration (HIME) to\ndetect variable-length motifs with a large enumeration range in million-scale\ntime series. We show in the experiments that the scalability of the proposed\nalgorithm is significantly better than that of the state-of-the-art algorithm.\nMoreover, the motif length range detected by HIME is considerably larger than\nprevious sequence-matching based approximate variable-length motif discovery\napproach. We demonstrate that HIME can efficiently detect meaningful\nvariable-length motifs in long, real world time series.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:34:38 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Gao", "Yifeng", ""], ["Lin", "Jessica", ""]]}, {"id": "1802.05134", "submitter": "Kamil Khadiev", "authors": "Kamil Khadiev, Aliya Khadieva, Mansur Ziatdinov, Dmitry Kravchenko,\n  Alexander Rivosh, Ramis Yamilov and Ilnaz Mannapov", "title": "Quantum versus Classical Online Streaming Algorithms with Advice", "comments": "arXiv admin note: substantial text overlap with arXiv:1710.09595", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online algorithms with respect to the competitive ratio. Here, we\ninvestigate quantum and classical one-way automata with non-constant size of\nmemory (streaming algorithms) as a model for online algorithms. We construct\nproblems that can be solved by quantum online streaming algorithms better than\nby classical ones in a case of logarithmic or sublogarithmic size of memory,\neven if classical online algorithms get advice bits. Furthermore, we show that\na quantum online algorithm with a constant number of qubits can be better than\nany deterministic online algorithm with a constant number of advice bits and\nunlimited computational power.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 12:50:12 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 11:07:18 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Khadiev", "Kamil", ""], ["Khadieva", "Aliya", ""], ["Ziatdinov", "Mansur", ""], ["Kravchenko", "Dmitry", ""], ["Rivosh", "Alexander", ""], ["Yamilov", "Ramis", ""], ["Mannapov", "Ilnaz", ""]]}, {"id": "1802.05239", "submitter": "Phillip Bradford", "authors": "Phillip G. Bradford", "title": "Efficient Exact Paths For Dyck and semi-Dyck Labeled Path Reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exact path length problem is to determine if there is a path of a given\nfixed cost between two vertices. This paper focuses on the exact path problem\nfor costs $-1,0$ or $+1$ between all pairs of vertices in an edge-weighted\ndigraph. The edge weights are from $\\{ -1, +1 \\}$. In this case, this paper\ngives an $\\widetilde{O}(n^{\\omega})$ exact path solution. Here $\\omega$ is the\nbest exponent for matrix multiplication and $\\widetilde{O}$ is the asymptotic\nupper-bound mod polylog factors.\n  Variations of this algorithm determine which pairs of digraph nodes have Dyck\nor semi-Dyck labeled paths between them, assuming two parenthesis. Therefore,\ndetermining digraph reachability for Dyck or semi-Dyck labeled paths costs\n$\\widetilde{O}(n^{\\omega})$. A path label is made by concatenating all symbols\nalong the path's edges.\n  The exact path length problem has many applications. These applications\ninclude the labeled path problems given here, which in turn, also have numerous\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 17:56:22 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 21:10:25 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Bradford", "Phillip G.", ""]]}, {"id": "1802.05387", "submitter": "Vlad-Andrei Munteanu", "authors": "Vlad-Andrei Munteanu", "title": "Strongly connected components-Algorithm for finding the strongly\n  connected components of a graph", "comments": "7 pages, 5 sequences of code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed graph G (V, E) is strongly connected if and only if, for a pair of\nvertices X and Y from V, there exists a path from X to Y and a path from Y to\nX. In Computer Science, the partition of a graph in strongly connected\ncomponents is represented by the partition of all vertices from the graph, so\nthat for any two vertices, X and Y, from the same partition, there exists a\npath from X to Y and a path from Y to X and for any two vertices, U and V, from\ndifferent partition, the property is not met. The algorithm presented below is\nmeant to find the partition of a given graph in strongly connected components\nin O (numberOfNodes + numberOfEdges * log* (numberOfNodes)), where log*\nfunction stands for iterated logarithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 02:10:39 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Munteanu", "Vlad-Andrei", ""]]}, {"id": "1802.05399", "submitter": "Thodoris Lykouris", "authors": "Thodoris Lykouris, Sergei Vassilvitskii", "title": "Competitive caching with machine learned advice", "comments": "Preliminary versions appeared in ICML 18 and SysML 18. The current\n  version improves the presentation of the suggested framework (Section 2.2),\n  provides a more clear discussion on how it can be more broadly applied, and\n  fixes some more minor presentation issues in other sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional online algorithms encapsulate decision making under uncertainty,\nand give ways to hedge against all possible future events, while guaranteeing a\nnearly optimal solution as compared to an offline optimum. On the other hand,\nmachine learning algorithms are in the business of extrapolating patterns found\nin the data to predict the future, and usually come with strong guarantees on\nthe expected generalization error.\n  In this work we develop a framework for augmenting online algorithms with a\nmachine learned oracle to achieve competitive ratios that provably improve upon\nunconditional worst case lower bounds when the oracle has low error. Our\napproach treats the oracle as a complete black box, and is not dependent on its\ninner workings, or the exact distribution of its errors.\n  We apply this framework to the traditional caching problem -- creating an\neviction strategy for a cache of size $k$. We demonstrate that naively\nfollowing the oracle's recommendations may lead to very poor performance, even\nwhen the average error is quite low. Instead we show how to modify the Marker\nalgorithm to take into account the oracle's predictions, and prove that this\ncombined approach achieves a competitive ratio that both (i) decreases as the\noracle's error decreases, and (ii) is always capped by $O(\\log k)$, which can\nbe achieved without any oracle input. We complement our results with an\nempirical evaluation of our algorithm on real world datasets, and show that it\nperforms well empirically even using simple off-the-shelf predictions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 04:30:04 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 21:42:13 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 15:50:02 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 14:36:05 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Lykouris", "Thodoris", ""], ["Vassilvitskii", "Sergei", ""]]}, {"id": "1802.05471", "submitter": "L\\'aszl\\'o Kozma", "authors": "L\\'aszl\\'o Kozma, Thatchaphol Saranurak", "title": "Smooth heaps and a dual view of self-adjusting data structures", "comments": "Presented at STOC 2018, light revision, additional figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new connection between self-adjusting binary search trees (BSTs)\nand heaps, two fundamental, extensively studied, and practically relevant\nfamilies of data structures. Roughly speaking, we map an arbitrary heap\nalgorithm within a natural model, to a corresponding BST algorithm with the\nsame cost on a dual sequence of operations (i.e. the same sequence with the\nroles of time and key-space switched). This is the first general transformation\nbetween the two families of data structures.\n  There is a rich theory of dynamic optimality for BSTs (i.e. the theory of\ncompetitiveness between BST algorithms). The lack of an analogous theory for\nheaps has been noted in the literature. Through our connection, we transfer all\ninstance-specific lower bounds known for BSTs to a general model of heaps,\ninitiating a theory of dynamic optimality for heaps.\n  On the algorithmic side, we obtain a new, simple and efficient heap\nalgorithm, which we call the smooth heap. We show the smooth heap to be the\nheap-counterpart of Greedy, the BST algorithm with the strongest proven and\nconjectured properties from the literature, widely believed to be\ninstance-optimal. Assuming the optimality of Greedy, the smooth heap is also\noptimal within our model of heap algorithms. As corollaries of results known\nfor Greedy, we obtain instance-specific upper bounds for the smooth heap, with\napplications in adaptive sorting.\n  Intriguingly, the smooth heap, although derived from a non-practical BST\nalgorithm, is simple and easy to implement (e.g. it stores no auxiliary data\nbesides the keys and tree pointers). It can be seen as a variation on the\npopular pairing heap data structure, extending it with a \"power-of-two-choices\"\ntype of heuristic.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 10:45:34 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 22:03:58 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Kozma", "L\u00e1szl\u00f3", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "1802.05490", "submitter": "Carl Philipp Reh", "authors": "Adri\\`a Gasc\\'on, Markus Lohrey, Sebastian Maneth, Carl Philipp Reh\n  and Kurt Sieber", "title": "Grammar-based Compression of Unranked Trees", "comments": "Extended version of a paper at CSR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce forest straight-line programs (FSLPs) as a compressed\nrepresentation of unranked ordered node-labelled trees. FSLPs are based on the\noperations of forest algebra and generalize tree straight-line programs. We\ncompare the succinctness of FSLPs with two other compression schemes for\nunranked trees: top dags and tree straight-line programs of first-child/next\nsibling encodings. Efficient translations between these formalisms are\nprovided. Finally, we show that equality of unranked trees in the setting where\ncertain symbols are associative or commutative can be tested in polynomial\ntime. This generalizes previous results for testing isomorphism of compressed\nunordered ranked trees.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 11:34:51 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Gasc\u00f3n", "Adri\u00e0", ""], ["Lohrey", "Markus", ""], ["Maneth", "Sebastian", ""], ["Reh", "Carl Philipp", ""], ["Sieber", "Kurt", ""]]}, {"id": "1802.05501", "submitter": "Dariusz Dereniowski", "authors": "Dariusz Dereniowski and Dorota Osula and Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Finding small-width connected path decompositions in polynomial time", "comments": null, "journal-ref": "Theoretical Computer Science 794: 85-100 (2019)", "doi": "10.1016/j.tcs.2019.03.039", "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A connected path decomposition of a simple graph $G$ is a path decomposition\n$(X_1,\\ldots,X_l)$ such that the subgraph of $G$ induced by $X_1\\cup\\cdots\\cup\nX_i$ is connected for each $i\\in\\{1,\\ldots,l\\}$. The connected pathwidth of $G$\nis then the minimum width over all connected path decompositions of $G$. We\nprove that for each fixed $k$, the connected pathwidth of any input graph can\nbe computed in polynomial-time. This answers an open question raised by Fedor\nV. Fomin during the GRASTA 2017 workshop, since connected pathwidth is\nequivalent to the connected (monotone) node search game.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 12:09:39 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 12:43:54 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dereniowski", "Dariusz", ""], ["Osula", "Dorota", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "1802.05559", "submitter": "Peter Chini", "authors": "Peter Chini, Roland Meyer, Prakash Saivasan", "title": "Fine-Grained Complexity of Safety Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fine-grained complexity of Leader Contributor Reachability (LCR)\nand Bounded-Stage Reachability (BSR), two variants of the safety verification\nproblem for shared memory concurrent programs. For both problems, the memory is\na single variable over a finite data domain. Our contributions are new\nverification algorithms and lower bounds. The latter are based on the\nExponential Time Hypothesis (ETH), the problem Set Cover, and\ncross-compositions.\n  LCR is the question whether a designated leader thread can reach an unsafe\nstate when interacting with a certain number of equal contributor threads. We\nsuggest two parameterizations: (1) By the size of the data domain D and the\nsize of the leader L, and (2) by the size of the contributors C. We present\nalgorithms for both cases. The key techniques are compact witnesses and dynamic\nprogramming. The algorithms run in O*((L(D+1))^(LD) * D^D) and O*(2^C) time,\nshowing that both parameterizations are fixed-parameter tractable. We\ncomplement the upper bounds by (matching) lower bounds based on ETH and Set\nCover. Moreover, we prove the absence of polynomial kernels.\n  For BSR, we consider programs involving t different threads. We restrict the\nanalysis to computations where the write permission changes s times between the\nthreads. BSR asks whether a given configuration is reachable via such an\ns-stage computation. When parameterized by P, the maximum size of a thread, and\nt, the interesting observation is that the problem has a large number of\ndifficult instances. Formally, we show that there is no polynomial kernel, no\ncompression algorithm that reduces the size of the data domain D or the number\nof stages s to a polynomial dependence on P and t. This indicates that symbolic\nmethods may be harder to find for this problem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 14:26:10 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 14:55:41 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 17:02:16 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Chini", "Peter", ""], ["Meyer", "Roland", ""], ["Saivasan", "Prakash", ""]]}, {"id": "1802.05582", "submitter": "Louis Esperet", "authors": "Pierre Aboulker, Marthe Bonamy, Nicolas Bousquet, Louis Esperet", "title": "Distributed coloring in sparse graphs with fewer colors", "comments": "16 pages, 4 figures - An extended abstract of this work was presented\n  at PODC'18 (ACM Symposium on Principles of Distributed Computing)", "journal-ref": "Electronic Journal of Combinatorics 26(4) (2019), P4.20", "doi": null, "report-no": null, "categories": "math.CO cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with efficiently coloring sparse graphs in the\ndistributed setting with as few colors as possible. According to the celebrated\nFour Color Theorem, planar graphs can be colored with at most 4 colors, and the\nproof gives a (sequential) quadratic algorithm finding such a coloring. A\nnatural problem is to improve this complexity in the distributed setting. Using\nthe fact that planar graphs contain linearly many vertices of degree at most 6,\nGoldberg, Plotkin, and Shannon obtained a deterministic distributed algorithm\ncoloring $n$-vertex planar graphs with 7 colors in $O(\\log n)$ rounds. Here, we\nshow how to color planar graphs with 6 colors in $\\mbox{polylog}(n)$ rounds.\nOur algorithm indeed works more generally in the list-coloring setting and for\nsparse graphs (for such graphs we improve by at least one the number of colors\nresulting from an efficient algorithm of Barenboim and Elkin, at the expense of\na slightly worst complexity). Our bounds on the number of colors turn out to be\nquite sharp in general. Among other results, we show that no distributed\nalgorithm can color every $n$-vertex planar graph with 4 colors in $o(n)$\nrounds.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 14:49:19 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 08:38:31 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 10:46:05 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 11:57:13 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Aboulker", "Pierre", ""], ["Bonamy", "Marthe", ""], ["Bousquet", "Nicolas", ""], ["Esperet", "Louis", ""]]}, {"id": "1802.05623", "submitter": "Chung-Shou Liao", "authors": "Hao-Ting Wei, Wing-Kai Hon, Paul Horn, Chung-Shou Liao, Kunihiko\n  Sadakane", "title": "An $O(1)$-Approximation Algorithm for Dynamic Weighted Vertex Cover with\n  Soft Capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers the (soft) capacitated vertex cover problem in a dynamic\nsetting. This problem generalizes the dynamic model of the vertex cover\nproblem, which has been intensively studied in recent years. Given a\ndynamically changing vertex-weighted graph $G=(V,E)$, which allows edge\ninsertions and edge deletions, the goal is to design a data structure that\nmaintains an approximate minimum vertex cover while satisfying the capacity\nconstraint of each vertex. That is, when picking a copy of a vertex $v$ in the\ncover, the number of $v$'s incident edges covered by the copy is up to a given\ncapacity of $v$. We extend Bhattacharya et al.'s work [SODA'15 and ICALP'15] to\nobtain a deterministic primal-dual algorithm for maintaining a constant-factor\napproximate minimum capacitated vertex cover with $O(\\log n / \\epsilon)$\namortized update time, where $n$ is the number of vertices in the graph. The\nalgorithm can be extended to (1) a more general model in which each edge is\nassociated with a nonuniform and unsplittable demand, and (2) the more general\ncapacitated set cover problem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 15:36:45 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 13:59:20 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Wei", "Hao-Ting", ""], ["Hon", "Wing-Kai", ""], ["Horn", "Paul", ""], ["Liao", "Chung-Shou", ""], ["Sadakane", "Kunihiko", ""]]}, {"id": "1802.05662", "submitter": "Andrew Frohmader", "authors": "Andrew Frohmader", "title": "List Heaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple extension of the binary heap, the List Heap. We\nuse List Heaps to demonstrate the idea of adaptive heaps: heaps whose\nperformance is a function of both the size of the problem instance and the\ndisorder of the problem instance. We focus on the presortedness of the input\nsequence as a measure of disorder for the problem instance. A number of\npractical applications that rely on heaps deal with input that is not random.\nEven random input contains presorted subsequences. Devising heaps that exploit\nthis structure may provide a means for improving practical performance. We\npresent some basic empirical tests to support this claim. Additionally,\nadaptive heaps may provide an interesting direction for theoretical\ninvestigation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 16:57:36 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Frohmader", "Andrew", ""]]}, {"id": "1802.05791", "submitter": "Oren Weimann", "authors": "Pawe{\\l} Gawrychowski, Liran Markin, Oren Weimann", "title": "A Faster FPTAS for #Knapsack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $W = \\{w_1,\\ldots, w_n\\}$ of non-negative integer weights and an\ninteger $C$, the #Knapsack problem asks to count the number of distinct subsets\nof $W$ whose total weight is at most $C$. In the more general integer version\nof the problem, the subsets are multisets. That is, we are also given a set $\n\\{u_1,\\ldots, u_n\\}$ and we are allowed to take up to $u_i$ items of weight\n$w_i$.\n  We present a deterministic FPTAS for #Knapsack running in\n$O(n^{2.5}\\varepsilon^{-1.5}\\log(n \\varepsilon^{-1})\\log (n \\varepsilon))$\ntime. The previous best deterministic algorithm [FOCS 2011] runs in $O(n^3\n\\varepsilon^{-1} \\log(n\\varepsilon^{-1}))$ time (see also [ESA 2014] for a\nlogarithmic factor improvement). The previous best randomized algorithm [STOC\n2003] runs in $O(n^{2.5} \\sqrt{\\log (n\\varepsilon^{-1}) } + \\varepsilon^{-2}\nn^2 )$ time. Therefore, in the natural setting of constant $\\varepsilon$, we\nclose the gap between the $\\tilde O(n^{2.5})$ randomized algorithm and the\n$\\tilde O(n^3)$ deterministic algorithm.\n  For the integer version with $U = \\max_i \\{u_i\\}$, we present a deterministic\nFPTAS running in $O(n^{2.5}\\varepsilon^{-1.5}\\log(n\\varepsilon^{-1} \\log U)\\log\n(n \\varepsilon) \\log^2 U)$ time. The previous best deterministic algorithm\n[APPROX 2016] runs in $O(n^3\\varepsilon^{-1}\\log(n \\varepsilon^{-1} \\log U)\n\\log^2 U)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 23:21:58 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Markin", "Liran", ""], ["Weimann", "Oren", ""]]}, {"id": "1802.05843", "submitter": "Hector Zenil", "authors": "Hector Zenil, Narsis A. Kiani, Felipe S. Abrah\\~ao, Antonio\n  Rueda-Toicen, Allan A. Zea and Jesper Tegn\\'er", "title": "Minimal Algorithmic Information Loss Methods for Dimension Reduction,\n  Feature Selection and Network Sparsification", "comments": "23 pages in double column including Appendix, online implementation\n  at http://complexitycalculator.com/MILS/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of unsupervised, domain-free, and (asymptotically)\nmodel-independent algorithms based on the principles of algorithmic probability\nand information theory designed to minimize the loss of algorithmic\ninformation, including a lossless-compression-based lossy compression\nalgorithm. The methods can select and coarse-grain data in an\nalgorithmic-complexity fashion (without the use of popular compression\nalgorithms) by collapsing regions that may procedurally be regenerated from a\ncomputable candidate model. We show that the method can preserve the salient\nproperties of objects and perform dimension reduction, denoising, feature\nselection, and network sparsification. As validation case, we demonstrate that\nthe method preserves all the graph-theoretic indices measured on a well-known\nset of synthetic and real-world networks of very different nature, ranging from\ndegree distribution and clustering coefficient to edge betweenness and degree\nand eigenvector centralities, achieving equal or significantly better results\nthan other data reduction and some of the leading network sparsification\nmethods. The methods (InfoRank, MILS) can also be applied to applications such\nas image segmentation based on algorithmic probability.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 06:13:08 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 15:45:20 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 14:11:54 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2018 15:46:46 GMT"}, {"version": "v5", "created": "Mon, 16 Jul 2018 04:46:58 GMT"}, {"version": "v6", "created": "Tue, 9 Apr 2019 22:35:00 GMT"}, {"version": "v7", "created": "Sat, 11 Apr 2020 20:39:51 GMT"}, {"version": "v8", "created": "Wed, 23 Sep 2020 21:56:21 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Zenil", "Hector", ""], ["Kiani", "Narsis A.", ""], ["Abrah\u00e3o", "Felipe S.", ""], ["Rueda-Toicen", "Antonio", ""], ["Zea", "Allan A.", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1802.05859", "submitter": "Martin Koutecky", "authors": "Martin Kouteck\\'y, Asaf Levin, Shmuel Onn", "title": "A Parameterized Strongly Polynomial Algorithm for Block Structured\n  Integer Programs", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ICALP.2018.85", "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of $n$-fold integer programming has been recently emerging as an\nimportant tool in parameterized complexity. The input to an $n$-fold integer\nprogram (IP) consists of parameter $A$, dimension $n$, and numerical data of\nbinary encoding length $L$. It was known for some time that such programs can\nbe solved in polynomial time using $O(n^{g(A)}L)$ arithmetic operations where\n$g$ is an exponential function of the parameter. In 2013 it was shown that it\ncan be solved in fixed-parameter tractable (FPT) time using $O(f(A)n^3L)$\narithmetic operations for a single-exponential function $f$. This, and a faster\nalgorithm for a special case of combinatorial $n$-fold IP, have led to several\nvery recent breakthroughs in the parameterized complexity of scheduling,\nstringology, and computational social choice. In 2015 it was shown that it can\nbe solved in strongly polynomial time using $O(n^{g(A)})$ arithmetic\noperations.\n  Here we establish a result which subsumes all three of the above results by\nshowing that $n$-fold IP can be solved in strongly polynomial FPT time using\n$O(f(A)n^3)$ arithmetic operations. In fact, our results are much more general,\nbriefly outlined as follows.\n  - There is a strongly polynomial algorithm for ILP whenever a so-called\nGraver-best oracle is realizable for it.\n  - Graver-best oracles for the large classes of multi-stage stochastic and\ntree-fold ILPs can be realized in FPT time. Together with the previous oracle\nalgorithm, this newly shows two large classes of ILP to be strongly polynomial;\nin contrast, only few classes of ILP were previously known to be strongly\npolynomial.\n  - We show that ILP is FPT parameterized by the largest coefficient\n$\\|A\\|_\\infty$ and the primal or dual treedepth of $A$, and that this\nparameterization cannot be relaxed, signifying substantial progress in\nunderstanding the parameterized complexity of ILP.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 08:14:53 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Kouteck\u00fd", "Martin", ""], ["Levin", "Asaf", ""], ["Onn", "Shmuel", ""]]}, {"id": "1802.05873", "submitter": "Shunhao Oh", "authors": "Shunhao Oh and Seth Gilbert", "title": "A Reallocation Algorithm for Online Split Packing of Circles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Split Packing algorithm \\cite{splitpacking_ws, splitpackingsoda,\nsplitpacking} is an offline algorithm that packs a set of circles into\ntriangles and squares up to critical density. In this paper, we develop an\nonline alternative to Split Packing to handle an online sequence of insertions\nand deletions, where the algorithm is allowed to reallocate circles into new\npositions at a cost proportional to their areas. The algorithm can be used to\npack circles into squares and right angled triangles. If only insertions are\nconsidered, our algorithm is also able to pack to critical density, with an\namortised reallocation cost of $O(c\\log \\frac{1}{c})$ for squares, and\n$O(c(1+s^2)\\log_{1+s^2}\\frac{1}{c})$ for right angled triangles, where $s$ is\nthe ratio of the lengths of the second shortest side to the shortest side of\nthe triangle, when inserting a circle of area $c$. When insertions and\ndeletions are considered, we achieve a packing density of $(1-\\epsilon)$ of the\ncritical density, where $\\epsilon>0$ can be made arbitrarily small, with an\namortised reallocation cost of $O(c(1+s^2)\\log_{1+s^2}\\frac{1}{c} +\nc\\frac{1}{\\epsilon})$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 09:21:58 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 02:38:04 GMT"}, {"version": "v3", "created": "Wed, 21 Nov 2018 07:40:13 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Oh", "Shunhao", ""], ["Gilbert", "Seth", ""]]}, {"id": "1802.05903", "submitter": "Maria Chiara Angelini", "authors": "Maria Chiara Angelini", "title": "Parallel Tempering for the planted clique problem", "comments": "12 pages, 5 figures", "journal-ref": "J. Stat. Mech. (2018) 073404", "doi": "10.1088/1742-5468/aace2c", "report-no": null, "categories": "cond-mat.dis-nn cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical information threshold for the planted clique problem is\n$2\\log_2(N)$, however no polynomial algorithm is known to recover a planted\nclique of size $O(N^{1/2-\\epsilon})$, $\\epsilon>0$. In this paper we will apply\na standard method for the analysis of disordered models, the Parallel-Tempering\n(PT) algorithm, to the clique problem, showing numerically that its\ntime-scaling in the hard region is indeed polynomial for the analyzed sizes. We\nalso apply PT to a different but connected model, the Sparse Planted\nIndependent Set problem. In this situation thresholds should be sharper and\nfinite size corrections should be less important. Also in this case PT shows a\npolynomial scaling in the hard region for the recovery.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 11:49:21 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 09:56:59 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Angelini", "Maria Chiara", ""]]}, {"id": "1802.05905", "submitter": "Kitty Meeks", "authors": "Jessica Enright, Kitty Meeks and Fiona Skerman", "title": "Assigning times to minimise reachability in temporal graphs", "comments": "Author final version, to appear in Journal of Computer and System\n  Sciences. Material from the previous version has been reorganised\n  substantially, and some results have been strengthened", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal graphs (in which edges are active at specified times) are of\nparticular relevance for spreading processes on graphs, e.g.~the spread of\ndisease or dissemination of information. Motivated by real-world applications,\nmodification of static graphs to control this spread has proven a rich topic\nfor previous research. Here, we introduce a new type of modification for\ntemporal graphs: the number of active times for each edge is fixed, but we can\nchange the relative order in which (sets of) edges are active. We investigate\nthe problem of determining an ordering of edges that minimises the maximum\nnumber of vertices reachable from any single starting vertex;\nepidemiologically, this corresponds to the worst-case number of vertices\ninfected in a single disease outbreak. We study two versions of this problem,\nboth of which we show to be $\\NP$-hard, and identify cases in which the problem\ncan be solved or approximated efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 12:08:55 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 18:19:14 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 10:38:01 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2020 15:39:19 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Enright", "Jessica", ""], ["Meeks", "Kitty", ""], ["Skerman", "Fiona", ""]]}, {"id": "1802.05906", "submitter": "Tomohiro I", "authors": "Hideo Bannai, Travis Gagie and Tomohiro I", "title": "Refining the $r$-index", "comments": "An extended version of the paper presented at CPM 2018 under the\n  title \"Online LZ77 parsing and matching statistics with RLBWTs\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gagie, Navarro and Prezza's $r$-index (SODA, 2018) promises to speed up DNA\nalignment and variation calling by allowing us to index entire genomic\ndatabases, provided certain obstacles can be overcome. In this paper we first\nstrengthen and simplify Policriti and Prezza's Toehold Lemma (DCC '16;\nAlgorithmica, 2017), which inspired the $r$-index and plays an important role\nin its implementation. We then show how to update the $r$-index efficiently\nafter adding a new genome to the database, which is likely to be vital in\npractice. As a by-product of this result, we obtain an online version of\nPolicriti and Prezza's algorithm for constructing the LZ77 parse from a\nrun-length compressed Burrows-Wheeler Transform. Our experiments demonstrate\nthe practicality of all three of these results. Finally, we show how to augment\nthe $r$-index such that, given a new genome and fast random access to the\ndatabase, we can quickly compute the matching statistics and maximal exact\nmatches of the new genome with respect to the database.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 12:19:07 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 14:46:50 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 13:45:26 GMT"}, {"version": "v4", "created": "Fri, 13 Apr 2018 05:09:07 GMT"}, {"version": "v5", "created": "Thu, 14 Feb 2019 14:57:51 GMT"}, {"version": "v6", "created": "Thu, 4 Jul 2019 13:38:02 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Bannai", "Hideo", ""], ["Gagie", "Travis", ""], ["I", "Tomohiro", ""]]}, {"id": "1802.06026", "submitter": "Felix Reidl", "authors": "Felix Reidl and Magnus Wahlstr\\\"om", "title": "Parameterized Algorithms for Zero Extension and Metric Labelling\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problems ZERO EXTENSION and METRIC LABELLING under the\nparadigm of parameterized complexity. These are natural, well-studied problems\nwith important applications, but have previously not received much attention\nfrom parameterized complexity.\n  Depending on the chosen cost function $\\mu$, we find that different\nalgorithmic approaches can be applied to design FPT-algorithms: for arbitrary\n$\\mu$ we parameterized by the number of edges that cross the cut (not the cost)\nand show how to solve ZERO EXTENSION in time $O(|D|^{O(k^2)} n^4 \\log n)$ using\nrandomized contractions. We improve this running time with respect to both\nparameter and input size to $O(|D|^{O(k)} m)$ in the case where $\\mu$ is a\nmetric. We further show that the problem admits a polynomial sparsifier, that\nis, a kernel of size $O(k^{|D|+1})$ that is independent of the metric $\\mu$.\n  With the stronger condition that $\\mu$ is described by the distances of\nleaves in a tree, we parameterize by a gap parameter $(q - p)$ between the cost\nof a true solution $q$ and a `discrete relaxation' $p$ and achieve a running\ntime of $O(|D|^{q-p} |T|m + |T|\\phi(n,m))$ where $T$ is the size of the tree\nover which $\\mu$ is defined and $\\phi(n,m)$ is the running time of a max-flow\ncomputation. We achieve a similar running for the more general METRIC\nLABELLING, while also allowing $\\mu$ to be the distance metric between an\narbitrary subset of nodes in a tree using tools from the theory of VCSPs. We\nexpect the methods used in the latter result to have further applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 16:53:44 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Reidl", "Felix", ""], ["Wahlstr\u00f6m", "Magnus", ""]]}, {"id": "1802.06030", "submitter": "Axel Bacher", "authors": "Axel Bacher", "title": "Improving the Florentine algorithms: recovering algorithms for Motzkin\n  and Schr\\\"oder paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present random sampling procedures for Motzkin and Schr\\\"oder paths,\nfollowing previous work on Dyck paths. Our algorithms follow the anticipated\nrejection method of the Florentine algorithms (Barcucci et al. 1994+), but\nintroduce a recovery idea to greatly reduce the probability of rejection. They\nuse an optimal amount of randomness and achieve a better time complexity than\nthe Florentine algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 17:07:10 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 17:10:28 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Bacher", "Axel", ""]]}, {"id": "1802.06052", "submitter": "Lin Chen", "authors": "Lin Chen, Hamed Hassani, Amin Karbasi", "title": "Online Continuous Submodular Maximization", "comments": "Accepted by AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an online optimization process, where the\nobjective functions are not convex (nor concave) but instead belong to a broad\nclass of continuous submodular functions. We first propose a variant of the\nFrank-Wolfe algorithm that has access to the full gradient of the objective\nfunctions. We show that it achieves a regret bound of $O(\\sqrt{T})$ (where $T$\nis the horizon of the online optimization problem) against a\n$(1-1/e)$-approximation to the best feasible solution in hindsight. However, in\nmany scenarios, only an unbiased estimate of the gradients are available. For\nsuch settings, we then propose an online stochastic gradient ascent algorithm\nthat also achieves a regret bound of $O(\\sqrt{T})$ regret, albeit against a\nweaker $1/2$-approximation to the best feasible solution in hindsight. We also\ngeneralize our results to $\\gamma$-weakly submodular functions and prove the\nsame sublinear regret bounds. Finally, we demonstrate the efficiency of our\nalgorithms on a few problem instances, including non-convex/non-concave\nquadratic programs, multilinear extensions of submodular set functions, and\nD-optimal design.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 17:56:48 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Chen", "Lin", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1802.06060", "submitter": "Xiaofeng Yang", "authors": "Xiaofeng Yang, Deepak Ajwani, Wolfgang Gatterbauer, Patrick K.\n  Nicholson, Mirek Riedewald, Alessandra Sala", "title": "Any-k: Anytime Top-k Tree Pattern Retrieval in Labeled Graphs", "comments": "To appear in WWW 2018", "journal-ref": null, "doi": "10.1145/3178876.3186115", "report-no": null, "categories": "cs.SI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in areas as diverse as recommendation systems, social network\nanalysis, semantic search, and distributed root cause analysis can be modeled\nas pattern search on labeled graphs (also called \"heterogeneous information\nnetworks\" or HINs). Given a large graph and a query pattern with node and edge\nlabel constraints, a fundamental challenge is to nd the top-k matches ac-\ncording to a ranking function over edge and node weights. For users, it is di\ncult to select value k . We therefore propose the novel notion of an any-k\nranking algorithm: for a given time budget, re- turn as many of the top-ranked\nresults as possible. Then, given additional time, produce the next lower-ranked\nresults quickly as well. It can be stopped anytime, but may have to continues\nuntil all results are returned. This paper focuses on acyclic patterns over\narbitrary labeled graphs. We are interested in practical algorithms that\neffectively exploit (1) properties of heterogeneous networks, in particular\nselective constraints on labels, and (2) that the users often explore only a\nfraction of the top-ranked results. Our solution, KARPET, carefully integrates\naggressive pruning that leverages the acyclic nature of the query, and\nincremental guided search. It enables us to prove strong non-trivial time and\nspace guarantees, which is generally considered very hard for this type of\ngraph search problem. Through experimental studies we show that KARPET achieves\nrunning times in the order of milliseconds for tree patterns on large networks\nwith millions of nodes and edges.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 18:21:54 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 19:21:00 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 21:29:10 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Yang", "Xiaofeng", ""], ["Ajwani", "Deepak", ""], ["Gatterbauer", "Wolfgang", ""], ["Nicholson", "Patrick K.", ""], ["Riedewald", "Mirek", ""], ["Sala", "Alessandra", ""]]}, {"id": "1802.06065", "submitter": "Pierre-Louis Giscard", "authors": "P-L. Giscard and R. C. Wilson", "title": "A Centrality Measure for Cycles and Subgraphs II", "comments": null, "journal-ref": null, "doi": "10.1007/s41109-018-0064-5", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent work we introduced a measure of importance for groups of vertices\nin a complex network. This centrality for groups is always between 0 and 1 and\ninduces the eigenvector centrality over vertices. Furthermore, its value over\nany group is the fraction of all network flows intercepted by this group. Here\nwe provide the rigorous mathematical constructions underpinning these results\nvia a semi-commutative extension of a number theoretic sieve. We then\nestablished further relations between the eigenvector centrality and the\ncentrality proposed here, showing that the latter is a proper extension of the\nformer to groups of nodes. We finish by comparing the centrality proposed here\nwith the notion of group-centrality introduced by Everett and Borgatti on two\nreal-world networks: the Wolfe's dataset and the protein-protein interaction\nnetwork of the yeast \\textit{Saccharomyces cerevisiae}. In this latter case, we\ndemonstrate that the centrality is able to distinguish protein complexes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 18:44:10 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Giscard", "P-L.", ""], ["Wilson", "R. C.", ""]]}, {"id": "1802.06204", "submitter": "Bin Fu", "authors": "Bin Fu, Pengfei Gu, and Yuming Zhao", "title": "Approximate Set Union Via Approximate Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an randomized approximation algorithm for the size of set union\nproblem $\\arrowvert A_1\\cup A_2\\cup...\\cup A_m\\arrowvert$, which given a list\nof sets $A_1,...,A_m$ with approximate set size $m_i$ for $A_i$ with $m_i\\in\n\\left((1-\\beta_L)|A_i|, (1+\\beta_R)|A_i|\\right)$, and biased random generators\nwith $Prob(x=\\randomElm(A_i))\\in \\left[{1-\\alpha_L\\over |A_i|},{1+\\alpha_R\\over\n|A_i|}\\right]$ for each input set $A_i$ and element $x\\in A_i,$ where $i=1, 2,\n..., m$. The approximation ratio for $\\arrowvert A_1\\cup A_2\\cup...\\cup\nA_m\\arrowvert$ is in the range $[(1-\\epsilon)(1-\\alpha_L)(1-\\beta_L),\n(1+\\epsilon)(1+\\alpha_R)(1+\\beta_R)]$ for any $\\epsilon\\in (0,1)$, where\n$\\alpha_L, \\alpha_R, \\beta_L,\\beta_R\\in (0,1)$. The complexity of the algorithm\nis measured by both time complexity, and round complexity. The algorithm is\nallowed to make multiple membership queries and get random elements from the\ninput sets in one round. Our algorithm makes adaptive accesses to input sets\nwith multiple rounds. Our algorithm gives an approximation scheme with\n$O(\\setCount\\cdot(\\log \\setCount)^{O(1)})$ running time and $O(\\log m)$ rounds,\nwhere $m$ is the number of sets. Our algorithm can handle input sets that can\ngenerate random elements with bias, and its approximation ratio depends on the\nbias. Our algorithm gives a flexible tradeoff with time complexity\n$O\\left(\\setCount^{1+\\xi}\\right)$ and round complexity $O\\left({1\\over\n\\xi}\\right)$ for any $\\xi\\in(0,1)$.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 07:37:40 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 02:02:57 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 03:52:43 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Fu", "Bin", ""], ["Gu", "Pengfei", ""], ["Zhao", "Yuming", ""]]}, {"id": "1802.06212", "submitter": "Chien-Chung Huang", "authors": "Chien-Chung Huang and Naonori Kakimura", "title": "Multi-Pass Streaming Algorithms for Monotone Submodular Function\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider maximizing a monotone submodular function under a cardinality\nconstraint or a knapsack constraint in the streaming setting. In particular,\nthe elements arrive sequentially and at any point of time, the algorithm has\naccess to only a small fraction of the data stored in primary memory. We\npropose the following streaming algorithms taking $O(\\varepsilon^{-1})$ passes:\n  ----a $(1-e^{-1}-\\varepsilon)$-approximation algorithm for the\ncardinality-constrained problem ---- a $(0.5-\\varepsilon)$-approximation\nalgorithm for the knapsack-constrained problem.\n  Both of our algorithms run in $O^\\ast(n)$ time, using $O^\\ast(K)$ space,\nwhere $n$ is the size of the ground set and $K$ is the size of the knapsack.\nHere the term $O^\\ast$ hides a polynomial of $\\log K$ and $\\varepsilon^{-1}$.\nOur streaming algorithms can also be used as fast approximation algorithms. In\nparticular, for the cardinality-constrained problem, our algorithm takes\n$O(n\\varepsilon^{-1} \\log (\\varepsilon^{-1}\\log K) )$ time, improving on the\nalgorithm of Badanidiyuru and Vondr\\'{a}k that takes $O(n \\varepsilon^{-1} \\log\n(\\varepsilon^{-1} K) )$ time.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 08:46:50 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Huang", "Chien-Chung", ""], ["Kakimura", "Naonori", ""]]}, {"id": "1802.06271", "submitter": "Shang-En Huang", "authors": "Shang-En Huang and Seth Pettie", "title": "Lower Bounds on Sparse Spanners, Emulators, and Diameter-reducing\n  shortcuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove better lower bounds on additive spanners and emulators, which are\nlossy compression schemes for undirected graphs, as well as lower bounds on\nshortcut sets, which reduce the diameter of directed graphs. We show that any\n$O(n)$-size shortcut set cannot bring the diameter below $\\Omega(n^{1/6})$, and\nthat any $O(m)$-size shortcut set cannot bring it below $\\Omega(n^{1/11})$.\nThese improve Hesse's [Hesse03] lower bound of $\\Omega(n^{1/17})$. By combining\nthese constructions with Abboud and Bodwin's [AbboudB17] edge-splitting\ntechnique, we get additive stretch lower bounds of $+\\Omega(n^{1/11})$ for\n$O(n)$-size spanners and $+\\Omega(n^{1/18})$ for $O(n)$-size emulators. These\nimprove Abboud and Bodwin's $+\\Omega(n^{1/22})$ lower bounds.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 18:32:17 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 03:00:44 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Huang", "Shang-En", ""], ["Pettie", "Seth", ""]]}, {"id": "1802.06289", "submitter": "Christoph Hunkenschr\\\"oder", "authors": "Friedrich Eisenbrand (1), Christoph Hunkenschr\\\"oder (1), Kim-Manuel\n  Klein (1) ((1) \\'Ecole polytechnique f\\'ed\\'erale de Lausanne)", "title": "Faster Algorithms for Integer Programs with Block Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider integer programming problems $\\max \\{ c^T x : \\mathcal{A} x = b,\nl \\leq x \\leq u, x \\in \\mathbb{Z}^{nt}\\}$ where $\\mathcal{A}$ has a (recursive)\nblock-structure generalizing \"$n$-fold integer programs\" which recently\nreceived considerable attention in the literature. An $n$-fold IP is an integer\nprogram where $\\mathcal{A}$ consists of $n$ repetitions of submatrices $A \\in\n\\mathbb{Z}^{r \\times t}$ on the top horizontal part and $n$ repetitions of a\nmatrix $B \\in \\mathbb{Z}^{s \\times t}$ on the diagonal below the top part.\nInstead of allowing only two types of block matrices, one for the horizontal\nline and one for the diagonal, we generalize the $n$-fold setting to allow for\narbitrary matrices in every block. We show that such an integer program can be\nsolved in time $n^2 t^2 {\\phi} \\cdot (rs{\\Delta})^{\\mathcal{O}(rs^2+ sr^2)}$\n(ignoring logarithmic factors). Here ${\\Delta}$ is an upper bound on the\nlargest absolute value of an entry of $\\mathcal{A}$ and ${\\phi}$ is the largest\nbinary encoding length of a coefficient of $c$. This improves upon the\npreviously best algorithm of Hemmecke, Onn and Romanchuk that runs in time\n$n^3t^3 {\\phi} \\cdot {\\Delta}^{\\mathcal{O}(t^2s)}$. In particular, our\nalgorithm is not exponential in the number $t$ of columns of $A$ and $B$.\n  Our algorithm is based on a new upper bound on the $l_1$-norm of an element\nof the \"Graver basis\" of an integer matrix and on a proximity bound between the\nLP and IP optimal solutions tailored for IPs with block structure. These new\nbounds rely on the \"Steinitz Lemma\".\n  Furthermore, we extend our techniques to the recently introduced \"tree-fold\nIPs\", where we again present a more efficient algorithm in a generalized\nsetting.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 20:43:57 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Eisenbrand", "Friedrich", "", "\u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne"], ["Hunkenschr\u00f6der", "Christoph", "", "\u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne"], ["Klein", "Kim-Manuel", "", "\u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne"]]}, {"id": "1802.06328", "submitter": "Peter Clote", "authors": "Amir H. Bayegan and Peter Clote", "title": "Minimum length RNA folding trajectories", "comments": "38 pages with 26 figures and additional 11 page appendix containing 3\n  tables and supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kinfold and KFOLD programs for RNA folding kinetics implement the\nGillespie algorithm to generate stochastic folding trajectories from an initial\nstructure s to a target structure t, in which each intermediate secondary\nstructure is obtained from its predecessor by the addition, removal or shift of\na single base pair. Define MS2 distance between secondary structures s and t to\nbe the minimum path length to refold s to t, where a move from MS2 is applied\nin each step. We describe algorithms to compute the shortest MS2 folding\ntrajectory between any two given RNA secondary structures. These algorithms\ninclude an optimal integer programming (IP) algorithm, an accurate and\nefficient near-optimal algorithm, a greedy algorithm, a branch-and-bound\nalgorithm, and an optimal algorithm if one allows intermediate structures to\ncontain pseudoknots. Our optimal IP [resp. near-optimal IP] algorithm maximizes\n[resp. approximately maximizes] the number of shifts and minimizes [resp.\napproximately minimizes] the number of base pair additions and removals by\napplying integer programming to (essentially) solve the minimum feedback vertex\nset (FVS) problem for the RNA conflict digraph, then applies topological sort\nto tether subtrajectories into the final optimal folding trajectory. We prove\nNP-hardness of the problem to determine the minimum barrier energy over all\npossible MS2 folding pathways, and conjecture that computing the MS2 distance\nbetween arbitrary secondary structures is NP-hard. Since our optimal IP\nalgorithm relies on the FVS, known to be NP-complete for arbitrary digraphs, we\ncompare the family of RNA conflict digraphs with the following classes of\ndigraphs (planar, reducible flow graph, Eulerian, and tournament) for which FVS\nis known to be either polynomial time computable or NP-hard. Source code\navailable at http://bioinformatics.bc.edu/clotelab/MS2distance/.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 03:41:43 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Bayegan", "Amir H.", ""], ["Clote", "Peter", ""]]}, {"id": "1802.06361", "submitter": "Jimmy Wu", "authors": "Moses Charikar, Yonatan Naamad, Jimmy Wu", "title": "On Finding Dense Common Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the recently introduced problem of finding dense common subgraphs:\nGiven a sequence of graphs that share the same vertex set, the goal is to find\na subset of vertices $S$ that maximizes some aggregate measure of the density\nof the subgraphs induced by $S$ in each of the given graphs. Different choices\nfor the aggregation function give rise to variants of the problem that were\nstudied recently. We settle many of the questions left open by previous works,\nshowing NP-hardness, hardness of approximation, non-trivial approximation\nalgorithms, and an integrality gap for a natural relaxation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 10:52:52 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Charikar", "Moses", ""], ["Naamad", "Yonatan", ""], ["Wu", "Jimmy", ""]]}, {"id": "1802.06369", "submitter": "Solon Pissis", "authors": "Panagiotis Charalampopoulos, Maxime Crochemore, Costas S. Iliopoulos,\n  Tomasz Kociumaka, Solon P. Pissis, Jakub Radoszewski, Wojciech Rytter, and\n  Tomasz Wale\\'n", "title": "Linear-Time Algorithm for Long LCF with $k$ Mismatches", "comments": "submitted to CPM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Longest Common Factor with $k$ Mismatches (LCF$_k$) problem, we are\ngiven two strings $X$ and $Y$ of total length $n$, and we are asked to find a\npair of maximal-length factors, one of $X$ and the other of $Y$, such that\ntheir Hamming distance is at most $k$. Thankachan et al. show that this problem\ncan be solved in $\\mathcal{O}(n \\log^k n)$ time and $\\mathcal{O}(n)$ space for\nconstant $k$. We consider the LCF$_k$($\\ell$) problem in which we assume that\nthe sought factors have length at least $\\ell$, and the LCF$_k$($\\ell$) problem\nfor $\\ell=\\Omega(\\log^{2k+2} n)$, which we call the Long LCF$_k$ problem. We\nuse difference covers to reduce the Long LCF$_k$ problem to a task involving\n$m=\\mathcal{O}(n/\\log^{k+1}n)$ synchronized factors. The latter can be solved\nin $\\mathcal{O}(m \\log^{k+1}m)$ time, which results in a linear-time algorithm\nfor Long LCF$_k$. In general, our solution to LCF$_k$($\\ell$) for arbitrary\n$\\ell$ takes $\\mathcal{O}(n + n \\log^{k+1} n/\\sqrt{\\ell})$ time.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 13:04:21 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Charalampopoulos", "Panagiotis", ""], ["Crochemore", "Maxime", ""], ["Iliopoulos", "Costas S.", ""], ["Kociumaka", "Tomasz", ""], ["Pissis", "Solon P.", ""], ["Radoszewski", "Jakub", ""], ["Rytter", "Wojciech", ""], ["Wale\u0144", "Tomasz", ""]]}, {"id": "1802.06440", "submitter": "Kyriakos Axiotis", "authors": "Kyriakos Axiotis and Christos Tzamos", "title": "Capacitated Dynamic Programming: Faster Knapsack and Graph Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental problems in Computer Science is the Knapsack\nproblem. Given a set of n items with different weights and values, it asks to\npick the most valuable subset whose total weight is below a capacity threshold\nT. Despite its wide applicability in various areas in Computer Science,\nOperations Research, and Finance, the best known running time for the problem\nis O(Tn). The main result of our work is an improved algorithm running in time\nO(TD), where D is the number of distinct weights. Previously, faster runtimes\nfor Knapsack were only possible when both weights and values are bounded by M\nand V respectively, running in time O(nMV) [Pisinger'99]. In comparison, our\nalgorithm implies a bound of O(nM^2) without any dependence on V, or O(nV^2)\nwithout any dependence on M. Additionally, for the unbounded Knapsack problem,\nwe provide an algorithm running in time O(M^2) or O(V^2). Both our algorithms\nmatch recent conditional lower bounds shown for the Knapsack problem [Cygan et\nal'17, K\\\"unnemann et al'17].\n  We also initiate a systematic study of general capacitated dynamic\nprogramming, of which Knapsack is a core problem. This problem asks to compute\nthe maximum weight path of length k in an edge- or node-weighted directed\nacyclic graph. In a graph with m edges, these problems are solvable by dynamic\nprogramming in time O(km), and we explore under which conditions the dependence\non k can be eliminated. We identify large classes of graphs where this is\npossible and apply our results to obtain linear time algorithms for the problem\nof k-sparse Delta-separated sequences. The main technical innovation behind our\nresults is identifying and exploiting concavity that appears in relaxations and\nsubproblems of the tasks we consider.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 20:37:55 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 21:21:19 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Axiotis", "Kyriakos", ""], ["Tzamos", "Christos", ""]]}, {"id": "1802.06478", "submitter": "Kazuya Haraguchi", "authors": "Kazuya Haraguchi", "title": "An Efficient Local Search for the Minimum Independent Dominating Set\n  Problem", "comments": "18 pages, presented at SEA2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we propose an efficient local search for the minimum\nindependent dominating set problem. We consider a local search that uses\n$k$-swap as the neighborhood operation. Given a feasible solution $S$, it is\nthe operation of obtaining another feasible solution by dropping exactly $k$\nvertices from $S$ and then by adding any number of vertices to it. We show\nthat, when $k=2$, (resp., $k=3$ and a given solution is minimal with respect to\n2-swap), we can find an improved solution in the neighborhood or conclude that\nno such solution exists in $O(n\\Delta)$ (resp., $O(n\\Delta^3)$) time, where $n$\ndenotes the number of vertices and $\\Delta$ denotes the maximum degree. We\ndevelop a metaheuristic algorithm that repeats the proposed local search and\nthe plateau search iteratively, where the plateau search examines solutions of\nthe same size as the current solution that are obtainable by exchanging a\nsolution vertex and a non-solution vertex. The algorithm is so effective that,\namong 80 DIMACS graphs, it updates the best-known solution size for five graphs\nand performs as well as existing methods for the remaining graphs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 01:11:16 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 11:46:41 GMT"}, {"version": "v3", "created": "Sun, 18 Aug 2019 10:25:05 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Haraguchi", "Kazuya", ""]]}, {"id": "1802.06511", "submitter": "Yota Otachi", "authors": "Takehiro Ito and Yota Otachi", "title": "Reconfiguration of Colorable Sets in Classes of Perfect Graphs", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of vertices in a graph is c-colorable if the subgraph induced by the\nset has a proper c-coloring. In this paper, we study the problem of finding a\nstep-by-step transformation (reconfiguration) between two c-colorable sets in\nthe same graph. This problem generalizes the well-studied Independent Set\nReconfiguration problem. As the first step toward a systematic understanding of\nthe complexity of this general problem, we study the problem on classes of\nperfect graphs. We first focus on interval graphs and give a combinatorial\ncharacterization of the distance between two c-colorable sets. This gives a\nlinear-time algorithm for finding an actual shortest reconfiguration sequence\nfor interval graphs. Since interval graphs are exactly the graphs that are\nsimultaneously chordal and co-comparability, we then complement the positive\nresult by showing that even deciding reachability is PSPACE-complete for\nchordal graphs and for co-comparability graphs. The hardness for chordal graphs\nholds even for split graphs. We also consider the case where c is a fixed\nconstant and show that in such a case the reachability problem is\npolynomial-time solvable for split graphs but still PSPACE-complete for\nco-comparability graphs. The complexity of this case for chordal graphs remains\nunsettled. As by-products, our positive results give the first polynomial-time\nsolvable cases (split graphs and interval graphs) for Feedback Vertex Set\nReconfiguration.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 03:37:32 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Ito", "Takehiro", ""], ["Otachi", "Yota", ""]]}, {"id": "1802.06532", "submitter": "Takeharu Shiraga", "authors": "Takeharu Shiraga", "title": "Discrepancy Analysis of a New Randomized Diffusion Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an arbitrary initial configuration of discrete loads over vertices of a\ndistributed graph, we consider the problem of minimizing the {\\em discrepancy}\nbetween the maximum and minimum loads among all vertices. For this problem,\nthis paper is concerned with the ability of natural diffusion-based iterative\nalgorithms: at each discrete and synchronous time step on an algorithm, each\nvertex is allowed to distribute its loads to each neighbor (including itself)\nwithout occurring negative loads or using the information of previous time\nsteps.\n  In this setting, this paper presents a new {\\em randomized} diffusion\nalgorithm like multiple random walks. Our algorithm archives $O(\\sqrt{d \\log\nN})$ discrepancy for any $d$-regular graph with $N$ vertices with high\nprobability, while {\\em deterministic} diffusion algorithms have $\\Omega(d)$\nlower bound. Furthermore, we succeed in generalizing our algorithm to any\nsymmetric round matrix. This yields that $O(\\sqrt{ d_{\\max} \\log N})$\ndiscrepancy for arbitrary graphs without using the information of maximum\ndegree $d_{\\max}$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 07:10:15 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 07:15:16 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 10:23:04 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Shiraga", "Takeharu", ""]]}, {"id": "1802.06545", "submitter": "Kasper Green Larsen", "authors": "Raphael Clifford, Allan Gr{\\o}nlund, Kasper Green Larsen, Tatiana\n  Starikovskaya", "title": "Upper and lower bounds for dynamic data structures on strings", "comments": "Accepted at STACS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a range of simply stated dynamic data structure problems on\nstrings. An update changes one symbol in the input and a query asks us to\ncompute some function of the pattern of length $m$ and a substring of a longer\ntext. We give both conditional and unconditional lower bounds for variants of\nexact matching with wildcards, inner product, and Hamming distance computation\nvia a sequence of reductions. As an example, we show that there does not exist\nan $O(m^{1/2-\\varepsilon})$ time algorithm for a large range of these problems\nunless the online Boolean matrix-vector multiplication conjecture is false. We\nalso provide nearly matching upper bounds for most of the problems we consider.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 08:29:40 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Clifford", "Raphael", ""], ["Gr\u00f8nlund", "Allan", ""], ["Larsen", "Kasper Green", ""], ["Starikovskaya", "Tatiana", ""]]}, {"id": "1802.06621", "submitter": "Tung Mai", "authors": "Tung Mai and Vijay V. Vazirani", "title": "A Natural Generalization of Stable Matching Solved via New Insights into\n  Ideal Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a natural generalization of stable matching to the maximum weight\nstable matching problem and we obtain a combinatorial polynomial time algorithm\nfor it by reducing it to the problem of finding a maximum weight ideal cut in a\nDAG. We give the first polynomial time algorithm for the latter problem; this\nalgorithm is also combinatorial.\n  The combinatorial nature of our algorithms not only means that they are\nefficient but also that they enable us to obtain additional structural and\nalgorithmic results:\n  - We show that the set, $\\cal M'$, of maximum weight stable matchings forms a\nsublattice $\\cal L'$ of the lattice $\\cal L$ of all stable matchings.\n  - We give an efficient algorithm for finding boy-optimal and girl-optimal\nmatchings in $\\cal M'$.\n  - We generalize the notion of rotation, a central structural notion in the\ncontext of the stable matching problem, to meta-rotation. Just as rotations\nhelp traverse the lattice of all stable matchings, macro-rotations help\ntraverse the sublattice over $\\cal M'$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 13:33:52 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 02:36:16 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Mai", "Tung", ""], ["Vazirani", "Vijay V.", ""]]}, {"id": "1802.06636", "submitter": "J\\'er\\'emie Chalopin", "authors": "Evangelos Bampas, J\\'er\\'emie Chalopin, Shantanu Das, Jan Hackfeld,\n  Christina Karousatou", "title": "Maximal Exploration of Trees with Energy-Constrained Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of exploring an unknown tree with a team of $k$\ninitially colocated mobile agents. Each agent has limited energy and cannot, as\na result, traverse more than $B$ edges. The goal is to maximize the number of\nnodes collectively visited by all agents during the execution. Initially, the\nagents have no knowledge about the structure of the tree, but they gradually\ndiscover the topology as they traverse new edges. We assume that the agents can\ncommunicate with each other at arbitrary distances. Therefore the knowledge\nobtained by one agent after traversing an edge is instantaneously transmitted\nto the other agents. We propose an algorithm that divides the tree into\nsubtrees during the exploration process and makes a careful trade-off between\nbreadth-first and depth-first exploration. We show that our algorithm is\n3-competitive compared to an optimal solution that we could obtain if we knew\nthe map of the tree in advance. While it is easy to see that no algorithm can\nbe better than 2-competitive, we give a non-trivial lower bound of 2.17 on the\ncompetitive ratio of any online algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 14:11:50 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Bampas", "Evangelos", ""], ["Chalopin", "J\u00e9r\u00e9mie", ""], ["Das", "Shantanu", ""], ["Hackfeld", "Jan", ""], ["Karousatou", "Christina", ""]]}, {"id": "1802.06676", "submitter": "Manuela Fischer", "authors": "Manuela Fischer and Mohsen Ghaffari", "title": "A Simple Parallel and Distributed Sampling Technique: Local Glauber\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\emph{Sampling} constitutes an important tool in a variety of areas: from\nmachine learning and combinatorial optimization to computational physics and\nbiology. A central class of sampling algorithms is the \\emph{Markov Chain Monte\nCarlo} method, based on the construction of a Markov chain with the desired\nsampling distribution as its stationary distribution. Many of the traditional\nMarkov chains, such as the \\emph{Glauber dynamics}, do not scale well with\nincreasing dimension. To address this shortcoming, we propose a simple local\nupdate rule based on the Glauber dynamics that leads to efficient parallel and\ndistributed algorithms for sampling from Gibbs distributions.\n  Concretely, we present a Markov chain that mixes in $O(\\log n)$ rounds when\nDobrushin's condition for the Gibbs distribution is satisfied. This improves\nover the \\emph{LubyGlauber} algorithm by Feng, Sun, and Yin [PODC'17], which\nneeds $O(\\Delta \\log n)$ rounds, and their \\emph{LocalMetropolis} algorithm,\nwhich converges in $O(\\log n)$ rounds but requires a considerably stronger\nmixing condition. Here, $n$ denotes the number of nodes in the graphical model\ninducing the Gibbs distribution, and $\\Delta$ its maximum degree. In\nparticular, our method can sample a uniform proper coloring with $\\alpha\n\\Delta$ colors in $O(\\log n)$ rounds for any $\\alpha>2$, which almost matches\nthe threshold of the sequential Glauber dynamics and improves on the $\\alpha>2\n+\\sqrt{2}$ threshold of Feng et al.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 15:43:28 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 07:45:57 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Fischer", "Manuela", ""], ["Ghaffari", "Mohsen", ""]]}, {"id": "1802.06686", "submitter": "Yitong Yin", "authors": "Weiming Feng and Yitong Yin", "title": "On Local Distributed Sampling and Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classic distributed graph problems, each instance on a graph specifies a\nspace of feasible solutions (e.g. all proper ($\\Delta+1$)-list-colorings of the\ngraph), and the task of distributed algorithm is to construct a feasible\nsolution using local information.\n  We study distributed sampling and counting problems, in which each instance\nspecifies a joint distribution of feasible solutions. The task of distributed\nalgorithm is to sample from this joint distribution, or to locally measure the\nvolume of the probability space via the marginal probabilities. The latter task\nis also known as inference, which is a local counterpart of counting.\n  For self-reducible classes of instances, the following equivalences are\nestablished in the LOCAL model up to polylogarithmic factors:\n  $\\bullet$ For all joint distributions, approximate inference and approximate\nsampling are computationally equivalent.\n  $\\bullet$ For all joint distributions defined by local constraints, exact\nsampling is reducible to either one of the above tasks.\n  $\\bullet$ If further, sequentially constructing a feasible solution is\ntrivial locally, then all above tasks are easy if and only if the joint\ndistribution exhibits strong spatial mixing.\n  Combining with the state of the arts of strong spatial mixing, we obtain\nefficient sampling algorithms in the LOCAL model for various important sampling\nproblems, including: an $O(\\sqrt{\\Delta}\\log^3n)$-round algorithm for exact\nsampling matchings in graphs with maximum degree $\\Delta$, and an\n$O(\\log^3n)$-round algorithm for sampling according to the hardcore model\n(weighted independent sets) in the uniqueness regime, which along with the\n$\\Omega(\\mathrm{diam})$ lower bound in arXiv:1702.00142 for sampling according\nto the hardcore model in the non-uniqueness regime, gives the first\ncomputational phase transition for distributed sampling.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 16:06:32 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Feng", "Weiming", ""], ["Yin", "Yitong", ""]]}, {"id": "1802.06701", "submitter": "Leon Kellerhals", "authors": "Matthias Bentert, Alexander Dittmann, Leon Kellerhals, Andr\\'e\n  Nichterlein and Rolf Niedermeier", "title": "An Adaptive Version of Brandes' Algorithm for Betweenness Centrality", "comments": "An extended abstract of this work appears in the proceedings of the\n  29th International Symposium on Algorithms and Computation (ISAAC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness centrality---measuring how many shortest paths pass through a\nvertex---is one of the most important network analysis concepts for assessing\nthe relative importance of a vertex. The well-known algorithm of Brandes [J.\nMath. Sociol.~'01] computes, on an $n$-vertex and $m$-edge graph, the\nbetweenness centrality of all vertices in $O(nm)$ worst-case time. In later\nwork, significant empirical speedups were achieved by preprocessing degree-one\nvertices and by graph partitioning based on cut vertices. We contribute an\nalgorithmic treatment of degree-two vertices, which turns out to be much richer\nin mathematical structure than the case of degree-one vertices. Based on these\nthree algorithmic ingredients, we provide a strengthened worst-case running\ntime analysis for betweenness centrality algorithms. More specifically, we\nprove an adaptive running time bound $O(kn)$, where $k < m$ is the size of a\nminimum feedback edge set of the input graph.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 16:58:28 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 13:34:14 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 11:56:08 GMT"}, {"version": "v4", "created": "Tue, 12 May 2020 18:00:21 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Bentert", "Matthias", ""], ["Dittmann", "Alexander", ""], ["Kellerhals", "Leon", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""]]}, {"id": "1802.06716", "submitter": "Nathan Cordner", "authors": "Nathan Cordner", "title": "Two Algorithms to Compute Symmetry Groups for Landau-Ginzburg Models", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landau-Ginzburg mirror symmetry studies isomorphisms between graded Frobenius\nalgebras, known as A- and B-models. Fundamental to constructing these models is\nthe computation of the finite, Abelian $\\textit{maximal symmetry group}$\n$G_{W}^{\\max}$ of a given polynomial $W$. For $\\textit{invertible}$\npolynomials, which have the same number of monomials as variables, a generating\nset for this group can be computed efficiently by inverting the\n$\\textit{polynomial exponent matrix}$. However, this method does not work for\n$\\textit{noninvertible}$ polynomials with more monomials than variables since\nthe resulting exponent matrix is no longer square.\n  A previously conjectured algorithm to address this problem relies on\nintersecting groups generated from $\\textit{submatrices}$ of the exponent\nmatrix. We prove that this method is correct, but intractable in general. We\novercome intractability by presenting a group isomorphism based on the Smith\nnormal form of the exponent matrix. We demonstrate an algorithm to compute\n$G_{W}^{\\max}$ via this isomorphism, and show its efficiency in all cases.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 17:26:36 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 19:46:32 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Cordner", "Nathan", ""]]}, {"id": "1802.06742", "submitter": "Mika\\\"el Rabie", "authors": "Marthe Bonamy, Paul Ouvrard, Mika\\\"el Rabie, Jukka Suomela, Jara Uitto", "title": "Distributed Recoloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two colorings of a graph, we consider the following problem: can we\nrecolor the graph from one coloring to the other through a series of elementary\nchanges, such that the graph is properly colored after each step?\n  We introduce the notion of distributed recoloring: The input graph represents\na network of computers that needs to be recolored. Initially, each node is\naware of its own input color and target color. The nodes can exchange messages\nwith each other, and eventually each node has to stop and output its own\nrecoloring schedule, indicating when and how the node changes its color. The\nrecoloring schedules have to be globally consistent so that the graph remains\nproperly colored at each point, and we require that adjacent nodes do not\nchange their colors simultaneously.\n  We are interested in the following questions: How many communication rounds\nare needed (in the LOCAL model of distributed computing) to find a recoloring\nschedule? What is the length of the recoloring schedule? And how does the\npicture change if we can use extra colors to make recoloring easier?\n  The main contributions of this work are related to distributed recoloring\nwith one extra color in the following graph classes: trees, $3$-regular graphs,\nand toroidal grids.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 18:16:40 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 11:18:25 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Bonamy", "Marthe", ""], ["Ouvrard", "Paul", ""], ["Rabie", "Mika\u00ebl", ""], ["Suomela", "Jukka", ""], ["Uitto", "Jara", ""]]}, {"id": "1802.06748", "submitter": "Manuela Fischer", "authors": "Sebastian Brandt and Manuela Fischer and Jara Uitto", "title": "Breaking the Linear-Memory Barrier in MPC: Fast MIS on Trees with\n  Strongly Sublinear Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, studying fundamental graph problems in the \\emph{Massively Parallel\nComputation (MPC) framework, inspired by the MapReduce paradigm, has gained a\nlot of attention. An assumption common to a vast majority of approaches is to\nallow $\\widetilde{\\Omega}(n)$ memory per machine, where $n$ is the number of\nnodes in the graph and $\\widetilde{\\Omega}$ hides polylogarithmic factors.\nHowever, as pointed out by Karloff et al. [SODA'10] and Czumaj et al.\n[STOC'18], it might be unrealistic for a single machine to have linear or only\nslightly sublinear memory.\n  In this paper, we thus study a more practical variant of the MPC model which\nonly requires substantially sublinear or even subpolynomial memory per machine.\nIn contrast to the linear-memory MPC model and also to streaming algorithms, in\nthis low-memory MPC setting, a single machine will only see a small number of\nnodes in the graph. We introduce a new and strikingly simple technique to cope\nwith this imposed locality.\n  In particular, we show that the Maximal Independent Set (MIS) problem can be\nsolved efficiently, that is, in $O(\\log^3 \\log n)$ rounds, when the input graph\nis a tree. This constitutes an almost exponential speed-up over the low-memory\nMPC algorithm in $O(\\sqrt{\\log n})$-algorithm in a concurrent work by Ghaffari\nand Uitto [SODA'19] and substantially reduces the local memory from\n$\\widetilde{\\Omega}(n)$ required by the recent $O(\\log \\log n)$-round MIS\nalgorithm of Ghaffari et al. [PODC'18] to $n^{\\alpha}$ for any $\\alpha>0$,\nwithout incurring a significant loss in the round complexity. Moreover, it\ndemonstrates how to make use of the all-to-all communication in the MPC model\nto almost exponentially improve on the corresponding bound in the\n$\\mathsf{LOCAL}$ and $\\mathsf{PRAM}$ models by Lenzen and Wattenhofer\n[PODC'11].\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 18:29:32 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 07:38:39 GMT"}, {"version": "v3", "created": "Sat, 25 May 2019 19:33:09 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Brandt", "Sebastian", ""], ["Fischer", "Manuela", ""], ["Uitto", "Jara", ""]]}, {"id": "1802.06872", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Adrian Kosowski, Przemys{\\l}aw Uzna\\'nski", "title": "Population Protocols Are Fast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A population protocol describes a set of state change rules for a population\nof $n$ indistinguishable finite-state agents (automata), undergoing random\npairwise interactions. Within this very basic framework, it is possible to\nresolve a number of fundamental tasks in distributed computing, including:\nleader election, aggregate and threshold functions on the population, such as\nmajority computation, and plurality consensus. For the first time, we show that\nsolutions to all of these problems can be obtained \\emph{quickly} using\nfinite-state protocols. For any input, the designed finite-state protocols\nconverge under a fair random scheduler to an output which is correct with high\nprobability in expected $O(\\mathrm{poly} \\log n)$ parallel time. In the same\nsetting, we also show protocols which always reach a valid solution, in\nexpected parallel time $O(n^\\varepsilon)$, where the number of states of the\ninteracting automata depends only on the choice of $\\varepsilon>0$. The stated\ntime bounds hold for \\emph{any} semi-linear predicate computable in the\npopulation protocol framework.\n  The key ingredient of our result is the decentralized design of a hierarchy\nof phase-clocks, which tick at different rates, with the rates of adjacent\nclocks separated by a factor of $\\Theta(\\log n)$. The construction of this\nclock hierarchy relies on a new protocol composition technique, combined with\nan adapted analysis of a self-organizing process of oscillatory dynamics. This\nclock hierarchy is used to provide nested synchronization primitives, which\nallow us to view the population in a global manner and design protocols using a\nhigh-level imperative programming language with a (limited) capacity for loops\nand branching instructions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 21:49:01 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 20:43:45 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Kosowski", "Adrian", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1802.06905", "submitter": "Grace Dinh", "authors": "James Demmel, Grace Dinh", "title": "Communication-Optimal Convolutional Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently executing convolutional neural nets (CNNs) is important in many\nmachine-learning tasks. Since the cost of moving a word of data, either between\nlevels of a memory hierarchy or between processors over a network, is much\nhigher than the cost of an arithmetic operation, minimizing data movement is\ncritical to performance optimization. In this paper, we present both new lower\nbounds on data movement needed for CNNs, and optimal sequential algorithms that\nattain these lower bounds. In most common cases, our optimal algorithms can\nattain significantly more data reuse than matrix multiplication.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 23:20:19 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 04:00:58 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Demmel", "James", ""], ["Dinh", "Grace", ""]]}, {"id": "1802.06942", "submitter": "Ehsan Kazemi", "authors": "Ehsan Kazemi and Lin Chen and Sanjoy Dasgupta and Amin Karbasi", "title": "Comparison Based Learning from Weak Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing interest in learning algorithms that involve interaction\nbetween human and machine. Comparison-based queries are among the most natural\nways to get feedback from humans. A challenge in designing comparison-based\ninteractive learning algorithms is coping with noisy answers. The most common\nfix is to submit a query several times, but this is not applicable in many\nsituations due to its prohibitive cost and due to the unrealistic assumption of\nindependent noise in different repetitions of the same query.\n  In this paper, we introduce a new weak oracle model, where a non-malicious\nuser responds to a pairwise comparison query only when she is quite sure about\nthe answer. This model is able to mimic the behavior of a human in noise-prone\nregions. We also consider the application of this weak oracle model to the\nproblem of content search (a variant of the nearest neighbor search problem)\nthrough comparisons. More specifically, we aim at devising efficient algorithms\nto locate a target object in a database equipped with a dissimilarity metric\nvia invocation of the weak comparison oracle. We propose two algorithms termed\nWORCS-I and WORCS-II (Weak-Oracle Comparison-based Search), which provably\nlocate the target object in a number of comparisons close to the entropy of the\ntarget distribution. While WORCS-I provides better theoretical guarantees,\nWORCS-II is applicable to more technically challenging scenarios where the\nalgorithm has limited access to the ranking dissimilarity between objects. A\nseries of experiments validate the performance of our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 02:57:25 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Kazemi", "Ehsan", ""], ["Chen", "Lin", ""], ["Dasgupta", "Sanjoy", ""], ["Karbasi", "Amin", ""]]}, {"id": "1802.06953", "submitter": "Weiming Feng", "authors": "Weiming Feng, Thomas P. Hayes, Yitong Yin", "title": "Distributed Symmetry Breaking in Sampling (Optimal Distributed Randomly\n  Coloring with Fewer Colors)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of almost-uniform sampling proper $q$-colorings of a\ngraph whose maximum degree is $\\Delta$. A famous result, discovered\nindependently by Jerrum(1995) and Salas and Sokal(1997), is that, assuming $q >\n(2+\\delta) \\Delta$, the Glauber dynamics (a.k.a. single-site dynamics) for this\nproblem has mixing time $O(n \\log n)$, where $n$ is the number of vertices, and\nthus provides a nearly linear time sampling algorithm for this problem. A\nnatural question is the extent to which this algorithm can be parallelized.\nPrevious work Feng, Sun and Yin [PODC'17] has shown that a $O(\\Delta \\log n)$\ntime parallelized algorithm is possible, and that $\\Omega(\\log n)$ time is\nnecessary.\n  We give a distributed sampling algorithm, which we call the Lazy Local\nMetropolis Algorithm, that achieves an optimal parallelization of this classic\nalgorithm. It improves its predecessor, the Local Metropolis algorithm of Feng,\nSun and Yin [PODC'17], by introducing a step of distributed symmetry breaking\nthat helps the mixing of the distributed sampling algorithm.\n  For sampling almost-uniform proper $q$-colorings of graphs $G$ on $n$\nvertices, we show that the Lazy Local Metropolis algorithm achieves an optimal\n$O(\\log n)$ mixing time if either of the following conditions is true for an\narbitrary constant $\\delta>0$:\n  $\\bullet$ $q\\ge(2+\\delta)\\Delta$, on general graphs with maximum degree\n$\\Delta$;\n  $\\bullet$ $q \\geq (\\alpha^* + \\delta)\\Delta$, where $\\alpha^* \\approx 1.763$\nsatisfies $\\alpha^* = \\mathrm{e}^{1/\\alpha^*}$, on graphs with sufficiently\nlarge maximum degree $\\Delta\\ge \\Delta_0(\\delta)$ and girth at least $9$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 03:52:39 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 14:02:58 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 14:38:41 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Feng", "Weiming", ""], ["Hayes", "Thomas P.", ""], ["Yin", "Yitong", ""]]}, {"id": "1802.06992", "submitter": "Samira Daruki", "authors": "Aditya Bhaskara, Samira Daruki, Suresh Venkatasubramanian", "title": "Sublinear Algorithms for MAXCUT and Correlation Clustering", "comments": "29 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sublinear algorithms for two fundamental graph problems, MAXCUT and\ncorrelation clustering. Our focus is on constructing core-sets as well as\ndeveloping streaming algorithms for these problems. Constant space algorithms\nare known for dense graphs for these problems, while $\\Omega(n)$ lower bounds\nexist (in the streaming setting) for sparse graphs.\n  Our goal in this paper is to bridge the gap between these extremes. Our first\nresult is to construct core-sets of size $\\tilde{O}(n^{1-\\delta})$ for both the\nproblems, on graphs with average degree $n^{\\delta}$ (for any $\\delta >0$).\nThis turns out to be optimal, under the exponential time hypothesis (ETH). Our\ncore-set analysis is based on studying random-induced sub-problems of\noptimization problems. To the best of our knowledge, all the known results in\nour parameter range rely crucially on near-regularity assumptions. We avoid\nthese by using a biased sampling approach, which we analyze using recent\nresults on concentration of quadratic functions. We then show that our\nconstruction yields a 2-pass streaming $(1+\\epsilon)$-approximation for both\nproblems; the algorithm uses $\\tilde{O}(n^{1-\\delta})$ space, for graphs of\naverage degree $n^\\delta$.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 07:23:28 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Daruki", "Samira", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1802.07041", "submitter": "Uri Zwick", "authors": "Haim Kaplan, L\\'aszl\\'o Kozma, Or Zamir, Uri Zwick", "title": "Selection from heaps, row-sorted matrices and $X+Y$ using soft heaps", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use soft heaps to obtain simpler optimal algorithms for selecting the\n$k$-th smallest item, and the set of~$k$ smallest items, from a heap-ordered\ntree, from a collection of sorted lists, and from $X+Y$, where $X$ and $Y$ are\ntwo unsorted sets. Our results match, and in some ways extend and improve,\nclassical results of Frederickson (1993) and Frederickson and Johnson (1982).\nIn particular, for selecting the $k$-th smallest item, or the set of~$k$\nsmallest items, from a collection of~$m$ sorted lists we obtain a new optimal\n\"output-sensitive\" algorithm that performs only $O(m+\\sum_{i=1}^m \\log(k_i+1))$\ncomparisons, where $k_i$ is the number of items of the $i$-th list that belong\nto the overall set of~$k$ smallest items.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 10:10:45 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Kaplan", "Haim", ""], ["Kozma", "L\u00e1szl\u00f3", ""], ["Zamir", "Or", ""], ["Zwick", "Uri", ""]]}, {"id": "1802.07073", "submitter": "Ilija Bogunovic", "authors": "Ilija Bogunovic, Junyao Zhao, Volkan Cevher", "title": "Robust Maximization of Non-Submodular Objectives", "comments": "Revision of Section 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a monotone set function subject to a\ncardinality constraint $k$ in the setting where some number of elements $\\tau$\nis deleted from the returned set. The focus of this work is on the worst-case\nadversarial setting. While there exist constant-factor guarantees when the\nfunction is submodular, there are no guarantees for non-submodular objectives.\nIn this work, we present a new algorithm Oblivious-Greedy and prove the first\nconstant-factor approximation guarantees for a wider class of non-submodular\nobjectives. The obtained theoretical bounds are the first constant-factor\nbounds that also hold in the linear regime, i.e. when the number of deletions\n$\\tau$ is linear in $k$. Our bounds depend on established parameters such as\nthe submodularity ratio and some novel ones such as the inverse curvature. We\nbound these parameters for two important objectives including support selection\nand variance reduction. Finally, we numerically demonstrate the robust\nperformance of Oblivious-Greedy for these two objectives on various datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:02:01 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 10:58:37 GMT"}, {"version": "v3", "created": "Sat, 2 May 2020 11:24:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bogunovic", "Ilija", ""], ["Zhao", "Junyao", ""], ["Cevher", "Volkan", ""]]}, {"id": "1802.07080", "submitter": "Lene M. Favrholdt", "authors": "Joan Boyar, Lene M. Favrholdt, Kim S. Larsen", "title": "Relative Worst-Order Analysis: A Survey", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative worst-order analysis is a technique for assessing the relative\nquality of online algorithms. We survey the most important results obtained\nwith this technique and compare it with other quality measures.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:21:32 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Boyar", "Joan", ""], ["Favrholdt", "Lene M.", ""], ["Larsen", "Kim S.", ""]]}, {"id": "1802.07090", "submitter": "R. Krithika", "authors": "R. Krithika, Abhishek Sahu, Saket Saurabh and Meirav Zehavi", "title": "The Parameterized Complexity of Packing Arc-Disjoint Cycles in\n  Tournaments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph $D$ on $n$ vertices and a positive integer $k$, the\nArc-Disjoint Cycle Packing problem is to determine whether $D$ has $k$\narc-disjoint cycles. This problem is known to be W[1]-hard in general directed\ngraphs. In this paper, we initiate a systematic study on the parameterized\ncomplexity of the problem restricted to tournaments. We show that the problem\nis fixed-parameter tractable and admits a polynomial kernel when parameterized\nby the solution size $k$. In particular, we show that it can be solved in\n$2^{\\mathcal{O}(k \\log k)} n^{\\mathcal{O}(1)}$ time and has a kernel with\n$\\mathcal{O}(k)$ vertices. The primary ingredient in both these results is a\nmin-max theorem that states that every tournament either contains $k$\narc-disjoint triangles or has a feedback arc set of size at most $6k$. Our\nbelief is that this combinatorial result is of independent interest and could\nbe useful in other problems related to cycles in tournaments.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 12:43:05 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Krithika", "R.", ""], ["Sahu", "Abhishek", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1802.07098", "submitter": "Ehsan Kazemi", "authors": "Moran Feldman and Amin Karbasi and Ehsan Kazemi", "title": "Do Less, Get More: Streaming Submodular Maximization with Subsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop the first one-pass streaming algorithm for\nsubmodular maximization that does not evaluate the entire stream even once. By\ncarefully subsampling each element of data stream, our algorithm enjoys the\ntightest approximation guarantees in various settings while having the smallest\nmemory footprint and requiring the lowest number of function evaluations. More\nspecifically, for a monotone submodular function and a $p$-matchoid constraint,\nour randomized algorithm achieves a $4p$ approximation ratio (in expectation)\nwith $O(k)$ memory and $O(km/p)$ queries per element ($k$ is the size of the\nlargest feasible solution and $m$ is the number of matroids used to define the\nconstraint). For the non-monotone case, our approximation ratio increases only\nslightly to $4p+2-o(1)$. To the best or our knowledge, our algorithm is the\nfirst that combines the benefits of streaming and subsampling in a novel way in\norder to truly scale submodular maximization to massive machine learning\nproblems. To showcase its practicality, we empirically evaluated the\nperformance of our algorithm on a video summarization application and observed\nthat it outperforms the state-of-the-art algorithm by up to fifty fold, while\nmaintaining practically the same utility.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 13:11:48 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Feldman", "Moran", ""], ["Karbasi", "Amin", ""], ["Kazemi", "Ehsan", ""]]}, {"id": "1802.07144", "submitter": "Alexander Noe", "authors": "Alexandra Henzinger, Alexander Noe, Christian Schulz", "title": "ILP-based Local Search for Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing high-quality graph partitions is a challenging problem with\nnumerous applications. In this paper, we present a novel meta-heuristic for the\nbalanced graph partitioning problem. Our approach is based on integer linear\nprograms that solve the partitioning problem to optimality. However, since\nthose programs typically do not scale to large inputs, we adapt them to\nheuristically improve a given partition. We do so by defining a much smaller\nmodel that allows us to use symmetry breaking and other techniques that make\nthe approach scalable. For example, in Walshaw's well-known benchmark tables we\nare able to improve roughly half of all entries when the number of blocks is\nhigh.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 15:11:23 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Henzinger", "Alexandra", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""]]}, {"id": "1802.07175", "submitter": "Stefan Kratsch", "authors": "Benjamin Burton and Sergio Cabello and Stefan Kratsch and William\n  Pettersson", "title": "The parameterized complexity of finding a 2-sphere in a simplicial\n  complex", "comments": "A preliminary version of this paper appeared in Proc. of 34th\n  Symposium on Theoretical Aspects of Computer Science (STACS 2017)", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2017.18", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a subcomplex K' of a simplicial complex K\nsuch that K' is homeomorphic to the 2-dimensional sphere, S^2. We study two\nvariants of this problem. The first asks if there exists such a K' with at most\nk triangles, and we show that this variant is W[1]-hard and, assuming ETH,\nadmits no O(n^{o(sqrt(k))}) time algorithm. We also give an algorithm that is\ntight with regards to this lower bound. The second problem is the dual of the\nfirst, and asks if K' can be found by removing at most k triangles from K. This\nvariant has an immediate O(3^k poly(|K|)) time algorithm, and we show that it\nadmits a polynomial kernelization to O(k^2) triangles, as well as a polynomial\ncompression to a weighted version with bit-size O(k log k).\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 16:15:11 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Burton", "Benjamin", ""], ["Cabello", "Sergio", ""], ["Kratsch", "Stefan", ""], ["Pettersson", "William", ""]]}, {"id": "1802.07177", "submitter": "Shay Solomon", "authors": "Shirel Attali and Merav Parter and David Peleg and Shay Solomon", "title": "Wireless Expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an extended notion of expansion suitable for radio\nnetworks. A graph $G=(V,E)$ is called an $(\\alpha_w, \\beta_w)$-{wireless\nexpander} if for every subset $S \\subseteq V$ s.t. $|S|\\leq \\alpha_w \\cdot\n|V|$, there exists a subset $S'\\subseteq S$ s.t. there are at least $\\beta_w\n\\cdot |S|$ vertices in $V\\backslash S$ adjacent in $G$ to exactly one vertex in\n$S'$. The main question we ask is the following: to what extent are ordinary\nexpanders also good {wireless} expanders? We answer this question in a nearly\ntight manner. On the positive side, we show that any $(\\alpha, \\beta)$-expander\nwith maximum degree $\\Delta$ and $\\beta\\geq 1/\\Delta$ is also a $(\\alpha_w,\n\\beta_w)$ wireless expander for $\\beta_w = \\Omega(\\beta / \\log (2 \\cdot\n\\min\\{\\Delta / \\beta, \\Delta \\cdot \\beta\\}))$. Thus the wireless expansion is\nsmaller than the ordinary expansion by at most a factor logarithmic in\n$\\min\\{\\Delta / \\beta, \\Delta \\cdot \\beta\\}$, which depends on the graph\n\\emph{average degree} rather than maximum degree; e.g., for low arboricity\ngraphs, the wireless expansion matches the ordinary expansion up to a constant.\nWe complement this positive result by presenting an explicit construction of a\n\"bad\" $(\\alpha, \\beta)$-expander for which the wireless expansion is $\\beta_w =\nO(\\beta / \\log (2 \\cdot \\min\\{\\Delta / \\beta, \\Delta \\cdot \\beta\\})$.\n  We also analyze the theoretical properties of wireless expanders and their\nconnection to unique neighbor expanders, and demonstrate their applicability:\nOur results yield improved bounds for the {spokesmen election problem} that was\nintroduced in the seminal paper of Chlamtac and Weinstein (1991) to devise\nefficient broadcasting for multihop radio networks. Our negative result yields\na significantly simpler proof than that from the seminal paper of Kushilevitz\nand Mansour (1998) for a lower bound on the broadcast time in radio networks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 16:17:30 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Attali", "Shirel", ""], ["Parter", "Merav", ""], ["Peleg", "David", ""], ["Solomon", "Shay", ""]]}, {"id": "1802.07209", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim and Victor Khazanov", "title": "Distributed Symmetry-Breaking Algorithms for Congested Cliques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {Congested Clique} is a distributed-computing model for single-hop\nnetworks with restricted bandwidth that has been very intensively studied\nrecently. It models a network by an $n$-vertex graph in which any pair of\nvertices can communicate one with another by transmitting $O(\\log n )$ bits in\neach round. Various problems have been studied in this setting, but for some of\nthem the best-known results are those for general networks. In this paper we\ndevise significantly improved algorithms for various symmetry-breaking\nproblems, such as forests-decompositions, vertex-colorings, and maximal\nindependent set.\n  We analyze the running time of our algorithms as a function of the arboricity\n$a$ of a clique subgraph that is given as input. Our algorithms are especially\nefficient in Trees, planar graphs, graphs with constant genus, and many other\ngraphs that have bounded arboricity, but unbounded size. We obtain\n$O(a)$-forest-decomposition algorithm with $O(\\log a)$ time that improves the\npreviously-known $O(\\log n)$ time, $O(a^{2 + \\epsilon})$-coloring in $O(\\log^*\nn)$ time that improves upon an $O(\\log n)$-time algorithm, $O(a)$-coloring in\n$O(a^{\\epsilon})$-time that improves upon several previous algorithms, and a\nmaximal independent set algorithm with $O(\\sqrt a)$ time that improves at least\nquadratically upon the state-of-the-art for small and moderate values of $a$.\n  Those results are achieved using several techniques. First, we produce a\nforest decomposition with a helpful structure called {$H$-partition} within\n$O(\\log a)$ rounds. In general graphs this structure requires $\\Theta(\\log n)$\ntime, but in Congested Cliques we are able to compute it faster. We employ this\nstructure in conjunction with partitioning techniques that allow us to solve\nvarious symmetry-breaking problems efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 17:20:16 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Barenboim", "Leonid", ""], ["Khazanov", "Victor", ""]]}, {"id": "1802.07229", "submitter": "Gautam Kamath", "authors": "Steve Hanneke, Adam Kalai, Gautam Kamath, Christos Tzamos", "title": "Actively Avoiding Nonsense in Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generative model may generate utter nonsense when it is fit to maximize the\nlikelihood of observed data. This happens due to \"model error,\" i.e., when the\ntrue data generating distribution does not fit within the class of generative\nmodels being learned. To address this, we propose a model of active\ndistribution learning using a binary invalidity oracle that identifies some\nexamples as clearly invalid, together with random positive examples sampled\nfrom the true distribution. The goal is to maximize the likelihood of the\npositive examples subject to the constraint of (almost) never generating\nexamples labeled invalid by the oracle. Guarantees are agnostic compared to a\nclass of probability distributions. We show that, while proper learning often\nrequires exponentially many queries to the invalidity oracle, improper\ndistribution learning can be done using polynomially many queries.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 18:08:53 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Hanneke", "Steve", ""], ["Kalai", "Adam", ""], ["Kamath", "Gautam", ""], ["Tzamos", "Christos", ""]]}, {"id": "1802.07301", "submitter": "Marco Mondelli", "authors": "Marco Mondelli and Andrea Montanari", "title": "On the Connection Between Learning Two-Layers Neural Networks and Tensor\n  Decomposition", "comments": "41 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish connections between the problem of learning a two-layer neural\nnetwork and tensor decomposition. We consider a model with feature vectors\n$\\boldsymbol x \\in \\mathbb R^d$, $r$ hidden units with weights $\\{\\boldsymbol\nw_i\\}_{1\\le i \\le r}$ and output $y\\in \\mathbb R$, i.e., $y=\\sum_{i=1}^r\n\\sigma( \\boldsymbol w_i^{\\mathsf T}\\boldsymbol x)$, with activation functions\ngiven by low-degree polynomials. In particular, if $\\sigma(x) =\na_0+a_1x+a_3x^3$, we prove that no polynomial-time learning algorithm can\noutperform the trivial predictor that assigns to each example the response\nvariable $\\mathbb E(y)$, when $d^{3/2}\\ll r\\ll d^2$. Our conclusion holds for a\n`natural data distribution', namely standard Gaussian feature vectors\n$\\boldsymbol x$, and output distributed according to a two-layer neural network\nwith random isotropic weights, and under a certain complexity-theoretic\nassumption on tensor decomposition. Roughly speaking, we assume that no\npolynomial-time algorithm can substantially outperform current methods for\ntensor decomposition based on the sum-of-squares hierarchy.\n  We also prove generalizations of this statement for higher degree polynomial\nactivations, and non-random weight vectors. Remarkably, several existing\nalgorithms for learning two-layer networks with rigorous guarantees are based\non tensor decomposition. Our results support the idea that this is indeed the\ncore computational difficulty in learning such networks, under the stated\ngenerative model for the data. As a side result, we show that under this model\nlearning the network requires accurate learning of its weights, a property that\ndoes not hold in a more general setting.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 19:40:32 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 21:05:02 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 10:34:16 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Mondelli", "Marco", ""], ["Montanari", "Andrea", ""]]}, {"id": "1802.07375", "submitter": "Samson Zhou", "authors": "Funda Erg\\\"un, Elena Grigorescu, Erfan Sadeqi Azer, Samson Zhou", "title": "Periodicity in Data Streams with Wildcards", "comments": "To appear at CSR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of detecting periodic trends within a string $S$\nof length $n$, arriving in the streaming model, containing at most $k$ wildcard\ncharacters, where $k=o(n)$. A wildcard character is a special character that\ncan be assigned any other character. We say $S$ has wildcard-period $p$ if\nthere exists an assignment to each of the wildcard characters so that in the\nresulting stream the length $n-p$ prefix equals the length $n-p$ suffix. We\npresent a two-pass streaming algorithm that computes wildcard-periods of $S$\nusing $\\mathcal{O}(k^3\\,\\mathsf{polylog}\\,n)$ bits of space, while we also show\nthat this problem cannot be solved in sublinear space in one pass. We then give\na one-pass randomized streaming algorithm that computes all wildcard-periods\n$p$ of $S$ with $p<\\frac{n}{2}$ and no wildcard characters appearing in the\nlast $p$ symbols of $S$, using $\\mathcal{O}(k^3\\mathsf{polylog}\\, n)$ space.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 23:33:23 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 16:30:03 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2018 02:55:57 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Erg\u00fcn", "Funda", ""], ["Grigorescu", "Elena", ""], ["Azer", "Erfan Sadeqi", ""], ["Zhou", "Samson", ""]]}, {"id": "1802.07382", "submitter": "Elad Tolochinsky", "authors": "Elad Tolochinsky, Dan Feldman", "title": "Generic Coreset for Scalable Learning of Monotonic Kernels: Logistic\n  Regression, Sigmoid and more", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coreset (or core-set) in this paper is a small weighted \\emph{subset} $Q$ of\nthe input set $P$ with respect to a given \\emph{monotonic} function\n$\\phi:\\REAL\\to\\REAL$ that \\emph{provably} approximates its fitting loss\n$\\sum_{p\\in P}f(p\\cdot x)$ to \\emph{any} given $x\\in\\REAL^d$. Using $Q$ we can\nobtain approximation of $x^*$ that minimizes this loss, by running\n\\emph{existing} optimization algorithms on $Q$. We provide: (I) a lower bound\nthat proves that there are sets with no coresets smaller than $n=|P|$ , (II) a\nproof that a small coreset of size near-logarithmic in $n$ exists for\n\\emph{any} input $P$, under natural assumption that holds e.g. for logistic\nregression and the sigmoid activation function. (III) a generic algorithm that\ncomputes $Q$ in $O(nd+n\\log n)$ expected time, (IV) extensive experimental\nresults with open code and benchmarks that show that the coresets are even\nsmaller in practice. Existing papers (e.g.[Huggins,Campbell,Broderick 2016])\nsuggested only specific coresets for specific input sets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 00:16:53 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 01:22:47 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Tolochinsky", "Elad", ""], ["Feldman", "Dan", ""]]}, {"id": "1802.07439", "submitter": "Jatin Batra", "authors": "Jatin Batra, Naveen Garg, Amit Kumar", "title": "Constant Factor Approximation Algorithm for Weighted Flow Time on a\n  Single Machine in Pseudo-polynomial time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the weighted flow-time problem on a single machine, we are given a set of\nn jobs, where each job has a processing requirement p_j, release date r_j and\nweight w_j. The goal is to find a preemptive schedule which minimizes the sum\nof weighted flow-time of jobs, where the flow-time of a job is the difference\nbetween its completion time and its released date. We give the first\npseudo-polynomial time constant approximation algorithm for this problem. The\nrunning time of our algorithm is polynomial in n, the number of jobs, and P,\nwhich is the ratio of the largest to the smallest processing requirement of a\njob. Our algorithm relies on a novel reduction of this problem to a\ngeneralization of the multi-cut problem on trees, which we call the Demand\nMulti-Cut problem. Even though we do not give a constant factor approximation\nalgorithm for the Demand Multi-Cut problem on trees, we show that the specific\ninstances of Demand Multi-Cut obtained by reduction from weighted flow-time\nproblem instances have more structure in them, and we are able to employ\ntechniques based on dynamic programming. Our dynamic programming algorithm\nrelies on showing that there are near optimal solutions which have nice\nsmoothness properties, and we exploit these properties to reduce the size of DP\ntable.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 06:36:26 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 06:44:00 GMT"}, {"version": "v3", "created": "Sun, 19 Aug 2018 10:35:48 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Batra", "Jatin", ""], ["Garg", "Naveen", ""], ["Kumar", "Amit", ""]]}, {"id": "1802.07440", "submitter": "Telikepalli Kavitha", "authors": "Telikepalli Kavitha", "title": "Max-size popular matchings and extensions", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the max-size popular matching problem in a roommates instance G =\n(V,E) with strict preference lists. A matching M is popular if there is no\nmatching M' in G such that the vertices that prefer M' to M outnumber those\nthat prefer M to M'. We show it is NP-hard to compute a max-size popular\nmatching in G. This is in contrast to the tractability of this problem in\nbipartite graphs where a max-size popular matching can be computed in linear\ntime. We define a subclass of max-size popular matchings called strongly\ndominant matchings and show a linear time algorithm to solve the strongly\ndominant matching problem in a roommates instance.\n  We consider a generalization of the max-size popular matching problem in\nbipartite graphs: this is the max-weight popular matching problem where there\nis also an edge weight function w and we seek a popular matching of largest\nweight. We show this is an NP-hard problem and this is so even when w(e) is\neither 1 or 2 for every edge e. We also show an algorithm with running time\nO*(2^{n/4}) to find a max-weight popular matching matching in G = (A U B,E)$ on\nn vertices.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 06:43:21 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Kavitha", "Telikepalli", ""]]}, {"id": "1802.07444", "submitter": "Chen Luo", "authors": "Chen Luo, Anshumali Shrivastava", "title": "Scaling-up Split-Merge MCMC with Locality Sensitive Sampling (LSS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential and\npopular variants of MCMC for problems when an MCMC state consists of an unknown\nnumber of components. It is well known that state-of-the-art methods for\nsplit-merge MCMC do not scale well. Strategies for rapid mixing requires smart\nand informative proposals to reduce the rejection rate. However, all known\nsmart proposals involve expensive operations to suggest informative\ntransitions. As a result, the cost of each iteration is prohibitive for massive\nscale datasets. It is further known that uninformative but computationally\nefficient proposals, such as random split-merge, leads to extremely slow\nconvergence. This tradeoff between mixing time and per update cost seems hard\nto get around.\n  In this paper, we show a sweet spot. We leverage some unique properties of\nweighted MinHash, which is a popular LSH, to design a novel class of\nsplit-merge proposals which are significantly more informative than random\nsampling but at the same time efficient to compute. Overall, we obtain a\nsuperior tradeoff between convergence and per update cost. As a direct\nconsequence, our proposals are around 6X faster than the state-of-the-art\nsampling methods on two large real datasets KDDCUP and PubMed with several\nmillions of entities and thousands of clusters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 07:03:32 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 20:36:49 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 05:06:40 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Luo", "Chen", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1802.07510", "submitter": "Andreas Loukas", "authors": "Andreas Loukas, Pierre Vandergheynst", "title": "Spectrally approximating large graphs with smaller graphs", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does coarsening affect the spectrum of a general graph? We provide\nconditions such that the principal eigenvalues and eigenspaces of a coarsened\nand original graph Laplacian matrices are close. The achieved approximation is\nshown to depend on standard graph-theoretic properties, such as the degree and\neigenvalue distributions, as well as on the ratio between the coarsened and\nactual graph sizes. Our results carry implications for learning methods that\nutilize coarsening. For the particular case of spectral clustering, they imply\nthat coarse eigenvectors can be used to derive good quality assignments even\nwithout refinement---this phenomenon was previously observed, but lacked formal\njustification.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 10:58:25 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Loukas", "Andreas", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1802.07515", "submitter": "Pijus Simonaitis", "authors": "Pijus Simonaitis, Annie Chateau, Krister M. Swenson", "title": "A framework for cost-constrained genome rearrangement under Double Cut\n  and Join", "comments": "A significantly improved version of this paper \"A General Framework\n  for Genome Rearrangement with Biological Constraints\" was published in RECOMB\n  International conference on Comparative Genomics 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of genome rearrangement has many flavours, but they all are somehow\ntied to edit distances on variations of a multi-graph called the breakpoint\ngraph. We study a weighted 2-break distance on Eulerian 2-edge-colored\nmulti-graphs, which generalizes weighted versions of several Double Cut and\nJoin problems, including those on genomes with unequal gene content. We affirm\nthe connection between cycle decompositions and edit scenarios first discovered\nwith the Sorting By Reversals problem. Using this we show that the problem of\nfinding a parsimonious scenario of minimum cost on an Eulerian 2-edge-colored\nmulti-graph - with a general cost function for 2-breaks - can be solved by\ndecomposing the problem into independent instances on simple alternating\ncycles. For breakpoint graphs, and a more constrained cost function, based on\ncoloring the vertices, we give a polynomial-time algorithm for finding a\nparsimonious 2-break scenario of minimum cost, while showing that finding a\nnon-parsimonious 2-break scenario of minimum cost is NP-Hard.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 11:12:45 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 09:55:12 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Simonaitis", "Pijus", ""], ["Chateau", "Annie", ""], ["Swenson", "Krister M.", ""]]}, {"id": "1802.07600", "submitter": "Danny Hucke", "authors": "Moses Ganardi, Danny Hucke, Markus Lohrey", "title": "Randomized sliding window algorithms for regular languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sliding window algorithm receives a stream of symbols and has to output at\neach time instant a certain value which only depends on the last $n$ symbols.\nIf the algorithm is randomized, then at each time instant it produces an\nincorrect output with probability at most $\\epsilon$, which is a constant error\nbound. This work proposes a more relaxed definition of correctness which is\nparameterized by the error bound $\\epsilon$ and the failure ratio $\\phi$: A\nrandomized sliding window algorithm is required to err with probability at most\n$\\epsilon$ at a portion of $1-\\phi$ of all time instants of an input stream.\nThis work continues the investigation of sliding window algorithms for regular\nlanguages. In previous works a trichotomy theorem was shown for deterministic\nalgorithms: the optimal space complexity is either constant, logarithmic or\nlinear in the window size. The main results of this paper concerns three\nnatural settings (randomized algorithms with failure ratio zero and\nrandomized/deterministic algorithms with bounded failure ratio) and provide\nnatural language theoretic characterizations of the space complexity classes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:58:10 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Ganardi", "Moses", ""], ["Hucke", "Danny", ""], ["Lohrey", "Markus", ""]]}, {"id": "1802.07632", "submitter": "Davis Issac", "authors": "L. Sunil Chandran and Yun Kuen Cheung and Davis Issac", "title": "Spanning Tree Congestion and Computation of Generalized\n  Gy\\H{o}ri-Lov\\'{a}sz Partition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a natural problem in graph sparsification, the Spanning Tree\nCongestion (\\STC) problem. Informally, the \\STC problem seeks a spanning tree\nwith no tree-edge \\emph{routing} too many of the original edges. The root of\nthis problem dates back to at least 30 years ago, motivated by applications in\nnetwork design, parallel computing and circuit design. Variants of the problem\nhave also seen algorithmic applications as a preprocessing step of several\nimportant graph algorithms.\n  For any general connected graph with $n$ vertices and $m$ edges, we show that\nits STC is at most $\\mathcal{O}(\\sqrt{mn})$, which is asymptotically optimal\nsince we also demonstrate graphs with STC at least $\\Omega(\\sqrt{mn})$. We\npresent a polynomial-time algorithm which computes a spanning tree with\ncongestion $\\mathcal{O}(\\sqrt{mn}\\cdot \\log n)$. We also present another\nalgorithm for computing a spanning tree with congestion\n$\\mathcal{O}(\\sqrt{mn})$; this algorithm runs in sub-exponential time when $m =\n\\omega(n \\log^2 n)$.\n  For achieving the above results, an important intermediate theorem is\n\\emph{generalized Gy\\H{o}ri-Lov\\'{a}sz theorem}, for which Chen et al. gave a\nnon-constructive proof. We give the first elementary and constructive proof by\nproviding a local search algorithm with running time $\\mathcal{O}^*\\left( 4^n\n\\right)$, which is a key ingredient of the above-mentioned sub-exponential time\nalgorithm. We discuss a few consequences of the theorem concerning graph\npartitioning, which might be of independent interest.\n  We also show that for any graph which satisfies certain \\emph{expanding\nproperties}, its STC is at most $\\mathcal{O}(n)$, and a corresponding spanning\ntree can be computed in polynomial time. We then use this to show that a random\ngraph has STC $\\Theta(n)$ with high probability.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 16:03:46 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 12:49:46 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Chandran", "L. Sunil", ""], ["Cheung", "Yun Kuen", ""], ["Issac", "Davis", ""]]}, {"id": "1802.07647", "submitter": "Christian Konrad", "authors": "Christian Konrad", "title": "MIS in the Congested Clique Model in $O(\\log \\log \\Delta)$ Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a maximal independent set (MIS) algorithm that runs in $O(\\log \\log\n\\Delta)$ rounds in the congested clique model, where $\\Delta$ is the maximum\ndegree of the input graph. This improves upon the $O(\\frac{\\log(\\Delta) \\cdot\n\\log \\log \\Delta}{\\sqrt{\\log n}} + \\log \\log \\Delta )$ rounds algorithm of\n[Ghaffari, PODC '17], where $n$ is the number of vertices of the input graph.\n  In the first stage of our algorithm, we simulate the first\n$O(\\frac{n}{\\text{poly} \\log n})$ iterations of the sequential random order\nGreedy algorithm for MIS in the congested clique model in $O(\\log \\log \\Delta)$\nrounds. This thins out the input graph relatively quickly: After this stage,\nthe maximum degree of the residual graph is poly-logarithmic. In the second\nstage, we run the MIS algorithm of [Ghaffari, PODC '17] on the residual graph,\nwhich completes in $O(\\log \\log \\Delta)$ rounds on graphs of poly-logarithmic\ndegree.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 16:21:34 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Konrad", "Christian", ""]]}, {"id": "1802.07684", "submitter": "Konrad Simon Ph.D.", "authors": "Konrad Simon and J\\\"orn Behrens", "title": "Multiscale finite elements through advection-induced coordinates for\n  transient advection-diffusion equations", "comments": "26 pages, 13 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Long simulation times in climate sciences typically require coarse grids due\nto computational constraints. Nonetheless, unresolved subscale information\nsignificantly influences the prognostic variables and can not be neglected for\nreliable long term simulations. This is typically done via parametrizations but\ntheir coupling to the coarse grid variables often involves simple heuristics.\nWe explore a novel up-scaling approach inspired by multi-scale finite element\nmethods. These methods are well established in porous media applications, where\nmostly stationary or quasi stationary situations prevail. In\nadvection-dominated problems arising in climate simulations the approach needs\nto be adjusted. We do so by performing coordinate transforms that make the\neffect of transport milder in the vicinity of coarse element boundaries. The\nidea of our method is quite general and we demonstrate it as a proof-of-concept\non a one-dimensional passive advection-diffusion equation with oscillatory\nbackground velocity and diffusion.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 17:28:16 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Simon", "Konrad", ""], ["Behrens", "J\u00f6rn", ""]]}, {"id": "1802.07863", "submitter": "Kazuhiro Kurita", "authors": "Kazuhiro Kurita, Kunihiro Wasa, Hiroki Arimura, and Takeaki Uno", "title": "Efficient Enumeration of Dominating Sets for Sparse Graphs", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ISAAC.2018.8", "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A dominating set $D$ of a graph $G$ is a set of vertices such that any vertex\nin $G$ is in $D$ or its neighbor is in $D$. Enumeration of minimal dominating\nsets in a graph is one of central problems in enumeration study since\nenumeration of minimal dominating sets corresponds to enumeration of minimal\nhypergraph transversal. However, enumeration of dominating sets including\nnon-minimal ones has not been received much attention. In this paper, we\naddress enumeration problems for dominating sets from sparse graphs which are\ndegenerate graphs and graphs with large girth, and we propose two algorithms\nfor solving the problems. The first algorithm enumerates all the dominating\nsets for a $k$-degenerate graph in $O(k)$ time per solution using $O(n + m)$\nspace, where $n$ and $m$ are respectively the number of vertices and edges in\nan input graph. That is, the algorithm is optimal for graphs with constant\ndegeneracy such as trees, planar graphs, $H$-minor free graphs with some fixed\n$H$. The second algorithm enumerates all the dominating sets in constant time\nper solution for input graphs with girth at least nine.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 00:56:53 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 11:54:37 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 16:12:24 GMT"}, {"version": "v4", "created": "Sat, 29 Sep 2018 03:54:10 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Kurita", "Kazuhiro", ""], ["Wasa", "Kunihiro", ""], ["Arimura", "Hiroki", ""], ["Uno", "Takeaki", ""]]}, {"id": "1802.07932", "submitter": "David Harvey", "authors": "David Harvey and Joris van der Hoeven", "title": "Faster integer multiplication using short lattice vectors", "comments": "16 pages", "journal-ref": "Open Book Series 2 (2019) 293-310", "doi": "10.2140/obs.2019.2.293", "report-no": null, "categories": "cs.SC cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that $n$-bit integers may be multiplied in $O(n \\log n \\, 4^{\\log^*\nn})$ bit operations. This complexity bound had been achieved previously by\nseveral authors, assuming various unproved number-theoretic hypotheses. Our\nproof is unconditional, and depends in an essential way on Minkowski's theorem\nconcerning lattice vectors in symmetric convex sets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 08:01:59 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Harvey", "David", ""], ["van der Hoeven", "Joris", ""]]}, {"id": "1802.07944", "submitter": "Anthony Labarre", "authors": "Laurent Bulteau and Danny Hermelin and Anthony Labarre and St\\'ephane\n  Vialette", "title": "The Clever Shopper Problem", "comments": "15 pages, 3 figures, to appear at the 13th International Computer\n  Science Symposium in Russia (CSR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a variant of the so-called \"Internet Shopping Problem\"\nintroduced by Blazewicz et al. (2010), where a customer wants to buy a list of\nproducts at the lowest possible total cost from shops which offer discounts\nwhen purchases exceed a certain threshold. Although the problem is NP-hard, we\nprovide exact algorithms for several cases, e.g. when each shop sells only two\nitems, and an FPT algorithm for the number of items, or for the number of shops\nwhen all prices are equal. We complement each result with hardness proofs in\norder to draw a tight boundary between tractable and intractable cases.\nFinally, we give an approximation algorithm and hardness results for the\nproblem of maximising the sum of discounts.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 08:58:30 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Bulteau", "Laurent", ""], ["Hermelin", "Danny", ""], ["Labarre", "Anthony", ""], ["Vialette", "St\u00e9phane", ""]]}, {"id": "1802.07967", "submitter": "Ofer Neiman", "authors": "Michael Elkin and Ofer Neiman", "title": "Near Isometric Terminal Embeddings for Doubling Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a metric space $(X,d)$, a set of terminals $K\\subseteq X$, and a\nparameter $t\\ge 1$, we consider metric structures (e.g., spanners, distance\noracles, embedding into normed spaces) that preserve distances for all pairs in\n$K\\times X$ up to a factor of $t$, and have small size (e.g. number of edges\nfor spanners, dimension for embeddings). While such terminal (aka source-wise)\nmetric structures are known to exist in several settings, no terminal spanner\nor embedding with distortion close to 1, i.e., $t=1+\\epsilon$ for some small\n$0<\\epsilon<1$, is currently known.\n  Here we devise such terminal metric structures for {\\em doubling} metrics,\nand show that essentially any metric structure with distortion $1+\\epsilon$ and\nsize $s(|X|)$ has its terminal counterpart, with distortion $1+O(\\epsilon)$ and\nsize $s(|K|)+1$. In particular, for any doubling metric on $n$ points, a set of\n$k=o(n)$ terminals, and constant $0<\\epsilon<1$, there exists:\n  (1) A spanner with stretch $1+\\epsilon$ for pairs in $K\\times X$, with\n$n+o(n)$ edges.\n  (2) A labeling scheme with stretch $1+\\epsilon$ for pairs in $K\\times X$,\nwith label size $\\approx \\log k$.\n  (3) An embedding into $\\ell_\\infty^d$ with distortion $1+\\epsilon$ for pairs\nin $K\\times X$, where $d=O(\\log k)$.\n  Moreover, surprisingly, the last two results apply if only $K$ is a doubling\nmetric, while $X$ can be arbitrary.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 10:26:17 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Elkin", "Michael", ""], ["Neiman", "Ofer", ""]]}, {"id": "1802.08014", "submitter": "Bin Yang", "authors": "Huiping Liu, Cheqing Jin, Bin Yang, Aoying Zhou", "title": "Finding Top-k Optimal Sequenced Routes -- Full Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by many practical applications in logistics and\nmobility-as-a-service, we study the top-k optimal sequenced routes (KOSR)\nquerying on large, general graphs where the edge weights may not satisfy the\ntriangle inequality, e.g., road network graphs with travel times as edge\nweights. The KOSR querying strives to find the top-k optimal routes (i.e., with\nthe top-k minimal total costs) from a given source to a given destination,\nwhich must visit a number of vertices with specific vertex categories (e.g.,\ngas stations, restaurants, and shopping malls) in a particular order (e.g.,\nvisiting gas stations before restaurants and then shopping malls).\n  To efficiently find the top-k optimal sequenced routes, we propose two\nalgorithms PruningKOSR and StarKOSR. In PruningKOSR, we define a dominance\nrelationship between two partially-explored routes. The partially-explored\nroutes that can be dominated by other partially-explored routes are postponed\nbeing extended, which leads to a smaller searching space and thus improves\nefficiency. In StarKOSR, we further improve the efficiency by extending routes\nin an A* manner. With the help of a judiciously designed heuristic estimation\nthat works for general graphs, the cost of partially explored routes to the\ndestination can be estimated such that the qualified complete routes can be\nfound early. In addition, we demonstrate the high extensibility of the proposed\nalgorithms by incorporating Hop Labeling, an effective label indexing technique\nfor shortest path queries, to further improve efficiency. Extensive experiments\non multiple real-world graphs demonstrate that the proposed methods\nsignificantly outperform the baseline method. Furthermore, when k=1, StarKOSR\nalso outperforms the state-of-the-art method for the optimal sequenced route\nqueries.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 12:46:07 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Liu", "Huiping", ""], ["Jin", "Cheqing", ""], ["Yang", "Bin", ""], ["Zhou", "Aoying", ""]]}, {"id": "1802.08183", "submitter": "Lin Chen", "authors": "Lin Chen, Christopher Harshaw, Hamed Hassani, Amin Karbasi", "title": "Projection-Free Online Optimization with Stochastic Gradient: From\n  Convexity to Submodularity", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online optimization has been a successful framework for solving large-scale\nproblems under computational constraints and partial information. Current\nmethods for online convex optimization require either a projection or exact\ngradient computation at each step, both of which can be prohibitively expensive\nfor large-scale applications. At the same time, there is a growing trend of\nnon-convex optimization in machine learning community and a need for online\nmethods. Continuous DR-submodular functions, which exhibit a natural\ndiminishing returns condition, have recently been proposed as a broad class of\nnon-convex functions which may be efficiently optimized. Although online\nmethods have been introduced, they suffer from similar problems. In this work,\nwe propose Meta-Frank-Wolfe, the first online projection-free algorithm that\nuses stochastic gradient estimates. The algorithm relies on a careful sampling\nof gradients in each round and achieves the optimal $O( \\sqrt{T})$ adversarial\nregret bounds for convex and continuous submodular optimization. We also\npropose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single\nstochastic gradient estimate in each round and achieves an $O(T^{2/3})$\nstochastic regret bound for convex and continuous submodular optimization. We\napply our methods to develop a novel \"lifting\" framework for the online\ndiscrete submodular maximization and also see that they outperform current\nstate-of-the-art techniques on various experiments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 17:13:36 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 21:53:04 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 00:36:28 GMT"}, {"version": "v4", "created": "Thu, 14 Jun 2018 01:10:34 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Chen", "Lin", ""], ["Harshaw", "Christopher", ""], ["Hassani", "Hamed", ""], ["Karbasi", "Amin", ""]]}, {"id": "1802.08227", "submitter": "Danial Dervovic", "authors": "Danial Dervovic, Mark Herbster, Peter Mountney, Simone Severini,\n  Na\\\"iri Usher, Leonard Wossnig", "title": "Quantum linear systems algorithms: a primer", "comments": "55 pages, 5 figures, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Harrow-Hassidim-Lloyd (HHL) quantum algorithm for sampling from the\nsolution of a linear system provides an exponential speed-up over its classical\ncounterpart. The problem of solving a system of linear equations has a wide\nscope of applications, and thus HHL constitutes an important algorithmic\nprimitive. In these notes, we present the HHL algorithm and its improved\nversions in detail, including explanations of the constituent sub- routines.\nMore specifically, we discuss various quantum subroutines such as quantum phase\nestimation and amplitude amplification, as well as the important question of\nloading data into a quantum computer, via quantum RAM. The improvements to the\noriginal algorithm exploit variable-time amplitude amplification as well as a\nmethod for implementing linear combinations of unitary operations (LCUs) based\non a decomposition of the operators using Fourier and Chebyshev series.\nFinally, we discuss a linear solver based on the quantum singular value\nestimation (QSVE) subroutine.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:29:21 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Dervovic", "Danial", ""], ["Herbster", "Mark", ""], ["Mountney", "Peter", ""], ["Severini", "Simone", ""], ["Usher", "Na\u00efri", ""], ["Wossnig", "Leonard", ""]]}, {"id": "1802.08237", "submitter": "Slobodan Mitrovi\\'c", "authors": "Mohsen Ghaffari, Themis Gouleakis, Christian Konrad, Slobodan\n  Mitrovi\\'c, Ronitt Rubinfeld", "title": "Improved Massively Parallel Computation Algorithms for MIS, Matching,\n  and Vertex Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present $O(\\log\\log n)$-round algorithms in the Massively Parallel\nComputation (MPC) model, with $\\tilde{O}(n)$ memory per machine, that compute a\nmaximal independent set, a $1+\\epsilon$ approximation of maximum matching, and\na $2+\\epsilon$ approximation of minimum vertex cover, for any $n$-vertex graph\nand any constant $\\epsilon>0$. These improve the state of the art as follows:\n  - Our MIS algorithm leads to a simple $O(\\log\\log \\Delta)$-round MIS\nalgorithm in the Congested Clique model of distributed computing, which\nimproves on the $\\tilde{O}(\\sqrt{\\log \\Delta})$-round algorithm of Ghaffari\n[PODC'17].\n  - Our $O(\\log\\log n)$-round $(1+\\epsilon)$-approximate maximum matching\nalgorithm simplifies or improves on the following prior work: $O(\\log^2\\log\nn)$-round $(1+\\epsilon)$-approximation algorithm of Czumaj et al. [STOC'18] and\n$O(\\log\\log n)$-round $(1+\\epsilon)$-approximation algorithm of Assadi et al.\n[SODA'19].\n  - Our $O(\\log\\log n)$-round $(2+\\epsilon)$-approximate minimum vertex cover\nalgorithm improves on an $O(\\log\\log n)$-round $O(1)$-approximation of Assadi\net al. [arXiv'17].\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:48:48 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 18:41:27 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 17:35:25 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Gouleakis", "Themis", ""], ["Konrad", "Christian", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1802.08252", "submitter": "Jeremy Reizenstein", "authors": "Jeremy Reizenstein, Benjamin Graham", "title": "The iisignature library: efficient calculation of iterated-integral\n  signatures and log signatures", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterated-integral signatures and log signatures are vectors calculated from a\npath that characterise its shape. They come from the theory of differential\nequations driven by rough paths, and also have applications in statistics and\nmachine learning. We present algorithms for efficiently calculating these\nsignatures, and benchmark their performance. We release the methods as a Python\npackage.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 14:29:30 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Reizenstein", "Jeremy", ""], ["Graham", "Benjamin", ""]]}, {"id": "1802.08318", "submitter": "Uthaipon Tantipongpipat", "authors": "Aleksandar Nikolov and Mohit Singh and Uthaipon Tao Tantipongpipat", "title": "Proportional Volume Sampling and Approximation Algorithms for A-Optimal\n  Design", "comments": "Add that proportional volume sampling also solves D-optimal and\n  generalized ratio problem. Add some reference from last version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal design problems where the goal is to choose a set of\nlinear measurements to obtain the most accurate estimate of an unknown vector\nin $d$ dimensions. We study the $A$-optimal design variant where the objective\nis to minimize the average variance of the error in the maximum likelihood\nestimate of the vector being measured. The problem also finds applications in\nsensor placement in wireless networks, sparse least squares regression, feature\nselection for $k$-means clustering, and matrix approximation. In this paper, we\nintroduce proportional volume sampling to obtain improved approximation\nalgorithms for $A$-optimal design. Our main result is to obtain improved\napproximation algorithms for the $A$-optimal design problem by introducing the\nproportional volume sampling algorithm. Our results nearly optimal bounds in\nthe asymptotic regime when the number of measurements done, $k$, is\nsignificantly more than the dimension $d$. We also give first approximation\nalgorithms when $k$ is small including when $k=d$. The proportional\nvolume-sampling algorithm also gives approximation algorithms for other optimal\ndesign objectives such as $D$-optimal design and generalized ratio objective\nmatching or improving previous best known results. Interestingly, we show that\na similar guarantee cannot be obtained for the $E$-optimal design problem. We\nalso show that the $A$-optimal design problem is NP-hard to approximate within\na fixed constant when $k=d$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 22:02:42 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 21:17:45 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 14:45:10 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2018 19:05:11 GMT"}, {"version": "v5", "created": "Tue, 17 Jul 2018 15:10:52 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Nikolov", "Aleksandar", ""], ["Singh", "Mohit", ""], ["Tantipongpipat", "Uthaipon Tao", ""]]}, {"id": "1802.08372", "submitter": "Weijun Xie", "authors": "Mohit Singh and Weijun Xie", "title": "Approximation Algorithms for D-optimal Design", "comments": "34 pages, accepted by Mathematics of Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental design is a classical statistics problem and its aim is to\nestimate an unknown $m$-dimensional vector $\\beta$ from linear measurements\nwhere a Gaussian noise is introduced in each measurement. For the combinatorial\nexperimental design problem, the goal is to pick $k$ out of the given $n$\nexperiments so as to make the most accurate estimate of the unknown parameters,\ndenoted as $\\hat{\\beta}$. In this paper, we will study one of the most robust\nmeasures of error estimation - $D$-optimality criterion, which corresponds to\nminimizing the volume of the confidence ellipsoid for the estimation error\n$\\beta-\\hat{\\beta}$. The problem gives rise to two natural variants depending\non whether repetitions of experiments are allowed or not. We first propose an\napproximation algorithm with a $\\frac1e$-approximation for the $D$-optimal\ndesign problem with and without repetitions, giving the first constant factor\napproximation for the problem. We then analyze another sampling approximation\nalgorithm and prove that it is $(1-\\epsilon)$-approximation if $k\\geq\n\\frac{4m}{\\epsilon}+\\frac{12}{\\epsilon^2}\\log(\\frac{1}{\\epsilon})$ for any\n$\\epsilon \\in (0,1)$. Finally, for $D$-optimal design with repetitions, we\nstudy a different algorithm proposed by literature and show that it can improve\nthis asymptotic approximation ratio.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 02:48:16 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 14:27:44 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Singh", "Mohit", ""], ["Xie", "Weijun", ""]]}, {"id": "1802.08509", "submitter": "Gaurav Rattan", "authors": "Martin Grohe, Gaurav Rattan, Gerhard J. Woeginger", "title": "Graph Similarity and Approximate Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph similarity problem, also known as approximate graph isomorphism or\ngraph matching problem, has been extensively studied in the machine learning\ncommunity, but has not received much attention in the algorithms community:\nGiven two graphs $G,H$ of the same order $n$ with adjacency matrices $A_G,A_H$,\na well-studied measure of similarity is the Frobenius distance \\[\n\\mathrm{dist}(G,H):=\\min_{\\pi}\\|A_G^\\pi-A_H\\|_F, \\] where $\\pi$ ranges over all\npermutations of the vertex set of $G$, where $A_G^\\pi$ denotes the matrix\nobtained from $A_G$ by permuting rows and columns according to $\\pi$, and where\n$\\|M\\|_F$ is the Frobenius norm of a matrix $M$. The (weighted) graph\nsimilarity problem, denoted by SIM (WSIM), is the problem of computing this\ndistance for two graphs of same order. This problem is closely related to the\nnotoriously hard quadratic assignment problem (QAP), which is known to be\nNP-hard even for severely restricted cases.\n  It is known that SIM (WSIM) is NP-hard; we strengthen this hardness result by\nshowing that the problem remains NP-hard even for the class of trees.\nIdentifying the boundary of tractability for WSIM is best done in the framework\nof linear algebra. We show that WSIM is NP-hard as long as one of the matrices\nhas unbounded rank or negative eigenvalues: hence, the realm of tractability is\nrestricted to positive semi-definite matrices of bounded rank. Our main result\nis a polynomial time algorithm for the special case where one of the matrices\nhas a bounded clustering number, a parameter arising from spectral graph\ndrawing techniques.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 12:54:40 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Grohe", "Martin", ""], ["Rattan", "Gaurav", ""], ["Woeginger", "Gerhard J.", ""]]}, {"id": "1802.08513", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Jerry Li and Ludwig Schmidt", "title": "Fast and Sample Near-Optimal Algorithms for Learning Multidimensional\n  Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of robustly learning multi-dimensional histograms. A\n$d$-dimensional function $h: D \\rightarrow \\mathbb{R}$ is called a\n$k$-histogram if there exists a partition of the domain $D \\subseteq\n\\mathbb{R}^d$ into $k$ axis-aligned rectangles such that $h$ is constant within\neach such rectangle. Let $f: D \\rightarrow \\mathbb{R}$ be a $d$-dimensional\nprobability density function and suppose that $f$ is $\\mathrm{OPT}$-close, in\n$L_1$-distance, to an unknown $k$-histogram (with unknown partition). Our goal\nis to output a hypothesis that is $O(\\mathrm{OPT}) + \\epsilon$ close to $f$, in\n$L_1$-distance. We give an algorithm for this learning problem that uses $n =\n\\tilde{O}_d(k/\\epsilon^2)$ samples and runs in time $\\tilde{O}_d(n)$. For any\nfixed dimension, our algorithm has optimal sample complexity, up to logarithmic\nfactors, and runs in near-linear time. Prior to our work, the time complexity\nof the $d=1$ case was well-understood, but significant gaps in our\nunderstanding remained even for $d=2$.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 13:07:32 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Li", "Jerry", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1802.08563", "submitter": "Andreas Emil Feldmann", "authors": "Andreas Emil Feldmann and Daniel Marx", "title": "The Parameterized Hardness of the k-Center Problem in Transportation\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the hardness of the $k$-Center problem on inputs that\nmodel transportation networks. For the problem, a graph $G=(V,E)$ with edge\nlengths and an integer $k$ are given and a center set $C\\subseteq V$ needs to\nbe chosen such that $|C|\\leq k$. The aim is to minimize the maximum distance of\nany vertex in the graph to the closest center. This problem arises in many\napplications of logistics, and thus it is natural to consider inputs that model\ntransportation networks. Such inputs are often assumed to be planar graphs, low\ndoubling metrics, or bounded highway dimension graphs. For each of these\nmodels, parameterized approximation algorithms have been shown to exist. We\ncomplement these results by proving that the $k$-Center problem is W[1]-hard on\nplanar graphs of constant doubling dimension, where the parameter is the\ncombination of the number of centers $k$, the highway dimension $h$, and the\npathwidth $p$. Moreover, under the Exponential Time Hypothesis there is no\n$f(k,p,h)\\cdot n^{o(p+\\sqrt{k+h})}$ time algorithm for any computable function\n$f$. Thus it is unlikely that the optimum solution to $k$-Center can be found\nefficiently, even when assuming that the input graph abides to all of the above\nmodels for transportation networks at once!\n  Additionally we give a simple parameterized $(1+\\varepsilon)$-approximation\nalgorithm for inputs of doubling dimension $d$ with runtime\n$(k^k/\\varepsilon^{O(kd)})\\cdot n^{O(1)}$. This generalizes a previous result,\nwhich considered inputs in $D$-dimensional $L_q$ metrics.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 14:47:33 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 12:18:52 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 09:02:59 GMT"}, {"version": "v4", "created": "Mon, 2 Mar 2020 09:55:42 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Feldmann", "Andreas Emil", ""], ["Marx", "Daniel", ""]]}, {"id": "1802.08577", "submitter": "Stefan Lendl", "authors": "Ante \\'Custi\\'c and Stefan Lendl", "title": "On Streaming Algorithms for the Steiner Cycle and Path Cover Problem on\n  Interval Graphs and Falling Platforms in Video Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simplified model for platform game levels with falling\nplatforms based on interval graphs and show that solvability of such levels\ncorresponds to finding Steiner cycles or Steiner paths in the corresponding\ngraphs. Linear time algorithms are obtained for both of these problems. We also\nstudy these algorithms as streaming algorithms and analyze the necessary memory\nwith respect to the maximum number of intervals contained in another interval.\nThis corresponds to understanding which parts of a level have to be visible at\neach point to allow the player to make optimal deterministic decisions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:03:19 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["\u0106usti\u0107", "Ante", ""], ["Lendl", "Stefan", ""]]}, {"id": "1802.08637", "submitter": "Carlos Cardonha", "authors": "David Bergman, Merve Bodur, Carlos Cardonha, Andre A. Cire", "title": "Network Models for Multiobjective Discrete Optimization", "comments": "32 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a novel framework for solving multiobjective discrete\noptimization problems with an arbitrary number of objectives. Our framework\nformulates these problems as network models, in that enumerating the Pareto\nfrontier amounts to solving a multicriteria shortest path problem in an\nauxiliary network. We design techniques for exploiting the network model in\norder to accelerate the identification of the Pareto frontier, most notably a\nnumber of operations to simplify the network by removing nodes and arcs while\npreserving the set of nondominated solutions. We show that the proposed\nframework yields orders-of-magnitude performance improvements over existing\nstate-of-the-art algorithms on five problem classes containing both linear and\nnonlinear objective functions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 16:55:55 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 02:50:20 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Bergman", "David", ""], ["Bodur", "Merve", ""], ["Cardonha", "Carlos", ""], ["Cire", "Andre A.", ""]]}, {"id": "1802.08656", "submitter": "Angela Wuu", "authors": "Angela Wuu", "title": "Homomorphism Extension", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We define the Homomorphism Extension (HomExt) problem: given a group $G$, a\nsubgroup $M \\leq G$ and a homomorphism $\\varphi: M \\to H$, decide whether or\nnot there exists a homomorphism $\\widetilde{\\varphi}: G\\to H$ extending\n$\\varphi$, i.e., $\\widetilde{\\varphi}|_M = \\varphi$. This problem arose in the\ncontext of list-decoding homomorphism codes but is also of independent\ninterest, both as a problem in computational group theory and as a new and\nnatural problem in NP of unsettled complexity status.\n  We consider the case $H=S_m$ (the symmetric group of degree $m$), i.e.,\n$\\varphi : G \\to H$ is a $G$-action on a set of $m$ elements. We assume $G\\le\nS_n$ is given as a permutation group by a list of generators. We characterize\nthe equivalence classes of extensions in terms of a multidimensional oracle\nsubset-sum problem. From this we infer that for bounded $G$ the HomExt problem\ncan be solved in polynomial time.\n  Our main result concerns the case $G=A_n$ (the alternating group of degree\n$n$) for variable $n$ under the assumption that the index of $M$ in $G$ is\nbounded by poly$(n)$. We solve this case in polynomial time for all $m <\n2^{n-1}/\\sqrt{n}$. This is the case with direct relevance to homomorphism codes\n(Babai, Black, and Wuu, arXiv 2018); it is used as a component of one of the\nmain algorithms in that paper.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 17:45:35 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 19:49:37 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Wuu", "Angela", ""]]}, {"id": "1802.08663", "submitter": "Amirbehshad Shahrasbi", "authors": "Bernhard Haeupler, Amirbehshad Shahrasbi and Madhu Sudan", "title": "Synchronization Strings: List Decoding for Insertions and Deletions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study codes that are list-decodable under insertions and deletions.\nSpecifically, we consider the setting where a codeword over some finite\nalphabet of size $q$ may suffer from $\\delta$ fraction of adversarial deletions\nand $\\gamma$ fraction of adversarial insertions. A code is said to be\n$L$-list-decodable if there is an (efficient) algorithm that, given a received\nword, reports a list of $L$ codewords that include the original codeword.\n  Using the concept of synchronization strings, introduced by the first two\nauthors [STOC 2017], we show some surprising results. We show that for every\n$0\\leq\\delta<1$, every $0\\leq\\gamma<\\infty$ and every $\\epsilon>0$ there exist\nefficient codes of rate $1-\\delta-\\epsilon$ and constant alphabet (so\n$q=O_{\\delta,\\gamma,\\epsilon}(1)$) and sub-logarithmic list sizes. We stress\nthat the fraction of insertions can be arbitrarily large and the rate is\nindependent of this parameter. Our result sheds light on the remarkable\nasymmetry between the impact of insertions and deletions from the point of view\nof error-correction: Whereas deletions cost in the rate of the code, insertion\ncosts are borne by the adversary and not the code!\n  We also prove several tight bounds on the parameters of list-decodable insdel\ncodes. In particular, we show that the alphabet size of insdel codes needs to\nbe exponentially large in $\\epsilon^{-1}$, where $\\epsilon$ is the gap to\ncapacity above. Our result even applies to settings where the unique-decoding\ncapacity equals the list-decoding capacity and when it does so, it shows that\nthe alphabet size needs to be exponentially large in the gap to capacity. This\nis sharp contrast to the Hamming error model where alphabet size polynomial in\n$\\epsilon^{-1}$ suffices for unique decoding and also shows that the\nexponential dependence on the alphabet size in previous works that constructed\ninsdel codes is actually necessary!\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:09:14 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Shahrasbi", "Amirbehshad", ""], ["Sudan", "Madhu", ""]]}, {"id": "1802.08676", "submitter": "Dimitrios Alanis", "authors": "D. Alanis, P. Botsinis, Z. Babar, H. V. Nguyen, D. Chandra, S. X. Ng\n  and L. Hanzo", "title": "A Quantum-Search-Aided Dynamic Programming Framework for Pareto Optimal\n  Routing in Wireless Multihop Networks", "comments": "Accepted in IEEE Transactions on Communications, 2018. In press", "journal-ref": null, "doi": "10.1109/TCOMM.2018.2803068", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wireless Multihop Networks (WMHNs) have to strike a trade-off among diverse\nand often conflicting Quality-of-Service (QoS) requirements. The resultant\nsolutions may be included by the Pareto Front under the concept of Pareto\nOptimality. However, the problem of finding all the Pareto-optimal routes in\nWMHNs is classified as NP-hard, since the number of legitimate routes increases\nexponentially, as the nodes proliferate. Quantum Computing offers an attractive\nframework of rendering the Pareto-optimal routing problem tractable. In this\ncontext, a pair of quantum-assisted algorithms have been proposed, namely the\nNon-Dominated Quantum Optimization (NDQO) and the Non-Dominated Quantum\nIterative Optimization (NDQIO). However, their complexity is proportional to\n$\\sqrt{N}$, where $N$ corresponds to the total number of legitimate routes,\nthus still failing to find the solutions in \"polynomial time\". As a remedy, we\ndevise a dynamic programming framework and propose the so-called Evolutionary\nQuantum Pareto Optimization (EQPO) algorithm. We analytically characterize the\ncomplexity imposed by the EQPO algorithm and demonstrate that it succeeds in\nsolving the Pareto-optimal routing problem in polynomial time. Finally, we\ndemonstrate by simulations that the EQPO algorithm achieves a complexity\nreduction, which is at least an order of magnitude, when compared to its\npredecessors, albeit at the cost of a modest heuristic accuracy reduction.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:52:12 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Alanis", "D.", ""], ["Botsinis", "P.", ""], ["Babar", "Z.", ""], ["Nguyen", "H. V.", ""], ["Chandra", "D.", ""], ["Ng", "S. X.", ""], ["Hanzo", "L.", ""]]}, {"id": "1802.08876", "submitter": "Holger Dell", "authors": "Holger Dell, Martin Grohe, Gaurav Rattan", "title": "Lov\\'asz Meets Weisfeiler and Leman", "comments": "Proceedings version to appear at the 45th International Colloquium on\n  Automata, Languages, and Programming (ICALP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we relate a beautiful theory by Lov\\'asz with a popular\nheuristic algorithm for the graph isomorphism problem, namely the color\nrefinement algorithm and its k-dimensional generalization known as the\nWeisfeiler-Leman algorithm. We prove that two graphs G and H are\nindistinguishable by the color refinement algorithm if and only if, for all\ntrees T, the number Hom(T,G) of homomorphisms from T to G equals the\ncorresponding number Hom(T,H) for H.\n  There is a natural system of linear equations whose nonnegative integer\nsolutions correspond to the isomorphisms between two graphs. The nonnegative\nreal solutions to this system are called fractional isomorphisms, and two\ngraphs are fractionally isomorphic if and only if the color refinement\nalgorithm cannot distinguish them (Tinhofer 1986, 1991). We show that, if we\ndrop the nonnegativity constraints, that is, if we look for arbitrary real\nsolutions, then a solution to the linear system exists if and only if, for all\nt, the two graphs have the same number of length-t walks.\n  We lift the results for trees to an equivalence between numbers of\nhomomorphisms from graphs of tree width k, the k-dimensional Weisfeiler-Leman\nalgorithm, and the level-k Sherali-Adams relaxation of our linear program. We\nalso obtain a partial result for graphs of bounded path width and solutions to\nour system where we drop the nonnegativity constraints. A consequence of our\nresults is a quasi-linear time algorithm to decide whether, for two given\ngraphs G and H, there is a tree T with Hom(T,G) = Hom(T,H).\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 16:23:46 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 13:18:02 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Dell", "Holger", ""], ["Grohe", "Martin", ""], ["Rattan", "Gaurav", ""]]}, {"id": "1802.08898", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Nisheeth K. Vishnoi", "title": "Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from\nhigh-dimensional distributions in Statistics and Machine learning. HMC is known\nto run very efficiently in practice and its popular second-order \"leapfrog\"\nimplementation has long been conjectured to run in $d^{1/4}$ gradient\nevaluations. Here we show that this conjecture is true when sampling from\nstrongly log-concave target distributions that satisfy a weak third-order\nregularity property associated with the input data. Our regularity condition is\nweaker than the Lipschitz Hessian property and allows us to show faster\nconvergence bounds for a much larger class of distributions than would be\npossible with the usual Lipschitz Hessian constant alone. Important\ndistributions that satisfy our regularity condition include posterior\ndistributions used in Bayesian logistic regression for which the data satisfies\nan \"incoherence\" property. Our result compares favorably with the best\navailable bounds for the class of strongly log-concave distributions, which\ngrow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our\nsimulations on synthetic data suggest that, when our regularity condition is\nsatisfied, leapfrog HMC performs better than its competitors -- both in terms\nof accuracy and in terms of the number of gradient evaluations it requires.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 19:23:21 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 00:27:58 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 16:26:27 GMT"}, {"version": "v4", "created": "Thu, 2 Aug 2018 15:31:10 GMT"}, {"version": "v5", "created": "Thu, 9 Aug 2018 18:24:09 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1802.08955", "submitter": "Brahim Chaourar", "authors": "Brahim Chaourar", "title": "On the Broadcast Routing Problem in Computer Networks", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G = (V, E)$, and a vertex $r\\in V$, an $r$-acyclic\norientation of $G$ is an orientation $OE$ of the edges of $G$ such that the\ndigraph $OG = (V, OE)$ is acyclic and $r$ is the unique vertex with indegree\nequal to 0. For $w\\in \\mathbb{R}^E_+$, $k(G, w)$ is the value of the\n$w$-maximum packing of $r$-arborescences for all $r\\in V$ and all $r$-acyclic\norientations $OE$ of $G$. In this case, the Broadcast Routing (in Computers\nNetworks) Problem (BRP) is to compute $k(G, w)$, by finding an optimal $r$ and\nan optimal $r$-acyclic orientation. BRP is a mathematical formulation of\nmultipath broadcast routing in computer networks. In this paper, we provide a\npolynomial time algorithm to solve BRP in outerplanar graphs. Outerplanar\ngraphs are encountered in many applications such as computational geometry,\nrobotics, etc.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 05:26:17 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 16:23:20 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 23:26:50 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chaourar", "Brahim", ""]]}, {"id": "1802.09001", "submitter": "Batya Kenig", "authors": "Batya Kenig", "title": "The Complexity of the Possible Winner Problem over Partitioned\n  Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Possible-Winner problem asks, given an election where the voters'\npreferences over the set of candidates is partially specified, whether a\ndistinguished candidate can become a winner. In this work, we consider the\ncomputational complexity of Possible-Winner under the assumption that the voter\npreferences are $partitioned$. That is, we assume that every voter provides a\ncomplete order over sets of incomparable candidates (e.g., candidates are\nranked by their level of education). We consider elections with partitioned\nprofiles over positional scoring rules, with an unbounded number of candidates,\nand unweighted voters. Our first result is a polynomial time algorithm for\nvoting rules with $2$ distinct values, which include the well-known\n$k$-approval voting rule. We then go on to prove NP-hardness for a class of\nrules that contain all voting rules that produce scoring vectors with at least\n$4$ distinct values.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 13:21:40 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Kenig", "Batya", ""]]}, {"id": "1802.09007", "submitter": "Du\\v{s}an Knop", "authors": "Kate\\v{r}ina Altmanov\\'a and Du\\v{s}an Knop and Martin Kouteck\\'y", "title": "Evaluating and Tuning n-fold Integer Programming", "comments": "24 pages", "journal-ref": null, "doi": "10.4230/LIPIcs.SEA.2018.10", "report-no": null, "categories": "cs.DS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, algorithmic breakthroughs in stringology, computational\nsocial choice, scheduling, etc., were achieved by applying the theory of\nso-called $n$-fold integer programming. An $n$-fold integer program (IP) has a\nhighly uniform block structured constraint matrix. Hemmecke, Onn, and Romanchuk\n[Math. Programming, 2013] showed an algorithm with runtime $a^{O(rst + r^2s)}\nn^3$, where $a$ is the largest coefficient, $r,s$, and $t$ are dimensions of\nblocks of the constraint matrix and $n$ is the total dimension of the IP; thus,\nan algorithm efficient if the blocks are of small size and with small\ncoefficients. The algorithm works by iteratively improving a feasible solution\nwith augmenting steps, and $n$-fold IPs have the special property that\naugmenting steps are guaranteed to exist in a not-too-large neighborhood.\n  We have implemented the algorithm and learned the following along the way.\nThe original algorithm is practically unusable, but we discover a series of\nimprovements which make its evaluation possible. Crucially, we observe that a\ncertain constant in the algorithm can be treated as a tuning parameter, which\nyields an efficient heuristic (essentially searching in a\nsmaller-than-guaranteed neighborhood). Furthermore, the algorithm uses an\noverly expensive strategy to find a \"best\" step, while finding only an\n\"approximatelly best\" step is much cheaper, yet sufficient for quick\nconvergence. Using this insight, we improve the asymptotic dependence on $n$\nfrom $n^3$ to $n^2 \\log n$.\n  We show that decreasing the tuning parameter initially leads to an increased\nnumber of iterations needed for convergence and eventually to getting stuck in\nlocal optima, as expected. However, surprisingly small values of the parameter\nalready exhibit good behavior. Second, our new strategy for finding\n\"approximatelly best\" steps wildly outperforms the original construction.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 13:59:12 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 12:06:51 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Altmanov\u00e1", "Kate\u0159ina", ""], ["Knop", "Du\u0161an", ""], ["Kouteck\u00fd", "Martin", ""]]}, {"id": "1802.09104", "submitter": "Shuai Xu", "authors": "Ning Xie, Shuai Xu, Yekun Xu", "title": "A New Algorithm for Finding Closest Pair of Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ vectors $x_0, x_1, \\ldots, x_{n-1}$ in $\\{0,1\\}^{m}$, how to find\ntwo vectors whose pairwise Hamming distance is minimum? This problem is known\nas the \\emph{Closest Pair Problem}. If these vectors are generated uniformly at\nrandom except two of them are correlated with Pearson-correlation coefficient\n$\\rho$, then the problem is called the \\emph{Light Bulb Problem}. In this work,\nwe propose a novel coding-based scheme for the Closest Pair Problem. We design\nboth randomized and deterministic algorithms, which achieve the best-known\nrunning time when the length of input vectors $m$ is small and the minimum\ndistance is very small compared to $m$. Specifically, the running time of our\nrandomized algorithm is $O(n\\log^{2}n\\cdot 2^{c m} \\cdot \\mathrm{poly}(m))$ and\nthe running time of our deterministic algorithm is $O(n\\log{n}\\cdot 2^{c' m}\n\\cdot \\mathrm{poly}(m))$, where $c$ and $c'$ are constants depending only on\nthe (relative) distance of the closest pair. When applied to the Light Bulb\nProblem, our result yields state-of-the-art deterministic running time when the\nPearson-correlation coefficient $\\rho$ is very large. Specifically, when $\\rho\n\\geq 0.9933$, our deterministic algorithm runs faster than the previously best\ndeterministic algorithm (Alman, SOSA 2019).\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 23:10:19 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 23:44:52 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 23:15:34 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Xie", "Ning", ""], ["Xu", "Shuai", ""], ["Xu", "Yekun", ""]]}, {"id": "1802.09110", "submitter": "Marko Mitrovic", "authors": "Marko Mitrovic, Moran Feldman, Andreas Krause, Amin Karbasi", "title": "Submodularity on Hypergraphs: From Sets to Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a nutshell, submodular functions encode an intuitive notion of diminishing\nreturns. As a result, submodularity appears in many important machine learning\ntasks such as feature selection and data summarization. Although there has been\na large volume of work devoted to the study of submodular functions in recent\nyears, the vast majority of this work has been focused on algorithms that\noutput sets, not sequences. However, in many settings, the order in which we\noutput items can be just as important as the items themselves.\n  To extend the notion of submodularity to sequences, we use a directed graph\non the items where the edges encode the additional value of selecting items in\na particular order. Existing theory is limited to the case where this\nunderlying graph is a directed acyclic graph. In this paper, we introduce two\nnew algorithms that provably give constant factor approximations for general\ngraphs and hypergraphs having bounded in or out degrees. Furthermore, we show\nthe utility of our new algorithms for real-world applications in movie\nrecommendation, online link prediction, and the design of course sequences for\nMOOCs.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 00:07:17 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 22:19:36 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Mitrovic", "Marko", ""], ["Feldman", "Moran", ""], ["Krause", "Andreas", ""], ["Karbasi", "Amin", ""]]}, {"id": "1802.09111", "submitter": "Gramoz Goranci", "authors": "Gramoz Goranci, Monika Henzinger, Pan Peng", "title": "Dynamic Effective Resistances and Approximate Schur Complement on\n  Separable Graphs", "comments": "Extended abstract to appear at the 26th Annual European Symposium on\n  Algorithms (ESA) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of dynamically maintaining (approximate) all-pairs\neffective resistances in separable graphs, which are those that admit an\n$n^{c}$-separator theorem for some $c<1$. We give a fully dynamic algorithm\nthat maintains $(1+\\varepsilon)$-approximations of the all-pairs effective\nresistances of an $n$-vertex graph $G$ undergoing edge insertions and deletions\nwith $\\tilde{O}(\\sqrt{n}/\\varepsilon^2)$ worst-case update time and\n$\\tilde{O}(\\sqrt{n}/\\varepsilon^2)$ worst-case query time, if $G$ is guaranteed\nto be $\\sqrt{n}$-separable (i.e., it is taken from a class satisfying a\n$\\sqrt{n}$-separator theorem) and its separator can be computed in\n$\\tilde{O}(n)$ time. Our algorithm is built upon a dynamic algorithm for\nmaintaining \\emph{approximate Schur complement} that approximately preserves\npairwise effective resistances among a set of terminals for separable graphs,\nwhich might be of independent interest.\n  We complement our result by proving that for any two fixed vertices $s$ and\n$t$, no incremental or decremental algorithm can maintain the $s-t$ effective\nresistance for $\\sqrt{n}$-separable graphs with worst-case update time\n$O(n^{1/2-\\delta})$ and query time $O(n^{1-\\delta})$ for any $\\delta>0$, unless\nthe Online Matrix Vector Multiplication (OMv) conjecture is false.\n  We further show that for \\emph{general} graphs, no incremental or decremental\nalgorithm can maintain the $s-t$ effective resistance problem with worst-case\nupdate time $O(n^{1-\\delta})$ and query-time $O(n^{2-\\delta})$ for any $\\delta\n>0$, unless the OMv conjecture is false.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 00:15:44 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 08:57:57 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Goranci", "Gramoz", ""], ["Henzinger", "Monika", ""], ["Peng", "Pan", ""]]}, {"id": "1802.09118", "submitter": "Yonatan Naamad", "authors": "Moses Charikar, Yonatan Naamad, Jennifer Rexford, X. Kelvin Zou", "title": "Multi-Commodity Flow with In-Network Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern networks run \"middleboxes\" that offer services ranging from network\naddress translation and server load balancing to firewalls, encryption, and\ncompression. In an industry trend known as Network Functions Virtualization\n(NFV), these middleboxes run as virtual machines on any commodity server, and\nthe switches steer traffic through the relevant chain of services. Network\nadministrators must decide how many middleboxes to run, where to place them,\nand how to direct traffic through them, based on the traffic load and the\nserver and network capacity. Rather than placing specific kinds of middleboxes\non each processing node, we argue that server virtualization allows each server\nnode to host all middlebox functions, and simply vary the fraction of resources\ndevoted to each one. This extra flexibility fundamentally changes the\noptimization problem the network administrators must solve to a new kind of\nmulti-commodity flow problem, where the traffic flows consume bandwidth on the\nlinks as well as processing resources on the nodes. We show that allocating\nresources to maximize the processed flow can be optimized exactly via a linear\nprogramming formulation, and to arbitrary accuracy via an efficient\ncombinatorial algorithm. Our experiments with real traffic and topologies show\nthat a joint optimization of node and link resources leads to an efficient use\nof bandwidth and processing capacity. We also study a class of design problems\nthat decide where to provide node capacity to best process and route a given\nset of demands, and demonstrate both approximation algorithms and hardness\nresults for these problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 01:07:32 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Charikar", "Moses", ""], ["Naamad", "Yonatan", ""], ["Rexford", "Jennifer", ""], ["Zou", "X. Kelvin", ""]]}, {"id": "1802.09205", "submitter": "Geppino Pucci", "authors": "Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci", "title": "Solving $k$-center Clustering (with Outliers) in MapReduce and\n  Streaming, almost as Accurately as Sequentially", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Center-based clustering is a fundamental primitive for data analysis and\nbecomes very challenging for large datasets. In this paper, we focus on the\npopular $k$-center variant which, given a set $S$ of points from some metric\nspace and a parameter $k<|S|$, requires to identify a subset of $k$ centers in\n$S$ minimizing the maximum distance of any point of $S$ from its closest\ncenter. A more general formulation, introduced to deal with noisy datasets,\nfeatures a further parameter $z$ and allows up to $z$ points of $S$ (outliers)\nto be disregarded when computing the maximum distance from the centers. We\npresent coreset-based 2-round MapReduce algorithms for the above two\nformulations of the problem, and a 1-pass Streaming algorithm for the case with\noutliers. For any fixed $\\epsilon>0$, the algorithms yield solutions whose\napproximation ratios are a mere additive term $\\epsilon$ away from those\nachievable by the best known polynomial-time sequential algorithms, a result\nthat substantially improves upon the state of the art. Our algorithms are\nrather simple and adapt to the intrinsic complexity of the dataset, captured by\nthe doubling dimension $D$ of the metric space. Specifically, our analysis\nshows that the algorithms become very space-efficient for the important case of\nsmall (constant) $D$. These theoretical results are complemented with a set of\nexperiments on real-world and synthetic datasets of up to over a billion\npoints, which show that our algorithms yield better quality solutions over the\nstate of the art while featuring excellent scalability, and that they also lend\nthemselves to sequential implementations much faster than existing ones.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 09:01:45 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 15:04:53 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 13:08:10 GMT"}, {"version": "v4", "created": "Fri, 12 Oct 2018 13:34:56 GMT"}, {"version": "v5", "created": "Wed, 16 Jan 2019 07:06:57 GMT"}, {"version": "v6", "created": "Tue, 1 Jun 2021 15:56:13 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Ceccarello", "Matteo", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""]]}, {"id": "1802.09333", "submitter": "Hanzhou Wu", "authors": "Hanzhou Wu, Wei Wang, Jing Dong, Hongxia Wang and Lizhi Xiong", "title": "The Cut and Dominating Set Problem in A Steganographer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A steganographer network corresponds to a graphic structure that the involved\nvertices (or called nodes) denote social entities such as the data encoders and\ndata decoders, and the associated edges represent any real communicable\nchannels or other social links that could be utilized for steganography. Unlike\ntraditional steganographic algorithms, a steganographer network models\nsteganographic communication by an abstract way such that the concerned\nunderlying characteristics of steganography are quantized as analyzable\nparameters in the network. In this paper, we will analyze two problems in a\nsteganographer network. The first problem is a passive attack to a\nsteganographer network where a network monitor has collected a list of\nsuspicious vertices corresponding to the data encoders or decoders. The network\nmonitor expects to break (disconnect) the steganographic communication down\nbetween the suspicious vertices while keeping the cost as low as possible. The\nsecond one relates to determining a set of vertices corresponding to the data\nencoders (senders) such that all vertices can share a message by neighbors. We\npoint that, the two problems are equivalent to the minimum cut problem and the\nminimum-weight dominating set problem.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 04:06:33 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Wu", "Hanzhou", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Wang", "Hongxia", ""], ["Xiong", "Lizhi", ""]]}, {"id": "1802.09478", "submitter": "Harald B\\\"ogeholz", "authors": "Harald B\\\"ogeholz, Michael Brand, Radu-Alexandru Todor", "title": "In-database connected component analysis", "comments": "major revision with new datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a Big Data-practical, SQL-implementable algorithm for efficiently\ndetermining connected components for graph data stored in a Massively Parallel\nProcessing (MPP) relational database. The algorithm described is a\nlinear-space, randomised algorithm, always terminating with the correct answer\nbut subject to a stochastic running time, such that for any $\\epsilon>0$ and\nany input graph $G=\\langle V, E \\rangle$ the algorithm terminates after\n$\\mathop{\\text{O}}(\\log |V|)$ SQL queries with probability of at least\n$1-\\epsilon$, which we show empirically to translate to a quasi-linear runtime\nin practice.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 17:55:03 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 11:18:15 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["B\u00f6geholz", "Harald", ""], ["Brand", "Michael", ""], ["Todor", "Radu-Alexandru", ""]]}, {"id": "1802.09503", "submitter": "Adam Polak", "authors": "Joanna Chybowska-Sok\\'o{\\l}, Grzegorz Gutowski, Konstanty\n  Junosza-Szaniawski, Patryk Mikos, Adam Polak", "title": "Online Coloring of Short Intervals", "comments": "APPROX 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.APPROX/RANDOM.2020.52", "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online graph coloring problem restricted to the intersection\ngraphs of intervals with lengths in $[1,\\sigma]$. For $\\sigma=1$ it is the\nclass of unit interval graphs, and for $\\sigma=\\infty$ the class of all\ninterval graphs. Our focus is on intermediary classes.\n  We present a $(1+\\sigma)$-competitive algorithm, which beats the state of the\nart for $1 < \\sigma < 2$, and proves that the problem we study can be strictly\neasier than online coloring of general interval graphs.\n  On the lower bound side, we prove that no algorithm is better than\n$5/3$-competitive for any $\\sigma>1$, nor better than $7/4$-competitive for any\n$\\sigma>2$, and that no algorithm beats the $5/2$ asymptotic competitive ratio\nfor all, arbitrarily large, values of $\\sigma$. That last result shows that the\nproblem we study can be strictly harder than unit interval coloring. Our main\ntechnical contribution is a recursive composition of strategies, which seems\nessential to prove any lower bound higher than $2$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 18:36:09 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 16:25:05 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 15:22:22 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 09:15:34 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chybowska-Sok\u00f3\u0142", "Joanna", ""], ["Gutowski", "Grzegorz", ""], ["Junosza-Szaniawski", "Konstanty", ""], ["Mikos", "Patryk", ""], ["Polak", "Adam", ""]]}, {"id": "1802.09515", "submitter": "Shay Solomon", "authors": "Haim Kaplan and Shay Solomon", "title": "Representations of Sparse Distributed Networks: A Locality-Sensitive\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1999, Brodal and Fagerberg (BF) gave an algorithm for maintaining a low\noutdegree orientation of a dynamic uniformly sparse graph. Specifically, for a\ndynamic graph on $n$-vertices, with arboricity bounded by $\\alpha$ at all\ntimes, the BF algorithm supports edge updates in $O(\\log n)$ amortized update\ntime, while keeping the maximum outdegree in the graph bounded by $O(\\alpha)$.\nSuch an orientation provides a basic data structure for uniformly sparse\ngraphs, which found applications to a plethora of dynamic graph algorithms.\n  A significant weakness of the BF algorithm is the possible \\emph{temporary}\nblowup of the maximum outdegree, following edge insertions. Although BF\neventually reduces all outdegrees to $O(\\alpha)$, local memory usage at the\nvertices, which is an important quality measure in distributed systems, cannot\nbe bounded. We show how to modify the BF algorithm to guarantee that the\noutdegrees of all vertices are bounded by $O(\\alpha)$ at all times, without\nhurting any of its other properties, and present an efficient distributed\nimplementation of the modified algorithm. This provides the \\emph{first}\nrepresentation of distributed networks in which the local memory usage at all\nvertices is bounded by the arboricity (which is essentially the average degree\nof the densest subgraph) rather than the maximum degree.\n  For settings where there are no local memory constraints, we take the\ntemporary outdegree blowup to the extreme and allow a permanent outdegree\nblowup. This allows us to address the second significant weakness of the BF\nalgorithm -- its inherently \\emph{global} nature: An insertion of an edge\n$(u,v)$ may trigger changes in the orientations of edges that are far away from\n$u$ and $v$. We suggest an alternative \\emph{local} scheme, which does not\nguarantee any outdegree bound on the vertices, yet is just as efficient as the\nBF scheme for various applications.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 18:59:54 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Kaplan", "Haim", ""], ["Solomon", "Shay", ""]]}, {"id": "1802.09578", "submitter": "Yining Wang", "authors": "Yining Wang, Yi Wu, Simon S. Du", "title": "Near-Linear Time Local Polynomial Nonparametric Estimation with Box\n  Kernels", "comments": "Accepted to INFORMS Journal on Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local polynomial regression (Fan and Gijbels 1996) is an important class of\nmethods for nonparametric density estimation and regression problems. However,\nstraightforward implementation of local polynomial regression has quadratic\ntime complexity which hinders its applicability in large-scale data analysis.\nIn this paper, we significantly accelerate the computation of local polynomial\nestimates by novel applications of multi-dimensional binary indexed trees\n(Fenwick 1994). Both time and space complexity of our proposed algorithm is\nnearly linear in the number of input data points. Simulation results confirm\nthe efficiency and effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:04:58 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 15:42:41 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Yining", ""], ["Wu", "Yi", ""], ["Du", "Simon S.", ""]]}, {"id": "1802.09610", "submitter": "Ruslan Shaydulin", "authors": "Ruslan Shaydulin and Ilya Safro", "title": "Aggregative Coarsening for Multilevel Hypergraph Partitioning", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.SEA.2018.2", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for many hypergraph problems, including partitioning, utilize\nmultilevel frameworks to achieve a good trade-off between the performance and\nthe quality of results. In this paper we introduce two novel aggregative\ncoarsening schemes and incorporate them within state-of-the-art hypergraph\npartitioner Zoltan. Our coarsening schemes are inspired by the algebraic\nmultigrid and stable matching approaches. We demonstrate the effectiveness of\nthe developed schemes as a part of multilevel hypergraph partitioning framework\non a wide range of problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 21:16:01 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 01:11:40 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Shaydulin", "Ruslan", ""], ["Safro", "Ilya", ""]]}, {"id": "1802.09617", "submitter": "Varsha Chauhan", "authors": "Varsha Chauhan, Alexander Gutfraind and Ilya Safro", "title": "Multiscale Planar Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of network representations of physical, biological, and social\nphenomena can help us better understand the structural and functional dynamics\nof their networks and formulate predictive models of these phenomena. However,\ndue to the scarcity of real-world network data owing to factors such as cost\nand effort required in collection of network data and the sensitivity of this\ndata towards theft and misuse, engineers and researchers often rely on\nsynthetic data for simulations, hypothesis testing, decision making, and\nalgorithm engineering. An important characteristic of infrastructure networks\nsuch as roads, water distribution and other utility systems is that they can be\nembedded in a plane, therefore to simulate these system we need realistic\nnetworks which are also planar. While the currently-available synthetic network\ngenerators can model networks that exhibit realism, they do not guarantee or\nachieve planarity. Therefore, in this paper we present a flexible algorithm\nthat can synthesize realistic networks that are planar. The method follows a\nmulti-scale randomized editing approach generating a hierarchy of coarsened\nnetworks of a given planar graph and introducing edits at various levels in the\nhierarchy. The method preserves the structural properties with minimal bias\nincluding the planarity of the network, while introducing realistic variability\nat multiple scales.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 21:28:35 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 23:59:14 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Chauhan", "Varsha", ""], ["Gutfraind", "Alexander", ""], ["Safro", "Ilya", ""]]}, {"id": "1802.09620", "submitter": "Dariusz Dereniowski", "authors": "Dariusz Dereniowski and Adam Sta\\'nski", "title": "On tradeoffs between width- and fill-like graph parameters", "comments": null, "journal-ref": "Theory of Computing Systems 63(3): 450-465 (2019)", "doi": "10.1007/s00224-018-9882-1", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider two two-criteria optimization problems: given an\ninput graph, the goal is to find its interval (or chordal) supergraph that\nminimizes the number of edges and its clique number simultaneously. For the\ninterval supergraph, the problem can be restated as simultaneous minimization\nof the pathwidth $pw(G)$ and the profile $p(G)$ of the input graph $G$. We\nprove that for an arbitrary graph $G$ and an integer\n$t\\in\\{1,\\ldots,pw(G)+1\\}$, there exists an interval supergraph $G'$ of $G$\nsuch that for its clique number it holds\n$\\omega(G')\\leq(1+\\frac{2}{t})(pw(G)+1)$ and the number of its edges is bounded\nby $|E(G')|\\leq(t+2)p(G)$. In other words, the pathwidth and the profile of a\ngraph can be simultaneously minimized within the factors of $1+\\frac{2}{t}$\n(plus a small constant) and $t+2$, respectively. Note that for a fixed $t$,\nboth upper bounds provide constant factor approximations. On the negative side,\nwe show an example that proves that, for some graphs, there is no solution in\nwhich both parameters are optimal.\n  In case of finding a chordal supergraph, the two corresponding graph\nparameters that reflect its clique size and number of edges are the treewidth\nand fill-in. We obtain that the treewidth and the fill-in problems are also\n`orthogonal' in the sense that for some graphs, a solution that minimizes one\nof those parameters cannot minimize the other. As a motivating example, we\nrecall graph searching games which illustrates a need of simultaneous\nminimization of these pairs of graph parameters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 21:44:36 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dereniowski", "Dariusz", ""], ["Sta\u0144ski", "Adam", ""]]}, {"id": "1802.09665", "submitter": "Michael O'Brien", "authors": "Jeremy Kun, Michael P. O'Brien, Marcin Pilipczuk, Blair D. Sullivan", "title": "Polynomial Treedepth Bounds in Linear Colorings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-treedepth colorings are an important tool for algorithms that exploit\nstructure in classes of bounded expansion; they guarantee subgraphs that use\nfew colors have bounded treedepth. These colorings have an implicit tradeoff\nbetween the total number of colors used and the treedepth bound, and prior\nempirical work suggests that the former dominates the run time of existing\nalgorithms in practice. We introduce $p$-linear colorings as an alternative to\nthe commonly used $p$-centered colorings. They can be efficiently computed in\nbounded expansion classes and use at most as many colors as $p$-centered\ncolorings. Although a set of $k<p$ colors from a $p$-centered coloring induces\na subgraph of treedepth at most $k$, the same number of colors from a\n$p$-linear coloring may induce subgraphs of larger treedepth. We establish a\npolynomial upper bound on the treedepth in general graphs, and give tighter\nbounds in trees and interval graphs via constructive coloring algorithms. We\nalso give a co-NP-completeness reduction for recognizing $p$-linear colorings\nand discuss ways to overcome this limitation in practice. This preprint extends\nresults that appeared in [9]; for full proofs omitted from [9], see previous\nversions of this preprint.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 01:13:43 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 15:56:32 GMT"}, {"version": "v3", "created": "Mon, 9 Jul 2018 20:00:06 GMT"}, {"version": "v4", "created": "Tue, 24 Jul 2018 23:58:19 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Kun", "Jeremy", ""], ["O'Brien", "Michael P.", ""], ["Pilipczuk", "Marcin", ""], ["Sullivan", "Blair D.", ""]]}, {"id": "1802.09709", "submitter": "Shay Solomon", "authors": "Sepehr Assadi and Krzysztof Onak and Baruch Schieber and Shay Solomon", "title": "Fully Dynamic Maximal Independent Set with Sublinear Update Time", "comments": "To appear in STOC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A maximal independent set (MIS) can be maintained in an evolving $m$-edge\ngraph by simply recomputing it from scratch in $O(m)$ time after each update.\nBut can it be maintained in time sublinear in $m$ in fully dynamic graphs?\n  We answer this fundamental open question in the affirmative. We present a\ndeterministic algorithm with amortized update time $O(\\min\\{\\Delta,m^{3/4}\\})$,\nwhere $\\Delta$ is a fixed bound on the maximum degree in the graph and $m$ is\nthe (dynamically changing) number of edges.\n  We further present a distributed implementation of our algorithm with\n$O(\\min\\{\\Delta,m^{3/4}\\})$ amortized message complexity, and $O(1)$ amortized\nround complexity and adjustment complexity (the number of vertices that change\ntheir output after each update). This strengthens a similar result by\nCensor-Hillel, Haramaty, and Karnin (PODC'16) that required an assumption of a\nnon-adaptive oblivious adversary.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 03:56:48 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Assadi", "Sepehr", ""], ["Onak", "Krzysztof", ""], ["Schieber", "Baruch", ""], ["Solomon", "Shay", ""]]}, {"id": "1802.09751", "submitter": "Stephen Mussmann", "authors": "Stephen Mussmann and Percy Liang", "title": "Generalized Binary Search For Split-Neighborly Problems", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In sequential hypothesis testing, Generalized Binary Search (GBS) greedily\nchooses the test with the highest information gain at each step. It is known\nthat GBS obtains the gold standard query cost of $O(\\log n)$ for problems\nsatisfying the $k$-neighborly condition, which requires any two tests to be\nconnected by a sequence of tests where neighboring tests disagree on at most\n$k$ hypotheses. In this paper, we introduce a weaker condition,\nsplit-neighborly, which requires that for the set of hypotheses two neighbors\ndisagree on, any subset is splittable by some test. For four problems that are\nnot $k$-neighborly for any constant $k$, we prove that they are\nsplit-neighborly, which allows us to obtain the optimal $O(\\log n)$ worst-case\nquery cost.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 07:30:32 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Mussmann", "Stephen", ""], ["Liang", "Percy", ""]]}, {"id": "1802.09828", "submitter": "Asaf Levin", "authors": "Ishai Kones and Asaf Levin", "title": "A unified framework for designing EPTAS's for load balancing on parallel\n  machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general load balancing problem on parallel machines. Our\nmachine environment in particular generalizes the standard models of identical\nmachines, and the model of uniformly related machines, as well as machines with\na constant number of types, and machines with activation costs. The objective\nfunctions that we consider contain in particular the makespan objective and the\nminimization of the $\\ell_p$-norm of the vector of loads of the machines both\nwith possibly job rejection.\n  We consider this general model and design an efficient polynomial time\napproximation scheme (EPTAS) that applies for all its previously studied\nspecial cases. This EPTAS improves the current best approximation scheme for\nsome of these cases where only a polynomial time approximation scheme (PTAS)\nwas known into an EPTAS.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 11:11:41 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Kones", "Ishai", ""], ["Levin", "Asaf", ""]]}, {"id": "1802.09933", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Yuanyuan Liu, Kaiwen Zhou, James Cheng, Kelvin K.W. Ng,\n  Yuichi Yoshida", "title": "Guaranteed Sufficient Decrease for Stochastic Variance Reduced Gradient\n  Optimization", "comments": "24 pages, 10 figures, AISTATS 2018. arXiv admin note: text overlap\n  with arXiv:1703.06807", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel sufficient decrease technique for\nstochastic variance reduced gradient descent methods such as SVRG and SAGA. In\norder to make sufficient decrease for stochastic optimization, we design a new\nsufficient decrease criterion, which yields sufficient decrease versions of\nstochastic variance reduction algorithms such as SVRG-SD and SAGA-SD as a\nbyproduct. We introduce a coefficient to scale current iterate and to satisfy\nthe sufficient decrease property, which takes the decisions to shrink, expand\nor even move in the opposite direction, and then give two specific update rules\nof the coefficient for Lasso and ridge regression. Moreover, we analyze the\nconvergence properties of our algorithms for strongly convex problems, which\nshow that our algorithms attain linear convergence rates. We also provide the\nconvergence guarantees of our algorithms for non-strongly convex problems. Our\nexperimental results further verify that our algorithms achieve significantly\nbetter performance than their counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 03:04:50 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Zhou", "Kaiwen", ""], ["Cheng", "James", ""], ["Ng", "Kelvin K. W.", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1802.10004", "submitter": "Timo de Wolff", "authors": "Mareike Dressler and Adam Kurpisz and Timo de Wolff", "title": "Optimization over the Boolean Hypercube via Sums of Nonnegative Circuit\n  Polynomials", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.AG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various key problems from theoretical computer science can be expressed as\npolynomial optimization problems over the boolean hypercube. One particularly\nsuccessful way to prove complexity bounds for these types of problems are based\non sums of squares (SOS) as nonnegativity certificates. In this article, we\ninitiate the analysis of optimization problems over the boolean hypercube via a\nrecent, alternative certificate called sums of nonnegative circuit polynomials\n(SONC). We show that key results for SOS based certificates remain valid:\nFirst, for polynomials, which are nonnegative over the $n$-variate boolean\nhypercube with constraints of degree $d$ there exists a SONC certificate of\ndegree at most $n+d$. Second, if there exists a degree $d$ SONC certificate for\nnonnegativity of a polynomial over the boolean hypercube, then there also\nexists a short degree $d$ SONC certificate, that includes at most $n^{O(d)}$\nnonnegative circuit polynomials.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:23:42 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Dressler", "Mareike", ""], ["Kurpisz", "Adam", ""], ["de Wolff", "Timo", ""]]}, {"id": "1802.10048", "submitter": "Matthias Bentert", "authors": "Matthias Bentert and Andr\\'e Nichterlein", "title": "Parameterized Complexity of Diameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diameter -- the task of computing the length of a longest shortest path -- is\na fundamental graph problem. Assuming the Strong Exponential Time Hypothesis,\nthere is no $O(n^{1.99})$-time algorithm even in sparse graphs [Roditty and\nWilliams, 2013]. To circumvent this lower bound we aim for algorithms with\nrunning time $f(k)(n+m)$ where $k$ is a parameter and $f$ is a function as\nsmall as possible. We investigate which parameters allow for such running\ntimes. To this end, we systematically explore a hierarchy of structural graph\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 17:53:27 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 10:22:22 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 15:46:52 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bentert", "Matthias", ""], ["Nichterlein", "Andr\u00e9", ""]]}, {"id": "1802.10189", "submitter": "Nikos Parotsidis", "authors": "Loukas Georgiadis, Giuseppe F. Italiano, Nikos Parotsidis", "title": "Incremental Strong Connectivity and 2-Connectivity in Directed Graphs", "comments": "Accepted to the 13th Latin American Theoretical INformatics Symposium\n  (LATIN 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present new incremental algorithms for maintaining data\nstructures that represent all connectivity cuts of size one in directed graphs\n(digraphs), and the strongly connected components that result by the removal of\neach of those cuts. We give a conditional lower bound that provides evidence\nthat our algorithms may be tight up to sub-polynomial factors. As an additional\nresult, with our approach we can also maintain dynamically the\n$2$-vertex-connected components of a digraph during any sequence of edge\ninsertions in a total of $O(mn)$ time. This matches the bounds for the\nincremental maintenance of the $2$-edge-connected components of a digraph.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 22:22:20 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Georgiadis", "Loukas", ""], ["Italiano", "Giuseppe F.", ""], ["Parotsidis", "Nikos", ""]]}, {"id": "1802.10199", "submitter": "Philipp Bamberger", "authors": "Philipp Bamberger, Fabian Kuhn, Yannic Maus", "title": "Local Distributed Algorithms in Highly Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper studies local distributed graph problems in highly dynamic\nnetworks. Communication and changes of the graph happen in synchronous rounds\nand our algorithms always, i.e., in every round, satisfy non-trivial\nguarantees, no matter how dynamic the network is.\n  We define a (in our view) natural generalization of static graph problems to\nthe dynamic graph setting. Throughout the execution of an algorithm we consider\na sliding window over the last $T$, e.g., polylogarithmic, rounds. Then, in\nsome round, the feasibility of an output only depends on the topology of the\ngraphs in the current sliding window and we call a feasible output a\n$T$-dynamic solution. The guarantees of a $T$-dynamic solution become stronger\nthe more stable the graph is during this sliding window and, in particular,\nthey coincide with the definition of the static graph problem if the graph is\nstatic throughout the window. We further present an abstract framework that\nallows to develop algorithms that output $T$-dynamic solutions in all rounds.\nThe resulting algorithms have another desirable property: If a constant\nneighborhood around some part of the graph is stable during an interval\n$[t_1,t_2]$, the algorithms compute a static solution for this part of the\ngraph throughout the interval $[t_1+T',t_2]$ for some (small) $T'>0$.\n  We demonstrate our generic framework with two sample problems that abstract\nbasic operations in dynamic networks, namely $\\textit{(degree+1)-vertex\ncoloring}$ and $\\textit{maximal independent set (MIS)}$. To illustrate the\ngiven guarantees consider the vertex coloring problem: The sliding window of\nour (randomized) algorithm is of length $T=O(\\log n)$ and any conflict between\ntwo nodes caused by a newly inserted edge is resolved within that time. During\nthis conflict resolving both nodes always output colors that are not in\nconflict with their respective 'old' neighbors.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 22:54:56 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 16:55:15 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 15:13:46 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Bamberger", "Philipp", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""]]}, {"id": "1802.10309", "submitter": "Abhinav Srivastav", "authors": "Giorgio Lucarelli, Benjamin Moseley, Nguyen Kim Thang, Abhinav\n  Srivastav and Denis Trystram", "title": "Online Non-preemptive Scheduling on Unrelated Machines with Rejections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a computer system schedules jobs there is typically a significant cost\nassociated with preempting a job during execution. This cost can be from the\nexpensive task of saving the memory's state and loading data into and out of\nmemory. It is desirable to schedule jobs non-preemptively to avoid the costs of\npreemption. There is a need for non-preemptive system schedulers on desktops,\nservers and data centers. Despite this need, there is a gap between theory and\npractice. Indeed, few non-preemptive \\emph{online} schedulers are known to have\nstrong foundational guarantees. This gap is likely due to strong lower bounds\non any online algorithm for popular objectives. Indeed, typical worst case\nanalysis approaches, and even resource augmented approaches such as speed\naugmentation, result in all algorithms having poor performance guarantees. This\npaper considers on-line non-preemptive scheduling problems in the worst-case\nrejection model where the algorithm is allowed to reject a small fraction of\njobs. By rejecting only a few jobs, this paper shows that the strong lower\nbounds can be circumvented. This approach can be used to discover algorithmic\nscheduling policies with desirable worst-case guarantees. Specifically, the\npaper presents algorithms for the following two objectives: minimizing the\ntotal flow-time and minimizing the total weighted flow-time plus energy under\nthe speed-scaling mechanism. The algorithms have a small constant competitive\nratio while rejecting only a constant fraction of jobs. Beyond specific\nresults, the paper asserts that alternative models beyond speed augmentation\nshould be explored to aid in the discovery of good schedulers in the face of\nthe requirement of being online and non-preemptive.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 08:40:04 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Lucarelli", "Giorgio", ""], ["Moseley", "Benjamin", ""], ["Thang", "Nguyen Kim", ""], ["Srivastav", "Abhinav", ""], ["Trystram", "Denis", ""]]}, {"id": "1802.10347", "submitter": "Mikko Berggren Ettienne", "authors": "Philip Bille, Mikko Berggren Ettienne, Travis Gagie, Inge Li G{\\o}rtz,\n  Nicola Prezza", "title": "Decompressing Lempel-Ziv Compressed Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decompressing the Lempel--Ziv 77 representation of\na string $S$ of length $n$ using a working space as close as possible to the\nsize $z$ of the input. The folklore solution for the problem runs in $O(n)$\ntime but requires random access to the whole decompressed text. Another\nfolklore solution is to convert LZ77 into a grammar of size $O(z\\log(n/z))$ and\nthen stream $S$ in linear time. In this paper, we show that $O(n)$ time and\n$O(z)$ working space can be achieved for constant-size alphabets. On general\nalphabets of size $\\sigma$, we describe (i) a trade-off achieving\n$O(n\\log^\\delta \\sigma)$ time and $O(z\\log^{1-\\delta}\\sigma)$ space for any\n$0\\leq \\delta\\leq 1$, and (ii) a solution achieving $O(n)$ time and\n$O(z\\log\\log (n/z))$ space. The latter solution, in particular, dominates both\nfolklore algorithms for the problem. Our solutions can, more generally, extract\nany specified subsequence of $S$ with little overheads on top of the linear\nrunning time and working space. As an immediate corollary, we show that our\ntechniques yield improved results for pattern matching problems on\nLZ77-compressed text.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 10:33:41 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 16:02:38 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Bille", "Philip", ""], ["Ettienne", "Mikko Berggren", ""], ["Gagie", "Travis", ""], ["G\u00f8rtz", "Inge Li", ""], ["Prezza", "Nicola", ""]]}, {"id": "1802.10351", "submitter": "Anja Huber", "authors": "Tobias Harks, Martin Hoefer, Anja Huber and Manuel Surek", "title": "Efficient Black-Box Reductions for Separable Cost Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cost sharing games with delays, a set of agents jointly allocates a finite\nsubset of resources. Each resource has a fixed cost that has to be shared by\nthe players, and each agent has a nonshareable player-specific delay for each\nresource. A prominent example is uncapacitated facility location (UFL), where\nfacilities need to be opened (at a shareable cost) and clients want to connect\nto opened facilities. Each client pays a cost share and his non-shareable\nphysical connection cost. Given any profile of subsets allocated by the agents,\na separable cost sharing protocol determines cost shares that satisfy budget\nbalance on every resource and separability over the resources. Moreover, a\nseparable protocol guarantees existence of pure Nash equilibria in the induced\nstrategic game for the agents. In this paper, we study separable cost sharing\nprotocols in several general combinatorial domains. We provide black-box\nreductions to reduce the design of a separable cost-sharing protocol to the\ndesign of an approximation algorithm for the underlying cost minimization\nproblem. In this way, we obtain new separable cost-sharing protocols in games\nbased on arbitrary player-specific matroids, single-source connection games\nwithout delays, and connection games on $n$-series-parallel graphs with delays.\nAll these reductions are efficiently computable - given an initial allocation\nprofile, we obtain a cheaper profile and separable cost shares turning the\nprofile into a pure Nash equilibrium. Hence, in these domains any approximation\nalgorithm can be used to obtain a separable cost sharing protocol with a price\nof stability bounded by the approximation factor.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 10:42:58 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Harks", "Tobias", ""], ["Hoefer", "Martin", ""], ["Huber", "Anja", ""], ["Surek", "Manuel", ""]]}, {"id": "1802.10386", "submitter": "Petr Golovach", "authors": "Petr A. Golovach, Pinar Heggernes, Athanasios L. Konstantinidis,\n  Paloma T. Lima, and Charis Papadopoulos", "title": "Parameterized Aspects of Strong Subgraph Closure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the role of triadic closures in social networks, and the\nimportance of finding a maximum subgraph avoiding a fixed pattern, we introduce\nand initiate the parameterized study of the Strong F-closure problem, where F\nis a fixed graph. This is a generalization of Strong Triadic Closure, whereas\nit is a relaxation of F-free Edge Deletion. In Strong F-closure, we want to\nselect a maximum number of edges of the input graph G, and mark them as strong\nedges, in the following way: whenever a subset of the strong edges forms a\nsubgraph isomorphic to F, then the corresponding induced subgraph of G is not\nisomorphic to F. Hence the subgraph of G defined by the strong edges is not\nnecessarily F-free, but whenever it contains a copy of F, there are additional\nedges in G to destroy that strong copy of F in G.\n  We study Strong F-closure from a parameterized perspective with various\nnatural parameterizations. Our main focus is on the number k of strong edges as\nthe parameter. We show that the problem is FPT with this parameterization for\nevery fixed graph F, whereas it does not admit a polynomial kernel even when F\n=P_3. In fact, this latter case is equivalent to the Strong Triadic Closure\nproblem, which motivates us to study this problem on input graphs belonging to\nwell known graph classes. We show that Strong Triadic Closure does not admit a\npolynomial kernel even when the input graph is a split graph, whereas it admits\na polynomial kernel when the input graph is planar, and even d-degenerate.\nFurthermore, on graphs of maximum degree at most 4, we show that Strong Triadic\nClosure is FPT with the above guarantee parameterization k - \\mu(G), where\n\\mu(G) is the maximum matching size of G. We conclude with some results on the\nparameterization of Strong F-closure by the number of edges of G that are not\nselected as strong.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 12:46:53 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Golovach", "Petr A.", ""], ["Heggernes", "Pinar", ""], ["Konstantinidis", "Athanasios L.", ""], ["Lima", "Paloma T.", ""], ["Papadopoulos", "Charis", ""]]}, {"id": "1802.10403", "submitter": "Daniel Vaz", "authors": "Parinya Chalermsook, Syamantak Das, Guy Even, Bundit Laekhanukit,\n  Daniel Vaz", "title": "Survivable Network Design for Group Connectivity in Low-Treewidth Graphs", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Group Steiner Tree problem (GST), we are given a (vertex or\nedge)-weighted graph $G=(V,E)$ on $n$ vertices, a root vertex $r$ and a\ncollection of groups $\\{S_i\\}_{i\\in[h]}: S_i\\subseteq V(G)$. The goal is to\nfind a min-cost subgraph $H$ that connects the root to every group. We consider\na fault-tolerant variant of GST, which we call Restricted (Rooted) Group SNDP.\nIn this setting, each group $S_i$ has a demand $k_i\\in[k],k\\in\\mathbb N$, and\nwe wish to find a min-cost $H\\subseteq G$ such that, for each group $S_i$,\nthere is a vertex in $S_i$ connected to the root via $k_i$ (vertex or edge)\ndisjoint paths.\n  While GST admits $O(\\log^2 n\\log h)$ approximation, its high connectivity\nvariants are Label-Cover hard, and for the vertex-weighted version, the\nhardness holds even when $k=2$. Previously, positive results were known only\nfor the edge-weighted version when $k=2$ [Gupta et al., SODA 2010; Khandekar et\nal., Theor. Comput. Sci., 2012] and for a relaxed variant where the disjoint\npaths may end at different vertices in a group [Chalermsook et al., SODA 2015].\n  Our main result is an $O(\\log n\\log h)$ approximation for Restricted Group\nSNDP that runs in time $n^{f(k, w)}$, where $w$ is the treewidth of $G$. This\nnearly matches the lower bound when $k$ and $w$ are constant. The key to\nachieving this result is a non-trivial extension of the framework in\n[Chalermsook et al., SODA 2017], which embeds all feasible solutions to the\nproblem into a dynamic program (DP) table. However, finding the optimal\nsolution in the DP table remains intractable. We formulate a linear program\nrelaxation for the DP and obtain an approximate solution via randomized\nrounding. This framework also allows us to systematically construct DP tables\nfor high-connectivity problems. As a result, we present new exact algorithms\nfor several variants of survivable network design problems in low-treewidth\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 13:38:40 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Chalermsook", "Parinya", ""], ["Das", "Syamantak", ""], ["Even", "Guy", ""], ["Laekhanukit", "Bundit", ""], ["Vaz", "Daniel", ""]]}, {"id": "1802.10488", "submitter": "Pierre Laroche", "authors": "Gais Alhadi, Imed Kacem, Pierre Laroche, and Izzeldin M. Osman", "title": "An Approximate Pareto Set for Minimizing the Maximum Lateness and\n  Makespan on Parallel Machines", "comments": "submitted to Sose 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the two-parallel machines scheduling problem, with the aim of\nminimizing the maximum lateness and the makespan. Formally, the problem is\ndefined as follows. We have to schedule a set J of n jobs on two identical\nmachines. Each job i in J has a processing time p_i and a delivery time q_i.\nEach machine can only perform one job at a given time. The machines are\navailable at time t=0 and each of them can process at most one job at a given\ntime. The problem is to find a sequence of jobs, with the objective of\nminimizing the maximum lateness L_max and the makespan C_max. With no loss of\ngenerality, we consider that all data are integers and that jobs are indexed in\nnon-increasing order of their delivery times: q_1 >= q_2 >= ... >= q_n. This\npaper proposes an exact algorithm (based on a dynamic programming) to generate\nthe complete Pareto Frontier in a pseudo-polynomial time. Then, we present an\nFPTAS (Fully Polynomial Time Approximation Scheme) to generate an approximate\nPareto Frontier, based on the conversion of the dynamic programming. The\nproposed FPTAS is strongly polynomial. Some numerical experiments are provided\nin order to compare the two proposed approaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 15:49:11 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Alhadi", "Gais", ""], ["Kacem", "Imed", ""], ["Laroche", "Pierre", ""], ["Osman", "Izzeldin M.", ""]]}, {"id": "1802.10566", "submitter": "Zeyu Zhang", "authors": "Amy Babay, Michael Dinitz, Zeyu Zhang", "title": "Characterizing Demand Graphs for (Fixed-Parameter) Shallow-Light Steiner\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Shallow-Light Steiner Network problem from a fixed-parameter\nperspective. Given a graph $G$, a distance bound $L$, and $p$ pairs of vertices\n$(s_1,t_1),\\cdots,(s_p,t_p)$, the objective is to find a minimum-cost subgraph\n$G'$ such that $s_i$ and $t_i$ have distance at most $L$ in $G'$ (for every $i\n\\in [p]$). Our main result is on the fixed-parameter tractability of this\nproblem with parameter $p$. We exactly characterize the demand structures that\nmake the problem \"easy\", and give FPT algorithms for those cases. In all other\ncases, we show that the problem is W$[1]$-hard. We also extend our results to\nhandle general edge lengths and costs, precisely characterizing which demands\nallow for good FPT approximation algorithms and which demands remain\nW$[1]$-hard even to approximate.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:13:23 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Babay", "Amy", ""], ["Dinitz", "Michael", ""], ["Zhang", "Zeyu", ""]]}]