[{"id": "1101.0021", "submitter": "Katarzyna Paluch", "authors": "Katarzyna Paluch", "title": "Popular b-matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that each member of a set of agents has a preference list of a subset\nof houses, possibly involving ties and each agent and house has their capacity\ndenoting the maximum number of correspondingly agents/houses that can be\nmatched to him/her/it. We want to find a matching $M$, for which there is no\nother matching $M'$ such that more agents prefer $M'$ to $M$ than $M$ to $M'$.\n(What it means that an agent prefers one matching to the other is explained in\nthe paper.) Popular matchings have been studied quite extensively, especially\nin the one-to-one setting. We provide a characterization of popular b-matchings\nfor two defintions of popularity, show some $NP$-hardness results and for\ncertain versions describe polynomial algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 Dec 2010 23:32:39 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Paluch", "Katarzyna", ""]]}, {"id": "1101.0039", "submitter": "Jiapu Zhang", "authors": "Jiapu Zhang", "title": "A Brief Review on Results and Computational Algorithms for Minimizing\n  the Lennard-Jones Potential", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DS physics.chem-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Lennard-Jones (LJ) Potential Energy Problem is to construct the most\nstable form of $N$ atoms of a molecule with the minimal LJ potential energy.\nThis problem has a simple mathematical form $f(x) = 4\\sum_{i=1}^N\n\\sum_{j=1,j<i}^N (\\frac{1}{\\tau_{ij}^6} - \\frac{1}{\\tau_{ij}^3} {subject to}\nx\\in \\mathbb{R}^n$, where $\\tau_{ij} = (x_{3i-2} - x_{3j-2})^2 + (x_{3i-1} -\nx_{3j-1})^2 + (x_{3i} - x_{3j})^2$, $(x_{3i-2},x_{3i-1},x_{3i})$ is the\ncoordinates of atom $i$ in $\\mathbb{R}^3$, $i,j=1,2,...,N(\\geq 2 \\quad\n\\text{integer})$, and $n=3N$; however it is a challenging and difficult problem\nfor many optimization methods when $N$ is larger. In this paper, a brief review\nand a bibliography of important computational algorithms on minimizing the LJ\npotential energy are introduced in Sections 1 and 2. Section 3 of this paper\nilluminates many beautiful graphs (gotten by the author nearly 10 years ago)\nfor the three dimensional structures of molecules with minimal LJ potential.\n", "versions": [{"version": "v1", "created": "Thu, 30 Dec 2010 06:30:03 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Zhang", "Jiapu", ""]]}, {"id": "1101.0056", "submitter": "Jagbeer Singh Prof.", "authors": "Jagbeer Singh", "title": "An Algorithm to Reduce the Time Complexity of Earliest Deadline First\n  Scheduling Algorithm in Real-Time System", "comments": "(Nine)Pages, (Two)figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper I have study to Reduce the time Complexity of Earliest Deadline\nFirst (EDF), a global scheduling scheme for Earliest Deadline First in Real\nTime System tasks on a Multiprocessors system. Several admission control\nalgorithms for Earliest Deadline First (EDF) are presented, both for hard and\nsoft real-time tasks. The average performance of these admission control\nalgorithms is compared with the performance of known partitioning schemes. I\nhave applied some modification to the global Earliest Deadline First (EDF)\nalgorithms to decrease the number of task migration and also to add\npredictability to its behavior. The Aim of this work is to provide a\nsensitivity analysis for task deadline context of multiprocessor system by\nusing a new approach of EFDF (Earliest Feasible Deadline First) algorithm. In\norder to decrease the number of migrations we prevent a job from moving one\nprocessor to another processor if it is among the m higher priority jobs.\nTherefore, a job will continue its execution on the same processor if possible\n(processor affinity). The result of these comparisons outlines some situations\nwhere one scheme is preferable over the other. Partitioning schemes are better\nsuited for hard real-time systems, while a global scheme is preferable for soft\nreal-time systems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Dec 2010 08:50:49 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Singh", "Jagbeer", ""]]}, {"id": "1101.0080", "submitter": "Hiroshi Sakamoto", "authors": "Naoya Kishiue, Masaya Nakahara, Shirou Maruyama, Hiroshi Sakamoto", "title": "A Searchable Compressed Edit-Sensitive Parsing", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical data structures for the edit-sensitive parsing (ESP) are proposed.\nGiven a string S, its ESP tree is equivalent to a context-free grammar G\ngenerating just S, which is represented by a DAG. Using the succinct data\nstructures for trees and permutations, G is decomposed to two LOUDS bit strings\nand single array in (1+\\epsilon)n\\log n+4n+o(n) bits for any 0<\\epsilon <1 and\nthe number n of variables in G. The time to count occurrences of P in S is in\nO(\\frac{1}{\\epsilon}(m\\log n+occ_c(\\log m\\log u)), whereas m = |P|, u = |S|,\nand occ_c is the number of occurrences of a maximal common subtree in ESPs of P\nand S. The efficiency of the proposed index is evaluated by the experiments\nconducted on several benchmarks complying with the other compressed indexes.\n", "versions": [{"version": "v1", "created": "Thu, 30 Dec 2010 13:04:56 GMT"}, {"version": "v2", "created": "Tue, 4 Jan 2011 16:29:59 GMT"}, {"version": "v3", "created": "Sun, 9 Jan 2011 21:41:02 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Kishiue", "Naoya", ""], ["Nakahara", "Masaya", ""], ["Maruyama", "Shirou", ""], ["Sakamoto", "Hiroshi", ""]]}, {"id": "1101.0160", "submitter": "Carlos Barron-Romero", "authors": "Carlos Barron-Romero", "title": "The Complexity of Euclidian 2 Dimension Travelling Salesman Problem\n  versus General Assign Problem, NP is not P", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the differences between two NP problems. It focuses in\nthe Euclidian 2 Dimension Travelling Salesman Problems and General Assign\nProblems. The main results are the triangle reduction to verify the solution in\npolynomial time for the former and for the later the solution to the Noted\nConjecture of the NP-Class, NP is not P.\n", "versions": [{"version": "v1", "created": "Thu, 30 Dec 2010 21:05:23 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Barron-Romero", "Carlos", ""]]}, {"id": "1101.0382", "submitter": "Mazen Al Borno", "authors": "Mazen Al Borno", "title": "Reduction in Solving Some Integer Least Squares Problems", "comments": "109 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NA cs.SY math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving an integer least squares (ILS) problem usually consists of two\nstages: reduction and search. This thesis is concerned with the reduction\nprocess for the ordinary ILS problem and the ellipsoid-constrained ILS problem.\nFor the ordinary ILS problem, we dispel common misconceptions on the reduction\nstage in the literature and show what is crucial to the efficiency of the\nsearch process. The new understanding allows us to design a new reduction\nalgorithm which is more efficient than the well-known LLL reduction algorithm.\nNumerical stability is taken into account in designing the new reduction\nalgorithm. For the ellipsoid-constrained ILS problem, we propose a new\nreduction algorithm which, unlike existing algorithms, uses all the available\ninformation. Simulation results indicate that new algorithm can greatly reduce\nthe computational cost of the search process when the measurement noise is\nlarge.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jan 2011 02:52:46 GMT"}, {"version": "v2", "created": "Sun, 30 Jan 2011 01:02:17 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Borno", "Mazen Al", ""]]}, {"id": "1101.0768", "submitter": "Markus Jalsenius", "authors": "Raphael Clifford and Markus Jalsenius", "title": "Tight Cell-Probe Bounds for Online Integer Multiplication and\n  Convolution", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show tight bounds for both online integer multiplication and convolution\nin the cell-probe model with word size w. For the multiplication problem, one\npair of digits, each from one of two n digit numbers that are to be multiplied,\nis given as input at step i. The online algorithm outputs a single new digit\nfrom the product of the numbers before step i+1. We give a Theta((d/w)*log n)\nbound on average per output digit for this problem where 2^d is the maximum\nvalue of a digit. In the convolution problem, we are given a fixed vector V of\nlength n and we consider a stream in which numbers arrive one at a time. We\noutput the inner product of V and the vector that consists of the last n\nnumbers of the stream. We show a Theta((d/w)*log n) bound for the number of\nprobes required per new number in the stream. All the bounds presented hold\nunder randomisation and amortisation. Multiplication and convolution are\ncentral problems in the study of algorithms which also have the widest range of\npractical applications.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jan 2011 17:08:23 GMT"}, {"version": "v2", "created": "Wed, 9 Feb 2011 16:38:12 GMT"}, {"version": "v3", "created": "Tue, 30 Aug 2011 17:38:06 GMT"}, {"version": "v4", "created": "Thu, 23 Feb 2012 22:08:51 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Clifford", "Raphael", ""], ["Jalsenius", "Markus", ""]]}, {"id": "1101.1007", "submitter": "Bart de Keijzer", "authors": "Haris Aziz and Bart de Keijzer", "title": "Complexity of coalition structure generation", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the coalition structure generation problem in which the goal is to\npartition the players into exhaustive and disjoint coalitions so as to maximize\nthe social welfare. One of our key results is a general polynomial-time\nalgorithm to solve the problem for all coalitional games provided that player\ntypes are known and the number of player types is bounded by a constant. As a\ncorollary, we obtain a polynomial-time algorithm to compute an optimal\npartition for weighted voting games with a constant number of weight values and\nfor coalitional skill games with a constant number of skills. We also consider\nwell-studied and well-motivated coalitional games defined compactly on\ncombinatorial domains. For these games, we characterize the complexity of\ncomputing an optimal coalition structure by presenting polynomial-time\nalgorithms, approximation algorithms, or NP-hardness and inapproximability\nlower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 15:41:33 GMT"}, {"version": "v2", "created": "Thu, 6 Jan 2011 10:12:39 GMT"}, {"version": "v3", "created": "Mon, 31 Jan 2011 12:01:02 GMT"}, {"version": "v4", "created": "Tue, 8 Feb 2011 16:31:38 GMT"}, {"version": "v5", "created": "Wed, 23 Mar 2011 16:42:16 GMT"}, {"version": "v6", "created": "Mon, 20 Jun 2011 12:42:56 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Aziz", "Haris", ""], ["de Keijzer", "Bart", ""]]}, {"id": "1101.1256", "submitter": "Christoph Durr", "authors": "Johanne Cohen, Christoph Durr and Nguyen Kim Thang", "title": "Non-clairvoyant Scheduling Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a scheduling game, each player owns a job and chooses a machine to execute\nit. While the social cost is the maximal load over all machines (makespan), the\ncost (disutility) of each player is the completion time of its own job. In the\ngame, players may follow selfish strategies to optimize their cost and\ntherefore their behaviors do not necessarily lead the game to an equilibrium.\nEven in the case there is an equilibrium, its makespan might be much larger\nthan the social optimum, and this inefficiency is measured by the price of\nanarchy -- the worst ratio between the makespan of an equilibrium and the\noptimum. Coordination mechanisms aim to reduce the price of anarchy by\ndesigning scheduling policies that specify how jobs assigned to a same machine\nare to be scheduled. Typically these policies define the schedule according to\nthe processing times as announced by the jobs. One could wonder if there are\npolicies that do not require this knowledge, and still provide a good price of\nanarchy. This would make the processing times be private information and avoid\nthe problem of truthfulness. In this paper we study these so-called\nnon-clairvoyant policies. In particular, we study the RANDOM policy that\nschedules the jobs in a random order without preemption, and the EQUI policy\nthat schedules the jobs in parallel using time-multiplexing, assigning each job\nan equal fraction of CPU time.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jan 2011 17:05:50 GMT"}], "update_date": "2011-01-07", "authors_parsed": [["Cohen", "Johanne", ""], ["Durr", "Christoph", ""], ["Thang", "Nguyen Kim", ""]]}, {"id": "1101.1266", "submitter": "Brijnesh Jain", "authors": "Brijnesh Jain and Klaus Obermayer", "title": "Extending Bron Kerbosch for Solving the Maximum Weight Clique Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution extends the Bron Kerbosch algorithm for solving the maximum\nweight clique problem, where continuous-valued weights are assigned to both,\nvertices and edges. We applied the proposed algorithm to graph matching\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jan 2011 17:44:46 GMT"}], "update_date": "2011-01-07", "authors_parsed": [["Jain", "Brijnesh", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1101.1710", "submitter": "Aditya Bhaskara", "authors": "Aditya Bhaskara, Moses Charikar, Rajsekar Manokaran, Aravindan\n  Vijayaraghavan", "title": "On Quadratic Programming with a Ratio Objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic Programming (QP) is the well-studied problem of maximizing over\n{-1,1} values the quadratic form \\sum_{i \\ne j} a_{ij} x_i x_j. QP captures\nmany known combinatorial optimization problems, and assuming the unique games\nconjecture, semidefinite programming techniques give optimal approximation\nalgorithms. We extend this body of work by initiating the study of Quadratic\nProgramming problems where the variables take values in the domain {-1,0,1}.\nThe specific problems we study are\n  QP-Ratio : \\max_{\\{-1,0,1\\}^n} \\frac{\\sum_{i \\not = j} a_{ij} x_i x_j}{\\sum\nx_i^2}, and Normalized QP-Ratio : \\max_{\\{-1,0,1\\}^n} \\frac{\\sum_{i \\not = j}\na_{ij} x_i x_j}{\\sum d_i x_i^2}, where d_i = \\sum_j |a_{ij}|\n  We consider an SDP relaxation obtained by adding constraints to the natural\neigenvalue (or SDP) relaxation for this problem. Using this, we obtain an\n$\\tilde{O}(n^{1/3})$ algorithm for QP-ratio. We also obtain an\n$\\tilde{O}(n^{1/4})$ approximation for bipartite graphs, and better algorithms\nfor special cases. As with other problems with ratio objectives (e.g. uniform\nsparsest cut), it seems difficult to obtain inapproximability results based on\nP!=NP. We give two results that indicate that QP-Ratio is hard to approximate\nto within any constant factor. We also give a natural distribution on instances\nof QP-Ratio for which an n^\\epsilon approximation (for \\epsilon roughly 1/10)\nseems out of reach of current techniques.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jan 2011 05:57:39 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2011 23:29:30 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Charikar", "Moses", ""], ["Manokaran", "Rajsekar", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1101.1941", "submitter": "Konstantin Makarychev", "authors": "Howard Karloff, Flip Korn, Konstantin Makarychev, Yuval Rabani", "title": "On Parsimonious Explanations for 2-D Tree- and Linearly-Ordered Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the \"explanation problem\" for tree- and linearly-ordered\narray data, a problem motivated by database applications and recently solved\nfor the one-dimensional tree-ordered case. In this paper, one is given a matrix\nA whose rows and columns have semantics: special subsets of the rows and\nspecial subsets of the columns are meaningful, others are not. A submatrix in A\nis said to be meaningful if and only if it is the cross product of a meaningful\nrow subset and a meaningful column subset, in which case we call it an \"allowed\nrectangle.\" The goal is to \"explain\" A as a sparse sum of weighted allowed\nrectangles. Specifically, we wish to find as few weighted allowed rectangles as\npossible such that, for all i,j, a_{ij} equals the sum of the weights of all\nrectangles which include cell (i,j).\n  In this paper we consider the natural cases in which the matrix dimensions\nare tree-ordered or linearly-ordered. In the tree-ordered case, we are given a\nrooted tree T1 whose leaves are the rows of A and another, T2, whose leaves are\nthe columns. Nodes of the trees correspond in an obvious way to the sets of\ntheir leaf descendants. In the linearly-ordered case, a set of rows or columns\nis meaningful if and only if it is contiguous.\n  For tree-ordered data, we prove the explanation problem NP-Hard and give a\nrandomized 2-approximation algorithm for it. For linearly-ordered data, we\nprove the explanation problem NP-Hard and give a 2.56-approximation algorithm.\nTo our knowledge, these are the first results for the problem of sparsely and\nexactly representing matrices by weighted rectangles.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jan 2011 20:15:03 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Karloff", "Howard", ""], ["Korn", "Flip", ""], ["Makarychev", "Konstantin", ""], ["Rabani", "Yuval", ""]]}, {"id": "1101.2123", "submitter": "Sandor P. Fekete", "authors": "Sandor P. Fekete, Alexander Kroeller, Martin Lorek, Marc Pfetsch", "title": "Disruption Management with Rescheduling of Trips and Vehicle\n  Circulations", "comments": "14 pages, 4 figures, to appear in the 5th ASME/ASCE/IEEE Joint Rail\n  Conference 2011 (JRC 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a combined approach for the recovery of a timetable by\nrescheduling trips and vehicle circulations for a rail-based transportation\nsystem subject to disruptions. We propose a novel event-based integer\nprogramming (IP) model. Features include shifting and canceling of trips as\nwell as modifying the vehicle schedules by changing or truncating the\ncirculations. The objective maximizes the number of recovered trips, possibly\nwith delay, while guaranteeing a conflict-free new timetable for the estimated\ntime window of the disruption. We demonstrate the usefulness of our approach\nthrough experiments for real-life test instances of relevant size, arising from\nthe subway system of Vienna. We focus on scenarios in which one direction of\none track is blocked, and trains have to be scheduled through this bottleneck.\nSolving these instances is made possible by contracting parts of the underlying\nevent-activity graph; this allows a significant size reduction of the IP.\nUsually, the solutions found within one minute are of good quality and can be\nused as good estimates of recovery plans in an online context.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jan 2011 13:19:59 GMT"}], "update_date": "2011-01-12", "authors_parsed": [["Fekete", "Sandor P.", ""], ["Kroeller", "Alexander", ""], ["Lorek", "Martin", ""], ["Pfetsch", "Marc", ""]]}, {"id": "1101.2170", "submitter": "Simone Linz", "authors": "Maria Luisa Bonet, Simone Linz, and Katherine St. John", "title": "The Complexity of Finding Multiple Solutions to Betweenness and Quartet\n  Compatibility", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that two important problems that have applications in computational\nbiology are ASP-complete, which implies that, given a solution to a problem, it\nis NP-complete to decide if another solution exists. We show first that a\nvariation of Betweenness, which is the underlying problem of questions related\nto radiation hybrid mapping, is ASP-complete. Subsequently, we use that result\nto show that Quartet Compatibility, a fundamental problem in phylogenetics that\nasks whether a set of quartets can be represented by a parent tree, is also\nASP-complete. The latter result shows that Steel's \\sc Quartet Challenge, which\nasks whether a solution to Quartet Compatibility is unique, is coNP-complete.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jan 2011 17:48:54 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2011 07:57:11 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bonet", "Maria Luisa", ""], ["Linz", "Simone", ""], ["John", "Katherine St.", ""]]}, {"id": "1101.2245", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich and Michael Mitzenmacher", "title": "Invertible Bloom Lookup Tables", "comments": "contains 4 figures, showing experimental performance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a version of the Bloom filter data structure that supports not\nonly the insertion, deletion, and lookup of key-value pairs, but also allows a\ncomplete listing of its contents with high probability, as long the number of\nkey-value pairs is below a designed threshold. Our structure allows the number\nof key-value pairs to greatly exceed this threshold during normal operation.\nExceeding the threshold simply temporarily prevents content listing and reduces\nthe probability of a successful lookup. If later entries are deleted to return\nthe structure below the threshold, everything again functions appropriately. We\nalso show that simple variations of our structure are robust to certain\nstandard errors, such as the deletion of a key without a corresponding\ninsertion or the insertion of two distinct values for a key. The properties of\nour structure make it suitable for several applications, including database and\nnetworking applications that we highlight.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 00:30:10 GMT"}, {"version": "v2", "created": "Tue, 3 May 2011 23:29:38 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2015 00:10:28 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Goodrich", "Michael T.", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1101.2360", "submitter": "Sandor P. Fekete", "authors": "Sandor P. Fekete, Joseph S. B. Mitchell, Christiane Schmidt", "title": "Minimum Covering with Travel Cost", "comments": "17 pages, 12 figures; extended abstract appears in ISAAC 2009, full\n  version to appear in Journal of Combinatorial Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a polygon and a visibility range, the Myopic Watchman Problem with\nDiscrete Vision (MWPDV) asks for a closed path P and a set of scan points S,\nsuch that (i) every point of the polygon is within visibility range of a scan\npoint; and (ii) path length plus weighted sum of scan number along the tour is\nminimized. Alternatively, the bicriteria problem (ii') aims at minimizing both\nscan number and tour length. We consider both lawn mowing (in which tour and\nscan points may leave P) and milling (in which tour, scan points and visibility\nmust stay within P) variants for the MWPDV; even for simple special cases,\nthese problems are NP-hard.\n  We show that this problem is NP-hard, even for the special cases of\nrectilinear polygons and L_\\infty scan range 1, and negligible small travel\ncost or negligible travel cost. For rectilinear MWPDV milling in grid polygons\nwe present a 2.5-approximation with unit scan range; this holds for the\nbicriteria version, thus for any linear combination of travel cost and scan\ncost. For grid polygons and circular unit scan range, we describe a bicriteria\n4-approximation. These results serve as stepping stones for the general case of\ncircular scans with scan radius r and arbitrary polygons of feature size a, for\nwhich we extend the underlying ideas to a pi(r/a}+(r+1)/2) bicriteria\napproximation algorithm. Finally, we describe approximation schemes for MWPDV\nlawn mowing and milling of grid polygons, for fixed ratio between scan cost and\ntravel cost.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 13:50:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Fekete", "Sandor P.", ""], ["Mitchell", "Joseph S. B.", ""], ["Schmidt", "Christiane", ""]]}, {"id": "1101.2637", "submitter": "Gautam Prakriya", "authors": "Samir Datta and Gautam Prakriya", "title": "Planarity Testing Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planarity Testing is the problem of determining whether a given graph is\nplanar while planar embedding is the corresponding construction problem. The\nbounded space complexity of these problems has been determined to be exactly\nLogspace by Allender and Mahajan with the aid of Reingold's result.\nUnfortunately, the algorithm is quite daunting and generalizing it to say, the\nbounded genus case seems a tall order.\n  In this work, we present a simple planar embedding algorithm running in\nlogspace. We hope this algorithm will be more amenable to generalization. The\nalgorithm is based on the fact that 3-connected planar graphs have a unique\nembedding, a variant of Tutte's criterion on conflict graphs of cycles and an\nexplicit change of cycle basis.% for planar graphs.\n  We also present a logspace algorithm to find obstacles to planarity, viz. a\nKuratowski minor, if the graph is non-planar. To the best of our knowledge this\nis the first logspace algorithm for this problem.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jan 2011 19:19:11 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Datta", "Samir", ""], ["Prakriya", "Gautam", ""]]}, {"id": "1101.2883", "submitter": "Ankur Moitra", "authors": "Nicole Immorlica, Adam Tauman Kalai, Brendan Lucier, Ankur Moitra,\n  Andrew Postlewaite, Moshe Tennenholtz", "title": "Dueling Algorithms", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit classic algorithmic search and optimization problems from the\nperspective of competition. Rather than a single optimizer minimizing expected\ncost, we consider a zero-sum game in which an optimization problem is presented\nto two players, whose only goal is to outperform the opponent. Such games are\ntypically exponentially large zero-sum games, but they often have a rich\ncombinatorial structure. We provide general techniques by which such structure\ncan be leveraged to find minmax-optimal and approximate minmax-optimal\nstrategies. We give examples of ranking, hiring, compression, and binary search\nduels, among others. We give bounds on how often one can beat the classic\noptimization algorithms in such duels.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jan 2011 20:09:03 GMT"}], "update_date": "2011-01-17", "authors_parsed": [["Immorlica", "Nicole", ""], ["Kalai", "Adam Tauman", ""], ["Lucier", "Brendan", ""], ["Moitra", "Ankur", ""], ["Postlewaite", "Andrew", ""], ["Tennenholtz", "Moshe", ""]]}, {"id": "1101.2940", "submitter": "Hadas Shachnai", "authors": "Ariel Kulik, Hadas Shachnai and Tami Tamir", "title": "Approximations for Monotone and Non-monotone Submodular Maximization\n  with Knapsack Constraints", "comments": "A preliminary version of this paper appeared in the Proceedings of\n  the 20th Annual ACM-SIAM Symposium on Discrete Algorithms, New York, January\n  2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular maximization generalizes many fundamental problems in discrete\noptimization, including Max-Cut in directed/undirected graphs, maximum\ncoverage, maximum facility location and marketing over social networks.\n  In this paper we consider the problem of maximizing any submodular function\nsubject to $d$ knapsack constraints, where $d$ is a fixed constant. We\nestablish a strong relation between the discrete problem and its continuous\nrelaxation, obtained through {\\em extension by expectation} of the submodular\nfunction. Formally, we show that, for any non-negative submodular function, an\n$\\alpha$-approximation algorithm for the continuous relaxation implies a\nrandomized $(\\alpha - \\eps)$-approximation algorithm for the discrete problem.\nWe use this relation to improve the best known approximation ratio for the\nproblem to $1/4- \\eps$, for any $\\eps > 0$, and to obtain a nearly optimal\n$(1-e^{-1}-\\eps)-$approximation ratio for the monotone case, for any $\\eps>0$.\nWe further show that the probabilistic domain defined by a continuous solution\ncan be reduced to yield a polynomial size domain, given an oracle for the\nextension by expectation. This leads to a deterministic version of our\ntechnique.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jan 2011 04:21:35 GMT"}], "update_date": "2011-01-18", "authors_parsed": [["Kulik", "Ariel", ""], ["Shachnai", "Hadas", ""], ["Tamir", "Tami", ""]]}, {"id": "1101.2973", "submitter": "Salman Fadaei", "authors": "Salman Fadaei, MohammadAmin Fazli, MohammadAli Safari", "title": "Maximizing Non-monotone Submodular Set Functions Subject to Different\n  Constraints: Combined Algorithms", "comments": "There was an older version of the paper on arXiv. We update it to the\n  latest version. In particular, there was an error in the proof of Theorem 2.\n  We fixed it. The approximation remains the same as before", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing constrained non-monotone submodular\nfunctions and provide approximation algorithms that improve existing algorithms\nin terms of either the approximation factor or simplicity. Our algorithms\ncombine existing local search and greedy based algorithms. Different\nconstraints that we study are exact cardinality and multiple knapsack\nconstraints. For the multiple-knapsack constraints we achieve a\n$(0.25-2\\epsilon)$-factor algorithm.\n  We also show, as our main contribution, how to use the continuous greedy\nprocess for non-monotone functions and, as a result, obtain a $0.13$-factor\napproximation algorithm for maximization over any solvable down-monotone\npolytope. The continuous greedy process has been previously used for maximizing\nsmooth monotone submodular function over a down-monotone polytope\n\\cite{CCPV08}. This implies a 0.13-approximation for several discrete problems,\nsuch as maximizing a non-negative submodular function subject to a matroid\nconstraint and/or multiple knapsack constraints.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jan 2011 11:30:05 GMT"}, {"version": "v2", "created": "Mon, 21 Feb 2011 14:38:31 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2011 11:03:19 GMT"}, {"version": "v4", "created": "Sun, 10 Jul 2011 05:54:33 GMT"}, {"version": "v5", "created": "Mon, 29 Feb 2016 09:43:58 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Fadaei", "Salman", ""], ["Fazli", "MohammadAmin", ""], ["Safari", "MohammadAli", ""]]}, {"id": "1101.3182", "submitter": "Ondrej Moris", "authors": "Petr Hlineny and Ondrej Moris", "title": "Multi-Stage Improved Route Planning Approach: theoretical foundations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to the static route planning problem, based on a multi-staging\nconcept and a \\emph{scope} notion, is presented. The main goal (besides implied\nefficiency of planning) of our approach is to address---with a solid\ntheoretical foundation---the following two practically motivated aspects: a\n\\emph{route comfort} and a very \\emph{limited storage} space of a small\nnavigation device, which both do not seem to be among the chief objectives of\nmany other studies. We show how our novel idea can tackle both these seemingly\nunrelated aspects at once, and may also contribute to other established route\nplanning approaches with which ours can be naturally combined. We provide a\ntheoretical proof that our approach efficiently computes exact optimal routes\nwithin this concept, as well as we demonstrate with experimental results on\npublicly available road networks of the US the good practical performance of\nthe solution.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jan 2011 11:04:58 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2011 21:11:14 GMT"}], "update_date": "2011-11-16", "authors_parsed": [["Hlineny", "Petr", ""], ["Moris", "Ondrej", ""]]}, {"id": "1101.3267", "submitter": "Shmuel Onn", "authors": "Raymond Hemmecke, Shmuel Onn, Lyubov Romanchuk", "title": "N-fold integer programming in cubic time", "comments": null, "journal-ref": "Mathematical Programming, 137:325--341, 2013", "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  N-fold integer programming is a fundamental problem with a variety of natural\napplications in operations research and statistics. Moreover, it is universal\nand provides a new, variable-dimension, parametrization of all of integer\nprogramming. The fastest algorithm for $n$-fold integer programming predating\nthe present article runs in time $O(n^{g(A)}L)$ with $L$ the binary length of\nthe numerical part of the input and $g(A)$ the so-called Graver complexity of\nthe bimatrix $A$ defining the system. In this article we provide a drastic\nimprovement and establish an algorithm which runs in time $O(n^3 L)$ having\ncubic dependency on $n$ regardless of the bimatrix $A$. Our algorithm can be\nextended to separable convex piecewise affine objectives as well, and also to\nsystems defined by bimatrices with variable entries. Moreover, it can be used\nto define a hierarchy of approximations for any integer programming problem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jan 2011 17:05:50 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Hemmecke", "Raymond", ""], ["Onn", "Shmuel", ""], ["Romanchuk", "Lyubov", ""]]}, {"id": "1101.3396", "submitter": "Imen Harbaoui Dridi", "authors": "Imen Harbaoui Dridi (LAGIS, ACS), Ryan Kammarti (ACS), Pierre Borne\n  (LAGIS), Mekki Ksouri (ACS)", "title": "Multi-objective Optimization For The Dynamic Multi-Pickup and Delivery\n  Problem with Time Windows", "comments": "arXiv admin note: text overlap with arXiv:1101.3396", "journal-ref": "META'2010, Tunisia (2010)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PDPTW is an optimization vehicles routing problem which must meet\nrequests for transport between suppliers and customers satisfying precedence,\ncapacity and time constraints. We present, in this paper, a genetic algorithm\nfor multi-objective optimization of a dynamic multi pickup and delivery problem\nwith time windows (Dynamic m-PDPTW). We propose a brief literature review of\nthe PDPTW, present our approach based on Pareto dominance method and lower\nbounds, to give a satisfying solution to the Dynamic m-PDPTW minimizing the\ncompromise between total travel cost and total tardiness time. Computational\nresults indicate that the proposed algorithm gives good results with a total\ntardiness equal to zero with a tolerable cost.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jan 2011 08:14:47 GMT"}], "update_date": "2013-02-03", "authors_parsed": [["Dridi", "Imen Harbaoui", "", "LAGIS, ACS"], ["Kammarti", "Ryan", "", "ACS"], ["Borne", "Pierre", "", "LAGIS"], ["Ksouri", "Mekki", "", "ACS"]]}, {"id": "1101.3448", "submitter": "Johannes Fischer", "authors": "Johannes Fischer", "title": "Inducing the LCP-Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to modify the linear-time construction algorithm for suffix\narrays based on induced sorting (Nong et al., DCC'09) such that it computes the\narray of longest common prefixes (LCP-array) as well. Practical tests show that\nthis outperforms recent LCP-array construction algorithms (Gog and Ohlebusch,\nALENEX'11).\n", "versions": [{"version": "v1", "created": "Tue, 18 Jan 2011 12:58:02 GMT"}], "update_date": "2011-01-19", "authors_parsed": [["Fischer", "Johannes", ""]]}, {"id": "1101.3620", "submitter": "Konstantin Voevodski", "authors": "Konstantin Voevodski, Maria-Florina Balcan, Heiko Roglin, Shang-Hua\n  Teng, Yu Xia", "title": "Clustering Protein Sequences Given the Approximation Stability of the\n  Min-Sum Objective Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of efficiently clustering protein sequences in a limited\ninformation setting. We assume that we do not know the distances between the\nsequences in advance, and must query them during the execution of the\nalgorithm. Our goal is to find an accurate clustering using few queries. We\nmodel the problem as a point set $S$ with an unknown metric $d$ on $S$, and\nassume that we have access to \\emph{one versus all} distance queries that given\na point $s \\in S$ return the distances between $s$ and all other points. Our\none versus all query represents an efficient sequence database search program\nsuch as BLAST, which compares an input sequence to an entire data set. Given a\nnatural assumption about the approximation stability of the \\emph{min-sum}\nobjective function for clustering, we design a provably accurate clustering\nalgorithm that uses few one versus all queries. In our empirical study we show\nthat our method compares favorably to well-established clustering algorithms\nwhen we compare computationally derived clusterings to gold-standard manual\nclassifications.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jan 2011 05:29:24 GMT"}, {"version": "v2", "created": "Sat, 28 May 2011 03:29:27 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Voevodski", "Konstantin", ""], ["Balcan", "Maria-Florina", ""], ["Roglin", "Heiko", ""], ["Teng", "Shang-Hua", ""], ["Xia", "Yu", ""]]}, {"id": "1101.3682", "submitter": "Daniel Roche", "authors": "Mark Giesbrecht and Daniel S. Roche", "title": "Diversification improves interpolation", "comments": "26 pages, pdfLaTeX. Preliminary version to appear at ISSAC 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of interpolating an unknown multivariate polynomial\nwith coefficients taken from a finite field or as numerical approximations of\ncomplex numbers. Building on the recent work of Garg and Schost, we improve on\nthe best-known algorithm for interpolation over large finite fields by\npresenting a Las Vegas randomized algorithm that uses fewer black box\nevaluations. Using related techniques, we also address numerical interpolation\nof sparse polynomials with complex coefficients, and provide the first provably\nstable algorithm (in the sense of relative error) for this problem, at the cost\nof modestly more evaluations. A key new technique is a randomization which\nmakes all coefficients of the unknown polynomial distinguishable, producing\nwhat we call a diverse polynomial. Another departure from most previous\napproaches is that our algorithms do not rely on root finding as a subroutine.\nWe show how these improvements affect the practical performance with trial\nimplementations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jan 2011 13:14:32 GMT"}, {"version": "v2", "created": "Mon, 24 Jan 2011 21:05:01 GMT"}, {"version": "v3", "created": "Sun, 3 Apr 2011 16:25:50 GMT"}], "update_date": "2011-04-05", "authors_parsed": [["Giesbrecht", "Mark", ""], ["Roche", "Daniel S.", ""]]}, {"id": "1101.3804", "submitter": "Abhimanyu Das", "authors": "Abhimanyu Das and David Kempe", "title": "Estimating the Average of a Lipschitz-Continuous Function from One\n  Sample", "comments": null, "journal-ref": "European Symposium on Algorithms 2010", "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the average of a Lipschitz continuous\nfunction $f$ defined over a metric space, by querying $f$ at only a single\npoint. More specifically, we explore the role of randomness in drawing this\nsample. Our goal is to find a distribution minimizing the expected estimation\nerror against an adversarially chosen Lipschitz continuous function. Our work\nfalls into the broad class of estimating aggregate statistics of a function\nfrom a small number of carefully chosen samples. The general problem has a wide\nrange of practical applications in areas as diverse as sensor networks, social\nsciences and numerical analysis. However, traditional work in numerical\nanalysis has focused on asymptotic bounds, whereas we are interested in the\n\\emph{best} algorithm. For arbitrary discrete metric spaces of bounded doubling\ndimension, we obtain a PTAS for this problem. In the special case when the\npoints lie on a line, the running time improves to an FPTAS. Both algorithms\nare based on approximately solving a linear program with an infinite set of\nconstraints, by using an approximate separation oracle. For\nLipschitz-continuous functions over $[0,1]$, we calculate the precise\nachievable error as $1-\\frac{\\sqrt{3}}{2} \\approx 0.134$, which improves upon\nthe \\quarter which is best possible for deterministic algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jan 2011 00:36:00 GMT"}], "update_date": "2011-01-21", "authors_parsed": [["Das", "Abhimanyu", ""], ["Kempe", "David", ""]]}, {"id": "1101.3953", "submitter": "Barry Wittman", "authors": "Greg N. Frederickson and Barry Wittman", "title": "Two Multivehicle Routing Problems with Unit-Time Windows", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two multivehicle routing problems are considered in the framework that a\nvisit to a location must take place during a specific time window in order to\nbe counted and all time windows are the same length. In the first problem, the\ngoal is to visit as many locations as possible using a fixed number of\nvehicles. In the second, the goal is to visit all locations using the smallest\nnumber of vehicles possible. For the first problem, we present an approximation\nalgorithm whose output path collects a reward within a constant factor of\noptimal for any fixed number of vehicles. For the second problem, our algorithm\nfinds a 6-approximation to the problem on a tree metric, whenever a single\nvehicle could visit all locations during their time windows.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jan 2011 16:39:00 GMT"}], "update_date": "2011-01-21", "authors_parsed": [["Frederickson", "Greg N.", ""], ["Wittman", "Barry", ""]]}, {"id": "1101.3960", "submitter": "Barry Wittman", "authors": "Greg N. Frederickson and Barry Wittman", "title": "Speedup in the Traveling Repairman Problem with Constrained Time Windows", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bicriteria approximation algorithm is presented for the unrooted traveling\nrepairman problem, realizing increased profit in return for increased speedup\nof repairman motion. The algorithm generalizes previous results from the case\nin which all time windows are the same length to the case in which their\nlengths can range between l and 2. This analysis can extend to any range of\ntime window lengths, following our earlier techniques. This relationship\nbetween repairman profit and speedup is applicable over a range of values that\nis dependent on the cost of putting the input in an especially desirable form,\ninvolving what are called \"trimmed windows.\" For time windows with lengths\nbetween 1 and 2, the range of values for speedup $s$ for which our analysis\nholds is $1 \\leq s \\leq 6$. In this range, we establish an approximation ratio\nthat is constant for any specific value of $s$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jan 2011 16:51:55 GMT"}], "update_date": "2011-01-21", "authors_parsed": [["Frederickson", "Greg N.", ""], ["Wittman", "Barry", ""]]}, {"id": "1101.3973", "submitter": "Fabio Pasqualetti", "authors": "Fabio Pasqualetti, Antonio Franchi, Francesco Bullo", "title": "On cooperative patrolling: optimal trajectories, complexity analysis,\n  and approximation algorithms", "comments": "Preprint submitted to IEEE Transaction on Robotics", "journal-ref": "IEEE Transaction on Robotics, vol. 28, issue 3, pp. 592-606,\n  06/2012", "doi": "10.1109/TRO.2011.2179580", "report-no": null, "categories": "math.CO cs.DS cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subject of this work is the patrolling of an environment with the aid of\na team of autonomous agents. We consider both the design of open-loop\ntrajectories with optimal properties, and of distributed control laws\nconverging to optimal trajectories. As performance criteria, the refresh time\nand the latency are considered, i.e., respectively, time gap between any two\nvisits of the same region, and the time necessary to inform every agent about\nan event occurred in the environment. We associate a graph with the\nenvironment, and we study separately the case of a chain, tree, and cyclic\ngraph. For the case of chain graph, we first describe a minimum refresh time\nand latency team trajectory, and we propose a polynomial time algorithm for its\ncomputation. Then, we describe a distributed procedure that steers the robots\ntoward an optimal trajectory. For the case of tree graph, a polynomial time\nalgorithm is developed for the minimum refresh time problem, under the\ntechnical assumption of a constant number of robots involved in the patrolling\ntask. Finally, we show that the design of a minimum refresh time trajectory for\na cyclic graph is NP-hard, and we develop a constant factor approximation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jan 2011 17:25:43 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2011 23:02:51 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2011 17:46:55 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Pasqualetti", "Fabio", ""], ["Franchi", "Antonio", ""], ["Bullo", "Francesco", ""]]}, {"id": "1101.4065", "submitter": "Sebastian Kreft", "authors": "Sebastian Kreft and Gonzalo Navarro", "title": "Self-Index Based on LZ77", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first self-index based on the Lempel-Ziv 1977 compression\nformat (LZ77). It is particularly competitive for highly repetitive text\ncollections such as sequence databases of genomes of related species, software\nrepositories, versioned document collections, and temporal text databases. Such\ncollections are extremely compressible but classical self-indexes fail to\ncapture that source of compressibility. Our self-index takes in practice a few\ntimes the space of the text compressed with LZ77 (as little as 2.6 times),\nextracts 1--2 million characters of the text per second, and finds patterns at\na rate of 10--50 microseconds per occurrence. It is smaller (up to one half)\nthan the best current self-index for repetitive collections, and faster in many\ncases.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jan 2011 02:43:16 GMT"}], "update_date": "2011-01-24", "authors_parsed": [["Kreft", "Sebastian", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1101.4068", "submitter": "Stephane Durocher", "authors": "Stephane Durocher and Jason Morrison", "title": "Linear-Space Data Structures for Range Mode Query in Arrays", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mode of a multiset $S$ is an element $a \\in S$ of maximum multiplicity;\nthat is, $a$ occurs at least as frequently as any other element in $S$. Given a\nlist $A[1:n]$ of $n$ items, we consider the problem of constructing a data\nstructure that efficiently answers range mode queries on $A$. Each query\nconsists of an input pair of indices $(i, j)$ for which a mode of $A[i:j]$ must\nbe returned. We present an $O(n^{2-2\\epsilon})$-space static data structure\nthat supports range mode queries in $O(n^\\epsilon)$ time in the worst case, for\nany fixed $\\epsilon \\in [0,1/2]$. When $\\epsilon = 1/2$, this corresponds to\nthe first linear-space data structure to guarantee $O(\\sqrt{n})$ query time. We\nthen describe three additional linear-space data structures that provide\n$O(k)$, $O(m)$, and $O(|j-i|)$ query time, respectively, where $k$ denotes the\nnumber of distinct elements in $A$ and $m$ denotes the frequency of the mode of\n$A$. Finally, we examine generalizing our data structures to higher dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jan 2011 03:23:57 GMT"}], "update_date": "2011-01-24", "authors_parsed": [["Durocher", "Stephane", ""], ["Morrison", "Jason", ""]]}, {"id": "1101.4446", "submitter": "Andrew Drucker", "authors": "Andrew Drucker", "title": "High-Confidence Predictions under Adversarial Uncertainty", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the setting in which the bits of an unknown infinite binary sequence\nx are revealed sequentially to an observer. We show that very limited\nassumptions about x allow one to make successful predictions about unseen bits\nof x. First, we study the problem of successfully predicting a single 0 from\namong the bits of x. In our model we have only one chance to make a prediction,\nbut may do so at a time of our choosing. We describe and motivate this as the\nproblem of a frog who wants to cross a road safely.\n  Letting N_t denote the number of 1s among the first t bits of x, we say that\nx is \"eps-weakly sparse\" if lim inf (N_t/t) <= eps. Our main result is a\nrandomized algorithm that, given any eps-weakly sparse sequence x, predicts a 0\nof x with success probability as close as desired to 1 - \\eps. Thus we can\nperform this task with essentially the same success probability as under the\nmuch stronger assumption that each bit of x takes the value 1 independently\nwith probability eps. We apply this result to show how to successfully predict\na bit (0 or 1) under a broad class of possible assumptions on the sequence x.\nThe assumptions are stated in terms of the behavior of a finite automaton M\nreading the bits of x.\n  We also propose and solve a variant of the well-studied \"ignorant\nforecasting\" problem. For every eps > 0, we give a randomized forecasting\nalgorithm S_eps that, given sequential access to a binary sequence x, makes a\nprediction of the form: \"A p fraction of the next N bits will be 1s.\" (The\nalgorithm gets to choose p, N, and the time of the prediction.) For any fixed\nsequence x, the forecast fraction p is accurate to within +-eps with\nprobability 1 - eps.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jan 2011 05:35:05 GMT"}], "update_date": "2011-01-25", "authors_parsed": [["Drucker", "Andrew", ""]]}, {"id": "1101.4450", "submitter": "Daniel Golovin", "authors": "Daniel Golovin and Andreas Krause", "title": "Adaptive Submodular Optimization under Matroid Constraints", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important problems in discrete optimization require maximization of a\nmonotonic submodular function subject to matroid constraints. For these\nproblems, a simple greedy algorithm is guaranteed to obtain near-optimal\nsolutions. In this article, we extend this classic result to a general class of\nadaptive optimization problems under partial observability, where each choice\ncan depend on observations resulting from past choices. Specifically, we prove\nthat a natural adaptive greedy algorithm provides a $1/(p+1)$ approximation for\nthe problem of maximizing an adaptive monotone submodular function subject to\n$p$ matroid constraints, and more generally over arbitrary $p$-independence\nsystems. We illustrate the usefulness of our result on a complex adaptive\nmatch-making application.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jan 2011 05:59:06 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Golovin", "Daniel", ""], ["Krause", "Andreas", ""]]}, {"id": "1101.4491", "submitter": "Anthony Perez", "authors": "Christophe Paul and Anthony Perez and St\\'ephan Thomass\\'e", "title": "Conflict Packing: an unifying technique to obtain polynomial kernels for\n  editing problems on dense instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a technique that we call Conflict Packing in the context of\nkernelization, obtaining (and improving) several polynomial kernels for editing\nproblems on dense instances. We apply this technique on several well-studied\nproblems: Feedback Arc Set in (Bipartite) Tournaments, Dense Rooted Triplet\nInconsistency and Betweenness in Tournaments. For the former, one is given a\n(bipartite) tournament $T = (V,A)$ and seeks a set of at most $k$ arcs whose\nreversal in $T$ results in an acyclic (bipartite) tournament. While a linear\nvertex-kernel is already known for the first problem, using the Conflict\nPacking allows us to find a so-called safe partition, the central tool of the\nkernelization algorithm in, with simpler arguments. For the case of bipartite\ntournaments, the same technique allows us to obtain a quadratic vertex-kernel.\nAgain, such a kernel was already known to exist, using the concept of so-called\nbimodules. We believe however that providing an unifying technique to cope with\nsuch problems is interesting. Regarding Dense Rooted Triplet Inconsistency, one\nis given a set of vertices $V$ and a dense collection $\\mathcal{R}$ of rooted\nbinary trees over three vertices of $V$ and seeks a rooted tree over $V$\ncontaining all but at most $k$ triplets from $\\mathcal{R}$. As a main\nconsequence of our technique, we prove that the Dense Rooted Triplet\nInconsistency problem admits a linear vertex-kernel. This result improves the\nbest known bound of $O(k^2)$ vertices for this problem. Finally, we use this\ntechnique to obtain a linear vertex-kernel for Betweenness in Tournaments,\nwhere one is given a set of vertices $V$ and a dense collection $\\mathcal{R}$\nof so-called betweenness triplets and seeks a linear ordering of the vertices\ncontaining all but at most $k$ triplets from $\\mathcal{R}$.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jan 2011 10:42:46 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2014 15:04:22 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2014 06:53:04 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Paul", "Christophe", ""], ["Perez", "Anthony", ""], ["Thomass\u00e9", "St\u00e9phan", ""]]}, {"id": "1101.4609", "submitter": "Alberto Pettarin", "authors": "Alberto Pettarin, Andrea Pietracaprina, Geppino Pucci, Eli Upfal", "title": "Tight Bounds on Information Dissemination in Sparse Mobile Networks", "comments": "19 pages; we rewrote Lemma 4, fixing a claim which was not fully\n  justified in the first version of the draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the growing interest in mobile systems, we study the dynamics of\ninformation dissemination between agents moving independently on a plane.\nFormally, we consider $k$ mobile agents performing independent random walks on\nan $n$-node grid. At time $0$, each agent is located at a random node of the\ngrid and one agent has a rumor. The spread of the rumor is governed by a\ndynamic communication graph process ${G_t(r) | t \\geq 0}$, where two agents are\nconnected by an edge in $G_t(r)$ iff their distance at time $t$ is within their\ntransmission radius $r$. Modeling the physical reality that the speed of radio\ntransmission is much faster than the motion of the agents, we assume that the\nrumor can travel throughout a connected component of $G_t$ before the graph is\naltered by the motion. We study the broadcast time $T_B$ of the system, which\nis the time it takes for all agents to know the rumor. We focus on the sparse\ncase (below the percolation point $r_c \\approx \\sqrt{n/k}$) where, with high\nprobability, no connected component in $G_t$ has more than a logarithmic number\nof agents and the broadcast time is dominated by the time it takes for many\nindependent random walks to meet each other. Quite surprisingly, we show that\nfor a system below the percolation point the broadcast time does not depend on\nthe relation between the mobility speed and the transmission radius. In fact,\nwe prove that $T_B = \\tilde{O}(n / \\sqrt{k})$ for any $0 \\leq r < r_c$, even\nwhen the transmission range is significantly larger than the mobility range in\none step, giving a tight characterization up to logarithmic factors. Our result\ncomplements a recent result of Peres et al. (SODA 2011) who showed that above\nthe percolation point the broadcast time is polylogarithmic in $k$.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jan 2011 18:08:19 GMT"}, {"version": "v2", "created": "Tue, 1 Feb 2011 19:29:37 GMT"}], "update_date": "2011-02-02", "authors_parsed": [["Pettarin", "Alberto", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Upfal", "Eli", ""]]}, {"id": "1101.5345", "submitter": "Omri Weinstein", "authors": "Dana Ron, Ronitt Rubinfeld, Muli Safra, Omri Weinstein", "title": "Approximating the Influence of a monotone Boolean function in\n  O(\\sqrt{n}) query complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\em Total Influence} ({\\em Average Sensitivity) of a discrete function\nis one of its fundamental measures. We study the problem of approximating the\ntotal influence of a monotone Boolean function \\ifnum\\plusminus=1 $f:\n\\{\\pm1\\}^n \\longrightarrow \\{\\pm1\\}$, \\else $f: \\bitset^n \\to \\bitset$, \\fi\nwhich we denote by $I[f]$. We present a randomized algorithm that approximates\nthe influence of such functions to within a multiplicative factor of $(1\\pm\n\\eps)$ by performing $O(\\frac{\\sqrt{n}\\log n}{I[f]} \\poly(1/\\eps)) $ queries. %\n\\mnote{D: say something about technique?} We also prove a lower bound of %\n$\\Omega(\\frac{\\sqrt{n/\\log n}}{I[f]})$ $\\Omega(\\frac{\\sqrt{n}}{\\log n \\cdot\nI[f]})$ on the query complexity of any constant-factor approximation algorithm\nfor this problem (which holds for $I[f] = \\Omega(1)$), % and $I[f] =\nO(\\sqrt{n}/\\log n)$), hence showing that our algorithm is almost optimal in\nterms of its dependence on $n$. For general functions we give a lower bound of\n$\\Omega(\\frac{n}{I[f]})$, which matches the complexity of a simple sampling\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jan 2011 17:31:59 GMT"}], "update_date": "2011-01-28", "authors_parsed": [["Ron", "Dana", ""], ["Rubinfeld", "Ronitt", ""], ["Safra", "Muli", ""], ["Weinstein", "Omri", ""]]}, {"id": "1101.5376", "submitter": "Chris Thachuk", "authors": "Chris Thachuk", "title": "Succincter Text Indexing with Wildcards", "comments": "10 pages, 3 additional pages for supporting proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of indexing text with wildcard positions, motivated by\nthe challenge of aligning sequencing data to large genomes that contain\nmillions of single nucleotide polymorphisms (SNPs)---positions known to differ\nbetween individuals. SNPs modeled as wildcards can lead to more informed and\nbiologically relevant alignments. We improve the space complexity of previous\napproaches by giving a succinct index requiring $(2 + o(1))n \\log \\sigma + O(n)\n+ O(d \\log n) + O(k \\log k)$ bits for a text of length $n$ over an alphabet of\nsize $\\sigma$ containing $d$ groups of $k$ wildcards. A key to the space\nreduction is a result we give showing how any compressed suffix array can be\nsupplemented with auxiliary data structures occupying $O(n) + O(d \\log\n\\frac{n}{d})$ bits to also support efficient dictionary matching queries. The\nquery algorithm for our wildcard index is faster than previous approaches using\nreasonable working space. More importantly our new algorithm greatly reduces\nthe query working space to $O(d m + m \\log n)$ bits. We note that compared to\nprevious results this reduces the working space by two orders of magnitude when\naligning short read data to the Human genome.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jan 2011 20:09:34 GMT"}], "update_date": "2011-01-28", "authors_parsed": [["Thachuk", "Chris", ""]]}, {"id": "1101.5407", "submitter": "Tom Kamphans", "authors": "Michael A. Bender and S\\'andor P. Fekete and Tom Kamphans and Nils\n  Schweer", "title": "Maintaining Arrays of Contiguous Objects", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider methods for dynamically storing a set of different\nobjects (\"modules\") in a physical array. Each module requires one free\ncontiguous subinterval in order to be placed. Items are inserted or removed,\nresulting in a fragmented layout that makes it harder to insert further\nmodules. It is possible to relocate modules, one at a time, to another free\nsubinterval that is contiguous and does not overlap with the current location\nof the module. These constraints clearly distinguish our problem from classical\nmemory allocation. We present a number of algorithmic results, including a\nbound of Theta(n^2) on physical sorting if there is a sufficiently large free\nspace and sum up NP-hardness results for arbitrary initial layouts. For online\nscenarios in which modules arrive one at a time, we present a method that\nrequires O(1) moves per insertion or deletion and amortized cost O(m_i log M)\nper insertion or deletion, where m_i is the module's size, M is the size of the\nlargest module and costs for moves are linear in the size of a module.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jan 2011 22:48:51 GMT"}], "update_date": "2011-01-31", "authors_parsed": [["Bender", "Michael A.", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Kamphans", "Tom", ""], ["Schweer", "Nils", ""]]}, {"id": "1101.5410", "submitter": "Panteleimon Rodis", "authors": "Panteleimon Rodis", "title": "Connection errors in networks of linear features and the application of\n  geometrical reduction in spatial data algorithms", "comments": "14 pages, 4 spatial algorithms, 3 illustrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study on connection errors in networks of linear features and\nmethods of error detection. We model networks with special connection\nspecifications as networks with hierarchically connected features and define\nerrors considering the spatial relationships and the functionality of the\nnetwork elements. A general definition of the problem of the detection of\nconnection errors which takes into account the functionality of the network\nelements is discussed. Then a series of spatial algorithms that solve different\naspects of the problem is presented. We also define and analyze the notion of\ngeometrical reduction as a method of achieving efficient performance. In the\nlast section the undecidability of algorithmic error correction is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jan 2011 23:02:52 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2011 20:49:37 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2012 21:42:15 GMT"}, {"version": "v4", "created": "Sat, 4 Apr 2015 04:23:49 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Rodis", "Panteleimon", ""]]}, {"id": "1101.5506", "submitter": "Rodrigo Canovas", "authors": "Nieves R. Brisaboa and Rodrigo C\\'anovas and Miguel A.\n  Mart\\'inez-Prieto and Gonzalo Navarro", "title": "Compressed String Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of storing a set of strings --- a string dictionary --- in\ncompact form appears naturally in many cases. While classically it has\nrepresented a small part of the whole data to be processed (e.g., for Natural\nLanguage processing or for indexing text collections), more recent applications\nin Web engines, Web mining, RDF graphs, Internet routing, Bioinformatics, and\nmany others, make use of very large string dictionaries, whose size is a\nsignificant fraction of the whole data. Thus novel approaches to compress them\nefficiently are necessary. In this paper we experimentally compare time and\nspace performance of some existing alternatives, as well as new ones we\npropose. We show that space reductions of up to 20% of the original size of the\nstrings is possible while supporting fast dictionary searches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jan 2011 11:08:46 GMT"}], "update_date": "2011-01-31", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["C\u00e1novas", "Rodrigo", ""], ["Mart\u00ednez-Prieto", "Miguel A.", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1101.5518", "submitter": "Kitty Meeks", "authors": "Kitty Meeks and Alexander Scott", "title": "The complexity of Free-Flood-It on 2xn boards", "comments": null, "journal-ref": null, "doi": "10.1016/j.tcs.2013.06.010", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the complexity of problems related to the combinatorial game\nFree-Flood-It, in which players aim to make a coloured graph monochromatic with\nthe minimum possible number of flooding operations. Our main result is that\ncomputing the length of an optimal sequence is fixed parameter tractable (with\nthe number of colours present as a parameter) when restricted to rectangular\n2xn boards. We also show that, when the number of colours is unbounded, the\nproblem remains NP-hard on such boards. This resolves a question of Clifford,\nJalsenius, Montanaro and Sach (2010).\n", "versions": [{"version": "v1", "created": "Fri, 28 Jan 2011 11:51:32 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2012 17:35:45 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2013 12:46:46 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Meeks", "Kitty", ""], ["Scott", "Alexander", ""]]}, {"id": "1101.5586", "submitter": "Naveen Garg", "authors": "Nishita Aggarwal, Naveen Garg, Swati Gupta", "title": "A 4/3-approximation for TSP on cubic 3-edge-connected graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a polynomial time 4/3 approximation algorithm for TSP on metrics\narising from the metric completion of cubic 3-edge connected graphs.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jan 2011 17:45:05 GMT"}], "update_date": "2011-01-31", "authors_parsed": [["Aggarwal", "Nishita", ""], ["Garg", "Naveen", ""], ["Gupta", "Swati", ""]]}, {"id": "1101.5711", "submitter": "Tal Orenshtein", "authors": "Tal Orenshtein and Igor Shinkar", "title": "Greedy Random Walk", "comments": null, "journal-ref": "Combinator. Probab. Comp. 23 (2014) 269-289", "doi": "10.1017/S0963548313000552", "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a discrete time self interacting random process on graphs, which we\ncall Greedy Random Walk. The walker is located initially at some vertex. As\ntime evolves, each vertex maintains the set of adjacent edges touching it that\nhave not been crossed yet by the walker. At each step, the walker being at some\nvertex, picks an adjacent edge among the edges that have not traversed thus far\naccording to some (deterministic or randomized) rule. If all the adjacent edges\nhave already been traversed, then an adjacent edge is chosen uniformly at\nrandom. After picking an edge the walk jumps along it to the neighboring\nvertex. We show that the expected edge cover time of the greedy random walk is\nlinear in the number of edges for certain natural families of graphs. Examples\nof such graphs include the complete graph, even degree expanders of logarithmic\ngirth, and the hypercube graph. We also show that GRW is transient in $\\Z^d$\nfor all $d \\geq 3$.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jan 2011 17:50:37 GMT"}, {"version": "v2", "created": "Mon, 28 Feb 2011 12:22:57 GMT"}, {"version": "v3", "created": "Sun, 25 Sep 2011 16:20:52 GMT"}, {"version": "v4", "created": "Thu, 7 Jun 2012 07:06:55 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Orenshtein", "Tal", ""], ["Shinkar", "Igor", ""]]}, {"id": "1101.5753", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Robert Krauthgamer", "title": "Fault-Tolerant Spanners: Better and Simpler", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural requirement of many distributed structures is fault-tolerance:\nafter some failures, whatever remains from the structure should still be\neffective for whatever remains from the network. In this paper we examine\nspanners of general graphs that are tolerant to vertex failures, and\nsignificantly improve their dependence on the number of faults $r$, for all\nstretch bounds.\n  For stretch $k \\geq 3$ we design a simple transformation that converts every\n$k$-spanner construction with at most $f(n)$ edges into an $r$-fault-tolerant\n$k$-spanner construction with at most $O(r^3 \\log n) \\cdot f(2n/r)$ edges.\nApplying this to standard greedy spanner constructions gives $r$-fault tolerant\n$k$-spanners with $\\tilde O(r^{2} n^{1+\\frac{2}{k+1}})$ edges. The previous\nconstruction by Chechik, Langberg, Peleg, and Roddity [STOC 2009] depends\nsimilarly on $n$ but exponentially on $r$ (approximately like $k^r$).\n  For the case $k=2$ and unit-length edges, an $O(r \\log n)$-approximation\nalgorithm is known from recent work of Dinitz and Krauthgamer [arXiv 2010],\nwhere several spanner results are obtained using a common approach of rounding\na natural flow-based linear programming relaxation. Here we use a different\n(stronger) LP relaxation and improve the approximation ratio to $O(\\log n)$,\nwhich is, notably, independent of the number of faults $r$. We further\nstrengthen this bound in terms of the maximum degree by using the \\Lovasz Local\nLemma.\n  Finally, we show that most of our constructions are inherently local by\ndesigning equivalent distributed algorithms in the LOCAL model of distributed\ncomputation.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jan 2011 09:35:13 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Dinitz", "Michael", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1101.5805", "submitter": "Matteo Riondato", "authors": "Matteo Riondato, Mert Akdere, Ugur Cetintemel, Stanley B. Zdonik, Eli\n  Upfal", "title": "The VC-Dimension of Queries and Selectivity Estimation Through Sampling", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel method, based on the statistical concept of the\nVapnik-Chervonenkis dimension, to evaluate the selectivity (output cardinality)\nof SQL queries - a crucial step in optimizing the execution of large scale\ndatabase and data-mining operations. The major theoretical contribution of this\nwork, which is of independent interest, is an explicit bound to the\nVC-dimension of a range space defined by all possible outcomes of a collection\n(class) of queries. We prove that the VC-dimension is a function of the maximum\nnumber of Boolean operations in the selection predicate and of the maximum\nnumber of select and join operations in any individual query in the collection,\nbut it is neither a function of the number of queries in the collection nor of\nthe size (number of tuples) of the database. We leverage on this result and\ndevelop a method that, given a class of queries, builds a concise random sample\nof a database, such that with high probability the execution of any query in\nthe class on the sample provides an accurate estimate for the selectivity of\nthe query on the original large database. The error probability holds\nsimultaneously for the selectivity estimates of all queries in the collection,\nthus the same sample can be used to evaluate the selectivity of multiple\nqueries, and the sample needs to be refreshed only following major changes in\nthe database. The sample representation computed by our method is typically\nsufficiently small to be stored in main memory. We present extensive\nexperimental results, validating our theoretical analysis and demonstrating the\nadvantage of our technique when compared to complex selectivity estimation\ntechniques used in PostgreSQL and the Microsoft SQL Server.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jan 2011 19:13:43 GMT"}, {"version": "v2", "created": "Wed, 2 Feb 2011 15:57:00 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2011 20:56:03 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Riondato", "Matteo", ""], ["Akdere", "Mert", ""], ["Cetintemel", "Ugur", ""], ["Zdonik", "Stanley B.", ""], ["Upfal", "Eli", ""]]}, {"id": "1101.5876", "submitter": "Kitty Meeks", "authors": "Kitty Meeks and Alexander Scott", "title": "The complexity of flood-filling games on graphs", "comments": "More typos corrected!", "journal-ref": null, "doi": "10.1016/j.dam.2011.09.001", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the complexity of problems related to the combinatorial game\nFree-Flood-It, in which players aim to make a coloured graph monochromatic with\nthe minimum possible number of flooding operations. Although computing the\nminimum number of moves required to flood an arbitrary graph is known to be\nNP-hard, we demonstrate a polynomial time algorithm to compute the minimum\nnumber of moves required to link each pair of vertices. We apply this result to\ncompute in polynomial time the minimum number of moves required to flood a\npath, and an additive approximation to this quantity for an arbitrary k x n\nboard, coloured with a bounded number of colours, for any fixed k. On the other\nhand, we show that, for k>=3, determining the minimum number of moves required\nto flood a k x n board coloured with at least four colours remains NP-hard.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 08:42:14 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2011 11:31:39 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2011 18:58:01 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Meeks", "Kitty", ""], ["Scott", "Alexander", ""]]}, {"id": "1101.5915", "submitter": "Walter Quattrociocchi", "authors": "Sara Brunetti, Elena Lodi, Walter Quattrociocchi", "title": "Dynamic Monopolies in Colored Tori", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\em information diffusion} has been modeled as the spread of an\ninformation within a group through a process of social influence, where the\ndiffusion is driven by the so called {\\em influential network}. Such a process,\nwhich has been intensively studied under the name of {\\em viral marketing}, has\nthe goal to select an initial good set of individuals that will promote a new\nidea (or message) by spreading the \"rumor\" within the entire social network\nthrough the word-of-mouth. Several studies used the {\\em linear threshold\nmodel} where the group is represented by a graph, nodes have two possible\nstates (active, non-active), and the threshold triggering the adoption\n(activation) of a new idea to a node is given by the number of the active\nneighbors.\n  The problem of detecting in a graph the presence of the minimal number of\nnodes that will be able to activate the entire network is called {\\em target\nset selection} (TSS). In this paper we extend TSS by allowing nodes to have\nmore than two colors. The multicolored version of the TSS can be described as\nfollows: let $G$ be a torus where every node is assigned a color from a finite\nset of colors. At each local time step, each node can recolor itself, depending\non the local configurations, with the color held by the majority of its\nneighbors. We study the initial distributions of colors leading the system to a\nmonochromatic configuration of color $k$, focusing on the minimum number of\ninitial $k$-colored nodes. We conclude the paper by providing the time\ncomplexity to achieve the monochromatic configuration.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 11:22:45 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Brunetti", "Sara", ""], ["Lodi", "Elena", ""], ["Quattrociocchi", "Walter", ""]]}, {"id": "1101.5944", "submitter": "Oksana Denysyuk", "authors": "Oksana Denysyuk and Luis Rodrigues", "title": "Random Walk on Directed Dynamic Graphs", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic graphs have emerged as an appropriate model to capture the changing\nnature of many modern networks, such as peer-to-peer overlays and mobile ad hoc\nnetworks. Most of the recent research on dynamic networks has only addressed\nthe undirected dynamic graph model. However, realistic networks such as the\nones identified above are directed. In this paper we present early work in\naddressing the properties of directed dynamic graphs. In particular, we explore\nthe problem of random walk in such graphs. We assume the existence of an\noblivious adversary that makes arbitrary changes in every communication round.\nWe explore the problem of covering the dynamic graph, that even in the static\ncase can be exponential, and we establish an upper bound O(d_max n^3 log^2 n)\nof the cover time for balanced dynamic graphs.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 13:01:30 GMT"}, {"version": "v2", "created": "Tue, 1 Feb 2011 15:29:58 GMT"}], "update_date": "2011-02-02", "authors_parsed": [["Denysyuk", "Oksana", ""], ["Rodrigues", "Luis", ""]]}, {"id": "1101.6038", "submitter": "Jose Antonio Martin H", "authors": "Jose Antonio Martin H", "title": "A polynomial 3-colorability algorithm with automatic generation of NO\n  3-colorability (i.e. Co-NP) short proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an algorithm for determining 3-colorability, i.e. the decision\nproblem (YES/NO), in planar graphs is presented. The algorithm, although not\nexact (it could produce false positives) has two very important features: (i)\nit has polynomial complexity and (ii) for every \"NO\" answer, a \"short\" proof is\ngenerated, which is of much interest since 3-colorability is a NP-complete\nproblem and thus its complementary problem is in Co-NP. Hence the algorithm is\nexact when it determines that a given planar graph is not 3-colorable since\nthis is verifiable via an automatic generation of short formal proofs (also\nhuman-readable).\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 17:47:39 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["H", "Jose Antonio Martin", ""]]}]