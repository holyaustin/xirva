[{"id": "1706.00034", "submitter": "Chaitanya Swamy", "authors": "Andr\\'e Linhares and Chaitanya Swamy", "title": "Improved Algorithms for MST and Metric-TSP Interdiction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the {\\em MST-interdiction} problem: given a multigraph $G = (V,\nE)$, edge weights $\\{w_e\\geq 0\\}_{e \\in E}$, interdiction costs $\\{c_e\\geq\n0\\}_{e \\in E}$, and an interdiction budget $B\\geq 0$, the goal is to remove a\nset $R\\subseteq E$ of edges of total interdiction cost at most $B$ so as to\nmaximize the $w$-weight of an MST of $G-R:=(V,E\\setminus R)$.\n  Our main result is a $4$-approximation algorithm for this problem. This\nimproves upon the previous-best $14$-approximation~\\cite{Zenklusen15}. Notably,\nour analysis is also significantly simpler and cleaner than the one\nin~\\cite{Zenklusen15}. Whereas~\\cite{Zenklusen15} uses a greedy algorithm with\nan involved analysis to extract a good interdiction set from an over-budget\nset, we utilize a generalization of knapsack called the {\\em tree knapsack\nproblem} that nicely captures the key combinatorial aspects of this \"extraction\nproblem.\" We prove a simple, yet strong, LP-relative approximation bound for\ntree knapsack, which leads to our improved guarantees for MST interdiction. Our\nalgorithm and analysis are nearly tight, as we show that one cannot achieve an\napproximation ratio better than 3 relative to the upper bound used in our\nanalysis (and the one in~\\cite{Zenklusen15}).\n  Our guarantee for MST-interdiction yields an $8$-approximation for {\\em\nmetric-TSP interdiction} (improving over the $28$-approximation\nin~\\cite{Zenklusen15}). We also show that the {\\em maximum-spanning-tree\ninterdiction} problem is at least as hard to approximate as the minimization\nversion of densest-$k$-subgraph.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 18:13:16 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Linhares", "Andr\u00e9", ""], ["Swamy", "Chaitanya", ""]]}, {"id": "1706.00148", "submitter": "Shunsuke Inenaga", "authors": "Temma Nakamura, Shunsuke Inenaga, Hideo Bannai, and Masayuki Takeda", "title": "Order preserving pattern matching on trees and DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The order preserving pattern matching (OPPM) problem is, given a pattern\nstring $p$ and a text string $t$, find all substrings of $t$ which have the\nsame relative orders as $p$. In this paper, we consider two variants of the\nOPPM problem where a set of text strings is given as a tree or a DAG. We show\nthat the OPPM problem for a single pattern $p$ of length $m$ and a text tree\n$T$ of size $N$ can be solved in $O(m+N)$ time if the characters of $p$ are\ndrawn from an integer alphabet of polynomial size. The time complexity becomes\n$O(m \\log m + N)$ if the pattern $p$ is over a general ordered alphabet. We\nthen show that the OPPM problem for a single pattern and a text DAG is\nNP-complete.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 01:58:59 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 08:42:32 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Nakamura", "Temma", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1706.00195", "submitter": "Demetres Christofides", "authors": "Kleitos Papadopoulos and Demetres Christofides", "title": "A fast algorithm for the gas station problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the gas station problem we want to find the cheapest path between two\nvertices of an $n$-vertex graph. Our car has a specific fuel capacity and at\neach vertex we can fill our car with gas, with the fuel cost depending on the\nvertex. Furthermore, we are allowed at most $\\Delta$ stops for refuelling.\n  In this short paper we provide an algorithm solving the problem in $O(\\Delta\nn^2 + n^2\\log{n})$ steps improving an earlier result by Khuller, Malekian and\nMestre.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 08:09:28 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Papadopoulos", "Kleitos", ""], ["Christofides", "Demetres", ""]]}, {"id": "1706.00617", "submitter": "Christophe Paul", "authors": "Florian Barbero, Christophe Paul, Micha{\\l} Pilipczuk", "title": "Exploring the complexity of layout parameters in tournaments and\n  semi-complete digraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple digraph is semi-complete if for any two of its vertices $u$ and $v$,\nat least one of the arcs $(u,v)$ and $(v,u)$ is present. We study the\ncomplexity of computing two layout parameters of semi-complete digraphs:\ncutwidth and optimal linear arrangement (OLA). We prove that: (1) Both\nparameters are $\\mathsf{NP}$-hard to compute and the known exact and\nparameterized algorithms for them have essentially optimal running times,\nassuming the Exponential Time Hypothesis; (2) The cutwidth parameter admits a\nquadratic Turing kernel, whereas it does not admit any polynomial kernel unless\n$\\mathsf{NP}\\subseteq \\mathsf{coNP}/\\textrm{poly}$. By contrast, OLA admits a\nlinear kernel. These results essentially complete the complexity analysis of\ncomputing cutwidth and OLA on semi-complete digraphs. Our techniques can be\nalso used to analyze the sizes of minimal obstructions for having small\ncutwidth under the induced subdigraph relation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 10:23:12 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Barbero", "Florian", ""], ["Paul", "Christophe", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1706.00911", "submitter": "Mehdy Roayaei", "authors": "Mehdy Roayaei, MohammadReza Razzazi", "title": "Inferring protein-protein interaction and protein-DNA interaction\n  directions based on cause-effect pairs in undirected and mixed networks", "comments": "11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: Given an undirected (mixed) network and a\nset of ordered source-target, or cause-effect pairs, direct all edges so as to\nmaximize the number of pairs that admit a directed source-target path. This is\ncalled maximum graph orientation problem, and has applications in understanding\ninteractions in protein-protein interaction networks and protein-DNA\ninteraction networks. We have studied the problem on both undirected and mixed\nnetworks. In the undirected case, we determine the parameterized complexity of\nthe problem (for non-fixed and fixed paths) with respect to the number of\nsatisfied pairs, which has been an open problem. Also, we present an exact\nalgorithm which outperforms the previous algorithms on trees with bounded\nnumber of leaves. In addition, we present a parameterized-approximation\nalgorithm with respect to a parameter named the number of backbones of a tree.\nIn the mixed case, we present polynomial-time algorithms for the problem on\npaths and cycles, and an FPT-algorithm based on the combined parameter the\nnumber of arcs and the number of pairs on general graphs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 08:16:01 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Roayaei", "Mehdy", ""], ["Razzazi", "MohammadReza", ""]]}, {"id": "1706.00990", "submitter": "Prashant Pandey", "authors": "Prashant Pandey, Michael A. Bender, and Rob Johnson", "title": "A Fast x86 Implementation of Select", "comments": "Keywords and phrases Succinct data structures, Rank and Select\n  operation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank and select are fundamental operations in succinct data structures, that\nis, data structures whose space consumption approaches the\ninformation-theoretic optimal. The performance of these primitives is central\nto the overall performance of succinct data structures.\n  Traditionally, the select operation is the harder to implement efficiently,\nand most prior implementations of select on machine words use 50--80 machine\ninstructions. (In contrast, rank on machine words can be implemented in only a\nhandful of instructions on machines that support POPCOUNT.) However, recently\nPandey et al. gave a new implementation of machine-word select that uses only\nfour x86 machine instructions; two of which were introduced in Intel's Haswell\nCPUs.\n  In this paper, we investigate the impact of this new implementation of\nmachine-word select on the performance of general bit-vector-select. We first\ncompare Pandey et al.'s machine-word select to the state-of-the-art\nimplementations of Zhou et al. (which is not specific to Haswell) and Gog et\nal. (which uses some Haswell-specific instructions). We exhibit a speedup of 2X\nto 4X.\n  We then study the impact of plugging Pandey et al.'s machine-word select into\ntwo state-of-the-art bit-vector-select implementations. Both Zhou et al.'s and\nGog et al.'s select implementations perform a single machine-word select\noperation for each bit-vector select. We replaced the machine-word select with\nthe new implementation and compared performance. Even though there is only a\nsingle machine- word select operation, we still obtained speedups of 20% to\n68%. We found that the new select not only reduced the number of instructions\nrequired for each bit-vector select, but also improved CPU instruction cache\nperformance and memory-access parallelism.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 18:52:57 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Pandey", "Prashant", ""], ["Bender", "Michael A.", ""], ["Johnson", "Rob", ""]]}, {"id": "1706.01081", "submitter": "Mingda Qiao", "authors": "Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, Ruosong Wang", "title": "Nearly Optimal Sampling Algorithms for Combinatorial Pure Exploration", "comments": "Accepted to COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the combinatorial pure exploration problem Best-Set in stochastic\nmulti-armed bandits. In a Best-Set instance, we are given $n$ arms with unknown\nreward distributions, as well as a family $\\mathcal{F}$ of feasible subsets\nover the arms. Our goal is to identify the feasible subset in $\\mathcal{F}$\nwith the maximum total mean using as few samples as possible. The problem\ngeneralizes the classical best arm identification problem and the top-$k$ arm\nidentification problem, both of which have attracted significant attention in\nrecent years. We provide a novel instance-wise lower bound for the sample\ncomplexity of the problem, as well as a nontrivial sampling algorithm, matching\nthe lower bound up to a factor of $\\ln|\\mathcal{F}|$. For an important class of\ncombinatorial families, we also provide polynomial time implementation of the\nsampling algorithm, using the equivalence of separation and optimization for\nconvex program, and approximate Pareto curves in multi-objective optimization.\nWe also show that the $\\ln|\\mathcal{F}|$ factor is inevitable in general\nthrough a nontrivial lower bound construction. Our results significantly\nimprove several previous results for several important combinatorial\nconstraints, and provide a tighter understanding of the general Best-Set\nproblem.\n  We further introduce an even more general problem, formulated in geometric\nterms. We are given $n$ Gaussian arms with unknown means and unit variance.\nConsider the $n$-dimensional Euclidean space $\\mathbb{R}^n$, and a collection\n$\\mathcal{O}$ of disjoint subsets. Our goal is to determine the subset in\n$\\mathcal{O}$ that contains the $n$-dimensional vector of the means. The\nproblem generalizes most pure exploration bandit problems studied in the\nliterature. We provide the first nearly optimal sample complexity upper and\nlower bounds for the problem.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 14:27:17 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Chen", "Lijie", ""], ["Gupta", "Anupam", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""], ["Wang", "Ruosong", ""]]}, {"id": "1706.01172", "submitter": "Wei Wu", "authors": "Wei Wu, Bin Li, Ling Chen, Chengqi Zhang, Philip S. Yu", "title": "Improved Consistent Weighted Sampling Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Min-Hash is a popular technique for efficiently estimating the Jaccard\nsimilarity of binary sets. Consistent Weighted Sampling (CWS) generalizes the\nMin-Hash scheme to sketch weighted sets and has drawn increasing interest from\nthe community. Due to its constant-time complexity independent of the values of\nthe weights, Improved CWS (ICWS) is considered as the state-of-the-art CWS\nalgorithm. In this paper, we revisit ICWS and analyze its underlying mechanism\nto show that there actually exists dependence between the two components of the\nhash-code produced by ICWS, which violates the condition of independence. To\nremedy the problem, we propose an Improved ICWS (I$^2$CWS) algorithm which not\nonly shares the same theoretical computational complexity as ICWS but also\nabides by the required conditions of the CWS scheme. The experimental results\non a number of synthetic data sets and real-world text data sets demonstrate\nthat our I$^2$CWS algorithm can estimate the Jaccard similarity more\naccurately, and also compete with or outperform the compared methods, including\nICWS, in classification and top-$K$ retrieval, after relieving the underlying\ndependence.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 01:17:13 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wu", "Wei", ""], ["Li", "Bin", ""], ["Chen", "Ling", ""], ["Zhang", "Chengqi", ""], ["Yu", "Philip S.", ""]]}, {"id": "1706.01230", "submitter": "Gabriel Istrate", "authors": "J\\'anos Balogh, Cosmin Bonchi\\c{s}, Diana Dini\\c{s}, Gabriel Istrate\n  and Ioan Todinca", "title": "On the heapability of finite partial orders", "comments": null, "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 22 no.\n  1, Combinatorics (June 29, 2020) dmtcs:6540", "doi": "10.23638/DMTCS-22-1-17", "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the partitioning of partial orders into a minimal number of\nheapable subsets. We prove a characterization result reminiscent of the proof\nof Dilworth's theorem, which yields as a byproduct a flow-based algorithm for\ncomputing such a minimal decomposition. On the other hand, in the particular\ncase of sets and sequences of intervals we prove that this minimal\ndecomposition can be computed by a simple greedy-type algorithm. The paper ends\nwith a couple of open problems related to the analog of the Ulam-Hammersley\nproblem for decompositions of sets and sequences of random intervals into\nheapable sets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 07:59:56 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 10:19:18 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 11:23:18 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 08:44:29 GMT"}, {"version": "v5", "created": "Mon, 8 Jun 2020 13:10:33 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Balogh", "J\u00e1nos", ""], ["Bonchi\u015f", "Cosmin", ""], ["Dini\u015f", "Diana", ""], ["Istrate", "Gabriel", ""], ["Todinca", "Ioan", ""]]}, {"id": "1706.01260", "submitter": "Raphael Clifford", "authors": "Peter Clifford and Rapha\\\"el Clifford", "title": "The Classical Complexity of Boson Sampling", "comments": "15 pages. To appear in SODA '18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS quant-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical complexity of the exact Boson Sampling problem where\nthe objective is to produce provably correct random samples from a particular\nquantum mechanical distribution. The computational framework was proposed by\nAaronson and Arkhipov in 2011 as an attainable demonstration of `quantum\nsupremacy', that is a practical quantum computing experiment able to produce\noutput at a speed beyond the reach of classical (that is non-quantum) computer\nhardware. Since its introduction Boson Sampling has been the subject of intense\ninternational research in the world of quantum computing. On the face of it,\nthe problem is challenging for classical computation. Aaronson and Arkhipov\nshow that exact Boson Sampling is not efficiently solvable by a classical\ncomputer unless $P^{\\#P} = BPP^{NP}$ and the polynomial hierarchy collapses to\nthe third level.\n  The fastest known exact classical algorithm for the standard Boson Sampling\nproblem takes $O({m + n -1 \\choose n} n 2^n )$ time to produce samples for a\nsystem with input size $n$ and $m$ output modes, making it infeasible for\nanything but the smallest values of $n$ and $m$. We give an algorithm that is\nmuch faster, running in $O(n 2^n + \\operatorname{poly}(m,n))$ time and $O(m)$\nadditional space. The algorithm is simple to implement and has low constant\nfactor overheads. As a consequence our classical algorithm is able to solve the\nexact Boson Sampling problem for system sizes far beyond current photonic\nquantum computing experimentation, thereby significantly reducing the\nlikelihood of achieving near-term quantum supremacy in the context of Boson\nSampling.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 10:31:00 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 20:12:57 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Clifford", "Peter", ""], ["Clifford", "Rapha\u00ebl", ""]]}, {"id": "1706.01347", "submitter": "Nimrod Talmon", "authors": "Roee David, Nimrod Talmon", "title": "Balanced Facilities on Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph G with n vertices and k players, each of which is placing a\nfacility on one of the vertices of G, we define the score of the i'th player to\nbe the number of vertices for which, among all players, the facility placed by\nthe i'th player is the closest. A placement is balanced if all players get\nroughly the same score. A graph is balanced if all placements on it are\nbalanced. Viewing balancedness as a desired property in various scenarios, in\nthis paper we study balancedness properties of graphs, concentrating on random\ngraphs and on expanders. We show that, while both random graphs and expanders\ntend to have good balancedness properties, random graphs are, in general, more\nbalanced. In addition, we formulate and prove intractability of the\ncombinatorial problem of deciding whether a given graph is balanced; then,\nbuilding upon our analysis on random graphs and expanders, we devise two\nefficient algorithms which, with high probability, generate balancedness\ncertificates. Our first algorithm is based on graph traversal, while the other\nrelies on spectral properties.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 14:26:01 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["David", "Roee", ""], ["Talmon", "Nimrod", ""]]}, {"id": "1706.01382", "submitter": "Cameron Musco", "authors": "Nancy Lynch, Cameron Musco, Merav Parter", "title": "Neuro-RAM Unit with Applications to Similarity Testing and Compression\n  in Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.DS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed algorithms implemented in a simplified biologically\ninspired model for stochastic spiking neural networks. We focus on tradeoffs\nbetween computation time and network complexity, along with the role of\nrandomness in efficient neural computation.\n  It is widely accepted that neural computation is inherently stochastic. In\nrecent work, we explored how this stochasticity could be leveraged to solve the\n`winner-take-all' leader election task. Here, we focus on using randomness in\nneural algorithms for similarity testing and compression. In the most basic\nsetting, given two $n$-length patterns of firing neurons, we wish to\ndistinguish if the patterns are equal or $\\epsilon$-far from equal.\n  Randomization allows us to solve this task with a very compact network, using\n$O \\left (\\frac{\\sqrt{n}\\log n}{\\epsilon}\\right)$ auxiliary neurons, which is\nsublinear in the input size. At the heart of our solution is the design of a\n$t$-round neural random access memory, or indexing network, which we call a\nneuro-RAM. This module can be implemented with $O(n/t)$ auxiliary neurons and\nis useful in many applications beyond similarity testing.\n  Using a VC dimension-based argument, we show that the tradeoff between\nruntime and network size in our neuro-RAM is nearly optimal. Our result has\nseveral implications -- since our neuro-RAM can be implemented with\ndeterministic threshold gates, it shows that, in contrast to similarity\ntesting, randomness does not provide significant computational advantages for\nthis problem. It also establishes a separation between feedforward networks\nwhose gates spike with sigmoidal probability functions, and well-studied\ndeterministic sigmoidal networks, whose gates output real number sigmoidal\nvalues, and which can implement a neuro-RAM much more efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 15:43:40 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 17:34:32 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Lynch", "Nancy", ""], ["Musco", "Cameron", ""], ["Parter", "Merav", ""]]}, {"id": "1706.01449", "submitter": "Firas Abuzaid", "authors": "Firas Abuzaid, Geet Sethi, Peter Bailis, Matei Zaharia", "title": "To Index or Not to Index: Optimizing Exact Maximum Inner Product Search", "comments": "12 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact Maximum Inner Product Search (MIPS) is an important task that is widely\npertinent to recommender systems and high-dimensional similarity search. The\nbrute-force approach to solving exact MIPS is computationally expensive, thus\nspurring recent development of novel indexes and pruning techniques for this\ntask. In this paper, we show that a hardware-efficient brute-force approach,\nblocked matrix multiply (BMM), can outperform the state-of-the-art MIPS solvers\nby over an order of magnitude, for some -- but not all -- inputs.\n  In this paper, we also present a novel MIPS solution, MAXIMUS, that takes\nadvantage of hardware efficiency and pruning of the search space. Like BMM,\nMAXIMUS is faster than other solvers by up to an order of magnitude, but again\nonly for some inputs. Since no single solution offers the best runtime\nperformance for all inputs, we introduce a new data-dependent optimizer,\nOPTIMUS, that selects online with minimal overhead the best MIPS solver for a\ngiven input. Together, OPTIMUS and MAXIMUS outperform state-of-the-art MIPS\nsolvers by 3.2$\\times$ on average, and up to 10.9$\\times$, on widely studied\nMIPS datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:56:43 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 22:08:15 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 00:52:25 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Abuzaid", "Firas", ""], ["Sethi", "Geet", ""], ["Bailis", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "1706.01508", "submitter": "Morgan Shirley", "authors": "Glencora Borradaile and Morgan Shirley", "title": "Time-dependent shortest paths in bounded treewidth graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a proof that the number of breakpoints in the arrival function\nbetween two terminals in graphs of treewidth $w$ is $n^{O(\\log^2 w)}$ when the\nedge arrival functions are piecewise linear. This is an improvement on the\nbound of $n^{\\Theta(\\log n)}$ by Foschini, Hershberger, and Suri for graphs\nwithout any bound on treewidth. We provide an algorithm for calculating this\narrival function using star-mesh transformations, a generalization of the\nwye-delta-wye transformations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 19:22:36 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Borradaile", "Glencora", ""], ["Shirley", "Morgan", ""]]}, {"id": "1706.01623", "submitter": "Longkun Guo l", "authors": "Longkun Guo and Hong Shen", "title": "Approximation Algorithms for Minimizing Maximum Sensor Movement for Line\n  Barrier Coverage in the Plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a line barrier and a set of mobile sensors distributed in the plane,\nthe Minimizing Maximum Sensor Movement problem (MMSM) for\n\\textcolor{black}{line barrier coverage} is to compute relocation positions for\nthe sensors in the plane such that the barrier is entirely covered by the\nmonitoring area of the sensors while the maximum relocation movement (distance)\nis minimized. Its weaker version, decision MMSM is to determine whether the\nbarrier can be covered by the sensors within a given relocation distance bound\n$D\\in\\mathbb{Z}^{+}$.\n  This paper presents three approximation algorithms for decision MMSM. The\nfirst is a simple greedy approach, which runs in time $O(n\\log n)$ and achieves\na maximum movement $D^{*}+2r_{max}$, where $n$ is the number of the sensors,\n$D^{*}$ is the maximum movement of an optimal solution and $r_{max}$ is the\nmaximum radii of the sensors. The second and the third algorithms improve the\nmaximum movement to $D^{*}+r_{max}$ , running in time $O(n^{7}L)$ and\n$O(R^{2}\\sqrt{\\frac{M}{\\log R}})$ by applying linear programming (LP) rounding\nand maximal matching tchniques respecitvely, where $R=\\sum2r_{i}$, which is\n$O(n)$ in practical scenarios of uniform sensing radius for all sensors, and\n$M\\leq n\\max r_{i}$. Applying the above algorithms for $O(\\log(d_{max}))$ time\nin binary search immediately yields solutions to MMSM with the same performance\nguarantee. In addition, we also give a factor-2 approximation algorithm which\ncan be used to improve the performance of the first three algorithms when\n$r_{max}>D^{*}$. As shown in \\cite{dobrev2015complexity}, the 2-D MMSM problem\nadmits no FPTAS as it is strongly NP-complete, so our algorithms arguably\nachieve the best possible ratio.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 06:51:02 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Guo", "Longkun", ""], ["Shen", "Hong", ""]]}, {"id": "1706.02019", "submitter": "Imed Kacem Prof.", "authors": "Imed Kacem, Christophe Rapine", "title": "Approximation Algorithms for the Open Shop Problem with Delivery Times", "comments": "The short version has been published in the Proceeding of Int Conf on\n  Computers & Industrial Engineering (CIE46), 29-31 October 2016, Tianjin /\n  China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the open shop scheduling problem where the jobs\nhave delivery times. The minimization criterion is the maximum lateness of the\njobs. This problem is known to be NP-hard, even restricted to only 2 machines.\nWe establish that any list scheduling algorithm has a performance ratio of 2.\nFor a fixed number of machines, we design a polynomial time approximation\nscheme (PTAS) which represents the best possible result due to the strong\nNP-hardness of the problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 01:39:37 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Kacem", "Imed", ""], ["Rapine", "Christophe", ""]]}, {"id": "1706.02075", "submitter": "Shmuel Onn", "authors": "Martin Koutecky, Asaf Levin, Syed M. Meesum, Shmuel Onn", "title": "Approximate Shifted Combinatorial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shifted combinatorial optimization is a new nonlinear optimization framework,\nwhich is a broad extension of standard combinatorial optimization, involving\nthe choice of several feasible solutions at a time. It captures well studied\nand diverse problems ranging from congestive to partitioning problems. In\nparticular, every standard combinatorial optimization problem has its shifted\ncounterpart, which is typically much harder. Here we initiate a study of\napproximation algorithms for this broad optimization framework.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 07:36:19 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Koutecky", "Martin", ""], ["Levin", "Asaf", ""], ["Meesum", "Syed M.", ""], ["Onn", "Shmuel", ""]]}, {"id": "1706.02200", "submitter": "Rodolphe Giroudeau", "authors": "Rodolphe Giroudeau (MAORE), Jean-Claude K\\\"onig (MAORE), Benoit\n  Darties (Le2i, MAORE), Gilles Simonin", "title": "Bounds and approximation results for scheduling coupled-tasks with\n  compatibility constraints", "comments": null, "journal-ref": "15th International Conference on Project Management and\n  Scheduling, pp.94-97, 2016", "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is devoted to propose some lower and upper bounds for the\ncoupled-tasks scheduling problem in presence of compatibility constraints\naccording to classical complexity hypothesis ($\\mathcal{P} \\neq \\mathcal{NP}$,\n$\\mathcal{ETH}$). Moreover, we develop an efficient polynomial-time\napproximation algorithm for the specific case for which the topology describing\nthe compatibility constraints is a quasi split-graph.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:14:28 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Giroudeau", "Rodolphe", "", "MAORE"], ["K\u00f6nig", "Jean-Claude", "", "MAORE"], ["Darties", "Benoit", "", "Le2i, MAORE"], ["Simonin", "Gilles", ""]]}, {"id": "1706.02202", "submitter": "Benoit Darties", "authors": "Gilles Simonin (MAORE), Benoit Darties (Le2i, MAORE), Rodolphe\n  Giroudeau (MAORE), Jean-Claude K\\\"onig (MAORE)", "title": "Isomorphic coupled-task scheduling problem with compatibility\n  constraints on a single processor", "comments": null, "journal-ref": "Journal of Scheduling, Springer Verlag, 2011, 14 (5), pp.501-509", "doi": "10.1007/s10951-010-0193-x", "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem presented in this paper is a generalization of the usual\ncoupled-tasks scheduling problem in presence of compatibility constraints. The\nreason behind this study is the data acquisition problem for a submarine\ntorpedo. We investigate a particular configuration for coupled tasks (any task\nis divided into two sub-tasks separated by an idle time), in which the idle\ntime of a coupled task is equal to the sum of durations of its two sub-tasks.\nWe prove -completeness of the minimization of the schedule length, we show that\nfinding a solution to our problem amounts to solving a graph problem, which in\nitself is close to the minimum-disjoint-path cover (min-DCP) problem. We design\na (3a+2b)/(2a+2b)-approximation, where a and b (the processing time of the two\nsub-tasks) are two input data such as a>b>0, and that leads to a ratio between\n3/2 and 5/4. Using a polynomial-time algorithm developed for some class of\ngraph of min-DCP, we show that the ratio decreases to 1.37 .\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:14:50 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Simonin", "Gilles", "", "MAORE"], ["Darties", "Benoit", "", "Le2i, MAORE"], ["Giroudeau", "Rodolphe", "", "MAORE"], ["K\u00f6nig", "Jean-Claude", "", "MAORE"]]}, {"id": "1706.02205", "submitter": "Florian Sch\\\"afer", "authors": "Florian Sch\\\"afer and T. J. Sullivan and Houman Owhadi", "title": "Compression, inversion, and approximate PCA of dense kernel matrices at\n  near-linear computational complexity", "comments": "52 pages. A high level summary of this work can be found under\n  https://f-t-s.github.io/projects/cholesky/", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CC cs.DS cs.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense kernel matrices $\\Theta \\in \\mathbb{R}^{N \\times N}$ obtained from\npoint evaluations of a covariance function $G$ at locations $\\{ x_{i} \\}_{1\n\\leq i \\leq N} \\subset \\mathbb{R}^{d}$ arise in statistics, machine learning,\nand numerical analysis. For covariance functions that are Green's functions of\nelliptic boundary value problems and homogeneously-distributed sampling points,\nwe show how to identify a subset $S \\subset \\{ 1 , \\dots , N \\}^2$, with $\\# S\n= O ( N \\log (N) \\log^{d} ( N /\\epsilon ) )$, such that the zero fill-in\nincomplete Cholesky factorisation of the sparse matrix $\\Theta_{ij} 1_{( i, j )\n\\in S}$ is an $\\epsilon$-approximation of $\\Theta$. This factorisation can\nprovably be obtained in complexity $O ( N \\log( N ) \\log^{d}( N /\\epsilon) )$\nin space and $O ( N \\log^{2}( N ) \\log^{2d}( N /\\epsilon) )$ in time, improving\nupon the state of the art for general elliptic operators; we further present\nnumerical evidence that $d$ can be taken to be the intrinsic dimension of the\ndata set rather than that of the ambient space. The algorithm only needs to\nknow the spatial configuration of the $x_{i}$ and does not require an analytic\nrepresentation of $G$. Furthermore, this factorization straightforwardly\nprovides an approximate sparse PCA with optimal rate of convergence in the\noperator norm. Hence, by using only subsampling and the incomplete Cholesky\nfactorization, we obtain, at nearly linear complexity, the compression,\ninversion and approximate PCA of a large class of covariance matrices. By\ninverting the order of the Cholesky factorization we also obtain a solver for\nelliptic PDE with complexity $O ( N \\log^{d}( N /\\epsilon) )$ in space and $O (\nN \\log^{2d}( N /\\epsilon) )$ in time, improving upon the state of the art for\ngeneral elliptic operators.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:26:14 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 22:22:57 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 19:36:49 GMT"}, {"version": "v4", "created": "Wed, 6 May 2020 20:11:23 GMT"}, {"version": "v5", "created": "Fri, 30 Oct 2020 18:34:14 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sch\u00e4fer", "Florian", ""], ["Sullivan", "T. J.", ""], ["Owhadi", "Houman", ""]]}, {"id": "1706.02214", "submitter": "Benoit Darties", "authors": "Benoit Darties (Le2i), Rodolphe Giroudeau (MAORE), Jean-Claude K\\\"onig\n  (MAORE), Gilles Simonin", "title": "Some complexity and approximation results for coupled-tasks scheduling\n  problem according to topology", "comments": null, "journal-ref": "RAIRO - Operations Research, EDP Sciences, 2016, 50, pp.781 - 795", "doi": "10.1051/ro/2016034", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the makespan minimization coupled-tasks problem in presence of\ncompatibility constraints with a specified topology. In particular, we focus on\nstretched coupled-tasks, i.e. coupled-tasks having the same sub-tasks execution\ntime and idle time duration. We study several problems in framework of classic\ncomplexity and approximation for which the compatibility graph is bipartite\n(star, chain,. . .). In such a context, we design some efficient\npolynomial-time approximation algorithms for an intractable scheduling problem\naccording to some parameters.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:37:35 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Darties", "Benoit", "", "Le2i"], ["Giroudeau", "Rodolphe", "", "MAORE"], ["K\u00f6nig", "Jean-Claude", "", "MAORE"], ["Simonin", "Gilles", ""]]}, {"id": "1706.02586", "submitter": "Amir Ali Ahmadi", "authors": "Amir Ali Ahmadi and Anirudha Majumdar", "title": "DSOS and SDSOS Optimization: More Tractable Alternatives to Sum of\n  Squares and Semidefinite Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, optimization theory has been greatly impacted by the advent\nof sum of squares (SOS) optimization. The reliance of this technique on\nlarge-scale semidefinite programs however, has limited the scale of problems to\nwhich it can be applied. In this paper, we introduce DSOS and SDSOS\noptimization as linear programming and second-order cone programming-based\nalternatives to sum of squares optimization that allow one to trade off\ncomputation time with solution quality. These are optimization problems over\ncertain subsets of sum of squares polynomials (or equivalently subsets of\npositive semidefinite matrices), which can be of interest in general\napplications of semidefinite programming where scalability is a limitation. We\nshow that some basic theorems from SOS optimization which rely on results from\nreal algebraic geometry are still valid for DSOS and SDSOS optimization.\nFurthermore, we show with numerical experiments from diverse application\nareas---polynomial optimization, statistics and machine learning, derivative\npricing, and control theory---that with reasonable tradeoffs in accuracy, we\ncan handle problems at scales that are currently significantly beyond the reach\nof traditional sum of squares approaches. Finally, we provide a review of\nrecent techniques that bridge the gap between our DSOS/SDSOS approach and the\nSOS approach at the expense of additional running time. The Supplementary\nMaterial of the paper introduces an accompanying MATLAB package for DSOS and\nSDSOS optimization.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 13:52:23 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 20:08:30 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2018 19:51:04 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Majumdar", "Anirudha", ""]]}, {"id": "1706.02682", "submitter": "Theja Tulabandhula", "authors": "Arpita Biswas, Ragavendran Gopalakrishnan, Theja Tulabandhula, Asmita\n  Metrewar, Koyel Mukherjee, Raja Subramaniam Thangaraj", "title": "Impact of Detour-Aware Policies on Maximizing Profit in Ridesharing", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides efficient solutions to maximize profit for commercial\nridesharing services, under a pricing model with detour-based discounts for\npassengers. We propose greedy heuristics for real-time ride matching that offer\ndifferent trade-offs between optimality and speed. Simulations on New York City\n(NYC) taxi trip data show that our heuristics are up to 90% optimal and 10^5\ntimes faster than the (necessarily) exponential-time optimal algorithm.\n  Commercial ridesharing service providers generate significant savings by\nmatching multiple ride requests using heuristic methods. The resulting savings\nare typically shared between the service provider (in the form of increased\nprofit) and the ridesharing passengers (in the form of discounts). It is not\nclear a priori how this split should be effected, since higher discounts would\nencourage more ridesharing, thereby increasing total savings, but the fraction\nof savings taken as profit is reduced. We simulate a scenario where the\ndecisions of the passengers to opt for ridesharing depend on the discount\noffered by the service provider. We provide an adaptive learning algorithm\nIDFLA that learns the optimal profit-maximizing discount factor for the\nprovider. An evaluation over NYC data shows that IDFLA, on average, learns the\noptimal discount factor in under 16 iterations.\n  Finally, we investigate the impact of imposing a detour-aware routing policy\nbased on sequential individual rationality, a recently proposed concept. Such\nrestricted policies offer a better ride experience, increasing the provider's\nmarket share, but at the cost of decreased average per-ride profit due to the\nreduced number of matched rides. We construct a model that captures these\nopposing effects, wherein simulations based on NYC data show that a 7% increase\nin market share would suffice to offset the decreased average per-ride profit.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 16:57:44 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Biswas", "Arpita", ""], ["Gopalakrishnan", "Ragavendran", ""], ["Tulabandhula", "Theja", ""], ["Metrewar", "Asmita", ""], ["Mukherjee", "Koyel", ""], ["Thangaraj", "Raja Subramaniam", ""]]}, {"id": "1706.02783", "submitter": "Mathias B{\\ae}k Tejs Knudsen", "authors": "Mathias B{\\ae}k Tejs Knudsen", "title": "Linear Hashing is Awesome", "comments": "A preliminary version appeared at FOCS'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the hash function $h(x) = ((ax+b) \\bmod p) \\bmod n$ where $a,b$\nare chosen uniformly at random from $\\{0,1,\\ldots,p-1\\}$. We prove that when we\nuse $h(x)$ in hashing with chaining to insert $n$ elements into a table of size\n$n$ the expected length of the longest chain is\n$\\tilde{O}\\!\\left(n^{1/3}\\right)$. The proof also generalises to give the same\nbound when we use the multiply-shift hash function by Dietzfelbinger et al.\n[Journal of Algorithms 1997].\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 22:25:46 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Knudsen", "Mathias B\u00e6k Tejs", ""]]}, {"id": "1706.02828", "submitter": "Juan David Arcila Moreno", "authors": "Juan David Arcila Moreno, Santiago Passos and Mauricio Toro", "title": "On-line Assembling Mitochondrial DNA from de novo transcriptome", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused in designing an efficient on-line algorithm to\nreconstruct a DNA sequence and search the genes in it, we assume that the\nsegment have no mutation or reading error, the algorithm is based on de Bruijn\nGraph for reconstructing the DNA from the segments taking k-mers large enough\nno to generate cycles, once the sequence is ready a Boyer-Moore's algorithm\nimplementation is used to search the genes inside de sequence using starts and\nstop codons, this solution give a high performance when all genes can be found,\nand there is no need to read all the segments to reach maximum number of genes,\nbut due to the online nature one cannot be sure about the finals genes given\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 04:04:18 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Moreno", "Juan David Arcila", ""], ["Passos", "Santiago", ""], ["Toro", "Mauricio", ""]]}, {"id": "1706.02885", "submitter": "Satoshi Koide", "authors": "Satoshi Koide, Yukihiro Tadokoro, Chuan Xiao, Yoshiharu Ishikawa", "title": "CiNCT: Compression and retrieval for massive vehicular trajectories via\n  relative movement labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a compressed data structure for moving object\ntrajectories in a road network, which are represented as sequences of road\nedges. Unlike existing compression methods for trajectories in a network, our\nmethod supports pattern matching and decompression from an arbitrary position\nwhile retaining a high compressibility with theoretical guarantees.\nSpecifically, our method is based on FM-index, a fast and compact data\nstructure for pattern matching. To enhance the compression, we incorporate the\nsparsity of road networks into the data structure. In particular, we present\nthe novel concepts of relative movement labeling and PseudoRank, each\ncontributing to significant reductions in data size and query processing time.\nOur theoretical analysis and experimental studies reveal the advantages of our\nproposed method as compared to existing trajectory compression methods and\nFM-index variants.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 10:25:55 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 12:28:12 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Koide", "Satoshi", ""], ["Tadokoro", "Yukihiro", ""], ["Xiao", "Chuan", ""], ["Ishikawa", "Yoshiharu", ""]]}, {"id": "1706.02939", "submitter": "Kyle Fox", "authors": "Pankaj K. Agarwal, Kyle Fox, Oren Salzman", "title": "An Efficient Algorithm for Computing High-Quality Paths amid Polygonal\n  Obstacles", "comments": "A preliminary version of this work appear in the Proceedings of the\n  27th Annual ACM-SIAM Symposium on Discrete Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a path-planning problem amid a set $\\mathcal{O}$ of obstacles in\n$\\mathbb{R}^2$, in which we wish to compute a short path between two points\nwhile also maintaining a high clearance from $\\mathcal{O}$; the clearance of a\npoint is its distance from a nearest obstacle in $\\mathcal{O}$. Specifically,\nthe problem asks for a path minimizing the reciprocal of the clearance\nintegrated over the length of the path. We present the first polynomial-time\napproximation scheme for this problem. Let $n$ be the total number of obstacle\nvertices and let $\\varepsilon \\in (0,1]$. Our algorithm computes in time\n$O(\\frac{n^2}{\\varepsilon ^2} \\log \\frac{n}{\\varepsilon})$ a path of total cost\nat most $(1+\\varepsilon)$ times the cost of the optimal path.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 13:11:37 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Fox", "Kyle", ""], ["Salzman", "Oren", ""]]}, {"id": "1706.03035", "submitter": "Dominik K\\\"oppl", "authors": "Johannes Fischer and Dominik K\\\"oppl", "title": "Practical Evaluation of Lempel-Ziv-78 and Lempel-Ziv-Welch Tries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first thorough practical study of the Lempel-Ziv-78 and the\nLempel-Ziv-Welch computation based on trie data structures. With a careful\nselection of trie representations we can beat well-tuned popular trie data\nstructures like Judy, m-Bonsai or Cedar.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 16:49:35 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Fischer", "Johannes", ""], ["K\u00f6ppl", "Dominik", ""]]}, {"id": "1706.03065", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Towards balanced clustering - part 1 (preliminaries)", "comments": "21 pages, 17 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article contains a preliminary glance at balanced clustering problems.\nBasic balanced structures and combinatorial balanced problems are briefly\ndescribed. A special attention is targeted to various balance/unbalance indices\n(including some new versions of the indices): by cluster cardinality, by\ncluster weights, by inter-cluster edge/arc weights, by cluster element\nstructure (for element multi-type clustering). Further, versions of\noptimization clustering problems are suggested (including multicriteria problem\nformulations). Illustrative numerical examples describe calculation of balance\nindices and element multi-type balance clustering problems (including example\nfor design of student teams).\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 19:34:58 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1706.03108", "submitter": "Gengchun Xu", "authors": "Qian-Ping Gu and Gengchun Xu", "title": "Constant Query Time $(1 + \\epsilon)$-Approximate Distance Oracle for\n  Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a $(1+\\epsilon)$-approximate distance oracle with $O(1)$ query time\nfor an undirected planar graph $G$ with $n$ vertices and non-negative edge\nlengths. For $\\epsilon>0$ and any two vertices $u$ and $v$ in $G$, our oracle\ngives a distance $\\tilde{d}(u,v)$ with stretch $(1+\\epsilon)$ in $O(1)$ time.\nThe oracle has size $O(n\\log n ((\\log n)/\\epsilon+f(\\epsilon)))$ and\npre-processing time $O(n\\log n((\\log^3 n)/\\epsilon^2+f(\\epsilon)))$, where\n$f(\\epsilon)=2^{O(1/\\epsilon)}$. This is the first $(1+\\epsilon)$-approximate\ndistance oracle with $O(1)$ query time independent of $\\epsilon$ and the size\nand pre-processing time nearly linear in $n$, and improves the query time\n$O(1/\\epsilon)$ of previous $(1+\\epsilon)$-approximate distance oracle with\nsize nearly linear in $n$.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 20:02:36 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Gu", "Qian-Ping", ""], ["Xu", "Gengchun", ""]]}, {"id": "1706.03175", "submitter": "Zhao Song", "authors": "Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, Inderjit S.\n  Dhillon", "title": "Recovery Guarantees for One-hidden-layer Neural Networks", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider regression problems with one-hidden-layer neural\nnetworks (1NNs). We distill some properties of activation functions that lead\nto $\\mathit{local~strong~convexity}$ in the neighborhood of the ground-truth\nparameters for the 1NN squared-loss objective. Most popular nonlinear\nactivation functions satisfy the distilled properties, including rectified\nlinear units (ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation\nfunctions that are also smooth, we show $\\mathit{local~linear~convergence}$\nguarantees of gradient descent under a resampling rule. For homogeneous\nactivations, we show tensor methods are able to initialize the parameters to\nfall into the local strong convexity region. As a result, tensor initialization\nfollowed by gradient descent is guaranteed to recover the ground truth with\nsample complexity $ d \\cdot \\log(1/\\epsilon) \\cdot \\mathrm{poly}(k,\\lambda )$\nand computational complexity $n\\cdot d \\cdot \\mathrm{poly}(k,\\lambda) $ for\nsmooth homogeneous activations with high probability, where $d$ is the\ndimension of the input, $k$ ($k\\leq d$) is the number of hidden nodes,\n$\\lambda$ is a conditioning property of the ground-truth parameter matrix\nbetween the input layer and the hidden layer, $\\epsilon$ is the targeted\nprecision and $n$ is the number of samples. To the best of our knowledge, this\nis the first work that provides recovery guarantees for 1NNs with both sample\ncomplexity and computational complexity $\\mathit{linear}$ in the input\ndimension and $\\mathit{logarithmic}$ in the precision.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 02:56:39 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zhong", "Kai", ""], ["Song", "Zhao", ""], ["Jain", "Prateek", ""], ["Bartlett", "Peter L.", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1706.03177", "submitter": "Ren\\'e van Bevern", "authors": "Matthias Bentert and Ren\\'e van Bevern and Andr\\'e Nichterlein and\n  Rolf Niedermeier and Pavel V. Smirnov", "title": "Parameterized Algorithms for Power-Efficiently Connecting Wireless\n  Sensor Networks: Theory and Experiments", "comments": "Additional experiments, lower bounds strengthened to metric case,\n  added kernelization lower bounds", "journal-ref": null, "doi": "10.1287/ijoc.2020.1045", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an NP-hard problem motivated by energy-efficiently maintaining the\nconnectivity of a symmetric wireless communication network: Given an\nedge-weighted $n$-vertex graph, find a connected spanning subgraph of minimum\ncost, where the cost is determined by letting each vertex pay the most\nexpensive edge incident to it in the subgraph. On the negative side, we show\nthat $o(\\log n)$-approximating the difference $d$ between the optimal solution\ncost and a natural lower bound is NP-hard and that, under the Exponential Time\nHypothesis, there are no exact algorithms running in $2^{o(n)}$ time or in\n$f(d)\\cdot n^{O(1)}$ time for any computable function $f$. Moreover, we show\nthat the special case of connecting $c$ network components with minimum\nadditional cost generally cannot be polynomial-time reduced to instances of\nsize $c^{O(1)}$ unless the polynomial-time hierarchy collapses. On the positive\nside, we provide an algorithm that reconnects $O(\\log n)$ connected components\nwith minimum additional cost in polynomial time. These algorithms are motivated\nby application scenarios of monitoring areas or where an existing sensor\nnetwork may fall apart into several connected components due to sensor faults.\nIn experiments, the algorithm outperforms CPLEX with known ILP formulations\nwhen $n$ is sufficiently large compared to $c$.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 03:45:43 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 08:22:04 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 08:35:43 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bentert", "Matthias", ""], ["van Bevern", "Ren\u00e9", ""], ["Nichterlein", "Andr\u00e9", ""], ["Niedermeier", "Rolf", ""], ["Smirnov", "Pavel V.", ""]]}, {"id": "1706.03184", "submitter": "Warut Suksompong", "authors": "Charles E. Leiserson, Tao B. Schardl, Warut Suksompong", "title": "Upper Bounds on Number of Steals in Rooted Trees", "comments": "18 pages, 5 figures", "journal-ref": "Theory of Computing Systems, 58(2):223-240 (2016)", "doi": "10.1007/s00224-015-9613-9", "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by applications in parallel computing, we analyze the setting of\nwork stealing in multithreaded computations. We obtain tight upper bounds on\nthe number of steals when the computation can be modeled by rooted trees. In\nparticular, we show that if the computation with $n$ processors starts with one\nprocessor having a complete $k$-ary tree of height $h$ (and the remaining $n-1$\nprocessors having nothing), the maximum possible number of steals is\n$\\sum_{i=1}^n(k-1)^i\\binom{h}{i}$.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 05:38:20 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 16:28:29 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Leiserson", "Charles E.", ""], ["Schardl", "Tao B.", ""], ["Suksompong", "Warut", ""]]}, {"id": "1706.03316", "submitter": "Kai Zheng", "authors": "Kai Zheng, Wenlong Mou, Liwei Wang", "title": "Collect at Once, Use Effectively: Making Non-interactive Locally Private\n  Learning Possible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-interactive Local Differential Privacy (LDP) requires data analysts to\ncollect data from users through noisy channel at once. In this paper, we extend\nthe frontiers of Non-interactive LDP learning and estimation from several\naspects. For learning with smooth generalized linear losses, we propose an\napproximate stochastic gradient oracle estimated from non-interactive LDP\nchannel, using Chebyshev expansion. Combined with inexact gradient methods, we\nobtain an efficient algorithm with quasi-polynomial sample complexity bound.\nFor the high-dimensional world, we discover that under $\\ell_2$-norm assumption\non data points, high-dimensional sparse linear regression and mean estimation\ncan be achieved with logarithmic dependence on dimension, using random\nprojection and approximate recovery. We also extend our methods to Kernel Ridge\nRegression. Our work is the first one that makes learning and estimation\npossible for a broad range of learning tasks under non-interactive LDP model.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 07:04:50 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zheng", "Kai", ""], ["Mou", "Wenlong", ""], ["Wang", "Liwei", ""]]}, {"id": "1706.03412", "submitter": "Evgeny Burnaev", "authors": "Vladislav Ishimtsev, Ivan Nazarov, Alexander Bernstein and Evgeny\n  Burnaev", "title": "Conformal k-NN Anomaly Detector for Univariate Data Streams", "comments": "15 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies in time-series data give essential and often actionable information\nin many applications. In this paper we consider a model-free anomaly detection\nmethod for univariate time-series which adapts to non-stationarity in the data\nstream and provides probabilistic abnormality scores based on the conformal\nprediction paradigm. Despite its simplicity the method performs on par with\ncomplex prediction-based models on the Numenta Anomaly Detection benchmark and\nthe Yahoo! S5 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 21:45:24 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Ishimtsev", "Vladislav", ""], ["Nazarov", "Ivan", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.03473", "submitter": "Eunpyeong Hong", "authors": "Eunpyeong Hong, Yasuaki Kobayashi, Akihiro Yamamoto", "title": "Improved Methods for Computing Distances between Unordered Trees Using\n  Integer Programming", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kondo et al. (DS 2014) proposed methods for computing distances between\nunordered rooted trees by transforming an instance of the distance computing\nproblem into an instance of the integer programming problem. They showed that\nthe tree edit distance, segmental distance, and bottom-up segmental distance\nproblem can be respectively transformed into an integer program which has\n$O(nm)$ variables and $O(n^2m^2)$ constraints, where $n$ and $m$ are the number\nof nodes of input trees. In this work, we propose new integer programming\nformulations for these three distances and the bottom-up distance by applying\ndynamic programming approach. We divide the tree edit distance problem into\n$O(nm)$ subproblems each of which has only $O(n + m)$ constraints. For the\nother three distances, each subproblem can be reduced to a maximum weighted\nmatching problem in a bipartite graph which can be solved in polynomial time.\nIn order to evaluate our methods, we compare our method to the previous one due\nto Kondo et al. The experimental results show that the performance of our\nmethods have been improved remarkably compared to that of the previous method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 05:47:23 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Hong", "Eunpyeong", ""], ["Kobayashi", "Yasuaki", ""], ["Yamamoto", "Akihiro", ""]]}, {"id": "1706.03568", "submitter": "Manuel Malatyali", "authors": "Pascal Bemmann, Felix Biermeier, Jan B\\\"urmann, Arne Kemper, Till\n  Knollmann, Steffen Knorr, Nils Kothe, Alexander M\\\"acker, Manuel Malatyali,\n  Friedhelm Meyer auf der Heide, S\\\"oren Riechers, Johannes Schaefer, Jannik\n  Sundermeier", "title": "Monitoring of Domain-Related Problems in Distributed Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a network in which $n$ distributed nodes are connected to a single\nserver. Each node continuously observes a data stream consisting of one value\nper discrete time step. The server has to continuously monitor a given\nparameter defined over all information available at the distributed nodes. That\nis, in any time step $t$, it has to compute an output based on all values\ncurrently observed across all streams. To do so, nodes can send messages to the\nserver and the server can broadcast messages to the nodes. The objective is the\nminimisation of communication while allowing the server to compute the desired\noutput.\n  We consider monitoring problems related to the domain $D_t$ defined to be the\nset of values observed by at least one node at time $t$. We provide randomised\nalgorithms for monitoring $D_t$, (approximations of) the size $|D_t|$ and the\nfrequencies of all members of $D_t$. Besides worst-case bounds, we also obtain\nimproved results when inputs are parameterised according to the similarity of\nobservations between consecutive time steps. This parameterisation allows to\nexclude inputs with rapid and heavy changes, which usually lead to the\nworst-case bounds but might be rather artificial in certain scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 11:25:04 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Bemmann", "Pascal", ""], ["Biermeier", "Felix", ""], ["B\u00fcrmann", "Jan", ""], ["Kemper", "Arne", ""], ["Knollmann", "Till", ""], ["Knorr", "Steffen", ""], ["Kothe", "Nils", ""], ["M\u00e4cker", "Alexander", ""], ["Malatyali", "Manuel", ""], ["der Heide", "Friedhelm Meyer auf", ""], ["Riechers", "S\u00f6ren", ""], ["Schaefer", "Johannes", ""], ["Sundermeier", "Jannik", ""]]}, {"id": "1706.03583", "submitter": "Baharan Mirzasoleiman", "authors": "Baharan Mirzasoleiman, Stefanie Jegelka, Andreas Krause", "title": "Streaming Non-monotone Submodular Maximization: Personalized Video\n  Summarization on the Fly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for real time analysis of rapidly producing data streams (e.g.,\nvideo and image streams) motivated the design of streaming algorithms that can\nefficiently extract and summarize useful information from massive data \"on the\nfly\". Such problems can often be reduced to maximizing a submodular set\nfunction subject to various constraints. While efficient streaming methods have\nbeen recently developed for monotone submodular maximization, in a wide range\nof applications, such as video summarization, the underlying utility function\nis non-monotone, and there are often various constraints imposed on the\noptimization problem to consider privacy or personalization. We develop the\nfirst efficient single pass streaming algorithm, Streaming Local Search, that\nfor any streaming monotone submodular maximization algorithm with approximation\nguarantee $\\alpha$ under a collection of independence systems ${\\cal I}$,\nprovides a constant $1/\\big(1+2/\\sqrt{\\alpha}+1/\\alpha\n+2d(1+\\sqrt{\\alpha})\\big)$ approximation guarantee for maximizing a\nnon-monotone submodular function under the intersection of ${\\cal I}$ and $d$\nknapsack constraints. Our experiments show that for video summarization, our\nmethod runs more than 1700 times faster than previous work, while maintaining\npractically the same performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 11:58:57 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 14:57:15 GMT"}, {"version": "v3", "created": "Tue, 26 Dec 2017 09:58:13 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Mirzasoleiman", "Baharan", ""], ["Jegelka", "Stefanie", ""], ["Krause", "Andreas", ""]]}, {"id": "1706.03607", "submitter": "Edith Cohen", "authors": "Edith Cohen, Shiri Chechik, Haim Kaplan", "title": "Clustering Small Samples with Quality Guarantees: Adaptivity with\n  One2all pps", "comments": "17 pages, 2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering of data points is a fundamental tool in data analysis. We consider\npoints $X$ in a relaxed metric space, where the triangle inequality holds\nwithin a constant factor. The {\\em cost} of clustering $X$ by $Q$ is\n$V(Q)=\\sum_{x\\in X} d_{xQ}$. Two basic tasks, parametrized by $k \\geq 1$, are\n{\\em cost estimation}, which returns (approximate) $V(Q)$ for queries $Q$ such\nthat $|Q|=k$ and {\\em clustering}, which returns an (approximate) minimizer of\n$V(Q)$ of size $|Q|=k$. With very large data sets $X$, we seek efficient\nconstructions of small samples that act as surrogates to the full data for\nperforming these tasks. Existing constructions that provide quality guarantees\nare either worst-case, and unable to benefit from structure of real data sets,\nor make explicit strong assumptions on the structure. We show here how to avoid\nboth these pitfalls using adaptive designs.\n  At the core of our design is the {\\em one2all} construction of\nmulti-objective probability-proportional-to-size (pps) samples: Given a set $M$\nof centroids and $\\alpha \\geq 1$, one2all efficiently assigns probabilities to\npoints so that the clustering cost of {\\em each} $Q$ with cost $V(Q) \\geq\nV(M)/\\alpha$ can be estimated well from a sample of size $O(\\alpha\n|M|\\epsilon^{-2})$. For cost queries, we can obtain worst-case sample size\n$O(k\\epsilon^{-2})$ by applying one2all to a bicriteria approximation $M$, but\nwe adaptively balance $|M|$ and $\\alpha$ to further reduce sample size. For\nclustering, we design an adaptive wrapper that applies a base clustering\nalgorithm to a sample $S$. Our wrapper uses the smallest sample that provides\nstatistical guarantees that the quality of the clustering on the sample carries\nover to the full data set. We demonstrate experimentally the huge gains of\nusing our adaptive instead of worst-case methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 13:05:46 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 10:27:20 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Cohen", "Edith", ""], ["Chechik", "Shiri", ""], ["Kaplan", "Haim", ""]]}, {"id": "1706.03698", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Felix Reidl, Magnus Wahlstr\\\"om, Meirav Zehavi", "title": "Designing Deterministic Polynomial-Space Algorithms by Color-Coding\n  Multivariate Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several powerful techniques have been developed to design\n{\\em randomized} polynomial-space parameterized algorithms. In this paper, we\nintroduce an enhancement of color coding to design deterministic\npolynomial-space parameterized algorithms. Our approach aims at reducing the\nnumber of random choices by exploiting the special structure of a solution.\nUsing our approach, we derive the following deterministic algorithms (see\nIntroduction for problem definitions).\n  1. Polynomial-space $O^*(3.86^k)$-time (exponential-space $O^*(3.41^k)$-time)\nalgorithm for {\\sc $k$-Internal Out-Branching}, improving upon the previously\nfastest {\\em exponential-space} $O^*(5.14^k)$-time algorithm for this problem.\n  2. Polynomial-space $O^*((2e)^{k+o(k)})$-time (exponential-space\n$O^*(4.32^k)$-time) algorithm for {\\sc $k$-Colorful Out-Branching} on\narc-colored digraphs and {\\sc $k$-Colorful Perfect Matching} on planar\nedge-colored graphs.\n  To obtain our polynomial space algorithms, we show that $(n,k,\\alpha\nk)$-splitters ($\\alpha\\ge 1$) and in particular $(n,k)$-perfect hash families\ncan be enumerated one by one with polynomial delay.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 15:47:38 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 14:25:50 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Gutin", "Gregory", ""], ["Reidl", "Felix", ""], ["Wahlstr\u00f6m", "Magnus", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1706.03887", "submitter": "Lin Yang", "authors": "Vladimir Braverman, Gereon Frahling, Harry Lang, Christian Sohler, Lin\n  F. Yang", "title": "Clustering High Dimensional Dynamic Data Streams", "comments": "33 pages, a preliminary version of this paper is presented on ICML\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present data streaming algorithms for the $k$-median problem in\nhigh-dimensional dynamic geometric data streams, i.e. streams allowing both\ninsertions and deletions of points from a discrete Euclidean space $\\{1, 2,\n\\ldots \\Delta\\}^d$. Our algorithms use $k \\epsilon^{-2} poly(d \\log \\Delta)$\nspace/time and maintain with high probability a small weighted set of points (a\ncoreset) such that for every set of $k$ centers the cost of the coreset\n$(1+\\epsilon)$-approximates the cost of the streamed point set. We also provide\nalgorithms that guarantee only positive weights in the coreset with additional\nlogarithmic factors in the space and time complexities. We can use this\npositively-weighted coreset to compute a $(1+\\epsilon)$-approximation for the\n$k$-median problem by any efficient offline $k$-median algorithm. All previous\nalgorithms for computing a $(1+\\epsilon)$-approximation for the $k$-median\nproblem over dynamic data streams required space and time exponential in $d$.\nOur algorithms can be generalized to metric spaces of bounded doubling\ndimension.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 02:05:11 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Braverman", "Vladimir", ""], ["Frahling", "Gereon", ""], ["Lang", "Harry", ""], ["Sohler", "Christian", ""], ["Yang", "Lin F.", ""]]}, {"id": "1706.03951", "submitter": "Shmuel Onn", "authors": "Antoine Deza, Asaf Levin, Syed M. Meesum, Shmuel Onn", "title": "Optimization over Degree Sequences", "comments": null, "journal-ref": "SIAM Journal on Discrete Mathematics, 32:2067--2079, 2018", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study the problem of optimizing arbitrary functions over\ndegree sequences of hypergraphs and multihypergraphs. We show that over\nmultihypergraphs the problem can be solved in polynomial time. For hypergraphs,\nwe show that deciding if a given sequence is the degree sequence of a\n3-hypergraph is NP-complete, thereby solving a 30 year long open problem. This\nimplies that optimization over hypergraphs is hard already for simple concave\nfunctions. In contrast, we show that for graphs, if the functions at vertices\nare the same, then the problem is polynomial time solvable. We also provide\npositive results for convex optimization over multihypergraphs and graphs and\nexploit connections to degree sequence polytopes and threshold graphs. We then\nelaborate on connections to the emerging theory of shifted combinatorial\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:21:50 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 10:44:01 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Deza", "Antoine", ""], ["Levin", "Asaf", ""], ["Meesum", "Syed M.", ""], ["Onn", "Shmuel", ""]]}, {"id": "1706.04065", "submitter": "Marcin Pilipczuk", "authors": "Tomasz Kociumaka and Marcin Pilipczuk", "title": "Deleting vertices to graphs of bounded genus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a problem of deleting a minimum number of vertices from a graph\nto obtain a graph embeddable on a surface of a given Euler genus is solvable in\ntime $2^{C_g \\cdot k^2 \\log k} n^{O(1)}$, where $k$ is the size of the deletion\nset, $C_g$ is a constant depending on the Euler genus $g$ of the target\nsurface, and $n$ is the size of the input graph. On the way to this result, we\ndevelop an algorithm solving the problem in question in time $2^{O((t+g) \\log\n(t+g))} n$, given a tree decomposition of the input graph of width $t$. The\nresults generalize previous algorithms for the surface being a sphere by Marx\nand Schlotter [Algorithmica 2012], Kawarabayashi [FOCS 2009], and Jansen,\nLokshtanov, and Saurabh [SODA 2014].\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 13:49:36 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Kociumaka", "Tomasz", ""], ["Pilipczuk", "Marcin", ""]]}, {"id": "1706.04082", "submitter": "Stephen Smith", "authors": "Bahman Gharesifard and Stephen L. Smith", "title": "Distributed Submodular Maximization with Limited Information", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of distributed submodular maximization problems in which\neach agent must choose a single strategy from its strategy set. The global\nobjective is to maximize a submodular function of the strategies chosen by each\nagent. When choosing a strategy, each agent has access to only a limited number\nof other agents' choices. For each of its strategies, an agent can evaluate its\nmarginal contribution to the global objective given its information. The main\nobjective is to investigate how this limitation of information about the\nstrategies chosen by other agents affects the performance when agents make\nchoices according to a local greedy algorithm. In particular, we provide lower\nbounds on the performance of greedy algorithms for submodular maximization,\nwhich depend on the clique number of a graph that captures the information\nstructure. We also characterize graph-theoretic upper bounds in terms of the\nchromatic number of the graph. Finally, we demonstrate how certain graph\nproperties limit the performance of the greedy algorithm. Simulations on\nseveral common models for random networks demonstrate our results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 15:05:39 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Gharesifard", "Bahman", ""], ["Smith", "Stephen L.", ""]]}, {"id": "1706.04097", "submitter": "Yingyu Liang", "authors": "Yuanzhi Li, Yingyu Liang", "title": "Provable Alternating Gradient Descent for Non-negative Matrix\n  Factorization with Strong Correlations", "comments": "Accepted to the International Conference on Machine Learning (ICML),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization is a basic tool for decomposing data into\nthe feature and weight matrices under non-negativity constraints, and in\npractice is often solved in the alternating minimization framework. However, it\nis unclear whether such algorithms can recover the ground-truth feature matrix\nwhen the weights for different features are highly correlated, which is common\nin applications. This paper proposes a simple and natural alternating gradient\ndescent based algorithm, and shows that with a mild initialization it provably\nrecovers the ground-truth in the presence of strong correlations. In most\ninteresting cases, the correlation can be in the same order as the highest\npossible. Our analysis also reveals its several favorable features including\nrobustness to noise. We complement our theoretical results with empirical\nstudies on semi-synthetic datasets, demonstrating its advantage over several\npopular methods in recovering the ground-truth.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 14:39:59 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""]]}, {"id": "1706.04178", "submitter": "Jerry Li", "authors": "Dan Alistarh, Justin Kopinsky, Jerry Li, Giorgi Nadiradze", "title": "The Power of Choice in Priority Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following random process: we are given $n$ queues, into which\nelements of increasing labels are inserted uniformly at random. To remove an\nelement, we pick two queues at random, and remove the element of lower label\n(higher priority) among the two. The cost of a removal is the rank of the label\nremoved, among labels still present in any of the queues, that is, the distance\nfrom the optimal choice at each step. Variants of this strategy are prevalent\nin state-of-the-art concurrent priority queue implementations. Nonetheless, it\nis not known whether such implementations provide any rank guarantees, even in\na sequential model.\n  We answer this question, showing that this strategy provides surprisingly\nstrong guarantees: Although the single-choice process, where we always insert\nand remove from a single randomly chosen queue, has degrading cost, going to\ninfinity as we increase the number of steps, in the two choice process, the\nexpected rank of a removed element is $O( n )$ while the expected worst-case\ncost is $O( n \\log n )$. These bounds are tight, and hold irrespective of the\nnumber of steps for which we run the process.\n  The argument is based on a new technical connection between \"heavily loaded\"\nballs-into-bins processes and priority scheduling.\n  Our analytic results inspire a new concurrent priority queue implementation,\nwhich improves upon the state of the art in terms of practical performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 17:45:20 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Alistarh", "Dan", ""], ["Kopinsky", "Justin", ""], ["Li", "Jerry", ""], ["Nadiradze", "Giorgi", ""]]}, {"id": "1706.04225", "submitter": "Joel Friedman", "authors": "Joel Friedman", "title": "Inner Rank and Lower Bounds for Matrix Multiplication", "comments": "Errors in many of the results (starting with those of Section 4) due\n  to an \"exchange of indices.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a notion of {\\em inner rank} as a tool for obtaining lower bounds\non the rank of matrix multiplication tensors. We use it to give a short proof\nthat the border rank (and therefore rank) of the tensor associated with\n$n\\times n$ matrix multiplication over an arbitrary field is at least\n$2n^2-n+1$. While inner rank does not provide improvements to currently known\nlower bounds, we argue that this notion merits further study.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 19:04:00 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 18:39:49 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Friedman", "Joel", ""]]}, {"id": "1706.04255", "submitter": "Petr Golovach", "authors": "Fedor V. Fomin, Petr A. Golovach and Dimitrios M. Thilikos", "title": "Structured Connectivity Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the algorithmic study of the following \"structured augmentation\"\nquestion: is it possible to increase the connectivity of a given graph G by\nsuperposing it with another given graph H? More precisely, graph F is the\nsuperposition of G and H with respect to injective mapping \\phi: V(H)->V(G) if\nevery edge uv of F is either an edge of G, or \\phi^{-1}(u)\\phi^{-1}(v) is an\nedge of H. We consider the following optimization problem. Given graphs G,H,\nand a weight function \\omega assigning non-negative weights to pairs of\nvertices of V(G), the task is to find \\varphi of minimum weight\n\\omega(\\phi)=\\sum_{xy\\in E(H)}\\omega(\\phi(x)\\varphi(y)) such that the edge\nconnectivity of the superposition F of G and H with respect to \\phi is higher\nthan the edge connectivity of G. Our main result is the following \"dichotomy\"\ncomplexity classification. We say that a class of graphs C has bounded\nvertex-cover number, if there is a constant t depending on C only such that the\nvertex-cover number of every graph from C does not exceed t. We show that for\nevery class of graphs C with bounded vertex-cover number, the problems of\nsuperposing into a connected graph F and to 2-edge connected graph F, are\nsolvable in polynomial time when H\\in C. On the other hand, for any hereditary\nclass C with unbounded vertex-cover number, both problems are NP-hard when H\\in\nC. For the unweighted variants of structured augmentation problems, i.e. the\nproblems where the task is to identify whether there is a superposition of\ngraphs of required connectivity, we provide necessary and sufficient\ncombinatorial conditions on the existence of such superpositions. These\nconditions imply polynomial time algorithms solving the unweighted variants of\nthe problems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 21:06:37 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "1706.04368", "submitter": "Lorenzo Severini", "authors": "Gianlorenzo D'Angelo, Lorenzo Severini, Yllka Velaj", "title": "Recommending links through influence maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The link recommendation problem consists in suggesting a set of links to the\nusers of a social network in order to increase their social circles and the\nconnectivity of the network. Link recommendation is extensively studied in the\ncontext of social networks and of general complex networks due to its wide\nrange of applications. Most of the existing link recommendation methods\nestimate the likelihood that a link is adopted by users and recommend links\nthat are likely to be established. However, most of such methods overlook the\nimpact that the suggested links have on the capability of the network to spread\ninformation. Indeed, such capability is directly correlated with both the\nengagement of a single user and the revenue of online social networks. In this\npaper, we study link recommendation systems from the point of view of\ninformation diffusion. In detail, we consider the problem in which we are\nallowed to spend a given budget to create new links so to suggest a bounded\nnumber of possible persons to whom become friend in order to maximize the\ninfluence of a given set of nodes. We model the influence diffusion in a\nnetwork with the popular Independent Cascade model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 09:01:00 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["D'Angelo", "Gianlorenzo", ""], ["Severini", "Lorenzo", ""], ["Velaj", "Yllka", ""]]}, {"id": "1706.04680", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Lorenzo Orecchia", "title": "Accelerated Extra-Gradient Descent: A Novel Accelerated First-Order\n  Method", "comments": "Appeared in Proc. ITCS'18, conference version available at:\n  http://drops.dagstuhl.de/opus/volltexte/2018/8356/", "journal-ref": null, "doi": "10.4230/LIPIcs.ITCS.2018.23", "report-no": null, "categories": "math.OC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel accelerated first-order method that achieves the\nasymptotically optimal convergence rate for smooth functions in the first-order\noracle model. To this day, Nesterov's Accelerated Gradient Descent (AGD) and\nvariations thereof were the only methods achieving acceleration in this\nstandard blackbox model. In contrast, our algorithm is significantly different\nfrom AGD, as it relies on a predictor-corrector approach similar to that used\nby Mirror-Prox [Nemirovski, 2004] and Extra-Gradient Descent [Korpelevich,\n1977] in the solution of convex-concave saddle point problems. For this reason,\nwe dub our algorithm Accelerated Extra-Gradient Descent (AXGD). Its\nconstruction is motivated by the discretization of an accelerated\ncontinuous-time dynamics [Krichene et al., 2015] using the classical method of\nimplicit Euler discretization. Our analysis explicitly shows the effects of\ndiscretization through a conceptually novel primal-dual viewpoint. Moreover, we\nshow that the method is quite general: it attains optimal convergence rates for\nother classes of objectives (e.g., those with generalized smoothness properties\nor that are non-smooth and Lipschitz-continuous) using the appropriate choices\nof step lengths. Finally, we present experiments showing that our algorithm\nmatches the performance of Nesterov's method, while appearing more robust to\nnoise in some cases.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 22:07:18 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 20:44:41 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1706.04708", "submitter": "Matias Korman", "authors": "Jean-Fran\\c{c}ois Baffier and Yago Diez and Matias Korman", "title": "Experimental Study of Compressed Stack Algorithms in Limited Memory\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\em compressed stack} is a data structure designed by Barba {\\em et al.}\n(Algorithmica 2015) that allows to reduce the amount of memory needed by an\nalgorithm (at the cost of increasing its runtime). In this paper we introduce\nthe first implementation of this data structure and make its source code\npublicly available.\n  Together with the implementation we analyze the performance of the compressed\nstack. In our synthetic experiments, considering different test scenarios and\nusing data sizes ranging up to $2^{30}$ elements, we compare it with the\nclassic (uncompressed) stack, both in terms of runtime and memory used.\n  Our experiments show that the compressed stack needs significantly less\nmemory than the usual stack (this difference is significant for inputs\ncontaining $2000$ or more elements). Overall, with a proper choice of\nparameters, we can save a significant amount of space (from two to four orders\nof magnitude) with a small increase in the runtime ($2.32$ times slower on\naverage than the classic stack). These results holds even in test scenarios\nspecifically designed to be challenging for the compressed stack.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 00:47:55 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Baffier", "Jean-Fran\u00e7ois", ""], ["Diez", "Yago", ""], ["Korman", "Matias", ""]]}, {"id": "1706.04722", "submitter": "Hung Cao", "authors": "Hung Cao and Monica Wachowicz", "title": "The design of a streaming analytical workflow for processing massive\n  transit feeds", "comments": "data ingestion, data contextualization, Hadoop MapReduce, transit\n  networks, streaming data analytics, mobility context. Present at the 2nd\n  International Symposium on Spatiotemporal Computing August 7-9 2017 at\n  Harvard University, Cambridge, Massachusetts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving and analyzing transit feeds relies on working with analytical\nworkflows that can handle the massive volume of data streams that are relevant\nto understand the dynamics of transit networks which are entirely deterministic\nin the geographical space in which they takes place. In this paper, we consider\nthe fundamental issues in developing a streaming analytical workflow for\nanalyzing the continuous arrival of multiple, unbounded transit data feeds for\nautomatically processing and enriching them with additional information\ncontaining higher level concepts accordingly to a particular mobility context.\nThis workflow consists of three tasks: (1) stream data retrieval for creating\ntime windows; (2) data cleaning for handling missing data, overlap data or\nredundant data; and (3) data contextualization for computing actual arrival and\ndeparture times as well as the stops and moves during a bus trip, and also\nperforming mobility context computation. The workflow was implemented in a\nHadoop cloud ecosystem using data streams from the CODIAC Transit System of the\ncity of Moncton, NB. The Map() function of MapReduce is used to retrieve and\nbundle data streams into numerous clusters which are subsequently handled in a\nparallel manner by the Reduce() function in order to execute the data\ncontextualization step. The results validate the need for cloud computing for\nachieving high performance and scalability, however, due to the delay in\ncomputing and networking, it is clear that data cleaning tasks should not only\nbe deployed using a cloud environment, paving the way to combine it with fog\ncomputing in the near future.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 02:23:51 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 16:36:17 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Cao", "Hung", ""], ["Wachowicz", "Monica", ""]]}, {"id": "1706.04746", "submitter": "Yannic Maus", "authors": "Mohsen Ghaffari, Juho Hirvonen, Fabian Kuhn, Yannic Maus, Jukka\n  Suomela, Jara Uitto", "title": "Improved Distributed Degree Splitting and Edge Coloring", "comments": null, "journal-ref": null, "doi": "10.1007/s00446-018-00346-8", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree splitting problem requires coloring the edges of a graph red or\nblue such that each node has almost the same number of edges in each color, up\nto a small additive discrepancy. The directed variant of the problem requires\norienting the edges such that each node has almost the same number of incoming\nand outgoing edges, again up to a small additive discrepancy.\n  We present deterministic distributed algorithms for both variants, which\nimprove on their counterparts presented by Ghaffari and Su [SODA'17]: our\nalgorithms are significantly simpler and faster, and have a much smaller\ndiscrepancy. This also leads to a faster and simpler deterministic algorithm\nfor $(2+o(1))\\Delta$-edge-coloring, improving on that of Ghaffari and Su.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 06:00:07 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 20:00:00 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Hirvonen", "Juho", ""], ["Kuhn", "Fabian", ""], ["Maus", "Yannic", ""], ["Suomela", "Jukka", ""], ["Uitto", "Jara", ""]]}, {"id": "1706.04764", "submitter": "Yanhao Wang", "authors": "Yanhao Wang and Yuchen Li and Kian-Lee Tan", "title": "Efficient Representative Subset Selection over Sliding Windows", "comments": "26 pages, 9 figures, to appear in IEEE Transactions on Knowledge and\n  Data Engineering (TKDE). 2018", "journal-ref": null, "doi": "10.1109/TKDE.2018.2854182", "report-no": null, "categories": "cs.DS cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Representative subset selection (RSS) is an important tool for users to draw\ninsights from massive datasets. Existing literature models RSS as the\nsubmodular maximization problem to capture the \"diminishing returns\" property\nof the representativeness of selected subsets, but often only has a single\nconstraint (e.g., cardinality), which limits its applications in many\nreal-world problems. To capture the data recency issue and support different\ntypes of constraints, we formulate dynamic RSS in data streams as maximizing\nsubmodular functions subject to general $d$-knapsack constraints (SMDK) over\nsliding windows. We propose a \\textsc{KnapWindow} framework (KW) for SMDK. KW\nutilizes the \\textsc{KnapStream} algorithm (KS) for SMDK in append-only streams\nas a subroutine. It maintains a sequence of checkpoints and KS instances over\nthe sliding window. Theoretically, KW is\n$\\frac{1-\\varepsilon}{1+d}$-approximate for SMDK. Furthermore, we propose a\n\\textsc{KnapWindowPlus} framework (KW$^{+}$) to improve upon KW. KW$^{+}$\nbuilds an index \\textsc{SubKnapChk} to manage the checkpoints and KS instances.\n\\textsc{SubKnapChk} deletes a checkpoint whenever it can be approximated by its\nsuccessors. By keeping much fewer checkpoints, KW$^{+}$ achieves higher\nefficiency than KW while still guaranteeing a\n$\\frac{1-\\varepsilon'}{2+2d}$-approximate solution for SMDK. Finally, we\nevaluate the efficiency and solution quality of KW and KW$^{+}$ in real-world\ndatasets. The experimental results demonstrate that KW achieves more than two\norders of magnitude speedups over the batch baseline and preserves high-quality\nsolutions for SMDK over sliding windows. KW$^{+}$ further runs 5-10 times\nfaster than KW while providing solutions with equivalent or even better\nutilities.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 07:59:57 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 14:45:58 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wang", "Yanhao", ""], ["Li", "Yuchen", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1706.04889", "submitter": "Wolfgang Dvo\\v{r}\\'ak", "authors": "Krishnendu Chatterjee and Wolfgang Dvo\\v{r}\\'ak and Monika Henzinger\n  and Veronika Loitzenbauer", "title": "Improved Set-based Symbolic Algorithms for Parity Games", "comments": "An extended abstract has been accepted at CSL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph games with {\\omega}-regular winning conditions provide a mathematical\nframework to analyze a wide range of problems in the analysis of reactive\nsystems and programs (such as the synthesis of reactive systems, program\nrepair, and the verification of branching time properties). Parity conditions\nare canonical forms to specify {\\omega}-regular winning conditions. Graph games\nwith parity conditions are equivalent to {\\mu}-calculus model checking, and\nthus a very important algorithmic problem. Symbolic algorithms are of great\nsignificance because they provide scalable algorithms for the analysis of large\nfinite-state systems, as well as algorithms for the analysis of infinite-state\nsystems with finite quotient. A set-based symbolic algorithm uses the basic set\noperations and the one-step predecessor operators. We consider graph games with\n$n$ vertices and parity conditions with $c$ priorities. While many explicit\nalgorithms exist for graph games with parity conditions, for set-based symbolic\nalgorithms there are only two algorithms (notice that we use space to refer to\nthe number of sets stored by a symbolic algorithm): (a) the basic algorithm\nthat requires $O(n^c)$ symbolic operations and linear space; and (b) an\nimproved algorithm that requires $O(n^{c/2+1})$ symbolic operations but also\n$O(n^{c/2+1})$ space (i.e., exponential space). In this work we present two\nset-based symbolic algorithms for parity games: (a) our first algorithm\nrequires $O(n^{c/2+1})$ symbolic operations and only requires linear space; and\n(b) developing on our first algorithm, we present an algorithm that requires\n$O(n^{c/3+1})$ symbolic operations and only linear space. We also present the\nfirst linear space set-based symbolic algorithm for parity games that requires\nat most a sub-exponential number of symbolic operations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 14:30:39 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Dvo\u0159\u00e1k", "Wolfgang", ""], ["Henzinger", "Monika", ""], ["Loitzenbauer", "Veronika", ""]]}, {"id": "1706.04928", "submitter": "Andrea Sportiello", "authors": "Fr\\'ed\\'erique Bassino, Tsinjo Rakotoarimalala and Andrea Sportiello", "title": "The complexity of the Multiple Pattern Matching Problem for random\n  strings", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalise a multiple string pattern matching algorithm, recently proposed\nby Fredriksson and Grabowski [J. Discr. Alg. 7, 2009], to deal with arbitrary\ndictionaries on an alphabet of size $s$. If $r_m$ is the number of words of\nlength $m$ in the dictionary, and $\\phi(r) = \\max_m \\ln(s\\, m\\, r_m)/m$, the\ncomplexity rate for the string characters to be read by this algorithm is at\nmost $\\kappa_{{}_\\textrm{UB}}\\, \\phi(r)$ for some constant\n$\\kappa_{{}_\\textrm{UB}}$. On the other side, we generalise the classical lower\nbound of Yao [SIAM J. Comput. 8, 1979], for the problem with a single pattern,\nto deal with arbitrary dictionaries, and determine it to be at least\n$\\kappa_{{}_\\textrm{LB}}\\, \\phi(r)$. This proves the optimality of the\nalgorithm, improving and correcting previous claims.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 15:28:42 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 09:52:24 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Bassino", "Fr\u00e9d\u00e9rique", ""], ["Rakotoarimalala", "Tsinjo", ""], ["Sportiello", "Andrea", ""]]}, {"id": "1706.04939", "submitter": "Leon Ladewig", "authors": "Klaus Jansen, Kim-Manuel Klein, Maria Kosche, Leon Ladewig", "title": "Online Strip Packing with Polynomial Migration", "comments": "An extended abstract of the paper has been published in APPROX 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the relaxed online strip packing problem: Rectangular items\narrive online and have to be packed without rotations into a strip of fixed\nwidth such that the packing height is minimized. Thereby, repacking of\npreviously packed items is allowed. The amount of repacking is measured by the\nmigration factor, defined as the total size of repacked items divided by the\nsize of the arriving item. First, we show that no algorithm with constant\nmigration factor can produce solutions with asymptotic ratio better than 4/3.\nAgainst this background, we allow amortized migration, i.e. to save migration\nfor a later time step. As a main result, we present an AFPTAS with asymptotic\nratio $1 + \\mathcal{O}(\\epsilon)$ for any $\\epsilon > 0$ and amortized\nmigration factor polynomial in $1 / \\epsilon$. To our best knowledge, this is\nthe first algorithm for online strip packing considered in a repacking model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 15:57:36 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 08:27:04 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Jansen", "Klaus", ""], ["Klein", "Kim-Manuel", ""], ["Kosche", "Maria", ""], ["Ladewig", "Leon", ""]]}, {"id": "1706.05069", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Thomas Steinke", "title": "Generalization for Adaptively-chosen Estimators via Stable Median", "comments": "To appear in Conference on Learning Theory (COLT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets are often reused to perform multiple statistical analyses in an\nadaptive way, in which each analysis may depend on the outcomes of previous\nanalyses on the same dataset. Standard statistical guarantees do not account\nfor these dependencies and little is known about how to provably avoid\noverfitting and false discovery in the adaptive setting. We consider a natural\nformalization of this problem in which the goal is to design an algorithm that,\ngiven a limited number of i.i.d.~samples from an unknown distribution, can\nanswer adaptively-chosen queries about that distribution.\n  We present an algorithm that estimates the expectations of $k$ arbitrary\nadaptively-chosen real-valued estimators using a number of samples that scales\nas $\\sqrt{k}$. The answers given by our algorithm are essentially as accurate\nas if fresh samples were used to evaluate each estimator. In contrast, prior\nwork yields error guarantees that scale with the worst-case sensitivity of each\nestimator. We also give a version of our algorithm that can be used to verify\nanswers to such queries where the sample complexity depends logarithmically on\nthe number of queries $k$ (as in the reusable holdout technique).\n  Our algorithm is based on a simple approximate median algorithm that\nsatisfies the strong stability guarantees of differential privacy. Our\ntechniques provide a new approach for analyzing the generalization guarantees\nof differentially private algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 20:21:17 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Feldman", "Vitaly", ""], ["Steinke", "Thomas", ""]]}, {"id": "1706.05193", "submitter": "Sebastien Tixeuil", "authors": "Arnaud Sangnier (IRIF), Nathalie Sznajder (MoVe), Maria Potop-Butucaru\n  (NPA, LINCS), S\\'ebastien Tixeuil (NPA, IUF, LINCS)", "title": "Parameterized Verification of Algorithms for Oblivious Robots on a Ring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DS cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study verification problems for autonomous swarms of mobile robots that\nself-organize and cooperate to solve global objectives. In particular, we focus\nin this paper on the model proposed by Suzuki and Yamashita of anonymous robots\nevolving in a discrete space with a finite number of locations (here, a ring).\nA large number of algorithms have been proposed working for rings whose size is\nnot a priori fixed and can be hence considered as a parameter. Handmade\ncorrectness proofs of these algorithms have been shown to be error-prone, and\nrecent attention had been given to the application of formal methods to\nautomatically prove those. Our work is the first to study the verification\nproblem of such algorithms in the parameter-ized case. We show that safety and\nreachability problems are undecidable for robots evolving asynchronously. On\nthe positive side, we show that safety properties are decidable in the\nsynchronous case, as well as in the asynchronous case for a particular class of\nalgorithms. Several properties on the protocol can be decided as well. Decision\nprocedures rely on an encoding in Presburger arithmetics formulae that can be\nverified by an SMT-solver. Feasibility of our approach is demonstrated by the\nencoding of several case studies.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 09:38:21 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Sangnier", "Arnaud", "", "IRIF"], ["Sznajder", "Nathalie", "", "MoVe"], ["Potop-Butucaru", "Maria", "", "NPA, LINCS"], ["Tixeuil", "S\u00e9bastien", "", "NPA, IUF, LINCS"]]}, {"id": "1706.05248", "submitter": "Amir Kafshdar Goharshady", "authors": "Amir Kafshdar Goharshady, Mohammad Reza Hooshmandasl, Mohsen\n  Alambardar Meybodi", "title": "[1, 2]-sets and [1, 2]-total Sets in Trees with Algorithms", "comments": null, "journal-ref": null, "doi": "10.1016/j.dam.2015.06.014", "report-no": null, "categories": "math.CO cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set $S \\subseteq V$ of the graph $G = (V, E)$ is called a $[1, 2]$-set of\n$G$ if any vertex which is not in $S$ has at least one but no more than two\nneighbors in $S$. A set $S \\subseteq V$ is called a $[1, 2]$-total set of $G$\nif any vertex of $G$, no matter in $S$ or not, is adjacent to at least one but\nnot more than two vertices in $S$. In this paper we introduce a linear\nalgorithm for finding the cardinality of the smallest $[1, 2]$-sets and $[1,\n2]$-total sets of a tree and extend it to a more generalized version for $[i,\nj]$-sets, a generalization of $[1, 2]$-sets. This answers one of the open\nproblems proposed in [5]. Then since not all trees have $[1, 2]$-total sets, we\ndevise a recursive method for generating all the trees that do have such sets.\nThis method also constructs every $[1, 2]$-total set of each tree that it\ngenerates.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 12:38:10 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Goharshady", "Amir Kafshdar", ""], ["Hooshmandasl", "Mohammad Reza", ""], ["Meybodi", "Mohsen Alambardar", ""]]}, {"id": "1706.05320", "submitter": "Stanley P. Y. Fung", "authors": "Stanley P. Y. Fung", "title": "Optimal Online Two-way Trading with Bounded Number of Transactions", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a two-way trading problem, where investors buy and sell a stock\nwhose price moves within a certain range. Naturally they want to maximize their\nprofit. Investors can perform up to $k$ trades, where each trade must involve\nthe full amount. We give optimal algorithms for three different models which\ndiffer in the knowledge of how the price fluctuates. In the first model, there\nare global minimum and maximum bounds $m$ and $M$. We first show an optimal\nlower bound of $\\varphi$ (where $\\varphi=M/m$) on the competitive ratio for one\ntrade, which is the bound achieved by trivial algorithms. Perhaps surprisingly,\nwhen we consider more than one trade, we can give a better algorithm that loses\nonly a factor of $\\varphi^{2/3}$ (rather than $\\varphi$) per additional trade.\nSpecifically, for $k$ trades the algorithm has competitive ratio\n$\\varphi^{(2k+1)/3}$. Furthermore we show that this ratio is the best possible\nby giving a matching lower bound. In the second model, $m$ and $M$ are not\nknown in advance, and just $\\varphi$ is known. We show that this only costs us\nan extra factor of $\\varphi^{1/3}$, i.e., both upper and lower bounds become\n$\\varphi^{(2k+2)/3}$. Finally, we consider the bounded daily return model where\ninstead of a global limit, the fluctuation from one day to the next is bounded,\nand again we give optimal algorithms, and interestingly one of them resembles\ncommon trading strategies that involve stop loss limits.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 15:36:56 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Fung", "Stanley P. Y.", ""]]}, {"id": "1706.05429", "submitter": "Paul Medvedev", "authors": "Paul Medvedev", "title": "Modeling Biological Problems in Computer Science: A Case Study in Genome\n  Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computer scientists working in bioinformatics/computational biology, we\noften face the challenge of coming up with an algorithm to answer a biological\nquestion. This occurs in many areas, such as variant calling, alignment, and\nassembly. In this tutorial, we use the example of the genome assembly problem\nto demonstrate how to go from a question in the biological realm to a solution\nin the computer science realm. We show the modeling process step-by-step,\nincluding all the intermediate failed attempts.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 21:00:34 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 20:45:21 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Medvedev", "Paul", ""]]}, {"id": "1706.05476", "submitter": "Zijian Li", "authors": "Zijian Li, Xun Jian, Xiang Lian, Lei Chen", "title": "An Efficient Probabilistic Approach for Graph Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph similarity search is a common and fundamental operation in graph\ndatabases. One of the most popular graph similarity measures is the Graph Edit\nDistance (GED) mainly because of its broad applicability and high\ninterpretability. Despite its prevalence, exact GED computation is proved to be\nNP-hard, which could result in unsatisfactory computational efficiency on large\ngraphs. However, exactly accurate search results are usually unnecessary for\nreal-world applications especially when the responsiveness is far more\nimportant than the accuracy. Thus, in this paper, we propose a novel\nprobabilistic approach to efficiently estimate GED, which is further leveraged\nfor the graph similarity search. Specifically, we first take branches as\nelementary structures in graphs, and introduce a novel graph similarity measure\nby comparing branches between graphs, i.e., Graph Branch Distance (GBD), which\ncan be efficiently calculated in polynomial time. Then, we formulate the\nrelationship between GED and GBD by considering branch variations as the result\nascribed to graph edit operations, and model this process by probabilistic\napproaches. By applying our model, the GED between any two graphs can be\nefficiently estimated by their GBD, and these estimations are finally utilized\nin the graph similarity search. Extensive experiments show that our approach\nhas better accuracy, efficiency and scalability than other comparable methods\nin the graph similarity search over real and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 05:25:10 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 19:42:42 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Li", "Zijian", ""], ["Jian", "Xun", ""], ["Lian", "Xiang", ""], ["Chen", "Lei", ""]]}, {"id": "1706.05598", "submitter": "Tengyu Ma", "authors": "Rong Ge and Tengyu Ma", "title": "On the Optimization Landscape of Tensor Decompositions", "comments": "Best paper in the NIPS 2016 Workshop on Nonconvex Optimization for\n  Machine Learning: Theory and Practice. In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex optimization with local search heuristics has been widely used in\nmachine learning, achieving many state-of-art results. It becomes increasingly\nimportant to understand why they can work for these NP-hard problems on typical\ndata. The landscape of many objective functions in learning has been\nconjectured to have the geometric property that \"all local optima are\n(approximately) global optima\", and thus they can be solved efficiently by\nlocal search algorithms. However, establishing such property can be very\ndifficult.\n  In this paper, we analyze the optimization landscape of the random\nover-complete tensor decomposition problem, which has many applications in\nunsupervised learning, especially in learning latent variable models. In\npractice, it can be efficiently solved by gradient ascent on a non-convex\nobjective. We show that for any small constant $\\epsilon > 0$, among the set of\npoints with function values $(1+\\epsilon)$-factor larger than the expectation\nof the function, all the local maxima are approximate global maxima.\nPreviously, the best-known result only characterizes the geometry in small\nneighborhoods around the true components. Our result implies that even with an\ninitialization that is barely better than the random guess, the gradient ascent\nalgorithm is guaranteed to solve this problem.\n  Our main technique uses Kac-Rice formula and random matrix theory. To our\nbest knowledge, this is the first time when Kac-Rice formula is successfully\napplied to counting the number of local minima of a highly-structured random\npolynomial with dependent coefficients.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 01:18:42 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1706.05628", "submitter": "Bin Sheng", "authors": "Bin Sheng, Yuefang Sun", "title": "An improved kernel for the cycle contraction problem", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of modifying a given graph to satisfy certain properties has been\none of the central topics in parameterized tractability study. In this paper,\nwe study the cycle contraction problem, which makes a graph into a cycle by\nedge contractions. The problem has been studied {by Belmonte et al. [IPEC\n2013]} who obtained a linear kernel with at most $6k+6$ vertices. We provide an\nimproved kernel with at most $5k+4$ vertices for it in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 10:01:12 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Sheng", "Bin", ""], ["Sun", "Yuefang", ""]]}, {"id": "1706.05670", "submitter": "Marieke van der Wegen", "authors": "Jelco M. Bodewes, Hans L. Bodlaender, Gunther Cornelissen, Marieke van\n  der Wegen", "title": "Recognizing hyperelliptic graphs in polynomial time", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a new set of multigraph parameters was defined, called\n\"gonalities\". Gonality bears some similarity to treewidth, and is a relevant\ngraph parameter for problems in number theory and multigraph algorithms.\nMultigraphs of gonality 1 are trees. We consider so-called \"hyperelliptic\ngraphs\" (multigraphs of gonality 2) and provide a safe and complete sets of\nreduction rules for such multigraphs, showing that for three of the flavors of\ngonality, we can recognize hyperelliptic graphs in O(n log n+m) time, where n\nis the number of vertices and m the number of edges of the multigraph.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 15:35:10 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 15:17:36 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Bodewes", "Jelco M.", ""], ["Bodlaender", "Hans L.", ""], ["Cornelissen", "Gunther", ""], ["van der Wegen", "Marieke", ""]]}, {"id": "1706.05697", "submitter": "Henrik B\\\"a\\\"arnhielm", "authors": "John N. Bray and Henrik B\\\"a\\\"arnhielm", "title": "A new method for recognising Suzuki groups", "comments": null, "journal-ref": null, "doi": "10.1016/j.jalgebra.2017.05.040", "report-no": null, "categories": "math.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for constructive recognition of the Suzuki groups\nin their natural representations. The algorithm runs in Las Vegas polynomial\ntime given a discrete logarithm oracle. An implementation is available in the\nMagma computer algebra system.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 18:26:18 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Bray", "John N.", ""], ["B\u00e4\u00e4rnhielm", "Henrik", ""]]}, {"id": "1706.05698", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "SuperMinHash - A New Minwise Hashing Algorithm for Jaccard Similarity\n  Estimation", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm for calculating hash signatures of sets\nwhich can be directly used for Jaccard similarity estimation. The new approach\nis an improvement over the MinHash algorithm, because it has a better runtime\nbehavior and the resulting signatures allow a more precise estimation of the\nJaccard index.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 18:33:30 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "1706.05736", "submitter": "Joel Tropp", "authors": "Joel A. Tropp, Alp Yurtsever, Madeleine Udell, Volkan Cevher", "title": "Fixed-Rank Approximation of a Positive-Semidefinite Matrix from\n  Streaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several important applications, such as streaming PCA and semidefinite\nprogramming, involve a large-scale positive-semidefinite (psd) matrix that is\npresented as a sequence of linear updates. Because of storage limitations, it\nmay only be possible to retain a sketch of the psd matrix. This paper develops\na new algorithm for fixed-rank psd approximation from a sketch. The approach\ncombines the Nystrom approximation with a novel mechanism for rank truncation.\nTheoretical analysis establishes that the proposed method can achieve any\nprescribed relative error in the Schatten 1-norm and that it exploits the\nspectral decay of the input matrix. Computer experiments show that the proposed\nmethod dominates alternative techniques for fixed-rank psd matrix approximation\nacross a wide range of examples.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 22:13:45 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Tropp", "Joel A.", ""], ["Yurtsever", "Alp", ""], ["Udell", "Madeleine", ""], ["Cevher", "Volkan", ""]]}, {"id": "1706.05738", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Ilias Diakonikolas, Alistair Stewart", "title": "Fourier-Based Testing for Families of Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the general problem of testing whether an unknown distribution\nbelongs to a specified family of distributions. More specifically, given a\ndistribution family $\\mathcal{P}$ and sample access to an unknown discrete\ndistribution $\\mathbf{P}$, we want to distinguish (with high probability)\nbetween the case that $\\mathbf{P} \\in \\mathcal{P}$ and the case that\n$\\mathbf{P}$ is $\\epsilon$-far, in total variation distance, from every\ndistribution in $\\mathcal{P}$. This is the prototypical hypothesis testing\nproblem that has received significant attention in statistics and, more\nrecently, in theoretical computer science.\n  The sample complexity of this general inference task depends on the\nunderlying family $\\mathcal{P}$. The gold standard in distribution property\ntesting is to design sample-optimal and computationally efficient algorithms\nfor this task. The main contribution of this work is a simple and general\ntesting technique that is applicable to all distribution families whose Fourier\nspectrum satisfies a certain approximate sparsity property. To the best of our\nknowledge, ours is the first use of the Fourier transform in the context of\ndistribution testing.\n  We apply our Fourier-based framework to obtain near sample-optimal and\ncomputationally efficient testers for the following fundamental distribution\nfamilies: Sums of Independent Integer Random Variables (SIIRVs), Poisson\nMultinomial Distributions (PMDs), and Discrete Log-Concave Distributions. For\nthe first two, ours are the first non-trivial testers in the literature, vastly\ngeneralizing previous work on testing Poisson Binomial Distributions. For the\nthird, our tester improves on prior work in both sample and time complexity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 22:28:20 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 03:05:23 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Diakonikolas", "Ilias", ""], ["Stewart", "Alistair", ""]]}, {"id": "1706.05761", "submitter": "Dawei Huang", "authors": "Dawei Huang, Seth Pettie", "title": "Approximate Generalized Matching: $f$-Factors and $f$-Edge Covers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present linear time approximation schemes for several\ngeneralized matching problems on nonbipartite graphs. Our results include\n$O_\\epsilon(m)$-time algorithms for $(1-\\epsilon)$-maximum weight $f$-factor\nand $(1+\\epsilon)$-approximate minimum weight $f$-edge cover. As a byproduct,\nwe also obtain direct algorithms for the exact cardinality versions of these\nproblems running in $O(m\\sqrt{f(V)})$ time.\n  The technical contributions of this work include an efficient method for\nmaintaining {\\em relaxed complementary slackness} in generalized matching\nproblems and approximation-preserving reductions between the $f$-factor and\n$f$-edge cover problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 01:36:58 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 22:07:30 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 15:03:29 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Huang", "Dawei", ""], ["Pettie", "Seth", ""]]}, {"id": "1706.05815", "submitter": "Isaac Goldstein", "authors": "Isaac Goldstein, Tsvi Kopelowitz, Moshe Lewenstein and Ely Porat", "title": "How Hard is it to Find (Honest) Witnesses?", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2016.45", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years much effort was put into developing polynomial-time\nconditional lower bounds for algorithms and data structures in both static and\ndynamic settings. Along these lines we suggest a framework for proving\nconditional lower bounds based on the well-known 3SUM conjecture. Our framework\ncreates a \\emph{compact representation} of an instance of the 3SUM problem\nusing hashing and domain specific encoding. This compact representation admits\nfalse solutions to the original 3SUM problem instance which we reveal and\neliminate until we find a true solution. In other words, from all\n\\emph{witnesses} (candidate solutions) we figure out if an \\emph{honest} one (a\ntrue solution) exists. This enumeration of witnesses is used to prove\nconditional lower bound on \\emph{reporting} problems that generate all\nwitnesses. In turn, these reporting problems are reduced to various decision\nproblems. These help to enumerate the witnesses by constructing appropriate\nsearch data structures. Hence, 3SUM-hardness of the decision problems is\ndeduced.\n  We utilize this framework to show conditional lower bounds for several\nvariants of convolutions, matrix multiplication and string problems. Our\nframework uses a strong connection between all of these problems and the\nability to find \\emph{witnesses}.\n  While these specific applications are used to demonstrate the techniques of\nour framework, we believe that this novel framework is useful for many other\nproblems as well.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 07:53:40 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Goldstein", "Isaac", ""], ["Kopelowitz", "Tsvi", ""], ["Lewenstein", "Moshe", ""], ["Porat", "Ely", ""]]}, {"id": "1706.05826", "submitter": "Di Wang", "authors": "Di Wang, Kimon Fountoulakis, Monika Henzinger, Michael W. Mahoney,\n  Satish Rao", "title": "Capacity Releasing Diffusion for Speed and Locality", "comments": "Appeared in ICML 2017. Current version added reference and discussion\n  of work on generalized Cheeger's inequalities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusions and related random walk procedures are of central importance in\nmany areas of machine learning, data analysis, and applied mathematics. Because\nthey spread mass agnostically at each step in an iterative manner, they can\nsometimes spread mass \"too aggressively,\" thereby failing to find the \"right\"\nclusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process,\nwhich is both faster and stays more local than the classical spectral diffusion\nprocess. As an application, we use our CRD Process to develop an improved local\nalgorithm for graph clustering. Our local graph clustering method can find\nlocal clusters in a model of clustering where one begins the CRD Process in a\ncluster whose vertices are connected better internally than externally by an\n$O(\\log^2 n)$ factor, where $n$ is the number of nodes in the cluster. Thus,\nour CRD Process is the first local graph clustering algorithm that is not\nsubject to the well-known quadratic Cheeger barrier. Our result requires a\ncertain smoothness condition, which we expect to be an artifact of our\nanalysis. Our empirical evaluation demonstrates improved results, in particular\nfor realistic social graphs where there are moderately good---but not very\ngood---clusters.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 08:18:04 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 08:58:07 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Wang", "Di", ""], ["Fountoulakis", "Kimon", ""], ["Henzinger", "Monika", ""], ["Mahoney", "Michael W.", ""], ["Rao", "Satish", ""]]}, {"id": "1706.05847", "submitter": "Isaac Goldstein", "authors": "Isaac Goldstein, Tsvi Kopelowitz, Moshe Lewenstein and Ely Porat", "title": "Conditional Lower Bounds for Space/Time Tradeoffs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-62127-2_36", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years much effort has been concentrated towards achieving\npolynomial time lower bounds on algorithms for solving various well-known\nproblems. A useful technique for showing such lower bounds is to prove them\nconditionally based on well-studied hardness assumptions such as 3SUM, APSP,\nSETH, etc. This line of research helps to obtain a better understanding of the\ncomplexity inside P.\n  A related question asks to prove conditional space lower bounds on data\nstructures that are constructed to solve certain algorithmic tasks after an\ninitial preprocessing stage. This question received little attention in\nprevious research even though it has potential strong impact.\n  In this paper we address this question and show that surprisingly many of the\nwell-studied hard problems that are known to have conditional polynomial time\nlower bounds are also hard when concerning space. This hardness is shown as a\ntradeoff between the space consumed by the data structure and the time needed\nto answer queries. The tradeoff may be either smooth or admit one or more\nsingularity points.\n  We reveal interesting connections between different space hardness\nconjectures and present matching upper bounds. We also apply these hardness\nconjectures to both static and dynamic problems and prove their conditional\nspace hardness.\n  We believe that this novel framework of polynomial space conjectures can play\nan important role in expressing polynomial space lower bounds of many important\nalgorithmic problems. Moreover, it seems that it can also help in achieving a\nbetter understanding of the hardness of their corresponding problems in terms\nof time.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 09:34:05 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 12:18:48 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Goldstein", "Isaac", ""], ["Kopelowitz", "Tsvi", ""], ["Lewenstein", "Moshe", ""], ["Porat", "Ely", ""]]}, {"id": "1706.05893", "submitter": "Simon Wacker", "authors": "Simon Wacker", "title": "Signal Machine And Cellular Automaton Time-Optimal Quasi-Solutions Of\n  The Firing Squad/Mob Synchronisation Problem On Connected Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a time-optimal quasi-solution of the firing mob synchronisation\nproblem over finite, connected, and undirected multigraphs whose maximum\ndegrees are uniformly bounded by a constant. It is only a quasi-solution\nbecause its number of states depends on the graph or, from another perspective,\ndoes not depend on the graph but is countably infinite. To construct this\nquasi-solution we introduce signal machines over continuum representations of\nsuch multigraphs and construct a signal machine whose discretisation is a\ncellular automaton that quasi-solves the problem. This automaton uses a\ntime-optimal solution of the firing squad synchronisation problem in dimension\none with one general at one end to synchronise edges, and freezes and thaws the\nsynchronisation of edges in such a way that all edges synchronise at the same\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 11:47:45 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Wacker", "Simon", ""]]}, {"id": "1706.06084", "submitter": "Du\\v{s}an Knop", "authors": "Pavel Dvo\\v{r}\\'ak, Eduard Eiben, Robert Ganian, Du\\v{s}an Knop, and\n  Sebastian Ordyniak", "title": "Solving Integer Linear Programs with a Small Number of Global Variables\n  and Constraints", "comments": "24 pages; an extended abstract appeared in proceedings of IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer Linear Programming (ILP) has a broad range of applications in various\nareas of artificial intelligence. Yet in spite of recent advances, we still\nlack a thorough understanding of which structural restrictions make ILP\ntractable. Here we study ILP instances consisting of a small number of \"global\"\nvariables and/or constraints such that the remaining part of the instance\nconsists of small and otherwise independent components; this is captured in\nterms of a structural measure we call fracture backdoors which generalizes, for\ninstance, the well-studied class of N -fold ILP instances.\n  Our main contributions can be divided into three parts. First, we formally\ndevelop fracture backdoors and obtain exact and approximation algorithms for\ncomputing these. Second, we exploit these backdoors to develop several new\nparameterized algorithms for ILP; the performance of these algorithms will\nnaturally scale based on the number of global variables or constraints in the\ninstance. Finally, we complement the developed algorithms with matching lower\nbounds. Altogether, our results paint a near-complete complexity landscape of\nILP with respect to fracture backdoors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:54:14 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 09:33:09 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Dvo\u0159\u00e1k", "Pavel", ""], ["Eiben", "Eduard", ""], ["Ganian", "Robert", ""], ["Knop", "Du\u0161an", ""], ["Ordyniak", "Sebastian", ""]]}, {"id": "1706.06086", "submitter": "Marcin Pilipczuk", "authors": "Nikolai Karpov and Marcin Pilipczuk and Anna Zych-Pawlewicz", "title": "An exponential lower bound for cut sparsifiers in planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an edge-weighted graph $G$ with a set $Q$ of $k$ terminals, a mimicking\nnetwork is a graph with the same set of terminals that exactly preserves the\nsizes of minimum cuts between any partition of the terminals. A natural\nquestion in the area of graph compression is to provide as small mimicking\nnetworks as possible for input graph $G$ being either an arbitrary graph or\ncoming from a specific graph class.\n  In this note we show an exponential lower bound for cut mimicking networks in\nplanar graphs: there are edge-weighted planar graphs with $k$ terminals that\nrequire $2^{k-2}$ edges in any mimicking network. This nearly matches an upper\nbound of $O(k 2^{2k})$ of Krauthgamer and Rika [SODA 2013, arXiv:1702.05951]\nand is in sharp contrast with the $O(k^2)$ upper bound under the assumption\nthat all terminals lie on a single face [Goranci, Henzinger, Peng,\narXiv:1702.01136]. As a side result we show a hard instance for the\ndouble-exponential upper bounds given by Hagerup, Katajainen, Nishimura, and\nRagde~[JCSS 1998], Khan and Raghavendra~[IPL 2014], and Chambers and\nEppstein~[JGAA 2013].\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:57:08 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 09:36:46 GMT"}, {"version": "v3", "created": "Sat, 30 Dec 2017 17:44:15 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Karpov", "Nikolai", ""], ["Pilipczuk", "Marcin", ""], ["Zych-Pawlewicz", "Anna", ""]]}, {"id": "1706.06274", "submitter": "Raghu Meka", "authors": "Adam Klivans, Raghu Meka", "title": "Learning Graphical Models Using Multiplicative Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple, multiplicative-weight update algorithm for learning\nundirected graphical models or Markov random fields (MRFs). The approach is\nnew, and for the well-studied case of Ising models or Boltzmann machines, we\nobtain an algorithm that uses a nearly optimal number of samples and has\nquadratic running time (up to logarithmic factors), subsuming and improving on\nall prior work. Additionally, we give the first efficient algorithm for\nlearning Ising models over general alphabets.\n  Our main application is an algorithm for learning the structure of t-wise\nMRFs with nearly-optimal sample complexity (up to polynomial losses in\nnecessary terms that depend on the weights) and running time that is\n$n^{O(t)}$. In addition, given $n^{O(t)}$ samples, we can also learn the\nparameters of the model and generate a hypothesis that is close in statistical\ndistance to the true MRF. All prior work runs in time $n^{\\Omega(d)}$ for\ngraphs of bounded degree d and does not generate a hypothesis close in\nstatistical distance even for t=3. We observe that our runtime has the correct\ndependence on n and t assuming the hardness of learning sparse parities with\nnoise.\n  Our algorithm--the Sparsitron-- is easy to implement (has only one parameter)\nand holds in the on-line setting. Its analysis applies a regret bound from\nFreund and Schapire's classic Hedge algorithm. It also gives the first solution\nto the problem of learning sparse Generalized Linear Models (GLMs).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 05:41:31 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Klivans", "Adam", ""], ["Meka", "Raghu", ""]]}, {"id": "1706.06466", "submitter": "Lorenzo Severini", "authors": "Gianlorenzo D'Angelo, Lorenzo Severini, Yllka Velaj", "title": "Selecting nodes and buying links to maximize the information diffusion\n  in a network", "comments": "Extended version of the paper accepted at the 42nd International\n  Symposium on Mathematical Foundations of Computer Science (MFCS 2017). arXiv\n  admin note: text overlap with arXiv:1706.04368", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Independent Cascade Model (ICM) is a widely studied model that aims to\ncapture the dynamics of the information diffusion in social networks and in\ngeneral complex networks. In this model, we can distinguish between active\nnodes which spread the information and inactive ones. The process starts from a\nset of initially active nodes called seeds. Recursively, currently active nodes\ncan activate their neighbours according to a probability distribution on the\nset of edges. After a certain number of these recursive cycles, a large number\nof nodes might become active. The process terminates when no further node gets\nactivated. Starting from the work of Domingos and Richardson, several studies\nhave been conducted with the aim of shaping a given diffusion process so as to\nmaximize the number of activated nodes at the end of the process. One of the\nmost studied problems has been formalized by Kempe et al. and consists in\nfinding a set of initial seeds that maximizes the expected number of active\nnodes under a budget constraint. In this paper we study a generalization of the\nproblem of Kempe et al. in which we are allowed to spend part of the budget to\ncreate new edges incident to the seeds. That is, the budget can be spent to buy\nseeds or edges according to a cost function. The problem does not admin a PTAS,\nunless $P=NP$. We propose two approximation algorithms: the former one gives an\napproximation ratio that depends on the edge costs and increases when these\ncosts are high; the latter algorithm gives a constant approximation guarantee\nwhich is greater than that of the first algorithm when the edge costs can be\nsmall.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 08:13:28 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["D'Angelo", "Gianlorenzo", ""], ["Severini", "Lorenzo", ""], ["Velaj", "Yllka", ""]]}, {"id": "1706.06514", "submitter": "Christiane Spisla", "authors": "Michael J\\\"unger, Petra Mutzel, Christiane Spisla", "title": "Orthogonal Compaction Using Additional Bends", "comments": "Submitted to 25th International Symposium on Graph Drawing & Network\n  Visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compacting orthogonal drawings is a challenging task. Usually algorithms try\nto compute drawings with small area or edge length while preserving the\nunderlying orthogonal shape. We present a one-dimensional compaction algorithm\nthat alters the orthogonal shape of edges for better geometric results. An\nexperimental evaluation shows that we were able to reduce the total edge length\nand the drawing area, but at the expense of additional bends.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 15:20:45 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["J\u00fcnger", "Michael", ""], ["Mutzel", "Petra", ""], ["Spisla", "Christiane", ""]]}, {"id": "1706.06565", "submitter": "Kanstantsin Pashkovich", "authors": "Jochen K\\\"onemann, Neil Olver, Kanstantsin Pashkovich, R. Ravi,\n  Chaitanya Swamy, Jens Vygen", "title": "On the Integrality Gap of the Prize-Collecting Steiner Forest LP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the prize-collecting Steiner forest (PCSF) problem, we are given an\nundirected graph $G=(V,E)$, edge costs $\\{c_e\\geq 0\\}_{e\\in E}$, terminal pairs\n$\\{(s_i,t_i)\\}_{i=1}^k$, and penalties $\\{\\pi_i\\}_{i=1}^k$ for each terminal\npair; the goal is to find a forest $F$ to minimize $c(F)+\\sum_{i:\n(s_i,t_i)\\text{ not connected in }F}\\pi_i$. The Steiner forest problem can be\nviewed as the special case where $\\pi_i=\\infty$ for all $i$. It was widely\nbelieved that the integrality gap of the natural (and well-studied)\nlinear-programming (LP) relaxation for PCSF is at most 2. We dispel this belief\nby showing that the integrality gap of this LP is at least $9/4$. This holds\neven for planar graphs. We also show that using this LP, one cannot devise a\nLagrangian-multiplier-preserving (LMP) algorithm with approximation guarantee\nbetter than $4$. Our results thus show a separation between the integrality\ngaps of the LP-relaxations for prize-collecting and non-prize-collecting (i.e.,\nstandard) Steiner forest, as well as the approximation ratios achievable\nrelative to the optimal LP solution by LMP- and non-LMP- approximation\nalgorithms for PCSF. For the special case of prize-collecting Steiner tree\n(PCST), we prove that the natural LP relaxation admits basic feasible solutions\nwith all coordinates of value at most $1/3$ and all edge variables positive.\nThus, we rule out the possibility of approximating PCST with guarantee better\nthan $3$ using a direct iterative rounding method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 17:41:43 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["K\u00f6nemann", "Jochen", ""], ["Olver", "Neil", ""], ["Pashkovich", "Kanstantsin", ""], ["Ravi", "R.", ""], ["Swamy", "Chaitanya", ""], ["Vygen", "Jens", ""]]}, {"id": "1706.06654", "submitter": "Merve Asiler", "authors": "Merve Asiler, Adnan Yaz{\\i}c{\\i}", "title": "BB-Graph: A Subgraph Isomorphism Algorithm for Efficiently Querying Big\n  Graph Databases", "comments": "This is a big data study on subgraph isomorphism problem. We\n  developed a new algorithm. We have studied on very big Neo4j graph databases", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The big graph database model provides strong modeling for complex\napplications and efficient querying. However, it is still a big challenge to\nfind all exact matches of a query graph in a big graph database, which is known\nas the subgraph isomorphism problem. The current subgraph isomorphism\napproaches are built on Ullmann's idea of focusing on the strategy of pruning\nout the irrelevant candidates. Nevertheless, the existing pruning techniques\nneed much more improvement to efficiently handle complex queries. Moreover,\nmany of those existing algorithms need large indices requiring extra memory\nconsumption. Motivated by these, we introduce a new subgraph isomorphism\nalgorithm, named as BB-Graph, for querying big graph databases efficiently\nwithout requiring a large data structure to be stored in main memory. We test\nand compare our proposed BB-Graph algorithm with two popular existing\napproaches, GraphQL and Cypher. Our experiments are done on three different\ndata sets; (1) a very big graph database of a real-life population database,\n(2) a graph database of a simulated bank database, and (3) the publicly\navailable World Cup big graph database. We show that our solution performs\nbetter than those algorithms mentioned here for most of the query types\nexperimented on these big databases.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 20:20:25 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 15:31:17 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Asiler", "Merve", ""], ["Yaz\u0131c\u0131", "Adnan", ""]]}, {"id": "1706.06685", "submitter": "Marc Heinrich", "authors": "Nicolas Bousquet, Marc Heinrich", "title": "Computing maximum cliques in $B_2$-EPG graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EPG graphs, introduced by Golumbic et al. in 2009, are edge-intersection\ngraphs of paths on an orthogonal grid. The class $B_k$-EPG is the subclass of\nEPG graphs where the path on the grid associated to each vertex has at most $k$\nbends. Epstein et al. showed in 2013 that computing a maximum clique in\n$B_1$-EPG graphs is polynomial. As remarked in [Heldt et al., 2014], when the\nnumber of bends is at least $4$, the class contains $2$-interval graphs for\nwhich computing a maximum clique is an NP-hard problem. The complexity status\nof the Maximum Clique problem remains open for $B_2$ and $B_3$-EPG graphs. In\nthis paper, we show that we can compute a maximum clique in polynomial time in\n$B_2$-EPG graphs given a representation of the graph.\n  Moreover, we show that a simple counting argument provides a\n${2(k+1)}$-approximation for the coloring problem on $B_k$-EPG graphs without\nknowing the representation of the graph. It generalizes a result of [Epstein et\nal, 2013] on $B_1$-EPG graphs (where the representation was needed).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 22:18:16 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Bousquet", "Nicolas", ""], ["Heinrich", "Marc", ""]]}, {"id": "1706.06697", "submitter": "Florian Gross", "authors": "Florian Gross", "title": "Index Search Algorithms for Databases and Modern CPUs", "comments": "Master Thesis with survey on hardware optimized index search\n  algorithms; 70 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, many different indexing techniques and search algorithms have\nbeen proposed, including CSS-trees, CSB+ trees, k-ary binary search, and fast\narchitecture sensitive tree search. There have also been papers on how best to\nset the many different parameters of these index structures, such as the node\nsize of CSB+ trees.\n  These indices have been proposed because CPU speeds have been increasing at a\ndramatically higher rate than memory speeds, giving rise to the Von Neumann\nCPU--Memory bottleneck. To hide the long latencies caused by memory access, it\nhas become very important to well-utilize the features of modern CPUs. In order\nto drive down the average number of CPU clock cycles required to execute CPU\ninstructions, and thus increase throughput, it has become important to achieve\na good utilization of CPU resources. Some of these are the data and instruction\ncaches, and the translation lookaside buffers. But it also has become important\nto avoid branch misprediction penalties, and utilize vectorization provided by\nCPUs in the form of SIMD instructions.\n  While the layout of index structures has been heavily optimized for the data\ncache of modern CPUs, the instruction cache has been neglected so far. In this\npaper, we present NitroGen, a framework for utilizing code generation for\nspeeding up index traversal in main memory database systems. By bringing\ntogether data and code, we make index structures use the dormant resource of\nthe instruction cache. We show how to combine index compilation with previous\napproaches, such as binary tree search, cache-sensitive tree search, and the\narchitecture-sensitive tree search presented by Kim et al.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 23:01:52 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Gross", "Florian", ""]]}, {"id": "1706.06805", "submitter": "Henning Meyerhenke", "authors": "Michael Wegner, Oskar Taubert, Alexander Schug, Henning Meyerhenke", "title": "Maxent-Stress Optimization of 3D Biomolecular Models", "comments": "Accepted by 25th Annual European Symposium on Algorithms (ESA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing a biomolecule's structure is inherently linked to and a prerequisite\nfor any detailed understanding of its function. Significant effort has gone\ninto developing technologies for structural characterization. These\ntechnologies do not directly provide 3D structures; instead they typically\nyield noisy and erroneous distance information between specific entities such\nas atoms or residues, which have to be translated into consistent 3D models.\n  Here we present an approach for this translation process based on\nmaxent-stress optimization. Our new approach extends the original graph drawing\nmethod for the new application's specifics by introducing additional\nconstraints and confidence values as well as algorithmic components. Extensive\nexperiments demonstrate that our approach infers structural models (i. e.,\nsensible 3D coordinates for the molecule's atoms) that correspond well to the\ndistance information, can handle noisy and error-prone data, and is\nconsiderably faster than established tools. Our results promise to allow domain\nscientists nearly-interactive structural modeling based on distance\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 09:27:56 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Wegner", "Michael", ""], ["Taubert", "Oskar", ""], ["Schug", "Alexander", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "1706.06806", "submitter": "Rakesh Venkat", "authors": "Yuval Rabani, Rakesh Venkat", "title": "Approximating Sparsest Cut in Low Rank Graphs via Embeddings from\n  Approximately Low-Dimensional Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of embedding a finite set of points $\\{x_1, \\ldots,\nx_n\\} \\in \\mathbb{R}^d$ that satisfy $\\ell_2^2$ triangle inequalities into\n$\\ell_1$, when the points are approximately low-dimensional. Goemans\n(unpublished, appears in a work of [Magen and Moharammi, 2008]) showed that\nsuch points residing in \\emph{exactly} $d$ dimensions can be embedded into\n$\\ell_1$ with distortion at most $\\sqrt{d}$. We prove the following robust\nanalogue of this statement: if there exists a $r$-dimensional subspace $\\Pi$\nsuch that the projections onto this subspace satisfy $\\sum_{i,j \\in [n]}\\Vert\n\\Pi x_i - \\Pi x_j \\Vert _2^2 \\geq \\Omega(1) \\sum_{i,j \\in [n]}\\Vert x_i - x_j\n\\Vert _2^2$, then there is an embedding of the points into $\\ell_1$ with\n$O(\\sqrt{r})$ average distortion. A consequence of this result is that the\nintegrality gap of the well-known Goemans-Linial SDP relaxation for the Uniform\nSparsest Cut problem is $O(\\sqrt{r})$ on graphs $G$ whose $r$-th smallest\nnormalized eigenvalue of the Laplacian satisfies $\\lambda_r(G)/n \\geq\n\\Omega(1)\\Phi_{SDP} (G)$. Our result improves upon the previously known bound\nof $O(r)$ on the average distortion, and the integrality gap of the\nGoemans-Linial SDP under the same preconditions, proven in the previous works\nof [Deshpande and Venkat, 2014] and [Deshpande, Harsha and Venkat, 2016].\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 09:38:37 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Rabani", "Yuval", ""], ["Venkat", "Rakesh", ""]]}, {"id": "1706.06931", "submitter": "Rasmus Ibsen-Jensen", "authors": "Krishnendu Chatterjee, Rasmus Ibsen-Jensen, Martin A. Nowak", "title": "Faster Monte-Carlo Algorithms for Fixation Probability of the Moran\n  Process on Undirected Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary graph theory studies the evolutionary dynamics in a population\nstructure given as a connected graph. Each node of the graph represents an\nindividual of the population, and edges determine how offspring are placed. We\nconsider the classical birth-death Moran process where there are two types of\nindividuals, namely, the residents with fitness 1 and mutants with fitness r.\nThe fitness indicates the reproductive strength. The evolutionary dynamics\nhappens as follows: in the initial step, in a population of all resident\nindividuals a mutant is introduced, and then at each step, an individual is\nchosen proportional to the fitness of its type to reproduce, and the offspring\nreplaces a neighbor uniformly at random. The process stops when all individuals\nare either residents or mutants. The probability that all individuals in the\nend are mutants is called the fixation probability. We present faster\npolynomial-time Monte-Carlo algorithms for finidng the fixation probability on\nundirected graphs. Our algorithms are always at least a factor O(n^2/log n)\nfaster as compared to the previous algorithms, where n is the number of nodes,\nand is polynomial even if r is given in binary. We also present lower bounds\nshowing that the upper bound on the expected number of effective steps we\npresent is asymptotically tight for undirected graphs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:35:24 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Ibsen-Jensen", "Rasmus", ""], ["Nowak", "Martin A.", ""]]}, {"id": "1706.06940", "submitter": "Szymon Grabowski", "authors": "Szymon Grabowski, Tomasz Kowalski", "title": "Faster batched range minimum queries", "comments": "Accepted to Prague Stringology Conference 2017. Compared to v1, bugs\n  in Table 2 were fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Range Minimum Query (RMQ) is an important building brick of many compressed\ndata structures and string matching algorithms. Although this problem is\nessentially solved in theory, with sophisticated data structures allowing for\nconstant time queries, there are scenarios in which the number of queries, $q$,\nis rather small and given beforehand, which encourages to use a simpler\napproach. A recent work by Alzamel et al. starts with contracting the input\narray to a much shorter one, with its size proportional to $q$. In this work,\nwe build upon their solution, speeding up handling small batches of queries by\na factor of 3.8--7.8 (the gap grows with $q$). The key idea that helped us\nachieve this advantage is adapting the well-known Sparse Table technique to\nwork on blocks, with speculative block minima comparisons. We also propose an\neven much faster (but possibly using more space) variant without the array\ncontraction.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:50:32 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 18:24:04 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Grabowski", "Szymon", ""], ["Kowalski", "Tomasz", ""]]}, {"id": "1706.07290", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "New Cardinality Estimation Methods for HyperLogLog Sketches", "comments": "9 pages, 10 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:1702.01284", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents new cardinality estimation methods for data sets recorded\nby HyperLogLog sketches. A simple derivation of the original estimator was\nfound, that also gives insight how to correct its deficiencies. The result is\nan improved estimator that is unbiased over the full cardinality range, is easy\ncomputable, and does not rely on empirically determined data as previous\napproaches. Based on the maximum likelihood principle a second unbiased\nestimation method is presented which can also be extended to estimate\ncardinalities of union, intersection, or relative complements of two sets that\nare both represented as HyperLogLog sketches. Experimental results show that\nthis approach is more precise than the conventional technique using the\ninclusion-exclusion principle.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 19:14:12 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "1706.07357", "submitter": "Yin Tat Lee", "authors": "Yin Tat Lee, Aaron Sidford, Santosh S. Vempala", "title": "Efficient Convex Optimization with Membership Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a convex function over a convex set\ngiven access only to an evaluation oracle for the function and a membership\noracle for the set. We give a simple algorithm which solves this problem with\n$\\tilde{O}(n^2)$ oracle calls and $\\tilde{O}(n^3)$ additional arithmetic\noperations. Using this result, we obtain more efficient reductions among the\nfive basic oracles for convex sets and functions defined by Gr\\\"otschel, Lovasz\nand Schrijver.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 15:07:24 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1706.07418", "submitter": "Szymon Dudycz", "authors": "Szymon Dudycz and Katarzyna Paluch", "title": "Optimal General Matchings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph $G=(V,E)$ and for each vertex $v \\in V$ a subset $B(v)$ of the\nset $\\{0,1,\\ldots, d_G(v)\\}$ a $B$-matching of $G$ is any set $F \\subseteq E$\nsuch that $d_F(v) \\in B(v)$ for each vertex $v$. The general matching problem\nasks the existence of a $B$-matching in a given graph. A set $B(v)$ is said to\nhave a {\\em gap of length} $p$ if there exists a number $k \\in B(v)$ such that\n$k+1, \\ldots, k+p \\notin B(v)$ and $k+p+1 \\in B(v)$. Without any restrictions\nthe general matching problem is NP-complete. However, if no set $B(v)$ contains\na gap of length greater than $1$, then the problem can be solved in polynomial\ntime and Cornuejols \\cite{Cor} presented an algorithm for finding a\n$B$-matching, if it exists. In this paper we consider a version of the general\nmatching problem, in which we are interested in finding a $B$-matching having a\nmaximum (or minimum) number of edges.\n  We present the first polynomial time algorithm for the maximum weight\n$B$-matching for the case when no set $B(v)$ contains a gap of length greater\nthan $1$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:48:45 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 14:07:32 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 13:44:43 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Dudycz", "Szymon", ""], ["Paluch", "Katarzyna", ""]]}, {"id": "1706.07475", "submitter": "Arne Leitert", "authors": "Arne Leitert, Feodor F. Dragan", "title": "Parameterized Approximation Algorithms for some Location Problems in\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop efficient parameterized, with additive error, approximation\nalgorithms for the (Connected) $r$-Domination problem and the (Connected)\n$p$-Center problem for unweighted and undirected graphs. Given a graph $G$, we\nshow how to construct a (connected) $\\big(r + \\mathcal{O}(\\mu)\n\\big)$-dominating set $D$ with $|D| \\leq |D^*|$ efficiently. Here, $D^*$ is a\nminimum (connected) $r$-dominating set of $G$ and $\\mu$ is our graph parameter,\nwhich is the tree-breadth or the cluster diameter in a layering partition of\n$G$. Additionally, we show that a $+ \\mathcal{O}(\\mu)$-approximation for the\n(Connected) $p$-Center problem on $G$ can be computed in polynomial time. Our\ninterest in these parameters stems from the fact that in many real-world\nnetworks, including Internet application networks, web networks, collaboration\nnetworks, social networks, biological networks, and others, and in many\nstructured classes of graphs these parameters are small constants.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 19:56:51 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Leitert", "Arne", ""], ["Dragan", "Feodor F.", ""]]}, {"id": "1706.07510", "submitter": "Arya Mazumdar", "authors": "Arya Mazumdar, Barna Saha", "title": "Clustering with Noisy Queries", "comments": "Prior versions of some of the results have appeared before in\n  arXiv:1604.01839. In this version we rewrote several proofs for clarity, and\n  included many new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate a rigorous theoretical study of clustering with\nnoisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is to\nrecover the true clustering by asking minimum number of pairwise queries to an\noracle. Oracle can answer queries of the form : \"do elements $u$ and $v$ belong\nto the same cluster?\" -- the queries can be asked interactively (adaptive\nqueries), or non-adaptively up-front, but its answer can be erroneous with\nprobability $p$. In this paper, we provide the first information theoretic\nlower bound on the number of queries for clustering with noisy oracle in both\nsituations. We design novel algorithms that closely match this query complexity\nlower bound, even when the number of clusters is unknown. Moreover, we design\ncomputationally efficient algorithms both for the adaptive and non-adaptive\nsettings. The problem captures/generalizes multiple application scenarios. It\nis directly motivated by the growing body of work that use crowdsourcing for\n{\\em entity resolution}, a fundamental and challenging data mining task aimed\nto identify all records in a database referring to the same entity. Here crowd\nrepresents the noisy oracle, and the number of queries directly relates to the\ncost of crowdsourcing. Another application comes from the problem of {\\em sign\nedge prediction} in social network, where social interactions can be both\npositive and negative, and one must identify the sign of all pair-wise\ninteractions by querying a few pairs. Furthermore, clustering with noisy oracle\nis intimately connected to correlation clustering, leading to improvement\ntherein. Finally, it introduces a new direction of study in the popular {\\em\nstochastic block model} where one has an incomplete stochastic block model\nmatrix to recover the clusters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:22:04 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Mazumdar", "Arya", ""], ["Saha", "Barna", ""]]}, {"id": "1706.07604", "submitter": "Ren\\'e Sitters", "authors": "Ren\\'e Sitters and Liya Yang", "title": "A $(2 + \\epsilon)$-approximation for precedence constrained single\n  machine scheduling with release dates and total weighted completion time\n  objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a $(2 + \\epsilon)$-approximation algorithm for minimizing total\nweighted completion time on a single machine under release time and precedence\nconstraints. This settles a recent conjecture made in [18]\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 09:12:01 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Sitters", "Ren\u00e9", ""], ["Yang", "Liya", ""]]}, {"id": "1706.07669", "submitter": "Steve Hanneke", "authors": "Steve Hanneke, Liu Yang", "title": "Testing Piecewise Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the query complexity of property testing for general\npiecewise functions on the real line, in the active and passive property\ntesting settings. The results are proven under an abstract zero-measure\ncrossings condition, which has as special cases piecewise constant functions\nand piecewise polynomial functions. We find that, in the active testing\nsetting, the query complexity of testing general piecewise functions is\nindependent of the number of pieces. We also identify the optimal dependence on\nthe number of pieces in the query complexity of passive testing in the special\ncase of piecewise constant functions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 12:29:52 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:25:43 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1706.07719", "submitter": "Arya Mazumdar", "authors": "Arya Mazumdar, Barna Saha", "title": "Query Complexity of Clustering with Side Information", "comments": "A prior version of this work appeared in arxiv previously, see\n  arxiv:1604.01839. This paper contains a new efficient Monte Carlo algorithm\n  that has not appeared before, and a stronger lower bound. Some proofs have\n  been rewritten for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose, we are given a set of $n$ elements to be clustered into $k$\n(unknown) clusters, and an oracle/expert labeler that can interactively answer\npair-wise queries of the form, \"do two elements $u$ and $v$ belong to the same\ncluster?\". The goal is to recover the optimum clustering by asking the minimum\nnumber of queries. In this paper, we initiate a rigorous theoretical study of\nthis basic problem of query complexity of interactive clustering, and provide\nstrong information theoretic lower bounds, as well as nearly matching upper\nbounds. Most clustering problems come with a similarity matrix, which is used\nby an automated process to cluster similar points together. Our main\ncontribution in this paper is to show the dramatic power of side information\naka similarity matrix on reducing the query complexity of clustering. A\nsimilarity matrix represents noisy pair-wise relationships such as one computed\nby some function on attributes of the elements. A natural noisy model is where\nsimilarity values are drawn independently from some arbitrary probability\ndistribution $f_+$ when the underlying pair of elements belong to the same\ncluster, and from some $f_-$ otherwise. We show that given such a similarity\nmatrix, the query complexity reduces drastically from $\\Theta(nk)$ (no\nsimilarity matrix) to $O(\\frac{k^2\\log{n}}{\\cH^2(f_+\\|f_-)})$ where $\\cH^2$\ndenotes the squared Hellinger divergence. Moreover, this is also\ninformation-theoretic optimal within an $O(\\log{n})$ factor. Our algorithms are\nall efficient, and parameter free, i.e., they work without any knowledge of $k,\nf_+$ and $f_-$, and only depend logarithmically with $n$. Along the way, our\nwork also reveals intriguing connection to popular community detection models\nsuch as the {\\em stochastic block model}, significantly generalizes them, and\nopens up many venues for interesting future research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 14:24:32 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Mazumdar", "Arya", ""], ["Saha", "Barna", ""]]}, {"id": "1706.07829", "submitter": "Radi Muhammad Reza", "authors": "Radi Muhammad Reza, Mohammed Eunus Ali, Muhammad Aamir Cheema", "title": "The Optimal Route and Stops for a Group of Users in a Road Network", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the advancement of the GPS-enabled cellular technologies, the\nlocation-based services (LBS) have gained in popularity. Nowadays, an\nincreasingly larger number of map-based applications enable users to ask a\nwider variety of queries. Researchers have studied the ride-sharing, the\ncarpooling, the vehicle routing, and the collective travel planning problems\nextensively in recent years. Collective traveling has the benefit of being\nenvironment-friendly by reducing the global travel cost, the greenhouse gas\nemission, and the energy consumption. In this paper, we introduce several\noptimization problems to recommend a suitable route and stops of a vehicle, in\na road network, for a group of users intending to travel collectively. The goal\nof each problem is to minimize the aggregate cost of the individual travelers'\npaths and the shared route under various constraints. First, we formulate the\nproblem of determining the optimal pair of end-stops, given a set of queries\nthat originate and terminate near the two prospective end regions. We outline a\nbaseline polynomial-time algorithm and propose a new faster solution - both\ncalculating an exact answer. In our approach, we utilize the path-coherence\nproperty of road networks to develop an efficient algorithm. Second, we define\nthe problem of calculating the optimal route and intermediate stops of a\nvehicle that picks up and drops off passengers en-route, given its start and\nend stoppages, and a set of path queries from users. We outline an exact\nsolution of both time and space complexities exponential in the number of\nqueries. Then, we propose a novel polynomial-time-and-space heuristic algorithm\nthat performs reasonably well in practice. We also analyze several variants of\nthis problem under different constraints. Last, we perform extensive\nexperiments that demonstrate the efficiency and accuracy of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 18:34:24 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Reza", "Radi Muhammad", ""], ["Ali", "Mohammed Eunus", ""], ["Cheema", "Muhammad Aamir", ""]]}, {"id": "1706.07851", "submitter": "Kyle Kloster", "authors": "Kyle Kloster, Philipp Kuinke, Michael P. O'Brien, Felix Reidl,\n  Fernando S\\'anchez Villaamil, Blair D. Sullivan, and Andrew van der Poel", "title": "A practical fpt algorithm for Flow Decomposition and transcript assembly", "comments": "Introduces software package Toboggan: Version 1.0.\n  http://dx.doi.org/10.5281/zenodo.821634", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Flow Decomposition problem, which asks for the smallest set of weighted\npaths that \"covers\" a flow on a DAG, has recently been used as an important\ncomputational step in transcript assembly. We prove the problem is in FPT when\nparameterized by the number of paths by giving a practical linear fpt\nalgorithm. Further, we implement and engineer a Flow Decomposition solver based\non this algorithm, and evaluate its performance on RNA-sequence data.\nCrucially, our solver finds exact solutions while achieving runtimes\ncompetitive with a state-of-the-art heuristic. Finally, we contextualize our\ndesign choices with two hardness results related to preprocessing and weight\nrecovery. Specifically, $k$-Flow Decomposition does not admit polynomial\nkernels under standard complexity assumptions, and the related problem of\nassigning (known) weights to a given set of paths is NP-hard.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 19:48:03 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 15:35:07 GMT"}, {"version": "v3", "created": "Wed, 30 Aug 2017 16:06:16 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Kloster", "Kyle", ""], ["Kuinke", "Philipp", ""], ["O'Brien", "Michael P.", ""], ["Reidl", "Felix", ""], ["Villaamil", "Fernando S\u00e1nchez", ""], ["Sullivan", "Blair D.", ""], ["van der Poel", "Andrew", ""]]}, {"id": "1706.07975", "submitter": "Rodrigo de Lamare", "authors": "Zhaocheng Yang, Rodrigo C. de Lamare and Weijian Liu", "title": "Sparsity-Based STAP Design Based on Alternating Direction Method with\n  Gain/Phase Errors", "comments": "7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel sparsity-based space-time adaptive processing (STAP)\ntechnique based on the alternating direction method to overcome the severe\nperformance degradation caused by array gain/phase (GP) errors. The proposed\nalgorithm reformulates the STAP problem as a joint optimization problem of the\nspatio-Doppler profile and GP errors in both single and multiple snapshots, and\nintroduces a target detector using the reconstructed spatio-Doppler profiles.\nSimulations are conducted to illustrate the benefits of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 15:52:19 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Yang", "Zhaocheng", ""], ["de Lamare", "Rodrigo C.", ""], ["Liu", "Weijian", ""]]}, {"id": "1706.08050", "submitter": "Matthew  Johnson", "authors": "Nina Chiarelli, Tatiana R. Hartinger, Matthew Johnson, Martin\n  Milani\\v{c}, Dani\\\"el Paulusma", "title": "Minimum Connected Transversals in Graphs: New Hardness Results and\n  Tractable Cases Using the Price of Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a systematic study in the computational complexity of the\nconnected variant of three related transversal problems: Vertex Cover, Feedback\nVertex Set, and Odd Cycle Transversal. Just like their original counterparts,\nthese variants are NP-complete for general graphs. A graph $G$ is $H$-free for\nsome graph $H$ if $G$ contains no induced subgraph isomorphic to $H$. It is\nknown that Connected Vertex Cover is NP-complete even for $H$-free graphs if\n$H$ contains a claw or a cycle. We show that the two other connected variants\nalso remain NP-complete if $H$ contains a cycle or claw. In the remaining case\n$H$ is a linear forest. We show that Connected Vertex Cover, Connected Feedback\nVertex Set, and Connected Odd Cycle Transversal are polynomial-time solvable\nfor $sP_2$-free graphs for every constant $s\\geq 1$. For proving these results\nwe use known results on the price of connectivity for vertex cover, feedback\nvertex set, and odd cycle transversal. This is the first application of the\nprice of connectivity that results in polynomial-time algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 08:00:56 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 09:01:08 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 15:27:57 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Chiarelli", "Nina", ""], ["Hartinger", "Tatiana R.", ""], ["Johnson", "Matthew", ""], ["Milani\u010d", "Martin", ""], ["Paulusma", "Dani\u00ebl", ""]]}, {"id": "1706.08115", "submitter": "Arnold Filtser", "authors": "Arnold Filtser", "title": "Steiner Point Removal with Distortion $O(\\log k)$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Steiner point removal (SPR) problem, we are given a weighted graph\n$G=(V,E)$ and a set of terminals $K\\subset V$ of size $k$. The objective is to\nfind a minor $M$ of $G$ with only the terminals as its vertex set, such that\nthe distance between the terminals will be preserved up to a small\nmultiplicative distortion. Kamma, Krauthgamer and Nguyen [KKN15] used a\nball-growing algorithm with exponential distributions to show that the\ndistortion is at most $O(\\log^5 k)$. Cheung [Che17] improved the analysis of\nthe same algorithm, bounding the distortion by $O(\\log^2 k)$. We improve the\nanalysis of this ball-growing algorithm even further, bounding the distortion\nby $O(\\log k)$.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 14:19:32 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 09:16:42 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Filtser", "Arnold", ""]]}, {"id": "1706.08325", "submitter": "Matti Karppa", "authors": "Tommi Junttila (1), Matti Karppa (1), Petteri Kaski (1), Jukka Kohonen\n  (1) ((1) Aalto University, Department of Computer Science)", "title": "An adaptive prefix-assignment technique for symmetry reduction", "comments": "Updated manuscript submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique for symmetry reduction that adaptively\nassigns a prefix of variables in a system of constraints so that the generated\nprefix-assignments are pairwise nonisomorphic under the action of the symmetry\ngroup of the system. The technique is based on McKay's canonical extension\nframework [J.~Algorithms 26 (1998), no.~2, 306--324]. Among key features of the\ntechnique are (i) adaptability---the prefix sequence can be user-prescribed and\ntruncated for compatibility with the group of symmetries; (ii)\nparallelizability---prefix-assignments can be processed in parallel\nindependently of each other; (iii) versatility---the method is applicable\nwhenever the group of symmetries can be concisely represented as the\nautomorphism group of a vertex-colored graph; and (iv) implementability---the\nmethod can be implemented relying on a canonical labeling map for\nvertex-colored graphs as the only nontrivial subroutine. To demonstrate the\npractical applicability of our technique, we have prepared an experimental\nopen-source implementation of the technique and carry out a set of experiments\nthat demonstrate ability to reduce symmetry on hard instances. Furthermore, we\ndemonstrate that the implementation effectively parallelizes to compute\nclusters with multiple nodes via a message-passing interface.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 11:18:27 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 10:58:31 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Junttila", "Tommi", "", "Aalto University, Department of Computer Science"], ["Karppa", "Matti", "", "Aalto University, Department of Computer Science"], ["Kaski", "Petteri", "", "Aalto University, Department of Computer Science"], ["Kohonen", "Jukka", "", "Aalto University, Department of Computer Science"]]}, {"id": "1706.08550", "submitter": "Amir Ali Ahmadi", "authors": "Amir Ali Ahmadi and Jeffrey Zhang", "title": "Semidefinite Programming and Nash Equilibria in Bimatrix Games", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the power of semidefinite programming (SDP) for finding additive\n$epsilon$-approximate Nash equilibria in bimatrix games. We introduce an SDP\nrelaxation for a quadratic programming formulation of the Nash equilibrium (NE)\nproblem and provide a number of valid inequalities to improve the quality of\nthe relaxation. If a rank-1 solution to this SDP is found, then an exact NE can\nbe recovered. We show that for a strictly competitive game, our SDP is\nguaranteed to return a rank-1 solution. We propose two algorithms based on\niterative linearization of smooth nonconvex objective functions whose global\nminima by design coincide with rank-1 solutions. Empirically, we demonstrate\nthat these algorithms often recover solutions of rank at most two and $epsilon$\nclose to zero. Furthermore, we prove that if a rank-2 solution to our SDP is\nfound, then a 5/11-NE can be recovered for any game, or a 1/3-NE for a\nsymmetric game. We then show how our SDP approach can address two (NP-hard)\nproblems of economic interest: finding the maximum welfare achievable under any\nNE, and testing whether there exists a NE where a particular set of strategies\nis not played. Finally, we show the connection between our SDP and the first\nlevel of the Lasserre/sum of squares hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 18:18:58 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 21:48:13 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2019 04:12:32 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Zhang", "Jeffrey", ""]]}, {"id": "1706.08591", "submitter": "Nikolaos Sahinidis", "authors": "Satyajith Amaran, Nikolaos V. Sahinidis, Bikram Sharda, Scott J. Bury", "title": "Simulation optimization: A review of algorithms and applications", "comments": null, "journal-ref": null, "doi": "10.1007/s10479-015-2019-x", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation Optimization (SO) refers to the optimization of an objective\nfunction subject to constraints, both of which can be evaluated through a\nstochastic simulation. To address specific features of a particular\nsimulation---discrete or continuous decisions, expensive or cheap simulations,\nsingle or multiple outputs, homogeneous or heterogeneous noise---various\nalgorithms have been proposed in the literature. As one can imagine, there\nexist several competing algorithms for each of these classes of problems. This\ndocument emphasizes the difficulties in simulation optimization as compared to\nmathematical programming, makes reference to state-of-the-art algorithms in the\nfield, examines and contrasts the different approaches used, reviews some of\nthe diverse applications that have been tackled by these methods, and\nspeculates on future directions in the field.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 20:53:33 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Amaran", "Satyajith", ""], ["Sahinidis", "Nikolaos V.", ""], ["Sharda", "Bikram", ""], ["Bury", "Scott J.", ""]]}, {"id": "1706.08601", "submitter": "Nikolaos Sahinidis", "authors": "Yash Puranik, Nikolaos V. Sahinidis", "title": "Domain reduction techniques for global NLP and MINLP optimization", "comments": null, "journal-ref": null, "doi": "10.1007/s10601-016-9267-5", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization solvers routinely utilize presolve techniques, including model\nsimplification, reformulation and domain reduction techniques. Domain reduction\ntechniques are especially important in speeding up convergence to the global\noptimum for challenging nonconvex nonlinear programming (NLP) and mixed-integer\nnonlinear programming (MINLP) optimization problems. In this work, we survey\nthe various techniques used for domain reduction of NLP and MINLP optimization\nproblems. We also present a computational analysis of the impact of these\ntechniques on the performance of various widely available global solvers on a\ncollection of 1740 test problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 21:22:57 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Puranik", "Yash", ""], ["Sahinidis", "Nikolaos V.", ""]]}, {"id": "1706.08672", "submitter": "Tselil Schramm", "authors": "Tselil Schramm and David Steurer", "title": "Fast and robust tensor decomposition with applications to dictionary\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop fast spectral algorithms for tensor decomposition that match the\nrobustness guarantees of the best known polynomial-time algorithms for this\nproblem based on the sum-of-squares (SOS) semidefinite programming hierarchy.\n  Our algorithms can decompose a 4-tensor with $n$-dimensional orthonormal\ncomponents in the presence of error with constant spectral norm (when viewed as\nan $n^2$-by-$n^2$ matrix). The running time is $n^5$ which is close to linear\nin the input size $n^4$.\n  We also obtain algorithms with similar running time to learn sparsely-used\northogonal dictionaries even when feature representations have constant\nrelative sparsity and non-independent coordinates.\n  The only previous polynomial-time algorithms to solve these problem are based\non solving large semidefinite programs. In contrast, our algorithms are easy to\nimplement directly and are based on spectral projections and tensor-mode\nrearrangements.\n  Or work is inspired by recent of Hopkins, Schramm, Shi, and Steurer (STOC'16)\nthat shows how fast spectral algorithms can achieve the guarantees of SOS for\naverage-case problems. In this work, we introduce general techniques to capture\nthe guarantees of SOS for worst-case problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 05:12:39 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Schramm", "Tselil", ""], ["Steurer", "David", ""]]}, {"id": "1706.08721", "submitter": "Satoshi Takabe", "authors": "Satoshi Takabe, Takanori Maehara, and Koji Hukushima", "title": "Typical Approximation Performance for Maximum Coverage Problem", "comments": "10 pages, 6 figures", "journal-ref": "Phys. Rev. E 97, 022138 (2018)", "doi": "10.1103/PhysRevE.97.022138", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigated typical performance of approximation algorithms known\nas belief propagation, greedy algorithm, and linear-programming relaxation for\nmaximum coverage problems on sparse biregular random graphs. After using the\ncavity method for a corresponding hard-core lattice--gas model, results show\nthat two distinct thresholds of replica-symmetry and its breaking exist in the\ntypical performance threshold of belief propagation. In the low-density region,\nthe superiority of three algorithms in terms of a typical performance threshold\nis obtained by some theoretical analyses. Although the greedy algorithm and\nlinear-programming relaxation have the same approximation ratio in worst-case\nperformance, their typical performance thresholds are mutually different,\nindicating the importance of typical performance. Results of numerical\nsimulations validate the theoretical analyses and imply further mutual\nrelations of approximation algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 08:32:13 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 06:31:02 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Takabe", "Satoshi", ""], ["Maehara", "Takanori", ""], ["Hukushima", "Koji", ""]]}, {"id": "1706.08923", "submitter": "Christophe Guyeux", "authors": "Jean-Fran\\c{c}ois Couchot, Pierre-Cyrille Heam, Christophe Guyeux,\n  Qianxue Wang, and Jacques M. Bahi", "title": "Traversing a n-cube without Balanced Hamiltonian Cycle to Generate\n  Pseudorandom Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new class of Pseudorandom Number Generators. The\ngenerators are based on traversing a n-cube where a Balanced Hamiltonian Cycle\nhas been removed. The construction of such generators is automatic for small\nnumber of bits, but remains an open problem when this number becomes large. A\nrunning example is used throughout the paper. Finally, first statistical\nexperiments of these generators are presented, they show how efficient and\npromising the proposed approach seems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 09:20:52 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Couchot", "Jean-Fran\u00e7ois", ""], ["Heam", "Pierre-Cyrille", ""], ["Guyeux", "Christophe", ""], ["Wang", "Qianxue", ""], ["Bahi", "Jacques M.", ""]]}, {"id": "1706.09052", "submitter": "Daniel Paulusma", "authors": "\\\"Oznur Ya\\c{s}ar Diner and Dani\\\"el Paulusma and Christophe Picouleau\n  and Bernard Ries", "title": "Contraction and Deletion Blockers for Perfect Graphs and $H$-free Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following problem: for given integers $d$, $k$ and graph $G$,\ncan we reduce some fixed graph parameter $\\pi$ of $G$ by at least $d$ via at\nmost $k$ graph operations from some fixed set $S$? As parameters we take the\nchromatic number $\\chi$, clique number $\\omega$ and independence number\n$\\alpha$, and as operations we choose the edge contraction ec and vertex\ndeletion vd. We determine the complexity of this problem for $S=\\{\\mbox{ec}\\}$\nand $S=\\{\\mbox{vd}\\}$ and $\\pi\\in \\{\\chi,\\omega,\\alpha\\}$ for a number of\nsubclasses of perfect graphs. We use these results to determine the complexity\nof the problem for $S=\\{\\mbox{ec}\\}$ and $S=\\{\\mbox{vd}\\}$ and $\\pi\\in\n\\{\\chi,\\omega,\\alpha\\}$ restricted to $H$-free graphs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 21:16:25 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Diner", "\u00d6znur Ya\u015far", ""], ["Paulusma", "Dani\u00ebl", ""], ["Picouleau", "Christophe", ""], ["Ries", "Bernard", ""]]}, {"id": "1706.09066", "submitter": "Ignasi Sau", "authors": "J\\'ulio Ara\\'ujo, Victor A. Campos, Ana Karolinna Maia, Ignasi Sau,\n  Ana Silva", "title": "On the complexity of finding internally vertex-disjoint long directed\n  paths", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two positive integers $k$ and $\\ell$, a $(k \\times \\ell)$-spindle is the\nunion of $k$ pairwise internally vertex-disjoint directed paths with $\\ell$\narcs between two vertices $u$ and $v$. We are interested in the (parameterized)\ncomplexity of several problems consisting in deciding whether a given digraph\ncontains a subdivision of a spindle, which generalize both the Maximum Flow and\nLongest Path problems. We obtain the following complexity dichotomy: for a\nfixed $\\ell \\geq 1$, finding the largest $k$ such that an input digraph $G$\ncontains a subdivision of a $(k \\times \\ell)$-spindle is polynomial-time\nsolvable if $\\ell \\leq 3$, and NP-hard otherwise. We place special emphasis on\nfinding spindles with exactly two paths and present FPT algorithms that are\nasymptotically optimal under the ETH. These algorithms are based on the\ntechnique of representative families in matroids, and use also color-coding as\na subroutine. Finally, we study the case where the input graph is acyclic, and\npresent several algorithmic and hardness results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 22:17:20 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Ara\u00fajo", "J\u00falio", ""], ["Campos", "Victor A.", ""], ["Maia", "Ana Karolinna", ""], ["Sau", "Ignasi", ""], ["Silva", "Ana", ""]]}, {"id": "1706.09185", "submitter": "Oren Weimann", "authors": "Pawe{\\l} Gawrychowski, Nadav Krasnopolsky, Shay Mozes, Oren Weimann", "title": "Dispersion on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the $k$-dispersion problem, we need to select $k$ nodes of a given graph\nso as to maximize the minimum distance between any two chosen nodes. This can\nbe seen as a generalization of the independent set problem, where the goal is\nto select nodes so that the minimum distance is larger than 1. We design an\noptimal $O(n)$ time algorithm for the dispersion problem on trees consisting of\n$n$ nodes, thus improving the previous $O(n\\log n)$ time solution from 1997.\n  We also consider the weighted case, where the goal is to choose a set of\nnodes of total weight at least $W$. We present an $O(n\\log^2n)$ algorithm\nimproving the previous $O(n\\log^4 n)$ solution. Our solution builds on the\nsearch version (where we know the minimum distance $\\lambda$ between the chosen\nnodes) for which we present tight $\\Theta(n\\log n)$ upper and lower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 09:41:19 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Gawrychowski", "Pawe\u0142", ""], ["Krasnopolsky", "Nadav", ""], ["Mozes", "Shay", ""], ["Weimann", "Oren", ""]]}, {"id": "1706.09230", "submitter": "Caishi Fang", "authors": "Caishi Fang", "title": "Accelerations for Graph Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two main results. First, by only one conjecture\n(Conjecture 2.9) for recognizing a vertex symmetric graph, which is the hardest\ntask for our problem, we construct an algorithm for finding an isomorphism\nbetween two graphs in polynomial time $ O(n^{3}) $. Second, without that\nconjecture, we prove the algorithm to be of quasi-polynomial time $\nO(n^{1.5\\log n}) $. The conjectures in this paper are correct for all graphs of\nsize no larger than $ 5 $ and all graphs we have encountered. At least the\nconjecture for determining if a graph is vertex symmetric is quite true\nintuitively. We are not able to prove them by hand, so we have planned to find\npossible counterexamples by a computer. We also introduce new concepts like\ncollapse pattern and collapse tomography, which play important roles in our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 06:03:16 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Fang", "Caishi", ""]]}, {"id": "1706.09339", "submitter": "Amer Mouawad", "authors": "Eduard Eiben, Mithilesh Kumar, Amer E. Mouawad, Fahad Panolan, and\n  Sebastian Siebertz", "title": "Lossy Kernels for Connected Dominating Set on Sparse Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $\\alpha > 1$, an $\\alpha$-approximate (bi-)kernel is a polynomial-time\nalgorithm that takes as input an instance $(I, k)$ of a problem $\\mathcal{Q}$\nand outputs an instance $(I',k')$ (of a problem $\\mathcal{Q}'$) of size bounded\nby a function of $k$ such that, for every $c\\geq 1$, a $c$-approximate solution\nfor the new instance can be turned into a $(c\\cdot\\alpha)$-approximate solution\nof the original instance in polynomial time. This framework of lossy\nkernelization was recently introduced by Lokshtanov et al. We study Connected\nDominating Set (and its distance-$r$ variant) parameterized by solution size on\nsparse graph classes like biclique-free graphs, classes of bounded expansion,\nand nowhere dense classes. We prove that for every $\\alpha>1$, Connected\nDominating Set admits a polynomial-size $\\alpha$-approximate (bi-)kernel on all\nthe aforementioned classes. Our results are in sharp contrast to the\nkernelization complexity of Connected Dominating Set, which is known to not\nadmit a polynomial kernel even on $2$-degenerate graphs and graphs of bounded\nexpansion, unless $\\textsf{NP} \\subseteq \\textsf{coNP/poly}$. We complement our\nresults by the following conditional lower bound. We show that if a class\n$\\mathcal{C}$ is somewhere dense and closed under taking subgraphs, then for\nsome value of $r\\in\\mathbb{N}$ there cannot exist an $\\alpha$-approximate\nbi-kernel for the (Connected) Distance-$r$ Dominating Set problem on\n$\\mathcal{C}$ for any $\\alpha>1$ (assuming the Gap Exponential Time\nHypothesis).\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 15:55:48 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 17:16:54 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Eiben", "Eduard", ""], ["Kumar", "Mithilesh", ""], ["Mouawad", "Amer E.", ""], ["Panolan", "Fahad", ""], ["Siebertz", "Sebastian", ""]]}, {"id": "1706.09355", "submitter": "Indranil Banerjee", "authors": "Indranil Banerjee, Dana Richards", "title": "New Results On Routing Via Matchings On Graphs", "comments": "15 Pages, 5 Figures , 21st International Symposium on Fundamentals of\n  Computation Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present some new complexity results on the routing time of a\ngraph under the \\textit{routing via matching} model. This is a parallel routing\nmodel which was introduced by Alon et al\\cite{alon1994routing}. The model can\nbe viewed as a communication scheme on a distributed network. The nodes in the\nnetwork can communicate via matchings (a step), where a node exchanges data\n(pebbles) with its matched partner. Let $G$ be a connected graph with vertices\nlabeled from $\\{1,...,n\\}$ and the destination vertices of the pebbles are\ngiven by a permutation $\\pi$. The problem is to find a minimum step routing\nscheme for the input permutation $\\pi$. This is denoted as the routing time\n$rt(G,\\pi)$ of $G$ given $\\pi$. In this paper we characterize the complexity of\nsome known problems under the routing via matching model and discuss their\nrelationship to graph connectivity and clique number. We also introduce some\nnew problems in this domain, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 16:40:54 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Banerjee", "Indranil", ""], ["Richards", "Dana", ""]]}, {"id": "1706.09356", "submitter": "Pawe{\\l} Naroski", "authors": "Zbigniew Lonc, Pawe{\\l} Naroski, and Pawe{\\l} Rz\\k{a}\\.zewski", "title": "Tight Euler tours in uniform hypergraphs - computational aspects", "comments": null, "journal-ref": "Discrete Mathematics & Theoretical Computer Science, Vol. 19 no.\n  3, Analysis of Algorithms (September 26, 2017) dmtcs:3934", "doi": "10.23638/DMTCS-19-3-2", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By a tight tour in a $k$-uniform hypergraph $H$ we mean any sequence of its\nvertices $(w_0,w_1,\\ldots,w_{s-1})$ such that for all $i=0,\\ldots,s-1$ the set\n$e_i=\\{w_i,w_{i+1}\\ldots,w_{i+k-1}\\}$ is an edge of $H$ (where operations on\nindices are computed modulo $s$) and the sets $e_i$ for $i=0,\\ldots,s-1$ are\npairwise different. A tight tour in $H$ is a tight Euler tour if it contains\nall edges of $H$. We prove that the problem of deciding if a given $3$-uniform\nhypergraph has a tight Euler tour is NP-complete, and that it cannot be solved\nin time $2^{o(m)}$ (where $m$ is the number of edges in the input hypergraph),\nunless the ETH fails. We also present an exact exponential algorithm for the\nproblem, whose time complexity matches this lower bound, and the space\ncomplexity is polynomial. In fact, this algorithm solves a more general problem\nof computing the number of tight Euler tours in a given uniform hypergraph.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 16:42:34 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 15:57:19 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Lonc", "Zbigniew", ""], ["Naroski", "Pawe\u0142", ""], ["Rz\u0105\u017cewski", "Pawe\u0142", ""]]}, {"id": "1706.09487", "submitter": "Nikolai Karpov", "authors": "Ivan Bliznets and Nikolai Karpov", "title": "Parameterized Algorithms for Partitioning Graphs into Highly Connected\n  Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a well-known and important problem with numerous applications.\nThe graph-based model is one of the typical cluster models. In the graph model,\nclusters are generally defined as cliques. However, such an approach might be\ntoo restrictive as in some applications, not all objects from the same cluster\nmust be connected. That is why different types of cliques relaxations often\nconsidered as clusters.\n  In our work, we consider a problem of partitioning graph into clusters and a\nproblem of isolating cluster of a special type whereby cluster we mean highly\nconnected subgraph. Initially, such clusterization was proposed by Hartuv and\nShamir. And their HCS clustering algorithm was extensively applied in practice.\nIt was used to cluster cDNA fingerprints, to find complexes in protein-protein\ninteraction data, to group protein sequences hierarchically into superfamily\nand family clusters, to find families of regulatory RNA structures. The HCS\nalgorithm partitions graph in highly connected subgraphs. However, it is\nachieved by deletion of not necessarily the minimum number of edges. In our\nwork, we try to minimize the number of edge deletions. We consider problems\nfrom the parameterized point of view where the main parameter is a number of\nallowed edge deletions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 21:12:40 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Bliznets", "Ivan", ""], ["Karpov", "Nikolai", ""]]}, {"id": "1706.09593", "submitter": "Nil Mamano", "authors": "David Eppstein, Michael Goodrich, Doruk Korkmaz, Nil Mamano", "title": "Defining Equitable Geographic Districts in Road Networks via Stable\n  Matching", "comments": "9 pages, 4 figures, to appear in 25th ACM SIGSPATIAL International\n  Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL\n  2017) November 7-10, 2017, Redondo Beach, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method for defining geographic districts in road\nnetworks using stable matching. In this approach, each geographic district is\ndefined in terms of a center, which identifies a location of interest, such as\na post office or polling place, and all other network vertices must be labeled\nwith the center to which they are associated. We focus on defining geographic\ndistricts that are equitable, in that every district has the same number of\nvertices and the assignment is stable in terms of geographic distance. That is,\nthere is no unassigned vertex-center pair such that both would prefer each\nother over their current assignments. We solve this problem using a version of\nthe classic stable matching problem, called symmetric stable matching, in which\nthe preferences of the elements in both sets obey a certain symmetry. In our\ncase, we study a graph-based version of stable matching in which nodes are\nstably matched to a subset of nodes denoted as centers, prioritized by their\nshortest-path distances, so that each center is apportioned a certain number of\nnodes. We show that, for a planar graph or road network with $n$ nodes and $k$\ncenters, the problem can be solved in $O(n\\sqrt{n}\\log n)$ time, which improves\nupon the $O(nk)$ runtime of using the classic Gale-Shapley stable matching\nalgorithm when $k$ is large. Finally, we provide experimental results on road\nnetworks for these algorithms and a heuristic algorithm that performs better\nthan the Gale-Shapley algorithm for any range of values of $k$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 06:41:11 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 15:10:25 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Eppstein", "David", ""], ["Goodrich", "Michael", ""], ["Korkmaz", "Doruk", ""], ["Mamano", "Nil", ""]]}, {"id": "1706.09884", "submitter": "Jerry Li", "authors": "Jerry Li and Aleksander Madry and John Peebles and Ludwig Schmidt", "title": "On the Limitations of First-Order Approximation in GAN Dynamics", "comments": "18 pages, 4 figures, accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Generative Adversarial Networks (GANs) have demonstrated promising\nperformance on multiple vision tasks, their learning dynamics are not yet well\nunderstood, both in theory and in practice. To address this issue, we study GAN\ndynamics in a simple yet rich parametric model that exhibits several of the\ncommon problematic convergence behaviors such as vanishing gradients, mode\ncollapse, and diverging or oscillatory behavior. In spite of the non-convex\nnature of our model, we are able to perform a rigorous theoretical analysis of\nits convergence behavior. Our analysis reveals an interesting dichotomy: a GAN\nwith an optimal discriminator provably converges, while first order\napproximations of the discriminator steps lead to unstable GAN dynamics and\nmode collapse. Our result suggests that using first order discriminator steps\n(the de-facto standard in most existing GAN setups) might be one of the factors\nthat makes GAN training challenging in practice.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:52:44 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 23:00:03 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Li", "Jerry", ""], ["Madry", "Aleksander", ""], ["Peebles", "John", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1706.09937", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Dan Alistarh, Bart{\\l}omiej Dudek, Adrian Kosowski, David Soloveichik,\n  Przemys{\\l}aw Uzna\\'nski", "title": "Robust Detection in Leak-Prone Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to electronic computation, chemical computation is noisy and\nsusceptible to a variety of sources of error, which has prevented the\nconstruction of robust complex systems. To be effective, chemical algorithms\nmust be designed with an appropriate error model in mind. Here we consider the\nmodel of chemical reaction networks that preserve molecular count (population\nprotocols), and ask whether computation can be made robust to a natural model\nof unintended \"leak\" reactions. Our definition of leak is motivated by both the\nparticular spurious behavior seen when implementing chemical reaction networks\nwith DNA strand displacement cascades, as well as the unavoidable side\nreactions in any implementation due to the basic laws of chemistry. We develop\na new \"Robust Detection\" algorithm for the problem of fast (logarithmic time)\nsingle molecule detection, and prove that it is robust to this general model of\nleaks. Besides potential applications in single molecule detection, the\nerror-correction ideas developed here might enable a new class of\nrobust-by-design chemical algorithms. Our analysis is based on a non-standard\nhybrid argument, combining ideas from discrete analysis of population protocols\nwith classic Markov chain techniques.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 19:52:17 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 13:32:21 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Alistarh", "Dan", ""], ["Dudek", "Bart\u0142omiej", ""], ["Kosowski", "Adrian", ""], ["Soloveichik", "David", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "1706.09966", "submitter": "Denis Pankratov", "authors": "Allan Borodin, Denis Pankratov, Amirali Salehi-Abari", "title": "On Conceptually Simple Algorithms for Variants of Online Bipartite\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a series of results regarding conceptually simple algorithms for\nbipartite matching in various online and related models. We first consider a\ndeterministic adversarial model. The best approximation ratio possible for a\none-pass deterministic online algorithm is $1/2$, which is achieved by any\ngreedy algorithm. D\\\"urr et al. recently presented a $2$-pass algorithm called\nCategory-Advice that achieves approximation ratio $3/5$. We extend their\nalgorithm to multiple passes. We prove the exact approximation ratio for the\n$k$-pass Category-Advice algorithm for all $k \\ge 1$, and show that the\napproximation ratio converges to the inverse of the golden ratio\n$2/(1+\\sqrt{5}) \\approx 0.618$ as $k$ goes to infinity. The convergence is\nextremely fast --- the $5$-pass Category-Advice algorithm is already within\n$0.01\\%$ of the inverse of the golden ratio.\n  We then consider a natural greedy algorithm in the online stochastic IID\nmodel---MinDegree. This algorithm is an online version of a well-known and\nextensively studied offline algorithm MinGreedy. We show that MinDegree cannot\nachieve an approximation ratio better than $1-1/e$, which is guaranteed by any\nconsistent greedy algorithm in the known IID model.\n  Finally, following the work in Besser and Poloczek, we depart from an\nadversarial or stochastic ordering and investigate a natural randomized\nalgorithm (MinRanking) in the priority model. Although the priority model\nallows the algorithm to choose the input ordering in a general but well defined\nway, this natural algorithm cannot obtain the approximation of the Ranking\nalgorithm in the ROM model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 22:04:06 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Borodin", "Allan", ""], ["Pankratov", "Denis", ""], ["Salehi-Abari", "Amirali", ""]]}, {"id": "1706.10030", "submitter": "Leonid Sokolinsky", "authors": "Irina Sokolinskaya and Leonid B. Sokolinsky", "title": "On the Solution of Linear Programming Problems in the Age of Big Data", "comments": "Parallel Computational Technologies - 11th International Conference,\n  PCT 2017, Kazan, Russia, April 3-7, 2017, Proceedings (to be published in\n  Communications in Computer and Information Science, vol. 753)", "journal-ref": "Communications in Computer and Information Science, vol. 753. pp.\n  86-100. Springer, Cham (2017)", "doi": "10.1007/978-3-319-67035-5_7", "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Big Data phenomenon has spawned large-scale linear programming problems.\nIn many cases, these problems are non-stationary. In this paper, we describe a\nnew scalable algorithm called NSLP for solving high-dimensional, non-stationary\nlinear programming problems on modern cluster computing systems. The algorithm\nconsists of two phases: Quest and Targeting. The Quest phase calculates a\nsolution of the system of inequalities defining the constraint system of the\nlinear programming problem under the condition of dynamic changes in input\ndata. To this end, the apparatus of Fejer mappings is used. The Targeting phase\nforms a special system of points having the shape of an n-dimensional\naxisymmetric cross. The cross moves in the n-dimensional space in such a way\nthat the solution of the linear programming problem is located all the time in\nan \"-vicinity of the central point of the cross.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 05:44:36 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 01:36:08 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Sokolinskaya", "Irina", ""], ["Sokolinsky", "Leonid B.", ""]]}, {"id": "1706.10047", "submitter": "Claire Pennarun", "authors": "Paul Dorbec (UNICAEN), Antonio Gonz\\'alez, Claire Pennarun (LaBRI)", "title": "Power domination in maximal planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power domination in graphs emerged from the problem of monitoring an\nelectrical system by placing as few measurement devices in the system as\npossible. It corresponds to a variant of domination that includes the\npossibility of propagation. For measurement devices placed on a set S of\nvertices of a graph G, the set of monitored vertices is initially the set S\ntogether with all its neighbors. Then iteratively, whenever some monitored\nvertex v has a single neighbor u not yet monitored, u gets monitored. A set S\nis said to be a power dominating set of the graph G if all vertices of G\neventually are monitored. The power domination number of a graph is the minimum\nsize of a power dominating set. In this paper, we prove that any maximal planar\ngraph of order n $\\ge$ 6 admits a power dominating set of size at most (n--2)/4 .\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 07:37:54 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 13:23:59 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 16:33:29 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Dorbec", "Paul", "", "UNICAEN"], ["Gonz\u00e1lez", "Antonio", "", "LaBRI"], ["Pennarun", "Claire", "", "LaBRI"]]}, {"id": "1706.10061", "submitter": "Isamu Furuya", "authors": "Isamu Furuya, Takuya Kida", "title": "Compaction of Church Numerals for Higher-Order Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we address the problem of compacting Church numerals. Church\nnumerals appear as a representation of the repetitive part of data in\nhigher-order compression. We propose a novel decomposition scheme for a natural\nnumber using tetration, which leads to a compact representation of\n$\\lambda$-terms equivalent to the original Church numerals. For natural number\n$n$, we prove that the size of the $\\lambda$-term obtained by the proposed\nmethod is $O(({\\rm slog}_{2}n)^{\\log n/ \\log \\log n})$. Moreover, we\nquantitatively confirmed experimentally that the proposed method outperforms a\nbinary expression of Church numerals when $n$ is less than approximately 10000.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 08:36:57 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 05:00:34 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Furuya", "Isamu", ""], ["Kida", "Takuya", ""]]}, {"id": "1706.10094", "submitter": "Mikko Berggren Ettienne", "authors": "Philip Bille, Mikko Berggren Ettienne, Inge Li G{\\o}rtz, Hjalte Wedel\n  Vildh{\\o}j", "title": "Time-Space Trade-Offs for Lempel-Ziv Compressed Indexing", "comments": null, "journal-ref": null, "doi": "10.1016/j.tcs.2017.12.021", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $S$, the \\emph{compressed indexing problem} is to preprocess\n$S$ into a compressed representation that supports fast \\emph{substring\nqueries}. The goal is to use little space relative to the compressed size of\n$S$ while supporting fast queries. We present a compressed index based on the\nLempel--Ziv 1977 compression scheme. We obtain the following time-space\ntrade-offs: For constant-sized alphabets; (i) $O(m + occ \\lg\\lg n)$ time using\n$O(z\\lg(n/z)\\lg\\lg z)$ space, or (ii) $O(m(1 + \\frac{\\lg^\\epsilon z}{\\lg(n/z)})\n+ occ(\\lg\\lg n + \\lg^\\epsilon z))$ time using $O(z\\lg(n/z))$ space. For integer\nalphabets polynomially bounded by $n$; (iii) $O(m(1 + \\frac{\\lg^\\epsilon\nz}{\\lg(n/z)}) + occ(\\lg\\lg n + \\lg^\\epsilon z))$ time using $O(z(\\lg(n/z) +\n\\lg\\lg z))$ space, or (iv) $O(m + occ(\\lg\\lg n + \\lg^{\\epsilon} z))$ time using\n$O(z(\\lg(n/z) + \\lg^{\\epsilon} z))$ space, where $n$ and $m$ are the length of\nthe input string and query string respectively, $z$ is the number of phrases in\nthe LZ77 parse of the input string, $occ$ is the number of occurrences of the\nquery in the input and $\\epsilon > 0$ is an arbitrarily small constant. In\nparticular, (i) improves the leading term in the query time of the previous\nbest solution from $O(m\\lg m)$ to $O(m)$ at the cost of increasing the space by\na factor $\\lg \\lg z$. Alternatively, (ii) matches the previous best space\nbound, but has a leading term in the query time of $O(m(1+\\frac{\\lg^{\\epsilon}\nz}{\\lg (n/z)}))$. However, for any polynomial compression ratio, i.e., $z =\nO(n^{1-\\delta})$, for constant $\\delta > 0$, this becomes $O(m)$. Our index\nalso supports extraction of any substring of length $\\ell$ in $O(\\ell +\n\\lg(n/z))$ time. Technically, our results are obtained by novel extensions and\ncombinations of existing data structures of independent interest, including a\nnew batched variant of weak prefix search.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 10:02:10 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 10:45:07 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Bille", "Philip", ""], ["Ettienne", "Mikko Berggren", ""], ["G\u00f8rtz", "Inge Li", ""], ["Vildh\u00f8j", "Hjalte Wedel", ""]]}, {"id": "1706.10110", "submitter": "Kasper Green Larsen", "authors": "Casper Benjamin Freksen and Kasper Green Larsen", "title": "On Using Toeplitz and Circulant Matrices for Johnson-Lindenstrauss\n  Transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Johnson-Lindenstrauss lemma is one of the corner stone results in\ndimensionality reduction. It says that given $N$, for any set of $N$ vectors $X\n\\subset \\mathbb{R}^n$, there exists a mapping $f : X \\to \\mathbb{R}^m$ such\nthat $f(X)$ preserves all pairwise distances between vectors in $X$ to within\n$(1 \\pm \\varepsilon)$ if $m = O(\\varepsilon^{-2} \\lg N)$. Much effort has gone\ninto developing fast embedding algorithms, with the Fast Johnson-Lindenstrauss\ntransform of Ailon and Chazelle being one of the most well-known techniques.\nThe current fastest algorithm that yields the optimal $m =\nO(\\varepsilon^{-2}\\lg N)$ dimensions has an embedding time of $O(n \\lg n +\n\\varepsilon^{-2} \\lg^3 N)$. An exciting approach towards improving this, due to\nHinrichs and Vyb\\'iral, is to use a random $m \\times n$ Toeplitz matrix for the\nembedding. Using Fast Fourier Transform, the embedding of a vector can then be\ncomputed in $O(n \\lg m)$ time. The big question is of course whether $m =\nO(\\varepsilon^{-2} \\lg N)$ dimensions suffice for this technique. If so, this\nwould end a decades long quest to obtain faster and faster\nJohnson-Lindenstrauss transforms. The current best analysis of the embedding of\nHinrichs and Vyb\\'iral shows that $m = O(\\varepsilon^{-2}\\lg^2 N)$ dimensions\nsuffices. The main result of this paper, is a proof that this analysis\nunfortunately cannot be tightened any further, i.e., there exists a set of $N$\nvectors requiring $m = \\Omega(\\varepsilon^{-2} \\lg^2 N)$ for the Toeplitz\napproach to work.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 10:46:28 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 08:23:55 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Freksen", "Casper Benjamin", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "1706.10191", "submitter": "Adalat Jabrayilov", "authors": "Adalat Jabrayilov and Petra Mutzel", "title": "New Integer Linear Programming Models for the Vertex Coloring Problem", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vertex coloring problem asks for the minimum number of colors that can be\nassigned to the vertices of a given graph such that for all vertices v the\ncolor of v is different from the color of any of its neighbors. The problem is\nNP-hard. Here, we introduce new integer linear programming formulations based\non partial orderings. They have the advantage that they are as simple to work\nwith as the classical assignment formulation, since they can be fed directly\ninto a standard integer linear programming solver. We evaluate our new models\nusing Gurobi and show that our new simple approach is a good alternative to the\nbest state-of-the-art approaches for the vertex coloring problem. In our\ncomputational experiments, we compare our formulations with the classical\nassignment formulation and the representatives formulation on a large set of\nbenchmark graphs as well as randomly generated graphs of varying size and\ndensity. The evaluation shows that one of the new models dominates both\nformulations for sparse graphs, while the representatives formulation is the\nbest for dense graphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:37:16 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 16:00:15 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Jabrayilov", "Adalat", ""], ["Mutzel", "Petra", ""]]}, {"id": "1706.10195", "submitter": "Thom Castermans", "authors": "Thom Castermans and Bettina Speckmann and Frank Staals and Kevin\n  Verbeek", "title": "Agglomerative Clustering of Growing Squares", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study an agglomerative clustering problem motivated by interactive glyphs\nin geo-visualization. Consider a set of disjoint square glyphs on an\ninteractive map. When the user zooms out, the glyphs grow in size relative to\nthe map, possibly with different speeds. When two glyphs intersect, we wish to\nreplace them by a new glyph that captures the information of the intersecting\nglyphs.\n  We present a fully dynamic kinetic data structure that maintains a set of $n$\ndisjoint growing squares. Our data structure uses $O(n (\\log n \\log\\log n)^2)$\nspace, supports queries in worst case $O(\\log^3 n)$ time, and updates in\n$O(\\log^7 n)$ amortized time. This leads to an $O(n\\alpha(n)\\log^7 n)$ time\nalgorithm to solve the agglomerative clustering problem. This is a significant\nimprovement over the current best $O(n^2)$ time algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:43:33 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 09:40:55 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Castermans", "Thom", ""], ["Speckmann", "Bettina", ""], ["Staals", "Frank", ""], ["Verbeek", "Kevin", ""]]}, {"id": "1706.10209", "submitter": "Mahdi Jafari Siavoshani", "authors": "Mahdi Jafari Siavoshani, Ali Pourmiri, Seyed Pooya Shariatpanahi", "title": "Storage, Communication, and Load Balancing Trade-off in Distributed\n  Cache Networks", "comments": "This is the journal version of our earlier work [arXiv:1610.05961]\n  presented at International Parallel & Distributed Processing Symposium\n  (IPDPS), 2017. This manuscript is 15 pages and contains 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.DS cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider load balancing in a network of caching servers delivering\ncontents to end users. Randomized load balancing via the so-called power of two\nchoices is a well-known approach in parallel and distributed systems. In this\nframework, we investigate the tension between storage resources, communication\ncost, and load balancing performance. To this end, we propose a randomized load\nbalancing scheme which simultaneously considers cache size limitation and\nproximity in the server redirection process.\n  In contrast to the classical power of two choices setup, since the memory\nlimitation and the proximity constraint cause correlation in the server\nselection process, we may not benefit from the power of two choices. However,\nwe prove that in certain regimes of problem parameters, our scheme results in\nthe maximum load of order $\\Theta(\\log\\log n)$ (here $n$ is the network size).\nThis is an exponential improvement compared to the scheme which assigns each\nrequest to the nearest available replica. Interestingly, the extra\ncommunication cost incurred by our proposed scheme, compared to the nearest\nreplica strategy, is small. Furthermore, our extensive simulations show that\nthe trade-off trend does not depend on the network topology and library\npopularity profile details.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 14:12:32 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Siavoshani", "Mahdi Jafari", ""], ["Pourmiri", "Ali", ""], ["Shariatpanahi", "Seyed Pooya", ""]]}, {"id": "1706.10228", "submitter": "Adam Karczmarz", "authors": "Jacob Holm, Giuseppe F. Italiano, Adam Karczmarz, Jakub {\\L}\\k{a}cki,\n  Eva Rotenberg, Piotr Sankowski", "title": "Contracting a Planar Graph Efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data structure that can maintain a simple planar graph under\nedge contractions in linear total time. The data structure supports adjacency\nqueries and provides access to neighbor lists in $O(1)$ time. Moreover, it can\nreport all the arising self-loops and parallel edges.\n  By applying the data structure, we can achieve optimal running times for\ndecremental bridge detection, 2-edge connectivity, maximal 3-edge connected\ncomponents, and the problem of finding a unique perfect matching for a static\nplanar graph. Furthermore, we improve the running times of algorithms for\nseveral planar graph problems, including decremental 2-vertex and 3-edge\nconnectivity, and we show that using our data structure in a black-box manner,\none obtains conceptually simple optimal algorithms for computing MST and\n5-coloring in planar graphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 14:54:41 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Holm", "Jacob", ""], ["Italiano", "Giuseppe F.", ""], ["Karczmarz", "Adam", ""], ["\u0141\u0105cki", "Jakub", ""], ["Rotenberg", "Eva", ""], ["Sankowski", "Piotr", ""]]}, {"id": "1706.10284", "submitter": "Konstantinos Mampentzidis", "authors": "Gerth St{\\o}lting Brodal, Konstantinos Mampentzidis", "title": "Cache Oblivious Algorithms for Computing the Triplet Distance Between\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing the triplet distance between two rooted\nunordered trees with $n$ labeled leafs. Introduced by Dobson 1975, the triplet\ndistance is the number of leaf triples that induce different topologies in the\ntwo trees. The current theoretically best algorithm is an $\\mathrm{O}(n \\log\nn)$ time algorithm by Brodal et al. (SODA 2013). Recently Jansson and Rajaby\nproposed a new algorithm that, while slower in theory, requiring $\\mathrm{O}(n\n\\log^3 n)$ time, in practice it outperforms the theoretically faster\n$\\mathrm{O}(n \\log n)$ algorithm. Both algorithms do not scale to external\nmemory. We present two cache oblivious algorithms that combine the best of both\nworlds. The first algorithm is for the case when the two input trees are binary\ntrees and the second a generalized algorithm for two input trees of arbitrary\ndegree. Analyzed in the RAM model, both algorithms require $\\mathrm{O}(n \\log\nn)$ time, and in the cache oblivious model $\\mathrm{O}(\\frac{n}{B} \\log_{2}\n\\frac{n}{M})$ I/Os. Their relative simplicity and the fact that they scale to\nexternal memory makes them achieve the best practical performance. We note that\nthese are the first algorithms that scale to external memory, both in theory\nand practice, for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 17:32:26 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 14:51:30 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2019 13:56:07 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Brodal", "Gerth St\u00f8lting", ""], ["Mampentzidis", "Konstantinos", ""]]}]