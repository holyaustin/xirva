[{"id": "0901.0205", "submitter": "Julia Chuzhoy", "authors": "Deeparnab Chakrabarty, Julia Chuzhoy, Sanjeev Khanna", "title": "On Allocating Goods to Maximize Fairness", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $m$ agents and a set of $n$ items, where agent $A$ has utility\n$u_{A,i}$ for item $i$, our goal is to allocate items to agents to maximize\nfairness. Specifically, the utility of an agent is the sum of its utilities for\nitems it receives, and we seek to maximize the minimum utility of any agent.\nWhile this problem has received much attention recently, its approximability\nhas not been well-understood thus far: the best known approximation algorithm\nachieves an $\\tilde{O}(\\sqrt{m})$-approximation, and in contrast, the best\nknown hardness of approximation stands at 2.\n  Our main result is an approximation algorithm that achieves an\n$\\tilde{O}(n^{\\eps})$ approximation for any $\\eps=\\Omega(\\log\\log n/\\log n)$ in\ntime $n^{O(1/\\eps)}$. In particular, we obtain poly-logarithmic approximation\nin quasi-polynomial time, and for any constant $\\eps > 0$, we obtain\n$O(n^{\\eps})$ approximation in polynomial time. An interesting aspect of our\nalgorithm is that we use as a building block a linear program whose integrality\ngap is $\\Omega(\\sqrt m)$. We bypass this obstacle by iteratively using the\nsolutions produced by the LP to construct new instances with significantly\nsmaller integrality gaps, eventually obtaining the desired approximation.\n  We also investigate the special case of the problem, where every item has a\nnon-zero utility for at most two agents. We show that even in this restricted\nsetting the problem is hard to approximate upto any factor better tha 2, and\nshow a factor $(2+\\eps)$-approximation algorithm running in time\n$poly(n,1/\\eps)$ for any $\\eps>0$. This special case can be cast as a graph\nedge orientation problem, and our algorithm can be viewed as a generalization\nof Eulerian orientations to weighted graphs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2009 01:24:26 GMT"}], "update_date": "2009-01-05", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Chuzhoy", "Julia", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "0901.0290", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Nicolae Tapus", "title": "Offline Algorithmic Techniques for Several Content Delivery Problems in\n  Some Restricted Types of Distributed Systems", "comments": "Proceedings of the International Workshop on High Performance Grid\n  Middleware (HiPerGrid), pp. 65-72, Bucharest, Romania, 21-22 November, 2008.\n  (ISSN: 2065-0701)", "journal-ref": "Proceedings of the International Workshop on High Performance Grid\n  Middleware (HiPerGrid), pp. 65-72, Bucharest, Romania, 2008. (ISSN:\n  2065-0701)", "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider several content delivery problems (broadcast and\nmulticast, in particular) in some restricted types of distributed systems (e.g.\noptical Grids and wireless sensor networks with tree-like topologies). For each\nproblem we provide efficient algorithmic techniques for computing optimal\ncontent delivery strategies. The techniques we present are offline, which means\nthat they can be used only when full information is available and the problem\nparameters do not fluctuate too much.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2009 21:53:57 GMT"}], "update_date": "2009-01-06", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Tapus", "Nicolae", ""]]}, {"id": "0901.0291", "submitter": "Mugurel Ionut Andreica", "authors": "Alexandra Carpen-Amarie, Mugurel Ionut Andreica, Valentin Cristea", "title": "An Algorithm for File Transfer Scheduling in Grid Environments", "comments": "Proceedings of the International Workshop on High Performance Grid\n  Middleware (HiPerGrid), pp. 33-40, Bucharest, Romania, 21-22 November, 2008.\n  (ISSN: 2065-0701)", "journal-ref": "Proceedings of the International Workshop on High Performance Grid\n  Middleware (HiPerGrid), pp. 33-40, Bucharest, Romania, 2008. (ISSN:\n  2065-0701)", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the data transfer scheduling problem for Grid\nenvironments, presenting a centralized scheduler developed with dynamic and\nadaptive features. The algorithm offers a reservation system for user transfer\nrequests that allocates them transfer times and bandwidth, according to the\nnetwork topology and the constraints the user specified for the requests. This\npaper presents the projects related to the data transfer field, the design of\nthe framework for which the scheduler was built, the main features of the\nscheduler, the steps for transfer requests rescheduling and two tests that\nillustrate the system's behavior for different types of transfer requests.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2009 22:03:02 GMT"}], "update_date": "2009-01-06", "authors_parsed": [["Carpen-Amarie", "Alexandra", ""], ["Andreica", "Mugurel Ionut", ""], ["Cristea", "Valentin", ""]]}, {"id": "0901.0501", "submitter": "Stefan Kiefer", "authors": "Morten K\\\"uhnrich, Stefan Schwoon, Ji\\v{r}\\'i Srba, Stefan Kiefer", "title": "Interprocedural Dataflow Analysis over Weight Domains with Infinite\n  Descending Chains", "comments": "technical report for a FOSSACS'09 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalized fixed-point equations over idempotent semirings and\nprovide an efficient algorithm for the detection whether a sequence of Kleene's\niterations stabilizes after a finite number of steps. Previously known\napproaches considered only bounded semirings where there are no infinite\ndescending chains. The main novelty of our work is that we deal with semirings\nwithout the boundedness restriction. Our study is motivated by several\napplications from interprocedural dataflow analysis. We demonstrate how the\nreachability problem for weighted pushdown automata can be reduced to solving\nequations in the framework mentioned above and we describe a few applications\nto demonstrate its usability.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2009 16:47:21 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2009 16:00:09 GMT"}], "update_date": "2009-01-06", "authors_parsed": [["K\u00fchnrich", "Morten", ""], ["Schwoon", "Stefan", ""], ["Srba", "Ji\u0159\u00ed", ""], ["Kiefer", "Stefan", ""]]}, {"id": "0901.0930", "submitter": "Jan Tusch", "authors": "Marc M\\\"orig, Dieter Rautenbach, Michiel Smid, Jan Tusch", "title": "An \\Omega(n log n) lower bound for computing the sum of even-ranked\n  elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sequence A of 2n real numbers, the Even-Rank-Sum problem asks for the\nsum of the n values that are at the even positions in the sorted order of the\nelements in A. We prove that, in the algebraic computation-tree model, this\nproblem has time complexity \\Theta(n log n). This solves an open problem posed\nby Michael Shamos at the Canadian Conference on Computational Geometry in 2008.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2009 21:55:59 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2009 09:53:31 GMT"}], "update_date": "2009-03-23", "authors_parsed": [["M\u00f6rig", "Marc", ""], ["Rautenbach", "Dieter", ""], ["Smid", "Michiel", ""], ["Tusch", "Jan", ""]]}, {"id": "0901.1140", "submitter": "Khaled Elbassioni", "authors": "Khaled Elbassioni, Rajiv Raman, Saurabh Ray, Rene Sitters", "title": "On Profit-Maximizing Pricing for the Highway and Tollbooth Problems", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-04645-2_25", "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the \\emph{tollbooth problem}, we are given a tree $\\bT=(V,E)$ with $n$\nedges, and a set of $m$ customers, each of whom is interested in purchasing a\npath on the tree. Each customer has a fixed budget, and the objective is to\nprice the edges of $\\bT$ such that the total revenue made by selling the paths\nto the customers that can afford them is maximized. An important special case\nof this problem, known as the \\emph{highway problem}, is when $\\bT$ is\nrestricted to be a line.\n  For the tollbooth problem, we present a randomized $O(\\log n)$-approximation,\nimproving on the current best $O(\\log m)$-approximation. We also study a\nspecial case of the tollbooth problem, when all the paths that customers are\ninterested in purchasing go towards a fixed root of $\\bT$. In this case, we\npresent an algorithm that returns a $(1-\\epsilon)$-approximation, for any\n$\\epsilon > 0$, and runs in quasi-polynomial time. On the other hand, we rule\nout the existence of an FPTAS by showing that even for the line case, the\nproblem is strongly NP-hard. Finally, we show that in the \\emph{coupon model},\nwhen we allow some items to be priced below zero to improve the overall profit,\nthe problem becomes even APX-hard.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2009 21:23:37 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2009 14:06:17 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2009 16:35:48 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Elbassioni", "Khaled", ""], ["Raman", "Rajiv", ""], ["Ray", "Saurabh", ""], ["Sitters", "Rene", ""]]}, {"id": "0901.1155", "submitter": "Itai Benjamini", "authors": "Itai Benjamini, Yury Makarychev", "title": "Balanced allocation: Memory performance tradeoffs", "comments": "Published in at http://dx.doi.org/10.1214/11-AAP804 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2012, Vol. 22, No. 4, 1642-1649", "doi": "10.1214/11-AAP804", "report-no": "IMS-AAP-AAP804", "categories": "cs.DS cs.DM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we sequentially put $n$ balls into $n$ bins. If we put each ball into\na random bin then the heaviest bin will contain ${\\sim}\\log n/\\log\\log n$ balls\nwith high probability. However, Azar, Broder, Karlin and Upfal [SIAM J. Comput.\n29 (1999) 180--200] showed that if each time we choose two bins at random and\nput the ball in the least loaded bin among the two, then the heaviest bin will\ncontain only ${\\sim}\\log\\log n$ balls with high probability. How much memory do\nwe need to implement this scheme? We need roughly $\\log\\log\\log n$ bits per\nbin, and $n\\log\\log\\log n$ bits in total. Let us assume now that we have\nlimited amount of memory. For each ball, we are given two random bins and we\nhave to put the ball into one of them. Our goal is to minimize the load of the\nheaviest bin. We prove that if we have $n^{1-\\delta}$ bits then the heaviest\nbin will contain at least $\\Omega(\\delta\\log n/\\log\\log n)$ balls with high\nprobability. The bound is tight in the communication complexity model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2009 00:23:33 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2012 07:22:48 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Benjamini", "Itai", ""], ["Makarychev", "Yury", ""]]}, {"id": "0901.1427", "submitter": "Sourav Chakraborty", "authors": "Sourav Chakraborty, Nikhil Devanur", "title": "An Online Multi-unit Auction with Improved Competitive Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the best known competitive ratio (from 1/4 to 1/2), for the online\nmulti-unit allocation problem, where the objective is to maximize the\nsingle-price revenue. Moreover, the competitive ratio of our algorithm tends to\n1, as the bid-profile tends to ``smoothen''. This algorithm is used as a\nsubroutine in designing truthful auctions for the same setting: the allocation\nhas to be done online, while the payments can be decided at the end of the day.\nEarlier, a reduction from the auction design problem to the allocation problem\nwas known only for the unit-demand case. We give a reduction for the general\ncase when the bidders have decreasing marginal utilities. The problem is\ninspired by sponsored search auctions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2009 10:00:38 GMT"}], "update_date": "2009-01-13", "authors_parsed": [["Chakraborty", "Sourav", ""], ["Devanur", "Nikhil", ""]]}, {"id": "0901.1563", "submitter": "Bruno Escoffier", "authors": "Nicolas Bourgeois, Bruno Escoffier, Vangelis Th. Paschos, Johan M.M\n  van Rooij", "title": "Fast Algorithms for Max Independent Set in Graphs of Small Average\n  Degree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max Independent Set (MIS) is a paradigmatic problem in theoretical computer\nscience and numerous studies tackle its resolution by exact algorithms with\nnon-trivial worst-case complexity. The best such complexity is, to our\nknowledge, the $O^*(1.1889^n)$ algorithm claimed by J.M. Robson (T.R. 1251-01,\nLaBRI, Univ. Bordeaux I, 2001) in his unpublished technical report. We also\nquote the $O^*(1.2210^n)$ algorithm by Fomin and al. (in Proc. SODA'06, pages\n18-25, 2006), that is the best published result about MIS.\n  In this paper we settle MIS in (connected) graphs with \"small\" average\ndegree, more precisely with average degree at most 3, 4, 5 and 6. Dealing with\ngraphs of average degree at most 3, the best bound known is the recent\n$O^*(1.0977^n)$ bound by N. Bourgeois and al. in Proc. IWPEC'08, pages 55-65,\n2008). Here we improve this result down to $O^*(1.0854^n)$ by proposing finer\nand more powerful reduction rules.\n  We then propose a generic method showing how improvement of the worst-case\ncomplexity for MIS in graphs of average degree $d$ entails improvement of it in\nany graph of average degree greater than $d$ and, based upon it, we tackle MIS\nin graphs of average degree 4, 5 and 6.\n  For MIS in graphs with average degree 4, we provide an upper complexity bound\nof $O^*(1.1571^n)$ that outperforms the best known bound of $O^*(1.1713^n)$ by\nR. Beigel (Proc. SODA'99, pages 856-857, 1999).\n  For MIS in graphs of average degree at most 5 and 6, we provide bounds of\n$O^*(1.1969^n)$ and $O^*(1.2149^n)$, respectively, that improve upon the\ncorresponding bounds of $O^*(1.2023^n)$ and $O^*(1.2172^n)$ in graphs of\nmaximum degree 5 and 6 by (Fomin et al., 2006).\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2009 12:40:32 GMT"}], "update_date": "2009-01-13", "authors_parsed": [["Bourgeois", "Nicolas", ""], ["Escoffier", "Bruno", ""], ["Paschos", "Vangelis Th.", ""], ["van Rooij", "Johan M. M", ""]]}, {"id": "0901.1684", "submitter": "Riccardo Zecchina", "authors": "M. Bayati, A. Braunstein, R. Zecchina", "title": "A rigorous analysis of the cavity equations for the minimum spanning\n  tree", "comments": "5 pages, 1 figure", "journal-ref": "J. Math. Phys. 49, 125206 (2008)", "doi": "10.1063/1.2982805", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a new general representation for the Minimum Weight Steiner Tree\n(MST) problem which translates the topological connectivity constraint into a\nset of local conditions which can be analyzed by the so called cavity equations\ntechniques. For the limit case of the Spanning tree we prove that the fixed\npoint of the algorithm arising from the cavity equations leads to the global\noptimum.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2009 23:07:59 GMT"}], "update_date": "2009-01-14", "authors_parsed": [["Bayati", "M.", ""], ["Braunstein", "A.", ""], ["Zecchina", "R.", ""]]}, {"id": "0901.1696", "submitter": "Julien Langou", "authors": "Fred G. Gustavson, Jerzy Wasniewski, Jack J. Dongarra and Julien\n  Langou", "title": "Rectangular Full Packed Format for Cholesky's Algorithm: Factorization,\n  Solution and Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new data format for storing triangular, symmetric, and\nHermitian matrices called RFPF (Rectangular Full Packed Format). The standard\ntwo dimensional arrays of Fortran and C (also known as full format) that are\nused to represent triangular and symmetric matrices waste nearly half of the\nstorage space but provide high performance via the use of Level 3 BLAS.\nStandard packed format arrays fully utilize storage (array space) but provide\nlow performance as there is no Level 3 packed BLAS. We combine the good\nfeatures of packed and full storage using RFPF to obtain high performance via\nusing Level 3 BLAS as RFPF is a standard full format representation. Also, RFPF\nrequires exactly the same minimal storage as packed format. Each LAPACK full\nand/or packed triangular, symmetric, and Hermitian routine becomes a single new\nRFPF routine based on eight possible data layouts of RFPF. This new RFPF\nroutine usually consists of two calls to the corresponding LAPACK full format\nroutine and two calls to Level 3 BLAS routines. This means {\\it no} new\nsoftware is required. As examples, we present LAPACK routines for Cholesky\nfactorization, Cholesky solution and Cholesky inverse computation in RFPF to\nillustrate this new work and to describe its performance on several commonly\nused computer platforms. Performance of LAPACK full routines using RFPF versus\nLAPACK full routines using standard format for both serial and SMP parallel\nprocessing is about the same while using half the storage. Performance gains\nare roughly one to a factor of 43 for serial and one to a factor of 97 for SMP\nparallel times faster using vendor LAPACK full routines with RFPF than with\nusing vendor and/or reference packed routines.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2009 01:08:27 GMT"}], "update_date": "2009-01-14", "authors_parsed": [["Gustavson", "Fred G.", ""], ["Wasniewski", "Jerzy", ""], ["Dongarra", "Jack J.", ""], ["Langou", "Julien", ""]]}, {"id": "0901.1761", "submitter": "Beat Gfeller", "authors": "Beat Gfeller, Peter Sanders", "title": "Towards Optimal Range Medians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: given an unsorted array of $n$ elements,\nand a sequence of intervals in the array, compute the median in each of the\nsubarrays defined by the intervals. We describe a simple algorithm which uses\nO(n) space and needs $O(n\\log k + k\\log n)$ time to answer the first $k$\nqueries. This improves previous algorithms by a logarithmic factor and matches\na lower bound for $k=O(n)$.\n  Since the algorithm decomposes the range of element values rather than the\narray, it has natural generalizations to higher dimensional problems -- it\nreduces a range median query to a logarithmic number of range counting queries.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2009 14:46:50 GMT"}], "update_date": "2009-01-14", "authors_parsed": [["Gfeller", "Beat", ""], ["Sanders", "Peter", ""]]}, {"id": "0901.1849", "submitter": "David Doty", "authors": "David Doty", "title": "Randomized Self-Assembly for Exact Shapes", "comments": "Conference version accepted to FOCS 2009. Present version accepted to\n  SIAM Journal on Computing, which adds new sections on arbitrary scaled\n  shapes, smooth trade-off between specifying bits of n through concentrations\n  versus hardcoded tile types, and construction that uses concentrations\n  arbitrarily close to uniform to fix potential thermodynamic problems with\n  model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working in Winfree's abstract tile assembly model, we show that a\nconstant-size tile assembly system can be programmed through relative tile\nconcentrations to build an n x n square with high probability, for any\nsufficiently large n. This answers an open question of Kao and Schweller\n(Randomized Self-Assembly for Approximate Shapes, ICALP 2008), who showed how\nto build an approximately n x n square using tile concentration programming,\nand asked whether the approximation could be made exact with high probability.\nWe show how this technique can be modified to answer another question of Kao\nand Schweller, by showing that a constant-size tile assembly system can be\nprogrammed through tile concentrations to assemble arbitrary finite *scaled\nshapes*, which are shapes modified by replacing each point with a c x c block\nof points, for some integer c. Furthermore, we exhibit a smooth tradeoff\nbetween specifying bits of n via tile concentrations versus specifying them via\nhard-coded tile types, which allows tile concentration programming to be\nemployed for specifying a fraction of the bits of \"input\" to a tile assembly\nsystem, under the constraint that concentrations can only be specified to a\nlimited precision. Finally, to account for some unrealistic aspects of the tile\nconcentration programming model, we show how to modify the construction to use\nonly concentrations that are arbitrarily close to uniform.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2009 20:55:01 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2009 20:06:49 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2009 05:57:06 GMT"}, {"version": "v4", "created": "Mon, 5 Oct 2009 18:10:14 GMT"}, {"version": "v5", "created": "Mon, 5 Oct 2009 20:16:03 GMT"}, {"version": "v6", "created": "Fri, 16 Jul 2010 18:24:31 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Doty", "David", ""]]}, {"id": "0901.1886", "submitter": "Frederic Didier", "authors": "Frederic Didier", "title": "Efficient erasure decoding of Reed-Solomon codes", "comments": "4 pages, submitted to ISIT 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical algorithm to decode erasures of Reed-Solomon codes\nover the q elements binary field in O(q \\log_2^2 q) time where the constant\nimplied by the O-notation is very small. Asymptotically fast algorithms based\non fast polynomial arithmetic were already known, but even if their complexity\nis similar, they are mostly impractical. By comparison our algorithm uses only\na few Walsh transforms and has been easily implemented.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2009 17:05:50 GMT"}], "update_date": "2009-01-15", "authors_parsed": [["Didier", "Frederic", ""]]}, {"id": "0901.1908", "submitter": "Pat Morin", "authors": "Sebastien Collette, Vida Dujmovic, John Iacono, Stefan Langerman, and\n  Pat Morin", "title": "Entropy, Triangulation, and Point Location in Planar Subdivisions", "comments": "19 pages, 4 figures, lots of formulas", "journal-ref": "ACM Transactions on Algorithms (TALG), Volume 8 Issue 3, July 2012\n  Article No. 29", "doi": "10.1145/2229163.2229173", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data structure is presented for point location in connected planar\nsubdivisions when the distribution of queries is known in advance. The data\nstructure has an expected query time that is within a constant factor of\noptimal. More specifically, an algorithm is presented that preprocesses a\nconnected planar subdivision G of size n and a query distribution D to produce\na point location data structure for G. The expected number of point-line\ncomparisons performed by this data structure, when the queries are distributed\naccording to D, is H + O(H^{2/3}+1) where H=H(G,D) is a lower bound on the\nexpected number of point-line comparisons performed by any linear decision tree\nfor point location in G under the query distribution D. The preprocessing\nalgorithm runs in O(n log n) time and produces a data structure of size O(n).\nThese results are obtained by creating a Steiner triangulation of G that has\nnear-minimum entropy.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2009 23:39:46 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Collette", "Sebastien", ""], ["Dujmovic", "Vida", ""], ["Iacono", "John", ""], ["Langerman", "Stefan", ""], ["Morin", "Pat", ""]]}, {"id": "0901.2151", "submitter": "Bogdan Danila", "authors": "Yudong Sun, Bogdan Danila, Kresimir Josic, and Kevin E. Bassler", "title": "Improved community structure detection using a modified fine tuning\n  strategy", "comments": "6 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.1209/0295-5075/86/28004", "report-no": null, "categories": "cs.CY cond-mat.stat-mech cs.DS physics.comp-ph physics.soc-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The community structure of a complex network can be determined by finding the\npartitioning of its nodes that maximizes modularity. Many of the proposed\nalgorithms for doing this work by recursively bisecting the network. We show\nthat this unduely constrains their results, leading to a bias in the size of\nthe communities they find and limiting their effectivness. To solve this\nproblem, we propose adding a step to the existing algorithms that does not\nincrease the order of their computational complexity. We show that, if this\nstep is combined with a commonly used method, the identified constraint and\nresulting bias are removed, and its ability to find the optimal partitioning is\nimproved. The effectiveness of this combined algorithm is also demonstrated by\nusing it on real-world example networks. For a number of these examples, it\nachieves the best results of any known algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2009 00:40:26 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Sun", "Yudong", ""], ["Danila", "Bogdan", ""], ["Josic", "Kresimir", ""], ["Bassler", "Kevin E.", ""]]}, {"id": "0901.2645", "submitter": "Vincent Limouzy", "authors": "Michel Habib (LIAFA), Vincent Limouzy", "title": "On some simplicial elimination schemes for chordal graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here some results on particular elimination schemes for chordal\ngraphs, namely we show that for any chordal graph we can construct in linear\ntime a simplicial elimination scheme starting with a pending maximal clique\nattached via a minimal separator maximal (resp. minimal) under inclusion among\nall minimal separators.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2009 15:23:29 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2009 16:07:37 GMT"}], "update_date": "2009-02-17", "authors_parsed": [["Habib", "Michel", "", "LIAFA"], ["Limouzy", "Vincent", ""]]}, {"id": "0901.2747", "submitter": "Fahad Saeed", "authors": "Fahad Saeed and Ashfaq Khokhar", "title": "An Overview of Multiple Sequence Alignment Systems", "comments": "24 pages, 15 figures, Technical Report Parallel Algorithms &\n  Multimedia System Laboratory", "journal-ref": null, "doi": null, "report-no": "PAMS-05-2007", "categories": "cs.DS q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An overview of current multiple alignment systems to date are described.The\nuseful algorithms, the procedures adopted and their limitations are\npresented.We also present the quality of the alignments obtained and in which\ncases(kind of alignments, kind of sequences etc) the particular systems are\nuseful.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2009 23:31:09 GMT"}], "update_date": "2009-01-20", "authors_parsed": [["Saeed", "Fahad", ""], ["Khokhar", "Ashfaq", ""]]}, {"id": "0901.2751", "submitter": "Fahad Saeed", "authors": "Fahad Saeed", "title": "Pyro-Align: Sample-Align based Multiple Alignment system for\n  Pyrosequencing Reads of Large Number", "comments": "6 pages, 1 figure, Technical Report, Department of Biosystems Science\n  and Engineering, ETH Zurich Switzerland", "journal-ref": null, "doi": null, "report-no": "DBSSE-08-2008", "categories": "cs.DS cs.DC q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pyro-Align is a multiple alignment program specifically designed for\npyrosequencing reads of huge number. Multiple sequence alignment is shown to be\nNP-hard and heuristics are designed for approximate solutions. Multiple\nsequence alignment of pyrosequenceing reads is complex mainly because of 2\nfactors. One being the huge number of reads, making the use of traditional\nheuristics,that scale very poorly for large number, unsuitable. The second\nreason is that the alignment cannot be performed arbitrarily, because the\nposition of the reads with respect to the original genome is important and has\nto be taken into account.In this report we present a short description of the\nmultiple alignment system for pyrosequencing reads.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2009 00:26:18 GMT"}], "update_date": "2009-01-20", "authors_parsed": [["Saeed", "Fahad", ""]]}, {"id": "0901.2847", "submitter": "Dominique Rossin", "authors": "Mathilde Bouvel (LIAFA), Cedric Chauve, Marni Mishna, Dominique Rossin\n  (LIAFA)", "title": "Average-case analysis of perfect sorting by reversals", "comments": null, "journal-ref": "CPM'09, Lille : France (2009)", "doi": null, "report-no": null, "categories": "math.CO cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sequence of reversals that takes a signed permutation to the identity is\nperfect if at no step a common interval is broken. Determining a parsimonious\nperfect sequence of reversals that sorts a signed permutation is NP-hard. Here\nwe show that, despite this worst-case analysis, with probability one, sorting\ncan be done in polynomial time. Further, we find asymptotic expressions for the\naverage length and number of reversals in commuting permutations, an\ninteresting sub-class of signed permutations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2009 13:11:17 GMT"}], "update_date": "2009-05-18", "authors_parsed": [["Bouvel", "Mathilde", "", "LIAFA"], ["Chauve", "Cedric", "", "LIAFA"], ["Mishna", "Marni", "", "LIAFA"], ["Rossin", "Dominique", "", "LIAFA"]]}, {"id": "0901.2897", "submitter": "Pawel Gawrychowski", "authors": "Pawel Gawrychowski, Artur Jez, Lukasz Jez", "title": "Online validation of the pi and pi' failure functions", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let pi_w denote the failure function of the Morris-Pratt algorithm for a word\nw. In this paper we study the following problem: given an integer array\nA[1..n], is there a word w over arbitrary alphabet such that A[i]=pi_w[i] for\nall i? Moreover, what is the minimum required cardinality of the alphabet? We\ngive a real time linear algorithm for this problem in the unit-cost RAM model\nwith \\Theta(log n) bits word size. Our algorithm returns a word w over minimal\nalphabet such that pi_w = A as well and uses just o(n) words of memory. Then we\nconsider function pi' instead of pi and give an online O(n log n) algorithm for\nthis case. This is the first polynomial algorithm for online version of this\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2009 20:12:21 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2009 20:57:00 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Jez", "Artur", ""], ["Jez", "Lukasz", ""]]}, {"id": "0901.2900", "submitter": "Ankit Sharma", "authors": "Manoj Gupta (1), Ankit Sharma (1) ((1) Indian Institute of Technology\n  Kanpur)", "title": "An O(log(n)) Fully Dynamic Algorithm for Maximum matching in a tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have developed a fully-dynamic algorithm for maintaining\ncardinality of maximum-matching in a tree using the construction of top-trees.\nThe time complexities are as follows:\n  1. Initialization Time: $O(n(log(n)))$ to build the Top-tree. 2. Update Time:\n$O(log(n))$ 3. Query Time: O(1) to query the cardinality of maximum-matching\nand $O(log(n))$ to find if a particular edge is matched.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2009 17:30:28 GMT"}], "update_date": "2009-01-20", "authors_parsed": [["Gupta", "Manoj", ""], ["Sharma", "Ankit", ""]]}, {"id": "0901.3299", "submitter": "Matthias Mnich", "authors": "Leo van Iersel, Matthias Mnich", "title": "Computing Rooted and Unrooted Maximum Consistent Supertrees", "comments": "This paper has been withdrawn by the authors due to an error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A chief problem in phylogenetics and database theory is the computation of a\nmaximum consistent tree from a set of rooted or unrooted trees. A standard\ninput are triplets, rooted binary trees on three leaves, or quartets, unrooted\nbinary trees on four leaves. We give exact algorithms constructing rooted and\nunrooted maximum consistent supertrees in time O(2^n n^5 m^2 log(m)) for a set\nof m triplets (quartets), each one distinctly leaf-labeled by some subset of n\nlabels. The algorithms extend to weighted triplets (quartets). We further\npresent fast exact algorithms for constructing rooted and unrooted maximum\nconsistent trees in polynomial space. Finally, for a set T of m rooted or\nunrooted trees with maximum degree D and distinctly leaf-labeled by some subset\nof a set L of n labels, we compute, in O(2^{mD} n^m m^5 n^6 log(m)) time, a\ntree distinctly leaf-labeled by a maximum-size subset X of L that all trees in\nT, when restricted to X, are consistent with.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2009 16:04:46 GMT"}, {"version": "v2", "created": "Fri, 28 May 2010 06:18:27 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["van Iersel", "Leo", ""], ["Mnich", "Matthias", ""]]}, {"id": "0901.3348", "submitter": "Stephen Vavasis", "authors": "Brendan Ames, Stephen Vavasis", "title": "Nuclear norm minimization for the planted clique and biclique problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of finding a maximum clique in a graph and finding a\nmaximum-edge biclique in a bipartite graph. Both problems are NP-hard. We write\nboth problems as matrix-rank minimization and then relax them using the nuclear\nnorm. This technique, which may be regarded as a generalization of compressive\nsensing, has recently been shown to be an effective way to solve rank\noptimization problems. In the special cases that the input graph has a planted\nclique or biclique (i.e., a single large clique or biclique plus diversionary\nedges), our algorithm successfully provides an exact solution to the original\ninstance. For each problem, we provide two analyses of when our algorithm\nsucceeds. In the first analysis, the diversionary edges are placed by an\nadversary. In the second, they are placed at random. In the case of random\nedges for the planted clique problem, we obtain the same bound as Alon,\nKrivelevich and Sudakov as well as Feige and Krauthgamer, but we use different\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2009 20:08:21 GMT"}], "update_date": "2009-01-22", "authors_parsed": [["Ames", "Brendan", ""], ["Vavasis", "Stephen", ""]]}, {"id": "0901.3657", "submitter": "Joris van der Hoeven", "authors": "Alin Bostan, Muhammad Chowdhury, Joris van der Hoeven, Eric Schost", "title": "Homotopy methods for multiplication modulo triangular sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the cost of multiplication modulo triangular families of\npolynomials. Following previous work by Li, Moreno Maza and Schost, we propose\nan algorithm that relies on homotopy and fast evaluation-interpolation\ntechniques. We obtain a quasi-linear time complexity for substantial families\nof examples, for which no such result was known before. Applications are given\nto notably addition of algebraic numbers in small characteristic.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2009 11:35:57 GMT"}], "update_date": "2009-01-26", "authors_parsed": [["Bostan", "Alin", ""], ["Chowdhury", "Muhammad", ""], ["van der Hoeven", "Joris", ""], ["Schost", "Eric", ""]]}, {"id": "0901.3699", "submitter": "Alan Frieze", "authors": "Alan Frieze, Pall Melsted", "title": "Randomly colouring simple hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of constructing a (near) random proper $q$-colouring of\na simple k-uniform hypergraph with n vertices and maximum degree \\Delta.\n(Proper in that no edge is mono-coloured and simple in that two edges have\nmaximum intersection of size one). We give conditions on q,\\Delta so that if\nthese conditions are satisfied, Glauber dynamics will converge in O(n\\log n)\ntime from a random (improper) start. The interesting thing here is that for\nk\\geq 3 we can take q=o(\\D).\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2009 15:33:42 GMT"}], "update_date": "2009-01-26", "authors_parsed": [["Frieze", "Alan", ""], ["Melsted", "Pall", ""]]}, {"id": "0901.3754", "submitter": "Vahab Mirrokni", "authors": "Eyal Even-dar, Yishay Mansour, Vahab Mirrokni, S. Muthukrishnan, Uri\n  Nadav", "title": "Bid Optimization in Broad-Match Ad auctions", "comments": "World Wide Web Conference (WWW09), 10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ad auctions in sponsored search support ``broad match'' that allows an\nadvertiser to target a large number of queries while bidding only on a limited\nnumber. While giving more expressiveness to advertisers, this feature makes it\nchallenging to optimize bids to maximize their returns: choosing to bid on a\nquery as a broad match because it provides high profit results in one bidding\nfor related queries which may yield low or even negative profits.\n  We abstract and study the complexity of the {\\em bid optimization problem}\nwhich is to determine an advertiser's bids on a subset of keywords (possibly\nusing broad match) so that her profit is maximized. In the query language model\nwhen the advertiser is allowed to bid on all queries as broad match, we present\nan linear programming (LP)-based polynomial-time algorithm that gets the\noptimal profit. In the model in which an advertiser can only bid on keywords,\nie., a subset of keywords as an exact or broad match, we show that this problem\nis not approximable within any reasonable approximation factor unless P=NP. To\ndeal with this hardness result, we present a constant-factor approximation when\nthe optimal profit significantly exceeds the cost. This algorithm is based on\nrounding a natural LP formulation of the problem. Finally, we study a budgeted\nvariant of the problem, and show that in the query language model, one can find\ntwo budget constrained ad campaigns in polynomial time that implement the\noptimal bidding strategy. Our results are the first to address bid optimization\nunder the broad match feature which is common in ad auctions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2009 19:09:01 GMT"}], "update_date": "2009-01-26", "authors_parsed": [["Even-dar", "Eyal", ""], ["Mansour", "Yishay", ""], ["Mirrokni", "Vahab", ""], ["Muthukrishnan", "S.", ""], ["Nadav", "Uri", ""]]}, {"id": "0901.4002", "submitter": "Giorgio Lucarelli", "authors": "Giorgio Lucarelli, Ioannis Milis, Vangelis Th. Paschos", "title": "Max Edge Coloring of Trees", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the weighted generalization of the edge coloring problem where the\nweight of each color class (matching) equals to the weight of its heaviest edge\nand the goal is to minimize the sum of the colors' weights. We present a\n3/2-approximation algorithm for trees.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 13:27:34 GMT"}], "update_date": "2009-01-27", "authors_parsed": [["Lucarelli", "Giorgio", ""], ["Milis", "Ioannis", ""], ["Paschos", "Vangelis Th.", ""]]}, {"id": "0901.4201", "submitter": "Stephane Martin", "authors": "Denis Lugiez (LIF), St\\'ephane Martin (LIF)", "title": "Peer to Peer Optimistic Collaborative Editing on XML-like trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative editing consists in editing a common document shared by several\nindependent sites. This may give rise to conficts when two different users\nperform simultaneous uncompatible operations. Centralized systems solve this\nproblem by using locks that prevent some modifications to occur and leave the\nresolution of confict to users. On the contrary, peer to peer (P2P) editing\ndoesn't allow locks and the optimistic approach uses a Integration\nTransformation IT that reconciliates the conficting operations and ensures\nconvergence (all copies are identical on each site). Two properties TP1 and\nTP2, relating the set of allowed operations Op and the transformation IT, have\nbeen shown to ensure the correctness of the process. The choice of the set Op\nis crucial to define an integration operation that satisfies TP1 and TP2. Many\nexisting algorithms don't satisfy these properties and are indeed incorrect\ni.e. convergence is not guaranteed. No algorithm enjoying both properties is\nknown for strings and little work has been done for XML trees in a pure P2P\nframework (that doesn't use time-stamps for instance). We focus on editing\nunranked unordered labeled trees, so-called XML-like trees that are considered\nfor instance in the Harmony pro ject. We show that no transformation satisfying\nTP1 and TP2 can exist for a first set of operations but we show that TP1 and\nTP2 hold for a richer set of operations. We show how to combine our approach\nwith any convergent editing process on strings (not necessarily based on\nintegration transformation) to get a convergent process.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2009 09:09:52 GMT"}], "update_date": "2009-01-28", "authors_parsed": [["Lugiez", "Denis", "", "LIF"], ["Martin", "St\u00e9phane", "", "LIF"]]}, {"id": "0901.4323", "submitter": "Joris van der Hoeven", "authors": "Joris van der Hoeven, Gr\\'egoire Lecerf", "title": "On the bit-complexity of sparse polynomial multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present fast algorithms for the product of two multivariate\npolynomials in sparse representation. The bit complexity of our algorithms are\nstudied in detail for various types of coefficients, and we derive new\ncomplexity results for the power series multiplication in many variables. Our\nalgorithms are implemented and freely available within the Mathemagix software.\nWe show that their theoretical costs are well-reflected in practice.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2009 21:48:35 GMT"}], "update_date": "2009-01-28", "authors_parsed": [["van der Hoeven", "Joris", ""], ["Lecerf", "Gr\u00e9goire", ""]]}, {"id": "0901.4417", "submitter": "Marcel Wild", "authors": "Marcel Wild", "title": "ALLSAT compressed with wildcards: All, or all maximum independent sets", "comments": "The best way to efficiently generate all maximum anticliques of a\n  bipartite graph, a problem left open in v3, is settled in the present version\n  v4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An odd cycle cover is a vertex set whose removal makes a graph bipartite. We\nshow that if a $k$-element odd cycle cover of a graph with w vertices is known\nthen all $N$ maximum anticliques (= independent sets) can be generated in time\n$O(2^k w^3 + N w^2))$. Generating ${\\it all}\\ N'$ anticliques (maximum or not)\nis easier and works for arbitrary graphs in time $O(N'w^2)$. In fact the use of\nwildcards allows to compactly generate the anticliques in clusters.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2009 09:09:36 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2010 16:08:40 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 15:24:08 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 13:15:22 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Wild", "Marcel", ""]]}]