[{"id": "1808.00425", "submitter": "Inbal Livni Navon", "authors": "Irit Dinur and Prahladh Harsha and Tali Kaufman and Inbal Livni Navon\n  and Amnon Ta Shma", "title": "List Decoding with Double Samplers", "comments": null, "journal-ref": "In Proc. 30th ACM-SIAM Symposium on Discrete Algorithms (SODA)\n  (San Diego, 6-9 January), pages 2134-2153, 2019", "doi": "10.1137/1.9781611975482.129", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We strengthen the notion of \"double samplers\", first introduced by Dinur and\nKaufman [Proc. 58th FOCS, 2017], which are samplers with additional\ncombinatorial properties, and whose existence we prove using high dimensional\nexpanders.\n  The ABNNR code construction [IEEE Trans. Inform. Theory, 38(2):509--516,\n1992] achieves large distance by starting with a base code $C$ with moderate\ndistance, and then amplifying the distance using a sampler. We show that if the\nsampler is part of a larger double sampler then the construction has an\nefficient list-decoding algorithm. Our algorithm works even if the ABNNR\nconstruction is not applied to a base code $C$ but to any string. In this case\nthe resulting code is approximate-list-decodable, i.e. the output list contains\nan approximation to the original input.\n  Our list-decoding algorithm works as follows: it uses a local voting scheme\nfrom which it constructs a unique games constraint graph. The constraint graph\nis an expander, so we can solve unique games efficiently. These solutions are\nthe output of the list decoder. This is a novel use of a unique games algorithm\nas a subroutine in a decoding procedure, as opposed to the more common\nsituation in which unique games are used for demonstrating hardness results.\n  Double samplers and high dimensional expanders are akin to pseudorandom\nobjects in their utility, but they greatly exceed random objects in their\ncombinatorial properties. We believe that these objects hold significant\npotential for coding theoretic constructions and view this work as\ndemonstrating the power of double samplers in this context.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:10:42 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 13:15:33 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Dinur", "Irit", ""], ["Harsha", "Prahladh", ""], ["Kaufman", "Tali", ""], ["Navon", "Inbal Livni", ""], ["Shma", "Amnon Ta", ""]]}, {"id": "1808.00481", "submitter": "Yuanhao Wei", "authors": "Yuanhao Wei", "title": "Space Complexity of Implementing Large Shared Registers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove two new space lower bounds for the problem of implementing a large\nshared register using smaller physical shared registers. We focus on the case\nwhere both the implemented and physical registers are single-writer, which\nmeans they can be accessed concurrently by multiple readers but only by a\nsingle writer. To strengthen our lower bounds, we let the physical registers be\natomic and we only require the implemented register to be regular. Furthermore,\nthe lower bounds hold for obstruction-free implementations, which means they\nalso hold for lock-free and wait-free implementations.\n  If $m$ is the number values representable by the large register and $b$ is\nthe number of values representable by each physical register, our first lower\nbound says that any obstruction-free implementation that has an invisible\nreader requires at least $\\lceil \\frac{m-1}{b-1} \\rceil$ physical registers. A\nreader is considered invisible if it never writes to shared registers. This\nlower bound is tight for the invisible reader case. We also prove a $\\lceil\n\\min(\\frac{m-1}{b-1}, r+\\frac{\\log{m}}{\\log{b}}) \\rceil$ space lower bound for\nthe general case, which covers both visible and invisible readers. In this\nbound, $r$ represents the number of readers.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 18:00:50 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Wei", "Yuanhao", ""]]}, {"id": "1808.00621", "submitter": "David Kempe", "authors": "David Kempe, Mark Klein", "title": "A Class of Weighted TSPs with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications to poaching and burglary prevention, we define a\nclass of weighted Traveling Salesman Problems on metric spaces. The goal is to\noutput an infinite (though typically periodic) tour that visits the n points\nrepeatedly, such that no point goes unvisited for \"too long.\" More\nspecifically, we consider two objective functions for each point x. The maximum\nobjective is simply the maximum duration of any absence from x, while the\nquadratic objective is the normalized sum of squares of absence lengths from x.\nFor periodic tours, the quadratic objective captures the expected duration of\nabsence from x at a uniformly random point in time during the tour. The overall\nobjective is then the weighted maximum of the individual points' objectives.\nWhen a point has weight w_x, the absences under an optimal tour should be\nroughly a 1/w_x fraction of the absences from points of weight 1. Thus, the\nobjective naturally encourages visiting high-weight points more frequently, and\nat roughly evenly spaced intervals.\n  We give a polynomial-time combinatorial algorithm whose output is\nsimultaneously an O(log n) approximation under both objectives. We prove that\nup to constant factors, approximation guarantees for the quadratic objective\ndirectly imply the same guarantees for a natural security patrol game defined\nin recent work.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 01:24:23 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Kempe", "David", ""], ["Klein", "Mark", ""]]}, {"id": "1808.00643", "submitter": "James Allen Fill", "authors": "James Allen Fill and Wei-Chun Hung", "title": "On the tails of the limiting QuickSort density", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give upper and lower asymptotic bounds for the left tail and for the right\ntail of the continuous limiting QuickSort density f that are nearly matching in\neach tail. The bounds strengthen results from a paper of Svante Janson (2015)\nconcerning the corresponding distribution function F. Furthermore, we obtain\nsimilar bounds on absolute values of derivatives of f of each order.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 02:47:14 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Fill", "James Allen", ""], ["Hung", "Wei-Chun", ""]]}, {"id": "1808.00674", "submitter": "Junichi Teruyama", "authors": "Kazuo Iwama, Junichi Teruyama, Shuntaro Tsuyama", "title": "Reconstructing Strings from Substrings: Optimal Randomized and\n  Average-Case Algorithms", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem called \"String reconstruction from substrings\" is a mathematical\nmodel of sequencing by hybridization that plays an important role in DNA\nsequencing. In this problem, we are given a blackbox oracle holding an unknown\nstring ${\\mathcal X}$ and are required to obtain (reconstruct) ${\\mathcal X}$\nthrough \"substring queries\" $Q(S)$. $Q(S)$ is given to the oracle with a string\n$S$ and the answer of the oracle is Yes if ${\\mathcal X}$ includes $S$ as a\nsubstring and No otherwise. Our goal is to minimize the number of queries for\nthe reconstruction. In this paper, we deal with only binary strings for\n${\\mathcal X}$ whose length $n$ is given in advance by using a sequence of good\n$S$'s. In 1995, Skiena and Sundaram first studied this problem and obtained an\nalgorithm whose query complexity is $n+O(\\log n)$. Its information theoretic\nlower bound is $n$, and they posed an obvious open question; if we can remove\nthe $O(\\log n)$ additive term. No progress has been made until now. This paper\ngives two partially positive answers to this open question. One is a randomized\nalgorithm whose query complexity is $n+O(1)$ with high probability and the\nother is an average-case algorithm also having a query complexity of $n+O(1)$\non average. The $n$ lower bound is still true for both cases, and hence they\nare optimal up to an additive constant.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 05:53:17 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Iwama", "Kazuo", ""], ["Teruyama", "Junichi", ""], ["Tsuyama", "Shuntaro", ""]]}, {"id": "1808.00691", "submitter": "Gopinath Mishra", "authors": "Anup Bhattacharya, Arijit Bishnu, Arijit Ghosh and Gopinath Mishra", "title": "On Triangle Estimation using Tripartite Independent Set Queries", "comments": "27 pages. A preliminary version has been appeared in ISAAC'2019. This\n  version contains improved bound on query complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number of triangles in a graph is one of the most fundamental\nproblems in sublinear algorithms. In this work, we provide an algorithm that\napproximately counts the number of triangles in a graph using only\npolylogarithmic queries when \\emph{the number of triangles on any edge in the\ngraph is polylogarithmically bounded}. Our query oracle {\\em Tripartite\nIndependent Set} (TIS) takes three disjoint sets of vertices $A$, $B$ and $C$\nas inputs, and answers whether there exists a triangle having one endpoint in\neach of these three sets. Our query model generally belongs to the class of\n\\emph{group queries} (Ron and Tsur, ACM ToCT, 2016; Dell and Lapinskas, STOC\n2018) and in particular is inspired by the {\\em Bipartite Independent Set}\n(BIS) query oracle of Beame {\\em et al.} (ITCS 2018). We extend the algorithmic\nframework of Beame {\\em et al.}, with \\tis replacing \\bis, for approximately\ncounting triangles in graphs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 07:15:14 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 03:50:44 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 18:01:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bhattacharya", "Anup", ""], ["Bishnu", "Arijit", ""], ["Ghosh", "Arijit", ""], ["Mishra", "Gopinath", ""]]}, {"id": "1808.00838", "submitter": "Sidhanth Mohanty", "authors": "Ofer Grossman, Bernhard Haeupler, Sidhanth Mohanty", "title": "Algorithms for Noisy Broadcast under Erasures", "comments": "Appeared in ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The noisy broadcast model was first studied in [Gallager, TranInf'88] where\nan $n$-character input is distributed among $n$ processors, so that each\nprocessor receives one input bit. Computation proceeds in rounds, where in each\nround each processor broadcasts a single character, and each reception is\ncorrupted independently at random with some probability $p$. [Gallager,\nTranInf'88] gave an algorithm for all processors to learn the input in\n$O(\\log\\log n)$ rounds with high probability. Later, a matching lower bound of\n$\\Omega(\\log\\log n)$ was given in [Goyal, Kindler, Saks; SICOMP'08].\n  We study a relaxed version of this model where each reception is erased and\nreplaced with a `?' independently with probability $p$. In this relaxed model,\nwe break past the lower bound of [Goyal, Kindler, Saks; SICOMP'08] and obtain\nan $O(\\log^* n)$-round algorithm for all processors to learn the input with\nhigh probability. We also show an $O(1)$-round algorithm for the same problem\nwhen the alphabet size is $\\Omega(\\mathrm{poly}(n))$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 14:38:40 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Grossman", "Ofer", ""], ["Haeupler", "Bernhard", ""], ["Mohanty", "Sidhanth", ""]]}, {"id": "1808.00963", "submitter": "Timo Bingmann", "authors": "Timo Bingmann", "title": "Scalable String and Suffix Sorting: Algorithms, Techniques, and Tools", "comments": "396 pages, dissertation, Karlsruher Instituts f\\\"ur Technologie\n  (2018). arXiv admin note: text overlap with arXiv:1101.3448 by other authors", "journal-ref": null, "doi": "10.5445/IR/1000085031", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This dissertation focuses on two fundamental sorting problems: string sorting\nand suffix sorting. The first part considers parallel string sorting on\nshared-memory multi-core machines, the second part external memory suffix\nsorting using the induced sorting principle, and the third part distributed\nexternal memory suffix sorting with a new distributed algorithmic big data\nframework named Thrill.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 16:38:49 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Bingmann", "Timo", ""]]}, {"id": "1808.01028", "submitter": "Andr\\'e Vignatti", "authors": "Santiago Viertel and Andr\\'e Lu\\'is Vignatti", "title": "Small World Model based on a Sphere Homeomorphic Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a small world model over the octahedron surface and relate its\ndistances with those of embedded spheres, preserving constant bounded\ndistortions. The model builds networks with both number of vertices and size\n$\\Theta\\left(n^2\\right)$, where $n$ is the size parameter. It generates\nlong-range edges with probability proportional to the inverse square of the\ndistance between the vertices. We show a greedy routing algorithm that finds\npaths in the small world network with $\\mathcal{O}\\left(\\log^2n\\right)$\nexpected size. The probability of creating cycles of size three (C3) with\nlong-range edges in a vertex is $\\mathcal{O}\\left(\\log^{-1}n\\right)$.\nFurthermore, there are $\\Theta\\left(n^2\\right)$ expected number of C3's in the\nentire network.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 21:13:02 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Viertel", "Santiago", ""], ["Vignatti", "Andr\u00e9 Lu\u00eds", ""]]}, {"id": "1808.01071", "submitter": "Shunsuke Inenaga", "authors": "Noriki Fujisato, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai,\n  Masayuki Takeda", "title": "Right-to-left online construction of parameterized position heaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two strings of equal length are said to parameterized match if there is a\nbijection that maps the characters of one string to those of the other string,\nso that two strings become identical. The parameterized pattern matching\nproblem is, given two strings $T$ and $P$, to find the occurrences of\nsubstrings in $T$ that parameterized match $P$. Diptarama et al. [Position\nHeaps for Parameterized Strings, CPM 2017] proposed an indexing data structure\ncalled parameterized position heaps, and gave a left-to-right online\nconstruction algorithm. In this paper, we present a right-to-left online\nconstruction algorithm for parameterized position heaps. For a text string $T$\nof length $n$ over two kinds of alphabets $\\Sigma$ and $\\Pi$ of respective size\n$\\sigma$ and $\\pi$, our construction algorithm runs in $O(n \\log(\\sigma +\n\\pi))$ time with $O(n)$ space. Our right-to-left parameterized position heaps\nsupport pattern matching queries in $O(m \\log (\\sigma + \\pi) + m \\pi +\n\\mathit{pocc}))$ time, where $m$ is the length of a query pattern $P$ and\n$\\mathit{pocc}$ is the number of occurrences to report. Our construction and\npattern matching algorithms are as efficient as Diptarama et al.'s algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 02:24:26 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Fujisato", "Noriki", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1808.01132", "submitter": "Kai Chen", "authors": "Kai Chen, Perry Groot, Jinsong Chen, and Elena Marchiori", "title": "Generalized Spectral Mixture Kernels for Multi-Task Gaussian Processes", "comments": "17 pages, 33 figures. arXiv admin note: text overlap with\n  arXiv:1808.02266", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Task Gaussian processes (MTGPs) have shown a significant progress both\nin expressiveness and interpretation of the relatedness between different\ntasks: from linear combinations of independent single-output Gaussian processes\n(GPs), through the direct modeling of the cross-covariances such as spectral\nmixture kernels with phase shift, to the design of multivariate covariance\nfunctions based on spectral mixture kernels which model delays among tasks in\naddition to phase differences, and which provide a parametric interpretation of\nthe relatedness across tasks. In this paper we further extend expressiveness\nand interpretability of MTGPs models and introduce a new family of kernels\ncapable to model nonlinear correlations between tasks as well as dependencies\nbetween spectral mixtures, including time and phase delay. Specifically, we use\ngeneralized convolution spectral mixture kernels for modeling dependencies at\nspectral mixture level, and coupling coregionalization for discovering task\nlevel correlations. The proposed kernels for MTGP are validated on artificial\ndata and compared with existing MTGPs methods on three real-world experiments.\nResults indicate the benefits of our more expressive representation with\nrespect to performance and interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 09:42:45 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 15:33:19 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 16:53:35 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 08:51:41 GMT"}, {"version": "v5", "created": "Sun, 14 Oct 2018 13:34:31 GMT"}, {"version": "v6", "created": "Mon, 17 Dec 2018 11:24:48 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Chen", "Kai", ""], ["Groot", "Perry", ""], ["Chen", "Jinsong", ""], ["Marchiori", "Elena", ""]]}, {"id": "1808.01278", "submitter": "Kevin Tian", "authors": "Aaron Sidford, Kevin Tian", "title": "Coordinate Methods for Accelerating $\\ell_\\infty$ Regression and Faster\n  Approximate Maximum Flow", "comments": "A preliminary version appeared in FOCS 2018, with an error in the\n  accelerated coordinate descent proof. Originally we claimed $m +\n  \\sqrt{ns}/\\epsilon$ for our approximate maximum flow runtime; this version\n  obtains $m + (n + \\sqrt{ns})/\\epsilon$. The $\\ell_\\infty$ regression results\n  have been substantially improved, with dependence $c$ on column sparsity\n  (formerly $c^{2.5}$)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide faster algorithms for approximately solving $\\ell_{\\infty}$\nregression, a fundamental problem prevalent in both combinatorial and\ncontinuous optimization. In particular, we provide accelerated coordinate\ndescent methods capable of provably exploiting dynamic measures of coordinate\nsmoothness, and apply them to $\\ell_\\infty$ regression over a box to give\nalgorithms which converge in $k$ iterations at a $O(1/k)$ rate. Our algorithms\ncan be viewed as an alternative approach to the recent breakthrough result of\nSherman [She17] which achieves a similar runtime improvement over classic\nalgorithmic approaches, i.e. smoothing and gradient descent, which either\nconverge at a $O(1/\\sqrt{k})$ rate or have running times with a worse\ndependence on problem parameters. Our runtimes match those of [She17] across a\nbroad range of parameters and achieve improvement in certain structured cases.\n  We demonstrate the efficacy of our result by providing faster algorithms for\nthe well-studied maximum flow problem. Directly leveraging our accelerated\n$\\ell_\\infty$ regression algorithms imply a $\\tilde{O}\\left(m +\n\\sqrt{mn}/\\epsilon\\right)$ runtime to compute an $\\epsilon$-approximate maximum\nflow for an undirected graph with $m$ edges and $n$ vertices, generically\nimproving upon the previous best known runtime of\n$\\tilde{O}\\left(m/\\epsilon\\right)$ in [She17] whenever the graph is slightly\ndense. We further design an algorithm adapted to the structure of the\nregression problem induced by maximum flow obtaining a runtime of\n$\\tilde{O}\\left(m + \\max(n, \\sqrt{ns})/\\epsilon\\right)$, where $s$ is the\nsquared $\\ell_2$ norm of the congestion of any optimal flow. Moreover, we show\nhow to leverage this result to achieve improved exact algorithms for maximum\nflow on a variety of unit capacity graphs. We hope that our work serves as an\nimportant step towards achieving even faster maximum flow algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 17:58:31 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 17:49:31 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Sidford", "Aaron", ""], ["Tian", "Kevin", ""]]}, {"id": "1808.01394", "submitter": "Albert Cheu", "authors": "Albert Cheu and Adam Smith and Jonathan Ullman and David Zeber and\n  Maxim Zhilyaev", "title": "Distributed Differential Privacy via Shuffling", "comments": "Updated to correct a typo", "journal-ref": null, "doi": "10.1007/978-3-030-17653-2_13", "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing scalable, robust protocols for computing\nstatistics about sensitive data. Specifically, we look at how best to design\ndifferentially private protocols in a distributed setting, where each user\nholds a private datum. The literature has mostly considered two models: the\n\"central\" model, in which a trusted server collects users' data in the clear,\nwhich allows greater accuracy; and the \"local\" model, in which users\nindividually randomize their data, and need not trust the server, but accuracy\nis limited. Attempts to achieve the accuracy of the central model without a\ntrusted server have so far focused on variants of cryptographic MPC, which\nlimits scalability.\n  In this paper, we initiate the analytic study of a shuffled model for\ndistributed differentially private algorithms, which lies between the local and\ncentral models. This simple-to-implement model, a special case of the ESA\nframework of [Bittau et al., '17], augments the local model with an anonymous\nchannel that randomly permutes a set of user-supplied messages. For sum\nqueries, we show that this model provides the power of the central model while\navoiding the need to trust a central server and the complexity of cryptographic\nsecure function evaluation. More generally, we give evidence that the power of\nthe shuffled model lies strictly between those of the central and local models:\nfor a natural restriction of the model, we show that shuffled protocols for a\nwidely studied selection problem require exponentially higher sample complexity\nthan do central-model protocols.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 00:24:12 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 04:40:39 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 19:52:46 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Cheu", "Albert", ""], ["Smith", "Adam", ""], ["Ullman", "Jonathan", ""], ["Zeber", "David", ""], ["Zhilyaev", "Maxim", ""]]}, {"id": "1808.01539", "submitter": "Shankar Sastry", "authors": "Shankar Prasad Sastry", "title": "A 2D Advancing-Front Delaunay Mesh Refinement Algorithm", "comments": "Submitted to SODA 2019; 32 pages; 22 lemmas/theorems; 13 figures\n  (with more subfigures); Since the submission, the definition of the kth layer\n  of vertices was changed to get tighter bounds. Consequently, Lemma 5.8, 5.9,\n  5.10, and Theorem 5.11 have been updated. As a result, Lemma 5.14 changes a\n  little, and the proof of Lemma 5.18 has been corrected. Typos corrected", "journal-ref": null, "doi": "10.1016/j.comgeo.2021.101772", "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a generalization of Chew's first algorithm for Delaunay mesh\nrefinement. In his algorithm, Chew splits the line segments of the input planar\nstraight line graph (PSLG) into shorter subsegments whose lengths are nearly\nidentical. The constrained Delaunay triangulation of the subsegments is refined\nbased on the length of the radii of the circumcircles of the triangles. This\nalgorithm produces a uniform mesh, whose minimum angle can be at most $\\pi/6$.\nMy algorithm generates both truly Delaunay and constrained Delaunay\nsize-optimal meshes. In my algorithm, I split the line segments of the input\nPSLG such that their lengths are asymptotically proportional to the local\nfeature size (LFS) by solving ordinary differential equations (ODEs) that map\npoints from a closed 1D interval to points on the input line segments in the\nPSLG. I then refine the Delaunay triangulation (truly or constrained) of the\nPSLG by inserting off-center Steiner vertices of \"skinny\" triangles while\nprioritizing such triangles with shortest edges first. As in Chew's algorithm,\nI show that the Steiner vertices do not encroach upon any subsegment of the\nPSLG. The off-center insertion algorithm places Steiner vertices in an\nadvancing front manner such that we obtain a size-optimal Delaunay mesh (truly\nor constrained) if the desired minimum angle is less than $\\pi/6$. In addition,\neven in the presence of a small angle $\\phi < \\pi/2$ in the PSLG, the bound on\nthe minimum angle \"across\" the small angle tends to\n$\\arctan{((\\sin{\\phi})/(2-\\cos(\\phi))}$ as the PSLG is progressively refined.\nAlso, the bound on the maximum angle across any small input angle tends to\n$\\pi/2 + \\phi/2$ as the PSLG is progressively refined.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 22:47:25 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 20:52:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sastry", "Shankar Prasad", ""]]}, {"id": "1808.01774", "submitter": "Milo\\v{s} Chrom\\'y", "authors": "Milo\\v{s} Chrom\\'y (1) and Petr Ku\\v{c}era (1) ((1) Charles\n  University, Faculty of Mathematics and Physics, Department of Theoretical\n  Computer Science and Mathematical Logic)", "title": "Phase Transition in Matched Formulas and a Heuristic for Biclique\n  Satisfiability", "comments": "Conference version submitted to SOFSEM 2018\n  (https://beda.dcs.fmph.uniba.sk/sofsem2019/) 18 pages(17 without refernces),\n  3 figures, 8 tables, an algorithm pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matched formula is a CNF formula whose incidence graph admits a matching\nwhich matches a distinct variable to every clause. We study phase transition in\na context of matched formulas and their generalization of biclique satisfiable\nformulas. We have performed experiments to find a phase transition of property\n\"being matched\" with respect to the ratio $m/n$ where $m$ is the number of\nclauses and $n$ is the number of variables of the input formula $\\varphi$. We\ncompare the results of experiments to a theoretical lower bound which was shown\nby Franco and Gelder (2003). Any matched formula is satisfiable, moreover, it\nremains satisfiable even if we change polarities of any literal occurrences.\nSzeider (2005) generalized matched formulas into two classes having the same\nproperty -- var-satisfiable and biclique satisfiable formulas. A formula is\nbiclique satisfiable if its incidence graph admits covering by pairwise\ndisjoint bounded bicliques. Recognizing if a formula is biclique satisfiable is\nNP-complete. In this paper we describe a heuristic algorithm for recognizing\nwhether a formula is biclique satisfiable and we evaluate it by experiments on\nrandom formulas. We also describe an encoding of the problem of checking\nwhether a formula is biclique satisfiable into SAT and we use it to evaluate\nthe performance of our heuristic\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 08:37:44 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Chrom\u00fd", "Milo\u0161", ""], ["Ku\u010dera", "Petr", ""]]}, {"id": "1808.01786", "submitter": "Laszlo Csirmaz", "authors": "Laszlo Csirmaz", "title": "Inner approximation algorithm for solving linear multiobjective\n  optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benson's outer approximation algorithm and its variants are the most\nfrequently used methods for solving linear multiobjective optimization\nproblems. These algorithms have two intertwined components: one-dimensional\nlinear optimization one one hand, and a combinatorial part closely related to\nvertex numeration on the other. Their separation provides a deeper insight into\nBenson's algorithm, and points toward a dual approach. Two skeletal algorithms\nare defined which focus on the combinatorial part. Using different\nsingle-objective optimization problems - called oracle calls - yield different\nalgorithms, such as a sequential convex hull algorithm, another version of\nBenson's algorithm with the theoretically best possible iteration count, the\ndual algorithm of Ehrgott, L\\\"ohne and Shao, and the new algorithm. The new\nalgorithm has several advantages. First, the corresponding one-dimensional\noptimization problem uses the original constraints without adding any extra\nvariables or constraints. Second, its iteration count meets the theoretically\nbest possible one. As a dual algorithm, it is sequential: in each iteration it\nproduces an extremal solution, thus can be aborted when a satisfactory solution\nis found. The Pareto front can be \"probed\" or \"scanned\" from several directions\nat any moment without adversely affecting the efficiency. Finally, it is well\nsuited to handle highly degenerate problems where there are many linear\ndependencies among the constraints. On problems with ten or more objectives the\nimplementation shows a significant increase in efficiency compared to Bensolve\n- due to the reduced number of iterations and the improved combinatorial\nhandling.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 09:05:24 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 12:53:12 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Csirmaz", "Laszlo", ""]]}, {"id": "1808.01949", "submitter": "Ferdinando Fioretto", "authors": "Ferdinando Fioretto and Pascal Van Hentenryck", "title": "OptStream: Releasing Time Series Privately", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research (JAIR) Vol. 65 (2019)", "doi": "10.1613/jair.1.11583", "report-no": null, "categories": "cs.CR cs.AI cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of machine learning and optimization operate on data\nstreams. While these datasets are fundamental to fuel decision-making\nalgorithms, often they contain sensitive information about individuals and\ntheir usage poses significant privacy risks. Motivated by an application in\nenergy systems, this paper presents OPTSTREAM, a novel algorithm for releasing\ndifferentially private data streams under the w-event model of privacy.\nOPTSTREAM is a 4-step procedure consisting of sampling, perturbation,\nreconstruction, and post-processing modules. First, the sampling module selects\na small set of points to access in each period of interest. Then, the\nperturbation module adds noise to the sampled data points to guarantee privacy.\nNext, the reconstruction module reassembles non-sampled data points from the\nperturbed sample points. Finally, the post-processing module uses convex\noptimization over the private output of the previous modules, as well as the\nprivate answers of additional queries on the data stream, to improve accuracy\nby redistributing the added noise. OPTSTREAM is evaluated on a test case\ninvolving the release of a real data stream from the largest European\ntransmission operator. Experimental results show that OPTSTREAM may not only\nimprove the accuracy of state-of-the-art methods by at least one order of\nmagnitude but also supports accurate load forecasting on the private data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 14:54:42 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 13:24:03 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Fioretto", "Ferdinando", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1808.01984", "submitter": "Arash Nouri", "authors": "Anil Maheshwari, Arash Nouri, J\\\"org-R\\\"udiger Sack", "title": "Time-Dependent Shortest Path Queries Among Growing Discs", "comments": "16 pages, 9 figures, abridged version submitted to CCCG 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of time-dependent collision-free shortest paths has\nreceived a fair amount of attention. Here, we study the problem of computing a\ntime-dependent shortest path among growing discs which has been previously\nstudied for the instance where the departure times are fixed. We address a more\ngeneral setting: For two given points $s$ and $d$, we wish to determine the\nfunction $\\mathcal{A}(t)$ which is the minimum arrival time at $d$ for any\ndeparture time $t$ at $s$. We present a $(1+\\epsilon)$-approximation algorithm\nfor computing $\\mathcal{A}(t)$. As part of preprocessing, we execute $O({1\n\\over \\epsilon} \\log({\\mathcal{V}_{r} \\over \\mathcal{V}_{c}}))$ shortest path\ncomputations for fixed departure times, where $\\mathcal{V}_{r}$ is the maximum\nspeed of the robot and $\\mathcal{V}_{c}$ is the minimum growth rate of the\ndiscs. For any query departure time $t \\geq 0$ from $s$, we can approximate the\nminimum arrival time at the destination in $O(\\log ({1 \\over \\epsilon}) +\n\\log\\log({\\mathcal{V}_{r} \\over \\mathcal{V}_{c}}))$ time, within a factor of\n$1+\\epsilon$ of optimal. Since we treat the shortest path computations as\nblack-box functions, for different settings of growing discs, we can plug-in\ndifferent shortest path algorithms. Thus, the exact time complexity of our\nalgorithm is determined by the running time of the shortest path computations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 16:23:39 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Maheshwari", "Anil", ""], ["Nouri", "Arash", ""], ["Sack", "J\u00f6rg-R\u00fcdiger", ""]]}, {"id": "1808.02066", "submitter": "Stratos Idreos", "authors": "Stratos Idreos, Kostas Zoumpatianos, Brian Hentschel, Michael S.\n  Kester, Demi Guo", "title": "The Internals of the Data Calculator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data structures are critical in any data-driven scenario, but they are\nnotoriously hard to design due to a massive design space and the dependence of\nperformance on workload and hardware which evolve continuously. We present a\ndesign engine, the Data Calculator, which enables interactive and\nsemi-automated design of data structures. It brings two innovations. First, it\noffers a set of fine-grained design primitives that capture the first\nprinciples of data layout design: how data structure nodes lay data out, and\nhow they are positioned relative to each other. This allows for a structured\ndescription of the universe of possible data structure designs that can be\nsynthesized as combinations of those primitives. The second innovation is\ncomputation of performance using learned cost models. These models are trained\non diverse hardware and data profiles and capture the cost properties of\nfundamental data access primitives (e.g., random access). With these models, we\nsynthesize the performance cost of complex operations on arbitrary data\nstructure designs without having to: 1) implement the data structure, 2) run\nthe workload, or even 3) access the target hardware. We demonstrate that the\nData Calculator can assist data structure designers and researchers by\naccurately answering rich what-if design questions on the order of a few\nseconds or minutes, i.e., computing how the performance (response time) of a\ngiven data structure design is impacted by variations in the: 1) design, 2)\nhardware, 3) data, and 4) query workloads. This makes it effortless to test\nnumerous designs and ideas before embarking on lengthy implementation,\ndeployment, and hardware acquisition steps. We also demonstrate that the Data\nCalculator can synthesize entirely new designs, auto-complete partial designs,\nand detect suboptimal design choices.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 18:46:18 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Idreos", "Stratos", ""], ["Zoumpatianos", "Kostas", ""], ["Hentschel", "Brian", ""], ["Kester", "Michael S.", ""], ["Guo", "Demi", ""]]}, {"id": "1808.02162", "submitter": "David Eppstein", "authors": "David Eppstein and Daniel Lokshtanov", "title": "The Parameterized Complexity of Finding Point Sets with Hereditary\n  Properties", "comments": "15 pages, 5 figures. To appear at the 13th International Symposium on\n  Parameterized and Exact Computation (IPEC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider problems where the input is a set of points in the plane and an\ninteger $k$, and the task is to find a subset $S$ of the input points of size\n$k$ such that $S$ satisfies some property. We focus on properties that depend\nonly on the order type of the points and are monotone under point removals. We\nshow that not all such problems are fixed-parameter tractable parameterized by\n$k$, by exhibiting a property defined by three forbidden patterns for which\nfinding a $k$-point subset with the property is $\\mathrm{W}[1]$-complete and\n(assuming the exponential time hypothesis) cannot be solved in time\n$n^{o(k/\\log k)}$. However, we show that problems of this type are\nfixed-parameter tractable for all properties that include all collinear point\nsets, properties that exclude at least one convex polygon, and properties\ndefined by a single forbidden pattern.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 00:13:24 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Eppstein", "David", ""], ["Lokshtanov", "Daniel", ""]]}, {"id": "1808.02174", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Cody Freitag, Himanshu Tyagi", "title": "Test without Trust: Optimal Locally Private Distribution Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DM cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distribution testing when the samples can only be\naccessed using a locally differentially private mechanism and focus on two\nrepresentative testing questions of identity (goodness-of-fit) and independence\ntesting for discrete distributions. We are concerned with two settings: First,\nwhen we insist on using an already deployed, general-purpose locally\ndifferentially private mechanism such as the popular RAPPOR or the recently\nintroduced Hadamard Response for collecting data, and must build our tests\nbased on the data collected via this mechanism; and second, when no such\nrestriction is imposed, and we can design a bespoke mechanism specifically for\ntesting. For the latter purpose, we introduce the Randomized Aggregated Private\nTesting Optimal Response (RAPTOR) mechanism which is remarkably simple and\nrequires only one bit of communication per sample.\n  We propose tests based on these mechanisms and analyze their sample\ncomplexities. Each proposed test can be implemented efficiently. In each case\n(barring one), we complement our performance bounds for algorithms with\ninformation-theoretic lower bounds and establish sample optimality of our\nproposed algorithm. A peculiar feature that emerges is that our sample-optimal\nalgorithm based on RAPTOR uses public-coins, and any test based on RAPPOR or\nHadamard Response, which are both private-coin mechanisms, requires\nsignificantly more samples.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 01:18:57 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Freitag", "Cody", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1808.02227", "submitter": "Rad Niazadeh", "authors": "Moses Charikar, Vaggos Chatziafratis, Rad Niazadeh", "title": "Hierarchical Clustering better than Average-Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Clustering (HC) is a widely studied problem in exploratory data\nanalysis, usually tackled by simple agglomerative procedures like\naverage-linkage, single-linkage or complete-linkage. In this paper we focus on\ntwo objectives, introduced recently to give insight into the performance of\naverage-linkage clustering: a similarity based HC objective proposed by\n[Moseley and Wang, 2017] and a dissimilarity based HC objective proposed by\n[Cohen-Addad et al., 2018]. In both cases, we present tight counterexamples\nshowing that average-linkage cannot obtain better than 1/3 and 2/3\napproximations respectively (in the worst-case), settling an open question\nraised in [Moseley and Wang, 2017]. This matches the approximation ratio of a\nrandom solution, raising a natural question: can we beat average-linkage for\nthese objectives? We answer this in the affirmative, giving two new algorithms\nbased on semidefinite programming with provably better guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 06:47:01 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Charikar", "Moses", ""], ["Chatziafratis", "Vaggos", ""], ["Niazadeh", "Rad", ""]]}, {"id": "1808.02346", "submitter": "Fernando M\\'ario De Oliveira Filho", "authors": "Fernando M\\'ario de Oliveira Filho, Frank Vallentin", "title": "On the integrality gap of the maximum-cut semidefinite programming\n  relaxation in fixed dimension", "comments": "17 pages, 2 figures", "journal-ref": "Discrete Analysis, 2020:10", "doi": "10.19086/da.14164", "report-no": null, "categories": "math.OC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a factor-revealing convex optimization problem for the\nintegrality gap of the maximum-cut semidefinite programming relaxation: for\neach $n \\geq 2$ we present a convex optimization problem whose optimal value is\nthe largest possible ratio between the value of an optimal rank-$n$ solution to\nthe relaxation and the value of an optimal cut. This problem is then used to\ncompute lower bounds for the integrality gap.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 13:16:40 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 12:56:43 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 13:35:23 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Filho", "Fernando M\u00e1rio de Oliveira", ""], ["Vallentin", "Frank", ""]]}, {"id": "1808.02348", "submitter": "Vladimir Braverman", "authors": "Vladimir Braverman", "title": "Approximations of Schatten Norms via Taylor Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider symmetric, positive semidefinite (SPSD) matrix $A$\nand present two algorithms for computing the $p$-Schatten norm $\\|A\\|_p$. The\nfirst algorithm works for any SPSD matrix $A$. The second algorithm works for\nnon-singular SPSD matrices and runs in time that depends on $\\kappa =\n{\\lambda_1(A)\\over \\lambda_n(A)}$, where $\\lambda_i(A)$ is the $i$-th\neigenvalue of $A$. Our methods are simple and easy to implement and can be\nextended to general matrices. Our algorithms improve, for a range of\nparameters, recent results of Musco, Netrapalli, Sidford, Ubaru and Woodruff\n(ITCS 2018) and match the running time of the methods by Han, Malioutov, Avron,\nand Shin (SISC 2017) while avoiding computations of coefficients of Chebyshev\npolynomials.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 13:17:42 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Braverman", "Vladimir", ""]]}, {"id": "1808.02359", "submitter": "Till Fluschnik", "authors": "Max-Jonathan Luckow and Till Fluschnik", "title": "On the Computational Complexity of Length- and Neighborhood-Constrained\n  Path Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding paths in graphs is a fundamental graph-theoretic task. In this work,\nwe we are concerned with finding a path with some constraints on its length and\nthe number of vertices neighboring the path, that is, being outside of and\nincident with the path. Herein, we consider short and long paths on the one\nside, and small and large neighborhoods on the other side---yielding four\ndecision problems. We show that all four problems are NP-complete, even in\nplanar graphs with small maximum degree. Moreover, we study all four variants\nwhen parameterized by a bound $k$ on the length of the path, by a bound $\\ell$\non the size of neighborhood, and by $k + \\ell$.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 13:32:31 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 12:07:11 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Luckow", "Max-Jonathan", ""], ["Fluschnik", "Till", ""]]}, {"id": "1808.02517", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas, Maryam Fazel, Lorenzo Orecchia", "title": "Fair Packing and Covering on a Relative Scale", "comments": "To appear in SIAM Journal on Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fair resource allocation is a fundamental optimization problem with\napplications in operations research, networking, and economic and game theory.\nResearch in these areas has led to the general acceptance of a class of\n$\\alpha$-fair utility functions parameterized by $\\alpha \\in [0, \\infty]$. We\nconsider $\\alpha$-fair packing -- the problem of maximizing $\\alpha$-fair\nutilities under positive linear constraints -- and provide a simple first-order\nmethod for solving it with relative-error guarantees. The method has a\nsignificantly lower convergence time than the state of the art, and to analyze\nit, we leverage the Approximate Duality Gap Technique, which provides an\nintuitive interpretation of the convergence argument. Finally, we introduce a\nnatural counterpart of $\\alpha$-fairness for minimization problems and motivate\nits usage in the context of fair task allocation. This generalization yields\n$\\alpha$-fair covering problems, for which we provide the first\nnear-linear-time solvers with relative-error guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 18:55:04 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 18:02:14 GMT"}, {"version": "v3", "created": "Sun, 15 Nov 2020 20:18:18 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Fazel", "Maryam", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1808.02545", "submitter": "Sai Krishna Kanth Hari", "authors": "Sai Krishna Kanth Hari, Sivakumar Rathinam, Swaroop Darbha,\n  Krishnamoorthy Kalyanam, Satyanarayana Gupta Manyam, David Casbeer", "title": "Persistent Monitoring of Dynamically Changing Environments Using an\n  Unmanned Vehicle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of planning a closed walk $\\mathcal W$ for a UAV to\npersistently monitor a finite number of stationary targets with equal\npriorities and dynamically changing properties. A UAV must physically visit the\ntargets in order to monitor them and collect information therein. The frequency\nof monitoring any given target is specified by a target revisit time, $i.e.$,\nthe maximum allowable time between any two successive visits to the target. The\nproblem considered in this paper is the following: Given $n$ targets and $k\n\\geq n$ allowed visits to them, find an optimal closed walk $\\mathcal W^*(k)$\nso that every target is visited at least once and the maximum revisit time over\nall the targets, $\\mathcal R(\\mathcal W(k))$, is minimized. We prove the\nfollowing: If $k \\geq n^2-n$, $\\mathcal R(\\mathcal W^*(k))$ (or simply,\n$\\mathcal R^*(k)$) takes only two values: $\\mathcal R^*(n)$ when $k$ is an\nintegral multiple of $n$, and $\\mathcal R^*(n+1)$ otherwise. This result\nsuggests significant computational savings - one only needs to determine\n$\\mathcal W^*(n)$ and $\\mathcal W^*(n+1)$ to construct an optimal solution\n$\\mathcal W^*(k)$. We provide MILP formulations for computing $\\mathcal W^*(n)$\nand $\\mathcal W^*(n+1)$. Furthermore, for {\\it any} given $k$, we prove that\n$\\mathcal R^*(k) \\geq \\mathcal R^*(k+n)$.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 20:30:02 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 19:34:06 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Hari", "Sai Krishna Kanth", ""], ["Rathinam", "Sivakumar", ""], ["Darbha", "Swaroop", ""], ["Kalyanam", "Krishnamoorthy", ""], ["Manyam", "Satyanarayana Gupta", ""], ["Casbeer", "David", ""]]}, {"id": "1808.02546", "submitter": "Hossein Esfandiari", "authors": "Hossein Esfandiari, Silvio Lattanzi, Vahab Mirrokni", "title": "Parallel and Streaming Algorithms for K-Core Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-core decomposition is a fundamental primitive in many machine\nlearning and data mining applications. We present the first distributed and the\nfirst streaming algorithms to compute and maintain an approximate $k$-core\ndecomposition with provable guarantees. Our algorithms achieve rigorous bounds\non space complexity while bounding the number of passes or number of rounds of\ncomputation. We do so by presenting a new powerful sketching technique for\n$k$-core decomposition, and then by showing it can be computed efficiently in\nboth streaming and MapReduce models. Finally, we confirm the effectiveness of\nour sketching technique empirically on a number of publicly available graphs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 20:31:39 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 22:24:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Esfandiari", "Hossein", ""], ["Lattanzi", "Silvio", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1808.02568", "submitter": "Jacob Holm", "authors": "Jacob Holm and Eva Rotenberg", "title": "Good $r$-divisions Imply Optimal Amortised Decremental Biconnectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data structure that, given a graph $G$ of $n$ vertices and $m$\nedges, and a suitable pair of nested $r$-divisions of $G$, preprocesses $G$ in\n$O(m+n)$ time and handles any series of edge-deletions in $O(m)$ total time\nwhile answering queries to pairwise biconnectivity in worst-case $O(1)$ time.\nIn case the vertices are not biconnected, the data structure can return a\ncutvertex separating them in worst-case $O(1)$ time.\n  As an immediate consequence, this gives optimal amortized decremental\nbiconnectivity, 2-edge connectivity, and connectivity for large classes of\ngraphs, including planar graphs and other minor free graphs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 22:22:30 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 10:12:43 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Holm", "Jacob", ""], ["Rotenberg", "Eva", ""]]}, {"id": "1808.02591", "submitter": "Dexin Li", "authors": "Dexin Li", "title": "A practical Single Source Shortest Path algorithm for random directed\n  graphs with arbitrary weight in expecting linear time", "comments": "Proved theoretically", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I present an algorithm called Raffica algorithm for\nSingle-Source Shortest Path(SSSP). On random graph, this algorithm has linear\ntime complexity(in expect). More precisely, the random graph uses configuration\nmodel, and the weights are distributed mostly positively. It is also linear for\nrandom grid graphs. Despite I made an assumption on the weights of the random\ngraph, this algorithm is able to solve SSSP with arbitrary weights; when a\nnegative cycle exists, this algorithm can find it out once traversed. The\nalgorithm has a lot of appliances.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 00:56:55 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 15:33:02 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 23:40:39 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 15:58:32 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Li", "Dexin", ""]]}, {"id": "1808.02692", "submitter": "Yli\\`es Falcone", "authors": "Antoine El-Hokayem and Yli\\`es Falcone", "title": "On the Monitoring of Decentralized Specifications Semantics, Properties,\n  Analysis, and Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DS cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define two complementary approaches to monitor decentralized systems. The\nfirst relies on those with a centralized specification, i.e, when the\nspecification is written for the behavior of the entire system. To do so, our\napproach introduces a data-structure that i) keeps track of the execution of an\nautomaton, ii) has predictable parameters and size, and iii) guarantees strong\neventual consistency. The second approach defines decentralized specifications\nwherein multiple specifications are provided for separate parts of the system.\nWe study two properties of decentralized specifications pertaining to\nmonitorability and compatibility between specification and architecture. We\nalso present a general algorithm for monitoring decentralized specifications.\nWe map three existing algorithms to our approaches and provide a framework for\nanalyzing their behavior. Furthermore, we introduce THEMIS, a framework for\ndesigning such decentralized algorithms and simulating their behavior. We show\nthe usage of THEMIS to compare multiple algorithms and verify the trends\npredicted by the analysis by studying two scenarios: a synthetic benchmark and\na real example.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 09:40:35 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["El-Hokayem", "Antoine", ""], ["Falcone", "Yli\u00e8s", ""]]}, {"id": "1808.02800", "submitter": "Arnold Filtser", "authors": "Arnold Filtser", "title": "Steiner Point Removal with distortion $O(\\log k)$, using the\n  Noisy-Voronoi algorithm", "comments": "A preliminary version was published at SODA'18. The name was slightly\n  modified to emphasize the fact that we analyze a different algorithm in this\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Steiner Point Removal (SPR) problem, we are given a weighted graph\n$G=(V,E)$ and a set of terminals $K\\subset V$ of size $k$. The objective is to\nfind a minor $M$ of $G$ with only the terminals as its vertex set, such that\ndistances between the terminals will be preserved up to a small multiplicative\ndistortion. Kamma, Krauthgamer and Nguyen [SICOMP2015] devised a ball-growing\nalgorithm with exponential distributions to show that the distortion is at most\n$O(\\log^5 k)$. Cheung [SODA2018] improved the analysis of the same algorithm,\nbounding the distortion by $O(\\log^2 k)$. We devise a novel and simpler\nalgorithm (called the Noisy Voronoi algorithm) which incurs distortion $O(\\log\nk)$. This algorithm can be implemented in almost linear time ($O(|E|\\log\n|V|)$).\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 14:37:22 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Filtser", "Arnold", ""]]}, {"id": "1808.02815", "submitter": "Sariel Har-Peled", "authors": "Linda Cai, Sariel Har-Peled, Simiao Ye", "title": "Separators for Planar Graphs that are Almost Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that a connected planar graph with $n$ vertices and $n+\\mu$ edges\nhas a vertex separator of size $O( \\sqrt{\\mu} + 1)$, and this separator can be\ncomputed in linear time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 15:16:48 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Cai", "Linda", ""], ["Har-Peled", "Sariel", ""], ["Ye", "Simiao", ""]]}, {"id": "1808.02859", "submitter": "Xianghui Zhong", "authors": "Stefan Hougardy and Xianghui Zhong", "title": "Hard to Solve Instances of the Euclidean Traveling Salesman Problem", "comments": null, "journal-ref": null, "doi": "10.1007/s12532-020-00184-5", "report-no": null, "categories": "cs.DM cs.CC cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well known $4/3$ conjecture states that the integrality ratio of the\nsubtour LP is at most $4/3$ for metric Traveling Salesman instances. We present\na family of Euclidean Traveling Salesman instances for which we prove that the\nintegrality ratio of the subtour LP converges to $4/3$. These instances (using\nthe rounded Euclidean norm) turn out to be hard to solve exactly with Concorde,\nthe fastest existing exact TSP solver. For a 200 vertex instance from our\nfamily of Euclidean Traveling Salesman instances Concorde needs several days of\nCPU time. This is more than 1,000,000 times the runtime for a TSPLIB instance\nof similar size. Thus our new family of Euclidean Traveling Salesman instances\nmay serve as new benchmark instances for TSP algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 16:58:56 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 16:15:05 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 17:35:23 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Hougardy", "Stefan", ""], ["Zhong", "Xianghui", ""]]}, {"id": "1808.02985", "submitter": "Doo-Hyun Cho", "authors": "Doo-Hyun Cho, Dae-Sung Jang, and Han-Lim Choi", "title": "Sampling-Based Tour Generation of Arbitrarily Oriented Dubins Sensor\n  Platforms", "comments": "33 pages, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a formulation and develops a novel procedure for a fleet\nof unmanned aerial vehicles (UAVs) from the perspective of remotely executable\ntasks. In a complex mission environment, the characteristics of vehicles can be\ndifferent in terms of sensing capability, range, direction, or the motion\nconstraints. The purpose of this paper is to find a set of paths that minimizes\nthe sum of costs while every task region is visited exactly once under certain\nreasonable assumptions. The heterogeneous multi-UAV path planning problem is\nformulated as a generalized, heterogeneous, multiple depot traveling salesmen\nproblem (GHMDATSP), which is a variant of the traveling salesman problem. The\nproposed transformation procedure changes an instance of the GHMDATSP into a\nformat of an Asymmetric, Traveling Salesman Problem (ATSP) to obtain tours for\nwhich the total cost of a fleet of vehicles is minimized. The instance of the\nATSP is solved using the Lin-Kernighan-Helsgaun heuristic, and the result is\ninversely transformed to the GHMDATSP-formatted instance to obtain a set of\ntours. An additional local optimization based path refinement process helps\nobtain a high-quality solution. Numerical experiments investigate and confirm\nfor the validity and applicability of the proposed procedure.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 01:08:09 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Cho", "Doo-Hyun", ""], ["Jang", "Dae-Sung", ""], ["Choi", "Han-Lim", ""]]}, {"id": "1808.03085", "submitter": "Francesco Cellinese", "authors": "Francesco Cellinese, Gianlorenzo D'Angelo, Gianpiero Monaco, Yllka\n  Velaj", "title": "Generalized budgeted submodular set function maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a generalization of the well-known budgeted maximum\ncoverage problem. We are given a ground set of elements and a set of bins. The\ngoal is to find a subset of elements along with an associated set of bins, such\nthat the overall cost is at most a given budget, and the profit is maximized.\nEach bin has its own cost and the cost of each element depends on its\nassociated bin. The profit is measured by a monotone submodular function over\nthe elements. We first present an algorithm that guarantees an approximation\nfactor of $\\frac{1}{2}\\left(1-\\frac{1}{e^\\alpha}\\right)$, where $\\alpha \\leq 1$\nis the approximation factor of an algorithm for a sub-problem. We give two\npolynomial-time algorithms to solve this sub-problem. The first one gives us\n$\\alpha=1- \\epsilon$ if the costs satisfies a specific condition, which is\nfulfilled in several relevant cases, including the unitary costs case and the\nproblem of maximizing a monotone submodular function under a knapsack\nconstraint. The second one guarantees $\\alpha=1-\\frac{1}{e}-\\epsilon$ for the\ngeneral case. The gap between our approximation guarantees and the known\ninapproximability bounds is $\\frac{1}{2}$.\n  We extend our algorithm to a bi-criterion approximation algorithm in which we\nare allowed to spend an extra budget up to a factor $\\beta\\geq 1$ to guarantee\na $\\frac{1}{2}\\left(1-\\frac{1}{e^{\\alpha\\beta}}\\right)$-approximation. If we\nset $\\beta=\\frac{1}{\\alpha}\\ln \\left(\\frac{1}{2\\epsilon}\\right)$, the algorithm\nachieves an approximation factor of $\\frac{1}{2}-\\epsilon$, for any arbitrarily\nsmall $\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 11:08:00 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Cellinese", "Francesco", ""], ["D'Angelo", "Gianlorenzo", ""], ["Monaco", "Gianpiero", ""], ["Velaj", "Yllka", ""]]}, {"id": "1808.03307", "submitter": "Barbara Geissmann", "authors": "Barbara Geissmann", "title": "Longest Increasing Subsequence under Persistent Comparison Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of computing a longest increasing subsequence in a\nsequence $S$ of $n$ distinct elements in the presence of persistent comparison\nerrors. In this model, every comparison between two elements can return the\nwrong result with some fixed (small) probability $ p $, and comparisons cannot\nbe repeated. Computing the longest increasing subsequence exactly is impossible\nin this model, therefore, the objective is to identify a subsequence that (i)\nis indeed increasing and (ii) has a length that approximates the length of the\nlongest increasing subsequence.\n  We present asymptotically tight upper and lower bounds on both the\napproximation factor and the running time. In particular, we present an\nalgorithm that computes an $O(\\log n)$-approximation in time $O(n\\log n)$, with\nhigh probability. This approximation relies on the fact that that we can\napproximately sort $n$ elements in $O(n\\log n)$ time such that the maximum\ndislocation of an element is at most $O(\\log n)$. For the lower bounds, we\nprove that (i) there is a set of sequences, such that on a sequence picked\nrandomly from this set every algorithm must return an $\\Omega(\\log\nn)$-approximation with high probability, and (ii) any $O(\\log n)$-approximation\nalgorithm for longest increasing subsequence requires $\\Omega(n \\log n)$\ncomparisons, even in the absence of errors.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 19:06:40 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Geissmann", "Barbara", ""]]}, {"id": "1808.03367", "submitter": "Jake Wellens", "authors": "Jake Wellens", "title": "A note on partial rejection sampling for the hard disks model in the\n  plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we slightly improve the guarantees obtained by Guo and Jerrum\nfor sampling from the hard disks model in the plane via partial rejection\nsampling. Our proof makes use of the fact that if one spreads apart a\ncollection of disks in the plane, the area of the union of the disks cannot\ndecrease.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 23:01:09 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Wellens", "Jake", ""]]}, {"id": "1808.03411", "submitter": "Ranveer Singh", "authors": "Ranveer Singh, Naomi Shaked-Monderer, Avi Berman", "title": "Linear time algorithm to check the singularity of block graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A block graph is a graph in which every block is a complete graph. Let $G$ be\na block graph and let $A(G)$ be its (0,1)-adjacency matrix. Graph $G$ is called\nnonsingular (singular) if $A(G)$ is nonsingular (singular). Characterizing\nnonsingular block graphs is an interesting open problem proposed by Bapat and\nRoy in 2013. In this article, we give a linear time algorithm to check whether\na given block graph is singular or not.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 05:04:03 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 08:05:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Singh", "Ranveer", ""], ["Shaked-Monderer", "Naomi", ""], ["Berman", "Avi", ""]]}, {"id": "1808.03412", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Xiaoqi Chen, Gil Einziger, Ori Rottenstreich", "title": "Efficient Measurement on Programmable Switches Using Probabilistic\n  Recirculation", "comments": "To appear in IEEE ICNP 2018", "journal-ref": null, "doi": "10.1109/ICNP.2018.00047", "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable network switches promise flexibility and high throughput,\nenabling applications such as load balancing and traffic engineering. Network\nmeasurement is a fundamental building block for such applications, including\ntasks such as the identification of heavy hitters (largest flows) or the\ndetection of traffic changes.\n  However, high-throughput packet processing architectures place certain\nlimitations on the programming model, such as restricted branching, limited\ncapability for memory access, and a limited number of processing stages. These\nlimitations restrict the types of measurement algorithms that can run on\nprogrammable switches. In this paper, we focus on the RMT programmable\nhigh-throughput switch architecture, and carefully examine its constraints on\ndesigning measurement algorithms. We demonstrate our findings while solving the\nheavy hitter problem.\n  We introduce PRECISION, an algorithm that uses \\emph{Probabilistic\nRecirculation} to find top flows on a programmable switch. By recirculating a\nsmall fraction of packets, PRECISION simplifies the access to stateful memory\nto conform with RMT limitations and achieves higher accuracy than previous\nheavy hitter detection algorithms that avoid recirculation. We also analyze the\neffect of each architectural constraint on the measurement accuracy and provide\ninsights for measurement algorithm designers.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 05:19:31 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 07:47:02 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Basat", "Ran Ben", ""], ["Chen", "Xiaoqi", ""], ["Einziger", "Gil", ""], ["Rottenstreich", "Ori", ""]]}, {"id": "1808.03494", "submitter": "Kamil Khadiev", "authors": "Kamil Khadiev, Dmitry Kravchenko", "title": "On the Complexity of Solving Subtraction Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms for solving Subtraction games, which sometimes are\nreferred to as one-heap Nim games. We describe a quantum algorithm which is\napplicable to any game on DAG, and show that its query compexity for solving an\narbitrary Subtraction game of $n$ stones is $O(n^{3/2}\\log n)$. The best known\ndeterministic algorithms for solving such games are based on the dynamic\nprogramming approach. We show that this approach is asymptotically optimal and\nthat classical query complexity for solving a Subtraction game is generally\n$\\Theta(n^2)$. This paper perhaps is the first explicit \"quantum\" contribution\nto algorithmic game theory.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 11:45:22 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Khadiev", "Kamil", ""], ["Kravchenko", "Dmitry", ""]]}, {"id": "1808.03496", "submitter": "Robert Ganian", "authors": "Robert Ganian, Sebastian Ordyniak", "title": "The Power of Cut-Based Parameters for Computing Edge Disjoint Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the classical Edge Disjoint Paths (EDP) problem, where\none is given an undirected graph $G$ and a set of terminal pairs $P$ and asks\nwhether $G$ contains a set of pairwise edge-disjoint paths connecting every\nterminal pair in $P$. Our aim is to identify structural properties (parameters)\nof graphs which allow the efficient solution of EDP without restricting the\nplacement of terminals in $P$ in any way. In this setting, EDP is known to\nremain NP-hard even on extremely restricted graph classes, such as graphs with\na vertex cover of size $3$.\n  We present three results which use edge-separator based parameters to chart\nnew islands of tractability in the complexity landscape of EDP. Our first and\nmain result utilizes the fairly recent structural parameter treecut width (a\nparameter with fundamental ties to graph immersions and graph cuts): we obtain\na polynomial-time algorithm for EDP on every graph class of bounded treecut\nwidth. Our second result shows that EDP parameterized by treecut width is\nunlikely to be fixed-parameter tractable. Our final, third result is a\npolynomial kernel for EDP parameterized by the size of a minimum feedback edge\nset in the graph.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 11:48:30 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Ganian", "Robert", ""], ["Ordyniak", "Sebastian", ""]]}, {"id": "1808.03526", "submitter": "Maximilien Burq", "authors": "Itai Ashlagi, Maximilien Burq, Chinmoy Dutta, Patrick Jaillet, Amin\n  Saberi, Chris Sholley", "title": "Maximum Weight Online Matching with Deadlines", "comments": "arXiv admin note: text overlap with arXiv:1803.01285", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of matching agents who arrive at a marketplace over time\nand leave after d time periods. Agents can only be matched while they are\npresent in the marketplace. Each pair of agents can yield a different match\nvalue, and the planner's goal is to maximize the total value over a finite time\nhorizon. First we study the case in which vertices arrive in an adversarial\norder. We provide a randomized 0.25-competitive algorithm building on a result\nby Feldman et al. (2009) and Lehman et al. (2006). We extend the model to the\ncase in which departure times are drawn independently from a distribution with\nnon-decreasing hazard rate, for which we establish a 1/8-competitive algorithm.\n  When the arrival order is chosen uniformly at random, we show that a batching\nalgorithm, which computes a maximum-weighted matching every (d+1) periods, is\n0.279-competitive.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 04:03:08 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Ashlagi", "Itai", ""], ["Burq", "Maximilien", ""], ["Dutta", "Chinmoy", ""], ["Jaillet", "Patrick", ""], ["Saberi", "Amin", ""], ["Sholley", "Chris", ""]]}, {"id": "1808.03553", "submitter": "Dekel Tsur", "authors": "Amir Carmel, Dekel Tsur, Michal Ziv-Ukelson", "title": "Dynamic all scores matrices for LCS score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of aligning two strings A,B in order to determine their\nsimilarity is fundamental in the field of pattern matching. An important\nconcept in this domain is the \"all scores matrix\" that encodes the local\nalignment comparison of two strings. Namely, let K denote the all scores matrix\ncontaining the alignment score of every substring of B with A, and let J denote\nthe all scores matrix containing the alignment score of every suffix of B with\nevery prefix of A.\n  In this paper we consider the problem of maintaining an all scores matrix\nwhere the scoring function is the LCS score, while supporting single character\nprepend and append operations to A and N. Our algorithms exploit the sparsity\nparameters L=LCS(A,B) and Delta = |B|-L. For the matrix K we propose an\nalgorithm that supports incremental operations to both ends of A in O(Delta)\ntime. Whilst for the matrix J we propose an algorithm that supports a single\ntype of incremental operation, either a prepend operation to A or an append\noperation to B, in O(L) time. This structure can also be extended to support\nboth operations simultaneously in O(L log log L) time.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 14:05:28 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Carmel", "Amir", ""], ["Tsur", "Dekel", ""], ["Ziv-Ukelson", "Michal", ""]]}, {"id": "1808.03561", "submitter": "Matthew  Johnson", "authors": "Laurent Bulteau, Konrad K. Dabrowski, Guillaume Fertin, Matthew\n  Johnson, Daniel Paulusma, Stephane Vialette", "title": "Finding a Small Number of Colourful Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A partition $(V_1,\\ldots,V_k)$ of the vertex set of a graph $G$ with a (not\nnecessarily proper) colouring $c$ is colourful if no two vertices in any $V_i$\nhave the same colour and every set $V_i$ induces a connected graph. The\nCOLOURFUL PARTITION problem is to decide whether a coloured graph $(G,c)$ has a\ncolourful partition of size at most $k$. This problem is closely related to the\nCOLOURFUL COMPONENTS problem, which is to decide whether a graph can be\nmodified into a graph whose connected components form a colourful partition by\ndeleting at most $p$ edges. Nevertheless we show that COLOURFUL PARTITION and\nCOLOURFUL COMPONENTS may have different complexities for restricted instances.\nWe tighten known NP-hardness results for both problems and in addition we prove\nnew hardness and tractability results for COLOURFUL PARTITION. Using these\nresults we complete our paper with a thorough parameterized study of COLOURFUL\nPARTITION.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 14:26:29 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Bulteau", "Laurent", ""], ["Dabrowski", "Konrad K.", ""], ["Fertin", "Guillaume", ""], ["Johnson", "Matthew", ""], ["Paulusma", "Daniel", ""], ["Vialette", "Stephane", ""]]}, {"id": "1808.03566", "submitter": "Ahmad Hassanat", "authors": "Ahmad B. Hassanat", "title": "Greedy Algorithms for Approximating the Diameter of Machine Learning\n  Datasets in Multidimensional Euclidean Space", "comments": "15 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the diameter of a dataset in multidimensional Euclidean space is a\nwell-established problem, with well-known algorithms. However, most of the\nalgorithms found in the literature do not scale well with large values of data\ndimension, so the time complexity grows exponentially in most cases, which\nmakes these algorithms impractical. Therefore, we implemented 4 simple greedy\nalgorithms to be used for approximating the diameter of a multidimensional\ndataset; these are based on minimum/maximum l2 norms, hill climbing search,\nTabu search and Beam search approaches, respectively. The time complexity of\nthe implemented algorithms is near-linear, as they scale near-linearly with\ndata size and its dimensions. The results of the experiments (conducted on\ndifferent machine learning data sets) prove the efficiency of the implemented\nalgorithms and can therefore be recommended for finding the diameter to be used\nby different machine learning applications when needed.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 14:35:38 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Hassanat", "Ahmad B.", ""]]}, {"id": "1808.03633", "submitter": "Theo McKenzie", "authors": "Theo McKenzie, Hermish Mehta, Luca Trevisan", "title": "A New Algorithm for the Robust Semi-random Independent Set Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a general semi-random version of the planted\nindependent set problem in a model initially proposed by Feige and Kilian,\nwhich has a large proportion of adversarial edges.\n  We give a new deterministic algorithm that finds a list of independent sets,\none of which, with high probability, is the planted one, provided that the\nplanted set has size $k=\\Omega(n^{2/3})$. This improves on Feige and Kilian's\noriginal randomized algorithm, which with high probability recovers an\nindependent set of size at least $k$ when $k=\\alpha n$ where $\\alpha$ is a\nconstant.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 17:42:05 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 21:53:01 GMT"}, {"version": "v3", "created": "Sun, 4 Nov 2018 02:31:59 GMT"}, {"version": "v4", "created": "Wed, 30 Oct 2019 05:49:52 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["McKenzie", "Theo", ""], ["Mehta", "Hermish", ""], ["Trevisan", "Luca", ""]]}, {"id": "1808.03658", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "The effective entropy of next/previous larger/smaller value queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of storing the minimum number of bits required to answer\nnext/previous larger/smaller value queries on an array $A$ of $n$ numbers,\nwithout storing $A$. We show that these queries can be answered by storing at\nmost $3.701 n$ bits. Our result improves the result of Jo and Satti [TCS 2016]\nthat gives an upper bound of $4.088n$ bits for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 18:06:01 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1808.03713", "submitter": "Inbal Talgam-Cohen", "authors": "Paul D\\\"utting and Tim Roughgarden and Inbal Talgam-Cohen", "title": "Simple versus Optimal Contracts", "comments": "30 pages, 4 figures, Latex; minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic principal-agent model of contract theory, in which a\nprincipal designs an outcome-dependent compensation scheme to incentivize an\nagent to take a costly and unobservable action. When all of the model\nparameters---including the full distribution over principal rewards resulting\nfrom each agent action---are known to the designer, an optimal contract can in\nprinciple be computed by linear programming. In addition to their demanding\ninformational requirements, such optimal contracts are often complex and\nunintuitive, and do not resemble contracts used in practice.\n  This paper examines contract theory through the theoretical computer science\nlens, with the goal of developing novel theory to explain and justify the\nprevalence of relatively simple contracts, such as linear (pure commission)\ncontracts. First, we consider the case where the principal knows only the first\nmoment of each action's reward distribution, and we prove that linear contracts\nare guaranteed to be worst-case optimal, ranging over all reward distributions\nconsistent with the given moments. Second, we study linear contracts from a\nworst-case approximation perspective, and prove several tight parameterized\napproximation bounds.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 21:51:24 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 11:45:41 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2020 13:54:57 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Roughgarden", "Tim", ""], ["Talgam-Cohen", "Inbal", ""]]}, {"id": "1808.03880", "submitter": "Yaron Singer", "authors": "Eric Balkanski and Yaron Singer", "title": "Parallelization does not Accelerate Convex Optimization: Adaptivity\n  Lower Bounds for Non-smooth Convex Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the limitations of parallelization in convex\noptimization. A convenient approach to study parallelization is through the\nprism of \\emph{adaptivity} which is an information theoretic measure of the\nparallel runtime of an algorithm [BS18]. Informally, adaptivity is the number\nof sequential rounds an algorithm needs to make when it can execute\npolynomially-many queries in parallel at every round. For combinatorial\noptimization with black-box oracle access, the study of adaptivity has recently\nled to exponential accelerations in parallel runtime and the natural question\nis whether dramatic accelerations are achievable for convex optimization.\n  For the problem of minimizing a non-smooth convex function $f:[0,1]^n\\to\n\\mathbb{R}$ over the unit Euclidean ball, we give a tight lower bound that\nshows that even when $\\texttt{poly}(n)$ queries can be executed in parallel,\nthere is no randomized algorithm with $\\tilde{o}(n^{1/3})$ rounds of adaptivity\nthat has convergence rate that is better than those achievable with a\none-query-per-round algorithm. A similar lower bound was obtained by Nemirovski\n[Nem94], however that result holds for the $\\ell_{\\infty}$-setting instead of\n$\\ell_2$. In addition, we also show a tight lower bound that holds for\nLipschitz and strongly convex functions.\n  At the time of writing this manuscript we were not aware of Nemirovski's\nresult. The construction we use is similar to the one in [Nem94], though our\nanalysis is different. Due to the close relationship between this work and\n[Nem94], we view the research contribution of this manuscript limited and it\nshould serve as an instructful approach to understanding lower bounds for\nparallel optimization.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 01:56:17 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 19:29:57 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Balkanski", "Eric", ""], ["Singer", "Yaron", ""]]}, {"id": "1808.03978", "submitter": "Sandip Sinha", "authors": "Sandip Sinha and Omri Weinstein", "title": "Local Decodability of the Burrows-Wheeler Transform", "comments": "The following two technical typos were fixed: (1) On page 2,\n  following Theorem 1, the decoding time of a contiguous substring of size\n  $\\ell$ was corrected from $O(t + \\ell)$ to $O(t + \\ell \\cdot \\lg t)$. (2) In\n  the statement of Theorem 2, the query time to count occurrences of patterns\n  of length $\\ell$ was corrected to $O(t \\ell)$, independent of the number of\n  occurrences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler Transform (BWT) is among the most influential discoveries\nin text compression and DNA storage. It is a reversible preprocessing step that\nrearranges an $n$-letter string into runs of identical characters (by\nexploiting context regularities), resulting in highly compressible strings, and\nis the basis of the \\texttt{bzip} compression program. Alas, the decoding\nprocess of BWT is inherently sequential and requires $\\Omega(n)$ time even to\nretrieve a \\emph{single} character.\n  We study the succinct data structure problem of locally decoding short\nsubstrings of a given text under its \\emph{compressed} BWT, i.e., with small\nadditive redundancy $r$ over the \\emph{Move-To-Front} (\\texttt{bzip})\ncompression. The celebrated BWT-based FM-index (FOCS '00), as well as other\nrelated literature, yield a trade-off of $r=\\tilde{O}(n/\\sqrt{t})$ bits, when a\nsingle character is to be decoded in $O(t)$ time. We give a near-quadratic\nimprovement $r=\\tilde{O}(n\\lg(t)/t)$. As a by-product, we obtain an\n\\emph{exponential} (in $t$) improvement on the redundancy of the FM-index for\ncounting pattern-matches on compressed text. In the interesting regime where\nthe text compresses to $n^{1-o(1)}$ bits, these results provide an $\\exp(t)$\n\\emph{overall} space reduction. For the local decoding problem of BWT, we also\nprove an $\\Omega(n/t^2)$ cell-probe lower bound for \"symmetric\" data\nstructures.\n  We achieve our main result by designing a compressed partial-sums (Rank) data\nstructure over BWT. The key component is a \\emph{locally-decodable}\nMove-to-Front (MTF) code: with only $O(1)$ extra bits per block of length\n$n^{\\Omega(1)}$, the decoding time of a single character can be decreased from\n$\\Omega(n)$ to $O(\\lg n)$. This result is of independent interest in\nalgorithmic information theory.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 18:16:59 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 06:09:03 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Sinha", "Sandip", ""], ["Weinstein", "Omri", ""]]}, {"id": "1808.03994", "submitter": "Bachir El Khadir", "authors": "Amir Ali Ahmadi and Bachir El Khadir", "title": "Time-Varying Semidefinite Programs", "comments": "Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study time-varying semidefinite programs (TV-SDPs), which are semidefinite\nprograms whose data (and solutions) are functions of time. Our focus is on the\nsetting where the data varies polynomially with time. We show that under a\nstrict feasibility assumption, restricting the solutions to also be polynomial\nfunctions of time does not change the optimal value of the TV-SDP. Moreover, by\nusing a Positivstellensatz on univariate polynomial matrices, we show that the\nbest polynomial solution of a given degree to a TV-SDP can be found by solving\na semidefinite program of tractable size. We also provide a sequence of dual\nproblems which can be cast as SDPs and that give upper bounds on the optimal\nvalue of a TV-SDP (in maximization form). We prove that under a boundedness\nassumption, this sequence of upper bounds converges to the optimal value of the\nTV-SDP. Under the same assumption, we also show that the optimal value of the\nTV-SDP is attained. We demonstrate the efficacy of our algorithms on a\nmaximum-flow problem with time-varying edge capacities, a wireless coverage\nproblem with time-varying coverage requirements, and on bi-objective\nsemidefinite optimization where the goal is to approximate the Pareto curve in\none shot.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 19:54:59 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 04:29:58 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Khadir", "Bachir El", ""]]}, {"id": "1808.04062", "submitter": "Bin Fu", "authors": "Qilong Feng and Bin Fu", "title": "Speeding Up Constrained $k$-Means Through 2-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the constrained 2-means problem, we present a\n$O\\left(dn+d({1\\over\\epsilon})^{O({1\\over \\epsilon})}\\log n\\right)$ time\nalgorithm. It generates a collection $U$ of approximate center pairs $(c_1,\nc_2)$ such that one of pairs in $U$ can induce a $(1+\\epsilon)$-approximation\nfor the problem. The existing approximation scheme for the constrained 2-means\nproblem takes $O(({1\\over\\epsilon})^{O({1\\over \\epsilon})}dn)$ time, and the\nexisting approximation scheme for the constrained $k$-means problem takes\n$O(({k\\over\\epsilon})^{O({k\\over \\epsilon})}dn)$ time. Using the method\ndeveloped in this paper, we point out that every existing approximating scheme\nfor the constrained $k$-means so far with time $C(k, n, d, \\epsilon)$ can be\ntransformed to a new approximation scheme with time complexity ${C(k, n, d,\n\\epsilon)/ k^{\\Omega({1\\over\\epsilon})}}$.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 04:35:04 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Feng", "Qilong", ""], ["Fu", "Bin", ""]]}, {"id": "1808.04155", "submitter": "Giorgi Nadiradze", "authors": "Dan Alistarh, Trevor Brown, Justin Kopinsky, Giorgi Nadiradze", "title": "Relaxed Schedulers Can Efficiently Parallelize Iterative Algorithms", "comments": "PODC 2018, pages 377-386 in proceedings", "journal-ref": null, "doi": "10.1145/3212734.3212756", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant progress in understanding the parallelism inherent\nto iterative sequential algorithms: for many classic algorithms, the depth of\nthe dependence structure is now well understood, and scheduling techniques have\nbeen developed to exploit this shallow dependence structure for efficient\nparallel implementations. A related, applied research strand has studied\nmethods by which certain iterative task-based algorithms can be efficiently\nparallelized via relaxed concurrent priority schedulers. These allow for high\nconcurrency when inserting and removing tasks, at the cost of executing\nsuperfluous work due to the relaxed semantics of the scheduler.\n  In this work, we take a step towards unifying these two research directions,\nby showing that there exists a family of relaxed priority schedulers that can\nefficiently and deterministically execute classic iterative algorithms such as\ngreedy maximal independent set (MIS) and matching. Our primary result shows\nthat, given a randomized scheduler with an expected relaxation factor of $k$ in\nterms of the maximum allowed priority inversions on a task, and any graph on\n$n$ vertices, the scheduler is able to execute greedy MIS with only an additive\nfactor of poly($k$) expected additional iterations compared to an exact (but\nnot scalable) scheduler. This counter-intuitive result demonstrates that the\noverhead of relaxation when computing MIS is not dependent on the input size or\nstructure of the input graph. Experimental results show that this overhead can\nbe clearly offset by the gain in performance due to the highly scalable\nscheduler. In sum, we present an efficient method to deterministically\nparallelize iterative sequential algorithms, with provable runtime guarantees\nin terms of the number of executed tasks to completion.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 11:47:51 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Alistarh", "Dan", ""], ["Brown", "Trevor", ""], ["Kopinsky", "Justin", ""], ["Nadiradze", "Giorgi", ""]]}, {"id": "1808.04185", "submitter": "Dekel Tsur", "authors": "Dekel Tsur", "title": "Faster deterministic parameterized algorithm for k-Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the k-Path problem, the input is a directed graph $G$ and an integer\n$k\\geq 1$, and the goal is to decide whether there is a simple directed path in\n$G$ with exactly $k$ vertices. We give a deterministic algorithm for k-Path\nwith time complexity $O^*(2.554^k)$. This improves the previously best\ndeterministic algorithm for this problem of Zehavi [ESA 2015] whose time\ncomplexity is $O^*(2.597^k)$. The technique used by our algorithm can also be\nused to obtain faster deterministic algorithms for k-Tree, r-Dimensional\nk-Matching, Graph Motif, and Partial Cover.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 12:58:08 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 07:28:58 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Tsur", "Dekel", ""]]}, {"id": "1808.04360", "submitter": "Yang Liu", "authors": "Yang Liu, Sebastien Blandin, Samitha Samaranayake", "title": "Stochastic on-time arrival problem in transit networks", "comments": "29 pages; 12 figures. This manuscript version is made available under\n  the CC-BY-NC-ND 4.0 license\n  https://creativecommons.org/licenses/by-nc-nd/4.0/", "journal-ref": null, "doi": "10.1016/j.trb.2018.11.013", "report-no": null, "categories": "cs.DS cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the stochastic on-time arrival problem in transit\nnetworks where both the travel time and the waiting time for transit services\nare stochastic. A specific challenge of this problem is the combinatorial\nsolution space due to the unknown ordering of transit line arrivals. We propose\na network structure appropriate to the online decision-making of a passenger,\nincluding boarding, waiting and transferring. In this framework, we design a\ndynamic programming algorithm that is pseudo-polynomial in the number of\ntransit stations and travel time budget, and exponential in the number of\ntransit lines at a station, which is a small number in practice. To reduce the\nsearch space, we propose a definition of transit line dominance, and techniques\nto identify dominance, which decrease the computation time by up to 90% in\nnumerical experiments. Extensive numerical experiments are conducted on both a\nsynthetic network and the Chicago transit network.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 02:45:21 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 19:28:16 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Liu", "Yang", ""], ["Blandin", "Sebastien", ""], ["Samaranayake", "Samitha", ""]]}, {"id": "1808.04602", "submitter": "Peter Sanders", "authors": "Peter Sanders", "title": "Hashing with Linear Probing and Referential Integrity", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a variant of linear probing hash tables that never moves elements\nand thus supports referential integrity, i.e., pointers to elements remain\nvalid while this element is in the hash table. This is achieved by the folklore\nmethod of marking some table entries as formerly occupied (tombstones). The\ninnovation is that the number of tombstones is minimized. Experiments indicate\nthat this allows an unbounded number of operations with bounded overhead\ncompared to linear probing without tombstones (and without referential\nintegrity).\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 09:57:43 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Sanders", "Peter", ""]]}, {"id": "1808.04651", "submitter": "Stephan Beyer", "authors": "Stephan Beyer, Markus Chimani, Joachim Spoerhase", "title": "A Simple Primal-Dual Approximation Algorithm for 2-Edge-Connected\n  Spanning Subgraphs", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and natural approximation algorithm for the problem of\nfinding a 2-edge-connected spanning subgraph of minimum total edge cost in a\ngraph. The algorithm maintains a spanning forest starting with an empty edge\nset. In each iteration, a new edge incident to a leaf is selected in a natural\ngreedy manner and added to the forest. If this produces a cycle, this cycle is\ncontracted. This growing phase ends when the graph has been contracted into a\nsingle node and a subsequent cleanup step removes redundant edges in reverse\norder.\n  We analyze the algorithm using the primal-dual method showing that its\nsolution value is at most 3 times the optimum. Although this only matches the\nratio of existing primal-dual algorithms, we require only a single growing\nphase, thereby addressing a question by Williamson. Also, we consider our\nalgorithm to be not only conceptually simpler than the known approximation\nalgorithms but also easier to implement in its entirety. For n and m being the\nnumber of nodes and edges, respectively, it runs in O(min{nm, m + n^2 log n})\ntime and O(m) space without data structures more sophisticated than binary\nheaps and graphs, and without graph algorithms beyond depth-first search.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 12:21:13 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 18:23:52 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Beyer", "Stephan", ""], ["Chimani", "Markus", ""], ["Spoerhase", "Joachim", ""]]}, {"id": "1808.04807", "submitter": "Ashish Chiplunkar", "authors": "Ashish Chiplunkar, Michael Kapralov, Sanjeev Khanna, Aida Mousavifar,\n  Yuval Peres", "title": "Testing Graph Clusterability: Algorithms and Lower Bounds", "comments": "Appears in FOCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing graph cluster structure: given access to a\ngraph $G=(V, E)$, can we quickly determine whether the graph can be partitioned\ninto a few clusters with good inner conductance, or is far from any such graph?\nThis is a generalization of the well-studied problem of testing graph\nexpansion, where one wants to distinguish between the graph having good\nexpansion (i.e.\\ being a good single cluster) and the graph having a sparse cut\n(i.e.\\ being a union of at least two clusters). A recent work of Czumaj, Peng,\nand Sohler (STOC'15) gave an ingenious sublinear time algorithm for testing\n$k$-clusterability in time $\\tilde{O}(n^{1/2} \\text{poly}(k))$: their algorithm\nimplicitly embeds a random sample of vertices of the graph into Euclidean\nspace, and then clusters the samples based on estimates of Euclidean distances\nbetween the points. This yields a very efficient testing algorithm, but only\nworks if the cluster structure is very strong: it is necessary to assume that\nthe gap between conductances of accepted and rejected graphs is at least\nlogarithmic in the size of the graph $G$. In this paper we show how one can\nleverage more refined geometric information, namely angles as opposed to\ndistances, to obtain a sublinear time tester that works even when the gap is a\nsufficiently large constant. Our tester is based on the singular value\ndecomposition of a natural matrix derived from random walk transition\nprobabilities from a small sample of seed nodes.\n  We complement our algorithm with a matching lower bound on the query\ncomplexity of testing clusterability. Our lower bound is based on a novel\nproperty testing problem, which we analyze using Fourier analytic tools. As a\nbyproduct of our techniques, we also achieve new lower bounds for the problem\nof approximating MAX-CUT value in sublinear time.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 17:37:27 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 14:11:48 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chiplunkar", "Ashish", ""], ["Kapralov", "Michael", ""], ["Khanna", "Sanjeev", ""], ["Mousavifar", "Aida", ""], ["Peres", "Yuval", ""]]}, {"id": "1808.04863", "submitter": "Denis Pankratov", "authors": "Allan Borodin, Christodoulos Karavasilis, Denis Pankratov", "title": "An Experimental Study of Algorithms for Online Bipartite Matching", "comments": "37 pages, 36 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform an experimental study of algorithms for online bipartite matching\nunder the known i.i.d. input model with integral types. In the last decade,\nthere has been substantial effort in designing complex algorithms with the goal\nof improving worst-case approximation ratios. Our goal is to determine how\nthese algorithms perform on more practical instances rather than worst-case\ninstances. In particular, we are interested in whether the ranking of the\nalgorithms by their worst-case performance is consistent with the ranking of\nthe algorithms by their average-case/practical performance. We are also\ninterested in whether preprocessing times and implementation difficulties that\nare introduced by these algorithms are justified in practice. To that end we\nevaluate these algorithms on different random inputs as well as real-life\ninstances obtained from publicly available repositories. We compare these\nalgorithms against several simple greedy-style algorithms. Most of the complex\nalgorithms in the literature are presented as being non-greedy (i.e., an\nalgorithm can intentionally skip matching a node that has available neighbors)\nto simplify the analysis. Every such algorithm can be turned into a greedy one\nwithout hurting its worst-case performance. On our benchmarks, non-greedy\nversions of these algorithms perform much worse than their greedy versions.\nGreedy versions perform about as well as the simplest greedy algorithm by\nitself. This, together with our other findings, suggests that simplest greedy\nalgorithms are competitive with the state-of-the-art worst-case algorithms for\nonline bipartite matching on many average-case and practical input families.\nGreediness is by far the most important property of online algorithms for\nbipartite matching.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 19:06:47 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Borodin", "Allan", ""], ["Karavasilis", "Christodoulos", ""], ["Pankratov", "Denis", ""]]}, {"id": "1808.04995", "submitter": "John Michael Goddard Kallaugher", "authors": "John Kallaugher, Michael Kapralov, Eric Price", "title": "The Sketching Complexity of Graph and Hypergraph Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph counting is a fundamental primitive in graph processing, with\napplications in social network analysis (e.g., estimating the clustering\ncoefficient of a graph), database processing and other areas. The space\ncomplexity of subgraph counting has been studied extensively in the literature,\nbut many natural settings are still not well understood. In this paper we\nrevisit the subgraph (and hypergraph) counting problem in the sketching model,\nwhere the algorithm's state as it processes a stream of updates to the graph is\na linear function of the stream. This model has recently received a lot of\nattention in the literature, and has become a standard model for solving\ndynamic graph streaming problems.\n  In this paper we give a tight bound on the sketching complexity of counting\nthe number of occurrences of a small subgraph $H$ in a bounded degree graph $G$\npresented as a stream of edge updates. Specifically, we show that the space\ncomplexity of the problem is governed by the fractional vertex cover number of\nthe graph $H$. Our subgraph counting algorithm implements a natural vertex\nsampling approach, with sampling probabilities governed by the vertex cover of\n$H$. Our main technical contribution lies in a new set of Fourier analytic\ntools that we develop to analyze multiplayer communication protocols in the\nsimultaneous communication model, allowing us to prove a tight lower bound. We\nbelieve that our techniques are likely to find applications in other settings.\nBesides giving tight bounds for all graphs $H$, both our algorithm and lower\nbounds extend to the hypergraph setting, albeit with some loss in space\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 07:58:32 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Kallaugher", "John", ""], ["Kapralov", "Michael", ""], ["Price", "Eric", ""]]}, {"id": "1808.05017", "submitter": "Benjamin Bergougnoux", "authors": "Benjamin Bergougnoux, Florent Capelli, Mamadou Moustapha Kant\\'e", "title": "Counting Minimal Transversals of $\\beta$-Acyclic Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that one can count in polynomial time the number of minimal\ntransversals of $\\beta$-acyclic hypergraphs. In consequence, we can count in\npolynomial time the number of minimal dominating sets of strongly chordal\ngraphs, continuing the line of research initiated in [M.M. Kant\\'e and T. Uno,\nCounting Minimal Dominating Sets, TAMC'17].\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 09:49:44 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Bergougnoux", "Benjamin", ""], ["Capelli", "Florent", ""], ["Kant\u00e9", "Mamadou Moustapha", ""]]}, {"id": "1808.05256", "submitter": "Hemant Malik", "authors": "Hemant Malik, Ovidiu Daescu and Ramaswamy Chandrasekaran", "title": "Edge Disjoint Spanning Trees in an Undirected Graph with E=2(V-1)", "comments": "Lemma 5 need to be updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given a connected undirected graph G = [V; E] where |E| =2(|V| -1), we\npresent two algorithms to check if G can be decomposed into two edge disjoint\nspanning trees, and provide such a decomposition when it exists. Unlike\nprevious algorithms for finding edge disjoint spanning trees in general\nundirected graphs, based on matroids and complex in description, our algorithms\nare based on simple graph reduction techniques and thus easy to describe and\nimplement. Moreover, the running time for our solutions is asymptotically\nfaster. Specifically, ours are the first algorithms to achieve a running time\nthat is a polylog factor from linear, approaching the 1974 linear time\nalgorithm of Robert E. Tarjan for directed graphs. A direct implication of our\nresult is that minimally rigid graphs, also called Laman graphs, can be\nrecognized in almost linear time, thus answering a long standing open problem.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 19:02:39 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 21:41:55 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Malik", "Hemant", ""], ["Daescu", "Ovidiu", ""], ["Chandrasekaran", "Ramaswamy", ""]]}, {"id": "1808.05458", "submitter": "Alexander Noe", "authors": "Monika Henzinger, Alexander Noe and Christian Schulz", "title": "Shared-memory Exact Minimum Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum cut problem for an undirected edge-weighted graph asks us to\ndivide its set of nodes into two blocks while minimizing the weight sum of the\ncut edges. In this paper, we engineer the fastest known exact algorithm for the\nproblem.\n  State-of-the-art algorithms like the algorithm of Padberg and Rinaldi or the\nalgorithm of Nagamochi, Ono and Ibaraki identify edges that can be contracted\nto reduce the graph size such that at least one minimum cut is maintained in\nthe contracted graph. Our algorithm achieves improvements in running time over\nthese algorithms by a multitude of techniques. First, we use a recently\ndeveloped fast and parallel \\emph{inexact} minimum cut algorithm to obtain a\nbetter bound for the problem. Then we use reductions that depend on this bound,\nto reduce the size of the graph much faster than previously possible. We use\nimproved data structures to further improve the running time of our algorithm.\nAdditionally, we parallelize the contraction routines of Nagamochi, Ono and\nIbaraki. Overall, we arrive at a system that outperforms the fastest\nstate-of-the-art solvers for the \\emph{exact} minimum cut problem\nsignificantly.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 13:18:43 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Henzinger", "Monika", ""], ["Noe", "Alexander", ""], ["Schulz", "Christian", ""]]}, {"id": "1808.05497", "submitter": "Rajesh Jayaram", "authors": "Rajesh Jayaram, David P. Woodruff", "title": "Perfect $L_p$ Sampling in a Data Stream", "comments": "An earlier version of this work appeared in FOCS 2018, but contained\n  an error in the derandomization. In this version, we correct this issue,\n  albeit with a (log log n)^2 -factor increase in the space required to\n  derandomize the algorithm for p<2. Our results in the random oracle model and\n  for p = 2 are unaffected. We also give alternative algorithms and additional\n  applications.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we resolve the one-pass space complexity of $L_p$ sampling for\n$p \\in (0,2)$. Given a stream of updates (insertions and deletions) to the\ncoordinates of an underlying vector $f \\in \\mathbb{R}^n$, a perfect $L_p$\nsampler must output an index $i$ with probability $|f_i|^p/\\|f\\|_p^p$, and is\nallowed to fail with some probability $\\delta$. So far, for $p > 0$ no\nalgorithm has been shown to solve the problem exactly using $\\text{poly}( \\log\nn)$-bits of space. In 2010, Monemizadeh and Woodruff introduced an approximate\n$L_p$ sampler, which outputs $i$ with probability $(1 \\pm \\nu)|f_i|^p\n/\\|f\\|_p^p$, using space polynomial in $\\nu^{-1}$ and $\\log(n)$. The space\ncomplexity was later reduced by Jowhari, Sa\\u{g}lam, and Tardos to roughly\n$O(\\nu^{-p} \\log^2 n \\log \\delta^{-1})$ for $p \\in (0,2)$, which tightly\nmatches the $\\Omega(\\log^2 n \\log \\delta^{-1})$ lower bound in terms of $n$ and\n$\\delta$, but is loose in terms of $\\nu$.\n  Given these nearly tight bounds, it is perhaps surprising that no lower bound\nexists in terms of $\\nu$---not even a bound of $\\Omega(\\nu^{-1})$ is known. In\nthis paper, we explain this phenomenon by demonstrating the existence of an\n$O(\\log^2 n \\log \\delta^{-1})$-bit perfect $L_p$ sampler for $p \\in (0,2)$.\nThis shows that $\\nu$ need not factor into the space of an $L_p$ sampler, which\ncloses the complexity of the problem for this range of $p$. For $p=2$, our\nbound is $O(\\log^3 n \\log \\delta^{-1})$-bits, which matches the prior best\nknown upper bound in terms of $n,\\delta$, but has no dependence on $\\nu$. For\n$p<2$, our bound holds in the random oracle model, matching the lower bounds in\nthat model. Moreover, we show that our algorithm can be derandomized with only\na $O((\\log \\log n)^2)$ blow-up in the space (and no blow-up for $p=2$). Our\nderandomization technique is general, and can be used to derandomize a large\nclass of linear sketches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:00:44 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 18:23:52 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 14:22:20 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Jayaram", "Rajesh", ""], ["Woodruff", "David P.", ""]]}, {"id": "1808.05566", "submitter": "Florian Meier", "authors": "Hafsteinn Einarsson, Marcelo Matheus Gauy, Johannes Lengler, Florian\n  Meier, Asier Mujika, Angelika Steger, Felix Weissenberger", "title": "The linear hidden subset problem for the (1+1) EA with scheduled and\n  adaptive mutation rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unbiased $(1+1)$ evolutionary algorithms on linear functions with an\nunknown number $n$ of bits with non-zero weight. Static algorithms achieve an\noptimal runtime of $O(n (\\ln n)^{2+\\epsilon})$, however, it remained unclear\nwhether more dynamic parameter policies could yield better runtime guarantees.\nWe consider two setups: one where the mutation rate follows a fixed schedule,\nand one where it may be adapted depending on the history of the run. For the\nfirst setup, we give a schedule that achieves a runtime of $(1\\pm o(1))\\beta n\n\\ln n$, where $\\beta \\approx 3.552$, which is an asymptotic improvement over\nthe runtime of the static setup. Moreover, we show that no schedule admits a\nbetter runtime guarantee and that the optimal schedule is essentially unique.\nFor the second setup, we show that the runtime can be further improved to\n$(1\\pm o(1)) e n \\ln n$, which matches the performance of algorithms that know\n$n$ in advance.\n  Finally, we study the related model of initial segment uncertainty with\nstatic position-dependent mutation rates, and derive asymptotically optimal\nlower bounds. This answers a question by Doerr, Doerr, and K\\\"otzing.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 16:15:13 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Einarsson", "Hafsteinn", ""], ["Gauy", "Marcelo Matheus", ""], ["Lengler", "Johannes", ""], ["Meier", "Florian", ""], ["Mujika", "Asier", ""], ["Steger", "Angelika", ""], ["Weissenberger", "Felix", ""]]}, {"id": "1808.05662", "submitter": "Jakub Pachocki", "authors": "Timothy Chu, Michael B. Cohen, Jakub W. Pachocki and Richard Peng", "title": "Constant Arboricity Spectral Sparsifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that every graph is spectrally similar to the union of a constant\nnumber of forests. Moreover, we show that Spielman-Srivastava sparsifiers are\nthe union of O(logn) forests. This result can be used to estimate boundaries of\nsmall subsets of vertices in nearly optimal query time.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 19:54:17 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Chu", "Timothy", ""], ["Cohen", "Michael B.", ""], ["Pachocki", "Jakub W.", ""], ["Peng", "Richard", ""]]}, {"id": "1808.05676", "submitter": "Bhaskar DasGupta", "authors": "Bhaskar DasGupta, Mano Vikash Janardhanan and Farzane Yahyanejad", "title": "Why did the shape of your network change? (On detecting network\n  anomalies via non-local curvatures)", "comments": "Final revised version; to appear in Algorithmica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $Anomaly$ $detection$ problems (also called $change$-$point$ $detection$\nproblems) have been studied in data mining, statistics and computer science\nover the last several decades in applications such as medical condition\nmonitoring and weather change detection. In recent days, however, anomaly\ndetection problems have become increasing more relevant in the context of\n$network$ $science$ since useful insights for many complex systems in biology,\nfinance and social science are often obtained by representing them via\nnetworks. Notions of local and non-local curvatures of higher-dimensional\ngeometric shapes and topological spaces play a $fundamental$ role in physics\nand mathematics in characterizing anomalous behaviours of these higher\ndimensional entities. However, using curvature measures to detect anomalies in\nnetworks is not yet very common. To this end, a main goal in this paper to\nformulate and analyze curvature analysis methods to provide the foundations of\nsystematic approaches to find $critical$ $components$ and $detect$ $anomalies$\nin networks. For this purpose, we use two measures of network curvatures which\ndepend on non-trivial global properties, such as distributions of geodesics and\nhigher-order correlations among nodes, of the given network. Based on these\nmeasures, we precisely formulate several computational problems related to\nanomaly detection in static or dynamic networks, and provide non-trivial\ncomputational complexity results for these problems. This paper must $not$ be\nviewed as delivering the final word on appropriateness and suitability of\nspecific curvature measures. Instead, it is our hope that this paper will\nstimulate and motivate further theoretical or empirical research concerning the\nexciting interplay between notions of curvatures from network and non-network\ndomains, a $much$ desired goal in our opinion.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 20:41:27 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 21:42:01 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 20:20:04 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["DasGupta", "Bhaskar", ""], ["Janardhanan", "Mano Vikash", ""], ["Yahyanejad", "Farzane", ""]]}, {"id": "1808.05731", "submitter": "Ankur Moitra", "authors": "Allen Liu and Ankur Moitra", "title": "Efficiently Learning Mixtures of Mallows Models", "comments": "35 pages", "journal-ref": "FOCS 2018", "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of Mallows models are a popular generative model for ranking data\ncoming from a heterogeneous population. They have a variety of applications\nincluding social choice, recommendation systems and natural language\nprocessing. Here we give the first polynomial time algorithm for provably\nlearning the parameters of a mixture of Mallows models with any constant number\nof components. Prior to our work, only the two component case had been settled.\nOur analysis revolves around a determinantal identity of Zagier which was\nproven in the context of mathematical physics, which we use to show polynomial\nidentifiability and ultimately to construct test functions to peel off one\ncomponent at a time.\n  To complement our upper bounds, we show information-theoretic lower bounds on\nthe sample complexity as well as lower bounds against restricted families of\nalgorithms that make only local queries. Together, these results demonstrate\nvarious impediments to improving the dependence on the number of components.\nThey also motivate the study of learning mixtures of Mallows models from the\nperspective of beyond worst-case analysis. In this direction, we show that when\nthe scaling parameters of the Mallows models have separation, there are much\nfaster learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 02:24:23 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Liu", "Allen", ""], ["Moitra", "Ankur", ""]]}, {"id": "1808.05765", "submitter": "Chao Xu", "authors": "Chandra Chekuri, Kent Quanrud, Chao Xu", "title": "LP Relaxation and Tree Packing for Minimum $k$-cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Karger used spanning tree packings to derive a near linear-time randomized\nalgorithm for the global minimum cut problem as well as a bound on the number\nof approximate minimum cuts. This is a different approach from his well-known\nrandom contraction algorithm. Thorup developed a fast deterministic algorithm\nfor the minimum $k$-cut problem via greedy recursive tree packings.\n  In this paper we revisit properties of an LP relaxation for $k$-cut proposed\nby Naor and Rabani, and analyzed by Chekuri, Guha and Naor. We show that the\ndual of the LP yields a tree packing, that when combined with an upper bound on\nthe integrality gap for the LP, easily and transparently extends Karger's\nanalysis for mincut to the $k$-cut problem. In addition to the simplicity of\nthe algorithm and its analysis, this allows us to improve the running time of\nThorup's algorithm by a factor of $n$. We also improve the bound on the number\nof $\\alpha$-approximate $k$-cuts. Second, we give a simple proof that the\nintegrality gap of the LP is $2(1-1/n)$. Third, we show that an optimum\nsolution to the LP relaxation, for all values of $k$, is fully determined by\nthe principal sequence of partitions of the input graph. This allows us to\nrelate the LP relaxation to the Lagrangian relaxation approach of Barahona and\nRavi and Sinha; it also shows that the idealized recursive tree packing\nconsidered by Thorup gives an optimum dual solution to the LP. This work arose\nfrom an effort to understand and simplify the results of Thorup.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 05:51:59 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Chekuri", "Chandra", ""], ["Quanrud", "Kent", ""], ["Xu", "Chao", ""]]}, {"id": "1808.05879", "submitter": "Damien Desfontaines", "authors": "Damien Desfontaines, Andreas Lochbihler, and David Basin", "title": "Cardinality Estimators do not Preserve Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardinality estimators like HyperLogLog are sketching algorithms that\nestimate the number of distinct elements in a large multiset. Their use in\nprivacy-sensitive contexts raises the question of whether they leak private\ninformation. In particular, can they provide any privacy guarantees while\npreserving their strong aggregation properties? We formulate an abstract notion\nof cardinality estimators, that captures this aggregation requirement: one can\nmerge sketches without losing precision. We propose an attacker model and a\ncorresponding privacy definition, strictly weaker than differential privacy: we\nassume that the attacker has no prior knowledge of the data. We then show that\nif a cardinality estimator satisfies this definition, then it cannot have a\nreasonable level of accuracy. We prove similar results for weaker versions of\nour definition, and analyze the privacy of existing algorithms, showing that\ntheir average privacy loss is significant, even for multisets with large\ncardinalities. We conclude that strong aggregation requirements are\nincompatible with any reasonable definition of privacy, and that cardinality\nestimators should be considered as sensitive as raw data. We also propose risk\nmitigation strategies for their real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 14:26:00 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 14:53:18 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 22:27:26 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Desfontaines", "Damien", ""], ["Lochbihler", "Andreas", ""], ["Basin", "David", ""]]}, {"id": "1808.06251", "submitter": "Hiroki Kanezashi", "authors": "Hiroki Kanezashi and Toyotaro Suzumura", "title": "An incremental local-first community detection method for dynamic graphs", "comments": "8 pages, 7 figures and 3 pseudo codes, 2016 IEEE International\n  Conference on Big Data (Big Data)", "journal-ref": null, "doi": "10.1109/BigData.2016.7840991", "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detections for large-scale real world networks have been more\npopular in social analytics. In particular, dynamically growing network\nanalyses become important to find long-term trends and detect anomalies. In\norder to analyze such networks, we need to obtain many snapshots and apply same\nanalytic methods to them. However, it is inefficient to extract communities\nfrom these whole newly generated networks with little differences every time,\nand then it is impossible to follow the network growths in the real time. We\nproposed an incremental community detection algorithm for high-volume graph\nstreams. It is based on the top of a well-known batch-oriented algorithm named\nDEMON[1]. We also evaluated performance and precisions of our proposed\nincremental algorithm with real-world big networks with up to 410,236 vertices\nand 2,439,437 edges and computed in less than one second to detect communities\nin an incremental fashion - which achieves up to 107 times faster than the\noriginal algorithm without sacrificing accuracies.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 20:16:17 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Kanezashi", "Hiroki", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1808.06394", "submitter": "Christian Schulz", "authors": "Sebastian Schlag, Matthias Schmitt, Christian Schulz", "title": "Faster Support Vector Machines", "comments": "Extended version of the ALENEX'19 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time complexity of support vector machines (SVMs) prohibits training on\nhuge data sets with millions of data points. Recently, multilevel approaches to\ntrain SVMs have been developed to allow for time-efficient training on huge\ndata sets. While regular SVMs perform the entire training in one -- time\nconsuming -- optimization step, multilevel SVMs first build a hierarchy of\nproblems decreasing in size that resemble the original problem and then train\nan SVM model for each hierarchy level, benefiting from the solved models of\nprevious levels. We present a faster multilevel support vector machine that\nuses a label propagation algorithm to construct the problem hierarchy.\nExtensive experiments indicate that our approach is up to orders of magnitude\nfaster than the previous fastest algorithm while having comparable\nclassification quality. For example, already one of our sequential solvers is\non average a factor 15 faster than the parallel ThunderSVM algorithm, while\nhaving similar classification quality.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 11:38:46 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 13:21:23 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 10:41:44 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Schlag", "Sebastian", ""], ["Schmitt", "Matthias", ""], ["Schulz", "Christian", ""]]}, {"id": "1808.06411", "submitter": "Christian Schulz", "authors": "Sebastian Schlag, Christian Schulz, Daniel Seemaier, Darren Strash", "title": "Scalable Edge Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-centric distributed computations have appeared as a recent technique to\nimprove the shortcomings of think-like-a-vertex algorithms on large scale-free\nnetworks. In order to increase parallelism on this model, edge partitioning -\npartitioning edges into roughly equally sized blocks - has emerged as an\nalternative to traditional (node-based) graph partitioning. In this work, we\ngive a distributed memory parallel algorithm to compute high-quality edge\npartitions in a scalable way. Our algorithm scales to networks with billions of\nedges, and runs efficiently on thousands of PEs. Our technique is based on a\nfast parallelization of split graph construction and a use of advanced node\npartitioning algorithms. Our extensive experiments show that our algorithm has\nhigh quality on large real-world networks and large hyperbolic random graphs,\nwhich have a power law degree distribution and are therefore specifically\ntargeted by edge partitioning\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 12:09:31 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 13:14:42 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Schlag", "Sebastian", ""], ["Schulz", "Christian", ""], ["Seemaier", "Daniel", ""], ["Strash", "Darren", ""]]}, {"id": "1808.06651", "submitter": "Ilya Mironov", "authors": "Vitaly Feldman, Ilya Mironov, Kunal Talwar, Abhradeep Thakurta", "title": "Privacy Amplification by Iteration", "comments": "Extended abstract appears in Foundations of Computer Science (FOCS)\n  2018", "journal-ref": null, "doi": "10.1109/FOCS.2018.00056", "report-no": null, "categories": "cs.LG cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many commonly used learning algorithms work by iteratively updating an\nintermediate solution using one or a few data points in each iteration.\nAnalysis of differential privacy for such algorithms often involves ensuring\nprivacy of each step and then reasoning about the cumulative privacy cost of\nthe algorithm. This is enabled by composition theorems for differential privacy\nthat allow releasing of all the intermediate results. In this work, we\ndemonstrate that for contractive iterations, not releasing the intermediate\nresults strongly amplifies the privacy guarantees.\n  We describe several applications of this new analysis technique to solving\nconvex optimization problems via noisy stochastic gradient descent. For\nexample, we demonstrate that a relatively small number of non-private data\npoints from the same distribution can be used to close the gap between private\nand non-private convex optimization. In addition, we demonstrate that we can\nachieve guarantees similar to those obtainable using the\nprivacy-amplification-by-sampling technique in several natural settings where\nthat technique cannot be applied.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 18:49:32 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 23:43:40 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Feldman", "Vitaly", ""], ["Mironov", "Ilya", ""], ["Talwar", "Kunal", ""], ["Thakurta", "Abhradeep", ""]]}, {"id": "1808.06705", "submitter": "Paul Burkhardt", "authors": "Paul Burkhardt", "title": "Graph connectivity in log steps using label propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fastest deterministic algorithms for connected components take\nlogarithmic time and perform superlinear work on a Parallel Random Access\nMachine (PRAM). These algorithms maintain a spanning forest by merging and\ncompressing trees, which requires pointer-chasing operations that increase\nmemory access latency and are limited to shared-memory systems. Many of these\nPRAM algorithms are also very complicated to implement. Another popular method\nis \"leader-contraction\" where the challenge is to select a constant fraction of\nleaders that are adjacent to a constant fraction of non-leaders with high\nprobability. Instead we investigate label propagation because it is\ndeterministic and does not rely on pointer-chasing. Label propagation exchanges\nrepresentative labels within a component using simple graph traversal, but it\nis inherently difficult to complete in a sublinear number of steps. We are able\nto solve the problems with label propagation for graph connectivity.\n  We introduce a surprisingly simple framework for deterministic graph\nconnectivity using label propagation that is easily adaptable to many\ncomputational models. It propagates directed edges and alternates edge\ndirection to achieve linear edge count each step and sublinear convergence. We\npresent new algorithms in PRAM, Stream, and MapReduce for a simple, undirected\ngraph $G=(V,E)$ with $n=|V|$ vertices, $m=|E|$ edges. Our approach takes $O(m)$\nwork each step, but we can only prove logarithmic convergence on a path graph.\nIt was conjectured by Liu and Tarjan (2019) to take $O(\\log n)$ steps or\npossibly $O(\\log^2 n)$ steps. We leave the proof of convergence as an open\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 22:01:08 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 17:09:03 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 13:23:35 GMT"}, {"version": "v4", "created": "Mon, 3 May 2021 17:14:58 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Burkhardt", "Paul", ""]]}, {"id": "1808.06900", "submitter": "Matthias Brust R.", "authors": "Matthias R. Brust, Gr\\'egoire Danoy, Pascal Bouvry, Dren Gashi,\n  Himadri Pathak, Mike P. Gon\\c{c}alves", "title": "Defending against Intrusion of Malicious UAVs with Networked UAV Defense\n  Swarms", "comments": "IEEE Conference on Local Computer Networks (LCN), 2017", "journal-ref": null, "doi": "10.1109/LCN.Workshops.2017.71", "report-no": null, "categories": "cs.NI cs.DS cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, companies such as Amazon, Alibaba, and even pizza chains are\npushing forward to use drones, also called UAVs (Unmanned Aerial Vehicles), for\nservice provision, such as package and food delivery. As governments intend to\nuse these immense economic benefits that UAVs have to offer, urban planners are\nmoving forward to incorporate so-called UAV flight zones and UAV highways in\ntheir smart city designs. However, the high-speed mobility and behavior\ndynamics of UAVs need to be monitored to detect and, subsequently, to deal with\nintruders, rogue drones, and UAVs with a malicious intent. This paper proposes\na UAV defense system for the purpose of intercepting and escorting a malicious\nUAV outside the flight zone. The proposed UAV defense system consists of a\ndefense UAV swarm, which is capable to self-organize its defense formation in\nthe event of intruder detection, and chase the malicious UAV as a networked\nswarm. Modular design principles have been used for our fully localized\napproach. We developed an innovative auto-balanced clustering process to\nrealize the intercept- and capture-formation. As it turned out, the resulting\nnetworked defense UAV swarm is resilient against communication losses. Finally,\na prototype UAV simulator has been implemented. Through extensive simulations,\nwe show the feasibility and performance of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 13:56:02 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 14:10:04 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Brust", "Matthias R.", ""], ["Danoy", "Gr\u00e9goire", ""], ["Bouvry", "Pascal", ""], ["Gashi", "Dren", ""], ["Pathak", "Himadri", ""], ["Gon\u00e7alves", "Mike P.", ""]]}, {"id": "1808.06932", "submitter": "Matthew Fahrbach", "authors": "Matthew Fahrbach, Vahab Mirrokni, Morteza Zadimoghaddam", "title": "Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and\n  Query Complexity", "comments": "12 pages, 8 figures", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning (ICML 2019)", "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular maximization is a general optimization problem with a wide range\nof applications in machine learning (e.g., active learning, clustering, and\nfeature selection). In large-scale optimization, the parallel running time of\nan algorithm is governed by its adaptivity, which measures the number of\nsequential rounds needed if the algorithm can execute polynomially-many\nindependent oracle queries in parallel. While low adaptivity is ideal, it is\nnot sufficient for an algorithm to be efficient in practice---there are many\napplications of distributed submodular optimization where the number of\nfunction evaluations becomes prohibitively expensive. Motivated by these\napplications, we study the adaptivity and query complexity of submodular\nmaximization. In this paper, we give the first constant-factor approximation\nalgorithm for maximizing a non-monotone submodular function subject to a\ncardinality constraint $k$ that runs in $O(\\log(n))$ adaptive rounds and makes\n$O(n \\log(k))$ oracle queries in expectation. In our empirical study, we use\nthree real-world applications to compare our algorithm with several benchmarks\nfor non-monotone submodular maximization. The results demonstrate that our\nalgorithm finds competitive solutions using significantly fewer rounds and\nqueries.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 12:04:17 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 14:43:10 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Mirrokni", "Vahab", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1808.06954", "submitter": "Robert Ganian", "authors": "Robert Ganian, Sebastian Ordyniak, C. S. Rahul", "title": "Group Activity Selection with Few Agent Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Group Activity Selection Problem (GASP) models situations where a group\nof agents needs to be distributed to a set of activities while taking into\naccount preferences of the agents w.r.t. individual activities and activity\nsizes. The problem, along with its two previously proposed variants sGASP and\ngGASP, has been studied in the parameterized complexity setting with various\nparameterizations, such as number of agents, number of activities and solution\nsize. However, the complexity of the problem parameterized by the number of\ntypes of agents, a parameter motivated and proposed already in the paper that\nintroduced GASP, has so far remained open.\n  In this paper we establish the complexity map for GASP, sGASP and gGASP when\nthe number of types of agents is the parameter. Our positive results,\nconsisting of one fixed-parameter algorithm and one XP algorithm, rely on a\ncombination of novel Subset Sum machinery (which may be of general interest)\nand identifying certain compression steps which allow us to focus on solutions\nwhich are \"acyclic\". These algorithms are complemented by matching lower\nbounds, which among others answer an open question of Gupta, Roy, Saurabh and\nZehavi (2017). In this direction, the techniques used to establish\nW[1]-hardness of sGASP are of particular interest: as an intermediate step, we\nuse Sidon sequences to show the W[1]-hardness of a highly restricted variant of\nmulti-dimensional Subset Sum, which may find applications in other settings as\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 15:02:52 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Ganian", "Robert", ""], ["Ordyniak", "Sebastian", ""], ["Rahul", "C. S.", ""]]}, {"id": "1808.06958", "submitter": "Farzane Yahyanejad", "authors": "Farzane Yahyanejad, Bahram Sadeghi Bigham", "title": "Greedy Harmony Search Algorithm for the Hop Constrained Connected\n  Facility Location", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, robust and efficient harmony search algorithm for the\nHop Constrained Connected Facility Location problem (HCConFL). The HCConFL\nproblem is NP-hard that models the design of data-management and\ntelecommunication networks in a manner of reliability. In this paper, we\ncustomize harmony search algorithm to solve the HCConFL problem. To arrive to\nquick, optimal cost of each solution, we use a new greedy approach expanding\nidea of Kruskal algorithm in our objective function. We also use a new greedy\nmethod combined with harmony search to obtain a good approximation in an\nefficient computational time. The algorithm was evaluated on the standard OR\nLibrary benchmarks. Computational results show that with high frequencies the\nmodified harmony search algorithm produces optimal solutions to all benchmarks\nvery quickly. We also solve the problem with another heuristic algorithm\nincluding the variable neighborhood search, the tabu search, to evaluate our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 15:06:55 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 16:04:04 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 15:29:54 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Yahyanejad", "Farzane", ""], ["Bigham", "Bahram Sadeghi", ""]]}, {"id": "1808.06981", "submitter": "Farzane Yahyanejad", "authors": "Farzane Yahyanejad, Bahram Sadeghi Bigham", "title": "Iterated Greedy Algorithms for the Hop-Constrained Steiner Tree Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hop-Constrained Steiner Tree problem (HCST) is challenging NP-hard\nproblem arising in the design of centralized telecommunication networks where\nthe reliability constraints matter. In this paper three iterative greedy\nalgorithms are described to find efficient optimized solution to solve HCST on\nboth sparse and dense graphs. In the third algorithm, we adopt the idea of\nKruskal algorithm for the HCST problem to reach a better solution. This is the\nfirst time such algorithm is utilized in a problem with hop-constrained\ncondition. Computational results on a number of problem instances are derived\nfrom well-known benchmark instances of Steiner problem in graphs. We compare\nthree algorithms with a previously known method (Voss's algorithm) in term of\neffectiveness, and show that the cost of the third proposed method has been\nnoticeably improved significantly, 34.60% in hop 10 on dense graphs and 3.34%\nin hop 3 on sparse graphs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 15:57:31 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Yahyanejad", "Farzane", ""], ["Bigham", "Bahram Sadeghi", ""]]}, {"id": "1808.07226", "submitter": "Vishesh Jain", "authors": "Vishesh Jain, Frederic Koehler, Andrej Risteski", "title": "Mean-field approximation, convex hierarchies, and the optimality of\n  correlation rounding: a unified perspective", "comments": "This version: minor formatting changes, added grant acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The free energy is a key quantity of interest in Ising models, but\nunfortunately, computing it in general is computationally intractable. Two\npopular (variational) approximation schemes for estimating the free energy of\ngeneral Ising models (in particular, even in regimes where correlation decay\ndoes not hold) are: (i) the mean-field approximation with roots in statistical\nphysics, which estimates the free energy from below, and (ii) hierarchies of\nconvex relaxations with roots in theoretical computer science, which estimate\nthe free energy from above. We show, surprisingly, that the tight regime for\nboth methods to compute the free energy to leading order is identical.\n  More precisely, we show that the mean-field approximation is within\n$O((n\\|J\\|_{F})^{2/3})$ of the free energy, where $\\|J\\|_F$ denotes the\nFrobenius norm of the interaction matrix of the Ising model. This\nsimultaneously subsumes both the breakthrough work of Basak and Mukherjee, who\nshowed the tight result that the mean-field approximation is within $o(n)$\nwhenever $\\|J\\|_{F} = o(\\sqrt{n})$, as well as the work of Jain, Koehler, and\nMossel, who gave the previously best known non-asymptotic bound of\n$O((n\\|J\\|_{F})^{2/3}\\log^{1/3}(n\\|J\\|_{F}))$. We give a simple, algorithmic\nproof of this result using a convex relaxation proposed by Risteski based on\nthe Sherali-Adams hierarchy, automatically giving sub-exponential time\napproximation schemes for the free energy in this entire regime. Our\nalgorithmic result is tight under Gap-ETH.\n  We furthermore combine our techniques with spin glass theory to prove (in a\nstrong sense) the optimality of correlation rounding, refuting a recent\nconjecture of Allen, O'Donnell, and Zhou. Finally, we give the tight\ngeneralization of all of these results to $k$-MRFs, capturing as a special case\nprevious work on approximating MAX-$k$-CSP.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 05:43:16 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 02:06:06 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Jain", "Vishesh", ""], ["Koehler", "Frederic", ""], ["Risteski", "Andrej", ""]]}, {"id": "1808.07536", "submitter": "Chao Tian", "authors": "Chao Tian, Hua Sun, Jun Chen", "title": "Capacity-Achieving Private Information Retrieval Codes with Optimal\n  Message Size and Upload Cost", "comments": "Revised with more examples to improve the readability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new capacity-achieving code for the private information\nretrieval (PIR) problem, and show that it has the minimum message size (being\none less than the number of servers) and the minimum upload cost (being roughly\nlinear in the number of messages) among a general class of capacity-achieving\ncodes, and in particular, among all capacity-achieving linear codes. Different\nfrom existing code constructions, the proposed code is asymmetric, and this\nasymmetry appears to be the key factor leading to the optimal message size and\nthe optimal upload cost. The converse results on the message size and the\nupload cost are obtained by a strategic analysis of the information theoretic\nproof of the PIR capacity, from which a set of critical properties of any\ncapacity-achieving code in the code class of interest is extracted. The\nsymmetry structure of the PIR problem is then analyzed, which allows us to\nconstruct symmetric codes from asymmetric ones, yielding a meaningful bridge\nbetween the proposed code and existing ones in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 19:34:22 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 23:13:12 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Tian", "Chao", ""], ["Sun", "Hua", ""], ["Chen", "Jun", ""]]}, {"id": "1808.07957", "submitter": "Ilya Volkovich", "authors": "Bruno Grenet and Ilya Volkovich", "title": "One (more) line on the most Ancient Algorithm in History", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new simple and short (\"one-line\") analysis for the runtime of the\nwell-known Euclidean Algorithm. While very short simple, the obtained upper\nbound in near-optimal.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 21:44:37 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 20:06:00 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Grenet", "Bruno", ""], ["Volkovich", "Ilya", ""]]}, {"id": "1808.08376", "submitter": "Antoine Genitrini", "authors": "Olivier Bodini and Antoine Genitrini and Mehdi Naima", "title": "Ranked Schr\\\"oder Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biology, a phylogenetic tree is a tool to represent the evolutionary\nrelationship between species. Unfortunately, the classical Schr\\\"oder tree\nmodel is not adapted to take into account the chronology between the branching\nnodes. In particular, it does not answer the question: how many different\nphylogenetic stories lead to the creation of n species and what is the average\ntime to get there? In this paper, we enrich this model in two distinct ways in\norder to obtain two ranked tree models for phylogenetics, i.e. models coding\nchronology. For that purpose, we first develop a model of (strongly) increasing\nSchr\\\"oder trees, symbolically described in the classical context of increasing\nlabeling. Then we introduce a generalization for the labeling with some unusual\norder constraint in Analytic Combinatorics (namely the weakly increasing\ntrees). Although these models are direct extensions of the Schr\\\"oder tree\nmodel, it appears that they are also in one-to-one correspondence with several\nclassical combinatorial objects. Through the paper, we present these links,\nexhibit some parameters in typical large trees and conclude the studies with\nefficient uniform samplers.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 08:31:51 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 12:07:22 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2018 20:58:16 GMT"}, {"version": "v4", "created": "Sat, 5 Jan 2019 22:02:11 GMT"}, {"version": "v5", "created": "Mon, 14 Jan 2019 11:23:57 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Bodini", "Olivier", ""], ["Genitrini", "Antoine", ""], ["Naima", "Mehdi", ""]]}, {"id": "1808.08419", "submitter": "Manuela Fischer", "authors": "Yi-Jun Chang, Manuela Fischer, Mohsen Ghaffari, Jara Uitto, Yufan\n  Zheng", "title": "The Complexity of $(\\Delta + 1)$Coloring inCongested Clique, Massively\n  Parallel Computation,and Centralized Local Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new randomized algorithms that improve the complexity of the\nclassic $(\\Delta+1)$-coloring problem, and its generalization\n$(\\Delta+1)$-list-coloring, in three well-studied models of distributed,\nparallel, and centralized computation:\n  Distributed Congested Clique: We present an $O(1)$-round randomized algorithm\nfor $(\\Delta+1)$-list coloring in the congested clique model of distributed\ncomputing. This settles the asymptotic complexity of this problem. It moreover\nimproves upon the $O(\\log^\\ast \\Delta)$-round randomized algorithms of Parter\nand Su [DISC'18] and $O((\\log\\log \\Delta)\\cdot \\log^\\ast \\Delta)$-round\nrandomized algorithm of Parter [ICALP'18].\n  Massively Parallel Computation: We present a $(\\Delta+1)$-list coloring\nalgorithm with round complexity $O(\\sqrt{\\log\\log n})$ in the Massively\nParallel Computation (MPC) model with strongly sublinear memory per machine.\nThis algorithm uses a memory of $O(n^{\\alpha})$ per machine, for any desirable\nconstant $\\alpha>0$, and a total memory of $\\widetilde{O}(m)$, where $m$ is the\nsize of the graph. Notably, this is the first coloring algorithm with\nsublogarithmic round complexity, in the sublinear memory regime of MPC. For the\nquasilinear memory regime of MPC, an $O(1)$-round algorithm was given very\nrecently by Assadi et al. [SODA'19].\n  Centralized Local Computation: We show that $(\\Delta+1)$-list coloring can be\nsolved with $\\Delta^{O(1)} \\cdot O(\\log n)$ query complexity, in the\ncentralized local computation model. The previous state-of-the-art for\n$(\\Delta+1)$-list coloring in the centralized local computation model are based\non simulation of known LOCAL algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 12:17:41 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 08:17:58 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2018 17:04:10 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Fischer", "Manuela", ""], ["Ghaffari", "Mohsen", ""], ["Uitto", "Jara", ""], ["Zheng", "Yufan", ""]]}, {"id": "1808.08494", "submitter": "Nicole Wein", "authors": "Arturs Backurs, Liam Roditty, Gilad Segal, Virginia Vassilevska\n  Williams, Nicole Wein", "title": "Towards Tight Approximation Bounds for Graph Diameter and Eccentricities", "comments": "Revised to implement referee comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the most important graph parameters is the Diameter, the largest\ndistance between any two vertices. There are no known very efficient algorithms\nfor computing the Diameter exactly. Thus, much research has been devoted to how\nfast this parameter can be approximated. Chechik et al. showed that the\ndiameter can be approximated within a multiplicative factor of $3/2$ in\n$\\tilde{O}(m^{3/2})$ time. Furthermore, Roditty and Vassilevska W. showed that\nunless the Strong Exponential Time Hypothesis (SETH) fails, no\n$O(n^{2-\\epsilon})$ time algorithm can achieve an approximation factor better\nthan $3/2$ in sparse graphs. Thus the above algorithm is essentially optimal\nfor sparse graphs for approximation factors less than $3/2$. It was, however,\ncompletely plausible that a $3/2$-approximation is possible in linear time. In\nthis work we conditionally rule out such a possibility by showing that unless\nSETH fails no $O(m^{3/2-\\epsilon})$ time algorithm can achieve an approximation\nfactor better than $5/3$.\n  Another fundamental set of graph parameters are the Eccentricities. The\nEccentricity of a vertex $v$ is the distance between $v$ and the farthest\nvertex from $v$. Chechik et al. showed that the Eccentricities of all vertices\ncan be approximated within a factor of $5/3$ in $\\tilde{O}(m^{3/2})$ time and\nAbboud et al. showed that no $O(n^{2-\\epsilon})$ algorithm can achieve better\nthan $5/3$ approximation in sparse graphs. We show that the runtime of the\n$5/3$ approximation algorithm is also optimal under SETH. We also show that no\nnear-linear time algorithm can achieve a better than $2$ approximation for the\nEccentricities and that this is essentially tight: we give an algorithm that\napproximates Eccentricities within a $2+\\delta$ factor in $\\tilde{O}(m/\\delta)$\ntime for any $0<\\delta<1$. This beats all Eccentricity algorithms in Cairo et\nal.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 01:31:57 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 18:32:37 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Backurs", "Arturs", ""], ["Roditty", "Liam", ""], ["Segal", "Gilad", ""], ["Williams", "Virginia Vassilevska", ""], ["Wein", "Nicole", ""]]}, {"id": "1808.08686", "submitter": "Glenn Galvizo Jr", "authors": "Glenn Galvizo, Lipyeow Lim", "title": "Empirical Analysis of Common Subgraph Isomorphism Approaches to the\n  Lost-in-Space Star Identification Problem", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The process of identifying stars is integral toward stellar based orientation\ndetermination in spacecraft. Star identification involves matching points in an\nimage of the sky with stars in an astronomical catalog. A unified framework for\nidentification was created and used to analyze six variations of methods based\non their approach to star set identification, obtaining a single image to\ncatalog star set match, and uniquely mapping each star in a image star set to a\ncatalog star set. Each method was presented an artificial image, and aspects\nthat were interchangeable among each process were normalized. Given an image\nwith false stars, the Pyramid method has the highest average accuracy and is\nthe fastest of the six. Given an image where each star's true position is\ndistributed randomly (Gaussian noise), the Spherical Triangle method's accuracy\nis the least sensitive.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 04:45:29 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Galvizo", "Glenn", ""], ["Lim", "Lipyeow", ""]]}, {"id": "1808.08691", "submitter": "Alantha Newman", "authors": "Adrien Argento and Pierre Charbit and Alantha Newman", "title": "Explicit 3-colorings for exponential graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph $H$ and integer $k \\geq 1$, two functions $f, g$ from $V(H)$ into\n$\\{1, \\dots, k\\}$ are adjacent if for all edges $uv$ of $H$, $f(u) \\neq g(v)$.\nThe graph of all such functions is the exponential graph $K_k^H$. El-Zahar and\nSauer proved that if $\\chi(H) \\geq 4$, then $K_3^H$ is 3-chromatic. Tardif\nshowed that, implicit in their proof, is an algorithm for 3-coloring $K_3^H$\nwhose time complexity is polynomial in the size of $K_3^H$. Tardif then asked\nif there is an \"explicit\" algorithm for finding such a coloring: Essentially,\ngiven a function $f$ belonging to a 3-chromatic component of $K_3^H$, can we\nassign a color to this vertex in time polynomial in the size of $H$? The main\nresult of this paper is to present such an algorithm, answering Tardif's\nquestion affirmatively. Our algorithm yields an alternative proof of the\ntheorem of El-Zahar and Sauer that the categorical product of two 4-chromatic\ngraphs is 4-chromatic.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 05:40:01 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 19:43:58 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Argento", "Adrien", ""], ["Charbit", "Pierre", ""], ["Newman", "Alantha", ""]]}, {"id": "1808.08772", "submitter": "Manuel Sorge", "authors": "Iyad Kanj, Christian Komusiewicz, Manuel Sorge, Erik Jan van Leeuwen", "title": "Solving Partition Problems Almost Always Requires Pushing Many Vertices\n  Around", "comments": "Full version of the corresponding article in the Proceedings of the\n  26th Annual European Symposium on Algorithms (ESA '18), 35 pages, 7 figures", "journal-ref": null, "doi": "10.4230/LIPIcs.ESA.2018.51", "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental graph problem is to recognize whether the vertex set of a graph\n$G$ can be bipartitioned into sets $A$ and $B$ such that $G[A]$ and $G[B]$\nsatisfy properties $\\Pi_A$ and $\\Pi_B$, respectively. This so-called\n$(\\Pi_A,\\Pi_B)$-Recognition problem generalizes amongst others the recognition\nof $3$-colorable, bipartite, split, and monopolar graphs. In this paper, we\nstudy whether certain fixed-parameter tractable $(\\Pi_A,\\Pi_B)$-Recognition\nproblems admit polynomial kernels. In our study, we focus on the first level\nabove triviality, where $\\Pi_A$ is the set of $P_3$-free graphs (disjoint\nunions of cliques, or cluster graphs), the parameter is the number of clusters\nin the cluster graph $G[A]$, and $\\Pi_B$ is characterized by a set\n$\\mathcal{H}$ of connected forbidden induced subgraphs. We prove that, under\nthe assumption that NP is not a subset of coNP/poly,\n\\textsc{$(\\Pi_A,\\Pi_B)$-Recognition} admits a polynomial kernel if and only if\n$\\mathcal{H}$ contains a graph with at most $2$ vertices. In both the\nkernelization and the lower bound results, we exploit the properties of a\npushing process, which is an algorithmic technique used recently by Heggerness\net al. and by Kanj et al. to obtain fixed-parameter algorithms for many cases\nof $(\\Pi_A,\\Pi_B)$-Recognition, as well as several other problems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 10:34:22 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 18:07:27 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kanj", "Iyad", ""], ["Komusiewicz", "Christian", ""], ["Sorge", "Manuel", ""], ["van Leeuwen", "Erik Jan", ""]]}, {"id": "1808.08817", "submitter": "Martin Milani\\v{c}", "authors": "Ademir Hujdurovi\\'c, Martin Milani\\v{c}, Bernard Ries", "title": "Detecting strong cliques", "comments": "arXiv admin note: text overlap with arXiv:1609.06961", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A strong clique in a graph is a clique intersecting every maximal independent\nset. We study the computational complexity of six algorithmic decision problems\nrelated to strong cliques in graphs and almost completely determine their\ncomplexity in the classes of chordal graphs, weakly chordal graphs, line graphs\nand their complements, and graphs of maximum degree at most three. Our results\nrely on connections with matchings and relate to several graph properties\nstudied in the literature, including well-covered graphs, localizable graphs,\nand general partition graphs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 16:09:02 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Hujdurovi\u0107", "Ademir", ""], ["Milani\u010d", "Martin", ""], ["Ries", "Bernard", ""]]}, {"id": "1808.08905", "submitter": "Soledad Villar", "authors": "Richard Kueng and Dustin G. Mixon and Soledad Villar", "title": "Fair redistricting is hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gerrymandering is a long-standing issue within the U.S. political system, and\nit has received scrutiny recently by the U.S. Supreme Court. In this note, we\nprove that deciding whether there exists a fair redistricting among legal maps\nis NP-hard. To make this precise, we use simplified notions of \"legal\" and\n\"fair\" that account for desirable traits such as geographic compactness of\ndistricts and sufficient representation of voters. The proof of our result is\ninspired by the work of Mahanjan, Minbhorkar and Varadarajan that proves that\nplanar k-means is NP-hard.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 16:17:25 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Kueng", "Richard", ""], ["Mixon", "Dustin G.", ""], ["Villar", "Soledad", ""]]}, {"id": "1808.08925", "submitter": "Alessandra Tappini", "authors": "Patrizio Angelini, Peter Eades, Seok-Hee Hong, Karsten Klein, Stephen\n  Kobourov, Giuseppe Liotta, Alfredo Navarra, Alessandra Tappini", "title": "Turning Cliques into Paths to Achieve Planarity", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by hybrid graph representations, we introduce and study the\nfollowing beyond-planarity problem, which we call $h$-Clique2Path Planarity:\nGiven a graph $G$, whose vertices are partitioned into subsets of size at most\n$h$, each inducing a clique, remove edges from each clique so that the subgraph\ninduced by each subset is a path, in such a way that the resulting subgraph of\n$G$ is planar. We study this problem when $G$ is a simple topological graph,\nand establish its complexity in relation to $k$-planarity. We prove that\n$h$-Clique2Path Planarity is NP-complete even when $h=4$ and $G$ is a simple\n$3$-plane graph, while it can be solved in linear time, for any $h$, when $G$\nis $1$-plane.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 16:55:54 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 12:57:26 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Angelini", "Patrizio", ""], ["Eades", "Peter", ""], ["Hong", "Seok-Hee", ""], ["Klein", "Karsten", ""], ["Kobourov", "Stephen", ""], ["Liotta", "Giuseppe", ""], ["Navarra", "Alfredo", ""], ["Tappini", "Alessandra", ""]]}, {"id": "1808.09117", "submitter": "Harshita Kona", "authors": "Kona Harshita and N. Sadagopan", "title": "On Some Combinatorial Problems in Cographs", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of graphs that can be constructed from isolated vertices by\ndisjoint union and graph join operations are called cographs. These graphs can\nbe represented in a tree-like representation termed parse tree or cotree. In\nthis paper, we study some popular combinatorial problems restricted to\ncographs. We first present a structural characterization of minimal vertex\nseparators in cographs. Further, we show that listing all minimal vertex\nseparators and the complexity of some constrained vertex separators are\npolynomial-time solvable in cographs. We propose polynomial-time algorithms for\nconnectivity augmentation problems and its variants in cographs, preserving the\ncograph property. Finally, using the dynamic programming paradigm, we present a\ngeneric framework to solve classical optimization problems such as the longest\npath, the Steiner path and the minimum leaf spanning tree problems restricted\nto cographs, our framework yields polynomial-time algorithms for all three\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 04:50:49 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Harshita", "Kona", ""], ["Sadagopan", "N.", ""]]}, {"id": "1808.09266", "submitter": "Anupam Prakash", "authors": "Iordanis Kerenidis and Anupam Prakash", "title": "A Quantum Interior Point Method for LPs and SDPs", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a quantum interior point method with worst case running time\n$\\widetilde{O}(\\frac{n^{2.5}}{\\xi^{2}} \\mu \\kappa^3 \\log (1/\\epsilon))$ for\nSDPs and $\\widetilde{O}(\\frac{n^{1.5}}{\\xi^{2}} \\mu \\kappa^3 \\log\n(1/\\epsilon))$ for LPs, where the output of our algorithm is a pair of matrices\n$(S,Y)$ that are $\\epsilon$-optimal $\\xi$-approximate SDP solutions. The factor\n$\\mu$ is at most $\\sqrt{2}n$ for SDPs and $\\sqrt{2n}$ for LP's, and $\\kappa$ is\nan upper bound on the condition number of the intermediate solution matrices.\nFor the case where the intermediate matrices for the interior point method are\nwell conditioned, our method provides a polynomial speedup over the best known\nclassical SDP solvers and interior point based LP solvers, which have a worst\ncase running time of $O(n^{6})$ and $O(n^{3.5})$ respectively. Our results\nbuild upon recently developed techniques for quantum linear algebra and pave\nthe way for the development of quantum algorithms for a variety of applications\nin optimization and machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 13:10:03 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Kerenidis", "Iordanis", ""], ["Prakash", "Anupam", ""]]}, {"id": "1808.09317", "submitter": "Polina Rozenshtein", "authors": "Polina Rozenshtein, Francesco Bonchi, Aristides Gionis, Mauro Sozio\n  and Nikolaj Tatti", "title": "Finding events in temporal networks: Segmentation meets densest-subgraph\n  discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of discovering a timeline of events in a\ntemporal network. We model events as dense subgraphs that occur within\nintervals of network activity. We formulate the event-discovery task as an\noptimization problem, where we search for a partition of the network timeline\ninto k non-overlapping intervals, such that the intervals span subgraphs with\nmaximum total density. The output is a sequence of dense subgraphs along with\ncorresponding time intervals, capturing the most interesting events during the\nnetwork lifetime.\n  A naive solution to our optimization problem has polynomial but prohibitively\nhigh running time complexity. We adapt existing recent work on dynamic\ndensest-subgraph discovery and approximate dynamic programming to design a fast\napproximation algorithm. Next, to ensure richer structure, we adjust the\nproblem formulation to encourage coverage of a larger set of nodes. This\nproblem is NP-hard even for static graphs. However, on static graphs a simple\ngreedy algorithm leads to approximate solution due to submodularity. We\nextended this greedy approach for the case of temporal networks. However, the\napproximation guarantee does not hold. Nevertheless, according to the\nexperiments, the algorithm finds good quality solutions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 14:15:24 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 20:04:37 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Rozenshtein", "Polina", ""], ["Bonchi", "Francesco", ""], ["Gionis", "Aristides", ""], ["Sozio", "Mauro", ""], ["Tatti", "Nikolaj", ""]]}, {"id": "1808.09363", "submitter": "Wei Chen", "authors": "Wei Chen", "title": "An Issue in the Martingale Analysis of the Influence Maximization\n  Algorithm IMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explains a subtle issue in the martingale analysis of the IMM\nalgorithm, a state-of-the-art influence maximization algorithm. Two workarounds\nare proposed to fix the issue, both requiring minor changes on the algorithm\nand incurring a slight penalty on the running time of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 02:59:03 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 05:41:58 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 08:57:35 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Chen", "Wei", ""]]}, {"id": "1808.09376", "submitter": "Alain Barrat", "authors": "Edoardo Galimberti, Alain Barrat, Francesco Bonchi, Ciro Cattuto,\n  Francesco Gullo", "title": "Mining (maximal) span-cores from temporal networks", "comments": null, "journal-ref": "CIKM 2018, October 22-26, 2018, Torino, Italy", "doi": "10.1145/3269206.3271767", "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing temporal networks, a fundamental task is the identification of\ndense structures (i.e., groups of vertices that exhibit a large number of\nlinks), together with their temporal span (i.e., the period of time for which\nthe high density holds). We tackle this task by introducing a notion of\ntemporal core decomposition where each core is associated with its span: we\ncall such cores span-cores.\n  As the total number of time intervals is quadratic in the size of the\ntemporal domain $T$ under analysis, the total number of span-cores is quadratic\nin $|T|$ as well. Our first contribution is an algorithm that, by exploiting\ncontainment properties among span-cores, computes all the span-cores\nefficiently. Then, we focus on the problem of finding only the maximal\nspan-cores, i.e., span-cores that are not dominated by any other span-core by\nboth the coreness property and the span. We devise a very efficient algorithm\nthat exploits theoretical findings on the maximality condition to directly\ncompute the maximal ones without computing all span-cores.\n  Experimentation on several real-world temporal networks confirms the\nefficiency and scalability of our methods. Applications on temporal networks,\ngathered by a proximity-sensing infrastructure recording face-to-face\ninteractions in schools, highlight the relevance of the notion of (maximal)\nspan-core in analyzing social dynamics and detecting/correcting anomalies in\nthe data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 15:56:54 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Galimberti", "Edoardo", ""], ["Barrat", "Alain", ""], ["Bonchi", "Francesco", ""], ["Cattuto", "Ciro", ""], ["Gullo", "Francesco", ""]]}, {"id": "1808.09410", "submitter": "Luca Elias Sch\\\"afer", "authors": "Luca E. Sch\\\"afer, Tobias Dietz, Nicolas Fr\\\"ohlich, Stefan Ruzika,\n  Jos\\'e Rui Figueira", "title": "Shortest Paths with Ordinal Weights", "comments": "24 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the single-source-single-destination \"shortest\" paths problem\nin acyclic graphs with ordinal weighted arc costs. We define the concepts of\nordinal dominance and efficiency for paths and their associated ordinal levels,\nrespectively. Further, we show that the number of ordinally non-dominated paths\nvectors from the source node to every other node in the graph is polynomially\nbounded and we propose a polynomial time labeling algorithm for solving the\nproblem of finding the set of ordinally non-dominated paths vectors from source\nto sink.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 16:59:23 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 13:32:20 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Sch\u00e4fer", "Luca E.", ""], ["Dietz", "Tobias", ""], ["Fr\u00f6hlich", "Nicolas", ""], ["Ruzika", "Stefan", ""], ["Figueira", "Jos\u00e9 Rui", ""]]}, {"id": "1808.09531", "submitter": "Seyed-Vahid Sanei-Mehri", "authors": "Seyed-Vahid Sanei-Mehri, Apurba Das, Srikanta Tirthapura", "title": "Enumerating Top-k Quasi-Cliques", "comments": "10 pages", "journal-ref": "2018 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData.2018.8622352", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-cliques are dense incomplete subgraphs of a graph that generalize the\nnotion of cliques. Enumerating quasi-cliques from a graph is a robust way to\ndetect densely connected structures with applications to bio-informatics and\nsocial network analysis. However, enumerating quasi-cliques in a graph is a\nchallenging problem, even harder than the problem of enumerating cliques. We\nconsider the enumeration of top-k degree-based quasi-cliques, and make the\nfollowing contributions: (1) We show that even the problem of detecting if a\ngiven quasi-clique is maximal (i.e. not contained within another quasi-clique)\nis NP-hard (2) We present a novel heuristic algorithm KernelQC to enumerate the\nk largest quasi-cliques in a graph. Our method is based on identifying kernels\nof extremely dense subgraphs within a graph, following by growing subgraphs\naround these kernels, to arrive at quasi-cliques with the required densities\n(3) Experimental results show that our algorithm accurately enumerates\nquasi-cliques from a graph, is much faster than current state-of-the-art\nmethods for quasi-clique enumeration (often more than three orders of magnitude\nfaster), and can scale to larger graphs than current methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 20:31:42 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Sanei-Mehri", "Seyed-Vahid", ""], ["Das", "Apurba", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1808.09548", "submitter": "Mark Jerrum", "authors": "Heng Guo and Mark Jerrum", "title": "Approximately counting bases of bicircular matroids", "comments": "v3: updated introduction and a slightly better run-time bound. v4:\n  minor revisions; this version is accepted for publication in Combinatorics,\n  Probability and Computing (CPC)", "journal-ref": "Combinator. Probab. Comp. 30 (2021) 124-135", "doi": "10.1017/S0963548320000292", "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a fully polynomial-time randomised approximation scheme (FPRAS) for\nthe number of bases in a bicircular matroids. This is a natural class of\nmatroids for which counting bases exactly is #P-hard and yet approximate\ncounting can be done efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 21:14:33 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 08:53:19 GMT"}, {"version": "v3", "created": "Sat, 24 Nov 2018 19:03:09 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 15:55:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Guo", "Heng", ""], ["Jerrum", "Mark", ""]]}, {"id": "1808.09669", "submitter": "Ankit Garg", "authors": "Ankit Garg and Rafael Oliveira", "title": "Recent progress on scaling algorithms and applications", "comments": "Survey written for EATCS complexity column; issue June 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling problems have a rich and diverse history, and thereby have found\nnumerous applications in several fields of science and engineering. For\ninstance, the matrix scaling problem has had applications ranging from\ntheoretical computer science to telephone forecasting, economics, statistics,\noptimization, among many other fields. Recently, a generalization of matrix\nscaling known as operator scaling has found applications in non-commutative\nalgebra, invariant theory, combinatorics and algebraic complexity; and a\nfurther generalization (tensor scaling) has found more applications in quantum\ninformation theory, geometric complexity theory and invariant theory. In this\nsurvey, we will describe in detail the scaling problems mentioned above,\nshowing how alternating minimization algorithms naturally arise in this\nsetting, and we shall present a general framework to rigorously analyze such\nalgorithms. These simple problems and algorithms are not just applicable to\ndiverse mathematical and CS areas, but also serve to bring out deep connections\nbetween them. As this framework makes extensive use of concepts from invariant\ntheory, we also provide a very gentle introduction to basic concepts of\ninvariant theory and how they are used to analyze alternating minimization\nalgorithms for the scaling problems. This survey is intended for a general\ncomputer science audience, and the only background required is basic knowledge\nof calculus and linear algebra, thereby making it accessible to graduate\nstudents and even to advanced undergraduates.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 07:54:51 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Garg", "Ankit", ""], ["Oliveira", "Rafael", ""]]}, {"id": "1808.09931", "submitter": "Peter Stumpf", "authors": "Guido Br\\\"uckner, Ignaz Rutter, Peter Stumpf", "title": "Level Planarity: Transitivity vs. Even Crossings", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Fulek et al. have presented Hanani-Tutte results for (radial) level\nplanarity, i.e., a graph is (radial) level planar if it admits a (radial) level\ndrawing where any two (independent) edges cross an even number of times. We\nshow that the 2-Sat formulation of level planarity testing due to Randerath et\nal. is equivalent to the strong Hanani-Tutte theorem for level planarity.\nFurther, we show that this relationship carries over to radial level planarity,\nwhich yields a novel polynomial-time algorithm for testing radial level\nplanarity.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 17:18:59 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Br\u00fcckner", "Guido", ""], ["Rutter", "Ignaz", ""], ["Stumpf", "Peter", ""]]}, {"id": "1808.09987", "submitter": "Alina Ene", "authors": "Alina Ene, Huy L. Nguyen, Adrian Vladu", "title": "Submodular Maximization with Matroid and Packing Constraints in Parallel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing the multilinear extension of a\nsubmodular function subject a single matroid constraint or multiple packing\nconstraints with a small number of adaptive rounds of evaluation queries.\n  We obtain the first algorithms with low adaptivity for submodular\nmaximization with a matroid constraint. Our algorithms achieve a\n$1-1/e-\\epsilon$ approximation for monotone functions and a $1/e-\\epsilon$\napproximation for non-monotone functions, which nearly matches the best\nguarantees known in the fully adaptive setting. The number of rounds of\nadaptivity is $O(\\log^2{n}/\\epsilon^3)$, which is an exponential speedup over\nthe existing algorithms.\n  We obtain the first parallel algorithm for non-monotone submodular\nmaximization subject to packing constraints. Our algorithm achieves a\n$1/e-\\epsilon$ approximation using $O(\\log(n/\\epsilon) \\log(1/\\epsilon)\n\\log(n+m)/ \\epsilon^2)$ parallel rounds, which is again an exponential speedup\nin parallel time over the existing algorithms. For monotone functions, we\nobtain a $1-1/e-\\epsilon$ approximation in\n$O(\\log(n/\\epsilon)\\log(m)/\\epsilon^2)$ parallel rounds. The number of parallel\nrounds of our algorithm matches that of the state of the art algorithm for\nsolving packing LPs with a linear objective.\n  Our results apply more generally to the problem of maximizing a diminishing\nreturns submodular (DR-submodular) function.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 18:04:37 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 03:09:35 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Vladu", "Adrian", ""]]}, {"id": "1808.10016", "submitter": "Ronald Rivest", "authors": "Ronald L. Rivest", "title": "Consistent Sampling with Replacement", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a very simple method for `consistent sampling' that allows for\nsampling with replacement. The method extends previous approaches to consistent\nsampling, which assign a pseudorandom real number to each element, and sample\nthose with the smallest associated numbers. When sampling with replacement, our\nextension gives the item sampled a new, larger, associated pseudorandom number,\nand returns it to the pool of items being sampled.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 19:05:48 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Rivest", "Ronald L.", ""]]}, {"id": "1808.10056", "submitter": "Rachel Cummings", "authors": "Rachel Cummings, Sara Krehbiel, Yajun Mei, Rui Tuo, Wanrong Zhang", "title": "Differentially Private Change-Point Detection", "comments": null, "journal-ref": "Proceedings of the 32nd International Conference on Neural\n  Information Processing Systems (2018) Pages 10848-10857", "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The change-point detection problem seeks to identify distributional changes\nat an unknown change-point k* in a stream of data. This problem appears in many\nimportant practical settings involving personal data, including\nbiosurveillance, fault detection, finance, signal detection, and security\nsystems. The field of differential privacy offers data analysis tools that\nprovide powerful worst-case privacy guarantees. We study the statistical\nproblem of change-point detection through the lens of differential privacy. We\ngive private algorithms for both online and offline change-point detection,\nanalyze these algorithms theoretically, and provide empirical validation of our\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 22:17:24 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Cummings", "Rachel", ""], ["Krehbiel", "Sara", ""], ["Mei", "Yajun", ""], ["Tuo", "Rui", ""], ["Zhang", "Wanrong", ""]]}, {"id": "1808.10262", "submitter": "Arya Mazumdar", "authors": "Arya Mazumdar", "title": "Capacity of Locally Recoverable Codes", "comments": "Invited paper to the Information Theory Workshop (ITW) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DM cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in distributed storage, the notion of a locally\nrecoverable code (LRC) was introduced a few years back. In an LRC, any\ncoordinate of a codeword is recoverable by accessing only a small number of\nother coordinates. While different properties of LRCs have been well-studied,\ntheir performance on channels with random erasures or errors has been mostly\nunexplored. In this note, we analyze the performance of LRCs over such\nstochastic channels. In particular, for input-symmetric discrete memoryless\nchannels, we give a tight characterization of the gap to Shannon capacity when\nLRCs are used over the channel.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 13:12:14 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Mazumdar", "Arya", ""]]}, {"id": "1808.10316", "submitter": "Nicole Wein", "authors": "Krzysztof Onak, Baruch Schieber, Shay Solomon, Nicole Wein", "title": "Fully Dynamic MIS in Uniformly Sparse Graphs", "comments": "appeared in ICALP 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maintaining a maximal independent set (MIS) in a\ndynamic graph subject to edge insertions and deletions. Recently, Assadi, Onak,\nSchieber and Solomon (STOC 2018) showed that an MIS can be maintained in\nsublinear (in the dynamically changing number of edges) amortized update time.\nIn this paper we significantly improve the update time for uniformly sparse\ngraphs. Specifically, for graphs with arboricity $\\alpha$, the amortized update\ntime of our algorithm is $O(\\alpha^2 \\cdot \\log^2 n)$, where $n$ is the number\nof vertices. For low arboricity graphs, which include, for example, minor-free\ngraphs as well as some classes of `real world' graphs, our update time is\npolylogarithmic. Our update time improves the result of Assadi et al. for all\ngraphs with arboricity bounded by $m^{3/8 - \\epsilon}$, for any constant\n$\\epsilon > 0$. This covers much of the range of possible values for\narboricity, as the arboricity of a general graph cannot exceed $m^{1/2}$.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 14:33:29 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Onak", "Krzysztof", ""], ["Schieber", "Baruch", ""], ["Solomon", "Shay", ""], ["Wein", "Nicole", ""]]}, {"id": "1808.10364", "submitter": "Giacomo Ortali", "authors": "Giacomo Ortali, Ioannis G. Tollis", "title": "Algorithms and Bounds for Drawing Directed Graphs", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach to visualize directed graphs and\ntheir hierarchies that completely departs from the classical four-phase\nframework of Sugiyama and computes readable hierarchical visualizations that\ncontain the complete reachability information of a graph. Additionally, our\napproach has the advantage that only the necessary edges are drawn in the\ndrawing, thus reducing the visual complexity of the resulting drawing.\nFurthermore, most problems involved in our framework require only polynomial\ntime. Our framework offers a suite of solutions depending upon the\nrequirements, and it consists of only two steps: (a) the cycle removal step (if\nthe graph contains cycles) and (b) the channel decomposition and hierarchical\ndrawing step. Our framework does not introduce any dummy vertices and it keeps\nthe vertices of a channel vertically aligned. The time complexity of the main\ndrawing algorithms of our framework is $O(kn)$, where $k$ is the number of\nchannels, typically much smaller than $n$ (the number of vertices).\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:48:12 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Ortali", "Giacomo", ""], ["Tollis", "Ioannis G.", ""]]}, {"id": "1808.10370", "submitter": "Gwena\\\"el Joret", "authors": "Samuel Fiorini and Gwena\\\"el Joret and Oliver Schaudt", "title": "Improved approximation algorithms for hitting 3-vertex paths", "comments": "Minor changes following referees' comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deleting a minimum cost set of vertices from a given\nvertex-weighted graph in such a way that the resulting graph has no induced\npath on three vertices. This problem is often called cluster vertex deletion in\nthe literature and admits a straightforward 3-approximation algorithm since it\nis a special case of the vertex cover problem on a 3-uniform hypergraph.\nRecently, You, Wang, and Cao described an efficient 5/2-approximation algorithm\nfor the unweighted version of the problem. Our main result is a\n9/4-approximation algorithm for arbitrary weights, using the local ratio\ntechnique. We further conjecture that the problem admits a 2-approximation\nalgorithm and give some support for the conjecture. This is in sharp contrast\nwith the fact that the similar problem of deleting vertices to eliminate all\ntriangles in a graph is known to be UGC-hard to approximate to within a ratio\nbetter than 3, as proved by Guruswami and Lee.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:59:02 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 10:35:30 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Fiorini", "Samuel", ""], ["Joret", "Gwena\u00ebl", ""], ["Schaudt", "Oliver", ""]]}, {"id": "1808.10470", "submitter": "Michael Bekos", "authors": "Patrizio Angelini, Michael A. Bekos, Henry F\\\"orster, Michael Kaufmann", "title": "On RAC Drawings of Graphs with one Bend per Edge", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A k-bend right-angle-crossing drawing or (k-bend RAC drawing}, for short) of\na graph is a polyline drawing where each edge has at most k bends and the\nangles formed at the crossing points of the edges are 90 degrees. Accordingly,\na graph that admits a k-bend RAC drawing is referred to as k-bend\nright-angle-crossing graph (or k-bend RAC, for short).\n  In this paper, we continue the study of the maximum edge-density of 1-bend\nRAC graphs. We show that an n-vertex 1-bend RAC graph cannot have more than\n$5.5n-O(1)$ edges. We also demonstrate that there exist infinitely many\nn-vertex 1-bend RAC graphs with exactly $5n-O(1)$ edges. Our results improve\nboth the previously known best upper bound of $6.5n-O(1)$ edges and the\ncorresponding lower bound of $4.5n-O(\\sqrt{n})$ edges by Arikushi et al.\n(Comput. Geom. 45(4), 169--177 (2012)).\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 18:05:26 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Angelini", "Patrizio", ""], ["Bekos", "Michael A.", ""], ["F\u00f6rster", "Henry", ""], ["Kaufmann", "Michael", ""]]}, {"id": "1808.10519", "submitter": "Michael Bekos", "authors": "Michael A. Bekos, Henry F\\\"orster, Christian Geckeler, Lukas\n  Holl\\\"ander, Michael Kaufmann, Amad\\\"aus M. Spallek, Jan Splett", "title": "A Heuristic Approach towards Drawings of Graphs with High Crossing\n  Resolution", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crossing resolution of a non-planar drawing of a graph is the value of\nthe minimum angle formed by any pair of crossing edges. Recent experiments have\nshown that the larger the crossing resolution is, the easier it is to read and\ninterpret a drawing of a graph. However, maximizing the crossing resolution\nturns out to be an NP-hard problem in general and only heuristic algorithms are\nknown that are mainly based on appropriately adjusting force-directed\nalgorithms.\n  In this paper, we propose a new heuristic algorithm for the crossing\nresolution maximization problem and we experimentally compare it against the\nknown approaches from the literature. Our experimental evaluation indicates\nthat the new heuristic produces drawings with better crossing resolution, but\nthis comes at the cost of slightly higher aspect ratio, especially when the\ninput graph is large.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 21:01:53 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Bekos", "Michael A.", ""], ["F\u00f6rster", "Henry", ""], ["Geckeler", "Christian", ""], ["Holl\u00e4nder", "Lukas", ""], ["Kaufmann", "Michael", ""], ["Spallek", "Amad\u00e4us M.", ""], ["Splett", "Jan", ""]]}, {"id": "1808.10530", "submitter": "Paris Siminelakis", "authors": "Moses Charikar, Paris Siminelakis", "title": "Hashing-Based-Estimators for Kernel Density in High Dimensions", "comments": "A preliminary version of this paper appeared in FOCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of points $P\\subset \\mathbb{R}^{d}$ and a kernel $k$, the Kernel\nDensity Estimate at a point $x\\in\\mathbb{R}^{d}$ is defined as\n$\\mathrm{KDE}_{P}(x)=\\frac{1}{|P|}\\sum_{y\\in P} k(x,y)$. We study the problem\nof designing a data structure that given a data set $P$ and a kernel function,\nreturns *approximations to the kernel density* of a query point in *sublinear\ntime*. We introduce a class of unbiased estimators for kernel density\nimplemented through locality-sensitive hashing, and give general theorems\nbounding the variance of such estimators. These estimators give rise to\nefficient data structures for estimating the kernel density in high dimensions\nfor a variety of commonly used kernels. Our work is the first to provide\ndata-structures with theoretical guarantees that improve upon simple random\nsampling in high dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 21:42:00 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Charikar", "Moses", ""], ["Siminelakis", "Paris", ""]]}, {"id": "1808.10536", "submitter": "Henry F\\\"orster", "authors": "Evmorfia Argyriou and Sabine Cornelsen and Henry F\\\"orster and Michael\n  Kaufmann and Martin N\\\"ollenburg and Yoshio Okamoto and Chrysanthi\n  Raftopoulou and Alexander Wolff", "title": "Orthogonal and Smooth Orthogonal Layouts of 1-Planar Graphs with Low\n  Edge Complexity", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While orthogonal drawings have a long history, smooth orthogonal drawings\nhave been introduced only recently. So far, only planar drawings or drawings\nwith an arbitrary number of crossings per edge have been studied. Recently, a\nlot of research effort in graph drawing has been directed towards the study of\nbeyond-planar graphs such as 1-planar graphs, which admit a drawing where each\nedge is crossed at most once. In this paper, we consider graphs with a fixed\nembedding. For 1-planar graphs, we present algorithms that yield orthogonal\ndrawings with optimal curve complexity and smooth orthogonal drawings with\nsmall curve complexity. For the subclass of outer-1-planar graphs, which can be\ndrawn such that all vertices lie on the outer face, we achieve optimal curve\ncomplexity for both, orthogonal and smooth orthogonal drawings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 22:07:42 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 10:12:30 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Argyriou", "Evmorfia", ""], ["Cornelsen", "Sabine", ""], ["F\u00f6rster", "Henry", ""], ["Kaufmann", "Michael", ""], ["N\u00f6llenburg", "Martin", ""], ["Okamoto", "Yoshio", ""], ["Raftopoulou", "Chrysanthi", ""], ["Wolff", "Alexander", ""]]}, {"id": "1808.10650", "submitter": "Andreas Loukas", "authors": "Andreas Loukas", "title": "Graph reduction with spectral and cut guarantees", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can one reduce the size of a graph without significantly altering its basic\nproperties? The graph reduction problem is hereby approached from the\nperspective of restricted spectral approximation, a modification of the\nspectral similarity measure used for graph sparsification. This choice is\nmotivated by the observation that restricted approximation carries strong\nspectral and cut guarantees, and that it implies approximation results for\nunsupervised learning problems relying on spectral embeddings.\n  The paper then focuses on coarsening---the most common type of graph\nreduction. Sufficient conditions are derived for a small graph to approximate a\nlarger one in the sense of restricted similarity. These findings give rise to\nnearly-linear algorithms that, compared to both standard and advanced graph\nreduction methods, find coarse graphs of improved quality, often by a large\nmargin, without sacrificing speed.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 09:51:12 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 15:04:24 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Loukas", "Andreas", ""]]}, {"id": "1808.10658", "submitter": "Ran Duan", "authors": "Ran Duan, Kaifeng Lyu, Hongxun Wu and Yuanhang Xie", "title": "Single-Source Bottleneck Path Algorithm Faster than Sorting for Sparse\n  Graphs", "comments": "15 pages, improved version of the paper appeared in ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a directed graph $G=(V,E)$ with a capacity on every edge, a\n\\emph{bottleneck path} (or \\emph{widest path}) between two vertices is a path\nmaximizing the minimum capacity of edges in the path. For the single-source\nall-destination version of this problem in directed graphs, the previous best\nalgorithm runs in $O(m+n\\log n)$ ($m=|E|$ and $n=|V|$) time, by Dijkstra search\nwith Fibonacci heap [Fredman and Tarjan 1987]. We improve this time bound to\n$O(m\\sqrt{\\log n})$, thus it is the first algorithm which breaks the time bound\nof classic Fibonacci heap when $m=o(n\\sqrt{\\log n})$. It is a Las-Vegas\nrandomized approach. By contrast, the s-t bottleneck path has an algorithm with\nrunning time $O(m\\beta(m,n))$ [Chechik et al. 2016], where\n$\\beta(m,n)=\\min\\{k\\geq 1: \\log^{(k)}n\\leq\\frac{m}{n}\\}$.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 10:09:44 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Duan", "Ran", ""], ["Lyu", "Kaifeng", ""], ["Wu", "Hongxun", ""], ["Xie", "Yuanhang", ""]]}, {"id": "1808.10661", "submitter": "Arthur Kramer", "authors": "Arthur Kramer, Mauro Dell'Amico, Manuel Iori", "title": "Enhanced arc-flow formulations to minimize weighted completion time on\n  identical parallel machines", "comments": "25 pages", "journal-ref": null, "doi": "10.1016/j.ejor.2018.11.039", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scheduling a set of jobs on a set of identical\nparallel machines, with the aim of minimizing the total weighted completion\ntime. The problem has been solved in the literature with a number of\nmathematical formulations, some of which require the implementation of tailored\nbranch-and-price methods. In our work, we solve the problem instead by means of\nnew arc-flow formulations, by first representing it on a capacitated network\nand then invoking a mixed integer linear model with a pseudo-polynomial number\nof variables and constraints. According to our computational tests, existing\nformulations from the literature can solve to proven optimality benchmark\ninstances with up to 100 jobs, whereas our most performing arc-flow formulation\nsolves all instances with up to 400 jobs and provides very low gap for larger\ninstances with up to 1000 jobs.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 10:18:32 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kramer", "Arthur", ""], ["Dell'Amico", "Mauro", ""], ["Iori", "Manuel", ""]]}, {"id": "1808.10734", "submitter": "Vera Traub", "authors": "Vera Traub and Jens Vygen", "title": "An improved upper bound on the integrality ratio for the $s$-$t$-path\n  TSP", "comments": null, "journal-ref": "Operations Research Letters 47 (2019), 225--228", "doi": null, "report-no": null, "categories": "cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an improved analysis of the best-of-many Christofides algorithm with\nlonely edge deletion, which was proposed by Seb\\H{o} and van Zuylen [2016].\nThis implies an improved upper bound on the integrality ratio of the standard\nLP relaxation for the $s$-$t$-path TSP.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 13:27:00 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 12:57:48 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Traub", "Vera", ""], ["Vygen", "Jens", ""]]}, {"id": "1808.10738", "submitter": "Elena Arseneva", "authors": "Elena Arseneva, Prosenjit Bose, Pilar Cano, Anthony D'Angelo, Vida\n  Dujmovic, Fabrizio Frati, Stefan Langerman and Alessandra Tappini", "title": "Pole Dancing: 3D Morphs for Tree Drawings", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question whether a crossing-free 3D morph between two\nstraight-line drawings of an $n$-vertex tree can be constructed consisting of a\nsmall number of linear morphing steps. We look both at the case in which the\ntwo given drawings are two-dimensional and at the one in which they are\nthree-dimensional. In the former setting we prove that a crossing-free 3D morph\nalways exists with $O(\\log n)$ steps, while for the latter $\\Theta(n)$ steps\nare always sufficient and sometimes necessary.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 13:49:44 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 09:19:41 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Arseneva", "Elena", ""], ["Bose", "Prosenjit", ""], ["Cano", "Pilar", ""], ["D'Angelo", "Anthony", ""], ["Dujmovic", "Vida", ""], ["Frati", "Fabrizio", ""], ["Langerman", "Stefan", ""], ["Tappini", "Alessandra", ""]]}, {"id": "1808.10787", "submitter": "Abhranil Chatterjee", "authors": "V. Arvind and Abhranil Chatterjee and Rajit Datta and Partha\n  Mukhopadhyay", "title": "Univariate Ideal Membership Parameterized by Rank, Degree, and Number of\n  Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Let $\\mathbb{F}[X]$ be the polynomial ring over the variables $X=\\{x_1,x_2,\n\\ldots, x_n\\}$. An ideal $I=\\langle p_1(x_1), \\ldots, p_n(x_n)\\rangle$\ngenerated by univariate polynomials $\\{p_i(x_i)\\}_{i=1}^n$ is a\n\\emph{univariate ideal}. We study the ideal membership problem for the\nunivariate ideals and show the following results.\n  \\item Let $f(X)\\in\\mathbb{F}[\\ell_1, \\ldots, \\ell_r]$ be a (low rank)\npolynomial given by an arithmetic circuit where $\\ell_i : 1\\leq i\\leq r$ are\nlinear forms, and $I=\\langle p_1(x_1), \\ldots, p_n(x_n)\\rangle$ be a univariate\nideal. Given $\\vec{\\alpha}\\in {\\mathbb{F}}^n$, the (unique) remainder $f(X)\n\\pmod I$ can be evaluated at $\\vec{\\alpha}$ in deterministic time\n$d^{O(r)}\\cdot poly(n)$, where $d=\\max\\{\\deg(f),\\deg(p_1)\\ldots,\\deg(p_n)\\}$.\nThis yields an $n^{O(r)}$ algorithm for minimum vertex cover in graphs with\nrank-$r$ adjacency matrices. It also yields an $n^{O(r)}$ algorithm for\nevaluating the permanent of a $n\\times n$ matrix of rank $r$, over any field\n$\\mathbb{F}$. Over $\\mathbb{Q}$, an algorithm of similar run time for low rank\npermanent is due to Barvinok[Bar96] via a different technique.\n  \\item Let $f(X)\\in\\mathbb{F}[X]$ be given by an arithmetic circuit of degree\n$k$ ($k$ treated as fixed parameter) and $I=\\langle p_1(x_1), \\ldots,\np_n(x_n)\\rangle$. We show in the special case when $I=\\langle x_1^{e_1},\n\\ldots, x_n^{e_n}\\rangle$, we obtain a randomized $O^*(4.08^k)$ algorithm that\nuses $poly(n,k)$ space.\n  \\item Given $f(X)\\in\\mathbb{F}[X]$ by an arithmetic circuit and $I=\\langle\np_1(x_1), \\ldots, p_k(x_k) \\rangle$, membership testing is $W[1]$-hard,\nparameterized by $k$. The problem is $MINI[1]$-hard in the special case when\n$I=\\langle x_1^{e_1}, \\ldots, x_k^{e_k}\\rangle$.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:38:53 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 08:53:01 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Arvind", "V.", ""], ["Chatterjee", "Abhranil", ""], ["Datta", "Rajit", ""], ["Mukhopadhyay", "Partha", ""]]}, {"id": "1808.10826", "submitter": "Fabrizio Frati", "authors": "Giordano Da Lozzo, Giuseppe Di Battista, Fabrizio Frati, Maurizio\n  Patrignani, and Vincenzo Roselli", "title": "Upward Planar Morphs", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018) The current version is the\n  extended one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that, given two topologically-equivalent upward planar straight-line\ndrawings of an $n$-vertex directed graph $G$, there always exists a morph\nbetween them such that all the intermediate drawings of the morph are upward\nplanar and straight-line. Such a morph consists of $O(1)$ morphing steps if $G$\nis a reduced planar $st$-graph, $O(n)$ morphing steps if $G$ is a planar\n$st$-graph, $O(n)$ morphing steps if $G$ is a reduced upward planar graph, and\n$O(n^2)$ morphing steps if $G$ is a general upward planar graph. Further, we\nshow that $\\Omega(n)$ morphing steps might be necessary for an upward planar\nmorph between two topologically-equivalent upward planar straight-line drawings\nof an $n$-vertex path.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 16:03:51 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 14:20:00 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 13:41:39 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""], ["Roselli", "Vincenzo", ""]]}, {"id": "1808.10841", "submitter": "Michael Bekos", "authors": "Jawaherul Md. Alam, Michael A. Bekos, Martin Gronemann, Michael\n  Kaufmann, Sergey Pupyrev", "title": "Queue Layouts of Planar 3-Trees", "comments": "Appears in the Proceedings of the 26th International Symposium on\n  Graph Drawing and Network Visualization (GD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A queue layout of a graph G consists of a linear order of the vertices of G\nand a partition of the edges of G into queues, so that no two independent edges\nof the same queue are nested. The queue number of G is the minimum number of\nqueues required by any queue layout of G.\n  In this paper, we continue the study of the queue number of planar 3-trees.\nAs opposed to general planar graphs, whose queue number is not known to be\nbounded by a constant, the queue number of planar 3-trees has been shown to be\nat most seven. In this work, we improve the upper bound to five. We also show\nthat there exist planar 3-trees, whose queue number is at least four; this is\nthe first example of a planar graph with queue number greater than three.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 16:59:06 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 20:32:45 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Alam", "Jawaherul Md.", ""], ["Bekos", "Michael A.", ""], ["Gronemann", "Martin", ""], ["Kaufmann", "Michael", ""], ["Pupyrev", "Sergey", ""]]}]