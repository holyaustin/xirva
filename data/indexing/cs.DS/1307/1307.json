[{"id": "1307.0024", "submitter": "Daan Wilmer", "authors": "Daan Wilmer", "title": "Investigation of \"Enhancing flexibility and robustness in multi-agent\n  task scheduling\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wilson et al. propose a measure of flexibility in project scheduling problems\nand propose several ways of distributing flexibility over tasks without\noverrunning the deadline. These schedules prove quite robust: delays of some\ntasks do not necessarily lead to delays of subsequent tasks. The number of\ntasks that finish late depends, among others, on the way of distributing\nflexibility.\n  In this paper I study the different flexibility distributions proposed by\nWilson et al. and the differences in number of violations (tasks that finish\ntoo late). I show one factor in the instances that causes differences in the\nnumber of violations, as well as two properties of the flexibility distribution\nthat cause them to behave differently. Based on these findings, I propose three\nnew flexibility distributions. Depending on the nature of the delays, these new\nflexibility distributions perform as good as or better than the distributions\nby Wilson et al.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 20:20:27 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Wilmer", "Daan", ""]]}, {"id": "1307.0099", "submitter": "Emanuele Giaquinta", "authors": "Kimmo Fredriksson, Emanuele Giaquinta", "title": "On a compact encoding of the swap automaton", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a string $P$ of length $m$ over an alphabet $\\Sigma$ of size $\\sigma$,\na swapped version of $P$ is a string derived from $P$ by a series of local\nswaps, i.e., swaps of adjacent symbols, such that each symbol can participate\nin at most one swap. We present a theoretical analysis of the nondeterministic\nfinite automaton for the language $\\bigcup_{P'\\in\\Pi_P}\\Sigma^*P'$ (swap\nautomaton for short), where $\\Pi_P$ is the set of swapped versions of $P$. Our\nstudy is based on the bit-parallel simulation of the same automaton due to\nFredriksson, and reveals an interesting combinatorial property that links the\nautomaton to the one for the language $\\Sigma^*P$. By exploiting this property\nand the method presented by Cantone et al. (2010), we obtain a bit-parallel\nencoding of the swap automaton which takes $O(\\sigma^2\\ceil{k/w})$ space and\nallows one to simulate the automaton on a string of length $n$ in time\n$O(n\\ceil{k/w})$, where $\\ceil{m/\\sigma}\\le k\\le m$.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 12:53:49 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Fredriksson", "Kimmo", ""], ["Giaquinta", "Emanuele", ""]]}, {"id": "1307.0441", "submitter": "Jakub Z\\'avodn\\'y", "authors": "Nurzhan Bakibayev, Tom\\'a\\v{s} Ko\\v{c}isk\\'y, Dan Olteanu, and Jakub\n  Z\\'avodn\\'y", "title": "Aggregation and Ordering in Factorised Databases", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to data analysis involves understanding and manipulating\nsuccinct representations of data. In earlier work, we put forward a succinct\nrepresentation system for relational data called factorised databases and\nreported on the main-memory query engine FDB for select-project-join queries on\nsuch databases.\n  In this paper, we extend FDB to support a larger class of practical queries\nwith aggregates and ordering. This requires novel optimisation and evaluation\ntechniques. We show how factorisation coupled with partial aggregation can\neffectively reduce the number of operations needed for query evaluation. We\nalso show how factorisations of query results can support enumeration of tuples\nin desired orders as efficiently as listing them from the unfactorised, sorted\nresults.\n  We experimentally observe that FDB can outperform off-the-shelf relational\nengines by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 17:03:44 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Bakibayev", "Nurzhan", ""], ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Olteanu", "Dan", ""], ["Z\u00e1vodn\u00fd", "Jakub", ""]]}, {"id": "1307.0531", "submitter": "Kirk Pruhs", "authors": "Ahmed Abousamra, David P. Bunde, Kirk Pruhs", "title": "An Experimental Comparison of Speed Scaling Algorithms with Deadline\n  Feasibility Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the first, and most well studied, speed scaling problem in the\nalgorithmic literature: where the scheduling quality of service measure is a\ndeadline feasibility constraint, and where the power objective is to minimize\nthe total energy used. Four online algorithms for this problem have been\nproposed in the algorithmic literature. Based on the best upper bound that can\nbe proved on the competitive ratio, the ranking of the online algorithms from\nbest to worst is: $\\qOA$, $\\OA$, $\\AVR$, $\\BKP$. As a test case on the\neffectiveness of competitive analysis to predict the best online algorithm, we\nreport on an experimental \"horse race\" between these algorithms using instances\nbased on web server traces. Our main conclusion is that the ranking of our\nalgorithms based on their performance in our experiments is identical to the\norder predicted by competitive analysis. This ranking holds over a large range\nof possible power functions, and even if the power objective is temperature.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 21:02:49 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Abousamra", "Ahmed", ""], ["Bunde", "David P.", ""], ["Pruhs", "Kirk", ""]]}, {"id": "1307.0571", "submitter": "Marius Nicolae", "authors": "Marius Nicolae and Sanguthevar Rajasekaran", "title": "Efficient Sequential and Parallel Algorithms for Planted Motif Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motif searching is an important step in the detection of rare events\noccurring in a set of DNA or protein sequences. One formulation of the problem\nis known as (l,d)-motif search or Planted Motif Search (PMS). In PMS we are\ngiven two integers l and d and n biological sequences. We want to find all\nsequences of length l that appear in each of the input sequences with at most d\nmismatches. The PMS problem is NP-complete. PMS algorithms are typically\nevaluated on certain instances considered challenging. This paper presents an\nexact parallel PMS algorithm called PMS8. PMS8 is the first algorithm to solve\nthe challenging (l,d) instances (25,10) and (26,11). PMS8 is also efficient on\ninstances with larger l and d such as (50,21). This paper also introduces\nnecessary and sufficient conditions for 3 l-mers to have a common d-neighbor.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 01:51:06 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Nicolae", "Marius", ""], ["Rajasekaran", "Sanguthevar", ""]]}, {"id": "1307.0624", "submitter": "Fei Chen", "authors": "T-H. Hubert Chan and Fei Chen", "title": "A Primal-Dual Continuous LP Method on the Multi-choice Multi-best\n  Secretary Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The J-choice K-best secretary problem, also known as the (J,K)-secretary\nproblem, is a generalization of the classical secretary problem. An algorithm\nfor the (J,K)-secretary problem is allowed to make J choices and the payoff to\nbe maximized is the expected number of items chosen among the K best items.\n  Previous works analyzed the case when the total number n of items is finite,\nand considered what happens when n grows. However, for general J and K, the\noptimal solution for finite n is difficult to analyze. Instead, we prove a\nformal connection between the finite model and the infinite model, where there\nare countably infinite number of items, each attached with a random arrival\ntime drawn independently and uniformly from [0,1].\n  We use primal-dual continuous linear programming techniques to analyze a\nclass of infinite algorithms, which are general enough to capture the\nasymptotic behavior of the finite model with large number of items. Our\ntechniques allow us to prove that the optimal solution can be achieved by the\n(J,K)-Threshold Algorithm, which has a nice \"rational description\" for the case\nK = 1.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 08:36:04 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Chan", "T-H. Hubert", ""], ["Chen", "Fei", ""]]}, {"id": "1307.0920", "submitter": "Sadagopan Narasimhan", "authors": "K.Ilambharathi, G.S.N.V.Venkata Manik, N.Sadagopan, B.Sivaselvan", "title": "Domain Specific Hierarchical Huffman Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the classical data compression problem for domain\nspecific texts. It is well-known that classical Huffman algorithm is optimal\nwith respect to prefix encoding and the compression is done at character level.\nSince many data transfer are domain specific, for example, downloading of\nlecture notes, web-blogs, etc., it is natural to think of data compression in\nlarger dimensions (i.e. word level rather than character level). Our framework\nemploys a two-level compression scheme in which the first level identifies\nfrequent patterns in the text using classical frequent pattern algorithms. The\nidentified patterns are replaced with special strings and to acheive a better\ncompression ratio the length of a special string is ensured to be shorter than\nthe length of the corresponding pattern. After this transformation, on the\nresultant text, we employ classical Huffman data compression algorithm. In\nshort, in the first level compression is done at word level and in the second\nlevel it is at character level. Interestingly, this two level compression\ntechnique for domain specific text outperforms classical Huffman technique. To\nsupport our claim, we have presented both theoretical and simulation results\nfor domain specific texts.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 06:19:08 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Ilambharathi", "K.", ""], ["Manik", "G. S. N. V. Venkata", ""], ["Sadagopan", "N.", ""], ["Sivaselvan", "B.", ""]]}, {"id": "1307.1372", "submitter": "Kishore Kumar Gajula", "authors": "G.Kishore Kumar and V.K.Jayaraman", "title": "Clustering of Complex Networks and Community Detection Using Group\n  Search Optimization", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group Search Optimizer(GSO) is one of the best algorithms, is very new in the\nfield of Evolutionary Computing. It is very robust and efficient algorithm,\nwhich is inspired by animal searching behaviour. The paper describes an\napplication of GSO to clustering of networks. We have tested GSO against five\nstandard benchmark datasets, GSO algorithm is proved very competitive in terms\nof accuracy and convergence speed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 15:22:35 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 09:11:13 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Kumar", "G. Kishore", ""], ["Jayaraman", "V. K.", ""]]}, {"id": "1307.1406", "submitter": "Marius Nicolae", "authors": "Marius Nicolae and Sanguthevar Rajasekaran", "title": "On string matching with k mismatches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider several variants of the pattern matching problem.\nIn particular, we investigate the following problems: 1) Pattern matching with\nk mismatches; 2) Approximate counting of mismatches; and 3) Pattern matching\nwith mismatches. The distance metric used is the Hamming distance. We present\nsome novel algorithms and techniques for solving these problems. Both\ndeterministic and randomized algorithms are offered. Variants of these problems\nwhere there could be wild cards in either the text or the pattern or both are\nconsidered. An experimental evaluation of these algorithms is also presented.\nThe source code is available at http://www.engr.uconn.edu/~man09004/kmis.zip.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 16:56:09 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Nicolae", "Marius", ""], ["Rajasekaran", "Sanguthevar", ""]]}, {"id": "1307.1417", "submitter": "Marius Nicolae", "authors": "Sanguthevar Rajasekaran and Marius Nicolae", "title": "An Elegant Algorithm for the Construction of Suffix Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suffix array is a data structure that finds numerous applications in\nstring processing problems for both linguistic texts and biological data. It\nhas been introduced as a memory efficient alternative for suffix trees. The\nsuffix array consists of the sorted suffixes of a string. There are several\nlinear time suffix array construction algorithms (SACAs) known in the\nliterature. However, one of the fastest algorithms in practice has a worst case\nrun time of $O(n^2)$. The problem of designing practically and theoretically\nefficient techniques remains open. In this paper we present an elegant\nalgorithm for suffix array construction which takes linear time with high\nprobability; the probability is on the space of all possible inputs. Our\nalgorithm is one of the simplest of the known SACAs and it opens up a new\ndimension of suffix array construction that has not been explored until now.\nOur algorithm is easily parallelizable. We offer parallel implementations on\nvarious parallel models of computing. We prove a lemma on the $\\ell$-mers of a\nrandom string which might find independent applications. We also present\nanother algorithm that utilizes the above algorithm. This algorithm is called\nRadixSA and has a worst case run time of $O(n\\log{n})$. RadixSA introduces an\nidea that may find independent applications as a speedup technique for other\nSACAs. An empirical comparison of RadixSA with other algorithms on various\ndatasets reveals that our algorithm is one of the fastest algorithms to date.\nThe C++ source code is freely available at\nhttp://www.engr.uconn.edu/~man09004/radixSA.zip\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 17:10:08 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Rajasekaran", "Sanguthevar", ""], ["Nicolae", "Marius", ""]]}, {"id": "1307.1424", "submitter": "Phillippe Samer", "authors": "Phillippe Samer, Sebasti\\'an Urrutia", "title": "A branch and cut algorithm for minimum spanning trees under conflict\n  constraints", "comments": null, "journal-ref": null, "doi": "10.1007/s11590-014-0750-x", "report-no": null, "categories": "cs.DS math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approaches for the exact solution of the \\NP--hard minimum spanning\ntree problem under conflict constraints. Given a graph $G(V,E)$ and a set $C\n\\subset E \\times E$ of conflicting edge pairs, the problem consists of finding\na conflict-free minimum spanning tree, i.e. feasible solutions are allowed to\ninclude at most one of the edges from each pair in $C$. The problem was\nintroduced recently in the literature, with several results on its complexity\nand approximability. Some formulations and both exact and heuristic algorithms\nwere also discussed, but computational results indicate considerably large\nduality gaps and a lack of optimality certificates for benchmark instances. In\nthis paper, we build on the representation of conflict constraints using an\nauxiliary conflict graph $\\hat{G}(E,C)$, where stable sets correspond to\nconflict-free subsets of $E$. We introduce a general preprocessing method and a\nbranch and cut algorithm using an IP formulation with exponentially sized\nclasses of valid inequalities for both the spanning tree and the stable set\npolytopes. Encouraging computational results indicate that the dual bounds of\nour approach are significantly stronger than those previously available,\nalready in the initial LP relaxation, and we are able to provide new\nfeasibility and optimality certificates.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 17:47:09 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 18:08:25 GMT"}, {"version": "v3", "created": "Mon, 30 Jun 2014 13:36:45 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Samer", "Phillippe", ""], ["Urrutia", "Sebasti\u00e1n", ""]]}, {"id": "1307.1428", "submitter": "Dominik Kempa", "authors": "Juha K\\\"arkk\\\"ainen, Dominik Kempa, Simon J. Puglisi", "title": "Lempel-Ziv Parsing in External Memory", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/DCC.2014.78", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, computing the LZ factorization (or LZ77 parsing) of a string has\nbeen a requisite and computationally intensive step in many diverse\napplications, including text indexing and data compression. Many algorithms for\nLZ77 parsing have been discovered over the years; however, despite the\nincreasing need to apply LZ77 to massive data sets, no algorithm to date scales\nto inputs that exceed the size of internal memory. In this paper we describe\nthe first algorithm for computing the LZ77 parsing in external memory. Our\nalgorithm is fast in practice and will allow the next generation of text\nindexes to be realised for massive strings and string collections.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 17:53:54 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["K\u00e4rkk\u00e4inen", "Juha", ""], ["Kempa", "Dominik", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1307.1447", "submitter": "Ricardo Corr\\^ea", "authors": "Ricardo C. Corr\\^ea and Pablo M. S. Farias", "title": "Linear Time Computation of the Maximal Linear and Circular Sums of\n  Multiple Independent Insertions into a Sequence", "comments": "13 pages, 4 figures, 2 tables. Accepted for journal publication", "journal-ref": null, "doi": "10.1016/j.tcs.2016.11.005", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximal sum of a sequence \"A\" of \"n\" real numbers is the greatest sum of\nall elements of any strictly contiguous and possibly empty subsequence of \"A\",\nand it can be computed in \"O(n)\" time by means of Kadane's algorithm. Letting\n\"A^(x -> p)\" denote the sequence which results from inserting a real number \"x\"\nbetween elements \"A[p-1]\" and \"A[p]\", we show how the maximal sum of \"A^(x ->\np)\" can be computed in \"O(1)\" worst-case time for any given \"x\" and \"p\",\nprovided that an \"O(n)\" time preprocessing step has already been executed on\n\"A\". In particular, this implies that, given \"m\" pairs \"(x_0, p_0), ...,\n(x_{m-1}, p_{m-1})\", we can compute the maximal sums of sequences \"A^(x_0 ->\np_0), ..., A^(x_{m-1} -> p_{m-1})\" in \"O(n+m)\" time, which matches the lower\nbound imposed by the problem input size, and also improves on the\nstraightforward strategy of applying Kadane's algorithm to each sequence\n\"A^(x_i -> p_i)\", which takes a total of \"Theta(n.m)\" time. Our main\ncontribution, however, is to obtain the same time bound for the more\ncomplicated problem of computing the greatest sum of all elements of any\nstrictly or circularly contiguous and possibly empty subsequence of \"A^(x ->\np)\". Our algorithms are easy to implement in practice, and they were motivated\nby and find application in a buffer minimization problem on wireless mesh\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 18:45:57 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 21:56:26 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 20:03:53 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Corr\u00eaa", "Ricardo C.", ""], ["Farias", "Pablo M. S.", ""]]}, {"id": "1307.1516", "submitter": "Shiri Chechik", "authors": "Ittai Abraham and Shiri Chechik", "title": "Dynamic Decremental Approximate Distance Oracles with $(1+\\epsilon, 2)$\n  stretch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a decremental approximate Distance Oracle that obtains stretch of\n$1+\\epsilon$ multiplicative and 2 additive and has $\\hat{O}(n^{5/2})$ total\ncost (where $\\hat{O}$ notation suppresses polylogarithmic and\n$n^{O(1)/\\sqrt{n}}$ factors). The best previous results with $\\hat{O}(n^{5/2})$\ntotal cost obtained stretch $3+\\epsilon$.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 04:05:11 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Abraham", "Ittai", ""], ["Chechik", "Shiri", ""]]}, {"id": "1307.1560", "submitter": "Radoslaw Glowinski", "authors": "Rados{\\l}aw G{\\l}owinski, Wojciech Rytter", "title": "Compressed Pattern-Matching with Ranked Variables in Zimin Words", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zimin words are very special finite words which are closely related to the\npattern-avoidability problem. This problem consists in testing if an instance\nof a given pattern with variables occurs in almost all words over any finite\nalphabet. The problem is not well understood, no polynomial time algorithm is\nknown and its NP-hardness is also not known. The pattern-avoidability problem\nis equivalent to searching for a pattern (with variables) in a Zimin word. The\nmain difficulty is potentially exponential size of Zimin words. We use special\nproperties of Zimin words, especially that they are highly compressible, to\ndesign efficient algorithms for special version of the pattern-matching, called\nhere ranked matching. It gives a new interpretation of Zimin algorithm in\ncompressed setting. We discuss the structure of rankings of variables and\ncompressed representations of values of variables. Moreover, for a ranked\nmatching we present efficient algorithms to find the shortest instance and the\nnumber of valuations of instances of the pattern.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 09:28:07 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["G\u0142owinski", "Rados\u0142aw", ""], ["Rytter", "Wojciech", ""]]}, {"id": "1307.1690", "submitter": "Nitish Korula", "authors": "Nitish Korula and Silvio Lattanzi", "title": "An efficient reconciliation algorithm for social networks", "comments": "23 pages, 4 figures. To appear in VLDB 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People today typically use multiple online social networks (Facebook,\nTwitter, Google+, LinkedIn, etc.). Each online network represents a subset of\ntheir \"real\" ego-networks. An interesting and challenging problem is to\nreconcile these online networks, that is, to identify all the accounts\nbelonging to the same individual. Besides providing a richer understanding of\nsocial dynamics, the problem has a number of practical applications. At first\nsight, this problem appears algorithmically challenging. Fortunately, a small\nfraction of individuals explicitly link their accounts across multiple\nnetworks; our work leverages these connections to identify a very large\nfraction of the network.\n  Our main contributions are to mathematically formalize the problem for the\nfirst time, and to design a simple, local, and efficient parallel algorithm to\nsolve it. We are able to prove strong theoretical guarantees on the algorithm's\nperformance on well-established network models (Random Graphs, Preferential\nAttachment). We also experimentally confirm the effectiveness of the algorithm\non synthetic and real social network data sets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 18:57:59 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 00:05:52 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Korula", "Nitish", ""], ["Lattanzi", "Silvio", ""]]}, {"id": "1307.1728", "submitter": "Andrea Sportiello", "authors": "Frederique Bassino and Andrea Sportiello", "title": "Linear-time generation of specifiable combinatorial structures: general\n  theory and first examples", "comments": "10 pages + 5 title/biblio/append., submitted to SODA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various specifiable combinatorial structures, with d extensive parameters,\ncan be exactly sampled both by the recursive method, with linear arithmetic\ncomplexity if a heavy preprocessing is performed, or by the Boltzmann method,\nwith complexity Theta(n^{1+d/2}). We discuss a modified recursive method,\ncrucially based on the asymptotic expansion of the associated saddle-point\nintegrals, which can be adopted for a large number of such structures (e.g.\npartitions, permutations, lattice walks, trees, random graphs, all with a\nvariety of prescribed statistics and/or constraints). The new algorithm\nrequires no preprocessing, still it has linear complexity on average. In terms\nof bit complexity, instead of arithmetic, we only have extra logarithmic\nfactors. For many families of structures, this provides, at our knowledge, the\nonly known quasi-linear generators. We present the general theory, and detail a\nspecific example: the partitions of n elements into k non-empty blocks, counted\nby the Stirling numbers of the second kind. These objects are involved in the\nexact sampling of minimal automata with prescribed alphabet size and number of\nstates, which is thus performed here with average Theta(n ln n) bit complexity,\noutbreaking all previously known Theta(n^{3/2}) algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 22:19:06 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Bassino", "Frederique", ""], ["Sportiello", "Andrea", ""]]}, {"id": "1307.1772", "submitter": "Sadagopan Narasimhan", "authors": "Surabhi Jain and N.Sadagopan", "title": "Simpler Sequential and Parallel Biconnectivity Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a connected graph, a vertex separator is a set of vertices whose removal\ncreates at least two components and a minimum vertex separator is a vertex\nseparator of least cardinality. The vertex connectivity refers to the size of a\nminimum vertex separator. For a connected graph $G$ with vertex connectivity $k\n(k \\geq 1)$, the connectivity augmentation refers to a set $S$ of edges whose\naugmentation to $G$ increases its vertex connectivity by one. A minimum\nconnectivity augmentation of $G$ is the one in which $S$ is minimum. In this\npaper, we focus our attention on connectivity augmentation of trees. Towards\nthis end, we present a new sequential algorithm for biconnectivity augmentation\nin trees by simplifying the algorithm reported in \\cite{nsn}. The simplicity is\nachieved with the help of edge contraction tool. This tool helps us in getting\na recursive subproblem preserving all connectivity information. Subsequently,\nwe present a parallel algorithm to obtain a minimum connectivity augmentation\nset in trees. Our parallel algorithm essentially follows the overall structure\nof sequential algorithm. Our implementation is based on CREW PRAM model with\n$O(\\Delta)$ processors, where $\\Delta$ refers to the maximum degree of a tree.\nWe also show that our parallel algorithm is optimal whose processor-time\nproduct is O(n) where $n$ is the number of vertices of a tree, which is an\nimprovement over the parallel algorithm reported in \\cite{hsu}.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 12:18:55 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Jain", "Surabhi", ""], ["Sadagopan", "N.", ""]]}, {"id": "1307.1774", "submitter": "Anna Adamaszek", "authors": "Anna Adamaszek and Andreas Wiese", "title": "Approximation Schemes for Maximum Weight Independent Set of Rectangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Maximum Weight Independent Set of Rectangles (MWISR) problem we are\ngiven a set of n axis-parallel rectangles in the 2D-plane, and the goal is to\nselect a maximum weight subset of pairwise non-overlapping rectangles. Due to\nmany applications, e.g. in data mining, map labeling and admission control, the\nproblem has received a lot of attention by various research communities. We\npresent the first (1+epsilon)-approximation algorithm for the MWISR problem\nwith quasi-polynomial running time 2^{poly(log n/epsilon)}. In contrast, the\nbest known polynomial time approximation algorithms for the problem achieve\nsuperconstant approximation ratios of O(log log n) (unweighted case) and O(log\nn / log log n) (weighted case).\n  Key to our results is a new geometric dynamic program which recursively\nsubdivides the plane into polygons of bounded complexity. We provide the\ntechnical tools that are needed to analyze its performance. In particular, we\npresent a method of partitioning the plane into small and simple areas such\nthat the rectangles of an optimal solution are intersected in a very controlled\nmanner. Together with a novel application of the weighted planar graph\nseparator theorem due to Arora et al. this allows us to upper bound our\napproximation ratio by (1+epsilon).\n  Our dynamic program is very general and we believe that it will be useful for\nother settings. In particular, we show that, when parametrized properly, it\nprovides a polynomial time (1+epsilon)-approximation for the special case of\nthe MWISR problem when each rectangle is relatively large in at least one\ndimension. Key to this analysis is a method to tile the plane in order to\napproximately describe the topology of these rectangles in an optimal solution.\nThis technique might be a useful insight to design better polynomial time\napproximation algorithms or even a PTAS for the MWISR problem.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 12:41:17 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Adamaszek", "Anna", ""], ["Wiese", "Andreas", ""]]}, {"id": "1307.1805", "submitter": "Michele Scquizzato", "authors": "Michele Scquizzato and Francesco Silvestri", "title": "Communication Lower Bounds for Distributed-Memory Computations", "comments": "Minor edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give lower bounds on the communication complexity required to solve\nseveral computational problems in a distributed-memory parallel machine, namely\nstandard matrix multiplication, stencil computations, comparison sorting, and\nthe Fast Fourier Transform. We revisit the assumptions under which preceding\nresults were derived and provide new lower bounds which use much weaker and\nappropriate hypotheses. Our bounds rely on a mild assumption on work\ndistribution, and strengthen previous results which require either the\ncomputation to be balanced among the processors, or specific initial\ndistributions of the input data, or an upper bound on the size of processors'\nlocal memories.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 18:40:47 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 21:51:59 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Scquizzato", "Michele", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1307.1915", "submitter": "Frank Gurski", "authors": "Frank Gurski, Jochen Rethmann, Egon Wanke", "title": "Complexity of the FIFO Stack-Up Problem", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the combinatorial FIFO stack-up problem. In delivery industry, bins\nhave to be stacked-up from conveyor belts onto pallets with respect to customer\norders. Given k sequences q_1, ..., q_k of labeled bins and a positive integer\np, the aim is to stack-up the bins by iteratively removing the first bin of one\nof the k sequences and put it onto an initially empty pallet of unbounded\ncapacity located at one of p stack-up places. Bins with different pallet labels\nhave to be placed on different pallets, bins with the same pallet label have to\nbe placed on the same pallet. After all bins for a pallet have been removed\nfrom the given sequences, the corresponding stack-up place will be cleared and\nbecomes available for a further pallet. The FIFO stack-up problem is to find a\nstack-up sequence such that all pallets can be build-up with the available p\nstack-up places. In this paper, we introduce two digraph models for the FIFO\nstack-up problem, namely the processing graph and the sequence graph. We show\nthat there is a processing of some list of sequences with at most p stack-up\nplaces if and only if the sequence graph of this list has directed pathwidth at\nmost p-1. This connection implies that the FIFO stack-up problem is NP-complete\nin general, even if there are at most 6 bins for every pallet and that the\nproblem can be solved in polynomial time, if the number p of stack-up places is\nassumed to be fixed. Further the processing graph allows us to show that the\nproblem can be solved in polynomial time, if the number k of sequences is\nassumed to be fixed.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2013 21:02:11 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2013 13:28:19 GMT"}, {"version": "v3", "created": "Sat, 7 Jun 2014 11:10:25 GMT"}, {"version": "v4", "created": "Mon, 20 Apr 2015 08:38:13 GMT"}, {"version": "v5", "created": "Fri, 2 Oct 2015 13:16:56 GMT"}, {"version": "v6", "created": "Thu, 15 Oct 2015 09:24:04 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Gurski", "Frank", ""], ["Rethmann", "Jochen", ""], ["Wanke", "Egon", ""]]}, {"id": "1307.2187", "submitter": "Micha{\\l} Pilipczuk", "authors": "D\\'aniel Marx, Micha{\\l} Pilipczuk", "title": "Everything you always wanted to know about the parameterized complexity\n  of Subgraph Isomorphism (but were afraid to ask)", "comments": "85 pages, 16 figures; program and input data file can be found as\n  ancillary files. Version [v2]: revised conclusions, ancillary files added\n  properly. Version [v3]: added a remark about fixed-parameter tractability of\n  the Conjoining Matching problem following from Lemma 3.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two graphs $H$ and $G$, the Subgraph Isomorphism problem asks if $H$ is\nisomorphic to a subgraph of $G$. While NP-hard in general, algorithms exist for\nvarious parameterized versions of the problem: for example, the problem can be\nsolved (1) in time $2^{O(|V(H)|)}\\cdot n^{O(\\tw(H))}$ using the color-coding\ntechnique of Alon, Yuster, and Zwick; (2) in time $f(|V(H)|,\\tw(G))\\cdot n$\nusing Courcelle's Theorem; (3) in time $f(|V(H)|,\\genus(G))\\cdot n$ using a\nresult on first-order model checking by Frick and Grohe; or (4) in time\n$f(\\maxdeg(H))\\cdot n^{O(\\tw(G)})$ for connected $H$ using the algorithm of\nMatou\\v{s}ek and Thomas. Already this small sample of results shows that the\nway an algorithm can depend on the parameters is highly nontrivial and subtle.\n  We develop a framework involving 10 relevant parameters for each of $H$ and\n$G$ (such as treewidth, pathwidth, genus, maximum degree, number of vertices,\nnumber of components, etc.), and ask if an algorithm with running time \\[\nf_1(p_1,p_2,..., p_\\ell)\\cdot n^{f_2(p_{\\ell+1},..., p_k)} \\] exist, where each\nof $p_1,..., p_k$ is one of the 10 parameters depending only on $H$ or $G$. We\nshow that {\\em all} the questions arising in this framework are answered by a\nset of 11 maximal positive results (algorithms) and a set of 17 maximal\nnegative results (hardness proofs); some of these results already appear in the\nliterature, while others are new in this paper.\n  On the algorithmic side, our study reveals for example that an unexpected\ncombination of bounded degree, genus, and feedback vertex set number of $G$\ngives rise to a highly nontrivial algorithm for Subgraph Isomorphism. On the\nhardness side, we present W[1]-hardness proofs under extremely restricted\nconditions, such as when $H$ is a bounded-degree tree of constant pathwidth and\n$G$ is a planar graph of bounded pathwidth.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 17:47:45 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 15:51:26 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2013 10:33:09 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Marx", "D\u00e1niel", ""], ["Pilipczuk", "Micha\u0142", ""]]}, {"id": "1307.2205", "submitter": "Aleksander M{\\ka}dry", "authors": "Aleksander Madry", "title": "Navigating Central Path with Electrical Flows: from Flows to Matchings,\n  and Back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $\\tilde{O}(m^{10/7})=\\tilde{O}(m^{1.43})$-time algorithm for\nthe maximum s-t flow and the minimum s-t cut problems in directed graphs with\nunit capacities. This is the first improvement over the sparse-graph case of\nthe long-standing $O(m \\min(\\sqrt{m},n^{2/3}))$ time bound due to Even and\nTarjan [EvenT75]. By well-known reductions, this also establishes an\n$\\tilde{O}(m^{10/7})$-time algorithm for the maximum-cardinality bipartite\nmatching problem. That, in turn, gives an improvement over the celebrated\ncelebrated $O(m \\sqrt{n})$ time bound of Hopcroft and Karp [HK73] whenever the\ninput graph is sufficiently sparse.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 19:13:00 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 19:32:59 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2013 10:50:15 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["Madry", "Aleksander", ""]]}, {"id": "1307.2262", "submitter": "Huiwen Yu", "authors": "Martin Furer and Huiwen Yu", "title": "Approximate the k-Set Packing Problem by Local Improvements", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms based on local improvements for the $k$-Set Packing\nproblem. The well-known local improvement algorithm by Hurkens and Schrijver\nhas been improved by Sviridenko and Ward from $\\frac{k}{2}+\\epsilon$ to\n$\\frac{k+2}{3}$, and by Cygan to $\\frac{k+1}{3}+\\epsilon$ for any $\\epsilon>0$.\nIn this paper, we achieve the approximation ratio $\\frac{k+1}{3}+\\epsilon$ for\nthe $k$-Set Packing problem using a simple polynomial-time algorithm based on\nthe method by Sviridenko and Ward. With the same approximation guarantee, our\nalgorithm runs in time singly exponential in $\\frac{1}{\\epsilon^2}$, while the\nrunning time of Cygan's algorithm is doubly exponential in\n$\\frac{1}{\\epsilon}$. On the other hand, we construct an instance with locality\ngap $\\frac{k+1}{3}$ for any algorithm using local improvements of size\n$O(n^{1/5})$, here $n$ is the total number of sets. Thus, our approximation\nguarantee is optimal with respect to results achievable by algorithms based on\nlocal improvements.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 20:34:45 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2013 21:25:15 GMT"}, {"version": "v3", "created": "Tue, 10 Jun 2014 20:57:11 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Furer", "Martin", ""], ["Yu", "Huiwen", ""]]}, {"id": "1307.2274", "submitter": "Nicholas Harvey", "authors": "Nicholas J. A. Harvey, Neil Olver", "title": "Pipage Rounding, Pessimistic Estimators and Matrix Concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pipage rounding is a dependent random sampling technique that has several\ninteresting properties and diverse applications. One property that has been\nparticularly useful is negative correlation of the resulting vector.\nUnfortunately negative correlation has its limitations, and there are some\nfurther desirable properties that do not seem to follow from existing\ntechniques. In particular, recent concentration results for sums of independent\nrandom matrices are not known to extend to a negatively dependent setting.\n  We introduce a simple but useful technique called concavity of pessimistic\nestimators. This technique allows us to show concentration of submodular\nfunctions and concentration of matrix sums under pipage rounding. The former\nresult answers a question of Chekuri et al. (2009). To prove the latter result,\nwe derive a new variant of Lieb's celebrated concavity theorem in matrix\nanalysis.\n  We provide numerous applications of these results. One is to spectrally-thin\ntrees, a spectral analog of the thin trees that played a crucial role in the\nrecent breakthrough on the asymmetric traveling salesman problem. We show a\npolynomial time algorithm that, given a graph where every edge has effective\nconductance at least $\\kappa$, returns an $O(\\kappa^{-1} \\cdot \\log n / \\log\n\\log n)$-spectrally-thin tree. There are further applications to rounding of\nsemidefinite programs, to the column subset selection problem, and to a\ngeometric question of extracting a nearly-orthonormal basis from an isotropic\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 21:19:00 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Harvey", "Nicholas J. A.", ""], ["Olver", "Neil", ""]]}, {"id": "1307.2313", "submitter": "Oren Weimann", "authors": "Pawel Gawrychowski, Shay Mozes and Oren Weimann", "title": "Improved Submatrix Maximum Queries in Monge Matrices", "comments": "Appeared in ICALP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient data structures for submatrix maximum queries in Monge\nmatrices and Monge partial matrices. For $n\\times n$ Monge matrices, we give a\ndata structure that requires O(n) space and answers submatrix maximum queries\nin $O(\\log n)$ time. The best previous data structure [Kaplan et al., SODA`12]\nrequired $O(n \\log n)$ space and $O(\\log^2 n)$ query time. We also give an\nalternative data structure with constant query-time and $ O(n^{1+\\varepsilon})$\nconstruction time and space for any fixed $\\varepsilon<1$. For $n\\times n$ {\\em\npartial} Monge matrices we obtain a data structure with O(n) space and $O(\\log\nn \\cdot \\alpha(n))$ query time. The data structure of Kaplan et al. required\n$O(n \\log n \\cdot \\alpha(n))$ space and $O(\\log^2 n)$ query time.\n  Our improvements are enabled by a technique for exploiting the structure of\nthe upper envelope of Monge matrices to efficiently report column maxima in\nskewed rectangular Monge matrices. We hope this technique can be useful in\nobtaining faster search algorithms in Monge partial matrices. In addition, we\ngive a linear upper bound on the number of breakpoints in the upper envelope of\na Monge partial matrix. This shows that the inverse Ackermann $\\alpha(n)$ term\nin the analysis of the data structure of Kaplan et. al is superfluous.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 01:14:48 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2013 14:57:38 GMT"}, {"version": "v3", "created": "Tue, 15 Apr 2014 17:30:39 GMT"}, {"version": "v4", "created": "Thu, 12 Oct 2017 12:20:39 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Gawrychowski", "Pawel", ""], ["Mozes", "Shay", ""], ["Weimann", "Oren", ""]]}, {"id": "1307.2347", "submitter": "Alexandru I. Tomescu", "authors": "Romeo Rizzi, Alexandru I. Tomescu", "title": "Combinatorial decomposition approaches for efficient counting and random\n  generation FPTASes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a combinatorial decomposition for a counting problem, we resort to the\nsimple scheme of approximating large numbers by floating-point representations\nin order to obtain efficient Fully Polynomial Time Approximation Schemes\n(FPTASes) for it. The number of bits employed for the exponent and the mantissa\nwill depend on the error parameter $0 < \\varepsilon \\leq 1$ and on the\ncharacteristics of the problem. Accordingly, we propose the first FPTASes with\n$1 \\pm \\varepsilon$ relative error for counting and generating uniformly at\nrandom a labeled DAG with a given number of vertices. This is accomplished\nstarting from a classical recurrence for counting DAGs, whose values we\napproximate by floating-point numbers.\n  After extending these results to other families of DAGs, we show how the same\napproach works also with problems where we are given a compact representation\nof a combinatorial ensemble and we are asked to count and sample elements from\nit. We employ here the floating-point approximation method to transform the\nclassic pseudo-polynomial algorithm for counting 0/1 Knapsack solutions into a\nvery simple FPTAS with $1 - \\varepsilon$ relative error. Its complexity\nimproves upon the recent result (\\v{S}tefankovi\\v{c} et al., SIAM J. Comput.,\n2012), and, when $\\varepsilon^{-1} = \\Omega(n)$, also upon the best-known\nrandomized algorithm (Dyer, STOC, 2003). To show the versatility of this\ntechnique, we also apply it to a recent generalization of the problem of\ncounting 0/1 Knapsack solutions in an arc-weighted DAG, obtaining a faster and\nsimpler FPTAS than the existing one.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 07:01:16 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2013 21:27:47 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Rizzi", "Romeo", ""], ["Tomescu", "Alexandru I.", ""]]}, {"id": "1307.2415", "submitter": "Orgad Keller", "authors": "Avinatan Hassidim, Orgad Keller, Moshe Lewenstein, Liam Roditty", "title": "Finding the Minimum-Weight k-Path", "comments": "To appear at WADS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a weighted $n$-vertex graph $G$ with integer edge-weights taken from a\nrange $[-M,M]$, we show that the minimum-weight simple path visiting $k$\nvertices can be found in time $\\tilde{O}(2^k \\poly(k) M n^\\omega) = O^*(2^k\nM)$. If the weights are reals in $[1,M]$, we provide a\n$(1+\\varepsilon)$-approximation which has a running time of $\\tilde{O}(2^k\n\\poly(k) n^\\omega(\\log\\log M + 1/\\varepsilon))$. For the more general problem\nof $k$-tree, in which we wish to find a minimum-weight copy of a $k$-node tree\n$T$ in a given weighted graph $G$, under the same restrictions on edge weights\nrespectively, we give an exact solution of running time $\\tilde{O}(2^k \\poly(k)\nM n^3) $ and a $(1+\\varepsilon)$-approximate solution of running time\n$\\tilde{O}(2^k \\poly(k) n^3(\\log\\log M + 1/\\varepsilon))$. All of the above\nalgorithms are randomized with a polynomially-small error probability.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 12:21:15 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Hassidim", "Avinatan", ""], ["Keller", "Orgad", ""], ["Lewenstein", "Moshe", ""], ["Roditty", "Liam", ""]]}, {"id": "1307.2520", "submitter": "Nirman Kumar", "authors": "Nirman Kumar and Benjamin Raichel", "title": "Fault Tolerant Clustering Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In discrete k-center and k-median clustering, we are given a set of points P\nin a metric space M, and the task is to output a set C \\subseteq ? P, |C| = k,\nsuch that the cost of clustering P using C is as small as possible. For\nk-center, the cost is the furthest a point has to travel to its nearest center,\nwhereas for k-median, the cost is the sum of all point to nearest center\ndistances. In the fault-tolerant versions of these problems, we are given an\nadditional parameter 1 ?\\leq \\ell \\leq ? k, such that when computing the cost\nof clustering, points are assigned to their \\ell-th nearest-neighbor in C,\ninstead of their nearest neighbor. We provide constant factor approximation\nalgorithms for these problems that are both conceptually simple and highly\npractical from an implementation stand-point.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 17:24:18 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Kumar", "Nirman", ""], ["Raichel", "Benjamin", ""]]}, {"id": "1307.2521", "submitter": "Geevarghese Philip", "authors": "Stefan Kratsch, Geevarghese Philip, Saurabh Ray", "title": "Point Line Cover: The Easy Kernel is Essentially Tight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input to the NP-hard Point Line Cover problem (PLC) consists of a set $P$\nof $n$ points on the plane and a positive integer $k$, and the question is\nwhether there exists a set of at most $k$ lines which pass through all points\nin $P$. A simple polynomial-time reduction reduces any input to one with at\nmost $k^2$ points. We show that this is essentially tight under standard\nassumptions. More precisely, unless the polynomial hierarchy collapses to its\nthird level, there is no polynomial-time algorithm that reduces every instance\n$(P,k)$ of PLC to an equivalent instance with $O(k^{2-\\epsilon})$ points, for\nany $\\epsilon>0$. This answers, in the negative, an open problem posed by\nLokshtanov (PhD Thesis, 2009).\n  Our proof uses the machinery for deriving lower bounds on the size of kernels\ndeveloped by Dell and van Melkebeek (STOC 2010). It has two main ingredients:\nWe first show, by reduction from Vertex Cover, that PLC---conditionally---has\nno kernel of total size $O(k^{2-\\epsilon})$ bits. This does not directly imply\nthe claimed lower bound on the number of points, since the best known\npolynomial-time encoding of a PLC instance with $n$ points requires\n$\\omega(n^{2})$ bits. To get around this we build on work of Goodman et al.\n(STOC 1989) and devise an oracle communication protocol of cost $O(n\\log n)$\nfor PLC; its main building block is a bound of $O(n^{O(n)})$ for the order\ntypes of $n$ points that are not necessarily in general position, and an\nexplicit algorithm that enumerates all possible order types of n points. This\nprotocol and the lower bound on total size together yield the stated lower\nbound on the number of points.\n  While a number of essentially tight polynomial lower bounds on total sizes of\nkernels are known, our result is---to the best of our knowledge---the first to\nshow a nontrivial lower bound for structural/secondary parameters.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 17:25:50 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Kratsch", "Stefan", ""], ["Philip", "Geevarghese", ""], ["Ray", "Saurabh", ""]]}, {"id": "1307.2531", "submitter": "Marcin Bienkowski", "authors": "Marcin Bienkowski, Jaroslaw Byrka, Marek Chrobak, {\\L}ukasz Je\\.z,\n  Ji\\v{r}\\'i Sgall", "title": "Better Approximation Bounds for the Joint Replenishment Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Joint Replenishment Problem (JRP) deals with optimizing shipments of\ngoods from a supplier to retailers through a shared warehouse. Each shipment\ninvolves transporting goods from the supplier to the warehouse, at a fixed cost\nC, followed by a redistribution of these goods from the warehouse to the\nretailers that ordered them, where transporting goods to a retailer $\\rho$ has\na fixed cost $c_\\rho$. In addition, retailers incur waiting costs for each\norder. The objective is to minimize the overall cost of satisfying all orders,\nnamely the sum of all shipping and waiting costs.\n  JRP has been well studied in Operations Research and, more recently, in the\narea of approximation algorithms. For arbitrary waiting cost functions, the\nbest known approximation ratio is 1.8. This ratio can be reduced to 1.574 for\nthe JRP-D model, where there is no cost for waiting but orders have deadlines.\nAs for hardness results, it is known that the problem is APX-hard and that the\nnatural linear program for JRP has integrality gap at least 1.245. Both results\nhold even for JRP-D. In the online scenario, the best lower and upper bounds on\nthe competitive ratio are 2.64 and 3, respectively. The lower bound of 2.64\napplies even to the restricted version of JRP, denoted JRP-L, where the waiting\ncost function is linear.\n  We provide several new approximation results for JRP. In the offline case, we\ngive an algorithm with ratio 1.791, breaking the barrier of 1.8. In the online\ncase, we show a lower bound of 2.754 on the competitive ratio for JRP-L (and\nthus JRP as well), improving the previous bound of 2.64. We also study the\nonline version of JRP-D, for which we prove that the optimal competitive ratio\nis 2.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 18:12:15 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Bienkowski", "Marcin", ""], ["Byrka", "Jaroslaw", ""], ["Chrobak", "Marek", ""], ["Je\u017c", "\u0141ukasz", ""], ["Sgall", "Ji\u0159\u00ed", ""]]}, {"id": "1307.2536", "submitter": "Andrew Mastin", "authors": "Andrew Mastin, Patrick Jaillet", "title": "Greedy Online Bipartite Matching on Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the average performance of online greedy matching algorithms on\n$G(n,n,p)$, the random bipartite graph with $n$ vertices on each side and edges\noccurring independently with probability $p=p(n)$. In the online model,\nvertices on one side of the graph are given up front while vertices on the\nother side arrive sequentially; when a vertex arrives its edges are revealed\nand it must be immediately matched or dropped. We begin by analyzing the\n\\textsc{oblivious} algorithm, which tries to match each arriving vertex to a\nrandom neighbor, even if the neighbor has already been matched. The algorithm\nis shown to have a performance ratio of at least $1-1/e$ for all monotonic\nfunctions $p(n)$, where the performance ratio is defined asymptotically as the\nratio of the expected matching size given by the algorithm to the expected\nmaximum matching size. Next we show that the conventional \\textsc{greedy}\nalgorithm, which assigns each vertex to a random unmatched neighbor, has a\nperformance ratio of at least 0.837 for all monotonic functions $p(n)$. Under\nthe $G(n,n,p)$ model, the performance of \\textsc{greedy} is equivalent to the\nperformance of the well known \\textsc{ranking} algorithm, so our results show\nthat \\textsc{ranking} has a performance ratio of at least 0.837. We finally\nconsider vertex-weighted bipartite matching. Our proofs are based on simple\ndifferential equations that describe the evolution of the matching process.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 18:24:09 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Mastin", "Andrew", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1307.2582", "submitter": "Adilson Enio Motter", "authors": "Sean P. Cornelius and Adilson E. Motter", "title": "NECO - A scalable algorithm for NEtwork COntrol", "comments": "Source codes available at\n  http://www.nature.com/protocolexchange/system/uploads/2647/original/neco_source.zip", "journal-ref": "Protocol Exchange (2013) doi:10.1038/protex.2013.063 Published\n  online 27 June 2013", "doi": "10.1038/protex.2013.063", "report-no": null, "categories": "math.OC cond-mat.dis-nn cs.DS nlin.AO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for the control of complex networks and other\nnonlinear, high-dimensional dynamical systems. The computational approach is\nbased on the recently-introduced concept of compensatory perturbations --\nintentional alterations to the state of a complex system that can drive it to a\ndesired target state even when there are constraints on the perturbations that\nforbid reaching the target state directly. Included here is ready-to-use\nsoftware that can be applied to identify eligible control interventions in a\ngeneral system described by coupled ordinary differential equations, whose\nspecific form can be specified by the user. The algorithm is highly scalable,\nwith the computational cost scaling as the number of dynamical variables to the\npower 2.5.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 20:05:10 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Cornelius", "Sean P.", ""], ["Motter", "Adilson E.", ""]]}, {"id": "1307.2696", "submitter": "Fei Chen", "authors": "T-H. Hubert Chan, Fei Chen, Xiaowei Wu and Zhichao Zhao", "title": "Ranking on Arbitrary Graphs: Rematch via Continuous LP with Monotone and\n  Boundary Condition Constraints", "comments": "Corrected references in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by online advertisement and exchange settings, greedy randomized\nalgorithms for the maximum matching problem have been studied, in which the\nalgorithm makes (random) decisions that are essentially oblivious to the input\ngraph. Any greedy algorithm can achieve performance ratio 0.5, which is the\nexpected number of matched nodes to the number of nodes in a maximum matching.\n  Since Aronson, Dyer, Frieze and Suen proved that the Modified Randomized\nGreedy (MRG) algorithm achieves performance ratio 0.5 + \\epsilon (where\n\\epsilon = frac{1}{400000}) on arbitrary graphs in the mid-nineties, no further\nattempts in the literature have been made to improve this theoretical ratio for\narbitrary graphs until two papers were published in FOCS 2012. Poloczek and\nSzegedy also analyzed the MRG algorithm to give ratio 0.5039, while Goel and\nTripathi used experimental techniques to analyze the Ranking algorithm to give\nratio 0.56. However, we could not reproduce the experimental results of Goel\nand Tripathi.\n  In this paper, we revisit the Ranking algorithm using the LP framework.\nSpecial care is given to analyze the structural properties of the Ranking\nalgorithm in order to derive the LP constraints, of which one known as the\n\\emph{boundary} constraint requires totally new analysis and is crucial to the\nsuccess of our LP.\n  We use continuous LP relaxation to analyze the limiting behavior as the\nfinite LP grows. Of particular interest are new duality and complementary\nslackness characterizations that can handle the monotone and the boundary\nconstraints in continuous LP. We believe our work achieves the currently best\ntheoretical performance ratio of \\frac{2(5-\\sqrt{7})}{9} \\approx 0.523 on\narbitrary graphs. Moreover, experiments suggest that Ranking cannot perform\nbetter than 0.724 in general.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 06:52:19 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2013 05:43:25 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Chan", "T-H. Hubert", ""], ["Chen", "Fei", ""], ["Wu", "Xiaowei", ""], ["Zhao", "Zhichao", ""]]}, {"id": "1307.2724", "submitter": "A. Emre Cetin", "authors": "A. Emre Cetin", "title": "The technique of in-place associative sorting", "comments": "34 Pages. arXiv admin note: substantial text overlap with\n  arXiv:1209.0572, arXiv:1210.1771, arXiv:1209.3668, arXiv:1209.1942,\n  arXiv:1209.4714", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the first place, a novel, yet straightforward in-place integer\nvalue-sorting algorithm is presented. It sorts in linear time using constant\namount of additional memory for storing counters and indices beside the input\narray. The technique is inspired from the principal idea behind one of the\nordinal theories of \"serial order in behavior\" and explained by the analogy\nwith the three main stages in the formation and retrieval of memory in\ncognitive neuroscience: (i) practicing, (ii) storage and (iii) retrieval. It is\nfurther improved in terms of time complexity as well as specialized for\ndistinct integers, though still improper for rank-sorting.\n  Afterwards, another novel, yet straightforward technique is introduced which\nmakes this efficient value-sorting technique proper for rank-sorting. Hence,\ngiven an array of n elements each have an integer key, the technique sorts the\nelements according to their integer keys in linear time using only constant\namount of additional memory. The devised technique is very practical and\nefficient outperforming bucket sort, distribution counting sort and address\ncalculation sort family of algorithms making it attractive in almost every case\neven when space is not a critical resource.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 09:13:35 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Cetin", "A. Emre", ""]]}, {"id": "1307.2735", "submitter": "Shri Prakash Dwivedi", "authors": "Shri Prakash Dwivedi", "title": "An Efficient Multiplication Algorithm Using Nikhilam Method", "comments": "Extended version to appear in ITC 2013", "journal-ref": null, "doi": "10.1049/cp.2013.2209", "report-no": null, "categories": "cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication is one of the most important operation in computer arithmetic.\nMany integer operations such as squaring, division and computing reciprocal\nrequire same order of time as multiplication whereas some other operations such\nas computing GCD and residue operation require at most a factor of $\\log n$\ntime more than multiplication. We propose an integer multiplication algorithm\nusing Nikhilam method of Vedic mathematics which can be used to multiply two\nbinary numbers efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 09:58:07 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Dwivedi", "Shri Prakash", ""]]}, {"id": "1307.2806", "submitter": "Max Klimm", "authors": "Yann Disser and Max Klimm and Nicole Megow and Sebastian Stiller", "title": "Packing a Knapsack of Unknown Capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of packing a knapsack without knowing its capacity.\nWhenever we attempt to pack an item that does not fit, the item is discarded;\nif the item fits, we have to include it in the packing. We show that there is\nalways a policy that packs a value within factor 2 of the optimum packing,\nirrespective of the actual capacity. If all items have unit density, we achieve\na factor equal to the golden ratio. Both factors are shown to be best possible.\nIn fact, we obtain the above factors using packing policies that are universal\nin the sense that they fix a particular order of the items and try to pack the\nitems in this order, independent of the observations made while packing. We\ngive efficient algorithms computing these policies. On the other hand, we show\nthat, for any alpha>1, the problem of deciding whether a given universal policy\nachieves a factor of alpha is coNP-complete. If alpha is part of the input, the\nsame problem is shown to be coNP-complete for items with unit densities.\nFinally, we show that it is coNP-hard to decide, for given alpha, whether a set\nof items admits a universal policy with factor alpha, even if all items have\nunit densities.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 14:40:38 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Disser", "Yann", ""], ["Klimm", "Max", ""], ["Megow", "Nicole", ""], ["Stiller", "Sebastian", ""]]}, {"id": "1307.2808", "submitter": "Jian Li", "authors": "Mohammadtaghi Hajiaghayi, Wei Hu, Jian Li, Shi Li, Barna Saha", "title": "A Constant Factor Approximation Algorithm for Fault-Tolerant k-Median", "comments": "19 pages", "journal-ref": null, "doi": "10.1137/1.9781611973402.1", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the fault-tolerant $k$-median problem and give the\n\\emph{first} constant factor approximation algorithm for it. In the\nfault-tolerant generalization of classical $k$-median problem, each client $j$\nneeds to be assigned to at least $r_j \\ge 1$ distinct open facilities. The\nservice cost of $j$ is the sum of its distances to the $r_j$ facilities, and\nthe $k$-median constraint restricts the number of open facilities to at most\n$k$. Previously, a constant factor was known only for the special case when all\n$r_j$s are the same, and a logarithmic approximation ratio for the general\ncase. In addition, we present the first polynomial time algorithm for the\nfault-tolerant $k$-median problem on a path or a HST by showing that the\ncorresponding LP always has an integral optimal solution.\n  We also consider the fault-tolerant facility location problem, where the\nservice cost of $j$ can be a weighted sum of its distance to the $r_j$\nfacilities. We give a simple constant factor approximation algorithm,\ngeneralizing several previous results which only work for nonincreasing weight\nvectors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 14:41:58 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Hajiaghayi", "Mohammadtaghi", ""], ["Hu", "Wei", ""], ["Li", "Jian", ""], ["Li", "Shi", ""], ["Saha", "Barna", ""]]}, {"id": "1307.2855", "submitter": "Zeyuan Allen Zhu", "authors": "Lorenzo Orecchia, Zeyuan Allen Zhu", "title": "Flow-Based Algorithms for Local Graph Clustering", "comments": "A shorter version of this paper has appeared in the proceedings of\n  the 25th ACM-SIAM Symposium on Discrete Algorithms (SODA) 2014", "journal-ref": null, "doi": "10.1137/1.9781611973402.94", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a subset S of vertices of an undirected graph G, the cut-improvement\nproblem asks us to find a subset S that is similar to A but has smaller\nconductance. A very elegant algorithm for this problem has been given by\nAndersen and Lang [AL08] and requires solving a small number of\nsingle-commodity maximum flow computations over the whole graph G. In this\npaper, we introduce LocalImprove, the first cut-improvement algorithm that is\nlocal, i.e. that runs in time dependent on the size of the input set A rather\nthan on the size of the entire graph. Moreover, LocalImprove achieves this\nlocal behaviour while essentially matching the same theoretical guarantee as\nthe global algorithm of Andersen and Lang.\n  The main application of LocalImprove is to the design of better\nlocal-graph-partitioning algorithms. All previously known local algorithms for\ngraph partitioning are random-walk based and can only guarantee an output\nconductance of O(\\sqrt{OPT}) when the target set has conductance OPT \\in [0,1].\nVery recently, Zhu, Lattanzi and Mirrokni [ZLM13] improved this to O(OPT /\n\\sqrt{CONN}) where the internal connectivity parameter CONN \\in [0,1] is\ndefined as the reciprocal of the mixing time of the random walk over the graph\ninduced by the target set. In this work, we show how to use LocalImprove to\nobtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). This\nyields the first flow-based algorithm. Moreover, its performance strictly\noutperforms the ones based on random walks and surprisingly matches that of the\nbest known global algorithm, which is SDP-based, in this parameter regime\n[MMV12].\n  Finally, our results show that spectral methods are not the only viable\napproach to the construction of local graph partitioning algorithm and open\ndoor to the study of algorithms with even better approximation and locality\nguarantees.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 17:04:35 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2013 19:44:03 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Orecchia", "Lorenzo", ""], ["Zhu", "Zeyuan Allen", ""]]}, {"id": "1307.2863", "submitter": "Vojta Tuma", "authors": "Zdenek Dvorak, Martin Kupec, Vojtech Tuma", "title": "Dynamic Data Structure for Tree-Depth Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic data structure for representing a graph $G$ with\ntree-depth at most $D$. Tree-depth is an important graph parameter which arose\nin the study of sparse graph classes.\n  The structure allows addition and removal of edges and vertices such that the\nresulting graph still has tree-depth at most $D$, in time bounds depending only\non $D$. A tree-depth decomposition of the graph is maintained explicitly.\n  This makes the data structure useful for dynamization of static algorithms\nfor graphs with bounded tree-depth. As an example application, we give a\ndynamic data structure for MSO-property testing, with time bounds for removal\ndepending only on $D$ and constant-time testing of the property, while the time\nfor the initialization and insertion also depends on the size of the formula\nexpressing the property.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 17:47:20 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Dvorak", "Zdenek", ""], ["Kupec", "Martin", ""], ["Tuma", "Vojtech", ""]]}, {"id": "1307.2908", "submitter": "Haris Aziz", "authors": "Haris Aziz and Chun Ye", "title": "Cake Cutting Algorithms for Piecewise Constant and Piecewise Uniform\n  Valuations", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cake cutting is one of the most fundamental settings in fair division and\nmechanism design without money. In this paper, we consider different levels of\nthree fundamental goals in cake cutting: fairness, Pareto optimality, and\nstrategyproofness. In particular, we present robust versions of envy-freeness\nand proportionality that are not only stronger than their standard\ncounter-parts but also have less information requirements. We then focus on\ncake cutting with piecewise constant valuations and present three desirable\nalgorithms: CCEA (Controlled Cake Eating Algorithm), MEA (Market Equilibrium\nAlgorithm) and CSD (Constrained Serial Dictatorship). CCEA is polynomial-time,\nrobust envy-free, and non-wasteful. It relies on parametric network flows and\nrecent generalizations of the probabilistic serial algorithm. For the subdomain\nof piecewise uniform valuations, we show that it is also group-strategyproof.\nThen, we show that there exists an algorithm (MEA) that is polynomial-time,\nenvy-free, proportional, and Pareto optimal. MEA is based on computing a\nmarket-based equilibrium via a convex program and relies on the results of\nReijnierse and Potters [24] and Devanur et al. [15]. Moreover, we show that MEA\nand CCEA are equivalent to mechanism 1 of Chen et. al. [12] for piecewise\nuniform valuations. We then present an algorithm CSD and a way to implement it\nvia randomization that satisfies strategyproofness in expectation, robust\nproportionality, and unanimity for piecewise constant valuations. For the case\nof two agents, it is robust envy-free, robust proportional, strategyproof, and\npolynomial-time. Many of our results extend to more general settings in cake\ncutting that allow for variable claims and initial endowments. We also show a\nfew impossibility results to complement our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 20:02:18 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2013 23:42:28 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2013 10:19:16 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Aziz", "Haris", ""], ["Ye", "Chun", ""]]}, {"id": "1307.2982", "submitter": "Mohammad Norouzi", "authors": "Mohammad Norouzi, Ali Punjani, David J. Fleet", "title": "Fast Exact Search in Hamming Space with Multi-Index Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in representing image data and feature descriptors\nusing compact binary codes for fast near neighbor search. Although binary codes\nare motivated by their use as direct indices (addresses) into a hash table,\ncodes longer than 32 bits are not being used as such, as it was thought to be\nineffective. We introduce a rigorous way to build multiple hash tables on\nbinary code substrings that enables exact k-nearest neighbor search in Hamming\nspace. The approach is storage efficient and straightforward to implement.\nTheoretical analysis shows that the algorithm exhibits sub-linear run-time\nbehavior for uniformly distributed codes. Empirical results show dramatic\nspeedups over a linear scan baseline for datasets of up to one billion codes of\n64, 128, or 256 bits.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 05:52:21 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2013 02:36:21 GMT"}, {"version": "v3", "created": "Fri, 25 Apr 2014 01:31:55 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Norouzi", "Mohammad", ""], ["Punjani", "Ali", ""], ["Fleet", "David J.", ""]]}, {"id": "1307.3033", "submitter": "Armin Wei{\\ss}", "authors": "Stefan Edelkamp, Armin Wei{\\ss}", "title": "QuickXsort: Efficient Sorting with n log n - 1.399n +o(n) Comparisons on\n  Average", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we generalize the idea of QuickHeapsort leading to the notion\nof QuickXsort. Given some external sorting algorithm X, QuickXsort yields an\ninternal sorting algorithm if X satisfies certain natural conditions.\n  With QuickWeakHeapsort and QuickMergesort we present two examples for the\nQuickXsort-construction. Both are efficient algorithms that incur approximately\nn log n - 1.26n +o(n) comparisons on the average. A worst case of n log n +\nO(n) comparisons can be achieved without significantly affecting the average\ncase.\n  Furthermore, we describe an implementation of MergeInsertion for small n.\nTaking MergeInsertion as a base case for QuickMergesort, we establish a\nworst-case efficient sorting algorithm calling for n log n - 1.3999n + o(n)\ncomparisons on average. QuickMergesort with constant size base cases shows the\nbest performance on practical inputs: when sorting integers it is slower by\nonly 15% to STL-Introsort.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 09:46:33 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Edelkamp", "Stefan", ""], ["Wei\u00df", "Armin", ""]]}, {"id": "1307.3073", "submitter": "Sylvain Guillemot", "authors": "Sylvain Guillemot and D\\'aniel Marx", "title": "Finding small patterns in permutations in linear time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two permutations $\\sigma$ and $\\pi$, the \\textsc{Permutation Pattern}\nproblem asks if $\\sigma$ is a subpattern of $\\pi$. We show that the problem can\nbe solved in time $2^{O(\\ell^2\\log \\ell)}\\cdot n$, where $\\ell=|\\sigma|$ and\n$n=|\\pi|$. In other words, the problem is fixed-parameter tractable\nparameterized by the size of the subpattern to be found.\n  We introduce a novel type of decompositions for permutations and a\ncorresponding width measure. We present a linear-time algorithm that either\nfinds $\\sigma$ as a subpattern of $\\pi$, or finds a decomposition of $\\pi$\nwhose width is bounded by a function of $|\\sigma|$. Then we show how to solve\nthe \\textsc{Permutation Pattern} problem in linear time if a bounded-width\ndecomposition is given in the input.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 11:50:48 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2013 10:30:13 GMT"}], "update_date": "2013-11-01", "authors_parsed": [["Guillemot", "Sylvain", ""], ["Marx", "D\u00e1niel", ""]]}, {"id": "1307.3080", "submitter": "Erez Kantor", "authors": "Erez Kantor and Shay Kutten", "title": "Optimal competitiveness for Symmetric Rectilinear Steiner Arborescence\n  and related problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present optimal competitive algorithms for two interrelated known problems\ninvolving Steiner Arborescence. One is the continuous problem of the Symmetric\nRectilinear Steiner Arborescence (SRSA), studied by Berman and Coulston.\n  A very related, but discrete problem (studied separately in the past) is the\nonline Multimedia Content Delivery (MCD) problem on line networks, presented\noriginally by Papadimitriu, Ramanathan, and Rangan. An efficient content\ndelivery was modeled as a low cost Steiner arborescence in a grid of\nnetwork*time they defined. We study here the version studied by Charikar,\nHalperin, and Motwani (who used the same problem definitions, but removed some\nconstraints on the inputs).\n  The bounds on the competitive ratios introduced separately in the above\npapers are similar for the two problems: O(log N) for the continuous problem\nand O(log n) for the network problem, where N was the number of terminals to\nserve, and n was the size of the network. The lower bounds were Omega(sqrt{log\nN}) and Omega(sqrt{log n}) correspondingly. Berman and Coulston conjectured\nthat both the upper bound and the lower bound could be improved.\n  We disprove this conjecture and close these quadratic gaps for both problems.\nWe first present an O(sqrt{log n}) deterministic competitive algorithm for MCD\non the line, matching the lower bound. We then translate this algorithm to\nbecome a competitive optimal algorithm O(sqrt{log N}) for SRSA. Finally, we\ntranslate the latter back to solve MCD problem, this time competitive optimally\neven in the case that the number of requests is small (that is, O(min{sqrt{log\nn},sqrt{log N}})). We also present a Omega(sqrt[3]{log n}) lower bound on the\ncompetitiveness of any randomized algorithm. Some of the techniques may be\nuseful in other contexts.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 12:02:18 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Kantor", "Erez", ""], ["Kutten", "Shay", ""]]}, {"id": "1307.3102", "submitter": "Vitaly Feldman", "authors": "Maria Florina Balcan, Vitaly Feldman", "title": "Statistical Active Learning Algorithms for Noise Tolerance and\n  Differential Privacy", "comments": "Extended abstract appears in NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a framework for designing efficient active learning algorithms\nthat are tolerant to random classification noise and are\ndifferentially-private. The framework is based on active learning algorithms\nthat are statistical in the sense that they rely on estimates of expectations\nof functions of filtered random examples. It builds on the powerful statistical\nquery framework of Kearns (1993).\n  We show that any efficient active statistical learning algorithm can be\nautomatically converted to an efficient active learning algorithm which is\ntolerant to random classification noise as well as other forms of\n\"uncorrelated\" noise. The complexity of the resulting algorithms has\ninformation-theoretically optimal quadratic dependence on $1/(1-2\\eta)$, where\n$\\eta$ is the noise rate.\n  We show that commonly studied concept classes including thresholds,\nrectangles, and linear separators can be efficiently actively learned in our\nframework. These results combined with our generic conversion lead to the first\ncomputationally-efficient algorithms for actively learning some of these\nconcept classes in the presence of random classification noise that provide\nexponential improvement in the dependence on the error $\\epsilon$ over their\npassive counterparts. In addition, we show that our algorithms can be\nautomatically converted to efficient active differentially-private algorithms.\nThis leads to the first differentially-private active learning algorithms with\nexponential label savings over the passive case.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 13:31:21 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2013 02:13:05 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 05:38:04 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 06:41:07 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Feldman", "Vitaly", ""]]}, {"id": "1307.3192", "submitter": "Thomas Kesselheim", "authors": "Oliver G\\\"obel, Martin Hoefer, Thomas Kesselheim, Thomas Schleiden,\n  Berthold V\\\"ocking", "title": "Online Independent Set Beyond the Worst-Case: Secretaries, Prophets, and\n  Periods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate online algorithms for maximum (weight) independent set on\ngraph classes with bounded inductive independence number like, e.g., interval\nand disk graphs with applications to, e.g., task scheduling and spectrum\nallocation. In the online setting, it is assumed that nodes of an unknown graph\narrive one by one over time. An online algorithm has to decide whether an\narriving node should be included into the independent set. Unfortunately, this\nnatural and practically relevant online problem cannot be studied in a\nmeaningful way within a classical competitive analysis as the competitive ratio\non worst-case input sequences is lower bounded by $\\Omega(n)$.\n  As a worst-case analysis is pointless, we study online independent set in a\nstochastic analysis. Instead of focussing on a particular stochastic input\nmodel, we present a generic sampling approach that enables us to devise online\nalgorithms achieving performance guarantees for a variety of input models. In\nparticular, our analysis covers stochastic input models like the secretary\nmodel, in which an adversarial graph is presented in random order, and the\nprophet-inequality model, in which a randomly generated graph is presented in\nadversarial order. Our sampling approach bridges thus between stochastic input\nmodels of quite different nature. In addition, we show that our approach can be\napplied to a practically motivated admission control setting.\n  Our sampling approach yields an online algorithm for maximum independent set\nwith competitive ratio $O(\\rho^2)$ with respect to all of the mentioned\nstochastic input models. for graph classes with inductive independence number\n$\\rho$. The approach can be extended towards maximum-weight independent set by\nlosing only a factor of $O(\\log n)$ in the competitive ratio with $n$ denoting\nthe (expected) number of nodes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 17:28:37 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["G\u00f6bel", "Oliver", ""], ["Hoefer", "Martin", ""], ["Kesselheim", "Thomas", ""], ["Schleiden", "Thomas", ""], ["V\u00f6cking", "Berthold", ""]]}, {"id": "1307.3301", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Jan Vondrak", "title": "Optimal Bounds on Approximation of Submodular and XOS Functions by\n  Juntas", "comments": "Extended abstract appears in proceedings of FOCS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the approximability of several classes of real-valued\nfunctions by functions of a small number of variables ({\\em juntas}). Our main\nresults are tight bounds on the number of variables required to approximate a\nfunction $f:\\{0,1\\}^n \\rightarrow [0,1]$ within $\\ell_2$-error $\\epsilon$ over\nthe uniform distribution: 1. If $f$ is submodular, then it is $\\epsilon$-close\nto a function of $O(\\frac{1}{\\epsilon^2} \\log \\frac{1}{\\epsilon})$ variables.\nThis is an exponential improvement over previously known results. We note that\n$\\Omega(\\frac{1}{\\epsilon^2})$ variables are necessary even for linear\nfunctions. 2. If $f$ is fractionally subadditive (XOS) it is $\\epsilon$-close\nto a function of $2^{O(1/\\epsilon^2)}$ variables. This result holds for all\nfunctions with low total $\\ell_1$-influence and is a real-valued analogue of\nFriedgut's theorem for boolean functions. We show that $2^{\\Omega(1/\\epsilon)}$\nvariables are necessary even for XOS functions.\n  As applications of these results, we provide learning algorithms over the\nuniform distribution. For XOS functions, we give a PAC learning algorithm that\nruns in time $2^{poly(1/\\epsilon)} poly(n)$. For submodular functions we give\nan algorithm in the more demanding PMAC learning model (Balcan and Harvey,\n2011) which requires a multiplicative $1+\\gamma$ factor approximation with\nprobability at least $1-\\epsilon$ over the target distribution. Our uniform\ndistribution algorithm runs in time $2^{poly(1/(\\gamma\\epsilon))} poly(n)$.\nThis is the first algorithm in the PMAC model that over the uniform\ndistribution can achieve a constant approximation factor arbitrarily close to 1\nfor all submodular functions. As follows from the lower bounds in (Feldman et\nal., 2013) both of these algorithms are close to optimal. We also give\napplications for proper learning, testing and agnostic learning with value\nqueries of these classes.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 00:41:01 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2013 19:06:49 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2015 07:13:28 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Feldman", "Vitaly", ""], ["Vondrak", "Jan", ""]]}, {"id": "1307.3621", "submitter": "Ilias Diakonikolas", "authors": "Constantinos Daskalakis, Anindya De, Ilias Diakonikolas, Ankur Moitra,\n  Rocco A. Servedio", "title": "A Polynomial-time Approximation Scheme for Fault-tolerant Distributed\n  Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem which has received considerable attention in systems\nliterature because of its applications to routing in delay tolerant networks\nand replica placement in distributed storage systems. In abstract terms the\nproblem can be stated as follows: Given a random variable $X$ generated by a\nknown product distribution over $\\{0,1\\}^n$ and a target value $0 \\leq \\theta\n\\leq 1$, output a non-negative vector $w$, with $\\|w\\|_1 \\le 1$, which\nmaximizes the probability of the event $w \\cdot X \\ge \\theta$. This is a\nchallenging non-convex optimization problem for which even computing the value\n$\\Pr[w \\cdot X \\ge \\theta]$ of a proposed solution vector $w$ is #P-hard.\n  We provide an additive EPTAS for this problem which, for constant-bounded\nproduct distributions, runs in $ \\poly(n) \\cdot 2^{\\poly(1/\\eps)}$ time and\noutputs an $\\eps$-approximately optimal solution vector $w$ for this problem.\nOur approach is inspired by, and extends, recent structural results from the\ncomplexity-theoretic study of linear threshold functions. Furthermore, in spite\nof the objective function being non-smooth, we give a \\emph{unicriterion} PTAS\nwhile previous work for such objective functions has typically led to a\n\\emph{bicriterion} PTAS. We believe our techniques may be applicable to get\nunicriterion PTAS for other non-smooth objective functions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 07:41:01 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["De", "Anindya", ""], ["Diakonikolas", "Ilias", ""], ["Moitra", "Ankur", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1307.3650", "submitter": "Thomas Kalinowski", "authors": "Natashia Boland, Thomas Kalinowski, Simranjit Kaur", "title": "Scheduling arc shut downs in a network to maximize flow over time with a\n  bounded number of jobs per time period", "comments": null, "journal-ref": "Journal of Combinatorial Optimization, Volume 32, Issue 3, pp\n  885-905, 2016", "doi": "10.1007/s10878-015-9910-x", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of scheduling maintenance on arcs of a capacitated\nnetwork so as to maximize the total flow from a source node to a sink node over\na set of time periods. Maintenance on an arc shuts down the arc for the\nduration of the period in which its maintenance is scheduled, making its\ncapacity zero for that period. A set of arcs is designated to have maintenance\nduring the planning period, which will require each to be shut down for exactly\none time period. In general this problem is known to be NP-hard, and several\nspecial instance classes have been studied. Here we propose an additional\nconstraint which limits the number of maintenance jobs per time period, and we\nstudy the impact of this on the complexity.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 14:55:12 GMT"}, {"version": "v2", "created": "Thu, 1 Jan 2015 23:24:08 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 21:47:41 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Boland", "Natashia", ""], ["Kalinowski", "Thomas", ""], ["Kaur", "Simranjit", ""]]}, {"id": "1307.3692", "submitter": "Shen Chen Xu", "authors": "Gary L. Miller, Richard Peng, Shen Chen Xu", "title": "Parallel Graph Decompositions Using Random Shifts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an improved parallel algorithm for decomposing an undirected\nunweighted graph into small diameter pieces with a small fraction of the edges\nin between. These decompositions form critical subroutines in a number of graph\nalgorithms. Our algorithm builds upon the shifted shortest path approach\nintroduced in [Blelloch, Gupta, Koutis, Miller, Peng, Tangwongsan, SPAA 2011].\nBy combining various stages of the previous algorithm, we obtain a\nsignificantly simpler algorithm with the same asymptotic guarantees as the best\nsequential algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 03:41:36 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Miller", "Gary L.", ""], ["Peng", "Richard", ""], ["Xu", "Shen Chen", ""]]}, {"id": "1307.3699", "submitter": "Kai-Min Chung", "authors": "Kai-Min Chung, Zhenming Liu, Rafael Pass", "title": "Statistically-secure ORAM with $\\tilde{O}(\\log^2 n)$ Overhead", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a simple, statistically secure, ORAM with computational\noverhead $\\tilde{O}(\\log^2 n)$; previous ORAM protocols achieve only\ncomputational security (under computational assumptions) or require\n$\\tilde{\\Omega}(\\log^3 n)$ overheard. An additional benefit of our ORAM is its\nconceptual simplicity, which makes it easy to implement in both software and\n(commercially available) hardware.\n  Our construction is based on recent ORAM constructions due to Shi, Chan,\nStefanov, and Li (Asiacrypt 2011) and Stefanov and Shi (ArXiv 2012), but with\nsome crucial modifications in the algorithm that simplifies the ORAM and enable\nour analysis. A central component in our analysis is reducing the analysis of\nour algorithm to a \"supermarket\" problem; of independent interest (and of\nimportance to our analysis,) we provide an upper bound on the rate of \"upset\"\ncustomers in the \"supermarket\" problem.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 04:32:12 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Chung", "Kai-Min", ""], ["Liu", "Zhenming", ""], ["Pass", "Rafael", ""]]}, {"id": "1307.3736", "submitter": "Pablo D. Azar", "authors": "Pablo D. Azar, Robert Kleinberg, S. Matthew Weinberg", "title": "Prophet Inequalities with Limited Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical prophet inequality, a gambler observes a sequence of\nstochastic rewards $V_1,...,V_n$ and must decide, for each reward $V_i$,\nwhether to keep it and stop the game or to forfeit the reward forever and\nreveal the next value $V_i$. The gambler's goal is to obtain a constant\nfraction of the expected reward that the optimal offline algorithm would get.\nRecently, prophet inequalities have been generalized to settings where the\ngambler can choose $k$ items, and, more generally, where he can choose any\nindependent set in a matroid. However, all the existing algorithms require the\ngambler to know the distribution from which the rewards $V_1,...,V_n$ are\ndrawn.\n  The assumption that the gambler knows the distribution from which\n$V_1,...,V_n$ are drawn is very strong. Instead, we work with the much simpler\nassumption that the gambler only knows a few samples from this distribution. We\nconstruct the first single-sample prophet inequalities for many settings of\ninterest, whose guarantees all match the best possible asymptotically,\n\\emph{even with full knowledge of the distribution}. Specifically, we provide a\nnovel single-sample algorithm when the gambler can choose any $k$ elements\nwhose analysis is based on random walks with limited correlation. In addition,\nwe provide a black-box method for converting specific types of solutions to the\nrelated \\emph{secretary problem} to single-sample prophet inequalities, and\napply it to several existing algorithms. Finally, we provide a constant-sample\nprophet inequality for constant-degree bipartite matchings.\n  We apply these results to design the first posted-price and multi-dimensional\nauction mechanisms with limited information in settings with asymmetric\nbidders.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 13:26:26 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Azar", "Pablo D.", ""], ["Kleinberg", "Robert", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1307.3757", "submitter": "Anupam Gupta", "authors": "Albert Gu, Anupam Gupta, Amit Kumar", "title": "The Power of Deferral: Maintaining a Constant-Competitive Steiner Tree\n  Online", "comments": "An extended abstract appears in the 45th ACM Symposium on the Theory\n  of Computing (STOC), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online Steiner tree problem, a sequence of points is revealed\none-by-one: when a point arrives, we only have time to add a single edge\nconnecting this point to the previous ones, and we want to minimize the total\nlength of edges added. For two decades, we know that the greedy algorithm\nmaintains a tree whose cost is O(log n) times the Steiner tree cost, and this\nis best possible. But suppose, in addition to the new edge we add, we can\nchange a single edge from the previous set of edges: can we do much better? Can\nwe maintain a tree that is constant-competitive?\n  We answer this question in the affirmative. We give a primal-dual algorithm,\nand a novel dual-based analysis, that makes only a single swap per step (in\naddition to adding the edge connecting the new point to the previous ones), and\nsuch that the tree's cost is only a constant times the optimal cost.\n  Previous results for this problem gave an algorithm that performed an\namortized constant number of swaps: for each n, the number of swaps in the\nfirst n steps was O(n). We also give a simpler tight analysis for this\namortized case.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 17:19:41 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 02:34:40 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Gu", "Albert", ""], ["Gupta", "Anupam", ""], ["Kumar", "Amit", ""]]}, {"id": "1307.3794", "submitter": "Umang Bhaskar", "authors": "Umang Bhaskar, Katrina Ligett and Leonard J. Schulman", "title": "The Network Improvement Problem for Equilibrium Routing", "comments": "27 pages (including abstract), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In routing games, agents pick their routes through a network to minimize\ntheir own delay. A primary concern for the network designer in routing games is\nthe average agent delay at equilibrium. A number of methods to control this\naverage delay have received substantial attention, including network tolls,\nStackelberg routing, and edge removal.\n  A related approach with arguably greater practical relevance is that of\nmaking investments in improvements to the edges of the network, so that, for a\ngiven investment budget, the average delay at equilibrium in the improved\nnetwork is minimized. This problem has received considerable attention in the\nliterature on transportation research and a number of different algorithms have\nbeen studied. To our knowledge, none of this work gives guarantees on the\noutput quality of any polynomial-time algorithm. We study a model for this\nproblem introduced in transportation research literature, and present both\nhardness results and algorithms that obtain nearly optimal performance\nguarantees.\n  - We first show that a simple algorithm obtains good approximation guarantees\nfor the problem. Despite its simplicity, we show that for affine delays the\napproximation ratio of 4/3 obtained by the algorithm cannot be improved.\n  - To obtain better results, we then consider restricted topologies. For\ngraphs consisting of parallel paths with affine delay functions we give an\noptimal algorithm. However, for graphs that consist of a series of parallel\nlinks, we show the problem is weakly NP-hard.\n  - Finally, we consider the problem in series-parallel graphs, and give an\nFPTAS for this case.\n  Our work thus formalizes the intuition held by transportation researchers\nthat the network improvement problem is hard, and presents topology-dependent\nalgorithms that have provably tight approximation guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 23:16:23 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2013 20:32:57 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Bhaskar", "Umang", ""], ["Ligett", "Katrina", ""], ["Schulman", "Leonard J.", ""]]}, {"id": "1307.3822", "submitter": "Bang Ye Wu", "authors": "Bang Ye Wu", "title": "A simple approximation algorithm for the internal Steiner minimum tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a metric graph $G=(V,E)$ and $R\\subset V$, the internal Steiner minimum\ntree problem asks for a minimum weight Steiner tree spanning $R$ such that\nevery vertex in $R$ is not a leaf. This note shows a simple polynomial-time\n$2\\rho$-approximation algorithm, in which $\\rho$ is the approximation ratio for\nthe Steiner minimum tree problem. The result improves the previous best\napproximation ratio $2\\rho+1$ for the problem. The ratio is not currently best\nbut the algorithm is very simple.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 05:56:54 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 08:12:23 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Wu", "Bang Ye", ""]]}, {"id": "1307.3872", "submitter": "Andrea Farruggia", "authors": "Andrea Farruggia, Paolo Ferragina, Antonio Frangioni, Rossano\n  Venturini", "title": "Bicriteria data compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of massive datasets (and the consequent design of high-performing\ndistributed storage systems) have reignited the interest of the scientific and\nengineering community towards the design of lossless data compressors which\nachieve effective compression ratio and very efficient decompression speed.\nLempel-Ziv's LZ77 algorithm is the de facto choice in this scenario because of\nits decompression speed and its flexibility in trading decompression speed\nversus compressed-space efficiency. Each of the existing implementations offers\na trade-off between space occupancy and decompression speed, so software\nengineers have to content themselves by picking the one which comes closer to\nthe requirements of the application in their hands. Starting from these\npremises, and for the first time in the literature, we address in this paper\nthe problem of trading optimally, and in a principled way, the consumption of\nthese two resources by introducing the Bicriteria LZ77-Parsing problem, which\nformalizes in a principled way what data-compressors have traditionally\napproached by means of heuristics. The goal is to determine an LZ77 parsing\nwhich minimizes the space occupancy in bits of the compressed file, provided\nthat the decompression time is bounded by a fixed amount (or vice-versa). This\nway, the software engineer can set its space (or time) requirements and then\nderive the LZ77 parsing which optimizes the decompression speed (or the space\noccupancy, respectively). We solve this problem efficiently in O(n log^2 n)\ntime and optimal linear space within a small, additive approximation, by\nproving and deploying some specific structural properties of the weighted graph\nderived from the possible LZ77-parsings of the input file. The preliminary set\nof experiments shows that our novel proposal dominates all the highly\nengineered competitors, hence offering a win-win situation in theory&practice.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 10:14:56 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Farruggia", "Andrea", ""], ["Ferragina", "Paolo", ""], ["Frangioni", "Antonio", ""], ["Venturini", "Rossano", ""]]}, {"id": "1307.3877", "submitter": "A. Emre Cetin", "authors": "A. Emre Cetin", "title": "Idempotent permutations", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Together with a characteristic function, idempotent permutations uniquely\ndetermine idempotent maps, as well as their linearly ordered arrangement\nsimultaneously. Furthermore, in-place linear time transformations are possible\nbetween them. Hence, they may be important for succinct data structures,\ninformation storing, sorting and searching.\n  In this study, their combinatorial interpretation is given and their\napplication on sorting is examined. Given an array of n integer keys each in\n[1,n], if it is allowed to modify the keys in the range [-n,n], idempotent\npermutations make it possible to obtain linearly ordered arrangement of the\nkeys in O(n) time using only 4log(n) bits, setting the theoretical lower bound\nof time and space complexity of sorting. If it is not allowed to modify the\nkeys out of the range [1,n], then n+4log(n) bits are required where n of them\nis used to tag some of the keys.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 10:28:19 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Cetin", "A. Emre", ""]]}, {"id": "1307.4164", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "Mohit Singh and L\\'aszl\\'o A. V\\'egh", "title": "Approximating Minimum Cost Connectivity Orientation and Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate problems addressing combined connectivity augmentation and\norientations settings. We give a polynomial-time 6-approximation algorithm for\nfinding a minimum cost subgraph of an undirected graph $G$ that admits an\norientation covering a nonnegative crossing $G$-supermodular demand function,\nas defined by Frank. An important example is $(k,\\ell)$-edge-connectivity, a\ncommon generalization of global and rooted edge-connectivity.\n  Our algorithm is based on a non-standard application of the iterative\nrounding method. We observe that the standard linear program with cut\nconstraints is not amenable and use an alternative linear program with\npartition and co-partition constraints instead. The proof requires a new type\nof uncrossing technique on partitions and co-partitions.\n  We also consider the problem setting when the cost of an edge can be\ndifferent for the two possible orientations. The problem becomes substantially\nmore difficult already for the simpler requirement of $k$-edge-connectivity.\nKhanna, Naor, and Shepherd showed that the integrality gap of the natural\nlinear program is at most $4$ when $k=1$ and conjectured that it is constant\nfor all fixed $k$. We disprove this conjecture by showing an $\\Omega(|V|)$\nintegrality gap even when $k=2$.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 05:08:27 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 16:46:01 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Singh", "Mohit", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "1307.4257", "submitter": "Anna Adamaszek", "authors": "Anna Adamaszek and Andreas Wiese", "title": "A QPTAS for Maximum Weight Independent Set of Polygons with\n  Polylogarithmically Many Vertices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Weight Independent Set of Polygons problem is a fundamental\nproblem in computational geometry. Given a set of weighted polygons in the\n2-dimensional plane, the goal is to find a set of pairwise non-overlapping\npolygons with maximum total weight. Due to its wide range of applications, the\nMWISP problem and its special cases have been extensively studied both in the\napproximation algorithms and the computational geometry community. Despite a\nlot of research, its general case is not well-understood. Currently the best\nknown polynomial time algorithm achieves an approximation ratio of n^(epsilon)\n[Fox and Pach, SODA 2011], and it is not even clear whether the problem is\nAPX-hard. We present a (1+epsilon)-approximation algorithm, assuming that each\npolygon in the input has at most a polylogarithmic number of vertices. Our\nalgorithm has quasi-polynomial running time.\n  We use a recently introduced framework for approximating maximum weight\nindependent set in geometric intersection graphs. The framework has been used\nto construct a QPTAS in the much simpler case of axis-parallel rectangles. We\nextend it in two ways, to adapt it to our much more general setting. First, we\nshow that its technical core can be reduced to the case when all input polygons\nare triangles. Secondly, we replace its key technical ingredient which is a\nmethod to partition the plane using only few edges such that the objects\nstemming from the optimal solution are evenly distributed among the resulting\nfaces and each object is intersected only a few times. Our new procedure for\nthis task is not more complex than the original one, and it can handle the\narising difficulties due to the arbitrary angles of the polygons. Note that\nalready this obstacle makes the known analysis for the above framework fail.\nAlso, in general it is not well understood how to handle this difficulty by\nefficient approximation algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 12:25:24 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Adamaszek", "Anna", ""], ["Wiese", "Andreas", ""]]}, {"id": "1307.4258", "submitter": "Tobias Harks", "authors": "Martin Gairing and Tobias Harks and Max Klimm", "title": "Complexity and Approximation of the Continuous Network Design Problem", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a classical problem in transportation, known as the continuous\n(bilevel) network design problem, CNDP for short. We are given a graph for\nwhich the latency of each edge depends on the ratio of the edge flow and the\ncapacity installed. The goal is to find an optimal investment in edge\ncapacities so as to minimize the sum of the routing cost of the induced Wardrop\nequilibrium and the investment cost. While this problem is considered as\nchallenging in the literature, its complexity status was still unknown. We\nclose this gap showing that CNDP is strongly NP-complete and APX-hard, both on\ndirected and undirected networks and even for instances with affine latencies.\n  As for the approximation of the problem, we first provide a detailed analysis\nfor a heuristic studied by Marcotte for the special case of monomial latency\nfunctions (Mathematical Programming, Vol.~34, 1986). Specifically, we derive a\nclosed form expression of its approximation guarantee for arbitrary sets S of\nallowed latency functions. Second, we propose a different approximation\nalgorithm and show that it has the same approximation guarantee. As our final\n-- and arguably most interesting -- result regarding approximation, we show\nthat using the better of the two approximation algorithms results in a strictly\nimproved approximation guarantee for which we give a closed form expression.\nFor affine latencies, e.g., this algorithm achieves a 1.195-approximation which\nimproves on the 5/4 that has been shown before by Marcotte. We finally discuss\nthe case of hard budget constraints on the capacity investment.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 12:28:11 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 14:15:07 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Gairing", "Martin", ""], ["Harks", "Tobias", ""], ["Klimm", "Max", ""]]}, {"id": "1307.4289", "submitter": "Ren\\'e Sitters", "authors": "Ren\\'e Sitters", "title": "Polynomial time approximation schemes for the traveling repairman and\n  other minimum latency problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial time, $(1+\\epsilon)$-approximation algorithm for the\ntraveling repairman problem (TRP) in the Euclidean plane and on weighted trees.\nThis improves on the known quasi-polynomial time approximation schemes for\nthese problems. The algorithm is based on a simple technique that reduces the\nTRP to what we call the \\emph{segmented TSP}. Here, we are given numbers\n$l_1,\\dots,l_K$ and $n_1,\\dots,n_K$ and we need to find a path that visits at\nleast $n_h$ points within path distance $l_h$ from the starting point for all\n$h\\in\\{1,\\dots,K\\}$. A solution is $\\alpha$-approximate if at least $n_h$\npoints are visited within distance $\\alpha l_h$. It is shown that any algorithm\nthat is $\\alpha$-approximate for \\emph{every constant} $K$ in some metric\nspace, gives an $\\alpha(1+\\epsilon)$-approximation for the TRP in the same\nmetric space. Subsequently, approximation schemes are given for this segmented\nTSP problem in the plane and on weighted trees. The segmented TSP with only one\nsegment ($K=1$) is equivalent to the $k$-TSP for which a\n$(2+\\epsilon)$-approximation is known for a general metric space. Hence, this\napproach through the segmented TSP gives new impulse for improving on the\n3.59-approximation for TRP in a general metric space. A similar reduction\napplies to many other minimum latency problems. To illustrate the strength of\nthis approach we apply it to the well-studied scheduling problem of minimizing\ntotal weighted completion time under precedence constraints, $1|prec|\\sum\nw_{j}C_{j}$, and present a polynomial time approximation scheme for the case of\ninterval order precedence constraints. This improves on the known\n$3/2$-approximation for this problem. Both approximation schemes apply as well\nif release dates are added to the problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 14:38:01 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2013 10:40:19 GMT"}, {"version": "v3", "created": "Fri, 19 Sep 2014 12:22:50 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Sitters", "Ren\u00e9", ""]]}, {"id": "1307.4334", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "L\\'aszl\\'o A. V\\'egh and Giacomo Zambelli", "title": "A polynomial projection-type algorithm for linear programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple O([n^5/\\log n]L) algorithm for linear programming\nfeasibility, that can be considered as a polynomial-time implementation of the\nrelaxation method. Our work draws from Chubanov's \"Divide-and-Conquer\"\nalgorithm [4], where the recursion is replaced by a simple and more efficient\niterative method. A similar approach was used in a more recent paper of\nChubanov [6].\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 16:34:24 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 16:13:20 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2013 15:51:25 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""], ["Zambelli", "Giacomo", ""]]}, {"id": "1307.4339", "submitter": "Gregory Puleo", "authors": "Farzad Farnoud (Hassanzadeh) and Lili Su and Gregory J. Puleo and\n  Olgica Milenkovic", "title": "Computing Similarity Distances Between Rankings", "comments": "32 pages, 14 figures. Corrected proof of unbalanced case", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing distances between rankings that take into\naccount similarities between candidates. The need for evaluating such distances\nis governed by applications as diverse as rank aggregation, bioinformatics,\nsocial sciences and data storage. The problem may be summarized as follows:\nGiven two rankings and a positive cost function on transpositions that depends\non the similarity of the candidates involved, find a smallest cost sequence of\ntranspositions that converts one ranking into another. Our focus is on costs\nthat may be described via special metric-tree structures and on complete\nrankings modeled as permutations. The presented results include a\nquadratic-time algorithm for finding a minimum cost decomposition for simple\ncycles, and a quadratic-time, $4/3$-approximation algorithm for permutations\nthat contain multiple cycles. The proposed methods rely on investigating a\nnewly introduced balancing property of cycles embedded in trees, cycle-merging\nmethods, and shortest path optimization techniques.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 16:50:37 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 02:46:48 GMT"}, {"version": "v3", "created": "Sat, 4 Oct 2014 22:40:00 GMT"}, {"version": "v4", "created": "Wed, 19 Nov 2014 17:25:40 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Farnoud", "Farzad", "", "Hassanzadeh"], ["Su", "Lili", ""], ["Puleo", "Gregory J.", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1307.4355", "submitter": "Sudipto Guha", "authors": "Kook Jin Ahn and Sudipto Guha", "title": "Near Linear Time Approximation Schemes for Uncapacitated and Capacitated\n  b--Matching Problems in Nonbipartite Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first near optimal approximation schemes for the\n  maximum weighted (uncapacitated or capacitated) $b$--matching\n  problems for non-bipartite graphs that run in time (near) linear in\n  the number of edges. For any $\\delta>3/\\sqrt{n}$ the algorithm\n  produces a $(1-\\delta)$ approximation in $O(m \\poly(\\delta^{-1},\\log\n  n))$ time. We provide fractional solutions for the standard linear\n  programming formulations for these problems and subsequently also\n  provide (near) linear time approximation schemes\n  for rounding the fractional solutions.\n  Through these problems as a vehicle, we also present several ideas\n  in the context of solving linear programs approximately using fast\n  primal-dual algorithms. First, even though the dual of these\n  problems have exponentially many variables and an efficient exact\n  computation of dual weights is infeasible, we show that we can\n  efficiently compute and use a sparse approximation of the dual\n  weights using a combination of (i) adding perturbation to the\n  constraints of the polytope and (ii) amplification followed by\n  thresholding of the dual weights. Second, we show that\n  approximation algorithms can be used to reduce the width of the\n  formulation, and faster convergence.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 17:43:07 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 14:24:30 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2015 18:57:27 GMT"}, {"version": "v4", "created": "Mon, 18 Jun 2018 05:51:53 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Ahn", "Kook Jin", ""], ["Guha", "Sudipto", ""]]}, {"id": "1307.4359", "submitter": "Sudipto Guha", "authors": "Kook Jin Ahn and Sudipto Guha", "title": "Access to Data and Number of Iterations: Dual Primal Algorithms for\n  Maximum Matching under Resource Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider graph algorithms in models of computation where the\nspace usage (random accessible storage, in addition to the read only input) is\nsublinear in the number of edges $m$ and the access to input data is\nconstrained. These questions arises in many natural settings, and in particular\nin the analysis of MapReduce or similar algorithms that model constrained\nparallelism with sublinear central processing. In SPAA 2011, Lattanzi etal.\nprovided a $O(1)$ approximation of maximum matching using $O(p)$ rounds of\niterative filtering via mapreduce and $O(n^{1+1/p})$ space of central\nprocessing for a graph with $n$ nodes and $m$ edges.\n  We focus on weighted nonbipartite maximum matching in this paper. For any\nconstant $p>1$, we provide an iterative sampling based algorithm for computing\na $(1-\\epsilon)$-approximation of the weighted nonbipartite maximum matching\nthat uses $O(p/\\epsilon)$ rounds of sampling, and $O(n^{1+1/p})$ space. The\nresults extends to $b$-Matching with small changes. This paper combines\nadaptive sketching literature and fast primal-dual algorithms based on relaxed\nDantzig-Wolfe decision procedures. Each round of sampling is implemented\nthrough linear sketches and executed in a single round of MapReduce. The paper\nalso proves that nonstandard linear relaxations of a problem, in particular\npenalty based formulations, are helpful in mapreduce and similar settings in\nreducing the adaptive dependence of the iterations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 17:48:49 GMT"}, {"version": "v2", "created": "Fri, 25 Jul 2014 17:48:13 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 19:47:28 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Ahn", "Kook Jin", ""], ["Guha", "Sudipto", ""]]}, {"id": "1307.4420", "submitter": "Lily Briggs", "authors": "Lily Briggs", "title": "On the Complexity of a Matching Problem with Asymmetric Weights", "comments": "9 pages; v2 fixed typos, made minor clarifications, and added author\n  affiliation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present complexity results regarding a matching-type problem related to\nstructural controllability of dynamical systems modelled on graphs.\nControllability of a dynamical system is the ability to choose certain inputs\nin order to drive the system from any given state to any desired state; a graph\nis said to be structurally controllable if it represents the structure of a\ncontrollable system. We define the Orientation Control Matching problem (OCM)\nto be the problem of orienting an undirected graph in a manner that maximizes\nits structural controllability. A generalized version, the Asymmetric\nOrientation Control Matching problem (AOCM), allows for asymmetric weights on\nthe possible directions of each edge. These problems are closely related to\n2-matchings, disjoint path covers, and disjoint cycle covers. We prove using\nreductions that OCM is polynomially solvable, while AOCM is much harder; we\nshow that it is NP-complete as well as APX-hard.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 20:32:45 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 14:44:48 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Briggs", "Lily", ""]]}, {"id": "1307.4473", "submitter": "EPTCS", "authors": "Krishnendu Chatterjee, Monika Henzinger, Sebastian Krinninger,\n  Veronika Loitzenbauer", "title": "Approximating the minimum cycle mean", "comments": "In Proceedings GandALF 2013, arXiv:1307.4162", "journal-ref": "EPTCS 119, 2013, pp. 136-149", "doi": "10.4204/EPTCS.119.13", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider directed graphs where each edge is labeled with an integer weight\nand study the fundamental algorithmic question of computing the value of a\ncycle with minimum mean weight. Our contributions are twofold: (1) First we\nshow that the algorithmic question is reducible in O(n^2) time to the problem\nof a logarithmic number of min-plus matrix multiplications of n-by-n matrices,\nwhere n is the number of vertices of the graph. (2) Second, when the weights\nare nonnegative, we present the first (1 + {\\epsilon})-approximation algorithm\nfor the problem and the running time of our algorithm is \\tilde(O)(n^\\omega\nlog^3(nW/{\\epsilon}) / {\\epsilon}), where O(n^\\omega) is the time required for\nthe classic n-by-n matrix multiplication and W is the maximum value of the\nweights.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 01:42:22 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Henzinger", "Monika", ""], ["Krinninger", "Sebastian", ""], ["Loitzenbauer", "Veronika", ""]]}, {"id": "1307.4518", "submitter": "Zeyu Zhang", "authors": "Jian Li and Zeyu Zhang", "title": "Ranking with Diverse Intents and Correlated Contents", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following document ranking problem: We have a collection of\ndocuments, each containing some topics (e.g. sports, politics, economics). We\nalso have a set of users with diverse interests. Assume that user u is\ninterested in a subset I_u of topics. Each user u is also associated with a\npositive integer K_u, which indicates that u can be satisfied by any K_u topics\nin I_u. Each document s contains information for a subset C_s of topics. The\nobjective is to pick one document at a time such that the average satisfying\ntime is minimized, where a user's satisfying time is the first time that at\nleast K_u topics in I_u are covered in the documents selected so far. Our main\nresult is an O({\\rho})-approximation algorithm for the problem, where {\\rho} is\nthe algorithmic integrality gap of the linear programming relaxation of the set\ncover instance defined by the documents and topics. This result generalizes the\nconstant approximations for generalized min-sum set cover and ranking with\nunrelated intents and the logarithmic approximation for the problem of ranking\nwith submodular valuations (when the submodular function is the coverage\nfunction), and can be seen as an interpolation between these results. We\nfurther extend our model to the case when each user may interest in more than\none sets of topics and when the user's valuation function is XOS, and obtain\nsimilar results for these models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 06:53:29 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 03:10:34 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Li", "Jian", ""], ["Zhang", "Zeyu", ""]]}, {"id": "1307.4521", "submitter": "Adam Kasperski", "authors": "Adam Kasperski, Pawel Zielinski", "title": "Bottleneck combinatorial optimization problems with uncertain costs and\n  the OWA criterion", "comments": "arXiv admin note: text overlap with arXiv:1305.5339", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a class of bottleneck combinatorial optimization problems with\nuncertain costs is discussed. The uncertainty is modeled by specifying a\ndiscrete scenario set containing a finite number of cost vectors, called\nscenarios. In order to choose a solution the Ordered Weighted Averaging\naggregation operator (shortly OWA) is applied. The OWA operator generalizes\ntraditional criteria in decision making under uncertainty such as the maximum,\nminimum, average, median, or Hurwicz criterion. New complexity and\napproximation results in this area are provided. These results are general and\nremain valid for many problems, in particular for a wide class of network\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 07:22:42 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Kasperski", "Adam", ""], ["Zielinski", "Pawel", ""]]}, {"id": "1307.4543", "submitter": "Xiaodong Wang", "authors": "Lei Wang, Xiaodong Wang, Yingjie Wu, and Daxin Zhu", "title": "Complete Solutions for a Combinatorial Puzzle in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a single player game consisting of $n$ black checkers\nand $m$ white checkers, called shifting the checkers. We have proved that the\nminimum number of steps needed to play the game for general $n$ and $m$ is $nm\n+ n + m$. We have also presented an optimal algorithm to generate an optimal\nmove sequence of the game consisting of $n$ black checkers and $m$ white\ncheckers, and finally, we present an explicit solution for the general game.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 08:59:38 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Wang", "Lei", ""], ["Wang", "Xiaodong", ""], ["Wu", "Yingjie", ""], ["Zhu", "Daxin", ""]]}, {"id": "1307.4927", "submitter": "Yoichi Iwata", "authors": "Yoichi Iwata, Keigo Oka, Yuichi Yoshida", "title": "Linear-Time FPT Algorithms via Network Flow", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of parameterized complexity, to cope with NP-Hard problems, we\nintroduce a parameter k besides the input size n, and we aim to design\nalgorithms (called FPT algorithms) that run in O(f(k)n^d) time for some\nfunction f(k) and constant d. Though FPT algorithms have been successfully\ndesigned for many problems, typically they are not sufficiently fast because of\nhuge f(k) and d. In this paper, we give FPT algorithms with small f(k) and d\nfor many important problems including Odd Cycle Transversal and Almost 2-SAT.\nMore specifically, we can choose f(k) as a single exponential (4^k) and d as\none, that is, linear in the input size. To the best of our knowledge, our\nalgorithms achieve linear time complexity for the first time for these\nproblems. To obtain our algorithms for these problems, we consider a large\nclass of integer programs, called BIP2. Then we show that, in linear time, we\ncan reduce BIP2 to Vertex Cover Above LP preserving the parameter k, and we can\ncompute an optimal LP solution for Vertex Cover Above LP using network flow.\nThen, we perform an exhaustive search by fixing half-integral values in the\noptimal LP solution for Vertex Cover Above LP. A bottleneck here is that we\nneed to recompute an LP optimal solution after branching. To address this\nissue, we exploit network flow to update the optimal LP solution in linear\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 12:58:15 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Iwata", "Yoichi", ""], ["Oka", "Keigo", ""], ["Yoshida", "Yuichi", ""]]}, {"id": "1307.5108", "submitter": "Thomas Rothvoss", "authors": "Michel X. Goemans and Thomas Rothvoss", "title": "Polynomiality for Bin Packing with a Constant Number of Item Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the bin packing problem with d different item sizes s_i and item\nmultiplicities a_i, where all numbers are given in binary encoding. This\nproblem formulation is also known as the 1-dimensional cutting stock problem.\n  In this work, we provide an algorithm which, for constant d, solves bin\npacking in polynomial time. This was an open problem for all d >= 3.\n  In fact, for constant d our algorithm solves the following problem in\npolynomial time: given two d-dimensional polytopes P and Q, find the smallest\nnumber of integer points in P whose sum lies in Q.\n  Our approach also applies to high multiplicity scheduling problems in which\nthe number of copies of each job type is given in binary encoding and each type\ncomes with certain parameters such as release dates, processing times and\ndeadlines. We show that a variety of high multiplicity scheduling problems can\nbe solved in polynomial time if the number of job types is constant.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 01:02:18 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 19:27:16 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Goemans", "Michel X.", ""], ["Rothvoss", "Thomas", ""]]}, {"id": "1307.5230", "submitter": "Ashwin Pananjady", "authors": "Vivek Kumar Bagaria, Ashwin Pananjady and Rahul Vaze", "title": "Optimally Approximating the Coverage Lifetime of Wireless Sensor\n  Networks", "comments": "submitted to IEEE/ACM Transactions on Networking, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing the lifetime of coverage (MLCP) of\ntargets in a wireless sensor network with battery-limited sensors. We first\nshow that the MLCP cannot be approximated within a factor less than $\\ln n$ by\nany polynomial time algorithm, where $n$ is the number of targets. This\nprovides closure to the long-standing open problem of showing optimality of\npreviously known $\\ln n$ approximation algorithms. We also derive a new $\\ln n$\napproximation to the MLCP by showing a $\\ln n$ approximation to the maximum\ndisjoint set cover problem (DSCP), which has many advantages over previous MLCP\nalgorithms, including an easy extension to the $k$-coverage problem. We then\npresent an improvement (in certain cases) to the $\\ln n$ algorithm in terms of\na newly defined quantity \"expansiveness\" of the network. For the special\none-dimensional case, where each sensor can monitor a contiguous region of\npossibly different lengths, we show that the MLCP solution is equal to the DSCP\nsolution, and can be found in polynomial time. Finally, for the special\ntwo-dimensional case, where each sensor can monitor a circular area with a\ngiven radius around itself, we combine existing results to derive a\n$1+\\epsilon$ approximation algorithm for solving MLCP for any $\\epsilon >0$.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 14:29:59 GMT"}, {"version": "v2", "created": "Sat, 28 Jun 2014 05:12:21 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Bagaria", "Vivek Kumar", ""], ["Pananjady", "Ashwin", ""], ["Vaze", "Rahul", ""]]}, {"id": "1307.5290", "submitter": "Johannes Dams", "authors": "Johannes Dams and Martin Hoefer and Thomas Kesselheim", "title": "Jamming-Resistant Learning in Wireless Networks", "comments": "22 pages, 2 figures, typos removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider capacity maximization in wireless networks under adversarial\ninterference conditions. There are n links, each consisting of a sender and a\nreceiver, which repeatedly try to perform a successful transmission. In each\ntime step, the success of attempted transmissions depends on interference\nconditions, which are captured by an interference model (e.g. the SINR model).\nAdditionally, an adversarial jammer can render a (1-delta)-fraction of time\nsteps unsuccessful. For this scenario, we analyze a framework for distributed\nlearning algorithms to maximize the number of successful transmissions. Our\nmain result is an algorithm based on no-regret learning converging to an\nO(1/delta)-approximation. It provides even a constant-factor approximation when\nthe jammer exactly blocks a (1-delta)-fraction of time steps. In addition, we\nconsider a stochastic jammer, for which we obtain a constant-factor\napproximation after a polynomial number of time steps. We also consider more\ngeneral settings, in which links arrive and depart dynamically, and where each\nsender tries to reach multiple receivers. Our algorithms perform favorably in\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 17:38:12 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 05:50:10 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Dams", "Johannes", ""], ["Hoefer", "Martin", ""], ["Kesselheim", "Thomas", ""]]}, {"id": "1307.5296", "submitter": "Neal E. Young", "authors": "Monik Khare and Claire Mathieu and Neal E. Young", "title": "First-Come-First-Served for Online Slot Allocation and Huffman Coding", "comments": "ACM-SIAM Symposium on Discrete Algorithms (SODA) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can one choose a good Huffman code on the fly, without knowing the underlying\ndistribution? Online Slot Allocation (OSA) models this and similar problems:\nThere are n slots, each with a known cost. There are n items. Requests for\nitems are drawn i.i.d. from a fixed but hidden probability distribution p.\nAfter each request, if the item, i, was not previously requested, then the\nalgorithm (knowing the slot costs and the requests so far, but not p) must\nplace the item in some vacant slot j(i). The goal is to minimize the sum, over\nthe items, of the probability of the item times the cost of its assigned slot.\n  The optimal offline algorithm is trivial: put the most probable item in the\ncheapest slot, the second most probable item in the second cheapest slot, etc.\nThe optimal online algorithm is First Come First Served (FCFS): put the first\nrequested item in the cheapest slot, the second (distinct) requested item in\nthe second cheapest slot, etc. The optimal competitive ratios for any online\nalgorithm are 1+H(n-1) ~ ln n for general costs and 2 for concave costs. For\nlogarithmic costs, the ratio is, asymptotically, 1: FCFS gives cost opt + O(log\nopt).\n  For Huffman coding, FCFS yields an online algorithm (one that allocates\ncodewords on demand, without knowing the underlying probability distribution)\nthat guarantees asymptotically optimal cost: at most opt + 2 log(1+opt) + 2.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 17:56:24 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2013 20:24:32 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Khare", "Monik", ""], ["Mathieu", "Claire", ""], ["Young", "Neal E.", ""]]}, {"id": "1307.5299", "submitter": "Paul D\\\"utting", "authors": "Paul Duetting and Robert Kleinberg", "title": "Polymatroid Prophet Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a gambler and a prophet who observe a sequence of independent,\nnon-negative numbers. The gambler sees the numbers one-by-one whereas the\nprophet sees the entire sequence at once. The goal of both is to decide on\nfractions of each number they want to keep so as to maximize the weighted\nfractional sum of the numbers chosen.\n  The classic result of Krengel and Sucheston (1977-78) asserts that if both\nthe gambler and the prophet can pick one number, then the gambler can do at\nleast half as well as the prophet. Recently, Kleinberg and Weinberg (2012) have\ngeneralized this result to settings where the numbers that can be chosen are\nsubject to a matroid constraint.\n  In this note we go one step further and show that the bound carries over to\nsettings where the fractions that can be chosen are subject to a polymatroid\nconstraint. This bound is tight as it is already tight for the simple setting\nwhere the gambler and the prophet can pick only one number. An interesting\napplication of our result is in mechanism design, where it leads to improved\nresults for various problems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 18:11:44 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Duetting", "Paul", ""], ["Kleinberg", "Robert", ""]]}, {"id": "1307.5519", "submitter": "Anton Eremeev", "authors": "Anton V. Eremeev, Julia V. Kovalenko", "title": "Optimal Recombination in Genetic Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys results on complexity of the optimal recombination problem\n(ORP), which consists in finding the best possible offspring as a result of a\nrecombination operator in a genetic algorithm, given two parent solutions. We\nconsider efficient reductions of the ORPs, allowing to establish polynomial\nsolvability or NP-hardness of the ORPs, as well as direct proofs of hardness\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 11:29:16 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Eremeev", "Anton V.", ""], ["Kovalenko", "Julia V.", ""]]}, {"id": "1307.5547", "submitter": "Ross M. McConnell", "authors": "Ross M. McConnell and Yahav Nussbaum", "title": "Linear-Time Recognition of Probe Interval Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interval graph for a set of intervals on a line consists of one vertex\nfor each interval, and an edge for each intersecting pair of intervals. A probe\ninterval graph is a variant that is motivated by an application to genomics,\nwhere the intervals are partitioned into two sets: probes and non-probes. The\ngraph has an edge between two vertices if they intersect and at least one of\nthem is a probe. We give a linear-time algorithm for determining whether a\ngiven graph and partition of vertices into probes and non-probes is a probe\ninterval graph. If it is, we give a layout of intervals that proves this. We\ncan also determine whether the layout of the intervals is uniquely constrained\nwithin the same time bound. As part of the algorithm, we solve the\nconsecutive-ones probe matrix problem in linear time, develop algorithms for\noperating on PQ trees, and give results that relate PQ trees for different\nsubmatrices of a consecutive-ones matrix.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 17:16:50 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["McConnell", "Ross M.", ""], ["Nussbaum", "Yahav", ""]]}, {"id": "1307.5582", "submitter": "Anupam Gupta", "authors": "Anupam Gupta and Kunal Talwar", "title": "Random Rates for 0-Extension and Low-Diameter Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of partitioning an arbitrary metric space into pieces of\ndiameter at most \\Delta, such every pair of points is separated with relatively\nlow probability. We propose a rate-based algorithm inspired by\nmultiplicatively-weighted Voronoi diagrams, and prove it has optimal\ntrade-offs. This also gives us another logarithmic approximation algorithm for\nthe 0-extension problem.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 02:05:58 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Gupta", "Anupam", ""], ["Talwar", "Kunal", ""]]}, {"id": "1307.5674", "submitter": "Masoumeh Vali", "authors": "Masoumeh Vali", "title": "Solving Traveling Salesman Problem by Marker Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use marker method and propose a new mutation operator that\nselects the nearest neighbor among all near neighbors solving Traveling\nSalesman Problem.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 12:16:08 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Vali", "Masoumeh", ""]]}, {"id": "1307.5697", "submitter": "Aziz Erkal Selman", "authors": "Martin Grohe, Kristian Kersting, Martin Mladenov, Erkal Selman", "title": "Dimension Reduction via Colour Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colour refinement is a basic algorithmic routine for graph isomorphism\ntesting, appearing as a subroutine in almost all practical isomorphism solvers.\nIt partitions the vertices of a graph into \"colour classes\" in such a way that\nall vertices in the same colour class have the same number of neighbours in\nevery colour class. Tinhofer (Disc. App. Math., 1991), Ramana, Scheinerman, and\nUllman (Disc. Math., 1994) and Godsil (Lin. Alg. and its App., 1997)\nestablished a tight correspondence between colour refinement and fractional\nisomorphisms of graphs, which are solutions to the LP relaxation of a natural\nILP formulation of graph isomorphism.\n  We introduce a version of colour refinement for matrices and extend existing\nquasilinear algorithms for computing the colour classes. Then we generalise the\ncorrespondence between colour refinement and fractional automorphisms and\ndevelop a theory of fractional automorphisms and isomorphisms of matrices.\n  We apply our results to reduce the dimensions of systems of linear equations\nand linear programs. Specifically, we show that any given LP L can efficiently\nbe transformed into a (potentially) smaller LP L' whose number of variables and\nconstraints is the number of colour classes of the colour refinement algorithm,\napplied to a matrix associated with the LP. The transformation is such that we\ncan easily (by a linear mapping) map both feasible and optimal solutions back\nand forth between the two LPs. We demonstrate empirically that colour\nrefinement can indeed greatly reduce the cost of solving linear programs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 13:34:44 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 13:28:47 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Grohe", "Martin", ""], ["Kersting", "Kristian", ""], ["Mladenov", "Martin", ""], ["Selman", "Erkal", ""]]}, {"id": "1307.5899", "submitter": "Deepak Rajan", "authors": "Peter Lindstrom and Deepak Rajan", "title": "Optimal Hierarchical Layouts for Cache-Oblivious Search Trees", "comments": "Extended version with proofs added to the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general framework for generating cache-oblivious\nlayouts for binary search trees. A cache-oblivious layout attempts to minimize\ncache misses on any hierarchical memory, independent of the number of memory\nlevels and attributes at each level such as cache size, line size, and\nreplacement policy. Recursively partitioning a tree into contiguous subtrees\nand prescribing an ordering amongst the subtrees, Hierarchical Layouts\ngeneralize many commonly used layouts for trees such as in-order, pre-order and\nbreadth-first. They also generalize the various flavors of the van Emde Boas\nlayout, which have previously been used as cache-oblivious layouts.\nHierarchical Layouts thus unify all previous attempts at deriving layouts for\nsearch trees.\n  The paper then derives a new locality measure (the Weighted Edge Product)\nthat mimics the probability of cache misses at multiple levels, and shows that\nlayouts that reduce this measure perform better. We analyze the various degrees\nof freedom in the construction of Hierarchical Layouts, and investigate the\nrelative effect of each of these decisions in the construction of\ncache-oblivious layouts. Optimizing the Weighted Edge Product for complete\nbinary search trees, we introduce the MinWEP layout, and show that it\noutperforms previously used cache-oblivious layouts by almost 20%.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 21:41:44 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2013 17:53:50 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Lindstrom", "Peter", ""], ["Rajan", "Deepak", ""]]}, {"id": "1307.5934", "submitter": "Zizhuo Wang", "authors": "Xiao Alison Chen, Zizhuo Wang", "title": "A Near-Optimal Dynamic Learning Algorithm for Online Matching Problems\n  with Concave Returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online matching problem with concave returns. This problem is\na significant generalization of the Adwords allocation problem and has vast\napplications in online advertising. In this problem, a sequence of items arrive\nsequentially and each has to be allocated to one of the bidders, who bid a\ncertain value for each item. At each time, the decision maker has to allocate\nthe current item to one of the bidders without knowing the future bids and the\nobjective is to maximize the sum of some concave functions of each bidder's\naggregate value. In this work, we propose an algorithm that achieves\nnear-optimal performance for this problem when the bids arrive in a random\norder and the input data satisfies certain conditions. The key idea of our\nalgorithm is to learn the input data pattern dynamically: we solve a sequence\nof carefully chosen partial allocation problems and use their optimal solutions\nto assist with the future decision. Our analysis belongs to the primal-dual\nparadigm, however, the absence of linearity of the objective function and the\ndynamic feature of the algorithm makes our analysis quite unique.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 03:24:28 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 17:39:58 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2015 09:43:57 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chen", "Xiao Alison", ""], ["Wang", "Zizhuo", ""]]}, {"id": "1307.6429", "submitter": "Youming Qiao", "authors": "G\\'abor Ivanyos, Marek Karpinski, Youming Qiao, Miklos Santha", "title": "Generalized Wong sequences and their applications to Edmonds' problems", "comments": "25 pages; improved presentation; fix some gaps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design two deterministic polynomial time algorithms for variants of a\nproblem introduced by Edmonds in 1967: determine the rank of a matrix M whose\nentries are homogeneous linear polynomials over the integers. Given a linear\nsubspace B of the n by n matrices over some field F, we consider the following\nproblems: symbolic matrix rank (SMR) is the problem to determine the maximum\nrank among matrices in B, symbolic determinant identity testing (SDIT) is the\nquestion to decide whether there exists a nonsingular matrix in B. The\nconstructive versions of these problems are asking to find a matrix of maximum\nrank, respectively a nonsingular matrix, if there exists one.\n  Our first algorithm solves the constructive SMR when B is spanned by unknown\nrank one matrices, answering an open question of Gurvits. Our second algorithm\nsolves the constructive SDIT when B is spanned by triangularizable matrices,\nbut the triangularization is not given explicitly. Both algorithms work over\nfinite fields of size at least n+1 and over the rational numbers, and the first\nalgorithm actually solves (the non-constructive) SMR independently from the\nfield size. Our main tool to obtain these results is to generalize Wong\nsequences, a classical method to deal with pairs of matrices, to the case of\npairs of matrix spaces.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 14:13:44 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 06:20:33 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Ivanyos", "G\u00e1bor", ""], ["Karpinski", "Marek", ""], ["Qiao", "Youming", ""], ["Santha", "Miklos", ""]]}, {"id": "1307.6462", "submitter": "Travis Gagie", "authors": "Hector Ferrada, Travis Gagie, Tommi Hirvola, Simon J. Puglisi", "title": "AliBI: An Alignment-Based Index for Genomic Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With current hardware and software, a standard computer can now hold in RAM\nan index for approximate pattern matching on about half a dozen human genomes.\nSequencing technologies have improved so quickly, however, that scientists will\nsoon demand indexes for thousands of genomes. Whereas most researchers who have\naddressed this problem have proposed completely new kinds of indexes, we\nrecently described a simple technique that scales standard indexes to work on\nmore genomes. Our main idea was to filter the dataset with LZ77, build a\nstandard index for the filtered file, and then create a hybrid of that standard\nindex and an LZ77-based index. In this paper we describe how to our technique\nto use alignments instead of LZ77, in order to simplify and speed up both\npreprocessing and random access.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 15:42:23 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Ferrada", "Hector", ""], ["Gagie", "Travis", ""], ["Hirvola", "Tommi", ""], ["Puglisi", "Simon J.", ""]]}, {"id": "1307.6505", "submitter": "Aleksandar Ilic", "authors": "Aleksandar Ilic", "title": "On the variable common due date, minimal tardy jobs bicriteria\n  two-machine flow shop problem with ordered machines", "comments": "6 pages, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a special case of the ordinary NP-hard two-machine flow shop\nproblem with the objective of determining simultaneously a minimal common due\ndate and the minimal number of tardy jobs. In [S. S. Panwalkar, C. Koulamas, An\nO(n^2) algorithm for the variable common due date, minimal tardy jobs\nbicriteria two-machine flow shop problem with ordered machines, European\nJournal of Operational Research 221 (2012), 7-13.], the authors presented\nquadratic algorithm for the problem when each job has its smaller processing\ntime on the first machine. In this note, we improve the running time of the\nalgorithm to O(n log n) by efficient implementation using recently introduced\nmodified binary tree data structure.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 17:41:00 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 17:49:32 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Ilic", "Aleksandar", ""]]}, {"id": "1307.6627", "submitter": "Anastasios Sidiropoulos", "authors": "Anupam Gupta, Anastasios Sidiropoulos", "title": "Minimum d-dimensional arrangement with fixed points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Minimum $d$-Dimensional Arrangement Problem (d-dimAP) we are given a\ngraph with edge weights, and the goal is to find a 1-1 map of the vertices into\n$\\mathbb{Z}^d$ (for some fixed dimension $d\\geq 1$) minimizing the total\nweighted stretch of the edges. This problem arises in VLSI placement and chip\ndesign.\n  Motivated by these applications, we consider a generalization of d-dimAP,\nwhere the positions of some of the vertices (pins) is fixed and specified as\npart of the input. We are asked to extend this partial map to a map of all the\nvertices, again minimizing the weighted stretch of edges. This generalization,\nwhich we refer to as d-dimAP+, arises naturally in these application domains\n(since it can capture blocked-off parts of the board, or the requirement of\npower-carrying pins to be in certain locations, etc.). Perhaps surprisingly,\nvery little is known about this problem from an approximation viewpoint.\n  For dimension $d=2$, we obtain an $O(k^{1/2} \\cdot \\log n)$-approximation\nalgorithm, based on a strengthening of the spreading-metric LP for 2-dimAP. The\nintegrality gap for this LP is shown to be $\\Omega(k^{1/4})$. We also show that\nit is NP-hard to approximate 2-dimAP+ within a factor better than\n$\\Omega(k^{1/4-\\eps})$. We also consider a (conceptually harder, but\npractically even more interesting) variant of 2-dimAP+, where the target space\nis the grid $\\mathbb{Z}_{\\sqrt{n}} \\times \\mathbb{Z}_{\\sqrt{n}}$, instead of\nthe entire integer lattice $\\mathbb{Z}^2$. For this problem, we obtain a $O(k\n\\cdot \\log^2{n})$-approximation using the same LP relaxation. We complement\nthis upper bound by showing an integrality gap of $\\Omega(k^{1/2})$, and an\n$\\Omega(k^{1/2-\\eps})$-inapproximability result.\n  Our results naturally extend to the case of arbitrary fixed target dimension\n$d\\geq 1$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 04:08:51 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Gupta", "Anupam", ""], ["Sidiropoulos", "Anastasios", ""]]}, {"id": "1307.6628", "submitter": "Kaveh Shahbaz", "authors": "Kaveh Shahbaz", "title": "Applied Similarity Problems Using Frechet Distance", "comments": "arXiv admin note: text overlap with arXiv:1003.0460 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first part of this thesis, we consider an instance of Frechet distance\nproblem in which the speed of traversal along each segment of the curves is\nrestricted to be within a specfied range. This setting is more realistic than\nthe classical Frechet distance setting, specially in GIS applications. We also\nstudy this problem in the setting where the polygonal curves are inside a\nsimple polygon.\n  In the second part of this thesis, we present a data structure, called the\nfree-space map, that enables us to solve several variants of the Frechet\ndistance problem efficiently. Our data structure encapsulates all the\ninformation available in the free-space diagram, yet it is capable of answering\nmore general type of queries efficiently. Given that the free-space map has the\nsame size and construction time as the standard free-space diagram, it can be\nviewed as a powerful alternative to it. As part of the results in Part II of\nthe thesis, we exploit the free-space map to improve the long-standing bound\nfor computing the partial Frechet distance and obtain improved algorithms for\ncomputing the Frechet distance between two closed curves, and the so-called\nminimum/maximum walk problem. We also improve the map matching algorithm for\nthe case when the map is a directed acyclic graph.\n  As the last part of this thesis, given a point set S and a polygonal curve P\nin R^d, we study the problem of finding a polygonal curve Q through S, which\nhas a minimum Frechet distance to P. Furthermore, if the problem requires that\ncurve Q visits every point in S, we show it is NP-complete.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 04:40:29 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Shahbaz", "Kaveh", ""]]}, {"id": "1307.6789", "submitter": "Gonzalo Navarro", "authors": "Gonzalo Navarro and Yakov Nekrich", "title": "Optimal Top-k Document Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{D}$ be a collection of $D$ documents, which are strings over an\nalphabet of size $\\sigma$, of total length $n$. We describe a data structure\nthat uses linear space and and reports $k$ most relevant documents that contain\na query pattern $P$, which is a string of length $p$, in time $O(p/\\log_\\sigma\nn+k)$, which is optimal in the RAM model in the general case where $\\lg D =\n\\Theta(\\log n)$, and involves a novel RAM-optimal suffix tree search. Our\nconstruction supports an ample set of important relevance measures... [clip]\n  When $\\lg D = o(\\log n)$, we show how to reduce the space of the data\nstructure from $O(n\\log n)$ to $O(n(\\log\\sigma+\\log D+\\log\\log n))$ bits...\n[clip]\n  We also consider the dynamic scenario, where documents can be inserted and\ndeleted from the collection. We obtain linear space and query time\n$O(p(\\log\\log n)^2/\\log_\\sigma n+\\log n + k\\log\\log k)$, whereas insertions and\ndeletions require $O(\\log^{1+\\epsilon} n)$ time per symbol, for any constant\n$\\epsilon>0$.\n  Finally, we consider an extended static scenario where an extra parameter\n$par(P,d)$ is defined, and the query must retrieve only documents $d$ such that\n$par(P,d)\\in [\\tau_1,\\tau_2]$, where this range is specified at query time. We\nsolve these queries using linear space and $O(p/\\log_\\sigma n +\n\\log^{1+\\epsilon} n + k\\log^\\epsilon n)$ time, for any constant $\\epsilon>0$.\n  Our technique is to translate these top-$k$ problems into multidimensional\ngeometric search problems. As an additional bonus, we describe some\nimprovements to those problems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:26:38 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 23:59:56 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1307.6809", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "L\\'aszl\\'o A. V\\'egh", "title": "A strongly polynomial algorithm for generalized flow maximization", "comments": "minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A strongly polynomial algorithm is given for the generalized flow\nmaximization problem. It uses a new variant of the scaling technique, called\ncontinuous scaling. The main measure of progress is that within a strongly\npolynomial number of steps, an arc can be identified that must be tight in\nevery dual optimal solution, and thus can be contracted. As a consequence of\nthe result, we also obtain a strongly polynomial algorithm for the linear\nfeasibility problem with at most two nonzero entries per column in the\nconstraint matrix.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 16:33:52 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 14:12:24 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2015 22:15:35 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2016 20:55:24 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "1307.7089", "submitter": "Guohui Lin", "authors": "Weitian Tong, Zhi-Zhong Chen, Lusheng Wang, Yinfeng Xu, Jiuping Xu,\n  Randy Goebel, Guohui Lin", "title": "An approximation algorithm for the Bandpass-2 problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general Bandpass-$B$ problem is NP-hard and can be approximated by a\nreduction into the weighted $B$-set packing problem, with a worst case\nperformance ratio of $O(B^2)$. When $B = 2$, a maximum weight matching gives a\n2-approximation to the problem. In this paper, we call the Bandpass-2 problem\nsimply the Bandpass problem. The Bandpass problem can be viewed as a variation\nof the maximum traveling salesman problem, in which the edge weights are\ndynamic rather than given at the front. We present a ${426}{227}$-approximation\nalgorithm for the problem. Such an improved approximation is built on an\nintrinsic structural property proven for the optimal solution and several novel\nschemes to partition a $b$-matching into desired matchings.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 16:46:04 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Tong", "Weitian", ""], ["Chen", "Zhi-Zhong", ""], ["Wang", "Lusheng", ""], ["Xu", "Yinfeng", ""], ["Xu", "Jiuping", ""], ["Goebel", "Randy", ""], ["Lin", "Guohui", ""]]}, {"id": "1307.7259", "submitter": "Valmir Barbosa", "authors": "Leonardo I. L. Oliveira, Valmir C. Barbosa, F\\'abio Protti", "title": "The predecessor-existence problem for k-reversible processes", "comments": null, "journal-ref": "Theoretical Computer Science 562 (2015), 406-418", "doi": "10.1016/j.tcs.2014.10.018", "report-no": null, "categories": "cs.DS nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For k>=1, we consider the graph dynamical system known as a k-reversible\nprocess. In such process, each vertex in the graph has one of two possible\nstates at each discrete time. Each vertex changes its state between the present\ntime and the next if and only if it currently has at least k neighbors in a\nstate different than its own. Given a k-reversible process and a configuration\nof states assigned to the vertices, the Predecessor Existence problem consists\nof determining whether this configuration can be generated by the process from\nanother configuration within exactly one time step. We can also extend the\nproblem by asking for the number of configurations from which a given\nconfiguration is reachable within one time step. Predecessor Existence can be\nsolved in polynomial time for k=1, but for k>1 we show that it is NP-complete.\nWhen the graph in question is a tree we show how to solve it in O(n) time and\nhow to count the number of predecessor configurations in O(n^2) time. We also\nsolve Predecessor Existence efficiently for the specific case of 2-reversible\nprocesses when the maximum degree of a vertex in the graph is no greater than\n3. For this case we present an algorithm that runs in O(n) time.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 13:31:08 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Oliveira", "Leonardo I. L.", ""], ["Barbosa", "Valmir C.", ""], ["Protti", "F\u00e1bio", ""]]}, {"id": "1307.7307", "submitter": "Mudassir Shabbir", "authors": "Yessine Daadaa, Asif Jamshed, and Mudassir Shabbir", "title": "Network Decontamination with a Single Agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faults and viruses often spread in networked environments by propagating from\nsite to neighboring site. We model this process of {\\em network contamination}\nby graphs. Consider a graph $G=(V,E)$, whose vertex set is contaminated and our\ngoal is to decontaminate the set $V(G)$ using mobile decontamination agents\nthat traverse along the edge set of $G$. Temporal immunity $\\tau(G) \\ge 0$ is\ndefined as the time that a decontaminated vertex of $G$ can remain continuously\nexposed to some contaminated neighbor without getting infected itself. The\n\\emph{immunity number} of $G$, $\\iota_k(G)$, is the least $\\tau$ that is\nrequired to decontaminate $G$ using $k$ agents. We study immunity number for\nsome classes of graphs corresponding to network topologies and present upper\nbounds on $\\iota_1(G)$, in some cases with matching lower bounds. Variations of\nthis problem have been extensively studied in literature, but proposed\nalgorithms have been restricted to {\\em monotone} strategies, where a vertex,\nonce decontaminated, may not be recontaminated. We exploit nonmonotonicity to\ngive bounds which are strictly better than those derived using monotone\nstrategies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 20:56:18 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Daadaa", "Yessine", ""], ["Jamshed", "Asif", ""], ["Shabbir", "Mudassir", ""]]}, {"id": "1307.7364", "submitter": "Amit Weinstein", "authors": "Noga Alon, Rani Hod, Amit Weinstein", "title": "On active and passive testing", "comments": "16 pages", "journal-ref": "Combinator. Probab. Comp. 25 (2016) 1-20", "doi": "10.1017/S0963548315000292", "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a property of Boolean functions, what is the minimum number of queries\nrequired to determine with high probability if an input function satisfies this\nproperty or is \"far\" from satisfying it? This is a fundamental question in\nProperty Testing, where traditionally the testing algorithm is allowed to pick\nits queries among the entire set of inputs. Balcan, Blais, Blum and Yang have\nrecently suggested to restrict the tester to take its queries from a smaller\nrandom subset of polynomial size of the inputs. This model is called active\ntesting, and in the extreme case when the size of the set we can query from is\nexactly the number of queries performed it is known as passive testing.\n  We prove that passive or active testing of k-linear functions (that is, sums\nof k variables among n over Z_2) requires Theta(k*log n) queries, assuming k is\nnot too large. This extends the case k=1, (that is, dictator functions),\nanalyzed by Balcan et. al.\n  We also consider other classes of functions including low degree polynomials,\njuntas, and partially symmetric functions. Our methods combine algebraic,\ncombinatorial, and probabilistic techniques, including the Talagrand\nconcentration inequality and the Erdos--Rado theorem on Delta-systems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2013 14:02:44 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 21:40:04 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Alon", "Noga", ""], ["Hod", "Rani", ""], ["Weinstein", "Amit", ""]]}, {"id": "1307.7430", "submitter": "Heng Guo", "authors": "Jin-Yi Cai, Heng Guo and Tyson Williams", "title": "Holographic Algorithms Beyond Matchgates", "comments": "Inf. Comput., to appear. Author accepted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holographic algorithms introduced by Valiant are composed of two ingredients:\nmatchgates, which are gadgets realizing local constraint functions by weighted\nplanar perfect matchings, and holographic reductions, which show equivalences\namong problems with different descriptions via certain basis transformations.\nIn this paper, we replace matchgates in the paradigm above by the affine type\nand the product type constraint functions, which are known to be tractable in\ngeneral (not necessarily planar) graphs. More specifically, we present\npolynomial-time algorithms to decide if a given counting problem has a\nholographic reduction to another problem defined by the affine or product-type\nfunctions. Our algorithms also find a holographic transformation when one\nexists. We further present polynomial-time algorithms of the same decision and\nsearch problems for symmetric functions, where the complexity is measured in\nterms of the (exponentially more) succinct representations. The algorithm for\nthe symmetric case also shows that the recent dichotomy theorem for Holant\nproblems with symmetric constraints is efficiently decidable. Our proof\ntechniques are mainly algebraic, e.g., using stabilizers and orbits of group\nactions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 01:19:18 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 13:55:06 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Cai", "Jin-Yi", ""], ["Guo", "Heng", ""], ["Williams", "Tyson", ""]]}, {"id": "1307.7454", "submitter": "Mina Ghashami", "authors": "Mina Ghashami and Jeff M. Phillips", "title": "Relative Errors for Deterministic Low-Rank Matrix Approximations", "comments": "16 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider processing an n x d matrix A in a stream with row-wise updates\naccording to a recent algorithm called Frequent Directions (Liberty, KDD 2013).\nThis algorithm maintains an l x d matrix Q deterministically, processing each\nrow in O(d l^2) time; the processing time can be decreased to O(d l) with a\nslight modification in the algorithm and a constant increase in space. We show\nthat if one sets l = k+ k/eps and returns Q_k, a k x d matrix that is the best\nrank k approximation to Q, then we achieve the following properties: ||A -\nA_k||_F^2 <= ||A||_F^2 - ||Q_k||_F^2 <= (1+eps) ||A - A_k||_F^2 and where\npi_{Q_k}(A) is the projection of A onto the rowspace of Q_k then ||A -\npi_{Q_k}(A)||_F^2 <= (1+eps) ||A - A_k||_F^2.\n  We also show that Frequent Directions cannot be adapted to a sparse version\nin an obvious way that retains the l original rows of the matrix, as opposed to\na linear combination or sketch of the rows.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 03:50:16 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2013 21:45:01 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Ghashami", "Mina", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1307.7513", "submitter": "Vasanth Sena pesari", "authors": "P Vasanth Sena", "title": "An Approach Finding Frequent Items In Text Or Transactional Data Base By\n  Using BST To Improve The Efficiency Of Apriori Algorithm", "comments": "1 Algorithm. arXiv admin note: text overlap with arXiv:1009.4982 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining techniques have been widely used in various applications. Binary\nsearch tree based frequent items is an effective method for automatically\nrecognize the most frequent items, least frequent items and average frequent\nitems. This paper presents a new approach in order to find out frequent items.\nThe word frequent item refers to how many times the item appeared in the given\ninput. This approach is used to find out item sets in any order using familiar\napproach binary search tree. The method adapted here is in order to find out\nfrequent items by comparing and incrementing the counter variable in existing\ntransactional data base or text data. We are also representing different\napproaches in frequent item sets and also propose an algorithmic approach for\nthe problem solving\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 09:39:16 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Sena", "P Vasanth", ""]]}, {"id": "1307.7615", "submitter": "Raghu Meka", "authors": "Raghu Meka, Avi Wigderson", "title": "Association schemes, non-commutative polynomial concentration, and\n  sum-of-squares lower bounds for planted clique", "comments": "This paper has been withdrawn due to an error; \"Theorem 1.6\" from the\n  original manuscript which is used crucially in the proof of the main result\n  is not correct. We thank Gilles Pisier for pointing this out", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding cliques in random graphs and the closely related \"planted\" clique\nvariant, where a clique of size t is planted in a random G(n,1/2) graph, have\nbeen the focus of substantial study in algorithm design. Despite much effort,\nthe best known polynomial-time algorithms only solve the problem for t =\nTheta(sqrt(n)). Here we show that beating sqrt(n) would require substantially\nnew algorithmic ideas, by proving a lower bound for the problem in the\nsum-of-squares (or Lasserre) hierarchy, the most powerful class of\nsemi-definite programming algorithms we know of: r rounds of the sum-of-squares\nhierarchy can only solve the planted clique for t > sqrt(n)/(C log n)^(r^2).\nPreviously, no nontrivial lower bounds were known. Our proof is formulated as a\ndegree lower bound in the Positivstellensatz algebraic proof system, which is\nequivalent to the sum-of-squares hierarchy. The heart of our (average-case)\nlower bound is a proof that a certain random matrix derived from the input\ngraph is (with high probability) positive semidefinite. Two ingredients play an\nimportant role in this proof. The first is the classical theory of association\nschemes, applied to the average and variance of that random matrix. The second\nis a new large deviation inequality for matrix-valued polynomials. Our new tail\nestimate seems to be of independent interest and may find other applications,\nas it generalizes both the estimates on real-valued polynomials and on sums of\nindependent random matrices.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 15:25:24 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 16:38:45 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2013 01:23:35 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Meka", "Raghu", ""], ["Wigderson", "Avi", ""]]}, {"id": "1307.7751", "submitter": "Guoming Tang", "authors": "Guoming Tang, Kui Wu, Jingsheng Lei, Zhongqin Bi and Jiuyang Tang", "title": "From Landscape to Portrait: A New Approach for Outlier Detection in Load\n  Curve Data", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In power systems, load curve data is one of the most important datasets that\nare collected and retained by utilities. The quality of load curve data,\nhowever, is hard to guarantee since the data is subject to communication\nlosses, meter malfunctions, and many other impacts. In this paper, a new\napproach to analyzing load curve data is presented. The method adopts a new\nview, termed \\textit{portrait}, on the load curve data by analyzing the\nperiodic patterns in the data and re-organizing the data for ease of analysis.\nFurthermore, we introduce algorithms to build the virtual portrait load curve\ndata, and demonstrate its application on load curve data cleansing. Compared to\nexisting regression-based methods, our method is much faster and more accurate\nfor both small-scale and large-scale real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 21:59:30 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 17:22:07 GMT"}, {"version": "v3", "created": "Mon, 7 Apr 2014 19:17:23 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Tang", "Guoming", ""], ["Wu", "Kui", ""], ["Lei", "Jingsheng", ""], ["Bi", "Zhongqin", ""], ["Tang", "Jiuyang", ""]]}, {"id": "1307.7752", "submitter": "Bei Wang", "authors": "Harsh Bhatia and Bei Wang and Gregory Norgard and Valerio Pascucci and\n  Peer-Timo Bremer", "title": "Local, Smooth, and Consistent Jacobi Set Simplification", "comments": "24 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relation between two Morse functions defined on a common domain can be\nstudied in terms of their Jacobi set. The Jacobi set contains points in the\ndomain where the gradients of the functions are aligned. Both the Jacobi set\nitself as well as the segmentation of the domain it induces have shown to be\nuseful in various applications. Unfortunately, in practice functions often\ncontain noise and discretization artifacts causing their Jacobi set to become\nunmanageably large and complex. While there exist techniques to simplify Jacobi\nsets, these are unsuitable for most applications as they lack fine-grained\ncontrol over the process and heavily restrict the type of simplifications\npossible.\n  In this paper, we introduce a new framework that generalizes critical point\ncancellations in scalar functions to Jacobi sets in two dimensions. We focus on\nsimplifications that can be realized by smooth approximations of the\ncorresponding functions and show how this implies simultaneously simplifying\ncontiguous subsets of the Jacobi set. These extended cancellations form the\natomic operations in our framework, and we introduce an algorithm to\nsuccessively cancel subsets of the Jacobi set with minimal modifications\naccording to some user-defined metric. We prove that the algorithm is correct\nand terminates only once no more local, smooth and consistent simplifications\nare possible. We disprove a previous claim on the minimal Jacobi set for\nmanifolds with arbitrary genus and show that for simply connected domains, our\nalgorithm reduces a given Jacobi set to its simplest configuration.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 22:00:00 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Bhatia", "Harsh", ""], ["Wang", "Bei", ""], ["Norgard", "Gregory", ""], ["Pascucci", "Valerio", ""], ["Bremer", "Peer-Timo", ""]]}, {"id": "1307.7806", "submitter": "Aaron Darling", "authors": "Evgeny Kapun and Fedor Tsarev", "title": "On NP-Hardness of the Paired de Bruijn Sound Cycle Problem", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paired de Bruijn graph is an extension of de Bruijn graph incorporating\nmate pair information for genome assembly proposed by Mevdedev et al. However,\nunlike in an ordinary de Bruijn graph, not every path or cycle in a paired de\nBruijn graph will spell a string, because there is an additional soundness\nconstraint on the path. In this paper we show that the problem of checking if\nthere is a sound cycle in a paired de Bruijn graph is NP-hard in general case.\nWe also explore some of its special cases, as well as a modified version where\nthe cycle must also pass through every edge.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 04:15:52 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Kapun", "Evgeny", ""], ["Tsarev", "Fedor", ""]]}, {"id": "1307.7811", "submitter": "Aaron Darling", "authors": "Alexandru I. Tomescu, Anna Kuosmanen, Romeo Rizzi, and Veli M\\\"akinen", "title": "A Novel Combinatorial Method for Estimating Transcript Expression with\n  RNA-Seq: Bounding the Number of Paths", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNA-Seq technology offers new high-throughput ways for transcript\nidentification and quantification based on short reads, and has recently\nattracted great interest. The problem is usually modeled by a weighted splicing\ngraph whose nodes stand for exons and whose edges stand for split alignments to\nthe exons. The task consists of finding a number of paths, together with their\nexpression levels, which optimally explain the coverages of the graph under\nvarious fitness functions, such least sum of squares. In (Tomescu et al.\nRECOMB-seq 2013) we showed that under general fitness functions, if we allow a\npolynomially bounded number of paths in an optimal solution, this problem can\nbe solved in polynomial time by a reduction to a min-cost flow program. In this\npaper we further refine this problem by asking for a bounded number k of paths\nthat optimally explain the splicing graph. This problem becomes NP-hard in the\nstrong sense, but we give a fast combinatorial algorithm based on dynamic\nprogramming for it. In order to obtain a practical tool, we implement three\noptimizations and heuristics, which achieve better performance on real data,\nand similar or better performance on simulated data, than state-of-the-art\ntools Cufflinks, IsoLasso and SLIDE. Our tool, called Traph, is available at\nhttp://www.cs.helsinki.fi/gsa/traph/\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 04:42:47 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Tomescu", "Alexandru I.", ""], ["Kuosmanen", "Anna", ""], ["Rizzi", "Romeo", ""], ["M\u00e4kinen", "Veli", ""]]}, {"id": "1307.7813", "submitter": "Aaron Darling", "authors": "Gustavo Sacomoto, Vincent Lacroix, and Marie-France Sagot", "title": "A polynomial delay algorithm for the enumeration of bubbles with length\n  constraints in directed graphs and its application to the detection of\n  alternative splicing in RNA-seq data", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for enumerating bubbles with length constraints in\ndirected graphs. This problem arises in transcriptomics, where the question is\nto identify all alternative splicing events present in a sample of mRNAs\nsequenced by RNA-seq. This is the first polynomial-delay algorithm for this\nproblem and we show that in practice, it is faster than previous approaches.\nThis enables us to deal with larger instances and therefore to discover novel\nalternative splicing events, especially long ones, that were previously\noverseen using existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 04:48:08 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Sacomoto", "Gustavo", ""], ["Lacroix", "Vincent", ""], ["Sagot", "Marie-France", ""]]}, {"id": "1307.7820", "submitter": "Aaron Darling", "authors": "Balaji Venkatachalam, Dan Gusfield, and Yelena Frid", "title": "Faster Algorithms for RNA-folding using the Four-Russians method", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013). Editor's note: abstract was\n  shortened to comply with arxiv requirements. Full abstract in PDF", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The secondary structure that maximizes the number of non-crossing matchings\nbetween complimentary bases of an RNA sequence of length n can be computed in\nO(n^3) time using Nussinov's dynamic programming algorithm. The Four-Russians\nmethod is a technique that will reduce the running time for certain dynamic\nprogramming algorithms by a multiplicative factor after a preprocessing step\nwhere solutions to all smaller subproblems of a fixed size are exhaustively\nenumerated and solved. Frid and Gusfield designed an O(\\frac{n^3}{\\log n})\nalgorithm for RNA folding using the Four-Russians technique. In their algorithm\nthe preprocessing is interleaved with the algorithm computation. (Algo. Mol.\nBiol., 2010).\n  We simplify the algorithm and the analysis by doing the preprocessing once\nprior to the algorithm computation. We call this the two-vector method. We also\nshow variants where instead of exhaustive preprocessing, we only solve the\nsubproblems encountered in the main algorithm once and memoize the results. We\ngive a simple proof of correctness and explore the practical advantages over\nthe earlier method. The Nussinov algorithm admits an O(n^2) time parallel\nalgorithm. We show a parallel algorithm using the two-vector idea that improves\nthe time bound to O(\\frac{n^2}{log n}).\n  We discuss the organization of the data structures to exploit coalesced\nmemory access for fast running times. The ideas to organize the data structures\nalso help in improving the running time of the serial algorithms. For sequences\nof length up to 6000 bases the parallel algorithm takes only about 2.5 seconds\nand the two-vector serial method takes about 57 seconds on a desktop and 15\nseconds on a server. Among the serial algorithms, the two-vector and memoized\nversions are faster than the Frid-Gusfield algorithm by a factor of 3, and are\nfaster than Nussinov by up to a factor of 20.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 05:13:11 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Venkatachalam", "Balaji", ""], ["Gusfield", "Dan", ""], ["Frid", "Yelena", ""]]}, {"id": "1307.7821", "submitter": "Aaron Darling", "authors": "Jesper Jansson, Chuanqi Shen, and Wing-Kin Sung", "title": "Algorithms for the Majority Rule (+) Consensus Tree and the Frequency\n  Difference Consensus Tree", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two new deterministic algorithms for constructing\nconsensus trees. Given an input of k phylogenetic trees with identical leaf\nlabel sets and n leaves each, the first algorithm constructs the majority rule\n(+) consensus tree in O(kn) time, which is optimal since the input size is\nOmega(kn), and the second one constructs the frequency difference consensus\ntree in min(O(kn^2), O(kn (k+log^2 n))) time.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 05:24:12 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2013 05:40:35 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Jansson", "Jesper", ""], ["Shen", "Chuanqi", ""], ["Sung", "Wing-Kin", ""]]}, {"id": "1307.7824", "submitter": "Aaron Darling", "authors": "Sebastian B\\\"ocker, Stefan Canzar, and Gunnar W. Klau", "title": "The generalized Robinson-Foulds metric", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Robinson-Foulds (RF) metric is arguably the most widely used measure of\nphylogenetic tree similarity, despite its well-known shortcomings: For example,\nmoving a single taxon in a tree can result in a tree that has maximum distance\nto the original one; but the two trees are identical if we remove the single\ntaxon. To this end, we propose a natural extension of the RF metric that does\nnot simply count identical clades but instead, also takes similar clades into\nconsideration. In contrast to previous approaches, our model requires the\nmatching between clades to respect the structure of the two trees, a property\nthat the classical RF metric exhibits, too. We show that computing this\ngeneralized RF metric is, unfortunately, NP-hard. We then present a simple\nInteger Linear Program for its computation, and evaluate it by an\nall-against-all comparison of 100 trees from a benchmark data set. We find that\nmatchings that respect the tree structure differ significantly from those that\ndo not, underlining the importance of this natural condition.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 05:29:55 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["B\u00f6cker", "Sebastian", ""], ["Canzar", "Stefan", ""], ["Klau", "Gunnar W.", ""]]}, {"id": "1307.7825", "submitter": "Aaron Darling", "authors": "Constantinos Tsirogiannis and Brody Sandel", "title": "Computing the Skewness of the Phylogenetic Mean Pairwise Distance in\n  Linear Time", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phylogenetic Mean Pairwise Distance (MPD) is one of the most popular\nmeasures for computing the phylogenetic distance between a given group of\nspecies. More specifically, for a phylogenetic tree T and for a set of species\nR represented by a subset of the leaf nodes of T, the MPD of R is equal to the\naverage cost of all possible simple paths in T that connect pairs of nodes in\nR.\n  Among other phylogenetic measures, the MPD is used as a tool for deciding if\nthe species of a given group R are closely related. To do this, it is important\nto compute not only the value of the MPD for this group but also the\nexpectation, the variance, and the skewness of this metric. Although efficient\nalgorithms have been developed for computing the expectation and the variance\nthe MPD, there has been no approach so far for computing the skewness of this\nmeasure.\n  In the present work we describe how to compute the skewness of the MPD on a\ntree T optimally, in Theta(n) time; here n is the size of the tree T. So far\nthis is the first result that leads to an exact, let alone efficient,\ncomputation of the skewness for any popular phylogenetic distance measure.\nMoreover, we show how we can compute in Theta(n) time several interesting\nquantities in T that can be possibly used as building blocks for computing\nefficiently the skewness of other phylogenetic measures.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 05:37:17 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Tsirogiannis", "Constantinos", ""], ["Sandel", "Brody", ""]]}, {"id": "1307.7831", "submitter": "Aaron Darling", "authors": "Nicolas Wieseke, Matthias Bernt, and Martin Middendorf", "title": "Unifying Parsimonious Tree Reconciliation", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DS q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution is a process that is influenced by various environmental factors,\ne.g. the interactions between different species, genes, and biogeographical\nproperties. Hence, it is interesting to study the combined evolutionary history\nof multiple species, their genes, and the environment they live in. A common\napproach to address this research problem is to describe each individual\nevolution as a phylogenetic tree and construct a tree reconciliation which is\nparsimonious with respect to a given event model. Unfortunately, most of the\nprevious approaches are designed only either for host-parasite systems, for\ngene tree/species tree reconciliation, or biogeography. Hence, a method is\ndesirable, which addresses the general problem of mapping phylogenetic trees\nand covering all varieties of coevolving systems, including e.g., predator-prey\nand symbiotic relationships. To overcome this gap, we introduce a generalized\ncophylogenetic event model considering the combinatorial complete set of local\ncoevolutionary events. We give a dynamic programming based heuristic for\nsolving the maximum parsimony reconciliation problem in time O(n^2), for two\nphylogenies each with at most n leaves. Furthermore, we present an exact\nbranch-and-bound algorithm which uses the results from the dynamic programming\nheuristic for discarding partial reconciliations. The approach has been\nimplemented as a Java application which is freely available from\nhttp://pacosy.informatik.uni-leipzig.de/coresym.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 05:47:27 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Wieseke", "Nicolas", ""], ["Bernt", "Matthias", ""], ["Middendorf", "Martin", ""]]}, {"id": "1307.7842", "submitter": "Aaron Darling", "authors": "Laurent Bulteau, Guillaume Fertin, Christian Komusiewicz, and Irena\n  Rusu", "title": "A Fixed-Parameter Algorithm for Minimum Common String Partition with Few\n  Duplications", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the study of genome rearrangements, the NP-hard Minimum Common\nString Partition problems asks, given two strings, to split both strings into\nan identical set of blocks. We consider an extension of this problem to\nunbalanced strings, so that some elements may not be covered by any block. We\npresent an efficient fixed-parameter algorithm for the parameters number k of\nblocks and maximum occurrence d of a letter in either string. We then evaluate\nthis algorithm on bacteria genomes and synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 06:59:15 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Bulteau", "Laurent", ""], ["Fertin", "Guillaume", ""], ["Komusiewicz", "Christian", ""], ["Rusu", "Irena", ""]]}, {"id": "1307.7925", "submitter": "Aaron Darling", "authors": "Taku Onodera, Kunihiko Sadakane, and Tetsuo Shibuya", "title": "Detecting Superbubbles in Assembly Graphs", "comments": "Peer-reviewed and presented as part of the 13th Workshop on\n  Algorithms in Bioinformatics (WABI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE cs.DM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new concept of a subgraph class called a superbubble for\nanalyzing assembly graphs, and propose an efficient algorithm for detecting it.\nMost assembly algorithms utilize assembly graphs like the de Bruijn graph or\nthe overlap graph constructed from reads. From these graphs, many assembly\nalgorithms first detect simple local graph structures (motifs), such as tips\nand bubbles, mainly to find sequencing errors. These motifs are easy to detect,\nbut they are sometimes too simple to deal with more complex errors. The\nsuperbubble is an extension of the bubble, which is also important for\nanalyzing assembly graphs. Though superbubbles are much more complex than\nordinary bubbles, we show that they can be efficiently enumerated. We propose\nan average-case linear time algorithm (i.e., O(n+m) for a graph with n vertices\nand m edges) for graphs with a reasonable model, though the worst-case time\ncomplexity of our algorithm is quadratic (i.e., O(n(n+m))). Moreover, the\nalgorithm is practically very fast: Our experiments show that our algorithm\nruns in reasonable time with a single CPU core even against a very large graph\nof a whole human genome.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 11:44:34 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Onodera", "Taku", ""], ["Sadakane", "Kunihiko", ""], ["Shibuya", "Tetsuo", ""]]}, {"id": "1307.8037", "submitter": "L\\'aszl\\'o V\\'egh", "authors": "Nikhil R. Devanur and Jugal Garg and L\\'aszl\\'o A. V\\'egh", "title": "A Rational Convex Program for Linear Arrow-Debreu Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new, flow-type convex program describing equilibrium solutions to\nlinear Arrow-Debreu markets. Whereas convex formulations were previously known\n[Nenakov, Primak 83; Jain 07; Cornet '89], our program exhibits several new\nfeatures. It gives a simple necessary and sufficient condition and a concise\nproof of the existence and rationality of equilibria, settling an open question\nraised by Vazirani. As a consequence we also obtain a simple new proof of\nMertens's result that the equilibrium prices form a convex polyhedral set.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 16:42:45 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 16:44:29 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Devanur", "Nikhil R.", ""], ["Garg", "Jugal", ""], ["V\u00e9gh", "L\u00e1szl\u00f3 A.", ""]]}, {"id": "1307.8139", "submitter": "Esther Ezra", "authors": "Esther Ezra", "title": "A Size-Sensitive Discrepancy Bound for Set Systems of Bounded Primal\n  Shatter Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(X,\\S)$ be a set system on an $n$-point set $X$. The \\emph{discrepancy}\nof $\\S$ is defined as the minimum of the largest deviation from an even split,\nover all subsets of $S \\in \\S$ and two-colorings $\\chi$ on $X$. We consider the\nscenario where, for any subset $X' \\subseteq X$ of size $m \\le n$ and for any\nparameter $1 \\le k \\le m$, the number of restrictions of the sets of $\\S$ to\n$X'$ of size at most $k$ is only $O(m^{d_1} k^{d-d_1})$, for fixed integers $d\n> 0$ and $1 \\le d_1 \\le d$ (this generalizes the standard notion of\n\\emph{bounded primal shatter dimension} when $d_1 = d$). In this case we show\nthat there exists a coloring $\\chi$ with discrepancy bound $O^{*}(|S|^{1/2 -\nd_1/(2d)} n^{(d_1 - 1)/(2d)})$, for each $S \\in \\S$, where $O^{*}(\\cdot)$ hides\na polylogarithmic factor in $n$. This bound is tight up to a polylogarithmic\nfactor \\cite{Mat-95, Mat-99} and the corresponding coloring $\\chi$ can be\ncomputed in expected polynomial time using the very recent machinery of Lovett\nand Meka for constructive discrepancy minimization \\cite{LM-12}. Our bound\nimproves and generalizes the bounds obtained from the machinery of Har-Peled\nand Sharir \\cite{HS-11} (and the follow-up work in \\cite{SZ-12}) for points and\nhalfspaces in $d$-space for $d \\ge 3$.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 20:28:22 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Ezra", "Esther", ""]]}, {"id": "1307.8371", "submitter": "Pranjal Awasthi", "authors": "Pranjal Awasthi, Maria Florina Balcan, Philip M. Long", "title": "The Power of Localization for Efficiently Learning Linear Separators\n  with Noise", "comments": "Contains improved label complexity analysis communicated to us by\n  Steve Hanneke", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for designing computationally efficient learning\nalgorithms that are tolerant to noise, and demonstrate its effectiveness by\ndesigning algorithms with improved noise tolerance guarantees for learning\nlinear separators.\n  We consider both the malicious noise model and the adversarial label noise\nmodel. For malicious noise, where the adversary can corrupt both the label and\nthe features, we provide a polynomial-time algorithm for learning linear\nseparators in $\\Re^d$ under isotropic log-concave distributions that can\ntolerate a nearly information-theoretically optimal noise rate of $\\eta =\n\\Omega(\\epsilon)$. For the adversarial label noise model, where the\ndistribution over the feature vectors is unchanged, and the overall probability\nof a noisy label is constrained to be at most $\\eta$, we also give a\npolynomial-time algorithm for learning linear separators in $\\Re^d$ under\nisotropic log-concave distributions that can handle a noise rate of $\\eta =\n\\Omega\\left(\\epsilon\\right)$.\n  We show that, in the active learning model, our algorithms achieve a label\ncomplexity whose dependence on the error parameter $\\epsilon$ is\npolylogarithmic. This provides the first polynomial-time active learning\nalgorithm for learning linear separators in the presence of malicious noise or\nadversarial label noise.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 16:11:26 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2013 18:51:51 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2013 21:49:27 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2013 19:18:31 GMT"}, {"version": "v5", "created": "Mon, 16 Dec 2013 17:20:36 GMT"}, {"version": "v6", "created": "Fri, 3 Jan 2014 17:20:00 GMT"}, {"version": "v7", "created": "Fri, 7 Mar 2014 17:15:52 GMT"}, {"version": "v8", "created": "Wed, 12 Oct 2016 17:42:40 GMT"}, {"version": "v9", "created": "Sun, 3 Jun 2018 18:22:37 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Balcan", "Maria Florina", ""], ["Long", "Philip M.", ""]]}]