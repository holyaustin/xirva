[{"id": "1004.0334", "submitter": "Piotr Faliszewski", "authors": "Edith Elkind and Piotr Faliszewski", "title": "Approximation Algorithms for Campaign Management", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study electoral campaign management scenarios in which an external party\ncan buy votes, i.e., pay the voters to promote its preferred candidate in their\npreference rankings. The external party's goal is to make its preferred\ncandidate a winner while paying as little as possible. We describe a\n2-approximation algorithm for this problem for a large class of electoral\nsystems known as scoring rules. Our result holds even for weighted voters, and\nhas applications for campaign management in commercial settings. We also give\napproximation algorithms for our problem for two Condorcet-consistent rules,\nnamely, the Copeland rule and maximin.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 10:34:51 GMT"}, {"version": "v2", "created": "Wed, 24 Nov 2010 10:43:44 GMT"}, {"version": "v3", "created": "Thu, 25 Nov 2010 17:42:11 GMT"}], "update_date": "2010-11-29", "authors_parsed": [["Elkind", "Edith", ""], ["Faliszewski", "Piotr", ""]]}, {"id": "1004.0351", "submitter": "Srivathsan Srinivasagopalan", "authors": "Srivathsan Srinivasagopalan, Costas Busch, S.S. Iyengar", "title": "An Oblivious Spanning Tree for Buy-at-Bulk Network Design Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing a single spanning tree for the\nsingle-source buy-at-bulk network design problem for doubling-dimension graphs.\nWe compute a spanning tree to route a set of demands (or data) along a graph to\nor from a designated root node. The demands could be aggregated at (or\nsymmetrically distributed to) intermediate nodes where the fusion-cost is\nspecified by a non-negative concave function $f$. We describe a novel approach\nfor developing an oblivious spanning tree in the sense that it is independent\nof the number of data sources (or demands) and cost function at intermediate\nnodes. To our knowledge, this is the first paper to propose a single spanning\ntree solution to this problem (as opposed to multiple overlay trees). There has\nbeen no prior work where the tree is oblivious to both the fusion cost function\nand the set of sources (demands). We present a deterministic, polynomial-time\nalgorithm for constructing a spanning tree in low doubling graphs that\nguarantees $\\log^{3}D\\cdot\\log n$-approximation over the optimal cost, where\n$D$ is the diameter of the graph and $n$ the total number of nodes. With\nconstant fusion-cost function our spanning tree gives a $O(\\log^3\nD)$-approximation for every Steiner tree to the root.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2010 15:51:33 GMT"}], "update_date": "2010-04-05", "authors_parsed": [["Srinivasagopalan", "Srivathsan", ""], ["Busch", "Costas", ""], ["Iyengar", "S. S.", ""]]}, {"id": "1004.0403", "submitter": "Alessandro Colantonio", "authors": "Alessandro Colantonio and Roberto Di Pietro", "title": "CONCISE: Compressed 'n' Composable Integer Set", "comments": "Preprint submitted to Information Processing Letters, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bit arrays, or bitmaps, are used to significantly speed up set operations in\nseveral areas, such as data warehousing, information retrieval, and data\nmining, to cite a few. However, bitmaps usually use a large storage space, thus\nrequiring compression. Nevertheless, there is a space-time tradeoff among\ncompression schemes. The Word Aligned Hybrid (WAH) bitmap compression trades\nsome space to allow for bitwise operations without first decompressing bitmaps.\nWAH has been recognized as the most efficient scheme in terms of computation\ntime. In this paper we present CONCISE (Compressed 'n' Composable Integer Set),\na new scheme that enjoys significatively better performances than those of WAH.\nIn particular, when compared to WAH, our algorithm is able to reduce the\nrequired memory up to 50%, by having similar or better performance in terms of\ncomputation time. Further, we show that CONCISE can be efficiently used to\nmanipulate bitmaps representing sets of integral numbers in lieu of well-known\ndata structures such as arrays, lists, hashtables, and self-balancing binary\nsearch trees. Extensive experiments over synthetic data show the effectiveness\nof our approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2010 00:07:00 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Colantonio", "Alessandro", ""], ["Di Pietro", "Roberto", ""]]}, {"id": "1004.0424", "submitter": "Alexandru Popa Mr.", "authors": "Rapha\\\"el Clifford, Zvi Gotthilf, Moshe Lewenstein and Alexandru Popa", "title": "Restricted Common Superstring and Restricted Common Supersequence", "comments": "Submitted to WAOA 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\em shortest common superstring} and the {\\em shortest common\nsupersequence} are two well studied problems having a wide range of\napplications. In this paper we consider both problems with resource\nconstraints, denoted as the Restricted Common Superstring (shortly\n\\textit{RCSstr}) problem and the Restricted Common Supersequence (shortly\n\\textit{RCSseq}). In the \\textit{RCSstr} (\\textit{RCSseq}) problem we are given\na set $S$ of $n$ strings, $s_1$, $s_2$, $\\ldots$, $s_n$, and a multiset $t =\n\\{t_1, t_2, \\dots, t_m\\}$, and the goal is to find a permutation $\\pi : \\{1,\n\\dots, m\\} \\to \\{1, \\dots, m\\}$ to maximize the number of strings in $S$ that\nare substrings (subsequences) of $\\pi(t) = t_{\\pi(1)}t_{\\pi(2)}...t_{\\pi(m)}$\n(we call this ordering of the multiset, $\\pi(t)$, a permutation of $t$). We\nfirst show that in its most general setting the \\textit{RCSstr} problem is {\\em\nNP-complete} and hard to approximate within a factor of $n^{1-\\epsilon}$, for\nany $\\epsilon > 0$, unless P = NP. Afterwards, we present two separate\nreductions to show that the \\textit{RCSstr} problem remains NP-Hard even in the\ncase where the elements of $t$ are drawn from a binary alphabet or for the case\nwhere all input strings are of length two. We then present some approximation\nresults for several variants of the \\textit{RCSstr} problem. In the second part\nof this paper, we turn to the \\textit{RCSseq} problem, where we present some\nhardness results, tight lower bounds and approximation algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2010 07:14:53 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2010 18:56:01 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Clifford", "Rapha\u00ebl", ""], ["Gotthilf", "Zvi", ""], ["Lewenstein", "Moshe", ""], ["Popa", "Alexandru", ""]]}, {"id": "1004.0526", "submitter": "Gregory Gutin", "authors": "R. Crowston, G. Gutin, M. Jones, A. Yeo", "title": "A New Lower Bound on the Maximum Number of Satisfied Clauses in Max-SAT\n  and its Algorithmic Applications", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-17493-3_10", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pair of unit clauses is called conflicting if it is of the form $(x)$,\n$(\\bar{x})$. A CNF formula is unit-conflict free (UCF) if it contains no pair\nof conflicting unit clauses. Lieberherr and Specker (J. ACM 28, 1981) showed\nthat for each UCF CNF formula with $m$ clauses we can simultaneously satisfy at\nleast $\\pp m$ clauses, where $\\pp =(\\sqrt{5}-1)/2$. We improve the\nLieberherr-Specker bound by showing that for each UCF CNF formula $F$ with $m$\nclauses we can find, in polynomial time, a subformula $F'$ with $m'$ clauses\nsuch that we can simultaneously satisfy at least $\\pp m+(1-\\pp)m'+(2-3\\pp)n\"/2$\nclauses (in $F$), where $n\"$ is the number of variables in $F$ which are not in\n$F'$.\n  We consider two parameterized versions of MAX-SAT, where the parameter is the\nnumber of satisfied clauses above the bounds $m/2$ and $m(\\sqrt{5}-1)/2$. The\nformer bound is tight for general formulas, and the later is tight for UCF\nformulas. Mahajan and Raman (J. Algorithms 31, 1999) showed that every instance\nof the first parameterized problem can be transformed, in polynomial time, into\nan equivalent one with at most $6k+3$ variables and $10k$ clauses. We improve\nthis to $4k$ variables and $(2\\sqrt{5}+4)k$ clauses. Mahajan and Raman\nconjectured that the second parameterized problem is fixed-parameter tractable\n(FPT). We show that the problem is indeed FPT by describing a polynomial-time\nalgorithm that transforms any problem instance into an equivalent one with at\nmost $(7+3\\sqrt{5})k$ variables. Our results are obtained using our improvement\nof the Lieberherr-Specker bound above.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2010 18:46:00 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2010 10:40:22 GMT"}, {"version": "v3", "created": "Thu, 28 Oct 2010 07:55:52 GMT"}, {"version": "v4", "created": "Mon, 3 Jan 2011 13:03:08 GMT"}, {"version": "v5", "created": "Fri, 8 Jul 2011 13:45:31 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Crowston", "R.", ""], ["Gutin", "G.", ""], ["Jones", "M.", ""], ["Yeo", "A.", ""]]}, {"id": "1004.0558", "submitter": "John Augustine", "authors": "John Augustine, Sandip Das, Anil Maheshwari, Subhas Nandy, Sasanka Roy\n  and Swami Sarvattomananda", "title": "Querying for the Largest Empty Geometric Object in a Desired Location", "comments": "This version is a significant update of our earlier submission\n  arXiv:1004.0558v1. Apart from new variants studied in Sections 3 and 4, the\n  results have been improved in Section 5.Please note that the change in title\n  and abstract indicate that we have expanded the scope of the problems we\n  study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study new types of geometric query problems defined as follows: given a\ngeometric set $P$, preprocess it such that given a query point $q$, the\nlocation of the largest circle that does not contain any member of $P$, but\ncontains $q$ can be reported efficiently. The geometric sets we consider for\n$P$ are boundaries of convex and simple polygons, and point sets. While we\nprimarily focus on circles as the desired shape, we also briefly discuss empty\nrectangles in the context of point sets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2010 03:36:48 GMT"}, {"version": "v2", "created": "Thu, 16 Dec 2010 00:11:30 GMT"}], "update_date": "2010-12-17", "authors_parsed": [["Augustine", "John", ""], ["Das", "Sandip", ""], ["Maheshwari", "Anil", ""], ["Nandy", "Subhas", ""], ["Roy", "Sasanka", ""], ["Sarvattomananda", "Swami", ""]]}, {"id": "1004.0653", "submitter": "Oliver Kullmann", "authors": "Oliver Kullmann", "title": "Exact Ramsey Theory: Green-Tao numbers and SAT", "comments": "25 pages; a shortened version appears in LNCS (Springer), \"Theory and\n  Applications of Satisfiability Testing - SAT 2010\", editors O. Strichman and\n  S. Szeider. Revision contains new van-der-Waerden and Green-Tao numbers,\n  especially \"transversal numbers\", corresponding to independence numbers of\n  hypergraphs of arithmetic progressions. Some new comments discussing\n  behaviour of vdW- and GT-numbers.", "journal-ref": "Theory and Applications of Satisfiability Testing - SAT 2010, LNCS\n  6175, 352-362", "doi": "10.1007/978-3-642-14186-7_32", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the links between Ramsey theory in the integers, based on van der\nWaerden's theorem, and (boolean, CNF) SAT solving. We aim at using the problems\nfrom exact Ramsey theory, concerned with computing Ramsey-type numbers, as a\nrich source of test problems, where especially methods for solving hard\nproblems can be developed. In order to control the growth of the problem\ninstances, we introduce \"transversal extensions\" as a natural way of\nconstructing mixed parameter tuples (k_1, ..., k_m) for van-der-Waerden-like\nnumbers N(k_1, ..., k_m), such that the growth of these numbers is guaranteed\nto be linear. Based on Green-Tao's theorem we introduce the \"Green-Tao numbers\"\ngrt(k_1, ..., k_m), which in a sense combine the strict structure of van der\nWaerden problems with the (pseudo-)randomness of the distribution of prime\nnumbers. Using standard SAT solvers (look-ahead, conflict-driven, and local\nsearch) we determine the basic values. It turns out that already for this\nsingle form of Ramsey-type problems, when considering the best-performing\nsolvers a wide variety of solver types is covered. For m > 2 the problems are\nnon-boolean, and we introduce the \"generic translation scheme\", which offers an\ninfinite variety of translations (\"encodings\") and covers the known methods. In\nmost cases the special instance called \"nested translation\" proved to be far\nsuperior.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2010 16:14:31 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2010 18:47:14 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Kullmann", "Oliver", ""]]}, {"id": "1004.0744", "submitter": "Valentin Brimkov", "authors": "Valentin E. Brimkov", "title": "Patrolling a Street Network is Strongly NP-Complete but in P for Tree\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: Given a finite set of straight line\nsegments in the plane, determine the positions of a minimal number of points on\nthe segments, from which guards can see all segments. This problem can be\ninterpreted as looking for a minimal number of locations of policemen, guards,\ncameras or other sensors, that can observe a network of streets, corridors,\ntunnels, tubes, etc. We show that the problem is strongly NP-complete even for\na set of segments with a cubic graph structure, but in P for tree structures.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 01:06:27 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Brimkov", "Valentin E.", ""]]}, {"id": "1004.0902", "submitter": "Kimmo Fredriksson", "authors": "Kimmo Fredriksson", "title": "On building minimal automaton for subset matching queries", "comments": "Accepted to IPL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of building an index for a set $D$ of $n$ strings,\nwhere each string location is a subset of some finite integer alphabet of size\n$\\sigma$, so that we can answer efficiently if a given simple query string\n(where each string location is a single symbol) $p$ occurs in the set. That is,\nwe need to efficiently find a string $d \\in D$ such that $p[i] \\in d[i]$ for\nevery $i$. We show how to build such index in\n$O(n^{\\log_{\\sigma/\\Delta}(\\sigma)}\\log(n))$ average time, where $\\Delta$ is\nthe average size of the subsets. Our methods have applications e.g.\\ in\ncomputational biology (haplotype inference) and music information retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 17:20:03 GMT"}, {"version": "v2", "created": "Fri, 1 Oct 2010 08:15:22 GMT"}], "update_date": "2010-10-04", "authors_parsed": [["Fredriksson", "Kimmo", ""]]}, {"id": "1004.0995", "submitter": "David Doty", "authors": "David Doty, Matthew J. Patitz, Dustin Reishus, Robert T. Schweller,\n  Scott M. Summers", "title": "Strong Fault-Tolerance for Self-Assembly with Fuzzy Temperature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fault-tolerance in nanoscale algorithmic\nself-assembly. We employ a variant of Winfree's abstract Tile Assembly Model\n(aTAM), the two-handed aTAM, in which square \"tiles\" -- a model of molecules\nconstructed from DNA for the purpose of engineering self-assembled\nnanostructures -- aggregate according to specific binding sites of varying\nstrengths, and in which large aggregations of tiles may attach to each other,\nin contrast to the seeded aTAM, in which tiles aggregate one at a time to a\nsingle specially-designated \"seed\" assembly. We focus on a major cause of\nerrors in tile-based self-assembly: that of unintended growth due to \"weak\"\nstrength-1 bonds, which if allowed to persist, may be stabilized by subsequent\nattachment of neighboring tiles in the sense that at least energy 2 is now\nrequired to break apart the resulting assembly; i.e., the errant assembly is\nstable at temperature 2. We study a common self-assembly benchmark problem,\nthat of assembling an n x n square using O(log n) unique tile types, under the\ntwo-handed model of self-assembly. Our main result achieves a much stronger\nnotion of fault-tolerance than those achieved previously. Arbitrary strength-1\ngrowth is allowed (i.e., the temperature is \"fuzzy\" and may drift from 2 to 1\nfor arbitrarily long); however, any assembly that grows sufficiently to become\nstable at temperature 2 is guaranteed to assemble at temperature 2 into the\ncorrect final assembly of an n x n square. In other words, errors due to\ninsufficient attachment, which is the cause of errors studied in earlier papers\non fault-tolerance, are prevented absolutely in our main construction, rather\nthan only with high probability and for sufficiently small structures, as in\nprevious fault-tolerance studies.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 02:21:21 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Doty", "David", ""], ["Patitz", "Matthew J.", ""], ["Reishus", "Dustin", ""], ["Schweller", "Robert T.", ""], ["Summers", "Scott M.", ""]]}, {"id": "1004.1001", "submitter": "Marko A. Rodriguez", "authors": "Marko A. Rodriguez and Peter Neubauer", "title": "The Graph Traversal Pattern", "comments": null, "journal-ref": "chapter in Graph Data Management: Techniques and Applications,\n  eds. S. Sakr, E. Pardede, 2011", "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  A graph is a structure composed of a set of vertices (i.e.nodes, dots)\nconnected to one another by a set of edges (i.e.links, lines). The concept of a\ngraph has been around since the late 19$^\\text{th}$ century, however, only in\nrecent decades has there been a strong resurgence in both theoretical and\napplied graph research in mathematics, physics, and computer science. In\napplied computing, since the late 1960s, the interlinked table structure of the\nrelational database has been the predominant information storage and retrieval\nmodel. With the growth of graph/network-based data and the need to efficiently\nprocess such data, new data management systems have been developed. In contrast\nto the index-intensive, set-theoretic operations of relational databases, graph\ndatabases make use of index-free, local traversals. This article discusses the\ngraph traversal pattern and its use in computing.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 05:12:27 GMT"}], "update_date": "2010-12-24", "authors_parsed": [["Rodriguez", "Marko A.", ""], ["Neubauer", "Peter", ""]]}, {"id": "1004.1194", "submitter": "Oren Weimann", "authors": "Danny Hermelin, Gad M. Landau, Shir Landau, Oren Weimann", "title": "Unified Compression-Based Acceleration of Edit-Distance Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance problem is a classical fundamental problem in computer\nscience in general, and in combinatorial pattern matching in particular. The\nstandard dynamic programming solution for this problem computes the\nedit-distance between a pair of strings of total length O(N) in O(N^2) time. To\nthis date, this quadratic upper-bound has never been substantially improved for\ngeneral strings. However, there are known techniques for breaking this bound in\ncase the strings are known to compress well under a particular compression\nscheme. The basic idea is to first compress the strings, and then to compute\nthe edit distance between the compressed strings. As it turns out, practically\nall known o(N^2) edit-distance algorithms work, in some sense, under the same\nparadigm described above. It is therefore natural to ask whether there is a\nsingle edit-distance algorithm that works for strings which are compressed\nunder any compression scheme. A rephrasing of this question is to ask whether a\nsingle algorithm can exploit the compressibility properties of strings under\nany compression method, even if each string is compressed using a different\ncompression. In this paper we set out to answer this question by using straight\nline programs. These provide a generic platform for representing many popular\ncompression schemes including the LZ-family, Run-Length Encoding, Byte-Pair\nEncoding, and dictionary methods. For two strings of total length N having\nstraight-line program representations of total size n, we present an algorithm\nrunning in O(nN log(N/n)) time for computing the edit-distance of these two\nstrings under any rational scoring function, and an O(n^{2/3}N^{4/3}) time\nalgorithm for arbitrary scoring functions. Our new result, while providing a\nsigni cant speed up for highly compressible strings, does not surpass the\nquadratic time bound even in the worst case scenario.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 21:24:27 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Hermelin", "Danny", ""], ["Landau", "Gad M.", ""], ["Landau", "Shir", ""], ["Weimann", "Oren", ""]]}, {"id": "1004.1208", "submitter": "Pushkar Tripathi", "authors": "Pushkar Tripathi", "title": "A Deterministic Algorithm for the Vertex Connectivity Survivable Network\n  Design Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the vertex connectivity survivable network design problem we are given an\nundirected graph G = (V,E) and connectivity requirement r(u,v) for each pair of\nvertices u,v. We are also given a cost function on the set of edges. Our goal\nis to find the minimum cost subset of edges such that for every pair (u,v) of\nvertices we have r(u,v) vertex disjoint paths in the graph induced by the\nchosen edges. Recently, Chuzhoy and Khanna presented a randomized algorithm\nthat achieves a factor of O(k^3 log n) for this problem where k is the maximum\nconnectivity requirement. In this paper we derandomize their algorithm to get a\ndeterministic O(k^3 log n) factor algorithm. Another problem of interest is the\nsingle source version of the problem, where there is a special vertex s and all\nnon-zero connectivity requirements must involve s. We also give a deterministic\nO(k^2 log n) algorithm for this problem.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 23:29:43 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Tripathi", "Pushkar", ""]]}, {"id": "1004.1220", "submitter": "Ilya Safro", "authors": "Dorit Ron and Ilya Safro and Achi Brandt", "title": "Relaxation-based coarsening and multiscale graph organization", "comments": null, "journal-ref": null, "doi": null, "report-no": "ANL/MCS-P1696-1009", "categories": "cs.DS cs.NA", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper we generalize and improve the multiscale organization of graphs\nby introducing a new measure that quantifies the \"closeness\" between two nodes.\nThe calculation of the measure is linear in the number of edges in the graph\nand involves just a small number of relaxation sweeps. A similar notion of\ndistance is then calculated and used at each coarser level. We demonstrate the\nuse of this measure in multiscale methods for several important combinatorial\noptimization problems and discuss the multiscale graph organization.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 02:05:02 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Ron", "Dorit", ""], ["Safro", "Ilya", ""], ["Brandt", "Achi", ""]]}, {"id": "1004.1253", "submitter": "Ravindran Kannan", "authors": "Ravindran Kannan", "title": "Spectral Methods for Matrices and Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Spectral Methods have long been used for Principal Component Analysis,\nthis survey focusses on work over the last 15 years with three salient\nfeatures: (i) Spectral methods are useful not only for numerical problems, but\nalso discrete optimization problems (Constraint Optimization Problems - CSP's)\nlike the max. cut problem and similar mathematical considerations underlie both\nareas. (ii) Spectral methods can be extended to tensors. The theory and\nalgorithms for tensors are not as simple/clean as for matrices, but the survey\ndescribes methods for low-rank approximation which extend to tensors. These\ntensor approximations help us solve Max-$r$-CSP's for $r>2$ as well as\nnumerical tensor problems. (iii) Sampling on the fly plays a prominent role in\nthese methods. A primary result is that for any matrix, a random submatrix of\nrows/columns picked with probabilities proportional to the squared lengths (of\nrows/columns), yields estimates of the singular values as well as an\napproximation to the whole matrix.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 06:36:48 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Kannan", "Ravindran", ""]]}, {"id": "1004.1437", "submitter": "Cristina Fernandes", "authors": "Paulo Feofiloff, Cristina G. Fernandes, Carlos E. Ferreira, and Jose\n  Coelho de Pina", "title": "A note on Johnson, Minkoff and Phillips' algorithm for the\n  Prize-Collecting Steiner Tree Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primal-dual scheme has been used to provide approximation algorithms for\nmany problems. Goemans and Williamson gave a (2-1/(n-1))-approximation for the\nPrize-Collecting Steiner Tree Problem that runs in O(n^3 log n) time. it\napplies the primal-dual scheme once for each of the n vertices of the graph.\nJohnson, Minkoff and Phillips proposed a faster implementation of Goemans and\nWilliamson's algorithm. We give a proof that the approximation ratio of this\nimplementation is exactly 2.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 22:33:45 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2010 17:23:23 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Feofiloff", "Paulo", ""], ["Fernandes", "Cristina G.", ""], ["Ferreira", "Carlos E.", ""], ["de Pina", "Jose Coelho", ""]]}, {"id": "1004.1485", "submitter": "Somnath Sikdar", "authors": "Robert Ganian, Petr Hlin\\v{e}n\\'y, Joachim Kneis, Daniel Meister, Jan\n  Obdr\\v{z}\\'alek, Peter Rossmanith and Somnath Sikdar", "title": "Are there any good digraph width measures?", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-17493-3_14", "report-no": null, "categories": "cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several different measures for digraph width have appeared in the last few\nyears. However, none of them shares all the \"nice\" properties of treewidth:\nFirst, being \\emph{algorithmically useful} i.e. admitting polynomial-time\nalgorithms for all $\\MS1$-definable problems on digraphs of bounded width. And,\nsecond, having nice \\emph{structural properties} i.e. being monotone under\ntaking subdigraphs and some form of arc contractions. As for the former,\n(undirected) $\\MS1$ seems to be the least common denominator of all reasonably\nexpressive logical languages on digraphs that can speak about the edge/arc\nrelation on the vertex set.The latter property is a necessary condition for a\nwidth measure to be characterizable by some version of the cops-and-robber game\ncharacterizing the ordinary treewidth. Our main result is that \\emph{any\nreasonable} algorithmically useful and structurally nice digraph measure cannot\nbe substantially different from the treewidth of the underlying undirected\ngraph. Moreover, we introduce \\emph{directed topological minors} and argue that\nthey are the weakest useful notion of minors for digraphs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2010 07:50:34 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Ganian", "Robert", ""], ["Hlin\u011bn\u00fd", "Petr", ""], ["Kneis", "Joachim", ""], ["Meister", "Daniel", ""], ["Obdr\u017e\u00e1lek", "Jan", ""], ["Rossmanith", "Peter", ""], ["Sikdar", "Somnath", ""]]}, {"id": "1004.1569", "submitter": "Gokarna Sharma", "authors": "Gokarna Sharma, Costas Busch, Srikanta Tirthapura", "title": "A Streaming Approximation Algorithm for Klee's Measure Problem", "comments": "This paper has been withdrawn by the author due to a small technical\n  error in Algorithm 3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient estimation of frequency moments of a data stream in one-pass\nusing limited space and time per item is one of the most fundamental problem in\ndata stream processing. An especially important estimation is to find the\nnumber of distinct elements in a data stream, which is generally referred to as\nthe zeroth frequency moment and denoted by $F_0$. In this paper, we consider\nstreams of rectangles defined over a discrete space and the task is to compute\nthe total number of distinct points covered by the rectangles. This is known as\nthe Klee's measure problem in 2 dimensions. We present and analyze a randomized\nstreaming approximation algorithm which gives an $(\\epsilon,\n\\delta)$-approximation of $F_0$ for the total area of Klee's measure problem in\n2 dimensions. Our algorithm achieves the following complexity bounds: (a) the\namortized processing time per rectangle is $O(\\frac{1}{\\epsilon^4}\\log^3\nn\\log\\frac{1}{\\delta})$; (b) the space complexity is\n$O(\\frac{1}{\\epsilon^2}\\log n \\log\\frac{1}{\\delta})$ bits; and (c) the time to\nanswer a query for $F_0$ is $O(\\log\\frac{1}{\\delta})$, respectively. To our\nknowledge, this is the first streaming approximation for the Klee's measure\nproblem that achieves sub-polynomial bounds.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2010 14:59:08 GMT"}, {"version": "v2", "created": "Thu, 28 Oct 2010 18:34:22 GMT"}], "update_date": "2010-10-29", "authors_parsed": [["Sharma", "Gokarna", ""], ["Busch", "Costas", ""], ["Tirthapura", "Srikanta", ""]]}, {"id": "1004.1666", "submitter": "Anastasios Sidiropoulos", "authors": "Anastasios Sidiropoulos", "title": "Optimal stochastic planarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown by Indyk and Sidiropoulos [IS07] that any graph of genus\ng>0 can be stochastically embedded into a distribution over planar graphs with\ndistortion 2^O(g). This bound was later improved to O(g^2) by Borradaile, Lee\nand Sidiropoulos [BLS09]. We give an embedding with distortion O(log g), which\nis asymptotically optimal. Apart from the improved distortion, another\nadvantage of our embedding is that it can be computed in polynomial time. In\ncontrast, the algorithm of [BLS09] requires solving an NP-hard problem. Our\nresult implies in particular a reduction for a large class of geometric\noptimization problems from instances on genus-g graphs, to corresponding ones\non planar graphs, with a O(log g) loss factor in the approximation guarantee.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2010 01:09:04 GMT"}, {"version": "v2", "created": "Thu, 20 May 2010 22:31:36 GMT"}], "update_date": "2010-05-24", "authors_parsed": [["Sidiropoulos", "Anastasios", ""]]}, {"id": "1004.1672", "submitter": "Yixin Cao", "authors": "Yixin Cao, Jianer Chen, and Yang Liu", "title": "On Feedback Vertex Set: New Measure and New Structures", "comments": "Final version, to appear in Algorithmica", "journal-ref": null, "doi": "10.1007/s00453-014-9904-6", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new parameterized algorithm for the {feedback vertex set}\nproblem ({\\sc fvs}) on undirected graphs. We approach the problem by\nconsidering a variation of it, the {disjoint feedback vertex set} problem ({\\sc\ndisjoint-fvs}), which finds a feedback vertex set of size $k$ that has no\noverlap with a given feedback vertex set $F$ of the graph $G$. We develop an\nimproved kernelization algorithm for {\\sc disjoint-fvs} and show that {\\sc\ndisjoint-fvs} can be solved in polynomial time when all vertices in $G\n\\setminus F$ have degrees upper bounded by three. We then propose a new\nbranch-and-search process on {\\sc disjoint-fvs}, and introduce a new\nbranch-and-search measure. The process effectively reduces a given graph to a\ngraph on which {\\sc disjoint-fvs} becomes polynomial-time solvable, and the new\nmeasure more accurately evaluates the efficiency of the process. These\nalgorithmic and combinatorial studies enable us to develop an\n$O^*(3.83^k)$-time parameterized algorithm for the general {\\sc fvs} problem,\nimproving all previous algorithms for the problem.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2010 03:05:53 GMT"}, {"version": "v2", "created": "Mon, 2 Jun 2014 21:29:33 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Cao", "Yixin", ""], ["Chen", "Jianer", ""], ["Liu", "Yang", ""]]}, {"id": "1004.1729", "submitter": "Maciej Kurant", "authors": "Maciej Kurant, Athina Markopoulou, Patrick Thiran", "title": "On the bias of BFS", "comments": "9 pages", "journal-ref": "International Teletraffic Congress (ITC 22), 2010", "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.NI cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breadth First Search (BFS) and other graph traversal techniques are widely\nused for measuring large unknown graphs, such as online social networks. It has\nbeen empirically observed that an incomplete BFS is biased toward high degree\nnodes. In contrast to more studied sampling techniques, such as random walks,\nthe precise bias of BFS has not been characterized to date. In this paper, we\nquantify the degree bias of BFS sampling. In particular, we calculate the node\ndegree distribution expected to be observed by BFS as a function of the\nfraction of covered nodes, in a random graph $RG(p_k)$ with a given degree\ndistribution $p_k$. Furthermore, we also show that, for $RG(p_k)$, all commonly\nused graph traversal techniques (BFS, DFS, Forest Fire, and Snowball Sampling)\nlead to the same bias, and we show how to correct for this bias. To give a\nbroader perspective, we compare this class of exploration techniques to random\nwalks that are well-studied and easier to analyze. Next, we study by simulation\nthe effect of graph properties not captured directly by our model. We find that\nthe bias gets amplified in graphs with strong positive assortativity. Finally,\nwe demonstrate the above results by sampling the Facebook social network, and\nwe provide some practical guidelines for graph sampling in practice.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2010 17:36:43 GMT"}], "update_date": "2011-02-23", "authors_parsed": [["Kurant", "Maciej", ""], ["Markopoulou", "Athina", ""], ["Thiran", "Patrick", ""]]}, {"id": "1004.1808", "submitter": "Michael Trofimov", "authors": "Michael I. Trofimov", "title": "Polynomial Time Algorithm for Graph Isomorphism Testing", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with new polynomial time algorithm for graph isomorphism\ntesting.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2010 13:41:39 GMT"}, {"version": "v2", "created": "Tue, 21 Sep 2010 12:59:53 GMT"}, {"version": "v3", "created": "Sun, 19 Jun 2011 22:42:00 GMT"}, {"version": "v4", "created": "Wed, 14 Sep 2011 19:04:21 GMT"}, {"version": "v5", "created": "Thu, 30 May 2013 14:10:25 GMT"}, {"version": "v6", "created": "Tue, 18 Jun 2013 13:05:21 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Trofimov", "Michael I.", ""]]}, {"id": "1004.1823", "submitter": "Amit Kumar", "authors": "Amit Kumar and Ravindran Kannan", "title": "Clustering with Spectral Norm and the k-means Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much progress on efficient algorithms for clustering data\npoints generated by a mixture of $k$ probability distributions under the\nassumption that the means of the distributions are well-separated, i.e., the\ndistance between the means of any two distributions is at least $\\Omega(k)$\nstandard deviations. These results generally make heavy use of the generative\nmodel and particular properties of the distributions. In this paper, we show\nthat a simple clustering algorithm works without assuming any generative\n(probabilistic) model. Our only assumption is what we call a \"proximity\ncondition\": the projection of any data point onto the line joining its cluster\ncenter to any other cluster center is $\\Omega(k)$ standard deviations closer to\nits own center than the other center. Here the notion of standard deviations is\nbased on the spectral norm of the matrix whose rows represent the difference\nbetween a point and the mean of the cluster to which it belongs. We show that\nin the generative models studied, our proximity condition is satisfied and so\nwe are able to derive most known results for generative models as corollaries\nof our main result. We also prove some new results for generative models -\ne.g., we can cluster all but a small fraction of points only assuming a bound\non the variance. Our algorithm relies on the well known $k$-means algorithm,\nand along the way, we prove a result of independent interest -- that the\n$k$-means algorithm converges to the \"true centers\" even in the presence of\nspurious points provided the initial (estimated) centers are close enough to\nthe corresponding actual centers and all but a small fraction of the points\nsatisfy the proximity condition. Finally, we present a new technique for\nboosting the ratio of inter-center separation to standard deviation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2010 17:36:33 GMT"}], "update_date": "2010-04-13", "authors_parsed": [["Kumar", "Amit", ""], ["Kannan", "Ravindran", ""]]}, {"id": "1004.1854", "submitter": "Elliot Anshelevich", "authors": "Elliot Anshelevich and Martin Hoefer", "title": "Contribution Games in Social Networks", "comments": "36 pages, 2 figures", "journal-ref": "Algorithmica, Volume 63, 1-2 (2012), pp. 51--90", "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider network contribution games, where each agent in a social network\nhas a budget of effort that he can contribute to different collaborative\nprojects or relationships. Depending on the contribution of the involved agents\na relationship will flourish or drown, and to measure the success we use a\nreward function for each relationship. Every agent is trying to maximize the\nreward from all relationships that it is involved in. We consider pairwise\nequilibria of this game, and characterize the existence, computational\ncomplexity, and quality of equilibrium based on the types of reward functions\ninvolved. For example, when all reward functions are concave, we prove that the\nprice of anarchy is at most 2. For convex functions the same only holds under\nsome special but very natural conditions. A special focus of the paper are\nminimum effort games, where the reward of a relationship depends only on the\nminimum effort of any of the participants. Finally, we show tight bounds for\napproximate equilibria and convergence of dynamics in these games.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2010 23:39:21 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2010 07:18:08 GMT"}, {"version": "v3", "created": "Tue, 31 Aug 2010 15:02:00 GMT"}, {"version": "v4", "created": "Wed, 20 Apr 2011 21:06:49 GMT"}], "update_date": "2012-04-27", "authors_parsed": [["Anshelevich", "Elliot", ""], ["Hoefer", "Martin", ""]]}, {"id": "1004.1956", "submitter": "Gregory Gutin", "authors": "Gregory Gutin, Leo van Iersel, Matthias Mnich, Anders Yeo", "title": "All Ternary Permutation Constraint Satisfaction Problems Parameterized\n  Above Average Have Kernels with Quadratic Numbers of Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A ternary Permutation-CSP is specified by a subset $\\Pi$ of the symmetric\ngroup $\\mathcal S_3$. An instance of such a problem consists of a set of\nvariables $V$ and a multiset of constraints, which are ordered triples of\ndistinct variables of $V.$ The objective is to find a linear ordering $\\alpha$\nof $V$ that maximizes the number of triples whose ordering (under $\\alpha$)\nfollows a permutation in $\\Pi$. We prove that all ternary Permutation-CSPs\nparameterized above average have kernels with quadratic numbers of variables.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 13:52:59 GMT"}, {"version": "v2", "created": "Mon, 3 May 2010 18:50:07 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2011 14:36:10 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Gutin", "Gregory", ""], ["van Iersel", "Leo", ""], ["Mnich", "Matthias", ""], ["Yeo", "Anders", ""]]}, {"id": "1004.1986", "submitter": "Dmitry Savostyanov V.", "authors": "S. A. Goreinov, I. V. Oseledets and D. V. Savostyanov", "title": "Wedderburn rank reduction and Krylov subspace method for tensor\n  approximation. Part 1: Tucker case", "comments": "34 pages, 3 tables, 5 figures. Submitted to SIAM J. Scientific\n  Computing", "journal-ref": "SIAM J. Sci Comp, V 34(1), pp. A1-A27, 2012", "doi": "10.1137/100792056", "report-no": null, "categories": "math.NA cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New algorithms are proposed for the Tucker approximation of a 3-tensor, that\naccess it using only the tensor-by-vector-by-vector multiplication subroutine.\nIn the matrix case, Krylov methods are methods of choice to approximate the\ndominant column and row subspaces of a sparse or structured matrix given\nthrough the matrix-by-vector multiplication subroutine. Using the Wedderburn\nrank reduction formula, we propose an algorithm of matrix approximation that\ncomputes Krylov subspaces and allows generalization to the tensor case. Several\nvariants of proposed tensor algorithms differ by pivoting strategies, overall\ncost and quality of approximation. By convincing numerical experiments we show\nthat the proposed methods are faster and more accurate than the minimal Krylov\nrecursion, proposed recently by Elden and Savas.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 15:07:43 GMT"}, {"version": "v2", "created": "Tue, 19 Oct 2010 13:14:41 GMT"}], "update_date": "2012-02-06", "authors_parsed": [["Goreinov", "S. A.", ""], ["Oseledets", "I. V.", ""], ["Savostyanov", "D. V.", ""]]}, {"id": "1004.2033", "submitter": "Vincenzo Bonifaci", "authors": "Vincenzo Bonifaci and Alberto Marchetti-Spaccamela", "title": "Feasibility Analysis of Sporadic Real-Time Multiprocessor Task Systems", "comments": null, "journal-ref": "Algorithmica, 63(4):763-780, 2012", "doi": "10.1007/s00453-011-9505-6", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first algorithm for testing the feasibility of a system of\nsporadic real-time tasks on a set of identical processors, solving one major\nopen problem in the area of multiprocessor real-time scheduling [S.K. Baruah\nand K. Pruhs, Journal of Scheduling, 2009]. We also investigate the related\nnotion of schedulability and a notion that we call online feasibility. Finally,\nwe show that discrete-time schedules are as powerful as continuous-time\nschedules, which answers another open question in the above mentioned survey.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 19:43:20 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Bonifaci", "Vincenzo", ""], ["Marchetti-Spaccamela", "Alberto", ""]]}, {"id": "1004.2091", "submitter": "Richard Brent", "authors": "Richard P. Brent and Paul Zimmermann", "title": "An O(M(n) log n) algorithm for the Jacobi symbol", "comments": "Submitted to ANTS IX (Nancy, July 2010)", "journal-ref": "Proc. ANTS-IX (Nancy, 19-23 July 2010), Lecture Notes in Computer\n  Science, Vol. 6197, Springer-Verlag, 2010, 83-95", "doi": null, "report-no": null, "categories": "cs.DS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best known algorithm to compute the Jacobi symbol of two n-bit integers\nruns in time O(M(n) log n), using Sch\\\"onhage's fast continued fraction\nalgorithm combined with an identity due to Gauss. We give a different O(M(n)\nlog n) algorithm based on the binary recursive gcd algorithm of Stehl\\'e and\nZimmermann. Our implementation - which to our knowledge is the first to run in\ntime O(M(n) log n) - is faster than GMP's quadratic implementation for inputs\nlarger than about 10000 decimal digits.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 02:15:09 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2010 01:16:52 GMT"}], "update_date": "2010-11-29", "authors_parsed": [["Brent", "Richard P.", ""], ["Zimmermann", "Paul", ""]]}, {"id": "1004.2115", "submitter": "Maxim Babenko", "authors": "Maxim A. Babenko", "title": "A Faster Algorithm for the Maximum Even Factor Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a digraph $G = (VG,AG)$, an \\emph{even factor} $M \\subseteq AG$ is a\nsubset of arcs that decomposes into a collection of node-disjoint paths and\neven cycles. Even factors in digraphs were introduced by Geleen and Cunningham\nand generalize path matchings in undirected graphs. Finding an even factor of\nmaximum cardinality in a general digraph is known to be NP-hard but for the\nclass of \\emph{odd-cycle symmetric} digraphs the problem is polynomially\nsolvable. So far, the only combinatorial algorithm known for this task is due\nto Pap; it has the running time of $O(n^4)$ (hereinafter $n$ stands for the\nnumber of nodes in $G$). In this paper we present a novel \\emph{sparse\nrecovery} technique and devise an $O(n^3 \\log n)$-time algorithm for finding a\nmaximum cardinality even factor in an odd-cycle symmetric digraph.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 06:56:31 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2010 18:29:20 GMT"}], "update_date": "2010-04-15", "authors_parsed": [["Babenko", "Maxim A.", ""]]}, {"id": "1004.2189", "submitter": "Sodeif Ahadpour Kalkhoran", "authors": "S. Ahadpour, Y. Sadra", "title": "Randomness criteria in binary visibility graph perspective", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD cs.DS physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By means of a binary visibility graph, we present a novel method to study\nrandom binary sequences. The behavior of the some topological properties of the\nbinary visibility graph, such as the degree distribution, the clustering\ncoefficient, and the mean path length have been investigated. Several examples\nare then provided to show that the numerical simulations confirm the accuracy\nof the theorems for finite random binary sequences. Finally, in this paper we\npropose, for the first time, three topological properties of the binary\nvisibility graph as a randomness criteria.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2010 15:55:59 GMT"}], "update_date": "2010-04-14", "authors_parsed": [["Ahadpour", "S.", ""], ["Sadra", "Y.", ""]]}, {"id": "1004.2291", "submitter": "Ian Post", "authors": "Ashish Goel and Ian Post", "title": "One Tree Suffices: A Simultaneous O(1)-Approximation for Single-Sink\n  Buy-at-Bulk", "comments": "16 pages. To appear in FOCS 2010. Version 2 incorporates reviewer\n  feedback and has updated references. The approximation ratio in Theorem 4.4\n  is slightly improved, and Corollary 3.3 and Theorem 4.5 are new", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the single-sink buy-at-bulk problem with an unknown cost function.\nWe wish to route flow from a set of demand nodes to a root node, where the cost\nof routing x total flow along an edge is proportional to f(x) for some concave,\nnon-decreasing function f satisfying f(0)=0. We present a simple, fast,\ncombinatorial algorithm that takes a set of demands and constructs a single\ntree T such that for all f the cost f(T) is a 47.45-approximation of the\noptimal cost for that f. This is within a factor of 2.33 of the best\napproximation ratio currently achievable when the tree can be optimized for a\nspecific function. Trees achieving simultaneous O(1)-approximations for all\nconcave functions were previously not known to exist regardless of computation\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 23:56:25 GMT"}, {"version": "v2", "created": "Fri, 27 Aug 2010 08:33:11 GMT"}], "update_date": "2010-08-30", "authors_parsed": [["Goel", "Ashish", ""], ["Post", "Ian", ""]]}, {"id": "1004.2338", "submitter": "Chun-Cheng Lin", "authors": "Chun-Cheng Lin, Hsu-Chun Yen, Sheung-Hung Poon, Jia-Hao Fan", "title": "Complexity Analysis of Balloon Drawing for Rooted Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a balloon drawing of a tree, all the children under the same parent are\nplaced on the circumference of the circle centered at their parent, and the\nradius of the circle centered at each node along any path from the root\nreflects the number of descendants associated with the node. Among various\nstyles of tree drawings reported in the literature, the balloon drawing enjoys\na desirable feature of displaying tree structures in a rather balanced fashion.\nFor each internal node in a balloon drawing, the ray from the node to each of\nits children divides the wedge accommodating the subtree rooted at the child\ninto two sub-wedges. Depending on whether the two sub-wedge angles are required\nto be identical or not, a balloon drawing can further be divided into two\ntypes: even sub-wedge and uneven sub-wedge types. In the most general case, for\nany internal node in the tree there are two dimensions of freedom that affect\nthe quality of a balloon drawing: (1) altering the order in which the children\nof the node appear in the drawing, and (2) for the subtree rooted at each child\nof the node, flipping the two sub-wedges of the subtree. In this paper, we give\na comprehensive complexity analysis for optimizing balloon drawings of rooted\ntrees with respect to angular resolution, aspect ratio and standard deviation\nof angles under various drawing cases depending on whether the tree is of even\nor uneven sub-wedge type and whether (1) and (2) above are allowed. It turns\nout that some are NP-complete while others can be solved in polynomial time. We\nalso derive approximation algorithms for those that are intractable in general.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2010 07:35:05 GMT"}], "update_date": "2010-04-15", "authors_parsed": [["Lin", "Chun-Cheng", ""], ["Yen", "Hsu-Chun", ""], ["Poon", "Sheung-Hung", ""], ["Fan", "Jia-Hao", ""]]}, {"id": "1004.2393", "submitter": "John Augustine", "authors": "John Augustine and Nick Gravin", "title": "On the Continuous CNN Problem", "comments": null, "journal-ref": "Algorithms and computation. Part II. 2010 LNCS 6507, pp. 254-265", "doi": "10.1007/978-3-642-17514-5_22", "report-no": "MR2781393", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the (discrete) CNN problem, online requests appear as points in\n$\\mathbb{R}^2$. Each request must be served before the next one is revealed. We\nhave a server that can serve a request simply by aligning either its $x$ or $y$\ncoordinate with the request. The goal of the online algorithm is to minimize\nthe total $L_1$ distance traveled by the server to serve all the requests. The\nbest known competitive ratio for the discrete version is 879 (due to Sitters\nand Stougie).\n  We study the continuous version, in which, the request can move continuously\nin $\\mathbb{R}^2$ and the server must continuously serve the request. A simple\nadversarial argument shows that the lower bound on the competitive ratio of any\nonline algorithm for the continuous CNN problem is 3. Our main contribution is\nan online algorithm with competitive ratio $3+2 \\sqrt{3} \\approx 6.464$. Our\nanalysis is tight. The continuous version generalizes the discrete orthogonal\nCNN problem, in which every request must be $x$ or $y$ aligned with the\nprevious request. Therefore, Our result improves upon the previous best\ncompetitive ratio of 9 (due to Iwama and Yonezawa).\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2010 13:34:03 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2010 08:05:15 GMT"}, {"version": "v3", "created": "Thu, 16 Sep 2010 07:53:44 GMT"}], "update_date": "2012-06-20", "authors_parsed": [["Augustine", "John", ""], ["Gravin", "Nick", ""]]}, {"id": "1004.2418", "submitter": "Alan Frieze", "authors": "Tom Bohman, Alan Frieze, Eyal Lubetzky", "title": "A note on the random greedy triangle-packing algorithm", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random greedy algorithm for constructing a large partial\nSteiner-Triple-System is defined as follows. We begin with a complete graph on\n$n$ vertices and proceed to remove the edges of triangles one at a time, where\neach triangle removed is chosen uniformly at random from the collection of all\nremaining triangles. This stochastic process terminates once it arrives at a\ntriangle-free graph. In this note we show that with high probability the number\nof edges in the final graph is at most $ O\\big( n^{7/4}\\log^{5/4}n \\big) $.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2010 15:30:38 GMT"}], "update_date": "2010-04-15", "authors_parsed": [["Bohman", "Tom", ""], ["Frieze", "Alan", ""], ["Lubetzky", "Eyal", ""]]}, {"id": "1004.2565", "submitter": "Ning Chen", "authors": "Ning Chen, Xiaotie Deng, Arpita Ghosh", "title": "Competitive Equilibria in Matching Markets with Budgets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study competitive equilibria in the classic Shapley-Shubik assignment\nmodel with indivisible goods and unit-demand buyers, with budget constraints:\nbuyers can specify a maximum price they are willing to pay for each item,\nbeyond which they cannot afford the item. This single discontinuity introduced\nby the budget constraint fundamentally changes the properties of equilibria: in\nthe assignment model without budget constraints, a competitive equilibrium\nalways exists, and corresponds exactly to a stable matching. With budgets, a\ncompetitive equilibrium need not always exist. In addition, there are now two\ndistinct notions of stability, depending on whether both or only one of the\nbuyer and seller can strictly benefit in a blocking pair, that no longer\ncoincide due to the budget-induced discontinuity. We define weak and strong\nstability for the assignment model with transferable utilities, and show that\ncompetitive equilibria correspond exactly to strongly stable matchings.\n  We consider the algorithmic question of efficiently computing competitive\nequilibria in an extension of the assignment model with budgets, where each\nbuyer specifies his preferences over items using utility functions $u_{ij}$,\nwhere $u_{ij}(p_j)$ is the utility of buyer $i$ for item $j$ when its price is\n$p_j$. Our main result is a strongly polynomial time algorithm that decides\nwhether or not a competitive equilibrium exists and if yes, computes a minimum\none, for a general class of utility functions $u_{ij}$. This class of utility\nfunctions includes the standard quasi-linear utility model with a budget\nconstraint, and in addition, allows modeling marketplaces where, for example,\nbuyers only have a preference ranking amongst items subject to a maximum\npayment limit for each item, or where buyers want to optimize return on\ninvestment (ROI) instead of a quasi-linear utility and only know items'\nrelative values.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2010 07:21:42 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2010 12:31:22 GMT"}], "update_date": "2010-04-19", "authors_parsed": [["Chen", "Ning", ""], ["Deng", "Xiaotie", ""], ["Ghosh", "Arpita", ""]]}, {"id": "1004.2899", "submitter": "Justin Thaler", "authors": "Graham Cormode, Michael Mitzenmacher, and Justin Thaler", "title": "Streaming Graph Computations with a Helpful Advisor", "comments": "17 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the trend to outsource work to commercial cloud computing\nservices, we consider a variation of the streaming paradigm where a streaming\nalgorithm can be assisted by a powerful helper that can provide annotations to\nthe data stream. We extend previous work on such {\\em annotation models} by\nconsidering a number of graph streaming problems. Without annotations,\nstreaming algorithms for graph problems generally require significant memory;\nwe show that for many standard problems, including all graph problems that can\nbe expressed with totally unimodular integer programming formulations, only a\nconstant number of hash values are needed for single-pass algorithms given\nlinear-sized annotations. We also obtain a protocol achieving \\textit{optimal}\ntradeoffs between annotation length and memory usage for matrix-vector\nmultiplication; this result contributes to a trend of recent research on\nnumerical linear algebra in streaming models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2010 18:13:48 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2010 00:44:10 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Cormode", "Graham", ""], ["Mitzenmacher", "Michael", ""], ["Thaler", "Justin", ""]]}, {"id": "1004.2968", "submitter": "Jian Li", "authors": "Jian Li and Ke Yi and Qin Zhang", "title": "Clustering with diversity", "comments": "Extended abstract accepted in ICALP 2010. Keywords: Approximation\n  algorithm, k-center, k-anonymity, l-diversity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the {\\em clustering with diversity} problem: given a set of\ncolored points in a metric space, partition them into clusters such that each\ncluster has at least $\\ell$ points, all of which have distinct colors.\n  We give a 2-approximation to this problem for any $\\ell$ when the objective\nis to minimize the maximum radius of any cluster. We show that the\napproximation ratio is optimal unless $\\mathbf{P=NP}$, by providing a matching\nlower bound. Several extensions to our algorithm have also been developed for\nhandling outliers. This problem is mainly motivated by applications in\nprivacy-preserving data publication.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2010 16:30:08 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2010 13:37:54 GMT"}], "update_date": "2010-04-22", "authors_parsed": [["Li", "Jian", ""], ["Yi", "Ke", ""], ["Zhang", "Qin", ""]]}, {"id": "1004.2972", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and Marcin Pilipczuk and Michal Pilipczuk and Jakub Onufry\n  Wojtaszczyk", "title": "Subset feedback vertex set is fixed parameter tractable", "comments": "full version of a paper presented at ICALP'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical Feedback Vertex Set problem asks, for a given undirected graph\nG and an integer k, to find a set of at most k vertices that hits all the\ncycles in the graph G. Feedback Vertex Set has attracted a large amount of\nresearch in the parameterized setting, and subsequent kernelization and\nfixed-parameter algorithms have been a rich source of ideas in the field.\n  In this paper we consider a more general and difficult version of the\nproblem, named Subset Feedback Vertex Set (SUBSET-FVS in short) where an\ninstance comes additionally with a set S ? V of vertices, and we ask for a set\nof at most k vertices that hits all simple cycles passing through S. Because of\nits applications in circuit testing and genetic linkage analysis SUBSET-FVS was\nstudied from the approximation algorithms perspective by Even et al.\n[SICOMP'00, SIDMA'00].\n  The question whether the SUBSET-FVS problem is fixed-parameter tractable was\nposed independently by Kawarabayashi and Saurabh in 2009. We answer this\nquestion affirmatively. We begin by showing that this problem is\nfixed-parameter tractable when parametrized by |S|. Next we present an\nalgorithm which reduces the given instance to 2^k n^O(1) instances with the\nsize of S bounded by O(k^3), using kernelization techniques such as the\n2-Expansion Lemma, Menger's theorem and Gallai's theorem. These two facts allow\nus to give a 2^O(k log k) n^O(1) time algorithm solving the Subset Feedback\nVertex Set problem, proving that it is indeed fixed-parameter tractable.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2010 17:19:47 GMT"}, {"version": "v2", "created": "Wed, 3 Nov 2010 12:56:53 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2011 17:57:56 GMT"}], "update_date": "2011-08-02", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Michal", ""], ["Wojtaszczyk", "Jakub Onufry", ""]]}, {"id": "1004.3051", "submitter": "Thomas Rothvoss", "authors": "Fabrizio Grandoni and Thomas Rothvoss", "title": "Prizing on Paths: A PTAS for the Highway Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the highway problem, we are given an n-edge line graph (the highway), and\na set of paths (the drivers), each one with its own budget. For a given\nassignment of edge weights (the tolls), the highway owner collects from each\ndriver the weight of the associated path, when it does not exceed the budget of\nthe driver, and zero otherwise. The goal is choosing weights so as to maximize\nthe profit.\n  A lot of research has been devoted to this apparently simple problem. The\nhighway problem was shown to be strongly NP-hard only recently\n[Elbassioni,Raman,Ray-'09]. The best-known approximation is O(\\log n/\\log\\log\nn) [Gamzu,Segev-'10], which improves on the previous-best O(\\log n)\napproximation [Balcan,Blum-'06].\n  In this paper we present a PTAS for the highway problem, hence closing the\ncomplexity status of the problem. Our result is based on a novel randomized\ndissection approach, which has some points in common with Arora's quadtree\ndissection for Euclidean network design [Arora-'98]. The basic idea is\nenclosing the highway in a bounding path, such that both the size of the\nbounding path and the position of the highway in it are random variables. Then\nwe consider a recursive O(1)-ary dissection of the bounding path, in subpaths\nof uniform optimal weight. Since the optimal weights are unknown, we construct\nthe dissection in a bottom-up fashion via dynamic programming, while computing\nthe approximate solution at the same time. Our algorithm can be easily\nderandomized. We demonstrate the versatility of our technique by presenting\nPTASs for two variants of the highway problem: the tollbooth problem with a\nconstant number of leaves and the maximum-feasibility subsystem problem on\ninterval matrices. In both cases the previous best approximation factors are\npolylogarithmic [Gamzu,Segev-'10,Elbassioni,Raman,Ray,Sitters-'09].\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2010 19:07:56 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Grandoni", "Fabrizio", ""], ["Rothvoss", "Thomas", ""]]}, {"id": "1004.3105", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Fast normal random number generators on vector processors", "comments": "An old Technical Report, not published elsewhere. 6 pages. For\n  details see http://wwwmaths.anu.edu.au/~brent/pub/pub141.html", "journal-ref": null, "doi": null, "report-no": "Technical Report TR-CS-93-04, Computer Sciences Laboratory,\n  Australian National University, March 1993.", "categories": "cs.DS math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider pseudo-random number generators suitable for vector processors.\nIn particular, we describe vectorised implementations of the Box-Muller and\nPolar methods, and show that they give good performance on the Fujitsu VP2200.\nWe also consider some other popular methods, e.g. the Ratio method of Kinderman\nand Monahan (1977) (as improved by Leva (1992)), and the method of Von Neumann\nand Forsythe, and show why they are unlikely to be competitive with the Polar\nmethod on vector processors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 06:45:43 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2010 01:03:00 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3108", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Uses of randomness in computation", "comments": "An old Technical Report, not published elsewhere. 14 pages. For\n  further details see http://wwwmaths.anu.edu.au/~brent/pub/pub147.html", "journal-ref": null, "doi": null, "report-no": "Technical Report TR-CS-94-06, Computer Sciences Laboratory,\n  Australian National University, June 1994", "categories": "cs.DS math.CO math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random number generators are widely used in practical algorithms. Examples\ninclude simulation, number theory (primality testing and integer\nfactorization), fault tolerance, routing, cryptography, optimization by\nsimulated annealing, and perfect hashing. Complexity theory usually considers\nthe worst-case behaviour of deterministic algorithms, but it can also consider\naverage-case behaviour if it is assumed that the input data is drawn randomly\nfrom a given distribution. Rabin popularised the idea of \"probabilistic\"\nalgorithms, where randomness is incorporated into the algorithm instead of\nbeing assumed in the input data. Yao showed that there is a close connection\nbetween the complexity of probabilistic algorithms and the average-case\ncomplexity of deterministic algorithms. We give examples of the uses of\nrandomness in computation, discuss the contributions of Rabin, Yao and others,\nand mention some open questions. This is the text of an invited talk presented\nat \"Theory Day\", University of NSW, Sydney, 22 April 1994.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 07:05:35 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2010 01:11:53 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3114", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "A fast vectorised implementation of Wallace's normal random number\n  generator", "comments": "An old Technical Report, not published elsewhere. 9 pages. For\n  further details see http://wwwmaths.anu.edu.au/~brent/pub/pub170.html", "journal-ref": null, "doi": null, "report-no": "Technical Report TR-CS-97-07, Computer Sciences Laboratory,\n  Australian National University, April 1997", "categories": "cs.DS math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wallace has proposed a new class of pseudo-random generators for normal\nvariates. These generators do not require a stream of uniform pseudo-random\nnumbers, except for initialisation. The inner loops are essentially\nmatrix-vector multiplications and are very suitable for implementation on\nvector processors or vector/parallel processors such as the Fujitsu VPP300. In\nthis report we outline Wallace's idea, consider some variations on it, and\ndescribe a vectorised implementation RANN4 which is more than three times\nfaster than its best competitors (the Polar and Box-Muller methods) on the\nFujitsu VP2200 and VPP300.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 07:37:44 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3115", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Some long-period random number generators using shifts and xors", "comments": "11 pages", "journal-ref": "ANZIAM Journal 48 (CTAC2006), C188-C202, 2007", "doi": null, "report-no": null, "categories": "cs.DS math.NT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marsaglia recently introduced a class of xorshift random number generators\n(RNGs) with periods 2n-1 for n = 32, 64, etc. Here we give a generalisation of\nMarsaglia's xorshift generators in order to obtain fast and high-quality RNGs\nwith extremely long periods. RNGs based on primitive trinomials may be\nunsatisfactory because a trinomial has very small weight. In contrast, our\ngenerators can be chosen so that their minimal polynomials have large weight\n(number of nonzero terms). A computer search using Magma has found good\ngenerators for n a power of two up to 4096. These have been implemented in a\nfree software package xorgens.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 07:52:08 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3169", "submitter": "Richard Brent", "authors": "Richard P. Brent, Peter L. Montgomery and Herman J. J. te Riele", "title": "Factorizations of Cunningham numbers with bases 13 to 99", "comments": "A Technical Report (December 2000) not published elsewhere, submitted\n  for archival reasons. vi + 463 pages. For further details see\n  http://wwwmaths.anu.edu.au/~brent/pub/pub200.html", "journal-ref": null, "doi": null, "report-no": "PRG TR-14-00", "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Report updates the tables of factorizations of a^n +- 1 for 13 < a <\n100, previously published as CWI Report NM-R9212 (June 1992) and updated in CWI\nReport NM-R9419 (Update 1, September 1994) and CWI Report NM-R9609 (Update 2,\nMarch 1996). A total of 951 new entries in the tables are given here. The\nfactorizations are now complete for n < 76, and there are no composite\ncofactors smaller than 10^102.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 12:04:34 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2010 01:07:29 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Brent", "Richard P.", ""], ["Montgomery", "Peter L.", ""], ["Riele", "Herman J. J. te", ""]]}, {"id": "1004.3205", "submitter": "Aaron Roth", "authors": "Aaron Roth", "title": "Differential Privacy and the Fat-Shattering Dimension of Linear Queries", "comments": "Appears in APPROX 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the task of answering linear queries under the\nconstraint of differential privacy. This is a general and well-studied class of\nqueries that captures other commonly studied classes, including predicate\nqueries and histogram queries. We show that the accuracy to which a set of\nlinear queries can be answered is closely related to its fat-shattering\ndimension, a property that characterizes the learnability of real-valued\nfunctions in the agnostic-learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 14:19:56 GMT"}, {"version": "v2", "created": "Wed, 19 Jan 2011 16:00:48 GMT"}], "update_date": "2011-01-20", "authors_parsed": [["Roth", "Aaron", ""]]}, {"id": "1004.3304", "submitter": "Graham Cormode", "authors": "Amit Chakrabarti, Graham Cormode, Ranganath Kondapally and Andrew\n  McGregor", "title": "Information Cost Tradeoffs for Augmented Index and Streaming Language\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes three main contributions to the theory of communication\ncomplexity and stream computation. First, we present new bounds on the\ninformation complexity of AUGMENTED-INDEX. In contrast to analogous results for\nINDEX by Jain, Radhakrishnan and Sen [J. ACM, 2009], we have to overcome the\nsignificant technical challenge that protocols for AUGMENTED-INDEX may violate\nthe \"rectangle property\" due to the inherent input sharing. Second, we use\nthese bounds to resolve an open problem of Magniez, Mathieu and Nayak [STOC,\n2010] that asked about the multi-pass complexity of recognizing Dyck languages.\nThis results in a natural separation between the standard multi-pass model and\nthe multi-pass model that permits reverse passes. Third, we present the first\npassive memory checkers that verify the interaction transcripts of priority\nqueues, stacks, and double-ended queues. We obtain tight upper and lower bounds\nfor these problems, thereby addressing an important sub-class of the memory\nchecking framework of Blum et al. [Algorithmica, 1994].\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 20:51:06 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Chakrabarti", "Amit", ""], ["Cormode", "Graham", ""], ["Kondapally", "Ranganath", ""], ["McGregor", "Andrew", ""]]}, {"id": "1004.3320", "submitter": "Imran Pirwani", "authors": "Matt Gibson and Imran A. Pirwani", "title": "Approximation Algorithms for Dominating Set in Disk Graphs", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a lowest cost dominating set in a given\ndisk graph containing $n$ disks. The problem has been extensively studied on\nsubclasses of disk graphs, yet the best known approximation for disk graphs has\nremained $O(\\log n)$ -- a bound that is asymptotically no better than the\ngeneral case. We improve the status quo in two ways: for the unweighted case,\nwe show how to obtain a PTAS using the framework recently proposed\n(independently)by Mustafa and Ray [SoCG 09] and by Chan and Har-Peled [SoCG\n09]; for the weighted case where each input disk has an associated rational\nweight with the objective of finding a minimum cost dominating set, we give a\nrandomized algorithm that obtains a dominating set whose weight is within a\nfactor $2^{O(\\log^* n)}$ of a minimum cost solution, with high probability --\nthe technique follows the framework proposed recently by Varadarajan [STOC 10].\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 22:50:21 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Gibson", "Matt", ""], ["Pirwani", "Imran A.", ""]]}, {"id": "1004.3363", "submitter": "Bundit Laekhanukit", "authors": "Jittat Fakcharoenphol and Bundit Laekhanukit and Danupon Nanongkai", "title": "Faster Algorithms for Semi-Matching Problems", "comments": "ICALP 2010", "journal-ref": null, "doi": null, "report-no": "TALG-2011-0044.R2", "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding \\textit{semi-matching} in bipartite graphs\nwhich is also extensively studied under various names in the scheduling\nliterature. We give faster algorithms for both weighted and unweighted case.\n  For the weighted case, we give an $O(nm\\log n)$-time algorithm, where $n$ is\nthe number of vertices and $m$ is the number of edges, by exploiting the\ngeometric structure of the problem. This improves the classical $O(n^3)$\nalgorithms by Horn [Operations Research 1973] and Bruno, Coffman and Sethi\n[Communications of the ACM 1974].\n  For the unweighted case, the bound could be improved even further. We give a\nsimple divide-and-conquer algorithm which runs in $O(\\sqrt{n}m\\log n)$ time,\nimproving two previous $O(nm)$-time algorithms by Abraham [MSc thesis,\nUniversity of Glasgow 2003] and Harvey, Ladner, Lov\\'asz and Tamir [WADS 2003\nand Journal of Algorithms 2006]. We also extend this algorithm to solve the\n\\textit{Balance Edge Cover} problem in $O(\\sqrt{n}m\\log n)$ time, improving the\nprevious $O(nm)$-time algorithm by Harada, Ono, Sadakane and Yamashita [ISAAC\n2008].\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 07:06:38 GMT"}, {"version": "v2", "created": "Tue, 21 Sep 2010 10:53:10 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2011 20:23:30 GMT"}, {"version": "v4", "created": "Tue, 29 May 2012 21:56:13 GMT"}, {"version": "v5", "created": "Wed, 13 Jun 2012 00:15:23 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Fakcharoenphol", "Jittat", ""], ["Laekhanukit", "Bundit", ""], ["Nanongkai", "Danupon", ""]]}, {"id": "1004.3366", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Some integer factorization algorithms using elliptic curves", "comments": "Corrected version of a paper that appeared in Australian Computer\n  Science Communications 8 (1986), with postscript added 1998. For further\n  details see http://wwwmaths.anu.edu.au/~brent/pub/pub102.html", "journal-ref": "Australian Computer Science Communications 8 (1986), 149-163", "doi": null, "report-no": null, "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lenstra's integer factorization algorithm is asymptotically one of the\nfastest known algorithms, and is ideally suited for parallel computation. We\nsuggest a way in which the algorithm can be speeded up by the addition of a\nsecond phase. Under some plausible assumptions, the speedup is of order log(p),\nwhere p is the factor which is found. In practice the speedup is significant.\nWe mention some refinements which give greater speedup, an alternative way of\nimplementing a second phase, and the connection with Pollard's \"p-1\"\nfactorization algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 07:18:03 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3392", "submitter": "Siamak Tazari", "authors": "Siamak Tazari", "title": "Faster Approximation Schemes and Parameterized Algorithms on\n  H-Minor-Free and Odd-Minor-Free Graphs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-15155-2_56", "report-no": null, "categories": "cs.DS cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the running time of the general algorithmic technique known as\nBaker's approach (1994) on H-minor-free graphs from O(n^{f(|H|)}) to O(f(|H|)\nn^{O(1)}). The numerous applications include e.g. a 2-approximation for\ncoloring and PTASes for various problems such as dominating set and max-cut,\nwhere we obtain similar improvements.\n  On classes of odd-minor-free graphs, which have gained significant attention\nin recent time, we obtain a similar acceleration for a variant of the\nstructural decomposition theorem proved by Demaine et al. (2010). We use these\nalgorithms to derive faster 2-approximations; furthermore, we present the first\nPTASes and subexponential FPT-algorithms for independent set and vertex cover\non these graph classes using a novel dynamic programming technique.\n  We also introduce a technique to derive (nearly) subexponential parameterized\nalgorithms on H-minor-free graphs. Our technique applies, in particular, to\nproblems such as Steiner tree, (directed) subgraph with a property, (directed)\nlongest path, and (connected/independent) dominating set, on some or all proper\nminor-closed graph classes. We obtain as a corollary that all problems with a\nminor-monotone subexponential kernel and amenable to our technique can be\nsolved in subexponential FPT-time on H-minor free graphs. This results in a\ngeneral methodology for subexponential parameterized algorithms outside the\nframework of bidimensionality.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 09:54:55 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Tazari", "Siamak", ""]]}, {"id": "1004.3412", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Multiple-precision zero-finding methods and the complexity of elementary\n  function evaluation", "comments": "An old (1975) paper with a postscript describing more recent\n  developments. See also http://wwwmaths.anu.edu.au/~brent/pub/pub028.html", "journal-ref": "Analytic Computational Complexity (edited by J. F. Traub),\n  Academic Press, New York, 1975, 151-176", "doi": null, "report-no": "Interim Report ADA014059, Department of Computer Science,\n  Carnegie-Mellon University (July 1975), ii+26 pages", "categories": "cs.NA cs.CC cs.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider methods for finding high-precision approximations to simple zeros\nof smooth functions. As an application, we give fast methods for evaluating the\nelementary functions log(x), exp(x), sin(x) etc. to high precision. For\nexample, if x is a positive floating-point number with an n-bit fraction, then\n(under rather weak assumptions) an n-bit approximation to log(x) or exp(x) may\nbe computed in time asymptotically equal to 13M(n)lg(n), where M(n) is the time\nrequired to multiply floating-point numbers with n-bit fractions. Similar\nresults are given for the other elementary functions. Some analogies with\noperations on formal power series (over a field of characteristic zero) are\ndiscussed. In particular, it is possible to compute the first n terms in log(1\n+ a_1.x + ...) or exp(a_1.x + ...) in time O(M(n)), where M(n) is the time\nrequired to multiply two polynomials of degree n - 1. It follows that the first\nn terms in a q-th power (1 + a_1.x + ...)^q can be computed in time O(M(n)),\nindependent of q. One of the results of this paper is the \"Gauss-Legendre\" or\n\"Brent-Salamin\" algorithm for computing pi. This is the first quadratically\nconvergent algorithm for pi. It was also published in Brent [J. ACM 23 (1976),\n242-251], and independently by Salamin [Math. Comp. 30 (1976), 565-570].\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 11:27:07 GMT"}, {"version": "v2", "created": "Sun, 30 May 2010 02:59:53 GMT"}], "update_date": "2010-06-01", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.3452", "submitter": "Mugurel Ionut Andreica", "authors": "Mugurel Ionut Andreica, Vasile Deac, Stelian Tipa", "title": "Towards Providing Low-Risk and Economically Feasible Network Data\n  Transfer Services", "comments": "Proceedings of the 9th WSEAS International Conference on Multimedia,\n  Internet & Video Technologies (MIV), Budapest, Hungary, 3-5 September, 2009.\n  (ISBN: 978-960-474-114-4 / ISSN: 1790-5109)", "journal-ref": "Recent Advances in Signals & Systems, pp. 204-209, 2009. (ISBN:\n  978-960-474-114-4 / ISSN: 1790-5109)", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first part of this paper we present the first steps towards providing\nlow-risk and economically feasible network data transfer services. We introduce\nthree types of data transfer services and present general guidelines and\nalgorithms for managing service prices, risks and schedules. In the second part\nof the paper we solve two packet scheduling cost optimization problems and\npresent efficient algorithms for identifying maximum weight (k-level-)\ncaterpillar subtrees in tree networks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 13:58:06 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Andreica", "Mugurel Ionut", ""], ["Deac", "Vasile", ""], ["Tipa", "Stelian", ""]]}, {"id": "1004.3539", "submitter": "Jure Leskovec", "authors": "Jure Leskovec, Kevin J. Lang, Michael W. Mahoney", "title": "Empirical Comparison of Algorithms for Network Community Detection", "comments": null, "journal-ref": "WWW 2010: ACM WWW International Conference on World Wide Web, 2010", "doi": null, "report-no": null, "categories": "cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting clusters or communities in large real-world graphs such as large\nsocial or information networks is a problem of considerable interest. In\npractice, one typically chooses an objective function that captures the\nintuition of a network cluster as set of nodes with better internal\nconnectivity than external connectivity, and then one applies approximation\nalgorithms or heuristics to extract sets of nodes that are related to the\nobjective function and that \"look like\" good communities for the application of\ninterest. In this paper, we explore a range of network community detection\nmethods in order to compare them and to understand their relative performance\nand the systematic biases in the clusters they identify. We evaluate several\ncommon objective functions that are used to formalize the notion of a network\ncommunity, and we examine several different classes of approximation algorithms\nthat aim to optimize such objective functions. In addition, rather than simply\nfixing an objective and asking for an approximation to the best cluster of any\nsize, we consider a size-resolved version of the optimization problem.\nConsidering community quality as a function of its size provides a much finer\nlens with which to examine community detection algorithms, since objective\nfunctions and approximation algorithms often have non-obvious size-dependent\nbehavior.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 19:59:49 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Leskovec", "Jure", ""], ["Lang", "Kevin J.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1004.3630", "submitter": "Aleksandrs Slivkins", "authors": "Moshe Babaioff, Robert D. Kleinberg and Aleksandrs Slivkins", "title": "Truthful Mechanisms with Implicit Payment Computation", "comments": "This is a full version of the conference paper from ACM EC 2010,\n  merged with a multi-parameter extension (Section 8) from the follow-up paper\n  in ACM EC 2013 by the same authors. Apart from the revised presentation, this\n  version is updated to reflect the follow-up work and the current status of\n  open questions. The current version (v5) contains several minor bug fixes in\n  the proof of Lemma 7.10. J. of the ACM (JACM), Volume 62, Issue 2, May 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely believed that computing payments needed to induce truthful\nbidding is somehow harder than simply computing the allocation. We show that\nthe opposite is true: creating a randomized truthful mechanism is essentially\nas easy as a single call to a monotone allocation rule. Our main result is a\ngeneral procedure to take a monotone allocation rule for a single-parameter\ndomain and transform it (via a black-box reduction) into a randomized mechanism\nthat is truthful in expectation and individually rational for every\nrealization. The mechanism implements the same outcome as the original\nallocation rule with probability arbitrarily close to 1, and requires\nevaluating that allocation rule only once. We also provide an extension of this\nresult to multi-parameter domains and cycle-monotone allocation rules, under\nmild star-convexity and non-negativity hypotheses on the type space and\nallocation rule, respectively.\n  Because our reduction is simple, versatile, and general, it has many\napplications to mechanism design problems in which re-evaluating the allocation\nrule is either burdensome or informationally impossible. Applying our result to\nthe multi-armed bandit problem, we obtain truthful randomized mechanisms whose\nregret matches the information-theoretic lower bound up to logarithmic factors,\neven though prior work showed this is impossible for truthful deterministic\nmechanisms. We also present applications to offline mechanism design, showing\nthat randomization can circumvent a communication complexity lower bound for\ndeterministic payments computation, and that it can also be used to create\ntruthful shortest path auctions that approximate the welfare of the VCG\nallocation arbitrarily well, while having the same running time complexity as\nDijkstra's algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 06:32:23 GMT"}, {"version": "v2", "created": "Tue, 15 May 2012 23:04:01 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2012 21:43:15 GMT"}, {"version": "v4", "created": "Sun, 6 Jul 2014 18:28:58 GMT"}, {"version": "v5", "created": "Sun, 15 Nov 2015 19:42:54 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Babaioff", "Moshe", ""], ["Kleinberg", "Robert D.", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1004.3668", "submitter": "Viet Hung Nguyen", "authors": "Viet Hung Nguyen", "title": "Approximating the minimum directed tree cover", "comments": "13 pages", "journal-ref": null, "doi": "10.1007/978-3-642-17461-2_12", "report-no": null, "categories": "cs.DS cs.DM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed graph $G$ with non negative cost on the arcs, a directed\ntree cover of $G$ is a rooted directed tree such that either head or tail (or\nboth of them) of every arc in $G$ is touched by $T$. The minimum directed tree\ncover problem (DTCP) is to find a directed tree cover of minimum cost. The\nproblem is known to be $NP$-hard. In this paper, we show that the weighted Set\nCover Problem (SCP) is a special case of DTCP. Hence, one can expect at best to\napproximate DTCP with the same ratio as for SCP. We show that this expectation\ncan be satisfied in some way by designing a purely combinatorial approximation\nalgorithm for the DTCP and proving that the approximation ratio of the\nalgorithm is $\\max\\{2, \\ln(D^+)\\}$ with $D^+$ is the maximum outgoing degree of\nthe nodes in $G$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 10:13:49 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2010 10:41:02 GMT"}, {"version": "v3", "created": "Thu, 2 Sep 2010 11:25:57 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Nguyen", "Viet Hung", ""]]}, {"id": "1004.3702", "submitter": "Lizhi Du", "authors": "Lizhi Du", "title": "A Polynomial time Algorithm for Hamilton Cycle with maximum Degree 3,\n  3SAT", "comments": "12 pages. This time, I add a detailed polynomial time algorithm and\n  proof for 3SAT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the famous Rotation-Extension technique, by creating the new\nconcepts and methods: broad cycle, main segment, useful cut and insert,\ndestroying edges for a main segment, main goal Hamilton cycle, depth-first\nsearch tree, we develop a polynomial time algorithm for a famous NPC: the\nHamilton cycle problem. Thus we proved that NP=P. The key points of this paper\nare: 1) there are two ways to get a Hamilton cycle in exponential time: a full\npermutation of n vertices; or, chose n edges from all k edges, and check all\npossible combinations. The main problem is: how to avoid checking all\ncombinations of n edges from all edges. My algorithm can avoid this. Lemma 1\nand lemma 2 are very important. They are the foundation that we always can get\na good branch in the depth-first search tree and can get a series of destroying\nedges (all are bad edges) for this good branch in polynomial time. The\nextraordinary insights are: destroying edges, a tree contains each main segment\nat most one time at the same time, and dynamic combinations. The difficult part\nis to understand how to construct a main segment's series of destroying edges\nby dynamic combinations. The proof logic is: if there is at least on Hamilton\ncycle in the graph, we always can do useful cut and inserts until a Hamilton\ncycle is got. The times of useful cut and inserts are polynomial. So if at any\nstep we cannot have a useful cut and insert, this means that there are no\nHamilton cycles in the graph. In this version, I add a detailed polynomial time\nalgorithm and proof for 3SAT\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 04:39:27 GMT"}, {"version": "v10", "created": "Mon, 5 Nov 2012 01:44:46 GMT"}, {"version": "v11", "created": "Thu, 31 Jan 2013 11:15:53 GMT"}, {"version": "v12", "created": "Mon, 4 Nov 2013 14:09:42 GMT"}, {"version": "v13", "created": "Thu, 7 Aug 2014 02:01:59 GMT"}, {"version": "v14", "created": "Wed, 20 Aug 2014 01:20:04 GMT"}, {"version": "v15", "created": "Tue, 2 Sep 2014 15:17:04 GMT"}, {"version": "v16", "created": "Thu, 18 Sep 2014 12:10:37 GMT"}, {"version": "v17", "created": "Mon, 13 Oct 2014 03:52:56 GMT"}, {"version": "v18", "created": "Mon, 17 Nov 2014 10:54:13 GMT"}, {"version": "v19", "created": "Mon, 1 Dec 2014 11:05:59 GMT"}, {"version": "v2", "created": "Tue, 23 Nov 2010 22:00:44 GMT"}, {"version": "v20", "created": "Mon, 22 Dec 2014 04:48:29 GMT"}, {"version": "v21", "created": "Mon, 11 May 2015 12:11:22 GMT"}, {"version": "v22", "created": "Thu, 24 Sep 2015 13:53:17 GMT"}, {"version": "v23", "created": "Mon, 30 Nov 2015 04:27:21 GMT"}, {"version": "v24", "created": "Mon, 21 Mar 2016 14:41:54 GMT"}, {"version": "v25", "created": "Mon, 13 Jun 2016 06:25:23 GMT"}, {"version": "v26", "created": "Fri, 5 Aug 2016 18:15:23 GMT"}, {"version": "v27", "created": "Mon, 29 Aug 2016 02:17:42 GMT"}, {"version": "v28", "created": "Thu, 10 Nov 2016 05:51:30 GMT"}, {"version": "v29", "created": "Mon, 23 Jan 2017 14:35:27 GMT"}, {"version": "v3", "created": "Mon, 10 Jan 2011 02:50:16 GMT"}, {"version": "v30", "created": "Tue, 28 Feb 2017 13:16:20 GMT"}, {"version": "v31", "created": "Wed, 22 Mar 2017 12:20:11 GMT"}, {"version": "v32", "created": "Tue, 11 Apr 2017 08:21:53 GMT"}, {"version": "v33", "created": "Mon, 19 Jun 2017 11:57:26 GMT"}, {"version": "v34", "created": "Wed, 17 Jan 2018 12:32:05 GMT"}, {"version": "v35", "created": "Tue, 13 Feb 2018 04:04:57 GMT"}, {"version": "v36", "created": "Mon, 12 Mar 2018 11:17:48 GMT"}, {"version": "v37", "created": "Mon, 11 Jun 2018 01:14:42 GMT"}, {"version": "v38", "created": "Wed, 11 Jul 2018 10:27:51 GMT"}, {"version": "v39", "created": "Mon, 30 Jul 2018 22:02:52 GMT"}, {"version": "v4", "created": "Sun, 1 May 2011 01:32:02 GMT"}, {"version": "v40", "created": "Tue, 21 Aug 2018 00:01:46 GMT"}, {"version": "v41", "created": "Sun, 2 Sep 2018 23:24:26 GMT"}, {"version": "v42", "created": "Tue, 18 Sep 2018 07:54:59 GMT"}, {"version": "v43", "created": "Wed, 24 Oct 2018 01:58:41 GMT"}, {"version": "v44", "created": "Thu, 7 Feb 2019 04:25:15 GMT"}, {"version": "v45", "created": "Thu, 21 Mar 2019 11:10:18 GMT"}, {"version": "v46", "created": "Thu, 2 May 2019 01:32:57 GMT"}, {"version": "v47", "created": "Mon, 24 Jun 2019 00:56:03 GMT"}, {"version": "v48", "created": "Thu, 10 Oct 2019 07:09:08 GMT"}, {"version": "v49", "created": "Sun, 17 Nov 2019 01:38:36 GMT"}, {"version": "v5", "created": "Fri, 7 Oct 2011 01:39:26 GMT"}, {"version": "v50", "created": "Thu, 23 Jan 2020 05:49:11 GMT"}, {"version": "v51", "created": "Mon, 27 Apr 2020 00:24:06 GMT"}, {"version": "v52", "created": "Sun, 7 Jun 2020 21:56:02 GMT"}, {"version": "v53", "created": "Mon, 6 Jul 2020 10:07:25 GMT"}, {"version": "v54", "created": "Sun, 2 Aug 2020 22:55:43 GMT"}, {"version": "v55", "created": "Wed, 2 Sep 2020 01:02:09 GMT"}, {"version": "v56", "created": "Thu, 8 Oct 2020 01:05:54 GMT"}, {"version": "v57", "created": "Tue, 10 Nov 2020 14:01:31 GMT"}, {"version": "v58", "created": "Thu, 3 Dec 2020 06:27:25 GMT"}, {"version": "v59", "created": "Wed, 20 Jan 2021 11:52:23 GMT"}, {"version": "v6", "created": "Fri, 6 Apr 2012 11:16:37 GMT"}, {"version": "v60", "created": "Tue, 2 Feb 2021 01:58:47 GMT"}, {"version": "v61", "created": "Thu, 8 Apr 2021 07:36:54 GMT"}, {"version": "v62", "created": "Mon, 10 May 2021 00:01:29 GMT"}, {"version": "v7", "created": "Sun, 27 May 2012 08:15:49 GMT"}, {"version": "v8", "created": "Wed, 15 Aug 2012 12:11:34 GMT"}, {"version": "v9", "created": "Wed, 29 Aug 2012 06:39:31 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Du", "Lizhi", ""]]}, {"id": "1004.3716", "submitter": "Richard Brent", "authors": "Richard P. Brent, Franklin T. Luk and H. T. Kung", "title": "Some linear-time algorithms for systolic arrays", "comments": "Corrected version of an old (1983) paper. 23 pages. For further\n  details, see http://wwwmaths.anu.edu.au/~brent/pub/pub079.html", "journal-ref": "Information Processing 83 (edited by R.E.A. Mason), North-Holland,\n  Amsterdam, 1983, 865-876", "doi": null, "report-no": "Report TR-CS-82-15, DCS, Australian National University, December\n  1982", "categories": "cs.DS cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey some results on linear-time algorithms for systolic arrays. In\nparticular, we show how the greatest common divisor (GCD) of two polynomials of\ndegree n over a finite field can be computed in time O(n) on a linear systolic\narray of O(n) cells; similarly for the GCD of two n-bit binary numbers. We show\nhow n * n Toeplitz systems of linear equations can be solved in time O(n) on a\nlinear array of O(n) cells, each of which has constant memory size (independent\nof n). Finally, we outline how a two-dimensional square array of O(n)* O(n)\ncells can be used to solve (to working accuracy) the eigenvalue problem for a\nsymmetric real n* n matrix in time O(nS(n)). Here S(n) is a slowly growing\nfunction of n; for practical purposes S(n) can be regarded as a constant. In\naddition to their theoretical interest, these results have potential\napplications in the areas of error-correcting codes, symbolic and algebraic\ncomputations, signal processing and image processing.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 14:28:08 GMT"}], "update_date": "2010-04-22", "authors_parsed": [["Brent", "Richard P.", ""], ["Luk", "Franklin T.", ""], ["Kung", "H. T.", ""]]}, {"id": "1004.3782", "submitter": "Ping Li", "authors": "Ping Li", "title": "On Practical Algorithms for Entropy Estimation and the Improved Sample\n  Complexity of Compressed Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the p-th frequency moment of data stream is a very heavily studied\nproblem. The problem is actually trivial when p = 1, assuming the strict\nTurnstile model. The sample complexity of our proposed algorithm is essentially\nO(1) near p=1. This is a very large improvement over the previously believed\nO(1/eps^2) bound. The proposed algorithm makes the long-standing problem of\nentropy estimation an easy task, as verified by the experiments included in the\nappendix.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 19:55:55 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1004.4024", "submitter": "Vitaly Osipov", "authors": "Vitaly Osipov and Peter Sanders", "title": "n-Level Graph Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-level graph partitioning algorithm based on the extreme\nidea to contract only a single edge on each level of the hierarchy. This\nobviates the need for a matching algorithm and promises very good partitioning\nquality since there are very few changes between two levels. Using an efficient\ndata structure and new flexible ways to break local search improvements early,\nwe obtain an algorithm that scales to large inputs and produces the best known\npartitioning results for many inputs. For example, in Walshaw's well known\nbenchmark tables we achieve 155 improvements dominating the entries for large\ngraphs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 22:59:53 GMT"}], "update_date": "2010-04-26", "authors_parsed": [["Osipov", "Vitaly", ""], ["Sanders", "Peter", ""]]}, {"id": "1004.4057", "submitter": "Amit Deshpande", "authors": "Amit Deshpande and Luis Rademacher", "title": "Efficient volume sampling for row/column subset selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give efficient algorithms for volume sampling, i.e., for picking\n$k$-subsets of the rows of any given matrix with probabilities proportional to\nthe squared volumes of the simplices defined by them and the origin (or the\nsquared volumes of the parallelepipeds defined by these subsets of rows). This\nsolves an open problem from the monograph on spectral algorithms by Kannan and\nVempala. Our first algorithm for volume sampling $k$-subsets of rows from an\n$m$-by-$n$ matrix runs in $O(kmn^{\\omega} \\log n)$ arithmetic operations and a\nsecond variant of it for $(1+\\epsilon)$-approximate volume sampling runs in\n$O(mn \\log m \\cdot k^{2}/\\epsilon^{2} + m \\log^{\\omega} m \\cdot\nk^{2\\omega+1}/\\epsilon^{2\\omega} \\cdot \\log(k \\epsilon^{-1} \\log m))$\narithmetic operations, which is almost linear in the size of the input (i.e.,\nthe number of entries) for small $k$. Our efficient volume sampling algorithms\nimply several interesting results for low-rank matrix approximation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 06:53:25 GMT"}], "update_date": "2010-04-26", "authors_parsed": [["Deshpande", "Amit", ""], ["Rademacher", "Luis", ""]]}, {"id": "1004.4080", "submitter": "Debmalya Panigrahi", "authors": "Ramesh Hariharan and Debmalya Panigrahi", "title": "A General Framework for Graph Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a weighted graph $G$ and an error parameter $\\epsilon > 0$, the {\\em\ngraph sparsification} problem requires sampling edges in $G$ and giving the\nsampled edges appropriate weights to obtain a sparse graph $G_{\\epsilon}$\n(containing O(n\\log n) edges in expectation) with the following property: the\nweight of every cut in $G_{\\epsilon}$ is within a factor of $(1\\pm \\epsilon)$\nof the weight of the corresponding cut in $G$. We provide a generic framework\nthat sets out sufficient conditions for any particular sampling scheme to\nresult in good sparsifiers, and obtain a set of results by simple\ninstantiations of this framework. The results we obtain include the following:\n(1) We improve the time complexity of graph sparsification from O(m\\log^3 n) to\nO(m + n\\log^4 n) for graphs with polynomial edge weights. (2) We improve the\ntime complexity of graph sparsification from O(m\\log^3 n) to O(m\\log^2 n) for\ngraphs with arbitrary edge weights. (3) If the size of the sparsifier is\nallowed to be O(n\\log^2 n/\\epsilon^2) instead of O(n\\log n/\\epsilon^2), we\nimprove the time complexity of sparsification to O(m) for graphs with\npolynomial edge weights. (4) We show that sampling using standard\nconnectivities results in good sparsifiers, thus resolving an open question of\nBenczur and Karger. As a corollary, we give a simple proof of (a slightly\nweaker version of) a result due to Spielman and Srivastava showing that\nsampling using effective resistances produces good sparsifiers. (5) We give a\nsimple proof showing that sampling using strong connectivities results in good\nsparsifiers, a result obtained previously using a more involved proof by\nBenczur and Karger. A key ingredient of our proofs is a generalization of\nbounds on the number of small cuts in an undirected graph due to Karger; this\ngeneralization might be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 09:36:13 GMT"}], "update_date": "2010-04-26", "authors_parsed": [["Hariharan", "Ramesh", ""], ["Panigrahi", "Debmalya", ""]]}, {"id": "1004.4216", "submitter": "Alan Sexton", "authors": "Alan P. Sexton and Richard Swinbank", "title": "Symmetric M-tree", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": "CSR-04-2", "categories": "cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The M-tree is a paged, dynamically balanced metric access method that\nresponds gracefully to the insertion of new objects. To date, no algorithm has\nbeen published for the corresponding Delete operation. We believe this to be\nnon-trivial because of the design of the M-tree's Insert algorithm. We propose\na modification to Insert that overcomes this problem and give the corresponding\nDelete algorithm. The performance of the tree is comparable to the M-tree and\noffers additional benefits in terms of supported operations, which we briefly\ndiscuss.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 20:04:57 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Sexton", "Alan P.", ""], ["Swinbank", "Richard", ""]]}, {"id": "1004.4223", "submitter": "Gregory Valiant", "authors": "Ankur Moitra and Gregory Valiant", "title": "Settling the Polynomial Learnability of Mixtures of Gaussians", "comments": "43 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data drawn from a mixture of multivariate Gaussians, a basic problem is\nto accurately estimate the mixture parameters. We give an algorithm for this\nproblem that has a running time, and data requirement polynomial in the\ndimension and the inverse of the desired accuracy, with provably minimal\nassumptions on the Gaussians. As simple consequences of our learning algorithm,\nwe can perform near-optimal clustering of the sample points and density\nestimation for mixtures of k Gaussians, efficiently. The building blocks of our\nalgorithm are based on the work Kalai et al. [STOC 2010] that gives an\nefficient algorithm for learning mixtures of two Gaussians by considering a\nseries of projections down to one dimension, and applying the method of moments\nto each univariate projection. A major technical hurdle in Kalai et al. is\nshowing that one can efficiently learn univariate mixtures of two Gaussians. In\ncontrast, because pathological scenarios can arise when considering univariate\nprojections of mixtures of more than two Gaussians, the bulk of the work in\nthis paper concerns how to leverage an algorithm for learning univariate\nmixtures (of many Gaussians) to yield an efficient algorithm for learning in\nhigh dimensions. Our algorithm employs hierarchical clustering and rescaling,\ntogether with delicate methods for backtracking and recovering from failures\nthat can occur in our univariate algorithm. Finally, while the running time and\ndata requirements of our algorithm depend exponentially on the number of\nGaussians in the mixture, we prove that such a dependence is necessary.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 20:46:26 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Moitra", "Ankur", ""], ["Valiant", "Gregory", ""]]}, {"id": "1004.4240", "submitter": "Anirban Dasgupta", "authors": "Anirban Dasgupta and Ravi Kumar and Tam\\'as Sarl\\'os", "title": "A Sparse Johnson--Lindenstrauss Transform", "comments": "10 pages, conference version.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is a key algorithmic tool with many applications\nincluding nearest-neighbor search, compressed sensing and linear algebra in the\nstreaming model. In this work we obtain a {\\em sparse} version of the\nfundamental tool in dimension reduction --- the Johnson--Lindenstrauss\ntransform. Using hashing and local densification, we construct a sparse\nprojection matrix with just $\\tilde{O}(\\frac{1}{\\epsilon})$ non-zero entries\nper column. We also show a matching lower bound on the sparsity for a large\nclass of projection matrices. Our bounds are somewhat surprising, given the\nknown lower bounds of $\\Omega(\\frac{1}{\\epsilon^2})$ both on the number of rows\nof any projection matrix and on the sparsity of projection matrices generated\nby natural constructions.\n  Using this, we achieve an $\\tilde{O}(\\frac{1}{\\epsilon})$ update time per\nnon-zero element for a $(1\\pm\\epsilon)$-approximate projection, thereby\nsubstantially outperforming the $\\tilde{O}(\\frac{1}{\\epsilon^2})$ update time\nrequired by prior approaches. A variant of our method offers the same\nguarantees for sparse vectors, yet its $\\tilde{O}(d)$ worst case running time\nmatches the best approach of Ailon and Liberty.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 23:57:17 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Dasgupta", "Anirban", ""], ["Kumar", "Ravi", ""], ["Sarl\u00f3s", "Tam\u00e1s", ""]]}, {"id": "1004.4329", "submitter": "Joseph Shtok", "authors": "Joseph Shtok and Michael Elad", "title": "Analysis of Basis Pursuit Via Capacity Sets", "comments": null, "journal-ref": "Journal of Fourier Analysis and Applications, Volume 14, Numbers\n  5-6, December 2008, pp. 688-711", "doi": "10.1007/s00041-008-9036-y", "report-no": null, "categories": "cs.NA cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the sparsest solution $\\alpha$ for an under-determined linear system\nof equations $D\\alpha=s$ is of interest in many applications. This problem is\nknown to be NP-hard. Recent work studied conditions on the support size of\n$\\alpha$ that allow its recovery using L1-minimization, via the Basis Pursuit\nalgorithm. These conditions are often relying on a scalar property of $D$\ncalled the mutual-coherence. In this work we introduce an alternative set of\nfeatures of an arbitrarily given $D$, called the \"capacity sets\". We show how\nthose could be used to analyze the performance of the basis pursuit, leading to\nimproved bounds and predictions of performance. Both theoretical and numerical\nmethods are presented, all using the capacity values, and shown to lead to\nimproved assessments of the basis pursuit success in finding the sparest\nsolution of $D\\alpha=s$.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2010 06:28:58 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Shtok", "Joseph", ""], ["Elad", "Michael", ""]]}, {"id": "1004.4371", "submitter": "James Lee", "authors": "Jian Ding and James R. Lee and Yuval Peres", "title": "Cover times, blanket times, and majorizing measures", "comments": "Revisions to Section 3; added and rearranged some material on the\n  majorizing measures theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit a strong connection between cover times of graphs, Gaussian\nprocesses, and Talagrand's theory of majorizing measures. In particular, we\nshow that the cover time of any graph $G$ is equivalent, up to universal\nconstants, to the square of the expected maximum of the Gaussian free field on\n$G$, scaled by the number of edges in $G$. This allows us to resolve a number\nof open questions. We give a deterministic polynomial-time algorithm that\ncomputes the cover time to within an O(1) factor for any graph, answering a\nquestion of Aldous and Fill (1994). We also positively resolve the blanket time\nconjectures of Winkler and Zuckerman (1996), showing that for any graph, the\nblanket and cover times are within an O(1) factor. The best previous\napproximation factor for both these problems was $O((\\log \\log n)^2)$ for\n$n$-vertex graphs, due to Kahn, Kim, Lovasz, and Vu (2000).\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2010 18:19:47 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2010 07:23:10 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2010 22:16:52 GMT"}, {"version": "v4", "created": "Thu, 28 Apr 2011 22:43:42 GMT"}, {"version": "v5", "created": "Fri, 7 Oct 2011 00:48:24 GMT"}], "update_date": "2011-10-10", "authors_parsed": [["Ding", "Jian", ""], ["Lee", "James R.", ""], ["Peres", "Yuval", ""]]}, {"id": "1004.4420", "submitter": "Gerasimos Pollatos", "authors": "Eric Angel, Evripidis Bampis, Gerasimos G. Pollatos, and Vassilis\n  Zissimopoulos", "title": "Optimal Data Placement on Networks With Constant Number of Clients", "comments": null, "journal-ref": null, "doi": "10.1016/j.tcs.2013.03.025", "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce optimal algorithms for the problems of data placement (DP) and\npage placement (PP) in networks with a constant number of clients each of which\nhas limited storage availability and issues requests for data objects. The\nobjective for both problems is to efficiently utilize each client's storage\n(deciding where to place replicas of objects) so that the total incurred access\nand installation cost over all clients is minimized. In the PP problem an extra\nconstraint on the maximum number of clients served by a single client must be\nsatisfied. Our algorithms solve both problems optimally when all objects have\nuniform lengths. When objects lengths are non-uniform we also find the optimal\nsolution, albeit a small, asymptotically tight violation of each client's\nstorage size by $\\epsilon$lmax where lmax is the maximum length of the objects\nand $\\epsilon$ some arbitrarily small positive constant. We make no assumption\non the underlying topology of the network (metric, ultrametric etc.), thus\nobtaining the first non-trivial results for non-metric data placement problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 07:41:35 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Angel", "Eric", ""], ["Bampis", "Evripidis", ""], ["Pollatos", "Gerasimos G.", ""], ["Zissimopoulos", "Vassilis", ""]]}, {"id": "1004.4641", "submitter": "Daniel Roche", "authors": "Daniel S. Roche", "title": "Chunky and Equal-Spaced Polynomial Multiplication", "comments": "23 Pages, pdflatex, accepted to Journal of Symbolic Computation (JSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the product of two polynomials is an essential and basic problem in\ncomputer algebra. While most previous results have focused on the worst-case\ncomplexity, we instead employ the technique of adaptive analysis to give an\nimprovement in many \"easy\" cases. We present two adaptive measures and methods\nfor polynomial multiplication, and also show how to effectively combine them to\ngain both advantages. One useful feature of these algorithms is that they\nessentially provide a gradient between existing \"sparse\" and \"dense\" methods.\nWe prove that these approaches provide significant improvements in many cases\nbut in the worst case are still comparable to the fastest existing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 20:12:37 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2010 19:06:11 GMT"}], "update_date": "2010-07-20", "authors_parsed": [["Roche", "Daniel S.", ""]]}, {"id": "1004.4708", "submitter": "Michael Goodrich", "authors": "Michael T. Goodrich", "title": "Simulating Parallel Algorithms in the MapReduce Framework with\n  Applications to Parallel Computational Geometry", "comments": "Version of paper appearing in MASSIVE 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe efficient MapReduce simulations of parallel\nalgorithms specified in the BSP and PRAM models. We also provide some\napplications of these simulation results to problems in parallel computational\ngeometry for the MapReduce framework, which result in efficient MapReduce\nalgorithms for sorting, 1-dimensional all nearest-neighbors, 2-dimensional\nconvex hulls, 3-dimensional convex hulls, and fixed-dimensional linear\nprogramming. For the case when reducers can have a buffer size of\n$B=O(n^\\epsilon)$, for a small constant $\\epsilon>0$, all of our MapReduce\nalgorithms for these applications run in a constant number of rounds and have a\nlinear-sized message complexity, with high probability, while guaranteeing with\nhigh probability that all reducer lists are of size $O(B)$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 04:24:45 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Goodrich", "Michael T.", ""]]}, {"id": "1004.4710", "submitter": "Richard Brent", "authors": "Richard P. Brent and Paul Zimmermann", "title": "Modern Computer Arithmetic (version 0.5.1)", "comments": "Preliminary version of a book to be published by Cambridge University\n  Press. xvi+247 pages. Cite as \"Modern Computer Arithmetic, Version 0.5.1, 5\n  March 2010\". For further details, updates and errata see\n  http://wwwmaths.anu.edu.au/~brent/pub/pub226.html or\n  http://www.loria.fr/~zimmerma/mca/pub226.html", "journal-ref": "Cambridge Monographs on Computational and Applied Mathematics (No.\n  18), Cambridge University Press, November 2010, 236 pages", "doi": "10.1017/cbo9780511921698.001", "report-no": null, "categories": "cs.DS cs.NA math.NA math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a draft of a book about algorithms for performing arithmetic, and\ntheir implementation on modern computers. We are concerned with software more\nthan hardware - we do not cover computer architecture or the design of computer\nhardware. Instead we focus on algorithms for efficiently performing arithmetic\noperations such as addition, multiplication and division, and their connections\nto topics such as modular arithmetic, greatest common divisors, the Fast\nFourier Transform (FFT), and the computation of elementary and special\nfunctions. The algorithms that we present are mainly intended for\narbitrary-precision arithmetic. They are not limited by the computer word size,\nonly by the memory and time available for the computation. We consider both\ninteger and real (floating-point) computations. The book is divided into four\nmain chapters, plus an appendix. Our aim is to present the latest developments\nin a concise manner. At the same time, we provide a self-contained introduction\nfor the reader who is not an expert in the field, and exercises at the end of\neach chapter. Chapter titles are: 1, Integer Arithmetic; 2, Modular Arithmetic\nand the FFT; 3, Floating-Point Arithmetic; 4, Elementary and Special Function\nEvaluation; 5 (Appendix), Implementations and Pointers. The book also contains\na bibliography of 236 entries, index, summary of notation, and summary of\ncomplexities.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 04:42:33 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Brent", "Richard P.", ""], ["Zimmermann", "Paul", ""]]}, {"id": "1004.4864", "submitter": "Kaushik Sinha", "authors": "Mikhail Belkin and Kaushik Sinha", "title": "Polynomial Learning of Distribution Families", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of polynomial learnability of probability distributions,\nparticularly Gaussian mixture distributions, has recently received significant\nattention in theoretical computer science and machine learning. However,\ndespite major progress, the general question of polynomial learnability of\nGaussian mixture distributions still remained open. The current work resolves\nthe question of polynomial learnability for Gaussian mixtures in high dimension\nwith an arbitrary fixed number of components. The result on learning Gaussian\nmixtures relies on an analysis of distributions belonging to what we call\n\"polynomial families\" in low dimension. These families are characterized by\ntheir moments being polynomial in parameters and include almost all common\nprobability distributions as well as their mixtures and products. Using tools\nfrom real algebraic geometry, we show that parameters of any distribution\nbelonging to such a family can be learned in polynomial time and using a\npolynomial number of sample points. The result on learning polynomial families\nis quite general and is of independent interest. To estimate parameters of a\nGaussian mixture distribution in high dimensions, we provide a deterministic\nalgorithm for dimensionality reduction. This allows us to reduce learning a\nhigh-dimensional mixture to a polynomial number of parameter estimations in low\ndimension. Combining this reduction with the results on polynomial families\nyields our result on learning arbitrary Gaussian mixtures in high dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 16:59:43 GMT"}], "update_date": "2010-05-13", "authors_parsed": [["Belkin", "Mikhail", ""], ["Sinha", "Kaushik", ""]]}, {"id": "1004.4915", "submitter": "Michael Kapralov", "authors": "Ashish Goel and Michael Kapralov and Sanjeev Khanna", "title": "Graph Sparsification via Refinement Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph G'(V,E') is an \\eps-sparsification of G for some \\eps>0, if every\n(weighted) cut in G' is within (1\\pm \\eps) of the corresponding cut in G. A\ncelebrated result of Benczur and Karger shows that for every undirected graph\nG, an \\eps-sparsification with O(n\\log n/\\e^2) edges can be constructed in\nO(m\\log^2n) time. Applications to modern massive data sets often constrain\nalgorithms to use computation models that restrict random access to the input.\nThe semi-streaming model, in which the algorithm is constrained to use \\tilde\nO(n) space, has been shown to be a good abstraction for analyzing graph\nalgorithms in applications to large data sets. Recently, a semi-streaming\nalgorithm for graph sparsification was presented by Anh and Guha; the total\nrunning time of their implementation is \\Omega(mn), too large for applications\nwhere both space and time are important. In this paper, we introduce a new\ntechnique for graph sparsification, namely refinement sampling, that gives an\n\\tilde{O}(m) time semi-streaming algorithm for graph sparsification.\n  Specifically, we show that refinement sampling can be used to design a\none-pass streaming algorithm for sparsification that takes O(\\log\\log n) time\nper edge, uses O(\\log^2 n) space per node, and outputs an \\eps-sparsifier with\nO(n\\log^3 n/\\eps^2) edges. At a slightly increased space and time complexity,\nwe can reduce the sparsifier size to O(n \\log n/\\e^2) edges matching the\nBenczur-Karger result, while improving upon the Benczur-Karger runtime for\nm=\\omega(n\\log^3 n). Finally, we show that an \\eps-sparsifier with O(n \\log\nn/\\eps^2) edges can be constructed in two passes over the data and O(m) time\nwhenever m =\\Omega(n^{1+\\delta}) for some constant \\delta>0. As a by-product of\nour approach, we also obtain an O(m\\log\\log n+n \\log n) time streaming\nalgorithm to compute a sparse k-connectivity certificate of a graph.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 21:05:00 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Goel", "Ashish", ""], ["Kapralov", "Michael", ""], ["Khanna", "Sanjeev", ""]]}, {"id": "1004.5010", "submitter": "Marek Cygan", "authors": "Marek Cygan and Marcin Pilipczuk and Michal Pilipczuk and Jakub Onufry\n  Wojtaszczyk", "title": "The stubborn problem is stubborn no more (a polynomial algorithm for\n  3-compatible colouring and the stubborn list partition problem)", "comments": "Full version of the paper accepted to SODA'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the driving problems in the CSP area is the Dichotomy Conjecture,\nformulated in 1993 by Feder and Vardi [STOC'93], stating that for any fixed\nrelational structure G the Constraint Satisfaction Problem CSP(G) is either\nNP--complete or polynomial time solvable. A large amount of research has gone\ninto checking various specific cases of this conjecture. One such variant which\nattracted a lot of attention in the recent years is the LIST MATRIX PARTITION\nproblem. In 2004 Cameron et al. [SODA'04] classified almost all LIST MATRIX\nPARTITION variants for matrices of size at most four. The only case which\nresisted the classification became known as the STUBBORN PROBLEM. In this paper\nwe show a result which enables us to finish the classification - thus solving a\nproblem which resisted attacks for the last six years.\n  Our approach is based on a combinatorial problem known to be at least as hard\nas the STUBBORN PROBLEM - the 3-COMPATIBLE COLOURING problem. In this problem\nwe are given a complete graph with each edge assigned one of 3 possible colours\nand we want to assign one of those 3 colours to each vertex in such a way that\nno edge has the same colour as both of its endpoints. The tractability of the\n3-COMPATIBLE COLOURING problem has been open for several years and the best\nknown algorithm prior to this paper is due to Feder et al. [SODA'05] - a\nquasipolynomial algorithm with a n^O(log n / log log n) time complexity. In\nthis paper we present a polynomial-time algorithm for the 3-COMPATIBLE\nCOLOURING problem and consequently we prove a dichotomy for the k-COMPATIBLE\nCOLOURING problem.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2010 12:20:52 GMT"}, {"version": "v2", "created": "Fri, 15 Oct 2010 10:41:28 GMT"}], "update_date": "2010-11-13", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""], ["Pilipczuk", "Michal", ""], ["Wojtaszczyk", "Jakub Onufry", ""]]}, {"id": "1004.5012", "submitter": "Marcin Pilipczuk", "authors": "Marek Cygan and Marcin Pilipczuk", "title": "Bandwidth and Distortion Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we merge recent developments on exact algorithms for finding an\nordering of vertices of a given graph that minimizes bandwidth (the BANDWIDTH\nproblem) and for finding an embedding of a given graph into a line that\nminimizes distortion (the DISTORTION problem). For both problems we develop\nalgorithms that work in O(9.363^n) time and polynomial space. For BANDWIDTH,\nthis improves O^*(10^n) algorithm by Feige and Kilian from 2000, for DISTORTION\nthis is the first polynomial space exact algorithm that works in O(c^n) time we\nare aware of. As a byproduct, we enhance the O(5^{n+o(n)})-time and\nO^*(2^n)-space algorithm for DISTORTION by Fomin et al. to an algorithm working\nin O(4.383^n) time and space.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2010 12:29:28 GMT"}], "update_date": "2010-04-29", "authors_parsed": [["Cygan", "Marek", ""], ["Pilipczuk", "Marcin", ""]]}, {"id": "1004.5179", "submitter": "Mark Wilde", "authors": "Monireh Houshmand, Saied Hosseini-Khayat, and Mark M. Wilde", "title": "Minimal memory requirements for pearl-necklace encoders of quantum\n  convolutional codes", "comments": "30 pages, 9 figures, Accepted for publication in the IEEE\n  Transactions on Computers", "journal-ref": "IEEE Transactions on Computers vol. 61, no. 3, pages 299-312\n  (March 2012)", "doi": "10.1109/TC.2010.226", "report-no": null, "categories": "quant-ph cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major goals in quantum information processing is to reduce the\noverhead associated with the practical implementation of quantum protocols, and\noften, routines for quantum error correction account for most of this overhead.\nA particular technique for quantum error correction that may be useful for\nprotecting a stream of quantum information is quantum convolutional coding. The\nencoder for a quantum convolutional code has a representation as a\nconvolutional encoder or as a \"pearl-necklace\" encoder. In the pearl-necklace\nrepresentation, it has not been particularly clear in the research literature\nhow much quantum memory such an encoder would require for implementation. Here,\nwe offer an algorithm that answers this question. The algorithm first\nconstructs a weighted, directed acyclic graph where each vertex of the graph\ncorresponds to a gate string in the pearl-necklace encoder, and each path\nthrough the graph represents a path through non-commuting gates in the encoder.\nWe show that the weight of the longest path through the graph is equal to the\nminimal amount of memory needed to implement the encoder. A dynamic programming\nsearch through this graph determines the longest path. The running time for the\nconstruction of the graph and search through it is quadratic in the number of\ngate strings in the pearl-necklace encoder.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 03:44:13 GMT"}, {"version": "v2", "created": "Mon, 1 Nov 2010 13:56:06 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Houshmand", "Monireh", ""], ["Hosseini-Khayat", "Saied", ""], ["Wilde", "Mark M.", ""]]}, {"id": "1004.5186", "submitter": "Ilya Safro", "authors": "Ilya Safro and Boris Temkin", "title": "Multiscale approach for the network compression-friendly ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We present a fast multiscale approach for the network minimum logarithmic\narrangement problem. This type of arrangement plays an important role in a\nnetwork compression and fast node/link access operations. The algorithm is of\nlinear complexity and exhibits good scalability which makes it practical and\nattractive for using on large-scale instances. Its effectiveness is\ndemonstrated on a large set of real-life networks. These networks with\ncorresponding best-known minimization results are suggested as an open\nbenchmark for a research community to evaluate new methods for this problem.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 05:31:40 GMT"}], "update_date": "2010-04-30", "authors_parsed": [["Safro", "Ilya", ""], ["Temkin", "Boris", ""]]}, {"id": "1004.5285", "submitter": "Guillaume Cheze", "authors": "Guillaume Ch\\`eze (IMT)", "title": "Nearly Optimal Algorithms for the Decomposition of Multivariate Rational\n  Functions and the Extended L\\\"uroth's Theorem", "comments": null, "journal-ref": "Journal of Complexity 26, 4 (2010) 344-363", "doi": "10.1016/j.jco.2010.05.001", "report-no": null, "categories": "cs.SC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extended L\\\"uroth's Theorem says that if the transcendence degree of\n$\\KK(\\mathsf{f}_1,\\dots,\\mathsf{f}_m)/\\KK$ is 1 then there exists $f \\in\n\\KK(\\underline{X})$ such that $\\KK(\\mathsf{f}_1,\\dots,\\mathsf{f}_m)$ is equal\nto $\\KK(f)$. In this paper we show how to compute $f$ with a probabilistic\nalgorithm. We also describe a probabilistic and a deterministic algorithm for\nthe decomposition of multivariate rational functions. The probabilistic\nalgorithms proposed in this paper are softly optimal when $n$ is fixed and $d$\ntends to infinity. We also give an indecomposability test based on gcd\ncomputations and Newton's polytope. In the last section, we show that we get a\npolynomial time algorithm, with a minor modification in the exponential time\ndecomposition algorithm proposed by Gutierez-Rubio-Sevilla in 2001.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 13:46:26 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Ch\u00e8ze", "Guillaume", "", "IMT"]]}, {"id": "1004.5437", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Parallel algorithms in linear algebra", "comments": "17 pages. An old Technical Report, submitted for archival purposes.\n  For further details see http://wwwmaths.anu.edu.au/~brent/pub/pub128.html", "journal-ref": "Algorithms and Architectures: Proceedings of the Second NEC\n  Research Symposium (edited by T. Ishiguro), SIAM, Philadelphia, 1993, 54-72", "doi": null, "report-no": "Technical Report TR-CS-91-06, Computer Sciences Laboratory,\n  Australian National University, Canberra, August 1991, 17 pages", "categories": "cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an introduction to algorithms for fundamental linear\nalgebra problems on various parallel computer architectures, with the emphasis\non distributed-memory MIMD machines. To illustrate the basic concepts and key\nissues, we consider the problem of parallel solution of a nonsingular linear\nsystem by Gaussian elimination with partial pivoting. This problem has come to\nbe regarded as a benchmark for the performance of parallel machines. We\nconsider its appropriateness as a benchmark, its communication requirements,\nand schemes for data distribution to facilitate communication and load\nbalancing. In addition, we describe some parallel algorithms for orthogonal\n(QR) factorization and the singular value decomposition (SVD).\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 02:30:36 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.5466", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "On computing factors of cyclotomic polynomials", "comments": "21 pages. An old Technical Report, submitted for archival purposes.\n  For further details, see http://wwwmaths.anu.edu.au/~brent/pub/pub135.html", "journal-ref": "Mathematics of Computation 61 (1993), 131-149.", "doi": null, "report-no": "Technical Report TR-CS-92-13, Department of Computer Science,\n  Australian National University, September 1992, 21 pages.", "categories": "math.NT cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For odd square-free n > 1 the n-th cyclotomic polynomial satisfies an\nidentity of Gauss. There are similar identity of Aurifeuille, Le Lasseur and\nLucas. These identities all involve certain polynomials with integer\ncoefficients. We show how these coefficients can be computed by simple\nalgorithms which require O(n^2) arithmetic operations and work over the\nintegers. We also give explicit formulae and generating functions for the\npolynomials, and illustrate the application to integer factorization with some\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 08:18:42 GMT"}], "update_date": "2010-05-03", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1004.5600", "submitter": "Aleksandra Korolova", "authors": "Ashwin Machanavajjhala, Aleksandra Korolova, Atish Das Sarma", "title": "On the (Im)possibility of Preserving Utility and Privacy in Personalized\n  Social Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent surge of social networks like Facebook, new forms of\nrecommendations have become possible -- personalized recommendations of ads,\ncontent, and even new social and product connections based on one's social\ninteractions. In this paper, we study whether \"social recommendations\", or\nrecommendations that utilize a user's social network, can be made without\ndisclosing sensitive links between users. More precisely, we quantify the loss\nin utility when existing recommendation algorithms are modified to satisfy a\nstrong notion of privacy called differential privacy. We propose lower bounds\non the minimum loss in utility for any recommendation algorithm that is\ndifferentially private. We also propose two recommendation algorithms that\nsatisfy differential privacy, analyze their performance in comparison to the\nlower bound, both analytically and experimentally, and show that good private\nsocial recommendations are feasible only for a few users in the social network\nor for a lenient setting of privacy parameters.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 19:42:14 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Machanavajjhala", "Ashwin", ""], ["Korolova", "Aleksandra", ""], ["Sarma", "Atish Das", ""]]}]